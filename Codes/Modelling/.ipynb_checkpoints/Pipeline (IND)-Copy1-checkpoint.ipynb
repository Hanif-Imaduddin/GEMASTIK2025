{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84e85f11-1e2f-444b-8244-3dddc33e19b0",
   "metadata": {},
   "source": [
    "## Libraries and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31107097-d45f-47a2-9a5b-2baf44b8e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hanif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hanif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired,MaximalMarginalRelevance\n",
    "from bertopic.representation import OpenAI as OpenAI_BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import optuna\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "931982be-8821-4b85-94c2-1a57b67b5386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4eee1db6-ca58-4278-a12a-679370b82111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemover import StopWordRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e386210-97f7-47fc-9b7b-063b7ae28ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "def summarize_topic(keywords, docs):\n",
    "    prompt = f\"Saya mempunyai sebuah topik terkait literatur ilmiah terkait riset dan inovasi kecerdasan buatan yang dapat di-deskripsikan dengan beberapa kata kunci berikut: {keywords}\\n\\n\"\n",
    "    prompt += \"Di dialam topik ini, dokumen-dokumen berikut yang merupakan abstrak dari literatur ilmiah merupakan sebagian kecil namun representatif dari semua dokumen dalam topik ini:\\n\\n\"\n",
    "    prompt += \"\\n\\n\".join(docs[:5])  # ambil 5 dokumen saja\n",
    "    prompt += 'Berdasarkan informasi di atas, deskripsikan topik ini dan berikan label berupa satu kaliamat yang representatif terhadap topik ini dengan format JSON berikut:\\n {\"label_topik\":<label> , \"deskripsi\":<deskripsi>}'\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-2024-08-06\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": 'Kamu adalah asisten akademik yang merangkum topik. Anda adalah model yang hanya menjawab dalam format JSON valid. Jangan beri penjelasan. Berikan hanya output seperti ini: {\"label_topik\": \"...\", \"deskripsi\": \"...\"}'},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=300\n",
    "    )\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        match = re.search(r'\\{.*?\\}', content, re.DOTALL)\n",
    "        json_str = match.group(0)\n",
    "        result = json.loads(json_str)\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"JSON tidak valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51633753-2520-42cf-b110-efe9bcb42182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hanif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hanif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4eff65-c287-4075-876a-3e93537b50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class StopWatch():\n",
    "    def __init__(self):\n",
    "        self.__start_time = 0\n",
    "        self.__end_time = 0\n",
    "        self.__elapsed_time = 0\n",
    "    def start(self):\n",
    "        self.__start_time = time.time()\n",
    "        self.__end_time = 0\n",
    "    def stop(self):\n",
    "        self.__end_time = time.time()\n",
    "        self.__elapsed_time = self.__end_time - self.__start_time\n",
    "    def print(self):\n",
    "        detik = self.__elapsed_time\n",
    "\n",
    "        jam = detik // 3600\n",
    "        detik = detik % 3600\n",
    "\n",
    "        menit = detik // 60\n",
    "        detik = detik % 60\n",
    "\n",
    "        print('Waktu eksekusi: ',end='')\n",
    "        if jam != 0:\n",
    "            print(int(jam),'jam ',end='')\n",
    "        if menit != 0:\n",
    "            print(int(menit),'menit',end='')\n",
    "        print(f' {detik:.2f}','detik.')\n",
    "    def get_elapsed_time(self):\n",
    "        return self.__elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1079471c-5370-4279-a9ff-44707e41eb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StopWordRemoverFactory()\n",
    "stopword_remover = factory.create_stop_word_remover()  \n",
    "repeated_phrases = [\"kecerdasan buatan\"]\n",
    "\n",
    "additional_stop_words = ['ai']\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for word in additional_stop_words:\n",
    "    stop_words.add(word)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    \n",
    "    # URL removal\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Punctuation removal\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove repeated phrases\n",
    "    for phrase in repeated_phrases:\n",
    "        text = re.sub(phrase,\"\",text)\n",
    "\n",
    "    # Remove duplicate white space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    text = stopword_remover.remove(text) \n",
    "\n",
    "     # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Stopword removal\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ae5e7d6-7369-4ac1-8744-d7721420c558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(x):\n",
    "    if x[-8:]=='Collapse':\n",
    "        return x[:-8]\n",
    "    elif x[-9:] == ' … Expand':\n",
    "        return x[:-9]\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab0fdffb-d5ac-43c2-939d-08f4e1f6519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nTopic #{topic_idx + 1}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf8f0888-dba0-4555-8bf3-b63a2829b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_words(model, vectorizer):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics_words = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-11:-1]\n",
    "        top_words = [feature_names[i] for i in top_features_ind]\n",
    "        topics_words.append(top_words)\n",
    "\n",
    "    return topics_words\n",
    "\n",
    "def get_bert_topics_words(topic_model):\n",
    "    bert_topics_words = []\n",
    "    for topic_id in topic_model.get_topics().keys():\n",
    "        if topic_id == -1:\n",
    "            continue\n",
    "        topic_words = [word for word, _ in topic_model.get_topic(topic_id)]\n",
    "        bert_topics_words.append(topic_words)\n",
    "    return bert_topics_words\n",
    "\n",
    "def get_coherence(topics_words):\n",
    "    sw = StopWatch()\n",
    "    sw.start()\n",
    "    \n",
    "    coherence = CoherenceModel(\n",
    "                    topics=topics_words,\n",
    "                    texts=tokenized_docs,\n",
    "                    dictionary=dictionary,\n",
    "                    coherence='c_v'\n",
    "                ).get_coherence()\n",
    "\n",
    "    sw.stop()\n",
    "    sw.print()\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbf29648-bf01-413e-a12e-4bca8e4f765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset_dir' : '../../Datasets/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321737d-1bdf-49d2-86dd-3d78d69372b3",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62c072d-c252-44b4-84eb-49f2683f3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config['dataset_dir']+'artikel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be5a118-3d6c-49fc-9400-2548c7da9ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 877 entries, 0 to 876\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     877 non-null    object\n",
      " 1   abstract  877 non-null    object\n",
      " 2   pubdate   877 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 20.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7065f6f-4901-4f93-8ec1-e3fe607f750d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>pubdate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pemanfaatan Kecerdasan Buatan dalam Analisis R...</td>\n",
       "      <td>Purpose – Paper ini bertujuan untuk mengevalua...</td>\n",
       "      <td>31 October 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simulasi Perilaku Kecerdasan Buatan dengan Per...</td>\n",
       "      <td>Permainan Multiplayer Online Battle Arena (MOB...</td>\n",
       "      <td>15 July 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ketika Kecerdasan Buatan Menjadi Alat Kecurang...</td>\n",
       "      <td>This study aims to explore the influence of pe...</td>\n",
       "      <td>31 October 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANALISIS KECERDASAN BUATAN CHATGPT DALAM PENYE...</td>\n",
       "      <td>Penggunaan teknologi merupakan hal yang tidak ...</td>\n",
       "      <td>31 July 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pemanfaatan Kecerdasan Buatan (Ai) Dalam Menin...</td>\n",
       "      <td>Usaha Mikro, Kecil, dan Menengah (UMKM) play a...</td>\n",
       "      <td>22 December 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Pemanfaatan Kecerdasan Buatan dalam Analisis R...   \n",
       "1  Simulasi Perilaku Kecerdasan Buatan dengan Per...   \n",
       "2  Ketika Kecerdasan Buatan Menjadi Alat Kecurang...   \n",
       "3  ANALISIS KECERDASAN BUATAN CHATGPT DALAM PENYE...   \n",
       "4  Pemanfaatan Kecerdasan Buatan (Ai) Dalam Menin...   \n",
       "\n",
       "                                            abstract           pubdate  \n",
       "0  Purpose – Paper ini bertujuan untuk mengevalua...   31 October 2024  \n",
       "1  Permainan Multiplayer Online Battle Arena (MOB...      15 July 2024  \n",
       "2  This study aims to explore the influence of pe...   31 October 2024  \n",
       "3  Penggunaan teknologi merupakan hal yang tidak ...      31 July 2023  \n",
       "4  Usaha Mikro, Kecil, dan Menengah (UMKM) play a...  22 December 2023  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "008443a8-8176-4c1b-9717-07dbe1932ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pub_year'] = df['pubdate'].apply(lambda x: x[-4:]).astype(int)\n",
    "df.drop(columns=['pubdate'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4789f96b-5c46-4a3e-bcc8-8f028ff939d5",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engineering #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "597f4e0a-6016-423d-9c10-77df110ceff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tm = df.copy()\n",
    "df_tm['text'] = df_tm['title']+\" \"+df_tm['abstract']\n",
    "df_tm['text'] = df_tm['text'].apply(trim)\n",
    "\n",
    "df_tm = df_tm[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56636a1a-6dc4-43c3-ab42-a8286976d4c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waktu eksekusi:  0.99 detik.\n"
     ]
    }
   ],
   "source": [
    "sw = StopWatch()\n",
    "sw.start()\n",
    "df_tm['processed_text'] = df_tm['text'].apply(preprocess_text)\n",
    "sw.stop()\n",
    "sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd682d3-c146-4874-9a67-781a5dd0cb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pemanfaatan Kecerdasan Buatan dalam Analisis R...</td>\n",
       "      <td>pemanfaatan analisis return risiko saham lq45 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simulasi Perilaku Kecerdasan Buatan dengan Per...</td>\n",
       "      <td>simulasi perilaku peran support enchanter meng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ketika Kecerdasan Buatan Menjadi Alat Kecurang...</td>\n",
       "      <td>menjadi alat kecurangan tingkat lanjut tantang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANALISIS KECERDASAN BUATAN CHATGPT DALAM PENYE...</td>\n",
       "      <td>analisis chatgpt penyelesaian soal fisika berg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pemanfaatan Kecerdasan Buatan (Ai) Dalam Menin...</td>\n",
       "      <td>pemanfaatan meningkatkan efesiensi pengembanga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Pemanfaatan Kecerdasan Buatan dalam Analisis R...   \n",
       "1  Simulasi Perilaku Kecerdasan Buatan dengan Per...   \n",
       "2  Ketika Kecerdasan Buatan Menjadi Alat Kecurang...   \n",
       "3  ANALISIS KECERDASAN BUATAN CHATGPT DALAM PENYE...   \n",
       "4  Pemanfaatan Kecerdasan Buatan (Ai) Dalam Menin...   \n",
       "\n",
       "                                      processed_text  \n",
       "0  pemanfaatan analisis return risiko saham lq45 ...  \n",
       "1  simulasi perilaku peran support enchanter meng...  \n",
       "2  menjadi alat kecurangan tingkat lanjut tantang...  \n",
       "3  analisis chatgpt penyelesaian soal fisika berg...  \n",
       "4  pemanfaatan meningkatkan efesiensi pengembanga...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f05348-c939-4c76-b7a8-7f7b8f8f69be",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "302a083e-f2be-477c-84e9-cdd5120ea762",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_tm['processed_text'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d72145e1-d846-44bd-845a-1904b7d543d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_keyword(vectorizer,model):\n",
    "    componenets = model.components_\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    top_n = 10\n",
    "    keywords_per_topic = [\n",
    "        [feature_names[i] for i in topic.argsort()[:-top_n - 1:-1]] for topic in componenets\n",
    "    ]\n",
    "    return keywords_per_topic\n",
    "\n",
    "def select_representative_docs(model,topics):\n",
    "    topics_docs = {}\n",
    "    for topic_idx in range(model.n_components):\n",
    "        topic_strength = topics[:,topic_idx]\n",
    "        top_doc_indices = topic_strength.argsort()[::-1][:5]\n",
    "        topics_docs[topic_idx] = [docs[i] for i in top_doc_indices]\n",
    "    return topics_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0754ce3a-92ab-431a-854c-9b411a7536bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(model, topics, vectorizer):\n",
    "    r_docs = select_representative_docs(model,topics)\n",
    "    top_key = get_top_keyword(vectorizer,model)\n",
    "    topic_desc = {}\n",
    "    for topic_idx, keywords in enumerate(top_key):\n",
    "        docs_sample = r_docs[topic_idx]\n",
    "        result = summarize_topic(keywords, docs_sample)\n",
    "        topic_desc[topic_idx] = result\n",
    "    return topic_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf542d7b-5a6b-44e9-b8f5-2f08b622eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "tfidf = tfidf_vectorizer.fit_transform(df_tm['processed_text'])\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features=1000)\n",
    "count = count_vectorizer.fit_transform(df_tm['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb8bdd-7d97-41fd-b10d-7c8023602960",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ccc5ece-c762-4acf-9152-f3189fa841e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw = StopWatch()\n",
    "# sw.start()\n",
    "# nmf_model = NMF(n_components=10, random_state=42)\n",
    "# nmf_topics = nmf_model.fit_transform(tfidf)\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b645e42-fd08-43a5-b6ec-5c48a702d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['NMF_topic'] = np.argmax(nmf_topics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "791adde0-839c-4ba3-8a0e-dd72ef3d2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['NMF_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1e3f24-9a7a-4499-8895-4f147b0132fd",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "132e8ba0-8c87-4042-ad05-090640a44e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw = StopWatch()\n",
    "# sw.start()\n",
    "# lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "# lda_topics = lda_model.fit_transform(count)\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e7bac2b-9540-44af-9b94-e27cbb0f42a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['LDA_topic'] = np.argmax(lda_topics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8fef007-7e74-49a0-acad-a27bb0df5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['LDA_topic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ddce03-08ac-43f9-b94f-4676013c6513",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98899563-2f0d-4fba-8aa2-ce9ea4ed7a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw = StopWatch()\n",
    "# sw.start()\n",
    "# lsa_model = TruncatedSVD(n_components=10, random_state=42)\n",
    "# lsa_topics = lsa_model.fit_transform(tfidf)\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b91934d-11f5-49b2-9f92-4655cb99f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['LSA_topics'] = np.argmax(lsa_topics,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ee988c2-f623-40bc-a784-412e0a379ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['LSA_topics'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caa2eb7-afd9-4cd2-968b-873a779b8198",
   "metadata": {},
   "source": [
    "### BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c663bbe6-959b-4cf7-b5ef-69cbbfd41fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare sub-models\n",
    "# embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "# umap_model = UMAP(n_components=5, n_neighbors=50, random_state=42, metric=\"cosine\", verbose=True)\n",
    "# hdbscan_model = HDBSCAN(min_samples=20, gen_min_span_tree=True, prediction_data=True, min_cluster_size=20)\n",
    "# vectorizer_model = CountVectorizer(ngram_range=(1, 3), min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b75147fd-a0e1-4883-9156-243a23afcaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Representation models\n",
    "# representation_models = {\n",
    "#     \"KeyBERTInspired\": KeyBERTInspired(),\n",
    "#     \"MMR\": MaximalMarginalRelevance(diversity=0.3),\n",
    "#     \"KeyBERT + MMR\": [KeyBERTInspired(), MaximalMarginalRelevance(diversity=0.3)]\n",
    "# }\n",
    "\n",
    "# # Fit BERTopic\n",
    "# sw = StopWatch()\n",
    "# sw.start()\n",
    "\n",
    "# topic_model= BERTopic(\n",
    "#         embedding_model=embedding_model,\n",
    "#         umap_model=umap_model,\n",
    "#         hdbscan_model=hdbscan_model,\n",
    "#         vectorizer_model=vectorizer_model,\n",
    "#         representation_model=representation_models,\n",
    "#         verbose=True\n",
    "# ).fit(docs)\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdafa2dc-b886-4563-8308-00c5f4d423a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_info = topic_model.get_topic_info()  \n",
    "# id_to_name = topic_info.set_index(\"Topic\")[\"Name\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "754e7e58-2e9e-44c1-a0dd-b6c059c86a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f420d5c-667d-46d6-9b06-e36ec3338263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "723f01d2-d153-4348-997e-2aee237976e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw = StopWatch()\n",
    "# sw.start()\n",
    "\n",
    "# bert_topics, probs = topic_model.transform(docs)\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69b708cf-3015-45e7-8c24-858c5928e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tm['BERT_topic'] = bert_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c60e95-5fe3-4db8-9e11-482761d1d520",
   "metadata": {},
   "source": [
    "### Evaluating #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc0d117b-c877-43c1-99de-f4d510f31f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisasi dokumen\n",
    "tokenized_docs = [doc.split() for doc in docs]\n",
    "\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce38cee6-00b0-40df-a637-8496079b531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_topics_words = get_topics_words(nmf_model,tfidf_vectorizer)\n",
    "# lda_topics_words = get_topics_words(lda_model,count_vectorizer)\n",
    "# lsa_topics_words = get_topics_words(lsa_model,tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f2d6bc7-8f99-497e-85ce-f0d418f0848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_topics_words = get_bert_topics_words(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "230f07ba-e9ba-4fa8-b18a-e216d2ce06ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw.start()\n",
    "\n",
    "# coherence_dict = {\n",
    "#     'Model' : [\n",
    "#         'NMF',\n",
    "#         'LDA',\n",
    "#         'LSA',\n",
    "#         'BERTopic',\n",
    "#     ],\n",
    "#     'Coherence':[\n",
    "#         get_coherence(nmf_topics_words),\n",
    "#         get_coherence(lda_topics_words),\n",
    "#         get_coherence(lsa_topics_words),\n",
    "#         get_coherence(bert_topics_words),\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50855a1f-9928-4598-8fb8-a2a9d6c0a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence_df = pd.DataFrame(coherence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b62ecae-cad3-43b5-a488-05dcfc7286c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5c8ae3-696f-4af5-9d09-98a56f935477",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ae4f0419-d4d0-414e-8d55-55fdd1c860d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.4.0'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a945db1d-63d4-4c2d-bd30-472811913823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nmf_objective(trial):\n",
    "#     params = {\n",
    "#         'n_components': trial.suggest_int('n_components', 3, 30)\n",
    "#     }\n",
    "\n",
    "#     model = NMF(**params, random_state=42)\n",
    "#     topics = model.fit_transform(tfidf)\n",
    "\n",
    "#     topics_words = get_topics_words(model, tfidf_vectorizer)\n",
    "#     return get_coherence(topics_words)\n",
    "\n",
    "# def lda_objective(trial):\n",
    "#     params = {\n",
    "#         'n_components': trial.suggest_int('n_components', 3, 30)\n",
    "#     }\n",
    "\n",
    "#     model = LatentDirichletAllocation(**params, random_state=42)\n",
    "#     topics = model.fit_transform(count)\n",
    "\n",
    "#     topics_words = get_topics_words(model, count_vectorizer)\n",
    "#     return get_coherence(topics_words)\n",
    "\n",
    "# def lsa_objective(trial):\n",
    "#     params = {\n",
    "#         'n_components': trial.suggest_int('n_components', 3, 30)\n",
    "#     }\n",
    "\n",
    "#     model = TruncatedSVD(**params, random_state=42)\n",
    "#     topics = model.fit_transform(tfidf)\n",
    "\n",
    "#     topics_words = get_topics_words(model, tfidf_vectorizer)\n",
    "#     return get_coherence(topics_words)\n",
    "\n",
    "# def bert_objective(trial):\n",
    "#     embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "#     umap_model = UMAP(n_components=5, n_neighbors=50, random_state=42, metric=\"cosine\", verbose=True)\n",
    "#     hdbscan_model = HDBSCAN(min_samples=20, gen_min_span_tree=True, prediction_data=True, min_cluster_size=trial.suggest_int('min_cluster_size', 3, 30))\n",
    "#     vectorizer_model = CountVectorizer(ngram_range=(1, 3), min_df=5)\n",
    "\n",
    "#     representation_models = {\n",
    "#         \"KeyBERTInspired\": KeyBERTInspired()\n",
    "#     }\n",
    "\n",
    "#     topic_model= BERTopic(\n",
    "#             embedding_model=embedding_model,\n",
    "#             umap_model=umap_model,\n",
    "#             hdbscan_model=hdbscan_model,\n",
    "#             vectorizer_model=vectorizer_model,\n",
    "#             representation_model=representation_models,\n",
    "#             verbose=True\n",
    "#     ).fit(docs)\n",
    "\n",
    "#     bert_topics_words = get_bert_topics_words(topic_model)\n",
    "#     return get_coherence(bert_topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdae7e81-e67f-4260-af2f-0f9a6aa809db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sw.start()\n",
    "\n",
    "# nmf_study = optuna.create_study(direction='maximize')\n",
    "# nmf_study.optimize(nmf_objective,n_trials=20)\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5f5ad477-8cb6-4974-82c7-43a46a17c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_parmas = {'n_components': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "90b8d3e9-3ebc-4d0d-9cfb-bdf12a046be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sw.start()\n",
    "\n",
    "# lda_study = optuna.create_study(direction='maximize')\n",
    "# lda_study.optimize(lda_objective,n_trials=20)\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1095a2fe-4039-4e8a-8c00-d52321cdbb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_params = {'n_components': 16}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "24f8dc78-d8da-4f47-8e23-bfbc4329889c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sw.start()\n",
    "\n",
    "# lsa_study = optuna.create_study(direction='maximize')\n",
    "# lsa_study.optimize(lsa_objective,n_trials=20)\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "baee7603-e798-42cc-ad50-a436568fef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_params = {'n_components': 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "35ff2f7c-c9cf-4a97-a4b7-d7497c51bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw.start()\n",
    "\n",
    "# bert_study = optuna.create_study(direction='maximize')\n",
    "# bert_study.optimize(bert_objective,n_trials=5)\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1dffb9cf-46be-43c0-9fec-be12d908b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_params = {'min_cluster_size': 22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "99c82c52-795b-4c53-ba85-e24b1cb2be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = NMF(**nmf_parmas, random_state=42)\n",
    "nmf_topics = nmf_model.fit_transform(tfidf)\n",
    "\n",
    "lda_model = LatentDirichletAllocation(**lda_params, random_state=42)\n",
    "lda_topics = lda_model.fit_transform(count)\n",
    "\n",
    "lsa_model = TruncatedSVD(**lsa_params, random_state=42)\n",
    "lsa_topics = lsa_model.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50cb4fbe-5223-450a-93f6-b6043c2fa936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:32:10,802 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd44bc9c3164e6a8ee883b399248a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:32:13,494 - BERTopic - Embedding - Completed ✓\n",
      "2025-07-26 14:32:13,495 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(angular_rp_forest=True, metric='cosine', n_components=5, n_jobs=1, n_neighbors=50, random_state=42, verbose=True)\n",
      "Sat Jul 26 14:32:13 2025 Construct fuzzy simplicial set\n",
      "Sat Jul 26 14:32:14 2025 Finding Nearest Neighbors\n",
      "Sat Jul 26 14:32:25 2025 Finished Nearest Neighbor Search\n",
      "Sat Jul 26 14:32:30 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e105dac22cf546a2a6913c65adb183d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Sat Jul 26 14:32:33 2025 Finished embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:32:33,196 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-07-26 14:32:33,197 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-07-26 14:32:33,227 - BERTopic - Cluster - Completed ✓\n",
      "2025-07-26 14:32:33,230 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-07-26 14:32:35,649 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waktu eksekusi:  25.16 detik.\n"
     ]
    }
   ],
   "source": [
    "# Prepare sub-models\n",
    "embedding_model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "umap_model = UMAP(n_components=5, n_neighbors=50, random_state=42, metric=\"cosine\", verbose=True)\n",
    "hdbscan_model = HDBSCAN(min_samples=20, gen_min_span_tree=True, prediction_data=True, min_cluster_size=22)\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), min_df=5)\n",
    "\n",
    "# Representation models\n",
    "representation_models = {\n",
    "    \"KeyBERTInspired\": KeyBERTInspired(),\n",
    "    \"MMR\": MaximalMarginalRelevance(diversity=0.3),\n",
    "    \"KeyBERT + MMR\": [KeyBERTInspired(), MaximalMarginalRelevance(diversity=0.3)]\n",
    "}\n",
    "\n",
    "# Fit BERTopic\n",
    "sw = StopWatch()\n",
    "sw.start()\n",
    "\n",
    "topic_model= BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_models,\n",
    "        verbose=True\n",
    ").fit(docs)\n",
    "\n",
    "sw.stop()\n",
    "sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e70ad773-38d2-419c-ab34-7f8a06291496",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topics_words = get_topics_words(nmf_model,tfidf_vectorizer)\n",
    "lda_topics_words = get_topics_words(lda_model,count_vectorizer)\n",
    "lsa_topics_words = get_topics_words(lsa_model,tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc5b5716-2342-48bd-bae9-275b6fc46acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_topics_words = get_bert_topics_words(topic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "72e43ff4-ee30-4e09-97bc-1c9b9abaece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sw.start()\n",
    "\n",
    "# coherence_dict = {\n",
    "#     'Model' : [\n",
    "#         'NMF',\n",
    "#         'LDA',\n",
    "#         'LSA',\n",
    "#         'BERTopic',\n",
    "#     ],\n",
    "#     'Coherence':[\n",
    "#         get_coherence(nmf_topics_words),\n",
    "#         get_coherence(lda_topics_words),\n",
    "#         get_coherence(lsa_topics_words),\n",
    "#         get_coherence(bert_topics_words),\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# sw.stop()\n",
    "# sw.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7f3d5cf-04a4-4635-8f13-db444c200864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence_df = pd.DataFrame(coherence_dict)\n",
    "# coherence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac1f20-b42e-4236-85e0-025217797c66",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f07cc-f094-44e1-a8c3-f7561b6c94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7911d3aa-0e25-4e11-a071-303140ad7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8901a6ac-79c8-490e-a726-c1219757a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_key_words = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1729bb01-6d66-4643-a1a4-29563b048280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-26 14:33:06,768 - BERTopic - WARNING: When you use `pickle` to save/load a BERTopic model,please make sure that the environments in which you saveand load the model are **exactly** the same. The version of BERTopic,its dependencies, and python need to remain the same.\n"
     ]
    }
   ],
   "source": [
    "# topic_model.save('BERTopic IND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eff37b-6440-449c-8437-14d97edf853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(nmf_model,'nmf_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c01399fc-652a-4cb9-b404-64125c3c49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe24e9ef-25ca-4e67-8f08-df9a5107ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_key_words['BERTopic'] = topic_info['Representation'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "515c812f-e285-4115-9c3a-29b88a04fc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BERTopic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[pembelajaran, pendidikan, siswa, penelitian, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[teknologi, digital, indonesia, penelitian, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[chatgpt, chatbot, informasi, penelitian, maha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[pakar, sistem, metode, kesehatan, menggunakan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[pendidikan, pembelajaran, penelitian, bahasa,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[model, citra, klasifikasi, menggunakan, machi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[algoritma, karakter, machine, satu, perilaku,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            BERTopic\n",
       "1  [pembelajaran, pendidikan, siswa, penelitian, ...\n",
       "2  [teknologi, digital, indonesia, penelitian, da...\n",
       "3  [chatgpt, chatbot, informasi, penelitian, maha...\n",
       "4  [pakar, sistem, metode, kesehatan, menggunakan...\n",
       "5  [pendidikan, pembelajaran, penelitian, bahasa,...\n",
       "6  [model, citra, klasifikasi, menggunakan, machi...\n",
       "7  [algoritma, karakter, machine, satu, perilaku,..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c2af246-ff57-467a-bbc9-25bd2a9c4288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NMF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pendidikan, pembelajaran, islam, siswa, penel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[model, data, menggunakan, klasifikasi, akuras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[digital, teknologi, data, bisnis, penelitian,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[game, state, algoritma, permainan, npc, finit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[chatgpt, mahasiswa, penelitian, penggunaan, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[pakar, penyakit, sistem, chaining, forward, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[intelligence, artificial, learning, education...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[hukum, hak, cipta, perlindungan, indonesia, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[guru, pelatihan, siswa, pembelajaran, kegiata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[chatbot, informasi, aplikasi, layanan, sistem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[audit, auditor, kualitas, keuangan, covid19, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[50, era, manusia, society, 40, industri, revo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  NMF\n",
       "0   [pendidikan, pembelajaran, islam, siswa, penel...\n",
       "1   [model, data, menggunakan, klasifikasi, akuras...\n",
       "2   [digital, teknologi, data, bisnis, penelitian,...\n",
       "3   [game, state, algoritma, permainan, npc, finit...\n",
       "4   [chatgpt, mahasiswa, penelitian, penggunaan, k...\n",
       "5   [pakar, penyakit, sistem, chaining, forward, d...\n",
       "6   [intelligence, artificial, learning, education...\n",
       "7   [hukum, hak, cipta, perlindungan, indonesia, k...\n",
       "8   [guru, pelatihan, siswa, pembelajaran, kegiata...\n",
       "9   [chatbot, informasi, aplikasi, layanan, sistem...\n",
       "10  [audit, auditor, kualitas, keuangan, covid19, ...\n",
       "11  [50, era, manusia, society, 40, industri, revo..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pd.Series(get_top_keyword(tfidf_vectorizer,nmf_model)),columns=['NMF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7908bc-4790-40b5-addd-dcdbf25be936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nmf_sum = summarize(nmf_model,nmf_topics,tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330d6d8-bae5-472f-b917-3c41b8066a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"nmf_sum.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(nmf_sum, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee59935-1463-4f26-bf8a-a4991a43196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"nmf_sum.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
