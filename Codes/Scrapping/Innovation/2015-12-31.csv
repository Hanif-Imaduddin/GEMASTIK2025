title,authors,abstract,submitted_date,pdf_link
A survey on feature weighting based K-Means algorithms,Renato Cordeiro de Amorim,"In a real-world data set there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process. With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research. This paper elaborates on the concept of feature weighting and addresses these issues by critically analysing some of the most popular, or innovative, feature weighting mechanisms based in K-Means. △ Less","22 September, 2015",https://arxiv.org/pdf/1601.03483
Disambiguation of Patent Inventors and Assignees Using High-Resolution Geolocation Data,Greg Morrison;Massimo Riccaboni;Fabio Pammolli,"Patent data represent a significant source of information on innovation and the evolution of technology through networks of citations, co-invention and co-assignment of new patents. A major obstacle to extracting useful information from this data is the problem of name disambiguation: linking alternate spellings of individuals or institutions to a single identifier to uniquely determine the parties involved in the creation of a technology. In this paper, we describe a new algorithm that uses high-resolution geolocation to disambiguate both inventor and assignees on more than 3.6 million patents found in the European Patent Office (EPO), under the Patent Cooperation treaty (PCT), and in the US Patent and Trademark Office (USPTO). We show that our algorithm has both high precision and recall in comparison to a manual disambiguation of EPO assignee names in Boston and Paris, and show it performs well for a benchmark of USPTO inventor names that can be linked to a high-resolution address (but poorly for inventors that never provided a high quality address). The most significant benefit of this work is the high quality assignee disambiguation with worldwide coverage coupled with an inventor disambiguation that is competitive with other state of the art approaches. To our knowledge this is the broadest and most accurate simultaneous disambiguation and cross-linking of the inventor and assignee names for a significant fraction of patents in these three major patent collections. △ Less","13 December, 2015",https://arxiv.org/pdf/1601.01963
Crowds for Clouds: Recent Trends in Humanities Research Infrastructures,Tobias Blanke;Conny Kristel;Laurent Romary,"Humanities have convincingly argued that they need transnational research opportunities and through the digital transformation of their disciplines also have the means to proceed with it on an up to now unknown scale. The digital transformation of research and its resources means that many of the artifacts, documents, materials, etc. that interest humanities research can now be combined in new and innovative ways. Due to the digital transformations, (big) data and information have become central to the study of culture and society. Humanities research infrastructures manage, organise and distribute this kind of information and many more data objects as they becomes relevant for social and cultural research. △ Less","27 December, 2015",https://arxiv.org/pdf/1601.00533
Measuring Social Well Being in The Big Data Era: Asking or Listening?,Matteo Curti;Stefano Iacus;Giuseppe Porro;Elena Siletti,"The literature on well being measurement seems to suggest that ""asking"" for a self-evaluation is the only way to estimate a complete and reliable measure of well being. At the same time ""not asking"" is the only way to avoid biased evaluations due to self-reporting. Here we propose a method for estimating the welfare perception of a community simply ""listening"" to the conversations on Social Network Sites. The Social Well Being Index (SWBI) and its components are proposed through to an innovative technique of supervised sentiment analysis called iSA which scales to any language and big data. As main methodological advantages, this approach can estimate several aspects of social well being directly from self-declared perceptions, instead of approximating it through objective (but partial) quantitative variables like GDP; moreover self-perceptions of welfare are spontaneous and not obtained as answers to explicit questions that are proved to bias the result. As an application we evaluate the SWBI in Italy through the period 2012-2015 through the analysis of more than 143 millions of tweets. △ Less","22 December, 2015",https://arxiv.org/pdf/1512.07271
A Critical Survey Of Privacy Infrastructures,B H Priyanka;Ravi Prakash,"Over the last two decades, the scale and complexity of the Internet and its associated technologies built on the World Wide Web has grown exponentially with access to Internet as a facility occupying a prime place with other amenities of modern lives. In years to come, usage of Internet may unravel more pleasant surprises for us as far as novelty in its usage is concerned. As a democratic function of Internet, and relying on the open model on which it has been built, there has been concerted efforts in the direction of privacy protection and use of privacy enhancing tools which have gained tangible traction. Innovation in use of VPN, TLS/SSL and cryptographic tools are a testimony to it. Another popular tool is Tor, which has gained widespread popularity as it is being increasingly used by anonymity seeking users to effectively maintain their discretion while surfing the web. However, there is a darker side to increased proliferation of Internet in our everyday routine. We are certainly not living in a utopian age and there are potentials of misuse of Internet as well. Across every nook and cranny of Internet's sprawling virtual world, there are cyber criminals lurking n dangerous alleys to use the very same Internet as malevolent tool to abuse it and cause financial, physical and social harm to ordinary people. Failing to manage the widespread spawning of World Wide Web has rendered it weak against misuse. In last few decades especially, Internet has been inundated with malware, ransomware, viruses, Trojans, illegal spy tools and what not created with malignant sentiments. In this paper, we will analyze few of the subverting privacy infrastructures. △ Less","20 December, 2015",https://arxiv.org/pdf/1512.07207
Energy Consumption Forecasting for Smart Meters,Anshul Bansal;Susheel Kaushik Rompikuntla;Jaganadh Gopinadhan;Amanpreet Kaur;Zahoor Ahamed Kazi,"Earth, water, air, food, shelter and energy are essential factors required for human being to survive on the planet. Among this energy plays a key role in our day to day living including giving lighting, cooling and heating of shelter, preparation of food. Due to this interdependency, energy, specifically electricity, production and distribution became a high tech industry. Unlike other industries, the key differentiator of electricity industry is the product itself. It can be produced but cannot be stored for future; production and consumption happen almost in near real-time. This particular peculiarity of the industry is the key driver for Machine Learning and Data Science based innovations in this industry. There is always a gap between the demand and supply in the electricity market across the globe. To fill the gap and improve the service efficiency through providing necessary supply to the market, commercial as well as federal electricity companies employ forecasting techniques to predict the future demand and try to meet the demand and provide curtailment guidelines to optimise the electricity consumption/demand. In this paper the authors examine the application of Machine Learning algorithms, specifically Boosted Decision Tree Regression, to the modelling and forecasting of energy consumption for smart meters. The data used for this exercise is obtained from DECC data website. Along with this data, the methodology has been tested in Smart Meter data obtained from EMA Singapore. This paper focuses on feature engineering for time series forecasting using regression algorithms and deriving a methodology to create personalised electricity plans offers for household users based on usage history. △ Less","18 December, 2015",https://arxiv.org/pdf/1512.05979
Sampling and Reconstruction of Shapes with Algebraic Boundaries,Mitra Fatemi;Arash Amini;Martin Vetterli,"We present a sampling theory for a class of binary images with finite rate of innovation (FRI). Every image in our model is the restriction of \mathds{1}_{\{p\leq0\}} to the image plane, where \mathds{1} denotes the indicator function and p is some real bivariate polynomial. This particularly means that the boundaries in the image form a subset of an algebraic curve with the implicit polynomial p. We show that the image parameters --i.e., the polynomial coefficients-- satisfy a set of linear annihilation equations with the coefficients being the image moments. The inherent sensitivity of the moments to noise makes the reconstruction process numerically unstable and narrows the choice of the sampling kernels to polynomial reproducing kernels. As a remedy to these problems, we replace conventional moments with more stable \emph{generalized moments} that are adjusted to the given sampling kernel. The benefits are threefold: (1) it relaxes the requirements on the sampling kernels, (2) produces annihilation equations that are robust at numerical precision, and (3) extends the results to images with unbounded boundaries. We further reduce the sensitivity of the reconstruction process to noise by taking into account the sign of the polynomial at certain points, and sequentially enforcing measurement consistency. We consider various numerical experiments to demonstrate the performance of our algorithm in reconstructing binary images, including low to moderate noise levels and a range of realistic sampling kernels. △ Less","14 December, 2015",https://arxiv.org/pdf/1512.04388
BYOD and the Mobile Enterprise - Organisational challenges and solutions to adopt BYOD,Andre Sobers,"Bring Your Own Device, also known under the term BOYD refers to the trend in employees bringing their personal mobile devices into organisations to use as a primary device for their daily work activities. With the rapid development in computing technology in smartphones and tablet computers and innovations in mobile software and applications, mobile devices are becoming ever more powerful tools for consumers to access information. Consumers are becoming more inseparable from their personal mobile devices and development in mobile technologies within the consumer space has led to the significance of Consumerization. Enterprises everywhere want to introduce BYOD strategies to improve mobility and productivity of their employees. However making the necessary organizational changes to adopt BYOD may require a shift away from centralized systems towards more open enterprise systems and this change can present challenges to enterprises in particular over security, control, technology and policy to the traditional IT model within organisations. This paper explores some of the present challenges and solutions in relation to mobile security, technology and policy that enterprise systems within organisations can encounter. This paper also reviews real-life studies where such changes were made in organisations aiming to implement BYOD. This paper proposes a mobile enterprise model that aims to address security concerns and the challenges of technology and policy change. This paper ends with looking ahead to the future of mobile enterprise systems. △ Less","12 December, 2015",https://arxiv.org/pdf/1512.03911
Motion model transitions in GPS-IMU sensor fusion for user tracking in augmented reality,Erkan Bostanci,"Finding the position of the user is an important processing step for augmented reality (AR) applications. This paper investigates the use of different motion models in order to choose the most suitable one, and eventually reduce the Kalman filter errors in sensor fusion for such applications where the accuracy of user tracking is crucial. A Deterministic Finite Automaton (DFA) was employed using the innovation parameters of the filter. Results show that the approach presented here reduces the filter error compared to a static model and prevents filter divergence. The approach was tested on a simple AR game in order to justify the accuracy and performance of the algorithm. △ Less","9 December, 2015",https://arxiv.org/pdf/1512.02758
Emerging Cloud Computing Security Threats,Kamal Ahmat,Cloud computing is one of the latest emerging innovations of the modern internet and technological landscape. With everyone from the White house to major online technological leaders like Amazon and Google using or offering cloud computing services it is truly presents itself as an exciting and innovative method to store and use data on the internet. △ Less,"5 December, 2015",https://arxiv.org/pdf/1512.01701
PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by Comparable Corpora,Krzysztof Wołk;Krzysztof Marasek,"In this paper, we attempt to improve Statistical Machine Translation (SMT) systems on a very diverse set of language pairs (in both directions): Czech - English, Vietnamese - English, French - English and German - English. To accomplish this, we performed translation model training, created adaptations of training settings for each language pair, and obtained comparable corpora for our SMT systems. Innovative tools and data adaptation techniques were employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign were used to train language models, and to develop, tune, and test the system. In addition, we prepared Wikipedia-based comparable corpora for use with our SMT system. This data was specified as permissible for the IWSLT 2015 evaluation. We explored the use of domain adaptation techniques, symmetrized word alignment models, the unsupervised transliteration models and the KenLM language modeling tool. To evaluate the effects of different preparations on translation results, we conducted experiments and used the BLEU, NIST and TER metrics. Our results indicate that our approach produced a positive impact on SMT quality. △ Less","5 December, 2015",https://arxiv.org/pdf/1512.01639
On the Energy Efficiency Prospects of Network Function Virtualization,Rashid Mijumbi,"Network Function Virtualization (NFV) has recently received significant attention as an innovative way of deploying network services. By decoupling network functions from the physical equipment on which they run, NFV has been proposed as passage towards service agility, better time-to-market, and reduced Capital Expenses (CAPEX) and Operating Expenses (OPEX). One of the main selling points of NFV is its promise for better energy efficiency resulting from consolidation of resources as well as their more dynamic utilization. However, there are currently no studies or implementations which attach values to energy savings that can be expected, which could make it hard for Telecommunication Service Providers (TSPs) to make investment decisions. In this paper, we utilize Bell Labs' GWATT tool to estimate the energy savings that could result from the three main NFV use cases Virtualized Evolved Packet Core (VEPC), Virtualized Customer Premises Equipment (VCPE) and Virtualized Radio Access Network (VRAN). We determine that the part of the mobile network with the highest energy utilization prospects is the Evolved Packet Core (EPC) where virtualization of functions leads to a 22% reduction in energy consumption and a 32% enhancement in energy efficiency. △ Less","1 December, 2015",https://arxiv.org/pdf/1512.00215
Applying CMM Towards an m-Learning Context,Muasaad Alrasheedi;Luiz Fernando Capretz,"In the era of m-Learning, it is found that educational institutions have onus of incorporating the latest technological innovations that can be accepted and understood widely. While investigating the important theme of fast-paced development of emerging technologies in mobile communications, it is important to recognize the extent influence of these innovations using which society can communicate, learn, access information, and, additionally, interact. In addition, the usage of mobile technology in higher education needs not only the pervasive nature of the technology but also its disruptive nature that offers several challenges while incorporation in the area of teaching and learning. Therefore, recently, higher education institutions are looking at various ways of implementing m-Learning strategies, in order to offer solutions, which, in turn, can standardize the process of education and, additionally, replace those traditional didactic courses, focusing on m-Learning endless benefits. Some of the benefits are: the process of learning itself could be self-paced, whereas information could be easier accessed, adding to independent, discovery-oriented learning that becomes more engaging. Applying CMM successfully to design effective incorporation strategies of m-Learning, this research targets formulation of such a maturity model by which the process of m-Learning can be more effective and efficient. △ Less","27 November, 2015",https://arxiv.org/pdf/1511.08840
A GA based approach for task scheduling in multi-cloud environment,Tripti Tanaya Tejaswi;Md Azharuddin;P. K. Jana,"In multi-cloud environment, task scheduling has attracted a lot of attention due to NP-Complete nature of the problem. Moreover, it is very challenging due to heterogeneity of the cloud resources with varying capacities and functionalities. Therefore, minimizing the makespan for task scheduling is a challenging issue. In this paper, we propose a genetic algorithm (GA) based approach for solving task scheduling problem. The algorithm is described with innovative idea of fitness function derivation and mutation. The proposed algorithm is exposed to rigorous testing using various benchmark datasets and its performance is evaluated in terms of total makespan. △ Less","27 November, 2015",https://arxiv.org/pdf/1511.08707
Spatially Distributed Sampling and Reconstruction,Cheng Cheng;Yingchun Jiang;Qiyu Sun,"A spatially distributed system contains a large amount of agents with limited sensing, data processing, and communication capabilities. Recent technological advances have opened up possibilities to deploy spatially distributed systems for signal sampling and reconstruction. In this paper, we introduce a graph structure for a distributed sampling and reconstruction system by coupling agents in a spatially distributed system with innovative positions of signals. A fundamental problem in sampling theory is the robustness of signal reconstruction in the presence of sampling noises. For a distributed sampling and reconstruction system, the robustness could be reduced to the stability of its sensing matrix. In a traditional centralized sampling and reconstruction system, the stability of the sensing matrix could be verified by its central processor, but the above procedure is infeasible in a distributed sampling and reconstruction system as it is decentralized. In this paper, we split a distributed sampling and reconstruction system into a family of overlapping smaller subsystems, and we show that the stability of the sensing matrix holds if and only if its quasi-restrictions to those subsystems have uniform stability. This new stability criterion could be pivotal for the design of a robust distributed sampling and reconstruction system against supplement, replacement and impairment of agents, as we only need to check the uniform stability of affected subsystems. In this paper, we also propose an exponentially convergent distributed algorithm for signal reconstruction, that provides a suboptimal approximation to the original signal in the presence of bounded sampling noises. △ Less","26 November, 2015",https://arxiv.org/pdf/1511.08541
Adding Gradient Noise Improves Learning for Very Deep Networks,Arvind Neelakantan;Luke Vilnis;Quoc V. Le;Ilya Sutskever;Lukasz Kaiser;Karol Kurach;James Martens,"Deep feedforward and recurrent networks have achieved impressive results in many perception and language processing applications. This success is partially attributed to architectural innovations such as convolutional and long short-term memory networks. The main motivation for these architectural innovations is that they capture better domain knowledge, and importantly are easier to optimize than more basic architectures. Recently, more complex architectures such as Neural Turing Machines and Memory Networks have been proposed for tasks including question answering and general computation, creating a new set of optimization challenges. In this paper, we discuss a low-overhead and easy-to-implement technique of adding gradient noise which we find to be surprisingly effective when training these very deep architectures. The technique not only helps to avoid overfitting, but also can result in lower training loss. This method alone allows a fully-connected 20-layer deep network to be trained with standard gradient descent, even starting from a poor initialization. We see consistent improvements for many complex models, including a 72% relative reduction in error rate over a carefully-tuned baseline on a challenging question-answering task, and a doubling of the number of accurate binary multiplication models learned across 7,000 random restarts. We encourage further application of this technique to additional complex modern architectures. △ Less","20 November, 2015",https://arxiv.org/pdf/1511.06807
Development of Wireless Techniques in Data and Power Transmission - Application for Particle Physics Detectors,R. Brenner;S. Ceuterickx;C. Dehos;P. De Lurgio;Z. Djurcic;G. Drake;J. L. Gonzalez Gimenez;L. Gustafsson;D. W. Kim;E. Locci;D. Roehrich;A. Schoening;A. Siligaris;H. K. Soltveit;K. Ullaland;P. Vincent;D. Wiednert;S. Yang,"Wireless techniques have developed extremely fast over the last decade and using them for data and power transmission in particle physics detectors is not science- fiction any more. During the last years several research groups have independently thought of making it a reality. Wireless techniques became a mature field for research and new developments might have impact on future particle physics experiments. The Instrumentation Frontier was set up as a part of the SnowMass 2013 Community Summer Study [1] to examine the instrumentation R&D for the particle physics research over the coming decades: « To succeed we need to make technical and scientific innovation a priority in the field ». Wireless data transmission was identified as one of the innovations that could revolutionize the transmission of data out of the detector. Power delivery was another challenge mentioned in the same report. We propose a collaboration to identify the specific needs of different projects that might benefit from wireless techniques. The objective is to provide a common platform for research and development in order to optimize effectiveness and cost, with the aim of designing and testing wireless demonstrators for large instrumentation systems. △ Less","18 November, 2015",https://arxiv.org/pdf/1511.05807
Prominent but Less Productive: The Impact of Interdisciplinarity on Scientists' Research,Erin Leahey;Christine Beckman;Taryn Stanko,"Inter-disciplinary research (IDR) is being promoted by federal agencies and universities nationwide because it presumably spurs transformative, innovative science. In this paper we bring empirical data to assess whether IDR is indeed beneficial, and whether costs accompany potential benefits.","22 October, 2015",https://arxiv.org/pdf/1510.06802
Hierarchy of knowledge translation: from health problems to ad-hoc drug design,David Fajardo;Victor Castano,"An innovative approach to analyze the complexity of translating novel molecular entities and nanomaterials into pharmaceutical alternatives (i.e., knowledge translation, KT) is discussed. First, some key concepts on the organization and translation of the biomedical knowledge (paradigms, homophily, power law distributions, hierarchy, modularity, and research fronts) are reviewed. Then, we propose a model for the knowledge translation (KT) in Drug Discovery that considers the complexity of interdisciplinary communication. Specifically, we address two highly relevant aspects: 1) A successful KT requires the emergence of organized bodies of inter-and transdisciplinary research, and 2) The hierarchical and modular topological organization of these bodies of knowledge. We focused on a set of previously-published studies on KT which rely on a combination of network analysis and computer-assisted analysis of the contents of scientific literature and patents. The selected studies provide a duo of complementary perspectives: the demand of knowledge (cervical cancer and Ebola hemorrhagic fever) and the supply of knowledge (liposomes and nanoparticles to treat cancer and the paradigmatic Doxil, the first nanodrug to be approved). △ Less","26 October, 2015",https://arxiv.org/pdf/1510.06124
Temporal Matrix Factorization for Tracking Concept Drift in Individual User Preferences,Yung-Yin Lo;Wanjiun Liao;Cheng-Shang Chang,"The matrix factorization (MF) technique has been widely adopted for solving the rating prediction problem in recommender systems. The MF technique utilizes the latent factor model to obtain static user preferences (user latent vectors) and item characteristics (item latent vectors) based on historical rating data. However, in the real world user preferences are not static but full of dynamics. Though there are several previous works that addressed this time varying issue of user preferences, it seems (to the best of our knowledge) that none of them is specifically designed for tracking concept drift in individual user preferences. Motivated by this, we develop a Temporal Matrix Factorization approach (TMF) for tracking concept drift in each individual user latent vector. There are two key innovative steps in our approach: (i) we develop a modified stochastic gradient descent method to learn an individual user latent vector at each time step, and (ii) by the Lasso regression we learn a linear model for the transition of the individual user latent vectors. We test our method on a synthetic dataset and several real datasets. In comparison with the original MF, our experimental results show that our temporal method is able to achieve lower root mean square errors (RMSE) for both the synthetic and real datasets. One interesting finding is that the performance gain in RMSE is mostly from those users who indeed have concept drift in their user latent vectors at the time of prediction. In particular, for the synthetic dataset and the Ciao dataset, there are quite a few users with that property and the performance gains for these two datasets are roughly 20% and 5%, respectively. △ Less","18 October, 2015",https://arxiv.org/pdf/1510.05263
BLASX: A High Performance Level-3 BLAS Library for Heterogeneous Multi-GPU Computing,Linnan Wang;Wei Wu;Jianxiong Xiao;Yi Yang,"Basic Linear Algebra Subprograms (BLAS) are a set of low level linear algebra kernels widely adopted by applications involved with the deep learning and scientific computing. The massive and economic computing power brought forth by the emerging GPU architectures drives interest in implementation of compute-intensive level 3 BLAS on multi-GPU systems. In this paper, we investigate existing multi-GPU level 3 BLAS and present that 1) issues, such as the improper load balancing, inefficient communication, insufficient GPU stream level concurrency and data caching, impede current implementations from fully harnessing heterogeneous computing resources; 2) and the inter-GPU Peer-to-Peer(P2P) communication remains unexplored. We then present BLASX: a highly optimized multi-GPU level-3 BLAS. We adopt the concepts of algorithms-by-tiles treating a matrix tile as the basic data unit and operations on tiles as the basic task. Tasks are guided with a dynamic asynchronous runtime, which is cache and locality aware. The communication cost under BLASX becomes trivial as it perfectly overlaps communication and computation across multiple streams during asynchronous task progression. It also takes the current tile cache scheme one step further by proposing an innovative 2-level hierarchical tile cache, taking advantage of inter-GPU P2P communication. As a result, linear speedup is observable with BLASX under multi-GPU configurations; and the extensive benchmarks demonstrate that BLASX consistently outperforms the related leading industrial and academic projects such as cuBLAS-XT, SuperMatrix, MAGMA and PaRSEC. △ Less","16 October, 2015",https://arxiv.org/pdf/1510.05041
Modelling the Evolution of Programming Languages,Silvia Crafa,"Programming languages are engineered languages that allow to instruct a machine and share algorithmic information; they have a great influence on the society since they underlie almost every information technology artefact, and they are at the core of the current explosion of software technology. The history of programming languages is marked by innovations, diversifications, lateral transfers and social influences; moreover, it represents an intermediate case study between the evolution of human languages and the evolution of technology. In this paper we study the application of the Darwinian explanation to the programming languages evolution by discussing to what extent the evolutionary mechanisms distinctive of biology can be applied to this area. We show that a number of evolutionary building blocks can be recognised in the realm of computer languages, but we also identify critical issues. Far from being crystal clear, this fine-grained study shows to be a useful tool to assess recent results about programming languages phylogenies. Finally, we show that rich evolutionary patterns, such as co-evolution, macro-evolutionary trends, niche construction and exaptation, can be effectively applied to programming languages and provide for interesting explanatory tools. △ Less","15 October, 2015",https://arxiv.org/pdf/1510.04440
Elastic Resource Allocation for Distributed Graph Processing Platforms,Ravikant Dindokar;Yogesh Simmhan,"Distributed graph platforms like Pregel have used vertex- centric programming models to process the growing corpus of graph datasets using commodity clusters. The irregular structure of graphs cause load imbalances across machines operating on graph partitions, and this is exacerbated for non-stationary graph algorithms such as traversals, where not all parts of the graph are active at the same time. As a result, such graph platforms, even as they scale, do not make efficient use of distributed resources. Clouds offer elastic virtual machines that can be leveraged to improve the resource utilization for such platforms and hence reduce the monetary cost for their execution. In this paper, we propose strategies for elastic placement of graph partitions on Cloud VMs for subgraphcentric programming model to reduce the cost of execution compared to a static placement, even as we minimize the increase in makespan. These strategies are innovative in modeling the graph algorithms behavior. We validate our strategies for several graphs, using runtime tra- ces for their distributed execution of a Breadth First Search (BFS) algorithms on our subgraph-centric GoFFish graph platform. Our strategies are able to reduce the cost of exe- cution by up to 42%, compared to a static placement, while achieving a makespan that is within 29% of the optimal △ Less","12 October, 2015",https://arxiv.org/pdf/1510.03145
Sustainability: Scholarly Repository as an Enterprise,Oya Y. Rieger,"The expanding need for an open information sharing infrastructure to promote scholarly communication led to the pioneering establishment of arXiv.org, now maintained by the Cornell University Library. To be sustainable, the repository requires careful, long term planning for services, management and funding. The library is developing a sustainability model for arXiv.org, based on voluntary contributions and the ongoing participation and support of 200 libraries and research laboratories around the world. The sustainability initiative is based on a membership model and builds on arXiv's technical, service, financial and policy infrastructure. Five principles for sustainability drive development, starting with deep integration into the scholarly community. Also key are a clearly defined mandate and governance structure, a stable yet innovative technology platform, systematic creation of content policies and strong business planning strategies. Repositories like arXiv must consider usability and life cycle alongside values and trends in scholarly communication. To endure, they must also support and enhance their service by securing and managing resources and demonstrating responsible stewardship. △ Less","1 October, 2015",https://arxiv.org/pdf/1510.00322
Distributed Inference for Relay-Assisted Sensor Networks With Intermittent Measurements Over Fading Channels,Shanying Zhu;Yeng Chai Soh;Lihua Xie,"In this paper, we consider a general distributed estimation problem in relay-assisted sensor networks by taking into account time-varying asymmetric communications, fading channels and intermittent measurements. Motivated by centralized filtering algorithms, we propose a distributed innovation-based estimation algorithm by combining the measurement innovation (assimilation of new measurement) and local data innovation (incorporation of neighboring data). Our algorithm is fully distributed which does not need a fusion center. We establish theoretical results regarding asymptotic unbiasedness and consistency of the proposed algorithm. Specifically, in order to cope with time-varying asymmetric communications, we utilize an ordering technique and the generalized Perron complement to manipulate the first and second moment analyses in a tractable framework. Furthermore, we present a performance-oriented design of the proposed algorithm for energy-constrained networks based on the theoretical results. Simulation results corroborate the theoretical findings, thus demonstrating the effectiveness of the proposed algorithm. △ Less","30 September, 2015",https://arxiv.org/pdf/1509.09282
Supporting interoperability of collaborative networks through engineering of a service-based Mediation Information System (MISE 2.0),Frederick Benaben;Wenxin Mu;Nicolas Boissel-Dallier;Anne-Marie Barthe-Delanoë;Sarah Zribi;Herve Pingaud,"The Mediation Information System Engineering project is currently finishing its second iteration (MISE 2.0). The main objective of this scientific project is to provide any emerging collaborative situation with methods and tools to deploy a Mediation Information System (MIS). MISE 2.0 aims at defining and designing a service-based platform, dedicated to initiating and supporting the interoperability of collaborative situations among potential partners. This MISE 2.0 platform implements a model-driven engineering approach to the design of a service-oriented MIS dedicated to supporting the collaborative situation. This approach is structured in three layers, each providing their own key innovative points: (i) the gathering of individual and collaborative knowledge to provide appropriate collaborative business behaviour (key point: knowledge management, including semantics, exploitation and capitalization), (ii) deployment of a mediation information system able to computerize the previously deduced collaborative processes (key point: the automatic generation of collaborative workflows, including connection with existing devices or services) (iii) the management of the agility of the obtained collaborative network of organizations (key point: supervision of collaborative situations and relevant exploitation of the gathered data). MISE covers business issues (through BPM), technical issues (through an SOA) and agility issues of collaborative situations (through EDA). △ Less","30 September, 2015",https://arxiv.org/pdf/1509.09152
Citizen Science practices for Computational Social Sciences research: The conceptualization of Pop-Up Experiments,Oleguer Sagarra;Mario Gutiérrez-Roig;Isabelle Bonhoure;Josep Perelló,"Under the name of Citizen Science, many innovative practices in which volunteers partner with scientist to pose and answer real-world questions are quickly growing worldwide. Citizen Science can furnish ready made solutions with the active role of citizens. However, this framework is still far from being well stablished to become a standard tool for Computational Social Sciences research. We present our experience in bridging Computational Social Sciences with Citizen Science philosophy, which in our case has taken the form of what we call Pop-Up Experiments: Non-permanent, highly participatory collective experiments which blend features developed by Big Data methodologies and Behavioural Experiments protocols with ideals of Citizen Science. The main issues to take into account whenever planning experiments of this type are classified and discused grouped in three categories: public engagement, light infrastructure and knowledge return to citizens. We explain the solutions implemented providing practical examples grounded in our own experience in urban contexts (Barcelona, Spain). We hope that this work serves as guideline to groups willing to adopt and expand such \emph{in-vivo} practices and opens the debate about the possibilities (but also the limitations) that the Citizen Science framework can offer to study social phenomena. △ Less","22 September, 2015",https://arxiv.org/pdf/1509.06575
Challenges and Considerations for Utilizing Burst Buffers in High-Performance Computing,Melissa Romanus;Robert B. Ross;Manish Parashar,"As high-performance computing (HPC) moves into the exascale era, computer scientists and engineers must find innovative ways of transferring and processing unprecedented amounts of data. As the scale and complexity of the applications running on these machines increases, the cost of their interactions and data exchanges (in terms of latency, energy, runtime, etc.) can increase exponentially. In order to address I/O coordination and communication issues, computing vendors are developing an intermediate layer between compute nodes and the parallel file system composed of different types of memory (NVRAM, DRAM, SSD). These large scale memory appliances are being called 'burst buffers.' In this paper, we envision advanced memory at various levels of HPC hardware and derive potential use cases for how to take advantage of it. We then present the challenges and issues that arise when utilizing burst buffers in next-generation supercomputers and map the challenges to the use cases. Lastly, we discuss the emerging state-of-the-art burst buffer solutions that are expected to become available by the end of the year in new HPC systems and which use cases these implementations may satisfy. △ Less","29 September, 2015",https://arxiv.org/pdf/1509.05492
Domain-Specific Modeling and Code Generation for Cross-Platform Multi-Device Mobile Apps,Eric Umuhoza,"Nowadays, mobile devices constitute the most common computing device. This new computing model has brought intense competition among hardware and software providers who are continuously introducing increasingly powerful mobile devices and innovative OSs into the market. In consequence, cross-platform and multi-device development has become a priority for software companies that want to reach the widest possible audience. However, developing an application for several platforms implies high costs and technical complexity. Currently, there are several frameworks that allow cross-platform application development. However, these approaches still require manual programming. My research proposes to face the challenge of the mobile revolution by exploiting abstraction, modeling and code generation, in the spirit of the modern paradigm of Model Driven Engineering. △ Less","10 September, 2015",https://arxiv.org/pdf/1509.03109
"Modeling Quantum Optical Components, Pulses and Fiber Channels Using OMNeT++",Ryan D. L. Engle;Douglas D. Hodson;Michael R. Grimaila;Logan O. Mailloux;Colin V. McLaughlin;Gerald Baumgartner,"Quantum Key Distribution (QKD) is an innovative technology which exploits the laws of quantum mechanics to generate and distribute unconditionally secure cryptographic keys. While QKD offers the promise of unconditionally secure key distribution, real world systems are built from non-ideal components which necessitates the need to model and understand the impact these non-idealities have on system performance and security. OMNeT++ has been used as a basis to develop a simulation framework to support this endeavor. This framework, referred to as ""qkdX"" extends OMNeT++'s module and message abstractions to efficiently model optical components, optical pulses, operating protocols and processes. This paper presents the design of this framework including how OMNeT++'s abstractions have been utilized to model quantum optical components, optical pulses, fiber and free space channels. Furthermore, from our toolbox of created components, we present various notional and real QKD systems, which have been studied and analyzed. △ Less","10 September, 2015",https://arxiv.org/pdf/1509.03091
Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies,David Balduzzi;Muhammad Ghifary,"This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation. The algorithm is based on two innovations. Firstly, we present a temporal-difference based method for learning the gradient of the value-function. Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively. We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark. GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm. △ Less","10 September, 2015",https://arxiv.org/pdf/1509.03005
Rust-Bio - a fast and safe bioinformatics library,Johannes Köster,"We present Rust-Bio, the first general purpose bioinformatics library for the innovative Rust programming language. Rust-Bio leverages the unique combination of speed, memory safety and high-level syntax offered by Rust to provide a fast and safe set of bioinformatics algorithms and data structures with a focus on sequence analysis.","9 September, 2015",https://arxiv.org/pdf/1509.02796
Taming the hydra: the word problem and extreme integer compression,W. Dison;E. Einstein;T. R. Riley,"For a finitely presented group, the word problem asks for an algorithm which declares whether or not words on the generators represent the identity. The Dehn function is a complexity measure of a direct attack on the word problem by applying the defining relations. Dison & Riley showed that a ""hydra phenomenon"" gives rise to novel groups with extremely fast growing (Ackermannian) Dehn functions. Here we show that nevertheless, there are efficient (polynomial time) solutions to the word problems of these groups. Our main innovation is a means of computing efficiently with enormous integers which are represented in compressed forms by strings of Ackermann functions. △ Less","8 September, 2015",https://arxiv.org/pdf/1509.02557
Object Proposals for Text Extraction in the Wild,Lluis Gomez;Dimosthenis Karatzas,"Object Proposals is a recent computer vision technique receiving increasing interest from the research community. Its main objective is to generate a relatively small set of bounding box proposals that are most likely to contain objects of interest. The use of Object Proposals techniques in the scene text understanding field is innovative. Motivated by the success of powerful while expensive techniques to recognize words in a holistic way, Object Proposals techniques emerge as an alternative to the traditional text detectors. In this paper we study to what extent the existing generic Object Proposals methods may be useful for scene text understanding. Also, we propose a new Object Proposals algorithm that is specifically designed for text and compare it with other generic methods in the state of the art. Experiments show that our proposal is superior in its ability of producing good quality word proposals in an efficient way. The source code of our method is made publicly available. △ Less","8 September, 2015",https://arxiv.org/pdf/1509.02317
Identification and modeling of discoverers in online social systems,Matus Medo;Manuel S. Mariani;An Zeng;Yi-Cheng Zhang,"The dynamics of individuals is of essential importance for understanding the evolution of social systems. Most existing models assume that individuals in diverse systems, ranging from social networks to e-commerce, all tend to what is already popular. We develop an analytical time-aware framework which shows that when individuals make choices -- which item to buy, for example -- in online social systems, a small fraction of them is consistently successful in discovering popular items long before they actually become popular. We argue that these users, whom we refer to as discoverers, are fundamentally different from the previously known opinion leaders, influentials, and innovators. We use the proposed framework to demonstrate that discoverers are present in a wide range of systems. Once identified, they can be used to predict the future success of items. We propose a network model which reproduces the discovery patterns observed in the real data. Furthermore, data produced by the model pose a fundamental challenge to classical ranking algorithms which neglect the time of link creation and thus fail to discriminate between discoverers and ordinary users in the data. Our results open the door to qualitative and quantitative study of fine temporal patterns in social systems and have far-reaching implications for network modeling and algorithm design. △ Less","4 September, 2015",https://arxiv.org/pdf/1509.01477
Exploring Online Ad Images Using a Deep Convolutional Neural Network Approach,Michael Fire;Jonathan Schler,"Online advertising is a huge, rapidly growing advertising market in today's world. One common form of online advertising is using image ads. A decision is made (often in real time) every time a user sees an ad, and the advertiser is eager to determine the best ad to display. Consequently, many algorithms have been developed that calculate the optimal ad to show to the current user at the present time. Typically, these algorithms focus on variations of the ad, optimizing among different properties such as background color, image size, or set of images. However, there is a more fundamental layer. Our study looks at new qualities of ads that can be determined before an ad is shown (rather than online optimization) and defines which ads are most likely to be successful. We present a set of novel algorithms that utilize deep-learning image processing, machine learning, and graph theory to investigate online advertising and to construct prediction models which can foresee an image ad's success. We evaluated our algorithms on a dataset with over 260,000 ad images, as well as a smaller dataset specifically related to the automotive industry, and we succeeded in constructing regression models for ad image click rate prediction. The obtained results emphasize the great potential of using deep-learning algorithms to effectively and efficiently analyze image ads and to create better and more innovative online ads. Moreover, the algorithms presented in this paper can help predict ad success and can be applied to analyze other large-scale image corpora. △ Less","2 September, 2015",https://arxiv.org/pdf/1509.00568
Brewing Analytics Quality for Cloud Performance,Li Chen;Pooja Jain;Kingsum Chow;Emad Guirguis;Tony Wu,"Cloud computing has become increasingly popular. Many options of cloud deployments are available. Testing cloud performance would enable us to choose a cloud deployment based on the requirements. In this paper, we present an innovative process, implemented in software, to allow us to assess the quality of the cloud performance data. The process combines performance data from multiple machines, spanning across user experience data, workload performance metrics, and readily available system performance data. Furthermore, we discuss the major challenges of bringing raw data into tidy data formats in order to enable subsequent analysis, and describe how our process has several layers of assessment to validate the quality of the data processing procedure. We present a case study to demonstrate the effectiveness of our proposed process, and conclude our paper with several future research directions worth investigating. △ Less","31 August, 2015",https://arxiv.org/pdf/1509.00095
D4M: Bringing Associative Arrays to Database Engines,Vijay Gadepally;Jeremy Kepner;William Arcand;David Bestor;Bill Bergeron;Chansup Byun;Lauren Edwards;Matthew Hubbell;Peter Michaleas;Julie Mullen;Andrew Prout;Antonio Rosa;Charles Yee;Albert Reuther,"The ability to collect and analyze large amounts of data is a growing problem within the scientific community. The growing gap between data and users calls for innovative tools that address the challenges faced by big data volume, velocity and variety. Numerous tools exist that allow users to store, query and index these massive quantities of data. Each storage or database engine comes with the promise of dealing with complex data. Scientists and engineers who wish to use these systems often quickly find that there is no single technology that offers a panacea to the complexity of information. When using multiple technologies, however, there is significant trouble in designing the movement of information between storage and database engines to support an end-to-end application along with a steep learning curve associated with learning the nuances of each underlying technology. In this article, we present the Dynamic Distributed Dimensional Data Model (D4M) as a potential tool to unify database and storage engine operations. Previous articles on D4M have showcased the ability of D4M to interact with the popular NoSQL Accumulo database. Recently however, D4M now operates on a variety of backend storage or database engines while providing a federated look to the end user through the use of associative arrays. In order to showcase how new databases may be supported by D4M, we describe the process of building the D4M-SciDB connector and present performance of this connection. △ Less","28 August, 2015",https://arxiv.org/pdf/1508.07371
Shopper Analytics: a customer activity recognition system using a distributed RGB-D camera network,Daniele Liciotti;Marco Contigiani;Emanuele Frontoni;Adriano Mancini;Primo Zingaretti;Valerio Placidi,"The aim of this paper is to present an integrated system consisted of a RGB-D camera and a software able to monitor shoppers in intelligent retail environments. We propose an innovative low cost smart system that can understand the shoppers' behavior and, in particular, their interactions with the products in the shelves, with the aim to develop an automatic RGB-D technique for video analysis. The system of cameras detects the presence of people and univocally identifies them. Through the depth frames, the system detects the interactions of the shoppers with the products on the shelf and determines if a product is picked up or if the product is taken and then put back and finally, if there is not contact with the products. The system is low cost and easy to install, and experimental results demonstrated that its performances are satisfactory also in real environments. △ Less","27 August, 2015",https://arxiv.org/pdf/1508.06853
Component-Enhanced Chinese Character Embeddings,Yanran Li;Wenjie Li;Fei Sun;Sujian Li,"Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models. △ Less","26 August, 2015",https://arxiv.org/pdf/1508.06669
Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection,Kisuk Lee;Aleksandar Zlateski;Ashwin Vishwanathan;H. Sebastian Seung,"Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D max-pooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D-3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem. △ Less","19 August, 2015",https://arxiv.org/pdf/1508.04843
Looking at Software Sustainability and Productivity Challenges from NSF,Daniel S. Katz;Rajiv Ramnath,"This paper is a contribution to the Computational Science & Engineering Software Sustainability and Productivity Challenges (CSESSP Challenges) Workshop (https://www.nitrd.gov/csessp/), sponsored by the Networking and Information Technology Research and Development (NITRD) Software Design and Productivity (SDP) Coordinating Group, held October 15th-16th 2015 in Washington DC, USA. It introduces the role of software at the National Science Foundation (NSF) and the NSF Software Infrastructure for Sustained Innovation (SI2) program, then describes challenges that the SI2 program has identified, including funding models, career paths, incentives, training, interdisciplinary work, portability, and dissemination, as well as lesson that have been learned. △ Less","17 August, 2015",https://arxiv.org/pdf/1508.03348
Coreless 5G Mobile Network,Farooq Khan,"Todays mobile networks contain an increasing variety of proprietary hardware stifling innovation and leading to longer time-to-market for introduction of new services. We propose to replace the mobile core network nodes and interfaces with an Open Source SW implementation running on general purpose commodity hardware. The proposed open source approach referred to as coreless mobile network is expected to reduce cost, increase flexibility, improve innovation speed and accelerate time-to-market for introduction of new features and functionalities. A common Open Source SW framework will also enable automatic discovery and selection, seamless data mobility as well as unified charging and billing across cellular, WiFi, UAV and satellite access networks. △ Less","18 September, 2015",https://arxiv.org/pdf/1508.02052
Automatic 3D Liver Segmentation Using Sparse Representation of Global and Local Image Information via Level Set Formulation,Saif Dawood Salman Al-Shaikhli;Michael Ying Yang;Bodo Rosenhahn,"In this paper, a novel framework for automated liver segmentation via a level set formulation is presented. A sparse representation of both global (region-based) and local (voxel-wise) image information is embedded in a level set formulation to innovate a new cost function. Two dictionaries are build: A region-based feature dictionary and a voxel-wise dictionary. These dictionaries are learned, using the K-SVD method, from a public database of liver segmentation challenge (MICCAI-SLiver07). The learned dictionaries provide prior knowledge to the level set formulation. For the quantitative evaluation, the proposed method is evaluated using the testing data of MICCAI-SLiver07 database. The results are evaluated using different metric scores computed by the challenge organizers. The experimental results demonstrate the superiority of the proposed framework by achieving the highest segmentation accuracy (79.6\%) in comparison to the state-of-the-art methods. △ Less","4 October, 2015",https://arxiv.org/pdf/1508.01521
SOARAN: A Service-oriented Architecture for Radio Access Network Sharing in Evolving Mobile Networks,Jun He;Wei Song,"Mobile networks are undergoing fast evolution to software-defined networking (SDN) infrastructure in order to accommodate the ever-growing mobile traffic and overcome the network management nightmares caused by unremitting acceleration in technology innovations and evolution of the service market.Enabled by virtualized network functionalities, evolving carrier wireless networks tend to share radio access network (RAN) among multiple (virtual) network operators so as to increase network capacity and reduce expenses.However, existing RAN sharing models are operator-oriented, which expose extensive resource details, e.g. infrastructure and spectrum,to participating network operators for resource-sharing purposes. These old-fashioned models violate the design principles of SDN abstraction and are infeasible to manage the thriving traffic of on-demand customized services. This paper presents SOARAN, a service-oriented framework for RAN sharing in mobile networks evolving from LTE/LTE advanced to software-defined carrier wireless networks(SD-CWNs), which decouples network operators from radio resource by providing application-level differentiated services. SOARAN defines a serial of abstract applications with distinct Quality of Experience (QoE) requirements. The central controller periodically computes application-level resource allocation for each radio element with respect to runtime traffic demands and channel conditions, and disseminate these allocation decisions as service-oriented policies to respect element. The radio elements then independently determine flow-level resource allocation within each application to accomplish these policies. We formulate the application-level resource allocation as an optimization problem and develop a fast algorithm to solve it with a provably approximate guarantee. △ Less","2 August, 2015",https://arxiv.org/pdf/1508.00306
An Analytic Framework for Maritime Situation Analysis,Hamed Yaghoubi Shahir;Uwe Glässer;Amir Yaghoubi Shahir;Hans Wehn,"Maritime domain awareness is critical for protecting sea lanes, ports, harbors, offshore structures and critical infrastructures against common threats and illegal activities. Limited surveillance resources constrain maritime domain awareness and compromise full security coverage at all times. This situation calls for innovative intelligent systems for interactive situation analysis to assist marine authorities and security personal in their routine surveillance operations. In this article, we propose a novel situation analysis framework to analyze marine traffic data and differentiate various scenarios of vessel engagement for the purpose of detecting anomalies of interest for marine vessels that operate over some period of time in relative proximity to each other. The proposed framework views vessel behavior as probabilistic processes and uses machine learning to model common vessel interaction patterns. We represent patterns of interest as left-to-right Hidden Markov Models and classify such patterns using Support Vector Machines. △ Less","1 August, 2015",https://arxiv.org/pdf/1508.00181
Achieving secrecy without knowing the number of eavesdropper antennas,Biao He;Xiangyun Zhou;Thushara D. Abhayapala,"The existing research on physical layer security commonly assumes the number of eavesdropper antennas to be known. Although this assumption allows one to easily compute the achievable secrecy rate, it can hardly be realized in practice. In this paper, we provide an innovative approach to study secure communication systems without knowing the number of eavesdropper antennas by introducing the concept of spatial constraint into physical layer security. Specifically, the eavesdropper is assumed to have a limited spatial region to place (possibly an infinite number of) antennas. From a practical point of view, knowing the spatial constraint of the eavesdropper is much easier than knowing the number of eavesdropper antennas. We derive the achievable secrecy rates of the spatially-constrained system with and without friendly jamming. We show that a non-zero secrecy rate is achievable with the help of a friendly jammer, even if the eavesdropper places an infinite number of antennas in its spatial region. Furthermore, we find that the achievable secrecy rate does not monotonically increase with the jamming power, and hence, we obtain the closed-form solution of the optimal jamming power that maximizes the secrecy rate. △ Less","1 August, 2015",https://arxiv.org/pdf/1508.00105
In Need of Creative Mobile Service Ideas? Forget Adults and Ask Young Children,Ilona Kuzmickaja;Xiaofeng Wang;Daniel Graziotin;Gabriella Dodero;Pekka Abrahamsson,"It is well acknowledged that innovation is a key success factor in mobile service domain. Having creative ideas is the first critical step in the innovation process. Many studies suggest that customers are a valuable source of creative ideas. However, the literature also shows that adults may be constrained by existing technology frames, which are known to hinder creativity. Instead young children (aged 7-12) are considered digital natives yet are free from existing technology frames. This led us to study them as a potential source for creative mobile service ideas. A set of 41,000 mobile ideas obtained from a research project in 2006 granted us a unique opportunity to study the mobile service ideas from young children. We randomly selected two samples of ideas (N=400 each), one contained the ideas from young children, the other from adults (aged 17-50). These ideas were evaluated by several evaluators using an existing creativity framework. The results show that the mobile service ideas from the young children are significantly more original, transformational, implementable, and relevant than those from the adults. Therefore, this study shows that young children are better sources of novel and quality ideas than adults in the mobile services domain. This study bears significant contributions to the creativity and innovation research. It also indicates a new and valuable source for the companies that seek for creative ideas for innovative products and services. △ Less","24 July, 2015",https://arxiv.org/pdf/1507.06768
Mapping Big Data into Knowledge Space with Cognitive Cyber-Infrastructure,Hai Zhuge,"Big data research has attracted great attention in science, technology, industry and society. It is developing with the evolving scientific paradigm, the fourth industrial revolution, and the transformational innovation of technologies. However, its nature and fundamental challenge have not been recognized, and its own methodology has not been formed. This paper explores and answers the following questions: What is big data? What are the basic methods for representing, managing and analyzing big data? What is the relationship between big data and knowledge? Can we find a mapping from big data into knowledge space? What kind of infrastructure is required to support not only big data management and analysis but also knowledge discovery, sharing and management? What is the relationship between big data and science paradigm? What is the nature and fundamental challenge of big data computing? A multi-dimensional perspective is presented toward a methodology of big data computing. △ Less","18 July, 2015",https://arxiv.org/pdf/1507.06500
Super-Resolution Sparse MIMO-OFDM Channel Estimation Based on Spatial and Temporal Correlations,Zhen Gao;Linglong Dai;Zhaohua Lu;Chau Yuen;Zhaocheng Wang,"This letter proposes a parametric sparse multiple input multiple output (MIMO)-OFDM channel estimation scheme based on the finite rate of innovation (FRI) theory, whereby super-resolution estimates of path delays with arbitrary values can be achieved. Meanwhile, both the spatial and temporal correlations of wireless MIMO channels are exploited to improve the accuracy of the channel estimation. For outdoor communication scenarios, where wireless channels are sparse in nature, path delays of different transmit-receive antenna pairs share a common sparse pattern due to the spatial correlation of MIMO channels. Meanwhile, the channel sparse pattern is nearly unchanged during several adjacent OFDM symbols due to the temporal correlation of MIMO channels. By simultaneously exploiting those MIMO channel characteristics, the proposed scheme performs better than existing state-of-the-art schemes. Furthermore, by joint processing of signals associated with different antennas, the pilot overhead can be reduced under the framework of the FRI theory. △ Less","20 July, 2015",https://arxiv.org/pdf/1507.05441
Properties and constructions of coincident functions,Morgan Barbier;Hayat Cheballah;Jean-Marie Le Bars,"Extensive studies of Boolean functions are carried in many fields. The Mobius transform is often involved for these studies. In particular, it plays a central role in coincident functions, the class of Boolean functions invariant by this transformation. This class -- which has been recently introduced -- has interesting properties, in particular if we want to control both the Hamming weight and the degree. We propose an innovative way to handle the Mobius transform which allows the composition between several Boolean functions and the use of Shannon or Reed-Muller decompositions. Thus we benefit from a better knowledge of coin-cident functions and introduce new properties. We show experimentally that for many features, coincident functions look like any Boolean functions. △ Less","19 July, 2015",https://arxiv.org/pdf/1507.05316
Accessible Banking: Experiences and Future Directions,Bela Gor;David Aspinall,"This is a short position paper drawing on experience working with the UK banking industry and their disabled and ageing customers in the Business Disability Forum, a UK non-profit member organisation funded by a large body of UK private and public sector businesses. We describe some commonly reported problems of disabled customers who use modern banking technologies, relating them to UK law and best practice. We describe some of the recent banking industry innovations and the hope they may offer for improved inclusive and accessible multi-channel banking. △ Less","16 July, 2015",https://arxiv.org/pdf/1507.04578
A New Vision for Smart Objects and the Internet of Things: Mobile Robots and Long-Range UHF RFID Sensor Tags,Jennifer Wang;Erik Schluntz;Brian Otis;Travis Deyle,"We present a new vision for smart objects and the Internet of Things wherein mobile robots interact with wirelessly-powered, long-range, ultra-high frequency radio frequency identification (UHF RFID) tags outfitted with sensing capabilities. We explore the technology innovations driving this vision by examining recently-commercialized sensor tags that could be affixed-to or embedded-in objects or the environment to yield true embodied intelligence. Using a pair of autonomous mobile robots outfitted with UHF RFID readers, we explore several potential applications where mobile robots interact with sensor tags to perform tasks such as: soil moisture sensing, remote crop monitoring, infrastructure monitoring, water quality monitoring, and remote sensor deployment. △ Less","9 July, 2015",https://arxiv.org/pdf/1507.02373
On the Evaluation of RDF Distribution Algorithms Implemented over Apache Spark,Olivier Curé;Hubert Naacke;Mohamed-Amine Baazizi;Bernd Amann,"Querying very large RDF data sets in an efficient manner requires a sophisticated distribution strategy. Several innovative solutions have recently been proposed for optimizing data distribution with predefined query workloads. This paper presents an in-depth analysis and experimental comparison of five representative and complementary distribution approaches. For achieving fair experimental results, we are using Apache Spark as a common parallel computing framework by rewriting the concerned algorithms using the Spark API. Spark provides guarantees in terms of fault tolerance, high availability and scalability which are essential in such systems. Our different implementations aim to highlight the fundamental implementation-independent characteristics of each approach in terms of data preparation, load balancing, data replication and to some extent to query answering cost and performance. The presented measures are obtained by testing each system on one synthetic and one real-world data set over query workloads with differing characteristics and different partitioning constraints. △ Less","8 July, 2015",https://arxiv.org/pdf/1507.02321
Intelligent Data in the context of the internet-of-things,Rakhi Misuriya Gupta,"Advent of the Internet-of-Things will allow us to optimize equipment and resource usage, enabling increased efficiencies in automation and enabling new and more cost efficient business model. As tremendous growth opportunities emerge, so do the challenges such as diverse devices spanning across multiple networks, the need to manage the exponential growth of sensor generated data and to make sense of the huge influx of data in meaningful ways. The multitude of diversity can best be addressed by fundamentally opening up systems, architecture and applications. To go the next step and truly exploit the value of the sensor data would further require real-time analytics to gain intelligence and respond to events as they happen. Historical analysis can be used to look for trends, analyze collections of sensor data for correlation and formulate hints and suggestions based on usage and patterns. In this paper, we present a framework that overcomes diversity through its ability to flexibly represent sensor data on the internet. Business goals-driven information processing, information derived intelligence and information control as elements of the framework can further enable creation of new and innovative applications that enhance and exploit the value of Internet-of-Things. △ Less","6 July, 2015",https://arxiv.org/pdf/1507.01368
On Web-grid Implementation Using Single System Image,Marie Yvette B. de Robles;Zenith O. Arnejo;Jaderick P. Pabico,"With the latest innovations and trend towards personalizing users' web browsing experience, the web has been increasingly dominated by dynamic contents. However, delivering dynamic content remains a challenge due to the many dependencies involved in compiling the content, specifically personalized ones. This paper presents the use of Single System Image (SSI) clustering systems for a cheap, off-the-shelf, local lightweight distributed web-grid composed of desktop PCs. The three clustering systems considered in the study are Kerrighed, OpenSSI and openMosix. Through an online simulation technique, the performance savings achieved by the clustering systems were measured. Results showed that Kerrighed has the least number of missed requests while the response time is comparable with the rest. △ Less","4 July, 2015",https://arxiv.org/pdf/1507.01067
Business Models for e-Health: Evidence from Ten Case Studies,Chris Kimble,"An increasingly aging population and spiraling healthcare costs have made the search for financially viable healthcare models an imperative of this century. The careful and creative application of information technology can play a significant role in meeting that challenge. Valuable lessons can be learned from an analysis of ten innovative telemedicine and e-health initiatives. Having proven their effectiveness in addressing a variety of medical needs, they have progressed beyond small-scale implementations to become an established part of healthcare delivery systems around the world. △ Less","2 July, 2015",https://arxiv.org/pdf/1507.00553
Diffusion of innovations in Axelrod's model,Paulo F. C. Tilles;José F. Fontanari,"Axelrod's model for the dissemination of culture contains two key factors required to model the process of diffusion of innovations, namely, social influence (i.e., individuals become more similar when they interact) and homophily (i.e., individuals interact preferentially with similar others). The strength of these social influences are controlled by two parameters: F, the number of features that characterizes the cultures and q, the common number of states each feature can assume. Here we assume that the innovation is a new state of a cultural feature of a single individual -- the innovator -- and study how the innovation spreads through the networks among the individuals. For infinite regular lattices in one (1D) and two dimensions (2D), we find that initially the successful innovation spreads linearly with the time t, but in the long-time limit it spreads diffusively (\sim t^{1/2}) in 1D and sub-diffusively (\sim t/\ln t) in 2D. For finite lattices, the growth curves for the number of adopters are typically concave functions of t. For random graphs with a finite number of nodes N, we argue that the classical S-shaped growth curves result from a trade-off between the average connectivity K of the graph and the per feature diversity q. A large q is needed to reduce the pace of the initial spreading of the innovation and thus delimit the early-adopters stage, whereas a large K is necessary to ensure the onset of the take-off stage at which the number of adopters grows superlinearly with t. In an infinite random graph we find that the number of adopters of a successful innovation scales with t^γ with γ=1 for K> 2 and 1/2 < γ< 1 for K=2. We suggest that the exponent γ may be a useful index to characterize the process of diffusion of successful innovations in diverse scenarios. △ Less","7 September, 2015",https://arxiv.org/pdf/1506.08643
Characterization and Architectural Implications of Big Data Workloads,Lei Wang;Jianfeng Zhan;Zhen Jia;Rui Han,"Big data areas are expanding in a fast way in terms of increasing workloads and runtime systems, and this situation imposes a serious challenge to workload characterization, which is the foundation of innovative system and architecture design. The previous major efforts on big data benchmarking either propose a comprehensive but a large amount of workloads, or only select a few workloads according to so-called popularity, which may lead to partial or even biased observations. In this paper, on the basis of a comprehensive big data benchmark suite---BigDataBench, we reduced 77 workloads to 17 representative workloads from a micro-architectural perspective. On a typical state-of-practice platform---Intel Xeon E5645, we compare the representative big data workloads with SPECINT, SPECCFP, PARSEC, CloudSuite and HPCC. After a comprehensive workload characterization, we have the following observations. First, the big data workloads are data movement dominated computing with more branch operations, taking up to 92% percentage in terms of instruction mix, which places them in a different class from Desktop (SPEC CPU2006), CMP (PARSEC), HPC (HPCC) workloads. Second, corroborating the previous work, Hadoop and Spark based big data workloads have higher front-end stalls. Comparing with the traditional workloads i. e. PARSEC, the big data workloads have larger instructions footprint. But we also note that, in addition to varied instruction-level parallelism, there are significant disparities of front-end efficiencies among different big data workloads. Third, we found complex software stacks that fail to use state-of-practise processors efficiently are one of the main factors leading to high front-end stalls. For the same workloads, the L1I cache miss rates have one order of magnitude differences among diverse implementations with different software stacks. △ Less","25 June, 2015",https://arxiv.org/pdf/1506.07943
On Making Emerging Trusted Execution Environments Accessible to Developers,Thomas Nyman;Brian McGillion;N. Asokan,"New types of Trusted Execution Environment (TEE) architectures like TrustLite and Intel Software Guard Extensions (SGX) are emerging. They bring new features that can lead to innovative security and privacy solutions. But each new TEE environment comes with its own set of interfaces and programming paradigms, thus raising the barrier for entry for developers who want to make use of these TEEs. In this paper, we motivate the need for realizing standard TEE interfaces on such emerging TEE architectures and show that this exercise is not straightforward. We report on our on-going work in mapping GlobalPlatform standard interfaces to TrustLite and SGX. △ Less","30 June, 2015",https://arxiv.org/pdf/1506.07739
Fast 3D Synthetic Aperture Radar Imaging from Polarization-Diverse Measurements,Pierre Minvielle;Pierre Massaloux;Jean-François Giovannelli,"An innovative 3-D radar imaging technique is developed for fast and efficient identification and characterization of radar backscattering components of complex objects, when the collected scattered field is made of polarization-diverse measurements. In this context, all the polarimetric information seems irretrievably mixed. A direct model, derived from a simple but original extension of the widespread ""multiple scattering model"" leads to a high dimensional linear inverse problem. It is solved by a fast dedicated imaging algorithm that performs to determine at a time three huge 3-D scatterer maps which correspond to HH, VV and HV polarizations at emission and reception. It is applied successfully to various mock-ups and data sets collected from an accurate and dedicated 3D spherical experimental layout that provides concentric polarization-diverse RCS measurements. △ Less","24 June, 2015",https://arxiv.org/pdf/1506.07459
Comparing and evaluating extended Lambek calculi,Richard Moot,"Lambeks Syntactic Calculus, commonly referred to as the Lambek calculus, was innovative in many ways, notably as a precursor of linear logic. But it also showed that we could treat our grammatical framework as a logic (as opposed to a logical theory). However, though it was successful in giving at least a basic treatment of many linguistic phenomena, it was also clear that a slightly more expressive logical calculus was needed for many other cases. Therefore, many extensions and variants of the Lambek calculus have been proposed, since the eighties and up until the present day. As a result, there is now a large class of calculi, each with its own empirical successes and theoretical results, but also each with its own logical primitives. This raises the question: how do we compare and evaluate these different logical formalisms? To answer this question, I present two unifying frameworks for these extended Lambek calculi. Both are proof net calculi with graph contraction criteria. The first calculus is a very general system: you specify the structure of your sequents and it gives you the connectives and contractions which correspond to it. The calculus can be extended with structural rules, which translate directly into graph rewrite rules. The second calculus is first-order (multiplicative intuitionistic) linear logic, which turns out to have several other, independently proposed extensions of the Lambek calculus as fragments. I will illustrate the use of each calculus in building bridges between analyses proposed in different frameworks, in highlighting differences and in helping to identify problems. △ Less","18 June, 2015",https://arxiv.org/pdf/1506.05561
SuperGraph Visualization,Jose Rodrigues;Agma Traina;Christos Faloutsos;Caetano Traina,"Given a large social or computer network, how can we visualize it, find patterns, outliers, communities? Although several graph visualization tools exist, they cannot handle large graphs with hundred thousand nodes and possibly million edges. Such graphs bring two challenges: interactive visualization demands prohibitive processing power and, even if we could interactively update the visualization, the user would be overwhelmed by the excessive number of graphical items. To cope with this problem, we propose a formal innovation on the use of graph hierarchies that leads to GMine system. GMine promotes scalability using a hierarchy of graph partitions, promotes concomitant presentation for the graph hierarchy and for the original graph, and extends analytical possibilities with the integration of the graph partitions in an interactive environment. △ Less","15 June, 2015",https://arxiv.org/pdf/1506.04606
The WDAqua ITN: Answering Questions using Web Data,Christoph Lange;Saeedeh Shekarpour;Soren Auer,"WDAqua is a Marie Curie Innovative Training Network (ITN) and is funded under EU grant number 642795 and runs from January 2015 to December 2018. WDAqua aims at advancing the state of the art by intertwining training, research and innovation efforts, centered around one service: data-driven question answering. Question answering is immediately useful to a wide audience of end users, and we will demonstrate this in settings including e-commerce, public sector information, publishing and smart cities. Question answering also covers web science and data science broadly, leading to transferrable research results and to transferrable skills of the researchers who have finished our training programme. To ensure that our research improves question answering overall, every individual research project connects at least two of these steps. Intersectional secondments (within a consortium covering academia, research institutes and industrial research as well as network-wide workshops, R and D challenges and innovation projects further balance ground-breaking research and the needs of society and industry. Training-wise these offers equip early stage researchers with the expertise and transferable technical and non-technical skills that will allow them to pursue a successful career as an academic, decision maker, practitioner or entrepreneur. △ Less","10 June, 2015",https://arxiv.org/pdf/1506.04094
"GMine: A System for Scalable, Interactive Graph Visualization and Mining",Jose Rodrigues;Hanghang Tong;Agma Traina;Christos Faloutsos;Jure Leskovec,"Several graph visualization tools exist. However, they are not able to handle large graphs, and/or they do not allow interaction. We are interested on large graphs, with hundreds of thousands of nodes. Such graphs bring two challenges: the first one is that any straightforward interactive manipulation will be prohibitively slow. The second one is sensory overload: even if we could plot and replot the graph quickly, the user would be overwhelmed with the vast volume of information because the screen would be too cluttered as nodes and edges overlap each other. GMine system addresses both these issues, by using summarization and multi-resolution. GMine offers multi-resolution graph exploration by partitioning a given graph into a hierarchy of com-munities-within-communities and storing it into a novel R-tree-like structure which we name G-Tree. GMine offers summarization by implementing an innovative subgraph extraction algorithm and then visualizing its output. △ Less","11 June, 2015",https://arxiv.org/pdf/1506.03847
Spectral Representations for Convolutional Neural Networks,Oren Rippel;Jasper Snoek;Ryan P. Adams,"Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training. △ Less","11 June, 2015",https://arxiv.org/pdf/1506.03767
The Majority Illusion in Social Networks,Kristina Lerman;Xiaoran Yan;Xin-Zeng Wu,"Social behaviors are often contagious, spreading through a population as individuals imitate the decisions and choices of others. A variety of global phenomena, from innovation adoption to the emergence of social norms and political movements, arise as a result of people following a simple local rule, such as copy what others are doing. However, individuals often lack global knowledge of the behaviors of others and must estimate them from the observations of their friends' behaviors. In some cases, the structure of the underlying social network can dramatically skew an individual's local observations, making a behavior appear far more common locally than it is globally. We trace the origins of this phenomenon, which we call ""the majority illusion,"" to the friendship paradox in social networks. As a result of this paradox, a behavior that is globally rare may be systematically overrepresented in the local neighborhoods of many people, i.e., among their friends. Thus, the ""majority illusion"" may facilitate the spread of social contagions in networks and also explain why systematic biases in social perceptions, for example, of risky behavior, arise. Using synthetic and real-world networks, we explore how the ""majority illusion"" depends on network structure and develop a statistical model to calculate its magnitude in a network. △ Less","9 June, 2015",https://arxiv.org/pdf/1506.03022
Kinetics of Social Contagion,Zhongyuan Ruan;Gerardo Iniguez;Marton Karsai;Janos Kertesz,"Diffusion of information, behavioral patterns or innovations follows diverse pathways depending on a number of conditions, including the structure of the underlying social network, the sensitivity to peer pressure and the influence of media. Here we study analytically and by simulations a general model that incorporates threshold mechanism capturing sensitivity to peer pressure, the effect of `immune' nodes who never adopt, and a perpetual flow of external information. While any constant, non-zero rate of dynamically-introduced spontaneous adopters leads to global spreading, the kinetics by which the asymptotic state is approached shows rich behavior. In particular we find that, as a function of the immune node density, there is a transition from fast to slow spreading governed by entirely different mechanisms. This transition happens below the percolation threshold of network fragmentation, and has its origin in the competition between cascading behavior induced by adopters and blocking due to immune nodes. This change is accompanied by a percolation transition of the induced clusters. △ Less","30 October, 2015",https://arxiv.org/pdf/1506.00251
Transition-Based Dependency Parsing with Stack Long Short-Term Memory,Chris Dyer;Miguel Ballesteros;Wang Ling;Austin Matthews;Noah A. Smith,"We propose a technique for learning representations of parser states in transition-based dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks---the stack LSTM. Like the conventional stack data structures used in transition-based parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser's state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance. △ Less","29 May, 2015",https://arxiv.org/pdf/1505.08075
Large Graph Analysis in the GMine System,Jose F. Rodrigues Jr.;Hanghang Tong;Jia-Yu Pan;Agma J. M. Traina;Caetano Traina Jr.;Christos Faloutsos,"Current applications have produced graphs on the order of hundreds of thousands of nodes and millions of edges. To take advantage of such graphs, one must be able to find patterns, outliers and communities. These tasks are better performed in an interactive environment, where human expertise can guide the process. For large graphs, though, there are some challenges: the excessive processing requirements are prohibitive, and drawing hundred-thousand nodes results in cluttered images hard to comprehend. To cope with these problems, we propose an innovative framework suited for any kind of tree-like graph visual design. GMine integrates (a) a representation for graphs organized as hierarchies of partitions - the concepts of SuperGraph and Graph-Tree; and (b) a graph summarization methodology - CEPS. Our graph representation deals with the problem of tracing the connection aspects of a graph hierarchy with sub linear complexity, allowing one to grasp the neighborhood of a single node or of a group of nodes in a single click. As a proof of concept, the visual environment of GMine is instantiated as a system in which large graphs can be investigated globally and locally. △ Less","28 May, 2015",https://arxiv.org/pdf/1505.07777
Place Recognition with Event-based Cameras and a Neural Implementation of SeqSLAM,Michael Milford;Hanme Kim;Michael Mangan;Stefan Leutenegger;Tom Stone;Barbara Webb;Andrew Davison,"Event-based cameras offer much potential to the fields of robotics and computer vision, in part due to their large dynamic range and extremely high ""frame rates"". These attributes make them, at least in theory, particularly suitable for enabling tasks like navigation and mapping on high speed robotic platforms under challenging lighting conditions, a task which has been particularly challenging for traditional algorithms and camera sensors. Before these tasks become feasible however, progress must be made towards adapting and innovating current RGB-camera-based algorithms to work with event-based cameras. In this paper we present ongoing research investigating two distinct approaches to incorporating event-based cameras for robotic navigation: the investigation of suitable place recognition / loop closure techniques, and the development of efficient neural implementations of place recognition techniques that enable the possibility of place recognition using event-based cameras at very high frame rates using neuromorphic computing hardware. △ Less","18 May, 2015",https://arxiv.org/pdf/1505.04548
The quantitative and qualitative content analysis of marketing literature for innovative information systems: the Aldrich Archive,Sebastian Fass;Kevin Turner,"The Aldrich Archive is a collection of technical and marketing material covering the period from 1977 to 2000; the physical documents are in the process of being digitised and made available on the internet. The Aldrich Archive includes contemporaneous case studies of end-user computer systems that were used for marketing purposes. This paper analyses these case studies of innovative information systems 1980 - 1990 using a quantitative and qualitative content analysis. The major aim of this research paper is to find out how innovative information systems were marketed in the decade from 1980 to 1990. The paper uses a double-step content analysis and does not focus on one method of content analysis only. The reason for choosing this approach is to combine the advantages of both quantitative and qualitative content analysis. The results of the quantitative content analysis indicated that the focus of the marketing material would be on information management / information supply. But the qualitative analysis revealed that the focus is on monetary advantages. The strong focus on monetary advantages of information technology seems typical for the 1980s and 1990s. In 1987, Robert Solow stated you can see the computer age everywhere but in the productivity statistics. This paradox caused a lot of discussion: since the introduction of the IT productivity paradox the business value of information technology has been the topic of many debates by practitioners as well as by academics. △ Less","17 May, 2015",https://arxiv.org/pdf/1505.04401
A Prioritised Traffic Embedding Mechanism enabling a Public Safety Virtual Operator,Jonathan van de Belt;Hamed Ahmadi;Linda E. Doyle;Oriol Sallent,"Public Protection and Distaster Relief (PPDR) services can benefit greatly from the availability of mobile broadband communications in disaster and emergency scenarios. While undoubtedly offering full control and reliability, dedicated networks for PPDR have resulted in high operating costs and a lack of innovation in comparison to the commercial domain. Driven by the many benefits of broadband communications, PPDR operators are increasingly interested in adopting mainstream commercial technologies such as Long Term Evolution (LTE) in favour of expensive, dedicated narrow-band networks. In addition, the emergence of virtualization for wireless networks offers a new model for sharing infrastructure between several operators in a flexible and customizable manner. In this context, we propose a virtual Public Safety (PS) operator that relies on shared infrastructure of commercial LTE networks to deliver services to its users. We compare several methods of allocating spectrum resources between virtual operators at peak times and examine how this influences differing traffic services. We show that it is possible to provide services to the PS users reliably during both normal and emergency operation, and examine the impact on the commercial operators. △ Less","15 May, 2015",https://arxiv.org/pdf/1505.04148
Influence of Luddism on innovation diffusion,Andrew Mellor;Mauro Mobilia;Sidney Redner;Alastair M. Rucklidge;Jonathan A. Ward,"We generalize the classical Bass model of innovation diffusion to include a new class of agents --- Luddites --- that oppose the spread of innovation. Our model also incorporates ignorants, susceptibles, and adopters. When an ignorant and a susceptible meet, the former is converted to a susceptible at a given rate, while a susceptible spontaneously adopts the innovation at a constant rate. In response to the \emph{rate} of adoption, an ignorant may become a Luddite and permanently reject the innovation. Instead of reaching complete adoption, the final state generally consists of a population of Luddites, ignorants, and adopters. The evolution of this system is investigated analytically and by stochastic simulations. We determine the stationary distribution of adopters, the time needed to reach the final state, and the influence of the network topology on the innovation spread. Our model exhibits an important dichotomy: when the rate of adoption is low, an innovation spreads slowly but widely; in contrast, when the adoption rate is high, the innovation spreads rapidly but the extent of the adoption is severely limited by Luddites. △ Less","24 November, 2015",https://arxiv.org/pdf/1505.02020
Fast R-CNN,Ross Girshick,"This paper proposes a Fast Region-based Convolutional Network method (Fast R-CNN) for object detection. Fast R-CNN builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-CNN employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-CNN trains the very deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and achieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ (using Caffe) and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn. △ Less","27 September, 2015",https://arxiv.org/pdf/1504.08083
40 Gbps Access for Metro networks: Implications in terms of Sustainability and Innovation from an LCA Perspective,Reza Farrahi Moghaddam;Yves Lemieux;Mohamed Cheriet,"In this work, the implications of new technologies, more specifically the new optical FTTH technologies, are studied both from the functional and non-functional perspectives. In particular, some direct impacts are listed in the form of abandoning non-functional technologies, such as micro-registration, which would be implicitly required for having a functioning operation before arrival the new high-bandwidth access technologies. It is shown that such abandonment of non-functional best practices, which are mainly at the management level of ICT, immediately results in additional consumption and environmental footprint, and also there is a chance that some other new innovations might be 'missed.' Therefore, unconstrained deployment of these access technologies is not aligned with a possible sustainable ICT picture, except if they are regulated. An approach to pricing the best practices, including both functional and non-functional technologies, is proposed in order to develop a regulation and policy framework for a sustainable broadband access. △ Less","23 June, 2015",https://arxiv.org/pdf/1504.06262
An Elastic Image Registration Approach for Wireless Capsule Endoscope Localization,Isabel N. Figueiredo;Carlos Leal;Luís Pinto;Pedro N. Figueiredo;Richard Tsai,"Wireless Capsule Endoscope (WCE) is an innovative imaging device that permits physicians to examine all the areas of the Gastrointestinal (GI) tract. It is especially important for the small intestine, where traditional invasive endoscopies cannot reach. Although WCE represents an extremely important advance in medical imaging, a major drawback that remains unsolved is the WCE precise location in the human body during its operating time. This is mainly due to the complex physiological environment and the inherent capsule effects during its movement. When an abnormality is detected, in the WCE images, medical doctors do not know precisely where this abnormality is located relative to the intestine and therefore they can not proceed efficiently with the appropriate therapy. The primary objective of the present paper is to give a contribution to WCE localization, using image-based methods. The main focus of this work is on the description of a multiscale elastic image registration approach, its experimental application on WCE videos, and comparison with a multiscale affine registration. The proposed approach includes registrations that capture both rigid-like and non-rigid deformations, due respectively to the rigid-like WCE movement and the elastic deformation of the small intestine originated by the GI peristaltic movement. Under this approach a qualitative information about the WCE speed can be obtained, as well as the WCE location and orientation via projective geometry. The results of the experimental tests with real WCE video frames show the good performance of the proposed approach, when elastic deformations of the small intestine are involved in successive frames, and its superiority with respect to a multiscale affine image registration, which accounts for rigid-like deformations only and discards elastic deformations. △ Less","23 April, 2015",https://arxiv.org/pdf/1504.06206
"Contextualization of topics - browsing through terms, authors, journals and cluster allocations",Rob Koopman;Shenghui Wang;Andrea Scharnhorst,"This paper builds on an innovative Information Retrieval tool, Ariadne. The tool has been developed as an interactive network visualization and browsing tool for large-scale bibliographic databases. It basically allows to gain insights into a topic by contextualizing a search query (Koopman et al., 2015). In this paper, we apply the Ariadne tool to a far smaller dataset of 111,616 documents in astronomy and astrophysics. Labeled as the Berlin dataset, this data have been used by several research teams to apply and later compare different clustering algorithms. The quest for this team effort is how to delineate topics. This paper contributes to this challenge in two different ways. First, we produce one of the different cluster solution and second, we use Ariadne (the method behind it, and the interface - called LittleAriadne) to display cluster solutions of the different group members. By providing a tool that allows the visual inspection of the similarity of article clusters produced by different algorithms, we present a complementary approach to other possible means of comparison. More particular, we discuss how we can - with LittleAriadne - browse through the network of topical terms, authors, journals and cluster solutions in the Berlin dataset and compare cluster solutions as well as see their context. △ Less","16 April, 2015",https://arxiv.org/pdf/1504.04208
Promote the Industry Standard of Smart Home in China by Intelligent Router Technology,Hui Lin;Jianbiao Lin;Ke Ji;Jingjie Wang;Feng Lin,"The reason why smart home remains not popularized lies in bad product user experience, purchasing cost, and compatibility, and a lack of industry standard[1]. Echoing problems above, and having relentless devoted to software and hardware innovation and practice, we have independently developed a set of solution which is based on innovation and integration of router technology, mobile Internet technology,Internet of things technology,communication technology, digital-to-analog conversion and codec technology, and P2P technology among others. We have also established relevant protocols (without the application of protocols abroad). By doing this, we managed to establish a system with low and moderate price, superior performance, all-inclusive functions, easy installation, convenient portability, real-time reliability, security encryption, and the capability to manage home furnitures in an intelligent way. Only a new smart home system like this can inject new idea and energy into smart home industry and thus vigorously promote the establishment of smart home industry standard. △ Less","15 April, 2015",https://arxiv.org/pdf/1504.03912
Analyzing and Modeling Special Offer Campaigns in Location-based Social Networks,Ke Zhang;Konstantinos Pelechrinis;Theodoros Lappas,"The proliferation of mobile handheld devices in combination with the technological advancements in mobile computing has led to a number of innovative services that make use of the location information available on such devices. Traditional yellow pages websites have now moved to mobile platforms, giving the opportunity to local businesses and potential, near-by, customers to connect. These platforms can offer an affordable advertisement channel to local businesses. One of the mechanisms offered by location-based social networks (LBSNs) allows businesses to provide special offers to their customers that connect through the platform. We collect a large time-series dataset from approximately 14 million venues on Foursquare and analyze the performance of such campaigns using randomization techniques and (non-parametric) hypothesis testing with statistical bootstrapping. Our main finding indicates that this type of promotions are not as effective as anecdote success stories might suggest. Finally, we design classifiers by extracting three different types of features that are able to provide an educated decision on whether a special offer campaign for a local business will succeed or not both in short and long term. △ Less","9 April, 2015",https://arxiv.org/pdf/1504.02363
Design and Implementation of a 3D Undersea Camera System,Xida Chen;Steve Sutphen;Paul Macoun;Yee-Hong Yang,"In this paper, we present the design and development of an undersea camera system. The goal of our system is to provide a 3D model of the undersea habitat in a long-term continuous manner. The most important feature of our system is the use of multiple cameras and multiple projectors, which is able to provide accurate 3D models with an accuracy of a millimeter. By introducing projectors in our system, we can use many different structured light methods for different tasks. There are two main advantages comparing our system with using ROVs or AUVs. First, our system can provide continuous monitoring of the undersea habitat. Second, our system has a low hardware cost. Comparing to existing deployed camera systems, the advantage of our system is that it can provide accurate 3D models and provides opportunities for future development of innovative algorithms for undersea research. △ Less","7 April, 2015",https://arxiv.org/pdf/1504.01753
Simultaneously sparse and low-rank abundance matrix estimation for hyperspectral image unmixing,Paris Giampouras;Konstantinos Themelis;Athanasios Rontogiannis;Konstantinos Koutroumbas,"In a plethora of applications dealing with inverse problems, e.g. in image processing, social networks, compressive sensing, biological data processing etc., the signal of interest is known to be structured in several ways at the same time. This premise has recently guided the research to the innovative and meaningful idea of imposing multiple constraints on the parameters involved in the problem under study. For instance, when dealing with problems whose parameters form sparse and low-rank matrices, the adoption of suitably combined constraints imposing sparsity and low-rankness, is expected to yield substantially enhanced estimation results. In this paper, we address the spectral unmixing problem in hyperspectral images. Specifically, two novel unmixing algorithms are introduced, in an attempt to exploit both spatial correlation and sparse representation of pixels lying in homogeneous regions of hyperspectral images. To this end, a novel convex mixed penalty term is first defined consisting of the sum of the weighted \ell_1 and the weighted nuclear norm of the abundance matrix corresponding to a small area of the image determined by a sliding square window. This penalty term is then used to regularize a conventional quadratic cost function and impose simultaneously sparsity and row-rankness on the abundance matrix. The resulting regularized cost function is minimized by a) an incremental proximal sparse and low-rank unmixing algorithm and b) an algorithm based on the alternating minimization method of multipliers (ADMM). The effectiveness of the proposed algorithms is illustrated in experiments conducted both on simulated and real data. △ Less","14 October, 2015",https://arxiv.org/pdf/1504.01515
Adoption Factors for e-Malls in the SME Sector in Saudi Arabia,Adel A. Bahaddad;Rayed AlGhamdi;Salem Alkhalaf,"The small and medium-sized enterprise (SME) sector represents one of the fundamental pillars in the trade field. It contributes significantly to raising the economies of countries by providing significant numbers of job opportunities, which are beneficial to directly supporting national economies. One of the most important obstacles facing this sector in the information technology era is the lack of online trading channels with consumers, which require more financial support than the their capabilities. Therefore, e-Malls might be one of the best low-cost solutions to overcome this obstacle. Also, they provide electronic platforms that include most SME requirements for sales via electronic channels as well as offer essential technical support. According to a report published in 2013 by the Saudi Arabian Monetary Agency (SAMA), the percentage of SMEs is equivalent to 90% of the total number of companies in Saudi Arabia, which is numbered at 848,500. The e-Mall is a modern idea in Saudi Arabia that requires the use of the Disunion Of Innovation (DOI) approach to diffuse e-Malls through determining companies requirements and difficulties. Therefore, this paper focuses on the factors that help SMEs to adopt e-Malls. A quantitative questionnaire was conducted on 108 companies in Saudi Arabia to find what obstacles and requirements they face to adopt an e-Mall and focus on the factors affecting the implementation of this system, which are divided into organizational, technical and cultural factors. △ Less","5 April, 2015",https://arxiv.org/pdf/1504.01112
Fast algorithms for morphological operations using run-length encoded binary images,Gregor Ehrensperger;Alexander Ostermann;Felix Schwitzer,"This paper presents innovative algorithms to efficiently compute erosions and dilations of run-length encoded (RLE) binary images with arbitrary shaped structuring elements. An RLE image is given by a set of runs, where a run is a horizontal concatenation of foreground pixels. The proposed algorithms extract the skeleton of the structuring element and build distance tables of the input image, which are storing the distance to the next background pixel on the left and right hand sides. This information is then used to speed up the calculations of the erosion and dilation operator by enabling the use of techniques which allow to skip the analysis of certain pixels whenever a hit or miss occurs. Additionally the input image gets trimmed during the preprocessing steps on the base of two primitive criteria. Experimental results show the advantages over other algorithms. The source code of our algorithms is available in C++. △ Less","4 April, 2015",https://arxiv.org/pdf/1504.01052
Recent Development in Analog Computation - A Brief Overview,Yang Xue,"The recent development in analog computation is reviewed in this paper. Analog computation was used in many applications where power and energy efficiency is of paramount importance. It is shown that by using innovative architecture and circuit design, analog computation systems can achieve much higher energy efficiency than their digital counterparts, as they are able to exploit the computational power inherent to the devices and physics. However, these systems do suffer from some disadvantages, such as lower accuracy and speed, and designers have come up with novel approaches to overcome them. The paper provides an overview of analog computation systems, from basic components such as memory and arithmetic elements, to architecture and system design. △ Less","2 April, 2015",https://arxiv.org/pdf/1504.00450
Influence Maximization under The Non-progressive Linear Threshold Model,T. -H. Hubert Chan;Li Ning,"In the problem of influence maximization in information networks, the objective is to choose a set of initially active nodes subject to some budget constraints such that the expected number of active nodes over time is maximized. The linear threshold model has been introduced to study the opinion cascading behavior, for instance, the spread of products and innovations. In this paper, we we extends the classic linear threshold model [18] to capture the non-progressive be- havior. The information maximization problem under our model is proved to be NP-Hard, even for the case when the underlying network has no directed cycles. The first result of this paper is negative. In general, the objective function of the extended linear threshold model is no longer submodular, and hence the hill climbing approach that is commonly used in the existing studies is not applicable. Next, as the main result of this paper, we prove that if the underlying information network is directed acyclic, the objective function is submodular (and monotone). Therefore, in directed acyclic networks with a specified budget we can achieve 1/2 -approximation on maximizing the number of active nodes over a certain period of time by a deterministic algorithm, and achieve the (1 - 1/e )-approximation by a randomized algorithm. △ Less","1 April, 2015",https://arxiv.org/pdf/1504.00427
Error-Resilient Multicasting for Multi-View 3D Videos in Wireless Networks,Chi-Heng Lin;De-Nian Yang;Ji-Tang Lee;Wanjiun Liao,"With the emergence of naked-eye 3D mobile devices, mobile 3D video services are becoming increasingly important for video service providers, such as Youtube and Netflix, while multi-view 3D videos have the potential to inspire a variety of innovative applications. However, enabling multi-view 3D video services may overwhelm WiFi networks when every view of a video are multicasted. In this paper, therefore, we propose to incorporate depth-image-based rendering (DIBR), which allows each mobile client to synthesize the desired view from nearby left and right views, in order to effectively reduce the bandwidth consumption. Moreover, when each client suffers from packet losses, retransmissions incur additional bandwidth consumption and excess delay, which in turn undermines the quality of experience in video applications. To address the above issue, we first discover the merit of view protection via DIBR for multi-view video multicast using a mathematical analysis and then design a new protocol, named Multi-View Group Management Protocol (MVGMP), to support the dynamic join and leave of users and the change of desired views. The simulation results demonstrate that our protocol effectively reduces bandwidth consumption and increases the probability for each client to successfully playback the desired views in a multi-view 3D video. △ Less","16 October, 2015",https://arxiv.org/pdf/1503.08726
Towards Exascale Scientific Metadata Management,Spyros Blanas;Surendra Byna,"Advances in technology and computing hardware are enabling scientists from all areas of science to produce massive amounts of data using large-scale simulations or observational facilities. In this era of data deluge, effective coordination between the data production and the analysis phases hinges on the availability of metadata that describe the scientific datasets. Existing workflow engines have been capturing a limited form of metadata to provide provenance information about the identity and lineage of the data. However, much of the data produced by simulations, experiments, and analyses still need to be annotated manually in an ad hoc manner by domain scientists. Systematic and transparent acquisition of rich metadata becomes a crucial prerequisite to sustain and accelerate the pace of scientific innovation. Yet, ubiquitous and domain-agnostic metadata management infrastructure that can meet the demands of extreme-scale science is notable by its absence. To address this gap in scientific data management research and practice, we present our vision for an integrated approach that (1) automatically captures and manipulates information-rich metadata while the data is being produced or analyzed and (2) stores metadata within each dataset to permeate metadata-oblivious processes and to query metadata through established and standardized data access interfaces. We motivate the need for the proposed integrated approach using applications from plasma physics, climate modeling and neuroscience, and then discuss research challenges and possible solutions. △ Less","29 March, 2015",https://arxiv.org/pdf/1503.08482
Collective Dynamics of Hierarchical Networks,Liaquat Hossain;Rolf T. Wigand,"In an increasingly complex, mobile and interconnected world, we face growing threats of disasters, whether by chance or deliberately. Disruption of coordinated response and recovery efforts due to organizational, technical, procedural, random or deliberate attack could result in the risk of massive loss of life. This requires urgent action to explore the development of optimal information-sharing environments for promoting collective disaster response and preparedness using multijurisdictional hierarchical networks. Innovative approaches to information flow modeling and analysis for dealing with challenges of coordinating across multi layered agency structures as well as development of early warnings through social systems using social media analytics may be pivotal to timely responses to dealing with large scale disasters where response strategies need to be viewed as a shared responsibility. How do facilitate the development of collective disaster response in a multijurisdictional setting? How do we develop and test the level and effectiveness of shared multijurisdictional hierarchical networks for improved preparedness and response? What is the role of multi layered training and exercises in building the shared learning space for collective disaster preparedness and response? The aim of this is therefore to determine factors that may be responsible for affecting disaster response. △ Less","28 March, 2015",https://arxiv.org/pdf/1503.08264
Massive MIMO: Ten Myths and One Critical Question,Emil Björnson;Erik G. Larsson;Thomas L. Marzetta,"Wireless communications is one of the most successful technologies in modern years, given that an exponential growth rate in wireless traffic has been sustained for over a century (known as Cooper's law). This trend will certainly continue driven by new innovative applications; for example, augmented reality and internet-of-things. Massive MIMO (multiple-input multiple-output) has been identified as a key technology to handle orders of magnitude more data traffic. Despite the attention it is receiving from the communication community, we have personally witnessed that Massive MIMO is subject to several widespread misunderstandings, as epitomized by following (fictional) abstract: ""The Massive MIMO technology uses a nearly infinite number of high-quality antennas at the base stations. By having at least an order of magnitude more antennas than active terminals, one can exploit asymptotic behaviors that some special kinds of wireless channels have. This technology looks great at first sight, but unfortunately the signal processing complexity is off the charts and the antenna arrays would be so huge that it can only be implemented in millimeter wave bands."" The statements above are, in fact, completely false. In this overview article, we identify ten myths and explain why they are not true. We also ask a question that is critical for the practical adoption of the technology and which will require intense future research activities to answer properly. We provide references to key technical papers that support our claims, while a further list of related overview and technical papers can be found at the Massive MIMO Info Point: http://massivemimo.eu △ Less","18 August, 2015",https://arxiv.org/pdf/1503.06854
Persistence in voting behavior: stronghold dynamics in elections,Toni Pérez;Juan Fernández-Gracia;Jose J. Ramasco;Víctor M. Eguíluz,"Influence among individuals is at the core of collective social phenomena such as the dissemination of ideas, beliefs or behaviors, social learning and the diffusion of innovations. Different mechanisms have been proposed to implement inter-agent influence in social models from the voter model, to majority rules, to the Granoveter model. Here we advance in this direction by confronting the recently introduced Social Influence and Recurrent Mobility (SIRM) model, that reproduces generic features of vote-shares at different geographical levels, with data in the US presidential elections. Our approach incorporates spatial and population diversity as inputs for the opinion dynamics while individuals' mobility provides a proxy for social context, and peer imitation accounts for social influence. The model captures the observed stationary background fluctuations in the vote-shares across counties. We study the so-called political strongholds, i.e., locations where the votes-shares for a party are systematically higher than average. A quantitative definition of a stronghold by means of persistence in time of fluctuations in the voting spatial distribution is introduced, and results from the US Presidential Elections during the period 1980-2012 are analyzed within this framework. We compare electoral results with simulations obtained with the SIRM model finding a good agreement both in terms of the number and the location of strongholds. The strongholds duration is also systematically characterized in the SIRM model. The results compare well with the electoral results data revealing an exponential decay in the persistence of the strongholds with time. △ Less","23 March, 2015",https://arxiv.org/pdf/1503.06692
Data Science as a New Frontier for Design,Akin Osman;Kazakçi Mines,"The purpose of this paper is to contribute to the challenge of transferring know-how, theories and methods from design research to the design processes in information science and technologies. More specifically, we shall consider a domain, namely data-science, that is becoming rapidly a globally invested research and development axis with strong imperatives for innovation given the data deluge we are currently facing. We argue that, in order to rise to the data-related challenges that the society is facing, data-science initiatives should ensure a renewal of traditional research methodologies that are still largely based on trial-error processes depending on the talent and insights of a single (or a restricted group of) researchers. It is our claim that design theories and methods can provide, at least to some extent, the much-needed framework. We will use a worldwide data-science challenge organized to study a technical problem in physics, namely the detection of Higgs boson, as a use case to demonstrate some of the ways in which design theory and methods can help in analyzing and shaping the innovation dynamics in such projects. △ Less","20 March, 2015",https://arxiv.org/pdf/1503.06201
Distributed Time-Sensitive Task Selection in Mobile Crowdsensing,Man Hon Cheung;Richard Southwell;Fen Hou;Jianwei Huang,"With the rich set of embedded sensors installed in smartphones and the large number of mobile users, we witness the emergence of many innovative commercial mobile crowdsensing applications that combine the power of mobile technology with crowdsourcing to deliver time-sensitive and location-dependent information to their customers. Motivated by these real-world applications, we consider the task selection problem for heterogeneous users with different initial locations, movement costs, movement speeds, and reputation levels. Computing the social surplus maximization task allocation turns out to be an NP-hard problem. Hence we focus on the distributed case, and propose an asynchronous and distributed task selection (ADTS) algorithm to help the users plan their task selections on their own. We prove the convergence of the algorithm, and further characterize the computation time for users' updates in the algorithm. Simulation results suggest that the ADTS scheme achieves the highest Jain's fairness index and coverage comparing with several benchmark algorithms, while yielding similar user payoff to a greedy centralized benchmark. Finally, we illustrate how mobile users coordinate under the ADTS scheme based on some practical movement time data derived from Google Maps. △ Less","20 March, 2015",https://arxiv.org/pdf/1503.06007
"Does ""Like"" Really Mean Like? A Study of the Facebook Fake Like Phenomenon and an Efficient Countermeasure",Xinye Lin;Mingyuan Xia;Xue Liu,"Social networks help to bond people who share similar interests all over the world. As a complement, the Facebook ""Like"" button is an efficient tool that bonds people with the online information. People click on the ""Like"" button to express their fondness of a particular piece of information and in turn tend to visit webpages with high ""Like"" count. The important fact of the Like count is that it reflects the number of actual users who ""liked"" this information. However, according to our study, one can easily exploit the defects of the ""Like"" button to counterfeit a high ""Like"" count. We provide a proof-of-concept implementation of these exploits, and manage to generate 100 fake Likes in 5 minutes with a single account. We also reveal existing counterfeiting techniques used by some online sellers to achieve unfair advantage for promoting their products. To address this fake Like problem, we study the varying patterns of Like count and propose an innovative fake Like detection method based on clustering. To evaluate the effectiveness of our algorithm, we collect the Like count history of more than 9,000 websites. Our experiments successfully uncover 16 suspicious fake Like buyers that show abnormal Like count increase patterns. △ Less","30 March, 2015",https://arxiv.org/pdf/1503.05414
CENI: a Hybrid Framework for Efficiently Inferring Information Networks,Qingbo Hu;Sihong Xie;Shuyang Lin;Senzhang Wang;Philip Yu,"Nowadays, the message diffusion links among users or websites drive the development of countless innovative applications. However, in reality, it is easier for us to observe the timestamps when different nodes in the network react on a message, while the connections empowering the diffusion of the message remain hidden. This motivates recent extensive studies on the network inference problem: unveiling the edges from the records of messages disseminated through them. Existing solutions are computationally expensive, which motivates us to develop an efficient two-step general framework, Clustering Embedded Network Inference (CENI). CENI integrates clustering strategies to improve the efficiency of network inference. By clustering nodes directly on the timelines of messages, we propose two naive implementations of CENI: Infection-centric CENI and Cascade-centric CENI. Additionally, we point out the critical dimension problem of CENI: instead of one-dimensional timelines, we need to first project the nodes to an Euclidean space of certain dimension before clustering. A CENI adopting clustering method on the projected space can better preserve the structure hidden in the cascades, and generate more accurately inferred links. This insight sheds light on other related work attempting to discover or utilize the latent cluster structure in the disseminated messages. By addressing the critical dimension problem, we propose the third implementation of the CENI framework: Projection-based CENI. Through extensive experiments on two real datasets, we show that the three CENI models only need around 20% \sim 50% of the running time of state-of-the-art methods. Moreover, the inferred edges of Projection-based CENI preserves or even outperforms the effectiveness of state-of-the-art methods. △ Less","17 March, 2015",https://arxiv.org/pdf/1503.04927
Computer Assisted Parallel Program Generation,Shigeo Kawata,"Parallel computation is widely employed in scientific researches, engineering activities and product development. Parallel program writing itself is not always a simple task depending on problems solved. Large-scale scientific computing, huge data analyses and precise visualizations, for example, would require parallel computations, and the parallel computing needs the parallelization techniques. In this Chapter a parallel program generation support is discussed, and a computer-assisted parallel program generation system P-NCAS is introduced. Computer assisted problem solving is one of key methods to promote innovations in science and engineering, and contributes to enrich our society and our life toward a programming-free environment in computing science. Problem solving environments (PSE) research activities had started to enhance the programming power in 1970's. The P-NCAS is one of the PSEs; The PSE concept provides an integrated human-friendly computational software and hardware system to solve a target class of problems △ Less","15 March, 2015",https://arxiv.org/pdf/1503.04501
ICT and RFID in Education: Some Practical Aspects in Campus Life,Cristina Turcu;Cornel Turcu;Valentin Popa;Vasile Gaitan,"The paper summarizes our preliminary findings regarding the development and implementation of a newly proposed system based on ICT and RFID (Radio Frequency Identification) technologies for campus access and facility usage. It is generally acknowledged that any educational environment is highly dependent upon a wide range of resources or variables such as teaching staff, research and study areas, meeting and accommodation facilities, library services, restaurant and leisure facilities, etc. The system we have devised using ICT and RFID technologies supports not only authentic transactions among all university departments, but also interconnects all levels of academic life and activity. Thus, the utility of the system ranges from access control (student/ staff/ visitor identification), attendance tracking, library check-out services and voting to grade book consulting, inventory, cashless vending, parking, laundry and copying services. Physically, the system consists of several RFID gates/readers, a data server and some network stations, all of them requiring specific structuring and integration solutions. The system is quite different from already existing ones in that it proposes an innovative access solution. Thus, the search of the ID card holder in a database has been replaced by local processing. Since one and the same card is employed to perform a variety of operations, the system has immediate and numerous utilizations. △ Less","14 March, 2015",https://arxiv.org/pdf/1503.04286
Interactive Restless Multi-armed Bandit Game and Swarm Intelligence Effect,Shunsuke Yoshida;Masato Hisakado;Shintaro Mori,"We obtain the conditions for the emergence of the swarm intelligence effect in an interactive game of restless multi-armed bandit (rMAB). A player competes with multiple agents. Each bandit has a payoff that changes with a probability p_{c} per round. The agents and player choose one of three options: (1) Exploit (a good bandit), (2) Innovate (asocial learning for a good bandit among n_{I} randomly chosen bandits), and (3) Observe (social learning for a good bandit). Each agent has two parameters (c,p_{obs}) to specify the decision: (i) c, the threshold value for Exploit, and (ii) p_{obs}, the probability for Observe in learning. The parameters (c,p_{obs}) are uniformly distributed. We determine the optimal strategies for the player using complete knowledge about the rMAB. We show whether or not social or asocial learning is more optimal in the (p_{c},n_{I}) space and define the swarm intelligence effect. We conduct a laboratory experiment (67 subjects) and observe the swarm intelligence effect only if (p_{c},n_{I}) are chosen so that social learning is far more optimal than asocial learning. △ Less","13 March, 2015",https://arxiv.org/pdf/1503.03964
Predicting Virtual Learning Environment adoption - A case study,Sonam Penjor;Par-Ola Zander,"Purpose - To qualify the significance of Rogers' Diffusion of Innovations theory with regard to Virtual Learning Environments. To apply an existing Diffusion of Innovations instrument on a case organisation, the Royal University of Bhutan (RUB), in order to compare its results with previous findings. Descriptive statistics and logistic regression analysis were deployed to analyze adopter group memberships and predictor significance in Virtual Learning Environment adoption and use. Findings - The Diffusion of Innovations theory is not stable across organizations when it comes to predicting different user categories or the distribution of users. However, it was possible to achieve reliable results for virtual learning environments within a particular organization. Research limitations ND implications - The study questions scholarly attempts to establish models of this type across organizations. Practical implications - Professionals should be aware that cross-organizational generalizations from Diffusion Of Innovation findings within the domain of virtual learning environments may be very unreliable. Originality and value - The study challenges the massively cited Diffusion of Innovation literature. It provides data from Bhutan, which is underrepresented in empirical investigations. △ Less","18 August, 2015",https://arxiv.org/pdf/1503.02408
Measuring Technological Distance for Patent Mapping,Bowen Yan;Jianxi Luo,"Recent works in the information science literature have presented cases of using patent databases and patent classification information to construct network maps of technology fields, which aim to aid in competitive intelligence analysis and innovation decision making. Constructing such a patent network requires a proper measure of the distance between different classes of patents in the patent classification systems. Despite the existence of various distance measures in the literature, it is unclear how to consistently assess and compare them, and which ones to select for constructing patent technology network maps. This ambiguity has limited the development and applications of such technology maps. Herein, we propose to compare alternative distance measures and identify the superior ones by analyzing the differences and similarities in the structural properties of resulting patent network maps. Using United States patent data from 1976 to 2006 and International Patent Classification system, we compare 12 representative distance measures, which quantify inter-field knowledge base proximity, field-crossing diversification likelihood or frequency of innovation agents, and co-occurrences of patent classes in the same patents. Our comparative analyses suggest the patent technology network maps based on normalized co-reference and inventor diversification likelihood measures are the best representatives. △ Less","16 September, 2015",https://arxiv.org/pdf/1503.02373
Space proof complexity for random 3-CNFs,Patrick Bennett;Ilario Bonacina;Nicola Galesi;Tony Huynh;Mike Molloy;Paul Wollan,"We investigate the space complexity of refuting 3-CNFs in Resolution and algebraic systems. We prove that every Polynomial Calculus with Resolution refutation of a random 3-CNF φ in n variables requires, with high probability, Ω(n) distinct monomials to be kept simultaneously in memory. The same construction also proves that every Resolution refutation φ requires, with high probability, Ω(n) clauses each of width Ω(n) to be kept at the same time in memory. This gives a Ω(n^2) lower bound for the total space needed in Resolution to refute φ. These results are best possible (up to a constant factor). The main technical innovation is a variant of Hall's Lemma. We show that in bipartite graphs G with bipartition (L,R) and left-degree at most 3, L can be covered by certain families of disjoint paths, called VW-matchings, provided that L expands in R by a factor of (2-ε), for ε< 1/23. △ Less","2 April, 2015",https://arxiv.org/pdf/1503.01613
Sampling Sparse Signals on the Sphere: Algorithms and Applications,Ivan Dokmanic;Yue M. Lu,"We propose a sampling scheme that can perfectly reconstruct a collection of spikes on the sphere from samples of their lowpass-filtered observations. Central to our algorithm is a generalization of the annihilating filter method, a tool widely used in array signal processing and finite-rate-of-innovation (FRI) sampling. The proposed algorithm can reconstruct K spikes from (K+\sqrt{K})^2 spatial samples. This sampling requirement improves over previously known FRI sampling schemes on the sphere by a factor of four for large K. We showcase the versatility of the proposed algorithm by applying it to three different problems: 1) sampling diffusion processes induced by localized sources on the sphere, 2) shot noise removal, and 3) sound source localization (SSL) by a spherical microphone array. In particular, we show how SSL can be reformulated as a spherical sparse sampling problem. △ Less","3 March, 2015",https://arxiv.org/pdf/1502.07577
Optimal commitments in auctions with incomplete information,Zihe Wang;Pingzhong Tang,"We are interested in the problem of optimal commitments in rank-and-bid based auctions, a general class of auctions that include first price and all-pay auctions as special cases. Our main contribution is a novel approach to solve for optimal commitment in this class of auctions, for any continuous type distributions. Applying our approach, we are able to solve optimal commitments for first-price and all-pay auctions in closed-form for fairly general distribution settings. The optimal commitments functions in these auctions reveal two surprisingly opposite insights: in the optimal commitment, the leader bids passively when he has a low type. We interpret this as a credible way to alleviate competition and to collude. In sharp contrast, when his type is high enough, the leader sometimes would go so far as to bid above his own value. We interpret this as a credible way to threat. Combing both insights, we show via concrete examples that the leader is indeed willing to do so to secure more utility when his type is in the middle. Our main approach consists of a series of nontrivial innovations. In particular we put forward a concept called equal-bid function that connects both players' strategies, as well as a concept called equal-utility curve that smooths any leader strategy into a continuous and differentiable strategy. We believe these techniques and insights are general and can be applied to similar problems. △ Less","25 February, 2015",https://arxiv.org/pdf/1502.07431
Convergence Analysis using the Edge Laplacian: Robust Consensus of Nonlinear Multi-agent Systems via ISS Method,Zhiwen Zeng;Xiangke Wang;Zhiqiang Zheng,"This study develops an original and innovative matrix representation with respect to the information flow for networked multi-agent system. To begin with, the general concepts of the edge Laplacian of digraph are proposed with its algebraic properties. Benefit from this novel graph-theoretic tool, we can build a bridge between the consensus problem and the edge agreement problem; we also show that the edge Laplacian sheds a new light on solving the leaderless consensus problem. Based on the edge agreement framework, the technical challenges caused by unknown but bounded disturbances and inherently nonlinear dynamics can be well handled. In particular, we design an integrated procedure for a new robust consensus protocol that is based on a blend of algebraic graph theory and the newly developed cyclic-small-gain theorem. Besides, to highlight the intricate relationship between the original graph and cyclic-small-gain theorem, the concept of edge-interconnection graph is introduced for the first time. Finally, simulation results are provided to verify the theoretical analysis. △ Less","24 February, 2015",https://arxiv.org/pdf/1502.06732
Impact of Metro Cell Antenna Pattern and Downtilt in Heterogeneous Networks,Xiao Li;Robert W. Heath Jr.;Kevin Linehan;Ray Butler,"This article discusses the positive impact metro cell antennas with narrow vertical beamwidth and electrical downtilt can have on heterogeneous cellular networks. Using a model of random cell placement based on Poisson distribution, along with an innovative 3D building model that quantifies blockage due to shadowing, it is demonstrated that network spectral efficiency and average user throughput both increase as vertical beamwidth is decreased and downtilt is applied to metro cell transmission. Moreover, the network becomes more energy efficient. Importantly, these additional gains in network performance can be achieved without any cooperation or exchange of information between macro cell base stations and metro cells. △ Less","20 February, 2015",https://arxiv.org/pdf/1502.05782
Lessons from the Coinseminar,Peter Gloor;Maria Paasivaara;Christine Miller,This paper describes lessons learned from teaching a distributed virtual course on COINs (Collaborative Innovation Networks) over the last 12 years at five different sites located in four different time zones.,"18 February, 2015",https://arxiv.org/pdf/1502.05260
Knowledge-generating Efficiency in Innovation Systems: The relation between structural and temporal effects,Inga Ivanova;Loet Leydesdorff,"Using time series of US patents per million inhabitants, knowledge-generating cycles can be distinguished. These cycles partly coincide with Kondratieff long waves. The changes in the slopes between them indicate discontinuities in the knowledge-generating paradigms. The knowledge-generating paradigms can be modeled in terms of interacting dimensions (for example, in university-industry-government relations) that set limits to the maximal efficiency of innovation systems. The maximum values of the parameters in the model are of the same order as the regression coefficients of the empirical waves. The mechanism of the increase in the dimensionality is specified as self-organization which leads to the breaking of existing relations into the more diversified structure of a fractal-like network. This breaking can be modeled in analogy to 2D and 3D (Koch) snowflakes. The boost of knowledge generation leads to newly emerging technologies that can be expected to be more diversified and show shorter life cycles than before. Time spans of the knowledge-generating cycles can also be analyzed in terms of Fibonacci numbers. This perspective allows for forecasting expected dates of future possible paradigm changes. In terms of policy implications, this suggests a shift in focus from the manufacturing technologies to developing new organizational technologies and formats of human interactions △ Less","18 February, 2015",https://arxiv.org/pdf/1502.05145
CSAL: Self-adaptive Labeling based Clustering Integrating Supervised Learning on Unlabeled Data,Fangfang Li;Guandong Xu;Longbing Cao,"Supervised classification approaches can predict labels for unknown data because of the supervised training process. The success of classification is heavily dependent on the labeled training data. Differently, clustering is effective in revealing the aggregation property of unlabeled data, but the performance of most clustering methods is limited by the absence of labeled data. In real applications, however, it is time-consuming and sometimes impossible to obtain labeled data. The combination of clustering and classification is a promising and active approach which can largely improve the performance. In this paper, we propose an innovative and effective clustering framework based on self-adaptive labeling (CSAL) which integrates clustering and classification on unlabeled data. Clustering is first employed to partition data and a certain proportion of clustered data are selected by our proposed labeling approach for training classifiers. In order to refine the trained classifiers, an iterative process of Expectation-Maximization algorithm is devised into the proposed clustering framework CSAL. Experiments are conducted on publicly data sets to test different combinations of clustering algorithms and classification models as well as various training data labeling methods. The experimental results show that our approach along with the self-adaptive method outperforms other methods. △ Less","17 February, 2015",https://arxiv.org/pdf/1502.05111
"On Sex, Evolution, and the Multiplicative Weights Update Algorithm",Reshef Meir;David Parkes,"We consider a recent innovative theory by Chastain et al. on the role of sex in evolution [PNAS'14]. In short, the theory suggests that the evolutionary process of gene recombination implements the celebrated multiplicative weights updates algorithm (MWUA). They prove that the population dynamics induced by sexual reproduction can be precisely modeled by genes that use MWUA as their learning strategy in a particular coordination game. The result holds in the environments of \emph{weak selection}, under the assumption that the population frequencies remain a product distribution. We revisit the theory, eliminating both the requirement of weak selection and any assumption on the distribution of the population. Removing the assumption of product distributions is crucial, since as we show, this assumption is inconsistent with the population dynamics. We show that the marginal allele distributions induced by the population dynamics precisely match the marginals induced by a multiplicative weights update algorithm in this general setting, thereby affirming and substantially generalizing these earlier results. We further revise the implications for convergence and utility or fitness guarantees in coordination games. In contrast to the claim of Chastain et al.[PNAS'14], we conclude that the sexual evolutionary dynamics does not entail any property of the population distribution, beyond those already implied by convergence. △ Less","17 February, 2015",https://arxiv.org/pdf/1502.05056
FrogWild! -- Fast PageRank Approximations on Graph Engines,Ioannis Mitliagkas;Michael Borokhovich;Alexandros G. Dimakis;Constantine Caramanis,"We propose FrogWild, a novel algorithm for fast approximation of high PageRank vertices, geared towards reducing network costs of running traditional PageRank algorithms. Our algorithm can be seen as a quantized version of power iteration that performs multiple parallel random walks over a directed graph. One important innovation is that we introduce a modification to the GraphLab framework that only partially synchronizes mirror vertices. This partial synchronization vastly reduces the network traffic generated by traditional PageRank algorithms, thus greatly reducing the per-iteration cost of PageRank. On the other hand, this partial synchronization also creates dependencies between the random walks used to estimate PageRank. Our main theoretical innovation is the analysis of the correlations introduced by this partial synchronization process and a bound establishing that our approximation is close to the true PageRank vector. We implement our algorithm in GraphLab and compare it against the default PageRank implementation. We show that our algorithm is very fast, performing each iteration in less than one second on the Twitter graph and can be up to 7x faster compared to the standard GraphLab PageRank implementation. △ Less","14 February, 2015",https://arxiv.org/pdf/1502.04281
A Pseudo-Euclidean Iteration for Optimal Recovery in Noisy ICA,James Voss;Mikhail Belkin;Luis Rademacher,"Independent Component Analysis (ICA) is a popular model for blind signal separation. The ICA model assumes that a number of independent source signals are linearly mixed to form the observed signals. We propose a new algorithm, PEGI (for pseudo-Euclidean Gradient Iteration), for provable model recovery for ICA with Gaussian noise. The main technical innovation of the algorithm is to use a fixed point iteration in a pseudo-Euclidean (indefinite ""inner product"") space. The use of this indefinite ""inner product"" resolves technical issues common to several existing algorithms for noisy ICA. This leads to an algorithm which is conceptually simple, efficient and accurate in testing. Our second contribution is combining PEGI with the analysis of objectives for optimal recovery in the noisy ICA model. It has been observed that the direct approach of demixing with the inverse of the mixing matrix is suboptimal for signal recovery in terms of the natural Signal to Interference plus Noise Ratio (SINR) criterion. There have been several partial solutions proposed in the ICA literature. It turns out that any solution to the mixing matrix reconstruction problem can be used to construct an SINR-optimal ICA demixing, despite the fact that SINR itself cannot be computed from data. That allows us to obtain a practical and provably SINR-optimal recovery method for ICA with arbitrary Gaussian noise. △ Less","1 October, 2015",https://arxiv.org/pdf/1502.04148
Cornerstones of Sampling of Operator Theory,David Walnut;Götz E. Pfander;Thomas Kailath,"This paper reviews some results on the identifiability of classes of operators whose Kohn-Nirenberg symbols are band-limited (called band-limited operators), which we refer to as sampling of operators. We trace the motivation and history of the subject back to the original work of the third-named author in the late 1950s and early 1960s, and to the innovations in spread-spectrum communications that preceded that work. We give a brief overview of the NOMAC (Noise Modulation and Correlation) and Rake receivers, which were early implementations of spread-spectrum multi-path wireless communication systems. We examine in detail the original proof of the third-named author characterizing identifiability of channels in terms of the maximum time and Doppler spread of the channel, and do the same for the subsequent generalization of that work by Bello. The mathematical limitations inherent in the proofs of Bello and the third author are removed by using mathematical tools unavailable at the time. We survey more recent advances in sampling of operators and discuss the implications of the use of periodically-weighted delta-trains as identifiers for operator classes that satisfy Bello's criterion for identifiability, leading to new insights into the theory of finite-dimensional Gabor systems. We present novel results on operator sampling in higher dimensions, and review implications and generalizations of the results to stochastic operators, MIMO systems, and operators with unknown spreading domains. △ Less","11 February, 2015",https://arxiv.org/pdf/1502.03451
Semantics-based services for a low carbon society: An application on emissions trading system data and scenarios management,Cecilia Camporeale;Antonio De Nicola;Maria Luisa Villani,"A low carbon society aims at fighting global warming by stimulating synergic efforts from governments, industry and scientific communities. Decision support systems should be adopted to provide policy makers with possible scenarios, options for prompt countermeasures in case of side effects on environment, economy and society due to low carbon society policies, and also options for information management. A necessary precondition to fulfill this agenda is to face the complexity of this multi-disciplinary domain and to reach a common understanding on it as a formal specification. Ontologies are widely accepted means to share knowledge. Together with semantic rules, they enable advanced semantic services to manage knowledge in a smarter way. Here we address the European Emissions Trading System (EU-ETS) and we present a knowledge base consisting of the EREON ontology and a catalogue of rules. Then we describe two innovative semantic services to manage ETS data and information on ETS scenarios. △ Less","9 February, 2015",https://arxiv.org/pdf/1502.02417
Massively Multitask Networks for Drug Discovery,Bharath Ramsundar;Steven Kearnes;Patrick Riley;Dale Webster;David Konerding;Vijay Pande,"Massively multitask neural architectures provide a learning framework for drug discovery that synthesizes information from many distinct biological sources. To train these architectures at scale, we gather large amounts of data from public sources to create a dataset of nearly 40 million measurements across more than 200 biological targets. We investigate several aspects of the multitask framework by performing a series of empirical studies and obtain some interesting results: (1) massively multitask networks obtain predictive accuracies significantly better than single-task methods, (2) the predictive power of multitask networks improves as additional tasks and data are added, (3) the total amount of data and the total number of tasks both contribute significantly to multitask improvement, and (4) multitask networks afford limited transferability to tasks not in the training set. Our results underscore the need for greater data sharing and further algorithmic innovation to accelerate the drug discovery process. △ Less","6 February, 2015",https://arxiv.org/pdf/1502.02072
Graph Partitioning for Independent Sets,Sebastian Lamm;Peter Sanders;Christian Schulz,"Computing maximum independent sets in graphs is an important problem in computer science. In this paper, we develop an evolutionary algorithm to tackle the problem. The core innovations of the algorithm are very natural combine operations based on graph partitioning and local search algorithms. More precisely, we employ a state-of-the-art graph partitioner to derive operations that enable us to quickly exchange whole blocks of given independent sets. To enhance newly computed offsprings we combine our operators with a local search algorithm. Our experimental evaluation indicates that we are able to outperform state-of-the-art algorithms on a variety of instances. △ Less","5 February, 2015",https://arxiv.org/pdf/1502.01687
Big Data: Opportunities and Privacy Challenges,Hervais Simo Fhom,"Recent advances in data collection and computational statistics coupled with increases in computer processing power, along with the plunging costs of storage are making technologies to effectively analyze large sets of heterogeneous data ubiquitous. Applying such technologies (often referred to as big data technologies) to an ever growing number and variety of internal and external data sources, businesses and institutions can discover hidden correlations between data items, and extract actionable insights needed for innovation and economic growth. While on one hand big data technologies yield great promises, on the other hand, they raise critical security, privacy, and ethical issues, which if left unaddressed may become significant barriers to the fulfillment of expected opportunities and long-term success of big data. In this paper, we discuss the benefits of big data to individuals and society at large, focusing on seven key use cases: Big data for business optimization and customer analytics, big data and science, big data and health care, big data and finance, big data and the emerging energy distribution systems, big/open data as enablers of openness and efficiency in government, and big data security. In addition to benefits and opportunities, we discuss the security, privacy, and ethical issues at stake. △ Less","3 February, 2015",https://arxiv.org/pdf/1502.00823
Privacy-preserving Network Functionality Outsourcing,Junjie Shi;Yuan Zhang;Sheng Zhong,"Since the advent of software defined networks ({SDN}), there have been many attempts to outsource the complex and costly local network functionality, i.e. the middlebox, to the cloud in the same way as outsourcing computation and storage. The privacy issues, however, may thwart the enterprises' willingness to adopt this innovation since the underlying configurations of these middleboxes may leak crucial and confidential information which can be utilized by attackers. To address this new problem, we use firewall as an sample functionality and propose the first privacy preserving outsourcing framework and schemes in SDN. The basic technique that we exploit is a ground-breaking tool in cryptography, the \textit{cryptographic multilinear map}. In contrast to the infeasibility in efficiency if a naive approach is adopted, we devise practical schemes that can outsource the middlebox as a blackbox after \textit{obfuscating} it such that the cloud provider can efficiently perform the same functionality without knowing its underlying private configurations. Both theoretical analysis and experiments on real-world firewall rules demonstrate that our schemes are secure, accurate, and practical. △ Less","2 February, 2015",https://arxiv.org/pdf/1502.00389
Modified Fast Fractal Image Compression Algorithm in spatial domain,M. Salarian;H. Miar Naimi,"In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of searching it across [0,1]. Only the domain blocks with entropy greater than a threshold are considered as domain pool. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above, the remaining range blocks are split into four new smaller range blocks and the algorithm must be iterated for them, considered as the other step of encoding process. The algorithm has been examined for some of the well-known images and the results have been compared with the state-of-the-art algorithms. The experiments show that our proposed algorithm has considerably lower encoding time than the other where the encoded images are approximately the same in quality. △ Less","1 February, 2015",https://arxiv.org/pdf/1502.00324
Context-aware Computing in the Internet of Things: A Survey on Internet of Things From Industrial Market Perspective,Charith Perera;Chi Harold Liu Member;Srimal Jayawardena;Min Chen,"The Internet of Things (IoT) is a dynamic global information network consisting of Internet-connected objects, such as RFIDs, sensors, and actuators, as well as other instruments and smart appliances that are becoming an integral component of the Internet. Over the last few years, we have seen a plethora of IoT solutions making their way into the industry marketplace. Context-aware communication and computing has played a critical role throughout the last few years of ubiquitous computing and is expected to play a significant role in the IoT paradigm as well. In this article, we examine a variety of popular and innovative IoT solutions in terms of context-aware technology perspectives. More importantly, we evaluate these IoT solutions using a framework that we built around well-known context-aware computing theories. This survey is intended to serve as a guideline and a conceptual framework for contextaware product development and research in the IoT paradigm. It also provides a systematic exploration of existing IoT products in the marketplace and highlights a number of potentially significant research directions and trends. △ Less","31 January, 2015",https://arxiv.org/pdf/1502.00164
Particle swarm optimization for time series motif discovery,Joan Serrà;Josep Lluis Arcos,"Efficiently finding similar segments or motifs in time series data is a fundamental task that, due to the ubiquity of these data, is present in a wide range of domains and situations. Because of this, countless solutions have been devised but, to date, none of them seems to be fully satisfactory and flexible. In this article, we propose an innovative standpoint and present a solution coming from it: an anytime multimodal optimization algorithm for time series motif discovery based on particle swarms. By considering data from a variety of domains, we show that this solution is extremely competitive when compared to the state-of-the-art, obtaining comparable motifs in considerably less time using minimal memory. In addition, we show that it is robust to different implementation choices and see that it offers an unprecedented degree of flexibility with regard to the task. All these qualities make the presented solution stand out as one of the most prominent candidates for motif discovery in long time series streams. Besides, we believe the proposed standpoint can be exploited in further time series analysis and mining tasks, widening the scope of research and potentially yielding novel effective solutions. △ Less","29 January, 2015",https://arxiv.org/pdf/1501.07399
Extended Report: Fine-grained Recognition of Abnormal Behaviors for Early Detection of Mild Cognitive Impairment,Daniele Riboni;Claudio Bettini;Gabriele Civitarese;Zaffar Haider Janjua;Rim Helaoui,"According to the World Health Organization, the rate of people aged 60 or more is growing faster than any other age group in almost every country, and this trend is not going to change in a near future. Since senior citizens are at high risk of non communicable diseases requiring long-term care, this trend will challenge the sustainability of the entire health system. Pervasive computing can provide innovative methods and tools for early detecting the onset of health issues. In this paper we propose a novel method to detect abnormal behaviors of elderly people living at home. The method relies on medical models, provided by cognitive neuroscience researchers, describing abnormal activity routines that may indicate the onset of early symptoms of mild cognitive impairment. A non-intrusive sensor-based infrastructure acquires low-level data about the interaction of the individual with home appliances and furniture, as well as data from environmental sensors. Based on those data, a novel hybrid statistical-symbolical technique is used to detect the abnormal behaviors of the patient, which are communicated to the medical center. Differently from related works, our method can detect abnormal behaviors at a fine-grained level, thus providing an important tool to support the medical diagnosis. In order to evaluate our method we have developed a prototype of the system and acquired a large dataset of abnormal behaviors carried out in an instrumented smart home. Experimental results show that our technique is able to detect most anomalies while generating a small number of false positives. △ Less","22 January, 2015",https://arxiv.org/pdf/1501.05581
Sparse Bayesian Learning for EEG Source Localization,Sajib Saha;Frank de Hoog;Ya. I. Nesterets;Rajib Rana;M. Tahtali;T. E. Gureyev,"Purpose: Localizing the sources of electrical activity from electroencephalographic (EEG) data has gained considerable attention over the last few years. In this paper, we propose an innovative source localization method for EEG, based on Sparse Bayesian Learning (SBL). Methods: To better specify the sparsity profile and to ensure efficient source localization, the proposed approach considers grouping of the electrical current dipoles inside human brain. SBL is used to solve the localization problem in addition with imposed constraint that the electric current dipoles associated with the brain activity are isotropic. Results: Numerical experiments are conducted on a realistic head model that is obtained by segmentation of MRI images of the head and includes four major components, namely the scalp, the skull, the cerebrospinal fluid (CSF) and the brain, with appropriate relative conductivity values. The results demonstrate that the isotropy constraint significantly improves the performance of SBL. In a noiseless environment, the proposed method was 1 found to accurately (with accuracy of >75%) locate up to 6 simultaneously active sources, whereas for SBL without the isotropy constraint, the accuracy of finding just 3 simultaneously active sources was <75%. Conclusions: Compared to the state-of-the-art algorithms, the proposed method is potentially more consistent in specifying the sparsity profile of human brain activity and is able to produce better source localization for EEG. △ Less","19 January, 2015",https://arxiv.org/pdf/1501.04621
A Fast Fractal Image Compression Algorithm Using Predefined Values for Contrast Scaling,H. Miar Naimi;M. Salarian,"In this paper a new fractal image compression algorithm is proposed in which the time of encoding process is considerably reduced. The algorithm exploits a domain pool reduction approach, along with using innovative predefined values for contrast scaling factor, S, instead of scanning the parameter space [0,1]. Within this approach only domain blocks with entropies greater than a threshold are considered. As a novel point, it is assumed that in each step of the encoding process, the domain block with small enough distance shall be found only for the range blocks with low activity (equivalently low entropy). This novel point is used to find reasonable estimations of S, and use them in the encoding process as predefined values, mentioned above. The algorithm has been examined for some well-known images. This result shows that our proposed algorithm considerably reduces the encoding time producing images that are approximately the same in quality. △ Less","16 January, 2015",https://arxiv.org/pdf/1501.04140
Open-Source Software Implications in the Competitive Mobile Platforms Market,Salman Mian;Jose Teixeira;Eija Koskivaara,"The era of the PC platform left a legacy of competitive strategies for the future technologies to follow. However, this notion became more complicated, once the future grew out to be a present with huge bundle of innovative technologies, Internet capabilities, communication possibilities, and ease in life. A major step of moving from a product phone to a smart phone, eventually to a mobile device has created a new industry with humongous potential for further developments. The current mobile platform market is witnessing a platforms-war with big players such as Apple, Google, Nokia and Microsoft in a major role. An important aspect of today's mobile platform market is the contributions made through open source initiatives which promote innovation. This paper gives an insight into the open-source software strategies of the leading players and its implications on the market. It first gives a precise overview of the past leading to the current mobile platform market share state. Then it briefs about the open-source software components used and released by Apple, Google and Nokia platforms, leading to their mobile platform strategies with regard to open source. Finally, the paper assesses the situation from the point of view of communities of software developers complementing each platform. The authors identified relevant implications of the open-source phenomenon in the mobile-industry. △ Less","11 January, 2015",https://arxiv.org/pdf/1501.02476
Time Reversal-based Transmissions with Distributed Power Allocation for Two-Tier Networks,Vu Tran-Ha;Quang-Doanh Vu;Een-Kee Hong,"Radio pollution and power consumption problems lead to innovative development of green heterogeneous networks (HetNet). Time reversal (TR) technique which has been validated from wide- to narrow-band transmissions is evaluated as one of most prominent linear precoders with superior capability of harvesting signal energy. In this paper, we consider a new HetNet model, in which TR-employed femtocell is proposed to attain saving power benefits whereas macrocell utilizes the beam-forming algorithm based on zero-forcing principle, over frequency selective channels. In the considered HetNet, the practical case of limited signaling information exchanged via backhaul connections is also taken under advisement. We hence organize a distributed power loading strategy, in which macrocell users are treated with a superior priority compared to femtocell users. By Monte-Carlo simulation, the obtained results show that TR is preferred to zero-forcing in the perspective of beamforming technique for femtocell environments due to very high achievable gain in saving energy, and the validity of power loading strategy is verified over multipath channels. △ Less","8 January, 2015",https://arxiv.org/pdf/1501.01862
Optimal Radiometric Calibration for Camera-Display Communication,Wenjia Yuan;Eric Wengrowski;Kristin J. Dana;Ashwin Ashok;Marco Gruteser;Narayan Mandayam,"We present a novel method for communicating between a camera and display by embedding and recovering hidden and dynamic information within a displayed image. A handheld camera pointed at the display can receive not only the display image, but also the underlying message. These active scenes are fundamentally different from traditional passive scenes like QR codes because image formation is based on display emittance, not surface reflectance. Detecting and decoding the message requires careful photometric modeling for computational message recovery. Unlike standard watermarking and steganography methods that lie outside the domain of computer vision, our message recovery algorithm uses illumination to optically communicate hidden messages in real world scenes. The key innovation of our approach is an algorithm that performs simultaneous radiometric calibration and message recovery in one convex optimization problem. By modeling the photometry of the system using a camera-display transfer function (CDTF), we derive a physics-based kernel function for support vector machine classification. We demonstrate that our method of optimal online radiometric calibration (OORC) leads to an efficient and robust algorithm for computational messaging between nine commercial cameras and displays. △ Less","8 January, 2015",https://arxiv.org/pdf/1501.01744
Optimizing Path ORAM for Cloud Storage Applications,Nathan Wolfe;Ethan Zou;Ling Ren;Xiangyao Yu,"We live in a world where our personal data are both valuable and vulnerable to misappropriation through exploitation of security vulnerabilities in online services. For instance, Dropbox, a popular cloud storage tool, has certain security flaws that can be exploited to compromise a user's data, one of which being that a user's access pattern is unprotected. We have thus created an implementation of Path Oblivious RAM (Path ORAM) for Dropbox users to obfuscate path access information to patch this vulnerability. This implementation differs significantly from the standard usage of Path ORAM, in that we introduce several innovations, including a dynamically growing and shrinking tree architecture, multi-block fetching, block packing and the possibility for multi-client use. Our optimizations together produce about a 77% throughput increase and a 60% reduction in necessary tree size; these numbers vary with file size distribution. △ Less","7 January, 2015",https://arxiv.org/pdf/1501.01721
Super-resolution MRI Using Finite Rate of Innovation Curves,Greg Ongie;Mathews Jacob,"We propose a two-stage algorithm for the super-resolution of MR images from their low-frequency k-space samples. In the first stage we estimate a resolution-independent mask whose zeros represent the edges of the image. This builds off recent work extending the theory of sampling signals of finite rate of innovation (FRI) to two-dimensional curves. We enable its application to MRI by proposing extensions of the signal models allowed by FRI theory, and by developing a more robust and efficient means to determine the edge mask. In the second stage of the scheme, we recover the super-resolved MR image using the discretized edge mask as an image prior. We evaluate our scheme on simulated single-coil MR data obtained from analytical phantoms, and compare against total variation reconstructions. Our experiments show improved performance in both noiseless and noisy settings. △ Less","4 February, 2015",https://arxiv.org/pdf/1501.01697
Skincure: An Innovative Smart Phone-Based Application To Assist In Melanoma Early Detection And Prevention,Omar Abuzaghleh;Miad Faezipour;Buket D. Barkana,"Melanoma spreads through metastasis, and therefore it has been proven to be very fatal. Statistical evidence has revealed that the majority of deaths resulting from skin cancer are as a result of melanoma. Further investigations have shown that the survival rates in patients depend on the stage of the infection; early detection and intervention of melanoma implicates higher chances of cure. Clinical diagnosis and prognosis of melanoma is challenging since the processes are prone to misdiagnosis and inaccuracies due to doctors subjectivity. This paper proposes an innovative and fully functional smart-phone based application to assist in melanoma early detection and prevention. The application has two major components; the first component is a real-time alert to help users prevent skin burn caused by sunlight; a novel equation to compute the time for skin to burn is thereby introduced. The second component is an automated image analysis module which contains image acquisition, hair detection and exclusion, lesion segmentation, feature extraction, and classification. The proposed system exploits PH2 Dermoscopy image database from Pedro Hispano Hospital for development and testing purposes. The image database contains a total of 200 dermoscopy images of lesions, including normal, atypical, and melanoma cases. The experimental results show that the proposed system is efficient, achieving classification of the normal, atypical and melanoma images with accuracy of 96.3%, 95.7% and 97.5%, respectively. △ Less","5 January, 2015",https://arxiv.org/pdf/1501.01075
Sidecoin: a snapshot mechanism for bootstrapping a blockchain,Joseph Krug;Jack Peterson,"Sidecoin is a mechanism that allows a snapshot to be taken of Bitcoin's blockchain. We compile a list of Bitcoin's unspent transaction outputs, then use these outputs and their corresponding balances to bootstrap a new blockchain. This allows the preservation of Bitcoin's economic state in the context of a new blockchain, which may provide new features and technical innovations. △ Less","5 January, 2015",https://arxiv.org/pdf/1501.01039
Big Data Privacy in the Internet of Things Era,Charith Perera;Rajiv Ranjan;Lizhe Wang;Samee U. Khan;Albert Y. Zomaya,"Over the last few years, we have seen a plethora of Internet of Things (IoT) solutions, products and services, making their way into the industry's market-place. All such solution will capture a large amount of data pertaining to the environment, as well as their users. The objective of the IoT is to learn more and to serve better the system users. Some of these solutions may store the data locally on the devices ('things'), and others may store in the Cloud. The real value of collecting data comes through data processing and aggregation in large-scale where new knowledge can be extracted. However, such procedures can also lead to user privacy issues. This article discusses some of the main challenges of privacy in IoT, and opportunities for research and innovation. We also introduce some of the ongoing research efforts that address IoT privacy issues. △ Less","8 June, 2015",https://arxiv.org/pdf/1412.8339
"Fast, simple and accurate handwritten digit classification by training shallow neural network classifiers with the 'extreme learning machine' algorithm",Mark D. McDonnell;Migel D. Tissera;Tony Vladusich;André van Schaik;Jonathan Tapson,"Recent advances in training deep (multi-layer) architectures have inspired a renaissance in neural network use. For example, deep convolutional networks are becoming the default option for difficult tasks on large datasets, such as image and speech recognition. However, here we show that error rates below 1% on the MNIST handwritten digit benchmark can be replicated with shallow non-convolutional neural networks. This is achieved by training such networks using the 'Extreme Learning Machine' (ELM) approach, which also enables a very rapid training time (~10 minutes). Adding distortions, as is common practise for MNIST, reduces error rates even further. Our methods are also shown to be capable of achieving less than 5.5% error rates on the NORB image database. To achieve these results, we introduce several enhancements to the standard ELM algorithm, which individually and in combination can significantly improve performance. The main innovation is to ensure each hidden-unit operates only on a randomly sized and positioned patch of each image. This form of random `receptive field' sampling of the input ensures the input weight matrix is sparse, with about 90% of weights equal to zero. Furthermore, combining our methods with a small number of iterations of a single-batch backpropagation method can significantly reduce the number of hidden-units required to achieve a particular performance. Our close to state-of-the-art results for MNIST and NORB suggest that the ease of use and accuracy of the ELM algorithm for designing a single-hidden-layer neural network classifier should cause it to be given greater consideration either as a standalone method for simpler problems, or as the final classification stage in deep neural networks applied to more difficult problems. △ Less","22 July, 2015",https://arxiv.org/pdf/1412.8307
Clustering multi-way data: a novel algebraic approach,Eric Kernfeld;Shuchin Aeron;Misha Kilmer,"In this paper, we develop a method for unsupervised clustering of two-way (matrix) data by combining two recent innovations from different fields: the Sparse Subspace Clustering (SSC) algorithm [10], which groups points coming from a union of subspaces into their respective subspaces, and the t-product [18], which was introduced to provide a matrix-like multiplication for third order tensors. Our algorithm is analogous to SSC in that an ""affinity"" between different data points is built using a sparse self-representation of the data. Unlike SSC, we employ the t-product in the self-representation. This allows us more flexibility in modeling; infact, SSC is a special case of our method. When using the t-product, three-way arrays are treated as matrices whose elements (scalars) are n-tuples or tubes. Convolutions take the place of scalar multiplication. This framework allows us to embed the 2-D data into a vector-space-like structure called a free module over a commutative ring. These free modules retain many properties of complex inner-product spaces, and we leverage that to provide theoretical guarantees on our algorithm. We show that compared to vector-space counterparts, SSmC achieves higher accuracy and better able to cluster data with less preprocessing in some image clustering problems. In particular we show the performance of the proposed method on Weizmann face database, the Extended Yale B Face database and the MNIST handwritten digits database. △ Less","21 February, 2015",https://arxiv.org/pdf/1412.7056
DeepID-Net: Deformable Deep Convolutional Neural Networks for Object Detection,Wanli Ouyang;Xiaogang Wang;Xingyu Zeng;Shi Qiu;Ping Luo;Yonglong Tian;Hongsheng Li;Shuo Yang;Zhe Wang;Chen-Change Loy;Xiaoou Tang,"In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN \cite{girshick2014rich}, which was the state-of-the-art, from 31\% to 50.3\% on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1\%. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provide a global view for people to understand the deep learning object detection pipeline. △ Less","1 June, 2015",https://arxiv.org/pdf/1412.5661
Systemic risk analysis in reconstructed economic and financial networks,Giulio Cimini;Tiziano Squartini;Diego Garlaschelli;Andrea Gabrielli,"We address a fundamental problem that is systematically encountered when modeling complex systems: the limitedness of the information available. In the case of economic and financial networks, privacy issues severely limit the information that can be accessed and, as a consequence, the possibility of correctly estimating the resilience of these systems to events such as financial shocks, crises and cascade failures. Here we present an innovative method to reconstruct the structure of such partially-accessible systems, based on the knowledge of intrinsic node-specific properties and of the number of connections of only a limited subset of nodes. This information is used to calibrate an inference procedure based on fundamental concepts derived from statistical physics, which allows to generate ensembles of directed weighted networks intended to represent the real system, so that the real network properties can be estimated with their average values within the ensemble. Here we test the method both on synthetic and empirical networks, focusing on the properties that are commonly used to measure systemic risk. Indeed, the method shows a remarkable robustness with respect to the limitedness of the information available, thus representing a valuable tool for gaining insights on privacy-protected economic and financial systems. △ Less","20 May, 2015",https://arxiv.org/pdf/1411.7613
"Survey of End-to-End Mobile Network Measurement Testbeds, Tools, and Services",Utkarsh Goel;Mike P. Wittie;Kimberly C. Claffy;Andrew Le,"Mobile (cellular) networks enable innovation, but can also stifle it and lead to user frustration when network performance falls below expectations. As mobile networks become the predominant method of Internet access, developer, research, network operator, and regulatory communities have taken an increased interest in measuring end-to-end mobile network performance to, among other goals, minimize negative impact on application responsiveness. In this survey we examine current approaches to end-to-end mobile network performance measurement, diagnosis, and application prototyping. We compare available tools and their shortcomings with respect to the needs of researchers, developers, regulators, and the public. We intend for this survey to provide a comprehensive view of currently active efforts and some auspicious directions for future work in mobile network measurement and mobile application performance evaluation. △ Less","18 May, 2015",https://arxiv.org/pdf/1411.5003
Sparse Signal Processing Concepts for Efficient 5G System Design,Gerhard Wunder;Holger Boche;Thomas Strohmer;Peter Jung,"As it becomes increasingly apparent that 4G will not be able to meet the emerging demands of future mobile communication systems, the question what could make up a 5G system, what are the crucial challenges and what are the key drivers is part of intensive, ongoing discussions. Partly due to the advent of compressive sensing, methods that can optimally exploit sparsity in signals have received tremendous attention in recent years. In this paper we will describe a variety of scenarios in which signal sparsity arises naturally in 5G wireless systems. Signal sparsity and the associated rich collection of tools and algorithms will thus be a viable source for innovation in 5G wireless system design. We will discribe applications of this sparse signal processing paradigm in MIMO random access, cloud radio access networks, compressive channel-source network coding, and embedded security. We will also emphasize important open problem that may arise in 5G system design, for which sparsity will potentially play a key role in their solution. △ Less","26 January, 2015",https://arxiv.org/pdf/1411.0435
A New Approach to Customization of Collision Warning Systems to Individual Drivers,Ali Rakhshan;Evan Ray;Hossein Pishro-Nik,"This paper discusses the need for individualizing safety systems and proposes an approach including the Real-Time estimation of the distribution of brake response times for an individual driver. While maintaining high level of safety, the collision warning system should send ""tailored"" responses to the driver. This method could be the first step to show that safety applications would potentially benefit from customizing to individual drivers' characteristics using VANET. Our simulation results show that, as one of the imminent and preliminary outcomes of the new improved system, the number of false alarms will be reduced by more than 40%. We think this tactic can reach to even beyond the safety applications for designing the future innovative systems. △ Less","16 December, 2015",https://arxiv.org/pdf/1408.4111
Epidemic processes in complex networks,Romualdo Pastor-Satorras;Claudio Castellano;Piet Van Mieghem;Alessandro Vespignani,"In recent years the research community has accumulated overwhelming evidence for the emergence of complex and heterogeneous connectivity patterns in a wide range of biological and sociotechnical systems. The complex properties of real-world networks have a profound impact on the behavior of equilibrium and nonequilibrium phenomena occurring in various systems, and the study of epidemic spreading is central to our understanding of the unfolding of dynamical processes in complex networks. The theoretical analysis of epidemic spreading in heterogeneous networks requires the development of novel analytical frameworks, and it has produced results of conceptual and practical relevance. A coherent and comprehensive review of the vast research activity concerning epidemic processes is presented, detailing the successful theoretical approaches as well as making their limits and assumptions clear. Physicists, mathematicians, epidemiologists, computer, and social scientists share a common interest in studying epidemic spreading and rely on similar models for the description of the diffusion of pathogens, knowledge, and innovation. For this reason, while focusing on the main results and the paradigmatic models in infectious disease modeling, the major results concerning generalized social contagion processes are also presented. Finally, the research activity at the forefront in the study of epidemic spreading in coevolving, coupled, and time-varying networks is reported. △ Less","18 September, 2015",https://arxiv.org/pdf/1408.2701
Quality of Experience (QoE) beyond Quality of Service (QoS) as its baseline: QoE at the Interface of Experience Domains,Reza Farrahi Moghaddam;Mohamed Cheriet,"In this work, a new approach to the definition of the quality of experience is presented. By considering the quality of service as a baseline, that portion of the QoE that can be inferred from the QoS is excluded, and then the rest of the QoE is approached with the notion of QoE at a Boundary (QoEaaB). With the QoEaaB as the core of the proposed approach, various potential boundaries, and their associated unseen opportunities to improve the QoE are discussed. In particular, property, contract, SLA, and content are explored in terms of their boundaries and also their associated QoEaaB. With an interest in online video delivery, management of resource sharing and isolation associated with multi-tenant operations is considered. It is concluded that the proposed QoEaaB can bring a new perspective in QoE modeling and assessment toward a more enriched approach to improving the experience based on innovation and deep connectivity among actors. △ Less","9 February, 2015",https://arxiv.org/pdf/1407.5527
Discovery of Important Crossroads in Road Network using Massive Taxi Trajectories,Ming Xu;Jianping Wu;Yiman Du;Haohan Wang;Geqi Qi;Kezhen Hu;Yunpeng Xiao,"A major problem in road network analysis is discovery of important crossroads, which can provide useful information for transport planning. However, none of existing approaches addresses the problem of identifying network-wide important crossroads in real road network. In this paper, we propose a novel data-driven based approach named CRRank to rank important crossroads. Our key innovation is that we model the trip network reflecting real travel demands with a tripartite graph, instead of solely analysis on the topology of road network. To compute the importance scores of crossroads accurately, we propose a HITS-like ranking algorithm, in which a procedure of score propagation on our tripartite graph is performed. We conduct experiments on CRRank using a real-world dataset of taxi trajectories. Experiments verify the utility of CRRank. △ Less","18 September, 2015",https://arxiv.org/pdf/1407.2506
A Clustering Analysis of Tweet Length and its Relation to Sentiment,Matthew Mayo,"Sentiment analysis of Twitter data is performed. The researcher has made the following contributions via this paper: (1) an innovative method for deriving sentiment score dictionaries using an existing sentiment dictionary as seed words is explored, and (2) an analysis of clustered tweet sentiment scores based on tweet length is performed.","18 February, 2015",https://arxiv.org/pdf/1406.3287
Pattern Recognition in Narrative: Tracking Emotional Expression in Context,Fionn Murtagh;Adam Ganz,"Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel {\em Madame Bovary}. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives. △ Less","4 May, 2015",https://arxiv.org/pdf/1405.3539
Anatomy of Scientific Evolution,Jinhyuk Yun;Pan-Jun Kim;Hawoong Jeong,"The quest for historically impactful science and technology provides invaluable insight into the innovation dynamics of human society, yet many studies are limited to qualitative and small-scale approaches. Here, we investigate scientific evolution through systematic analysis of a massive corpus of digitized English texts between 1800 and 2008. Our analysis reveals great predictability for long-prevailing scientific concepts based on the levels of their prior usage. Interestingly, once a threshold of early adoption rates is passed even slightly, scientific concepts can exhibit sudden leaps in their eventual lifetimes. We developed a mechanistic model to account for such results, indicating that slowly-but-commonly adopted science and technology surprisingly tend to have higher innate strength than fast-and-commonly adopted ones. The model prediction for disciplines other than science was also well verified. Our approach sheds light on unbiased and quantitative analysis of scientific evolution in society, and may provide a useful basis for policy-making. △ Less","18 February, 2015",https://arxiv.org/pdf/1405.0917
The Composition Theorem for Differential Privacy,Peter Kairouz;Sewoong Oh;Pramod Viswanath,"Sequential querying of differentially private mechanisms degrades the overall privacy level. In this paper, we answer the fundamental question of characterizing the level of overall privacy degradation as a function of the number of queries and the privacy levels maintained by each privatization mechanism. Our solution is complete: we prove an upper bound on the overall privacy level and construct a sequence of privatization mechanisms that achieves this bound. The key innovation is the introduction of an operational interpretation of differential privacy (involving hypothesis testing) and the use of new data processing inequalities. Our result improves over the state-of-the-art, and has immediate applications in several problems studied in the literature including differentially private multi-party computation. △ Less","6 December, 2015",https://arxiv.org/pdf/1311.0776
Creative Gardens,Gerard Briscoe;Joseph Lockwood,"Can we move beyond simply networking creative individuals to establishing diverse communities of practice for innovation through discursive methods. Furthermore, can we digitise their creativity activities within an integrative socio-cultural collaborative technology platform that could then support distributed innovation. First, we consider the complexity of creative cultures from the perspective of design innovation, including how to nurture creativity activities in what we call Creative Gardens. Specifically, how they could grow, diverge, and combine, be- ing cultivated to nurture emergent, disruptive, collaborative innovation. Then, we consider the digitisation of Creative Gardens from the perspective of digital culture. Specifically, the tenets of Creative Gardens as dynamic and innovative communities. This includes considering the challenges and opportunities around digitisation, the influences around the connectivity with knowledge cultivation, and the potential for distributed innovation as collective intelligence to utilise diverse expertise. We conclude be considering the importance of the issues and questions raised, and their potential for the future. △ Less","18 February, 2015",https://arxiv.org/pdf/1309.5769
Characterizing co-NL by a group action,Clément Aubert;Thomas Seiller,"In a recent paper, Girard proposes to use his recent construction of a geometry of interaction in the hyperfinite factor in an innovative way to characterize complexity classes. We begin by giving a detailed explanation of both the choices and the motivations of Girard's definitions. We then provide a complete proof that the complexity class co-NL can be characterized using this new approach. We introduce as a technical tool the non-deterministic pointer machine, a concrete model to computes algorithms. △ Less","22 January, 2015",https://arxiv.org/pdf/1209.3422
Jigsaw percolation: What social networks can collaboratively solve a puzzle?,Charles D. Brummitt;Shirshendu Chatterjee;Partha S. Dey;David Sivakoff,"We introduce a new kind of percolation on finite graphs called jigsaw percolation. This model attempts to capture networks of people who innovate by merging ideas and who solve problems by piecing together solutions. Each person in a social network has a unique piece of a jigsaw puzzle. Acquainted people with compatible puzzle pieces merge their puzzle pieces. More generally, groups of people with merged puzzle pieces merge if the groups know one another and have a pair of compatible puzzle pieces. The social network solves the puzzle if it eventually merges all the puzzle pieces. For an Erdős-Rényi social network with n vertices and edge probability p_n, we define the critical value p_c(n) for a connected puzzle graph to be the p_n for which the chance of solving the puzzle equals 1/2. We prove that for the n-cycle (ring) puzzle, p_c(n)=Θ(1/\log n), and for an arbitrary connected puzzle graph with bounded maximum degree, p_c(n)=O(1/\log n) and ω(1/n^b) for any b>0. Surprisingly, with probability tending to 1 as the network size increases to infinity, social networks with a power-law degree distribution cannot solve any bounded-degree puzzle. This model suggests a mechanism for recent empirical claims that innovation increases with social density, and it might begin to show what social networks stifle creativity and what networks collectively innovate. △ Less","19 June, 2015",https://arxiv.org/pdf/1207.1927
