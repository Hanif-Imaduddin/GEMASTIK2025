title,authors,abstract,submitted_date,pdf_link
3D Convolution on RGB-D Point Clouds for Accurate Model-free Object Pose Estimation,Zhongang Cai;Cunjun Yu;Quang-Cuong Pham,"The conventional pose estimation of a 3D object usually requires the knowledge of the 3D model of the object. Even with the recent development in convolutional neural networks (CNNs), a 3D model is often necessary in the final estimation. In this paper, we propose a two-stage pipeline that takes in raw colored point cloud data and estimates an object's translation and rotation by running 3D convolutions on voxels. The pipeline is simple yet highly accurate: translation error is reduced to the voxel resolution (around 1 cm) and rotation error is around 5 degrees. The pipeline is also put to actual robotic grasping tests where it achieves above 90% success rate for test objects. Another innovation is that a motion capture system is used to automatically label the point cloud samples which makes it possible to rapidly collect a large amount of highly accurate real data for training the neural networks. △ Less","28 December, 2018",https://arxiv.org/pdf/1812.11284
"Open-endedness in AI systems, cellular evolution and intellectual discussions",Kushal Shah,"One of the biggest challenges that artificial intelligence (AI) research is facing in recent times is to develop algorithms and systems that are not only good at performing a specific intelligent task but also good at learning a very diverse of skills somewhat like humans do. In other words, the goal is to be able to mimic biological evolution which has produced all the living species on this planet and which seems to have no end to its creativity. The process of intellectual discussions is also somewhat similar to biological evolution in this regard and is responsible for many of the innovative discoveries and inventions that scientists and engineers have made in the past. In this paper, we present an information theoretic analogy between the process of discussions and the molecular dynamics within a cell, showing that there is a common process of information exchange at the heart of these two seemingly different processes, which can perhaps help us in building AI systems capable of open-ended innovation. We also discuss the role of consciousness in this process and present a framework for the development of open-ended AI systems. △ Less","28 December, 2018",https://arxiv.org/pdf/1812.10900
The Next Generation of Metadata-Oriented Testing of Research Software,Doug Mulholland;Paulo Alencar;Donald Cowan,"Research software refers to software development tools that accelerate discovery and simplifies access to digital infrastructures. However, although research software platforms can be built increasingly more innovative and powerful than ever before, with increasing complexity there is a greater risk of failure if unplanned for and untested program scenarios arise. As systems age and are changed by different programmers the risk of a change impacting the overall system increases. In contrast, systems that are built with less emphasis on program code and more emphasis on data that describes the application can be more readily changed and maintained by individuals who are less technically skilled but are often more familiar with the application domain. Such systems can also be tested using automatically generated advanced testing regimes. △ Less","25 December, 2018",https://arxiv.org/pdf/1812.10178
Self-Localization of Parking Robots Using Square-Like Landmarks,Canbo Ye;Guang Chen;Sanqing Qu;Qianyi Yang;Kai Chen;Jiatong Du;Ruien Hu,"In this paper, we present a framework for self-localization of parking robots in a parking lot innovatively using square-like landmarks, aiming to provide a positioning solution with low cost but high accuracy. It utilizes square structures common in parking lots such as pillars, corners or charging piles as robust landmarks and deduces the global pose of the robot in conjunction with an off-line map. The localization is performed in real-time via Particle Filter using a single line scanning LiDAR as main sensor, an odometry as secondary information sources. The system has been tested in a simulation environment built in V-REP, the result of which demonstrates its positioning accuracy below 0.20 m and a corresponding heading error below 1°. △ Less","23 December, 2018",https://arxiv.org/pdf/1812.09668
Intelligent Tutoring Systems: A Comprehensive Historical Survey with Recent Developments,Ali Alkhatlan;Jugal Kalita,"This paper provides interested beginners with an updated and detailed introduction to the field of Intelligent Tutoring Systems (ITS). ITSs are computer programs that use artificial intelligence techniques to enhance and personalize automation in teaching. This paper is a literature review that provides the following: First, a review of the history of ITS along with a discussion on the interface between human learning and computer tutors and how effective ITSs are in contemporary education. Second, the traditional architectural components of an ITS and their functions are discussed along with approaches taken by various ITSs. Finally, recent innovative ideas in ITS systems are presented. This paper concludes with some of the author's views regarding future work in the field of intelligent tutoring systems. △ Less","22 December, 2018",https://arxiv.org/pdf/1812.09628
Identifying interdisciplinarity through the disciplinary classification of co-authors of scientific publications,Giovanni Abramo;Ciriaco Andrea D'Angelo;Flavia Di Costa,"The growing complexity of challenges involved in scientific progress demands ever more frequent application of competencies and knowledge from different scientific fields. The present work analyzes the degree of collaboration among scientists from different disciplines in order to identify the most frequent ""combinations of knowledge"" in research activity. The methodology adopts an innovative bibliometric approach based on the disciplinary affiliation of publication co-authors. The field of observation includes all publications (173,134) indexed in the Science Citation Index Expanded (SCI-E) for the five years 2004-2008, authored by all scientists in the hard sciences (43,223) at Italian universities (68). The analysis examines 205 research fields grouped in nine disciplines. Identifying the fields with the highest potential of interdisciplinary collaboration is useful to inform research polices at national and regional levels, as well as management strategies at the institutional level. △ Less","21 December, 2018",https://arxiv.org/pdf/1812.09184
Impact of Social Media Posts in Real life Violence: A Case Study in Bangladesh,Jibon Naher;Matiur Rahman Minar,"Social Networking Site (SNS) is a great innovation of modern times. Facebook, Twitter etc. have become an everyday part of peoples' life. Among all SNSs, Facebook is the most popular social network all over the world. Bangladesh is no exception. People of Bangladesh use Facebook for social communication, online shopping, business, knowledge and experience sharing etc. As well as the various uses of SNSs, people sometimes find themselves involved in real life violence, provoked by some social media posts or activities. In this paper, we discussed some case studies in which real life violence is originated based on Facebook activities in Bangladesh. Facebook was used in these incidents intentionally or unintentionally mostly as a tool to trigger hatred and violence. We analyzed and discussed the real-world consequences of these virtual activities in social media. Lastly, we recommended possible future measurements to prevent such violence. △ Less","19 December, 2018",https://arxiv.org/pdf/1812.08660
Fault Diagnosis for Distributed Systems using Accuracy Technique,Poorva Kulkarni;Varsha Deshpande;Latika Sarna;Sumedha Shenolikar;Supriya Kelkar,"Distributed Systems involve two or more computer systems which may be situated at geographically distinct locations and are connected by a communication network. Due to failures in the communication link, faults arise which may make the entire system dysfunctional. To enable seamless operation of the distributed system, these faults need to be detected and located accurately. This paper examines various techniques of handling faults in distributed systems and proposes and innovative technique which uses percent accuracy for detecting faulty nodes in the system. Every node in the system acts as an initiator and votes for certifying faulty nodes in the system. This certification is done on the basis of percent accuracy value of each faulty node which should exceed a predefined threshold value to qualify node as faulty. As the threshold increases, the number of faulty nodes detected in the system reduces. This is a decentralized approach with no dependency on a single node to act as a leader for diagnosis. This technique is also applicable to ad-hoc networks, which are static in nature. △ Less","19 December, 2018",https://arxiv.org/pdf/1812.07771
Assistive robotic device: evaluation of intelligent algorithms,Audrey Lebrasseur;Josiane Lettre;François Routhier;Philippe Archambault;Alexandre Campeau-Lecours,"Assistive robotic devices can be used to help people with upper body disabilities gaining more autonomy in their daily life. Although basic motions such as positioning and orienting an assistive robot gripper in space allow performance of many tasks, it might be time consuming and tedious to perform more complex tasks. To overcome these difficulties, improvements can be implemented at different levels, such as mechanical design, control interfaces and intelligent control algorithms. In order to guide the design of solutions, it is important to assess the impact and potential of different innovations. This paper thus presents the evaluation of three intelligent algorithms aiming to improve the performance of the JACO robotic arm (Kinova Robotics). The evaluated algorithms are 'preset position', 'fluidity filter' and 'drinking mode'. The algorithm evaluation was performed with 14 motorized wheelchair's users and showed a statistically significant improvement of the robot's performance. △ Less","18 December, 2018",https://arxiv.org/pdf/1812.07342
Analogy Search Engine: Finding Analogies in Cross-Domain Research Papers,Jieli Zhou;Yuntao Zhou;Yi Xu,"In recent years, with the rapid proliferation of research publications in the field of Artificial Intelligence, it is becoming increasingly difficult for researchers to effectively keep up with all the latest research in one's own domains. However, history has shown that scientific breakthroughs often come from collaborations of researchers from different domains. Traditional search algorithms like Lexical search, which look for literal matches or synonyms and variants of the query words, are not effective for discovering cross-domain research papers and meeting the needs of researchers in this age of information overflow. In this paper, we developed and tested an innovative semantic search engine, Analogy Search Engine (ASE), for 2000 AI research paper abstracts across domains like Language Technologies, Robotics, Machine Learning, Computational Biology, Human Computer Interactions, etc. ASE combines recent theories and methods from Computational Analogy and Natural Language Processing to go beyond keyword-based lexical search and discover the deeper analogical relationships among research paper abstracts. We experimentally show that ASE is capable of finding more interesting and useful research papers than baseline elasticsearch. Furthermore, we believe that the methods used in ASE go beyond academic paper and will benefit many other document search tasks. △ Less","17 December, 2018",https://arxiv.org/pdf/1812.06974
Walking Through an Exploded Star: Rendering Supernova Remnant Cassiopeia A into Virtual Reality,Kimberly K. Arcand;Elaine Jiang;Sara Price;Megan Watzke;Tom Sgouros;Peter Edmonds,"NASA and other astrophysical data of the Cassiopeia A supernova remnant have been rendered into a three-dimensional virtual reality (VR) and augmented reality (AR) program, the first of its kind. This data-driven experience of a supernova remnant allows viewers to walk inside the leftovers from the explosion of a massive star, select the parts of the supernova remnant to engage with, and access descriptive texts on what the materials are. The basis of this program is a unique 3D model of the 340-year old remains of a stellar explosion, made by combining data from the NASA Chandra X-ray Observatory, Spitzer Space Telescope, and ground-based facilities. A collaboration between the Smithsonian Astrophysical Observatory and Brown University allowed the 3D astronomical data collected on Cassiopeia A to be featured in the VR/AR program, which is an innovation in digital technologies with public, education, and research-based impacts. △ Less","15 December, 2018",https://arxiv.org/pdf/1812.06237
A Survey on Blockchain Technology and Its Potential Applications in Distributed Control and Cooperative Robots,Ameer Tamoor Khan;Xinwei Cao;Shuai Li,"As a disruptive technology, blockchain, particularly its original form of bitcoin as a type of digital currency, has attracted great attentions. The innovative distributed decision making and security mechanism lay the technical foundation for its success, making us consider to penetrate the power of blockchain technology to distributed control and cooperative robotics, in which the distributed and secure mechanism is also highly demanded. Actually, security and distributed communication have long been unsolved problems in the field of distributed control and cooperative robotics. It has been reported on the network failure and intruder attacks of distributed control and multi-robotic systems. Blockchain technology provides promise to remedy this situation thoroughly. This work is intended to create a global picture of blockchain technology on its working principle and key elements in the language of control and robotics, to provide a shortcut for beginners to step into this research field. △ Less","20 December, 2018",https://arxiv.org/pdf/1812.05452
"Fission: A Provably Fast, Scalable, and Secure Permissionless Blockchain",Ke Liang,"We present Fission, a new permissionless blockchain that achieves scalability in both terms of system throughput and transaction confirmation time, while at the same time, retaining blockchain's core values of equality and decentralization. Fission overcomes the system throughput bottleneck by employing a novel Eager-Lazy pipeling model that achieves very high system throughputs via block pipelining, an adaptive partitioning mechanism that auto-scales to transaction volumes, and a provably secure energy-efficient consensus protocol to ensure security and robustness. Fission applies a hybrid network which consists of a relay network, and a peer-to-peer network. The goal of the relay network is to minimize the transaction confirmation time by minimizing the information propagation latency. To optimize the performance on the relay network in the presence of churn, dynamic network topologies, and network heterogeneity, we propose an ultra-fast game-theoretic relay selection algorithm that achieves near-optimal performance in a fully distributed manner. Fission's peer-to-peer network complements the relay network and provides a very high data availability via enabling users to contribute their storage and bandwidth for information dissemination (with incentive). We propose a distributed online data retrieval strategy that optimally offloads the relay network without degrading the system performance. By re-innovating all the core elements of the blockchain technology - computation, networking, and storage - in a holistic manner, Fission aims to achieve the best balance among scalability, security and decentralization. △ Less","14 December, 2018",https://arxiv.org/pdf/1812.05032
Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs,Claude Coulombe,"In practice, it is common to find oneself with far too little text data to train a deep neural network. This ""Big Data Wall"" represents a challenge for minority language communities on the Internet, organizations, laboratories and companies that compete the GAFAM (Google, Amazon, Facebook, Apple, Microsoft). While most of the research effort in text data augmentation aims on the long-term goal of finding end-to-end learning solutions, which is equivalent to ""using neural networks to feed neural networks"", this engineering work focuses on the use of practical, robust, scalable and easy-to-implement data augmentation pre-processing techniques similar to those that are successful in computer vision. Several text augmentation techniques have been experimented. Some existing ones have been tested for comparison purposes such as noise injection or the use of regular expressions. Others are modified or improved techniques like lexical replacement. Finally more innovative ones, such as the generation of paraphrases using back-translation or by the transformation of syntactic trees, are based on robust, scalable, and easy-to-use NLP Cloud APIs. All the text augmentation techniques studied, with an amplification factor of only 5, increased the accuracy of the results in a range of 4.3% to 21.6%, with significant statistical fluctuations, on a standardized task of text polarity prediction. Some standard deep neural network architectures were tested: the multilayer perceptron (MLP), the long short-term memory recurrent network (LSTM) and the bidirectional LSTM (biLSTM). Classical XGBoost algorithm has been tested with up to 2.5% improvements. △ Less","4 December, 2018",https://arxiv.org/pdf/1812.04718
Pible: Battery-Free Mote for Perpetual Indoor BLE Applications,Francesco Fraternali;Bharathan Balaji;Yuvraj Agarwal;Luca Benini;Rajesh Gupta,"Smart building applications require a large-scale deployment of sensors distributed across the environment. Recent innovations in smart environments are driven by wireless networked sensors as they are easy to deploy. However, replacing these batteries at scale is a non-trivial, labor-intensive task. Energy harvesting has emerged as a potential solution to avoid battery replacement but requires compromises such as application specific design, simplified communication protocol or reduced quality of service. We explore the design space of battery-free sensor nodes using commercial off the shelf components, and present Pible: a Perpetual Indoor BLE sensor node that leverages ambient light and can support numerous smart building applications. We analyze node-lifetime, quality of service and light availability trade-offs and present a predictive algorithm that adapts to changing lighting conditions to maximize node lifetime and application quality of service. Using a 20 node, 15-day deployment in a real building under varying lighting conditions, we show feasible applications that can be implemented using Pible and the boundary conditions under which they can fail. △ Less","27 November, 2018",https://arxiv.org/pdf/1812.04717
"Serverless Computing: One Step Forward, Two Steps Back",Joseph M. Hellerstein;Jose Faleiro;Joseph E. Gonzalez;Johann Schleier-Smith;Vikram Sreekanti;Alexey Tumanov;Chenggang Wu,"Serverless computing offers the potential to program the cloud in an autoscaling, pay-as-you go manner. In this paper we address critical gaps in first-generation serverless computing, which place its autoscaling potential at odds with dominant trends in modern computing: notably data-centric and distributed computing, but also open source and custom hardware. Put together, these gaps make current serverless offerings a bad fit for cloud innovation and particularly bad for data systems innovation. In addition to pinpointing some of the main shortfalls of current serverless architectures, we raise a set of challenges we believe must be met to unlock the radical potential that the cloud---with its exabytes of storage and millions of cores---should offer to innovative developers. △ Less","10 December, 2018",https://arxiv.org/pdf/1812.03651
MPTCP Linux Kernel Congestion Controls,Bruno Yuji Lino Kimura;Antonio Alfredo Frederico Loureiro,"MultiPath TCP (MPTCP) is a promising protocol which brings new light to the TCP/IP protocol stack ossification problem by means of an impactful innovation of the transport layer. A MPTCP connection consists of a set of one or more subflows, where each subflow offers an alternative path to reach a target remote end-system. However, simply applying the standard TCP congestion control on each subflow would give an unfair resource sharing. Various subflows of a connection would dispute bottleneck links with regular single-path TCP connections, leading them to starvation conditions. To deal with this concern, a multipath congestion control algorithm adjusts the sending operation of all subflows in a coupled fashion in order to achieve various objectives, e.g., friendliness, responsiveness, throughput improvement, and congestion balance. In this report, we describe the four coupled congestion control algorithms deployed in the MPTCP Linux kernel implementation, namely: LIA, OLIA, BALIA, and wVegas. We provide a concise material with technical details of each algorithm, while summarizing all of them together from a single notation. △ Less","7 December, 2018",https://arxiv.org/pdf/1812.03210
Stacked Dense U-Nets with Dual Transformers for Robust Face Alignment,Jia Guo;Jiankang Deng;Niannan Xue;Stefanos Zafeiriou,"Facial landmark localisation in images captured in-the-wild is an important and challenging problem. The current state-of-the-art revolves around certain kinds of Deep Convolutional Neural Networks (DCNNs) such as stacked U-Nets and Hourglass networks. In this work, we innovatively propose stacked dense U-Nets for this task. We design a novel scale aggregation network topology structure and a channel aggregation building block to improve the model's capacity without sacrificing the computational complexity and model size. With the assistance of deformable convolutions inside the stacked dense U-Nets and coherent loss for outside data transformation, our model obtains the ability to be spatially invariant to arbitrary input face images. Extensive experiments on many in-the-wild datasets, validate the robustness of the proposed method under extreme poses, exaggerated expressions and heavy occlusions. Finally, we show that accurate 3D face alignment can assist pose-invariant face recognition where we achieve a new state-of-the-art accuracy on CFP-FP. △ Less","5 December, 2018",https://arxiv.org/pdf/1812.01936
An Evolutionary Hierarchical Interval Type-2 Fuzzy Knowledge Representation System (EHIT2FKRS) for Travel Route Assignment,Mariam Zouari;Nesrine Baklouti;Javier Sanchez Medina;Mounir Ben Ayed;Adel M. Alimi,"Urban Traffic Networks are characterized by high dynamics of traffic flow and increased travel time, including waiting times. This leads to more complex road traffic management. The present research paper suggests an innovative advanced traffic management system based on Hierarchical Interval Type-2 Fuzzy Logic model optimized by the Particle Swarm Optimization (PSO) method. The aim of designing this system is to perform dynamic route assignment to relieve traffic congestion and limit the unexpected fluctuation effects on traffic flow. The suggested system is executed and simulated using SUMO, a well-known microscopic traffic simulator. For the present study, we have tested four large and heterogeneous metropolitan areas located in the cities of Sfax, Luxembourg, Bologna and Cologne. The experimental results proved the effectiveness of learning the Hierarchical Interval type-2 Fuzzy logic using real time particle swarm optimization technique PSO to accomplish multiobjective optimality regarding two criteria: number of vehicles that reach their destination and average travel time. The obtained results are encouraging, confirming the efficiency of the proposed system. △ Less","5 December, 2018",https://arxiv.org/pdf/1812.01893
Expanding search in the space of empirical ML,Bronwyn Woods,"As researchers and practitioners of applied machine learning, we are given a set of requirements on the problem to be solved, the plausibly obtainable data, and the computational resources available. We aim to find (within those bounds) reliably useful combinations of problem, data, and algorithm. An emphasis on algorithmic or technical novelty in ML conference publications leads to exploration of one dimension of this space. Data collection and ML deployment at scale in industry settings offers an environment for exploring the others. Our conferences and reviewing criteria can better support empirical ML by soliciting and incentivizing experimentation and synthesis independent of algorithmic innovation. △ Less","4 December, 2018",https://arxiv.org/pdf/1812.01495
Verlässliche Software im 21. Jahrhundert,Stefan Wagner;Matthias Tichy;Michael Felderer;Stefan Leue,"Software is the main innovation driver in many different areas, like cloud services, autonomous driving, connected medical devices, and high-frequency trading. All these areas have in common that they require high dependability. In this paper, we discuss challenges and research directions imposed by these new areas on guaranteeing the dependability. On the one hand challenges include characteristics of the systems themselves, e. g., open systems and ad-hoc structures. On the other hand, we see new aspects of dependability like behavioral traceability. △ Less","4 December, 2018",https://arxiv.org/pdf/1812.01434
A multi-class structured dictionary learning method using discriminant atom selection,R. E. Rolón;L. E. Di Persia;R. D. Spies;H. L. Rufiner,"In the last decade, traditional dictionary learning methods have been successfully applied to various pattern classification tasks. Although these methods produce sparse representations of signals which are robust against distortions and missing data, such representations quite often turn out to be unsuitable if the final objective is signal classification. In order to overcome or at least to attenuate such a weakness, several new methods which incorporate discriminative information into sparse-inducing models have emerged in recent years. In particular, methods for discriminative dictionary learning have shown to be more accurate (in terms of signal classification) than the traditional ones, which are only focused on minimizing the total representation error. In this work, we present both a novel multi-class discriminative measure and an innovative dictionary learning method. For a given dictionary, this new measure, which takes into account not only when a particular atom is used for representing signals coming from a certain class and the magnitude of its corresponding representation coefficient, but also the effect that such an atom has in the total representation error, is capable of efficiently quantifying the degree of discriminability of each one of the atoms. On the other hand, the new dictionary construction method yields dictionaries which are highly suitable for multi-class classification tasks. Our method was tested with a widely used database for handwritten digit recognition and compared with three state-of-the-art classification methods. The results show that our method significantly outperforms the other three achieving good recognition rates and additionally, reducing the computational cost of the classifier. △ Less","4 December, 2018",https://arxiv.org/pdf/1812.01389
ORACLE: Optimized Radio clAssification through Convolutional neuraL nEtworks,Kunal Sankhe;Mauro Belgiovine;Fan Zhou;Shamnaz Riyaz;Stratis Ioannidis;Kaushik Chowdhury,"This paper describes the architecture and performance of ORACLE, an approach for detecting a unique radio from a large pool of bit-similar devices (same hardware, protocol, physical address, MAC ID) using only IQ samples at the physical layer. ORACLE trains a convolutional neural network (CNN) that balances computational time and accuracy, showing 99\% classification accuracy for a 16-node USRP X310 SDR testbed and an external database of >100 COTS WiFi devices. Our work makes the following contributions: (i) it studies the hardware-centric features within the transmitter chain that causes IQ sample variations; (ii) for an idealized static channel environment, it proposes a CNN architecture requiring only raw IQ samples accessible at the front-end, without channel estimation or prior knowledge of the communication protocol; (iii) for dynamic channels, it demonstrates a principled method of feedback-driven transmitter-side modifications that uses channel estimation at the receiver to increase differentiability for the CNN classifier. The key innovation here is to intentionally introduce controlled imperfections on the transmitter side through software directives, while minimizing the change in bit error rate. Unlike previous work that imposes constant environmental conditions, ORACLE adopts the `train once deploy anywhere' paradigm with near-perfect device classification accuracy. △ Less","3 December, 2018",https://arxiv.org/pdf/1812.01124
A Consolidated Approach to Convolutional Neural Networks and the Kolmogorov Complexity,D Yoan L. Mekontchou Yomba,"The ability to precisely quantify similarity between various entities has been a fundamental complication in various problem spaces specifically in the classification of cellular images. Contemporary similarity measures applied in the domain of image processing proposed by the scientific community are mainly pursued in supervised settings. In this work, we will explore the innovative algorithmic normalized compression distance metric based on the information theoretic concept of Kolmogorov Complexity. Additionally we will observe its possible implementation in Convolutional Neural Networks to facilitate and automate the classification of Retinal Pigment Epithelial cell cultures for use in Age Related Macular Degeneration Stem Cell therapy in an unsupervised setting. △ Less","25 November, 2018",https://arxiv.org/pdf/1812.00888
An Energy-Efficient Transaction Model for the Blockchain-enabled Internet of Vehicles (IoV),Vishal Sharma,"The blockchain is a safe, reliable and innovative mechanism for managing numerous vehicles seeking connectivity. However, following the principles of the blockchain, the number of transactions required to update ledgers pose serious issues for vehicles as these may consume the maximum available energy. To resolve this, an efficient model is presented in this letter which is capable of handling the energy demands of the blockchain-enabled Internet of Vehicles (IoV) by optimally controlling the number of transactions through distributed clustering. Numerical results suggest that the proposed approach is 40.16% better in terms of energy conservation and 82.06% better in terms of the number of transactions required to share the entire blockchain-data compared with the traditional blockchain. △ Less","29 November, 2018",https://arxiv.org/pdf/1811.12610
ContextServ: Towards Model-Driven Development of Context-AwareWeb Services,Quan Z. Sheng;Jian Yu;Hanchuan Xu;Wei Emma Zhang;Anne H. H. Ngu;Jun Han;Ruilin Liu,"In the era of Web of Things and Services, Context-aware Web Services (CASs) are emerging as an important technology for building innovative context-aware applications. CASs enable the information integration from both the physical and virtual world, which affects human living. However, it is challenging to build CASs, due to the lack of context provisioning management approach and limited generic approach for formalizing the development process. We therefore propose ContextServ, a platform that uses a model-driven approach to support the full life cycle of CASs development, hence offering significant design and management flexibility. ContextServ implements a proposed UML-based modelling language ContextUML to support multiple modelling languages. It also supports dynamic adaptation of WS-BPEL based context-aware composite services by weaving context-aware rules into the process. Extensive experimental evaluations on ContextServ and its components showcase that ContextServ can support effective development and efficient execution of context-aware Web services. △ Less","19 December, 2018",https://arxiv.org/pdf/1811.12573
Adversarial Machine Learning And Speech Emotion Recognition: Utilizing Generative Adversarial Networks For Robustness,Siddique Latif;Rajib Rana;Junaid Qadir,"Deep learning has undoubtedly offered tremendous improvements in the performance of state-of-the-art speech emotion recognition (SER) systems. However, recent research on adversarial examples poses enormous challenges on the robustness of SER systems by showing the susceptibility of deep neural networks to adversarial examples as they rely only on small and imperceptible perturbations. In this study, we evaluate how adversarial examples can be used to attack SER systems and propose the first black-box adversarial attack on SER systems. We also explore potential defenses including adversarial training and generative adversarial network (GAN) to enhance robustness. Experimental evaluations suggest various interesting aspects of the effective utilization of adversarial examples useful for achieving robustness for SER systems opening up opportunities for researchers to further innovate in this space. △ Less","30 December, 2018",https://arxiv.org/pdf/1811.11402
The Fact Extraction and VERification (FEVER) Shared Task,James Thorne;Andreas Vlachos;Oana Cocarascu;Christos Christodoulopoulos;Arpit Mittal,"We present the results of the first Fact Extraction and VERification (FEVER) Shared Task. The task challenged participants to classify whether human-written factoid claims could be Supported or Refuted using evidence retrieved from Wikipedia. We received entries from 23 competing teams, 19 of which scored higher than the previously published baseline. The best performing system achieved a FEVER score of 64.21%. In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems. △ Less","30 November, 2018",https://arxiv.org/pdf/1811.10971
Cross-Technology Communications for Heterogeneous IoT Devices Through Artificial Doppler Shifts,Wei Wang;Shiyue He;Liang Sun;Tao Jiang;Qian Zhang,"Recent years have seen major innovations in developing energy-efficient wireless technologies such as Bluetooth Low Energy (BLE) for Internet of Things (IoT). Despite demonstrating significant benefits in providing low power transmission and massive connectivity, hardly any of these technologies have made it to directly connect to the Internet. Recent advances demonstrate the viability of direct communication among heterogeneous IoT devices with incompatible physical (PHY) layers. These techniques, however, require modifications in transmission power or time, which may affect the media access control (MAC) layer behaviors in legacy networks. In this paper, we argue that the frequency domain can serve as a free side channel with minimal interruptions to legacy networks. To this end, we propose DopplerFi, a communication framework that enables a two-way communication channel between BLE and Wi-Fi by injecting artificial Doppler shifts, which can be decoded by sensing the patterns in the Gaussian frequency shift keying (GFSK) demodulator and Channel State Information (CSI). The artificial Doppler shifts can be compensated by the inherent frequency synchronization module and thus have a negligible impact on legacy communications. Our evaluation using commercial off-the-shelf (COTS) BLE chips and 802.11-compliant testbeds have demonstrated that DopplerFi can achieve throughput up to 6.5~Kbps at the cost of merely less than 0.8% throughput loss. △ Less","27 November, 2018",https://arxiv.org/pdf/1811.10948
Eco-friendly Power Cost Minimization for Geo-distributed Data Centers Considering Workload Scheduling,Chunlei Sun;Xiangming Wen;Zhaoming Lu;Wenpeng Jing;Michele Zorzi,"The rapid development of renewable energy in the energy Internet is expected to alleviate the increasingly severe power problem in data centers, such as the huge power costs and pollution. This paper focuses on the eco-friendly power cost minimization for geo-distributed data centers supplied by multi-source power, where the geographical scheduling of workload and temporal scheduling of batteries' charging and discharging are both considered. Especially, we innovatively propose the Pollution Index Function to model the pollution of different kinds of power, which can encourage the use of cleaner power and improve power savings. We first formulate the eco-friendly power cost minimization problem as a multi-objective and mixed-integer programming problem, and then simplify it as a single-objective problem with integer constraints. Secondly, we propose a Sequential Convex Programming (SCP) algorithm to find the globally optimal non-integer solution of the simplified problem, which is non-convex, and then propose a low-complexity searching method to seek for the quasi-optimal mixed-integer solution of it. Finally, simulation results reveal that our method can improve the clean energy usage up to 50\%--60\% and achieve power cost savings up to 10\%--30\%, as well as reduce the delay of requests. △ Less","26 November, 2018",https://arxiv.org/pdf/1811.10738
Innovation Representation of Stochastic Processes with Application to Causal Inference,Amichai Painsky;Saharon Rosset;Meir Feder,"Typically, real-world stochastic processes are not easy to analyze. In this work we study the representation of any stochastic process as a memoryless innovation process triggering a dynamic system. We show that such a representation is always feasible for innovation processes taking values over a continuous set. However, the problem becomes more challenging when the alphabet size of the innovation is finite. In this case, we introduce both lossless and lossy frameworks, and provide closed-form solutions and practical algorithmic methods. In addition, we discuss the properties and uniqueness of our suggested approach. Finally, we show that the innovation representation problem has many applications. We focus our attention to Entropic Causal Inference, which has recently demonstrated promising performance, compared to alternative methods. △ Less","25 November, 2018",https://arxiv.org/pdf/1811.10071
Hydra: A Peer to Peer Distributed Training & Data Collection Framework,Vaibhav Mathur;Karanbir Chahal,"The world needs diverse and unbiased data to train deep learning models. Currently data comes from a variety of sources that are unmoderated to a large extent. The outcomes of training neural networks with unverified data yields biased models with various strains of homophobia, sexism and racism. Another trend observed in the world of deep learning is the rise of distributed training. Although cloud companies provide high performance compute for training models in the form of GPU's connected with a low latency network, using these services comes at a high cost. We propose Hydra, a system that seeks to solve both of these problems in a novel manner by proposing a decentralized distributed framework which utilizes the substantial amount of idle compute of everyday electronic devices like smartphones and desktop computers for training and data collection purposes. Hydra couples a specialized distributed training framework on a network of these low powered devices with a reward scheme that incentivizes users to provide high quality data to unleash the compute capability on this training framework. Such a system has the ability to capture data from a wide variety of diverse sources which has been an issue in the current scenario of deep learning. Hydra brings in several new innovations in training on low powered devices including a fault tolerant version of the All Reduce algorithm. Furthermore we introduce a reinforcement learning policy to decide the size of training jobs on different machines on a heterogeneous cluster of devices with varying network latencies for Synchronous SGD. The novel thing about such a network is the ability of each machine to shut down and resume training capabilities at any point of time without restarting the overall training. To enable such an asynchronous behaviour we propose a communication framework inspired by the Bittorrent protocol and the Kademlia DHT. △ Less","24 November, 2018",https://arxiv.org/pdf/1811.09878
Towards Realizing the Smart Product Traceability System,Dharmendra Kumar Mishra;Aicha Sekhari;Sébastien Henry;Dharmendra Mishra;Yacine Ouzrout;Ajay Shrestha;Abdelaziz Bouras,"The rapid technological enhancement and innovations in current days have changed people's thought. The use of Information Technology tools in people's daily life has changed their life style completely. The advent of various innovative smart products in the market has tremendous impact on people's lifestyle. They want to know their heart beat while they run, they need a smart car which makes them alert when they become sleepy while driving, and they need an IT tool which can make their home safer when they are out. These quests of people to the very extent have been fulfilled by development of many Meta products like wearables, smart phones, smart car etc. The concept of Meta product is based on the fact that they need to offer intelligent services to the users. In present day's Meta products market, the manufacturers have their own cloud based platform which is used by the products for which it is developed. The other products cannot use the common platform. In this paper, we propose a common cloud based platform that will be used by any Meta product's user to get different services. △ Less","7 November, 2018",https://arxiv.org/pdf/1811.09693
Smart Greybox Fuzzing,Van-Thuan Pham;Marcel Böhme;Andrew E. Santosa;Alexandru Răzvan Căciulescu;Abhik Roychoudhury,"Coverage-based greybox fuzzing (CGF) is one of the most successful methods for automated vulnerability detection. Given a seed file (as a sequence of bits), CGF randomly flips, deletes or bits to generate new files. CGF iteratively constructs (and fuzzes) a seed corpus by retaining those generated files which enhance coverage. However, random bitflips are unlikely to produce valid files (or valid chunks in files), for applications processing complex file formats. In this work, we introduce smart greybox fuzzing (SGF) which leverages a high-level structural representation of the seed file to generate new files. We define innovative mutation operators that work on the virtual file structure rather than on the bit level which allows SGF to explore completely new input domains while maintaining file validity. We introduce a novel validity-based power schedule that enables SGF to spend more time generating files that are more likely to pass the parsing stage of the program, which can expose vulnerabilities much deeper in the processing logic. Our evaluation demonstrates the effectiveness of SGF. On several libraries that parse structurally complex files, our tool AFLSmart explores substantially more paths (up to 200%) and exposes more vulnerabilities than baseline AFL. Our tool AFLSmart has discovered 42 zero-day vulnerabilities in widely-used, well-tested tools and libraries; so far 17 CVEs were assigned. △ Less","23 November, 2018",https://arxiv.org/pdf/1811.09447
Towards energy efficient buildings: how ICTs can convert advances?,Michael David;A. Aubry;W. Derigent,"This work is a positioning research paper for energy efficient building based on ICT solutions. Through the literature about the solutions for energy control of buildings during operational phase, a 3-layers model is proposed to integrate these solutions: first level consists in communication technologies, second level is about data modelling and third level is related to decision-making tools. For each level, key research topics and remaining problems are identified in order to achieve a concrete step forward. 1. CONTEXT AND PROBLEMATICS Through studies on ICT solutions for energy control of buildings, a 3-layers model is proposed to integrate these solutions and position a new way for energy efficiency. The building sector is the largest user of energy and CO 2 emitter in the EU, estimated at approximately 40% of the total consumption (Sharples et al., 1999). According to the International Panel on Climate Change (European Union, 2010), 30% of energy used in buildings could be reduced with net economic benefits by 2030. Such a reduction, however, is meaningless unless ""sustainability"" is considered. Because of these factors, healthy, sustainable, and energy efficient buildings have become active topics in international research; there is an urgent need for a new kind of high-technology driven and integrative research that should lead to the massive development of smart buildings and, in the medium term, smart cities. From a building lifecycle perspective, most of the energy (~80%) is consumed during the operational stage of the building (European Union, 2010) (Bilsen et al., 2013). Reducing building energy consumption may be addressed by the physical modifications which can be operated on a building like upgrading windows, heating systems or modifying thermic characteristics by insulating. Another possible path to reduce the energy consumption of a building is to use Information and Communication Technologies (ICT). According to the International Panel on Climate Change, a reduction of energy even greater than the 30% can be targeted by 2030 by considering ICT solutions. In support of this claim, some specialists believe that ICT-based solutions have the potential to enable 50-80% greenhouse gas reduction globally. In this respect, ICT innovation opens prospects for the development of a new range of new services highly available, flexible, safe, easy to integrate, and user friendly (Bilsen et al., 2013). This, in turn, should foster a sophisticated, reliable and fast communication infrastructure for the connection of various distributed elements (sensors, generators, substations...) that enables to exchange real-time data, information and knowledge needed to improve efficiency (e.g., to monitor and control energy consumption), reliability (e.g., to facilitate maintenance operations), flexibility (e.g., to integrate new rules to meet new consumer expectations), and investment returns, but also to induce a shift in consumer behaviour. △ Less","22 November, 2018",https://arxiv.org/pdf/1811.09160
Equality of Voice: Towards Fair Representation in Crowdsourced Top-K Recommendations,Abhijnan Chakraborty;Gourab K Patro;Niloy Ganguly;Krishna P. Gummadi;Patrick Loiseau,"To help their users to discover important items at a particular time, major websites like Twitter, Yelp, TripAdvisor or NYTimes provide Top-K recommendations (e.g., 10 Trending Topics, Top 5 Hotels in Paris or 10 Most Viewed News Stories), which rely on crowdsourced popularity signals to select the items. However, different sections of a crowd may have different preferences, and there is a large silent majority who do not explicitly express their opinion. Also, the crowd often consists of actors like bots, spammers, or people running orchestrated campaigns. Recommendation algorithms today largely do not consider such nuances, hence are vulnerable to strategic manipulation by small but hyper-active user groups. To fairly aggregate the preferences of all users while recommending top-K items, we borrow ideas from prior research on social choice theory, and identify a voting mechanism called Single Transferable Vote (STV) as having many of the fairness properties we desire in top-K item (s)elections. We develop an innovative mechanism to attribute preferences of silent majority which also make STV completely operational. We show the generalizability of our approach by implementing it on two different real-world datasets. Through extensive experimentation and comparison with state-of-the-art techniques, we show that our proposed approach provides maximum user satisfaction, and cuts down drastically on items disliked by most but hyper-actively promoted by a few users. △ Less","21 November, 2018",https://arxiv.org/pdf/1811.08690
Rebooting Research on Detecting Repackaged Android Apps: Literature Review and Benchmark,Li Li;Tegawendé Bissyandé;Jacques Klein,"Repackaging is a serious threat to the Android ecosystem as it deprives app developers of their benefits, contributes to spreading malware on users' devices, and increases the workload of market maintainers. In the space of six years, the research around this specific issue has produced 57 approaches which do not readily scale to millions of apps or are only evaluated on private datasets without, in general, tool support available to the community. Through a systematic literature review of the subject, we argue that the research is slowing down, where many state-of-the-art approaches have reported high-performance rates on closed datasets, which are unfortunately difficult to replicate and to compare against. In this work, we propose to reboot the research in repackaged app detection by providing a literature review that summarises the challenges and current solutions for detecting repackaged apps and by providing a large dataset that supports replications of existing solutions and implications of new research directions. We hope that these contributions will re-activate the direction of detecting repackaged apps and spark innovative approaches going beyond the current state-of-the-art. △ Less","20 November, 2018",https://arxiv.org/pdf/1811.08520
"VoCoG: An Intelligent, Non-Intrusive Assistant for Voice-based Collaborative Group-Viewing",Sumit Shekhar;Aditya Siddhant;Anindya Shankar Bhandari;Nishant Yadav,"There have been significant innovations in media technologies in the recent years. While these developments have improved experiences for individual users, design of multi-user interfaces still remains a challenge. A relatively unexplored area in this context, is enabling multiple users to enjoy shared viewing (e.g. deciding on movies to watch together). In particular, the challenge is to design an intelligent system which would enable viewers to explore together shows or movies they like, seamlessly. This is a complex design problem, as it requires the system to (i) assess affinities of individual users (movies or genres), (ii) combine individual preferences taking into account user-user interactions, and (iii) be non-intrusive simultaneously. The proposed system VoCoG, is an end-to-end intelligent system for collaborative viewing. VoCoG incorporates an online recommendation algorithm, efficient methods for analyzing natural conversation and a graph-based method to fuse preferences of multiple users. It takes user conversation as input, making it non-intrusive. A usability survey of the system indicates that the system provides a good experience to the users as well as relevant recommendations. Further analysis of the usage data reveals insights about the nature of conversation during the interaction sessions, final consensus among the users as well as ratings of varied user groups. △ Less","19 November, 2018",https://arxiv.org/pdf/1811.07547
Exploit the Connectivity: Multi-Object Tracking with TrackletNet,Gaoang Wang;Yizhou Wang;Haotian Zhang;Renshu Gu;Jenq-Neng Hwang,"Multi-object tracking (MOT) is an important and practical task related to both surveillance systems and moving camera applications, such as autonomous driving and robotic vision. However, due to unreliable detection, occlusion and fast camera motion, tracked targets can be easily lost, which makes MOT very challenging. Most recent works treat tracking as a re-identification (Re-ID) task, but how to combine appearance and temporal features is still not well addressed. In this paper, we propose an innovative and effective tracking method called TrackletNet Tracker (TNT) that combines temporal and appearance information together as a unified framework. First, we define a graph model which treats each tracklet as a vertex. The tracklets are generated by appearance similarity with CNN features and intersection-over-union (IOU) with epipolar constraints to compensate camera movement between adjacent frames. Then, for every pair of two tracklets, the similarity is measured by our designed multi-scale TrackletNet. Afterwards, the tracklets are clustered into groups which represent individual object IDs. Our proposed TNT has the ability to handle most of the challenges in MOT, and achieve promising results on MOT16 and MOT17 benchmark datasets compared with other state-of-the-art methods. △ Less","17 November, 2018",https://arxiv.org/pdf/1811.07258
Wing Expansion Menu - An approach for faster and more precise navigation with cascading pull-down menus,Manuel Zierl,"This paper presents a new design suggestion for cascading pull-down menus to make user interaction with it faster and therefore easier: The Wing Expansion Menu (WEM). The proposal is based on the Steering Law, which implies a wider steering path for menu items. Our Approach combines this enlargement with a heuristic function that provides a probability with which the user will select an menu item. The menu can also be adapted to a wide variety of situations using certain variables. A user study of a WEM against a standard pull-down menu showed an average improvement of 18.63% in user interaction speed. A second user study, which evaluated one of the significant innovations of the WEM compared to a similar approach, showed an average improvement of 7.01% in user interaction speed. △ Less","16 November, 2018",https://arxiv.org/pdf/1811.06874
Spatial Logics and Model Checking for Medical Imaging (Extended Version),Fabrizio Banci Buonamici;Gina Belmonte;Vincenzo Ciancia;Diego Latella;Mieke Massink,"Recent research on spatial and spatio-temporal model checking provides novel image analysis methodologies, rooted in logical methods for topological spaces. Medical Imaging (MI) is a field where such methods show potential for ground-breaking innovation. Our starting point is SLCS, the Spatial Logic for Closure Spaces -- Closure Spaces being a generalisation of topological spaces, covering also discrete space structures -- and topochecker, a model-checker for SLCS (and extensions thereof). We introduce the logical language ImgQL (""Image Query Language""). ImgQL extends SLCS with logical operators describing distance and region similarity. The spatio-temporal model checker topochecker is correspondingly enhanced with state-of-the-art algorithms, borrowed from computational image processing, for efficient implementation of distancebased operators, namely distance transforms. Similarity between regions is defined by means of a statistical similarity operator, based on notions from statistical texture analysis. We illustrate our approach by means of two examples of analysis of Magnetic Resonance images: segmentation of glioblastoma and its oedema, and segmentation of rectal carcinoma. △ Less","14 November, 2018",https://arxiv.org/pdf/1811.06065
Cross-lingual Short-text Matching with Deep Learning,Asmelash Teka Hadgu,"The problem of short text matching is formulated as follows: given a pair of sentences or questions, a matching model determines whether the input pair mean the same or not. Models that can automatically identify questions with the same meaning have a wide range of applications in question answering sites and modern chatbots. In this article, we describe the approach by team hahu to solve this problem in the context of the ""CIKM AnalytiCup 2018 - Cross-lingual Short-text Matching of Question Pairs"" that is sponsored by Alibaba. Our solution is an end-to-end system based on current advances in deep learning which avoids heavy feature-engineering and achieves improved performance over traditional machine-learning approaches. The log-loss scores for the first and second rounds of the contest are 0.35 and 0.39 respectively. The team was ranked 7th from 1027 teams in the overall ranking scheme by the organizers that consisted of the two contest scores as well as: innovation and system integrity, understanding data as well as practicality of the solution for business. △ Less","13 November, 2018",https://arxiv.org/pdf/1811.05569
Autonomic Intrusion Response in Distributed Computing using Big Data,Kleber Vieira;Fernando Koch;Joao Bosco Mangueira Sobral;Carlos Becker Westphall;Jorge Lopes de Souza Leao,"We introduce a method for Intrusion Detection based on the classification, understanding and prediction of behavioural deviance and potential threats, issuing recommendations, and acting to address eminent issues. Our work seeks a practical solutions to automate the process of identification and response to Cybersecurity threats in hybrid Distributed Computing environments through the analysis of large datasets generated during operations. We are motivated by the growth in utilisation of Cloud Computing and Edge Computing as the technology for business and social solutions. The technology mix and complex operation render these environments target to attacks like hijacking, man-in-the-middle, denial of service, phishing, and others. The Autonomous Intrusion Response System implements innovative models of data analysis and context-aware recommendation systems to respond to attacks and self-healing. We introduce a proof-of-concept implementation and evaluate against datasets from experimentation scenarios based on public and private clouds. The results present significant improvement in response effectiveness and potential to scale to large environments. △ Less","13 November, 2018",https://arxiv.org/pdf/1811.05407
Product Title Refinement via Multi-Modal Generative Adversarial Learning,Jianguo Zhang;Pengcheng Zou;Zhao Li;Yao Wan;Ye Liu;Xiuming Pan;Yu Gong;Philip S. Yu,"Nowadays, an increasing number of customers are in favor of using E-commerce Apps to browse and purchase products. Since merchants are usually inclined to employ redundant and over-informative product titles to attract customers' attention, it is of great importance to concisely display short product titles on limited screen of cell phones. Previous researchers mainly consider textual information of long product titles and lack of human-like view during training and evaluation procedure. In this paper, we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short product title generation, which innovatively incorporates image information, attribute tags from the product and the textual information from original long titles. MM-GAN treats short titles generation as a reinforcement learning process, where the generated titles are evaluated by the discriminator in a human-like view. △ Less","11 November, 2018",https://arxiv.org/pdf/1811.04498
CAPTAIN: Comprehensive Composition Assistance for Photo Taking,Farshid Farhat;Mohammad Mahdi Kamani;James Z. Wang,"Many people are interested in taking astonishing photos and sharing with others. Emerging hightech hardware and software facilitate ubiquitousness and functionality of digital photography. Because composition matters in photography, researchers have leveraged some common composition techniques to assess the aesthetic quality of photos computationally. However, composition techniques developed by professionals are far more diverse than well-documented techniques can cover. We leverage the vast underexplored innovations in photography for computational composition assistance. We propose a comprehensive framework, named CAPTAIN (Composition Assistance for Photo Taking), containing integrated deep-learned semantic detectors, sub-genre categorization, artistic pose clustering, personalized aesthetics-based image retrieval, and style set matching. The framework is backed by a large dataset crawled from a photo-sharing Website with mostly photography enthusiasts and professionals. The work proposes a sequence of steps that have not been explored in the past by researchers. The work addresses personal preferences for composition through presenting a ranked-list of photographs to the user based on user-specified weights in the similarity measure. The matching algorithm recognizes the best shot among a sequence of shots with respect to the user's preferred style set. We have conducted a number of experiments on the newly proposed components and reported findings. A user study demonstrates that the work is useful to those taking photos. △ Less","9 November, 2018",https://arxiv.org/pdf/1811.04184
Blockchain for social good: a quantitative analysis,Massimo Bartoletti;Tiziana Cimoli;Livio Pompianu;Sergio Serusi,"The rise of blockchain technologies has given a boost to social good projects, which are trying to exploit various characteristic features of blockchains: the quick and inexpensive transfer of cryptocurrency, the transparency of transactions, the ability to tokenize any kind of assets, and the increase in trustworthiness due to decentralization. However, the swift pace of innovation in blockchain technologies, and the hype that has surrounded their ""disruptive potential"", make it difficult to understand whether these technologies are applied correctly, and what one should expect when trying to apply them to social good projects. This paper addresses these issues, by systematically analysing a collection of 120 blockchain-enabled social good projects. Focussing on measurable and objective aspects, we try to answer various relevant questions: which features of blockchains are most commonly used? Do projects have success in fund raising? Are they making appropriate choices on the blockchain architecture? How many projects are released to the public, and how many are eventually abandoned? △ Less","2 November, 2018",https://arxiv.org/pdf/1811.03424
Approximate Neighbor Counting in Radio Networks,Calvin Newport;Chaodong Zheng,"For many distributed algorithms, neighborhood size is an important parameter. In radio networks, however, obtaining this information can be difficult due to ad hoc deployments and communication that occurs on a collision-prone shared channel. This paper conducts a comprehensive survey of the approximate neighbor counting problem, which requires nodes to obtain a constant factor approximation of the size of their network neighborhood. We produce new lower and upper bounds for three main variations of this problem in the radio network model: (a) the network is single-hop and every node must obtain an estimate of its neighborhood size; (b) the network is multi-hop and only a designated node must obtain an estimate of its neighborhood size; and (c) the network is multi-hop and every node must obtain an estimate of its neighborhood size. In studying these problem variations, we consider solutions with and without collision detection, and with both constant and high success probability. Some of our results are extensions of existing strategies, while others require technical innovations. We argue this collection of results provides insight into the nature of this well-motivated problem (including how it differs from related symmetry breaking tasks in radio networks), and provides a useful toolbox for algorithm designers tackling higher level problems that might benefit from neighborhood size estimates. △ Less","8 November, 2018",https://arxiv.org/pdf/1811.03278
YODA: Enabling computationally intensive contracts on blockchains with Byzantine and Selfish nodes,Sourav Das;Vinay Joseph Ribeiro;Abhijeet Anand,"One major shortcoming of permissionless blockchains such as Bitcoin and Ethereum is that they are unsuitable for running Computationally Intensive smart Contracts (CICs). This prevents such blockchains from running Machine Learning algorithms, Zero-Knowledge proofs, etc. which may need non-trivial computation. In this paper, we present YODA, which is to the best of our knowledge the first solution for efficient computation of CICs in permissionless blockchains with guarantees for a threat model with both Byzantine and selfish nodes. YODA selects one or more execution sets (ES) via Sortition to execute a particular CIC off-chain. One key innovation is the MultI-Round Adaptive Consensus using Likelihood Estimation (MIRACLE) algorithm based on sequential hypothesis testing. M I RACLE allows the execution sets to be small thus making YODA efficient while ensuring correct CIC execution with high probability. It adapts the number of ES sets automatically depending on the concentration of Byzantine nodes in the system and is optimal in terms of the expected number of ES sets used in certain scenarios. Through a suite of economic incentives and technical mechanisms such as the novel Randomness Inserted Contract Execution (RICE) algorithm, we force selfish nodes to behave honestly. We also prove that the honest behavior of selfish nodes is an approximate Nash Equilibrium. We present the system design and details of YODA and prove the security properties of MIRACLE and RICE. Our prototype implementation built on top of Ethereum demonstrates the ability of YODA to run CICs with orders of magnitude higher gas per unit time as well as total gas requirements than Ethereum currently supports. It also demonstrates the low overheads of RICE. △ Less","18 December, 2018",https://arxiv.org/pdf/1811.03265
Attention Fusion Networks: Combining Behavior and E-mail Content to Improve Customer Support,Stephane Fotso;Philip Spanoudes;Benjamin C. Ponedel;Brian Reynoso;Janet Ko,"Customer support is a central objective at Square as it helps us build and maintain great relationships with our sellers. In order to provide the best experience, we strive to deliver the most accurate and quasi-instantaneous responses to questions regarding our products. In this work, we introduce the Attention Fusion Network model which combines signals extracted from seller interactions on the Square product ecosystem, along with submitted email questions, to predict the most relevant solution to a seller's inquiry. We show that the innovative combination of two very different data sources that are rarely used together, using state-of-the-art deep learning systems outperforms, candidate models that are trained only on a single source. △ Less","12 November, 2018",https://arxiv.org/pdf/1811.03169
Emerging Applications of Reversible Data Hiding,Dongdong Hou;Weiming Zhang;Jiayang Liu;Siyan Zhou;Dongdong Chen;Nenghai Yu,"Reversible data hiding (RDH) is one special type of information hiding, by which the host sequence as well as the embedded data can be both restored from the marked sequence without loss. Beside media annotation and integrity authentication, recently some scholars begin to apply RDH in many other fields innovatively. In this paper, we summarize these emerging applications, including steganography, adversarial example, visual transformation, image processing, and give out the general frameworks to make these operations reversible. As far as we are concerned, this is the first paper to summarize the extended applications of RDH. △ Less","7 November, 2018",https://arxiv.org/pdf/1811.02928
Fast Adaptive Bilateral Filtering,Ruturaj G. Gavaskar;Kunal N. Chaudhury,"In the classical bilateral filter, a fixed Gaussian range kernel is used along with a spatial kernel for edge-preserving smoothing. We consider a generalization of this filter, the so-called adaptive bilateral filter, where the center and width of the Gaussian range kernel is allowed to change from pixel to pixel. Though this variant was originally proposed for sharpening and noise removal, it can also be used for other applications such as artifact removal and texture filtering. Similar to the bilateral filter, the brute-force implementation of its adaptive counterpart requires intense computations. While several fast algorithms have been proposed in the literature for bilateral filtering, most of them work only with a fixed range kernel. In this paper, we propose a fast algorithm for adaptive bilateral filtering, whose complexity does not scale with the spatial filter width. This is based on the observation that the concerned filtering can be performed purely in range space using an appropriately defined local histogram. We show that by replacing the histogram with a polynomial and the finite range-space sum with an integral, we can approximate the filter using analytic functions. In particular, an efficient algorithm is derived using the following innovations: the polynomial is fitted by matching its moments to those of the target histogram (this is done using fast convolutions), and the analytic functions are recursively computed using integration-by-parts. Our algorithm can accelerate the brute-force implementation by at least 20 \times, without perceptible distortions in the visual quality. We demonstrate the effectiveness of our algorithm for sharpening, JPEG deblocking, and texture filtering. △ Less","6 November, 2018",https://arxiv.org/pdf/1811.02308
Assessing public-private research collaboration: is it possible to compare university performance?,Giovanni Abramo;Ciriaco Andrea D'Angelo;Marco Solazzi,"It is widely recognized that collaboration between the public and private research sectors should be stimulated and supported, as a means of favoring innovation and regional development. This work takes a bibliometric approach, based on co-authorship of scientific publications, to propose a model for comparative measurement of the performance of public research institutions in collaboration with the domestic industry collaboration with the private sector. The model relies on an identification and disambiguation algorithm developed by the authors to link each publication to its real authors. An example of application of the model is given, for the case of the academic system and private enterprises in Italy. The study demonstrates that for each scientific discipline and each national administrative region, it is possible to measure the performance of individual universities in both intra-regional and extra-regional collaboration, normalized with respect to advantages of location. Such results may be useful in informing regional policies and merit-based public funding of research organizations. △ Less","5 November, 2018",https://arxiv.org/pdf/1811.01813
University-industry research collaboration: a model to assess university capability,Giovanni Abramo;Ciriaco Andrea D'Angelo;Flavia Di Costa,"Scholars and policy makers recognize that collaboration between industry and the public research institutions is a necessity for innovation and national economic development. This work presents an econometric model which expresses the university capability for collaboration with industry as a function of size, location and research quality. The field of observation is made of the census of 2001-2003 scientific articles in the hard sciences, co-authored by universities and private enterprises located in Italy. The analysis shows that research quality of universities has an impact higher than geographic distance on the capability for collaborating with industry. The model proposed and the measures that descend from it are suited for use at various levels of administration, to assist in realizing the ""third role"" of universities: the contribution to socio-economic development through public to private technology transfer. △ Less","5 November, 2018",https://arxiv.org/pdf/1811.01763
Communication Through Breath: Aerosol Transmission,Maryam Khalid;Osama Amin;Sajid Ahmed;Basem Shihada;Mohamed-Slim Alouini,"Exhaled breath can be used in retrieving information and creating innovative communication systems. It contains several volatile organic compounds (VOCs) and biological entities that can act as health biomarkers. For instance, the breath of infected human contains a nonnegligible amount of pathogenic aerosol that can spread or remain suspended in the atmosphere. Therefore, the exhaled breath can be exploited as a source's message in a communication setup to remotely scan the bio-information via an aerosol transmission channel. An overview of the basic configuration is presented along with a description of system components with a particular emphasis on channel modeling. Furthermore, the challenges that arise in theoretical analysis and system development are highlighted. Finally, several open issues are discussed to concretize the proposed communication concept. △ Less","4 November, 2018",https://arxiv.org/pdf/1811.01393
Building an Argument for the Use of Science Fiction in HCI Education,Philipp Jordan;Paula Alexandra Silva,"Science fiction literature, comics, cartoons and, in particular, audio-visual materials, such as science fiction movies and shows, can be a valuable addition in Human-computer interaction (HCI) Education. In this paper, we present an overview of research relative to future directions in HCI Education, distinct crossings of science fiction in HCI and Computer Science teaching and the Framework for 21st Century Learning. Next, we provide examples where science fiction can add to the future of HCI Education. In particular, we argue herein first that science fiction, as tangible and intangible cultural artifact, can serve as a trigger for creativity and innovation and thus, support us in exploring the design space. Second, science fiction, as a means to analyze yet-to-come HCI technologies, can assist us in developing an open-minded and reflective dialogue about technological futures, thus creating a singular base for critical thinking and problem solving. Provided that one is cognizant of its potential and limitations, we reason that science fiction can be a meaningful extension of selected aspects of HCI curricula and research. △ Less","2 November, 2018",https://arxiv.org/pdf/1811.01033
Gender differences in research collaboration,Giovanni Abramo;Ciriaco Andrea D'Angelo;Gianluca Murgia,"The debate on the role of women in the academic world has focused on various phenomena that could be at the root of the gender gap seen in many nations. However, in spite of the ever more collaborative character of scientific research, the issue of gender aspects in research collaborations has been treated in a marginal manner. In this article we apply an innovative bibliometric approach based on the propensity for collaboration by individual academics, which permits measurement of gender differences in the propensity to collaborate by fields, disciplines and forms of collaboration: intramural, extramural domestic and international. The analysis of the scientific production of Italian academics shows that women researchers register a greater capacity to collaborate in all the forms analyzed, with the exception of international collaboration, where there is still a gap in comparison to male colleagues. △ Less","31 October, 2018",https://arxiv.org/pdf/1810.13355
Variation in research collaboration patterns across academic ranks,Giovanni Abramo;Ciriaco Andrea D'Angelo;Gianluca Murgia,"The ability to activate and manage effective collaborations is becoming an increasingly important criteria in policies on academic career advancement. The rise of such policies leads to development of indicators that permit measurement of the propensity to collaborate for academics of different ranks, and to examine the role of several variables in collaboration, first among these being the researchers' disciplines. In this work we apply an innovative bibliometric approach based on individual propensity for collaboration to measure the differences in propensity across academic ranks, by discipline and for choice of collaboration forms - intramural, extramural domestic and international. The analysis is based on the scientific production of Italian academics for the period 2006 to 2010, totaling over 200,000 publications indexed in Web of Science. It shows that assistant professors register a propensity for intramural collaboration that is clearly greater than for professors of higher ranks. Vice versa, the higher ranks, but not quite so clearly, register greater propensity to collaborate at the international level. △ Less","31 October, 2018",https://arxiv.org/pdf/1810.13352
MULAN: A Blind and Off-Grid Method for Multichannel Echo Retrieval,Helena Peic Tukuljac;Antoine Deleforge;Rémi Gribonval,"This paper addresses the general problem of blind echo retrieval, i.e., given M sensors measuring in the discrete-time domain M mixtures of K delayed and attenuated copies of an unknown source signal, can the echo locations and weights be recovered? This problem has broad applications in fields such as sonars, seismol-ogy, ultrasounds or room acoustics. It belongs to the broader class of blind channel identification problems, which have been intensively studied in signal processing. Existing methods in the literature proceed in two steps: (i) blind estimation of sparse discrete-time filters and (ii) echo information retrieval by peak-picking on filters. The precision of these methods is fundamentally limited by the rate at which the signals are sampled: estimated echo locations are necessary on-grid, and since true locations never match the sampling grid, the weight estimation precision is impacted. This is the so-called basis-mismatch problem in compressed sensing. We propose a radically different approach to the problem, building on the framework of finite-rate-of-innovation sampling. The approach operates directly in the parameter-space of echo locations and weights, and enables near-exact blind and off-grid echo retrieval from discrete-time measurements. It is shown to outperform conventional methods by several orders of magnitude in precision. △ Less","31 October, 2018",https://arxiv.org/pdf/1810.13338
Assessing national strengths and weaknesses in research fields,Giovanni Abramo;Ciriaco Andrea D'Angelo,"National policies aimed at fostering the effectiveness of scientific systems should be based on reliable strategic analysis identifying strengths and weaknesses at field level. Approaches and indicators thus far proposed in the literature have not been completely satisfactory, since they fail to distinguish the effect of the size of production factors from that of their quality, particularly the quality of labor. The current work proposes an innovative ""input-oriented"" approach, which permits: i) estimation of national research performance in a field and comparison to that of other nations, independent of the size of their respective research staffs; and, for fields of comparable intensity of publication, ii) identification of the strong and weak research fields within a national research system on the basis of international comparison. In reference to the second objective, the proposed approach is applied to the Italian case, through the analysis of the 2006-2010 scientific production of the Italian academic system, in the 200 research fields where bibliometric analysis is meaningful. △ Less","30 October, 2018",https://arxiv.org/pdf/1810.12834
Imagination Based Sample Construction for Zero-Shot Learning,Gang Yang;Jinlu Liu;Xirong Li,"Zero-shot learning (ZSL) which aims to recognize unseen classes with no labeled training sample, efficiently tackles the problem of missing labeled data in image retrieval. Nowadays there are mainly two types of popular methods for ZSL to recognize images of unseen classes: probabilistic reasoning and feature projection. Different from these existing types of methods, we propose a new method: sample construction to deal with the problem of ZSL. Our proposed method, called Imagination Based Sample Construction (IBSC), innovatively constructs image samples of target classes in feature space by mimicking human associative cognition process. Based on an association between attribute and feature, target samples are constructed from different parts of various samples. Furthermore, dissimilarity representation is employed to select high-quality constructed samples which are used as labeled data to train a specific classifier for those unseen classes. In this way, zero-shot learning is turned into a supervised learning problem. As far as we know, it is the first work to construct samples for ZSL thus, our work is viewed as a baseline for future sample construction methods. Experiments on four benchmark datasets show the superiority of our proposed method. △ Less","29 October, 2018",https://arxiv.org/pdf/1810.12145
Using Machine Learning to Predict the Evolution of Physics Research,Wenyuan Liu;Stanisław Saganowski;Przemysław Kazienko;Siew Ann Cheong,"The advancement of science as outlined by Popper and Kuhn is largely qualitative, but with bibliometric data it is possible and desirable to develop a quantitative picture of scientific progress. Furthermore it is also important to allocate finite resources to research topics that have growth potential, to accelerate the process from scientific breakthroughs to technological innovations. In this paper, we address this problem of quantitative knowledge evolution by analysing the APS publication data set from 1981 to 2010. We build the bibliographic coupling and co-citation networks, use the Louvain method to detect topical clusters (TCs) in each year, measure the similarity of TCs in consecutive years, and visualize the results as alluvial diagrams. Having the predictive features describing a given TC and its known evolution in the next year, we can train a machine learning model to predict future changes of TCs, i.e., their continuing, dissolving, merging and splitting. We found the number of papers from certain journals, the degree, closeness, and betweenness to be the most predictive features. Additionally, betweenness increases significantly for merging events, and decreases significantly for splitting events. Our results represent a first step from a descriptive understanding of the Science of Science (SciSci), towards one that is ultimately prescriptive. △ Less","29 October, 2018",https://arxiv.org/pdf/1810.12116
Identification of physical processes via combined data-driven and data-assimilation methods,Haibin Chang;Dongxiao Zhang,"With the advent of modern data collection and storage technologies, data-driven approaches have been developed for discovering the governing partial differential equations (PDE) of physical problems. However, in the extant works the model parameters in the equations are either assumed to be known or have a linear dependency. Therefore, most of the realistic physical processes cannot be identified with the current data-driven PDE discovery approaches. In this study, an innovative framework is developed that combines data-driven and data-assimilation methods for simultaneously identifying physical processes and inferring model parameters. Spatiotemporal measurement data are first divided into a training data set and a testing data set. Using the training data set, a data-driven method is developed to learn the governing equation of the considered physical problem by identifying the occurred (or dominated) processes and selecting the proper empirical model. Through introducing a prediction error of the learned governing equation for the testing data set, a data-assimilation method is devised to estimate the uncertain model parameters of the selected empirical model. For the contaminant transport problem investigated, the results demonstrate that the proposed method can adequately identify the considered physical processes via concurrently discovering the corresponding governing equations and inferring uncertain parameters of nonlinear models, even in the presence of measurement errors. This work helps to broaden the applicable area of the research of data driven discovery of governing equations of physical problems. △ Less","29 October, 2018",https://arxiv.org/pdf/1810.11977
"Towards Smart City Innovation Under the Perspective of Software-Defined Networking, Artificial Intelligence and Big Data",Joberto S. B. Martins,"Smart city projects address many of the current problems afflicting high populated areas and cities and, as such, are a target for government, institutions and private organizations that plan to explore its foreseen advantages. In technical terms, smart city projects present a complex set of requirements including a large number users with highly different and heterogeneous requirements. In this scenario, this paper proposes and analyses the impact and perspectives on adopting software-defined networking and artificial intelligence as innovative approaches for smart city project development and deployment. Big data is also considered as an inherent element of most smart city project that must be tackled. A framework layered view is proposed with a discussion about software-defined networking and machine learning impacts on innovation followed by a use case that demonstrates the potential benefits of cognitive learning for smart cities. It is argued that the complexity of smart city projects do require new innovative approaches that potentially result in more efficient and intelligent systems. △ Less","27 October, 2018",https://arxiv.org/pdf/1810.11665
Learning and Management for Internet-of-Things: Accounting for Adaptivity and Scalability,Tianyi Chen;Sergio Barbarossa;Xin Wang;Georgios B. Giannakis;Zhi-Li Zhang,"Internet-of-Things (IoT) envisions an intelligent infrastructure of networked smart devices offering task-specific monitoring and control services. The unique features of IoT include extreme heterogeneity, massive number of devices, and unpredictable dynamics partially due to human interaction. These call for foundational innovations in network design and management. Ideally, it should allow efficient adaptation to changing environments, and low-cost implementation scalable to massive number of devices, subject to stringent latency constraints. To this end, the overarching goal of this paper is to outline a unified framework for online learning and management policies in IoT through joint advances in communication, networking, learning, and optimization. From the network architecture vantage point, the unified framework leverages a promising fog architecture that enables smart devices to have proximity access to cloud functionalities at the network edge, along the cloud-to-things continuum. From the algorithmic perspective, key innovations target online approaches adaptive to different degrees of nonstationarity in IoT dynamics, and their scalable model-free implementation under limited feedback that motivates blind or bandit approaches. The proposed framework aspires to offer a stepping stone that leads to systematic designs and analysis of task-specific learning and management schemes for IoT, along with a host of new research directions to build on. △ Less","27 October, 2018",https://arxiv.org/pdf/1810.11613
Integrating Transformer and Paraphrase Rules for Sentence Simplification,Sanqiang Zhao;Rui Meng;Daqing He;Saptono Andi;Parmanto Bambang,"Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from ma- chine translation studies and implicitly learned simplification mapping rules from normal- simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we pro- pose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state- of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at https://github.com/ Sanqiang/text_simplification. △ Less","26 October, 2018",https://arxiv.org/pdf/1810.11193
Investigating the Automatic Classification of Algae Using Fusion of Spectral and Morphological Characteristics of Algae via Deep Residual Learning,Jason L. Deglint;Chao Jin;Alexander Wong,"Under the impact of global climate changes and human activities, harmful algae blooms in surface waters have become a growing concern due to negative impacts on water related industries. Therefore, reliable and cost effective methods of quantifying the type and concentration of threshold levels of algae cells has become critical for ensuring successful water management. In this work, we present SAMSON, an innovative system to automatically classify multiple types of algae from different phyla groups by combining standard morphological features with their multi-wavelength signals. Two phyla with focused investigation in this study are the Cyanophyta phylum (blue-green algae), and the Chlorophyta phylum (green algae). We use a custom-designed microscopy imaging system which is configured to image water samples at two fluorescent wavelengths and seven absorption wavelengths using discrete-wavelength high-powered light emitting diodes (LEDs). Powered by computer vision and machine learning, we investigate the possibility and effectiveness of automatic classification using a deep residual convolutional neural network. More specifically, a classification accuracy of 96% was achieved in an experiment conducted with six different algae types. This high level of accuracy was achieved using a deep residual convolutional neural network that learns the optimal combination of spectral and morphological features. These findings elude to the possibility of leveraging a unique fingerprint of algae cell (i.e. spectral wavelengths and morphological features) to automatically distinguish different algae types. Our work herein demonstrates that, when coupled with multi-band fluorescence microscopy, machine learning algorithms can potentially be used as a robust and cost-effective tool for identifying and enumerating algae cells. △ Less","25 October, 2018",https://arxiv.org/pdf/1810.10889
Wearable Affective Robot,Min Chen;Jun Zhou;Guangming Tao;Jun Yang;Long Hu,"With the development of the artificial intelligence (AI), the AI applications have influenced and changed people's daily life greatly. Here, a wearable affective robot that integrates the affective robot, social robot, brain wearable, and wearable 2.0 is proposed for the first time. The proposed wearable affective robot is intended for a wide population, and we believe that it can improve the human health on the spirit level, meeting the fashion requirements at the same time. In this paper, the architecture and design of an innovative wearable affective robot, which is dubbed as Fitbot, are introduced in terms of hardware and algorithm's perspectives. In addition, the important functional component of the robot-brain wearable device is introduced from the aspect of the hardware design, EEG data acquisition and analysis, user behavior perception, and algorithm deployment, etc. Then, the EEG based cognition of user's behavior is realized. Through the continuous acquisition of the in-depth, in-breadth data, the Fitbot we present can gradually enrich user's life modeling and enable the wearable robot to recognize user's intention and further understand the behavioral motivation behind the user's emotion. The learning algorithm for the life modeling embedded in Fitbot can achieve better user's experience of affective social interaction. Finally, the application service scenarios and some challenging issues of a wearable affective robot are discussed. △ Less","25 October, 2018",https://arxiv.org/pdf/1810.10743
LincoSim: a web based HPC-cloud platform for automatic virtual towing tank analysis,Raffaele Ponzini;Francesco Salvadore,"In this work, we present a new web based HPC-cloud platform for automatic virtual towing tank analysis. It is well known that the design project of a new hull requires a continuous integration of shape hypothesis and hydrodynamics verifications using analytical tools, 3D computational methods, experimental facilities and sea keeping trial tests. The complexity and the cost of such design tools increase considerably moving from analytical tools to sea keeping trials. In order to perform a meaningful trade-off between costs and high quality data acquiring during the last decade the usage of 3D computational models has grown pushed also by well-known technological factors. Nevertheless, in the past, there were several limiting factors on the wide diffusion of 3D computational models to perform virtual towing tank data acquiring, from hardware and software costs to the specific technological skills needed. In this work we propose an innovative high-level approach which is embodied in the so-called LincoSim web application in which a hypothetical designer user can carry out the simulation only starting from its own geometry and a set of meaningful physical parameters. LincoSim automatically manages and hides to the user all the necessary details of CFD modelling and of HPC infrastructure usage allowing them to access, visualize and analyze the outputs from the same single access point made up from the web browser. In addition to the web interface, the platform includes a back-end server which implements a Cloud logic and can be connected to multiple HPC machines for computing. LincoSim is currently set up with finite volume Open-FOAM CFD engine. A preliminary validation campaign has been performed to assess the robustness and the reliability of the tool and is proposed as a novel approach for the development of Computer Aided Engineering (CAE) applications. △ Less","23 October, 2018",https://arxiv.org/pdf/1810.09830
"Design Challenges of Multi-UAV Systems in Cyber-Physical Applications: A Comprehensive Survey, and Future Directions",Reza Shakeri;Mohammed Ali Al-Garadi;Ahmed Badawy;Amr Mohamed;Tamer Khattab;Abdulla Al-Ali;Khaled A. Harras;Mohsen Guizani,"Unmanned Aerial Vehicles (UAVs) have recently rapidly grown to facilitate a wide range of innovative applications that can fundamentally change the way cyber-physical systems (CPSs) are designed. CPSs are a modern generation of systems with synergic cooperation between computational and physical potentials that can interact with humans through several new mechanisms. The main advantages of using UAVs in CPS application is their exceptional features, including their mobility, dynamism, effortless deployment, adaptive altitude, agility, adjustability, and effective appraisal of real-world functions anytime and anywhere. Furthermore, from the technology perspective, UAVs are predicted to be a vital element of the development of advanced CPSs. Therefore, in this survey, we aim to pinpoint the most fundamental and important design challenges of multi-UAV systems for CPS applications. We highlight key and versatile aspects that span the coverage and tracking of targets and infrastructure objects, energy-efficient navigation, and image analysis using machine learning for fine-grained CPS applications. Key prototypes and testbeds are also investigated to show how these practical technologies can facilitate CPS applications. We present and propose state-of-the-art algorithms to address design challenges with both quantitative and qualitative methods and map these challenges with important CPS applications to draw insightful conclusions on the challenges of each application. Finally, we summarize potential new directions and ideas that could shape future research in these areas. △ Less","23 October, 2018",https://arxiv.org/pdf/1810.09729
Single-Image SVBRDF Capture with a Rendering-Aware Deep Network,Valentin Deschaintre;Miika Aittala;Fredo Durand;George Drettakis;Adrien Bousseau,"Texture, highlights, and shading are some of many visual cues that allow humans to perceive material appearance in single pictures. Yet, recovering spatially-varying bi-directional reflectance distribution functions (SVBRDFs) from a single image based on such cues has challenged researchers in computer graphics for decades. We tackle lightweight appearance capture by training a deep neural network to automatically extract and make sense of these visual cues. Once trained, our network is capable of recovering per-pixel normal, diffuse albedo, specular albedo and specular roughness from a single picture of a flat surface lit by a hand-held flash. We achieve this goal by introducing several innovations on training data acquisition and network design. For training, we leverage a large dataset of artist-created, procedural SVBRDFs which we sample and render under multiple lighting directions. We further amplify the data by material mixing to cover a wide diversity of shading effects, which allows our network to work across many material classes. Motivated by the observation that distant regions of a material sample often offer complementary visual cues, we design a network that combines an encoder-decoder convolutional track for local feature extraction with a fully-connected track for global feature extraction and propagation. Many important material effects are view-dependent, and as such ambiguous when observed in a single image. We tackle this challenge by defining the loss as a differentiable SVBRDF similarity metric that compares the renderings of the predicted maps against renderings of the ground truth from several lighting and viewing directions. Combined together, these novel ingredients bring clear improvement over state of the art methods for single-shot capture of spatially varying BRDFs. △ Less","23 October, 2018",https://arxiv.org/pdf/1810.09718
Large scale visual place recognition with sub-linear storage growth,Huu Le;Michael Milford,"Robotic and animal mapping systems share many of the same objectives and challenges, but differ in one key aspect: where much of the research in robotic mapping has focused on solving the data association problem, the grid cell neurons underlying maps in the mammalian brain appear to intentionally break data association by encoding many locations with a single grid cell neuron. One potential benefit of this intentional aliasing is both sub-linear map storage and computational requirements growth with environment size, which we demonstrated in a previous proof-of-concept study that detected and encoded mutually complementary co-prime pattern frequencies in the visual map data. In this research, we solve several of the key theoretical and practical limitations of that prototype model and achieve significantly better sub-linear storage growth, a factor reduction in storage requirements per map location, scalability to large datasets on standard compute equipment and improved robustness to environments with visually challenging appearance change. These improvements are achieved through several innovations including a flexible user-driven choice mechanism for the periodic patterns underlying the new encoding method, a parallelized chunking technique that splits the map into sub-sections processed in parallel and a novel feature selection approach that selects only the image information most relevant to the encoded temporal patterns. We evaluate our techniques on two large benchmark datasets with the comparison to the previous state-of-the-art system, as well as providing a detailed analysis of system performance with respect to parameters such as required precision performance and the number of cyclic patterns encoded. △ Less","23 October, 2018",https://arxiv.org/pdf/1810.09660
Why is a Ravencoin Like a TokenDesk? An Exploration of Code Diversity in the Cryptocurrency Landscape,Pierre Reibel;Haaroon Yousaf;Sarah Meiklejohn,"Interest in cryptocurrencies has skyrocketed since their introduction a decade ago, with hundreds of billions of dollars now invested across a landscape of thousands of different cryptocurrencies. While there is significant diversity, there is also a significant number of scams as people seek to exploit the current popularity. In this paper, we seek to identify the extent of innovation in the cryptocurrency landscape using the open-source repositories associated with each one. Among other findings, we observe that while many cryptocurrencies are largely unchanged copies of Bitcoin, the use of Ethereum as a platform has enabled the deployment of cryptocurrencies with more diverse functionalities. △ Less","19 October, 2018",https://arxiv.org/pdf/1810.08420
Hybrid Feature Based SLAM Prototype,V. I Mebin Jose;D. J Binoj,"The development of data innovation as of late and the expanded limit, has permitted the acquaintance of artificial vision connected with SLAM, offering ascend to what is known as Visual SLAM. The objective of this paper is to build up a route framework dependent on Visual SLAM to get a robot to a fundamental and new condition, have the capacity to set and make a three-dimensional guide thereof, utilizing just as sources of info recording your way with a stereo vision camera. The consequence of this analysis is that the framework Visual SLAM together with the combination of Fast SLAM (combination of kalman with particulate filter and SIFT) perceive and recognize characteristic points in images so adequately exact and unambiguous. This framework uses MATLAB, since its adaptability and comfort for performing a wide range of tests. The program has been tested by inserting a prerecorded video input with a camera stereo in which a course is done by an office environment. The algorithm initially locates points of interest in a stereo frame captured by the camera. These will be located in 3D and they associate an identification descriptor. In the next frame, the camera likewise identified points of interest and it will be compared which of them have been previously detected by comparing their descriptors. This process is known as ""data association"" and its successful completion is fundamental to the SLAM algorithm. The position data of the robot and points interest stored in data structures known as ""particles"" that evolve independently. Its management is very important for the proper functioning of the algorithm Fast SLAM. The results are found to be satisfactory. △ Less","18 October, 2018",https://arxiv.org/pdf/1810.07230
"Cyber-Physical Systems, a new formal paradigm to model redundancy and resiliency",Mario Lezoche;Hervé Panetto,"Cyber-Physical Systems (CPS) are systems composed by a physical component that is controlled or monitored by a cyber-component, a computer-based algorithm. Advances in CPS technologies and science are enabling capability, adaptability, scalability, resiliency, safety, security, and usability that will far exceed the simple embedded systems of today. CPS technologies are transforming the way people interact with engineered systems. New smart CPS are driving innovation in various sectors such as agriculture, energy, transportation, healthcare, and manufacturing. They are leading the 4-th Industrial Revolution (Industry 4.0) that is having benefits thanks to the high flexibility of production. The Industry 4.0 production paradigm is characterized by high intercommunicating properties of its production elements in all the manufacturing processes. This is the reason it is a core concept how the systems should be structurally optimized to have the adequate level of redundancy to be satisfactorily resilient. This goal can benefit from formal methods well known in various scientific domains such as artificial intelligence. So, the current research concerns the proposal of a CPS meta-model and its instantiation. In this way it lists all kind of relationships that may occur between the CPSs themselves and between their (cyber-and physical-) components. Using the CPS meta-model formalization, with an adaptation of the Formal Concept Analysis (FCA) formal approach, this paper presents a way to optimize the modelling of CPS systems emphasizing their redundancy and their resiliency. △ Less","16 October, 2018",https://arxiv.org/pdf/1810.06911
A Simple Change Comparison Method for Image Sequences Based on Uncertainty Coefficient,Ruzhang Zhao;Yajun Fang;Berthold K. P. Horn,"For identification of change information in image sequences, most studies focus on change detection in one image sequence, while few studies have considered the change level comparison between two different image sequences. Moreover, most studies require the detection of image information in details, for example, object detection. Based on Uncertainty Coefficient(UC), this paper proposes an innovative method CCUC for change comparison between two image sequences. The proposed method is computationally efficient and simple to implement. The change comparison stems from video monitoring system. The limited number of provided screens and a large number of monitoring cameras require the videos or image sequences ordered by change level. We demonstrate this new method by applying it on two publicly available image sequences. The results are able to show the method can distinguish the different change level for sequences. △ Less","14 October, 2018",https://arxiv.org/pdf/1810.06055
A Novel Domain Adaptation Framework for Medical Image Segmentation,Amir Gholami;Shashank Subramanian;Varun Shenoy;Naveen Himthani;Xiangyu Yue;Sicheng Zhao;Peter Jin;George Biros;Kurt Keutzer,"We propose a segmentation framework that uses deep neural networks and introduce two innovations. First, we describe a biophysics-based domain adaptation method. Second, we propose an automatic method to segment white and gray matter, and cerebrospinal fluid, in addition to tumorous tissue. Regarding our first innovation, we use a domain adaptation framework that combines a novel multispecies biophysical tumor growth model with a generative adversarial model to create realistic looking synthetic multimodal MR images with known segmentation. Regarding our second innovation, we propose an automatic approach to enrich available segmentation data by computing the segmentation for healthy tissues. This segmentation, which is done using diffeomorphic image registration between the BraTS training data and a set of prelabeled atlases, provides more information for training and reduces the class imbalance problem. Our overall approach is not specific to any particular neural network and can be used in conjunction with existing solutions. We demonstrate the performance improvement using a 2D U-Net for the BraTS'18 segmentation challenge. Our biophysics based domain adaptation achieves better results, as compared to the existing state-of-the-art GAN model used to create synthetic data for training. △ Less","11 October, 2018",https://arxiv.org/pdf/1810.05732
Parity games and universal graphs,Thomas Colcombet;Nathanaël Fijalkow,"This paper is a contribution to the study of parity games and the recent constructions of three quasipolynomial time algorithms for solving them. We revisit a result of Czerwiński, Daviaud, Fijalkow, Jurdziński, Lazić, and Parys witnessing a quasipolynomial barrier for all three quasipolynomial time algorithms. The argument is that all three algorithms can be understood as constructing a so-called separating automaton, and to give a quasipolynomial lower bond on the size of separating automata. We give an alternative proof of this result. The key innovations of this paper are the notion of universal graphs and the idea of saturation. △ Less","19 October, 2018",https://arxiv.org/pdf/1810.05106
A Simple Way to Deal with Cherry-picking,Junpei Komiyama;Takanori Maehara,"Statistical hypothesis testing serves as statistical evidence for scientific innovation. However, if the reported results are intentionally biased, hypothesis testing no longer controls the rate of false discovery. In particular, we study such selection bias in machine learning models where the reporter is motivated to promote an algorithmic innovation. When the number of possible configurations (e.g., datasets) is large, we show that the reporter can falsely report an innovation even if there is no improvement at all. We propose a `post-reporting' solution to this issue where the bias of the reported results is verified by another set of results. The theoretical findings are supported by experimental results with synthetic and real-world datasets. △ Less","11 October, 2018",https://arxiv.org/pdf/1810.04996
The Fundamental Theorem of Algebra in ACL2,Ruben Gamboa;John Cowles,"We report on a verification of the Fundamental Theorem of Algebra in ACL2(r). The proof consists of four parts. First, continuity for both complex-valued and real-valued functions of complex numbers is defined, and it is shown that continuous functions from the complex to the real numbers achieve a minimum value over a closed square region. An important case of continuous real-valued, complex functions results from taking the traditional complex norm of a continuous complex function. We think of these continuous functions as having only one (complex) argument, but in ACL2(r) they appear as functions of two arguments. The extra argument is a ""context"", which is uninterpreted. For example, it could be other arguments that are held fixed, as in an exponential function which has a base and an exponent, either of which could be held fixed. Second, it is shown that complex polynomials are continuous, so the norm of a complex polynomial is a continuous real-valued function and it achieves its minimum over an arbitrary square region centered at the origin. This part of the proof benefits from the introduction of the ""context"" argument, and it illustrates an innovation that simplifies the proofs of classical properties with unbound parameters. Third, we derive lower and upper bounds on the norm of non-constant polynomials for inputs that are sufficiently far away from the origin. This means that a sufficiently large square can be found to guarantee that it contains the global minimum of the norm of the polynomial. Fourth, it is shown that if a given number is not a root of a non-constant polynomial, then it cannot be the global minimum. Finally, these results are combined to show that the global minimum must be a root of the polynomial. This result is part of a larger effort in the formalization of complex polynomials in ACL2(r). △ Less","9 October, 2018",https://arxiv.org/pdf/1810.04314
Multibeam for Joint Communication and Sensing Using Steerable Analog Antenna Arrays,J. Andrew Zhang;Xiaojing Huang;Y. Jay Guo;Jinhong Yuan;Robert W. Heath Jr,"Beamforming has great potential for joint communication and sensing (JCAS), which is becoming a demanding feature on many emerging platforms such as unmanned aerial vehicles and smart cars. Although beamforming has been extensively studied for communication and radar sensing respectively, its application in the joint system is not straightforward due to different beamforming requirements by communication and sensing. In this paper, we propose a novel multibeam framework using steerable analog antenna arrays, which allows seamless integration of communication and sensing. Different to conventional JCAS schemes that support JCAS using a single beam, our framework is based on the key innovation of multibeam technology: providing fixed subbeam for communication and packet-varying scanning subbeam for sensing, simultaneously from a single transmitting array. We provide a system architecture and protocols for the proposed framework, complying well with modern packet communication systems with multicarrier modulation. We also propose low-complexity and effective multibeam design and generation methods, which offer great flexibility in meeting different communication and sensing requirements. We further develop sensing parameter estimation algorithms using conventional digital Fourier transform and 1D compressive sensing techniques, matching well with the multibeam framework. Simulation results are provided and validate the effectiveness of our proposed framework, beamforming design methods and the sensing algorithms. △ Less","6 October, 2018",https://arxiv.org/pdf/1810.04105
DeepImageSpam: Deep Learning based Image Spam Detection,Amara Dinesh Kumar;Vinayakumar R;Soman KP,Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classification achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques △ Less,"3 October, 2018",https://arxiv.org/pdf/1810.03977
Outcome-Driven Open Innovation at NASA,Jennifer L Gustetic;Jason Crusan;Steve Rader;Sam Ortega,"In an increasingly connected and networked world, the National Aeronautics and Space Administration (NASA) recognizes the value of the public as a strategic partner in addressing some of our most pressing challenges. The agency is working to more effectively harness the expertise, ingenuity, and creativity of individual members of the public by enabling, accelerating, and scaling the use of open innovation approaches including prizes, challenges, and crowdsourcing. As NASA's use of open innovation tools to solve a variety of types of problems and advance of number of outcomes continues to grow, challenge design is also becoming more sophisticated as our expertise and capacity (personnel, platforms, and partners) grows and develops. NASA has recently pivoted from talking about the benefits of challenge-driven approaches, to the outcomes these types of activities yield. Challenge design should be informed by desired outcomes that align with NASA's mission. This paper provides several case studies of NASA open innovation activities and maps the outcomes of those activities to a successful set of outcomes that challenges can help drive alongside traditional tools such as contracts, grants and partnerships. △ Less","8 October, 2018",https://arxiv.org/pdf/1810.03426
Eiffel: Efficient and Flexible Software Packet Scheduling,Ahmed Saeed;Yimeng Zhao;Nandita Dukkipati;Mostafa Ammar;Ellen Zegura;Khaled Harras;Amin Vahdat,"Packet scheduling determines the ordering of packets in a queuing data structure with respect to some ranking function that is mandated by a scheduling policy. It is the core component in many recent innovations to optimize network performance and utilization. Our focus in this paper is on the design and deployment of packet scheduling in software. Software schedulers have several advantages over hardware including shorter development cycle and flexibility in functionality and deployment location. We substantially improve current software packet scheduling performance, while maintaining flexibility, by exploiting underlying features of packet ranking; namely, packet ranks are integers and, at any point in time, fall within a limited range of values. We introduce Eiffel, a novel programmable packet scheduling system. At the core of Eiffel is an integer priority queue based on the Find First Set (FFS) instruction and designed to support a wide range of policies and ranking functions efficiently. As an even more efficient alternative, we also propose a new approximate priority queue that can outperform FFS-based queues for some scenarios. To support flexibility, Eiffel introduces novel programming abstractions to express scheduling policies that cannot be captured by current, state-of-the-art scheduler programming models. We evaluate Eiffel in a variety of settings and in both kernel and userspace deployments. We show that it outperforms state of the art systems by 3-40x in terms of either number of cores utilized for network processing or number of flows given fixed processing capacity. △ Less","6 October, 2018",https://arxiv.org/pdf/1810.03060
An Edge-Computing Based Architecture for Mobile Augmented Reality,Jinke Ren;Yinghui He;Guan Huang;Guanding Yu;Yunlong Cai;Zhaoyang Zhang,"In order to mitigate the long processing delay and high energy consumption of mobile augmented reality (AR) applications, mobile edge computing (MEC) has been recently proposed and is envisioned as a promising means to deliver better quality of experience (QoE) for AR consumers. In this article, we first present a comprehensive AR overview, including the indispensable components of general AR applications, fashionable AR devices, and several existing techniques for overcoming the thorny latency and energy consumption problems. Then, we propose a novel hierarchical computation architecture by inserting an edge layer between the conventional user layer and cloud layer. Based on the proposed architecture, we further develop an innovated operation mechanism to improve the performance of mobile AR applications. Three key technologies are also discussed to further assist the proposed AR architecture. Simulation results are finally provided to verify that our proposals can significantly improve the latency and energy performance as compared against existing baseline schemes. △ Less","15 October, 2018",https://arxiv.org/pdf/1810.02509
Domain Specific Approximation for Object Detection,Ting-Wu Chin;Chia-Lin Yu;Matthew Halpern;Hasan Genc;Shiao-Li Tsao;Vijay Janapa Reddi,"There is growing interest in object detection in advanced driver assistance systems and autonomous robots and vehicles. To enable such innovative systems, we need faster object detection. In this work, we investigate the trade-off between accuracy and speed with domain-specific approximations, i.e. category-aware image size scaling and proposals scaling, for two state-of-the-art deep learning-based object detection meta-architectures. We study the effectiveness of applying approximation both statically and dynamically to understand the potential and the applicability of them. By conducting experiments on the ImageNet VID dataset, we show that domain-specific approximation has great potential to improve the speed of the system without deteriorating the accuracy of object detectors, i.e. up to 7.5x speedup for dynamic domain-specific approximation. To this end, we present our insights toward harvesting domain-specific approximation as well as devise a proof-of-concept runtime, AutoFocus, that exploits dynamic domain-specific approximation. △ Less","3 October, 2018",https://arxiv.org/pdf/1810.02010
Distributing and Obfuscating Firewalls via Oblivious Bloom Filter Evaluation,Ken Goss;Wei Jiang,"Firewalls have long been in use to protect local networks from threats of the larger Internet. Although firewalls are effective in preventing attacks initiated from outside, they are vulnerable to insider threats, e.g., malicious insiders may access and alter firewall configurations, and disable firewall services. In this paper, we develop an innovative distributed architecture to obliviously manage and evaluate firewalls to prevent both insider and external attacks oriented to the firewalls. Our proposed structure alleviates these issues by obfuscating the firewall rules or policies themselves, then distributing the function of evaluating these rules across multiple servers. Thus, both accessing and altering the rules are considerably more difficult thereby providing better protection to the local network as well as greater security for the firewall itself. We achieve this by integrating multiple areas of research such as secret sharing schemes and multi-party computation, as well as Bloom filters and Byzantine agreement protocols. Our resulting solution is an efficient and secure means by which a firewall may be distributed, and obfuscated while maintaining the ability for multiple servers to obliviously evaluate its functionality. △ Less","2 October, 2018",https://arxiv.org/pdf/1810.01571
Fusion of Monocular Vision and Radio-based Ranging for Global Scale Estimation and Drift Mitigation,Young-Hee Lee;Chen Zhu;Gabriele Giorgi;Christoph Günther,"Monocular vision-based Simultaneous Localization and Mapping (SLAM) is used for various purposes due to its advantages in cost, simple setup, as well as availability in the environments where navigation with satellites is not effective. However, camera motion and map points can be estimated only up to a global scale factor with monocular vision. Moreover, estimation error accumulates over time without bound, if the camera cannot detect the previously observed map points for closing a loop. We propose an innovative approach to estimate a global scale factor and reduce drifts in monocular vision-based localization with an additional single ranging link. Our method can be easily integrated with the back-end of monocular visual SLAM methods. We demonstrate our algorithm with real datasets collected on a rover, and show the evaluation results. △ Less","2 October, 2018",https://arxiv.org/pdf/1810.01346
Celer Network: Bring Internet Scale to Every Blockchain,Mo Dong;Qingkai Liang;Xiaozhou Li;Junda Liu,"Off-chain scaling techniques allow mutually distrustful parties to execute a contract locally among themselves instead of on the global blockchain. Parties involved in the transaction maintain a multi-signature fraud-proof off-chain replicated state machine, and only resort to on-chain consensus when absolutely necessary (e.g., when two parties disagree on a state). Off-chain scaling is the only way to support fully scale-out decentralized applications (""dApps"") with better privacy and no compromise on the trust and decentralization guarantees. It is the inflection point for blockchain mass adoption, and will be the engine behind all scalable dApps. Celer Network is an Internet-scale, trust-free, and privacy-preserving platform where everyone can quickly build, operate, and use highly scalable dApps. It is not a standalone blockchain but a networked system running on top of existing and future blockchains. It provides unprecedented performance and flexibility through innovation in off-chain scaling techniques and incentive-aligned cryptoeconomics. Celer Network embraces a layered architecture with clean abstractions that enable rapid evolution of each individual component, including a generalized state channel and sidechain suite that supports fast and generic off-chain state transitions; a provably optimal value transfer routing mechanism that achieves an order of magnitude higher throughput compared to state-of-the-art solutions; a powerful development framework and runtime for off-chain applications; and a new cryptoeconomic model that provides network effect, stable liquidity, and high availability for the off-chain ecosystem. △ Less","28 September, 2018",https://arxiv.org/pdf/1810.00037
Explainable Black-Box Attacks Against Model-based Authentication,Washington Garcia;Joseph I. Choi;Suman K. Adari;Somesh Jha;Kevin R. B. Butler,"Establishing unique identities for both humans and end systems has been an active research problem in the security community, giving rise to innovative machine learning-based authentication techniques. Although such techniques offer an automated method to establish identity, they have not been vetted against sophisticated attacks that target their core machine learning technique. This paper demonstrates that mimicking the unique signatures generated by host fingerprinting and biometric authentication systems is possible. We expose the ineffectiveness of underlying machine learning classification models by constructing a blind attack based around the query synthesis framework and utilizing Explainable-AI (XAI) techniques. We launch an attack in under 130 queries on a state-of-the-art face authentication system, and under 100 queries on a host authentication system. We examine how these attacks can be defended against and explore their limitations. XAI provides an effective means for adversaries to infer decision boundaries and provides a new way forward in constructing attacks against systems using machine learning models for authentication. △ Less","28 September, 2018",https://arxiv.org/pdf/1810.00024
Can female fertility management mobile apps be sustainable and contribute to female health care? Harnessing the power of patient generated data ; Analysis of the organizations active in this e-Health segment,Maki Miyamoto;L. F. Pau,"In recent years, personal health technologies have emerged that allow patients to collect a wide range of health-related data outside the clinic. These patient-generated data (PGD) reflect patients everyday behaviors including physical activity, mood, diet, sleep, and symptoms. However, major players and academics alike, have ignored the case where these patients or normal people are women. Is analyzed the eHealth segment of female fertility planning mobile apps (in US called: period trackers) and its possible extensions to other female health care mobile services. The market potential is very large although age segmentation applies. These apps help women record and plan their menstruation cycles, their fertility periods, and ease with relevant personalized advice all the uncomfort. As an illustration, the case of a European app service supplier is described in depth. The services of ten worldwide suppliers are compared in terms of functionality, adoption, organization, financial and business aspects. The research question: Can female fertility management mobile apps be sustainable and contribute to female health care, is researched by a combination of academic literature study, testing of 7 essential hypotheses, and a limited user driven experimental demand analysis. Quality and impact metrics from a user point of view are proposed. The conclusion is a moderate yes to the research question, with several conditions. Further research and innovative ideas, as well as marketing and strategic directions are provided, incl. associations with male fertility apps. △ Less","28 September, 2018",https://arxiv.org/pdf/1809.11042
Understanding the influence of Individual's Self-efficacy for Information Systems Security Innovation Adoption: A Systematic Literature Review,Mumtaz Abdul Hameed;Nalin Asanka Gamagedara Arachchilage,"Information Systems security cannot be fully apprehended if the user lacks the required knowledge and skills to effectively apply the safeguard measures. Knowledge and skills enhance one's self-efficacy. Individual self-efficacy is an important element in ensuring Information Systems safeguard effectiveness. In this research, we explore the role of individual's self-efficacy for Information Systems security adoption. The study uses the method of Systematic Literature Review using 42 extant studies to evaluate individual self- efficacy for Information Systems security innovation adoption. The systematic review findings reveal the appropriateness of the existing empirical investigations on the individual self-efficacy for Information Systems security adoption. Furthermore, the review results confirmed the significance of the relationship between individual self-efficacy and Information Systems security adoption. In addition, the study validates the past administration of the research on this subject in terms of sample size, sample subject and theoretical grounds. △ Less","28 September, 2018",https://arxiv.org/pdf/1809.10890
Programming at Exascale: Challenges and Innovations,Jalal Abdulbaqi,"Supercomputers become faster as hardware and software technologies continue to evolve. Current supercomputers are capable of 1015 floating point operations per second (FLOPS) that called Petascale system. The High Performance Computer (HPC) community is Looking forward to the system with capability of 1018 (FLOPS) that is called Exascale. Having a system to thousand times faster than the previous one produces challenges to the high performance computer (HPC) community. These challenges require innovation in software and hardware. In this paper, the challenges posed for programming at Exascale systems are reviewed and the developments in the main programming models and systems are surveyed. △ Less","24 September, 2018",https://arxiv.org/pdf/1809.10023
An Exploration of Mimic Architectures for Residual Network Based Spectral Mapping,Peter Plantinga;Deblin Bagchi;Eric Fosler-Lussier,"Spectral mapping uses a deep neural network (DNN) to map directly from noisy speech to clean speech. Our previous study found that the performance of spectral mapping improves greatly when using helpful cues from an acoustic model trained on clean speech. The mapper network learns to mimic the input favored by the spectral classifier and cleans the features accordingly. In this study, we explore two new innovations: we replace a DNN-based spectral mapper with a residual network that is more attuned to the goal of predicting clean speech. We also examine how integrating long term context in the mimic criterion (via wide-residual biLSTM networks) affects the performance of spectral mapping compared to DNNs. Our goal is to derive a model that can be used as a preprocessor for any recognition system; the features derived from our model are passed through the standard Kaldi ASR pipeline and achieve a WER of 9.3%, which is the lowest recorded word error rate for CHiME-2 dataset using only feature adaptation. △ Less","25 September, 2018",https://arxiv.org/pdf/1809.09756
On Using Blockchains for Safety-Critical Systems,Christian Berger;Birgit Penzenstadler;Olaf Drögehorn,"Innovation in the world of today is mainly driven by software. Companies need to continuously rejuvenate their product portfolios with new features to stay ahead of their competitors. For example, recent trends explore the application of blockchains to domains other than finance. This paper analyzes the state-of-the-art for safety-critical systems as found in modern vehicles like self-driving cars, smart energy systems, and home automation focusing on specific challenges where key ideas behind blockchains might be applicable. Next, potential benefits unlocked by applying such ideas are presented and discussed for the respective usage scenario. Finally, a research agenda is outlined to summarize remaining challenges for successfully applying blockchains to safety-critical cyber-physical systems. △ Less","24 September, 2018",https://arxiv.org/pdf/1809.08877
Data-Driven Design: Exploring new Structural Forms using Machine Learning and Graphic Statics,Lukas Fuhrimann;Vahid Moosavi;Patrick Ole Ohlbrock;Pierluigi Dacunto,"The aim of this research is to introduce a novel structural design process that allows architects and engineers to extend their typical design space horizon and thereby promoting the idea of creativity in structural design. The theoretical base of this work builds on the combination of structural form-finding and state-of-the-art machine learning algorithms. In the first step of the process, Combinatorial Equilibrium Modelling (CEM) is used to generate a large variety of spatial networks in equilibrium for given input parameters. In the second step, these networks are clustered and represented in a form-map through the implementation of a Self Organizing Map (SOM) algorithm. In the third step, the solution space is interpreted with the help of a Uniform Manifold Approximation and Projection algorithm (UMAP). This allows gaining important insights in the structure of the solution space. A specific case study is used to illustrate how the infinite equilibrium states of a given topology can be defined and represented by clusters. Furthermore, three classes, related to the non-linear interaction between the input parameters and the form space, are verified and a statement about the entire manifold of the solution space of the case study is made. To conclude, this work presents an innovative approach on how the manifold of a solution space can be grasped with a minimum amount of data and how to operate within the manifold in order to increase the diversity of solutions. △ Less","23 September, 2018",https://arxiv.org/pdf/1809.08660
Gamifying the Escape from the Engineering Method Prison - An Innovative Board Game to Teach the Essence Theory to Future Project Managers and Software Engineers,Kai-Kristian Kemell;Juhani Risku;Arthur Evensen;Pekka Abrahamsson;Aleksander Madsen Dahl;Lars Henrik Grytten;Agata Jedryszek;Petter Rostrup;Anh Nguyen-Duc,"Software Engineering is an engineering discipline but lacks a solid theoretical foundation. One effort in remedying this situation has been the SEMAT Essence specification. Essence consists of a language for modeling Software Engineering (SE) practices and methods and a kernel containing what its authors describe as being elements that are present in every software development project. In practice, it is a method agnostic project management tool for SE Projects. Using the language of the specification, Essence can be used to model any software development method or practice. Thus, the specification can potentially be applied to any software development context, making it a powerful tool. However, due to the manual work and the learning process involved in modeling practices with Essence, its initial adoption can be tasking for development teams. Due to the importance of project management in SE projects, new project management tools such as Essence are valuable, and facilitating their adoption is consequently important. To tackle this issue in the case of Essence, we present a game-based approach to teaching the use Essence. In this paper, we gamify the learning process by means of an innovative board game. The game is empirically validated in a study involving students from the IT faculty of University of Jyväskylä (n=61). Based on the results, we report the effectiveness of the game-based approach to teaching both Essence and SE project work. △ Less","23 September, 2018",https://arxiv.org/pdf/1809.08656
A Unified Framework for the Tractable Analysis of Multi-Antenna Wireless Networks,Xianghao Yu;Chang Li;Jun Zhang;Martin Haenggi;Khaled B. Letaief,"Densifying networks and deploying more antennas at each access point are two principal ways to boost the capacity of wireless networks. However, the complicated distributions of the signal power and the accumulated interference power, largely induced by various space-time processing techniques, make it highly challenging to quantitatively characterize the performance of multi-antenna networks. In this paper, using tools from stochastic geometry, a unified framework is developed for the analysis of such networks. The major results are two innovative representations of the coverage probability, which make the analysis of multi-antenna networks almost as tractable as the single-antenna case. One is expressed as an \ell_1-induced norm of a Toeplitz matrix, and the other is given in a finite sum form. With a compact representation, the former incorporates many existing analytical results on single- and multi-antenna networks as special cases, and leads to tractable expressions for evaluating the coverage probability in both ad hoc and cellular networks. While the latter is more complicated for numerical evaluation, it helps analytically gain key design insights. In particular, it helps prove that the coverage probability of ad hoc networks is a monotonically decreasing convex function of the transmitter density and that there exists a peak value of the coverage improvement when increasing the number of transmit antennas. On the other hand, in multi-antenna cellular networks, it is shown that the coverage probability is independent of the transmitter density and that the outage probability decreases exponentially as the number of transmit antennas increases. △ Less","21 September, 2018",https://arxiv.org/pdf/1809.08365
Enabling Ultra-Low Delay Teleorchestras using Software Defined Networking,Emmanouil Lakiotakis;Christos Liaskos;Xenofontas Dimitropoulos,"Ultra-low delay sensitive applications can afford delay only at the level of msec. An example of this application class are the Networked Music Performance (NMP) systems that describe a live music performance by geographically separate musicians over the Internet. The present work proposes a novel architecture for NMP systems, where the key-innovation is the close collaboration between the network and the application. Using SDN principles, the applications are enabled to adapt their internal audio signal processing, in order to cope with network delay increase. Thus, affordable end-to-end delay is provided to NMP users, even under considerable network congestion. △ Less","29 August, 2018",https://arxiv.org/pdf/1809.07864
On the Fly Orchestration of Unikernels: Tuning and Performance Evaluation of Virtual Infrastructure Managers,Pier Luigi Ventre;Paolo Lungaroni;Giuseppe Siracusano;Claudio Pisa;Florian Schmidt;Francesco Lombardo;Stefano Salsano,"Network operators are facing significant challenges meeting the demand for more bandwidth, agile infrastructures, innovative services, while keeping costs low. Network Functions Virtualization (NFV) and Cloud Computing are emerging as key trends of 5G network architectures, providing flexibility, fast instantiation times, support of Commercial Off The Shelf hardware and significant cost savings. NFV leverages Cloud Computing principles to move the data-plane network functions from expensive, closed and proprietary hardware to the so-called Virtual Network Functions (VNFs). In this paper we deal with the management of virtual computing resources (Unikernels) for the execution of VNFs. This functionality is performed by the Virtual Infrastructure Manager (VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We discuss the instantiation process of virtual resources and propose a generic reference model, starting from the analysis of three open source VIMs, namely OpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing the support for special-purpose Unikernels and aiming at reducing the duration of the instantiation process. We evaluate some performance aspects of the VIMs, considering both stock and tuned versions. The VIM extensions and performance evaluation tools are available under a liberal open source licence. △ Less","17 September, 2018",https://arxiv.org/pdf/1809.07701
Local Density Estimation in High Dimensions,Xian Wu;Moses Charikar;Vishnu Natchu,"An important question that arises in the study of high dimensional vector representations learned from data is: given a set \mathcal{D} of vectors and a query q, estimate the number of points within a specified distance threshold of q. We develop two estimators, LSH Count and Multi-Probe Count that use locality sensitive hashing to preprocess the data to accurately and efficiently estimate the answers to such questions via importance sampling. A key innovation is the ability to maintain a small number of hash tables via preprocessing data structures and algorithms that sample from multiple buckets in each hash table. We give bounds on the space requirements and sample complexity of our schemes, and demonstrate their effectiveness in experiments on a standard word embedding dataset. △ Less","20 September, 2018",https://arxiv.org/pdf/1809.07471
Projective Splitting with Forward Steps only Requires Continuity,Patrick R. Johnstone;Jonathan Eckstein,A recent innovation in projective splitting algorithms for monotone operator inclusions has been the development of a procedure using two forward steps instead of the customary proximal steps for operators that are Lipschitz continuous. This paper shows that the Lipschitz assumption is unnecessary when the forward steps are performed in finite-dimensional spaces: a backtracking linesearch yields a convergent algorithm for operators that are merely continuous with full domain. △ Less,"17 September, 2018",https://arxiv.org/pdf/1809.07180
"Powerful, transferable representations for molecules through intelligent task selection in deep multitask networks",Clyde Fare;Lukas Turcani;Edward O. Pyzer-Knapp,"Chemical representations derived from deep learning are emerging as a powerful tool in areas such as drug discovery and materials innovation. Currently, this methodology has three major limitations - the cost of representation generation, risk of inherited bias, and the requirement for large amounts of data. We propose the use of multi-task learning in tandem with transfer learning to address these limitations directly. In order to avoid introducing unknown bias into multi-task learning through the task selection itself, we calculate task similarity through pairwise task affinity, and use this measure to programmatically select tasks. We test this methodology on several real-world data sets to demonstrate its potential for execution in complex and low-data environments. Finally, we utilise the task similarity to further probe the expressiveness of the learned representation through a comparison to a commonly used cheminformatics fingerprint, and show that the deep representation is able to capture more expressive task-based information. △ Less","17 September, 2018",https://arxiv.org/pdf/1809.06334
An Approach to Handle Big Data Warehouse Evolution,Darja Solodovnikova;Laila Niedrite,"One of the purposes of Big Data systems is to support analysis of data gathered from heterogeneous data sources. Since data warehouses have been used for several decades to achieve the same goal, they could be leveraged also to provide analysis of data stored in Big Data systems. The problem of adapting data warehouse data and schemata to changes in these requirements as well as data sources has been studied by many researchers worldwide. However, innovative methods must be developed also to support evolution of data warehouses that are used to analyze data stored in Big Data systems. In this paper, we propose a data warehouse architecture that allows to perform different kinds of analytical tasks, including OLAP-like analysis, on big data loaded from multiple heterogeneous data sources with different latency and is capable of processing changes in data sources as well as evolving analysis requirements. The operation of the architecture is highly based on the metadata that are outlined in the paper. △ Less","12 September, 2018",https://arxiv.org/pdf/1809.04284
Efficiency and detectability of random reactive jamming in carrier sense wireless networks,Ni An;Steven Weber,"A natural basis for the detection of a wireless random reactive jammer (RRJ) is the perceived violation by the detector (typically located at the access point (AP)) of the carrier sensing protocol underpinning many wireless random access protocols (e.g., WiFi). Specifically, when the wireless medium is perceived by a station to be busy, a carrier sensing compliant station will avoid transmission while a RRJ station will often initiate transmission. However, hidden terminals (HTs), i.e., activity detected by the AP but not by the sensing station, complicate the use of carrier sensing as the basis for RRJ detection since they provide plausible deniability to a station suspected of being an RRJ. The RRJ has the dual objectives of avoiding detection and effectively disrupting communication, but there is an inherent performance tradeoff between these two objectives. In this paper we capture the behavior of both the RRJ and the compliant stations via a parsimonious Markov chain model, and pose the detection problem using the framework of Markov chain hypothesis testing. Our analysis yields the receiver operating characteristic of the detector, and the optimized behavior of the RRJ. While there has been extensive work in the literature on jamming detection, our innovation lies in leveraging carrier sensing as a natural and effective basis for detection. △ Less","12 September, 2018",https://arxiv.org/pdf/1809.04263
Visions and Challenges in Managing and Preserving Data to Measure Quality of Life,Vero Estrada-Galinanes;Katarzyna Wac,"Health-related data analysis plays an important role in self-knowledge, disease prevention, diagnosis, and quality of life assessment. With the advent of data-driven solutions, a myriad of apps and Internet of Things (IoT) devices (wearables, home-medical sensors, etc) facilitates data collection and provide cloud storage with a central administration. More recently, blockchain and other distributed ledgers became available as alternative storage options based on decentralised organisation systems. We bring attention to the human data bleeding problem and argue that neither centralised nor decentralised system organisations are a magic bullet for data-driven innovation if individual, community and societal values are ignored. The motivation for this position paper is to elaborate on strategies to protect privacy as well as to encourage data sharing and support open data without requiring a complex access protocol for researchers. Our main contribution is to outline the design of a self-regulated Open Health Archive (OHA) system with focus on quality of life (QoL) data. △ Less","6 September, 2018",https://arxiv.org/pdf/1809.01974
Travel Speed Prediction with a Hierarchical Convolutional Neural Network and Long Short-Term Memory Model Framework,Wei Wang;Xucheng Li,"Advanced travel information and warning, if provided accurately, can help road users avoid traffic congestion through dynamic route planning and behavior change. It also enables traffic control centres mitigate the impact of congestion by activating Intelligent Transport System (ITS) proactively. Deep learning has become increasingly popular in recent years, following a surge of innovative GPU technology, high-resolution, big datasets and thriving machine learning algorithms. However, there are few examples exploiting this emerging technology to develop applications for traffic prediction. This is largely due to the difficulty in capturing random, seasonal, non-linear, and spatio-temporal correlated nature of traffic data. In this paper, we propose a data-driven modelling approach with a novel hierarchical D-CLSTM-t deep learning model for short-term traffic speed prediction, a framework combined with convolutional neural network (CNN) and long short-term memory (LSTM) models. A deep CNN model is employed to learn the spatio-temporal traffic patterns of the input graphs, which are then fed into a deep LSTM model for sequence learning. To capture traffic seasonal variations, time of the day and day of the week indicators are fused with trained features. The model is trained end-to-end to predict travel speed in 15 to 90 minutes in the future. We compare the model performance against other baseline models including CNN, LGBM, LSTM, and traditional speed-flow curves. Experiment results show that the D-CLSTM-t outperforms other models considerably. Model tests show that speed upstream also responds sensibly to a sudden accident occurring downstream. Our D-CLSTM-t model framework is also highly scalable for future extension such as for network-wide traffic prediction, which can also be improved by including additional features such as weather, long term seasonality and accident information. △ Less","8 September, 2018",https://arxiv.org/pdf/1809.01887
Bicomp: A Bilayer Scalable Nakamoto Consensus Protocol,Zhenzhen Jiao;Rui Tian;Dezhong Shang;Hui Ding,"Blockchain has received great attention in recent years and motivated innovations in different scenarios. However, many vital issues which affect its performance are still open. For example, it is widely convinced that high level of security and scalability and full decentralization are still impossible to achieve simultaneously. In this paper, we propose Bicomp, a bilayer scalable Nakamoto consensus protocol, which is an approach based on high security and pure decentralized Nakamoto consensus, and with a significant improvement on scalability. In Bicomp, two kinds of blocks are generated, i.e., microblocks for concurrent transaction packaging in network, and macroblocks for leadership competition and chain formation. A leader is elected at beginning of each round by using a macroblock header from proof-of-work. An elected leader then receives and packages multiple microblocks mined by different nodes into one macroblock during its tenure, which results in a bilayer block structure. Such design limits a leader's power and encourages as many nodes as possible to participate in the process of packaging transactions, which promotes the sharding nature of the system. Furthermore, several mechanisms are carefully designed to reduce transaction overlapping and further limit a leader's power, among which a novel transaction diversity based metric is proposed as the second level criteria besides the longest-chain-first principle on selecting a legitimate chain when fork happens. Security issues and potential attacks to Bicomp are extensively discussed and experiments for evaluation are performed. From the experimental results based on 50 nodes all over the world, Bicomp achieves significant improvement on scalability than that of Bitcoin and Ethereum, while the security and decentralization merits are still preserved. △ Less","5 September, 2018",https://arxiv.org/pdf/1809.01593
A Bayesian framework for the analog reconstruction of kymographs from fluorescence microscopy data,Denis K. Samuylov;Gábor Székely;Grégory Paul,"Kymographs are widely used to represent and anal- yse spatio-temporal dynamics of fluorescence markers along curvilinear biological compartments. These objects have a sin- gular geometry, thus kymograph reconstruction is inherently an analog image processing task. However, the existing approaches are essentially digital: the kymograph photometry is sampled directly from the time-lapse images. As a result, such kymographs rely on raw image data that suffer from the degradations entailed by the image formation process and the spatio-temporal resolution of the imaging setup. In this work, we address these limitations and introduce a well-grounded Bayesian framework for the analog reconstruction of kymographs. To handle the movement of the object, we introduce an intrinsic description of kymographs using differential geometry: a kymograph is a photometry defined on a parameter space that is embedded in physical space by a time-varying map that follows the object geometry. We model the kymograph photometry as a Lévy innovation process, a flexible class of non-parametric signal priors. We account for the image formation process using the virtual microscope framework. We formulate a computationally tractable representation of the associated maximum a posteriori problem and solve it using a class of efficient and modular algorithms based on the alternating split Bregman. We assess the performance of our Bayesian framework on synthetic data and apply it to reconstruct the fluorescence dynamics along microtubules in vivo in the budding yeast S. cerevisiae. We demonstrate that our framework allows revealing patterns from single time-lapse data that are invisible on standard digital kymographs. △ Less","5 September, 2018",https://arxiv.org/pdf/1809.01590
Copenhagen at CoNLL--SIGMORPHON 2018: Multilingual Inflection in Context with Explicit Morphosyntactic Decoding,Yova Kementchedjhieva;Johannes Bjerva;Isabelle Augenstein,"This paper documents the Team Copenhagen system which placed first in the CoNLL--SIGMORPHON 2018 shared task on universal morphological reinflection, Task 2 with an overall accuracy of 49.87. Task 2 focuses on morphological inflection in context: generating an inflected word form, given the lemma of the word and the context it occurs in. Previous SIGMORPHON shared tasks have focused on context-agnostic inflection---the ""inflection in context"" task was introduced this year. We approach this with an encoder-decoder architecture over character sequences with three core innovations, all contributing to an improvement in performance: (1) a wide context window; (2) a multi-task learning approach with the auxiliary task of MSD prediction; (3) training models in a multilingual fashion. △ Less","5 September, 2018",https://arxiv.org/pdf/1809.01541
Resource and Competence (Internal) View vs. Environment and Market (External) View when defining a Business,Yngve Dahle;Martin Steinert;Anh Nguyen Duc;Roman Chizhevskiy,"Startups is a popular phenomenon that has a significant impact on global economy growth, innovation and society development. However, there is still insufficient understanding about startups, particularly, how to start a new business in the relation to consequent performance. Toward this knowledge, we have performed an empirical study regarding the differences between a Resource and Competence View (Internal) vs Environment and Market View (External) when defining a Business. 701 entrepreneurs have reflected on their startups on nine classes of Resources (values, vision, personal objectives, employees and partners, buildings and rental contracts, cash and credit, patents, IPR's and brands, products and services and finally revenues and grants) and three elements of the Business Mission (""KeyContribution"", ""KeyMarket"" and ""Distinction""). It seems to be a tendency to favour the Internal View over the External View. This tendency is clearer in Stable Economies (Europe) than in Emerging Economies (South Africa). There seems to be a co-variation between the tendency to favour the Internal View and the tendency to focus on adding Resources. Finally, we found that an order-based analysis seems to explain the differences between the two views better than a number-based method. △ Less","16 August, 2018",https://arxiv.org/pdf/1809.01487
Networking Research - A Reflection in the Middle Years,Henning Schulzrinne,"Networking is no longer a new area of computer science and engineering -- it has matured as a discipline and the major infrastructure it supports, the Internet, is long past being primarily a research artifact. I believe that we should consider ourselves as the civil engineers of the Internet, primarily helping to understand and improve a vast and critical infrastructure. This implies that implementing changes takes decades, not conference cycles, and that implementation is largely driven by compatibility with existing infrastructure and considerations of cost effectiveness, where resources that research focuses on, such as bandwidth and compute cycles, often play a much smaller role than limited organizational capacity for change. Telecommunications carriers, in particular, have become akin to airlines, largely operating equipment designed by others, with emphasis on marketing, not innovation. Even more than in other engineering disciplines, standards matter, whether set by standards bodies or dominant players. Given the multi-year time frames of standards and the limited willingness of national funding bodies to support standardization work, this makes research impact harder, as does the increasing complexity of cellular networks and barriers to entry that shut out most researchers from contributing to large parts of commercial mobile networks. △ Less","3 September, 2018",https://arxiv.org/pdf/1809.00623
"TRINITY: Coordinated Performance, Energy and Temperature Management in 3D Processor-Memory Stacks",Karthik Rao;William Song;Yorai Wardi;Sudhakar Yalamanchili,"The consistent demand for better performance has lead to innovations at hardware and microarchitectural levels. 3D stacking of memory and logic dies delivers an order of magnitude improvement in available memory bandwidth. The price paid however is, tight thermal constraints. In this paper, we study the complex multiphysics interactions between performance, energy and temperature. Using a cache coherent multicore processor cycle level simulator coupled with power and thermal estimation tools, we investigate the interactions between (a) thermal behaviors (b) compute and memory microarchitecture and (c) application workloads. The key insights from this exploration reveal the need to manage performance, energy and temperature in a coordinated fashion. Furthermore, we identify the concept of ""effective heat capacity"" i.e. the heat generated beyond which no further gains in performance is observed with increases in voltage-frequency of the compute logic. Subsequently, a real-time, numerical optimization based, application agnostic controller (TRINITY) is developed which intelligently manages the three parameters of interest. We observe up to 30\% improvement in Energy Delay^2 Product and up to 8 Kelvin lower core temperatures as compared to fixed frequencies. Compared to the \texttt{ondemand} Linux CPU DVFS governor, for similar energy efficiency, TRINITY keeps the cores cooler by 6 Kelvin which increases the lifetime reliability by up to 59\%. △ Less","9 September, 2018",https://arxiv.org/pdf/1808.09087
Recalibrating Fully Convolutional Networks with Spatial and Channel 'Squeeze & Excitation' Blocks,Abhijit Guha Roy;Nassir Navab;Christian Wachinger,"In a wide range of semantic segmentation tasks, fully convolutional neural networks (F-CNNs) have been successfully leveraged to achieve state-of-the-art performance. Architectural innovations of F-CNNs have mainly been on improving spatial encoding or network connectivity to aid gradient flow. In this article, we aim towards an alternate direction of recalibrating the learned feature maps adaptively; boosting meaningful features while suppressing weak ones. The recalibration is achieved by simple computational blocks that can be easily integrated in F-CNNs architectures. We draw our inspiration from the recently proposed 'squeeze & excitation' (SE) modules for channel recalibration for image classification. Towards this end, we introduce three variants of SE modules for segmentation, (i) squeezing spatially and exciting channel-wise, (ii) squeezing channel-wise and exciting spatially and (iii) joint spatial and channel 'squeeze & excitation'. We effectively incorporate the proposed SE blocks in three state-of-the-art F-CNNs and demonstrate a consistent improvement of segmentation accuracy on three challenging benchmark datasets. Importantly, SE blocks only lead to a minimal increase in model complexity of about 1.5%, while the Dice score increases by 4-9% in the case of U-Net. Hence, we believe that SE blocks can be an integral part of future F-CNN architectures. △ Less","23 August, 2018",https://arxiv.org/pdf/1808.08127
Defending against Intrusion of Malicious UAVs with Networked UAV Defense Swarms,Matthias R. Brust;Grégoire Danoy;Pascal Bouvry;Dren Gashi;Himadri Pathak;Mike P. Gonçalves,"Nowadays, companies such as Amazon, Alibaba, and even pizza chains are pushing forward to use drones, also called UAVs (Unmanned Aerial Vehicles), for service provision, such as package and food delivery. As governments intend to use these immense economic benefits that UAVs have to offer, urban planners are moving forward to incorporate so-called UAV flight zones and UAV highways in their smart city designs. However, the high-speed mobility and behavior dynamics of UAVs need to be monitored to detect and, subsequently, to deal with intruders, rogue drones, and UAVs with a malicious intent. This paper proposes a UAV defense system for the purpose of intercepting and escorting a malicious UAV outside the flight zone. The proposed UAV defense system consists of a defense UAV swarm, which is capable to self-organize its defense formation in the event of intruder detection, and chase the malicious UAV as a networked swarm. Modular design principles have been used for our fully localized approach. We developed an innovative auto-balanced clustering process to realize the intercept- and capture-formation. As it turned out, the resulting networked defense UAV swarm is resilient against communication losses. Finally, a prototype UAV simulator has been implemented. Through extensive simulations, we show the feasibility and performance of our approach. △ Less","2 September, 2018",https://arxiv.org/pdf/1808.06900
Study of Set-Membership Adaptive Kernel Algorithms,A. Flores;R. C. de Lamare,"In the last decade, a considerable research effort has been devoted to developing adaptive algorithms based on kernel functions. One of the main features of these algorithms is that they form a family of universal approximation techniques, solving problems with nonlinearities elegantly. In this paper, we present data-selective adaptive kernel normalized least-mean square (KNLMS) algorithms that can increase their learning rate and reduce their computational complexity. In fact, these methods deal with kernel expansions, creating a growing structure also known as the dictionary, whose size depends on the number of observations and their innovation. The algorithms described herein use an adaptive step-size to accelerate the learning and can offer an excellent tradeoff between convergence speed and steady state, which allows them to solve nonlinear filtering and estimation problems with a large number of parameters without requiring a large computational cost. The data-selective update scheme also limits the number of operations performed and the size of the dictionary created by the kernel expansion, saving computational resources and dealing with one of the major problems of kernel adaptive algorithms. A statistical analysis is carried out along with a computational complexity analysis of the proposed algorithms. Simulations show that the proposed KNLMS algorithms outperform existing algorithms in examples of nonlinear system identification and prediction of a time series originating from a nonlinear difference equation. △ Less","15 August, 2018",https://arxiv.org/pdf/1808.06536
"Collaborative Pressure Ulcer Prevention: An Automated Skin Damage and Pressure Ulcer Assessment Tool for Nursing Professionals, Patients, Family Members and Carers",Paul Fergus;Carl Chalmers;David Tully,"This paper describes the Pressure Ulcers Online Website, which is a first step solution towards a new and innovative platform for helping people to detect, understand and manage pressure ulcers. It outlines the reasons why the project has been developed and provides a central point of contact for pressure ulcer analysis and ongoing research. Using state-of-the-art technologies in convolutional neural networks and transfer learning along with end-to-end web technologies, this platform allows pressure ulcers to be analysed and findings to be reported. As the system evolves through collaborative partnerships, future versions will provide decision support functions to describe the complex characteristics of pressure ulcers along with information on wound care across multiple user boundaries. This project is therefore intended to raise awareness and support for people suffering with or providing care for pressure ulcers. △ Less","17 August, 2018",https://arxiv.org/pdf/1808.06503
Do software firms collaborate or compete? A model of coopetition in community-initiated OSS projects,Anh Nguyen-Duc;Daniela S. Cruzes;Snarby Terje;Pekka Abrahamsson,"[Background] An increasing number of commercial firms are participating in Open Source Software (OSS) projects to reduce their development cost and increase technical innovativeness. When collaborating with other firms whose sought values are conflicts of interests, firms may behave uncooperatively leading to harmful impacts on the common goal. [Aim] This study explores how software firms both collaborate and compete in OSS projects. [Method] We adopted a mixed research method on three OSS projects. [Result] We found that commercial firms participating in community-initiated OSS projects collaborate in various ways across the organizational boundaries. While most of firms contribute little, a small number of firms that are very active and account for large proportions of contributions. We proposed a conceptual model to explain for coopetition among software firms in OSS projects. The model shows two aspects of coopetition can be managed at the same time based on firm gatekeepers. [Conclusion] Firms need to operationalize their coopetition strategies to maximize value gained from participating in OSS projects. △ Less","16 August, 2018",https://arxiv.org/pdf/1808.06489
Intelligent Middle-Level Game Control,Amin Babadi;Kourosh Naderi;Perttu Hämäläinen,"We propose the concept of intelligent middle-level game control, which lies on a continuum of control abstraction levels between the following two dual opposites: 1) high-level control that translates player's simple commands into complex actions (such as pressing Space key for jumping), and 2) low-level control which simulates real-life complexities by directly manipulating, e.g., joint rotations of the character as it is done in the runner game QWOP. We posit that various novel control abstractions can be explored using recent advances in movement intelligence of game characters. We demonstrate this through design and evaluation of a novel 2-player martial arts game prototype. In this game, each player guides a simulated humanoid character by clicking and dragging body parts. This defines the cost function for an online continuous control algorithm that executes the requested movement. Our control algorithm uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) in a rolling horizon manner with custom population seeding techniques. Our playtesting data indicates that intelligent middle-level control results in producing novel and innovative gameplay without frustrating interface complexities. △ Less","19 August, 2018",https://arxiv.org/pdf/1808.06201
Confidential Encrypted Data Hiding and Retrieval Using QR Authentication System,Somdip Dey;Asoke Nath;Shalabh Agarwal,"Now, security and authenticity of data is a big challenge. To solve this problem, we propose an innovative method to authenticate the digital documents. In this paper, we propose a new method, where the marks obtained by a candidate will also be encoded in QR CodeTM in encrypted form, so that if an intruder tries to change the marks in the mark sheet then he can not do that in the QR CodeTM, because the encryption key is unknown to him. In this method, we encrypt the mark sheet data using the TTJSA encryption algorithm. The encrypted marks are entered inside QR code and that QR code is also printed with the original data of the mark sheet. The marks can then be retrieved from the QR code and can be decrypted using TTJSA decryption algorithm and then it can be verified with marks already there in the mark sheet. △ Less","17 August, 2018",https://arxiv.org/pdf/1808.05827
Jitter-compensated VHT and its application to WSN clock synchronization,Federico Terraneo;Fabiano Riccardi;Alberto Leva,"Accurate and energy-efficient clock synchronization is an enabler for many applications of Wireless Sensor Networks. A fine-grained synchronization is beneficial both at the system level, for example to favor deterministic radio protocols, and at the application level, when network-wide event timestamping is required. However, there is a tradeoff between the resolution of a WSN node's timekeeping device and its energy consumption. The Virtual High-resolution Timer (VHT) is an innovative solution, that was proposed to overcome this tradeoff. It combines a high-resolution oscillator to a low-power one, turning off the former when not needed. In this paper we improve VHT by first identifying the jitter of the low-power oscillator as the current limit to the technique, and then proposing an enhanced solution that synchronizes the fast and the slow clock, rejecting the said jitter. The improved VHT is also less demanding than the original technique in terms of hardware resources. Experimental results show the achieved advantages in terms of accuracy. △ Less","16 August, 2018",https://arxiv.org/pdf/1808.05696
IceBreaker: Solving Cold Start Problem for Video Recommendation Engines,Yaman Kumar;Agniv Sharma;Abhigyan Khaund;Akash Kumar;Ponnurangam Kumaraguru;Rajiv Ratn Shah,"Internet has brought about a tremendous increase in content of all forms and, in that, video content constitutes the major backbone of the total content being published as well as watched. Thus it becomes imperative for video recommendation engines such as Hulu to look for novel and innovative ways to recommend the newly added videos to their users. However, the problem with new videos is that they lack any sort of metadata and user interaction so as to be able to rate the videos for the consumers. To this effect, this paper introduces the several techniques we develop for the Content Based Video Relevance Prediction (CBVRP) Challenge being hosted by Hulu for the ACM Multimedia Conference 2018. We employ different architectures on the CBVRP dataset to make use of the provided frame and video level features and generate predictions of videos that are similar to the other videos. We also implement several ensemble strategies to explore complementarity between both the types of provided features. The obtained results are encouraging and will impel the boundaries of research for multimedia based video recommendation systems. △ Less","16 August, 2018",https://arxiv.org/pdf/1808.05636
Statistical Analysis Driven Optimized Deep Learning System for Intrusion Detection,Cosimo Ieracitano;Ahsan Adeel;Mandar Gogate;Kia Dashtipour;Francesco Carlo Morabito;Hadi Larijani;Ali Raza;Amir Hussain,"Attackers have developed ever more sophisticated and intelligent ways to hack information and communication technology systems. The extent of damage an individual hacker can carry out upon infiltrating a system is well understood. A potentially catastrophic scenario can be envisaged where a nation-state intercepting encrypted financial data gets hacked. Thus, intelligent cybersecurity systems have become inevitably important for improved protection against malicious threats. However, as malware attacks continue to dramatically increase in volume and complexity, it has become ever more challenging for traditional analytic tools to detect and mitigate threat. Furthermore, a huge amount of data produced by large networks has made the recognition task even more complicated and challenging. In this work, we propose an innovative statistical analysis driven optimized deep learning system for intrusion detection. The proposed intrusion detection system (IDS) extracts optimized and more correlated features using big data visualization and statistical analysis methods (human-in-the-loop), followed by a deep autoencoder for potential threat detection. Specifically, a pre-processing module eliminates the outliers and converts categorical variables into one-hot-encoded vectors. The feature extraction module discard features with null values and selects the most significant features as input to the deep autoencoder model (trained in a greedy-wise manner). The NSL-KDD dataset from the Canadian Institute for Cybersecurity is used as a benchmark to evaluate the feasibility and effectiveness of the proposed architecture. Simulation results demonstrate the potential of our proposed system and its outperformance as compared to existing state-of-the-art methods and recently published novel approaches. Ongoing work includes further optimization and real-time evaluation of our proposed IDS. △ Less","16 August, 2018",https://arxiv.org/pdf/1808.05633
NetScatter: Enabling Large-Scale Backscatter Networks,Mehrdad Hessar;Ali Najafi;Shyamnath Gollakota,"We present the first wireless protocol that scales to hundreds of concurrent transmissions from backscatter devices. Our key innovation is a distributed coding mechanism that works below the noise floor, operates on backscatter devices and can decode all the concurrent transmissions at the receiver using a single FFT operation. Our design addresses practical issues such as timing and frequency synchronization as well as the near-far problem. We deploy our design using a testbed of backscatter hardware and show that our protocol scales to concurrent transmissions from 256 devices using a bandwidth of only 500 kHz. Our results show throughput and latency improvements of 14--62x and 15--67x over existing approaches and 1--2 orders of magnitude higher transmission concurrency. △ Less","15 August, 2018",https://arxiv.org/pdf/1808.05195
Hashing with Linear Probing and Referential Integrity,Peter Sanders,"We describe a variant of linear probing hash tables that never moves elements and thus supports referential integrity, i.e., pointers to elements remain valid while this element is in the hash table. This is achieved by the folklore method of marking some table entries as formerly occupied (tombstones). The innovation is that the number of tombstones is minimized. Experiments indicate that this allows an unbounded number of operations with bounded overhead compared to linear probing without tombstones (and without referential integrity). △ Less","14 August, 2018",https://arxiv.org/pdf/1808.04602
Fast Convergence for Object Detection by Learning how to Combine Error Functions,Benjamin Schnieders;Karl Tuyls,"In this paper, we introduce an innovative method to improve the convergence speed and accuracy of object detection neural networks. Our approach, CONVERGE-FAST-AUXNET, is based on employing multiple, dependent loss metrics and weighting them optimally using an on-line trained auxiliary network. Experiments are performed in the well-known RoboCup@Work challenge environment. A fully convolutional segmentation network is trained on detecting objects' pickup points. We empirically obtain an approximate measure for the rate of success of a robotic pickup operation based on the accuracy of the object detection network. Our experiments show that adding an optimally weighted Euclidean distance loss to a network trained on the commonly used Intersection over Union (IoU) metric reduces the convergence time by 42.48%. The estimated pickup rate is improved by 39.90%. Compared to state-of-the-art task weighting methods, the improvement is 24.5% in convergence, and 15.8% on the estimated pickup rate. △ Less","13 August, 2018",https://arxiv.org/pdf/1808.04480
Technology utilization patterns and business growth in Small/Medium Enterprises,Chaitanya Dhareshwar,"Technology was created to support and grow Business. Modern business that uses technology efficiently, grows at a phenomenal rate (Statista.com, 2018). The assumption therefore is that businesses that utilize insufficient technology, or use technology inefficiently, experience reduced growth and possibly, business decline. Technological development holds great significance in most industries particularly in wastage reduction, process optimization and consequently bottom-line revenue enhancement and price-leadership. We've seen revolutionary technological development during the 20th century / 21st century thus far, (Ivanovic et al, 2015) and it's led to drastic growth in fields like communication, computer science, monitoring of operations, remote working, high performance analytics and many more. Some fields have even come into existence purely due to technology. Technological equipment cannot compensate for the skills, knowledge or creativity of human employees. However, expertise of the average employee can be greatly enhanced using intelligent software. Use of such equipment decreases the need for unskilled and semi-skilled workers - but can exponentially increase speed of performance for skilled workers. Innovations are key defining criteria for competitive differentiation - but some of these can be easily copied, which basically means that innovation and improvement are continuous processes. Process standardization comes through in a big way when technological solutions are applied in the work. It regulates/optimizes the number of employees needed, power consumption, potentially reduces wastage, drastically improves hygiene process (where relevant). The natural outcome is greater process efficiency and cost efficiency. Keywords: technology, innovation, process efficiency, standardization of process, waste reduction, continued improvement, business ROI. △ Less","12 August, 2018",https://arxiv.org/pdf/1808.03956
Round-Table Group Optimization for Sequencing Problems,Xiao-Feng Xie,"In this paper, a round-table group optimization (RTGO) algorithm is presented. RTGO is a simple metaheuristic framework using the insights of research on group creativity. In a cooperative group, the agents work in iterative sessions to search innovative ideas in a common problem landscape. Each agent has one base idea stored in its individual memory, and one social idea fed by a round-table group support mechanism in each session. The idea combination and improvement processes are respectively realized by using a recombination search (XS) strategy and a local search (LS) strategy, to build on the base and social ideas. RTGO is then implemented for solving two difficult sequencing problems, i.e., the flowshop scheduling problem and the quadratic assignment problem. The domain-specific LS strategies are adopted from existing algorithms, whereas a general XS class, called socially biased combination (SBX), is realized in a modular form. The performance of RTGO is then evaluated on commonly-used benchmark datasets. Good performance on different problems can be achieved by RTGO using appropriate SBX operators. Furthermore, RTGO is able to outperform some existing methods, including methods using the same LS strategies. △ Less","6 August, 2018",https://arxiv.org/pdf/1808.02185
The Internals of the Data Calculator,Stratos Idreos;Kostas Zoumpatianos;Brian Hentschel;Michael S. Kester;Demi Guo,"Data structures are critical in any data-driven scenario, but they are notoriously hard to design due to a massive design space and the dependence of performance on workload and hardware which evolve continuously. We present a design engine, the Data Calculator, which enables interactive and semi-automated design of data structures. It brings two innovations. First, it offers a set of fine-grained design primitives that capture the first principles of data layout design: how data structure nodes lay data out, and how they are positioned relative to each other. This allows for a structured description of the universe of possible data structure designs that can be synthesized as combinations of those primitives. The second innovation is computation of performance using learned cost models. These models are trained on diverse hardware and data profiles and capture the cost properties of fundamental data access primitives (e.g., random access). With these models, we synthesize the performance cost of complex operations on arbitrary data structure designs without having to: 1) implement the data structure, 2) run the workload, or even 3) access the target hardware. We demonstrate that the Data Calculator can assist data structure designers and researchers by accurately answering rich what-if design questions on the order of a few seconds or minutes, i.e., computing how the performance (response time) of a given data structure design is impacted by variations in the: 1) design, 2) hardware, 3) data, and 4) query workloads. This makes it effortless to test numerous designs and ideas before embarking on lengthy implementation, deployment, and hardware acquisition steps. We also demonstrate that the Data Calculator can synthesize entirely new designs, auto-complete partial designs, and detect suboptimal design choices. △ Less","6 August, 2018",https://arxiv.org/pdf/1808.02066
Technological conditions of mobile learning in high school,Natalya Rashevs`ka;Viktoriia Tkachuk,"This paper reviews the history of mobile learning, provides a definition of mobile learning. The properties, advantages and disadvantages of mobile learning, areas of its implementation at the Technical University and mobile learning tools were specified. The aim of the article is the analysis of the modern state of mobile learning and the determination of the conditions of its implementation at the high technical educational institutions. The process of the mobile learning in the national education system is in its formation stage. Nowadays the following stages of its development are formed, which are based on the availability of the technical means for the mobile learning and the mobile access implementation to educational resources. The mobile learning is the logical and innovation process in the education system, which is defined as a learning technology which uses the mobile devices, communication technology and intelligent user interfaces. △ Less","31 July, 2018",https://arxiv.org/pdf/1808.01989
Super Resolution Phase Retrieval for Sparse Signals,Gilles Baechler;Miranda Kreković;Juri Ranieri;Amina Chebira;Yue M. Lu;Martin Vetterli,"In a variety of fields, in particular those involving imaging and optics, we often measure signals whose phase is missing or has been irremediably distorted. Phase retrieval attempts to recover the phase information of a signal from the magnitude of its Fourier transform to enable the reconstruction of the original signal. Solving the phase retrieval problem is equivalent to recovering a signal from its auto-correlation function. In this paper, we assume the original signal to be sparse; this is a natural assumption in many applications, such as X-ray crystallography, speckle imaging and blind channel estimation. We propose an algorithm that resolves the phase retrieval problem in three stages: i) we leverage the finite rate of innovation sampling theory to super-resolve the auto-correlation function from a limited number of samples, ii) we design a greedy algorithm that identifies the locations of a sparse solution given the super-resolved auto-correlation function, iii) we recover the amplitudes of the atoms given their locations and the measured auto-correlation function. Unlike traditional approaches that recover a discrete approximation of the underlying signal, our algorithm estimates the signal on a continuous domain, which makes it the first of its kind. Along with the algorithm, we derive its performance bound with a theoretical analysis and propose a set of enhancements to improve its computational complexity and noise resilience. Finally, we demonstrate the benefits of the proposed method via a comparison against Charge Flipping, a notable algorithm in crystallography. △ Less","6 August, 2018",https://arxiv.org/pdf/1808.01961
Using Machine Learning Safely in Automotive Software: An Assessment and Adaption of Software Process Requirements in ISO 26262,Rick Salay;Krzysztof Czarnecki,"The use of machine learning (ML) is on the rise in many sectors of software development, and automotive software development is no different. In particular, Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS) are two areas where ML plays a significant role. In automotive development, safety is a critical objective, and the emergence of standards such as ISO 26262 has helped focus industry practices to address safety in a systematic and consistent way. Unfortunately, these standards were not designed to accommodate technologies such as ML or the type of functionality that is provided by an ADS and this has created a conflict between the need to innovate and the need to improve safety. In this report, we take steps to address this conflict by doing a detailed assessment and adaption of ISO 26262 for ML, specifically in the context of supervised learning. First we analyze the key factors that are the source of the conflict. Then we assess each software development process requirement (Part 6 of ISO 26262) for applicability to ML. Where there are gaps, we propose new requirements to address the gaps. Finally we discuss the application of this adapted and extended variant of Part 6 to ML development scenarios. △ Less","5 August, 2018",https://arxiv.org/pdf/1808.01614
Video Re-localization,Yang Feng;Lin Ma;Wei Liu;Tong Zhang;Jiebo Luo,"Many methods have been developed to help people find the video contents they want efficiently. However, there are still some unsolved problems in this area. For example, given a query video and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video? We define a distinctively new task, namely \textbf{video re-localization}, to address this scenario. Video re-localization is an important emerging technology implicating many applications, such as fast seeking in videos, video copy detection, video surveillance, etc. Meanwhile, it is also a challenging research task because the visual appearance of a semantic concept in videos can have large variations. The first hurdle to clear for the video re-localization task is the lack of existing datasets. It is labor expensive to collect pairs of videos with semantic coherence or correspondence and label the corresponding segments. We first exploit and reorganize the videos in ActivityNet to form a new dataset for video re-localization research, which consists of about 10,000 videos of diverse visual appearances associated with localized boundary information. Subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference video is matched against the attentively weighted query video. Consequently, the prediction of the starting and ending time is formulated as a classification problem based on the matching results. Extensive experimental results show that the proposed method outperforms the competing methods. Our code is available at: https://github.com/fengyang0317/video_reloc. △ Less","5 August, 2018",https://arxiv.org/pdf/1808.01575
FADE: Fast and Asymptotically efficient Distributed Estimator for dynamic networks,António Simões;João Xavier,"Consider a set of agents that wish to estimate a vector of parameters of their mutual interest. For this estimation goal, agents can sense and communicate. When sensing, an agent measures (in additive gaussian noise) linear combinations of the unknown vector of parameters. When communicating, an agent can broadcast information to a few other agents, by using the channels that happen to be randomly at its disposal at the time. To coordinate the agents towards their estimation goal, we propose a novel algorithm called FADE (Fast and Asymptotically efficient Distributed Estimator), in which agents collaborate at discrete time-steps; at each time-step, agents sense and communicate just once, while also updating their own estimate of the unknown vector of parameters. FADE enjoys five attractive features: first, it is an intuitive estimator, simple to derive; second, it withstands dynamic networks, that is, networks whose communication channels change randomly over time; third, it is strongly consistent in that, as time-steps play out, each agent's local estimate converges (almost surely) to the true vector of parameters; fourth, it is both asymptotically unbiased and efficient, which means that, across time, each agent's estimate becomes unbiased and the mean-square error (MSE) of each agent's estimate vanishes to zero at the same rate of the MSE of the optimal estimator at an almighty central node; fifth, and most importantly, when compared with a state-of-art consensus+innovation (CI) algorithm, it yields estimates with outstandingly lower mean-square errors, for the same number of communications -- for example, in a sparsely connected network model with 50 agents, we find through numerical simulations that the reduction can be dramatic, reaching several orders of magnitude. △ Less","31 July, 2018",https://arxiv.org/pdf/1807.11878
Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages,Yuxi Li;Jiuwei Li;Weiyao Lin;Jianguo Li,"Object detection has made great progress in the past few years along with the development of deep learning. However, most current object detection methods are resource hungry, which hinders their wide deployment to many resource restricted usages such as usages on always-on devices, battery-powered low-end devices, etc. This paper considers the resource and accuracy trade-off for resource-restricted usages during designing the whole object detection framework. Based on the deeply supervised object detection (DSOD) framework, we propose Tiny-DSOD dedicating to resource-restricted usages. Tiny-DSOD introduces two innovative and ultra-efficient architecture blocks: depthwise dense block (DDB) based backbone and depthwise feature-pyramid-network (D-FPN) based front-end. We conduct extensive experiments on three famous benchmarks (PASCAL VOC 2007, KITTI, and COCO), and compare Tiny-DSOD to the state-of-the-art ultra-efficient object detection solutions such as Tiny-YOLO, MobileNet-SSD (v1 & v2), SqueezeDet, Pelee, etc. Results show that Tiny-DSOD outperforms these solutions in all the three metrics (parameter-size, FLOPs, accuracy) in each comparison. For instance, Tiny-DSOD achieves 72.1% mAP with only 0.95M parameters and 1.06B FLOPs, which is by far the state-of-the-arts result with such a low resource requirement. △ Less","29 July, 2018",https://arxiv.org/pdf/1807.11013
The mobile information and educational environment of higher educational institution,N. Moiseienko;M. Moiseienko;S. Semerikov,"In the modern world in the conditions of informatization of society and high level of competition at the labor-market the problem of preparation of specialists appears to the use of modern information and of communication technologies. Modern higher educational establishment must become the core of innovative education with problem preparation of specialists of new generation. Especially it touches preparation professionally of competent teachers, capable easily to adapt oneself in a modern educational environment, be competitive in the conditions of modern labor-market. Purpose. To highlight the definition of mobile information and educational environment of higher educational institution. Results. Define the concept of mobile information and educational environment of a higher educational institution aiming to meet the educational and research needs of all users in providing the necessary e-resources anytime and anywhere. Conclusion. Mobile information and educational environment of a higher educational institution ensures the realization of a few preferences: effective using modern technical learning tools; attracting the best educators; implementation and supporting author's courses; ensuring purposeful development of students. In such environment every student have free access (independent from time and place) to any materials from the academic disciplines, while gaining for them the necessary practical skills, useful implements interaction, knowledge sharing, organizes continuous learning process. The creation and support of mobile information and educational environment of higher educational institution will bring the University activities to a qualitatively new level and enhance its competitiveness in modern conditions. △ Less","23 July, 2018",https://arxiv.org/pdf/1807.10656
Assurances in Software Testing: A Roadmap,Marcel Böhme,"As researchers, we already understand how to make testing more effective and efficient at finding bugs. However, as fuzzing (i.e., automated testing) becomes more widely adopted in practice, practitioners are asking: Which assurances does a fuzzing campaign provide that exposes no bugs? When is it safe to stop the fuzzer with a reasonable residual risk? How much longer should the fuzzer be run to achieve sufficient coverage? It is time for us to move beyond the innovation of increasingly sophisticated testing techniques, to build a body of knowledge around the explication and quantification of the testing process, and to develop sound methodologies to estimate and extrapolate these quantities with measurable accuracy. In our vision of the future practitioners leverage a rich statistical toolset to assess residual risk, to obtain statistical guarantees, and to analyze the cost-benefit trade-off for ongoing fuzzing campaigns. We propose a general framework as a first starting point to tackle this fundamental challenge and discuss a large number of concrete opportunities for future research. △ Less","17 December, 2018",https://arxiv.org/pdf/1807.10255
Data-Oriented Algorithm for Real-Time Estimation of Flow Rates and Flow Directions in a Water Distribution Network,Christophe Dumora;David Auber;Jérémie Bigot;Vincent Couallier;Cyril Leclerc,"The aim of this paper is to present how data collected from a water distribution network (WDN) can be used to reconstruct flow rate and flow direction all over the network to enhance knowledge and detection of unforeseen events. The methodological approach consists in modeling the WDN and all available sensor data related to the management of such a network in the form of a flow network graph G = (V, E, s, t, c), with V a set of nodes, E a set of edges whose elements are ordered pairs of distinct nodes, s a source node, t a sink node and c a capacity function on edges. Our objective is to reconstruct a real-valued function f(u,v): VxV => R on all the edges E in VxV from partial observations on a small number of nodes V = {1, ..., n}. This reconstruction method consists in a data-driven Ford-Fulkerson maximum-flow problem in a multi-source, multi-sink context using a constrained bidirectional breadth-first search based on Edmonds-Karp method. The innovative approach is its application in the context of smart cities to operate from sensor data, structural data from a geographical information system (GIS) and consumption estimates. △ Less","25 July, 2018",https://arxiv.org/pdf/1807.10147
The Hybrid Service Model of Electronic Resources Access in the Cloud-Based Learning Environment,Mariya Shyshkina,"Nowadays, the search for innovative technological solutions to the organization of access to electronic learning resources in the university and their configuration within the environment to fit the needs of users and to improve learning outcomes has become key issues. These solutions are based on the emerging tools among which cloud computing and ICT outsourcing have become very promising and important trends in research. The problems of providing access to electronic learning resources on the basis of cloud computing are the focus of the article. The article outlines the conceptual framework of the study by reviewing existing approaches and models for the cloud-based learning environment's architecture and design, including its advantages and disadvantages, and the features of its pedagogical application and the experience of it. The hybrid service model of access to learning resources within the university environment is described and proved. An empirical estimation of the proposed approach and current developments in its implementation are provided. △ Less","23 July, 2018",https://arxiv.org/pdf/1807.09264
Weak in the NEES?: Auto-tuning Kalman Filters with Bayesian Optimization,Zhaozhong Chen;Christoffer Heckman;Simon Julier;Nisar Ahmed,"Kalman filters are routinely used for many data fusion applications including navigation, tracking, and simultaneous localization and mapping problems. However, significant time and effort is frequently required to tune various Kalman filter model parameters, e.g. process noise covariance, pre-whitening filter models for non-white noise, etc. Conventional optimization techniques for tuning can get stuck in poor local minima and can be expensive to implement with real sensor data. To address these issues, a new ""black box"" Bayesian optimization strategy is developed for automatically tuning Kalman filters. In this approach, performance is characterized by one of two stochastic objective functions: normalized estimation error squared (NEES) when ground truth state models are available, or the normalized innovation error squared (NIS) when only sensor data is available. By intelligently sampling the parameter space to both learn and exploit a nonparametric Gaussian process surrogate function for the NEES/NIS costs, Bayesian optimization can efficiently identify multiple local minima and provide uncertainty quantification on its results. △ Less","23 July, 2018",https://arxiv.org/pdf/1807.08855
The Problems of Personnel Training for STEM Education in the Modern Innovative Learning and Research Environment,Mariya Shyshkina,"The aim of the article is to describe the problems of personnel training that arise in view of extension of the STEM approach to education, development of innovative technologies, in particular, virtualization, augmented reality, the use of ICT outsourcing in educational systems design. The object of research is the process of formation and development of the educational and scientific envi- ronment of educational institution. The subject of the study is the formation and development of the cloud-based learning and research environment for STEM education. The methods of research are: the analysis of publications on the prob- lem, generalization of domestic and foreign experience, theoretical analysis, sys- tem analysis, systematization and generalization of research facts and laws for the development and design of the model of the cloud-based learning environ- ment, substantiation of the main conclusions. The results of the research are the next: the concepts and the model of the cloud-based environment of STEM edu- cation is substantiated, the problems of personnel training at the present stage are outlined. △ Less","23 July, 2018",https://arxiv.org/pdf/1807.08562
Large scale evaluation of local image feature detectors on homography datasets,Karel Lenc;Andrea Vedaldi,"We present a large scale benchmark for the evaluation of local feature detectors. Our key innovation is the introduction of a new evaluation protocol which extends and improves the standard detection repeatability measure. The new protocol is better for assessment on a large number of images and reduces the dependency of the results on unwanted distractors such as the number of detected features and the feature magnification factor. Additionally, our protocol provides a comprehensive assessment of the expected performance of detectors under several practical scenarios. Using images from the recently-introduced HPatches dataset, we evaluate a range of state-of-the-art local feature detectors on two main tasks: viewpoint and illumination invariant detection. Contrary to previous detector evaluations, our study contains an order of magnitude more image sequences, resulting in a quantitative evaluation significantly more robust to over-fitting. We also show that traditional detectors are still very competitive when compared to recent deep-learning alternatives. △ Less","20 July, 2018",https://arxiv.org/pdf/1807.07939
About BIRDS project (Bioinformatics and Information Retrieval Data Structures Analysis and Design),Guillermo de Bernardo;Susana Ladra,"BIRDS stands for ""Bioinformatics and Information Retrieval Data Structures analysis and design"" and is a 4-year project (2016--2019) that has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 690941. The overall goal of BIRDS is to establish a long term international network involving leading researchers in the development of efficient data structures in the fields of Bioinformatics and Information Retrieval, to strengthen the partnership through the exchange of knowledge and expertise, and to develop integrated approaches to improve current approaches in both fields. The research will address challenges in storing, processing, indexing, searching and navigating genome-scale data by designing new algorithms and data structures for sequence analysis, networks representation or compressing and indexing repetitive data. BIRDS project is carried out by 7 research institutions from Australia (University of Melbourne), Chile (University of Chile and University of Concepción), Finland (University of Helsinki), Japan (Kyushu University), Portugal (Instituto de Engenharia de Sistemas e Computadores, Investigação e Desenvolvimento em Lisboa, INESC-ID), and Spain (University of A Coruña), and a Spanish SME (Enxenio S.L.). It is coordinated by the University of A Coruña (Spain). △ Less","19 July, 2018",https://arxiv.org/pdf/1807.07937
Complex Economic Activities Concentrate in Large Cities,Pierre-Alexandre Balland;Cristian Jara-Figueroa;Sergio Petralia;Mathieu Steijn;David Rigby;Cesar A. Hidalgo,"Why do some economic activities agglomerate more than others? And, why does the agglomeration of some economic activities continue to increase despite recent developments in communication and transportation technologies? In this paper, we present evidence that complex economic activities concentrate more in large cities. We find this to be true for technologies, scientific publications, industries, and occupations. Using historical patent data, we show that the urban concentration of complex economic activities has been continuously increasing since 1850. These findings suggest that the increasing urban concentration of jobs and innovation might be a consequence of the growing complexity of the economy. △ Less","20 July, 2018",https://arxiv.org/pdf/1807.07887
Conversation-Based Complex Event Management in Smart-Spaces,André Sousa Lago;Hugo Sereno Ferreira,"Smart space management can be done in many ways. On one hand, there are conversational assistants such as the Google Assistant or Amazon Alexa that enable users to comfortably interact with smart spaces with only their voice, but these have limited functionality and are usually limited to simple commands. On the other hand, there are visual interfaces such as IBM's Node-RED that enable complex features and dependencies between different devices. However, these are limited since they require users to have a technical knowledge of how the smart devices work and the system's interface is more complicated and harder to use since they require a computer. This project proposes a new conversational assistant - Jarvis - that combines the ease of use of current assistants with the operational complexity of the visual platforms. The goal of Jarvis is to make it easier to manage smart spaces by providing intuitive commands and useful features. Jarvis integrates with already existing user interfaces such as the Google Assistant, Slack or Facebook Messenger, making it very easy to integrate with existing systems. Jarvis also provides an innovative feature - causality queries - that enable users to ask it why something happened. For example, a user can ask ""why did the light turn on?"" to understand how the system works. △ Less","18 July, 2018",https://arxiv.org/pdf/1807.07047
Using semantic clustering to support situation awareness on Twitter: The case of World Views,Charlie Kingston;Jason R. C. Nurse;Ioannis Agrafiotis;Andrew Milich,"In recent years, situation awareness has been recognised as a critical part of effective decision making, in particular for crisis management. One way to extract value and allow for better situation awareness is to develop a system capable of analysing a dataset of multiple posts, and clustering consistent posts into different views or stories (or, world views). However, this can be challenging as it requires an understanding of the data, including determining what is consistent data, and what data corroborates other data. Attempting to address these problems, this article proposes Subject-Verb-Object Semantic Suffix Tree Clustering (SVOSSTC) and a system to support it, with a special focus on Twitter content. The novelty and value of SVOSSTC is its emphasis on utilising the Subject-Verb-Object (SVO) typology in order to construct semantically consistent world views, in which individuals---particularly those involved in crisis response---might achieve an enhanced picture of a situation from social media data. To evaluate our system and its ability to provide enhanced situation awareness, we tested it against existing approaches, including human data analysis, using a variety of real-world scenarios. The results indicated a noteworthy degree of evidence (e.g., in cluster granularity and meaningfulness) to affirm the suitability and rigour of our approach. Moreover, these results highlight this article's proposals as innovative and practical system contributions to the research field. △ Less","17 July, 2018",https://arxiv.org/pdf/1807.06588
Enhancing Middleware-based IoT Applications through Run-Time Pluggable QoS Management Mechanisms. Application to a oneM2M compliant IoT Middleware,Clovis Anicet Ouedraogo;Samir Medjiah;Christophe Chassot;Khalil Drira,"In the recent years, telecom and computer networks have witnessed new concepts and technologies through Network Function Virtualization (NFV) and Software-Defined Networking (SDN). SDN, which allows applications to have a control over the network, and NFV, which allows deploying network functions in virtualized environments, are two paradigms that are increasingly used for the Internet of Things (IoT). This Internet (IoT) brings the promise to interconnect billions of devices in the next few years rises several scientific challenges in particular those of the satisfaction of the quality of service (QoS) required by the IoT applications. In order to address this problem, we have identified two bottlenecks with respect to the QoS: the traversed networks and the intermediate entities that allows the application to interact with the IoT devices. In this paper, we first present an innovative vision of a ""network function"" with respect to their deployment and runtime environment. Then, we describe our general approach of a solution that consists in the dynamic, autonomous, and seamless deployment of QoS management mechanisms. We also describe the requirements for the implementation of such approach. Finally, we present a redirection mechanism, implemented as a network function, allowing the seamless control of the data path of a given middleware traffic. This mechanism is assessed through a use case related to vehicular transportation. △ Less","16 July, 2018",https://arxiv.org/pdf/1807.05729
Envision of an integrated information system for project-driven production in construction,Ricardo Antunes;Mani Poshdar,"Construction frequently appears at the bottom of productivity charts with decreasing indexes of productivity over the years. Lack of innovation and delayed adoption, informal processes or insufficient rigor and consistency in process execution, insufficient knowledge transfer from project to project, weak project monitoring, little cross- functional cooperation, little collaboration with suppliers, conservative company culture, and a shortage of young talent and people development are usual issues. Whereas work has been carried out on information technology and automation in construction their application is isolated without an interconnected information flow. This paper suggests a framework to address production issues on construction by implementing an integrated automatic supervisory control and data acquisition for management and operations. The system is divided into planning, monitoring, controlling, and executing groups clustering technologies to track both the project product and production. This research stands on the four pillars of manufacturing knowledge and lean production (production processes, production management, equipment/tool design, and automated systems and control). The framework offers benefits such as increased information flow, detection and prevention of overburdening equipment or labor (Muri) and production unevenness (Mura), reduction of waste (Muda), evidential and continuous process standardization and improvement, reuse and abstraction of project information across endeavors. △ Less","13 July, 2018",https://arxiv.org/pdf/1807.04966
Deterministic Policy Gradients With General State Transitions,Qingpeng Cai;Ling Pan;Pingzhong Tang,"We study a reinforcement learning setting, where the state transition function is a convex combination of a stochastic continuous function and a deterministic function. Such a setting generalizes the widely-studied stochastic state transition setting, namely the setting of deterministic policy gradient (DPG). We firstly give a simple example to illustrate that the deterministic policy gradient may be infinite under deterministic state transitions, and introduce a theoretical technique to prove the existence of the policy gradient in this generalized setting. Using this technique, we prove that the deterministic policy gradient indeed exists for a certain set of discount factors, and further prove two conditions that guarantee the existence for all discount factors. We then derive a closed form of the policy gradient whenever exists. Furthermore, to overcome the challenge of high sample complexity of DPG in this setting, we propose the Generalized Deterministic Policy Gradient (GDPG) algorithm. The main innovation of the algorithm is a new method of applying model-based techniques to the model-free algorithm, the deep deterministic policy gradient algorithm (DDPG). GDPG optimize the long-term rewards of the model-based augmented MDP subject to a constraint that the long-rewards of the MDP is less than the original one. We finally conduct extensive experiments comparing GDPG with state-of-the-art methods and the direct model-based extension method of DDPG on several standard continuous control benchmarks. Results demonstrate that GDPG substantially outperforms DDPG, the model-based extension of DDPG and other baselines in terms of both convergence and long-term rewards in most environments. △ Less","1 October, 2018",https://arxiv.org/pdf/1807.03708
The SAGE Project: a Storage Centric Approach for Exascale Computing,Sai Narasimhamurthy;Nikita Danilov;Sining Wu;Ganesan Umanesan;Steven Wei-der Chien;Sergio Rivas-Gomez;Ivy Bo Peng;Erwin Laure;Shaun de Witt;Dirk Pleiter;Stefano Markidis,"SAGE (Percipient StorAGe for Exascale Data Centric Computing) is a European Commission funded project towards the era of Exascale computing. Its goal is to design and implement a Big Data/Extreme Computing (BDEC) capable infrastructure with associated software stack. The SAGE system follows a ""storage centric"" approach as it is capable of storing and processing large data volumes at the Exascale regime. SAGE addresses the convergence of Big Data Analysis and HPC in an era of next-generation data centric computing. This convergence is driven by the proliferation of massive data sources, such as large, dispersed scientific instruments and sensors where data needs to be processed, analyzed and integrated into simulations to derive scientific and innovative insights. A first prototype of the SAGE system has been been implemented and installed at the Julich Supercomputing Center. The SAGE storage system consists of multiple types of storage device technologies in a multi-tier I/O hierarchy, including flash, disk, and non-volatile memory technologies. The main SAGE software component is the Seagate Mero Object Storage that is accessible via the Clovis API and higher level interfaces. The SAGE project also includes scientific applications for the validation of the SAGE concepts. The objective of this paper is to present the SAGE project concepts, the prototype of the SAGE platform and discuss the software architecture of the SAGE system. △ Less","6 July, 2018",https://arxiv.org/pdf/1807.03632
Verisimilar Image Synthesis for Accurate Detection and Recognition of Texts in Scenes,Fangneng Zhan;Shijian Lu;Chuhui Xue,"The requirement of large amounts of annotated images has become one grand challenge while training deep neural network models for various visual detection and recognition tasks. This paper presents a novel image synthesis technique that aims to generate a large amount of annotated scene text images for training accurate and robust scene text detection and recognition models. The proposed technique consists of three innovative designs. First, it realizes ""semantic coherent"" synthesis by embedding texts at semantically sensible regions within the background image, where the semantic coherence is achieved by leveraging the semantic annotations of objects and image regions that have been created in the prior semantic segmentation research. Second, it exploits visual saliency to determine the embedding locations within each semantic sensible region, which coincides with the fact that texts are often placed around homogeneous regions for better visibility in scenes. Third, it designs an adaptive text appearance model that determines the color and brightness of embedded texts by learning from the feature of real scene text images adaptively. The proposed technique has been evaluated over five public datasets and the experiments show its superior performance in training accurate and robust scene text detection and recognition models. △ Less","26 September, 2018",https://arxiv.org/pdf/1807.03021
Spatio-temporal variations in the urban rhythm: the travelling waves of crime,Marcos Oliveira;Eraldo Ribeiro;Carmelo Bastos-Filho;Ronaldo Menezes,"In the last decades, the notion that cities are in a state of equilibrium with a centralised organisation has given place to the viewpoint of cities in disequilibrium and organised from bottom to up. In this perspective, cities are evolving systems that exhibit emergent phenomena built from local decisions. While urban evolution promotes the emergence of positive social phenomena such as the formation of innovation hubs and the increase in cultural diversity, it also yields negative phenomena such as increases in criminal activity. Yet, we are still far from understanding the driving mechanisms of these phenomena. In particular, approaches to analyse urban phenomena are limited in scope by neglecting both temporal non-stationarity and spatial heterogeneity. In the case of criminal activity, we know for more than one century that crime peaks during specific times of the year, but the literature still fails to characterise the mobility of crime. Here we develop an approach to describe the spatial, temporal, and periodic variations in urban quantities. With crime data from 12 cities, we characterise how the periodicity of crime varies spatially across the city over time. We confirm one-year criminal cycles and show that this periodicity occurs unevenly across the city. These `waves of crime' keep travelling across the city: while cities have a stable number of regions with a circannual period, the regions exhibit non-stationary series. Our findings support the concept of cities in a constant change, influencing urban phenomena---in agreement with the notion of cities not in equilibrium. △ Less","2 November, 2018",https://arxiv.org/pdf/1807.02989
"Automated and Interpretable Patient ECG Profiles for Disease Detection, Tracking, and Discovery",Geoffrey H. Tison;Jeffrey Zhang;Francesca N. Delling;Rahul C. Deo,"The electrocardiogram or ECG has been in use for over 100 years and remains the most widely performed diagnostic test to characterize cardiac structure and electrical activity. We hypothesized that parallel advances in computing power, innovations in machine learning algorithms, and availability of large-scale digitized ECG data would enable extending the utility of the ECG beyond its current limitations, while at the same time preserving interpretability, which is fundamental to medical decision-making. We identified 36,186 ECGs from the UCSF database that were 1) in normal sinus rhythm and 2) would enable training of specific models for estimation of cardiac structure or function or detection of disease. We derived a novel model for ECG segmentation using convolutional neural networks (CNN) and Hidden Markov Models (HMM) and evaluated its output by comparing electrical interval estimates to 141,864 measurements from the clinical workflow. We built a 725-element patient-level ECG profile using downsampled segmentation data and trained machine learning models to estimate left ventricular mass, left atrial volume, mitral annulus e' and to detect and track four diseases: pulmonary arterial hypertension (PAH), hypertrophic cardiomyopathy (HCM), cardiac amyloid (CA), and mitral valve prolapse (MVP). CNN-HMM derived ECG segmentation agreed with clinical estimates, with median absolute deviations (MAD) as a fraction of observed value of 0.6% for heart rate and 4% for QT interval. Patient-level ECG profiles enabled quantitative estimates of left ventricular and mitral annulus e' velocity with good discrimination in binary classification models of left ventricular hypertrophy and diastolic function. Models for disease detection ranged from AUROC of 0.94 to 0.77 for MVP. Top-ranked variables for all models included known ECG characteristics along with novel predictors of these traits/diseases. △ Less","6 July, 2018",https://arxiv.org/pdf/1807.02569
The Data Science of Hollywood: Using Emotional Arcs of Movies to Drive Business Model Innovation in Entertainment Industries,Marco Del Vecchio;Alexander Kharlamov;Glenn Parry;Ganna Pogrebna,"Much of business literature addresses the issues of consumer-centric design: how can businesses design customized services and products which accurately reflect consumer preferences? This paper uses data science natural language processing methodology to explore whether and to what extent emotions shape consumer preferences for media and entertainment content. Using a unique filtered dataset of 6,174 movie scripts, we generate a mapping of screen content to capture the emotional trajectory of each motion picture. We then combine the obtained mappings into clusters which represent groupings of consumer emotional journeys. These clusters are used to predict overall success parameters of the movies including box office revenues, viewer satisfaction levels (captured by IMDb ratings), awards, as well as the number of viewers' and critics' reviews. We find that like books all movie stories are dominated by 6 basic shapes. The highest box offices are associated with the Man in a Hole shape which is characterized by an emotional fall followed by an emotional rise. This shape results in financially successful movies irrespective of genre and production budget. Yet, Man in a Hole succeeds not because it produces most ""liked"" movies but because it generates most ""talked about"" movies. Interestingly, a carefully chosen combination of production budget and genre may produce a financially successful movie with any emotional shape. Implications of this analysis for generating on-demand content and for driving business model innovation in entertainment industries are discussed. △ Less","10 July, 2018",https://arxiv.org/pdf/1807.02221
DNA Computing for Combinational Logic,Chuan Zhang;Lulu Ge;Yuchen Zhuang;Ziyuan Shen;Zhiwei Zhong;Zaichen Zhang;Xiaohu You,"With the progressive scale-down of semiconductor's feature size, people are looking forward to More Moore and More than Moore. In order to offer a possible alternative implementation process, people are trying to figure out a feasible transfer from silicon to molecular computing. Such transfer lies on bio-based modules programming with computer-like logic, aiming at realizing the Turing machine. To accomplish this, the DNA-based combinational logic is inevitably the first step we have taken care of. This timely overview paper introduces combinational logic synthesized in DNA computing from both analog and digital perspectives separately. State-of-the-art research progress is summarized for interested readers to quick understand DNA computing, initiate discussion on existing techniques and inspire innovation solutions. We hope this paper can pave the way for the future DNA computing synthesis. △ Less","5 July, 2018",https://arxiv.org/pdf/1807.02010
SGAD: Soft-Guided Adaptively-Dropped Neural Network,Zhisheng Wang;Fangxuan Sun;Jun Lin;Zhongfeng Wang;Bo Yuan,"Deep neural networks (DNNs) have been proven to have many redundancies. Hence, many efforts have been made to compress DNNs. However, the existing model compression methods treat all the input samples equally while ignoring the fact that the difficulties of various input samples being correctly classified are different. To address this problem, DNNs with adaptive dropping mechanism are well explored in this work. To inform the DNNs how difficult the input samples can be classified, a guideline that contains the information of input samples is introduced to improve the performance. Based on the developed guideline and adaptive dropping mechanism, an innovative soft-guided adaptively-dropped (SGAD) neural network is proposed in this paper. Compared with the 32 layers residual neural networks, the presented SGAD can reduce the FLOPs by 77% with less than 1% drop in accuracy on CIFAR-10. △ Less","3 July, 2018",https://arxiv.org/pdf/1807.01430
The Learning Technique of the SageMathCloud Use for Students Collaboration Support,Maiia Popel;Svitlana Shokalyuk;Mariya Shyshkina,"The article describes the advisable ways of the cloud-based systems use to support students collaboration in the process of math disciplines learning. The SageMathCloud-based component that aggregates electronic resources for several math disciplines training is introduced. The learning technique of the SageMathCloud use in the process of educational staff training is proposed. The expediency of this technique implementation for more active take up of innovative approaches, forms and methods of math training with the use of the cloud-based tools is substantiated. The experimental results of the SageMathCloud learning component introduction research along with the methods of its use that were elaborated in the course of the study are presented. The use of the evidence-based technique as improving the educational environment of the univer-sity, empowering access to electronic learning resources in the course of math training and engaging with this the educational community and also rising their ICT competence is grounded. △ Less","3 July, 2018",https://arxiv.org/pdf/1807.01120
Analytics for the Internet of Things: A Survey,Eugene Siow;Thanassis Tiropanis;Wendy Hall,"The Internet of Things (IoT) envisions a world-wide, interconnected network of smart physical entities. These physical entities generate a large amount of data in operation and as the IoT gains momentum in terms of deployment, the combined scale of those data seems destined to continue to grow. Increasingly, applications for the IoT involve analytics. Data analytics is the process of deriving knowledge from data, generating value like actionable insights from them. This article reviews work in the IoT and big data analytics from the perspective of their utility in creating efficient, effective and innovative applications and services for a wide spectrum of domains. We review the broad vision for the IoT as it is shaped in various communities, examine the application of data analytics across IoT domains, provide a categorisation of analytic approaches and propose a layered taxonomy from IoT data to analytics. This taxonomy provides us with insights on the appropriateness of analytical techniques, which in turn shapes a survey of enabling technology and infrastructure for IoT analytics. Finally, we look at some tradeoffs for analytics in the IoT that can shape future research. △ Less","3 July, 2018",https://arxiv.org/pdf/1807.00971
Harnessing AI for Speech Reconstruction using Multi-view Silent Video Feed,Yaman Kumar;Mayank Aggarwal;Pratham Nawal;Shin'ichi Satoh;Rajiv Ratn Shah;Roger Zimmerman,"Speechreading or lipreading is the technique of understanding and getting phonetic features from a speaker's visual features such as movement of lips, face, teeth and tongue. It has a wide range of multimedia applications such as in surveillance, Internet telephony, and as an aid to a person with hearing impairments. However, most of the work in speechreading has been limited to text generation from silent videos. Recently, research has started venturing into generating (audio) speech from silent video sequences but there have been no developments thus far in dealing with divergent views and poses of a speaker. Thus although, we have multiple camera feeds for the speech of a user, but we have failed in using these multiple video feeds for dealing with the different poses. To this end, this paper presents the world's first ever multi-view speech reading and reconstruction system. This work encompasses the boundaries of multimedia research by putting forth a model which leverages silent video feeds from multiple cameras recording the same subject to generate intelligent speech for a speaker. Initial results confirm the usefulness of exploiting multiple camera views in building an efficient speech reading and reconstruction system. It further shows the optimal placement of cameras which would lead to the maximum intelligibility of speech. Next, it lays out various innovative applications for the proposed system focusing on its potential prodigious impact in not just security arena but in many other multimedia analytics problems. △ Less","12 August, 2018",https://arxiv.org/pdf/1807.00619
Multi-Stage Complex Contagions in Random Multiplex Networks,Yong Zhuang;Osman Yağan,"Complex contagion models have been developed to understand a wide range of social phenomena such as adoption of cultural fads, the diffusion of belief, norms, and innovations in social networks, and the rise of collective action to join a riot. Most existing works focus on contagions where individuals' states are represented by {\em binary} variables, and propagation takes place over a single isolated network. However, characterization of an individual's standing on a given matter as a binary state might be overly simplistic as most of our opinions, feelings, and perceptions vary over more than two states. Also, most real-world contagions take place over multiple networks (e.g., Twitter and Facebook) or involve {\em multiplex} networks where individuals engage in different {\em types} of relationships (e.g., acquaintance, co-worker, family, etc.). To this end, this paper studies {\em multi-stage} complex contagions that take place over multi-layer or multiplex networks. Under a linear threshold based contagion model, we give analytic results for the probability and expected size of \textit{global} cascades, i.e., cases where a randomly chosen node can initiate a propagation that eventually reaches a {\em positive} fraction of the whole population. Analytic results are also confirmed and supported by an extensive numerical study. In particular, we demonstrate how the dynamics of complex contagions is affected by the extra weight exerted by \textit{hyper-active} nodes and by the structural properties of the networks involved. Among other things, we reveal an interesting connection between the assortativity of a network and the impact of \textit{hyper-active} nodes on the cascade size. △ Less","3 July, 2018",https://arxiv.org/pdf/1807.00454
Harnessing constrained resources in service industry via video analytics,Chun-Hung Cheng;Iyiola E. Olatunji,"Service industries contribute significantly to many developed and developing - economies. As their business activities expand rapidly, many service companies struggle to maintain customer's satisfaction due to sluggish service response caused by resource shortages. Anticipating resource shortages and proffering solutions before they happen is an effective way of reducing the adverse effect on operations. However, this proactive approach is very expensive in terms of capacity and labor costs. Many companies fall into productivity conundrum as they fail to find sufficient strong arguments to justify the cost of a new technology yet cannot afford not to invest in new technologies to match up with competitors. The question is whether there is an innovative solution to maximally utilize available resources and drastically reduce the effect that the shortages of resources may cause yet achieving high level of service quality at a low cost. This work demonstrates with a practical analysis of a trolley tracking system we designed and deployed at Hong Kong International Airport (HKIA) on how video analytics helps achieve management's goal of satisfying customer's needs via real-time detection and prevention of problems they may encounter during the service consumption process using existing video technology rather than adopting new technologies. This paper presents the integration of commercial video surveillance system with deep learning algorithms for video analytics. We show that our system can provide accurate decision when faced with total or partial occlusion with high accuracy and it significantly improves daily operation. It is envisioned that this work will heighten the appreciation of integrative technologies for resource management within the service industries and as a measure for real-time customer assistance. △ Less","30 June, 2018",https://arxiv.org/pdf/1807.00139
Toward modern educational IT-ecosystems: from learning management systems to digital platforms,Andrey Gorshenin,"The development of a learning management system (LMS) as a key service seems to be very effective for creation of educational digital platforms. Such platforms for both higher education institutions and various companies can provide the opportunities for networked forms of educational communication, improve the quality of the perception of innovative technologies and support tools for progress of talented youth as well as knowledge transfer. An example of such LMS is presented. The paper focuses on the demand for further development of learning management systems, their integration with modern digital platforms and potential exploitation as key services of such platforms in the context of the current educational trends of Industry 4.0 and the global trend towards a transition to a digital economy. The implementation of artificial intelligence technologies into the educational process is mentioned as an innovative way to form IT-ecosystems of modern education. △ Less","28 June, 2018",https://arxiv.org/pdf/1806.11154
Sustainable blockchain-enabled services: Smart contracts,Craig Wright;Antoaneta Serguieva,"This chapter contributes to evolving the versatility and complexity of blockchain-enabled services through extending the functionality of blockchain-enforced smart contracts. The contributions include: (i) a method for automated management of contracts with hierarchical conditionality structures through an hierarchy of intelligent agents and the use of hierarchical cryptographic key-pairs; (ii) a method for efficient and secure matching and transfer of smart- contract underlyings (entities) among disparate smart contracts/subcontracts; (iii) a method for producing an hierarchy of common secrets to facilitate hierarchical communication channels of increased security in the context of smart contracts/subcontracts/underlyings; and (iv) a method for building secure and optimized repositories through distributed hash tables in the context of contracts/ subcontracts/underlyings. These methods help providing services that allow both narrower and worldwide reach and distribution of resources. The longevity of the blockchain technology is achieved through continuous innovation. Blockchain-enabled services are potentially an efficient, secure, automated, and cost-effective alternative or complement to current service infrastructures in a range of domains (legal, medical, financial, government, IoT). △ Less","18 June, 2018",https://arxiv.org/pdf/1806.10638
Towards a Logic for Reasoning About LF Specifications,Mary Southern;Gopalan Nadathur,"We describe the development of a logic for reasoning about specifications in the Edinburgh Logical Framework (LF). In this logic, typing judgments in LF serve as atomic formulas, and quantification is permitted over contexts and terms that might appear in them. Further, contexts, which constitute type assignments to uniquely named variables that are modeled using the technical device of nominal constants, can be characterized via an inductive description of their structure. We present a semantics for such formulas and then consider the task of proving them. Towards this end, we restrict the collection of formulas we consider so as to ensure that they have normal forms upon which proof rules may be based. We then specifically discuss a proof rule that provides the basis for case analysis over LF typing judgments; this rule is the most complex and innovative one in the collection. We illustrate the proof system through an example. Finally, we discuss ongoing work and we relate our project to existing systems that have a similar goal. △ Less","26 June, 2018",https://arxiv.org/pdf/1806.10199
Evotype: Towards the Evolution of Type Stencils,Tiago Martins;João Correia;Ernesto Costa;Penousal Machado,"Typefaces are an essential resource employed by graphic designers. The increasing demand for innovative type design work increases the need for good technological means to assist the designer in the creation of a typeface. We present an evolutionary computation approach for the generation of type stencils to draw coherent glyphs for different characters. The proposed system employs a Genetic Algorithm to evolve populations of type stencils. The evaluation of each candidate stencil uses a hill climbing algorithm to search the best configurations to draw the target glyphs. We study the interplay between legibility, coherence and expressiveness, and show how our framework can be used in practice. △ Less","25 June, 2018",https://arxiv.org/pdf/1806.09731
Inferring Routing Preferences of Bicyclists from Sparse Sets of Trajectories,J. Oehrlein;A. Förster;D. Schunck;Y. Dehbi;R. Roscher;J. -H. Haunert,"Understanding the criteria that bicyclists apply when they choose their routes is crucial for planning new bicycle paths or recommending routes to bicyclists. This is becoming more and more important as city councils are becoming increasingly aware of limitations of the transport infrastructure and problems related to automobile traffic. Since different groups of cyclists have different preferences, however, searching for a single set of criteria is prone to failure. Therefore, in this paper, we present a new approach to classify trajectories recorded and shared by bicyclists into different groups and, for each group, to identify favored and unfavored road types. Based on these results we show how to assign weights to the edges of a graph representing the road network such that minimum-weight paths in the graph, which can be computed with standard shortest-path algorithms, correspond to adequate routes. Our method combines known algorithms for machine learning and the analysis of trajectories in an innovative way and, thereby, constitutes a new comprehensive solution for the problem of deriving routing preferences from initially unclassified trajectories. An important property of our method is that it yields reasonable results even if the given set of trajectories is sparse in the sense that it does not cover all segments of the cycle network. △ Less","24 June, 2018",https://arxiv.org/pdf/1806.09158
NFV and SDN - Key Technology Enablers for 5G Networks,Faqir Zarrar Yousaf;Michael Bredel;Sibylle Schaller;Fabian Schneider,"Communication networks are undergoing their next evolutionary step towards 5G. The 5G networks are envisioned to provide a flexible, scalable, agile and programmable network platform over which different services with varying requirements can be deployed and managed within strict performance bounds. In order to address these challenges a paradigm shift is taking place in the technologies that drive the networks, and thus their architecture. Innovative concepts and techniques are being developed to power the next generation mobile networks. At the heart of this development lie Network Function Virtualization and Software Defined Networking technologies, which are now recognized as being two of the key technology enablers for realizing 5G networks, and which have introduced a major change in the way network services are deployed and operated. For interested readers that are new to the field of SDN and NFV this paper provides an overview of both these technologies with reference to the 5G networks. Most importantly it describes how the two technologies complement each other and how they are expected to drive the networks of near future. △ Less","19 June, 2018",https://arxiv.org/pdf/1806.07316
Cross-country comparisons of scientific performance by focusing on post-apartheid South Africa,S. M. Hosseini Jenab,"This paper examines the scientific performance of South Africa since 1994 (post-apartheid) until 2014 in comparisons with the rest of the world, utilizing relative indicator. It provides a view over current standing of South Africa in the scientific world as well as its temporal evolution after the apartheid. This study focuses on four major aspects of scientific performance, namely quantity, productivity, impact and quality, as the main attributes of scientific perfomance on national level. These are measured by re-based (relative) publication, publication per population or GDP, citations and citations per publication respectively. The study focuses on scientific outputs (in the form of papers published in peer-reviewed journals) and their impact (measured by the citations they have received) to bring into a light a homogeneous comprehension of South Africa's scientific performance in all these four aspects. Indicators are adopted cautiously by considering the measures put forward recently for scientometrics indicators and their usage in the long-term comparisons studies. The temporal evolution of these indicators for South Africa are discussed in the context of three major groups of countries, namely African countries, developing countries, and developed (including BRICS) countries. It aims to examine the process of transition of South Africa from a developing world economy system into a knowledge-based and innovation-driven one of the developed world. The study reveals that South Africa has shown steady increase in its scientific performance during the studied period when compared to the rest of the world. However, due to the increasing competition from the other developing countries, South Africa's position stands the same during this period, while countries such as China, Iran, Turkey and Malaysia have shown great jump at least in the quantity of their scientific performance. △ Less","19 June, 2018",https://arxiv.org/pdf/1806.07122
Agricultural Robotics: The Future of Robotic Agriculture,Tom Duckett;Simon Pearson;Simon Blackmore;Bruce Grieve;Wen-Hua Chen;Grzegorz Cielniak;Jason Cleaversmith;Jian Dai;Steve Davis;Charles Fox;Pål From;Ioannis Georgilas;Richie Gill;Iain Gould;Marc Hanheide;Alan Hunter;Fumiya Iida;Lyudmila Mihalyova;Samia Nefti-Meziani;Gerhard Neumann;Paolo Paoletti;Tony Pridmore;Dave Ross;Melvyn Smith;Martin Stoelen,"Agri-Food is the largest manufacturing sector in the UK. It supports a food chain that generates over £108bn p.a., with 3.9m employees in a truly international industry and exports £20bn of UK manufactured goods. However, the global food chain is under pressure from population growth, climate change, political pressures affecting migration, population drift from rural to urban regions and the demographics of an aging global population. These challenges are recognised in the UK Industrial Strategy white paper and backed by significant investment via a Wave 2 Industrial Challenge Fund Investment (""Transforming Food Production: from Farm to Fork""). Robotics and Autonomous Systems (RAS) and associated digital technologies are now seen as enablers of this critical food chain transformation. To meet these challenges, this white paper reviews the state of the art in the application of RAS in Agri-Food production and explores research and innovation needs to ensure these technologies reach their full potential and deliver the necessary impacts in the Agri-Food sector. △ Less","2 August, 2018",https://arxiv.org/pdf/1806.06762
Feature Learning and Classification in Neuroimaging: Predicting Cognitive Impairment from Magnetic Resonance Imaging,Shan Shi;Farouk Nathoo,"Due to the rapid innovation of technology and the desire to find and employ biomarkers for neurodegenerative disease, high-dimensional data classification problems are routinely encountered in neuroimaging studies. To avoid over-fitting and to explore relationships between disease and potential biomarkers, feature learning and selection plays an important role in classifier construction and is an important area in machine learning. In this article, we review several important feature learning and selection techniques including lasso-based methods, PCA, the two-sample t-test, and stacked auto-encoders. We compare these approaches using a numerical study involving the prediction of Alzheimer's disease from Magnetic Resonance Imaging. △ Less","17 June, 2018",https://arxiv.org/pdf/1806.06415
Impact of Channel Models on the End-to-End Performance of mmWave Cellular Networks,Michele Polese;Michele Zorzi,"Communication at mmWave frequencies is one of the major innovations of the fifth generation of cellular networks, because of the potential multi-gigabit data rate given by the large amounts of available bandwidth. The mmWave channel, however, makes reliable communications particularly challenging, given the harsh propagation environment and the sensitivity to blockage. Therefore, proper modeling of the mmWave channel is fundamental for accurate results in system simulations of mmWave cellular networks. Nonetheless, complex models, such as the 3GPP channel model for frequencies above 6 GHz, may introduce a significant overhead in terms of computational complexity. In this paper we investigate the trade offs related to the accuracy and the simplicity of the channel model in end-to-end network simulations, and the impact on the performance evaluation of transport protocols. △ Less","15 June, 2018",https://arxiv.org/pdf/1806.06125
The Bass diffusion model on finite Barabasi-Albert networks,M. L. Bertotti;G. Modanese,"Using a mean-field network formulation of the Bass innovation diffusion model and exact results by Fotouhi and Rabbat on the degree correlations of Barabasi-Albert networks, we compute the times of the diffusion peak and compare them with those on scale-free networks which have the same scale-free exponent but different assortativity properties. We compare our results with those obtained by Caldarelli et al. for the SIS epidemic model with the spectral method applied to adjacency matrices. It turns out that diffusion times on finite Barabasi-Albert networks are at a minimum. This may be due to a little-known property of these networks: although the value of the assortativity coefficient is close to zero, they look disassortative if one considers only a bounded range of degrees, including the smallest ones, and slightly assortative on the range of the higher degrees. We also find that if the trickle-down character of the diffusion process is enhanced by a larger initial stimulus on the hubs (via a inhomogeneous linear term in the Bass model), the relative difference between the diffusion times for BA networks and uncorrelated networks is even larger, reaching for instance the 34% in a typical case on a network with 10^4 nodes. △ Less","18 June, 2018",https://arxiv.org/pdf/1806.05959
The Road to Success: Assessing the Fate of Linguistic Innovations in Online Communities,Marco Del Tredici;Raquel Fernández,"We investigate the birth and diffusion of lexical innovations in a large dataset of online social communities. We build on sociolinguistic theories and focus on the relation between the spread of a novel term and the social role of the individuals who use it, uncovering characteristics of innovators and adopters. Finally, we perform a prediction task that allows us to anticipate whether an innovation will successfully spread within a community. △ Less","15 June, 2018",https://arxiv.org/pdf/1806.05838
Neuromorphic Time-Dependent Pattern Classification with Organic Electrochemical Transistor Arrays,Sebastien Pecqueur;Maurizio Mastropasqua Talamo;David Guerin;Philippe Blanchard;Jean Roncali;Dominique Vuillaume;Fabien Alibart,"Based on bottom-up assembly of highly variable neural cells units, the nervous system can reach unequalled level of performances with respect to standard materials and devices used in microelectronic. Reproducing these basic concepts in hardware could potentially revolutionize materials and device engineering which are used for information processing. Here, we present an innovative approach that relies on both iono-electronic materials and intrinsic device physics to show pattern classification out of a 12-unit bio-sensing array. We use the reservoir computing and learning concept to demonstrate relevant computing based on the ionic dynamics in 400-nm channel-length organic electrochemical transistor (OECT). We show that this approach copes efficiently with the high level of variability obtained by bottom-up fabrication using a new electropolymerizable polymer, which enables iono-electronic device functionality and material stability in the electrolyte. We investigate the effect of the array size and variability on the performances for a real-time classification task paving the way to new embedded sensing and processing approaches. △ Less","12 June, 2018",https://arxiv.org/pdf/1806.04748
Potential of Augmented Reality for Intelligent Transportation Systems,Adnan Mahmood;Bernard Butler;Brendan Jennings,"Rapid advances in wireless communication technologies coupled with ongoing massive development in vehicular networking standards and innovations in computing, sensing, and analytics have paved the way for intelligent transportation systems (ITS) to develop rapidly in the near future. ITS provides a complete solution for the efficient and intelligent management of real-time traffic, wherein sensory data is collected from within the vehicles (i.e., via their onboard units) as well as data exchanged between the vehicles, between the vehicles and their supporting roadside infrastructure/network, among the vehicles and vulnerable pedestrians, subsequently paving the way for the realization of the futuristic Internet of Vehicles. The traditional intent of an ITS system is to detect, monitor, control, and subsequently reduce traffic congestion based on a real-time analysis of the data pertinent to certain patterns of the road traffic, including traffic density at a geographical area of interest, precise velocity of vehicles, current and predicted travelling trajectories and times, etc. However, merely relying on an ITS framework is not an optimal solution. In case of dense traffic environments, where communication broadcasts from hundreds of thousands of vehicles could potentially choke the entire network (and so could lead to fatal accidents in the case of autonomous vehicles that depend on reliable communications for their operational safety), a fall back to the traditional decentralized vehicular ad hoc network (VANET) approach becomes necessary. It is therefore of critical importance to enhance the situational awareness of vehicular drivers so as to enable them to make quick but well-founded manual decisions in such safety-critical situations. △ Less","10 June, 2018",https://arxiv.org/pdf/1806.04724
Talakat: Bullet Hell Generation through Constrained Map-Elites,Ahmed Khalifa;Scott Lee;Andy Nealen;Julian Togelius,"We describe a search-based approach to generating new levels for bullet hell games, which are action games characterized by and requiring avoidance of a very large amount of projectiles. Levels are represented using a domain-specific description language, and search in the space defined by this language is performed by a novel variant of the Map-Elites algorithm which incorporates a feasible- infeasible approach to constraint satisfaction. Simulation-based evaluation is used to gauge the fitness of levels, using an agent based on best-first search. The performance of the agent can be tuned according to the two dimensions of strategy and dexterity, making it possible to search for level configurations that require a specific combination of both. As far as we know, this paper describes the first generator for this game genre, and includes several algorithmic innovations. △ Less","13 June, 2018",https://arxiv.org/pdf/1806.04718
Multiview Two-Task Recursive Attention Model for Left Atrium and Atrial Scars Segmentation,Jun Chen;Guang Yang;Zhifan Gao;Hao Ni;Elsa Angelini;Raad Mohiaddin;Tom Wong;Yanping Zhang;Xiuquan Du;Heye Zhang;Jennifer Keegan;David Firmin,"Late Gadolinium Enhanced Cardiac MRI (LGE-CMRI) for detecting atrial scars in atrial fibrillation (AF) patients has recently emerged as a promising technique to stratify patients, guide ablation therapy and predict treatment success. Visualisation and quantification of scar tissues require a segmentation of both the left atrium (LA) and the high intensity scar regions from LGE-CMRI images. These two segmentation tasks are challenging due to the cancelling of healthy tissue signal, low signal-to-noise ratio and often limited image quality in these patients. Most approaches require manual supervision and/or a second bright-blood MRI acquisition for anatomical segmentation. Segmenting both the LA anatomy and the scar tissues automatically from a single LGE-CMRI acquisition is highly in demand. In this study, we proposed a novel fully automated multiview two-task (MVTT) recursive attention model working directly on LGE-CMRI images that combines a sequential learning and a dilated residual learning to segment the LA (including attached pulmonary veins) and delineate the atrial scars simultaneously via an innovative attention model. Compared to other state-of-the-art methods, the proposed MVTT achieves compelling improvement, enabling to generate a patient-specific anatomical and atrial scar assessment model. △ Less","12 June, 2018",https://arxiv.org/pdf/1806.04597
Convergence Rates for Projective Splitting,Patrick R. Johnstone;Jonathan Eckstein,"Projective splitting is a family of methods for solving inclusions involving sums of maximal monotone operators. First introduced by Eckstein and Svaiter in 2008, these methods have enjoyed significant innovation in recent years, becoming one of the most flexible operator splitting frameworks available. While weak convergence of the iterates to a solution has been established, there have been few attempts to study convergence rates of projective splitting. The purpose of this paper is to do so under various assumptions. To this end, there are three main contributions. First, in the context of convex optimization, we establish an O(1/k) ergodic function convergence rate. Second, for strongly monotone inclusions, strong convergence is established as well as an ergodic O(1/\sqrt{k}) convergence rate for the distance of the iterates to the solution. Finally, for inclusions featuring strong monotonicity and cocoercivity, linear convergence is established. △ Less","8 August, 2018",https://arxiv.org/pdf/1806.03920
Collaboration Diversity and Scientific Impact,Yuxiao Dong;Hao Ma;Jie Tang;Kuansan Wang,"The shift from individual effort to collaborative output has benefited science, with scientific work pursued collaboratively having increasingly led to more highly impactful research than that pursued individually. However, understanding of how the diversity of a collaborative team influences the production of knowledge and innovation is sorely lacking. Here, we study this question by breaking down the process of scientific collaboration of 32.9 million papers over the last five decades. We find that the probability of producing a top-cited publication increases as a function of the diversity of a team of collaborators---namely, the distinct number of institutions represented by the team. We discover striking phenomena where a smaller, yet more diverse team is more likely to generate highly innovative work than a relatively larger team within one institution. We demonstrate that the synergy of collaboration diversity is universal across different generations, research fields, and tiers of institutions and individual authors. Our findings suggest that collaboration diversity strongly and positively correlates with the production of scientific innovation, giving rise to the potential revolution of the policies used by funding agencies and authorities to fund research projects, and broadly the principles used to organize teams, organizations, and societies. △ Less","10 June, 2018",https://arxiv.org/pdf/1806.03694
Improving Surgical Training Phantoms by Hyperrealism: Deep Unpaired Image-to-Image Translation from Real Surgeries,Sandy Engelhardt;Raffaele De Simone;Peter M. Full;Matthias Karck;Ivo Wolf,"Current `dry lab' surgical phantom simulators are a valuable tool for surgeons which allows them to improve their dexterity and skill with surgical instruments. These phantoms mimic the haptic and shape of organs of interest, but lack a realistic visual appearance. In this work, we present an innovative application in which representations learned from real intraoperative endoscopic sequences are transferred to a surgical phantom scenario. The term hyperrealism is introduced in this field, which we regard as a novel subform of surgical augmented reality for approaches that involve real-time object transfigurations. For related tasks in the computer vision community, unpaired cycle-consistent Generative Adversarial Networks (GANs) have shown excellent results on still RGB images. Though, application of this approach to continuous video frames can result in flickering, which turned out to be especially prominent for this application. Therefore, we propose an extension of cycle-consistent GANs, named tempCycleGAN, to improve temporal consistency.The novel method is evaluated on captures of a silicone phantom for training endoscopic reconstructive mitral valve procedures. Synthesized videos show highly realistic results with regard to 1) replacement of the silicone appearance of the phantom valve by intraoperative tissue texture, while 2) explicitly keeping crucial features in the scene, such as instruments, sutures and prostheses. Compared to the original CycleGAN approach, tempCycleGAN efficiently removes flickering between frames. The overall approach is expected to change the future design of surgical training simulators since the generated sequences clearly demonstrate the feasibility to enable a considerably more realistic training experience for minimally-invasive procedures. △ Less","10 June, 2018",https://arxiv.org/pdf/1806.03627
Blockchain and Principles of Business Process Re-Engineering for Process Innovation,Fredrik Milani;Luciano Garcia-Banuelos,"Blockchain has emerged as one of the most promising and revolutionary technologies in the past years. Companies are exploring implementation of use cases in hope of significant gains in efficiencies. However, to achieve the impact hoped for, it is not sufficient to merely replace existing technologies. The current business processes must also be redesigned and innovated to enable realization of hoped for benefits. This conceptual paper provides a theoretical contribution on how blockchain technology and smart contracts potentially can, within the framework of the seven principles of business process re-engineering (BPR), enable process innovations. In this paper, we analyze the BPR principles in light of their applicability to blockchain-based solutions. We find these principles to be applicable and helpful in understanding how blockchain technology could enable transformational redesign of current processes. However, the viewpoint taken, should be expanded from intra- to inter-organizational processes operating within an ecosystem of separate organizational entities. In such a blockchain powered ecosystem, smart contracts take on a pivotal role, both as repositories of data and executioner of activities. △ Less","8 June, 2018",https://arxiv.org/pdf/1806.03054
Mix&Match - Agent Curricula for Reinforcement Learning,Wojciech Marian Czarnecki;Siddhant M. Jayakumar;Max Jaderberg;Leonard Hasenclever;Yee Whye Teh;Simon Osindero;Nicolas Heess;Razvan Pascanu,"We introduce Mix&Match (M&M) - a training framework designed to facilitate rapid and effective learning in RL agents, especially those that would be too slow or too challenging to train otherwise. The key innovation is a procedure that allows us to automatically form a curriculum over agents. Through such a curriculum we can progressively train more complex agents by, effectively, bootstrapping from solutions found by simpler agents. In contradistinction to typical curriculum learning approaches, we do not gradually modify the tasks or environments presented, but instead use a process to gradually alter how the policy is represented internally. We show the broad applicability of our method by demonstrating significant performance gains in three different experimental setups: (1) We train an agent able to control more than 700 actions in a challenging 3D first-person task; using our method to progress through an action-space curriculum we achieve both faster training and better final performance than one obtains using traditional methods. (2) We further show that M&M can be used successfully to progress through a curriculum of architectural variants defining an agents internal state. (3) Finally, we illustrate how a variant of our method can be used to improve agent performance in a multitask setting. △ Less","5 June, 2018",https://arxiv.org/pdf/1806.01780
BPjs --- a framework for modeling reactive systems using a scripting language and BP,Michael Bar-Sinai;Gera Weiss;Reut Shmuel,"We describe some progress towards a new common framework for model driven engineering, based on behavioral programming. The tool we have developed unifies almost all of the work done in behavioral programming so far, under a common set of interfaces. Its architecture supports pluggable event selection strategies, which can make models more intuitive and compact. Program state space can be traversed using various algorithms, such as DFS and A*. Furthermore, program state is represented in a way that enables scanning a state space using parallel and distributed algorithms. Executable models created with this tool can be directly embedded in Java applications, enabling a model-first approach to system engineering, where initially a model is created and verified, and then a working application is gradually built around the model. The model itself consists of a collection of small scripts written in JavaScript (hence ""BPjs""). Using a variety of case-studies, this paper shows how the combination of a lenient programming language with formal model analysis tools creates an efficient way of developing robust complex systems. Additionally, as we learned from an experimental course we ran, the usage of JavaScript make practitioners more amenable to using this system and, thus, model checking and model driven engineering. In addition to providing infrastructure for development and case-studies in behavioral programming, the tool is designed to serve as a common platform for research and innovation in behavioral programming and in model driven engineering in general. △ Less","3 June, 2018",https://arxiv.org/pdf/1806.00842
Closed-loop Bayesian Semantic Data Fusion for Collaborative Human-Autonomy Target Search,Luke Burks;Ian Loefgren;Luke Barbier;Jeremy Muesing;Jamison McGinley;Sousheel Vunnam;Nisar Ahmed,"In search applications, autonomous unmanned vehicles must be able to efficiently reacquire and localize mobile targets that can remain out of view for long periods of time in large spaces. As such, all available information sources must be actively leveraged -- including imprecise but readily available semantic observations provided by humans. To achieve this, this work develops and validates a novel collaborative human-machine sensing solution for dynamic target search. Our approach uses continuous partially observable Markov decision process (CPOMDP) planning to generate vehicle trajectories that optimally exploit imperfect detection data from onboard sensors, as well as semantic natural language observations that can be specifically requested from human sensors. The key innovation is a scalable hierarchical Gaussian mixture model formulation for efficiently solving CPOMDPs with semantic observations in continuous dynamic state spaces. The approach is demonstrated and validated with a real human-robot team engaged in dynamic indoor target search and capture scenarios on a custom testbed. △ Less","2 June, 2018",https://arxiv.org/pdf/1806.00727
"Miniaturized Microwave Devices and Antennas for Wearable, Implantable and Wireless Applications",Muhammad Ali Babar Abbasi,"This thesis presents a number of microwave devices and antennas that maintain high operational efficiency and are compact in size at the same time. One goal of this thesis is to address several miniaturization challenges of antennas and microwave components by using the theoretical principles of metamaterials, Metasurface coupling resonators and stacked radiators, in combination with the elementary antenna and transmission line theory. While innovating novel solutions, standards and specifications of next generation wireless and bio-medical applications were considered to ensure advancement in the respective scientific fields. Compact reconfigurable phase-shifter and a microwave cross-over based on negative-refractive-index transmission-line (NRI-TL) materialist unit cells is presented. A Metasurface based wearable sensor architecture is proposed, containing an electromagnetic band-gap (EBG) structure backed monopole antenna for off-body communication and a fork shaped antenna for efficient radiation towards the human body. A fully parametrized solution for an implantable antenna is proposed using metallic coated stacked substrate layers. Challenges and possible solutions for off-body, on-body, through-body and across-body communication have been investigated with an aid of computationally extensive simulations and experimental verification. Next, miniaturization and implementation of a UWB antenna along with an analytical model to predict the resonance is presented. Lastly, several miniaturized rectifiers designed specifically for efficient wireless power transfer are proposed, experimentally verified, and discussed. The study answered several research questions of applied electromagnetic in the field of bio-medicine and wireless communication. △ Less","1 June, 2018",https://arxiv.org/pdf/1806.00379
The One-Shot Crowdfunding Game,Itai Arieli;Moran Koren;Rann Smorodinsky,"The recent success of crowd-funding for supporting new and innovative products has been overwhelming with over 34 Billion Dollars raised in 2015. In many crowd-funding platforms, firms set a campaign goal and contributions are collected only if this goal is reached. At the time of the campaign, consumers are often uncertain as to the ex-post value of the product, the business model viability, or the seller's reliability. Consumer who commit to a contribution therefore gambles. This gamble is effected by the campaign's threshold. Contributions to campaigns with higher thresholds are collected only if a greater number of agents find the offering acceptable. Therefore, high threshold serves as a social insurance and thus in high-threshold campaigns, potential contributors feel more at ease with contributing. We introduce the crowdunding game and explore the contributor's dilemma in the context of experience goods. We discuss equilibrium existence and related social welfare, information aggregation and revenue implications. △ Less","30 May, 2018",https://arxiv.org/pdf/1805.11872
"Regions, Innovation Systems, and the North-South Divide in Italy",Loet Leydesdorff;Ivan Cucco,"Using firm-level data collected by Statistics Italy for 2008, 2011, and 2015, we examine the Triple-Helix synergy among geographical and size distributions of firms, and the NACE codes attributed to these firms, at the different levels of regional and national government. At which levels is innovation-systemness indicated? The contributions of regions to the Italian innovation system have increased, but synergy generation between regions and supra-regionally has remained at almost 45%. As against the statistical classification of Italy into twenty regions or into Northern, Central, and Southern Italy, the greatest synergy is retrieved by considering the country in terms of Northern and Southern Italy as two sub-systems, with Tuscany included as part of Northern Italy. We suggest that separate innovation strategies should be developed for these two parts of the country. The current focus on regions for innovation policies may to some extent be an artifact of the statistics and EU policies. In terms of sectors, both medium- and high-tech manufacturing (MHTM) and knowledge-intensive services (KIS) are proportionally integrated in the various regions. △ Less","30 May, 2018",https://arxiv.org/pdf/1805.11821
E-Voting with Blockchain: An E-Voting Protocol with Decentralisation and Voter Privacy,Freya Sheer Hardwick;Apostolos Gioulis;Raja Naeem Akram;Konstantinos Markantonakis,"Technology has positive impacts on many aspects of our social life. Designing a 24hour globally connected architecture enables ease of access to a variety of resources and services. Furthermore, technology like Internet has been a fertile ground for innovation and creativity. One of such disruptive innovation is blockchain -- a keystone of cryptocurrencies. The blockchain technology is presented as a game changer for many of the existing and emerging technologies/services. With its immutability property and decentralised architecture, it is taking centre stage in many services as an equalisation factor to the current parity between consumers and large corporations/governments. One of such potential applications of the blockchain is in e-voting schemes. The objective of such a scheme would be to provide a decentralised architecture to run and support a voting scheme that is open, fair and independently verifiable. In this paper, we propose potentially a new e-voting protocol that utilises the blockchain as a transparent ballot box. The protocol has been designed with adhering to the fundamental e-voting properties in mind as well as offering a degree of decentralisation and allowing for the voter to change/update their vote (within the permissible voting period). The paper highlights the pros and cons of using blockchain for such a proposal from practical point view in both development/deployment and usage contexts. Concluding the paper with a potential roadmap for blockchain technology to be able to support complex applications. △ Less","3 July, 2018",https://arxiv.org/pdf/1805.10258
GraphChallenge.org: Raising the Bar on Graph Analytic Performance,Siddharth Samsi;Vijay Gadepally;Michael Hurley;Michael Jones;Edward Kao;Sanjeev Mohindra;Paul Monticciolo;Albert Reuther;Steven Smith;William Song;Diane Staheli;Jeremy Kepner,"The rise of graph analytic systems has created a need for new ways to measure and compare the capabilities of graph processing systems. The MIT/Amazon/IEEE Graph Challenge has been developed to provide a well-defined community venue for stimulating research and highlighting innovations in graph analysis software, hardware, algorithms, and systems. GraphChallenge.org provides a wide range of pre-parsed graph data sets, graph generators, mathematically defined graph algorithms, example serial implementations in a variety of languages, and specific metrics for measuring performance. Graph Challenge 2017 received 22 submissions by 111 authors from 36 organizations. The submissions highlighted graph analytic innovations in hardware, software, algorithms, systems, and visualization. These submissions produced many comparable performance measurements that can be used for assessing the current state of the art of the field. There were numerous submissions that implemented the triangle counting challenge and resulted in over 350 distinct measurements. Analysis of these submissions show that their execution time is a strong function of the number of edges in the graph, N_e, and is typically proportional to N_e^{4/3} for large values of N_e. Combining the model fits of the submissions presents a picture of the current state of the art of graph analysis, which is typically 10^8 edges processed per second for graphs with 10^8 edges. These results are 30 times faster than serial implementations commonly used by many graph analysts and underscore the importance of making these performance benefits available to the broader community. Graph Challenge provides a clear picture of current graph analysis systems and underscores the need for new innovations to achieve high performance on very large graphs. △ Less","22 May, 2018",https://arxiv.org/pdf/1805.09675
Digital Transformation in Airport Ground Operations,Ivan Kovynyov;Ralf Mikut,"How has digital transformation changed airport ground operations? Although the relevant peer-reviewed literature emphasizes the role of cost savings as a key driver behind digitalization of airport ground operations, the focus is on data-driven, customer-centric innovations. This paper argues that ground handling agents are deploying new technologies mainly to boost process efficiency and to cut costs. Our research shows that ground handling agents are embracing current trends to craft new business models and develop new revenue streams. In this paper, we examine the ground handling agent's value chain and identify areas that are strongly affected by digital transformation and those that are not. We discuss different business scenarios for digital technology and link them with relevant research, such as automated service data capturing, new digital services for passengers, big data, indoor navigation, and wearables in airport ground operations. We assess the maturity level of discussed technologies using NASA technology readiness levels. △ Less","15 May, 2018",https://arxiv.org/pdf/1805.09142
Immersive Virtual Reality Serious Games for Evacuation Training and Research: A Systematic Literature Review,Zhenan Feng;Vicente A. González;Robert Amor;Ruggiero Lovreglio;Guillermo Cabrera,"An appropriate and safe behavior for exiting a facility is key to reducing injuries and increasing survival when facing an emergency evacuation in a building. Knowledge on the best evacuation practice is commonly delivered by traditional training approaches such as videos, posters, or evacuation drills, but they may become ineffective in terms of knowledge acquisition and retention. Serious games (SGs) are an innovative approach devoted to training and educating people in a gaming environment. Recently, increasing attention has been paid to immersive virtual reality (IVR)-based SGs for evacuation knowledge delivery and behavior assessment because they are highly engaging and promote greater cognitive learning. This paper aims to understand the development and implementation of IVR SGs in the context of building evacuation training and research, applied to various indoor emergencies such as fire and earthquake. Thus, a conceptual framework for effective design and implementation through the systematic literature review method was developed. As a result, this framework integrates critical aspects and provides connections between them, including pedagogical and behavioral impacts, gaming environment development, and outcome and participation experience measures. △ Less","13 September, 2018",https://arxiv.org/pdf/1805.09138
Teacher's Perception in the Classroom,Ömer Sümer;Patricia Goldberg;Kathleen Stürmer;Tina Seidel;Peter Gerjets;Ulrich Trautwein;Enkelejda Kasneci,"The ability for a teacher to engage all students in active learning processes in classroom constitutes a crucial prerequisite for enhancing students achievement. Teachers' attentional processes provide important insights into teachers' ability to focus their attention on relevant information in the complexity of classroom interaction and distribute their attention across students in order to recognize the relevant needs for learning. In this context, mobile eye tracking is an innovative approach within teaching effectiveness research to capture teachers' attentional processes while teaching. However, analyzing mobile eye-tracking data by hand is time consuming and still limited. In this paper, we introduce a new approach to enhance the impact of mobile eye tracking by connecting it with computer vision. In mobile eye tracking videos from an educational study using a standardized small group situation, we apply a state-ofthe-art face detector, create face tracklets, and introduce a novel method to cluster faces into the number of identity. Subsequently, teachers' attentional focus is calculated per student during a teaching unit by associating eye tracking fixations and face tracklets. To the best of our knowledge, this is the first work to combine computer vision and mobile eye tracking to model teachers' attention while instructing. △ Less","22 May, 2018",https://arxiv.org/pdf/1805.08897
Implementation of Chua's chaotic oscillator with an HP memristor,Muratkhan Abdirash;Irina Dolzhikova;Alex Pappachen James,"This paper proposes an innovative chaotic circuit based on Chua's oscillator. It combines traditional realization of a non-linear resistor in Chua's chaotic oscillator with a promising memristive circuitry. This mixed implementation connects old research works that were focused on diodes with relatively new research papers that are, now, concentrated on memristors. As a result, more reliable chaotic circuit with an HP memristor is obtained that could be used as a source of randomness. Dynamic behavior of the circuit is studied by obtaining fft analysis, different chaotic attractors and Lyapunov exponent spectrum. The results show that the addition of a memristor enhances chaotic behavior of the circuit while maintaining the same power dissipation. △ Less","18 May, 2018",https://arxiv.org/pdf/1805.08081
The Governance of Risks in Ridesharing: A Revelatory Case from Singapore,Yanwei Li;Araz Taeihagh;Martin de Jong,"Recently we have witnessed the worldwide adoption of many different types of innovative technologies, such as crowdsourcing, ridesharing, open and big data, aiming at delivering public services more efficiently and effectively. Among them, ridesharing has received substantial attention from decision-makers around the world. Because of the multitude of currently understood or potentially unknown risks associated with ridesharing (unemployment, insurance, information privacy, and environmental risk), governments in different countries apply different strategies to address such risks. Some governments prohibit the adoption of ridesharing altogether, while other governments promote it. In this article, we address the question of how risks involved in ridesharing are governed over time. We present an in-depth single case study on Singapore and examine how the Singaporean government has addressed risks in ridesharing over time. The Singaporean government has a strong ambition to become an innovation hub, and many innovative technologies have been adopted and promoted to that end. At the same time, decision-makers in Singapore are reputed for their proactive style of social governance. The example of Singapore can be regarded as a revelatory case study, helping us further to explore governance practices in other countries. Keywords: risk; ridesharing; transport; governance; innovative technologies; case study; Singapore △ Less","21 May, 2018",https://arxiv.org/pdf/1805.07885
Learning to Teach in Cooperative Multiagent Reinforcement Learning,Shayegan Omidshafiei;Dong-Ki Kim;Miao Liu;Gerald Tesauro;Matthew Riemer;Christopher Amato;Murray Campbell;Jonathan P. How,"Collective human knowledge has clearly benefited from the fact that innovations by individuals are taught to others through communication. Similar to human social groups, agents in distributed learning systems would likely benefit from communication to share knowledge and teach skills. The problem of teaching to improve agent learning has been investigated by prior works, but these approaches make assumptions that prevent application of teaching to general multiagent problems, or require domain expertise for problems they can apply to. This learning to teach problem has inherent complexities related to measuring long-term impacts of teaching that compound the standard multiagent coordination challenges. In contrast to existing works, this paper presents the first general framework and algorithm for intelligent agents to learn to teach in a multiagent environment. Our algorithm, Learning to Coordinate and Teach Reinforcement (LeCTR), addresses peer-to-peer teaching in cooperative multiagent reinforcement learning. Each agent in our approach learns both when and what to advise, then uses the received advice to improve local learning. Importantly, these roles are not fixed; these agents learn to assume the role of student and/or teacher at the appropriate moments, requesting and providing advice in order to improve teamwide performance and learning. Empirical comparisons against state-of-the-art teaching methods show that our teaching agents not only learn significantly faster, but also learn to coordinate in tasks where existing methods fail. △ Less","31 August, 2018",https://arxiv.org/pdf/1805.07830
Conditional Network Embeddings,Bo Kang;Jefrey Lijffijt;Tijl De Bie,"Network Embeddings (NEs) map the nodes of a given network into d-dimensional Euclidean space \mathbb{R}^d. Ideally, this mapping is such that `similar' nodes are mapped onto nearby points, such that the NE can be used for purposes such as link prediction (if `similar' means being `more likely to be connected') or classification (if `similar' means `being more likely to have the same label'). In recent years various methods for NE have been introduced, all following a similar strategy: defining a notion of similarity between nodes (typically some distance measure within the network), a distance measure in the embedding space, and a loss function that penalizes large distances for similar nodes and small distances for dissimilar nodes. A difficulty faced by existing methods is that certain networks are fundamentally hard to embed due to their structural properties: (approximate) multipartiteness, certain degree distributions, assortativity, etc. To overcome this, we introduce a conceptual innovation to the NE literature and propose to create \emph{Conditional Network Embeddings} (CNEs); embeddings that maximally add information with respect to given structural properties (e.g. node degrees, block densities, etc.). We use a simple Bayesian approach to achieve this, and propose a block stochastic gradient descent algorithm for fitting it efficiently. We demonstrate that CNEs are superior for link prediction and multi-label classification when compared to state-of-the-art methods, and this without adding significant mathematical or computational complexity. Finally, we illustrate the potential of CNE for network visualization. △ Less","16 October, 2018",https://arxiv.org/pdf/1805.07544
Deep Reinforcement Learning for Resource Management in Network Slicing,Rongpeng Li;Zhifeng Zhao;Qi Sun;Chi-Lin I;Chenyang Yang;Xianfu Chen;Minjian Zhao;Honggang Zhang,"Network slicing is born as an emerging business to operators, by allowing them to sell the customized slices to various tenants at different prices. In order to provide better-performing and cost-efficient services, network slicing involves challenging technical issues and urgently looks forward to intelligent innovations to make the resource management consistent with users' activities per slice. In that regard, deep reinforcement learning (DRL), which focuses on how to interact with the environment by trying alternative actions and reinforcing the tendency actions producing more rewarding consequences, is assumed to be a promising solution. In this paper, after briefly reviewing the fundamental concepts of DRL, we investigate the application of DRL in solving some typical resource management for network slicing scenarios, which include radio resource slicing and priority-based core network slicing, and demonstrate the advantage of DRL over several competing schemes through extensive simulations. Finally, we also discuss the possible challenges to apply DRL in network slicing from a general perspective. △ Less","20 November, 2018",https://arxiv.org/pdf/1805.06591
Career Transitions and Trajectories: A Case Study in Computing,Tara Safavi;Maryam Davoodi;Danai Koutra,"From artificial intelligence to network security to hardware design, it is well-known that computing research drives many important technological and societal advancements. However, less is known about the long-term career paths of the people behind these innovations. What do their careers reveal about the evolution of computing research? Which institutions were and are the most important in this field, and for what reasons? Can insights into computing career trajectories help predict employer retention? In this paper we analyze several decades of post-PhD computing careers using a large new dataset rich with professional information, and propose a versatile career network model, R^3, that captures temporal career dynamics. With R^3 we track important organizations in computing research history, analyze career movement between industry, academia, and government, and build a powerful predictive model for individual career transitions. Our study, the first of its kind, is a starting point for understanding computing research careers, and may inform employer recruitment and retention mechanisms at a time when the demand for specialized computational expertise far exceeds supply. △ Less","24 May, 2018",https://arxiv.org/pdf/1805.06534
Object detection at 200 Frames Per Second,Rakesh Mehta;Cemalettin Ozturk,"In this paper, we propose an efficient and fast object detector which can process hundreds of frames per second. To achieve this goal we investigate three main aspects of the object detection framework: network architecture, loss function and training data (labeled and unlabeled). In order to obtain compact network architecture, we introduce various improvements, based on recent work, to develop an architecture which is computationally light-weight and achieves a reasonable performance. To further improve the performance, while keeping the complexity same, we utilize distillation loss function. Using distillation loss we transfer the knowledge of a more accurate teacher network to proposed light-weight student network. We propose various innovations to make distillation efficient for the proposed one stage detector pipeline: objectness scaled distillation loss, feature map non-maximal suppression and a single unified distillation loss function for detection. Finally, building upon the distillation loss, we explore how much can we push the performance by utilizing the unlabeled data. We train our model with unlabeled data using the soft labels of the teacher network. Our final network consists of 10x fewer parameters than the VGG based object detection network and it achieves a speed of more than 200 FPS and proposed changes improve the detection accuracy by 14 mAP over the baseline on Pascal dataset. △ Less","16 May, 2018",https://arxiv.org/pdf/1805.06361
AMORE-UPF at SemEval-2018 Task 4: BiLSTM with Entity Library,Laura Aina;Carina Silberer;Ionut-Teodor Sorodoc;Matthijs Westera;Gemma Boleda,"This paper describes our winning contribution to SemEval 2018 Task 4: Character Identification on Multiparty Dialogues. It is a simple, standard model with one key innovation, an entity library. Our results show that this innovation greatly facilitates the identification of infrequent characters. Because of the generic nature of our model, this finding is potentially relevant to any task that requires effective learning from sparse or unbalanced data. △ Less","14 May, 2018",https://arxiv.org/pdf/1805.05370
Initial Access Frameworks for 3GPP NR at mmWave Frequencies,Marco Giordani;Michele Polese;Arnab Roy;Douglas Castor;Michele Zorzi,"The use of millimeter wave (mmWave) frequencies for communication will be one of the innovations of the next generation of cellular mobile networks (5G). It will provide unprecedented data rates, but is highly susceptible to rapid channel variations and suffers from severe isotropic pathloss. Highly directional antennas at the transmitter and the receiver will be used to compensate for these shortcomings and achieve sufficient link budget in wide area networks. However, directionality demands precise alignment of the transmitter and the receiver beams, an operation which has important implications for control plane procedures, such as initial access, and may increase the delay of the data transmission. This paper provides a comparison of measurement frameworks for initial access in mmWave cellular networks in terms of detection accuracy, reactiveness and overhead, using parameters recently standardized by the 3GPP and a channel model based on real-world measurements. We show that the best strategy depends on the specific environment in which the nodes are deployed, and provide guidelines to characterize the optimal choice as a function of the system parameters. △ Less","11 May, 2018",https://arxiv.org/pdf/1805.05187
CloudAR: A Cloud-based Framework for Mobile Augmented Reality,Wenxiao Zhang;Sikun Lin;Farshid Hassani Bijarbooneh;Hao Fei Cheng;And Pan Hui,"Computation capabilities of recent mobile devices enable natural feature processing for Augmented Reality (AR). However, mobile AR applications are still faced with scalability and performance challenges. In this paper, we propose CloudAR, a mobile AR framework utilizing the advantages of cloud and edge computing through recognition task offloading. We explore the design space of cloud-based AR exhaustively and optimize the offloading pipeline to minimize the time and energy consumption. We design an innovative tracking system for mobile devices which provides lightweight tracking in 6 degree of freedom (6DoF) and hides the offloading latency from users' perception. We also design a multi-object image retrieval pipeline that executes fast and accurate image recognition tasks on servers. In our evaluations, the mobile AR application built with the CloudAR framework runs at 30 frames per second (FPS) on average with precise tracking of only 1~2 pixel errors and image recognition of at least 97% accuracy. Our results also show that CloudAR outperforms one of the leading commercial AR framework in several performance metrics. △ Less","8 May, 2018",https://arxiv.org/pdf/1805.03060
Service Discovery for Hyperledger Fabric,Yacov Manevich;Artem Barger;Yoav Tock,"Hyperledger Fabric (HLF) is a modular and extensible permissioned blockchain platform released to open-source and hosted by the Linux Foundation. The platform's design exhibits principles required by enterprise grade business applications like supply-chains, financial transactions, asset management, food safety, and many more. For that end HLF introduces several innovations, two of which are smart contracts in general purpose languages (\emph{chaincode} in HLF), and flexible endorsement policies, which govern whether a transaction is considered valid. Typical blockchain applications are comprised of two tiers: the first tier focuses on the modelling of the data schema and embedding of business rules into the blockchain by means of smart contracts (\emph{chaincode}) and endorsment policies; and the second tier uses the SDK (Software Development Kit) provided by HLF to implement client side application logic. However there is a gap between the two tiers that hinders the rapid adoption of changes in the chaincode and endorsement policies within the client SDK. Currently, the chaincode location and endorsement policies are statically configured into the client SDK. This limits the reliability and availability of the client in the event of changes in the platform, and makes the platform more difficult to use. In this work we address and bridge the gap by describing the design and implementation of \emph{Service Discovery}. \emph{Service Discovery} provides APIs which allow dynamic discovery of the configuration required for the client SDK to interact with the platform, alleviating the client from the burden of maintaining it. This enables the client to rapidly adapt to changes in the platform, thus significantly improving the reliability of the application layer. It also makes the HLF platform more consumable, simplifying the job of creating blockchain applications. △ Less","5 May, 2018",https://arxiv.org/pdf/1805.02105
Advanced local motion patterns for macro and micro facial expression recognition,B. Allaert;IM. Bilasco;C. Djeraba,"In this paper, we develop a new method that recognizes facial expressions, on the basis of an innovative local motion patterns feature, with three main contributions. The first one is the analysis of the face skin temporal elasticity and face deformations during expression. The second one is a unified approach for both macro and micro expression recognition. And, the third one is the step forward towards in-the-wild expression recognition, dealing with challenges such as various intensity and various expression activation patterns, illumination variation and small head pose variations. Our method outperforms state-of-the-art methods for micro expression recognition and positions itself among top-rank state-of-the-art methods for macro expression recognition. △ Less","4 May, 2018",https://arxiv.org/pdf/1805.01951
Business Processes of High-Tech Enterprises,S. E. Pyatovsky,"This paper analyzes the results of Russia's current innovative activities. It shows the need to increase the level of return on investments in the innovative capacities of high-technology enterprises (HTEs). The paper describes the methods to help increase the competitiveness of HTEs based on the implementation of modern control methods for innovative HTEs. It analyzes HTE KPIs, and also describes the characteristics of the organizational structure of HTEs. HTEs are studied as an innovative self-training HTE, and their characteristics and system of competencies are analyzed. The paper likewise describes the management of information support for managerial decisions in self-training HTEs. It shows that a considerable share of innovative products in a highly competitive market requires accelerated promotion and new approaches to building the system of business processes (BPs) for self-training HTEs. It also shows the dependency of self-training HTE competitiveness on innovative management methods for manufacturing and technological processes (MTP). △ Less","4 May, 2018",https://arxiv.org/pdf/1805.01647
Smart contracts that are smart and can function as legal contracts - A Review of Semantic Blockchain and Distributed Ledger Technologies,Marcelle von Wendland,"Blockchain and Distributed ledger Technologies are increasingly becoming key enablers for vital innovation in financial services, manufacturing, government and other industries. One of the biggest challenges though is the level of support for semantics by most of the Block Chain and Distributed Ledger technologies. This paper reviews and categorises common block chain and DLT approaches and introduces a new approach to Blockchain / DLT promising to resolve the semantic problems inherent in other Blockchain / DLT approaches △ Less","26 April, 2018",https://arxiv.org/pdf/1805.01279
VINE: An Open Source Interactive Data Visualization Tool for Neuroevolution,Rui Wang;Jeff Clune;Kenneth O. Stanley,"Recent advances in deep neuroevolution have demonstrated that evolutionary algorithms, such as evolution strategies (ES) and genetic algorithms (GA), can scale to train deep neural networks to solve difficult reinforcement learning (RL) problems. However, it remains a challenge to analyze and interpret the underlying process of neuroevolution in such high dimensions. To begin to address this challenge, this paper presents an interactive data visualization tool called VINE (Visual Inspector for NeuroEvolution) aimed at helping neuroevolution researchers and end-users better understand and explore this family of algorithms. VINE works seamlessly with a breadth of neuroevolution algorithms, including ES and GA, and addresses the difficulty of observing the underlying dynamics of the learning process through an interactive visualization of the evolving agent's behavior characterizations over generations. As neuroevolution scales to neural networks with millions or more connections, visualization tools like VINE that offer fresh insight into the underlying dynamics of evolution become increasingly valuable and important for inspiring new innovations and applications. △ Less","3 May, 2018",https://arxiv.org/pdf/1805.01141
SAGE: Percipient Storage for Exascale Data Centric Computing,Sai Narasimhamurthy;Nikita Danilov;Sining Wu;Ganesan Umanesan;Stefano Markidis;Sergio Rivas-Gomez;Ivy Bo Peng;Erwin Laure;Dirk Pleiter;Shaun de Witt,"We aim to implement a Big Data/Extreme Computing (BDEC) capable system infrastructure as we head towards the era of Exascale computing - termed SAGE (Percipient StorAGe for Exascale Data Centric Computing). The SAGE system will be capable of storing and processing immense volumes of data at the Exascale regime, and provide the capability for Exascale class applications to use such a storage infrastructure. SAGE addresses the increasing overlaps between Big Data Analysis and HPC in an era of next-generation data centric computing that has developed due to the proliferation of massive data sources, such as large, dispersed scientific instruments and sensors, whose data needs to be processed, analyzed and integrated into simulations to derive scientific and innovative insights. Indeed, Exascale I/O, as a problem that has not been sufficiently dealt with for simulation codes, is appropriately addressed by the SAGE platform. The objective of this paper is to discuss the software architecture of the SAGE system and look at early results we have obtained employing some of its key methodologies, as the system continues to evolve. △ Less","1 May, 2018",https://arxiv.org/pdf/1805.00556
A Cooperative Freeway Merge Assistance System using Connected Vehicles,Md Salman Ahmed;Mohammad A Hoque;Jackeline Rios-Torres;Asad Khattak,"The rapid growth of traffic-related fatalities and injuries around the world including developed countries has drawn researchers' attention for conducting research on automated highway systems to improve road safety over the past few years. In addition, fuel expenses due to traffic congestion in the U.S. translate to billions of dollars annually. These issues are motivating researchers across many disciplines to develop strategies to implement automation in transportation. The advent of connected-vehicle (CV) technology has added a new dimension to the research. The CV technology allows a vehicle to communicate with roadside infrastructure (vehicle-to-infrastructure), and other vehicles (vehicle-to-vehicle) on roads wirelessly using dedicated short-range communication (DSRC) protocol. Collectively, the vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communication technologies are known as V2X technology. Automotive companies have started to include On-Board Units (OBUs) on latest automobiles which can run safety-critical and assistive applications using V2X technology. For example, US Department of Transportation has already launched various applications including but not limited to lane-change assistance, collision avoidance, SPaT for emergency and transit vehicles. Merge conflicts, especially when vehicles are trying to merge from ramps to freeways, are a significant source of collisions, traffic congestion and fuel use. This paper describes a novel freeway merge assistance system utilizing V2X technology with the help of the DSRC protocol. The freeway merge assistance system uses an innovative three-way handshaking protocol and provides advisories to drivers to guide the merging sequence. △ Less","29 April, 2018",https://arxiv.org/pdf/1805.00508
Reflections on digital innovation,Eric Monteiro,"The paper by Henfridsson et al. opens up a new agenda for IS research on the content and process of digital innovation. The crucial element in their perspective is the role of recombination in innovation. They supplement an emphasis on design recombination with a symmetrical emphasis on use recombination. While supporting Henfridsson et al.s overall argument, I point out how central parts overlap with and are extended in disciplines outside IS research. △ Less","30 April, 2018",https://arxiv.org/pdf/1804.11280
Violence originated from Facebook: A case study in Bangladesh,Matiur Rahman Minar;Jibon Naher,"Facebook as in social network is a great innovation of modern times. Among all social networking sites, Facebook is the most popular social network all over the world. Bangladesh is no exception. People use Facebook for various reasons e.g. social networking and communication, online shopping and business, knowledge and experience sharing etc. However, some recent incidents in Bangladesh, originated from or based on Facebook activities, led to arson and violence. Social network i.e. Facebook was used in these incidents mostly as a tool to trigger hatred and violence. This case study discusses these technology related incidents and recommends possible future measurements to prevent such violence. △ Less","31 March, 2018",https://arxiv.org/pdf/1804.11241
Handling Uncertainty in Social Lending Credit Risk Prediction with a Choquet Fuzzy Integral Model,Anahita Namvar;Mohsen Naderpour,"As one of the main business models in the financial technology field, peer-to-peer (P2P) lending has disrupted traditional financial services by providing an online platform for lending money that has remarkably reduced financial costs. However, the inherent uncertainty in P2P loans can result in huge financial losses for P2P platforms. Therefore, accurate risk prediction is critical to the success of P2P lending platforms. Indeed, even a small improvement in credit risk prediction would be of benefit to P2P lending platforms. This paper proposes an innovative credit risk prediction framework that fuses base classifiers based on a Choquet fuzzy integral. Choquet integral fusion improves creditworthiness evaluations by synthesizing the prediction results of multiple classifiers and finding the largest consistency between outcomes among conflicting and consistent results. The proposed model was validated through experimental analysis on a real- world dataset from a well-known P2P lending marketplace. The empirical results indicate that the combination of multiple classifiers based on fuzzy Choquet integrals outperforms the best base classifiers used in credit risk prediction to date. In addition, the proposed methodology is superior to some conventional combination techniques. △ Less","28 April, 2018",https://arxiv.org/pdf/1804.10796
Neural Particle Smoothing for Sampling from Conditional Sequence Models,Chu-Cheng Lin;Jason Eisner,"We introduce neural particle smoothing, a sequential Monte Carlo method for sampling annotations of an input string from a given probability model. In contrast to conventional particle filtering algorithms, we train a proposal distribution that looks ahead to the end of the input string by means of a right-to-left LSTM. We demonstrate that this innovation can improve the quality of the sample. To motivate our formal choices, we explain how our neural model and neural sampler can be viewed as low-dimensional but nonlinear approximations to working with HMMs over very large state spaces. △ Less","28 April, 2018",https://arxiv.org/pdf/1804.10747
5PEN TECHNOLOGY: A New Dawn in Homogeneous and Heterogeneous Computing,Osagie Scale Uwadia Maxwell;K. O. Obahiagbon;Osagie Joy Amenze;John-Otumu M. A,"This research work is a pair review into the conceptual frame work and innovation into Pen-style Personal Network Gadget Package (P-ISM) as inevitable tool to easy, fast and convenient access to the internet. Computing activities have increased the degree of people using personal computers (PCs), complicated packages and all form of social media applications (Apps.) have emerged within this short period. Meeting these trends (day to day activities) in more convenient form has led to the modern sophisticated garget such as Pen-Style Network Gadget Package (P-ISM) prototype. The growth in internet affects our lives in much better way than we know and its sustainability made 5 pen technology innovations a salt after. △ Less","5 April, 2018",https://arxiv.org/pdf/1804.10651
Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning,Jinquan Sun;Yinghuan Shi;Yang Gao;Lei Wang;Luping Zhou;Wanqi Yang;Dinggang Shen,"Due to low tissue contrast, irregular object appearance, and unpredictable location variation, segmenting the objects from different medical imaging modalities (e.g., CT, MR) is considered as an important yet challenging task. In this paper, we present a novel method for interactive medical image segmentation with the following merits. (1) Our design is fundamentally different from previous pure patch-based and image-based segmentation methods. We observe that during delineation, the physician repeatedly check the inside-outside intensity changing to determine the boundary, which indicates that comparison in an inside-outside manner is extremely important. Thus, we innovatively model our segmentation task as learning the representation of the bi-directional sequential patches, starting from (or ending in) the given central point of the object. This can be realized by our proposed ConvRNN network embedded with a gated memory propagation unit. (2) Unlike previous interactive methods (requiring bounding box or seed points), we only ask the physician to merely click on the rough central point of the object before segmentation, which could simultaneously enhance the performance and reduce the segmentation time. (3) We utilize our method in a multi-level framework for better performance. We systematically evaluate our method in three different segmentation tasks including CT kidney tumor, MR prostate, and PROMISE12 challenge, showing promising results compared with state-of-the-art methods. The code is available here: \href{https://github.com/sunalbert/Sequential-patch-based-segmentation}{Sequential-patch-based-segmentation}. △ Less","8 May, 2018",https://arxiv.org/pdf/1804.10481
Taichi: An Open-Source Computer Graphics Library,Yuanming Hu,"An ideal software system in computer graphics should be a combination of innovative ideas, solid software engineering and rapid development. However, in reality these requirements are seldom met simultaneously. In this paper, we present early results on an open-source library named Taichi (http://taichi.graphics), which alleviates this practical issue by providing an accessible, portable, extensible, and high-performance infrastructure that is reusable and tailored for computer graphics. As a case study, we share our experience in building a novel physical simulation system using Taichi. △ Less","24 April, 2018",https://arxiv.org/pdf/1804.09293
SITAN: Services for Fault-Tolerant Ad Hoc Networks with Unknown Participants,David R. Matos;Nuno Neves;Alysson Bessani,"The evolution of mobile devices with various capabilities (e.g., smartphones and tablets), together with their ability to collaborate in impromptu ad hoc networks, opens new opportunities for the design of innovative distributed applications. The development of these applications needs to address several difficulties, such as the unreliability of the network, the imprecise set of participants, or the presence of malicious nodes. In this paper we describe a middleware, called SITAN, that offers a number of communication, group membership and coordination services specially conceived for these settings. These services are implemented by a stack of Byzantine fault-tolerant protocols, enabling applications that are built on top of them to operate correctly despite the uncertainty of the environment. The protocol stack was implemented in Android and NS-3, which allowed the experimentation in representative scenarios. Overall, the results show that the protocols are able to finish their execution within a small time window, which is acceptable for various kinds of applications. △ Less","25 April, 2018",https://arxiv.org/pdf/1804.09107
"Automated Network Service Scaling in NFV: Concepts, Mechanisms and Scaling Workflow",Oscar Adamuz-Hinojosa;Jose Ordonez-Lucena;Pablo Ameigeiras;Juan J. Ramos-Munoz;Diego Lopez;Jesus Folgueira,"Next-generation systems are anticipated to be digital platforms supporting innovative services with rapidly changing traffic patterns. To cope with this dynamicity in a cost-efficient manner, operators need advanced service management capabilities such as those provided by NFV. NFV enables operators to scale network services with higher granularity and agility than today. For this end, automation is key. In search of this automation, the European Telecommunications Standards Institute (ETSI) has defined a reference NFV framework that make use of model-driven templates called Network Service Descriptors (NSDs) to operate network services through their lifecycle. For the scaling operation, an NSD defines a discrete set of instantiation levels among which a network service instance can be resized throughout its lifecycle. Thus, the design of these levels is key for ensuring an effective scaling. In this article, we provide an overview of the automation of the network service scaling operation in NFV, addressing the options and boundaries introduced by ETSI normative specifications. We start by providing a description of the NSD structure, focusing on how instantiation levels are constructed. For illustrative purposes, we propose an NSD for a representative NS. This NSD includes different instantiation levels that enable different ways to automatically scale this NS. Then, we show the different scaling procedures the NFV framework has available, and how it may automate their triggering. Finally, we propose an ETSI-compliant workflow to describe in detail a representative scaling procedure. This workflow clarifies the interactions and information exchanges between the functional blocks in the NFV framework when performing the scaling operation. △ Less","15 June, 2018",https://arxiv.org/pdf/1804.09089
SolidWorx: A Resilient and Trustworthy Transactive Platform for Smart and Connected Communities,Scott Eisele;Aron Laszka;Anastasia Mavridou;Abhishek Dubey,"Internet of Things and data sciences are fueling the development of innovative solutions for various applications in Smart and Connected Communities (SCC). These applications provide participants with the capability to exchange not only data but also resources, which raises the concerns of integrity, trust, and above all the need for fair and optimal solutions to the problem of resource allocation. This exchange of information and resources leads to a problem where the stakeholders of the system may have limited trust in each other. Thus, collaboratively reaching consensus on when, how, and who should access certain resources becomes problematic. This paper presents SolidWorx, a blockchain-based platform that provides key mechanisms required for arbitrating resource consumption across different SCC applications in a domain-agnostic manner. For example, it introduces and implements a hybrid-solver pattern, where complex optimization computation is handled off-blockchain while solution validation is performed by a smart contract. To ensure correctness, the smart contract of SolidWorx is generated and verified. △ Less","24 April, 2018",https://arxiv.org/pdf/1804.08133
Expert Finding in Community Question Answering: A Review,Sha Yuan;Yu Zhang;Jie Tang;Juan Bautista Cabotà,"The rapid development recently of Community Question Answering (CQA) satisfies users quest for professional and personal knowledge about anything. In CQA, one central issue is to find users with expertise and willingness to answer the given questions. Expert finding in CQA often exhibits very different challenges compared to traditional methods. Sparse data and new features violate fundamental assumptions of traditional recommendation systems. This paper focuses on reviewing and categorizing the current progress on expert finding in CQA. We classify all the existing solutions into four different categories: matrix factorization based models (MF-based models), gradient boosting tree based models (GBT-based models), deep learning based models (DL-based models) and ranking based models (R-based models). We find that MF-based models outperform other categories of models in the field of expert finding in CQA. Moreover, we use innovative diagrams to clarify several important concepts of ensemble learning, and find that ensemble models with several specific single models can further boosting the performance. Further, we compare the performance of different models on different types of matching tasks, including text vs. text, graph vs. text, audio vs. text and video vs. text. The results can help the model selection of expert finding in practice. Finally, we explore some potential future issues in expert finding research in CQA. △ Less","21 April, 2018",https://arxiv.org/pdf/1804.07958
Anonymous Single-Sign-On for n designated services with traceability,Jinguang Han;Liqun Chen;Steve Schneider;Helen Treharne;Stephan Wesemeyer,"Anonymous Single-Sign-On authentication schemes have been proposed to allow users to access a service protected by a verifier without revealing their identity which has become more important due to the introduction of strong privacy regulations. In this paper we describe a new approach whereby anonymous authentication to different verifiers is achieved via authorisation tags and pseudonyms. The particular innovation of our scheme is authentication can only occur between a user and its designated verifier for a service, and the verification cannot be performed by any other verifier. The benefit of this authentication approach is that it prevents information leakage of a user's service access information, even if the verifiers for these services collude which each other. Our scheme also supports a trusted third party who is authorised to de-anonymise the user and reveal her whole services access information if required. Furthermore, our scheme is lightweight because it does not rely on attribute or policy-based signature schemes to enable access to multiple services. The scheme's security model is given together with a security proof, an implementation and a performance evaluation. △ Less","19 April, 2018",https://arxiv.org/pdf/1804.07201
Ambient Assisted Living technologies from the perspectives of older people and professionals,Deepika Singh;Johannes Kropf;Sten Hanke;Andreas Holzinger,"Ambient Assisted Living (AAL) and Ambient Intelligence technologies are providing support to older people in living an independent and confident life by developing innovative ICT-based products, services, and systems. Despite significant advancement in AAL technologies and smart systems, they have still not found the way into the nursing home of the older people. The reasons are manifold. On one hand, the development of such systems lack in addressing the requirements of the older people and caregivers of the organization and the other is the unwillingness of the older people to make use of assistive systems. A qualitative study was performed at a nursing home to understand the needs and requirements of the residents and caregivers and their perspectives about the existing AAL technologies. △ Less","19 April, 2018",https://arxiv.org/pdf/1804.07151
Volur: Concurrent Edge/Core Route Control in Data Center Networks,Qiao Zhang;Danyang Zhuo;Vincent Liu;Petr Lapukhov;Simon Peter;Arvind Krishnamurthy;Thomas Anderson,"A perennial question in computer networks is where to place functionality among components of a distributed computer system. In data centers, one option is to move all intelligence to the edge, essentially relegating switches and middleboxes, regardless of their programmability, to simple static routing policies. Another is to add more intelligence to the middle of the network in the hopes that it can handle any issue that arises. This paper presents an architecture, called Volur, that provides a third option by facilitating the co-existence of an intelligent network with an intelligent edge. The key architectural principle of Volur is predictability of the network. We describe the key design requirements, and show through case studies how our approach facilitates more democratic innovation of all parts of the network. We also demonstrate the practicality of our architecture by describing how to implement the architecture on top of existing hardware and by deploying a prototype on top of a large production data center. △ Less","18 April, 2018",https://arxiv.org/pdf/1804.06945
Diachronic Usage Relatedness (DURel): A Framework for the Annotation of Lexical Semantic Change,Dominik Schlechtweg;Sabine Schulte im Walde;Stefanie Eckmann,"We propose a framework that extends synchronic polysemy annotation to diachronic changes in lexical meaning, to counteract the lack of resources for evaluating computational models of lexical semantic change. Our framework exploits an intuitive notion of semantic relatedness, and distinguishes between innovative and reductive meaning changes with high inter-annotator agreement. The resulting test set for German comprises ratings from five annotators for the relatedness of 1,320 use pairs across 22 target words. △ Less","17 April, 2018",https://arxiv.org/pdf/1804.06517
The emergent integrated network structure of scientific research,Jordan D. Dworkin;Russell T. Shinohara;Danielle S. Bassett,"The practice of scientific research is often thought of as individuals and small teams striving for disciplinary advances. Yet as a whole, this endeavor more closely resembles a complex system of natural computation, in which information is obtained, generated, and disseminated more effectively than would be possible by individuals acting in isolation. Currently, the structure of this integrated and innovative landscape of scientific ideas is not well understood. Here we use tools from network science to map the landscape of interconnected research topics covered in the multidisciplinary journal PNAS since 2000. We construct networks in which nodes represent topics of study and edges give the degree to which topics occur in the same papers. The network displays small-world architecture, with dense connectivity within scientific clusters and sparse connectivity between clusters. Notably, clusters tend not to align with assigned article classifications, but instead contain topics from various disciplines. Using a temporal graph, we find that small-worldness has increased over time, suggesting growing efficiency and integration of ideas. Finally, we define a novel measure of interdisciplinarity, which is positively associated with PNAS's impact factor. Broadly, this work suggests that complex and dynamic patterns of knowledge emerge from scientific research, and that structures reflecting intellectual integration may be beneficial for obtaining scientific insight. △ Less","17 April, 2018",https://arxiv.org/pdf/1804.06434
Artificial Plants - Vascular Morphogenesis Controller-guided growth of braided structures,Daniel Nicolas Hofstadler;Joshua Cherian Varughese;Stig Anton Nielsen;David Andres Leon;Phil Ayres;Payam Zahadat;Thomas Schmickl,"Natural plants are exemplars of adaptation through self-organisation and collective decision making. As such, they provide a rich source of inspiration for adaptive mechanisms in artificial systems. Plant growth - a structure development mechanism of continuous material accumulation that expresses encoded morphological features through environmental interactions - has been extensively explored in-silico. However, ex-silico scalable morphological adaptation through material accumulation remains an open challenge. In this paper, we present a novel type of biologically inspired modularity, and an approach to artificial growth that combines the benefits of material continuity through braiding with a distributed and decentralised plant-inspired Vascular Morphogenesis Controller (VMC). The controller runs on nodes that are capable of sensing and communicating with their neighbours. The nodes are embedded within the braided structure, which can be morphologically adapted based on collective decision making between nodes. Human agents realise the material adaptation by physically adding to the braided structure according to the suggestion of the embedded controller. This work offers a novel, tangible and accessible approach to embedding mechanisms of artificial growth and morphological adaptation within physically embodied systems, offering radically new functionalities, innovation potentials and approaches to continuous autonomous or steered design that could find application within fields contributing to the built environment, such as Architecture. △ Less","17 April, 2018",https://arxiv.org/pdf/1804.06343
Parallel Complexity Analysis with Temporal Session Types,Ankush Das;Jan Hoffmann;Frank Pfenning,"We study the problem of parametric parallel complexity analysis of concurrent, message-passing programs. To make the analysis local and compositional, it is based on a conservative extension of binary session types, which structure the type and direction of communication between processes and stand in a Curry-Howard correspondence with intuitionistic linear logic. The main innovation is to enrich session types with the temporal modalities next (\bigcirc A), always (\Box A), and eventually (\Diamond A), to additionally prescribe the timing of the exchanged messages in a way that is precise yet flexible. The resulting temporal session types uniformly express properties such as the message rate of a stream, the latency of a pipeline, the response time of a concurrent queue, or the span of a fork/join parallel program. The analysis is parametric in the cost model and the presentation focuses on communication cost as a concrete example. The soundness of the analysis is established by proofs of progress and type preservation using a timed multiset rewriting semantics. Representative examples illustrate the scope and usability of the approach. △ Less","16 April, 2018",https://arxiv.org/pdf/1804.06013
And Now for Something Completely Different: Visual Novelty in an Online Network of Designers,Johannes Wachs;Bálint Daróczy;Anikó Hannák;Katinka Páll;Christoph Riedl,"Novelty is a key ingredient of innovation but quantifying it is difficult. This is especially true for visual work like graphic design. Using designs shared on an online social network of professional digital designers, we measure visual novelty using statistical learning methods to compare an images features with those of images that have been created before. We then relate social network position to the novelty of the designers images. We find that on this professional platform, users with dense local networks tend to produce more novel but generally less successful images, with important exceptions. Namely, users making novel images while embedded in cohesive local networks are more successful. △ Less","23 April, 2018",https://arxiv.org/pdf/1804.05705
CoCo: Compact and Optimized Consolidation of Modularized Service Function Chains in NFV,Zili Meng;Jun Bi;Haiping Wang;Chen Sun;Hongxin Hu,"The modularization of Service Function Chains (SFCs) in Network Function Virtualization (NFV) could introduce significant performance overhead and resource efficiency degradation due to introducing frequent packet transfer and consuming much more hardware resources. In response, we exploit the lightweight and individually scalable features of elements in Modularized SFCs (MSFCs) and propose CoCo, a compact and optimized consolidation framework for MSFC in NFV. CoCo addresses the above problems in two ways. First, CoCo Optimized Placer pays attention to the problem of which elements to consolidate and provides a performance-aware placement algorithm to place MSFCs compactly and optimize the global packet transfer cost. Second, CoCo Individual Scaler innovatively introduces a push-aside scaling up strategy to avoid degrading performance and taking up new CPU cores. To support MSFC consolidation, CoCo also provides an automatic runtime scheduler to ensure fairness when elements are consolidated on CPU core. Our evaluation results show that CoCo achieves significant performance improvement and efficient resource utilization. △ Less","14 August, 2018",https://arxiv.org/pdf/1804.05468
Latent Geometry Inspired Graph Dissimilarities Enhance Affinity Propagation Community Detection in Complex Networks,Carlo Vittorio Cannistraci;Alessandro Muscoloni,"Affinity propagation is one of the most effective unsupervised pattern recognition algorithms for data clustering in high-dimensional feature space. However, the numerous attempts to test its performance for community detection in complex networks have been attaining results very far from the state of the art methods such as Infomap and Louvain. Yet, all these studies agreed that the crucial problem is to convert the unweighted network topology in a 'smart-enough' node dissimilarity matrix that is able to properly address the message passing procedure behind affinity propagation clustering. Here we introduce a conceptual innovation and we discuss how to leverage network latent geometry notions in order to design dissimilarity matrices for affinity propagation community detection. Our results demonstrate that the latent geometry inspired dissimilarity measures we design bring affinity propagation to equal or outperform current state of the art methods for community detection. These findings are solidly proven considering both synthetic 'realistic' networks (with known ground-truth communities) and real networks (with community metadata), even when the data structure is corrupted by noise artificially induced by missing or spurious connectivity. △ Less","29 August, 2018",https://arxiv.org/pdf/1804.04566
Nonlinear 3D Face Morphable Model,Luan Tran;Xiaoming Liu,"As a classic statistical model of 3D facial shape and texture, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of well-controlled 2D face images with associated 3D face scans, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of unconstrained face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, shape and texture parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and texture parameters to the 3D shape and texture, respectively. With the projection parameter, 3D shape, and texture, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment and 3D reconstruction. △ Less","26 August, 2018",https://arxiv.org/pdf/1804.03786
Enhancing Cybersecurity Skills by Creating Serious Games,Valdemar Švábenský;Jan Vykopal;Milan Cermak;Martin Laštovička,"Adversary thinking is an essential skill for cybersecurity experts, enabling them to understand cyber attacks and set up effective defenses. While this skill is commonly exercised by Capture the Flag games and hands-on activities, we complement these approaches with a key innovation: undergraduate students learn methods of network attack and defense by creating educational games in a cyber range. In this paper, we present the design of two courses, instruction and assessment techniques, as well as our observations over the last three semesters. The students report they had a unique opportunity to deeply understand the topic and practice their soft skills, as they presented their results at a faculty open day event. Their peers, who played the created games, rated the quality and educational value of the games overwhelmingly positively. Moreover, the open day raised awareness about cybersecurity and research and development in this field at our faculty. We believe that sharing our teaching experience will be valuable for instructors planning to introduce active learning of cybersecurity and adversary thinking. △ Less","3 April, 2018",https://arxiv.org/pdf/1804.03567
"Graph based Platform for Electricity Market Study, Education and Training",Tao Chen;Chen Yuan;Guangyi Liu;Renchang Dai,"With the further development of deregulated electricity market in many other countries around the world, a lot of challenges have been identified for market data management, network topology processing and fast market-clearance mechanism design. In this paper, a graph computing framework based on TigerGraph database is proposed to solve a security constrained unit commitment (SCUC) and security constrained economic dispatch (SCED) problem, with parallelized graph power flow (PGPF) and innovative LU decomposition techniques, for electricity market-clearance. It also provides a comprehensive visualization platform to demonstrate the market clearing results vividly, such as locational marginal price (LMP), and is able to be utilized for electricity market operators' education and training purpose. △ Less","3 April, 2018",https://arxiv.org/pdf/1804.03517
Novelty and Foreseeing Research Trends; The Case of Astrophysics and Astronomy,Attila Varga,"Metrics based on reference lists of research articles or on keywords have been used to predict citation impact. The concept behind such metrics is that original ideas stem from the reconfiguration of the structure of past knowledge, and therefore atypical combinations in the reference lists, keywords, or classification codes indicate future high impact research. The current paper serves as an introduction to this line of research for astronomers and also addresses some methodological questions of this field of innovation studies. It is still not clear if the choice of particular indexes, such as references to journals, articles, or specific bibliometric classification codes would affect the relationship between atypical combinations and citation impact. To understand more aspects of the innovation process, a new metric has been devised to measure to what extent researchers are able to anticipate the changing combinatorial trends of the future. Results show that the variant of the latter anticipation scores that is based on paper combinations is a good predictor of future citation impact of scholarly works. The study also shows that the effect of tested indexes vary with the aggregation level that was used to construct them. A detailed analysis of combinatorial novelty in the field reveals that certain sub-fields of astronomy and astrophysics have different roles in the reconfiguration in past knowledge. △ Less","8 April, 2018",https://arxiv.org/pdf/1804.02773
A Machine Learning Approach To Prevent Malicious Calls Over Telephony Networks,Huichen Li;Xiaojun Xu;Chang Liu;Teng Ren;Kun Wu;Xuezhi Cao;Weinan Zhang;Yong Yu;Dawn Song,"Malicious calls, i.e., telephony spams and scams, have been a long-standing challenging issue that causes billions of dollars of annual financial loss worldwide. This work presents the first machine learning-based solution without relying on any particular assumptions on the underlying telephony network infrastructures. The main challenge of this decade-long problem is that it is unclear how to construct effective features without the access to the telephony networks' infrastructures. We solve this problem by combining several innovations. We first develop a TouchPal user interface on top of a mobile App to allow users tagging malicious calls. This allows us to maintain a large-scale call log database. We then conduct a measurement study over three months of call logs, including 9 billion records. We design 29 features based on the results, so that machine learning algorithms can be used to predict malicious calls. We extensively evaluate different state-of-the-art machine learning approaches using the proposed features, and the results show that the best approach can reduce up to 90% unblocked malicious calls while maintaining a precision over 99.99% on the benign call traffic. The results also show the models are efficient to implement without incurring a significant latency overhead. We also conduct ablation analysis, which reveals that using 10 out of the 29 features can reach a performance comparable to using all features. △ Less","7 April, 2018",https://arxiv.org/pdf/1804.02566
"e-SAFE: Secure, Efficient and Forensics-Enabled Access to Implantable Medical Devices",Haotian Chi;Longfei Wu;Xiaojiang Du;Qiang Zeng;Paul Ratazzi,"To facilitate monitoring and management, modern Implantable Medical Devices (IMDs) are often equipped with wireless capabilities, which raise the risk of malicious access to IMDs. Although schemes are proposed to secure the IMD access, some issues are still open. First, pre-sharing a long-term key between a patient's IMD and a doctor's programmer is vulnerable since once the doctor's programmer is compromised, all of her patients suffer; establishing a temporary key by leveraging proximity gets rid of pre-shared keys, but as the approach lacks real authentication, it can be exploited by nearby adversaries or through man-in-the-middle attacks. Second, while prolonging the lifetime of IMDs is one of the most important design goals, few schemes explore to lower the communication and computation overhead all at once. Finally, how to safely record the commands issued by doctors for the purpose of forensics, which can be the last measure to protect the patients' rights, is commonly omitted in the existing literature. Motivated by these important yet open problems, we propose an innovative scheme e-SAFE, which significantly improves security and safety, reduces the communication overhead and enables IMD-access forensics. We present a novel lightweight compressive sensing based encryption algorithm to encrypt and compress the IMD data simultaneously, reducing the data transmission overhead by over 50% while ensuring high data confidentiality and usability. Furthermore, we provide a suite of protocols regarding device pairing, dual-factor authentication, and accountability-enabled access. The security analysis and performance evaluation show the validity and efficiency of the proposed scheme. △ Less","6 April, 2018",https://arxiv.org/pdf/1804.02447
Developing a K-ary malware using Blockchain,Joanna Moubarak;Eric Filiol;Maroun Chamoun,"Cyberattacks are nowadays moving rapidly. They are customized, multi-vector, staged in multiple flows and targeted. Moreover, new hacking playgrounds appeared to reach mobile network, modern architectures and smart cities. For that purpose, malware use different entry points and plug-ins. In addition, they are now deploying several techniques for obfuscation, camouflage and analysis resistance. On the other hand, antiviral protections are positioning innovative approaches exposing malicious indicators and anomalies, revealing assumptions of the limitations of the anti-antiviral mechanisms. Primarily, this paper exposes a state of art in computer virology and then introduces a new concept to create undetectable malware based on the blockchain technology. It summarizes techniques adopted by malicious software to avoid functionalities implemented for viral detection and presents the implementation of new viral techniques that leverage the blockchain network. △ Less","4 April, 2018",https://arxiv.org/pdf/1804.01488
Virtualized Application Function Chaining: Maximizing the Wearable System Lifetime,Harini Kolamunna;Kanchana Thilakarathna;Aruna Seneviratne,"The number of smart devices wear and carry by users is growing rapidly which is driven by innovative new smart wearables and interesting service o erings. This has led to applications that utilize multiple devices around the body to provide immersive environments such as mixed reality. These applications rely on a number of di erent types of functions such as sensing, communication and various types of processing, that require considerable resources. Thus one of the major challenges in supporting of these applications is dependent on the battery lifetime of the devices that provide the necessary functionality. The battery lifetime can be extended by either incorporating a battery with larger capacity and/or by utilizing the available resources e ciently. However, the increases in battery capacity are not keeping up with the demand and larger batteries add to both the weight and size of the device. Thus, the focus of this paper is to improve the battery e ciency through intelligent resources utilization. We show that, when the same resource is available on multiple devices that form part of the wearable system, and or is in close proximity, it is possible consider them as a resource pool and further utilize them intelligently to improve the system lifetime. Speci cally, we formulate the function allocation algorithm as a Mixed Integer Linear Programming (MILP) optimization problem and propose an e cient heuristic solution. The experimental data driven simulation results show that approximately 40-50% system battery life improvement can be achieved with proper function allocation and orchestration. △ Less","2 April, 2018",https://arxiv.org/pdf/1804.00739
The Maximum Trajectory Coverage Query in Spatial Databases,Mohammed Eunus Ali;Kaysar Abdullah;Shadman Saqib Eusuf;Farhana M. Choudhury;J. Shane Culpepper;Timos Sellis,"With the widespread use of GPS-enabled mobile devices, an unprecedented amount of trajectory data is becoming available from various sources such as Bikely, GPS-wayPoints, and Uber. The rise of innovative transportation services and recent break-throughs in autonomous vehicles will lead to the continued growth of trajectory data and related applications. Supporting these services in emerging platforms will require more efficient query processing in trajectory databases. In this paper, we propose two new coverage queries for trajectory databases: (i) k Maximizing Reverse Range Search on Trajectories (kMaxRRST); and (ii) a Maximum k Coverage Range Search on Trajectories (MaxkCovRST). We propose a novel index structure, the Trajectory Quadtree (TQ-tree) that utilizes a quadtree to hierarchically organize trajectories into different quadtree nodes, and then applies a z-ordering to further organize the trajectories by spatial locality inside each node. This structure is highly effective in pruning the trajectory search space, which is of independent interest. By exploiting the TQ-tree data structure, we develop a divide-and-conquer approach to compute the trajectory ""service value"", and a best-first strategy to explore the trajectories using the appropriate upper bound on the service value to efficiently process a kMaxRRST query. Moreover, to solve the MaxkCovRST, which is a non-submodular NP-hard problem, we propose a greedy approximation which also exploits the TQ-tree. We evaluate our algorithms through an extensive experimental study on several real datasets, and demonstrate that our TQ-tree based algorithms outperform common baselines by two to three orders of magnitude. △ Less","2 April, 2018",https://arxiv.org/pdf/1804.00599
"Accelerating Materials Development via Automation, Machine Learning, and High-Performance Computing",Juan Pablo Correa-Baena;Kedar Hippalgaonkar;Jeroen van Duren;Shaffiq Jaffer;Vijay R. Chandrasekhar;Vladan Stevanovic;Cyrus Wadia;Supratik Guha;Tonio Buonassisi,"Successful materials innovations can transform society. However, materials research often involves long timelines and low success probabilities, dissuading investors who have expectations of shorter times from bench to business. A combination of emergent technologies could accelerate the pace of novel materials development by 10x or more, aligning the timelines of stakeholders (investors and researchers), markets, and the environment, while increasing return-on-investment. First, tool automation enables rapid experimental testing of candidate materials. Second, high-throughput computing (HPC) concentrates experimental bandwidth on promising compounds by predicting and inferring bulk, interface, and defect-related properties. Third, machine learning connects the former two, where experimental outputs automatically refine theory and help define next experiments. We describe state-of-the-art attempts to realize this vision and identify resource gaps. We posit that over the coming decade, this combination of tools will transform the way we perform materials research. There are considerable first-mover advantages at stake, especially for grand challenges in energy and related fields, including computing, healthcare, urbanization, water, food, and the environment. △ Less","20 March, 2018",https://arxiv.org/pdf/1803.11246
Fine-Grained Energy and Performance Profiling framework for Deep Convolutional Neural Networks,Crefeda Faviola Rodrigues;Graham Riley;Mikel Lujan,"There is a huge demand for on-device execution of deep learning algorithms on mobile and embedded platforms. These devices present constraints on the application due to limited resources and power. Hence, developing energy-efficient solutions to address this issue will require innovation in algorithmic design, software and hardware. Such innovation requires benchmarking and characterization of Deep Neural Networks based on performance and energy-consumption alongside accuracy. However, current benchmarks studies in existing deep learning frameworks (for example, Caffe, Tensorflow, Torch and others) are based on performance of these applications on high-end CPUs and GPUs. In this work, we introduce a benchmarking framework called ""SyNERGY"" to measure the energy and time of 11 representative Deep Convolutional Neural Networks on embedded platforms such as NVidia Jetson TX1. We integrate ARM's Streamline Performance Analyser with standard deep learning frameworks such as Caffe and CuDNNv5, to study the execution behaviour of current deep learning models at a fine-grained level (or specific layers) on image processing tasks. In addition, we build an initial multi-variable linear regression model to predict energy consumption of unseen neural network models based on the number of SIMD instructions executed and main memory accesses of the CPU cores of the TX1 with an average relative test error rate of 8.04 +/- 5.96 %. Surprisingly, we find that it is possible to refine the model to predict the number of SIMD instructions and main memory accesses solely from the application's Multiply-Accumulate (MAC) counts, hence, eliminating the need for actual measurements. Our predicted results demonstrate 7.08 +/- 6.0 % average relative error over actual energy measurements of all 11 networks tested, except MobileNet. By including MobileNet the average relative test error increases to 17.33 +/- 12.2 %. △ Less","14 May, 2018",https://arxiv.org/pdf/1803.11151
New Symmetric and Planar Designs of Reversible Full-Adders/Subtractors in Quantum-Dot Cellular Automata,Moein Sarvaghad-Moghaddam;Ali A. Orouji,"Quantum-dot Cellular Automata (QCA) is one of the emerging nanotechnologies, promising alternative to CMOS technology due to faster speed, smaller size, lower power consumption, higher scale integration and higher switching frequency. Also, power dissipation is the main limitation of all the nano electronics design techniques including the QCA. Researchers have proposed the various mechanisms to limit this problem. Among them, reversible computing is considered as the reliable solution to lower the power dissipation. On the other hand, adders are fundamental circuits for most digital systems. In this paper, Innovation is divided to three sections. In the first section, a method for converting irreversible functions to a reversible one is presented. This method has advantages such as: converting of irreversible functions to reversible one directly and as optimal. So, in this method, sub-optimal methods of using of conventional reversible blocks such as Toffoli and Fredkin are not used, having of minimum number of garbage outputs and so on. Then, Using the method, two new symmetric and planar designs of reversible full-adders are presented. In the second section, a new symmetric, planar and fault tolerant five-input majority gate is proposed. Based on the designed gate, a reversible full-adder are presented. Also, for this gate, a fault-tolerant analysis is proposed. And in the third section, three new 8-bit reversible full-adder/subtractors are designed based on full-adders/subtractors proposed in the second section. The results are indicative of the outperformance of the proposed designs in comparison to the best available ones in terms of area, complexity, delay, reversible/irreversible layout, and also in logic level in terms of garbage outputs, control inputs, number of majority and NOT gates. △ Less","29 March, 2018",https://arxiv.org/pdf/1803.11016
"Fast, Flexible, Polyglot Instrumentation Support for Debuggers and other Tools",Michael Van De Vanter;Chris Seaton;Michael Haupt;Christian Humer;Thomas Würthinger,"Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations. Truffle's new Instrumentation Framework is language-agnostic and designed to derive high performance from the same technologies as do language implementations. Truffle Instrumentation includes: (1) low overhead capture of execution events by dynamically adding ""wrapper"" nodes to executing ASTs; (2) extensions to the Language Implementation Framework that allow per-language specialization, primarily for visual display of values and names, among others; and (3) versatile APIs and support services for implementing many kinds of tools without VM modification. Knowledge: It is now possible for a client in a production environment to insert (dynamically, with thread safety) an instrumentation probe that captures and reports abstractly specified execution events. A probe in fully optimized code imposes very low overhead until actually used to access (or modify) execution state. Event capture has enabled construction of numerous GraalVM services and tools that work for all implemented languages, either singly or in combination. Instrumentation has also proved valuable for implementing some traditionally tricky language features, as well as some GraalVM services such as placing bounds on resources consumed by running programs. Grounding: Tools for debugging (via multiple clients), profiling, statement counting, dynamic analysis, and others are now part of GraalVM or are in active development. Third parties have also used Truffle Instrumentation for innovative tool implementations. Importance: Experience with Truffle Instrumentation validates the notion that addressing developer tools support as a forethought can change expectations about the availability of practical, efficient tools for high-performance languages. Tool development becomes a natural part of language implementation, requiring little additional effort and offering the advantage of early and continuous availability. △ Less","27 March, 2018",https://arxiv.org/pdf/1803.10201
Proactive Empirical Assessment of New Language Feature Adoption via Automated Refactoring: The Case of Java 8 Default Methods,Raffi Khatchadourian;Hidehiko Masuhara,"Programming languages and platforms improve over time, sometimes resulting in new language features that offer many benefits. However, despite these benefits, developers may not always be willing to adopt them in their projects for various reasons. In this paper, we describe an empirical study where we assess the adoption of a particular new language feature. Studying how developers use (or do not use) new language features is important in programming language research and engineering because it gives designers insight into the usability of the language to create meaning programs in that language. This knowledge, in turn, can drive future innovations in the area. Here, we explore Java 8 default methods, which allow interfaces to contain (instance) method implementations. Default methods can ease interface evolution, make certain ubiquitous design patterns redundant, and improve both modularity and maintainability. A focus of this work is to discover, through a scientific approach and a novel technique, situations where developers found these constructs useful and where they did not, and the reasons for each. Although several studies center around assessing new language features, to the best of our knowledge, this kind of construct has not been previously considered. Despite their benefits, we found that developers did not adopt default methods in all situations. Our study consisted of submitting pull requests introducing the language feature to 19 real-world, open source Java projects without altering original program semantics. This novel assessment technique is proactive in that the adoption was driven by an automatic refactoring approach rather than waiting for developers to discover and integrate the feature themselves. In this way, we set forth best practices and patterns of using the language feature effectively earlier rather than later and are able to possibly guide (near) future language evolution. We foresee this technique to be useful in assessing other new language features, design patterns, and other programming idioms. △ Less","27 March, 2018",https://arxiv.org/pdf/1803.10198
Manufacturing processes hardware and software development to implement innovative technologies of aircraft manufacturing facilities management,S. E. Pyatovsky;A. N. Serdyuchenko,"The paper presents approaches to modern aircraft manufacturing facilities' competitiveness growth based on innovative technologies of management decisions implementation. The paper establishes a connection between the aircraft industry and the development of the national economy, along with the dependence of the Russian economy on international manufacturers of civil airplanes. Comparative statistics of civil aircrafts produced by Russian and international manufacturers are given. Comparative analysis of military and civil aviation is done. It is shown that project implementation based on Open Source and OLAP-technologies at a high-technology enterprise is a pre-requisite for the competitive growth of aircraft manufacturing facilities. △ Less","27 March, 2018",https://arxiv.org/pdf/1803.10017
Hiding in the Crowd: A Massively Distributed Algorithm for Private Averaging with Malicious Adversaries,Pierre Dellenbach;Aurélien Bellet;Jan Ramon,"The amount of personal data collected in our everyday interactions with connected devices offers great opportunities for innovative services fueled by machine learning, as well as raises serious concerns for the privacy of individuals. In this paper, we propose a massively distributed protocol for a large set of users to privately compute averages over their joint data, which can then be used to learn predictive models. Our protocol can find a solution of arbitrary accuracy, does not rely on a third party and preserves the privacy of users throughout the execution in both the honest-but-curious and malicious adversary models. Specifically, we prove that the information observed by the adversary (the set of maliciours users) does not significantly reduce the uncertainty in its prediction of private values compared to its prior belief. The level of privacy protection depends on a quantity related to the Laplacian matrix of the network graph and generally improves with the size of the graph. Furthermore, we design a verification procedure which offers protection against malicious users joining the service with the goal of manipulating the outcome of the algorithm. △ Less","27 March, 2018",https://arxiv.org/pdf/1803.09984
The Future of CISE Distributed Research Infrastructure,Jay Aikat;Ilya Baldin;Mark Berman;Joe Breen;Richard Brooks;Prasad Calyam;Jeff Chase;Wallace Chase;Russ Clark;Chip Elliott;Jim Griffioen;Dijiang Huang;Julio Ibarra;Tom Lehman;Inder Monga;Abrahim Matta;Christos Papadopoulos;Mike Reiter;Dipankar Raychaudhuri;Glenn Ricart;Robert Ricci;Paul Ruth;Ivan Seskar;Jerry Sobieski;Kobus Van der Merwe,"Shared research infrastructure that is globally distributed and widely accessible has been a hallmark of the networking community. This paper presents an initial snapshot of a vision for a possible future of mid-scale distributed research infrastructure aimed at enabling new types of research and discoveries. The paper is written from the perspective of ""lessons learned"" in constructing and operating the Global Environment for Network Innovations (GENI) infrastructure and attempts to project future concepts and solutions based on these lessons. The goal of this paper is to engage the community to contribute new ideas and to inform funding agencies about future research directions to realize this vision. △ Less","27 March, 2018",https://arxiv.org/pdf/1803.09886
A multilayer backpropagation saliency detection algorithm and its applications,Chunbiao Zhu;Ge Li,"Saliency detection is an active topic in the multimedia field. Most previous works on saliency detection focus on 2D images. However, these methods are not robust against complex scenes which contain multiple objects or complex backgrounds. Recently, depth information supplies a powerful cue for saliency detection. In this paper, we propose a multilayer backpropagation saliency detection algorithm based on depth mining by which we exploit depth cue from three different layers of images. The proposed algorithm shows a good performance and maintains the robustness in complex situations. Experiments' results show that the proposed framework is superior to other existing saliency approaches. Besides, we give two innovative applications by this algorithm, such as scene reconstruction from multiple images and small target object detection in video. △ Less","26 March, 2018",https://arxiv.org/pdf/1803.09659
"Go-Smart: Open-Ended, Web-Based Modelling of Minimally Invasive Cancer Treatments via a Clinical Domain Approach",Phil Weir;Roland Ellerweg;Stephen Payne;Dominic Reuter;Tuomas Alhonnoro;Philip Voglreiter;Panchatcharam Mariappan;Mika Pollari;Chang Sub Park;Peter Voigt;Tim van Oostenbrugge;Sebastian Fischer;Peter Kalmar;Jurgen Futterer;Philipp Stiegler;Stephan Zangos;Ronan Flanagan;Michael Moche;Marina Kolesnik,"Clinicians benefit from online treatment planning systems, through off-site accessibility, data sharing and professional interaction. As well as enhancing clinical value, incorporation of simulation tools affords innovative avenues for open-ended, multi-disciplinary research collaboration. An extensible system for clinicians, technicians, manufacturers and researchers to build on a simulation framework is presented. This is achieved using a domain model that relates entities from theoretical, engineering and clinical domains, allowing algorithmic generation of simulation configuration for several open source solvers. The platform is applied to Minimally Invasive Cancer Treatments (MICTs), allowing interventional radiologists to upload patient data, segment patient images and validate simulated treatments of radiofrequency ablation, cryoablation, microwave ablation and irreversible electroporation. A traditional radiology software layout is provided in-browser for clinical use, with simple, guided simulation, primarily for training and research. Developers and manufacturers access a web-based system to manage their own simulation components (equipment, numerical models and clinical protocols) and related parameters. This system is tested by interventional radiologists at four centres, using pseudonymized patient data, as part of the Go-Smart Project (http://gosmart-project.eu). The simulation technology is released as a set of open source components http://github.com/go-smart. △ Less","24 March, 2018",https://arxiv.org/pdf/1803.09166
A distance-based tool-set to track inconsistent urban structures through complex-networks,Gabriel Spadon;Bruno B. Machado;Danilo M. Eler;Jose Fernando Rodrigues-Jr,"Complex networks can be used for modeling street meshes and urban agglomerates. With such a model, many aspects of a city can be investigated to promote a better quality of life to its citizens. Along these lines, this paper proposes a set of distance-based pattern-discovery algorithmic instruments to improve urban structures modeled as complex networks, detecting nodes that lack access from/to points of interest in a given city. Furthermore, we introduce a greedy algorithm that is able to recommend improvements to the structure of a city by suggesting where points of interest are to be placed. We contribute to a thorough process to deal with complex networks, including mathematical modeling and algorithmic innovation. The set of our contributions introduces a systematic manner to treat a recurrent problem of broad interest in cities. △ Less","24 March, 2018",https://arxiv.org/pdf/1803.09136
2CoBel : An Efficient Belief Function Extension for Two-dimensional Continuous Spaces,Nicola Pellicanò;Sylvie Le Hégarat-Mascle;Emanuel Aldea,"This paper introduces an innovative approach for handling 2D compound hypotheses within the Belief Function Theory framework. We propose a polygon-based generic rep- resentation which relies on polygon clipping operators. This approach allows us to account in the computational cost for the precision of the representation independently of the cardinality of the discernment frame. For the BBA combination and decision making, we propose efficient algorithms which rely on hashes for fast lookup, and on a topological ordering of the focal elements within a directed acyclic graph encoding their interconnections. Additionally, an implementation of the functionalities proposed in this paper is provided as an open source library. Experimental results on a pedestrian localization problem are reported. The experiments show that the solution is accurate and that it fully benefits from the scalability of the 2D search space granularity provided by our representation. △ Less","23 March, 2018",https://arxiv.org/pdf/1803.08857
The ARM Scalable Vector Extension,Nigel Stephens;Stuart Biles;Matthias Boettcher;Jacob Eapen;Mbou Eyole;Giacomo Gabrielli;Matt Horsnell;Grigorios Magklis;Alejandro Martinez;Nathanael Premillieu;Alastair Reid;Alejandro Rico;Paul Walker,"This article describes the ARM Scalable Vector Extension (SVE). Several goals guided the design of the architecture. First was the need to extend the vector processing capability associated with the ARM AArch64 execution state to better address the computational requirements in domains such as high-performance computing, data analytics, computer vision, and machine learning. Second was the desire to introduce an extension that can scale across multiple implementations, both now and into the future, allowing CPU designers to choose the vector length most suitable for their power, performance, and area targets. Finally, the architecture should avoid imposing a software development cost as the vector length changes and where possible reduce it by improving the reach of compiler auto-vectorization technologies. SVE achieves these goals. It allows implementations to choose a vector register length between 128 and 2,048 bits. It supports a vector-length agnostic programming model that lets code run and scale automatically across all vector lengths without recompilation. Finally, it introduces several innovative features that begin to overcome some of the traditional barriers to autovectorization. △ Less","16 March, 2018",https://arxiv.org/pdf/1803.06185
Image Registration Based Flicker Solving in Video Face Replacement and Analysis Based Sub-pixel Image Registration,Xiaofang Wang;Guoqiang Xiang;Xinyue Zhang;Wei Wei,"In this paper, a framework of video face replacement is proposed and it deals with the flicker of swapped face in video sequence. This framework contains two main innovations: 1) the technique of image registration is exploited to align the source and target video faces for eliminating the flicker or jitter of the segmented video face sequence; 2) a fast subpixel image registration method is proposed for farther accuracy and efficiency. Unlike the priori works, it minimizes the overlapping region and takes spatiotemporal coherence into account. Flicker in resulted videos is usually caused by the frequently changed bound of the blending target face and unregistered faces between and along video sequences. The subpixel image registration method is proposed to solve the flicker problem. During the alignment process, integer pixel registration is formulated by maximizing the similarity of images with down sampling strategy speeding up the process and sub-pixel image registration is a single-step image match via analytic method. Experimental results show the proposed algorithm reduces the computation time and gets a high accuracy when conducting experiments on different data sets. △ Less","9 March, 2018",https://arxiv.org/pdf/1803.05851
"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",Nicolas Papernot;Patrick McDaniel,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures. △ Less","13 March, 2018",https://arxiv.org/pdf/1803.04765
Network Coding Function Virtualization,Tan Do-Duy;M. Angeles Vazquez Castro,"Network Functions Virtualization (NFV) and Network Coding (NC) have attracted much attention in recent years as key concepts for providing 5G networks with flexibility and differentiated reliability, respectively. In this paper, we present the integration of NC architectural design and NFV. In order to do so we first describe what we call a virtualization process upon our proposed architectural design of NC that should help to offer the reliability functionality to a network. The process consists of identifying the required functional entities of NC and analyzing when the functionality should be activated towards complexity/energy efficiency. The relevance of our proposed NC function virtualization is its applicability to any underlying physical network, satellite or hybrid thus enabling softwarization, and rapid innovative deployment. Finally, we validate our framework to a study case of geo-control of network reliability that is based on device's geographical location-based signal/network information. △ Less","12 March, 2018",https://arxiv.org/pdf/1803.04435
"Innovative Texture Database Collecting Approach and Feature Extraction Method based on Combination of Gray Tone Difference Matrixes, Local Binary Patterns,and K-means Clustering",Shervan Fekri-Ershad,"Texture analysis and classification are some of the problems which have been paid much attention by image processing scientists since late 80s. If texture analysis is done accurately, it can be used in many cases such as object tracking, visual pattern recognition, and face recognition.Since now, so many methods are offered to solve this problem. Against their technical differences, all of them used same popular databases to evaluate their performance such asBrodatz or Outex, which may be made their performance biased on these databases. In this paper, an approach is proposed to collect more efficient databases of texture images. The proposed approach is included two stages. The first one is developing feature representation based on gray tone difference matrixes and local binary patterns features and the next one is consisted an innovative algorithm which is based on K-means clustering to collect images based on evaluated features. In order to evaluate the performance of the proposed approach, a texture database is collected and fisher rate is computed for collected one and well known databases. Also, texture classification is evaluated based on offered feature extraction and the accuracy is compared by some state of the art texture classification methods. △ Less","12 March, 2018",https://arxiv.org/pdf/1803.04125
Attention-based Graph Neural Network for Semi-supervised Learning,Kiran K. Thekumparampil;Chong Wang;Sewoong Oh;Li-Jia Li,"Recently popularized graph neural networks achieve the state-of-the-art accuracy on a number of standard benchmark datasets for graph-based semi-supervised learning, improving significantly over existing approaches. These architectures alternate between a propagation layer that aggregates the hidden states of the local neighborhood and a fully-connected layer. Perhaps surprisingly, we show that a linear model, that removes all the intermediate fully-connected layers, is still able to achieve a performance comparable to the state-of-the-art models. This significantly reduces the number of parameters, which is critical for semi-supervised learning where number of labeled examples are small. This in turn allows a room for designing more innovative propagation layers. Based on this insight, we propose a novel graph neural network that removes all the intermediate fully-connected layers, and replaces the propagation layers with attention mechanisms that respect the structure of the graph. The attention mechanism allows us to learn a dynamic and adaptive local summary of the neighborhood to achieve more accurate predictions. In a number of experiments on benchmark citation networks datasets, we demonstrate that our approach outperforms competing methods. By examining the attention weights among neighbors, we show that our model provides some interesting insights on how neighbors influence each other. △ Less","9 March, 2018",https://arxiv.org/pdf/1803.03735
An exploratory study on how Internet of Things developing companies handle User Experience Requirements,Johanna Bergman;Thomas Olsson;Isabelle johansson;Kirsten Rassmus-Gröhn,"[Context and motivation] Internet of Things (IoT) is becoming common throughout everyday lives. However, the interaction is often different from when using e.g. computers and other smart devices. Furthermore, an IoT device is often dependent on several other systems, heavily impacting the user experience (UX). Finally, the domain is changing rapidly and is driven by technological innovation. [Question/problem] In this qualitative study, we explore how companies elicit UX requirements in the context of IoT. A key part of contemporary IoT development is also data-driven approaches. Thus, these are also considered in the study. [Principal idea / Results] There is a knowledge gap around data-driven methodologies, there are examples of companies that collect large amount of data but do not always know how to utilize it. Furthermore, many of the companies struggle to handle the larger system context, where their products and the UX they control are only one part of the complete IoT ecosystem. [Contribution] We provide qualitative empirical data from IoT developing companies. Based on our findings, we identify challenges for the companies and areas for future work. △ Less","8 March, 2018",https://arxiv.org/pdf/1803.03058
Challenges: Bridge between Cloud and IoT,Mohammad Riyaz Belgaum;Safeeullah Soomro;Zainab Alansari;Muhammad Alam;Shahrulniza Musa;Mazliham Mohd Suud,"In the real time processing, a new emerging technology where the need of connecting smart devices with cloud through Internet has raised. IoT devices processed information is to be stored and accessed anywhere needed with a support of powerful computing performance, efficient storage infrastructure for heterogeneous systems and software which configures and controls these different devices. A lot of challenges to be addressed are listed with this new emerging technology as it needs to be compatible with the upcoming 5G wireless devices too. In this paper, the benefits and challenges of this innovative paradigm along with the areas open to do research are shown. △ Less","5 February, 2018",https://arxiv.org/pdf/1803.02890
Bursty Human Dynamics,Márton Karsai;Hang-Hyun Jo;Kimmo Kaski,"Bursty dynamics is a common temporal property of various complex systems in Nature but it also characterises the dynamics of human actions and interactions. At the phenomenological level it is a feature of all systems that evolve heterogeneously over time by alternating between periods of low and high event frequencies. In such systems, bursts are identified as periods in which the events occur with a rapid pace within a short time-interval while these periods are separated by long periods of time with low frequency of events. As such dynamical patterns occur in a wide range of natural phenomena, their observation, characterisation, and modelling have been a long standing challenge in several fields of research. However, due to some recent developments in communication and data collection techniques it has become possible to follow digital traces of actions and interactions of humans from the individual up to the societal level. This led to several new observations of bursty phenomena in the new but largely unexplored area of human dynamics, which called for the renaissance to study these systems using research concepts and methodologies, including data analytics and modelling. As a result, a large amount of new insight and knowledge as well as innovations have been accumulated in the field, which provided us a timely opportunity to write this brief monograph to make an up-to-date review and summary of the observations, appropriate measures, modelling, and applications of heterogeneous bursty patterns occurring in the dynamics of human behaviour. △ Less","7 March, 2018",https://arxiv.org/pdf/1803.02580
Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks,Abhijit Guha Roy;Nassir Navab;Christian Wachinger,"Fully convolutional neural networks (F-CNNs) have set the state-of-the-art in image segmentation for a plethora of applications. Architectural innovations within F-CNNs have mainly focused on improving spatial encoding or network connectivity to aid gradient flow. In this paper, we explore an alternate direction of recalibrating the feature maps adaptively, to boost meaningful features, while suppressing weak ones. We draw inspiration from the recently proposed squeeze & excitation (SE) module for channel recalibration of feature maps for image classification. Towards this end, we introduce three variants of SE modules for image segmentation, (i) squeezing spatially and exciting channel-wise (cSE), (ii) squeezing channel-wise and exciting spatially (sSE) and (iii) concurrent spatial and channel squeeze & excitation (scSE). We effectively incorporate these SE modules within three different state-of-the-art F-CNNs (DenseNet, SD-Net, U-Net) and observe consistent improvement of performance across all architectures, while minimally effecting model complexity. Evaluations are performed on two challenging applications: whole brain segmentation on MRI scans (Multi-Atlas Labelling Challenge Dataset) and organ segmentation on whole body contrast enhanced CT scans (Visceral Dataset). △ Less","8 June, 2018",https://arxiv.org/pdf/1803.02579
Influencers identification in complex networks through reaction-diffusion dynamics,Flavio Iannelli;Manuel Sebastian Mariani;Igor M. Sokolov,"A pivotal idea in network science, marketing research and innovation diffusion theories is that a small group of nodes -- called influencers -- have the largest impact on social contagion and epidemic processes in networks. Despite the long-standing interest in the influencers identification problem in socio-economic and biological networks, there is not yet agreement on which is the best identification strategy. State-of-the-art strategies are typically based either on heuristic centrality metrics or on analytic arguments that only hold for specific network topologies or peculiar dynamical regimes. Here, we leverage the recently introduced random-walk effective distance -- a topological metric that estimates almost perfectly the arrival time of diffusive spreading processes on networks -- to introduce a new centrality metric which quantifies how close a node is to the other nodes. We show that the new centrality metric significantly outperforms state-of-the-art metrics in detecting the influencers for global contagion processes. Our findings reveal the essential role of the network effective distance for the influencers identification and lead us closer to the optimal solution of the problem. △ Less","14 November, 2018",https://arxiv.org/pdf/1803.01212
Autostacker: A Compositional Evolutionary Learning System,Boyuan Chen;Harvey Wu;Warren Mo;Ishanu Chattopadhyay;Hod Lipson,"We introduce an automatic machine learning (AutoML) modeling architecture called Autostacker, which combines an innovative hierarchical stacking architecture and an Evolutionary Algorithm (EA) to perform efficient parameter search. Neither prior domain knowledge about the data nor feature preprocessing is needed. Using EA, Autostacker quickly evolves candidate pipelines with high predictive accuracy. These pipelines can be used as is or as a starting point for human experts to build on. Autostacker finds innovative combinations and structures of machine learning models, rather than selecting a single model and optimizing its hyperparameters. Compared with other AutoML systems on fifteen datasets, Autostacker achieves state-of-art or competitive performance both in terms of test accuracy and time cost. △ Less","1 March, 2018",https://arxiv.org/pdf/1803.00684
Discontinuities in Citation Relations among Journals: Self-organized Criticality as a Model of Scientific Revolutions and Change,Loet Leydesdorff;Caroline S. Wagner;Lutz Bornmann,"Using three-year moving averages of the complete Journal Citation Reports 1994-2016 of the Science Citation Index and the Social Sciences Citation Index (combined), we analyze links between citing and cited journals in terms of (1) whether discontinuities among the networks of consecutive years have occurred; (2) are these discontinuities relatively isolated or networked? (3) Can these discontinuities be used as indicators of novelty, change, and innovation in the sciences? We examine each of the N2 links among the N journals across the years. We find power-laws for the top 10,000 instances of change, which we suggest interpreting in terms of ""self-organized criticality"": co-evolutions of avalanches in aggregated citation relations and meta-stable states in the knowledge base can be expected to drive the sciences towards the edges of chaos. The flux of journal-journal citations in new manuscripts may generate an avalanche in the meta-stable networks, but one can expect the effects to remain local (for example, within a specialty). The avalanches can be of any size; they reorient the relevant citation environments by inducing a rewrite of history in the affected partitions. △ Less","1 March, 2018",https://arxiv.org/pdf/1803.00554
Joint Inter-flow Network Coding and Opportunistic Routing in Multi-hop Wireless Mesh Networks: A Comprehensive Survey,Somayeh Kafaie;Yuanzhu Chen;Octavia A. Dobre;Mohamed Hossam Ahmed,"Network coding and opportunistic routing are two recognized innovative ideas to improve the performance of wireless networks by utilizing the broadcast nature of the wireless medium. In the last decade, there has been considerable research on how to synergize inter-flow network coding and opportunistic routing in a single joint protocol outperforming each in any scenario. This paper explains the motivation behind the integration of these two techniques, and highlights certain scenarios in which the joint approach may even degrade the performance, emphasizing the fact that their synergistic effect cannot be accomplished with a naive and perfunctory combination. This survey paper also provides a comprehensive taxonomy of the joint protocols in terms of their fundamental components and associated challenges, and compares existing joint protocols. We also present concluding remarks along with an outline of future research directions. △ Less","1 March, 2018",https://arxiv.org/pdf/1803.00474
Challenges and opportunities in visual interpretation of Big Data,Gourab Mitra,"We live in a world where data generation is omnipresent. Innovations in computer hardware in the last few decades coupled with increasingly reliable connectivity among them have fueled this phenomenon. We are constantly creating and consuming data across digital devices of varying form factors. Leveraging huge quantities of data involves making interpretations from it. However, interpreting data is still a difficult task. We need data analysts to help make decisions. These experts apply their domain knowledge, understanding of the problem space and numerical analysis to draw inferences from the data in order to support decision making. Existing tools and techniques for interference serve users making decisions with hard constraints. Consumer systems are often built to support exploratory data analysis in mind rather than sense making. △ Less","1 March, 2018",https://arxiv.org/pdf/1803.00459
Economic Implications of Blockchain Platforms,Jun Aoyagi;Daisuke Adachi,"In an economy with asymmetric information, the smart contract in the blockchain protocol mitigates uncertainty. Since, as a new trading platform, the blockchain triggers segmentation of market and differentiation of agents in both the sell and buy sides of the market, it recomposes the asymmetric information and generates spreads in asset price and quality between itself and a traditional platform. We show that marginal innovation and sophistication of the smart contract have non-monotonic effects on the trading value in the blockchain platform, its fundamental value, the price of cryptocurrency, and consumers' welfare. Moreover, a blockchain manager who controls the level of the innovation of the smart contract has an incentive to keep it lower than the first best when the underlying information asymmetry is not severe, leading to welfare loss for consumers. △ Less","9 October, 2018",https://arxiv.org/pdf/1802.10117
Missing Data in Sparse Transition Matrix Estimation for Sub-Gaussian Vector Autoregressive Processes,Amin Jalali;Rebecca Willett,"High-dimensional time series data exist in numerous areas such as finance, genomics, healthcare, and neuroscience. An unavoidable aspect of all such datasets is missing data, and dealing with this issue has been an important focus in statistics, control, and machine learning. In this work, we consider a high-dimensional estimation problem where a dynamical system, governed by a stable vector autoregressive model, is randomly and only partially observed at each time point. Our task amounts to estimating the transition matrix, which is assumed to be sparse. In such a scenario, where covariates are highly interdependent and partially missing, new theoretical challenges arise. While transition matrix estimation in vector autoregressive models has been studied previously, the missing data scenario requires separate efforts. Moreover, while transition matrix estimation can be studied from a high-dimensional sparse linear regression perspective, the covariates are highly dependent and existing results on regularized estimation with missing data from i.i.d.~covariates are not applicable. At the heart of our analysis lies 1) a novel concentration result when the innovation noise satisfies the convex concentration property, as well as 2) a new quantity for characterizing the interactions of the time-varying observation process with the underlying dynamical system. △ Less","26 February, 2018",https://arxiv.org/pdf/1802.09511
Lean Internal Startups for Software Product Innovation in Large Companies: Enablers and Inhibitors,Henry Edison;Nina M. Smørsgård;Xiaofeng Wang;Pekka Abrahamsson,"To compete in this age of disruption, large companies cannot rely on cost efficiency, lead time reduction and quality improvement. They are now looking for ways to innovate like startups. Meanwhile, the awareness and use of the Lean startup approach have grown rapidly amongst the software startup community in recent years. This study investigates how Lean internal startup facilitates software product innovation in large companies and identifies its enablers and inhibitors. A multiple case study approach is followed in the investigation. Two software product innovation projects from two large companies are examined, using a conceptual framework that is based on the method-in-action framework and extended with the previously developed Lean-Internal Corporate Venture model. Seven face-to-face in-depth interviews of the employees with different roles are conducted. Within-case analysis and cross-case comparison are applied to draw the findings from the cases. A generic process flow summarises the common key processes of Lean internal startups. The findings suggest that an internal startup that is initiated management or employees faces different challenges. A list of enablers of applying Lean startup in large companies are identified, including top management support and cross-functional team. Both cases face different inhibitors due to the different process of inception, objective of the team and type of the product. Our contributions are threefold. First, this study is one of the first attempt to investigate the use of Lean startup approach in large companies empirically. Second, the study shows the potential of the method-in-action framework to investigate the Lean startup approach in non-startup context. The third is a general process of Lean internal startup and the evidence of the enablers and inhibitors of implementing it, which are both theory-informed and empirically grounded. △ Less","23 February, 2018",https://arxiv.org/pdf/1802.09393
Novel Common Vehicle Information Model (CVIM) for Future Automotive Vehicle Big Data Marketplaces,Johannes Pillmann;Christian Wietfeld;Adrian Zarcula;Thomas Raugust;Daniel Calvo Alonso,"Even though connectivity services have been introduced in many of the most recent car models, access to vehicle data is currently limited due to its proprietary nature. The European project AutoMat has therefore developed an open Marketplace providing a single point of access for brand-independent vehicle data. Thereby, vehicle sensor data can be leveraged for the design and implementation of entirely new services even beyond trafficrelated applications (such as hyper-local traffic forecasts). This paper presents the architecture for a Vehicle Big Data Marketplace as enabler of cross-sectorial and innovative vehicle data services. Therefore, the novel Common Vehicle Information Model (CVIM) is defined as an open and harmonized data model, allowing the aggregation of brand-independent and generic data sets. Within this work the realization of a prototype CVIM and Marketplace implementation is presented. The two use-cases of local weather prediction and road quality measurements are introduced to show the applicability of the AutoMat concept and prototype to non-automotive application △ Less","21 February, 2018",https://arxiv.org/pdf/1802.09353
"Information and Communications Technologies for Sustainable Development Goals: State-of-the-Art, Needs and Perspectives",Jinsong Wu;Song Guo;Huawei Huang;William Liu;Yong Xiang,"In September 2015, the United Nations General Assembly accepted the 2030 Development Agenda, which has included 92 paragraphs, and the Paragraph 91 defined 17 sustainable development goals (SDGs) and 169 associated targets. The goal of this paper is to discover the correlations among SDGs and information and communications technologies (ICTs). This paper discusses the roles and opportunities that ICTs play in pursuing the SDGs. We identify a number of research gaps to those three pillars, social, economic, and environmental perspectives, of sustainable development. After extensive literature reviews on the SDG-related research initiatives and activities, we find that the majority of contributions to SDGs recognized by the IEEE and ACM research communities have mainly focused on the technical aspects, while there are lack of the holistic social good perspectives. Therefore, there are essential and urgent needs to raise the awareness and call for attentions on how to innovate and energize ICTs in order to best assist all nations to achieve the SDGs by 2030. △ Less","28 February, 2018",https://arxiv.org/pdf/1802.09345
A Model for Innovation Diffusion with Intergroup Suppression,Anirban Chakraborti;Syed S. Husain;Joseph Whitmeyer,"We present a new model for the diffusion of innovation. Here, the population is segmented into distinct groups. Adoption by a particular group of some cultural product may be inhibited both by large numbers of its own members already having adopted but also, in particular, by members of another group having adopted. Intergroup migration is also permitted. We determine the equilibrium points and carry out stability analysis for the model for a two-group population. We also simulate a discrete time version of the model. Lastly, we present data on tablet use in eight countries from 2012-2016 and show that the relationship between use in the ""under 25"" age group and ""55+"" age group conforms to the model. △ Less","24 February, 2018",https://arxiv.org/pdf/1802.08943
Budget Constrained Bidding by Model-free Reinforcement Learning in Display Advertising,Di Wu;Xiujun Chen;Xun Yang;Hao Wang;Qing Tan;Xiaoxun Zhang;Jian Xu;Kun Gai,"Real-time bidding (RTB) is an important mechanism in online display advertising, where a proper bid for each page view plays an essential role for good marketing results. Budget constrained bidding is a typical scenario in RTB where the advertisers hope to maximize the total value of the winning impressions under a pre-set budget constraint. However, the optimal bidding strategy is hard to be derived due to the complexity and volatility of the auction environment. To address these challenges, in this paper, we formulate budget constrained bidding as a Markov Decision Process and propose a model-free reinforcement learning framework to resolve the optimization problem. Our analysis shows that the immediate reward from environment is misleading under a critical resource constraint. Therefore, we innovate a reward function design methodology for the reinforcement learning problems with constraints. Based on the new reward design, we employ a deep neural network to learn the appropriate reward so that the optimal policy can be learned effectively. Different from the prior model-based work, which suffers from the scalability problem, our framework is easy to be deployed in large-scale industrial applications. The experimental evaluations demonstrate the effectiveness of our framework on large-scale real datasets. △ Less","23 October, 2018",https://arxiv.org/pdf/1802.08365
BigDataBench: A Scalable and Unified Big Data and AI Benchmark Suite,Wanling Gao;Jianfeng Zhan;Lei Wang;Chunjie Luo;Daoyi Zheng;Xu Wen;Rui Ren;Chen Zheng;Xiwen He;Hainan Ye;Haoning Tang;Zheng Cao;Shujie Zhang;Jiahui Dai,"Several fundamental changes in technology indicate domain-specific hardware and software co-design is the only path left. In this context, architecture, system, data management, and machine learning communities pay greater attention to innovative big data and AI algorithms, architecture, and systems. Unfortunately, complexity, diversity, frequently-changed workloads, and rapid evolution of big data and AI systems raise great challenges. First, the traditional benchmarking methodology that creates a new benchmark or proxy for every possible workload is not scalable, or even impossible for Big Data and AI benchmarking. Second, it is prohibitively expensive to tailor the architecture to characteristics of one or more application or even a domain of applications. We consider each big data and AI workload as a pipeline of one or more classes of units of computation performed on different initial or intermediate data inputs, each class of which we call a data motif. On the basis of our previous work that identifies eight data motifs taking up most of the run time of a wide variety of big data and AI workloads, we propose a scalable benchmarking methodology that uses the combination of one or more data motifs---to represent diversity of big data and AI workloads. Following this methodology, we present a unified big data and AI benchmark suite---BigDataBench 4.0, publicly available from~\url{http://prof.ict.ac.cn/BigDataBench}. This unified benchmark suite sheds new light on domain-specific hardware and software co-design: tailoring the system and architecture to characteristics of the unified eight data motifs other than one or more application case by case. Also, for the first time, we comprehensively characterize the CPU pipeline efficiency using the benchmarks of seven workload types in BigDataBench 4.0. △ Less","22 November, 2018",https://arxiv.org/pdf/1802.08254
"Collaboratively Learning the Best Option, Using Bounded Memory",Lili Su;Martin Zubeldia;Nancy Lynch,"We consider multi-armed bandit problems in social groups wherein each individual has bounded memory and shares the common goal of learning the best arm/option. We say an individual learns the best option if eventually (as t \to \infty) it pulls only the arm with the highest average reward. While this goal is provably impossible for an isolated individual, we show that, in social groups, this goal can be achieved easily with the aid of social persuasion, i.e., communication. Specifically, we study the learning dynamics wherein an individual sequentially decides on which arm to pull next based on not only its private reward feedback but also the suggestions provided by randomly chosen peers. Our learning dynamics are hard to analyze via explicit probabilistic calculations due to the stochastic dependency induced by social interaction. Instead, we employ the mean-field approximation method from statistical physics and we show: (1) With probability \to 1 as the social group size N \to \infty , every individual in the social group learns the best option. (2) Over an arbitrary finite time horizon [0, T], with high probability (in N), the fraction of individuals that prefer the best option grows to 1 exponentially fast as t increases (t\in [0, T]). A major innovation of our mean-filed analysis is a simple yet powerful technique to deal with absorbing states in the interchange of limits N \to \infty and t \to \infty . The mean-field approximation method allows us to approximate the probabilistic sample paths of our learning dynamics by a deterministic and smooth trajectory that corresponds to the unique solution of a well-behaved system of ordinary differential equations (ODEs). Such an approximation is desired because the analysis of a system of ODEs is relatively easier than that of the original stochastic system. △ Less","11 November, 2018",https://arxiv.org/pdf/1802.08159
DeepASL: Enabling Ubiquitous and Non-Intrusive Word and Sentence-Level Sign Language Translation,Biyi Fang;Jillian Co;Mi Zhang,"There is an undeniable communication barrier between deaf people and people with normal hearing ability. Although innovations in sign language translation technology aim to tear down this communication barrier, the majority of existing sign language translation systems are either intrusive or constrained by resolution or ambient lighting conditions. Moreover, these existing systems can only perform single-sign ASL translation rather than sentence-level translation, making them much less useful in daily-life communication scenarios. In this work, we fill this critical gap by presenting DeepASL, a transformative deep learning-based sign language translation technology that enables ubiquitous and non-intrusive American Sign Language (ASL) translation at both word and sentence levels. DeepASL uses infrared light as its sensing mechanism to non-intrusively capture the ASL signs. It incorporates a novel hierarchical bidirectional deep recurrent neural network (HB-RNN) and a probabilistic framework based on Connectionist Temporal Classification (CTC) for word-level and sentence-level ASL translation respectively. To evaluate its performance, we have collected 7,306 samples from 11 participants, covering 56 commonly used ASL words and 100 ASL sentences. DeepASL achieves an average 94.5% word-level translation accuracy and an average 8.2% word error rate on translating unseen ASL sentences. Given its promising performance, we believe DeepASL represents a significant step towards breaking the communication barrier between deaf people and hearing majority, and thus has the significant potential to fundamentally change deaf people's lives. △ Less","17 October, 2018",https://arxiv.org/pdf/1802.07584
Cooperative Robot Localization Using Event-triggered Estimation,Michael Ouimet;David Iglesias;Nisar Ahmed;Sonia Martinez,"This paper describes a novel communication-spare cooperative localization algorithm for a team of mobile unmanned robotic vehicles. Exploiting an event-based estimation paradigm, robots only send measurements to neighbors when the expected innovation for state estimation is high. Since agents know the event-triggering condition for measurements to be sent, the lack of a measurement is thus also informative and fused into state estimates. The robots use a Covariance Intersection (CI) mechanism to occasionally synchronize their local estimates of the full network state. In addition, heuristic balancing dynamics on the robots' CI-triggering thresholds ensure that, in large diameter networks, the local error covariances remains below desired bounds across the network. Simulations on both linear and nonlinear dynamics/measurement models show that the event-triggering approach achieves nearly optimal state estimation performance in a wide range of operating conditions, even when using only a fraction of the communication cost required by conventional full data sharing. The robustness of the proposed approach to lossy communications, as well as the relationship between network topology and CI-based synchronization requirements, are also examined. △ Less","20 February, 2018",https://arxiv.org/pdf/1802.07346
Simulating the Ridesharing Economy: The Individual Agent Metro-Washington Area Ridesharing Model,Joseph A. E. Shaheen,"The ridesharing economy is experiencing rapid growth and innovation. Companies such as Uber and Lyft are continuing to grow at a considerable pace while providing their platform as an organizing medium for ridesharing services, increasing consumer utility as well as employing thousands in part-time positions. However, many challenges remain in the modeling of ridesharing services, many of which are not currently under wide consideration. In this paper, an agent-based model is developed to simulate a ridesharing service in the Washington D.C. metropolitan region. The model is used to examine levels of utility gained for both riders (customers) and drivers (service providers) of a generic ridesharing service. A description of the Individual Agent Metro-Washington Area Ridesharing Model (IAMWARM) is provided, as well as a description of a typical simulation run. We investigate the financial gains of drivers for a 24-hour period under two scenarios and two spatial movement behaviors. The two spatial behaviors were random movement and Voronoi movement, which we describe. Both movement behaviors were tested under a stationary run conditions scenario and a variable run conditions scenario. We find that Voronoi movement increased drivers' utility gained but that emergence of this system property was only viable under variable scenario conditions. This result provides two important insights: The first is that driver movement decisions prior to passenger pickup can impact financial gain for the service and drivers, and consequently, rate of successful pickup for riders. The second is that this phenomenon is only evident under experimentation conditions where variability in passenger and driver arrival rates are administered. △ Less","18 February, 2018",https://arxiv.org/pdf/1802.07280
Agile Amulet: Real-Time Salient Object Detection with Contextual Attention,Pingping Zhang;Luyao Wang;Dong Wang;Huchuan Lu;Chunhua Shen,"This paper proposes an Agile Aggregating Multi-Level feaTure framework (Agile Amulet) for salient object detection. The Agile Amulet builds on previous works to predict saliency maps using multi-level convolutional features. Compared to previous works, Agile Amulet employs some key innovations to improve training and testing speed while also increase prediction accuracy. More specifically, we first introduce a contextual attention module that can rapidly highlight most salient objects or regions with contextual pyramids. Thus, it effectively guides the learning of low-layer convolutional features and tells the backbone network where to look. The contextual attention module is a fully convolutional mechanism that simultaneously learns complementary features and predicts saliency scores at each pixel. In addition, we propose a novel method to aggregate multi-level deep convolutional features. As a result, we are able to use the integrated side-output features of pre-trained convolutional networks alone, which significantly reduces the model parameters leading to a model size of 67 MB, about half of Amulet. Compared to other deep learning based saliency methods, Agile Amulet is of much lighter-weight, runs faster (30 fps in real-time) and achieves higher performance on seven public benchmarks in terms of both quantitative and qualitative evaluation. △ Less","19 February, 2018",https://arxiv.org/pdf/1802.06960
Technological research in the EU is less efficient than the world average. EU research policy risks Europeans' future,Alonso Rodriguez-Navarro;Ricardo Brito,"We have studied the efficiency of research in the EU by a percentile-based citation approach that analyzes the distribution of country papers among the world papers. Going up in the citation scale, the frequency of papers from efficient countries increases while the frequency from inefficient countries decreases. In the percentile-based approach, this trend, which is permanent at any citation level, is measured by the ep index that equals the Ptop 1%/Ptop 10% ratio. By using the ep index we demonstrate that EU research on fast-evolving technological topics is less efficient than the world average and that the EU is far from being able to compete with the most advanced countries. The ep index also shows that the USA is well ahead of the EU in both fast- and slow-evolving technologies, which suggests that the advantage of the USA over the EU in innovation is due to low research efficiency in the EU. In accord with some previous studies, our results show that the European Commission's ongoing claims about the excellence of EU research are based on a wrong diagnosis. The EU must focus its research policy on the improvement of its inefficient research. Otherwise, the future of Europeans is at risk. △ Less","27 June, 2018",https://arxiv.org/pdf/1802.06633
Capacitated Dynamic Programming: Faster Knapsack and Graph Algorithms,Kyriakos Axiotis;Christos Tzamos,"One of the most fundamental problems in Computer Science is the Knapsack problem. Given a set of n items with different weights and values, it asks to pick the most valuable subset whose total weight is below a capacity threshold T. Despite its wide applicability in various areas in Computer Science, Operations Research, and Finance, the best known running time for the problem is O(Tn). The main result of our work is an improved algorithm running in time O(TD), where D is the number of distinct weights. Previously, faster runtimes for Knapsack were only possible when both weights and values are bounded by M and V respectively, running in time O(nMV) [Pisinger'99]. In comparison, our algorithm implies a bound of O(nM^2) without any dependence on V, or O(nV^2) without any dependence on M. Additionally, for the unbounded Knapsack problem, we provide an algorithm running in time O(M^2) or O(V^2). Both our algorithms match recent conditional lower bounds shown for the Knapsack problem [Cygan et al'17, Künnemann et al'17]. We also initiate a systematic study of general capacitated dynamic programming, of which Knapsack is a core problem. This problem asks to compute the maximum weight path of length k in an edge- or node-weighted directed acyclic graph. In a graph with m edges, these problems are solvable by dynamic programming in time O(km), and we explore under which conditions the dependence on k can be eliminated. We identify large classes of graphs where this is possible and apply our results to obtain linear time algorithms for the problem of k-sparse Delta-separated sequences. The main technical innovation behind our results is identifying and exploiting concavity that appears in relaxations and subproblems of the tasks we consider. △ Less","12 July, 2018",https://arxiv.org/pdf/1802.06440
Consensus in Software Engineering: A Cognitive Mapping Study,Pontus Johnson;Paul Ralph;Mathias Ekstedt;Iaakov Exman;Michael Goedicke,"Background: Philosophers of science including Collins, Feyerabend, Kuhn and Latour have all emphasized the importance of consensus within scientific communities of practice. Consensus is important for maintaining legitimacy with outsiders, orchestrating future research, developing educational curricula and agreeing industry standards. Low consensus contrastingly undermines a field's reputation and hinders peer review. Aim: This paper aims to investigate the degree of consensus within the software engineering academic community concerning members' implicit theories of software engineering. Method: A convenience sample of 60 software engineering researchers produced diagrams describing their personal understanding of causal relationships between core software engineering constructs. The diagrams were then analyzed for patterns and clusters. Results: At least three schools of thought may be forming; however, their interpretation is unclear since they do not correspond to known divisions within the community (e.g. Agile vs. Plan-Driven methods). Furthermore, over one third of participants do not belong to any cluster. Conclusion: Although low consensus is common in social sciences, the rapid pace of innovation observed in software engineering suggests that high consensus is achievable given renewed commitment to empiricism and evidence-based practice. △ Less","17 February, 2018",https://arxiv.org/pdf/1802.06319
Innovation Initiatives in Large Software Companies: A Systematic Mapping Study,Henry Edison;Xiaofeng Wang;Ronald Jabangwe;Pekka Abrahamsson,"To keep the competitive advantage and adapt to changes in the market and technology, companies need to innovate in an organised, purposeful and systematic manner. However, due to their size and complexity, large companies tend to focus on maintaining their business, which can potentially lower their agility to innovate. This study aims to provide an overview of the current research on innovation initiatives and to identify the challenges of implementing the initiatives in the context of large software companies. The investigation was performed using a systematic mapping approach of published literature on corporate innovation and entrepreneurship. Then it was complemented with interviews with four experts with rich industry experience. Our study results suggest that, there is a lack of high quality empirical studies on innovation initiative in the context of large software companies. A total of 7 studies are conducted in such context, which reported 5 types of initiatives: intrapreneurship, bootlegging, internal venture, spin-off and crowdsourcing. Our study offers three contributions. First, this paper represents the map of existing literature on innovation initiatives inside large companies. The second contribution is to provide an innovation initiative tree. The third contribution is to identify key challenges faced by each initiative in large software companies. At the strategic and tactical levels, there is no difference between large software companies and other companies. At the operational level, large software companies are highly influenced by the advancement of Internet technology. Large software companies use open innovation paradigm as part of their innovation initiatives. We envision a future work is to further empirically evaluate the innovation initiative tree in large software companies, which involves more practitioners from different companies. △ Less","16 February, 2018",https://arxiv.org/pdf/1802.05951
How does undone science get funded? A bibliometric analysis linking rare diseases publications to national and European funding sources,Alex Rushforth;Alfredo Yegros-Yegros;Philippe Mongeon;Thed van Leeuwen,"One of the notable features of undone science debates is how formation of new interest groups becomes pivotal in mobilizing and championing emerging research on undone topics. Clearly money is one of the most important mediums through which different types of actors can support and steer scientists to work on undone topics. Yet which actors are more visible in their support for scientific research is something which has seldom been measured. This study delves into research funding in the context of rare diseases research, a topic which has evolved from the margins of medical research into a priority area articulated by many contemporary funding agencies. Rare diseases refer to conditions affecting relatively few people in a population. Given low incidences, interest groups have articulated a lack of attention within medical research compared to more common conditions. The rise to prominence of rare diseases in research funding policies is often explained in the science studies literature in terms of effective lobbying by social movements Likewise, innovative fundraising initiatives, infrastructure building, and close partnerships with research groups are other means through which interested actors have sought to build capacity for research into rare medical conditions. To date however systematic empirical evidence to compare the relative importance of different actors in funding rare disease research has not been produced. Building on interest in undone science in STS and science policy studies, our study hopes to map-out different kinds of funding actors and their influence on leading scientific research on rare diseases, by use of bibliometric tools. The approach we are developing relies on the use of Funding Acknowledgement data provided in Web of Science database. △ Less","16 February, 2018",https://arxiv.org/pdf/1802.05945
PRoST: Distributed Execution of SPARQL Queries Using Mixed Partitioning Strategies,Matteo Cossu;Michael Färber;Georg Lausen,"The rapidly growing size of RDF graphs in recent years necessitates distributed storage and parallel processing strategies. To obtain efficient query processing using computer clusters a wide variety of different approaches have been proposed. Related to the approach presented in the current paper are systems built on top of Hadoop HDFS, for example using Apache Accumulo or using Apache Spark. We present a new RDF store called PRoST (Partitioned RDF on Spark Tables) based on Apache Spark. PRoST introduces an innovative strategy that combines the Vertical Partitioning approach with the Property Table, two preexisting models for storing RDF datasets. We demonstrate that our proposal outperforms state-of-the-art systems w.r.t. the runtime for a wide range of query types and without any extensive precomputing phase. △ Less","16 February, 2018",https://arxiv.org/pdf/1802.05898
Channel Reconstruction-Based Hybrid Precoding for Millimeter Wave Multi-User MIMO Systems,Miguel R. Castellanos;Vasanthan Raghavan;Jung H. Ryu;Ozge H. Koymen;Junyi Li;David J. Love;Borja Peleato,"The focus of this paper is on multi-user MIMO transmissions for millimeter wave systems with a hybrid precoding architecture at the base-station. To enable multi-user transmissions, the base-station uses a cell-specific codebook of beamforming vectors over an initial beam alignment phase. Each user uses a user-specific codebook of beamforming vectors to learn the top-P (where P >= 1) beam pairs in terms of the observed SNR in a single-user setting. The top-P beam indices along with their SNRs are fed back from each user and the base-station leverages this information to generate beam weights for simultaneous transmissions. A typical method to generate the beam weights is to use only the best beam for each user and either steer energy along this beam, or to utilize this information to reduce multi-user interference. The other beams are used as fall back options to address blockage or mobility. Such an approach completely discards information learned about the channel condition(s) even though each user feeds back this information. With this background, this work develops an advanced directional precoding structure for simultaneous transmissions at the cost of an additional marginal feedback overhead. This construction relies on three main innovations: 1) Additional feedback to allow the base-station to reconstruct a rank-P approximation of the channel matrix between it and each user, 2) A zeroforcing structure that leverages this information to combat multi-user interference by remaining agnostic of the receiver beam knowledge in the precoder design, and 3) A hybrid precoding architecture that allows both amplitude and phase control at low-complexity and cost to allow the implementation of the zeroforcing structure. Numerical studies show that the proposed scheme results in a significant sum rate performance improvement over naive schemes even with a coarse initial beam alignment codebook. △ Less","14 February, 2018",https://arxiv.org/pdf/1802.05146
"\mathcal{CIRFE}
: A Distributed Random Fields Estimator",Anit Kumar Sahu;Dusan Jakovetic;Soummya Kar,"This paper presents a communication efficient distributed algorithm, \mathcal{CIRFE} of the \emph{consensus}+\emph{innovations} type, to estimate a high-dimensional parameter in a multi-agent network, in which each agent is interested in reconstructing only a few components of the parameter. This problem arises for example when monitoring the high-dimensional distributed state of a large-scale infrastructure with a network of limited capability sensors and where each sensor is tasked with estimating some local components of the state. At each observation sampling epoch, each agent updates its local estimate of the parameter components in its interest set by simultaneously processing the latest locally sensed information~(\emph{innovations}) and the parameter estimates from agents~(\emph{consensus}) in its communication neighborhood given by a time-varying possibly sparse graph. Under minimal conditions on the inter-agent communication network and the sensing models, almost sure convergence of the estimate sequence at each agent to the components of the true parameter in its interest set is established. Furthermore, the paper establishes the performance of \mathcal{CIRFE} in terms of asymptotic covariance of the estimate sequences and specifically characterizes the dependencies of the component wise asymptotic covariance in terms of the number of agents tasked with estimating it. Finally, simulation experiments demonstrate the efficacy of \mathcal{CIRFE}. △ Less","11 June, 2018",https://arxiv.org/pdf/1802.04943
Quantifying Uncertainty in Discrete-Continuous and Skewed Data with Bayesian Deep Learning,Thomas Vandal;Evan Kodra;Jennifer Dy;Sangram Ganguly;Ramakrishna Nemani;Auroop R. Ganguly,"Deep Learning (DL) methods have been transforming computer vision with innovative adaptations to other domains including climate change. For DL to pervade Science and Engineering (S&E) applications where risk management is a core component, well-characterized uncertainty estimates must accompany predictions. However, S&E observations and model-simulations often follow heavily skewed distributions and are not well modeled with DL approaches, since they usually optimize a Gaussian, or Euclidean, likelihood loss. Recent developments in Bayesian Deep Learning (BDL), which attempts to capture uncertainties from noisy observations, aleatoric, and from unknown model parameters, epistemic, provide us a foundation. Here we present a discrete-continuous BDL model with Gaussian and lognormal likelihoods for uncertainty quantification (UQ). We demonstrate the approach by developing UQ estimates on `DeepSD', a super-resolution based DL model for Statistical Downscaling (SD) in climate applied to precipitation, which follows an extremely skewed distribution. We find that the discrete-continuous models outperform a basic Gaussian distribution in terms of predictive accuracy and uncertainty calibration. Furthermore, we find that the lognormal distribution, which can handle skewed distributions, produces quality uncertainty estimates at the extremes. Such results may be important across S&E, as well as other domains such as finance and economics, where extremes are often of significant interest. Furthermore, to our knowledge, this is the first UQ model in SD where both aleatoric and epistemic uncertainties are characterized. △ Less","24 May, 2018",https://arxiv.org/pdf/1802.04742
Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,Nicolas Vasilache;Oleksandr Zinenko;Theodoros Theodoridis;Priya Goyal;Zachary DeVito;William S. Moses;Sven Verdoolaege;Andrew Adams;Albert Cohen,"Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff] △ Less","28 June, 2018",https://arxiv.org/pdf/1802.04730
Stochastic quasi-Newton with adaptive step lengths for large-scale problems,Adrian Wills;Thomas Schön,We provide a numerically robust and fast method capable of exploiting the local geometry when solving large-scale stochastic optimisation problems. Our key innovation is an auxiliary variable construction coupled with an inverse Hessian approximation computed using a receding history of iterates and gradients. It is the Markov chain nature of the classic stochastic gradient algorithm that enables this development. The construction offers a mechanism for stochastic line search adapting the step length. We numerically evaluate and compare against current state-of-the-art with encouraging performance on real-world benchmark problems where the number of observations and unknowns is in the order of millions. △ Less,"12 February, 2018",https://arxiv.org/pdf/1802.04310
Driving Simulator Platform for Development and Evaluation of Safety and Emergency Systems,Andrés E. Gómez;Tiago C. dos Santos;Carlos M. Massera;Arthur de M. Neto;Denis F. Wolf,"According to data from the United Nations, more than 3000 people have died each day in the world due to road traffic collision. Considering recent researches, the human error may be considered as the main responsible for these fatalities. Because of this, researchers seek alternatives to transfer the vehicle control from people to autonomous systems. However, providing this technological innovation for the people may demand complex challenges in the legal, economic and technological areas. Consequently, carmakers and researchers have divided the driving automation in safety and emergency systems that improve the driver perception on the road. This may reduce the human error. Therefore, the main contribution of this study is to propose a driving simulator platform to develop and evaluate safety and emergency systems, in the first design stage. This driving simulator platform has an advantage: a flexible software structure.This allows in the simulation one adaptation for development or evaluation of a system. The proposed driving simulator platform was tested in two applications: cooperative vehicle system development and the influence evaluation of a Driving Assistance System (\textit{DAS}) on a driver. In the cooperative vehicle system development, the results obtained show that the increment of the time delay in the communication among vehicles (V2V) is determinant for the system performance. On the other hand, in the influence evaluation of a \textit{DAS} in a driver, it was possible to conclude that the \textit{DAS'} model does not have the level of influence necessary in a driver to avoid an accident. △ Less","1 February, 2018",https://arxiv.org/pdf/1802.04104
Agile development for vulnerable populations: lessons learned and recommendations,Marcos Baez;Fabio Casati,"In this paper we draw attention to the challenges of managing software projects for vulnerable populations, i.e., people potentially exposed to harm or not capable of protecting their own interests. The focus on human aspects, and particularly, the inclusion of human-centered approaches, has been a popular topic in the software engineering community. We argue, however, that current literature provides little understanding and guidance on how to approach these type of scenarios. Here, we shed some light on the topic by reporting on our experiences in developing innovative solutions for the residential care scenario, outlining potential issues and recommendations. △ Less","25 January, 2018",https://arxiv.org/pdf/1802.04100
Understanding Chatbot-mediated Task Management,Carlos Toxtli;Andrés Monroy-Hernández;Justin Cranshaw,"Effective task management is essential to successful team collaboration. While the past decade has seen considerable innovation in systems that track and manage group tasks, these innovations have typically been outside of the principal communication channels: email, instant messenger, and group chat. Teams formulate, discuss, refine, assign, and track the progress of their collaborative tasks over electronic communication channels, yet they must leave these channels to update their task-tracking tools, creating a source of friction and inefficiency. To address this problem, we explore how bots might be used to mediate task management for individuals and teams. We deploy a prototype bot to eight different teams of information workers to help them create, assign, and keep track of tasks, all within their main communication channel. We derived seven insights for the design of future bots for coordinating work. △ Less","8 February, 2018",https://arxiv.org/pdf/1802.03109
High Performance Rearrangement and Multiplication Routines for Sparse Tensor Arithmetic,Adam P. Harrison;Dileepan Joseph,"Researchers are increasingly incorporating numeric high-order data, i.e., numeric tensors, within their practice. Just like the matrix/vector (MV) paradigm, the development of multi-purpose, but high-performance, sparse data structures and algorithms for arithmetic calculations, e.g., those found in Einstein-like notation, is crucial for the continued adoption of tensors. We use the example of high-order differential operators to illustrate this need. As sparse tensor arithmetic is an emerging research topic, with challenges distinct from the MV paradigm, many aspects require further articulation. We focus on three core facets. First, aligning with prominent voices in the field, we emphasise the importance of data structures able to accommodate the operational complexity of tensor arithmetic. However, we describe a linearised coordinate (LCO) data structure that provides faster and more memory-efficient sorting performance. Second, flexible data structures, like the LCO, rely heavily on sorts and permutations. We introduce an innovative permutation algorithm, based on radix sort, that is tailored to rearrange already-sorted sparse data, producing significant performance gains. Third, we introduce a novel poly-algorithm for sparse tensor products, where hyper-sparsity is a possibility. Different manifestations of hyper-sparsity demand their own approach, which our poly-algorithm is the first to provide. These developments are incorporated within our LibNT and NTToolbox software libraries. Benchmarks, frequently drawn from the high-order differential operators example, demonstrate the practical impact of our routines, with speed-ups of 40% or higher compared to alternative high-performance implementations. Comparisons against the MATLAB Tensor Toolbox show over 10 times speed improvements. Thus, these advancements produce significant practical improvements for sparse tensor arithmetic. △ Less","7 February, 2018",https://arxiv.org/pdf/1802.02619
Unified Analysis and Optimization of D2D Communications in Cellular Networks Over Fading Channels,Imene Trigui;Sofiene Affes,"This paper develops an innovative approach to the modeling and analysis of downlink cellular networks with device-to-device (D2D) transmissions. The analytical embodiment of the signal-to-noise and-interference ratio (SINR) analysis in general fading channels is unified due to the H-transform theory, a taxonomy never considered before in stochastic geometry-based cellular network modeling and analysis. The proposed framework has the potential, due to versatility of the Fox's H functions, of significantly simplifying the cumbersome analysis procedure and representation of D2D and cellular coverage, while subsuming those previously derived for all the known simple and composite fading models. By harnessing its tractability, the developed statistical machinery is employed to launch an investigation into the optimal design of coexisting D2D and cellular communications. We propose novel coverage-aware power control combined with opportunistic access control to maximize the area spectral efficiency (ASE) of D2D communications. Simulation results substantiate performance gains achieved by the proposed optimization framework in terms of cellular communication coverage probability, average D2D transmit power, and the ASE of D2D communications under different fading models and link- and network-level dynamics. △ Less","12 April, 2018",https://arxiv.org/pdf/1802.01618
Hierarchical Aggregation Approach for Distributed clustering of spatial datasets,Malika Bendechache;Nhien-An Le-Khac;M-Tahar Kechadi,"In this paper, we present a new approach of distributed clustering for spatial datasets, based on an innovative and efficient aggregation technique. This distributed approach consists of two phases: 1) local clustering phase, where each node performs a clustering on its local data, 2) aggregation phase, where the local clusters are aggregated to produce global clusters. This approach is characterised by the fact that the local clusters are represented in a simple and efficient way. And The aggregation phase is designed in such a way that the final clusters are compact and accurate while the overall process is efficient in both response time and memory allocation. We evaluated the approach with different datasets and compared it to well-known clustering techniques. The experimental results show that our approach is very promising and outperforms all those algorithms △ Less","1 February, 2018",https://arxiv.org/pdf/1802.00688
Robust Sequential Detection in Distributed Sensor Networks,Mark R. Leonard;Abdelhak M. Zoubir,"We consider the problem of sequential binary hypothesis testing with a distributed sensor network in a non-Gaussian noise environment. To this end, we present a general formulation of the Consensus + Innovations Sequential Probability Ratio Test (CISPRT). Furthermore, we introduce two different concepts for robustifying the CISPRT and propose four different algorithms, namely, the Least-Favorable-Density-CISPRT, the Median-CISPRT, the M-CISPRT, and the Myriad-CISPRT. Subsequently, we analyze their suitability for different binary hypothesis tests before verifying and evaluating their performance in a shift-in-mean and a shift-in-variance scenario. △ Less","1 February, 2018",https://arxiv.org/pdf/1802.00263
Modeling and Multi-objective Optimization of a Kind of Teaching Manipulator,Zhun Fan;Yugen You;Haodong Zheng;Guijie Zhu;Wenji Li;Shen Chen;Kalyanmoy Deb;Erik Goodman,"A new kind of six degree-of-freedom teaching manipulator without actuators is designed, for recording and conveniently setting a trajectory of an industrial robot. The device requires good gravity balance and operating force performance to ensure being controlled easily and fluently. In this paper, we propose a process for modeling the manipulator and then the model is used to formulate a multi-objective optimization problem to optimize the design of the testing manipulator. Three objectives, including total mass of the device, gravity balancing and operating force performance are analyzed and defined. A popular non-dominated sorting genetic algorithm (NSGA-II-CDP) is used to solve the optimization problem. The obtained solutions all outperform the design of a human expert. To extract design knowledge, an innovization study is performed to establish meaningful implicit relationship between the objective space and the decision space, which can be reused by the designer in future design process. △ Less","31 January, 2018",https://arxiv.org/pdf/1801.10599
Parallel Tracking and Verifying,Heng Fan;Haibin Ling,"Being intensively studied, visual object tracking has witnessed great advances in either speed (e.g., with correlation filters) or accuracy (e.g., with deep features). Real-time and high accuracy tracking algorithms, however, remain scarce. In this paper we study the problem from a new perspective and present a novel parallel tracking and verifying (PTAV) framework, by taking advantage of the ubiquity of multi-thread techniques and borrowing ideas from the success of parallel tracking and mapping in visual SLAM. The proposed PTAV framework is typically composed of two components, a (base) tracker T and a verifier V, working in parallel on two separate threads. The tracker T aims to provide a super real-time tracking inference and is expected to perform well most of the time; by contrast, the verifier V validates the tracking results and corrects T when needed. The key innovation is that, V does not work on every frame but only upon the requests from T; on the other end, T may adjust the tracking according to the feedback from V. With such collaboration, PTAV enjoys both the high efficiency provided by T and the strong discriminative power by V. Meanwhile, to adapt V to object appearance changes over time, we maintain a dynamic target template pool for adaptive verification, resulting in further performance improvements. In our extensive experiments on popular benchmarks including OTB2015, TC128, UAV20L and VOT2016, PTAV achieves the best tracking accuracy among all real-time trackers, and in fact even outperforms many deep learning based algorithms. Moreover, as a general framework, PTAV is very flexible with great potentials for future improvement and generalization. △ Less","29 January, 2018",https://arxiv.org/pdf/1801.10496
On Scale-out Deep Learning Training for Cloud and HPC,Srinivas Sridharan;Karthikeyan Vaidyanathan;Dhiraj Kalamkar;Dipankar Das;Mikhail E. Smorkalov;Mikhail Shiryaev;Dheevatsa Mudigere;Naveen Mellempudi;Sasikanth Avancha;Bharat Kaul;Pradeep Dubey,"The exponential growth in use of large deep neural networks has accelerated the need for training these deep neural networks in hours or even minutes. This can only be achieved through scalable and efficient distributed training, since a single node/card cannot satisfy the compute, memory, and I/O requirements of today's state-of-the-art deep neural networks. However, scaling synchronous Stochastic Gradient Descent (SGD) is still a challenging problem and requires continued research/development. This entails innovations spanning algorithms, frameworks, communication libraries, and system design. In this paper, we describe the philosophy, design, and implementation of Intel Machine Learning Scalability Library (MLSL) and present proof-points demonstrating scaling DL training on 100s to 1000s of nodes across Cloud and HPC systems. △ Less","24 January, 2018",https://arxiv.org/pdf/1801.08030
Estimation of Variance and Spatial Correlation Width for Fine-scale Measurement Error in Digital Elevation Model,Mykhail Uss;Benoit Vozel;Vladimir Lukin;Kacem Chehdi,"In this paper, we borrow from blind noise parameter estimation (BNPE) methodology early developed in the image processing field an original and innovative no-reference approach to estimate Digital Elevation Model (DEM) vertical error parameters without resorting to a reference DEM. The challenges associated with the proposed approach related to the physical nature of the error and its multifactor structure in DEM are discussed in detail. A suitable multivariate method is then developed for estimating the error in gridded DEM. It is built on a recently proposed vectorial BNPE method for estimating spatially correlated noise using Noise Informative areas and Fractal Brownian Motion. The newly multivariate method is derived to estimate the effect of the stacking procedure and that of the epipolar line error on local (fine-scale) standard deviation and autocorrelation function width of photogrammetric DEM measurement error. Applying the new estimator to ASTER GDEM2 and ALOS World 3D DEMs, good agreement of derived estimates with results available in the literature is evidenced. In future works, the proposed no-reference method for analyzing DEM error can be extended to a larger number of predictors for accounting for other factors influencing remote sensing (RS) DEM accuracy. △ Less","23 January, 2018",https://arxiv.org/pdf/1801.07740
Towards Low-Latency and Ultra-Reliable Virtual Reality,Mohammed S. Elbamby;Cristina Perfecto;Mehdi Bennis;Klaus Doppler,"Virtual Reality (VR) is expected to be one of the killer-applications in 5G networks. However, many technical bottlenecks and challenges need to be overcome to facilitate its wide adoption. In particular, VR requirements in terms of high-throughput, low-latency and reliable communication call for innovative solutions and fundamental research cutting across several disciplines. In view of this, this article discusses the challenges and enablers for ultra-reliable and low-latency VR. Furthermore, in an interactive VR gaming arcade case study, we show that a smart network design that leverages the use of mmWave communication, edge computing and proactive caching can achieve the future vision of VR over wireless. △ Less","23 January, 2018",https://arxiv.org/pdf/1801.07587
Realising the Right to Data Portability for the Domestic Internet of Things,Lachlan Urquhart;Neelima Sailaja;Derek McAuley,"There is an increasing role for the IT design community to play in regulation of emerging IT. Article 25 of the EU General Data Protection Regulation (GDPR) 2016 puts this on a strict legal basis by establishing the need for information privacy by design and default (PbD) for personal data-driven technologies. Against this backdrop, we examine legal, commercial and technical perspectives around the newly created legal right to data portability (RTDP) in GDPR. We are motivated by a pressing need to address regulatory challenges stemming from the Internet of Things (IoT). We need to find channels to support the protection of these new legal rights for users in practice. In Part I we introduce the internet of things and information PbD in more detail. We briefly consider regulatory challenges posed by the IoT and the nature and practical challenges surrounding the regulatory response of information privacy by design. In Part II, we look in depth at the legal nature of the RTDP, determining what it requires from IT designers in practice but also limitations on the right and how it relates to IoT. In Part III we focus on technical approaches that can support the realisation of the right. We consider the state of the art in data management architectures, tools and platforms that can provide portability, increased transparency and user control over the data flows. In Part IV, we bring our perspectives together to reflect on the technical, legal and business barriers and opportunities that will shape the implementation of the RTDP in practice, and how the relationships may shape emerging IoT innovation and business models. We finish with brief conclusions about the future for the RTDP and PbD in the IoT. △ Less","22 January, 2018",https://arxiv.org/pdf/1801.07189
Stable Phaseless Sampling and Reconstruction of Real-Valued Signals with Finite Rate of Innovations,Cheng Cheng;Qiyu Sun,"A spatial signal is defined by its evaluations on the whole domain. In this paper, we consider stable reconstruction of real-valued signals with finite rate of innovations (FRI), up to a sign, from their magnitude measurements on the whole domain or their phaseless samples on a discrete subset. FRI signals appear in many engineering applications such as magnetic resonance spectrum, ultra wide-band communication and electrocardiogram. For an FRI signal, we introduce an undirected graph to describe its topological structure. We establish the equivalence between the graph connectivity and phase retrievability of FRI signals, and we apply the graph connected component decomposition to find all FRI signals that have the same magnitude measurements as the original FRI signal has. We construct discrete sets with finite density explicitly so that magnitude measurements of FRI signals on the whole domain are determined by their samples taken on those discrete subsets. In this paper, we also propose a stable algorithm with linear complexity to reconstruct FRI signals from their phaseless samples on the above phaseless sampling set. The proposed algorithm is demonstrated theoretically and numerically to provide a suboptimal approximation to the original FRI signal in magnitude measurements. △ Less","16 January, 2018",https://arxiv.org/pdf/1801.05538
AliMe Assist: An Intelligent Assistant for Creating an Innovative E-commerce Experience,Feng-Lin Li;Minghui Qiu;Haiqing Chen;Xiongwei Wang;Xing Gao;Jun Huang;Juwei Ren;Zhongzhou Zhao;Weipeng Zhao;Lei Wang;Guwei Jin;Wei Chu,"We present AliMe Assist, an intelligent assistant designed for creating an innovative online shopping experience in E-commerce. Based on question answering (QA), AliMe Assist offers assistance service, customer service, and chatting service. It is able to take voice and text input, incorporate context to QA, and support multi-round interaction. Currently, it serves millions of customer questions per day and is able to address 85% of them. In this paper, we demonstrate the system, present the underlying techniques, and share our experience in dealing with real-world QA in the E-commerce field. △ Less","12 January, 2018",https://arxiv.org/pdf/1801.05032
Innovative Non-parametric Texture Synthesis via Patch Permutations,Ryan Webster,"In this work, we present a non-parametric texture synthesis algorithm capable of producing plausible images without copying large tiles of the exemplar. We focus on a simple synthesis algorithm, where we explore two patch match heuristics; the well known Bidirectional Similarity (BS) measure and a heuristic that finds near permutations using the solution of an entropy regularized optimal transport (OT) problem. Innovative synthesis is achieved with a small patch size, where global plausibility relies on the qualities of the match. For OT, less entropic regularization also meant near permutations and more plausible images. We examine the tile maps of the synthesized images, showing that they are indeed novel superpositions of the input and contain few or no verbatim copies. Synthesis results are compared to a statistical method, namely a random convolutional network. We conclude by remarking simple algorithms using only the input image can synthesize textures decently well and call for more modest approaches in future algorithm design. △ Less","14 January, 2018",https://arxiv.org/pdf/1801.04619
A Workload Analysis of NSF's Innovative HPC Resources Using XDMoD,Nikolay A. Simakov;Joseph P. White;Robert L. DeLeon;Steven M. Gallo;Matthew D. Jones;Jeffrey T. Palmer;Benjamin Plessinger;Thomas R. Furlani,"Workload characterization is an integral part of performance analysis of high performance computing (HPC) systems. An understanding of workload properties sheds light on resource utilization and can be used to inform performance optimization both at the software and system configuration levels. It can provide information on how computational science usage modalities are changing that could potentially aid holistic capacity planning for the wider HPC ecosystem. Here, we report on the results of a detailed workload analysis of the portfolio of supercomputers comprising the NSF Innovative HPC program in order to characterize its past and current workload and look for trends to understand the nature of how the broad portfolio of computational science research is being supported and how it is changing over time. The workload analysis also sought to illustrate a wide variety of usage patterns and performance requirements for jobs running on these systems. File system performance, memory utilization and the types of parallelism employed by users (MPI, threads, etc) were also studied for all systems for which job level performance data was available. △ Less","12 January, 2018",https://arxiv.org/pdf/1801.04306
Arhuaco: Deep Learning and Isolation Based Security for Distributed High-Throughput Computing,A. Gomez Ramirez;C. Lara;L. Betev;D. Bilanovic;U. Kebschull,"Grid computing systems require innovative methods and tools to identify cybersecurity incidents and perform autonomous actions i.e. without administrator intervention. They also require methods to isolate and trace job payload activity in order to protect users and find evidence of malicious behavior. We introduce an integrated approach of security monitoring via Security by Isolation with Linux Containers and Deep Learning methods for the analysis of real time data in Grid jobs running inside virtualized High-Throughput Computing infrastructure in order to detect and prevent intrusions. A dataset for malware detection in Grid computing is described. We show in addition the utilization of generative methods with Recurrent Neural Networks to improve the collected dataset. We present Arhuaco, a prototype implementation of the proposed methods. We empirically study the performance of our technique. The results show that Arhuaco outperforms other methods used in Intrusion Detection Systems for Grid Computing. The study is carried out in the ALICE Collaboration Grid, part of the Worldwide LHC Computing Grid. △ Less","12 January, 2018",https://arxiv.org/pdf/1801.04179
Multisensor Online Transfer Learning for 3D LiDAR-based Human Detection with a Mobile Robot,Zhi Yan;Li Sun;Tom Duckett;Nicola Bellotto,"Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier, exploiting a so-called trajectory probability. Our framework uses this probability to check whether new detections belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used. △ Less","31 July, 2018",https://arxiv.org/pdf/1801.04137
Evorus: A Crowd-powered Conversational Assistant Built to Automate Itself Over Time,Ting-Hao 'Kenneth' Huang;Joseph Chee Chang;Jeffrey P. Bigham,"Crowd-powered conversational assistants have been shown to be more robust than automated systems, but do so at the cost of higher response latency and monetary costs. A promising direction is to combine the two approaches for high quality, low latency, and low cost solutions. In this paper, we introduce Evorus, a crowd-powered conversational assistant built to automate itself over time by (i) allowing new chatbots to be easily integrated to automate more scenarios, (ii) reusing prior crowd answers, and (iii) learning to automatically approve response candidates. Our 5-month-long deployment with 80 participants and 281 conversations shows that Evorus can automate itself without compromising conversation quality. Crowd-AI architectures have long been proposed as a way to reduce cost and latency for crowd-powered systems; Evorus demonstrates how automation can be introduced successfully in a deployed system. Its architecture allows future researchers to make further innovation on the underlying automated components in the context of a deployed open domain dialog system. △ Less","9 January, 2018",https://arxiv.org/pdf/1801.02668
"A Novel Framework for DDoS Detectionin Huge Scale Networks, Thanksto QoS Features",Hamed Rezaei;Nima Ghazanfari motlagha;Yaghoub Farjamib;Mohammad Hossein Yektae,"It is not been a long time since the advent of cloud-based technology. However, in this short period of timeseveral advantages and disadvantages have been emerged. This is a problem solving technology with some threats as well. These threats and potential damages are not only limited to the cloud-based technologies, but they have always been against computer network infrastructures. One of these examples is Distributed Denial-of-Service (DDoS) intrusion which is of course one of the most complex and the most dangerous types of attacks. The impact of this type of attack, due to its powerful nature, is much higher on cloud systems since in case of occurrence, the service providers lose their services completely as well as their reputationand loyal customers. This, apparently,can even lead to the collapse of the stock and other destructive consequences. On the other hand, due to the properties of cloud service providers including large-scale infrastructures, DDoS intrusion detection algorithms need high sensitivity, innovation, and general improvements. Traditional structures of DDoS attack detection algorithms are designed for small-scale networks or at most for application camps. Lack of efficient algorithm is seemingly apparentfor the large-scale networks. Therefore, in this context we utilize standardmethods as well as a proposed hybrid protocol which is more appropriate in connection with cloud structures in order to detect DDoS attacks △ Less","7 January, 2018",https://arxiv.org/pdf/1801.02300
FlexONC: Joint Cooperative Forwarding and Network Coding with Precise Encoding Conditions,Somayeh Kafaie;Yuanzhu Chen;Mohamed Hossam Ahmed;Octavia A. Dobre,"In recent years, network coding has emerged as an innovative method that helps a wireless network approach its maximum capacity, by combining multiple unicasts in one broadcast. However, the majority of research conducted in this area is yet to fully utilize the broadcasting nature of wireless networks, and still assumes fixed route between the source and destination that every packet should travel through. This assumption not only limits coding opportunities, but can also cause buffer overflow in some specific intermediate nodes. Although some studies considered scattering of the flows dynamically in the network, they still face some limitations. This paper explains pros and cons of some prominent research in network coding and proposes a Flexible and Opportunistic Network Coding scheme (FlexONC) as a solution to such issues. Furthermore, this research discovers that the conditions used in previous studies to combine packets of different flows are overly optimistic and would affect the network performance adversarially. Therefore, we provide a more accurate set of rules for packet encoding. The experimental results show that FlexONC outperforms previous methods especially in networks with high bit error rate, by better utilizing redundant packets spread in the network. △ Less","7 January, 2018",https://arxiv.org/pdf/1801.02134
Network Coding with Link Layer Cooperation in Wireless Mesh Networks,Somayeh Kafaie;Yuanzhu Chen;Mohamed Hossam Ahmed;Octavia A. Dobre,"In recent years, network coding has emerged as an innovative method that helps wireless network approaches its maximum capacity, by combining multiple unicasts in one broadcast. However, the majority of research conducted in this area is yet to fully utilize the broadcasting nature of wireless networks, and still assumes fixed route between the source and destination that every packet should travel through. This assumption not only limits coding opportunities, but can also cause buffer overflow in some specific intermediate nodes. Although some studies considered scattering of the flows dynamically in the network, they still face some limitations. This paper explains pros and cons of some prominent research in network coding and proposes FlexONC (Flexible and Opportunistic Network Coding) as a solution to such issues. The performance results show that FlexONC outperforms previous methods especially in worse quality networks, by better utilizing redundant packets spread in the network. △ Less","6 January, 2018",https://arxiv.org/pdf/1801.02117
A First Step in the Co-Evolution of Blockchain and Ontologies: Towards Engineering an Ontology of Governance at the Blockchain Protocol Level,Henry M. Kim;Marek Laskowski;Ning Nan,"At the beginning of 2018, there is a growing belief that blockchain technologies constitute a revolutionary innovation in how we transfer value electronically. In that vein, blockchain may be a suitable complement to ontologies to achieve a big part of the vision of the semantic Web by Tim Berners-Lee. We believe that if this complementarity is to be achieved blockchain and ontologies must co-evolve. In this paper, we focus on what and how to engineer models, methods, designs, and implementations for this co-evolution. As a first step in this co-evolution, we propose a conceptual design of a governance ontology represented as meta-data tags to be embedded and instantiated in a smart contract at the blockchain protocol level. We develop this design by examining and analyzing smart contracts from the infamous The DAO experiment on the Ethereum blockchain. We believe there are two contributions of this paper: it serves to inform and implore the blockchain and ontology communities to recognize and collaborate with each other; and it outlines a roadmap for engineering artifacts to bridge the gap between blockchain community focus on protocol-level blockchain interoperability and the ontology community focus on semantic-level interoperability. △ Less","6 January, 2018",https://arxiv.org/pdf/1801.02027
The impact of bundling licensed and unlicensed wireless service,Xu Wang;Randall Berry,"Unlicensed spectrum has been viewed as a way to increase competition in wireless access and promote innovation in new technologies and business models. However, several recent papers have shown that the openness of such spectrum can also lead to it becoming over congested when used by competing wireless service providers (SPs). This in turn can result in the SPs making no profit and may deter them from entering the market. However, this prior work assumes that unlicensed access is a separate service from any service offered using licensed spectrum. Here, we instead consider the more common case were service providers bundle both licensed and unlicensed spectrum as a single service and offer this with a single price. We analyze a model for such a market and show that in this case SPs are able to gain higher profit than the case without bundling. It is also possible to get higher social welfare with bundling. Moreover, we explore the case where SPs are allowed to manage the customers' average percentage of time they receive service on unlicensed spectrum and characterize the social welfare gap between the profit maximizing and social welfare maximizing setting. △ Less","6 January, 2018",https://arxiv.org/pdf/1801.01989
DeepJ: Style-Specific Music Generation,Huanru Henry Mao;Taylor Shin;Garrison W. Cottrell,"Recent advances in deep neural networks have enabled algorithms to compose music that is comparable to music composed by humans. However, few algorithms allow the user to generate music with tunable parameters. The ability to tune properties of generated music will yield more practical benefits for aiding artists, filmmakers, and composers in their creative tasks. In this paper, we introduce DeepJ - an end-to-end generative model that is capable of composing music conditioned on a specific mixture of composer styles. Our innovations include methods to learn musical style and music dynamics. We use our model to demonstrate a simple technique for controlling the style of generated music as a proof of concept. Evaluation of our model using human raters shows that we have improved over the Biaxial LSTM approach. △ Less","2 January, 2018",https://arxiv.org/pdf/1801.00887
"Emo, Love, and God: Making Sense of Urban Dictionary, a Crowd-Sourced Online Dictionary",Dong Nguyen;Barbara McGillivray;Taha Yasseri,"The Internet facilitates large-scale collaborative projects and the emergence of Web 2.0 platforms, where producers and consumers of content unify, has drastically changed the information market. On the one hand, the promise of the ""wisdom of the crowd"" has inspired successful projects such as Wikipedia, which has become the primary source of crowd-based information in many languages. On the other hand, the decentralized and often un-monitored environment of such projects may make them susceptible to low quality content. In this work, we focus on Urban Dictionary, a crowd-sourced online dictionary. We combine computational methods with qualitative annotation and shed light on the overall features of Urban Dictionary in terms of growth, coverage and types of content. We measure a high presence of opinion-focused entries, as opposed to the meaning-focused entries that we expect from traditional dictionaries. Furthermore, Urban Dictionary covers many informal, unfamiliar words as well as proper nouns. Urban Dictionary also contains offensive content, but highly offensive content tends to receive lower scores through the dictionary's voting system. The low threshold to include new material in Urban Dictionary enables quick recording of new words and new meanings, but the resulting heterogeneous content can pose challenges in using Urban Dictionary as a source to study language innovation. △ Less","5 April, 2018",https://arxiv.org/pdf/1712.08647
Work Analysis with Resource-Aware Session Types,Ankush Das;Jan Hoffmann;Frank Pfenning,"While there exist several successful techniques for supporting programmers in deriving static resource bounds for sequential code, analyzing the resource usage of message-passing concurrent processes poses additional challenges. To meet these challenges, this article presents an analysis for statically deriving worst-case bounds on the total work performed by message-passing processes. To decompose interacting processes into components that can be analyzed in isolation, the analysis is based on novel resource-aware session types, which describe protocols and resource contracts for inter-process communication. A key innovation is that both messages and processes carry potential to share and amortize cost while communicating. To symbolically express resource usage in a setting without static data structures and intrinsic sizes, resource contracts describe bounds that are functions of interactions between processes. Resource-aware session types combine standard binary session types and type-based amortized resource analysis in a linear type system. This type system is formulated for a core session-type calculus of the language SILL and proved sound with respect to a multiset-based operational cost semantics that tracks the total number of messages that are exchanged in a system. The effectiveness of the analysis is demonstrated by analyzing standard examples from amortized analysis and the literature on session types and by a comparative performance analysis of different concurrent programs implementing the same interface. △ Less","26 April, 2018",https://arxiv.org/pdf/1712.08310
PacGAN: The power of two samples in generative adversarial networks,Zinan Lin;Ashish Khetan;Giulia Fanti;Sewoong Oh,"Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why existing approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell [Bla53]---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well. △ Less","2 November, 2018",https://arxiv.org/pdf/1712.04086
Blind Multiclass Ensemble Classification,Panagiotis A. Traganitis;Alba Pagès-Zamora;Georgios B. Giannakis,"The rising interest in pattern recognition and data analytics has spurred the development of innovative machine learning algorithms and tools. However, as each algorithm has its strengths and limitations, one is motivated to judiciously fuse multiple algorithms in order to find the ""best"" performing one, for a given dataset. Ensemble learning aims at such high-performance meta-algorithm, by combining the outputs from multiple algorithms. The present work introduces a blind scheme for learning from ensembles of classifiers, using a moment matching method that leverages joint tensor and matrix factorization. Blind refers to the combiner who has no knowledge of the ground-truth labels that each classifier has been trained on. A rigorous performance analysis is derived and the proposed scheme is evaluated on synthetic and real datasets. △ Less","20 July, 2018",https://arxiv.org/pdf/1712.02903
Effective Neural Solution for Multi-Criteria Word Segmentation,Han He;Lei Wu;Hua Yan;Zhimin Gao;Yi Feng;George Townsend,"We present a simple yet elegant solution to train a single joint model on multi-criteria corpora for Chinese Word Segmentation (CWS). Our novel design requires no private layers in model architecture, instead, introduces two artificial tokens at the beginning and ending of input sentence to specify the required target criteria. The rest of the model including Long Short-Term Memory (LSTM) layer and Conditional Random Fields (CRFs) layer remains unchanged and is shared across all datasets, keeping the size of parameter collection minimal and constant. On Bakeoff 2005 and Bakeoff 2008 datasets, our innovative design has surpassed both single-criterion and multi-criteria state-of-the-art learning results. To the best knowledge, our design is the first one that has achieved the latest high performance on such large scale datasets. Source codes and corpora of this paper are available on GitHub. △ Less","4 January, 2018",https://arxiv.org/pdf/1712.02856
Learning with Biased Complementary Labels,Xiyu Yu;Tongliang Liu;Mingming Gong;Dacheng Tao,"In this paper, we study the classification problem in which we have access to easily obtainable surrogate for true labels, namely complementary labels, which specify classes that observations do \textbf{not} belong to. Let Y and \bar{Y} be the true and complementary labels, respectively. We first model the annotation of complementary labels via transition probabilities P(\bar{Y}=i|Y=j), i\neq j\in\{1,\cdots,c\}, where c is the number of classes. Previous methods implicitly assume that P(\bar{Y}=i|Y=j), \forall i\neq j, are identical, which is not true in practice because humans are biased toward their own experience. For example, as shown in Figure 1, if an annotator is more familiar with monkeys than prairie dogs when providing complementary labels for meerkats, she is more likely to employ ""monkey"" as a complementary label. We therefore reason that the transition probabilities will be different. In this paper, we propose a framework that contributes three main innovations to learning with \textbf{biased} complementary labels: (1) It estimates transition probabilities with no bias. (2) It provides a general method to modify traditional loss functions and extends standard deep neural network classifiers to learn with biased complementary labels. (3) It theoretically ensures that the classifier learned with complementary labels converges to the optimal one learned with true labels. Comprehensive experiments on several benchmark datasets validate the superiority of our method to current state-of-the-art methods. △ Less","7 August, 2018",https://arxiv.org/pdf/1711.09535
Generative Adversarial Positive-Unlabelled Learning,Ming Hou;Brahim Chaib-draa;Chao Li;Qibin Zhao,"In this work, we consider the task of classifying binary positive-unlabeled (PU) data. The existing discriminative learning based PU models attempt to seek an optimal reweighting strategy for U data, so that a decent decision boundary can be found. However, given limited P data, the conventional PU models tend to suffer from overfitting when adapted to very flexible deep neural networks. In contrast, we are the first to innovate a totally new paradigm to attack the binary PU task, from perspective of generative learning by leveraging the powerful generative adversarial networks (GAN). Our generative positive-unlabeled (GenPU) framework incorporates an array of discriminators and generators that are endowed with different roles in simultaneously producing positive and negative realistic samples. We provide theoretical analysis to justify that, at equilibrium, GenPU is capable of recovering both positive and negative data distributions. Moreover, we show GenPU is generalizable and closely related to the semi-supervised classification. Given rather limited P data, experiments on both synthetic and real-world dataset demonstrate the effectiveness of our proposed framework. With infinite realistic and diverse sample streams generated from GenPU, a very flexible classifier can then be trained using deep neural networks. △ Less","4 April, 2018",https://arxiv.org/pdf/1711.08054
Zero-Annotation Object Detection with Web Knowledge Transfer,Qingyi Tao;Hao Yang;Jianfei Cai,"Object detection is one of the major problems in computer vision, and has been extensively studied. Most of the existing detection works rely on labor-intensive supervision, such as ground truth bounding boxes of objects or at least image-level annotations. On the contrary, we propose an object detection method that does not require any form of human annotation on target tasks, by exploiting freely available web images. In order to facilitate effective knowledge transfer from web images, we introduce a multi-instance multi-label domain adaption learning framework with two key innovations. First of all, we propose an instance-level adversarial domain adaptation network with attention on foreground objects to transfer the object appearances from web domain to target domain. Second, to preserve the class-specific semantic structure of transferred object features, we propose a simultaneous transfer mechanism to transfer the supervision across domains through pseudo strong label generation. With our end-to-end framework that simultaneously learns a weakly supervised detector and transfers knowledge across domains, we achieved significant improvements over baseline methods on the benchmark datasets. △ Less","1 August, 2018",https://arxiv.org/pdf/1711.05954
A New Hybrid-parameter Recurrent Neural Networks for Online Handwritten Chinese Character Recognition,Haiqing Ren;Weiqiang Wang,"The recurrent neural network (RNN) is appropriate for dealing with temporal sequences. In this paper, we present a deep RNN with new features and apply it for online handwritten Chinese character recognition. Compared with the existing RNN models, three innovations are involved in the proposed system. First, a new hidden layer function for RNN is proposed for learning temporal information better. we call it Memory Pool Unit (MPU). The proposed MPU has a simple architecture. Second, a new RNN architecture with hybrid parameter is presented, in order to increasing the expression capacity of RNN. The proposed hybrid-parameter RNN has parameter changes when calculating the iteration at temporal dimension. Third, we make a adaptation that all the outputs of each layer are stacked as the output of network. Stacked hidden layer states combine all the hidden layer states for increasing the expression capacity. Experiments are carried out on the IAHCC-UCAS2016 dataset and the CASIA-OLHWDB1.1 dataset. The experimental results show that the hybrid-parameter RNN obtain a better recognition performance with higher efficiency (fewer parameters and faster speed). And the proposed Memory Pool Unit is proved to be a simple hidden layer function and obtains a competitive recognition results. △ Less","29 July, 2018",https://arxiv.org/pdf/1711.02809
An Innovations Approach to Viterbi Decoding of Convolutional Codes,Masato Tajima,"We introduce the notion of innovations for Viterbi decoding of convolutional codes. First we define a kind of innovation corresponding to the received data, i.e., the input to a Viterbi decoder. Then the structure of a Scarce-State-Transition (SST) Viterbi decoder is derived in a natural manner. It is shown that the newly defined innovation is just the input to the main decoder in an SST Viterbi decoder and generates the same syndrome as the original received data does. A similar result holds for Quick-Look-In (QLI) codes as well. In this case, however, the precise innovation is not defined. We see that this innovation-like quantity is related to the linear smoothed estimate of the information. The essence of innovations approach to a linear filtering problem is first to whiten the observed data, and then to treat the resulting simpler white-noise observations problem. In our case, this corresponds to the reduction of decoding complexity in the main decoder in an SST Viterbi decoder. We show the distributions related to the main decoder (i.e., the input distribution and the state distribution in the code trellis for the main decoder) are much biased under moderately noisy conditions. We see that these biased distributions actually lead to the complexity reduction in the main decoder. Furthermore, it is shown that the proposed innovations approach can be extended to maximum-likelihood (ML) decoding of block codes as well. △ Less","3 November, 2018",https://arxiv.org/pdf/1710.11310
"Synergy in the Knowledge Base of U.S. Innovation Systems at National, State, and Regional Levels: The Contributions of High-Tech Manufacturing and Knowledge-Intensive Services",Loet Leydesdorff;Caroline S. Wagner;Igone Porto-Gomez;Jordan A. Comins;Fred Phillips,"Using information theory, we measure innovation systemness as synergy among size-classes, zip-codes, and technological classes (NACE-codes) for 8.5 million American companies. The synergy at the national level is decomposed at the level of states, Core-Based Statistical Areas (CBSA), and Combined Statistical Areas (CSA). We zoom in to the state of California and in more detail to Silicon Valley. Our results do not support the assumption of a national system of innovations in the U.S.A. Innovation systems appear to operate at the level of the states; the CBSA are too small, so that systemness spills across their borders. Decomposition of the sample in terms of high-tech manufacturing (HTM), medium-high-tech manufacturing (MHTM), knowledge-intensive services (KIS), and high-tech services (HTKIS) does not change this pattern, but refines it. The East Coast -- New Jersey, Boston, and New York -- and California are the major players, with Texas a third one in the case of HTKIS. Chicago and industrial centers in the Midwest also contribute synergy. Within California, Los Angeles contributes synergy in the sectors of manufacturing, the San Francisco area in KIS. Knowledge-intensive services in Silicon Valley and the Bay area -- a CSA composed of seven CBSA -- spill over to other regions and even globally. △ Less","19 November, 2018",https://arxiv.org/pdf/1710.11017
Local Word Vectors Guiding Keyphrase Extraction,Eirini Papagiannopoulou;Grigorios Tsoumakas,"Automated keyphrase extraction is a fundamental textual information processing task concerned with the selection of representative phrases from a document that summarize its content. This work presents a novel unsupervised method for keyphrase extraction, whose main innovation is the use of local word embeddings (in particular GloVe vectors), i.e., embeddings trained from the single document under consideration. We argue that such local representation of words and keyphrases are able to accurately capture their semantics in the context of the document they are part of, and therefore can help in improving keyphrase extraction quality. Empirical results offer evidence that indeed local representations lead to better keyphrase extraction results compared to both embeddings trained on very large third corpora or larger corpora consisting of several documents of the same scientific field and to other state-of-the-art unsupervised keyphrase extraction methods. △ Less","13 April, 2018",https://arxiv.org/pdf/1710.07503
UG^2: a Video Benchmark for Assessing the Impact of Image Restoration and Enhancement on Automatic Visual Recognition,Rosaura G. Vidal;Sreya Banerjee;Klemen Grm;Vitomir Struc;Walter J. Scheirer,"Advances in image restoration and enhancement techniques have led to discussion about how such algorithmscan be applied as a pre-processing step to improve automatic visual recognition. In principle, techniques like deblurring and super-resolution should yield improvements by de-emphasizing noise and increasing signal in an input image. But the historically divergent goals of the computational photography and visual recognition communities have created a significant need for more work in this direction. To facilitate new research, we introduce a new benchmark dataset called UG^2, which contains three difficult real-world scenarios: uncontrolled videos taken by UAVs and manned gliders, as well as controlled videos taken on the ground. Over 160,000 annotated frames forhundreds of ImageNet classes are available, which are used for baseline experiments that assess the impact of known and unknown image artifacts and other conditions on common deep learning-based object classification approaches. Further, current image restoration and enhancement techniques are evaluated by determining whether or not theyimprove baseline classification performance. Results showthat there is plenty of room for algorithmic innovation, making this dataset a useful tool going forward. △ Less","6 February, 2018",https://arxiv.org/pdf/1710.02909
A D2D-based Protocol for Ultra-Reliable Wireless Communications for Industrial Automation,Liang Liu;Wei Yu,"As one indispensable use case for the 5G wireless systems on the roadmap, ultra-reliable and low latency communications (URLLC) is a crucial requirement for the coming era of wireless industrial automation. This paper aims to develop communication techniques for making such a paradigm shift from the conventional human-type broadband communications to the emerging machine-type URLLC. One fundamental task for URLLC is to deliver a short command from the controller to each actuator within the stringent delay requirement and also with high-reliability in the downlink. Motivated by the geographic feature in industrial automation that in the factories many tasks are assigned to different groups of devices who work in close proximity to each other and thus can form clusters of reliable device-to-device (D2D) networks, this paper proposes a novel two-phase transmission protocol for achieving the above goal. Specifically, in the first phase within the latency requirement, the multi-antenna base station (BS) combines the messages of each group together and multicasts them to the corresponding groups; while in the second phase, the devices that have decoded the messages successfully, who are defined as the leaders, help relay the messages to the other devices in their groups. Under this protocol, we further design an innovative leader selection based beamforming strategy at the BS by utilizing the sparse optimization technique, which leads to the desired sparsity pattern in user activity, i.e., at least one leader exists in each group, in the first phase, thus making full utilization of the reliable D2D networks in the second phase. Simulation results are provided to show that the proposed two-phase transmission protocol considerably improves the reliability of the whole system within the stringent latency requirement as compared to other existing schemes for URLLC such as Occupy CoW. △ Less","14 May, 2018",https://arxiv.org/pdf/1710.01265
Cross-layer Balanced and Reliable Opportunistic Routing Algorithm for Mobile Ad Hoc Networks,Ning Li;Jose-Fernan Martinez-Ortega;Vicente Hernandez Diaz,"For improving the efficiency and the reliability of the opportunistic routing algorithm, in this paper, we propose the cross-layer and reliable opportunistic routing algorithm (CBRT) for Mobile Ad Hoc Networks, which introduces the improved efficiency fuzzy logic and humoral regulation inspired topology control into the opportunistic routing algorithm. In CBRT, the inputs of the fuzzy logic system are the relative variance (rv) of the metrics rather than the values of the metrics, which reduces the number of fuzzy rules dramatically. Moreover, the number of fuzzy rules does not increase when the number of inputs increases. For reducing the control cost, in CBRT, the node degree in the candidate relays set is a range rather than a constant number. The nodes are divided into different categories based on their node degree in the candidate relays set. The nodes adjust their transmission range based on which categories that they belong to. Additionally, for investigating the effection of the node mobility on routing performance, we propose a link lifetime prediction algorithm which takes both the moving speed and moving direction into account. In CBRT, the source node determines the relaying priorities of the relaying nodes based on their utilities. The relaying node which the utility is large will have high priority to relay the data packet. By these innovations, the network performance in CBRT is much better than that in ExOR, however, the computation complexity is not increased in CBRT. △ Less","19 May, 2018",https://arxiv.org/pdf/1710.00105
Recurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ Segmentation,Qihang Yu;Lingxi Xie;Yan Wang;Yuyin Zhou;Elliot K. Fishman;Alan L. Yuille,"We aim at segmenting small organs (e.g., the pancreas) from abdominal CT scans. As the target often occupies a relatively small region in the input image, deep neural networks can be easily confused by the complex and variable background. To alleviate this, researchers proposed a coarse-to-fine approach, which used prediction from the first (coarse) stage to indicate a smaller input region for the second (fine) stage. Despite its effectiveness, this algorithm dealt with two stages individually, which lacked optimizing a global energy function, and limited its ability to incorporate multi-stage visual cues. Missing contextual information led to unsatisfying convergence in iterations, and that the fine stage sometimes produced even lower segmentation accuracy than the coarse stage. This paper presents a Recurrent Saliency Transformation Network. The key innovation is a saliency transformation module, which repeatedly converts the segmentation probability map from the previous iteration as spatial weights and applies these weights to the current iteration. This brings us two-fold benefits. In training, it allows joint optimization over the deep networks dealing with different input scales. In testing, it propagates multi-stage visual information throughout iterations to improve segmentation accuracy. Experiments in the NIH pancreas segmentation dataset demonstrate the state-of-the-art accuracy, which outperforms the previous best by an average of over 2%. Much higher accuracies are also reported on several small organs in a larger dataset collected by ourselves. In addition, our approach enjoys better convergence properties, making it more efficient and reliable in practice. △ Less","7 April, 2018",https://arxiv.org/pdf/1709.04518
"Making ""fetch"" happen: The influence of social and linguistic context on nonstandard word growth and decline",Ian Stewart;Jacob Eisenstein,"In an online community, new words come and go: today's ""haha"" may be replaced by tomorrow's ""lol."" Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the system in which it takes part. To investigate the links between social and structural factors in language change, we undertake a large-scale analysis of nonstandard word growth in the online community Reddit. We find that dissemination across many linguistic contexts is a sign of growth: words that appear in more linguistic contexts grow faster and survive longer. We also find that social dissemination likely plays a less important role in explaining word growth and decline than previously hypothesized. △ Less","31 August, 2018",https://arxiv.org/pdf/1709.00345
Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork,Wei Ren Tan;Chee Seng Chan;Hernan Aguirre;Kiyoshi Tanaka,"This paper proposes a series of new approaches to improve Generative Adversarial Network (GAN) for conditional image synthesis and we name the proposed model as ArtGAN. One of the key innovation of ArtGAN is that, the gradient of the loss function w.r.t. the label (randomly assigned to each generated image) is back-propagated from the categorical discriminator to the generator. With the feedback from the label information, the generator is able to learn more efficiently and generate image with better quality. Inspired by recent works, an autoencoder is incorporated into the categorical discriminator for additional complementary information. Last but not least, we introduce a novel strategy to improve the image quality. In the experiments, we evaluate ArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results showed that our proposed model outperforms the state-of-the-art results on CIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN is able to generate plausible-looking images on Oxford-102 and CUB-200, as well as able to draw realistic artworks based on style, artist, and genre. The source code and models are available at: https://github.com/cs-chan/ArtGAN △ Less","23 August, 2018",https://arxiv.org/pdf/1708.09533
Divide-and-Conquer Checkpointing for Arbitrary Programs with No User Annotation,Jeffrey Mark Siskind;Barak A. Pearlmutter,"Classical reverse-mode automatic differentiation (AD) imposes only a small constant-factor overhead in operation count over the original computation, but has storage requirements that grow, in the worst case, in proportion to the time consumed by the original computation. This storage blowup can be ameliorated by checkpointing, a process that reorders application of classical reverse-mode AD over an execution interval to tradeoff space \vs\ time. Application of checkpointing in a divide-and-conquer fashion to strategically chosen nested execution intervals can break classical reverse-mode AD into stages which can reduce the worst-case growth in storage from linear to sublinear. Doing this has been fully automated only for computations of particularly simple form, with checkpoints spanning execution intervals resulting from a limited set of program constructs. Here we show how the technique can be automated for arbitrary computations. The essential innovation is to apply the technique at the level of the language implementation itself, thus allowing checkpoints to span any execution interval. △ Less","29 March, 2018",https://arxiv.org/pdf/1708.06799
Will SDN be part of 5G?,Zainab Zaidi;Vasilis Friderikos;Zarrar Yousaf;Simon Fletcher;Mischa Dohler;Hamid Aghvami,"For many, this is no longer a valid question and the case is considered settled with SDN/NFV (Software Defined Networking/Network Function Virtualization) providing the inevitable innovation enablers solving many outstanding management issues regarding 5G. However, given the monumental task of softwarization of radio access network (RAN) while 5G is just around the corner and some companies have started unveiling their 5G equipment already, the concern is very realistic that we may only see some point solutions involving SDN technology instead of a fully SDN-enabled RAN. This survey paper identifies all important obstacles in the way and looks at the state of the art of the relevant solutions. This survey is different from the previous surveys on SDN-based RAN as it focuses on the salient problems and discusses solutions proposed within and outside SDN literature. Our main focus is on fronthaul, backward compatibility, supposedly disruptive nature of SDN deployment, business cases and monetization of SDN related upgrades, latency of general purpose processors (GPP), and additional security vulnerabilities, softwarization brings along to the RAN. We have also provided a summary of the architectural developments in SDN-based RAN landscape as not all work can be covered under the focused issues. This paper provides a comprehensive survey on the state of the art of SDN-based RAN and clearly points out the gaps in the technology. △ Less","7 February, 2018",https://arxiv.org/pdf/1708.05096
Technology networks: the autocatalytic origins of innovation,Lorenzo Napolitano;Evangelos Evangelou;Emanuele Pugliese;Paolo Zeppini;Graham Room,"We analyse the autocatalytic structure of technological networks and evaluate its significance for the dynamics of innovation patenting. To this aim, we define a directed network of technological fields based on the International Patents Classification, in which a source node is connected to a receiver node via a link if patenting activity in the source field anticipates patents in the receiver field in the same region more frequently than we would expect at random. We show that the evolution of the technology network is compatible with the presence of a growing autocatalytic structure, i.e. a portion of the network in which technological fields mutually benefit from being connected to one another. We further show that technological fields in the core of the autocatalytic set display greater fitness, i.e. they tend to appear in a greater number of patents, thus suggesting the presence of positive spillovers as well as positive reinforcement. Finally, we observe that core shifts take place whereby different groups of technology fields alternate within the autocatalytic structure; this points to the importance of recombinant innovation taking place between close as well as distant fields of the hierarchical classification of technological fields. △ Less","19 April, 2018",https://arxiv.org/pdf/1708.03511
Harnessing Natural Experiments to Quantify the Causal Effect of Badges,Tomasz Kusmierczyk;Manuel Gomez-Rodriguez,"A wide variety of online platforms use digital badges to encourage users to take certain types of desirable actions. However, despite their growing popularity, their causal effect on users' behavior is not well understood. This is partly due to the lack of counterfactual data and the myriad of complex factors that influence users' behavior over time. As a consequence, their design and deployment lacks general principles. In this paper, we focus on first-time badges, which are awarded after a user takes a particular type of action for the first time, and study their causal effect by harnessing the delayed introduction of several badges in a popular Q&A website. In doing so, we introduce a novel causal inference framework for badges whose main technical innovations are a robust survival-based hypothesis testing procedure, which controls for the utility heterogeneity across users, and a bootstrap difference-in-differences method, which controls for the random fluctuations in users' behavior over time. We find that first-time badges steer users' behavior if the utility a user obtains from taking the corresponding action is sufficiently low, otherwise, the badge does not have a significant effect. Moreover, for badges that successfully steered user behavior, we perform a counterfactual analysis and show that they significantly improved the functioning of the site at a community level. △ Less","10 April, 2018",https://arxiv.org/pdf/1707.08160
Wide Inference Network for Image Denoising via Learning Pixel-distribution Prior,Peng Liu;Ruogu Fang,"We explore an innovative strategy for image denoising by using convolutional neural networks (CNN) to learn similar pixel-distribution features from noisy images. Many types of image noise follow a certain pixel-distribution in common, such as additive white Gaussian noise (AWGN). By increasing CNN's width with larger reception fields and more channels in each layer, CNNs can reveal the ability to extract more accurate pixel-distribution features. The key to our approach is a discovery that wider CNNs with more convolutions tend to learn the similar pixel-distribution features, which reveals a new strategy to solve low-level vision problems effectively that the inference mapping primarily relies on the priors behind the noise property instead of deeper CNNs with more stacked nonlinear layers. We evaluate our work, Wide inference Networks (WIN), on AWGN and demonstrate that by learning pixel-distribution features from images, WIN-based network consistently achieves significantly better performance than current state-of-the-art deep CNN-based methods in both quantitative and visual evaluations. \textit{Code and models are available at \url{https://github.com/cswin/WIN}}. △ Less","3 June, 2018",https://arxiv.org/pdf/1707.05414
Network dynamics of innovation processes,Iacopo Iacopini;Staša Milojević;Vito Latora,"We introduce a model for the emergence of innovations, in which cognitive processes are described as random walks on the network of links among ideas or concepts, and an innovation corresponds to the first visit of a node. The transition matrix of the random walk depends on the network weights, while in turn the weight of an edge is reinforced by the passage of a walker. The presence of the network naturally accounts for the mechanism of the adjacent possible, and the model reproduces both the rate at which novelties emerge and the correlations among them observed empirically. We show this by using synthetic networks and by studying real data sets on the growth of knowledge in different scientific disciplines. Edge-reinforced random walks on complex topologies offer a new modeling framework for the dynamics of correlated novelties and are another example of coevolution of processes and networks. △ Less","24 January, 2018",https://arxiv.org/pdf/1707.04239
A New Adaptive Video Super-Resolution Algorithm With Improved Robustness to Innovations,Ricardo Augusto Borsoi;Guilherme Holsbach Costa;José Carlos Moreira Bermudez,"In this paper, a new video super-resolution reconstruction (SRR) method with improved robustness to outliers is proposed. Although the R-LMS is one of the SRR algorithms with the best reconstruction quality for its computational cost, and is naturally robust to registration inaccuracies, its performance is known to degrade severely in the presence of innovation outliers. By studying the proximal point cost function representation of the R-LMS iterative equation, a better understanding of its performance under different situations is attained. Using statistical properties of typical innovation outliers, a new cost function is then proposed and two new algorithms are derived, which present improved robustness to outliers while maintaining computational costs comparable to that of R-LMS. Monte Carlo simulation results illustrate that the proposed method outperforms the traditional and regularized versions of LMS, and is competitive with state-of-the-art SRR methods at a much smaller computational cost. △ Less","17 August, 2018",https://arxiv.org/pdf/1706.04695
"FAST^2
: an Intelligent Assistant for Finding Relevant Papers",Zhe Yu;Tim Menzies,"Literature reviews are essential for any researcher trying to keep up to date with the burgeoning software engineering literature. FAST^2 is a novel tool for reducing the effort required for conducting literature reviews by assisting the researchers to find the next promising paper to read (among a set of unread papers). This paper describes FAST^2 and tests it on four large software engineering literature reviews conducted by Wahono (2015), Hall (2012), Radjenović (2013) and Kitchenham (2017). We find that FAST^2 is a faster and robust tool to assist researcher finding relevant SE papers which can compensate for the errors made by humans during the review process. The effectiveness of FAST^2 can be attributed to three key innovations: (1) a novel way of applying external domain knowledge (a simple two or three keyword search) to guide the initial selection of papers---which helps to find relevant research papers faster with less variances; (2) an estimator of the number of remaining relevant papers yet to be found---which in practical settings can be used to decide if the reviewing process needs to be terminated; (3) a novel self-correcting classification algorithm---automatically corrects itself, in cases where the researcher wrongly classifies a paper. △ Less","11 November, 2018",https://arxiv.org/pdf/1705.05420
End-to-End Simulation of 5G mmWave Networks,Marco Mezzavilla;Menglei Zhang;Michele Polese;Russell Ford;Sourjya Dutta;Sundeep Rangan;Michele Zorzi,"Due to its potential for multi-gigabit and low latency wireless links, millimeter wave (mmWave) technology is expected to play a central role in 5th generation cellular systems. While there has been considerable progress in understanding the mmWave physical layer, innovations will be required at all layers of the protocol stack, in both the access and the core network. Discrete-event network simulation is essential for end-to-end, cross-layer research and development. This paper provides a tutorial on a recently developed full-stack mmWave module integrated into the widely used open-source ns--3 simulator. The module includes a number of detailed statistical channel models as well as the ability to incorporate real measurements or ray-tracing data. The Physical (PHY) and Medium Access Control (MAC) layers are modular and highly customizable, making it easy to integrate algorithms or compare Orthogonal Frequency Division Multiplexing (OFDM) numerologies, for example. The module is interfaced with the core network of the ns--3 Long Term Evolution (LTE) module for full-stack simulations of end-to-end connectivity, and advanced architectural features, such as dual-connectivity, are also available. To facilitate the understanding of the module, and verify its correct functioning, we provide several examples that show the performance of the custom mmWave stack as well as custom congestion control algorithms designed specifically for efficient utilization of the mmWave channel. △ Less","5 February, 2018",https://arxiv.org/pdf/1705.02882
The Salesman's Improved Tours for Fundamental Classes,Sylvia Boyd;András Sebö,"Finding the exact integrality gap α for the LP relaxation of the metric Travelling Salesman Problem (TSP) has been an open problem for over thirty years, with little progress made. It is known that 4/3 \leq α\leq 3/2, and a famous conjecture states α= 4/3. For this problem, essentially two ""fundamental"" classes of instances have been proposed. This fundamental property means that in order to show that the integrality gap is at most ρ for all instances of metric TSP, it is sufficient to show it only for the instances in the fundamental class. However, despite the importance and the simplicity of such classes, no apparent effort has been deployed for improving the integrality gap bounds for them. In this paper we take a natural first step in this endeavour, and consider the 1/2-integer points of one such class. We successfully improve the upper bound for the integrality gap from 3/2 to 10/7 for a superclass of these points, as well as prove a lower bound of 4/3 for the superclass. Our methods involve innovative applications of tools from combinatorial optimization which have the potential to be more broadly applied. △ Less","29 October, 2018",https://arxiv.org/pdf/1705.02385
The Reactor: A fast and sample-efficient Actor-Critic agent for Reinforcement Learning,Audrunas Gruslys;Will Dabney;Mohammad Gheshlaghi Azar;Bilal Piot;Marc Bellemare;Remi Munos,"In this work we present a new agent architecture, called Reactor, which combines multiple algorithmic and architectural contributions to produce an agent with higher sample-efficiency than Prioritized Dueling DQN (Wang et al., 2016) and Categorical DQN (Bellemare et al., 2017), while giving better run-time performance than A3C (Mnih et al., 2016). Our first contribution is a new policy evaluation algorithm called Distributional Retrace, which brings multi-step off-policy updates to the distributional reinforcement learning setting. The same approach can be used to convert several classes of multi-step policy evaluation algorithms designed for expected value evaluation into distributional ones. Next, we introduce the \b{eta}-leave-one-out policy gradient algorithm which improves the trade-off between variance and bias by using action values as a baseline. Our final algorithmic contribution is a new prioritized replay algorithm for sequences, which exploits the temporal locality of neighboring observations for more efficient replay prioritization. Using the Atari 2600 benchmarks, we show that each of these innovations contribute to both the sample efficiency and final agent performance. Finally, we demonstrate that Reactor reaches state-of-the-art performance after 200 million frames and less than a day of training. △ Less","19 June, 2018",https://arxiv.org/pdf/1704.04651
"Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks",Andrea Soltoggio;Kenneth O. Stanley;Sebastian Risi,"Biological plastic neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifetime learning. The interplay of these elements leads to the emergence of adaptive behavior and intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed plastic neural networks with a large variety of dynamics, architectures, and plasticity rules: these artificial systems are composed of inputs, outputs, and plastic components that change in response to experiences in an environment. These systems may autonomously discover novel adaptive algorithms, and lead to hypotheses on the emergence of biological adaptation. EPANNs have seen considerable progress over the last two decades. Current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results. In particular, the limitations of hand-designed networks could be overcome by more flexible and innovative solutions. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and developments are presented. △ Less","8 August, 2018",https://arxiv.org/pdf/1703.10371
How Compressible are Innovation Processes?,Hamid Ghourchian;Arash Amini;Amin Gohari,"The sparsity and compressibility of finite-dimensional signals are of great interest in fields such as compressed sensing. The notion of compressibility is also extended to infinite sequences of i.i.d. or ergodic random variables based on the observed error in their nonlinear k-term approximation. In this work, we use the entropy measure to study the compressibility of continuous-domain innovation processes (alternatively known as white noise). Specifically, we define such a measure as the entropy limit of the doubly quantized (time and amplitude) process. This provides a tool to compare the compressibility of various innovation processes. It also allows us to identify an analogue of the concept of ""entropy dimension"" which was originally defined by Rényi for random variables. Particular attention is given to stable and impulsive Poisson innovation processes. Here, our results recognize Poisson innovations as the more compressible ones with an entropy measure far below that of stable innovations. While this result departs from the previous knowledge regarding the compressibility of fat-tailed distributions, our entropy measure ranks stable innovations according to their tail decay. △ Less","21 January, 2018",https://arxiv.org/pdf/1703.09537
Who Said What: Modeling Individual Labelers Improves Classification,Melody Y. Guan;Varun Gulshan;Andrew M. Dai;Geoffrey E. Hinton,"Data are often labeled by many different experts with each expert only labeling a small fraction of the data and each data point being labeled by several experts. This reduces the workload on individual experts and also gives a better estimate of the unobserved ground truth. When experts disagree, the standard approaches are to treat the majority opinion as the correct label or to model the correct label as a distribution. These approaches, however, do not make any use of potentially valuable information about which expert produced which label. To make use of this extra information, we propose modeling the experts individually and then learning averaging weights for combining them, possibly in sample-specific ways. This allows us to give more weight to more reliable experts and take advantage of the unique strengths of individual experts at classifying certain types of data. Here we show that our approach leads to improvements in computer-aided diagnosis of diabetic retinopathy. We also show that our method performs better than competing algorithms by Welinder and Perona (2010), and by Mnih and Hinton (2012). Our work offers an innovative approach for dealing with the myriad real-world settings that use expert opinions to define labels for training. △ Less","4 January, 2018",https://arxiv.org/pdf/1703.08774
Patchwork Kriging for Large-scale Gaussian Process Regression,Chiwoo Park;Daniel Apley,"This paper presents a new approach for Gaussian process (GP) regression for large datasets. The approach involves partitioning the regression input domain into multiple local regions with a different local GP model fitted in each region. Unlike existing local partitioned GP approaches, we introduce a technique for patching together the local GP models nearly seamlessly to ensure that the local GP models for two neighboring regions produce nearly the same response prediction and prediction error variance on the boundary between the two regions. This largely mitigates the well-known discontinuity problem that degrades the boundary accuracy of existing local partitioned GP methods. Our main innovation is to represent the continuity conditions as additional pseudo-observations that the differences between neighboring GP responses are identically zero at an appropriately chosen set of boundary input locations. To predict the response at any input location, we simply augment the actual response observations with the pseudo-observations and apply standard GP prediction methods to the augmented data. In contrast to heuristic continuity adjustments, this has an advantage of working within a formal GP framework, so that the GP-based predictive uncertainty quantification remains valid. Our approach also inherits a sparse block-like structure for the sample covariance matrix, which results in computationally efficient closed-form expressions for the predictive mean and variance. In addition, we provide a new spatial partitioning scheme based on a recursive space partitioning along local principal component directions, which makes the proposed approach applicable for regression domains having more than two dimensions. Using three spatial datasets and three higher dimensional datasets, we investigate the numerical performance of the approach and compare it to several state-of-the-art approaches. △ Less","7 July, 2018",https://arxiv.org/pdf/1701.06655
Decentralized RLS with Data-Adaptive Censoring for Regressions over Large-Scale Networks,Zifeng Wang;Zheng Yu;Qing Ling;Dimitris Berberidis;Georgios B. Giannakis,"The deluge of networked data motivates the development of algorithms for computation- and communication-efficient information processing. In this context, three data-adaptive censoring strategies are introduced to considerably reduce the computation and communication overhead of decentralized recursive least-squares (D-RLS) solvers. The first relies on alternating minimization and the stochastic Newton iteration to minimize a network-wide cost, which discards observations with small innovations. In the resultant algorithm, each node performs local data-adaptive censoring to reduce computations, while exchanging its local estimate with neighbors so as to consent on a network-wide solution. The communication cost is further reduced by the second strategy, which prevents a node from transmitting its local estimate to neighbors when the innovation it induces to incoming data is minimal. In the third strategy, not only transmitting, but also receiving estimates from neighbors is prohibited when data-adaptive censoring is in effect. For all strategies, a simple criterion is provided for selecting the threshold of innovation to reach a prescribed average data reduction. The novel censoring-based (C)D-RLS algorithms are proved convergent to the optimal argument in the mean-square deviation sense. Numerical experiments validate the effectiveness of the proposed algorithms in reducing computation and communication overhead. △ Less","12 January, 2018",https://arxiv.org/pdf/1612.08263
Rate-cost tradeoffs in control,Victoria Kostina;Babak Hassibi,"Consider a control problem with a communication channel connecting the observer of a linear stochastic system to the controller. The goal of the controller is to minimize a quadratic cost function in the state variables and control signal, known as the linear quadratic regulator (LQR). We study the fundamental tradeoff between the communication rate r bits/sec and the expected cost b. We obtain a lower bound on a certain rate-cost function, which quantifies the minimum directed mutual information between the channel input and output that is compatible with a target LQR cost. The rate-cost function has operational significance in multiple scenarios of interest: among others, it allows us to lower-bound the minimum communication rate for fixed and variable length quantization, and for control over noisy channels. We derive an explicit lower bound to the rate-cost function, which applies to the vector, non-Gaussian, and partially observed systems, thereby extending and generalizing an earlier explicit expression for the scalar Gaussian system, due to Tatikonda el al. The bound applies as long as the differential entropy of the system noise is not -\infty. It can be closely approached by a simple lattice quantization scheme that only quantizes the innovation, that is, the difference between the controller's belief about the current state and the true state. Via a separation principle between control and communication, similar results hold for causal lossy compression of additive noise Markov sources. Apart from standard dynamic programming arguments, our technical approach leverages the Shannon lower bound, develops new estimates for data compression with coding memory, and uses some recent results on high resolution variable-length vector quantization to prove that the new converse bounds are tight. △ Less","20 November, 2018",https://arxiv.org/pdf/1612.02126
Phase Coexistence for the Hard-Core Model on {\mathbb Z}^2,Antonio Blanca;Yuxuan Chen;David Galvin;Dana Randall;Prasad Tetali,"The hard-core model has attracted much attention across several disciplines, representing lattice gases in statistical physics and independent sets in discrete mathematics and computer science. On finite graphs, we are given a parameter λ, and an independent set I arises with probability proportional to λ^{|I|}. On infinite graphs a Gibbs measure is defined as a suitable limit with the correct conditional probabilities, and we are interested in determining when this limit is unique and when there is phase coexistence, i.e., existence of multiple Gibbs measures. It has long been conjectured that on {\mathbb Z}^2 this model has a critical value λ_c \approx 3.796 with the property that if λ< λ_c then it exhibits uniqueness of phase, while if λ> λ_c then there is phase coexistence. Much of the work to date on this problem has focused on the regime of uniqueness, with the state of the art being recent work of Sinclair, Srivastava, Štefankovič and Yin showing that there is a unique Gibbs measure for all λ< 2.538. Here we give the first non-trivial result in the other direction, showing that there are multiple Gibbs measures for all λ> 5.3506. There is some potential for lowering this bound, but with the methods we are using we cannot hope to replace 5.3506 with anything below about 4.8771. Our proof begins along the lines of the standard Peierls argument, but we add two innovations. First, following ideas of Kotecký and Randall, we construct an event that distinguishes two boundary conditions and always has long contours associated with it, obviating the need to accurately enumerate short contours. Second, we obtain improved bounds on the number of contours by relating them to a new class of self-avoiding walks on an oriented version of {\mathbb Z}^2. △ Less","30 March, 2018",https://arxiv.org/pdf/1611.01115
Immigrant community integration in world cities,Fabio Lamanna;Maxime Lenormand;María Henar Salas-Olmedo;Gustavo Romanillos;Bruno Gonçalves;José J. Ramasco,"As a consequence of the accelerated globalization process, today major cities all over the world are characterized by an increasing multiculturalism. The integration of immigrant communities may be affected by social polarization and spatial segregation. How are these dynamics evolving over time? To what extent the different policies launched to tackle these problems are working? These are critical questions traditionally addressed by studies based on surveys and census data. Such sources are safe to avoid spurious biases, but the data collection becomes an intensive and rather expensive work. Here, we conduct a comprehensive study on immigrant integration in 53 world cities by introducing an innovative approach: an analysis of the spatio-temporal communication patterns of immigrant and local communities based on language detection in Twitter and on novel metrics of spatial integration. We quantify the ""Power of Integration"" of cities --their capacity to spatially integrate diverse cultures-- and characterize the relations between different cultures when acting as hosts or immigrants. △ Less","14 March, 2018",https://arxiv.org/pdf/1611.01056
The societal impact of big data: A research roadmap for Europe,Martí Cuquet;Anna Fensel,"With its rapid growth and increasing adoption, big data is producing a substantial impact in society. Its usage is opening both opportunities such as new business models and economic gains and risks such as privacy violations and discrimination. Europe is in need of a comprehensive strategy to optimise the use of data for a societal benefit and increase the innovation and competitiveness of its productive activities. In this paper, we contribute to the definition of this strategy with a research roadmap to capture the economic, social and ethical, legal and political benefits associated with the use of big data in Europe. The present roadmap considers the positive and negative externalities associated with big data, maps research and innovation topics in the areas of data management, processing, analytics, protection, visualisation, as well as non-technical topics, to the externalities they can tackle, and provides a time frame to address these topics in order to deliver social impact, skills development and standardisation. Finally, it also identifies what sectors will be most benefited by each of the research efforts. The goal of the roadmap is to guide European research efforts to develop a socially responsible big data economy, and to allow stakeholders to identify and meet big data challenges and proceed with a shared understanding of the societal impact, positive and negative externalities and concrete problems worth investigating in future programmes. △ Less","26 March, 2018",https://arxiv.org/pdf/1610.06766
Number Theoretic Transforms for Secure Signal Processing,Alberto Pedrouzo-Ulloa;Juan Ramón Troncoso-Pastoriza;Fernando Pérez-González,"Multimedia contents are inherently sensitive signals that must be protected whenever they are outsourced to an untrusted environment. This problem becomes a challenge when the untrusted environment must perform some processing on the sensitive signals; a paradigmatic example is Cloud-based signal processing services. Approaches based on Secure Signal Processing (SSP) address this challenge by proposing novel mechanisms for signal processing in the encrypted domain and interactive secure protocols to achieve the goal of protecting signals without disclosing the sensitive information they convey. This work presents a novel and comprehensive set of approaches and primitives to efficiently process signals in an encrypted form, by using Number Theoretic Transforms (NTTs) in innovative ways. This usage of NTTs paired with appropriate signal pre- and post-coding enables a whole range of easily composable signal processing operations comprising, among others, filtering, generalized convolutions, matrix-based processing or error correcting codes. The main focus is on unattended processing, in which no interaction from the client is needed; for implementation purposes, efficient lattice-based somewhat homomorphic cryptosystems are used. We exemplify these approaches and evaluate their performance and accuracy, proving that the proposed framework opens up a wide variety of new applications for secured outsourced-processing of multimedia contents. △ Less","29 January, 2018",https://arxiv.org/pdf/1607.05229
Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change,William L. Hamilton;Jure Leskovec;Dan Jurafsky,"Understanding how words change their meanings over time is key to models of language and cultural evolution, but historical data on meaning is scarce, making theories hard to develop and test. Word embeddings show promise as a diachronic tool, but have not been carefully evaluated. We develop a robust methodology for quantifying semantic change by evaluating word embeddings (PPMI, SVD, word2vec) against known historical changes. We then use this methodology to reveal statistical laws of semantic evolution. Using six historical corpora spanning four languages and two centuries, we propose two quantitative laws of semantic change: (i) the law of conformity---the rate of semantic change scales with an inverse power-law of word frequency; (ii) the law of innovation---independent of frequency, words that are more polysemous have higher rates of semantic change. △ Less","25 October, 2018",https://arxiv.org/pdf/1605.09096
The wisdom of networks: A general adaptation and learning mechanism of complex systems: The network core triggers fast responses to known stimuli; innovations require the slow network periphery and are encoded by core-remodeling,Peter Csermely,"I hypothesize that re-occurring prior experience of complex systems mobilizes a fast response, whose attractor is encoded by their strongly connected network core. In contrast, responses to novel stimuli are often slow and require the weakly connected network periphery. Upon repeated stimulus, peripheral network nodes remodel the network core that encodes the attractor of the new response. This ""core-periphery learning"" theory reviews and generalizes the heretofore fragmented knowledge on attractor formation by neural networks, periphery-driven innovation and a number of recent reports on the adaptation of protein, neuronal and social networks. The coreperiphery learning theory may increase our understanding of signaling, memory formation, information encoding and decision-making processes. Moreover, the power of network periphery-related 'wisdom of crowds' inventing creative, novel responses indicates that deliberative democracy is a slow yet efficient learning strategy developed as the success of a billion-year evolution. △ Less","23 February, 2018",https://arxiv.org/pdf/1511.01238
