title,authors,abstract,submitted_date,pdf_link
Density-aware Dynamic Mobile Networks: Opportunities and Challenges,Ertan Onur;Shahram Mollahasani;Alperen Eroğlu;Nina Razi Moftakhar,"We experience a major paradigm change in mobile networks. The infrastructure of cellular networks becomes mobile as it is densified by using mobile and nomadic small cells to increase coverage and capacity. Furthermore, the innovative approaches such as green operation through sleep scheduling, user-controlled small cells, and end-to-end slicing will make the network highly dynamic. Mobile cells, while bringing many benefits, introduce many unconventional challenges that we present in this paper. We have to introduce novel techniques for adapting network functions, communication protocols and their parameters to network density. Especially when cells on wheels or wings are considered, static and man-made configurations will waste valuable resources such as spectrum or energy if density is not considered as an optimization parameter. In this paper, we present the existing density estimators. We analyze the impact of density on coverage, interference, mobility management, scalability, capacity, caching, routing protocols and energy consumption. We evaluate nomadic cells in dynamic networks in a comprehensive way and illustrate the potential objectives we can achieve by adapting mobile networks to base station density. The main challenges we may face by employing dynamic networks and how we can tackle these problems are discussed in detail. △ Less","25 December, 2017",https://arxiv.org/pdf/1712.09104
DancingLines: An Analytical Scheme to Depict Cross-Platform Event Popularity,Tianxiang Gao;Weiming Bao;Jinning Li;Xiaofeng Gao;Boyuan Kong;Yan Tang;Guihai Chen;Xuan Li,"Nowadays, events usually burst and are propagated online through multiple modern media like social networks and search engines. There exists various research discussing the event dissemination trends on individual medium, while few studies focus on event popularity analysis from a cross-platform perspective. Challenges come from the vast diversity of events and media, limited access to aligned datasets across different media and a great deal of noise in the datasets. In this paper, we design DancingLines, an innovative scheme that captures and quantitatively analyzes event popularity between pairwise text media. It contains two models: TF-SW, a semantic-aware popularity quantification model, based on an integrated weight coefficient leveraging Word2Vec and TextRank; and wDTW-CD, a pairwise event popularity time series alignment model matching different event phases adapted from Dynamic Time Warping. We also propose three metrics to interpret event popularity trends between pairwise social platforms. Experimental results on eighteen real-world event datasets from an influential social network and a popular search engine validate the effectiveness and applicability of our scheme. DancingLines is demonstrated to possess broad application potentials for discovering the knowledge of various aspects related to events and different media. △ Less","22 December, 2017",https://arxiv.org/pdf/1712.08550
Tracking the Diffusion of Named Entities,Leon Derczynski;Matthew Rowe,"Existing studies of how information diffuses across social networks have thus far concentrated on analysing and recovering the spread of deterministic innovations such as URLs, hashtags, and group membership. However investigating how mentions of real-world entities appear and spread has yet to be explored, largely due to the computationally intractable nature of performing large-scale entity extraction. In this paper we present, to the best of our knowledge, one of the first pieces of work to closely examine the diffusion of named entities on social media, using Reddit as our case study platform. We first investigate how named entities can be accurately recognised and extracted from discussion posts. We then use these extracted entities to study the patterns of entity cascades and how the probability of a user adopting an entity (i.e. mentioning it) is associated with exposures to the entity. We put these pieces together by presenting a parallelised diffusion model that can forecast the probability of entity adoption, finding that the influence of adoption between users can be characterised by their prior interactions -- as opposed to whether the users propagated entity-adoptions beforehand. Our findings have important implications for researchers studying influence and language, and for community analysts who wish to understand entity-level influence dynamics. △ Less","29 December, 2017",https://arxiv.org/pdf/1712.08349
COOJA Network Simulator: Exploring the Infinite Possible Ways to Compute the Performance Metrics of IOT Based Smart Devices to Understand the Working of IOT Based Compression & Routing Protocols,Tayyab Mehmood,"This paper demonstrates the scheme regarding Internet of Things (IOT) which is well thought-out the next generation of Internet. IOT explicitly elaborates the assimilation of human beings and physical systems, as they can cooperate with each other so leading towards a sort of encroachment in networking by interconnecting things together while making use of wireless embedded systems, said to be the building blocks of IOT, that are capable to be given an IP address and thus making them part of the global internet. Several essential approaches that entail in IOT and supports this innovation are being argued in this paper. 6LoWPAN (IPV6 Low Power Personal Area Networks) is a protocol used to appropriately and efficiently use IPV6 addresses. Control messages of RPL routing protocol for low power devices are discussed to understand the working of RPL protocol. In the end Contiki OS based COOJA Network simulator is used to demonstrate the working of how these routing and compression protocol works in real time simulation. △ Less","21 December, 2017",https://arxiv.org/pdf/1712.08303
Simulation of conventional cold-formed steel sections formed from Advanced High Strength Steel (AHSS),Hamid Foroughi;Benjamin W. Schafer,"The objective of this paper is to explore the potential impact of the use of advanced high strength steel (AHSS) to form traditional cold-formed steel structural members. In this study, shell finite element models are constructed, and geometric and material nonlinear collapse analysis performed, on simulated lipped channel cross-section cold-formed steel members roll-formed from AHSS. AHSS sheet is currently being used in automotive applications with thickness ranging from 0.35 to 0.8 mm (0.0138 to 0.0315 in.) and yield strengths from 350 to 1250 MPa (51 to 181 ksi). However, AHSS has not yet been employed in cold-formed steel construction. To assess the impact of the adoption of AHSS on cold-formed steel member strength a group of forty standard structural lipped channel cross-sections are chosen from the Steel Framing Industry Association product list and simulated with AHSS material properties. The stress-strain models used in this study are based on AHSS in production, including dual-phase and martensitic steels. The simulations consider compression with work on bending about the major axis in progress. Three different bracing conditions are employed so that the impact of local, distortional, and global buckling, including interactions can be explored. Due to the higher yield stresses of AHSS the potential for interaction and mode switching is anticipated to be greater in these members compared with conventional mild steels. The simulations provide a direct means to assess the increase in strength created by the application of AHSS, while also allowing for future exploration of the increase in buckling mode interaction, imperfection sensitivity, and strain demands inherent in the larger capacities. The work is intended to be an initial step in a longer-term effort to foster innovation in the application of new steels in cold-formed steel construction. △ Less","21 December, 2017",https://arxiv.org/pdf/1712.08037
Improvements to Inference Compilation for Probabilistic Programming in Large-Scale Scientific Simulators,Mario Lezcano Casado;Atilim Gunes Baydin;David Martinez Rubio;Tuan Anh Le;Frank Wood;Lukas Heinrich;Gilles Louppe;Kyle Cranmer;Karen Ng;Wahid Bhimji;Prabhat,"We consider the problem of Bayesian inference in the family of probabilistic models implicitly defined by stochastic generative models of data. In scientific fields ranging from population biology to cosmology, low-level mechanistic components are composed to create complex generative models. These models lead to intractable likelihoods and are typically non-differentiable, which poses challenges for traditional approaches to inference. We extend previous work in ""inference compilation"", which combines universal probabilistic programming and deep learning methods, to large-scale scientific simulators, and introduce a C++ based probabilistic programming library called CPProb. We successfully use CPProb to interface with SHERPA, a large code-base used in particle physics. Here we describe the technical innovations realized and planned for this library. △ Less","21 December, 2017",https://arxiv.org/pdf/1712.07901
Partial Labeled Gastric Tumor Segmentation via patch-based Reiterative Learning,Yang Nan;Gianmarc Coppola;Qiaokang Liang;Kunglin Zou;Wei Sun;Dan Zhang;Yaonan Wang;Guanzhen Yu,"Gastric cancer is the second leading cause of cancer-related deaths worldwide, and the major hurdle in biomedical image analysis is the determination of the cancer extent. This assignment has high clinical relevance and would generally require vast microscopic assessment by pathologists. Recent advances in deep learning have produced inspiring results on biomedical image segmentation, while its outcome is reliant on comprehensive annotation. This requires plenty of labor costs, for the ground truth must be annotated meticulously by pathologists. In this paper, a reiterative learning framework was presented to train our network on partial annotated biomedical images, and superior performance was achieved without any pre-trained or further manual annotation. We eliminate the boundary error of patch-based model through our overlapped region forecast algorithm. Through these advisable methods, a mean intersection over union coefficient (IOU) of 0.883 and mean accuracy of 91.09% on the partial labeled dataset was achieved, which made us win the 2017 China Big Data & Artificial Intelligence Innovation and Entrepreneurship Competitions. △ Less","20 December, 2017",https://arxiv.org/pdf/1712.07488
"Bibliometric Approximation of a Scientific Specialty by Combining Key Sources, Title Words, Authors and References",Nadine Rons,"Bibliometric methods for the analysis of highly specialized subjects are increasingly investigated and debated. Information and assessments well-focused at the specialty level can help make important decisions in research and innovation policy. This paper presents a novel method to approximate the specialty to which a given publication record belongs. The method partially combines sets of key values for four publication data fields: source, title, authors and references. The approach is founded in concepts defining research disciplines and scholarly communication, and in empirically observed regularities in publication data. The resulting specialty approximation consists of publications associated to the investigated publication record via key values for at least three of the four data fields. This paper describes the method and illustrates it with an application to publication records of individual scientists. The illustration also successfully tests the focus of the specialty approximation in terms of its ability to connect and help identify peers. Potential tracks for further investigation include analyses involving other kinds of specialized publication records, studies for a broader range of specialties, and exploration of the potential for diverse applications in research and research policy context. △ Less","19 December, 2017",https://arxiv.org/pdf/1712.07087
Graphic Narrative with Interactive Stylization Design,Ignacio Garcia-Dorado;Pascal Getreuer;Madison Le;Robin Debreuil;Alex Kauffmann;Peyman Milanfar,"We present a system to convert any set of images (e.g., a video clip or a photo album) into a storyboard. We aim to create multiple pleasing graphic representations of the content at interactive rates, so the user can explore and find the storyboard (images, layout, and stylization) that best suits their needs and taste. The main challenges of this work are: selecting the content images, placing them into panels, and applying a stylization. For the latter, we propose an interactive design tool to create new stylizations using a wide range of filter blocks. This approach unleashes the creativity by allowing the user to tune, modify, and intuitively design new sequences of filters. In parallel to this manual design, we propose a novel procedural approach that automatically assembles sequences of filters for innovative results. We aim to keep the algorithm complexity as low as possible such that it can run interactively on a mobile device. Our results include examples of styles designed using both our interactive and procedural tools, as well as their final composition into interesting and appealing storyboards. △ Less","18 December, 2017",https://arxiv.org/pdf/1712.06654
On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent,Xingwen Zhang;Jeff Clune;Kenneth O. Stanley,"Because stochastic gradient descent (SGD) has shown promise optimizing neural networks with millions of parameters and few if any alternatives are known to exist, it has moved to the heart of leading approaches to reinforcement learning (RL). For that reason, the recent result from OpenAI showing that a particular kind of evolution strategy (ES) can rival the performance of SGD-based deep RL methods with large neural networks provoked surprise. This result is difficult to interpret in part because of the lingering ambiguity on how ES actually relates to SGD. The aim of this paper is to significantly reduce this ambiguity through a series of MNIST-based experiments designed to uncover their relationship. As a simple supervised problem without domain noise (unlike in most RL), MNIST makes it possible (1) to measure the correlation between gradients computed by ES and SGD and (2) then to develop an SGD-based proxy that accurately predicts the performance of different ES population sizes. These innovations give a new level of insight into the real capabilities of ES, and lead also to some unconventional means for applying ES to supervised problems that shed further light on its differences from SGD. Incorporating these lessons, the paper concludes by demonstrating that ES can achieve 99% accuracy on MNIST, a number higher than any previously published result for any evolutionary method. While not by any means suggesting that ES should substitute for SGD in supervised learning, the suite of experiments herein enables more informed decisions on the application of ES within RL and other paradigms. △ Less","18 December, 2017",https://arxiv.org/pdf/1712.06564
Crack detection in beam structures with a novel Laplace based Wavelet Finite Element method,Shuaifang Zhang;Dongsheng Li;Wei Shen;Xiwen Zhang;Yu Liu,"Beam structure is one of the most widely used structures in mechanical engineering and civil engineering. Ultrasonic guided wave based crack identification is one of the most important and accepted approaches applied to detect unseen small flaws in structures. Numerical simulations of ultrasonic guided wave propagation have caught more and more attention due to the fast development of hardware and software in the last few years. From all the numerical simulation methods, wavelet based finite element method has been proved to be one of the most efficient methods due to its better spatial resolution, which means it needs fewer elements to get the same accuracy and it can improve the calculation cost significantly. However, it needs a very small time interval. Laplace transform can easily convert the time domain into a frequency domain and then revert it back to a time domain. Laplace transform has thus the advantage of finding better results with a very large time interval. which can save a lot of time cost. This paper will present an innovative method combining Laplace transform and the B-spline wavelet on interval (BSWI) finite element method. This novel method allows to get results with the same accuracy and with a significantly lower time cost, which would not only decrease the total number of elements in the structure but also increase the time integration interval. The numerical Laplace transform and BSWI finite element will be introduced. Moreover, this innovative method is applied to simulate the ultrasonic wave propagation in a beam structure in different materials. Numerical examples for crack identification in beam structures have been studied for verification. △ Less","18 December, 2017",https://arxiv.org/pdf/1712.06251
A Berkeley View of Systems Challenges for AI,Ion Stoica;Dawn Song;Raluca Ada Popa;David Patterson;Michael W. Mahoney;Randy Katz;Anthony D. Joseph;Michael Jordan;Joseph M. Hellerstein;Joseph E. Gonzalez;Ken Goldberg;Ali Ghodsi;David Culler;Pieter Abbeel,"With the increasing commoditization of computer vision, speech recognition and machine translation systems and the widespread deployment of learning-based back-end technologies such as digital advertising and intelligent infrastructures, AI (Artificial Intelligence) has moved from research labs to production. These changes have been made possible by unprecedented levels of data and computation, by methodological advances in machine learning, by innovations in systems software and architectures, and by the broad accessibility of these technologies. The next generation of AI systems promises to accelerate these developments and increasingly impact our lives via frequent interactions and making (often mission-critical) decisions on our behalf, often in highly personalized contexts. Realizing this promise, however, raises daunting challenges. In particular, we need AI systems that make timely and safe decisions in unpredictable environments, that are robust against sophisticated adversaries, and that can process ever increasing amounts of data across organizations and individuals without compromising confidentiality. These challenges will be exacerbated by the end of the Moore's Law, which will constrain the amount of data these technologies can store and process. In this paper, we propose several open research directions in systems, architectures, and security that can address these challenges and help unlock AI's potential to improve lives and society. △ Less","15 December, 2017",https://arxiv.org/pdf/1712.05855
CIM compliant multiplatform approach for cyber-physical energy system assessment,Minh Tri Le;Van Hoa Nguyen;Quoc Tuan Tran;Yvon Besanger;Thierry Braconnier;Antoine Labonne;Herve Buttin,"With high penetration of distributed renewable energy resources along with sophisticated automation and information technology, cyber-physical energy systems (CPES, i.e. Smart Grids here) requires a holistic approach to evaluate the integration at a system level, addressing all relevant domains. Hybrid cloud SCADA (Supervisory, Control And Data Acquisition), allowing laboratories to be linked in a consistent infrastructure can provide the support for such multi-platform experiments. This paper presents the procedure to implement a CIM (Common Information Model) compliant hybrid cloud SCADA, with database and client adaptive to change in system topology, as well as CIM library update. This innovative way ensures interoperability among the partner platforms and provides support to multi-platform holistic approach for CPES assessment. △ Less","3 December, 2017",https://arxiv.org/pdf/1712.05243
Improving Malware Detection Accuracy by Extracting Icon Information,Pedro Silva;Sepehr Akhavan-Masouleh;Li Li,"Detecting PE malware files is now commonly approached using statistical and machine learning models. While these models commonly use features extracted from the structure of PE files, we propose that icons from these files can also help better predict malware. We propose an innovative machine learning approach to extract information from icons. Our proposed approach consists of two steps: 1) extracting icon features using summary statics, histogram of gradients (HOG), and a convolutional autoencoder, 2) clustering icons based on the extracted icon features. Using publicly available data and by using machine learning experiments, we show our proposed icon clusters significantly boost the efficacy of malware prediction models. In particular, our experiments show an average accuracy increase of 10% when icon clusters are used in the prediction model. △ Less","10 December, 2017",https://arxiv.org/pdf/1712.03483
Columnar Database Techniques for Creating AI Features,Brad Carlile;Akiko Marti;Guy Delamarter,"Recent advances with in-memory columnar database techniques have increased the performance of analytical queries on very large databases and data warehouses. At the same time, advances in artificial intelligence (AI) algorithms have increased the ability to analyze data. We use the term AI to encompass both Deep Learning (DL or neural network) and Machine Learning (ML aka Big Data analytics). Our exploration of the AI full stack has led us to a cross-stack columnar database innovation that efficiently creates features for AI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to add to existing columnar database dictionaries in order to increase the efficiency of featurization by minimizing data movement and data duplication. We show how various forms of featurization (feature selection, feature extraction, and feature creation) can be efficiently calculated in a columnar database. The full stack AI investigation has also led us to propose an integrated columnar database and AI architecture. This architecture has information flows and feedback loops to improve the whole analytics cycle during multiple iterations of extracting data from the data sources, featurization, and analysis. △ Less","7 December, 2017",https://arxiv.org/pdf/1712.02882
SOT for MOT,Qizheng He;Jianan Wu;Gang Yu;Chi Zhang,"In this paper we present a robust tracker to solve the multiple object tracking (MOT) problem, under the framework of tracking-by-detection. As the first contribution, we innovatively combine single object tracking (SOT) algorithms with multiple object tracking algorithms, and our results show that SOT is a general way to strongly reduce the number of false negatives, regardless of the quality of detection. Another contribution is that we show with a deep learning based appearance model, it is easy to associate detections of the same object efficiently and also with high accuracy. This appearance model plays an important role in our MOT algorithm to correctly associate detections into long trajectories, and also in our SOT algorithm to discover new detections mistakenly missed by the detector. The deep neural network based model ensures the robustness of our tracking algorithm, which can perform data association in a wide variety of scenes. We ran comprehensive experiments on a large-scale and challenging dataset, the MOT16 benchmark, and results showed that our tracker achieved state-of-the-art performance based on both public and private detections. △ Less","4 December, 2017",https://arxiv.org/pdf/1712.01059
Impact Of Urban Technology Deployments On Local Commercial Activity,Stanislav Sobolevsky;Ekaterina Levitskaya;Henry Chan;Shefali Enaker;Joe Bailey;Marc Postle;Yuriy Loukachev;Melinda Rolfs;Constantine Kontokosta,"While smart city innovations seem to be a common and necessary response to increasing challenges of urbanization, foreseeing their impact on complex urban system is critical for informed decision making. Moreover, often the effect of urban interventions goes beyond the original expectations, including multiple indirect impacts. The present study considers the impact of two urban deployments, Citi Bike (bike sharing system) and LinkNYC kiosks, on the local commercial activity in the affected neighborhoods of New York City. The study uses anonymized and aggregated insights provided through a grant from the Mastercard Center for Inclusive Growth in order to provide initial data-driven evidence towards the hypothesis that proximity of Citi Bike stations incentivizes local sales at eating places, while LinkNYC kiosks help people, especially visitors, to navigate local businesses and thus incentivize commercial activity in different business categories. △ Less","2 December, 2017",https://arxiv.org/pdf/1712.00659
KIBS Innovative Entrepreneurship Networks on Social Media,José N. Franco-Riquelme;Isaac Lemus-Aguilar;Joaquín Ordieres-Meré,"The analysis of the use of social media for innovative entrepreneurship in the context has received little attention in the literature, especially in the context of Knowledge Intensive Business Services (KIBS). Therefore, this paper focuses on bridging this gap by applying text mining and sentiment analysis techniques to identify the innovative entrepreneurship reflected by these companies in their social media. Finally, we present and analyze the results of our quantitative analysis of 23.483 posts based on eleven Spanish and Italian consultancy KIBS Twitter Usernames and Keywords using data interpretation techniques such as clustering and topic modeling. This paper suggests that there is a significant gap between the perceived potential of social media and the entrepreneurial behaviors at the social context in business-to-business (B2B) companies. △ Less","30 November, 2017",https://arxiv.org/pdf/1711.11403
Towards Accurate Binary Convolutional Neural Network,Xiaofan Lin;Cong Zhao;Wei Pan,"We introduce a novel scheme to train binary convolutional neural networks (CNNs) -- CNNs with weights and activations constrained to {-1,+1} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations. △ Less","30 November, 2017",https://arxiv.org/pdf/1711.11294
"Emerging basic, clinical and translational research fronts in dental biomaterials R&D",David Fajardo-Ortiz;Pablo Jaramillo;Claudia Jaramillo;Raul Resendiz;Miguel Lara-Flores;Victor M. Castano,"The current (2007-2007) structure and content of dental materials research has been investigated by identifying and describing the emergent research fronts which can be related to basic, translational and clinical observation research. By a combination of network analysis and text mining of the literature on dental materials indexed in the Web of Science, we have identified eleven emerging research fronts. These fronts are related to different dental materials applications which are at different levels in the knowledge translation and biomedical innovation process. We identified fronts related to dominant designs like titanium implants, competing technologies like ceramics and composites applications to prothesis and restauration, and disruptive technologies like nanomaterials and mineral trioxide aggregates. Our results suggest the possible relation between the technological complexity of the dental materials and the level of advance in terms of knowledge translation. This is the first time the structure and content of research on dental materials research is analyzed. △ Less","29 November, 2017",https://arxiv.org/pdf/1711.11168
Spectral Element Methods for Liquid Metal Reactors Applications,Elia Merzari;Aleksandr Obabko;Paul Fischer,"Funded by the U.S. Department of Energy, the Nuclear Energy Advanced Modeling and Simulation (NEAMS) program aims to develop an integrated multiphysics simulation capability for the design and analysis of future generations of nuclear power plants. NEAMS embraces a multiresolution hierarchy designing the code suite structure to ultimately span the full range of length and time scales present in relevant reactor design and safety analyses. Advanced reactors, such as liquid metal reactors, rely on innovative component designs to meet cost and safety targets. In order to span a wider design range, advanced modeling and simulation capabilities that rely on minimal assumptions play an important role in optimizing the design. Over the past several years the NEAMS program has developed the integrated multiphysics code suite (thermal-hydraulics, structural analysis and neutronics) SHARP aimed at streamlining the prototyping of such components. For the simulation of fluid flow and heat transfer, SHARP focuses on the high-fidelity end, aiming primarily at turbulence-resolving techniques such large eddy simulation (LES) and direct numerical simulation (DNS). The computational fluid dynamics code (CFD) selected for SHARP is Nek5000, a state-of-the-art highly scalable tool employing the spectral element method (SEM). In this manuscript, to be published in a Von karman institute lecture series monograph on liquid metal reactors, we review the method and its implementation in Nek5000. We also examine several applications. We note that Nek5000 is also regularly employed for intermediate-fidelity approaches such as Reynolds-averaged Navier-Stokes (RANS) and for reduced-order models employing momentum sources or porous media, especially when coupled to neutronics modeling. △ Less","25 November, 2017",https://arxiv.org/pdf/1711.09307
How Software Development Group Leaders Influence Team Members Behavior,Fabio Q. B. da Silva;Cleviton V. F. Monteiro;Igor Ebrahim dos Santos;Luiz Fernando Capretz,"Evidence in the literature from several business sectors shows that exploratory and exploitative innovation strategies are complementarily important for competitiveness. Our empirical findings reinforced those evidences in the context of software development companies. The innovative behaviour of individuals is an essential ingredient to success in both types of innovations strategies and leaders can have a big influence on this behaviour. Adopting a leadership style that combines transactional and transformational practices is more likely to produce effective results in supporting innovative behaviour. In software development, project managers and other group leaders should be stimulated and supported in adopting such practices to create the conditions for innovative behaviour to thrive. △ Less","22 November, 2017",https://arxiv.org/pdf/1711.08524
"Multi-tier Drone Architecture for 5G/B5G Cellular Networks: Challenges, Trends, and Prospects",Silvia Sekander;Hina Tabassum;Ekram Hossain,"Drones (or unmanned aerial vehicles [UAVs]) are expected to be an important component of fifth generation (5G)/beyond 5G (B5G) cellular architectures that can potentially facilitate wireless broadcast or point-to-multipoint transmissions. The distinct features of various drones such as the maximum operational altitude, communication, coverage, computation, and endurance impel the use of a multi-tier architecture for future drone-cell networks. In this context, this article focuses on investigating the feasibility of multi-tier drone network architecture over traditional single-tier drone networks and identifying the scenarios in which drone networks can potentially complement the traditional RF-based terrestrial networks. We first identify the challenges associated with multi-tier drone networks as well as drone-assisted cellular networks. We then review the existing state-of-the-art innovations in drone networks and drone-assisted cellular networks. We then investigate the performance of a multi-tier drone network in terms of spectral efficiency of downlink transmission while illustrating the optimal intensity and altitude of drones in different tiers numerically. Our results demonstrate the specific network load conditions (i.e., ratio of user intensity and base station intensity) where deployment of drones can be beneficial (in terms of spectral efficiency of downlink transmission) for conventional terrestrial cellular networks. △ Less","19 November, 2017",https://arxiv.org/pdf/1711.08407
GraphGAN: Graph Representation Learning with Generative Adversarial Nets,Hongwei Wang;Jia Wang;Jialin Wang;Miao Zhao;Weinan Zhang;Fuzheng Zhang;Xing Xie;Minyi Guo,"The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces ""fake"" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines. △ Less","22 November, 2017",https://arxiv.org/pdf/1711.08267
Towards a Magnetically Actuated Laser Scanner for Endoscopic Microsurgeries,Alperen Acemoglu;Nikhil Deshpande;Leonardo S. Mattos,"This article presents the design and assembly of a novel magnetically actuated endoscopic laser scanner device. The device is designed to perform 2D position control and high speed scanning of a fiber-based laser for operation in narrow workspaces. The device includes laser focusing optics to allow non-contact incisions and tablet-based control interface for intuitive teleoperation. The performance of the proof-of-concept device is analysed through controllability and the usability studies. The computer-controlled high-speed scanning demonstrates repeatable results with 21 um precision and a stable response up to 48 Hz. Teleoperation user trials, were performed for trajectory-following tasks with 12 subjects, show an accuracy of 39 um. The innovative design of the device can be applied to both surgical and diagnostic (imaging) applications in endoscopic systems. △ Less","21 November, 2017",https://arxiv.org/pdf/1711.07777
Science Driven Innovations Powering Mobile Product: Cloud AI vs. Device AI Solutions on Smart Device,Deguang Kong,"Recent years have witnessed the increasing popularity of mobile devices (such as iphone) due to the convenience that it brings to human lives. On one hand, rich user profiling and behavior data (including per-app level, app-interaction level and system-interaction level) from heterogeneous information sources make it possible to provide much better services (such as recommendation, advertisement targeting) to customers, which further drives revenue from understanding users' behaviors and improving user' engagement. In order to delight the customers, intelligent personal assistants (such as Amazon Alexa, Google Home and Google Now) are highly desirable to provide real-time audio, video and image recognition, natural language understanding, comfortable user interaction interface, satisfactory recommendation and effective advertisement targeting. This paper presents the research efforts we have conducted on mobile devices which aim to provide much smarter and more convenient services by leveraging statistics and big data science, machine learning and deep learning, user modeling and marketing techniques to bring in significant user growth and user engagement and satisfactions (and happiness) on mobile devices. The developed new features are built at either cloud side or device side, harmonically working together to enhance the current service with the purpose of increasing users' happiness. We illustrate how we design these new features from system and algorithm perspective using different case studies, through which one can easily understand how science driven innovations help to provide much better service in technology and bring more revenue liftup in business. In the meantime, these research efforts have clear scientific contributions and published in top venues, which are playing more and more important roles for mobile AI products. △ Less","20 November, 2017",https://arxiv.org/pdf/1711.07580
Coopetition of software firms in Open source software ecosystems,Anh Nguyen Duc;Daniela S. Cruzes;Geir K. Hanssen;Terje Snarby;Pekka Abrahamsson,"Software firms participate in an ecosystem as a part of their innovation strategy to extend value creation beyond the firms boundary. Participation in an open and independent environment also implies the competition among firms with similar business models and targeted markets. Hence, firms need to consider potential opportunities and challenges upfront. This study explores how software firms interact with others in OSS ecosystems from a coopetition perspective. We performed a quantitative and qualitative analysis of three OSS projects. Finding shows that software firms emphasize the co-creation of common value and partly react to the potential competitiveness on OSS ecosystems. Six themes about coopetition were identified, including spanning gatekeepers, securing communication, open-core sourcing and filtering shared code. Our work contributes to software engineering research with a rich description of coopetition in OSS ecosystems. Moreover, we also come up with several implications for software firms in pursing a harmony participation in OSS ecosystems. △ Less","3 December, 2017",https://arxiv.org/pdf/1711.07049
Enhanced Group Sparse Beamforming for Green Cloud-RAN: A Random Matrix Approach,Yuanming Shi;Jun Zhang;Wei Chen;Khaled B. Letaief,"Group sparse beamforming is a general framework to minimize the network power consumption for cloud radio access networks (Cloud-RANs), which, however, suffers high computational complexity. In particular, a complex optimization problem needs to be solved to obtain the remote radio head (RRH) ordering criterion in each transmission block, which will help to determine the active RRHs and the associated fronthaul links. In this paper, we propose innovative approaches to reduce the complexity of this key step in group sparse beamforming. Specifically, we first develop a smoothed \ell_p-minimization approach with the iterative reweighted-\ell_2 algorithm to return a Karush-Kuhn-Tucker (KKT) point solution, as well as enhancing the capability of inducing group sparsity in the beamforming vectors. By leveraging the Lagrangian duality theory, we obtain closed-form solutions at each iteration to reduce the computational complexity. The well-structured solutions provide the opportunities to apply the large-dimensional random matrix theory to derive deterministic approximations for the RRH ordering criterion. Such an approach helps to guide the RRH selection only based on the statistical channel state information (CSI), which does not require frequent update, thereby significantly reducing the computation overhead. Simulation results shall demonstrate the performance gains of the proposed \ell_p-minimization approach, as well as the effectiveness of the large system analysis based framework for computing RRH ordering criterion. △ Less","19 November, 2017",https://arxiv.org/pdf/1711.06983
Cyclone: High Availability for Persistent Key Value Stores,Amitabha Roy;Subramanya R. Dulloor,"Persistent key value stores are an important component of many distributed data serving solutions with innovations targeted at taking advantage of growing flash speeds. Unfortunately their performance is hampered by the need to maintain and replicate a write ahead log to guarantee availability in the face of machine and storage failures. Cyclone is a replicated log plug-in for key value stores that systematically addresses various sources of this bottleneck. It uses a small amount of non-volatile memory directly addressable by the CPU - such as in the form of NVDIMMs or Intel 3DXPoint - to remove block oriented IO devices such as SSDs from the critical path for appending to the log. This enables it to address network overheads using an implementation of the RAFT consensus protocol that is designed around a userspace network stack to relieve the CPU of the burden of data copies. Finally, it provides a way to efficiently map the commutativity in key-value store APIs to the parallelism available in commodity NICs. Cyclone is able to replicate millions of small updates per second using only commodity 10 gigabit ethernet adapters. As a practical application, we use it to improve the performance (and availability) of RocksDB, a popular persistent key value store by an order of magnitude when compared to its own write ahead log without replication. △ Less","18 November, 2017",https://arxiv.org/pdf/1711.06964
The Cultural Evolution of National Constitutions,Daniel N. Rockmore;Chen Fang;Nicholas J. Foti;Tom Ginsburg;David C. Krakauer,"We explore how ideas from infectious disease and genetics can be used to uncover patterns of cultural inheritance and innovation in a corpus of 591 national constitutions spanning 1789 - 2008. Legal ""Ideas"" are encoded as ""topics"" - words statistically linked in documents - derived from topic modeling the corpus of constitutions. Using these topics we derive a diffusion network for borrowing from ancestral constitutions back to the US Constitution of 1789 and reveal that constitutions are complex cultural recombinants. We find systematic variation in patterns of borrowing from ancestral texts and ""biological""-like behavior in patterns of inheritance with the distribution of ""offspring"" arising through a bounded preferential-attachment process. This process leads to a small number of highly innovative (influential) constitutions some of which have yet to have been identified as so in the current literature. Our findings thus shed new light on the critical nodes of the constitution-making network. The constitutional network structure reflects periods of intense constitution creation, and systematic patterns of variation in constitutional life-span and temporal influence. △ Less","18 November, 2017",https://arxiv.org/pdf/1711.06899
"Fundamentals of the Extremely Green, Flexible, and Profitable 5G M2M Ubiquitous Communications for Remote e-Healthcare and other Social e-Applications",Alexander Markhasin,"The revolutionary trend of the up-to-date medicine can be formulated as wide introduction into basic medicine fields of electronic (e-health) and mobile (m-health) healthcare services and information applications. Unfortunately, all list of qualified m/e-healthcare services can be provided cost-effectively only in urban areas very good covered by broadband 4G/5G wireless communications. Unacceptably high investments are required into deployment of the optic core infrastructure for ubiquitous wide covering of sparsely populated rural, remote, and difficult for access (RRD) areas using the recent (4G) and forthcoming (5G) broadband radio access (RAN) centralized techniques, characterized by short cells ranges, because their profitability boundary exceeds several hundred residents per square km. Furthermore, the unprecedented requirements and new features of the forthcoming Internet of Things (IoT), machine-to-machine (M2M), and many other machine type IT-systems lead to a breakthrough in designing extremely green, flexible, and cost-effective technologies for future 5G wireless systems which will be able to reach in real time the performance extremums, trade-off optimums and fundamental limits. This paper examines the 5G PHY-MAC fundamentals and extremely approaches to creation of the profitable ubiquitous remote e/m-health services and telemedicine as the main innovation technology of popular healthcare and other social e-Applications for RRD territories. Proposed approaches lean on summarizing and develop the results of our previous works on RRD-adapted profitable ubiquitous green 4G/5G wireless multifunctional technologies. △ Less","17 November, 2017",https://arxiv.org/pdf/1711.06469
Assessing the Usability of a Novel System for Programming Education,Giovanni Vincenti;Scott Hilberg;James Braman;Michael Satzinger;Lily Cao,"The authors present the results of a simple usability test performed on line_explorer, an innovative tool aimed at letting students explore programming. The system offers an interactive environment where students can learn, review, and practice programming independently or through step-by-step instruction. Students in Information Technology, Computer Science, and Information Systems were surveyed. The findings show that students have interest in this tool, whereas some groups find this tool more interesting and useful. The findings will help refine the user interface for the next phase of testing which include changes for simplicity, usability and expanded topic content. Overall the survey on line_explorer in its current design phase seem more useful for IT and CS majors, however significant changes are still needed. △ Less","15 November, 2017",https://arxiv.org/pdf/1711.05649
TripletGAN: Training Generative Model with Triplet Loss,Gongze Cao;Yezhou Yang;Jie Lei;Cheng Jin;Yang Liu;Mingli Song,"As an effective way of metric learning, triplet loss has been widely used in many deep learning tasks, including face recognition and person-ReID, leading to many states of the arts. The main innovation of triplet loss is using feature map to replace softmax in the classification task. Inspired by this concept, we propose here a new adversarial modeling method by substituting the classification loss of discriminator with triplet loss. Theoretical proof based on IPM (Integral probability metric) demonstrates that such setting will help the generator converge to the given distribution theoretically under some conditions. Moreover, since triplet loss requires the generator to maximize distance within a class, we justify tripletGAN is also helpful to prevent mode collapse through both theory and experiment. △ Less","14 November, 2017",https://arxiv.org/pdf/1711.05084
Digitising Cultural Complexity: Representing Rich Cultural Data in a Big Data environment,Jennifer Edmond;Georgina Nugent Folan,"One of the major terminological forces driving ICT integration in research today is that of ""big data."" While the phrase sounds inclusive and integrative, ""big data"" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate ""big data"" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can ""be data"" once it is entered into the system as ""data,"" data cleaning and processing, together with the metadata and information architectures that structure and facilitate our cultural archives acquire a capacity to delimit what data are. This engenders a process of simplification that has major implications for the potential for future innovation within research environments that depend on rich material yet are increasingly mediated by digital technologies. This paper presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project which investigates the delimiting effect digital mediation and datafication has on rich, complex cultural data. The paper presents a systematic review of existing implicit definitions of data, elaborating on the implications of these definitions and highlighting the ways in which metadata and computational technologies can restrict the interpretative potential of data. It sheds light on the gap between analogue or augmented digital practices and fully computational ones, and the strategies researchers have developed to deal with this gap. The paper proposes a reconceptualisation of data as it is functionally employed within digitally-mediated research so as to incorporate and acknowledge the richness and complexity of our source materials. △ Less","13 November, 2017",https://arxiv.org/pdf/1711.04452
Cloud Computing and Content Management Systems: A Case Study in Macedonian Education,Jove Jankulovski;Pece Mitrevski,"Technologies have become inseparable of our lives, economy, and the society as a whole. For example, clouds provide numerous computing resources that can facilitate our lives, whereas the Content Management Systems (CMSs) can provide the right content for the right user. Thus, education must embrace these emerging technologies in order to prepare citizens for the 21st century. The research explored 'if' and 'how' Cloud Computing influences the application of CMSs, and 'if' and 'how' it fosters the usage of mobile technologies to access cloud resources. The analyses revealed that some of the respondents have sound experience in using clouds and in using CMSs. Nevertheless, it was evident that significant number of respondents have limited or no experience in cloud computing concepts, cloud security and CMSs, as well. Institutions of the system should update educational policies in order to enable education innovation, provide means and support, and continuously update/upgrade educational infrastructure. △ Less","9 November, 2017",https://arxiv.org/pdf/1711.04025
A New Proof Rule for Almost-Sure Termination,Annabelle McIver;Carroll Morgan;Benjamin Lucien Kaminski;Joost-Pieter Katoen,"An important question for a probabilistic program is whether the probability mass of all its diverging runs is zero, that is that it terminates ""almost surely"". Proving that can be hard, and this paper presents a new method for doing so; it is expressed in a program logic, and so applies directly to source code. The programs may contain both probabilistic- and demonic choice, and the probabilistic choices may depend on the current state. As do other researchers, we use variant functions (a.k.a. ""super-martingales"") that are real-valued and probabilistically might decrease on each loop iteration; but our key innovation is that the amount as well as the probability of the decrease are parametric. We prove the soundness of the new rule, indicate where its applicability goes beyond existing rules, and explain its connection to classical results on denumerable (non-demonic) Markov chains. △ Less","25 December, 2017",https://arxiv.org/pdf/1711.03588
IP Video Conferencing: A Tutorial,Roman Sorokin;Jean-Louis Rougier,"Video conferencing is a well-established area of communications, which have been studied for decades. Recently this area has received a new impulse due to significantly increased bandwidth of Local and Wide area networks, appearance of low-priced video equipment and development of web based media technologies. This paper presents the main techniques behind the modern IP-based videoconferencing services, with a particular focus on codecs, network protocols, architectures and standardization efforts. Questions of security and topologies are also tackled. A description of a typical video conference scenario is provided, demonstrating how the technologies, responsible for different conference aspects, are working together. Traditional industrial disposition as well as modern innovative approaches are both addressed. Current industry trends are highlighted in respect to the topics, described in the tutorial. Legacy analog/digital technologies, together with the gateways between the traditional and the IP videoconferencing systems, are not considered. △ Less","9 November, 2017",https://arxiv.org/pdf/1711.03418
Deep Stacking Networks for Low-Resource Chinese Word Segmentation with Transfer Learning,Jingjing Xu;Xu Sun;Sujian Li;Xiaoyan Cai;Bingzhen Wei,"In recent years, neural networks have proven to be effective in Chinese word segmentation. However, this promising performance relies on large-scale training data. Neural networks with conventional architectures cannot achieve the desired results in low-resource datasets due to the lack of labelled training data. In this paper, we propose a deep stacking framework to improve the performance on word segmentation tasks with insufficient data by integrating datasets from diverse domains. Our framework consists of two parts, domain-based models and deep stacking networks. The domain-based models are used to learn knowledge from different datasets. The deep stacking networks are designed to integrate domain-based models. To reduce model conflicts, we innovatively add communication paths among models and design various structures of deep stacking networks, including Gaussian-based Stacking Networks, Concatenate-based Stacking Networks, Sequence-based Stacking Networks and Tree-based Stacking Networks. We conduct experiments on six low-resource datasets from various domains. Our proposed framework shows significant performance improvements on all datasets compared with several strong baselines. △ Less","4 November, 2017",https://arxiv.org/pdf/1711.01427
Fuzzy clustering using linguistic-valued exponent,Hung Thai Le;Khang Ding Tran;Hung Van Le,"The purpose of this paper is to study the algorithm FCM and some of its famous innovations, analyse and discover the method of applying hedge algebra theory that uses algebra to represent linguistic-valued variables, to FCM. Then, this paper will propose a new FCM-based algorithm which uses hedge algebra to model FCM's exponent parameter. Finally, the design, analysis and implementation of the new algorithm as well some experimental results will be presented to prove our algorithm's capacity of solving clustering problems in practice. △ Less","29 October, 2017",https://arxiv.org/pdf/1711.01149
A Searchable Symmetric Encryption Scheme using BlockChain,Huige Li;Fangguo Zhang;Jiejie He;Haibo Tian,"At present, the cloud storage used in searchable symmetric encryption schemes (SSE) is provided in a private way, which cannot be seen as a true cloud. Moreover, the cloud server is thought to be credible, because it always returns the search result to the user, even they are not correct. In order to really resist this malicious adversary and accelerate the usage of the data, it is necessary to store the data on a public chain, which can be seen as a decentralized system. As the increasing amount of the data, the search problem becomes more and more intractable, because there does not exist any effective solution at present. In this paper, we begin by pointing out the importance of storing the data in a public chain. We then innovatively construct a model of SSE using blockchain(SSE-using-BC) and give its security definition to ensure the privacy of the data and improve the search efficiency. According to the size of data, we consider two different cases and propose two corresponding schemes. Lastly, the security and performance analyses show that our scheme is feasible and secure. △ Less","18 November, 2017",https://arxiv.org/pdf/1711.01030
Cyber-Automotive Simulation and Evaluation Platform for Vehicular Value Added Services,Raja Sattiraju;Pratip Chakraborty;Hans D. Schotten;Xiaohai Lin;Daniel Görges,"An easily moving and safe transportation is an indicator in any country in the world of economic growth and well-being. For the past 100 years, innovation within the automotive sector has brought major technological advances leading to safer, cleaner and more affordable vehicles. But for the most time since the inception of the moving assembly line for vehicle production by Henry Ford, the changes have been incremental / evolutionary. Thanks to the new possibilities due to the IT / wireless revolution, the automotive industry appears to be on the cusp of revolutionary change with potential to dramatically reshape not just the competitive landscape but also the way we interact with vehicles, and indeed the future design of our roads and cities. Apart from connected personal mobility, vehicles are also envisioned to provide Value Added Services (VAS) such as autonomous driving via Vehicle-to-Vehicle (V2V) and Vehicle-to-infrastructure (V2I), electric load balancing via Vehicle-to-Grid (V2G) solutions, communication solutions using Visible Light Communications (VLC) etc. The development and evaluation of vehicular VAS requires a modular and scalable multidisciplinary simulation platform. In this paper we propose a novel simulation platform named Cyber-Automotive Simulation \& Evaluation Platform (CASEP). The purpose of CASEP is to evaluate and visualize the gains of various vehicular VAS with special emphasis on commercial vehicle VAS. The use cases are evaluated with respect to the mission-specific performance indicators, thereby providing usable metrics for optimization. The visualization platform is being developed using the UNITY 3D engine, thereby enabling intuitive interaction as in real physics-based games △ Less","30 October, 2017",https://arxiv.org/pdf/1710.11564
Data Science: A Powerful Catalyst for Cross-Sector Collaborations to Transform the Future of Global Health - Developing a New Interactive Relational Mapping Tool,Barbara Bulc;Cassie Landers;Katherine Driscoll,"The increasingly complex and rapidly changing global health and socio-economic landscape requires fundamentally new ways of thinking, acting and collaborating to solve growing systems challenges. Cross-sectoral collaborations between governments, businesses, international organizations, private investors, academia and non-profits are essential for lasting success in achieving the Sustainable Development Goals (SDGs), and securing a prosperous future for the health and wellbeing of all people. Our aim is to use data science and innovative technologies to map diverse stakeholders and their initiatives around SDGs and specific health targets - with particular focus on SDG 3 (Good Health & Well Being) and SDG 17 (Partnerships for the Goals) - to accelerate cross-sector collaborations. Initially, the mapping tool focuses on Geneva, Switzerland as the world center of global health diplomacy with over 80 key stakeholders and influencers present. As we develop the next level pilot, we aim to build on users' interests, with a potential focus on non-communicable diseases (NCDs) as one of the emerging and most pressing global health issues that requires new collaborative approaches. Building on this pilot, we can later expand beyond only SDG 3 to other SDGs. △ Less","30 October, 2017",https://arxiv.org/pdf/1710.11039
A Framework for Over-the-air Reciprocity Calibration for TDD Massive MIMO Systems,Xiwen Jiang;Alexis Decurninge;Kalyana Gopala;Florian Kaltenberger;Maxime Guillaud;Dirk Slock;Luc Deneire,"One of the biggest challenges in operating massive multiple-input multiple-output systems is the acquisition of accurate channel state information at the transmitter. To take up this challenge, time division duplex is more favorable thanks to its channel reciprocity between downlink and uplink. However, while the propagation channel over the air is reciprocal, the radio-frequency front-ends in the transceivers are not. Therefore, calibration is required to compensate the RF hardware asymmetry. Although various over-the-air calibration methods exist to address the above problem, this paper offers a unified representation of these algorithms, providing a higher level view on the calibration problem, and introduces innovations on calibration methods. We present a novel family of calibration methods, based on antenna grouping, which improve accuracy and speed up the calibration process compared to existing methods. We then provide the Cramér-Rao bound as the performance evaluation benchmark and compare maximum likelihood and least squares estimators. We also differentiate between coherent and non-coherent accumulation of calibration measurements, and point out that enabling non-coherent accumulation allows the training to be spread in time, minimizing impact to the data service. Overall, these results have special value in allowing to design reciprocity calibration techniques that are both accurate and resource-effective. △ Less","30 October, 2017",https://arxiv.org/pdf/1710.10830
DevRank: Mining Influential Developers In Github,Zhifang Liao;Haozhi Jin;Yifan Li;Benhong Zhao;Jinsong Wu;Shengzong Liu,"As the social coding is becoming increasingly popular, understanding the influence of developers can benefit various applications, such as advertisement for new projects and innovations. However, most existing works have focused only on ranking influential nodes in non-weighted and homogeneous networks, which are not able to transfer proper importance scores to the real important node. To rank developers in Github, we define developer's influence on the capacity of attracting attention which can be measured by the number of followers obtained in the future. We further defined a new method, DevRank, which ranks the developers by influence propagation through heterogeneous network constructed according to user behaviors, including ""commit"" and ""follow"". Our experiment compares the performance between DevRank and some other link analysis algorithms, the results have shown that DevRank can improve the ranking accuracy. △ Less","28 October, 2017",https://arxiv.org/pdf/1710.10427
Biometrics-as-a-Service: A Framework to Promote Innovative Biometric Recognition in the Cloud,Veeru Talreja;Terry Ferrett;Matthew C. Valenti;Arun Ross,"Biometric recognition, or simply biometrics, is the use of biological attributes such as face, fingerprints or iris in order to recognize an individual in an automated manner. A key application of biometrics is authentication; i.e., using said biological attributes to provide access by verifying the claimed identity of an individual. This paper presents a framework for Biometrics-as-a-Service (BaaS) that performs biometric matching operations in the cloud, while relying on simple and ubiquitous consumer devices such as smartphones. Further, the framework promotes innovation by providing interfaces for a plurality of software developers to upload their matching algorithms to the cloud. When a biometric authentication request is submitted, the system uses a criteria to automatically select an appropriate matching algorithm. Every time a particular algorithm is selected, the corresponding developer is rendered a micropayment. This creates an innovative and competitive ecosystem that benefits both software developers and the consumers. As a case study, we have implemented the following: (a) an ocular recognition system using a mobile web interface providing user access to a biometric authentication service, and (b) a Linux-based virtual machine environment used by software developers for algorithm development and submission. △ Less","25 October, 2017",https://arxiv.org/pdf/1710.09183
Using Multi-Label Classification for Improved Question Answering,Ricardo Usbeck;Michael Hoffmann;Michael Röder;Jens Lehmann;Axel-Cyrille Ngonga Ngomo,"A plethora of diverse approaches for question answering over RDF data have been developed in recent years. While the accuracy of these systems has increased significantly over time, most systems still focus on particular types of questions or particular challenges in question answering. What is a curse for single systems is a blessing for the combination of these systems. We show in this paper how machine learning techniques can be applied to create a more accurate question answering metasystem by reusing existing systems. In particular, we develop a multi-label classification-based metasystem for question answering over 6 existing systems using an innovative set of 14 question features. The metasystem outperforms the best single system by 14% F-measure on the recent QALD-6 benchmark. Furthermore, we analyzed the influence and correlation of the underlying features on the metasystem quality. △ Less","24 October, 2017",https://arxiv.org/pdf/1710.08634
Deep Learning applied to Road Traffic Speed forecasting,Thomas Epelbaum;Fabrice Gamboa;Jean-Michel Loubes;Jessica Martin,"In this paper, we propose deep learning architectures (FNN, CNN and LSTM) to forecast a regression model for time dependent data. These algorithm's are designed to handle Floating Car Data (FCD) historic speeds to predict road traffic data. For this we aggregate the speeds into the network inputs in an innovative way. We compare the RMSE thus obtained with the results of a simpler physical model, and show that the latter achieves better RMSE accuracy. We also propose a new indicator, which evaluates the algorithms improvement when compared to a benchmark prediction. We conclude by questioning the interest of using deep learning methods for this specific regression task. △ Less","2 October, 2017",https://arxiv.org/pdf/1710.08266
Complex Contagions: A Decade in Review,Douglas Guilbeault;Joshua Becker;Damon Centola,"Since the publication of 'Complex Contagions and the Weakness of Long Ties' in 2007, complex contagions have been studied across an enormous variety of social domains. In reviewing this decade of research, we discuss recent advancements in applied studies of complex contagions, particularly in the domains of health, innovation diffusion, social media, and politics. We also discuss how these empirical studies have spurred complementary advancements in the theoretical modeling of contagions, which concern the effects of network topology on diffusion, as well as the effects of individual-level attributes and thresholds. In synthesizing these developments, we suggest three main directions for future research. The first concerns the study of how multiple contagions interact within the same network and across networks, in what may be called an ecology of contagions. The second concerns the study of how the structure of thresholds and their behavioral consequences can vary by individual and social context. The third area concerns the roles of diversity and homophily in the dynamics of complex contagion, including both diversity of demographic profiles among local peers, and the broader notion of structural diversity within a network. Throughout this discussion, we make an effort to highlight the theoretical and empirical opportunities that lie ahead. △ Less","20 October, 2017",https://arxiv.org/pdf/1710.07606
"Individuals, Institutions, and Innovation in the Debates of the French Revolution",Alexander T. J. Barron;Jenny Huang;Rebecca L. Spang;Simon DeDeo,"The French Revolution brought principles of ""liberty, equality, and brotherhood"" to bear on the day-to-day challenges of governing what was then the largest country in Europe. Its experiments provided a model for future revolutions and democracies across the globe, but this first modern revolution had no model to follow. Using reconstructed transcripts of debates held in the Revolution's first parliament, we present a quantitative analysis of how this system managed innovation. We use information theory to track the creation, transmission, and destruction of patterns of word-use across over 40,000 speeches and more than one thousand speakers. The parliament as a whole was biased toward the adoption of new patterns, but speakers' individual qualities could break these overall trends. Speakers on the left innovated at higher rates while speakers on the right acted, often successfully, to preserve prior patterns. Key players such as Robespierre (on the left) and Abbé Maury (on the right) played information-processing roles emblematic of their politics. Newly-created organizational functions---such as the Assembly's President and committee chairs---had significant effects on debate outcomes, and a distinct transition appears mid-way through the parliament when committees, external to the debate process, gain new powers to ""propose and dispose"" to the body as a whole. Taken together, these quantitative results align with existing qualitative interpretations but also reveal crucial information-processing dynamics that have hitherto been overlooked. Great orators had the public's attention, but deputies (mostly on the political left) who mastered the committee system gained new powers to shape revolutionary legislation. △ Less","18 October, 2017",https://arxiv.org/pdf/1710.06867
Maximum Value Matters: Finding Hot Topics in Scholarly Fields,Jinghao Zhao;Hao Wu;Fengyu Deng;Wentian Bao;Wencheng Tang;Luoyi Fu;Xinbing Wang,"Finding hot topics in scholarly fields can help researchers to keep up with the latest concepts, trends, and inventions in their field of interest. Due to the rarity of complete large-scale scholarly data, earlier studies target this problem based on manual topic extraction from a limited number of domains, with their focus solely on a single feature such as coauthorship, citation relations, and etc. Given the compromised effectiveness of such predictions, in this paper we use a real scholarly dataset from Microsoft Academic Graph, which provides more than 12000 topics in the field of Computer Science (CS), including 1200 venues, 14.4 million authors, 30 million papers and their citation relations over the period of 1950 till now. Aiming to find the topics that will trend in CS area, we innovatively formalize a hot topic prediction problem where, with joint consideration of both inter- and intra-topical influence, 17 different scientific features are extracted for comprehensive description of topic status. By leveraging all those 17 features, we observe good accuracy of topic scale forecasting after 5 and 10 years with R2 values of 0.9893 and 0.9646, respectively. Interestingly, our prediction suggests that the maximum value matters in finding hot topics in scholarly fields, primarily from three aspects: (1) the maximum value of each factor, such as authors' maximum h-index and largest citation number, provides three times the amount of information than the average value in prediction; (2) the mutual influence between the most correlated topics serve as the most telling factor in long-term topic trend prediction, interpreting that those currently exhibiting the maximum growth rates will drive the correlated topics to be hot in the future; (3) we predict in the next 5 years the top 100 fastest growing (maximum growth rate) topics that will potentially get the major attention in CS area. △ Less","18 October, 2017",https://arxiv.org/pdf/1710.06637
On Hashing-Based Approaches to Approximate DNF-Counting,Kuldeep S. Meel;Aditya A. Shrotri;Moshe Y. Vardi,"Propositional model counting is a fundamental problem in artificial intelligence with a wide variety of applications, such as probabilistic inference, decision making under uncertainty, and probabilistic databases. Consequently, the problem is of theoretical as well as practical interest. When the constraints are expressed as DNF formulas, Monte Carlo-based techniques have been shown to provide a fully polynomial randomized approximation scheme (FPRAS). For CNF constraints, hashing-based approximation techniques have been demonstrated to be highly successful. Furthermore, it was shown that hashing-based techniques also yield an FPRAS for DNF counting without usage of Monte Carlo sampling. Our analysis, however, shows that the proposed hashing-based approach to DNF counting provides poor time complexity compared to the Monte Carlo-based DNF counting techniques. Given the success of hashing-based techniques for CNF constraints, it is natural to ask: Can hashing-based techniques provide an efficient FPRAS for DNF counting? In this paper, we provide a positive answer to this question. To this end, we introduce two novel algorithmic techniques: \emph{Symbolic Hashing} and \emph{Stochastic Cell Counting}, along with a new hash family of \emph{Row-Echelon hash functions}. These innovations allow us to design a hashing-based FPRAS for DNF counting of similar complexity (up to polylog factors) as that of prior works. Furthermore, we expect these techniques to have potential applications beyond DNF counting. △ Less","14 October, 2017",https://arxiv.org/pdf/1710.05247
Power Synthesis of Maximally-Sparse Linear Arrays Radiating Shaped Patterns through a Compressive-Sensing Driven Strategy,A. F. Morabito;A. R. Laganà;G. Sorbello;T. Isernia,"We present an innovative approach to the synthesis of linear arrays having the least possible number of elements while radiating shaped beams lying in completely arbitrary power masks. The approach, based on theory and procedures lend from Compressive Sensing, has two innovative key features. First, it exploits at best the multiplicity of equivalent field solutions corresponding to the many different power patterns lying in the given mask. Second, it a-priori optimizes those parameters that affect the performance of Compressive Sensing. The overall problem is formulated as two convex programming routines plus one local optimization, with the inherent advantages in terms of computational time and solutions optimality. An extensive numerical comparison against state-of-the-art procedures proves the effectiveness of the approach. △ Less","13 October, 2017",https://arxiv.org/pdf/1710.04857
Towards In-Transit Analytics for Industry 4.0,Richard Hill;James Devitt;Ashiq Anjum;Muhammad Ali,"Industry 4.0, or Digital Manufacturing, is a vision of inter-connected services to facilitate innovation in the manufacturing sector. A fundamental requirement of innovation is the ability to be able to visualise manufacturing data, in order to discover new insight for increased competitive advantage. This article describes the enabling technologies that facilitate In-Transit Analytics, which is a necessary precursor for Industrial Internet of Things (IIoT) visualisation. △ Less","20 September, 2017",https://arxiv.org/pdf/1710.04121
Fine-Grained Prediction of Syntactic Typology: Discovering Latent Structure with Supervised Learning,Dingquan Wang;Jason Eisner,"We show how to predict the basic word-order facts of a novel language given only a corpus of part-of-speech (POS) sequences. We predict how often direct objects follow their verbs, how often adjectives follow their nouns, and in general the directionalities of all dependency relations. Such typological properties could be helpful in grammar induction. While such a problem is usually regarded as unsupervised learning, our innovation is to treat it as supervised learning, using a large collection of realistic synthetic languages as training data. The supervised learner must identify surface features of a language's POS sequence (hand-engineered or neural features) that correlate with the language's deeper structure (latent trees). In the experiment, we show: 1) Given a small set of real languages, it helps to add many synthetic languages to the training data. 2) Our system is robust even when the POS sequences include noise. 3) Our system on this task outperforms a grammar induction baseline by a large margin. △ Less","10 October, 2017",https://arxiv.org/pdf/1710.03877
Patent Citation Spectroscopy (PCS): Algorithmic retrieval of landmark patents,Jordan A Comins;Stephanie A Carmack;Loet Leydesdorff,"One essential component in the construction of patent landscapes in biomedical research and development (R&D) is identifying the most seminal patents. Hitherto, the identification of seminal patents required subject matter experts within biomedical areas. In this brief communication, we report an analytical method and tool, Patent Citation Spectroscopy (PCS), for rapidly identifying landmark patents in user-specified areas of biomedical innovation. PCS mines the cited references within large sets of patents and provides an estimate of the most historically impactful prior work. The efficacy of PCS is shown in two case studies of biomedical innovation with clinical relevance: (1) RNA interference and (2) cholesterol. PCS mined and analyzed 4,065 cited references related to patents on RNA interference and correctly identified the foundational patent of this technology, as independently reported by subject matter experts on RNAi intellectual property. Secondly, PCS was applied to a broad set of patents dealing with cholesterol - a case study chosen to reflect a more general, as opposed to expert, patent search query. PCS mined through 11,326 cited references and identified the seminal patent as that for Lipitor, the groundbreaking medication for treating high cholesterol as well as the pair of patents underlying Repatha. These cases suggest that PCS provides a useful method for identifying seminal patents in areas of biomedical innovation and therapeutics. The interactive tool is free-to-use at: www.leydesdorff.net/pcs/. △ Less","14 October, 2017",https://arxiv.org/pdf/1710.03349
Iterative PET Image Reconstruction Using Convolutional Neural Network Representation,Kuang Gong;Jiahui Guan;Kyungsang Kim;Xuezhu Zhang;Georges El Fakhri;Jinyi Qi;Quanzheng Li,"PET image reconstruction is challenging due to the ill-poseness of the inverse problem and limited number of detected photons. Recently deep neural networks have been widely and successfully used in computer vision tasks and attracted growing interests in medical imaging. In this work, we trained a deep residual convolutional neural network to improve PET image quality by using the existing inter-patient information. An innovative feature of the proposed method is that we embed the neural network in the iterative reconstruction framework for image representation, rather than using it as a post-processing tool. We formulate the objective function as a constraint optimization problem and solve it using the alternating direction method of multipliers (ADMM) algorithm. Both simulation data and hybrid real data are used to evaluate the proposed method. Quantification results show that our proposed iterative neural network method can outperform the neural network denoising and conventional penalized maximum likelihood methods. △ Less","9 October, 2017",https://arxiv.org/pdf/1710.03344
The Nuts and Bolts of Micropayments: A Survey,Syed Taha Ali;Dylan Clarke;Patrick McCorry,"In this paper, we undertake a comprehensive survey of key trends and innovations in the development of research-based and commercial micropayment systems. Based on our study, we argue that past solutions have largely failed because research has focused heavily on cryptographic and engineering innovation, whereas fundamental issues pertaining to usability, psychology, and economics have been neglected. We contextualize the range of existing challenges for micropayments systems, discuss potential deployment strategies, and identify critical stumbling blocks, some of which we believe researchers and developers have yet to fully recognize. We hope this effort will motivate and guide the development of micropayments systems. △ Less","9 October, 2017",https://arxiv.org/pdf/1710.02964
Equity in 311 Reporting: Understanding Socio-Spatial Differentials in the Propensity to Complain,Constantine Kontokosta;Boyeong Hong;Kristi Korsberg,"Cities across the United States are implementing information communication technologies in an effort to improve government services. One such innovation in e-government is the creation of 311 systems, offering a centralized platform where citizens can request services, report non-emergency concerns, and obtain information about the city via hotline, mobile, or web-based applications. The NYC 311 service request system represents one of the most significant links between citizens and city government, accounting for more than 8,000,000 requests annually. These systems are generating massive amounts of data that, when properly managed, cleaned, and mined, can yield significant insights into the real-time condition of the city. Increasingly, these data are being used to develop predictive models of citizen concerns and problem conditions within the city. However, predictive models trained on these data can suffer from biases in the propensity to make a request that can vary based on socio-economic and demographic characteristics of an area, cultural differences that can affect citizens' willingness to interact with their government, and differential access to Internet connectivity. Using more than 20,000,000 311 requests - together with building violation data from the NYC Department of Buildings and the NYC Department of Housing Preservation and Development; property data from NYC Department of City Planning; and demographic and socioeconomic data from the U.S. Census American Community Survey - we develop a two-step methodology to evaluate the propensity to complain: (1) we predict, using a gradient boosting regression model, the likelihood of heating and hot water violations for a given building, and (2) we then compare the actual complaint volume for buildings with predicted violations to quantify discrepancies across the City. △ Less","6 October, 2017",https://arxiv.org/pdf/1710.02452
Relocation in Car Sharing Systems with Shared Stackable Vehicles: Modelling Challenges and Outlook,Chiara Boldrini;Riccardo Incaini;Raffaele Bruno,"Car sharing is expected to reduce traffic congestion and pollution in cities while at the same time improving accessibility to public transport. However, the most popular form of car sharing, one-way car sharing, still suffers from the vehicle unbalance problem. Innovative solutions to this issue rely on custom vehicles with stackable capabilities: customers or operators can drive a train of vehicles if necessary, thus efficiently bringing several cars from an area with few requests to an area with many requests. However, how to model a car sharing system with stackable vehicles is an open problem in the related literature. In this paper, we propose a queueing theoretical model to fill this gap, and we use this model to derive an upper-bound on user-based relocation capabilities. We also validate, for the first time in the related literature, legacy queueing theoretical models against a trace of real car sharing data. Finally, we present preliminary results about the impact, on car availability, of simple user-based relocation heuristics with stackable vehicles. Our results indicate that user-based relocation schemes that exploit vehicle stackability can significantly improve car availability at stations. △ Less","3 October, 2017",https://arxiv.org/pdf/1710.01113
The Crowdfunding Game,Itai Arieli;Moran Koren;Rann Smorodinsky,"The recent success of crowdfunding for supporting new and innovative products has been overwhelming with over 34 Billion Dollars raised in 2015. In many crowdfunding platforms, firms set a campaign threshold and contributions are collected only if this threshold is reached. During the campaign, consumers are uncertain as to the ex-post value of the product, the business model viability, and the seller's reliability. Consumer who commit to a contribution therefore gambles. This gamble is effected by the campaign's threshold. Contributions to campaigns with higher thresholds are collected only if a greater number of agents find the offering acceptable. Therefore, high threshold serves as a social insurance and thus in high-threshold campaigns, potential contributors feel more at ease with contributing. We introduce the crowdfunding game and explore the contributor's dilemma in the context of experience goods. We discuss equilibrium existence and related social welfare, information aggregation and revenue implications. △ Less","1 October, 2017",https://arxiv.org/pdf/1710.00319
Bounded Rationality in Scholarly Knowledge Discovery,Kristina Lerman;Nathan Hodas;Hao Wu,"In an information-rich world, people's time and attention must be divided among rapidly changing information sources and the diverse tasks demanded of them. How people decide which of the many sources, such as scientific articles or patents, to read and use in their own work affects dissemination of scholarly knowledge and adoption of innovation. We analyze the choices people make about what information to propagate on the citation networks of Physical Review journals, US patents and legal opinions. We observe regularities in behavior consistent with human bounded rationality: rather than evaluate all available choices, people rely on simply cognitive heuristics to decide what information to attend to. We demonstrate that these heuristics bias choices, so that people preferentially propagate information that is easier to discover, often because it is newer or more popular. However, we do not find evidence that popular sources help to amplify the spread of information beyond making it more salient. Our paper provides novel evidence of the critical role that bounded rationality plays in the decisions to allocate attention in social communication. △ Less","30 September, 2017",https://arxiv.org/pdf/1710.00269
Efficient and Reliable Topology Control based Opportunistic Routing Algorithm for WSNs,Ning Li;Jose-Fernan Martinez-Ortega;Vicente Hernandez Diaz,"The opportunistic routing has advantages on improving the packet delivery ratio between source node and candidate set (PDRsc). However, considering the frequent topology variation in wireless sensor networks, how to improve and control the PDR has not been investigated in detail. Therefore, in this paper, we propose an efficient and reliable topology control based opportunistic routing algorithm (ERTO) which takes PDRsc into account. In ERTO, the interference and transmission power loss are taken into account during the calculation of PDRsc. The PDRsc, the expected energy consumption, and the relationship between transmission power and node degree are considered to calculate the optimal transmission power and relay node degree jointly. For improving the routing effective and reducing the calculation complexity, we introduce the multi-objective optimization into the topology control. During the routing process, nodes calculate the optimal transmission power and relay node degree according to the properties of Pareto optimal solution set, by which the optimal solutions can be selected. Based on these innovations, the energy consumption, the transmission delay, and the throughout have been improved greatly compared with the traditional power control based opportunistic routing algorithms. △ Less","29 September, 2017",https://arxiv.org/pdf/1709.10317
Edina: Building an Open Domain Socialbot with Self-dialogues,Ben Krause;Marco Damonte;Mihai Dobre;Daniel Duma;Joachim Fainberg;Federico Fancellu;Emmanuel Kahembwe;Jianpeng Cheng;Bonnie Webber,"We present Edina, the University of Edinburgh's social bot for the Amazon Alexa Prize competition. Edina is a conversational agent whose responses utilize data harvested from Amazon Mechanical Turk (AMT) through an innovative new technique we call self-dialogues. These are conversations in which a single AMT Worker plays both participants in a dialogue. Such dialogues are surprisingly natural, efficient to collect and reflective of relevant and/or trending topics. These self-dialogues provide training data for a generative neural network as well as a basis for soft rules used by a matching score component. Each match of a soft rule against a user utterance is associated with a confidence score which we show is strongly indicative of reply quality, allowing this component to self-censor and be effectively integrated with other components. Edina's full architecture features a rule-based system backing off to a matching score, backing off to a generative neural network. Our hybrid data-driven methodology thus addresses both coverage limitations of a strictly rule-based approach and the lack of guarantees of a strictly machine-learning approach. △ Less","28 September, 2017",https://arxiv.org/pdf/1709.09816
Generalized Sparse and Low-Rank Optimization for Ultra-Dense Networks,Yuanming Shi;Jun Zhang;Wei Chen;Khaled B. Letaief,"Ultra-dense network (UDN) is a promising technology to further evolve wireless networks and meet the diverse performance requirements of 5G networks. With abundant access points, each with communication, computation and storage resources, UDN brings unprecedented benefits, including significant improvement in network spectral efficiency and energy efficiency, greatly reduced latency to enable novel mobile applications, and the capability of providing massive access for Internet of Things (IoT) devices. However, such great promises come with formidable research challenges. To design and operate such complex networks with various types of resources, efficient and innovative methodologies will be needed. This motivates the recent introduction of highly structured and generalizable models for network optimization. In this article, we present some recently proposed large-scale sparse and low-rank frameworks for optimizing UDNs, supported by various motivating applications. A special attention is paid on algorithmic approaches to deal with nonconvex objective functions and constraints, as well as computational scalability. △ Less","26 September, 2017",https://arxiv.org/pdf/1709.09103
Delay based Duplicate Transmission Avoid (DDA) Coordination Scheme for Opportunistic routing,Ning Li;Jose-Fernan Martinez-Ortega;Vicente Hernandez Diaz,"Since the packet is transmitted to a set of relaying nodes in opportunistic routing strategy, so the transmission delay and the duplication transmission are serious. For reducing the transmission delay and the duplicate transmission, in this paper, we propose the delay based duplication transmission avoid (DDA) coordination scheme for opportunistic routing. In this coordination scheme, the candidate relaying nodes are divided into different fully connected relaying networks, so the duplicate transmission is avoided. Moreover, we propose the relaying network recognition algorithm which can be used to judge whether the sub-network is fully connected or not. The properties of the relaying networks are investigated in detail in this paper. When the fully connected relaying networks are got, they will be the basic units in the next hop relaying network selection. In this paper, we prove that the packet delivery ratio of the high priority relaying nodes in the relaying network has greater effection on the relaying delay than that of the low priority relaying nodes. According to this conclusion, in DDA, the relaying networks which the packet delivery ratios of the high priority relaying nodes are high have higher priority than that of the low one. During the next hop relaying network selection, the transmission delay, the network utility, and the packet delivery ratio are taken into account. By these innovations, the DDA can improve the network performance greatly than that of ExOR and SOAR. △ Less","25 September, 2017",https://arxiv.org/pdf/1709.08540
"Machine Learning for Networking: Workflow, Advances and Opportunities",Mowei Wang;Yong Cui;Xin Wang;Shihan Xiao;Junchen Jiang,"Recently, machine learning has been used in every possible field to leverage its amazing power. For a long time, the net-working and distributed computing system is the key infrastructure to provide efficient computational resource for machine learning. Networking itself can also benefit from this promising technology. This article focuses on the application of Machine Learning techniques for Networking (MLN), which can not only help solve the intractable old network questions but also stimulate new network applications. In this article, we summarize the basic workflow to explain how to apply the machine learning technology in the networking domain. Then we provide a selective survey of the latest representative advances with explanations on their design principles and benefits. These advances are divided into several network design objectives and the detailed information of how they perform in each step of MLN workflow is presented. Finally, we shed light on the new opportunities on networking design and community building of this new inter-discipline. Our goal is to provide a broad research guideline on networking with machine learning to help and motivate researchers to develop innovative algorithms, standards and frameworks. △ Less","16 November, 2017",https://arxiv.org/pdf/1709.08339
Probability Prediction based Reliable Opportunistic (PRO) Routing Algorithm for VANETs,Ning Li;Jose-Fernan Martinez-Ortega;Vicente Hernandez Diaz;Jose Antonio Sanchez Fernandez,"In the Vehicular ad hoc networks (VANETs), due to the high mobility of vehicles, the network parameters change frequently and the information which the sender maintains may outdate when it wants to transmit data packet to the receiver, so for improving the routing effective, we propose the probability prediction based reliable (PRO) opportunistic routing for VANETs. The PRO routing algorithm can predict the variation of Signal to Interference plus Noise Ratio (SINR) and packet queue length (PQL) in the receiver. The prediction results are used to determine the utility of each relaying vehicle in the candidate set. The calculation of the vehicle utility is weight based algorithm and the weights are the variances of SINR and PQL of the candidate relaying vehicles. The relaying priority of each relaying vehicle is determined by the value of the utility. By these innovations, the PRO can achieve better routing performance (such as the packet delivery ratio, the end-to-end delay, and the network throughput) than the SRPE, ExOR (street-centric), and GPSR routing algorithms. △ Less","24 September, 2017",https://arxiv.org/pdf/1709.08199
Emerging Topics in Assistive Reading Technology: From Presentation to Content Accessibility,Cynthia Chen;Peter Fay,"With the recent focus in the accessibility field, researchers from academia and industry have been very active in developing innovative techniques and tools for assistive technology. Especially with handheld devices getting ever powerful and being able to recognize the user's voice, screen magnification for individuals with low-vision, and eye tracking devices used in studies with individuals with physical and intellectual disabilities, the science field is quickly adapting and creating conclusions as well as products to help. In this paper, we will focus on new technology and tools to help make reading easier--including reformatting document presentation (for people with physical vision impairments) and text simplification to make information itself easier to interpret (for people with intellectual disabilities). A real-world case study is reported based on our experience to make documents more accessible. △ Less","26 September, 2017",https://arxiv.org/pdf/1709.08106
Inference in Graphical Models via Semidefinite Programming Hierarchies,Murat A. Erdogdu;Yash Deshpande;Andrea Montanari,"Maximum A posteriori Probability (MAP) inference in graphical models amounts to solving a graph-structured combinatorial optimization problem. Popular inference algorithms such as belief propagation (BP) and generalized belief propagation (GBP) are intimately related to linear programming (LP) relaxation within the Sherali-Adams hierarchy. Despite the popularity of these algorithms, it is well understood that the Sum-of-Squares (SOS) hierarchy based on semidefinite programming (SDP) can provide superior guarantees. Unfortunately, SOS relaxations for a graph with n vertices require solving an SDP with n^{Θ(d)} variables where d is the degree in the hierarchy. In practice, for d\ge 4, this approach does not scale beyond a few tens of variables. In this paper, we propose binary SDP relaxations for MAP inference using the SOS hierarchy with two innovations focused on computational efficiency. Firstly, in analogy to BP and its variants, we only introduce decision variables corresponding to contiguous regions in the graphical model. Secondly, we solve the resulting SDP using a non-convex Burer-Monteiro style method, and develop a sequential rounding procedure. We demonstrate that the resulting algorithm can solve problems with tens of thousands of variables within minutes, and outperforms BP and GBP on practical problems such as image denoising and Ising spin glasses. Finally, for specific graph types, we establish a sufficient condition for the tightness of the proposed partial SOS relaxation. △ Less","19 September, 2017",https://arxiv.org/pdf/1709.06525
A Deep Learning-based Framework for Conducting Stealthy Attacks in Industrial Control Systems,Cheng Feng;Tingting Li;Zhanxing Zhu;Deeph Chana,"Industrial control systems (ICS), which in many cases are components of critical national infrastructure, are increasingly being connected to other networks and the wider internet motivated by factors such as enhanced operational functionality and improved efficiency. However, set in this context, it is easy to see that the cyber attack surface of these systems is expanding, making it more important than ever that innovative solutions for securing ICS be developed and that the limitations of these solutions are well understood. The development of anomaly based intrusion detection techniques has provided capability for protecting ICS from the serious physical damage that cyber breaches are capable of delivering to them by monitoring sensor and control signals for abnormal activity. Recently, the use of so-called stealthy attacks has been demonstrated where the injection of false sensor measurements can be used to mimic normal control system signals, thereby defeating anomaly detectors whilst still delivering attack objectives. In this paper we define a deep learning-based framework which allows an attacker to conduct stealthy attacks with minimal a-priori knowledge of the target ICS. Specifically, we show that by intercepting the sensor and/or control signals in an ICS for a period of time, a malicious program is able to automatically learn to generate high-quality stealthy attacks which can achieve specific attack goals whilst bypassing a black box anomaly detector. Furthermore, we demonstrate the effectiveness of our framework for conducting stealthy attacks using two real-world ICS case studies. We contend that our results motivate greater attention on this area by the security community as we demonstrate that currently assumed barriers for the successful execution of such attacks are relaxed. △ Less","19 September, 2017",https://arxiv.org/pdf/1709.06397
Towards Building a Knowledge Base of Monetary Transactions from a News Collection,Jan R. Benetka;Krisztian Balog;Kjetil Nørvåg,"We address the problem of extracting structured representations of economic events from a large corpus of news articles, using a combination of natural language processing and machine learning techniques. The developed techniques allow for semi-automatic population of a financial knowledge base, which, in turn, may be used to support a range of data mining and exploration tasks. The key challenge we face in this domain is that the same event is often reported multiple times, with varying correctness of details. We address this challenge by first collecting all information pertinent to a given event from the entire corpus, then considering all possible representations of the event, and finally, using a supervised learning method, to rank these representations by the associated confidence scores. A main innovative element of our approach is that it jointly extracts and stores all attributes of the event as a single representation (quintuple). Using a purpose-built test set we demonstrate that our supervised learning approach can achieve 25% improvement in F1-score over baseline methods that consider the earliest, the latest or the most frequent reporting of the event. △ Less","17 September, 2017",https://arxiv.org/pdf/1709.05743
Clustering of Musical Pieces through Complex Networks: an Assessment over Guitar Solos,Stefano Ferretti,"Musical pieces can be modeled as complex networks. This fosters innovative ways to categorize music, paving the way towards novel applications in multimedia domains, such as music didactics, multimedia entertainment and digital music generation. Clustering these networks through their main metrics allows grouping similar musical tracks. To show the viability of the approach, we provide results on a dataset of guitar solos. △ Less","14 September, 2017",https://arxiv.org/pdf/1709.05193
Extending LTE into the Unlicensed Spectrum: Technical Analysis of the Proposed Variants,Mina Labib;Vuk Marojevic;Jeffrey H. Reed;Amir I. Zaghloul,"The commercial success of the Long Term Evolution (LTE) and the resulting growth in mobile data demand have urged cellular network operators to strive for new innovations. LTE in unlicensed spectrum has been proposed to allow cellular network operators to offload some of their data traffic by accessing the unlicensed 5 GHz frequency band. Currently, there are three proposed variants for LTE operation in the unlicensed band, namely LTE-U, Licensed Spectrum Access (LAA), and MulteFire. This paper provides a comparative analysis of these variants and explains the current regulations of the 5 GHz band in different parts of the world. We present the technical details of the three proposed versions and analyze them in terms of their operational features and coexistence capabilities to provide an R\&D perspective for their deployment and coexistence with legacy systems. △ Less","13 September, 2017",https://arxiv.org/pdf/1709.04458
Modeling Profit of Sliced 5G Networks for Advanced Network Resource Management and Slice Implementation,Bin Han;Shreya Tayade;Hans D. Schotten,"The core innovation in future 5G cellular networksnetwork slicing, aims at providing a flexible and efficient framework of network organization and resource management. The revolutionary network architecture based on slices, makes most of the current network cost models obsolete, as they estimate the expenditures in a static manner. In this paper, a novel methodology is proposed, in which a value chain in sliced networks is presented. Based on the proposed value chain, the profits generated by different slices are analyzed, and the task of network resource management is modeled as a multiobjective optimization problem. Setting strong assumptions, this optimization problem is analyzed starting from a simple ideal scenario. By removing the assumptions step-by-step, realistic but complex use cases are approached. Through this progressive analysis, technical challenges in slice implementation and network optimization are investigated under different scenarios. For each challenge, some potentially available solutions are suggested, and likely applications are also discussed. △ Less","12 September, 2017",https://arxiv.org/pdf/1709.03784
What does fault tolerant Deep Learning need from MPI?,Vinay Amatya;Abhinav Vishnu;Charles Siegel;Jeff Daily,"Deep Learning (DL) algorithms have become the de facto Machine Learning (ML) algorithm for large scale data analysis. DL algorithms are computationally expensive - even distributed DL implementations which use MPI require days of training (model learning) time on commonly studied datasets. Long running DL applications become susceptible to faults - requiring development of a fault tolerant system infrastructure, in addition to fault tolerant DL algorithms. This raises an important question: What is needed from MPI for de- signing fault tolerant DL implementations? In this paper, we address this problem for permanent faults. We motivate the need for a fault tolerant MPI specification by an in-depth consideration of recent innovations in DL algorithms and their properties, which drive the need for specific fault tolerance features. We present an in-depth discussion on the suitability of different parallelism types (model, data and hybrid); a need (or lack thereof) for check-pointing of any critical data structures; and most importantly, consideration for several fault tolerance proposals (user-level fault mitigation (ULFM), Reinit) in MPI and their applicability to fault tolerant DL implementations. We leverage a distributed memory implementation of Caffe, currently available under the Machine Learning Toolkit for Extreme Scale (MaTEx). We implement our approaches by ex- tending MaTEx-Caffe for using ULFM-based implementation. Our evaluation using the ImageNet dataset and AlexNet, and GoogLeNet neural network topologies demonstrates the effectiveness of the proposed fault tolerant DL implementation using OpenMPI based ULFM. △ Less","11 September, 2017",https://arxiv.org/pdf/1709.03316
Implicit Cooperative Positioning in Vehicular Networks,Gloria Soatti;Monica Nicoli;Nil Garcia;Benoit Denis;Ronald Raulefs;Henk Wymeersch,"Absolute positioning of vehicles is based on Global Navigation Satellite Systems (GNSS) combined with on-board sensors and high-resolution maps. In Cooperative Intelligent Transportation Systems (C-ITS), the positioning performance can be augmented by means of vehicular networks that enable vehicles to share location-related information. This paper presents an Implicit Cooperative Positioning (ICP) algorithm that exploits the Vehicle-to-Vehicle (V2V) connectivity in an innovative manner, avoiding the use of explicit V2V measurements such as ranging. In the ICP approach, vehicles jointly localize non-cooperative physical features (such as people, traffic lights or inactive cars) in the surrounding areas, and use them as common noisy reference points to refine their location estimates. Information on sensed features are fused through V2V links by a consensus procedure, nested within a message passing algorithm, to enhance the vehicle localization accuracy. As positioning does not rely on explicit ranging information between vehicles, the proposed ICP method is amenable to implementation with off-the-shelf vehicular communication hardware. The localization algorithm is validated in different traffic scenarios, including a crossroad area with heterogeneous conditions in terms of feature density and V2V connectivity, as well as a real urban area by using Simulation of Urban MObility (SUMO) for traffic data generation. Performance results show that the proposed ICP method can significantly improve the vehicle location accuracy compared to the stand-alone GNSS, especially in harsh environments, such as in urban canyons, where the GNSS signal is highly degraded or denied. △ Less","5 September, 2017",https://arxiv.org/pdf/1709.01282
Multi-task Dictionary Learning based Convolutional Neural Network for Computer aided Diagnosis with Longitudinal Images,Jie Zhang;Qingyang Li;Richard J. Caselli;Jieping Ye;Yalin Wang,"Algorithmic image-based diagnosis and prognosis of neurodegenerative diseases on longitudinal data has drawn great interest from computer vision researchers. The current state-of-the-art models for many image classification tasks are based on the Convolutional Neural Networks (CNN). However, a key challenge in applying CNN to biological problems is that the available labeled training samples are very limited. Another issue for CNN to be applied in computer aided diagnosis applications is that to achieve better diagnosis and prognosis accuracy, one usually has to deal with the longitudinal dataset, i.e., the dataset of images scanned at different time points. Here we argue that an enhanced CNN model with transfer learning for the joint analysis of tasks from multiple time points or regions of interests may have a potential to improve the accuracy of computer aided diagnosis. To reach this goal, we innovate a CNN based deep learning multi-task dictionary learning framework to address the above challenges. Firstly, we pre-train CNN on the ImageNet dataset and transfer the knowledge from the pre-trained model to the medical imaging progression representation, generating the features for different tasks. Then, we propose a novel unsupervised learning method, termed Multi-task Stochastic Coordinate Coding (MSCC), for learning different tasks by using shared and individual dictionaries and generating the sparse features required to predict the future cognitive clinical scores. We apply our new model in a publicly available neuroimaging cohort to predict clinical measures with two different feature sets and compare them with seven other state-of-the-art methods. The experimental results show our proposed method achieved superior results. △ Less","31 August, 2017",https://arxiv.org/pdf/1709.00042
"R^3
: Reinforced Reader-Ranker for Open-Domain Question Answering",Shuohang Wang;Mo Yu;Xiaoxiao Guo;Zhiguo Wang;Tim Klinger;Wei Zhang;Shiyu Chang;Gerald Tesauro;Bowen Zhou;Jing Jiang,"In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that ""reads"" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader (R^3), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets. △ Less","21 November, 2017",https://arxiv.org/pdf/1709.00023
Seminar Innovation Management - Winter Term 2017,Gerd Häusler;Aleksandra Milczarek;Markus Schreiter;Thomas Kästner;Florian Willomitzer;Andreas Maier;Florian Schiffers;Stefan Steidl;Temitope Paul Onanuga;Mathias Unberath;Florian Dötzer;Maike Stöve;Jonas Hajek;Christian Heidorn;Felix Häußler;Tobias Geimer;Johannes Wendel,"This document contains the results obtained by the Innovation Management Seminar in winter term 2017. In total 11 ideas have been developed by the team. In the document all 11 ideas show improvements for future applications in ophthalmology. The 11 ideas are AR/VR Glasses with Medical Applications, Augmented Reality Eye Surgery, Game Diagnosis, Intelligent Adapting Glasses, MD Facebook, Medical Crowd Segmentation, Personalized 3D Model of the Human Eye, Photoacoustic Contact Lens, Power Supply Smart Contact Lens, VR-Cornea and Head Mount for Fundus Imaging △ Less","22 August, 2017",https://arxiv.org/pdf/1708.09706
Stem-ming the Tide: Predicting STEM attrition using student transcript data,Lovenoor Aulck;Rohan Aras;Lysia Li;Coulter L'Heureux;Peter Lu;Jevin West,"Science, technology, engineering, and math (STEM) fields play growing roles in national and international economies by driving innovation and generating high salary jobs. Yet, the US is lagging behind other highly industrialized nations in terms of STEM education and training. Furthermore, many economic forecasts predict a rising shortage of domestic STEM-trained professions in the US for years to come. One potential solution to this deficit is to decrease the rates at which students leave STEM-related fields in higher education, as currently over half of all students intending to graduate with a STEM degree eventually attrite. However, little quantitative research at scale has looked at causes of STEM attrition, let alone the use of machine learning to examine how well this phenomenon can be predicted. In this paper, we detail our efforts to model and predict dropout from STEM fields using one of the largest known datasets used for research on students at a traditional campus setting. Our results suggest that attrition from STEM fields can be accurately predicted with data that is routinely collected at universities using only information on students' first academic year. We also propose a method to model student STEM intentions for each academic term to better understand the timing of STEM attrition events. We believe these results show great promise in using machine learning to improve STEM retention in traditional and non-traditional campus settings. △ Less","28 August, 2017",https://arxiv.org/pdf/1708.09344
How 5G (and concomitant technologies) will revolutionize healthcare,Siddique Latif;Junaid Qadir;Shahzad Farooq;Muhammad Ali Imran,"In this paper, we build the case that 5G and concomitant emerging technologies (such as IoT, big data, artificial intelligence, and machine learning) will transform global healthcare systems in the near future. Our optimism around 5G-enabled healthcare stems from a confluence of significant technical pushes that are already at play: apart from the availability of high-throughput low-latency wireless connectivity, other significant factors include the democratization of computing through cloud computing; the democratization of AI and cognitive computing (e.g., IBM Watson); and the commoditization of data through crowdsourcing and digital exhaust. These technologies together can finally crack a dysfunctional healthcare system that has largely been impervious to technological innovations. We highlight the persistent deficiencies of the current healthcare system, and then demonstrate how the 5G-enabled healthcare revolution can fix these deficiencies. We also highlight open technical research challenges, and potential pitfalls, that may hinder the development of such a 5G-enabled health revolution. △ Less","17 August, 2017",https://arxiv.org/pdf/1708.08746
A parameterized activation function for learning fuzzy logic operations in deep neural networks,Luke B. Godfrey;Michael S. Gashler,"We present a deep learning architecture for learning fuzzy logic expressions. Our model uses an innovative, parameterized, differentiable activation function that can learn a number of logical operations by gradient descent. This activation function allows a neural network to determine the relationships between its input variables and provides insight into the logical significance of learned network parameters. We provide a theoretical basis for this parameterization and demonstrate its effectiveness and utility by successfully applying our model to five classification problems from the UCI Machine Learning Repository. △ Less","11 September, 2017",https://arxiv.org/pdf/1708.08557
A European research roadmap for optimizing societal impact of big data on environment and energy efficiency,Martí Cuquet;Anna Fensel;Lorenzo Bigagli,"We present a roadmap to guide European research efforts towards a socially responsible big data economy that maximizes the positive impact of big data in environment and energy efficiency. The goal of the roadmap is to allow stakeholders and the big data community to identify and meet big data challenges, and to proceed with a shared understanding of the societal impact, positive and negative externalities, and concrete problems worth investigating. It builds upon a case study focused on the impact of big data practices in the context of Earth Observation that reveals both positive and negative effects in the areas of economy, society and ethics, legal frameworks and political issues. The roadmap identifies European technical and non-technical priorities in research and innovation to be addressed in the upcoming five years in order to deliver societal impact, develop skills and contribute to standardization. △ Less","25 August, 2017",https://arxiv.org/pdf/1708.07871
Structure constrained by metadata in networks of chess players,Nahuel Almeira;Ana Laura Schaigorodsky;Juan Ignacio Perotti;Orlando Vito Billoni,"Chess is an emblematic sport that stands out because of its age, popularity and complexity. It has served to study human behavior from the perspective of a wide number of disciplines, from cognitive skills such as memory and learning, to aspects like innovation and decision making. Given that an extensive documentation of chess games played throughout history is available, it is possible to perform detailed and statistically significant studies about this sport. Here we use one of the most extensive chess databases in the world to construct two networks of chess players. One of the networks includes games that were played over-the-board and the other contains games played on the Internet. We study the main topological characteristics of the networks, such as degree distribution and correlations, transitivity and community structure. We complement the structural analysis by incorporating players' level of play as node metadata. Although both networks are topologically different, we show that in both cases players gather in communities according to their expertise and that an emergent rich-club structure, composed by the top-rated players, is also present. △ Less","9 November, 2017",https://arxiv.org/pdf/1708.06694
Aadhaar Card: Challenges and Impact on Digital Transformation,Raja Siddharth Raju;Sukhdev Singh;Kiran Khatter,"Objectives: This paper presents a brief review on Aadhaar card, and discusses the scope and advantages of linking Aadhaar card to various systems. Further we present various cases in which Aadhaar card may pose security threats. The observations of Supreme Court of India are also presented in this paper followed by a discussion on the loopholes in the existing system. Methods: We conducted literature survey based on the various research articles, leading newspapers, case studies and the observations of Supreme Court of India, and categorized the various cases into three categories. Findings: Aadhaar project is one of the significant projects in India to bring the universal trend of digital innovation. The launch of this project was focused on the inter-operability of various e-governance functionalities to ensure the optimal utilization of Information, Communication and Technology Infrastructure. Towards this Government of India has recently made Aadhaar card mandatory for many government applications, and also has promoted Aadhaar enabled transactions. Improvements: There are many issues related to security and privacy of the Aadhaar data need to be addressed. This paper highlights such cases. △ Less","16 August, 2017",https://arxiv.org/pdf/1708.05117
Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction,Hossein Soleimani;James Hensman;Suchi Saria,"Missing data and noisy observations pose significant challenges for reliably predicting events from irregularly sampled multivariate time series (longitudinal) data. Imputation methods, which are typically used for completing the data prior to event prediction, lack a principled mechanism to account for the uncertainty due to missingness. Alternatively, state-of-the-art joint modeling techniques can be used for jointly modeling the longitudinal and event data and compute event probabilities conditioned on the longitudinal observations. These approaches, however, make strong parametric assumptions and do not easily scale to multivariate signals with many observations. Our proposed approach consists of several key innovations. First, we develop a flexible and scalable joint model based upon sparse multiple-output Gaussian processes. Unlike state-of-the-art joint models, the proposed model can explain highly challenging structure including non-Gaussian noise while scaling to large data. Second, we derive an optimal policy for predicting events using the distribution of the event occurrence estimated by the joint model. The derived policy trades-off the cost of a delayed detection versus incorrect assessments and abstains from making decisions when the estimated event probability does not satisfy the derived confidence criteria. Experiments on a large dataset show that the proposed framework significantly outperforms state-of-the-art techniques in event prediction. △ Less","15 August, 2017",https://arxiv.org/pdf/1708.04757
An ELU Network with Total Variation for Image Denoising,Tianyang Wang;Zhengrui Qin;Michelle Zhu,"In this paper, we propose a novel convolutional neural network (CNN) for image denoising, which uses exponential linear unit (ELU) as the activation function. We investigate the suitability by analyzing ELU's connection with trainable nonlinear reaction diffusion model (TNRD) and residual denoising. On the other hand, batch normalization (BN) is indispensable for residual denoising and convergence purpose. However, direct stacking of BN and ELU degrades the performance of CNN. To mitigate this issue, we design an innovative combination of activation layer and normalization layer to exploit and leverage the ELU network, and discuss the corresponding rationale. Moreover, inspired by the fact that minimizing total variation (TV) can be applied to image denoising, we propose a TV regularized L2 loss to evaluate the training effect during the iterations. Finally, we conduct extensive experiments, showing that our model outperforms some recent and popular approaches on Gaussian denoising with specific or randomized noise levels for both gray and color images. △ Less","14 August, 2017",https://arxiv.org/pdf/1708.04317
Trust architectures in payment systems: the great bifurcation,Dominique Boullier;Niranjan Sivakumar;Maxime Crepel;Stéphane Juguet,"Payments architectures are on the verge of a great bifurcation that must be documented in order to be debated. Google is moving towards a quasi bank while Apple and Google disseminate payment systems over smartphones. At the same time, block chain might become a distributed ledger introducing a radical new model of trusted third-party. The detailed history of credit card systems helps understand why the game of security has always been trigged by a delegation process of the risk to third parties and by the cat-and-mouse game of security and fraud. Technologies were designed to solve these issues but have always been closely related to innovations in institutional assemblages. These payments systems shape our social life and the stakes of trust that we put in these architectures require a truly political examination. △ Less","13 August, 2017",https://arxiv.org/pdf/1708.03864
Structure in scientific networks: towards predictions of research dynamism,Benjamin W. Stewart;Andy Rivas;Luat T. Vuong,"Certain areas of scientific research flourish while others lose advocates and attention. We are interested in whether structural patterns within citation networks correspond to the growth or decline of the research areas to which those networks belong. We focus on three topic areas within optical physics as a set of cases; those areas have developed along different trajectories: one continues to expand rapidly; another is on the wane after an earlier peak; the final area has re-emerged after a short waning period. These three areas have substantial overlaps in the types of equipment they use and general methodology; at the same time, their citation networks are largely independent of each other. For each of our three areas, we map the citation networks of the top-100 most-cited papers, published pre-1999. In order to quantify the structures of the selected articles' citation networks, we use a modified version of weak tie theory in tandem with entropy measures. Although the fortunes of a given research area are most obviously the result of accumulated innovations and impasses, our preliminary study provides evidence that these citation networks' emergent structures reflect those developments and may shape evolving conversations in the scholarly literature. △ Less","13 August, 2017",https://arxiv.org/pdf/1708.03850
Internet of Tangible Things (IoTT): Challenges and Opportunities for Tangible Interaction with IoT,Leonardo Angelini;Nadine Couture;Omar Abou Khaled;Elena Mugellini,"In the Internet of Things era, an increasing number of household devices and everyday objects are able to send to and retrieve information from the Internet, offering innovative services to the user. However, most of these devices provide only smartphone or web interfaces to control the IoT object properties and functions. As a result, generally, the interaction is disconnected from the physical world, decreasing the user experience and increasing the risk of isolating the user in digital bubbles. We argue that tangible interaction can counteract this trend and this paper discusses the potential benefits and the still open challenges of tangible interaction applied to the Internet of Things. To underline this need, we introduce the term Internet of Tangible Things. In the article, after an analysis of current open challenges for Human-Computer Interaction in IoT, we summarize current trends in tangible interaction and extrapolate eight tangible interaction properties that could be exploited for designing novel interactions with IoT objects. Through a systematic literature review of tangible interaction applied to IoT, we show what has been already explored in the systems that pioneered the field and the future explorations that still have to be conducted. △ Less","8 August, 2017",https://arxiv.org/pdf/1708.02664
A criterion for bubble merging in liquid metal: computational and experimental study,Mojtaba Barzegari;Hossein Bayani;S. M. H. Mirbagheri,"An innovative model is presented for merging of bubbles inside a liquid metal. The proposed model is based on forming a thin film (narrow channel) between merging bubbles during growth. Rupturing of the film occurs when an oscillation in velocity and pressure arises inside the channel followed by merging of the bubbles. The proposed model based on lattice Boltzmann Method is capable of simulating merging bubbles in micro, meso, and macro-scales with no limitation on the number of bubbles. Experimental studies reveal a good consistency between modeling results and real conditions. △ Less","4 August, 2017",https://arxiv.org/pdf/1708.01608
Correlation and Class Based Block Formation for Improved Structured Dictionary Learning,Nagendra Kumar;Rohit Sinha,"In recent years, the creation of block-structured dictionary has attracted a lot of interest. Learning such dictionaries involve two step process: block formation and dictionary update. Both these steps are important in producing an effective dictionary. The existing works mostly assume that the block structure is known a priori while learning the dictionary. For finding the unknown block structure given a dictionary commonly sparse agglomerative clustering (SAC) is used. It groups atoms based on their consistency in sparse coding with respect to the unstructured dictionary. This paper explores two innovations towards improving the reconstruction as well as the classification ability achieved with the block-structured dictionary. First, we propose a novel block structuring approach that makes use of the correlation among dictionary atoms. Unlike the SAC approach, which groups diverse atoms, in the proposed approach the blocks are formed by grouping the top most correlated atoms in the dictionary. The proposed block clustering approach is noted to yield significant reductions in redundancy as well as provides a direct control on the block size when compared with the existing SAC-based block structuring. Later, motivated by works using supervised \emph{a priori} known block structure, we also explore the incorporation of class information in the proposed block formation approach to further enhance the classification ability of the block dictionary. For assessment of the reconstruction ability with proposed innovations is done on synthetic data while the classification ability has been evaluated in large variability speaker verification task. △ Less","6 August, 2017",https://arxiv.org/pdf/1708.01448
Adversarial-Playground: A Visualization Suite Showing How Adversarial Examples Fool Deep Learning,Andrew P. Norton;Yanjun Qi,"Recent studies have shown that attackers can force deep learning models to misclassify so-called ""adversarial examples"": maliciously generated images formed by making imperceptible modifications to pixel values. With growing interest in deep learning for security applications, it is important for security experts and users of machine learning to recognize how learning systems may be attacked. Due to the complex nature of deep learning, it is challenging to understand how deep models can be fooled by adversarial examples. Thus, we present a web-based visualization tool, Adversarial-Playground, to demonstrate the efficacy of common adversarial methods against a convolutional neural network (CNN) system. Adversarial-Playground is educational, modular and interactive. (1) It enables non-experts to compare examples visually and to understand why an adversarial example can fool a CNN-based image classifier. (2) It can help security experts explore more vulnerability of deep learning as a software module. (3) Building an interactive visualization is challenging in this domain due to the large feature space of image classification (generating adversarial examples is slow in general and visualizing images are costly). Through multiple novel design choices, our tool can provide fast and accurate responses to user requests. Empirically, we find that our client-server division strategy reduced the response time by an average of 1.5 seconds per sample. Our other innovation, a faster variant of JSMA evasion algorithm, empirically performed twice as fast as JSMA and yet maintains a comparable evasion rate. Project source code and data from our experiments available at: https://github.com/QData/AdversarialDNN-Playground △ Less","1 August, 2017",https://arxiv.org/pdf/1708.00807
Reframing Societal Discourse as Requirements Negotiation: Vision Statement,Kurt Schneider;Oliver Karras;Anne Finger;Barbara Zibell,"Challenges in spatial planning include adjusting settlement patterns to increasing or shrinking populations; it also includes organizing food delivery in rural and peripheral environments. Discourse typically starts with an open problem and the search for a holistic and innovative solution. Software will often be needed to implement the innovation. Spatial planning problems are characterized by large and heterogeneous groups of stakeholders, such as municipalities, companies, interest groups, citizens, women and men, young people and children. Current techniques for participation are slow, laborious and costly, and they tend to miss out on many stakeholders or interest groups. We propose a triple shift in perspective: (1) Discourse is reframed as a requirements process with the explicit goal to state software, hardware, and organizational requirements. (2) Due to the above-mentioned characteristics of spatial planning problems, we suggest using techniques of requirements engineering (RE) and CrowdRE for getting stakeholders (e.g. user groups) involved. (3) We propose video as a medium for communicating problems, solution alternatives, and arguments effectively within a mixed crowd of officials, citizens, children and elderly people. Although few spatial planning problems can be solved by software alone, this new perspective helps to focus discussions anyway. RE techniques can assist in finding common ground despite the heterogeneous group of stakeholders, e.g. citizens. Digital requirements and video are well-suited for facilitating distribution, feedback, and discourse via the internet. In this paper, we propose this new perspective as a timely opportunity for the spatial planning domain - and as an increasingly important application domain of CrowdRE. △ Less","1 August, 2017",https://arxiv.org/pdf/1708.00279
Parallel Tracking and Verifying: A Framework for Real-Time and High Accuracy Visual Tracking,Heng Fan;Haibin Ling,"Being intensively studied, visual tracking has seen great recent advances in either speed (e.g., with correlation filters) or accuracy (e.g., with deep features). Real-time and high accuracy tracking algorithms, however, remain scarce. In this paper we study the problem from a new perspective and present a novel parallel tracking and verifying (PTAV) framework, by taking advantage of the ubiquity of multi-thread techniques and borrowing from the success of parallel tracking and mapping in visual SLAM. Our PTAV framework typically consists of two components, a tracker T and a verifier V, working in parallel on two separate threads. The tracker T aims to provide a super real-time tracking inference and is expected to perform well most of the time; by contrast, the verifier V checks the tracking results and corrects T when needed. The key innovation is that, V does not work on every frame but only upon the requests from T; on the other end, T may adjust the tracking according to the feedback from V. With such collaboration, PTAV enjoys both the high efficiency provided by T and the strong discriminative power by V. In our extensive experiments on popular benchmarks including OTB2013, OTB2015, TC128 and UAV20L, PTAV achieves the best tracking accuracy among all real-time trackers, and in fact performs even better than many deep learning based solutions. Moreover, as a general framework, PTAV is very flexible and has great rooms for improvement and generalization. △ Less","1 August, 2017",https://arxiv.org/pdf/1708.00153
Cloud Computing - Architecture and Applications,Jaydip Sen;Shanrong Zhao;Xiaoying Wang;Guojing Zhang;Mengqin Yang;Jian Wang;Yun Long;Sergey Andreev;Roman Florea;Aleksandr Ometov;Adam Surak;Yevgeni Koucheryavy;Muhammad Ahmad Ashraf;Waleed Tariq Sethi;Abdullah Alfakhri;Saleh Alshebeili;Amr Alasaad,"In the era of Internet of Things and with the explosive worldwide growth of electronic data volume, and associated need of processing, analysis, and storage of such humongous volume of data, it has now become mandatory to exploit the power of massively parallel architecture for fast computation. Cloud computing provides a cheap source of such computing framework for large volume of data for real-time applications. It is, therefore, not surprising to see that cloud computing has become a buzzword in the computing fraternity over the last decade. This book presents some critical applications in cloud frameworks along with some innovation design of algorithms and architecture for deployment in cloud environment. It is a valuable source of knowledge for researchers, engineers, practitioners, and graduate and doctoral students working in the field of cloud computing. It will also be useful for faculty members of graduate schools and universities. △ Less","29 July, 2017",https://arxiv.org/pdf/1707.09488
Learning Pixel-Distribution Prior with Wider Convolution for Image Denoising,Peng Liu;Ruogu Fang,"In this work, we explore an innovative strategy for image denoising by using convolutional neural networks (CNN) to learn pixel-distribution from noisy data. By increasing CNN's width with large reception fields and more channels in each layer, CNNs can reveal the ability to learn pixel-distribution, which is a prior existing in many different types of noise. The key to our approach is a discovery that wider CNNs tends to learn the pixel-distribution features, which provides the probability of that inference-mapping primarily relies on the priors instead of deeper CNNs with more stacked nonlinear layers. We evaluate our work: Wide inference Networks (WIN) on additive white Gaussian noise (AWGN) and demonstrate that by learning the pixel-distribution in images, WIN-based network consistently achieves significantly better performance than current state-of-the-art deep CNN-based methods in both quantitative and visual evaluations. \textit{Code and models are available at \url{https://github.com/cswin/WIN}}. △ Less","28 July, 2017",https://arxiv.org/pdf/1707.09135
Network Formation in the Sky: Unmanned Aerial Vehicles for Multi-hop Wireless Backhauling,Ursula Challita;Walid Saad,"To reap the benefits of dense small base station (SBS) deployment, innovative backhaul solutions are needed in order to manage scenarios in which high-speed ground backhaul links are either unavailable or limited in capacity. In this paper, a novel backhaul scheme that utilizes unmanned aerial vehicles (UAVs) as an on-demand flying network linking ground SBSs and the core network is proposed. The design of the aerial backhaul scheme is formulated as a network formation game among UAVs that seek to form a multi-hop backhaul network in the air. To solve this game, a myopic network formation algorithm which reaches a pairwise stable network upon convergence, is introduced. The proposed network formation algorithm enables the UAVs to form the necessary multi-hop backhaul network in a decentralized manner thus adapting the backhaul architecture to the dynamics of the network. Simulation results show that the proposed network formation algorithm achieves substantial performance gains in terms of both rate and delay reaching, respectively, up to 40% and 41% compared to the formation of direct communication links with the gateway node (for a network with 15 UAVs). △ Less","28 July, 2017",https://arxiv.org/pdf/1707.09132
Making the best of data derived from a daily practice in clinical legal medicine for research and practice - the example of Spe3dLab,Vincent Laugier;Eric Stindel;Alcibiade Lichterowicz;Séverine Ansart;Thomas Lefèvre,"Forensic science suffers from a lack of studies with high-quality design, such as randomized controlled trials (RCT). Evidence in forensic science may be of insufficient quality, which is a major concern. Results from RCT are criticized for providing artificial results that are not useful in real life and unfit for individualized prescription. Various sources of collected data (e.g. data collected in routine practice) could be exploited for distinct goals. Obstacles remain before such data can be practically accessed and used, including technical issues. We present an easy-to-use software dedicated to innovative data analyses for practitioners and researchers. We provide 2 examples in forensics. Spe3dLab has been developed by 3 French teams: a bioinformatics laboratory (LaTIM), a private partner (Tekliko) and a department of forensic medicine (Jean Verdier Hospital). It was designed to be open source, relying on documented and maintained libraries, query-oriented and capable of handling the entire data process from capture to export of best predictive models for their integration in information systems. Spe3dLab was used for 2 specific forensics applications: i) the search for multiple causal factors and ii) the best predictive model of the functional impairment (total incapacity to work, TIW) of assault survivors. 2,892 patients were included over a 6-month period. Time to evaluation was the only direct cause identified for TIW, and victim category was an indirect cause. The specificity and sensitivity of the predictive model were 99.9% and 90%, respectively. Spe3dLab is a quick and efficient tool for accessing observational, routinely collected data and performing innovative analyses. Analyses can be exported for validation and routine use by practitioners, e.g., for computer-aided evaluation of complex problems. It can provide a fully integrated solution for individualized medicine. △ Less","26 July, 2017",https://arxiv.org/pdf/1707.08454
Deep Interactive Region Segmentation and Captioning,Ali Sharifi Boroujerdi;Maryam Khanian;Michael Breuss,"With recent innovations in dense image captioning, it is now possible to describe every object of the scene with a caption while objects are determined by bounding boxes. However, interpretation of such an output is not trivial due to the existence of many overlapping bounding boxes. Furthermore, in current captioning frameworks, the user is not able to involve personal preferences to exclude out of interest areas. In this paper, we propose a novel hybrid deep learning architecture for interactive region segmentation and captioning where the user is able to specify an arbitrary region of the image that should be processed. To this end, a dedicated Fully Convolutional Network (FCN) named Lyncean FCN (LFCN) is trained using our special training data to isolate the User Intention Region (UIR) as the output of an efficient segmentation. In parallel, a dense image captioning model is utilized to provide a wide variety of captions for that region. Then, the UIR will be explained with the caption of the best match bounding box. To the best of our knowledge, this is the first work that provides such a comprehensive output. Our experiments show the superiority of the proposed approach over state-of-the-art interactive segmentation methods on several well-known datasets. In addition, replacement of the bounding boxes with the result of the interactive segmentation leads to a better understanding of the dense image captioning output as well as accuracy enhancement for the object detection in terms of Intersection over Union (IoU). △ Less","26 July, 2017",https://arxiv.org/pdf/1707.08364
Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering,Yi Tay;Luu Anh Tuan;Siu Cheung Hui,"The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, \textsc{HyperQA}, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind \textsc{HyperQA} is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks. △ Less","23 November, 2017",https://arxiv.org/pdf/1707.07847
Machine Teaching: A New Paradigm for Building Machine Learning Systems,Patrice Y. Simard;Saleema Amershi;David M. Chickering;Alicia Edelman Pelton;Soroush Ghorashi;Christopher Meek;Gonzalo Ramos;Jina Suh;Johan Verwey;Mo Wang;John Wernsing,"The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. While machine learning focuses on creating new algorithms and improving the accuracy of ""learners"", the machine teaching discipline focuses on the efficacy of the ""teachers"". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models. △ Less","10 August, 2017",https://arxiv.org/pdf/1707.06742
A Customisable Underwater Robot,Guido Schillaci;Fabio Schillaci;Verena V. Hafner,"We present a model of a configurable underwater drone, whose parts are optimised for 3D printing processes. We show how - through the use of printable adapters - several thrusters and ballast configurations can be implemented, allowing different maneuvering possibilities. After introducing the model and illustrating a set of possible configurations, we present a functional prototype based on open source hardware and software solutions. The prototype has been successfully tested in several dives in rivers and lakes around Berlin. The reliability of the printed models has been tested only in relatively shallow waters. However, we strongly believe that their availability as freely downloadable models will motivate the general public to build and to test underwater drones, thus speeding up the development of innovative solutions and applications. The models and their documentation will be available for download at the following link: https://adapt.informatik.hu-berlin.de/schillaci/underwater.html. △ Less","20 July, 2017",https://arxiv.org/pdf/1707.06564
Deliberative Platform Design: The case study of the online discussions in Decidim Barcelona,Pablo Aragón;Andreas Kaltenbrunner;Antonio Calleja-López;Andrés Pereira;Arnau Monterde;Xabier E. Barandiaran;Vicenç Gómez,"With the irruption of ICTs and the crisis of political representation, many online platforms have been developed with the aim of improving participatory democratic processes. However, regarding platforms for online petitioning, previous research has not found examples of how to effectively introduce discussions, a crucial feature to promote deliberation. In this study we focus on the case of Decidim Barcelona, the online participatory-democracy platform launched by the City Council of Barcelona in which proposals can be discussed with an interface that combines threaded discussions and comment alignment with the proposal. This innovative approach allows to examine whether neutral, positive or negative comments are more likely to generate discussion cascades. The results reveal that, with this interface, comments marked as negatively aligned with the proposal were more likely to engage users in online discussions and, therefore, helped to promote deliberative decision making. △ Less","20 July, 2017",https://arxiv.org/pdf/1707.06526
DICE Fault Injection Tool,Craig Sheridan;Darren Whigham;Matej Artač,"In this paper, we describe the motivation, innovation, design, running example and future development of a Fault Inject Tool (FIT). This tool enables controlled causing of cloud platform issues such as resource stress and service or VM outages, the purpose being to observe the subsequent effect on deployed applications. It is being designed for use in a DevOps workflow for tighter correlation between application design and cloud operation, although not limited to this usage, and helps improve resiliency for data intensive applications by bringing together fault tolerance, stress testing and benchmarking in a single tool. △ Less","20 July, 2017",https://arxiv.org/pdf/1707.06420
KAVAGait: Knowledge-Assisted Visual Analytics for Clinical Gait Analysis,Markus Wagner;Djordje Slijepcevic;Brian Horsak;Alexander Rind;Matthias Zeppelzauer;Wolfgang Aigner,"In 2014, more than 10 million people in the US were affected by an ambulatory disability. Thus, gait rehabilitation is a crucial part of health care systems. The quantification of human locomotion enables clinicians to describe and analyze a patient's gait performance in detail and allows them to base clinical decisions on objective data. These assessments generate a vast amount of complex data which need to be interpreted in a short time period. We conducted a design study in cooperation with gait analysis experts to develop a novel Knowledge-Assisted Visual Analytics solution for clinical Gait analysis (KAVAGait). KAVAGait allows the clinician to store and inspect complex data derived during clinical gait analysis. The system incorporates innovative and interactive visual interface concepts, which were developed based on the needs of clinicians. Additionally, an explicit knowledge store (EKS) allows externalization and storage of implicit knowledge from clinicians. It makes this information available for others, supporting the process of data inspection and clinical decision making. We validated our system by conducting expert reviews, a user study, and a case study. Results suggest that KAVAGait is able to support a clinician during clinical practice by visualizing complex gait data and providing knowledge of other clinicians. △ Less","14 December, 2017",https://arxiv.org/pdf/1707.06105
On the State of the Art of Evaluation in Neural Language Models,Gábor Melis;Chris Dyer;Phil Blunsom,"Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset. △ Less","20 November, 2017",https://arxiv.org/pdf/1707.05589
"Unfolding the innovation system for the development of countries: co-evolution of Science, Technology and Production",Emanuele Pugliese;Giulio Cimini;Aurelio Patelli;Andrea Zaccaria;Luciano Pietronero;Andrea Gabrielli,"We show that the space in which scientific, technological and economic developments interplay with each other can be mathematically shaped using pioneering multilayer network and complexity techniques. We build the tri-layered network of human activities (scientific production, patenting, and industrial production) and study the interactions among them, also taking into account the possible time delays. Within this construction we can identify which capabilities and prerequisites are needed to be competitive in a given activity, and even measure how much time is needed to transform, for instance, the technological know-how into economic wealth and scientific innovation, being able to make predictions with a very long time horizon. Quite unexpectedly, we find empirical evidence that the naive knowledge flow from science, to patents, to products is not supported by data, being instead technology the best predictor for industrial and scientific production for the next decades. △ Less","27 December, 2017",https://arxiv.org/pdf/1707.05146
Bayesian Optimization for Probabilistic Programs,Tom Rainforth;Tuan Anh Le;Jan-Willem van de Meent;Michael A. Osborne;Frank Wood,"We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization. △ Less","13 July, 2017",https://arxiv.org/pdf/1707.04314
Mapping spatial and temporal changes of global corporate research and development activities by conducting a bibliometric analysis,Gyorgy Csomos,"Corporate research and development (R&D) activities have long been highly concentrated in a handful of world cities. This is due to the fact that these cities (e.g., Tokyo, New York, London, and Paris) are home to the largest and most powerful transnational corporations and are globally important sites for innovative start-up firms that operate in the fastest growing industries. However, in tandem with the rapid technological changes of our age, corporate R&D activities have shifted towards newly emerging and now globally significant R&D centres, like San Jose, San Francisco, and Boston in the United States, and Beijing, Seoul, and Shenzhen in East Asia. In this paper, I will conduct a bibliometric analysis to define which cities are centres of corporate R&D activities, how different industries influence their performance, and what spatial tendencies characterise the period from 1980 to 2014. The bibliometric analysis is based upon an assumption that implies there is a close connection between the number of scientific articles published by a given firm and the volume of its R&D activity. Results show that firms headquartered in Tokyo, New York, London, and Paris published the largest combined number of scientific articles in the period from 1980 to 2014, but that the growth rate of the annual output of scientific articles was much greater in Boston, San Jose, Beijing, and Seoul, as well as some Taiwanese cities. Furthermore, it can also be seen that those cities that have the largest number of articles; i.e., that can be considered as the most significant sites of corporate R&D in which firms operate in fast-growing industries, are primarily in the pharmaceutical and information technology industries. For these reasons, some mid-sized cities that are home to globally significant pharmaceutical or information technology firms are also top corporate R&D hubs. △ Less","11 July, 2017",https://arxiv.org/pdf/1707.03177
Exploring the position of cities in global corporate research and development: a bibliometric analysis by two different geographical approaches,Gyorgy Csomos;Geza Toth,"Global cities are defined, on the one hand, as the major command and control centres of the world economy and, on the other hand, as the most significant sites of the production of innovation. As command and control centres, they are home to the headquarters of the most powerful MNCs of the global economy, while as sites for the production of innovation they are supposed to be the most important sites of corporate research and development (R&D) activities. In this paper, we conduct a bibliometric analysis of the data located in the Scopus and Forbes 2000 databases to reveal the correlation between the characteristics of the above global city definitions. We explore which cities are the major control points of the global corporate R&D (home city approach), and which cities are the most important sites of corporate R&D activities (host city approach). According to the home city approach we assign articles produced by companies to cities where the decision-making headquarters are located (i.e. to cities that control the companies' R&D activities), while according to the host city approach we assign articles to cities where the R&D activities are actually conducted. Given Sassen's global city concept, we expect global cities to be both the leading home cities and host cities. △ Less","9 July, 2017",https://arxiv.org/pdf/1707.02623
Efficient Context Management and Personalized User Recommendations in a Smart Social TV environment,Fotis Aisopos;Angelos Valsamis;Alexandros Psychas;Andreas Menychtas;Theodora Varvarigou,"With the emergence of Smart TV and related interconnected devices, second screen solutions have rapidly appeared to provide more content for end-users and enrich their TV experience. Given the various data and sources involved - videos, actors, social media and online databases- the aforementioned market poses great challenges concerning user context management and sophisticated recommendations that can be addressed to the end-users. This paper presents an innovative Context Management model and a related first and second screen recommendation service, based on a user-item graph analysis as well as collaborative filtering techniques in the context of a Dynamic Social & Media Content Syndication (SAM) platform. The model evaluation provided is based on datasets collected online, presenting a comparative analysis concerning efficiency and effectiveness of the current approach, and illustrating its added value. △ Less","9 July, 2017",https://arxiv.org/pdf/1707.02546
Adaptive user support in educational environments: A Bayesian Network approach,Adrian Stoica;Nikolaos Tselios;Christos Fidas,"This paper is concerned with the design and implementation of an innovative user support system in the frame of an open educational environment. The environment adapted is ModelsCreator (MC), an educational system supporting learning through modelling activities. The pupils typical interaction with the system was modelled us-ing Bayesian Belief Networks (BBN). This model has been used in ModelsCreator to build an adaptive help system providing the most useful guidelines according to the current state of interaction. A brief description of the system and an overview of application of Bayesian techniques to educational systems is presented together with discussion about the process of building of the Bayesian Network derived from actual student interaction data. A preliminary evaluation of the developed prototype indicates that the proposed approach produces systems with promising performance. △ Less","6 July, 2017",https://arxiv.org/pdf/1707.01895
Employee turnover prediction and retention policies design: a case study,Edouard Ribes;Karim Touahri;Benoît Perthame,"This paper illustrates the similarities between the problems of customer churn and employee turnover. An example of employee turnover prediction model leveraging classical machine learning techniques is developed. Model outputs are then discussed to design \& test employee retention policies. This type of retention discussion is, to our knowledge, innovative and constitutes the main value of this paper. △ Less","5 July, 2017",https://arxiv.org/pdf/1707.01377
From Big Data to Big Displays: High-Performance Visualization at Blue Brain,Stefan Eilemann;Marwan Abdellah;Nicolas Antille;Ahmet Bilgili;Grigory Chevtchenko;Raphael Dumusc;Cyrille Favreau;Juan Hernando;Daniel Nachbaur;Pawel Podhajski;Jafet Villafranca;Felix Schürmann,"Blue Brain has pushed high-performance visualization (HPV) to complement its HPC strategy since its inception in 2007. In 2011, this strategy has been accelerated to develop innovative visualization solutions through increased funding and strategic partnerships with other research institutions. We present the key elements of this HPV ecosystem, which integrates C++ visualization applications with novel collaborative display systems. We motivate how our strategy of transforming visualization engines into services enables a variety of use cases, not only for the integration with high-fidelity displays, but also to build service oriented architectures, to link into web applications and to provide remote services to Python applications. △ Less","30 June, 2017",https://arxiv.org/pdf/1706.10098
Grasp Pose Detection in Point Clouds,Andreas ten Pas;Marcus Gualtieri;Kate Saenko;Robert Platt,"Recently, a number of grasp detection methods have been proposed that can be used to localize robotic grasp configurations directly from sensor data without estimating object pose. The underlying idea is to treat grasp perception analogously to object detection in computer vision. These methods take as input a noisy and partially occluded RGBD image or point cloud and produce as output pose estimates of viable grasps, without assuming a known CAD model of the object. Although these methods generalize grasp knowledge to new objects well, they have not yet been demonstrated to be reliable enough for wide use. Many grasp detection methods achieve grasp success rates (grasp successes as a fraction of the total number of grasp attempts) between 75% and 95% for novel objects presented in isolation or in light clutter. Not only are these success rates too low for practical grasping applications, but the light clutter scenarios that are evaluated often do not reflect the realities of real world grasping. This paper proposes a number of innovations that together result in a significant improvement in grasp detection performance. The specific improvement in performance due to each of our contributions is quantitatively measured either in simulation or on robotic hardware. Ultimately, we report a series of robotic experiments that average a 93% end-to-end grasp success rate for novel objects presented in dense clutter. △ Less","29 June, 2017",https://arxiv.org/pdf/1706.09911
Service adoption spreading in online social networks,Gerardo Iñiguez;Zhongyuan Ruan;Kimmo Kaski;János Kertész;Márton Karsai,"The collective behaviour of people adopting an innovation, product or online service is commonly interpreted as a spreading phenomenon throughout the fabric of society. This process is arguably driven by social influence, social learning and by external effects like media. Observations of such processes date back to the seminal studies by Rogers and Bass, and their mathematical modelling has taken two directions: One paradigm, called simple contagion, identifies adoption spreading with an epidemic process. The other one, named complex contagion, is concerned with behavioural thresholds and successfully explains the emergence of large cascades of adoption resulting in a rapid spreading often seen in empirical data. The observation of real world adoption processes has become easier lately due to the availability of large digital social network and behavioural datasets. This has allowed simultaneous study of network structures and dynamics of online service adoption, shedding light on the mechanisms and external effects that influence the temporal evolution of behavioural or innovation adoption. These advancements have induced the development of more realistic models of social spreading phenomena, which in turn have provided remarkably good predictions of various empirical adoption processes. In this chapter we review recent data-driven studies addressing real-world service adoption processes. Our studies provide the first detailed empirical evidence of a heterogeneous threshold distribution in adoption. We also describe the modelling of such phenomena with formal methods and data-driven simulations. Our objective is to understand the effects of identified social mechanisms on service adoption spreading, and to provide potential new directions and open questions for future research. △ Less","29 June, 2017",https://arxiv.org/pdf/1706.09777
Enabling Prescription-based Health Apps,Venet Osmani;Stefano Forti;Oscar Mayora;Diego Conforti,"We describe an innovative framework for prescription of personalised health apps by integrating Personal Health Records (PHR) with disease-specific mobile applications for managing medical conditions and the communication with clinical professionals. The prescribed apps record multiple variables including medical history enriched with innovative features such as integration with medical monitoring devices and wellbeing trackers to provide patients and clinicians with a personalised support on disease management. Our framework is based on an existing PHR ecosystem called TreC, uniquely positioned between healthcare provider and the patients, which is being used by over 70.000 patients in Trentino region in Northern Italy. We also describe three important aspects of health app prescription and how medical information is automatically encoded through the TreC framework and is prescribed as a personalised app, ready to be installed in the patients' smartphone. △ Less","29 June, 2017",https://arxiv.org/pdf/1706.09407
Modelo de Aprendizaje Biocibernetico BLM,Rommel Salas,"Education in the digital period in which we live, is reaching challenges never before seen, preceded by phenomena that involve not only traditional social units, but also new virtual communities; Innovating is difficult, it is a challenge, however, we must think of new teaching methods that impact the current generation of students, who arrive with new needs and expectations. The construction of knowledge from the subject and the virtual world that surrounds it, establishes the basis for the development of a new model of teaching, where the classroom is the particular representation of a new physical-cybernetic ecosystem composed of the three large dimensions. Which are part of this new techno-social convergence (human - information - machine); Allowing an interrelation between the student, information, machine and the teacher; Using Biocybernetic methods of influence, control and replication, by means of the massive impact vector (i); In addition, the development of new strategies assisted by cybernetics and the updating of academic content according to the new teaching environment. Hence the importance of this study, which leads us to the need for a new model of transforming academic instruction, which is not based on a conglomerate of technological tools, but establishes a new educational and transformative model, based on ""Collaborative Thinking"" and the ubiquity of information, thus establishing the relationship between the subject and object of study, thus allowing us to establish the new Biocybernetic educational paradigm in the digital period. △ Less","27 June, 2017",https://arxiv.org/pdf/1706.09096
Democratizing Design for Future Computing Platforms,Luis Ceze;Mark D. Hill;Karthikeyan Sankaralingam;Thomas F. Wenisch,"Information and communications technology can continue to change our world. These advances will partially depend upon designs that synergistically combine software with specialized hardware. Today open-source software incubates rapid software-only innovation. The government can unleash software-hardware innovation with programs to develop open hardware components, tools, and design flows that simplify and reduce the cost of hardware design. Such programs will speed development for startup companies, established industry leaders, education, scientific research, and for government intelligence and defense platforms. △ Less","26 June, 2017",https://arxiv.org/pdf/1706.08597
Image Processing in Floriculture Using a robotic Mobile Platform,Juan Garcia-Torres;Diana Caro-Prieto,"Colombia has a privileged geographical location which makes it a cornerstone and equidistant point to all regional markets. The country has a great ecological diversity and it is one of the largest suppliers of flowers for US. Colombian flower companies have made innovations in the marketing process, using methods to reach all conditions for final consumers. This article develops a monitoring system for floriculture industries. The system was implemented in a robotic platform. This device has the ability to be programmed in different programming languages. The robot takes the necessary environment information from its camera. The algorithm of the monitoring system was developed with the image processing toolbox on Matlab. The implemented algorithm acquires images through its camera, it performs a preprocessing of the image, noise filter, enhancing of the color and adjusting the dimension in order to increase processing speed. Then, the image is segmented by color and with the binarized version of the image using morphological operations (erosion and dilation), extract relevant features such as centroid, perimeter and area. The data obtained from the image processing helps the robot with the automatic identification of objectives, orientation and move towards them. Also, the results generate a diagnostic quality of each object scanned. △ Less","26 June, 2017",https://arxiv.org/pdf/1706.08436
Integrating self-efficacy into a gamified approach to thwart phishing attacks,Nalin Asanka Gamagedara Arachchilage;Mumtaz Abdul Hameed,"Security exploits can include cyber threats such as computer programs that can disturb the normal behavior of computer systems (viruses), unsolicited e-mail (spam), malicious software (malware), monitoring software (spyware), attempting to make computer resources unavailable to their intended users (Distributed Denial-of-Service or DDoS attack), the social engineering, and online identity theft (phishing). One such cyber threat, which is particularly dangerous to computer users is phishing. Phishing is well known as online identity theft, which targets to steal victims' sensitive information such as username, password and online banking details. This paper focuses on designing an innovative and gamified approach to educate individuals about phishing attacks. The study asks how one can integrate self-efficacy, which has a co-relation with the user's knowledge, into an anti-phishing educational game to thwart phishing attacks? One of the main reasons would appear to be a lack of user knowledge to prevent from phishing attacks. Therefore, this research investigates the elements that influence (in this case, either conceptual or procedural knowledge or their interaction effect) and then integrate them into an anti-phishing educational game to enhance people's phishing prevention behaviour through their motivation. △ Less","23 June, 2017",https://arxiv.org/pdf/1706.07748
Named Entity Recognition with stack residual LSTM and trainable bias decoding,Quan Tran;Andrew MacKinlay;Antonio Jimeno Yepes,"Recurrent Neural Network models are the state-of-the-art for Named Entity Recognition (NER). We present two innovations to improve the performance of these models. The first innovation is the introduction of residual connections between the Stacked Recurrent Neural Network model to address the degradation problem of deep neural networks. The second innovation is a bias decoding mechanism that allows the trained system to adapt to non-differentiable and externally computed objectives, such as the entity-based F-measure. Our work improves the state-of-the-art results for both Spanish and English languages on the standard train/development/test split of the CoNLL 2003 Shared Task NER dataset. △ Less","11 July, 2017",https://arxiv.org/pdf/1706.07598
Dynamic patterns of knowledge flows across technological domains: empirical results and link prediction,Jieun Kim;Christopher L. Magee,"The purpose of this study is to investigate the structure and evolution of knowledge spillovers across technological domains. Specifically, dynamic patterns of knowledge flow among 29 technological domains, measured by patent citations for eight distinct periods, are identified and link prediction is tested for capability for forecasting the evolution in these cross-domain patent networks. The overall success of the predictions using the Katz metric implies that there is a tendency to generate increased knowledge flows mostly within the set of previously linked technological domains. This study contributes to innovation studies by characterizing the structural change and evolutionary behaviors in dynamic technology networks and by offering the basis for predicting the emergence of future technological knowledge flows. △ Less","21 June, 2017",https://arxiv.org/pdf/1706.07140
RSU Cloud and its Resource Management in support of Enhanced Vehicular Applications,Mohammad A. Salahuddin;Ala Al-Fuqaha;Mohsen Guizani;Soumaya Cherkaoui,"We propose Roadside Unit (RSU) Clouds as a novel way to offer non-safety application with QoS for VANETs. The architecture of RSU Clouds is delineated, and consists of traditional RSUs and specialized micro-datacenters and virtual machines (VMs) using Software Defined Networking (SDN). SDN offers the flexibility to migrate or replicate virtual services and reconfigure the data forwarding rules dynamically. However, frequent changes to service hosts and data flows not only result in degradation of services, but are also costly for service providers. In this paper, we use Mininet to analyze and formally quantify the reconfiguration overhead. Our unique RSU Cloud Resource Management (CRM) model jointly minimizes reconfiguration overhead, cost of service deployment and infrastructure routing delay. To the best of our knowledge, we are the first to utilize this approach. We compare the performance of purist approach to our Integer Linear Programming (ILP) model and our innovative heuristic for the CRM technique and discuss the results. We will show the benefits of a holistic approach in Cloud Resource Management with SDN. △ Less","21 June, 2017",https://arxiv.org/pdf/1706.06921
An online sequence-to-sequence model for noisy speech recognition,Chung-Cheng Chiu;Dieterich Lawson;Yuping Luo;George Tucker;Kevin Swersky;Ilya Sutskever;Navdeep Jaitly,"Generative models have long been the dominant approach for speech recognition. The success of these models however relies on the use of sophisticated recipes and complicated machinery that is not easily accessible to non-practitioners. Recent innovations in Deep Learning have given rise to an alternative - discriminative models called Sequence-to-Sequence models, that can almost match the accuracy of state of the art generative models. While these models are easy to train as they can be trained end-to-end in a single step, they have a practical limitation that they can only be used for offline recognition. This is because the models require that the entirety of the input sequence be available at the beginning of inference, an assumption that is not valid for instantaneous speech recognition. To address this problem, online sequence-to-sequence models were recently introduced. These models are able to start producing outputs as data arrives, and the model feels confident enough to output partial transcripts. These models, like sequence-to-sequence are causal - the output produced by the model until any time, t, affects the features that are computed subsequently. This makes the model inherently more powerful than generative models that are unable to change features that are computed from the data. This paper highlights two main contributions - an improvement to online sequence-to-sequence model training, and its application to noisy settings with mixed speech from two speakers. △ Less","16 June, 2017",https://arxiv.org/pdf/1706.06428
Scalable Co-Optimization of Morphology and Control in Embodied Machines,Nick Cheney;Josh Bongard;Vytas SunSpiral;Hod Lipson,"Evolution sculpts both the body plans and nervous systems of agents together over time. In contrast, in AI and robotics, a robot's body plan is usually designed by hand, and control policies are then optimized for that fixed design. The task of simultaneously co-optimizing the morphology and controller of an embodied robot has remained a challenge. In psychology, the theory of embodied cognition posits that behavior arises from a close coupling between body plan and sensorimotor control, which suggests why co-optimizing these two subsystems is so difficult: most evolutionary changes to morphology tend to adversely impact sensorimotor control, leading to an overall decrease in behavioral performance. Here, we further examine this hypothesis and demonstrate a technique for ""morphological innovation protection"", which temporarily reduces selection pressure on recently morphologically-changed individuals, thus enabling evolution some time to ""readapt"" to the new morphology with subsequent control policy mutations. We show the potential for this method to avoid local optima and converge to similar highly fit morphologies across widely varying initial conditions, while sustaining fitness improvements further into optimization. While this technique is admittedly only the first of many steps that must be taken to achieve scalable optimization of embodied machines, we hope that theoretical insight into the cause of evolutionary stagnation in current methods will help to enable the automation of robot design and behavioral training -- while simultaneously providing a testbed to investigate the theory of embodied cognition. △ Less","12 December, 2017",https://arxiv.org/pdf/1706.06133
Accelerating Innovation Through Analogy Mining,Tom Hope;Joel Chan;Aniket Kittur;Dafna Shahaf,"The availability of large idea repositories (e.g., the U.S. patent database) could significantly accelerate innovation and discovery by providing people with inspiration from solutions to analogous problems. However, finding useful analogies in these large, messy, real-world repositories remains a persistent challenge for either human or automated methods. Previous approaches include costly hand-created databases that have high relational structure (e.g., predicate calculus representations) but are very sparse. Simpler machine-learning/information-retrieval similarity metrics can scale to large, natural-language datasets, but struggle to account for structural similarity, which is central to analogy. In this paper we explore the viability and value of learning simpler structural representations, specifically, ""problem schemas"", which specify the purpose of a product and the mechanisms by which it achieves that purpose. Our approach combines crowdsourcing and recurrent neural networks to extract purpose and mechanism vector representations from product descriptions. We demonstrate that these learned vectors allow us to find analogies with higher precision and recall than traditional information-retrieval methods. In an ideation experiment, analogies retrieved by our models significantly increased people's likelihood of generating creative ideas compared to analogies retrieved by traditional methods. Our results suggest a promising approach to enabling computational analogy at scale is to learn and leverage weaker structural representations. △ Less","17 June, 2017",https://arxiv.org/pdf/1706.05585
A Survey on Non-Orthogonal Multiple Access for 5G Networks: Research Challenges and Future Trends,Zhiguo Ding;Xianfu Lei;George K. Karagiannidis;Robert Schober;Jihong Yuan;Vijay Bhargava,"Non-orthogonal multiple access (NOMA) is an essential enabling technology for the fifth generation (5G) wireless networks to meet the heterogeneous demands on low latency, high reliability, massive connectivity, improved fairness, and high throughput. The key idea behind NOMA is to serve multiple users in the same resource block, such as a time slot, subcarrier, or spreading code. The NOMA principle is a general framework, and several recently proposed 5G multiple access schemes can be viewed as special cases. This survey provides an overview of the latest NOMA research and innovations as well as their applications. Thereby, the papers published in this special issue are put into the content of the existing literature. Future research challenges regarding NOMA in 5G and beyond are also discussed. △ Less","16 June, 2017",https://arxiv.org/pdf/1706.05347
Deriving Compact Laws Based on Algebraic Formulation of a Data Set,Wenqing Xu;Mark Stalzer,"In various subjects, there exist compact and consistent relationships between input and output parameters. Discovering the relationships, or namely compact laws, in a data set is of great interest in many fields, such as physics, chemistry, and finance. While data discovery has made great progress in practice thanks to the success of machine learning in recent years, the development of analytical approaches in finding the theory behind the data is relatively slow. In this paper, we develop an innovative approach in discovering compact laws from a data set. By proposing a novel algebraic equation formulation, we convert the problem of deriving meaning from data into formulating a linear algebra model and searching for relationships that fit the data. Rigorous proof is presented in validating the approach. The algebraic formulation allows the search of equation candidates in an explicit mathematical manner. Searching algorithms are also proposed for finding the governing equations with improved efficiency. For a certain type of compact theory, our approach assures convergence and the discovery is computationally efficient and mathematically precise. △ Less","15 June, 2017",https://arxiv.org/pdf/1706.05123
Size invariance sector for an agent-based innovation diffusion model,Carlos E. Laciana;Gustavo Pereyra;Santiago L. Rovere,"It is shown that under certain conditions it is possible to model a complex system in a way that leads to results that do not depend on system size. As an example of complex system an innovation diffusion model is considered. In that model a set of individuals (the agents), which are interconnected, must decide if adopt or not an innovation. The agents are connected in a member of the networks family known as small worlds networks (SWN). It is found that for a subfamily of the SWN the saturation time and the form of the adoption curve are invariants respect to the change in the size of the system. △ Less","27 July, 2017",https://arxiv.org/pdf/1706.03859
Smarter Cities with Parked Cars as Roadside Units,Andre B. Reis;Susana Sargento;Ozan K. Tonguz,"Real-time monitoring of traffic density, road congestion, public transportation, and parking availability are key to realizing the vision of a smarter city and, with the advent of vehicular networking technologies such as IEEE 802.11p and WAVE, this information can now be gathered directly from the vehicles in an urban area. To act as a backbone to the network of moving vehicles, collecting, aggregating, and disseminating their information, the use of parked cars has been proposed as an alternative to costly deployments of fixed Roadside Units. In this paper, we introduce novel mechanisms for parking vehicles to self-organize and form efficient vehicular support networks that provide widespread coverage to a city. These mechanisms are innovative in their ability to keep the network of parked cars under continuous optimization, in their multi-criteria decision process that can be focused on key network performance metrics, and in their ability to manage the battery usage of each car, rotating roadside unit roles between vehicles as required. We also present the first comprehensive study of the performance of such an approach, via realistic modeling of mobility, parking, and communication, thorough simulations, and an experimental verification of concepts that are key to self-organization. Our analysis brings strong evidence that parked cars can serve as an alternative to fixed roadside units, and organize to form networks that can support smarter transportation and mobility. △ Less","7 June, 2017",https://arxiv.org/pdf/1706.02247
Improved User Tracking in 5G Millimeter Wave Mobile Networks via Refinement Operations,Marco Giordani;Michele Zorzi,"The millimeter wave (mmWave) frequencies offer the availability of huge bandwidths to provide unprecedented data rates to next-generation cellular mobile terminals. However, directional mmWave links are highly susceptible to rapid channel variations and suffer from severe isotropic pathloss. To face these impairments, this paper addresses the issue of tracking the channel quality of a moving user, an essential procedure for rate prediction, efficient handover and periodic monitoring and adaptation of the user's transmission configuration. The performance of an innovative tracking scheme, in which periodic refinements of the optimal steering direction are alternated to sparser refresh events, are analyzed in terms of both achievable data rate and energy consumption, and compared to those of a state-of-the-art approach. We aim at understanding in which circumstances the proposed scheme is a valid option to provide a robust and efficient mobility management solution. We show that our procedure is particularly well suited to highly variant and unstable mmWave environments. △ Less","1 June, 2017",https://arxiv.org/pdf/1706.00177
Feature Extraction for Machine Learning Based Crackle Detection in Lung Sounds from a Health Survey,Morten Grønnesby;Juan Carlos Aviles Solis;Einar Holsbø;Hasse Melbye;Lars Ailo Bongo,"In recent years, many innovative solutions for recording and viewing sounds from a stethoscope have become available. However, to fully utilize such devices, there is a need for an automated approach for detecting abnormal lung sounds, which is better than the existing methods that typically have been developed and evaluated using a small and non-diverse dataset. We propose a machine learning based approach for detecting crackles in lung sounds recorded using a stethoscope in a large health survey. Our method is trained and evaluated using 209 files with crackles classified by expert listeners. Our analysis pipeline is based on features extracted from small windows in audio files. We evaluated several feature extraction methods and classifiers. We evaluated the pipeline using a training set of 175 crackle windows and 208 normal windows. We did 100 cycles of cross validation where we shuffled training sets between cycles. For all the division between training and evaluation was 70%-30%. We found and evaluated a 5-dimenstional vector with four features from the time domain and one from the spectrum domain. We evaluated several classifiers and found SVM with a Radial Basis Function Kernel to perform best. Our approach had a precision of 86% and recall of 84% for classifying a crackle in a window, which is more accurate than found in studies of health personnel. The low-dimensional feature vector makes the SVM very fast. The model can be trained on a regular computer in 1.44 seconds, and 319 crackles can be classified in 1.08 seconds. Our approach detects and visualizes individual crackles in recorded audio files. It is accurate, fast, and has low resource requirements. It can be used to train health personnel or as part of a smartphone application for Bluetooth stethoscopes. △ Less","23 December, 2017",https://arxiv.org/pdf/1706.00005
The Impact of Digital Financial Services on Firm's Performance: a Literature Review,Tariq Abbasi;Hans Weigand,"Digital Financial Services continue to expand and replace the delivery of traditional banking services to the customers through innovative technologies to meet the growing complex needs and globalization challenges. These diversified digital products help the organizations (service providers) to improve their firm performance and to remain competitive in the market. It also assists in increasing market share to grow their profitability and improve financial position. There is a growing literature on Digital Financial Services and firm performance. At this point of the development, this paper systemically reviews the existing (within last one decade) amount of literature investigating the impact of DFS on firm performance, analyzes and identifies the research gaps. We identify 39 works that have appeared in a wide range of peer-reviewed scientific journals. We classify the methodologies and approaches that researchers have used to predict the effect of such services on the financial growth and profitability. We observe that despite rapid technological advancement in DFS during the last ten years, Digital Financial Services being the factor affecting firm performance did not get the reasonable attention in academic literature. One of the reason is that almost all the authors limit their research to banking sector while ignoring others particularly mobile network operators (providing branchless banking) and new non-banking entrants. We also notice that newer researchers often ignore past research and investigate the same issues. This study also makes several recommendations and suggest directions for future research in this still emerging field. △ Less","3 May, 2017",https://arxiv.org/pdf/1705.10294
Fully Automatic Segmentation and Objective Assessment of Atrial Scars for Longstanding Persistent Atrial Fibrillation Patients Using Late Gadolinium-Enhanced MRI,Guang Yang;Xiahai Zhuang;Habib Khan;Shouvik Haldar;Eva Nyktari;Lei Li;Rick Wage;Xujiong Ye;Greg Slabaugh;Raad Mohiaddin;Tom Wong;Jennifer Keegan;David Firmin,"Purpose: Atrial fibrillation (AF) is the most common cardiac arrhythmia and is correlated with increased morbidity and mortality. It is associated with atrial fibrosis, which may be assessed non-invasively using late gadolinium-enhanced (LGE) magnetic resonance imaging (MRI) where scar tissue is visualised as a region of signal enhancement. In this study, we proposed a novel fully automatic pipeline to achieve an accurate and objective atrial scarring segmentation and assessment of LGE MRI scans for the AF patients. Methods: Our fully automatic pipeline uniquely combined: (1) a multi-atlas based whole heart segmentation (MA-WHS) to determine the cardiac anatomy from an MRI Roadmap acquisition which is then mapped to LGE MRI, and (2) a super-pixel and supervised learning based approach to delineate the distribution and extent of atrial scarring in LGE MRI. Results: Both our MA-WHS and atrial scarring segmentation showed accurate delineations of cardiac anatomy (mean Dice = 89%) and atrial scarring (mean Dice =79%) respectively compared to the established ground truth from manual segmentation. Compared with previously studied methods with manual interventions, our innovative pipeline demonstrated comparable results, but was computed fully automatically. Conclusion: The proposed segmentation methods allow LGE MRI to be used as an objective assessment tool for localisation, visualisation and quantification of atrial scarring. △ Less","26 May, 2017",https://arxiv.org/pdf/1705.09529
Interactive Levy Flight in Interest Space,Fanqi Zeng;Li Gong;Jing Liu;Jiang Zhang;Qinghua Chen;Ruyue Xin,"Compared to the well-studied topic of human mobility in real geographic space, very few studies focus on human mobility in virtual space, such as interests, knowledge, ideas, and so forth. However, it relates to the issues of management of public opinions, knowledge diffusion, and innovation. In this paper, we assume that the interests of a group of online users can span a Euclidean space which is called interest space, and the transfers of user interests can be modeled as the Levy Flight on the interest space. To consider the interaction between users, we assume that the random walkers are not independent but interact each other indirectly via the digital resources in the interest space. The model can successfully reproduce a set of scaling laws for describing the growth of the attention flow networks of real online communities, and the ranges of the exponents of the scaling are similar with the empirical data. Further, we can infer parameters for describing the individual behaviors of the users according to the scaling laws of the empirical attention flow network. Our model can not only provide theoretical understanding on human online behaviors, but also has wide potential applications, such as dissemination and management of public opinions, online recommendation, etc. △ Less","26 May, 2017",https://arxiv.org/pdf/1705.09462
Mesh Model (MeMo): A Systematic Approach to Agile System Engineering,Amit Kumar Mishra,"Innovation and entrepreneurship have a very special role to play in creating sustainable development in the world. Engineering design plays a major role in innovation. These are not new facts. However this added to the fact that in current time knowledge seem to increase at an exponential rate, growing twice every few months. This creates a need to have newer methods to innovate with very little scope to fall short of the expectations from customers. In terms of reliable designing, system design tools and methodologies have been very helpful and have been in use in most engineering industries for decades now. But traditional system design is rigorous and rigid. As we can see, we need an innovation system that should be rigorous and flexible at the same time. We take our inspiration from biosphere, where some of the most rugged yet flexible plants are creepers which grow to create mesh. In this thematic paper we shall explain our approach to system engineering which we call the MeMo (Mesh Model) that fuses the rigor of system engineering with the flexibility of agile methods to create a scheme that can give rise to reliable innovation in the high risk market of today. △ Less","25 May, 2017",https://arxiv.org/pdf/1705.09170
Non-Linear Phase-Shifting of Haar Wavelets for Run-Time All-Frequency Lighting,Mais Alnasser;Hassan Foroosh,"This paper focuses on real-time all-frequency image-based rendering using an innovative solution for run-time computation of light transport. The approach is based on new results derived for non-linear phase shifting in the Haar wavelet domain. Although image-based methods for real-time rendering of dynamic glossy objects have been proposed, they do not truly scale to all possible frequencies and high sampling rates without trading storage, glossiness, or computational time, while varying both lighting and viewpoint. This is due to the fact that current approaches are limited to precomputed radiance transfer (PRT), which is prohibitively expensive in terms of memory requirements and real-time rendering when both varying light and viewpoint changes are required together with high sampling rates for high frequency lighting of glossy material. On the other hand, current methods cannot handle object rotation, which is one of the paramount issues for all PRT methods using wavelets. This latter problem arises because the precomputed data are defined in a global coordinate system and encoded in the wavelet domain, while the object is rotated in a local coordinate system. At the root of all the above problems is the lack of efficient run-time solution to the nontrivial problem of rotating wavelets (a non-linear phase-shift), which we solve in this paper. △ Less","20 May, 2017",https://arxiv.org/pdf/1705.07272
Local Information with Feedback Perturbation Suffices for Dictionary Learning in Neural Circuits,Tsung-Han Lin,"While the sparse coding principle can successfully model information processing in sensory neural systems, it remains unclear how learning can be accomplished under neural architectural constraints. Feasible learning rules must rely solely on synaptically local information in order to be implemented on spatially distributed neurons. We describe a neural network with spiking neurons that can address the aforementioned fundamental challenge and solve the L1-minimizing dictionary learning problem, representing the first model able to do so. Our major innovation is to introduce feedback synapses to create a pathway to turn the seemingly non-local information into local ones. The resulting network encodes the error signal needed for learning as the change of network steady states caused by feedback, and operates akin to the classical stochastic gradient descent method. △ Less","19 May, 2017",https://arxiv.org/pdf/1705.07149
Deep De-Aliasing for Fast Compressive Sensing MRI,Simiao Yu;Hao Dong;Guang Yang;Greg Slabaugh;Pier Luigi Dragotti;Xujiong Ye;Fangde Liu;Simon Arridge;Jennifer Keegan;David Firmin;Yike Guo,"Fast Magnetic Resonance Imaging (MRI) is highly in demand for many clinical applications in order to reduce the scanning cost and improve the patient experience. This can also potentially increase the image quality by reducing the motion artefacts and contrast washout. However, once an image field of view and the desired resolution are chosen, the minimum scanning time is normally determined by the requirement of acquiring sufficient raw data to meet the Nyquist-Shannon sampling criteria. Compressive Sensing (CS) theory has been perfectly matched to the MRI scanning sequence design with much less required raw data for the image reconstruction. Inspired by recent advances in deep learning for solving various inverse problems, we propose a conditional Generative Adversarial Networks-based deep learning framework for de-aliasing and reconstructing MRI images from highly undersampled data with great promise to accelerate the data acquisition process. By coupling an innovative content loss with the adversarial loss our de-aliasing results are more realistic. Furthermore, we propose a refinement learning procedure for training the generator network, which can stabilise the training with fast convergence and less parameter tuning. We demonstrate that the proposed framework outperforms state-of-the-art CS-MRI methods, in terms of reconstruction error and perceptual image quality. In addition, our method can reconstruct each image in 0.22ms--0.37ms, which is promising for real-time applications. △ Less","19 May, 2017",https://arxiv.org/pdf/1705.07137
Detect Kernel-Mode Rootkits via Real Time Logging & Controlling Memory Access,Igor Korkin;Satoshi Tanda,"Modern malware and spyware platforms attack existing antivirus solutions and even Microsoft PatchGuard. To protect users and business systems new technologies developed by Intel and AMD CPUs may be applied. To deal with the new malware we propose monitoring and controlling access to the memory in real time using Intel VT-x with EPT. We have checked this concept by developing MemoryMonRWX, which is a bare-metal hypervisor. MemoryMonRWX is able to track and trap all types of memory access: read, write, and execute. MemoryMonRWX also has the following competitive advantages: fine-grained analysis, support of multi-core CPUs and 64-bit Windows 10. MemoryMonRWX is able to protect critical kernel memory areas even when PatchGuard has been disabled by malware. Its main innovative features are as follows: guaranteed interception of every memory access, resilience, and low performance degradation. △ Less","18 May, 2017",https://arxiv.org/pdf/1705.06784
Rise of the humanbot,Ricard Sole,"The accelerated path of technological development, particularly at the interface between hardware and biology has been suggested as evidence for future major technological breakthroughs associated to our potential to overcome biological constraints. This includes the potential of becoming immortal, having expanded cognitive capacities thanks to hardware implants or the creation of intelligent machines. Here I argue that several relevant evolutionary and structural constraints might prevent achieving most (if not all) these innovations. Instead, the coming future will bring novelties that will challenge many other aspects of our life and that can be seen as other feasible singularities. One particularly important one has to do with the evolving interactions between humans and non-intelligent robots capable of learning and communication. Here I argue that a long term interaction can lead to a new class of ""agent"" (the humanbot). The way shared memories get tangled over time will inevitably have important consequences for both sides of the pair, whose identity as separated entities might become blurred and ultimately vanish. Understanding such hybrid systems requires a second-order neuroscience approach while posing serious conceptual challenges, including the definition of consciousness. △ Less","16 May, 2017",https://arxiv.org/pdf/1705.05935
Community structure of copper supply networks in the prehistoric Balkans: An independent evaluation of the archaeological record from the 7th to the 4th millennium BC,Miljana Radivojevic;Jelena Grujic,"Complex networks analyses of many physical, biological and social phenomena show remarkable structural regularities, yet, their application in studying human past interaction remains underdeveloped. Here, we present an innovative method for identifying community structures in the archaeological record that allow for independent evaluation of the copper using societies in the Balkans, from c. 6200 to c. 3200 BC. We achieve this by exploring modularity of networked systems of these societies across an estimated 3000 years. We employ chemical data of copper-based objects from 79 archaeological sites as the independent variable for detecting most densely interconnected sets of nodes with a modularity maximization method. Our results reveal three dominant modular structures across the entire period, which exhibit strong spatial and temporal significance. We interpret patterns of copper supply among prehistoric societies as reflective of social relations, which emerge as equally important as physical proximity. Although designed on a variable isolated from any archaeological and spatiotemporal information, our method provides archaeologically and spatiotemporally meaningful results. It produces models of human interaction and cooperation that can be evaluated independently of established archaeological systematics, and can find wide application on any quantitative data from archaeological and historical record. △ Less","28 May, 2017",https://arxiv.org/pdf/1705.05406
Design Criteria to Architect Continuous Experimentation for Self-Driving Vehicles,Federico Giaimo;Christian Berger,"The software powering today's vehicles surpasses mechatronics as the dominating engineering challenge due to its fast evolving and innovative nature. In addition, the software and system architecture for upcoming vehicles with automated driving functionality is already processing ~750MB/s - corresponding to over 180 simultaneous 4K-video streams from popular video-on-demand services. Hence, self-driving cars will run so much software to resemble ""small data centers on wheels"" rather than just transportation vehicles. Continuous Integration, Deployment, and Experimentation have been successfully adopted for software-only products as enabling methodology for feedback-based software development. For example, a popular search engine conducts ~250 experiments each day to improve the software based on its users' behavior. This work investigates design criteria for the software architecture and the corresponding software development and deployment process for complex cyber-physical systems, with the goal of enabling Continuous Experimentation as a way to achieve continuous software evolution. Our research involved reviewing related literature on the topic to extract relevant design requirements. The study is concluded by describing the software development and deployment process and software architecture adopted by our self-driving vehicle laboratory, both based on the extracted criteria. △ Less","12 June, 2017",https://arxiv.org/pdf/1705.05170
Piggybacking on an Autonomous Hauler: Business Models Enabling a System-of-Systems Approach to Mapping an Underground Mine,Markus Borg;Thomas Olsson;John Svensson,"With ever-increasing productivity targets in mining operations, there is a growing interest in mining automation. In future mines, remote-controlled and autonomous haulers will operate underground guided by LiDAR sensors. We envision reusing LiDAR measurements to maintain accurate mine maps that would contribute to both safety and productivity. Extrapolating from a pilot project on reliable wireless communication in Boliden's Kankberg mine, we propose establishing a system-of-systems (SoS) with LIDAR-equipped haulers and existing mapping solutions as constituent systems. SoS requirements engineering inevitably adds a political layer, as independent actors are stakeholders both on the system and SoS levels. We present four SoS scenarios representing different business models, discussing how development and operations could be distributed among Boliden and external stakeholders, e.g., the vehicle suppliers, the hauling company, and the developers of the mapping software. Based on eight key variation points, we compare the four scenarios from both technical and business perspectives. Finally, we validate our findings in a seminar with participants from the relevant stakeholders. We conclude that to determine which scenario is the most promising for Boliden, trade-offs regarding control, costs, risks, and innovation must be carefully evaluated. △ Less","15 May, 2017",https://arxiv.org/pdf/1705.05087
Flexible and Creative Chinese Poetry Generation Using Neural Memory,Jiyuan Zhang;Yang Feng;Dong Wang;Yang Wang;Andrew Abel;Shiyue Zhang;Andi Zhang,"It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles. △ Less","10 May, 2017",https://arxiv.org/pdf/1705.03773
Accurate ranking of influential spreaders in networks based on dynamically asymmetric link-impact,Ying Liu;Ming Tang;Younghae Do;Pak Ming Hui,"We propose an efficient and accurate measure for ranking spreaders and identifying the influential ones in spreading processes in networks. While the edges determine the connections among the nodes, their specific role in spreading should be considered explicitly. An edge connecting nodes i and j may differ in its importance for spreading from i to j and from j to i. The key issue is whether node j, after infected by i through the edge, would reach out to other nodes that i itself could not reach directly. It becomes necessary to invoke two unequal weights wij and wji characterizing the importance of an edge according to the neighborhoods of nodes i and j. The total asymmetric directional weights originating from a node leads to a novel measure si which quantifies the impact of the node in spreading processes. A s-shell decomposition scheme further assigns a s-shell index or weighted coreness to the nodes. The effectiveness and accuracy of rankings based on si and the weighted coreness are demonstrated by applying them to nine real-world networks. Results show that they generally outperform rankings based on the nodes' degree and k-shell index, while maintaining a low computational complexity. Our work represents a crucial step towards understanding and controlling the spread of diseases, rumors, information, trends, and innovations in networks. △ Less","10 May, 2017",https://arxiv.org/pdf/1705.03668
OMNIRank: Risk Quantification for P2P Platforms with Deep Learning,Honglun Zhang;Haiyang Wang;Xiaming Chen;Yongkun Wang;Yaohui Jin,"P2P lending presents as an innovative and flexible alternative for conventional lending institutions like banks, where lenders and borrowers directly make transactions and benefit each other without complicated verifications. However, due to lack of specialized laws, delegated monitoring and effective managements, P2P platforms may spawn potential risks, such as withdraw failures, investigation involvements and even runaway bosses, which cause great losses to lenders and are especially serious and notorious in China. Although there are abundant public information and data available on the Internet related to P2P platforms, challenges of multi-sourcing and heterogeneity matter. In this paper, we promote a novel deep learning model, OMNIRank, which comprehends multi-dimensional features of P2P platforms for risk quantification and produces scores for ranking. We first construct a large-scale flexible crawling framework and obtain great amounts of multi-source heterogeneous data of domestic P2P platforms since 2007 from the Internet. Purifications like duplication and noise removal, null handing, format unification and fusion are applied to improve data qualities. Then we extract deep features of P2P platforms via text comprehension, topic modeling, knowledge graph and sentiment analysis, which are delivered as inputs to OMNIRank, a deep learning model for risk quantification of P2P platforms. Finally, according to rankings generated by OMNIRank, we conduct flourish data visualizations and interactions, providing lenders with comprehensive information supports, decision suggestions and safety guarantees. △ Less","26 April, 2017",https://arxiv.org/pdf/1705.03497
Diving Performance Assessment by means of Video Processing,Stefano Frassinelli;Alessandro Niccolai;Riccardo E. Zich,The aim of this paper is to present a procedure for video analysis applied in an innovative way to diving performance assessment. Sport performance analysis is a trend that is growing exponentially for all level athletes. The technique here shown is based on two important requirements: flexibility and low cost. These two requirements lead to many problems in the video processing that have been faced and solved in this paper. △ Less,"9 May, 2017",https://arxiv.org/pdf/1705.03255
Citation sentence reuse behavior of scientists: A case study on massive bibliographic text dataset of computer science,Mayank Singh;Abhishek Niranjan;Divyansh Gupta;Nikhil Angad Bakshi;Animesh Mukherjee;Pawan Goyal,"Our current knowledge of scholarly plagiarism is largely based on the similarity between full text research articles. In this paper, we propose an innovative and novel conceptualization of scholarly plagiarism in the form of reuse of explicit citation sentences in scientific research articles. Note that while full-text plagiarism is an indicator of a gross-level behavior, copying of citation sentences is a more nuanced micro-scale phenomenon observed even for well-known researchers. The current work poses several interesting questions and attempts to answer them by empirically investigating a large bibliographic text dataset from computer science containing millions of lines of citation sentences. In particular, we report evidences of massive copying behavior. We also present several striking real examples throughout the paper to showcase widespread adoption of this undesirable practice. In contrast to the popular perception, we find that copying tendency increases as an author matures. The copying behavior is reported to exist in all fields of computer science; however, the theoretical fields indicate more copying than the applied fields. △ Less","6 May, 2017",https://arxiv.org/pdf/1705.02499
Interface and Data Biopolitics in the Age of Hyperconnectivity,Salvatore Iaconesi,"This article describes their biopolitical implications for design from psychological, cultural, legal, functional and aesthetic/perceptive ways, in the framework of Hyperconnectivity: the condition according to which person-to-person, person-to-machine and machine-to-machine communication progressively shift to networked and digital means. A definition is given for the terms of ""interface biopolitics"" and ""data biopolitics"", as well as evidence supporting these definitions and a description of the technological, theoretical and practice-based innovations bringing them into meaningful existence. Interfaces, algorithms, artificial intelligences of various types, the tendency in quantified self and the concept of ""information bubbles"" will be examined in terms of interface and data biopolitics, from the point of view of design, and for their implications in terms of freedoms, transparency, justice and accessibility to human rights. A working hypothesis is described for technologically relevant design practices and education processes, in order to confront with these issues in critical, ethical and inclusive ways. △ Less","6 May, 2017",https://arxiv.org/pdf/1705.02449
A Rural Lens on a Research Agenda for Intelligent Infrastructure,Ellen Zegura;Beki Grinter;Elizabeth Belding;Klara Nahrstedt,"A National Agenda for Intelligent Infrastructure is not complete without explicit consideration of the needs of rural communities. While the American population has urbanized, the United States depends on rural communities for agriculture, fishing, forestry, manufacturing and mining. Approximately 20% of the US population lives in rural areas with a skew towards aging adults. Further, nearly 25% of Veterans live in rural America. And yet, when intelligent infrastructure is imagined, it is often done so with implicit or explicit bias towards cities. In this brief we describe the unique opportunities for rural communities and offer an inclusive vision of intelligent infrastructure research. In this paper, we argue for a set of coordinated actions to ensure that rural Americans are not left behind in this digital revolution. These technological platforms and applications, supported by appropriate policy, will address key issues in transportation, energy, agriculture, public safety and health. We believe that rather than being a set of needs, the rural United States presents a set of exciting possibilities for novel innovation benefiting not just those living there, but the American economy more broadly △ Less","4 May, 2017",https://arxiv.org/pdf/1705.02004
"Intelligent Infrastructure for Smart Agriculture: An Integrated Food, Energy and Water System",Shashi Shekhar;Joe Colletti;Francisco Muñoz-Arriola;Lakshmish Ramaswamy;Chandra Krintz;Lav Varshney;Debra Richardson,"Agriculture provides economic opportunity through innovation; helps rural America to thrive; promotes agricultural production that better nourishes Americans; and aims to preserve natural resources through healthy private working lands, conservation, improved watersheds, and restored forests. From agricultural production to food supply, agriculture supports rural and urban economies across the U.S. It accounts for 10% of U.S. jobs and is currently creating new jobs in the growing field of data-driven farming. However, U.S. global competitiveness associated with food and nutrition security is at risk because of accelerated investments by many other countries in agriculture, food, energy, and resource management. To ensure U.S. global competitiveness and long-term food security, it is imperative that we build sustainable physical and cyber infrastructures to enable self-managing and sustainable farming. Such infrastructures should enable next generation precision-farms by harnessing modern and emerging technologies such as small satellites, broadband Internet, tele-operation, augmented reality, advanced data analytics, sensors, and robotics. △ Less","4 May, 2017",https://arxiv.org/pdf/1705.01993
Digital Grid: Transforming the Electric Power Grid into an Innovation Engine for the United States,Aranya Chakrabortty;Alex Huang,"The electric power grid is one of the largest and most complex infrastructures ever built by mankind. Modern civilization depends on it for industry production, human mobility, and comfortable living. However, many critical technologies such as the 60 Hz transformers were developed at the beginning of the 20th century and have changed very little since then.1 The traditional unidirectional power from the generation to the customer through the transmission-distribution grid has also changed nominally, but it no longer meets the need of the 21st century market energy customers. On one hand, 128m US residential customers pay $15B/per month for their utility bill, yet they have no option to select their energy supplier. In a world of where many traditional industries are transformed by digital Internet technology (Amazon, Ebay, Uber, Airbnb), the traditional electric energy market is lagging significantly behind. A move towards a true digital grid is needed. Such a digital grid requires a tight integration of the physical layer (energy and power) with digital and cyber information to allow an open and real time market akin to the world of e-commerce. Another major factor that is pushing for this radical transformation are the rapidly changing patterns in energy resources ownership and load flow. Driven by the decreasing cost in distributed solar, energy storage, electric vehicle, on site generation and microgrids, the high penetration of Distributed Energy Resource (DER) is shifting challenges substantially towards the edge of grid from the control point of view. The envisioned Digital Grid must facilitate the open competition and open innovation needed to accelerate of the adoption of new DER technologies while satisfying challenges in grid stability, data explosion and cyber security. △ Less","4 May, 2017",https://arxiv.org/pdf/1705.01925
Tramp Ship Scheduling Problem with Berth Allocation Considerations and Time-dependent Constraints,Francisco López-Ramos;Armando Guarnaschelli;José-Fernando Camacho-Vallejo;Laura Hervert-Escobar;Rosa G. González-Ramírez,"This work presents a model for the Tramp Ship Scheduling problem including berth allocation considerations, motivated by a real case of a shipping company. The aim is to determine the travel schedule for each vessel considering multiple docking and multiple time windows at the berths. This work is innovative due to the consideration of both spatial and temporal attributes during the scheduling process. The resulting model is formulated as a mixed-integer linear programming problem, and a heuristic method to deal with multiple vessel schedules is also presented. Numerical experimentation is performed to highlight the benefits of the proposed approach and the applicability of the heuristic. Conclusions and recommendations for further research are provided. △ Less","3 May, 2017",https://arxiv.org/pdf/1705.01681
Towards effective research recommender systems for repositories,Petr Knoth;Lucas Anastasiou;Aristotelis Charalampous;Matteo Cancellieri;Samuel Pearce;Nancy Pontika;Vaclav Bayer,"In this paper, we argue why and how the integration of recommender systems for research can enhance the functionality and user experience in repositories. We present the latest technical innovations in the CORE Recommender, which provides research article recommendations across the global network of repositories and journals. The CORE Recommender has been recently redeveloped and released into production in the CORE system and has also been deployed in several third-party repositories. We explain the design choices of this unique system and the evaluation processes we have in place to continue raising the quality of the provided recommendations. By drawing on our experience, we discuss the main challenges in offering a state-of-the-art recommender solution for repositories. We highlight two of the key limitations of the current repository infrastructure with respect to developing research recommender systems: 1) the lack of a standardised protocol and capabilities for exposing anonymised user-interaction logs, which represent critically important input data for recommender systems based on collaborative filtering and 2) the lack of a voluntary global sign-on capability in repositories, which would enable the creation of personalised recommendation and notification solutions based on past user interactions. △ Less","1 May, 2017",https://arxiv.org/pdf/1705.00578
Lifelong Learning CRF for Supervised Aspect Extraction,Lei Shu;Hu Xu;Bing Liu,"This paper makes a focused contribution to supervised aspect extraction. It shows that if the system has performed aspect extraction from many past domains and retained their results as knowledge, Conditional Random Fields (CRF) can leverage this knowledge in a lifelong learning manner to extract in a new domain markedly better than the traditional CRF without using this prior knowledge. The key innovation is that even after CRF training, the model can still improve its extraction with experiences in its applications. △ Less","29 April, 2017",https://arxiv.org/pdf/1705.00251
Charting the Complexity Landscape of Waypoint Routing,Saeed Akhoondian Amiri;Klaus-Tycho Foerster;Riko Jacob;Stefan Schmid,"Modern computer networks support interesting new routing models in which traffic flows from a source s to a destination t can be flexibly steered through a sequence of waypoints, such as (hardware) middleboxes or (virtualized) network functions, to create innovative network services like service chains or segment routing. While the benefits and technological challenges of providing such routing models have been articulated and studied intensively over the last years, much less is known about the underlying algorithmic traffic routing problems. This paper shows that the waypoint routing problem features a deep combinatorial structure, and we establish interesting connections to several classic graph theoretical problems. We find that the difficulty of the waypoint routing problem depends on the specific setting, and chart a comprehensive landscape of the computational complexity. In particular, we derive several NP-hardness results, but we also demonstrate that exact polynomial-time algorithms exist for a wide range of practically relevant scenarios. △ Less","31 August, 2017",https://arxiv.org/pdf/1705.00055
Kiwi - A Minimalist CP Solver,Renaud Hartert,"Kiwi is a minimalist and extendable Constraint Programming (CP) solver specifically designed for education. The particularities of Kiwi stand in its generic trailing state restoration mechanism and its modulable use of variables. By developing Kiwi, the author does not aim to provide an alternative to full featured constraint solvers but rather to provide readers with a basic architecture that will (hopefully) help them to understand the core mechanisms hidden under the hood of constraint solvers, to develop their own extended constraint solver, or to test innovative ideas. △ Less","1 May, 2017",https://arxiv.org/pdf/1705.00047
Dissecting Robotics - historical overview and future perspectives,Irati Zamalloa;Risto Kojcev;Alejandro Hernández;Iñigo Muguruza;Lander Usategui;Asier Bilbao;Víctor Mayoral,"Robotics is called to be the next technological revolution and estimations indicate that it will trigger the fourth industrial revolution. This article presents a review of some of the most relevant milestones that occurred in robotics over the last few decades and future perspectives. Despite the fact that, nowadays, robotics is an emerging field, the challenges in many technological aspects and more importantly bringing innovative solutions to the market still remain open. The need of reducing the integration time, costs and a common hardware infrastructure are discussed and further analysed in this work. We conclude with a discussion of the future perspectives of robotics as an engineering discipline and with suggestions for future research directions. △ Less","27 April, 2017",https://arxiv.org/pdf/1704.08617
Z3str3: A String Solver with Theory-aware Branching,Murphy Berzish;Yunhui Zheng;Vijay Ganesh,"We present a new string SMT solver, Z3str3, that is faster than its competitors Z3str2, Norn, CVC4, S3, and S3P over a majority of three industrial-strength benchmarks, namely Kaluza, PISA, and IBM AppScan. Z3str3 supports string equations, linear arithmetic over length function, and regular language membership predicate. The key algorithmic innovation behind the efficiency of Z3str3 is a technique we call theory-aware branching, wherein we modify Z3's branching heuristic to take into account the structure of theory literals to compute branching activities. In the traditional DPLL(T) architecture, the structure of theory literals is hidden from the DPLL(T) SAT solver because of the Boolean abstraction constructed over the input theory formula. By contrast, the theory-aware technique presented in this paper exposes the structure of theory literals to the DPLL(T) SAT solver's branching heuristic, thus enabling it to make much smarter decisions during its search than otherwise. As a consequence, Z3str3 has better performance than its competitors. △ Less","25 April, 2017",https://arxiv.org/pdf/1704.07935
Monte Carlo Tree Search with Sampled Information Relaxation Dual Bounds,Daniel R. Jiang;Lina Al-Kanj;Warren B. Powell,"Monte Carlo Tree Search (MCTS), most famously used in game-play artificial intelligence (e.g., the game of Go), is a well-known strategy for constructing approximate solutions to sequential decision problems. Its primary innovation is the use of a heuristic, known as a default policy, to obtain Monte Carlo estimates of downstream values for states in a decision tree. This information is used to iteratively expand the tree towards regions of states and actions that an optimal policy might visit. However, to guarantee convergence to the optimal action, MCTS requires the entire tree to be expanded asymptotically. In this paper, we propose a new technique called Primal-Dual MCTS that utilizes sampled information relaxation upper bounds on potential actions, creating the possibility of ""ignoring"" parts of the tree that stem from highly suboptimal choices. This allows us to prove that despite converging to a partial decision tree in the limit, the recommended action from Primal-Dual MCTS is optimal. The new approach shows significant promise when used to optimize the behavior of a single driver navigating a graph while operating on a ride-sharing platform. Numerical experiments on a real dataset of 7,000 trips in New Jersey suggest that Primal-Dual MCTS improves upon standard MCTS by producing deeper decision trees and exhibits a reduced sensitivity to the size of the action space. △ Less","19 April, 2017",https://arxiv.org/pdf/1704.05963
Application of Econometric Data Analysis Methods to Physics Software,Maria Grazia Pia;Elisabetta Ronchieri,"We report an investigation of data analysis methods derived from other disciplines, which we applied to physics software systems. They concern the analysis of inequality, trend analysis and the analysis of diversity. The analysis of inequality exploits statistical methods originating from econometrics; trend analysis is typical of economics and environmental sciences; the analysis of diversity is based on concepts derived from ecology and treats software as an ecosystem. To the best of our knowledge, this is an innovative exploration, as we could not find track of previous use of these methods in the experimental physics domains within the scope of the IEEE Nuclear Science Symposium. We applied these methods in the context of Geant4 physics validation and Geant4 maintainability assessment. △ Less","19 April, 2017",https://arxiv.org/pdf/1704.05920
"Gender Disparities in Science? Dropout, Productivity, Collaborations and Success of Male and Female Computer Scientists",Mohsen Jadidi;Fariba Karimi;Haiko Lietz;Claudia Wagner,"Scientific collaborations shape ideas as well as innovations and are both the substrate for, and the outcome of, academic careers. Recent studies show that gender inequality is still present in many scientific practices ranging from hiring to peer-review processes and grant applications. In this work, we investigate gender-specific differences in collaboration patterns of more than one million computer scientists over the course of 47 years. We explore how these patterns change over years and career ages and how they impact scientific success. Our results highlight that successful male and female scientists reveal the same collaboration patterns: compared to scientists in the same career age, they tend to collaborate with more colleagues than other scientists, seek innovations as brokers and establish longer-lasting and more repetitive collaborations. However, women are on average less likely to adapt the collaboration patterns that are related with success, more likely to embed into ego networks devoid of structural holes, and they exhibit stronger gender homophily as well as a consistently higher dropout rate than men in all career ages. △ Less","9 August, 2017",https://arxiv.org/pdf/1704.05801
"A Century of Science: Globalization of Scientific Collaborations, Citations, and Innovations",Yuxiao Dong;Hao Ma;Zhihong Shen;Kuansan Wang,"Progress in science has advanced the development of human society across history, with dramatic revolutions shaped by information theory, genetic cloning, and artificial intelligence, among the many scientific achievements produced in the 20th century. However, the way that science advances itself is much less well-understood. In this work, we study the evolution of scientific development over the past century by presenting an anatomy of 89 million digitalized papers published between 1900 and 2015. We find that science has benefited from the shift from individual work to collaborative effort, with over 90% of the world-leading innovations generated by collaborations in this century, nearly four times higher than they were in the 1900s. We discover that rather than the frequent myopic- and self-referencing that was common in the early 20th century, modern scientists instead tend to look for literature further back and farther around. Finally, we also observe the globalization of scientific development from 1900 to 2015, including 25-fold and 7-fold increases in international collaborations and citations, respectively, as well as a dramatic decline in the dominant accumulation of citations by the US, the UK, and Germany, from ~95% to ~50% over the same period. Our discoveries are meant to serve as a starter for exploring the visionary ways in which science has developed throughout the past century, generating insight into and an impact upon the current scientific innovations and funding policies. △ Less","4 October, 2017",https://arxiv.org/pdf/1704.05150
"A Comprehensive Review of Smart Wheelchairs: Past, Present and Future",Jesse Leaman;Hung M. La,"A smart wheelchair (SW) is a power wheelchair (PW) to which computers, sensors, and assistive technology are attached. In the past decade, there has been little effort to provide a systematic review of SW research. This paper aims to provide a complete state-of-the-art overview of SW research trends. We expect that the information gathered in this study will enhance awareness of the status of contemporary PW as well as SW technology, and increase the functional mobility of people who use PWs. We systematically present the international SW research effort, starting with an introduction to power wheelchairs and the communities they serve. Then we discuss in detail the SW and associated technological innovations with an emphasis on the most researched areas, generating the most interest for future research and development. We conclude with our vision for the future of SW research and how to best serve people with all types of disabilities. △ Less","18 May, 2017",https://arxiv.org/pdf/1704.04697
A Conceptual Model for the Organisational Adoption of Information System Security Innovations,Mumtaz Abdul Hameed;Nalin Asanka Gamagedara Arachchilage,"Information System (IS) Security threats is still a major concern for many organisations. However, most organisations fall short in achieving a successful adoption and implementation of IS security measures. In this paper, we developed a theoretical model for the adoption process of IS Security innovations in organisations. The model was derived by combining four theoretical models of innovation adoption, namely: Diffusion of Innovation theory (DOI), the Technology Acceptance Model (TAM), the Theory of Planned Behaviour (TPB) and the Technology-Organisation-Environment (TOE) framework. The model depicts IS security innovation adoption in organisations, as two decision proceedings. The adoption process from the initiation stage until the acquisition of innovation is considered as a decision made by organisation while the process of innovation assimilation is assumed as a result of the user acceptance of innovation within the organisation. In addition, the model describes the IS Security adoption process progressing in three sequential stages, i.e. pre-adoption, adoption- decision and post-adoption phases. The model also introduces several factors that influence the different stages of IS Security innovation adoption process. This study contributes to IS security literature by proposing an overall model of IS security adoption that includes organisational adoption and user acceptance of innovation in a single illustration. Also, IS security adoption model proposed in this study provides important practical implications for research and practice. △ Less","4 May, 2017",https://arxiv.org/pdf/1704.03867
The scientific influence of nations on global scientific and technological development,Aurelio Patelli;Giulio Cimini;Emanuele Pugliese;Andrea Gabrielli,"Determining how scientific achievements influence the subsequent process of knowledge creation is a fundamental step in order to build a unified ecosystem for studying the dynamics of innovation and competitiveness. Relying separately on data about scientific production on one side, through bibliometric indicators, and about technological advancements on the other side, through patents statistics, gives only a limited insight on the key interplay between science and technology which, as a matter of fact, move forward together within the innovation space. In this paper, using citation data of both research papers and patents, we quantify the direct influence of the scientific outputs of nations on further advancements in science and on the introduction of new technologies. Our analysis highlights the presence of geo-cultural clusters of nations with similar innovation system features, and unveils the heterogeneous coupled dynamics of scientific and technological advancements. This study represents a step forward in the buildup of an inclusive framework for knowledge creation and innovation. △ Less","30 October, 2017",https://arxiv.org/pdf/1704.03768
Modelling collaborative services: The COSEMO model,Thanh Thoa Pham Thi;Thang Le Dinh;Markus Helfert;Michel Leonard,"Despite the dominance of the service sector in the last decades, there is still a need for a strong foundation on service design and innovation. Little attention has paid on service modelling, particularly in the collaboration context. Collaboration is considered as one of solutions for surviving or sustaining the business in the high competitive atmosphere. Collaborative services require various service providers working together according to agreements between them, along with service consumers, in order to co-produce services. In this paper, we address crucial issues in collaborative services such as collaboration levels, sharing data and processes due to business inter-dependencies between service stakeholders. Afterward, we propose a model for Collaborative Service Modelling, which is able to cover identified issues. We also apply our proposed model to modelling an example of healthcare services in order to illustrate the relevance of our modelling approach to the matter in hand. △ Less","11 April, 2017",https://arxiv.org/pdf/1704.03740
Societal impacts of big data: challenges and opportunities in Europe,Martí Cuquet;Guillermo Vega-Gorgojo;Hans Lammerant;Rachel Finn;Umair ul Hassan,"This paper presents the risks and opportunities of big data and the potential social benefits it can bring. The research is based on an analysis of the societal impacts observed in a set of six case studies across different European sectors. These impacts are divided into economic, social and ethical, legal and political impacts, and affect areas such as improved efficiency, innovation and decision making, changing business models, dependency on public funding, participation, equality, discrimination and trust, data protection and intellectual property rights, private and public tensions and losing control to actors abroad. A special focus is given to the risks and opportunities coming from the legal framework and how to counter the negative impacts of big data. Recommendations are presented for four specific legal frameworks: copyright and database protection, protection of trade secrets, privacy and data protection and anti-discrimination. In addition, the potential social benefits of big data are exemplified in six domains: improved decision making and event detection; data-driven innovations and new business models; direct social, environmental and other citizen benefits; citizen participation, transparency and public trust; privacy-aware data practices; and big data for identifying discrimination. Several best practices are suggested to capture these benefits. △ Less","11 April, 2017",https://arxiv.org/pdf/1704.03361
"Passive Sensing and Communication Using Visible Light: Taxonomy, Challenges and Opportunities",Qing Wang;Marco Zuniga,"For more than a century, artificial lighting has served mainly for illumination. Only recently, we start to transform our lighting infrastructure to provide new services such as indoor localization and network connectivity. These innovative advancements rely on two key requirements: the ability to modulate light sources (for data transmission) and the presence of photodetectors on objects (for data reception). But not all lights can be modulated and most objects do not have photodetectors. To overcome these limitations, researchers are developing novel sensing and communication methods that exploit passive light sources, such as the sun, and that leverage the external surfaces of objects, such as fingers and car roofs, to create a new generation of cyber-physical systems based on visible light. In this article we propose a taxonomy to analyze these novel contributions. Our taxonomy allows us to identify the overarching principles, challenges and opportunities of this new rising area. △ Less","5 April, 2017",https://arxiv.org/pdf/1704.01331
KRC: KnowInG crowdsourcing platform supporting creativity and innovation,Fernando Ferri;Patrizia Grifoni;Maria Chiara Caschera;Arianna D'Ulizia;Caterina Pratico,"The deep financial and economic crisis, which still characterizes these years, requires searching for tools in order to enhance knowledge sharing, creativity and innovation. The Internet is one of these tools that represents a practically infinite source of resources. In this perspective, the KnowInG project, funded by the STC programme MED, is aimed at developing the KnowInG Resource Centre (KRC), a sociotechnical system that works as a multiplier of innovation. KRC was conceived as a crowdsourcing platform allowing people, universities, research centres, organizations and companies to be active actors of creative and innovation processes from a local to a transnational level. △ Less","4 April, 2017",https://arxiv.org/pdf/1704.00973
Customizing First Person Image Through Desired Actions,Shan Su;Jianbo Shi;Hyun Soo Park,"This paper studies a problem of inverse visual path planning: creating a visual scene from a first person action. Our conjecture is that the spatial arrangement of a first person visual scene is deployed to afford an action, and therefore, the action can be inversely used to synthesize a new scene such that the action is feasible. As a proof-of-concept, we focus on linking visual experiences induced by walking. A key innovation of this paper is a concept of ActionTunnel---a 3D virtual tunnel along the future trajectory encoding what the wearer will visually experience as moving into the scene. This connects two distinctive first person images through similar walking paths. Our method takes a first person image with a user defined future trajectory and outputs a new image that can afford the future motion. The image is created by combining present and future ActionTunnels in 3D where the missing pixels in adjoining area are computed by a generative adversarial network. Our work can provide a travel across different first person experiences in diverse real world scenes. △ Less","31 March, 2017",https://arxiv.org/pdf/1704.00098
MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction,Ayush Tewari;Michael Zollhöfer;Hyeongwoo Kim;Pablo Garrido;Florian Bernard;Patrick Pérez;Christian Theobalt,"In this work we propose a novel model-based deep convolutional autoencoder that addresses the highly challenging problem of reconstructing a 3D human face from a single in-the-wild color image. To this end, we combine a convolutional encoder network with an expert-designed generative model that serves as decoder. The core innovation is our new differentiable parametric decoder that encapsulates image formation analytically based on a generative model. Our decoder takes as input a code vector with exactly defined semantic meaning that encodes detailed face pose, shape, expression, skin reflectance and scene illumination. Due to this new way of combining CNN-based with model-based face reconstruction, the CNN-based encoder learns to extract semantically meaningful parameters from a single monocular input image. For the first time, a CNN encoder and an expert-designed generative model can be trained end-to-end in an unsupervised manner, which renders training on very large (unlabeled) real world data feasible. The obtained reconstructions compare favorably to current state-of-the-art approaches in terms of quality and richness of representation. △ Less","7 December, 2017",https://arxiv.org/pdf/1703.10580
Fleet management for autonomous vehicles: Online PDP under special constraints,Sahar Bsaybes;Alain Quilliot;Annegret K. Wagler,"The VIPAFLEET project consists in developing models and algorithms for man- aging a fleet of Individual Public Autonomous Vehicles (VIPA). Hereby, we consider a fleet of cars distributed at specified stations in an industrial area to supply internal transportation, where the cars can be used in different modes of circulation (tram mode, elevator mode, taxi mode). One goal is to develop and implement suitable algorithms for each mode in order to satisfy all the requests under an economic point of view by minimizing the total tour length. The innovative idea and challenge of the project is to develop and install a dynamic fleet management system that allows the operator to switch between the different modes within the different periods of the day according to the dynamic transportation demands of the users. We model the underlying online transportation system and propose a correspond- ing fleet management framework, to handle modes, demands and commands. We consider two modes of circulation, tram and elevator mode, propose for each mode appropriate on- line algorithms and evaluate their performance, both in terms of competitive analysis and practical behavior. △ Less","30 March, 2017",https://arxiv.org/pdf/1703.10565
"Modular, Fully-abstract Compilation by Approximate Back-translation",Dominique Devriese;Marco Patrignani;Frank Piessens;Steven Keuchel,"A compiler is fully-abstract if the compilation from source language programs to target language programs reflects and preserves behavioural equivalence. Such compilers have important security benefits, as they limit the power of an attacker interacting with the program in the target language to that of an attacker interacting with the program in the source language. Proving compiler full-abstraction is, however, rather complicated. A common proof technique is based on the back-translation of target-level program contexts to behaviourally-equivalent source-level contexts. However, constructing such a back- translation is problematic when the source language is not strong enough to embed an encoding of the target language. For instance, when compiling from STLC to ULC, the lack of recursive types in the former prevents such a back-translation. We propose a general and elegant solution for this problem. The key insight is that it suffices to construct an approximate back-translation. The approximation is only accurate up to a certain number of steps and conservative beyond that, in the sense that the context generated by the back-translation may diverge when the original would not, but not vice versa. Based on this insight, we describe a general technique for proving compiler full-abstraction and demonstrate it on a compiler from STLC to ULC. The proof uses asymmetric cross-language logical relations and makes innovative use of step-indexing to express the relation between a context and its approximate back-translation. The proof extends easily to common compiler patterns such as modular compilation and it, to the best of our knowledge, it is the first compiler full abstraction proof to have been fully mechanised in Coq. We believe this proof technique can scale to challenging settings and enable simpler, more scalable proofs of compiler full-abstraction. △ Less","24 October, 2017",https://arxiv.org/pdf/1703.09988
Admire framework: Distributed data mining on data grid platforms,Nhien-An Le-Khac;M-Tahar Kechadi;Joe Carthy,"In this paper, we present the ADMIRE architecture; a new framework for developing novel and innovative data mining techniques to deal with very large and distributed heterogeneous datasets in both commercial and academic applications. The main ADMIRE components are detailed as well as its interfaces allowing the user to efficiently develop and implement their data mining applications techniques on a Grid platform such as Globus ToolKit, DGET, etc. △ Less","28 March, 2017",https://arxiv.org/pdf/1703.09756
Categorizing User Sessions at Pinterest,Dorna Bandari;Shuo Xiang;Jure Leskovec,"Different users can use a given Internet application in many different ways. The ability to record detailed event logs of user in-application activity allows us to discover ways in which the application is being used. This enables personalization and also leads to important insights with actionable business and product outcomes. Here we study the problem of user session categorization, where the goal is to automatically discover categories/classes of user in-session behavior using event logs, and then consistently categorize each user session into the discovered classes. We develop a three stage approach which uses clustering to discover categories of sessions, then builds classifiers to classify new sessions into the discovered categories, and finally performs daily classification in a distributed pipeline. An important innovation of our approach is selecting a set of events as long-tail features, and replacing them with a new feature that is less sensitive to product experimentation and logging changes. This allows for robust and stable identification of session types even though the underlying application is constantly changing. We deploy the approach to Pinterest and demonstrate its effectiveness. We discover insights that have consequences for product monetization, growth, and design. Our solution classifies millions of user sessions daily and leads to actionable insights. △ Less","25 October, 2017",https://arxiv.org/pdf/1703.09662
Implications of the Fourth Industrial Age on Higher Education,Bo Xing;Tshilidzi Marwala,"Higher education in the fourth industrial revolution, HE 4.0, is a complex, dialectical and exciting opportunity which can potentially transform society for the better. The fourth industrial revolution is powered by artificial intelligence and it will transform the workplace from tasks based characteristics to the human centred characteristics. Because of the convergence of man and machine, it will reduce the subject distance between humanities and social science as well as science and technology. This will necessarily require much more interdisciplinary teaching, research and innovation. This paper explores the impact of HE 4.0 on the mission of a university which is teaching, research (including innovation) and service. △ Less","17 March, 2017",https://arxiv.org/pdf/1703.09643
"Formation {à} distance et outils num{é}riques pour l'enseignement sup{é}rieur et la recherche en Asie-Pacifique (Cambodge, Laos, Vietnam). Partie 02 : recommandations et feuille de route",Mokhtar Ben Henda,"This second part of a 2 volume-expertise is mainly based on the results of the inventory described in the first part. It is also based on a ""worrying"" statement in the terms of reference of this expert report 1, which states that ""Asia enjoys a favourable technological context [in terms of equipment according to UN statistics]. Nevertheless, digital technology is still hardly present in the practices of the member institutions of the Agency and the francophone university community; So distance education is not well developed: there are currently no French-language distance training courses offered by an establishment in Asia; The region has only 14 enrolled in ODL over the period 2010 - 2014; Only three institutions have responded to the AUF's ""Mooc"" calls for projects over the last two years, etc."". The terms of reference also indicate a state of deficiency ""in the Francophone digital campuses whose officials explain that the computer equipment are less and less used for individual practices"". The proliferation of mobile digital technologies that would normally constitute an important asset for the development of teaching practices and innovative research around the Francophone digital campuses has not lived up to expectations. The paper refers to another no less important detail that would explain the paradox between the proliferation of technological tools and the reduction in usage when it indicates that, in parallel, and contrary to the francophone campuses, In English a positive dynamics of integration of T ICE and distance"". The document provides concrete examples, such as the ASEAN Cyber University (ACU) program run by South Korea and its e-learning centers in Cambodia, Laos, Vietnam and Myanmar, The Vietnamese language and the fablab set up in the region since 2014 without the Francophonie being involved. A first hypothesis emerges from this premonitory observation that it is not technology that justifies the gradual demobilization (or even demotivation) of local actors to establish forms of Francophone partnerships for training and research. Nor is it a question of political will to improve technological infrastructure in digital training and research. Almost all the interviews carried out within the framework of this mission demonstrated the convergence of views and ambitious attitudes expressed by three categories of actors encountered:- political actors met in the ministries of education of the three countries who are aware of the importance of digital education and the added value generated by technologies for education. Each of the three countries has a regulatory framework and national technological innovation projects for education and digital education;- public and private academic institutions which, through their rectors, presidents and technical and pedagogical leaders, demonstrate their profound convictions for digital education (for reasons of quality, competitiveness and economic interest). However, given the rather centralized governance model at state level, the majority of academic institutions in the three countries are often awaiting the promulgation of legal texts (decrees, charters, conventions, directives , Etc.) that enable them to act and adopt innovative solutions in teaching and research;- Teacher-researchers relatively little consulted in this expertise, but sufficiently engaged as actors on the ground to be able to predict their points of view with regard to the use of digital in their pedagogical and research activities. Teachers and researchers with relatively modest incomes would inevitably have a decisive role in any academic reform on the digital level if concrete mobilizing arguments could compensate for their shortfalls by joining digital training or development projects or programs. △ Less","3 March, 2017",https://arxiv.org/pdf/1703.09641
Languages of Play: Towards semantic foundations for game interfaces,Chris Martens;Matthew A. Hammer,"Formal models of games help us account for and predict behavior, leading to more robust and innovative designs. While the games research community has proposed many formalisms for both the ""game half"" (game models, game description languages) and the ""human half"" (player modeling) of a game experience, little attention has been paid to the interface between the two, particularly where it concerns the player expressing her intent toward the game. We describe an analytical and computational toolbox based on programming language theory to examine the phenomenon sitting between control schemes and game rules, which we identify as a distinct player intent language for each game. △ Less","15 March, 2017",https://arxiv.org/pdf/1703.05410
"Network Slicing for 5G with SDN/NFV: Concepts, Architectures and Challenges",Jose Ordonez-Lucena;Pablo Ameigeiras;Diego Lopez;Juan J. Ramos-Munoz;Javier Lorca;Jesus Folgueira,"The fifth generation of mobile communications is anticipated to open up innovation opportunities for new industries such as vertical markets. However, these verticals originate myriad use cases with diverging requirements that future 5G networks have to efficiently support. Network slicing may be a natural solution to simultaneously accommodate over a common network infrastructure the wide range of services that vertical-specific use cases will demand. In this article, we present the network slicing concept, with a particular focus on its application to 5G systems. We start by summarizing the key aspects that enable the realization of so-called network slices. Then, we give a brief overview on the SDN architecture proposed by the ONF and show that it provides tools to support slicing. We argue that although such architecture paves the way for network slicing implementation, it lacks some essential capabilities that can be supplied by NFV. Hence, we analyze a proposal from the ETSI to incorporate the capabilities of SDN into the NFV architecture. Additionally, we present an example scenario that combines SDN and NFV technologies to address the realization of network slices. Finally, we summarize the open research issues with the purpose of motivating new advances in this field. △ Less","1 May, 2017",https://arxiv.org/pdf/1703.04676
Segmentation of skin lesions based on fuzzy classification of pixels and histogram thresholding,Jose Luis Garcia-Arroyo;Begonya Garcia-Zapirain,"This paper proposes an innovative method for segmentation of skin lesions in dermoscopy images developed by the authors, based on fuzzy classification of pixels and histogram thresholding.","10 March, 2017",https://arxiv.org/pdf/1703.03888
PACO: A System-Level Abstraction for On-Loading Contextual Data to Mobile Devices,Nathaniel Wendt;Christine Julien,"Spatiotemporal context is crucial in modern mobile applications that utilize increasing amounts of context to better predict events and user behaviors, requiring rich records of users' or devices' spatiotemporal histories. Maintaining these rich histories requires frequent sampling and indexed storage of spatiotemporal data that pushes the limits of resource-constrained mobile devices. Today's apps offload processing and storing contextual information, but this increases response time, often relies on the user's data connection, and runs the very real risk of revealing sensitive information. In this paper we motivate the feasibility of on-loading large amounts of context and introduce PACO (Programming Abstraction for Contextual On-loading), an architecture for on-loading data that optimizes for location and time while allowing flexibility in storing additional context. The PACO API's innovations enable on-loading very dense traces of information, even given devices' resource constraints. Using real-world traces and our implementation for Android, we demonstrate that PACO can support expressive application queries entirely on-device. Our quantitative evaluation assesses PACO's energy consumption, execution time, and spatiotemporal query accuracy. Further, PACO facilitates unified contextual reasoning across multiple applications and also supports user-controlled release of contextual data to other devices or the cloud; we demonstrate these assets through a proof-of-concept case study. △ Less","9 March, 2017",https://arxiv.org/pdf/1703.03504
LesionSeg: Semantic segmentation of skin lesions using Deep Convolutional Neural Network,Dhanesh Ramachandram;Terrance DeVries,"We present a method for skin lesion segmentation for the ISIC 2017 Skin Lesion Segmentation Challenge. Our approach is based on a Fully Convolutional Network architecture which is trained end to end, from scratch, on a limited dataset. Our semantic segmentation architecture utilizes several recent innovations in particularly in the combined use of (i) use of atrous convolutions to increase the effective field of view of the network's receptive field without increasing the number of parameters, (ii) the use of network-in-network
convolution layers to add capacity to the network and (iii) state-of-art super-resolution upsampling of predictions using subpixel CNN layers. We reported a mean IOU score of 0.642 on the validation set provided by the organisers. △ Less","14 March, 2017",https://arxiv.org/pdf/1703.03372
Joint Multichannel Deconvolution and Blind Source Separation,Ming Jiang;Jérôme Bobin;Jean-Luc Starck,"Blind Source Separation (BSS) is a challenging matrix factorization problem that plays a central role in multichannel imaging science. In a large number of applications, such as astrophysics, current unmixing methods are limited since real-world mixtures are generally affected by extra instrumental effects like blurring. Therefore, BSS has to be solved jointly with a deconvolution problem, which requires tackling a new inverse problem: deconvolution BSS (DBSS). In this article, we introduce an innovative DBSS approach, called DecGMCA, based on sparse signal modeling and an efficient alternative projected least square algorithm. Numerical results demonstrate that the DecGMCA algorithm performs very well on simulations. It further highlights the importance of jointly solving BSS and deconvolution instead of considering these two problems independently. Furthermore, the performance of the proposed DecGMCA algorithm is demonstrated on simulated radio-interferometric data. △ Less","14 May, 2017",https://arxiv.org/pdf/1703.02650
Graph of Virtual Actors (GOVA): a Big Data Analytics Architecture for IoT,The-Hien Dang-Ha;Davide Roverso;Roland Olsson,"With the emergence of cloud computing and sensor technologies, Big Data analytics for the Internet of Things (IoT) has become the main force behind many innovative solutions for our society's problems. This paper provides practical explanations for the question ""why is the number of Big Data applications that succeed and have an effect on our daily life so limited, compared with all of the solutions proposed and tested in the literature?"", with examples taken from Smart Grids. We argue that ""noninvariants"" are the most challenging issues in IoT applications, which can be easily revealed if we use the term ""invariant"" to replace the more common terms such as ""information"", ""knowledge"", or ""insight"" in any Big Data for IoT research. From our experience with developing Smart Grid applications, we produced a list of ""noninvariants"", which we believe to be the main causes of the gaps between Big Data in a laboratory and in practice in IoT applications. This paper also proposes Graph of Virtual Actors (GOVA) as a Big Data analytics architecture for IoT applications, which not only can solve the noninvariants issues, but can also quickly scale horizontally in terms of computation, data storage, caching requirements, and programmability of the system. △ Less","7 March, 2017",https://arxiv.org/pdf/1703.02510
Convex recovery of continuous domain piecewise constant images from non-uniform Fourier samples,Greg Ongie;Sampurna Biswas;Mathews Jacob,"We consider the recovery of a continuous domain piecewise constant image from its non-uniform Fourier samples using a convex matrix completion algorithm. We assume the discontinuities/edges of the image are localized to the zero levelset of a bandlimited function. This assumption induces linear dependencies between the Fourier coefficients of the image, which results in a two-fold block Toeplitz matrix constructed from the Fourier coefficients being low-rank. The proposed algorithm reformulates the recovery of the unknown Fourier coefficients as a structured low-rank matrix completion problem, where the nuclear norm of the matrix is minimized subject to structure and data constraints. We show that exact recovery is possible with high probability when the edge set of the image satisfies an incoherency property. We also show that the incoherency property is dependent on the geometry of the edge set curve, implying higher sampling burden for smaller curves. This paper generalizes recent work on the super-resolution recovery of isolated Diracs or signals with finite rate of innovation to the recovery of piecewise constant images. △ Less","4 March, 2017",https://arxiv.org/pdf/1703.01405
Easy over Hard: A Case Study on Deep Learning,Wei Fu;Tim Menzies,"While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work. For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That deep learning system took 14 hours to execute. We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results. The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning method. We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives. △ Less","24 June, 2017",https://arxiv.org/pdf/1703.00133
DTNC: A New Server-side Data Cleansing Framework for Cellular Trajectory Services,Jian Dai;Fei He;Wang-Chien Lee;Gang Chen;Beng Chin Ooi,"It is essential for the cellular network operators to provide cellular location services to meet the needs of their users and mobile applications. However, cellular locations, estimated by network-based methods at the server-side, bear with {\it high spatial errors} and {\it arbitrary missing locations}. Moreover, auxiliary sensor data at the client-side are not available to the operators. In this paper, we study the {\em cellular trajectory cleansing problem} and propose an innovative data cleansing framework, namely \underline{D}ynamic \underline{T}ransportation \underline{N}etwork based \underline{C}leansing (DTNC) to improve the quality of cellular locations delivered in online cellular trajectory services. We maintain a dynamic transportation network (DTN), which associates a network edge with a probabilistic distribution of travel times updated continuously. In addition, we devise an object motion model, namely, {\em travel-time-aware hidden semi-Markov model} ({\em TT-HsMM}), which is used to infer the most probable traveled edge sequences on DTN. To validate our ideas, we conduct a comprehensive evaluation using real-world cellular data provided by a major cellular network operator and a GPS dataset collected by smartphones as the ground truth. In the experiments, DTNC displays significant advantages over six state-of-the-art techniques. △ Less","3 March, 2017",https://arxiv.org/pdf/1703.00123
Multi-Sensor Data Pattern Recognition for Multi-Target Localization: A Machine Learning Approach,Kasthurirengan Suresh;Samuel Silva;Johnathan Votion;Yongcan Cao,"Data-target pairing is an important step towards multi-target localization for the intelligent operation of unmanned systems. Target localization plays a crucial role in numerous applications, such as search, and rescue missions, traffic management and surveillance. The objective of this paper is to present an innovative target location learning approach, where numerous machine learning approaches, including K-means clustering and supported vector machines (SVM), are used to learn the data pattern across a list of spatially distributed sensors. To enable the accurate data association from different sensors for accurate target localization, appropriate data pre-processing is essential, which is then followed by the application of different machine learning algorithms to appropriately group data from different sensors for the accurate localization of multiple targets. Through simulation examples, the performance of these machine learning algorithms is quantified and compared. △ Less","28 February, 2017",https://arxiv.org/pdf/1703.00084
Can Boltzmann Machines Discover Cluster Updates ?,Lei Wang,"Boltzmann machines are physics informed generative models with wide applications in machine learning. They can learn the probability distribution from an input dataset and generate new samples accordingly. Applying them back to physics, the Boltzmann machines are ideal recommender systems to accelerate Monte Carlo simulation of physical systems due to their flexibility and effectiveness. More intriguingly, we show that the generative sampling of the Boltzmann Machines can even discover unknown cluster Monte Carlo algorithms. The creative power comes from the latent representation of the Boltzmann machines, which learn to mediate complex interactions and identify clusters of the physical system. We demonstrate these findings with concrete examples of the classical Ising model with and without four spin plaquette interactions. Our results endorse a fresh research paradigm where intelligent machines are designed to create or inspire human discovery of innovative algorithms. △ Less","27 February, 2017",https://arxiv.org/pdf/1702.08586
Competing Bandits: Learning under Competition,Yishay Mansour;Aleksandrs Slivkins;Zhiwei Steven Wu,"Most modern systems strive to learn from interactions with users, and many engage in exploration: making potentially suboptimal choices for the sake of acquiring new information. We initiate a study of the interplay between exploration and competition--how such systems balance the exploration for learning and the competition for users. Here the users play three distinct roles: they are customers that generate revenue, they are sources of data for learning, and they are self-interested agents which choose among the competing systems. In our model, we consider competition between two multi-armed bandit algorithms faced with the same bandit instance. Users arrive one by one and choose among the two algorithms, so that each algorithm makes progress if and only if it is chosen. We ask whether and to what extent competition incentivizes the adoption of better bandit algorithms. We investigate this issue for several models of user response, as we vary the degree of rationality and competitiveness in the model. Our findings are closely related to the ""competition vs. innovation"" relationship, a well-studied theme in economics. △ Less","19 November, 2017",https://arxiv.org/pdf/1702.08533
Contextualization of topics: Browsing through the universe of bibliographic information,Rob Koopman;Shenghui Wang;Andrea Scharnhorst,"This paper describes how semantic indexing can help to generate a contextual overview of topics and visually compare clusters of articles. The method was originally developed for an innovative information exploration tool, called Ariadne, which operates on bibliographic databases with tens of millions of records. In this paper, the method behind Ariadne is further developed and applied to the research question of the special issue ""Same data, different results"" - the better understanding of topic (re-)construction by different bibliometric approaches. For the case of the Astro dataset of 111,616 articles in astronomy and astrophysics, a new instantiation of the interactive exploring tool, LittleAriadne, has been created. This paper contributes to the overall challenge to delineate and define topics in two different ways. First, we produce two clustering solutions based on vector representations of articles in a lexical space. These vectors are built on semantic indexing of entities associated with those articles. Second, we discuss how LittleAriadne can be used to browse through the network of topical terms, authors, journals, citations and various cluster solutions of the Astro dataset. More specifically, we treat the assignment of an article to the different clustering solutions as an additional element of its bibliographic record. Keeping the principle of semantic indexing on the level of such an extended list of entities of the bibliographic record, LittleAriadne in turn provides a visualization of the context of a specific clustering solution. It also conveys the similarity of article clusters produced by different algorithms, hence representing a complementary approach to other possible means of comparison. △ Less","27 February, 2017",https://arxiv.org/pdf/1702.08210
Knowledge Reuse for Customization: Metamodels in an Open Design Community for 3d Printing,Harris Kyriakou;Jeffrey V Nickerson;Gaurav Sabnis,"Theories of knowledge reuse posit two distinct processes: reuse for replication and reuse for innovation. We identify another distinct process, reuse for customization. Reuse for customization is a process in which designers manipulate the parameters of metamodels to produce models that fulfill their personal needs. We test hypotheses about reuse for customization in Thingiverse, a community of designers that shares files for three-dimensional printing. 3D metamodels are reused more often than the 3D models they generate. The reuse of metamodels is amplified when the metamodels are created by designers with greater community experience. Metamodels make the community's design knowledge available for reuse for customization-or further extension of the metamodels, a kind of reuse for innovation. △ Less","26 February, 2017",https://arxiv.org/pdf/1702.08072
Preventing Hospital Acquired Infections Through a Workflow-Based Cyber-Physical System,Maria Iuliana Bocicor;Arthur-Jozsef Molnar;Cristian Taslitchi,"Hospital acquired infections (HAI) are infections acquired within the hospital from healthcare workers, patients or from the environment, but which have no connection to the initial reason for the patient's hospital admission. HAI are a serious world-wide problem, leading to an increase in mortality rates, duration of hospitalisation as well as significant economic burden on hospitals. Although clear preventive guidelines exist, studies show that compliance to them is frequently poor. This paper details the software perspective for an innovative, business process software based cyber-physical system that will be implemented as part of a European Union-funded research project. The system is composed of a network of sensors mounted in different sites around the hospital, a series of wearables used by the healthcare workers and a server side workflow engine. For better understanding, we describe the system through the lens of a single, simple clinical workflow that is responsible for a significant portion of all hospital infections. The goal is that when completed, the system will be configurable in the sense of facilitating the creation and automated monitoring of those clinical workflows that when combined, account for over 90\% of hospital infections. △ Less","26 February, 2017",https://arxiv.org/pdf/1702.08010
Automation in Human-Machine Networks: How Increasing Machine Agency Affects Human Agency,Asbjørn Følstad;Vegard Engen;Ida Maria Haugstveit;Brian Pickering,"Efficient human-machine networks require productive interaction between human and machine actors. In this study, we address how a strengthening of machine agency, for example through increasing levels of automation, affect the human actors of the networks. Findings from case studies within air traffic management, crisis management, and crowd evacuation are presented, exemplifying how automation may strengthen the agency of human actors in the network through responsibility sharing and task allocation, and serve as a needed prerequisite of innovation and change. △ Less","24 February, 2017",https://arxiv.org/pdf/1702.07480
Software Defined Media: Virtualization of Audio-Visual Services,Manabu Tsukada;Keiko Ogawa;Masahiro Ikeda;Takuro Sone;Kenta Niwa;Shoichiro Saito;Takashi Kasuya;Hideki Sunahara;Hiroshi Esaki,"Internet-native audio-visual services are witnessing rapid development. Among these services, object-based audio-visual services are gaining importance. In 2014, we established the Software Defined Media (SDM) consortium to target new research areas and markets involving object-based digital media and Internet-by-design audio-visual environments. In this paper, we introduce the SDM architecture that virtualizes networked audio-visual services along with the development of smart buildings and smart cities using Internet of Things (IoT) devices and smart building facilities. Moreover, we design the SDM architecture as a layered architecture to promote the development of innovative applications on the basis of rapid advancements in software-defined networking (SDN). Then, we implement a prototype system based on the architecture, present the system at an exhibition, and provide it as an SDM API to application developers at hackathons. Various types of applications are developed using the API at these events. An evaluation of SDM API access shows that the prototype SDM platform effectively provides 3D audio reproducibility and interactiveness for SDM applications. △ Less","23 February, 2017",https://arxiv.org/pdf/1702.07452
FM Backscatter: Enabling Connected Cities and Smart Fabrics,Anran Wang;Vikram Iyer;Vamsi Talla;Joshua R. Smith;Shyamnath Gollakota,"This paper enables connectivity on everyday objects by transforming them into FM radio stations. To do this, we show for the first time that ambient FM radio signals can be used as a signal source for backscatter communication. Our design creates backscatter transmissions that can be decoded on any FM receiver including those in cars and smartphones. This enables us to achieve a previously infeasible capability: backscattering information to cars and smartphones in outdoor environments. Our key innovation is a modulation technique that transforms backscatter, which is a multiplication operation on RF signals, into an addition operation on the audio signals output by FM receivers. This enables us to embed both digital data as well as arbitrary audio into ambient analog FM radio signals. We build prototype hardware of our design and successfully embed audio transmissions over ambient FM signals. Further, we achieve data rates of up to 3.2 kbps and ranges of 5-60 feet, while consuming as little as 11.07μW of power. To demonstrate the potential of our design, we also fabricate our prototype on a cotton t-shirt by machine sewing patterns of a conductive thread to create a smart fabric that can transmit data to a smartphone. We also embed FM antennas into posters and billboards and show that they can communicate with FM receivers in cars and smartphones. △ Less","24 February, 2017",https://arxiv.org/pdf/1702.07044
Framework for an Innovative Perceptive Mobile Network Using Joint Communication and Sensing,J. Andrew Zhang;Antonio Cantoni;Xiaojing Huang;Y. Jay Guo;Robert W. Heath Jr,"In this paper, we develop a framework for an innovative perceptive mobile (i.e. cellular) network that integrates sensing with communication, and supports new applications widely in transportation, surveillance and environmental sensing. Three types of sensing methods implemented in the base-stations are proposed, using either uplink or downlink multiuser communication signals. The required changes to system hardware and major technical challenges are briefly discussed. We also demonstrate the feasibility of estimating sensing parameters via developing a compressive sensing based scheme and providing simulation results to validate its effectiveness. △ Less","21 February, 2017",https://arxiv.org/pdf/1702.06531
"Rethinking High Performance Computing Platforms: Challenges, Opportunities and Recommendations",Ole Weidner;Malcolm Atkinson;Adam Barker;Rosa Filgueira,"A new class of Second generation high-performance computing applications with heterogeneous, dynamic and data-intensive properties have an extended set of requirements, which cover application deployment, resource allocation, -control, and I/O scheduling. These requirements are not met by the current production HPC platform models and policies. This results in a loss of opportunity, productivity and innovation for new computational methods and tools. It also decreases effective system utilization for platform providers due to unsupervised workarounds and rogue resource management strategies implemented in application space. In this paper we critically discuss the dominant HPC platform model and describe the challenges it creates for second generation applications because of its asymmetric resource view, interfaces and software deployment policies. We present an extended, more symmetric and application-centric platform model that adds decentralized deployment, introspection, bidirectional control and information flow and more comprehensive resource scheduling. We describe cHPC: an early prototype of a non-disruptive implementation based on Linux Containers (LXC). It can operate alongside existing batch queuing systems and exposes a symmetric platform API without interfering with existing applications and usage modes. We see our approach as a viable, incremental next step in HPC platform evolution that benefits applications and platform providers alike. To demonstrate this further, we layout out a roadmap for future research and experimental evaluation. △ Less","27 February, 2017",https://arxiv.org/pdf/1702.05513
Models of Type Theory with Strict Equality,Paolo Capriotti,"This thesis introduces the idea of two-level type theory, an extension of Martin-Löf type theory that adds a notion of strict equality as an internal primitive. A type theory with a strict equality alongside the more conventional form of equality, the latter being of fundamental importance for the recent innovation of homotopy type theory (HoTT), was first proposed by Voevodsky, and is usually referred to as HTS. Here, we generalise and expand this idea, by developing a semantic framework that gives a systematic account of type formers for two-level systems, and proving a conservativity result relating back to a conventional type theory like HoTT. Finally, we show how a two-level theory can be used to provide partial solutions to open problems in HoTT. In particular, we use it to construct semi-simplicial types, and lay out the foundations of an internal theory of (\infty, 1)-categories. △ Less","16 February, 2017",https://arxiv.org/pdf/1702.04912
A Tractable Framework for Performance Analysis of Dense Multi-Antenna Networks,Xianghao Yu;Chang Li;Jun Zhang;Khaled B. Letaief,"Densifying the network and deploying more antennas at each access point are two principal ways to boost the capacity of wireless networks. However, due to the complicated distributions of random signal and interference channel gains, largely induced by various space-time processing techniques, it is highly challenging to quantitatively characterize the performance of dense multi-antenna networks. In this paper, using tools from stochastic geometry, a tractable framework is proposed for the analytical evaluation of such networks. The major result is an innovative representation of the coverage probability, as an induced \ell_1-norm of a Toeplitz matrix. This compact representation incorporates lots of existing analytical results on single- and multi-antenna networks as special cases, and its evaluation is almost as simple as the single-antenna case with Rayleigh fading. To illustrate its effectiveness, we apply the proposed framework to investigate two kinds of prevalent dense wireless networks, i.e., physical layer security aware networks and millimeter-wave networks. In both examples, in addition to tractable analytical results of relevant performance metrics, insightful design guidelines are also analytically obtained. △ Less","2 May, 2017",https://arxiv.org/pdf/1702.04573
The KISS principle in Software-Defined Networking: An architecture for Keeping It Simple and Secure,Diego Kreutz;Jiangshan Yu;Paulo Esteves-Verissimo;Catia Magalhaes;Fernando M. V. Ramos,"Security is an increasingly fundamental requirement in Software-Defined Networking (SDN). However, the pace of adoption of secure mechanisms has been slow, which we estimate to be a consequence of the performance overhead of traditional solutions and of the complexity of the support infrastructure required. As a first step to addressing these problems, we propose a modular secure SDN control plane communications architecture, KISS, with innovative solutions in the context of key distribution and secure channel support. A comparative analysis of the performance impact of essential security primitives guided our selection of basic primitives for KISS. We further propose iDVV, the integrated device verification value, a deterministic but indistinguishable-from-random secret code generation protocol, allowing the local but synchronized generation/verification of keys at both ends of the channel, even on a per-message basis. iDVV is expected to give an important contribution both to the robustness and simplification of the authentication and secure communication problems in SDN. We show that our solution, while offering the same security properties, outperforms reference alternatives, with performance improvements up to 30% over OpenSSL, and improvement in robustness based on a code footprint one order of magnitude smaller. Finally, we also prove and test randomness of the proposed algorithms. △ Less","2 November, 2017",https://arxiv.org/pdf/1702.04294
Mining Online Social Data for Detecting Social Network Mental Disorders,Hong-Han Shuai;Chih-Ya Shen;De-Nian Yang;Yi-Feng Lan;Wang-Chien Lee;Philip S. Yu;Ming-Syan Chen,"An increasing number of social network mental disorders (SNMDs), such as Cyber-Relationship Addiction, Information Overload, and Net Compulsion, have been recently noted. Symptoms of these mental disorders are usually observed passively today, resulting in delayed clinical intervention. In this paper, we argue that mining online social behavior provides an opportunity to actively identify SNMDs at an early stage. It is challenging to detect SNMDs because the mental factors considered in standard diagnostic criteria (questionnaire) cannot be observed from online social activity logs. Our approach, new and innovative to the practice of SNMD detection, does not rely on self-revealing of those mental factors via questionnaires. Instead, we propose a machine learning framework, namely, Social Network Mental Disorder Detection (SNMDD), that exploits features extracted from social network data to accurately identify potential cases of SNMDs. We also exploit multi-source learning in SNMDD and propose a new SNMD-based Tensor Model (STM) to improve the performance. Our framework is evaluated via a user study with 3126 online social network users. We conduct a feature analysis, and also apply SNMDD on large-scale datasets and analyze the characteristics of the three SNMD types. The results show that SNMDD is promising for identifying online social network users with potential SNMDs. △ Less","6 June, 2017",https://arxiv.org/pdf/1702.03872
ArtGAN: Artwork Synthesis with Conditional Categorical GANs,Wei Ren Tan;Chee Seng Chan;Hernan Aguirre;Kiyoshi Tanaka,"This paper proposes an extension to the Generative Adversarial Networks (GANs), namely as ARTGAN to synthetically generate more challenging and complex images such as artwork that have abstract characteristics. This is in contrast to most of the current solutions that focused on generating natural images such as room interiors, birds, flowers and faces. The key innovation of our work is to allow back-propagation of the loss function w.r.t. the labels (randomly assigned to each generated images) to the generator from the discriminator. With the feedback from the label information, the generator is able to learn faster and achieve better generated image quality. Empirically, we show that the proposed ARTGAN is capable to create realistic artwork, as well as generate compelling real world images that globally look natural with clear shape on CIFAR-10. △ Less","19 April, 2017",https://arxiv.org/pdf/1702.03410
Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights,Aojun Zhou;Anbang Yao;Yiwen Guo;Lin Xu;Yurong Chen,"This paper presents incremental network quantization (INQ), a novel method, targeting to efficiently convert any pre-trained full-precision convolutional neural network (CNN) model into a low-precision version whose weights are constrained to be either powers of two or zero. Unlike existing methods which are struggled in noticeable accuracy loss, our INQ has the potential to resolve this issue, as benefiting from two innovations. On one hand, we introduce three interdependent operations, namely weight partition, group-wise quantization and re-training. A well-proven measure is employed to divide the weights in each layer of a pre-trained CNN model into two disjoint groups. The weights in the first group are responsible to form a low-precision base, thus they are quantized by a variable-length encoding method. The weights in the other group are responsible to compensate for the accuracy loss from the quantization, thus they are the ones to be re-trained. On the other hand, these three operations are repeated on the latest re-trained group in an iterative manner until all the weights are converted into low-precision ones, acting as an incremental network quantization and accuracy enhancement procedure. Extensive experiments on the ImageNet classification task using almost all known deep CNN architectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the efficacy of the proposed method. Specifically, at 5-bit quantization, our models have improved accuracy than the 32-bit floating-point references. Taking ResNet-18 as an example, we further show that our quantized models with 4-bit, 3-bit and 2-bit ternary weights have improved or very similar accuracy against its 32-bit floating-point baseline. Besides, impressive results with the combination of network pruning and INQ are also reported. The code is available at https://github.com/Zhouaojun/Incremental-Network-Quantization. △ Less","25 August, 2017",https://arxiv.org/pdf/1702.03044
Robust Orchestration of Concurrent Application Workflows in Mobile Device Clouds,Parul Pandey;Hariharasudhan Viswanathan;Dario Pompili,"A hybrid mobile/fixed device cloud that harnesses sensing, computing, communication, and storage capabilities of mobile and fixed devices in the field as well as those of computing and storage servers in remote datacenters is envisioned. Mobile device clouds can be harnessed to enable innovative pervasive applications that rely on real-time, in-situ processing of sensor data collected in the field. To support concurrent mobile applications on the device cloud, a robust and secure distributed computing framework, called Maestro, is proposed. The key components of Maestro are (i) a task scheduling mechanism that employs controlled task replication in addition to task reallocation for robustness and (ii) Dedup for task deduplication among concurrent pervasive workflows. An architecture-based solution that relies on task categorization and authorized access to the categories of tasks is proposed for different levels of protection. Experimental evaluation through prototype testbed of Android- and Linux-based mobile devices as well as simulations is performed to demonstrate Maestro's capabilities. △ Less","5 January, 2017",https://arxiv.org/pdf/1702.02903
"Network-based methods for outcome prediction in the ""sample space""",Jessica Gliozzo,"In this thesis we present the novel semi-supervised network-based algorithm P-Net, which is able to rank and classify patients with respect to a specific phenotype or clinical outcome under study. The peculiar and innovative characteristic of this method is that it builds a network of samples/patients, where the nodes represent the samples and the edges are functional or genetic relationships between individuals (e.g. similarity of expression profiles), to predict the phenotype under study. In other words, it constructs the network in the ""sample space"" and not in the ""biomarker space"" (where nodes represent biomolecules (e.g. genes, proteins) and edges represent functional or genetic relationships between nodes), as usual in state-of-the-art methods. To assess the performances of P-Net, we apply it on three different publicly available datasets from patients afflicted with a specific type of tumor: pancreatic cancer, melanoma and ovarian cancer dataset, by using the data and following the experimental set-up proposed in two recently published papers [Barter et al., 2014, Winter et al., 2012]. We show that network-based methods in the ""sample space"" can achieve results competitive with classical supervised inductive systems. Moreover, the graph representation of the samples can be easily visualized through networks and can be used to gain visual clues about the relationships between samples, taking into account the phenotype associated or predicted for each sample. To our knowledge this is one of the first works that proposes graph-based algorithms working in the ""sample space"" of the biomolecular profiles of the patients to predict their phenotype or outcome, thus contributing to a novel research line in the framework of the Network Medicine. △ Less","4 February, 2017",https://arxiv.org/pdf/1702.01268
Joint Offloading and Computing Optimization in Wireless Powered Mobile-Edge Computing Systems,Feng Wang;Jie Xu;Xin Wang;Shuguang Cui,"Mobile-edge computing (MEC) and wireless power transfer (WPT) have been recognized as promising techniques in the Internet of Things (IoT) era to provide massive low-power wireless devices with enhanced computation capability and sustainable energy supply. In this paper, we propose a unified MEC-WPT design by considering a wireless powered multiuser MEC system, where a multi-antenna access point (AP) (integrated with an MEC server) broadcasts wireless power to charge multiple users and each user node relies on the harvested energy to execute computation tasks. With MEC, these users can execute their respective tasks locally by themselves or offload all or part of them to the AP based on a time division multiple access (TDMA) protocol. Building on the proposed model, we develop an innovative framework to improve the MEC performance, by jointly optimizing the energy transmit beamformer at the AP, the central processing unit (CPU) frequencies and the numbers of offloaded bits at the users, as well as the time allocation among users. Under this framework, we address a practical scenario where latency-limited computation is required. In this case, we develop an optimal resource allocation scheme that minimizes the AP's total energy consumption subject to the users' individual computation latency constraints. Leveraging the state-of-the-art optimization techniques, we derive the optimal solution in a semi-closed form. Numerical results demonstrate the merits of the proposed design over alternative benchmark schemes. △ Less","17 December, 2017",https://arxiv.org/pdf/1702.00606
WLAN Performance Analysis Ibrahim Group of industries Faisalabad Pakistan,Ahmed Mateen;Zulafiqar Ali;Tasleem Mustafa,"Now a days several organizations are moving their LAN foundation towards remote LAN frame work. The purpose for this is extremely straight forward multinational organizations needs their clients surprise about their office surroundings and they additionally need to make wire free environment in their workplaces. Much IT equipment moved on Wireless for instance all in one Pc portable workstations Wireless IP telephones. Another thing is that step by step WLAN innovation moving towards extraordinary effectiveness. In this exploration work Wireless LAN innovation running in Ibrahim Group gathering of commercial enterprises Faisalabad has been investigated in term of their equipment, Wireless signal quality, data transmission, auto channel moving, and security in WLAN system. This examination work required physical proving ground, some WLAN system analyzer (TamoSof throughput) software, hardware point of interest, security testing programming. The investigation displayed in this examination has fill two key needs. One determination is to accept this kind of system interconnection could be broke down utilizing the exploratory models of the two system bits (wired and remote pieces. Second key factor is to determine the security issue in WLAN. △ Less","31 January, 2017",https://arxiv.org/pdf/1702.00127
Combining Agile with Traditional V Model for Enhancement of Maturity in Software Development,Ahmed Mateen;Madiha Tabassum;Akmal Rehan,In the field of software engineering there are many new archetypes are introducing day to day Improve the efficiency and effectiveness of software development. Due to dynamic environment organizations are frequently exchanging their software constraint to meet their objectives. The propose research is a new approach by integrating the traditional V model and agile methodology to combining the strength of these models while minimizing their individual weakness.The fluctuating requirements of emerging a carried software system and accumulative cost of operational software are imposing researchers and experts to determine innovative and superior means for emerging software application at slight business or at enterprise level are viewing for. Agile methodology has its own benefits but there are deficiency several of the features of traditional software development methodologies that are essential for success. Thats why an embedded approach will be the right answer for software industry rather than a pure agile approach. This research shows how agile embedded traditional can play a vital role in development of software. A survey conducted to find the impact of this approach in industry. Both qualitative and quantitative analysis performed. △ Less,"31 January, 2017",https://arxiv.org/pdf/1702.00126
Click Through Rate Prediction for Contextual Advertisment Using Linear Regression,Muhammad Junaid Effendi;Syed Abbas Ali,"This research presents an innovative and unique way of solving the advertisement prediction problem which is considered as a learning problem over the past several years. Online advertising is a multi-billion-dollar industry and is growing every year with a rapid pace. The goal of this research is to enhance click through rate of the contextual advertisements using Linear Regression. In order to address this problem, a new technique propose in this paper to predict the CTR which will increase the overall revenue of the system by serving the advertisements more suitable to the viewers with the help of feature extraction and displaying the advertisements based on context of the publishers. The important steps include the data collection, feature extraction, CTR prediction and advertisement serving. The statistical results obtained from the dynamically used technique show an efficient outcome by fitting the data close to perfection for the LR technique using optimized feature selection. △ Less","30 January, 2017",https://arxiv.org/pdf/1701.08744
Innovative Approaches for efficiently Warehousing Complex Data from the Web,Fadila Bentayeb;Nora Maïz;Hadj Mahboubi;Cécile Favre;Sabine Loudcher;Nouria Harbi;Omar Boussaïd;Jérôme Darmont,"Research in data warehousing and OLAP has produced important technologies for the design, management and use of information systems for decision support. With the development of Internet, the availability of various types of data has increased. Thus, users require applications to help them obtaining knowledge from the Web. One possible solution to facilitate this task is to extract information from the Web, transform and load it to a Web Warehouse, which provides uniform access methods for automatic processing of the data. In this chapter, we present three innovative researches recently introduced to extend the capabilities of decision support systems, namely (1) the use of XML as a logical and physical model for complex data warehouses, (2) associating data mining to OLAP to allow elaborated analysis tasks for complex data and (3) schema evolution in complex data warehouses for personalized analyses. Our contributions cover the main phases of the data warehouse design process: data integration and modeling and user driven-OLAP analysis. △ Less","30 January, 2017",https://arxiv.org/pdf/1701.08643
Decentralized Prediction Market without Arbiters,Iddo Bentov;Alex Mizrahi;Meni Rosenfeld,"We consider a prediction market in which all aspects are controlled by market forces, in particular the correct outcomes of events are decided by the market itself rather than by trusted arbiters. This kind of a decentralized prediction market can sustain betting on events whose outcome may remain unresolved for a long or even unlimited time period, and can facilitate trades among participants who are spread across diverse geographical locations, may wish to remain anonymous and/or avoid burdensome identification procedures, and are distrustful of each other. We describe how a cryptocurrency such as Bitcoin can be enhanced to accommodate a truly decentralized prediction market, by employing an innovative variant of the Colored Coins concept. We examine the game-theoretic properties of our design, and offer extensions that enable other financial instruments as well as real-time exchange. △ Less","7 March, 2017",https://arxiv.org/pdf/1701.08421
Who With Whom And How?: Extracting Large Social Networks Using Search Engines,Stefan Siersdorfer;Philipp Kemkes;Hanno Ackermann;Sergej Zerr,"Social network analysis is leveraged in a variety of applications such as identifying influential entities, detecting communities with special interests, and determining the flow of information and innovations. However, existing approaches for extracting social networks from unstructured Web content do not scale well and are only feasible for small graphs. In this paper, we introduce novel methodologies for query-based search engine mining, enabling efficient extraction of social networks from large amounts of Web data. To this end, we use patterns in phrase queries for retrieving entity connections, and employ a bootstrapping approach for iteratively expanding the pattern set. Our experimental evaluation in different domains demonstrates that our algorithms provide high quality results and allow for scalable and efficient construction of social graphs. △ Less","28 January, 2017",https://arxiv.org/pdf/1701.08285
Faster Discovery of Faster System Configurations with Spectral Learning,Vivek Nair;Tim Menzies;Norbert Siegmund;Sven Apel,"Despite the huge spread and economical importance of configurable software systems, there is unsatisfactory support in utilizing the full potential of these systems with respect to finding performance-optimal configurations. Prior work on predicting the performance of software configurations suffered from either (a) requiring far too many sample configurations or (b) large variances in their predictions. Both these problems can be avoided using the WHAT spectral learner. WHAT's innovation is the use of the spectrum (eigenvalues) of the distance matrix between the configurations of a configurable software system, to perform dimensionality reduction. Within that reduced configuration space, many closely associated configurations can be studied by executing only a few sample configurations. For the subject systems studied here, a few dozen samples yield accurate and stable predictors - less than 10% prediction error, with a standard deviation of less than 2%. When compared to the state of the art, WHAT (a) requires 2 to 10 times fewer samples to achieve similar prediction accuracies, and (b) its predictions are more stable (i.e., have lower standard deviation). Furthermore, we demonstrate that predictive models generated by WHAT can be used by optimizers to discover system configurations that closely approach the optimal performance. △ Less","3 August, 2017",https://arxiv.org/pdf/1701.08106
Database Benchmarks,Jérôme Darmont,"The aim of this article is to present an overview of the major families of state-of-the-art data-base benchmarks, namely: relational benchmarks, object and object-relational benchmarks, XML benchmarks, and decision-support benchmarks, and to discuss the issues, tradeoffs and future trends in database benchmarking. We particularly focus on XML and decision-support benchmarks, which are currently the most innovative tools that are developed in this area. △ Less","27 January, 2017",https://arxiv.org/pdf/1701.08052
Biomedical Data Warehouses,Jérôme Darmont;Emerson Olivier,"The aim of this article is to present an overview of the existing biomedical data warehouses and to discuss the issues and future trends in this area. We illustrate this topic by presenting the design of an innovative, complex data warehouse for personal, anticipative medicine. △ Less","27 January, 2017",https://arxiv.org/pdf/1701.08028
Contextual Consent: Ethical Mining of Social Media for Health Research,Chris Norval;Tristan Henderson,"Social media are a rich source of insight for data mining and user-centred research, but the question of consent arises when studying such data without the express knowledge of the creator. Case studies that mine social data from users of online services such as Facebook and Twitter are becoming increasingly common. This has led to calls for an open discussion into how researchers can best use these vast resources to make innovative findings while still respecting fundamental ethical principles. In this position paper we highlight some key considerations for this topic and argue that the conditions of informed consent are often not being met, and that using social media data that some deem free to access and analyse may result in undesirable consequences, particularly within the domain of health research and other sensitive topics. We posit that successful exploitation of online personal data, particularly for health and other sensitive research, requires new and usable methods of obtaining consent from the user. △ Less","26 January, 2017",https://arxiv.org/pdf/1701.07765
Intelligent real-time MEMS sensor fusion and calibration,Dusan Nemec;Ales Janota;Marian Hrubos;Vojtech Simak,"This paper discusses an innovative adaptive heterogeneous fusion algorithm based on estimation of the mean square error of all variables used in real time processing. The algorithm is designed for a fusion between derivative and absolute sensors and is explained by the fusion of the 3-axial gyroscope, 3-axial accelerometer and 3-axial magnetometer into attitude and heading estimation. Our algorithm has similar error performance in the steady state but much faster dynamic response compared to the fixed-gain fusion algorithm. In comparison with the extended Kalman filter the proposed algorithm converges faster and takes less computational time. On the other hand, Kalman filter has smaller mean square output error in a steady state but becomes unstable if the estimated state changes too rapidly. Additionally, the noisy fusion deviation can be used in the process of calibration. The paper proposes and explains a real-time calibration method based on machine learning working in the online mode during run-time. This allows compensation of sensor thermal drift right in the sensors working environment without need of re-calibration in the laboratory. △ Less","26 January, 2017",https://arxiv.org/pdf/1701.07594
"Security and Privacy of performing Data Analytics in the cloud - A three-way handshake of Technology, Policy, and Management",Nidhi Rastogi;Marie Joan Kristine Gloria;James Hendler,"Cloud platform came into existence primarily to accelerate IT delivery and to promote innovation. To this point, it has performed largely well to the expectations of technologists, businesses and customers. The service aspect of this technology has paved the road for a faster set up of infrastructure and related goals for both startups and established organizations. This has further led to quicker delivery of many user-friendly applications to the market while proving to be a commercially viable option to companies with limited resources. On the technology front, the creation and adoption of this ecosystem has allowed easy collection of massive data from various sources at one place, where the place is sometimes referred as just the cloud. Efficient data mining can be performed on raw data to extract potentially useful information, which was not possible at this scale before. Targeted advertising is a common example that can help businesses. Despite these promising offerings, concerns around security and privacy of user information suppressed wider acceptance and an all-encompassing deployment of the cloud platform. In this paper, we discuss security and privacy concerns that occur due to data exchanging hands between a cloud servicer provider (CSP) and the primary cloud user - the data collector, from the content generator. We offer solutions that encompass technology, policy and sound management of the cloud service asserting that this approach has the potential to provide a holistic solution. △ Less","24 January, 2017",https://arxiv.org/pdf/1701.06828
An Upper Bound to Zero-Delay Rate Distortion via Kalman Filtering for Vector Gaussian Sources,Photios A. Stavrou;Jan Ostergaard;Charalambos D. Charalambous;Milan Derpich,"We deal with zero-delay source coding of a vector Gaussian autoregressive (AR) source subject to an average mean squared error (MSE) fidelity criterion. Toward this end, we consider the nonanticipative rate distortion function (NRDF) which is a lower bound to the causal and zero-delay rate distortion function (RDF). We use the realization scheme with feedback proposed in [1] to model the corresponding optimal ""test-channel"" of the NRDF, when considering vector Gaussian AR(1) sources subject to an average MSE distortion. We give conditions on the vector Gaussian AR(1) source to ensure asymptotic stationarity of the realization scheme (bounded performance). Then, we encode the vector innovations due to Kalman filtering via lattice quantization with subtractive dither and memoryless entropy coding. This coding scheme provides a tight upper bound to the zero-delay Gaussian RDF. We extend this result to vector Gaussian AR sources of any finite order. Further, we show that for infinite dimensional vector Gaussian AR sources of any finite order, the NRDF coincides with the zero-delay RDF. Our theoretical framework is corroborated with a simulation example. △ Less","21 August, 2017",https://arxiv.org/pdf/1701.06368
Security of Electronic Payment Systems: A Comprehensive Survey,Siamak Solat,"This comprehensive survey deliberated over the security of electronic payment systems. In our research, we focused on either dominant systems or new attempts and innovations to improve the level of security of the electronic payment systems. This survey consists of the Card-present (CP) transactions and a review of its dominant system i.e. EMV including several researches at Cambridge university to designate variant types of attacks against this standard which demonstrates lack of a secure ""offline"" authentication method that is one of the main purpose of using the smart cards instead of magnetic stripe cards which are not able to participate in authentication process, the evaluation of the EMV migration from RSA cryptosystem to ECC based cryptosystem 3. The evaluation of the Card-not-present transactions approaches including 3D Secure, 3D SET, SET/EMV and EMV/CAP, the impact of concept of Tokenization and the role of Blind Signatures schemes in electronic cash and E-payment systems, use of quantum key distribution (QKD) in electronic payment systems to achieve unconditional security rather than only computational assurance of the security level by using traditional cryptography, the evaluation of Near Field Communication (NFC) and the contactless payment systems such as Google wallet, Android Pay and Apple Pay, the assessment of the electronic currency and peer to peer payment systems such as Bitcoin. The criterion of our survey for the measurement and the judgment about the quality of the security in electronic payment systems was this quote: ""The security of a system is only as strong as its weakest link"" △ Less","17 January, 2017",https://arxiv.org/pdf/1701.04556
Towards prediction of rapid intensification in tropical cyclones with recurrent neural networks,Rohitash Chandra,"The problem where a tropical cyclone intensifies dramatically within a short period of time is known as rapid intensification. This has been one of the major challenges for tropical weather forecasting. Recurrent neural networks have been promising for time series problems which makes them appropriate for rapid intensification. In this paper, recurrent neural networks are used to predict rapid intensification cases of tropical cyclones from the South Pacific and South Indian Ocean regions. A class imbalanced problem is encountered which makes it very challenging to achieve promising performance. A simple strategy was proposed to include more positive cases for detection where the false positive rate was slightly improved. The limitations of building an efficient system remains due to the challenges of addressing the class imbalance problem encountered for rapid intensification prediction. This motivates further research in using innovative machine learning methods. △ Less","16 January, 2017",https://arxiv.org/pdf/1701.04518
Automatic Spatial Context-Sensitive Cloud/Cloud-Shadow Detection in Multi-Source Multi-Spectral Earth Observation Images: AutoCloud+,Andrea Baraldi,"The proposed Earth observation (EO) based value adding system (EO VAS), hereafter identified as AutoCloud+, consists of an innovative EO image understanding system (EO IUS) design and implementation capable of automatic spatial context sensitive cloud/cloud shadow detection in multi source multi spectral (MS) EO imagery, whether or not radiometrically calibrated, acquired by multiple platforms, either spaceborne or airborne, including unmanned aerial vehicles (UAVs). It is worth mentioning that the same EO IUS architecture is suitable for a large variety of EO based value adding products and services, including: (i) low level image enhancement applications, such as automatic MS image topographic correction, co registration, mosaicking and compositing, (ii) high level MS image land cover (LC) and LC change (LCC) classification and (iii) content based image storage/retrieval in massive multi source EO image databases (big data mining). △ Less","16 January, 2017",https://arxiv.org/pdf/1701.04256
Field-aware Factorization Machines in a Real-world Online Advertising System,Yuchin Juan;Damien Lefortier;Olivier Chapelle,"Predicting user response is one of the core machine learning tasks in computational advertising. Field-aware Factorization Machines (FFM) have recently been established as a state-of-the-art method for that problem and in particular won two Kaggle challenges. This paper presents some results from implementing this method in a production system that predicts click-through and conversion rates for display advertising and shows that this method it is not only effective to win challenges but is also valuable in a real-world prediction system. We also discuss some specific challenges and solutions to reduce the training time, namely the use of an innovative seeding algorithm and a distributed learning mechanism. △ Less","23 February, 2017",https://arxiv.org/pdf/1701.04099
Balancing Novelty and Salience: Adaptive Learning to Rank Entities for Timeline Summarization of High-impact Events,Tuan Tran;Claudia Niederée;Nattiya Kanhabua;Ujwal Gadiraju;Avishek Anand,"Long-running, high-impact events such as the Boston Marathon bombing often develop through many stages and involve a large number of entities in their unfolding. Timeline summarization of an event by key sentences eases story digestion, but does not distinguish between what a user remembers and what she might want to re-check. In this work, we present a novel approach for timeline summarization of high-impact events, which uses entities instead of sentences for summarizing the event at each individual point in time. Such entity summaries can serve as both (1) important memory cues in a retrospective event consideration and (2) pointers for personalized event exploration. In order to automatically create such summaries, it is crucial to identify the ""right"" entities for inclusion. We propose to learn a ranking function for entities, with a dynamically adapted trade-off between the in-document salience of entities and the informativeness of entities across documents, i.e., the level of new information associated with an entity for a time point under consideration. Furthermore, for capturing collective attention for an entity we use an innovative soft labeling approach based on Wikipedia. Our experiments on a real large news datasets confirm the effectiveness of the proposed methods. △ Less","14 January, 2017",https://arxiv.org/pdf/1701.03947
Mobile Robotic Fabrication at 1:1 scale: the In situ Fabricator,Markus Giftthaler;Timothy Sandy;Kathrin Dörfler;Ian Brooks;Mark Buckingham;Gonzalo Rey;Matthias Kohler;Fabio Gramazio;Jonas Buchli,"This paper presents the concept of an In situ Fabricator, a mobile robot intended for on-site manufacturing, assembly and digital fabrication. We present an overview of a prototype system, its capabilities, and highlight the importance of high-performance control, estimation and planning algorithms for achieving desired construction goals. Next, we detail on two architectural application scenarios: first, building a full-size undulating brick wall, which required a number of repositioning and autonomous localisation manoeuvres. Second, the Mesh Mould concrete process, which shows that an In situ Fabricator in combination with an innovative digital fabrication tool can be used to enable completely novel building technologies. Subsequently, important limitations and disadvantages of our approach are discussed. Based on that, we identify the need for a new type of robotic actuator, which facilitates the design of novel full-scale construction robots. We provide brief insight into the development of this actuator and conclude the paper with an outlook on the next-generation In situ Fabricator, which is currently under development. △ Less","13 January, 2017",https://arxiv.org/pdf/1701.03573
Digital Advertising Traffic Operation: Flow Management Analysis,Massimiliano Dal Mas,"In a Web Advertising Traffic Operation the Trafficking Routing Problem (TRP) consists in scheduling the management of Web Advertising (Adv) campaign between Trafficking campaigns in the most efficient way to oversee and manage relationship with partners and internal teams, managing expectations through integration and post-launch in order to ensure success for every stakeholders involved. For our own interest we did that independent research projects also through specific innovative tasks validate towards average working time declared on ""specification required"" by the main worldwide industry leading Advertising Agency. We present a Mixed Integer Linear Programming (MILP) formulation for end-to-end management of campaign workflow along a predetermined path and generalize it to include alternative path to oversee and manage detail-oriented relationship with partners and internal teams to achieve the goals above mentioned. To meet clients' KPIs, we consider an objective function that includes the punctuality indicators (the average waiting time and completion times) but also the main punctuality indicators (the average delay and the on time performance). Then we investigate their analytical relationships in the advertising domain through experiments based on real data from a Traffic Office. We show that the classic punctuality indicators are in contradiction with the task of reducing waiting times. We propose new indicators used for a synthesize analysis on projects or process changes for the wider team that are more sustainable, but also more relevant for stakeholders. We also show that the flow of a campaign (adv-ways) is the main bottleneck of a Traffic Office and that alternate paths cannot improve the performance indicators. △ Less","11 January, 2017",https://arxiv.org/pdf/1701.02729
Distributed Estimation of Dynamic Fields over Multi-agent Networks,Subhro Das;José M. F. Moura,"This work presents distributed algorithms for estimation of time-varying random fields over multi-agent/sensor networks. A network of sensors makes sparse and noisy local measurements of the dynamic field. Each sensor aims to obtain unbiased distributed estimates of the entire field with bounded mean-squared error (MSE) based on its own local observations and its neighbors' estimates. This work develops three novel distributed estimators: Pseudo-Innovations Kalman Filter (PIKF), Distributed Information Kalman Filter (DIKF) and Consensus+Innovations Kalman Filter (CIKF). We design the gain matrices such that the estimators achieve unbiased estimates with bounded MSE under minimal assumptions on the local observation and network communication models. This work establishes trade-offs between these three distributed estimators and demonstrates how they outperform existing solutions. We validate our results through extensive numerical evaluations. △ Less","10 January, 2017",https://arxiv.org/pdf/1701.02710
WiLiTV: A Low-Cost Wireless Framework for Live TV Services,Rajeev Kumar;Robert S Margolies;Rittwik Jana;Yong Liu;Shivendra Panwar,"With the evolution of HDTV and Ultra HDTV, the bandwidth requirement for IP-based TV content is rapidly increasing. Consumers demand uninterrupted service with a high Quality of Experience (QoE). Service providers are constantly trying to differentiate themselves by innovating new ways of distributing content more efficiently with lower cost and higher penetration. In this work, we propose a cost-efficient wireless framework (WiLiTV) for delivering live TV services, consisting of a mix of wireless access technologies (e.g. Satellite, WiFi and LTE overlay links). In the proposed architecture, live TV content is injected into the network at a few residential locations using satellite dishes. The content is then further distributed to other homes using a house-to-house WiFi network or via an overlay LTE network. Our problem is to construct an optimal TV distribution network with the minimum number of satellite injection points, while preserving the highest QoE, for different neighborhood densities. We evaluate the framework using realistic time-varying demand patterns and a diverse set of home location data. Our study demonstrates that the architecture requires 75 - 90% fewer satellite injection points, compared to traditional architectures. Furthermore, we show that most cost savings can be obtained using simple and practical relay routing solutions. △ Less","10 January, 2017",https://arxiv.org/pdf/1701.02669
An Efficient Randomized Algorithm for Rumor Blocking in Online Social Networks,Guangmo Tong;Weili Wu;Ling Guo;Deying Li;Cong Liu;Bin Liu;Ding-Zhu Du,"Social networks allow rapid spread of ideas and innovations while the negative information can also propagate widely. When the cascades with different opinions reaching the same user, the cascade arriving first is the most likely to be taken by the user. Therefore, once misinformation or rumor is detected, a natural containment method is to introduce a positive cascade competing against the rumor. Given a budget k, the rumor blocking problem asks for k seed users to trigger the spread of the positive cascade such that the number of the users who are not influenced by rumor can be maximized. The prior works have shown that the rumor blocking problem can be approximated within a factor of (1-1/e-δ) by a classic greedy algorithm combined with Monte Carlo simulation with the running time of O(\frac{k^3mn\ln n}{δ^2}), where n and m are the number of users and edges, respectively. Unfortunately, the Monte-Carlo-simulation-based methods are extremely time consuming and the existing algorithms either trade performance guarantees for practical efficiency or vice versa. In this paper, we present a randomized algorithm which runs in O(\frac{km\ln n}{δ^2}) expected time and provides a (1-1/e-δ)-approximation with a high probability. The experimentally results on both the real-world and synthetic social networks have shown that the proposed randomized rumor blocking algorithm is much more efficient than the state-of-the-art method and it is able to find the seed nodes which are effective in limiting the spread of rumor. △ Less","8 November, 2017",https://arxiv.org/pdf/1701.02368
"Multi-spectral Image Panchromatic Sharpening, Outcome and Process Quality Assessment Protocol",Andrea Baraldi;Francesca Despini;Sergio Teggi,"Multispectral (MS) image panchromatic (PAN) sharpening algorithms proposed to the remote sensing community are ever increasing in number and variety. Their aim is to sharpen a coarse spatial resolution MS image with a fine spatial resolution PAN image acquired simultaneously by a spaceborne or airborne Earth observation (EO) optical imaging sensor pair. Unfortunately, to date, no standard evaluation procedure for MS image PAN sharpening outcome and process is community agreed upon, in contrast with the Quality Assurance Framework for Earth Observation (QA4EO) guidelines proposed by the intergovernmental Group on Earth Observations (GEO). In general, process is easier to measure, outcome is more important. The original contribution of the present study is fourfold. First, existing procedures for quantitative quality assessment (Q2A) of the (sole) PAN sharpened MS product are critically reviewed. Their conceptual and implementation drawbacks are highlighted to be overcome for quality improvement. Second, a novel (to the best of these authors' knowledge, the first) protocol for Q2A of MS image PAN sharpening product and process is designed, implemented and validated by independent means. Third, within this protocol, an innovative categorization of spectral and spatial image quality indicators and metrics is presented. Fourth, according to this new taxonomy, an original third order isotropic multi scale gray level co occurrence matrix (TIMS GLCM) calculator and a TIMS GLCM texture feature extractor are proposed to replace popular second order GLCMs. △ Less","8 January, 2017",https://arxiv.org/pdf/1701.01942
Multi-Objective Software Suite of Two-Dimensional Shape Descriptors for Object-Based Image Analysis,Andrea Baraldi;João V. B. Soares,"In recent years two sets of planar (2D) shape attributes, provided with an intuitive physical meaning, were proposed to the remote sensing community by, respectively, Nagao & Matsuyama and Shackelford & Davis in their seminal works on the increasingly popular geographic object based image analysis (GEOBIA) paradigm. These two published sets of intuitive geometric features were selected as initial conditions by the present R&D software project, whose multi-objective goal was to accomplish: (i) a minimally dependent and maximally informative design (knowledge/information representation) of a general purpose, user and application independent dictionary of 2D shape terms provided with a physical meaning intuitive to understand by human end users and (ii) an effective (accurate, scale invariant, easy to use) and efficient implementation of 2D shape descriptors. To comply with the Quality Assurance Framework for Earth Observation guidelines, the proposed suite of geometric functions is validated by means of a novel quantitative quality assurance policy, centered on inter feature dependence (causality) assessment. This innovative multivariate feature validation strategy is alternative to traditional feature selection procedures based on either inductive data learning classification accuracy estimation, which is inherently case specific, or cross correlation estimation, because statistical cross correlation does not imply causation. The project deliverable is an original general purpose software suite of seven validated off the shelf 2D shape descriptors intuitive to use. Alternative to existing commercial or open source software libraries of tens of planar shape functions whose informativeness remains unknown, it is eligible for use in (GE)OBIA systems in operating mode, expected to mimic human reasoning based on a convergence of evidence approach. △ Less","2 February, 2017",https://arxiv.org/pdf/1701.01941
Alternating Minimization for Hybrid Precoding in Multiuser OFDM mmWave Systems,Xianghao Yu;Jun Zhang;Khaled B. Letaief,"Hybrid precoding is a cost-effective approach to support directional transmissions for millimeter wave (mmWave) communications. While existing works on hybrid precoding mainly focus on single-user single-carrier transmission, in practice multicarrier transmission is needed to combat the much increased bandwidth, and multiuser MIMO can provide additional spatial multiplexing gains. In this paper, we propose a new hybrid precoding structure for multiuser OFDM mmWave systems, which greatly simplifies the hybrid precoder design and is able to approach the performance of the fully digital precoder. In particular, two groups of phase shifters are combined to map the signals from radio frequency (RF) chains to antennas. Then an effective hybrid precoding algorithm based on alternating minimization (AltMin) is proposed, which will alternately optimize the digital and analog precoders. A major algorithmic innovation is a LASSO formulation for the analog precoder, which yields computationally efficient algorithms. Simulation results will show the performance gain of the proposed algorithm. Moreover, it will reveal that canceling the interuser interference is critical in multiuser OFDM hybrid precoding systems. △ Less","6 January, 2017",https://arxiv.org/pdf/1701.01567
Stochastic Planning and Lifted Inference,Roni Khardon;Scott Sanner,"Lifted probabilistic inference (Poole, 2003) and symbolic dynamic programming for lifted stochastic planning (Boutilier et al, 2001) were introduced around the same time as algorithmic efforts to use abstraction in stochastic systems. Over the years, these ideas evolved into two distinct lines of research, each supported by a rich literature. Lifted probabilistic inference focused on efficient arithmetic operations on template-based graphical models under a finite domain assumption while symbolic dynamic programming focused on supporting sequential decision-making in rich quantified logical action models and on open domain reasoning. Given their common motivation but different focal points, both lines of research have yielded highly complementary innovations. In this chapter, we aim to help close the gap between these two research areas by providing an overview of lifted stochastic planning from the perspective of probabilistic inference, showing strong connections to other chapters in this book. This also allows us to define Generalized Lifted Inference as a paradigm that unifies these areas and elucidates open problems for future research that can benefit both lifted inference and stochastic planning. △ Less","4 January, 2017",https://arxiv.org/pdf/1701.01048
FastMask: Segment Multi-scale Object Candidates in One Shot,Hexiang Hu;Shiyi Lan;Yuning Jiang;Zhimin Cao;Fei Sha,"Objects appear to scale differently in natural images. This fact requires methods dealing with object-centric tasks (e.g. object proposal) to have robust performance over variances in object scales. In the paper, we present a novel segment proposal framework, namely FastMask, which takes advantage of hierarchical features in deep convolutional neural networks to segment multi-scale objects in one shot. Innovatively, we adapt segment proposal network into three different functional components (body, neck and head). We further propose a weight-shared residual neck module as well as a scale-tolerant attentional head module for efficient one-shot inference. On MS COCO benchmark, the proposed FastMask outperforms all state-of-the-art segment proposal methods in average recall being 2~5 times faster. Moreover, with a slight trade-off in accuracy, FastMask can segment objects in near real time (~13 fps) with 800*600 resolution images, demonstrating its potential in practical applications. Our implementation is available on https://github.com/voidrank/FastMask. △ Less","11 April, 2017",https://arxiv.org/pdf/1612.08843
Strong Federations: An Interoperable Blockchain Solution to Centralized Third-Party Risks,Johnny Dilley;Andrew Poelstra;Jonathan Wilkins;Marta Piekarska;Ben Gorlick;Mark Friedenbach,"Bitcoin, the first peer-to-peer electronic cash system, opened the door to permissionless, private, and trustless transactions. Attempts to repurpose Bitcoin's underlying blockchain technology have run up against fundamental limitations to privacy, faithful execution, and transaction finality. We introduce \emph{Strong Federations}: publicly verifiable, Byzantine-robust transaction networks that facilitate movement of any asset between disparate markets, without requiring third-party trust. \emph{Strong Federations} enable commercial privacy, with support for transactions where asset types and amounts are opaque, while remaining publicly verifiable. As in Bitcoin, execution fidelity is cryptographically enforced; however, \emph{Strong Federations} significantly lower capital requirements for market participants by reducing transaction latency and improving interoperability. To show how this innovative solution can be applied today, we describe \emph{\liquid}: the first implementation of \emph{Strong Federations} deployed in a Financial Market. △ Less","30 January, 2017",https://arxiv.org/pdf/1612.05491
In-network Compression for Multiterminal Cascade MIMO Systems,Inaki Estella Aguerri;Abdellatif Zaidi,"We study the problem of receive beamforming in uplink cascade multiple-input multiple-output (MIMO) systems as an instance of that of cascade multiterminal source coding for lossy function computation. Using this connection, we develop two coding schemes for the second and show that their application leads to beamforming schemes for the first. In the first coding scheme, each terminal in the cascade sends a description of the source that it observes; the decoder reconstructs all sources, lossily, and then computes an estimate of the desired function. This scheme improves upon standard routing in that every terminal only compresses the innovation of its source w.r.t. the descriptions that are sent by the previous terminals in the cascade. In the second scheme, the desired function is computed gradually in the cascade network, and each terminal sends a finer description of it. In the context of uplink cascade MIMO systems, the application of these two schemes leads to centralized receive-beamforming and distributed receive-beamforming, respectively. Numerical results illustrate the performance of the proposed methods and show that they outperform standard routing. △ Less","6 March, 2017",https://arxiv.org/pdf/1612.01282
Dynamic Attention-controlled Cascaded Shape Regression Exploiting Training Data Augmentation and Fuzzy-set Sample Weighting,Zhen-Hua Feng;Josef Kittler;William Christmas;Patrik Huber;Xiao-Jun Wu,"We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic Attention-Controlled CSR (DAC-CSR), for robust facial landmark detection on unconstrained faces. Our DAC-CSR divides facial landmark detection into three cascaded sub-tasks: face bounding box refinement, general CSR and attention-controlled CSR. The first two stages refine initial face bounding boxes and output intermediate facial landmarks. Then, an online dynamic model selection method is used to choose appropriate domain-specific CSRs for further landmark refinement. The key innovation of our DAC-CSR is the fault-tolerant mechanism, using fuzzy set sample weighting for attention-controlled domain-specific model training. Moreover, we advocate data augmentation with a simple but effective 2D profile face generator, and context-aware feature extraction for better facial feature representation. Experimental results obtained on challenging datasets demonstrate the merits of our DAC-CSR over the state-of-the-art. △ Less","4 April, 2017",https://arxiv.org/pdf/1611.05396
"Towards Interconnected Virtual Reality: Opportunities, Challenges and Enablers",Ejder Baştuğ;Mehdi Bennis;Muriel Médard;Mérouane Debbah,"Just recently, the concept of augmented and virtual reality (AR/VR) over wireless has taken the entire 5G ecosystem by storm spurring an unprecedented interest from both academia, industry and others. Yet, the success of an immersive VR experience hinges on solving a plethora of grand challenges cutting across multiple disciplines. This article underscores the importance of VR technology as a disruptive use case of 5G (and beyond) harnessing the latest development of storage/memory, fog/edge computing, computer vision, artificial intelligence and others. In particular, the main requirements of wireless interconnected VR are described followed by a selection of key enablers, then, research avenues and their underlying grand challenges are presented. Furthermore, we examine three VR case studies and provide numerical results under various storage, computing and network configurations. Finally, this article exposes the limitations of current networks and makes the case for more theory, and innovations to spearhead VR for the masses. △ Less","24 March, 2017",https://arxiv.org/pdf/1611.05356
Sample Efficient Actor-Critic with Experience Replay,Ziyu Wang;Victor Bapst;Nicolas Heess;Volodymyr Mnih;Remi Munos;Koray Kavukcuoglu;Nando de Freitas,"This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method. △ Less","10 July, 2017",https://arxiv.org/pdf/1611.01224
Random Forests versus Neural Networks - What's Best for Camera Localization?,Daniela Massiceti;Alexander Krull;Eric Brachmann;Carsten Rother;Philip H. S. Torr,"This work addresses the task of camera localization in a known 3D scene given a single input RGB image. State-of-the-art approaches accomplish this in two steps: firstly, regressing for every pixel in the image its 3D scene coordinate and subsequently, using these coordinates to estimate the final 6D camera pose via RANSAC. To solve the first step, Random Forests (RFs) are typically used. On the other hand, Neural Networks (NNs) reign in many dense regression tasks, but are not test-time efficient. We ask the question: which of the two is best for camera localization? To address this, we make two method contributions: (1) a test-time efficient NN architecture which we term a ForestNet that is derived and initialized from a RF, and (2) a new fully-differentiable robust averaging technique for regression ensembles which can be trained end-to-end with a NN. Our experimental findings show that for scene coordinate regression, traditional NN architectures are superior to test-time efficient RFs and ForestNets, however, this does not translate to final 6D camera pose accuracy where RFs and ForestNets perform slightly better. To summarize, our best method, a ForestNet with a robust average, which has an equivalent fast and lightweight RF, improves over the state-of-the-art for camera localization on the 7-Scenes dataset. While this work focuses on scene coordinate regression for camera localization, our innovations may also be applied to other continuous regression tasks. △ Less","13 July, 2017",https://arxiv.org/pdf/1609.05797
Design of a Ternary Edge-Triggered D Flip-Flap-Flop for Multiple-Valued Sequential Logic,Reza Faghih Mirzaee;Niloofar Farahani,"Development of large computerized systems requires both combinational and sequential circuits. Registers and counters are two important examples of sequential circuits, which are widely used in practical applications like CPUs. The basic element of sequential logic is Flip-Flop, which stores an input value and returns two outputs (Q and Q_bar). This paper presents an innovative ternary D Flip-Flap-Flop, which offers circuit designers to customize their design by eliminating one of the outputs if it is not required. This unique feature of the new design leads to considerable power reduction in comparison with the previously presented structures. The proposed design is simulated and tested by HSPICE and 45 nm CMOS technology. △ Less","10 March, 2017",https://arxiv.org/pdf/1609.03897
Unsupervised Monocular Depth Estimation with Left-Right Consistency,Clément Godard;Oisin Mac Aodha;Gabriel J. Brostow,"Learning based methods have shown very promising results for the task of depth estimation in single images. However, most existing approaches treat depth prediction as a supervised regression problem and as a result, require vast quantities of corresponding ground truth depth data for training. Just recording quality depth data in a range of environments is a challenging problem. In this paper, we innovate beyond existing approaches, replacing the use of explicit depth data during training with easier-to-obtain binocular stereo footage. We propose a novel training objective that enables our convolutional neural network to learn to perform single image depth estimation, despite the absence of ground truth depth data. Exploiting epipolar geometry constraints, we generate disparity images by training our network with an image reconstruction loss. We show that solving for image reconstruction alone results in poor quality depth images. To overcome this problem, we propose a novel training loss that enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Our method produces state of the art results for monocular depth estimation on the KITTI driving dataset, even outperforming supervised methods that have been trained with ground truth depth. △ Less","12 April, 2017",https://arxiv.org/pdf/1609.03677
Empirically Grounded Agent-Based Models of Innovation Diffusion: A Critical Review,Haifeng Zhang;Yevgeniy Vorobeychik,"Innovation diffusion has been studied extensively in a variety of disciplines, including sociology, economics, marketing, ecology, and computer science. Traditional literature on innovation diffusion has been dominated by models of aggregate behavior and trends. However, the agent-based modeling (ABM) paradigm is gaining popularity as it captures agent heterogeneity and enables fine-grained modeling of interactions mediated by social and geographic networks. While most ABM work on innovation diffusion is theoretical, empirically grounded models are increasingly important, particularly in guiding policy decisions. We present a critical review of empirically grounded agent-based models of innovation diffusion, developing a categorization of this research based on types of agent models as well as applications. By connecting the modeling methodologies in the fields of information and innovation diffusion, we suggest that the maximum likelihood estimation framework widely used in the former is a promising paradigm for calibration of agent-based models for innovation diffusion. Although many advances have been made to standardize ABM methodology, we identify four major issues in model calibration and validation, and suggest potential solutions. △ Less","25 May, 2017",https://arxiv.org/pdf/1608.08517
Diffusion in Networks and the Unexpected Virtue of Burstiness,Mohammad Akbarpour;Matthew O. Jackson,"Whether an idea, information, infection, or innovation diffuses throughout a society depends not only on the structure of the network of interactions, but also on the timing of those interactions. Recent studies have shown that diffusion can fail on a network in which people are only active in ""bursts"", active for a while and then silent for a while, but diffusion could succeed on the same network if people were active in a more random Poisson manner. Those studies generally consider models in which nodes are active according to the same random timing process and then ask which timing is optimal. In reality, people differ widely in their activity patterns -- some are bursty and others are not. Here we show that, if people differ in their activity patterns, bursty behavior does not always hurt the diffusion, and in fact having some (but not all) of the population be bursty significantly helps diffusion. We prove that maximizing diffusion requires heterogeneous activity patterns across agents, and the overall maximizing pattern of agents' activity times does not involve any Poisson behavior. △ Less","18 December, 2017",https://arxiv.org/pdf/1608.07899
Urban Pulse: Capturing the Rhythm of Cities,Fabio Miranda;Harish Doraiswamy;Marcos Lage;Kai Zhao;Bruno Gonçalves;Luc Wilson;Mondrian Hsieh;Cláudio T. Silva,"Cities are inherently dynamic. Interesting patterns of behavior typically manifest at several key areas of a city over multiple temporal resolutions. Studying these patterns can greatly help a variety of experts ranging from city planners and architects to human behavioral experts. Recent technological innovations have enabled the collection of enormous amounts of data that can help in these studies. However, techniques using these data sets typically focus on understanding the data in the context of the city, thus failing to capture the dynamic aspects of the city. The goal of this work is to instead understand the city in the context of multiple urban data sets. To do so, we define the concept of an ""urban pulse"" which captures the spatio-temporal activity in a city across multiple temporal resolutions. The prominent pulses in a city are obtained using the topology of the data sets, and are characterized as a set of beats. The beats are then used to analyze and compare different pulses. We also design a visual exploration framework that allows users to explore the pulses within and across multiple cities under different conditions. Finally, we present three case studies carried out by experts from two different domains that demonstrate the utility of our framework. △ Less","29 December, 2017",https://arxiv.org/pdf/1608.06949
Evaluating Spintronic Devices Using The Modular Approach,Samiran Ganguly;Kerem Yunus Camsari;Supriyo Datta,"Over the past decade a large family of spintronic devices have been proposed as candidates for replacing CMOS for future digital logic circuits. Using the recently developed Modular Approach framework, we investigate and identify the physical bottlenecks and engineering challenges facing current spintronic devices. We then evaluate how systematic advancements in material properties and device design innovations impact the performance of spintronic devices, as a possible continuation of Moore's Law, even though some of these projections are speculative and may require technological breakthroughs. Lastly, we illustrate the use of the Modular Approach as an exploratory tool for probabilistic networks, using superparamagnetic magnets as building blocks for such networks. These building blocks leverage the inherent physics of stochastic spin-torque switching and could provide ultra-compact and efficient hardware for beyond-Boolean computational paradigms. △ Less","23 March, 2017",https://arxiv.org/pdf/1608.03681
The blockchain: a new framework for robotic swarm systems,Eduardo Castelló Ferrer,"Swarms of robots will revolutionize many industrial applications, from targeted material delivery to precision farming. However, several of the heterogeneous characteristics that make them ideal for certain future applications --- robot autonomy, decentralized control, collective emergent behavior, etc. --- hinder the evolution of the technology from academic institutions to real-world problems. Blockchain, an emerging technology originated in the Bitcoin field, demonstrates that by combining peer-to-peer networks with cryptographic algorithms a group of agents can reach an agreement on a particular state of affairs and record that agreement without the need for a controlling authority. The combination of blockchain with other distributed systems, such as robotic swarm systems, can provide the necessary capabilities to make robotic swarm operations more secure, autonomous, flexible and even profitable. This work explains how blockchain technology can provide innovative solutions to four emergent issues in the swarm robotics research field. New security, decision making, behavior differentiation and business models for swarm robotic systems are described by providing case scenarios and examples. Finally, limitations and possible future problems that arise from the combination of these two technologies are described. △ Less","24 June, 2017",https://arxiv.org/pdf/1608.00695
"""Open Innovation"" and ""Triple Helix"" Models of Innovation: Can Synergy in Innovation Systems Be Measured?",Loet Leydesdorff;Inga Ivanova,"The model of ""Open Innovations"" (OI) can be compared with the ""Triple Helix of University-Industry-Government Relations"" (TH) as attempts to find surplus value in bringing industrial innovation closer to public R&D. Whereas the firm is central in the model of OI, the TH adds multi-centeredness: in addition to firms, universities and (e.g., regional) governments can take leading roles in innovation eco-systems. In addition to the (transversal) technology transfer at each moment of time, one can focus on the dynamics in the feedback loops. Under specifiable conditions, feedback loops can be turned into feedforward ones that drive innovation eco-systems towards self-organization and the auto-catalytic generation of new options. The generation of options can be more important than historical realizations (""best practices"") for the longer-term viability of knowledge-based innovation systems. A system without sufficient options, for example, is locked-in. The generation of redundancy -- the Triple Helix indicator -- can be used as a measure of unrealized but technologically feasible options given a historical configuration. Different coordination mechanisms (markets, policies, knowledge) provide different perspectives on the same information and thus generate redundancy. Increased redundancy not only stimulates innovation in an eco-system by reducing the prevailing uncertainty; it also enhances the synergy in and innovativeness of an innovation system. △ Less","28 July, 2017",https://arxiv.org/pdf/1607.08090
Nonlinear Self-Interference Cancellation for Full-Duplex Radios: From Link- and System-Level Performance Perspectives,Min Soo Sim;MinKeun Chung;Dongkyu Kim;Jaehoon Chung;Dong Ku Kim;Chan-Byoung Chae,"One of the promising technologies for LTE Evolution is full-duplex radio, an innovation is expected to double the spectral efficiency. To realize full-duplex in practice, the main challenge is overcoming self-interference, and to do so, researchers have developed self-interference cancellation techniques. Since most wireless transceivers use power amplifiers, especially in cellular systems, researchers have revealed the importance of nonlinear self-interference cancellation. In this article, we first explore several nonlinear digital self-interference cancellation techniques. We then propose a low complexity pre-calibration-based nonlinear digital self-interference cancellation technique. Next we discuss issues about reference signal allocation and the overhead of each technique. For performance evaluations, we carry out extensive measurements through a real-time prototype and link-/system-level simulations. For link-level analysis, we measure the amount of cancelled self-interference for each technique. We also evaluate system-level performances through 3D ray-tracing-based simulations. Numerical results confirm the significant performance improvement over a half-duplex system even in interference-limited indoor environments. △ Less","14 February, 2017",https://arxiv.org/pdf/1607.01912
FSO-based Vertical Backhaul/Fronthaul Framework for 5G+ Wireless Networks,Mohamed Alzenad;Muhammad Zeeshan Shakir;Halim Yanikomeroglu;Mohamed-Slim Alouini,"The presence of a super high rate, but also cost-efficient, easy-to-deploy, and scalable, backhaul/fronthaul framework is essential in the upcoming fifth-generation (5G) wireless networks \& beyond. Motivated by the mounting interest in the unmanned flying platforms of various types including unmanned aerial vehicles (UAVs), drones, balloons, and high-altitude/medium-altitude/low-altitude platforms (HAPs/MAPs/LAPs), which we refer to as the networked flying platforms (NFPs), for providing communications services and the recent advances in free-space optics (FSO), this article investigates the feasibility of a novel vertical backhaul/fronthaul framework where the NFPs transport the backhaul/fronthaul traffic between the access and core networks via point-to-point FSO links. The performance of the proposed innovative approach is investigated under different weather conditions and a broad range of system parameters. Simulation results demonstrate that the FSO-based vertical backhaul/fronthaul framework can offer data rates higher than the baseline alternatives, and thus can be considered as a promising solution to the emerging backhaul/fronthaul requirements of the 5G+ wireless networks, particularly in the presence of ultra-dense heterogeneous small cells. The paper also presents the challenges that accompany such a novel framework and provides some key ideas towards overcoming these challenges. △ Less","3 May, 2017",https://arxiv.org/pdf/1607.01472
Data Sketching for Large-Scale Kalman Filtering,Dimitris Berberidis;Georgios B. Giannakis,"In an age of exponentially increasing data generation, performing inference tasks by utilizing the available information in its entirety is not always an affordable option. The present paper puts forth approaches to render tracking of large-scale dynamic processes via a Kalman filter affordable, by processing a reduced number of data. Three distinct methods are introduced for reducing the number of data involved in the correction step of the filter. Towards this goal, the first two methods employ random projections and innovation-based censoring to effect dimensionality reduction and measurement selection respectively. The third method achieves reduced complexity by leveraging sequential processing of observations and selecting a few informative updates based on an information-theoretic metric. Simulations on synthetic data, compare the proposed methods with competing alternatives, and corroborate their efficacy in terms of estimation accuracy over complexity reduction. Finally, monitoring large networks is considered as an application domain, with the proposed methods tested on Kronecker graphs to evaluate their efficiency in tracking traffic matrices and time-varying link costs. △ Less","6 January, 2017",https://arxiv.org/pdf/1606.08136
Sampling and Reconstruction of Sparse Signals on Circulant Graphs - An Introduction to Graph-FRI,Madeleine S. Kotzagiannidis;Pier Luigi Dragotti,"With the objective of employing graphs toward a more generalized theory of signal processing, we present a novel sampling framework for (wavelet-)sparse signals defined on circulant graphs which extends basic properties of Finite Rate of Innovation (FRI) theory to the graph domain, and can be applied to arbitrary graphs via suitable approximation schemes. At its core, the introduced Graph-FRI-framework states that any K-sparse signal on the vertices of a circulant graph can be perfectly reconstructed from its dimensionality-reduced representation in the graph spectral domain, the Graph Fourier Transform (GFT), of minimum size 2K. By leveraging the recently developed theory of e-splines and e-spline wavelets on graphs, one can decompose this graph spectral transformation into the multiresolution low-pass filtering operation with a graph e-spline filter, and subsequent transformation to the spectral graph domain; this allows to infer a distinct sampling pattern, and, ultimately, the structure of an associated coarsened graph, which preserves essential properties of the original, including circularity and, where applicable, the graph generating set. △ Less","20 October, 2017",https://arxiv.org/pdf/1606.08085
Sketching for Large-Scale Learning of Mixture Models,Nicolas Keriven;Anthony Bourrier;Rémi Gribonval;Patrick Pérez,"Learning parameters from voluminous data can be prohibitive in terms of memory and computational requirements. We propose a ""compressive learning"" framework where we estimate model parameters from a sketch of the training data. This sketch is a collection of generalized moments of the underlying probability distribution of the data. It can be computed in a single pass on the training set, and is easily computable on streams or distributed datasets. The proposed framework shares similarities with compressive sensing, which aims at drastically reducing the dimension of high-dimensional signals while preserving the ability to reconstruct them. To perform the estimation task, we derive an iterative algorithm analogous to sparse reconstruction algorithms in the context of linear inverse problems. We exemplify our framework with the compressive estimation of a Gaussian Mixture Model (GMM), providing heuristics on the choice of the sketching procedure and theoretical guarantees of reconstruction. We experimentally show on synthetic data that the proposed algorithm yields results comparable to the classical Expectation-Maximization (EM) technique while requiring significantly less memory and fewer computations when the number of database elements is large. We further demonstrate the potential of the approach on real large-scale data (over 10 8 training samples) for the task of model-based speaker verification. Finally, we draw some connections between the proposed framework and approximate Hilbert space embedding of probability distributions using random features. We show that the proposed sketching operator can be seen as an innovative method to design translation-invariant kernels adapted to the analysis of GMMs. We also use this theoretical framework to derive information preservation guarantees, in the spirit of infinite-dimensional compressive sensing. △ Less","5 May, 2017",https://arxiv.org/pdf/1606.02838
RCFD: A Novel Channel Access Scheme for Full-Duplex Wireless Networks Based on Contention in Time and Frequency Domains,Michele Luvisotto;Alireza Sadeghi;Farshad Lahouti;Stefano Vitturi;Michele Zorzi,"In the last years, the advancements in signal processing and integrated circuits technology allowed several research groups to develop working prototypes of in-band full-duplex wireless systems. The introduction of such a revolutionary concept is promising in terms of increasing network performance, but at the same time poses several new challenges, especially at the MAC layer. Consequently, innovative channel access strategies are needed to exploit the opportunities provided by full-duplex while dealing with the increased complexity derived from its adoption. In this direction, this paper proposes RTS/CTS in the Frequency Domain (RCFD), a MAC layer scheme for full-duplex ad hoc wireless networks, based on the idea of time-frequency channel contention. According to this approach, different OFDM subcarriers are used to coordinate how nodes access the shared medium. The proposed scheme leads to efficient transmission scheduling with the result of avoiding collisions and exploiting full-duplex opportunities. The considerable performance improvements with respect to standard and state-of-the-art MAC protocols for wireless networks are highlighted through both theoretical analysis and network simulations. △ Less","15 September, 2017",https://arxiv.org/pdf/1606.01038
Stop-and-Stare: Optimal Sampling Algorithms for Viral Marketing in Billion-scale Networks,Hung T. Nguyen;My T. Thai;Thang N. Dinh,"Influence Maximization (IM), that seeks a small set of key users who spread the influence widely into the network, is a core problem in multiple domains. It finds applications in viral marketing, epidemic control, and assessing cascading failures within complex systems. Despite the huge amount of effort, IM in billion-scale networks such as Facebook, Twitter, and World Wide Web has not been satisfactorily solved. Even the state-of-the-art methods such as TIM+ and IMM may take days on those networks. In this paper, we propose SSA and D-SSA, two novel sampling frameworks for IM-based viral marketing problems. SSA and D-SSA are up to 1200 times faster than the SIGMOD'15 best method, IMM, while providing the same (1-1/e-ε) approximation guarantee. Underlying our frameworks is an innovative Stop-and-Stare strategy in which they stop at exponential check points to verify (stare) if there is adequate statistical evidence on the solution quality. Theoretically, we prove that SSA and D-SSA are the first approximation algorithms that use (asymptotically) minimum numbers of samples, meeting strict theoretical thresholds characterized for IM. The absolute superiority of SSA and D-SSA are confirmed through extensive experiments on real network data for IM and another topic-aware viral marketing problem, named TVM. The source code is available at https://github.com/hungnt55/Stop-and-Stare △ Less","22 February, 2017",https://arxiv.org/pdf/1605.07990
Yum-me: A Personalized Nutrient-based Meal Recommender System,Longqi Yang;Cheng-Kang Hsieh;Hongjian Yang;Nicola Dell;Serge Belongie;Curtis Cole;Deborah Estrin,"Nutrient-based meal recommendations have the potential to help individuals prevent or manage conditions such as diabetes and obesity. However, learning people's food preferences and making recommendations that simultaneously appeal to their palate and satisfy nutritional expectations are challenging. Existing approaches either only learn high-level preferences or require a prolonged learning period. We propose Yum-me, a personalized nutrient-based meal recommender system designed to meet individuals' nutritional expectations, dietary restrictions, and fine-grained food preferences. Yum-me enables a simple and accurate food preference profiling procedure via a visual quiz-based user interface, and projects the learned profile into the domain of nutritionally appropriate food options to find ones that will appeal to the user. We present the design and implementation of Yum-me, and further describe and evaluate two innovative contributions. The first contriution is an open source state-of-the-art food image analysis model, named FoodDist. We demonstrate FoodDist's superior performance through careful benchmarking and discuss its applicability across a wide array of dietary applications. The second contribution is a novel online learning framework that learns food preference from item-wise and pairwise image comparisons. We evaluate the framework in a field study of 227 anonymous users and demonstrate that it outperforms other baselines by a significant margin. We further conducted an end-to-end validation of the feasibility and effectiveness of Yum-me through a 60-person user study, in which Yum-me improves the recommendation acceptance rate by 42.63%. △ Less","30 April, 2017",https://arxiv.org/pdf/1605.07722
Sampling Requirements for Stable Autoregressive Estimation,Abbas Kazemipour;Sina Miran;Piya Pal;Behtash Babadi;Min Wu,"We consider the problem of estimating the parameters of a linear univariate autoregressive model with sub-Gaussian innovations from a limited sequence of consecutive observations. Assuming that the parameters are compressible, we analyze the performance of the \ell_1-regularized least squares as well as a greedy estimator of the parameters and characterize the sampling trade-offs required for stable recovery in the non-asymptotic regime. In particular, we show that for a fixed sparsity level, stable recovery of AR parameters is possible when the number of samples scale sub-linearly with the AR order. Our results improve over existing sampling complexity requirements in AR estimation using the LASSO, when the sparsity level scales faster than the square root of the model order. We further derive sufficient conditions on the sparsity level that guarantee the minimax optimality of the \ell_1-regularized least squares estimate. Applying these techniques to simulated data as well as real-world datasets from crude oil prices and traffic speed data confirm our predicted theoretical performance gains in terms of estimation accuracy and model selection. △ Less","17 January, 2017",https://arxiv.org/pdf/1605.01436
Dynamics of beneficial epidemics,Andrew Berdahl;Christa Brelsford;Caterina De Bacco;Marion Dumas;Vanessa Ferdinand;Joshua A. Grochow;Laurent Hébert-Dufresne;Yoav Kallus;Christopher P. Kempes;Artemy Kolchinsky;Daniel B. Larremore;Eric Libby;Eleanor A. Power;Caitlin A. Stern;Brendan Tracey,"Pathogens can spread epidemically through populations. Beneficial contagions, such as viruses that enhance host survival or technological innovations that improve quality of life, also have the potential to spread epidemically. How do the dynamics of beneficial biological and social epidemics differ from those of detrimental epidemics? We investigate this question using three theoretical approaches. First, in the context of population genetics, we show that a horizontally-transmissible element that increases fitness, such as viral DNA, spreads superexponentially through a population, more quickly than a beneficial mutation. Second, in the context of behavioral epidemiology, we show that infections that cause increased connectivity lead to superexponential fixation in the population. Third, in the context of dynamic social networks, we find that preferences for increased global infection accelerate spread and produce superexponential fixation, but preferences for local assortativity halt epidemics by disconnecting the infected from the susceptible. We conclude that the dynamics of beneficial biological and social epidemics are characterized by the rapid spread of beneficial elements, which is facilitated in biological systems by horizontal transmission and in social systems by active spreading behavior of infected individuals. △ Less","17 February, 2017",https://arxiv.org/pdf/1604.02096
Interactive Tools and Tasks for the Hebrew Bible,Nicolai Winther-Nielsen,"This contribution to a special issue on ""Computer-aided processing of intertextuality"" in ancient texts will illustrate how using digital tools to interact with the Hebrew Bible offers new promising perspectives for visualizing the texts and for performing tasks in education and research. This contribution explores how the corpus of the Hebrew Bible created and maintained by the Eep Talstra Centre for Bible and Computer can support new methods for modern knowledge workers within the field of digital humanities and theology be applied to ancient texts, and how this can be envisioned as a new field of digital intertextuality. The article first describes how the corpus was used to develop the Bible Online Learner as a persuasive technology to enhance language learning with, in, and around a database that acts as the engine driving interactive tasks for learners. Intertextuality in this case is a matter of active exploration and ongoing practice. Furthermore, interactive corpus-technology has an important bearing on the task of textual criticism as a specialized area of research that depends increasingly on the availability of digital resources. Commercial solutions developed by software companies like Logos and Accordance offer a market-based intertextuality defined by the production of advanced digital resources for scholars and students as useful alternatives to often inaccessible and expensive printed versions. It is reasonable to expect that in the future interactive corpus technology will allow scholars to do innovative academic tasks in textual criticism and interpretation. We have already seen the emergence of promising tools for text categorization, analysis of translation shifts, and interpretation. Broadly speaking, interactive tools and tasks within the three areas of language learning, textual criticism, and Biblical studies illustrate a new kind of intertextuality emerging within digital humanities. △ Less","24 October, 2017",https://arxiv.org/pdf/1603.04236
From Duels to Battefields: Computing Equilibria of Blotto and Other Games,AmirMahdi Ahmadinejad;Sina Dehghani;MohammadTaghi Hajiaghayi;Brendan Lucier;Hamid Mahini;Saeed Seddighin,"We study the problem of computing Nash equilibria of zero-sum games. Many natural zero-sum games have exponentially many strategies, but highly structured payoffs. For example, in the well-studied Colonel Blotto game (introduced by Borel in 1921), players must divide a pool of troops among a set of battlefields with the goal of winning (i.e., having more troops in) a majority. The Colonel Blotto game is commonly used for analyzing a wide range of applications from the U.S presidential election, to innovative technology competitions, to advertisement, to sports. However, because of the size of the strategy space, standard methods for computing equilibria of zero-sum games fail to be computationally feasible. Indeed, despite its importance, only a few solutions for special variants of the problem are known. In this paper we show how to compute equilibria of Colonel Blotto games. Moreover, our approach takes the form of a general reduction: to find a Nash equilibrium of a zero-sum game, it suffices to design a separation oracle for the strategy polytope of any bilinear game that is payoff-equivalent. We then apply this technique to obtain the first polytime algorithms for a variety of games. In addition to Colonel Blotto, we also show how to compute equilibria in an infinite-strategy variant called the General Lotto game; this involves showing how to prune the strategy space to a finite subset before applying our reduction. We also consider the class of dueling games, first introduced by Immorlica et al. (2011). We show that our approach provably extends the class of dueling games for which equilibria can be computed: we introduce a new dueling game, the matching duel, on which prior methods fail to be computationally feasible but upon which our reduction can be applied. △ Less","20 January, 2017",https://arxiv.org/pdf/1603.00119
Recursive Distributed Detection for Composite Hypothesis Testing: Nonlinear Observation Models in Additive Gaussian Noise,Anit Kumar Sahu;Soummya Kar,"This paper studies recursive composite hypothesis testing in a network of sparsely connected agents. The network objective is to test a simple null hypothesis against a composite alternative concerning the state of the field, modeled as a vector of (continuous) unknown parameters determining the parametric family of probability measures induced on the agents' observation spaces under the hypotheses. Specifically, under the alternative hypothesis, each agent sequentially observes an independent and identically distributed time-series consisting of a (nonlinear) function of the true but unknown parameter corrupted by Gaussian noise, whereas, under the null, they obtain noise only. Two distributed recursive generalized likelihood ratio test type algorithms of the \emph{consensus+innovations} form are proposed, namely \mathcal{CIGLRT-L} and \mathcal{CIGLRT-NL}, in which the agents estimate the underlying parameter and in parallel also update their test decision statistics by simultaneously processing the latest local sensed information and information obtained from neighboring agents. For \mathcal{CIGLRT-NL}, for a broad class of nonlinear observation models and under a global observability condition, algorithm parameters which ensure asymptotically decaying probabilities of errors~(probability of miss and probability of false detection) are characterized. For \mathcal{CIGLRT-L}, a linear observation model is considered and upper bounds on large deviations decay exponent for the error probabilities are obtained. △ Less","21 February, 2017",https://arxiv.org/pdf/1601.04779
"Performance Analysis of an Unreliable M/G/1
Retrial Queue with Two-way Communication",Muthukrishnan Senthil Kumar;Aresh Dadlani;Kiseon Kim,"Efficient use of call center operators through technological innovations more often come at the expense of added operation management issues. In this paper, the stationary characteristics of an M/G/1 retrial queue is investigated where the single server, subject to active failures, primarily attends incoming calls and directs outgoing calls only when idle. The incoming calls arriving at the server follow a Poisson arrival process, while outgoing calls are made in an exponentially distributed time. On finding the server unavailable (either busy or temporarily broken down), incoming calls intrinsically join the virtual orbit from which they re-attempt for service at exponentially distributed time intervals. The system stability condition along with probability generating functions for the joint queue length distribution of the number of calls in the orbit and the state of the server are derived and evaluated numerically in the context of mean system size, server availability, failure frequency and orbit waiting time. △ Less","26 December, 2017",https://arxiv.org/pdf/1512.08609
Innovation Pursuit: A New Approach to Subspace Clustering,Mostafa Rahmani;George Atia,"In subspace clustering, a group of data points belonging to a union of subspaces are assigned membership to their respective subspaces. This paper presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of subspace clustering using a new geometrical idea whereby subspaces are identified based on their relative novelties. We present two frameworks in which the idea of innovation pursuit is used to distinguish the subspaces. Underlying the first framework is an iterative method that finds the subspaces consecutively by solving a series of simple linear optimization problems, each searching for a direction of innovation in the span of the data potentially orthogonal to all subspaces except for the one to be identified in one step of the algorithm. A detailed mathematical analysis is provided establishing sufficient conditions for iPursuit to correctly cluster the data. The proposed approach can provably yield exact clustering even when the subspaces have significant intersections. It is shown that the complexity of the iterative approach scales only linearly in the number of data points and subspaces, and quadratically in the dimension of the subspaces. The second framework integrates iPursuit with spectral clustering to yield a new variant of spectral-clustering-based algorithms. The numerical simulations with both real and synthetic data demonstrate that iPursuit can often outperform the state-of-the-art subspace clustering algorithms, more so for subspaces with significant intersections, and that it significantly improves the state-of-the-art result for subspace-segmentation-based face clustering. △ Less","26 November, 2017",https://arxiv.org/pdf/1512.00907
Understanding Human-Machine Networks: A Cross-Disciplinary Survey,Milena Tsvetkova;Taha Yasseri;Eric T. Meyer;J. Brian Pickering;Vegard Engen;Paul Walland;Marika Lüders;Asbjørn Følstad;George Bravos,"In the current hyper-connected era, modern Information and Communication Technology systems form sophisticated networks where not only do people interact with other people, but also machines take an increasingly visible and participatory role. Such human-machine networks (HMNs) are embedded in the daily lives of people, both for personal and professional use. They can have a significant impact by producing synergy and innovations. The challenge in designing successful HMNs is that they cannot be developed and implemented in the same manner as networks of machines nodes alone, nor following a wholly human-centric view of the network. The problem requires an interdisciplinary approach. Here, we review current research of relevance to HMNs across many disciplines. Extending the previous theoretical concepts of socio-technical systems, actor-network theory, cyber-physical-social systems, and social machines, we concentrate on the interactions among humans and between humans and machines. We identify eight types of HMNs: public-resource computing, crowdsourcing, web search engines, crowdsensing, online markets, social media, multiplayer online games and virtual worlds, and mass collaboration. We systematically select literature on each of these types and review it with a focus on implications for designing HMNs. Moreover, we discuss risks associated with HMNs and identify emerging design and development trends. △ Less","18 January, 2017",https://arxiv.org/pdf/1511.05324
Efficient Regression in Metric Spaces via Approximate Lipschitz Extension,Lee-Ad Gottlieb;Aryeh Kontorovich;Robert Krauthgamer,"We present a framework for performing efficient regression in general metric spaces. Roughly speaking, our regressor predicts the value at a new point by computing a Lipschitz extension --- the smoothest function consistent with the observed data --- after performing structural risk minimization to avoid overfitting. We obtain finite-sample risk bounds with minimal structural and noise assumptions, and a natural speed-precision tradeoff. The offline (learning) and online (prediction) stages can be solved by convex programming, but this naive approach has runtime complexity O(n^3), which is prohibitive for large datasets. We design instead a regression algorithm whose speed and generalization performance depend on the intrinsic dimension of the data, to which the algorithm adapts. While our main innovation is algorithmic, the statistical results may also be of independent interest. △ Less","24 April, 2017",https://arxiv.org/pdf/1111.4470
