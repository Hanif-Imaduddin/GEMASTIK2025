title,authors,abstract,submitted_date,pdf_link
Improved texture image classification through the use of a corrosion-inspired cellular automaton,Núbia Rosa da Silva;Pieter Van der Weeën;Bernard De Baets;Odemir Martinez Bruno,"In this paper, the problem of classifying synthetic and natural texture images is addressed. To tackle this problem, an innovative method is proposed that combines concepts from corrosion modeling and cellular automata to generate a texture descriptor. The core processes of metal (pitting) corrosion are identified and applied to texture images by incorporating the basic mechanisms of corrosion in the transition function of the cellular automaton. The surface morphology of the image is analyzed before and during the application of the transition function of the cellular automaton. In each iteration the cumulative mass of corroded product is obtained to construct each of the attributes of the texture descriptor. In a final step, this texture descriptor is used for image classification by applying Linear Discriminant Analysis. The method was tested on the well-known Brodatz and Vistex databases. In addition, in order to verify the robustness of the method, its invariance to noise and rotation were tested. To that end, different variants of the original two databases were obtained through addition of noise to and rotation of the images. The results showed that the method is effective for texture classification according to the high success rates obtained in all cases. This indicates the potential of employing methods inspired on natural phenomena in other fields. △ Less","25 December, 2014",https://arxiv.org/pdf/1412.7889
An Implementation Framework (IF) for the National Information Assurance and Cyber Security Strategy (NIACSS) of Jordan,Ahmed Otoom;Issa Atoum,"This paper proposes an implementation framework that lays out the ground for a coherent, systematic, and comprehensive approach to implement the National Information Assurance and Cyber Security Strategy (NIACSS) of Jordan. The Framework 1). Suggests a methodology to analyze the NIACSS, 2). Illustrates how the NIACSS analysis can be utilized to design strategic moves and develop an appropriate functional structure, and 3). proposes a set of adaptable strategic controls that govern the NIACSS implementation and allow achieving excellence, innovation, efficiency, and quality.The framework, if adopted, is expected to harvest several advantages within the following areas: information security implementation management, control and guidance, efforts consolidation, resource utilization, productive collaboration, and completeness. The framework is flexible and expandable; therefore, it can be generalized. △ Less","30 November, 2014",https://arxiv.org/pdf/1412.1141
"Υ
-DB: A system for data-driven hypothesis management and analytics",Bernardo Gonçalves;Frederico C. Silva;Fabio Porto,"The vision of Υ-DB introduces deterministic scientific hypotheses as a kind of uncertain and probabilistic data, and opens some key technical challenges for enabling data-driven hypothesis management and analytics. The Υ-DB system addresses those challenges throughout a design-by-synthesis pipeline that defines its architecture. It processes hypotheses from their XML-based extraction to encoding as uncertain and probabilistic U-relational data, and eventually to their conditioning in the presence of observations. In this demo we present a first prototype of the Υ-DB system. We showcase its core innovative features by means of use case scenarios in computational science in which the hypotheses are extracted from a model repository on the web and evaluated (rated/ranked) as probabilistic data. △ Less","26 November, 2014",https://arxiv.org/pdf/1411.7419
Analysis of Memory Ballooning Technique for Dynamic Memory Management of Virtual Machines (VMs),A B M Moniruzzaman,"Memory ballooning is dynamic memory management technique for virtual machines (VMs). Ballooning is a part of memory reclamation technique operations used by a hypervisor to allow the physical host system to retrieve unused memory from certain guest virtual machines (VMs) and share it with others. Memory ballooning allows the total amount ofRAM required by guest VMs to exceed the amount ofphysical RAM available on the host. Memory overcommitment enables a higher consolidation ratio in a hypervisor. Using memory overcommitment, users can consolidate VMs on a physical machine such that physical resources are utilized in an optimal manner while delivering good performance. Hence memory reclamation is an integral component ofmemory overcommitment. In this paper, we address that the basic cause of memory that ballooning is memory overcommitment from using memory-intensive virtual machines. We compared to others reclamation technique and identify Cost Associate with Memory Ballooning in state of Memory Overcommitment. The objective of this paper is to analyse memory ballooning technique for dynamic memory management of VMs. For this analysis, VMware based virtualization software e.g ESXi Server, vCenter Server, vSphere Client are installed and configured on the Centre for Innovation and Technology (CIT) Lab, DIU; for monitor and analyze VM performance for memory ballooning technique. The performance ofmemory ballooning technique is evaluated with two different test cases. The purpose is to help users understand, how this technique impact the performance. Finally, we presents the throughput ofheavy workload with different memory limits when using ballooning or swapping; and analyse VM performance issue for this technique. △ Less","26 November, 2014",https://arxiv.org/pdf/1411.7344
Patents used by NPE as an Open Information System in Web 2.0 - Two mini case studies,Abdelkader Baaziz;Luc Quoniam,"The Information Systems around patents are complex, their study coupled with a creative vision of ""out of the box"", overcomes the strict basic functions of the patent. We have, on several occasions, guiding research around the patent solely-based on information, since the writing of new patents ; invalidation of existing patents, the creation of value-added information and their links to other Information Systems. The traditional R&D based on heavy investments is one type of technology transfer. But, patent information is also, another powerful tool of technology transfer, innovation and creativity. Indeed, conduct research on the patent, from an academic viewpoint, although not always focusing only on financial revenue, can be considered as a form of ""Non Practicing Entities"" (NPE) activity, called rightly or wrongly ""Patent Trolls"". We'll see why the term ""patent troll"" for this activity is controversial and inappropriate. The research we will describe in this paper falls within this context. We show two case studies of efficient use of patent information in Emerging countries, the first concern the pharmaceutical industry in Brazil and the second, the oil industry in Algeria. △ Less","26 November, 2014",https://arxiv.org/pdf/1411.7225
Cross-Layer Software-Defined 5G Network,Mao Yang;Yong Li;Long Hu;Bo Li;Depeng Jin;Sheng Chen;Zhongjiang Yan,"In the past few decades, the world has witnessed a rapid growth in mobile communication and reaped great benefits from it. Even though the fourth generation (4G) mobile communication system is just being deployed worldwide, proliferating mobile demands call for newer wireless communication technologies with even better performance. Consequently, the fifth generation (5G) system is already emerging in the research field. However, simply evolving the current mobile networks can hardly meet such great expectations, because over the years the infrastructures have generally become ossified, closed, and vertically constructed. Aiming to establish a new paradigm for 5G mobile networks, in this article, we propose a cross-layer software-defined 5G network architecture. By jointly considering both the network layer and the physical layer together, we establish the two software-defined programmable components, the control plane and the cloud computing pool, which enable an effective control of the mobile network from the global perspective and benefit technological innovations. Specifically, by the cross-layer design for software-defining, the logically centralized and programmable control plane abstracts the control functions from the network layer down to the physical layer, through which we achieve the fine-grained controlling of mobile network, while the cloud computing pool provides powerful computing capability to implement the baseband data processing of multiple heterogeneous networks. We discuss the main challenges of our architecture, including the fine-grained control strategies, network virtualization, and programmability. The architecture significantly benefits the convergence towards heterogeneous networks and it enables much more controllable, programmable and evolvable mobile networks. △ Less","22 November, 2014",https://arxiv.org/pdf/1411.6189
Visual Noise from Natural Scene Statistics Reveals Human Scene Category Representations,Michelle R. Greene;Abraham P. Botros;Diane M. Beck;Li Fei-Fei,"Our perceptions are guided both by the bottom-up information entering our eyes, as well as our top-down expectations of what we will see. Although bottom-up visual processing has been extensively studied, comparatively little is known about top-down signals. Here, we describe REVEAL (Representations Envisioned Via Evolutionary ALgorithm), a method for visualizing an observer's internal representation of a complex, real-world scene, allowing us to, for the first time, visualize the top-down information in an observer's mind. REVEAL rests on two innovations for solving this high dimensional problem: visual noise that samples from natural image statistics, and a computer algorithm that collaborates with human observers to efficiently obtain a solution. In this work, we visualize observers' internal representations of a visual scene category (street) using an experiment in which the observer views the naturalistic visual noise and collaborates with the algorithm to externalize his internal representation. As no scene information was presented, observers had to use their internal knowledge of the target, matching it with the visual features in the noise. We matched reconstructed images with images of real-world street scenes to enhance visualization. Critically, we show that the visualized mental images can be used to predict rapid scene detection performance, as each observer had faster and more accurate responses to detecting real-world images that were the most similar to his reconstructed street templates. These results show that it is possible to visualize previously unobservable mental representations of real world stimuli. More broadly, REVEAL provides a general method for objectively examining the content of previously private, subjective mental experiences. △ Less","19 November, 2014",https://arxiv.org/pdf/1411.5331
Linguistic Descriptions for Automatic Generation of Textual Short-Term Weather Forecasts on Real Prediction Data,A. Ramos-Soto;A. Bugarín;S. Barro;J. Taboada,"We present in this paper an application which automatically generates textual short-term weather forecasts for every municipality in Galicia (NW Spain), using the real data provided by the Galician Meteorology Agency (MeteoGalicia). This solution combines in an innovative way computing with perceptions techniques and strategies for linguistic description of data together with a natural language generation (NLG) system. The application, named GALiWeather, extracts relevant information from weather forecast input data and encodes it into intermediate descriptions using linguistic variables and temporal references. These descriptions are later translated into natural language texts by the natural language generation system. The obtained forecast results have been thoroughly validated by an expert meteorologist from MeteoGalicia using a quality assessment methodology which covers two key dimensions of a text: the accuracy of its content and the correctness of its form. Following this validation GALiWeather will be released as a real service offering custom forecasts for a wide public. △ Less","18 November, 2014",https://arxiv.org/pdf/1411.4925
NAVI: Neighbor Aware Virtual Infrastructure for Information Dissemination in Vehicular Networks,Pedro M. d'Orey;Nitin Maslekar;Idoia de la Iglesia;Nikola K. Zahariev,"Vehicular Networks enable a vast number of innovative applications, which rely on the efficient exchange of information between vehicles. However, efficient and reliable data dissemination is a particularly challenging task in the context of vehicular networks due to the underlying properties of these networks, limited availability of network infrastructure and variable penetration rates for distinct communication technologies. This paper presents a novel system and mechanism for information dissemination based on virtual infrastructure selection in combination with multiple communication technologies. The system has been evaluated using a simulation framework, involving network simulation in conjugation with realistic vehicular mobility traces. The presented simulation results show the feasibility of the proposed mechanism to achieve maximum message penetration with reduced overhead. Compared with a cellular-based only solution, our mechanism shows that the judicious vehicle selection can lead to improved network utilization through the offload of traffic to the short-range communication network. △ Less","10 November, 2014",https://arxiv.org/pdf/1411.2373
Evolving intraday foreign exchange trading strategies utilizing multiple instruments price series,Simone Cirillo;Stefan Lloyd;Peter Nordin,"We propose a Genetic Programming architecture for the generation of foreign exchange trading strategies. The system's principal features are the evolution of free-form strategies which do not rely on any prior models and the utilization of price series from multiple instruments as input data. This latter feature constitutes an innovation with respect to previous works documented in literature. In this article we utilize Open, High, Low, Close bar data at a 5 minutes frequency for the AUD.USD, EUR.USD, GBP.USD and USD.JPY currency pairs. We will test the implementation analyzing the in-sample and out-of-sample performance of strategies for trading the USD.JPY obtained across multiple algorithm runs. We will also evaluate the differences between strategies selected according to two different criteria: one relies on the fitness obtained on the training set only, the second one makes use of an additional validation dataset. Strategy activity and trade accuracy are remarkably stable between in and out of sample results. From a profitability aspect, the two criteria both result in strategies successful on out-of-sample data but exhibiting different characteristics. The overall best performing out-of-sample strategy achieves a yearly return of 19%. △ Less","8 November, 2014",https://arxiv.org/pdf/1411.2153
Interactive Digital Learning Materials for Kindergarten Students in Bangladesh,Md. Baharul Islam;Md. Kabirul Islam;Arif Ahmed;Abu Kalam Shamsuddin,"The pedagogy of teaching and learning has changed with the proliferation of communication technology and it is necessary to develop interactive learning materials for children that may improve their learning, catching, and memorizing capabilities. Perhaps, one of the most important innovations in the age of technology is multimedia and its application. It is imperative to create high quality and realistic learning environment for children. Interactive learning materials can be easier to understand and deal with their first learning. We developed some interactive learning materials in the form of a video for Playgroup using multimedia application tools. This study investigated the impact of students' abilities to acquire new knowledge or skills through interactive learning materials. We visited one kindergartens (Nursery schools), interviewed class teachers about their teaching methods and level of students' ability of recognizing English alphabets, pictures, etc. The course teachers were provided interactive learning materials to show their playgroups for a number of sessions. The video included English alphabets with related words and pictures, and motivational fun. We noticed that almost all children were very interested to interact with their leaning video. The students were assessed individually and asked to recognize the alphabets, and pictures. The students adapted with their first alphabets very quickly. However, there were individual differences in their cognitive development. This interactive multimedia can be an alternative to traditional pedagogy for teaching playgroups. △ Less","7 November, 2014",https://arxiv.org/pdf/1411.2075
"Space proof complexity for random 3
-CNFs via a (2-ε)
-Hall's Theorem",Ilario Bonacina;Nicola Galesi;Tony Huynh;Paul Wollan,"We investigate the space complexity of refuting 3-CNFs in Resolution and algebraic systems. No lower bound for refuting any family of 3-CNFs was previously known for the total space in resolution or for the monomial space in algebraic systems. We prove that every Polynomial Calculus with Resolution refutation of a random 3-CNF φ in n variables requires, with high probability, Ω(n/\log n) distinct monomials to be kept simultaneously in memory. The same construction also proves that every Resolution refutation φ requires, with high probability, Ω(n/\log n) clauses each of width Ω(n/\log n) to be kept at the same time in memory. This gives a Ω(n^2/\log^2 n) lower bound for the total space needed in Resolution to refute φ. The main technical innovation is a variant of Hall's theorem. We show that in bipartite graphs G with bipartition (L,R) and left-degree at most 3, L can be covered by certain families of disjoint paths, called (2,4)-matchings, provided that L expands in R by a factor of (2-ε), for ε< \frac{1}{23}. △ Less","6 November, 2014",https://arxiv.org/pdf/1411.1619
Immersive and Collaborative Data Visualization Using Virtual Reality Platforms,Ciro Donalek;S. G. Djorgovski;Scott Davidoff;Alex Cioc;Anwell Wang;Giuseppe Longo;Jeffrey S. Norris;Jerry Zhang;Elizabeth Lawler;Stacy Yeh;Ashish Mahabal;Matthew Graham;Andrew Drake,"Effective data visualization is a key part of the discovery process in the era of big data. It is the bridge between the quantitative content of the data and human intuition, and thus an essential component of the scientific path from data into knowledge and understanding. Visualization is also essential in the data mining process, directing the choice of the applicable algorithms, and in helping to identify and remove bad data from the analysis. However, a high complexity or a high dimensionality of modern data sets represents a critical obstacle. How do we visualize interesting structures and patterns that may exist in hyper-dimensional data spaces? A better understanding of how we can perceive and interact with multi dimensional information poses some deep questions in the field of cognition technology and human computer interaction. To this effect, we are exploring the use of immersive virtual reality platforms for scientific data visualization, both as software and inexpensive commodity hardware. These potentially powerful and innovative tools for multi dimensional data visualization can also provide an easy and natural path to a collaborative data visualization and exploration, where scientists can interact with their data and their colleagues in the same visual space. Immersion provides benefits beyond the traditional desktop visualization tools: it leads to a demonstrably better perception of a datascape geometry, more intuitive data understanding, and a better retention of the perceived relationships in the data. △ Less","28 October, 2014",https://arxiv.org/pdf/1410.7670
"Improved Region-Growing and Combinatorial Algorithms for k
-Route Cut Problems",Guru Guruganesh;Laura Sanita;Chaitanya Swamy,"We study the {\em k-route} generalizations of various cut problems, the most general of which is \emph{k-route multicut} (k-MC) problem, wherein we have r source-sink pairs and the goal is to delete a minimum-cost set of edges to reduce the edge-connectivity of every source-sink pair to below k. The k-route extensions of multiway cut (k-MWC), and the minimum s-t cut problem (k-(s,t)-cut), are similarly defined. We present various approximation and hardness results for these k-route cut problems that improve the state-of-the-art for these problems in several cases. (i) For {\em k-route multiway cut}, we devise simple, but surprisingly effective, combinatorial algorithms that yield bicriteria approximation guarantees that markedly improve upon the previous-best guarantees. (ii) For {\em k-route multicut}, we design algorithms that improve upon the previous-best approximation factors by roughly an O(\sqrt{\log r})-factor, when k=2, and for general k and unit costs and any fixed violation of the connectivity threshold k. The main technical innovation is the definition of a new, powerful \emph{region growing} lemma that allows us to perform region-growing in a recursive fashion even though the LP solution yields a {\em different metric} for each source-sink pair. (iii) We complement these results by showing that the {\em k-route s-t cut} problem is at least as hard to approximate as the {\em densest-k-subgraph} (DkS) problem on uniform hypergraphs. △ Less","19 October, 2014",https://arxiv.org/pdf/1410.5105
Formative Assessment and its E-learning Implementation,S. M. Jacob;B. Issac,"Innovation in assessment is no more a choice in a tech-savvy instant age. The purpose of this study was to get more insight into the implementation of formative assessment through the e-learning tool called Black Board Learning System in our University. The proposal is to implement a series of weekly or fortnightly tests on the BB. These would have options to provide sufficient feedback as a follow up to the attempt of students. Responses from questionnaires were used to discover the important concerns in the perceptions of students on the proposed concept of continuous assessment and the BB Online test implementation. The results indicate that students support the idea mainly because they find the disintegration into small topic assessments as useful, coupled with the availability of immediate teacher feedback. What we intend is a culture of success, backed by a belief that all pupils can achieve the same. △ Less","17 October, 2014",https://arxiv.org/pdf/1410.4675
The mobile devices and its mobile learning usage analysis,S. M. Jacob;B. Issac,The usage of mobile devices for mobile learning is becoming increasingly popular. There is a new brand of students in the universities now-a-days who are easily connected to technology and innovative mobile devices. We attempt to do an analysis on a survey done with university students on mobile device usage for mobile learning purposes. This is to find the learning trends within the student community so that some of these popular practices could be encouraged to enhance learning among the student community. Both the quantitative and qualitative approaches are adopted in the analysis. The results are discussed and conclusions drawn in the end. △ Less,"16 October, 2014",https://arxiv.org/pdf/1410.4375
Refined Particle Swarm Intelligence Method for Abrupt Motion Tracking,Mei Kuan Lim;Chee Seng Chan;Dorothy Monekosso;Paolo Remagnino,"Conventional tracking solutions are not feasible in handling abrupt motion as they are based on smooth motion assumption or an accurate motion model. Abrupt motion is not subject to motion continuity and smoothness. To assuage this, we deem tracking as an optimisation problem and propose a novel abrupt motion tracker that based on swarm intelligence - the SwaTrack. Unlike existing swarm-based filtering methods, we first of all introduce an optimised swarm-based sampling strategy to tradeoff between the exploration and exploitation of the search space in search for the optimal proposal distribution. Secondly, we propose Dynamic Acceleration Parameters (DAP) allow on the fly tuning of the best mean and variance of the distribution for sampling. Such innovating idea of combining these strategies in an ingenious way in the PSO framework to handle the abrupt motion, which so far no existing works are found. Experimental results in both quantitative and qualitative had shown the effectiveness of the proposed method in tracking abrupt motions. △ Less","14 October, 2014",https://arxiv.org/pdf/1410.3744
Batch Codes through Dense Graphs without Short Cycles,Alexandros G. Dimakis;Anna Gal;Ankit Singh Rawat;Zhao Song,"Consider a large database of n data items that need to be stored using m servers. We study how to encode information so that a large number k of read requests can be performed in parallel while the rate remains constant (and ideally approaches one). This problem is equivalent to the design of multiset Batch Codes introduced by Ishai, Kushilevitz, Ostrovsky and Sahai [17]. We give families of multiset batch codes with asymptotically optimal rates of the form 1-1/\text{poly}(k) and a number of servers m scaling polynomially in the number of read requests k. An advantage of our batch code constructions over most previously known multiset batch codes is explicit and deterministic decoding algorithms and asymptotically optimal fault tolerance. Our main technical innovation is a graph-theoretic method of designing multiset batch codes using dense bipartite graphs with no small cycles. We modify prior graph constructions of dense, high-girth graphs to obtain our batch code results. We achieve close to optimal tradeoffs between the parameters for bipartite graph based batch codes. △ Less","10 October, 2014",https://arxiv.org/pdf/1410.2920
Computational Beauty: Aesthetic Judgment at the Intersection of Art and Science,Emily L. Spratt;Ahmed Elgammal,"In part one of the Critique of Judgment, Immanuel Kant wrote that ""the judgment of taste...is not a cognitive judgment, and so not logical, but is aesthetic.""\cite{Kant} While the condition of aesthetic discernment has long been the subject of philosophical discourse, the role of the arbiters of that judgment has more often been assumed than questioned. The art historian, critic, connoisseur, and curator have long held the esteemed position of the aesthetic judge, their training, instinct, and eye part of the inimitable subjective processes that Kant described as occurring upon artistic evaluation. Although the concept of intangible knowledge in regard to aesthetic theory has been much explored, little discussion has arisen in response to the development of new types of artificial intelligence as a challenge to the seemingly ineffable abilities of the human observer. This paper examines the developments in the field of computer vision analysis of paintings from canonical movements with the history of Western art and the reaction of art historians to the application of this technology in the field. Through an investigation of the ethical consequences of this innovative technology, the unquestioned authority of the art expert is challenged and the subjective nature of aesthetic judgment is brought to philosophical scrutiny once again. △ Less","29 September, 2014",https://arxiv.org/pdf/1410.2488
Sampling and Galerkin reconstruction in reproducing kernel spaces,Cheng Cheng;Yingchun Jiang;Qiyu Sun,"In this paper, we consider sampling in a reproducing kernel subspace of L^p. We introduce a pre-reconstruction operator associated with a sampling scheme and propose a Galerkin reconstruction in general Banach space setting. We show that the proposed Galerkin method provides a quasi-optimal approximation, and the corresponding Galerkin equations could be solved by an iterative approximation-projection algorithm. We also present detailed analysis and numerical simulations of the Galerkin method for reconstructing signals with finite rate of innovation. △ Less","7 October, 2014",https://arxiv.org/pdf/1410.1828
Using Professional Social Networking as an Innovative Method for Data Extraction: The ICT Alumni Index Case Study,Rasha Y. Tantawy;Ziad Farouk;Shereen Mohamed;Ahmed H. Yousef,"The lack of data regarding Information and Communications Technology sector alumni data is a known problem in several countries including Egypt. It is not clear what entry and senior jobs are occupied by alumni and which countries attract them. This affects the planning, design and execution of both the ICT sector and the Education sector. In this research, a joint team is formulated from the Technology Innovation and Entrepreneurship Center TIEC and the Ministry of Higher Education. This team is undertaking extensive analysis of the structure, distribution and development of the ICT skills and employment. △ Less","6 October, 2014",https://arxiv.org/pdf/1410.1348
A Systematic Approach to Setting Up Distributed Global Collaborations for Software-based Products in the Automotive Domain,Inna Smirnova,"There is an increasing need for organizations to collaborate with internal and external partners on a global scale for creating software-based products and services. Many aspects and risks need to be addressed when setting up such global collaborations. Different types of collaborations such as engineering collaborations or innovation-focused collaborations need to be considered. Further aspects such as cultural and social aspects, coordination, infrastructure, organizational change process, and communication issues need to be examined. Although there are already experiences available with respect to setting up global collaborations, they are mainly focusing on certain specific areas. An overall holistic approach that guides companies in systematically setting up global collaborations for software-based products is widely missing. The goal of this thesis is to analyze existing literature and related information and to extract topics that need be taken into account while establishing global software development collaborations - to identify solutions, risks, success factors, strategies, good experiences as well as good examples. This information is structured in a way so that it can be used by companies as a well-grounded holistic approach to guide companies effectively in setting up long-term global collaborations in the domain software development. The presented approach is based on scientific findings reported in literature, driven by industry needs, and confirmed by industry experts. △ Less","3 October, 2014",https://arxiv.org/pdf/1410.0973
CONCERT: A Cloud-Based Architecture for Next-generation Cellular Systems,Jingchu Liu;Tao Zhao;Sheng Zhou;Yu Cheng;Zhisheng Niu,"Cellular networks are one of the corner stones of our information-driven society. However, existing cellular systems have been seriously challenged by the explosion of mobile data traffic, the emergence of machine-type communications and the flourish of mobile Internet services. In this article, we propose CONCERT (CONvergence of Cloud and cEllulaR sysTems), a converged edge infrastructure for future cellular communications and mobile computing services. The proposed architecture is constructed based on the concept of control/data (C/D) plane decoupling. The data plane includes heterogeneous physical resources such as radio interfacing equipment, computational resources, and software-defined switches. The control plane jointly coordinates physical resources to present them as virtual resources, over which software-defined services including communications, computing, and management can be deployed in a flexible manner. Moreover, we introduce new designs for physical resources placement and task scheduling, so that CONCERT can overcome the drawbacks of the existing baseband-up centralization approach and better facilitate innovations in next-generation cellular networks. These advantages are demonstrated with application examples on the radio access networks (RANs) with C/D decoupled air interface, delay sensitive machine-type communications, and real-time mobile cloud gaming. We also discuss some fundamental research issues arising with the proposed architecture to illuminate future research directions. △ Less","1 October, 2014",https://arxiv.org/pdf/1410.0113
An agent-driven semantical identifier using radial basis neural networks and reinforcement learning,Christian Napoli;Giuseppe Pappalardo;Emiliano Tramontana,"Due to the huge availability of documents in digital form, and the deception possibility raise bound to the essence of digital documents and the way they are spread, the authorship attribution problem has constantly increased its relevance. Nowadays, authorship attribution,for both information retrieval and analysis, has gained great importance in the context of security, trust and copyright preservation. This work proposes an innovative multi-agent driven machine learning technique that has been developed for authorship attribution. By means of a preprocessing for word-grouping and time-period related analysis of the common lexicon, we determine a bias reference level for the recurrence frequency of the words within analysed texts, and then train a Radial Basis Neural Networks (RBPNN)-based classifier to identify the correct author. The main advantage of the proposed approach lies in the generality of the semantic analysis, which can be applied to different contexts and lexical domains, without requiring any modification. Moreover, the proposed system is able to incorporate an external input, meant to tune the classifier, and then self-adjust by means of continuous learning reinforcement. △ Less","30 September, 2014",https://arxiv.org/pdf/1409.8484
Multicast Group Management for Multi-View 3D Videos in Wireless Networks,Chi-Heng Lin;De-Nian Yang;Chih-Chung Lin;Wanjiun Liao,"With the emergence of 3D mobile devices available in the markets, mobile 3D video services become increasingly important for video service providers, such as Youtube and Netflix, while multi-view 3D videos are potential to bring out varied innovative applications. However, enabling multi-view 3D video services may overwhelm WiFi networks when we multicast every view of a video. In this paper, therefore, we propose to incorporate depth-image-based rendering (DIBR), which allows each mobile client to synthesize the desired view from nearby left and right views, to effectively reduce the bandwidth consumption. Moreover, due to varied channel conditions, each client may suffer from different packet loss probabilities, and retransmissions incur additional bandwidth consumption. To address this issue, we first analyze the merit of view protection via DIBR for multi-view video multicast and then design a new protocol, named Multi-View Group Management Protocol (MVGMP), for the dynamic group management of multicast users. Simulation results manifest that our protocol effectively reduces bandwidth consumption and increases the probability for each client to successfully playback the desired view of a multi-view 3D video. △ Less","2 December, 2014",https://arxiv.org/pdf/1409.8352
Recommending Investors for Crowdfunding Projects,Jisun An;Daniele Quercia;Jon Crowcroft,"To bring their innovative ideas to market, those embarking in new ventures have to raise money, and, to do so, they have often resorted to banks and venture capitalists. Nowadays, they have an additional option: that of crowdfunding. The name refers to the idea that funds come from a network of people on the Internet who are passionate about supporting others' projects. One of the most popular crowdfunding sites is Kickstarter. In it, creators post descriptions of their projects and advertise them on social media sites (mainly Twitter), while investors look for projects to support. The most common reason for project failure is the inability of founders to connect with a sufficient number of investors, and that is mainly because hitherto there has not been any automatic way of matching creators and investors. We thus set out to propose different ways of recommending investors found on Twitter for specific Kickstarter projects. We do so by conducting hypothesis-driven analyses of pledging behavior and translate the corresponding findings into different recommendation strategies. The best strategy achieves, on average, 84% of accuracy in predicting a list of potential investors' Twitter accounts for any given project. Our findings also produced key insights about the whys and wherefores of investors deciding to support innovative efforts. △ Less","12 October, 2014",https://arxiv.org/pdf/1409.7489
Engineering Autonomous Driving Software,Christian Berger;Bernhard Rumpe,"A larger number of people with heterogeneous knowledge and skills running a project together needs an adaptable, target, and skill-specific engineering process. This especially holds for a project to develop a highly innovative, autonomously driving vehicle to participate in the 2007 DARPA Urban Challenge. In this contribution, we present essential elements of a software and systems engineering process to develop a so-called artificial intelligence capable of driving autonomously in complex urban situations. The process itself includes agile concepts, like a test first approach, continuous integration of all software modules, and a reliable release and configuration management assisted by software tools in integrated development environments. However, one of the most important elements for an efficient and stringent development is the ability to efficiently test the behavior of the developed system in a flexible and modular system simulation for urban situations both interactively and unattendedly. We call this the simulate first approach. △ Less","22 September, 2014",https://arxiv.org/pdf/1409.6579
"An Energy Efficiency policy for Communications with C-RAN, ICN and Transition Smooth",Di Zhang;Zhenyu Zhou;Takuro Sato,"Towards next generation communications, Energy Efficiency (EE) attracts lots of attentions nowadays. Some innovative techniques have been proposed in prior literatures, especially the sleep mechanism of base station (BS). Yet how to sleep and when to sleep are still vague concepts. Another, most of the studies focus on the cellular section or core networks separately while integral and comprehensive version is neglected in prior literatures. In this paper,the integral optimization structure is studied based on cloud radio network (C-RAN) and information centric network (ICN) that raised latest combined with the sleep mode. The original C-RAN and ICN structures are amended in terms of reality application of sleep techniques. While adopting the sleep techniques both in core and cellular, apart from previous works, a transition smooth method that solve the current surge problems which is ignored before is further proposed. Based on the new method, it will be much more feasible to adopt the sleep techniques by knowing the appropriate occasion for transition between sleep and idle mode. Comprehensive computer based simulation results demonstrate that this integer proposal achieves better EE feature with negligible impact on quality of service (QoS) of user equipments (UEs). △ Less","21 September, 2014",https://arxiv.org/pdf/1409.5954
A new Watermarking Technique for Medical Image using Hierarchical Encryption,Med Karim Abdmouleh;Ali Khalfallah;Med Salim Bouhlel,"In recent years, characterized by the innovation of technology and the digital revolution, the field of media has become important. The transfer and exchange of multimedia data and duplication have become major concerns of researchers. Consequently, protecting copyrights and ensuring service safety is needed. Cryptography has a specific role, is to protect secret files against unauthorized access. In this paper, a hierarchical cryptosystem algorithm based on Logistic Map chaotic systems is proposed. The results show that the proposed method improves the security of the image. Experimental results on a database of 200 medical images show that the proposed method significantly gives better results. △ Less","16 September, 2014",https://arxiv.org/pdf/1409.4587
Simulating Noisy Channel Interaction,Mark Braverman;Jieming Mao,"We show that T rounds of interaction over the binary symmetric channel BSC_{1/2-ε} with feedback can be simulated with O(ε^2 T) rounds of interaction over a noiseless channel. We also introduce a more general ""energy cost"" model of interaction over a noisy channel. We show energy cost to be equivalent to external information complexity, which implies that our simulation results are unlikely to carry over to energy complexity. Our main technical innovation is a self-reduction from simulating a noisy channel to simulating a slightly-less-noisy channel, which may have other applications in the area of interactive compression. △ Less","15 September, 2014",https://arxiv.org/pdf/1409.4290
A New Course on Creativity in an Engineering Program: Foundations and Issues,Sophie Morin;Jean-Marc Robert;Liane Gabora,"The importance of innovation in the world's economy, now undeniable, draws great attention to the need to improve organizations' creative potential. In the last 60 years, hundreds of books have been written on the subject and hundreds of webpages display information on how to be more creative and achieve innovation. Several North American and European universities offer graduated programs in creativity. However, building an effective and validated creativity training program is not without challenges. Because of the nature of their work, engineers are often asked to be innovative. Without aiming for a degree in creativity, could future engineers benefit from training programs in creativity? This article presents the conceptual framework and pedagogical elements of a new course in creativity for engineering students. △ Less","13 September, 2014",https://arxiv.org/pdf/1409.3909
Instability and network effects in innovative markets,Paolo Sgrignoli;Elena Agliari;Raffaella Burioni;Augusto Schianchi,"We consider a network of interacting agents and we model the process of choice on the adoption of a given innovative product by means of statistical-mechanics tools. The modelization allows us to focus on the effects of direct interactions among agents in establishing the success or failure of the product itself. Mimicking real systems, the whole population is divided into two sub-communities called, respectively, Innovators and Followers, where the former are assumed to display more influence power. We study in detail and via numerical simulations on a random graph two different scenarios: no-feedback interaction, where innovators are cohesive and not sensitively affected by the remaining population, and feedback interaction, where the influence of followers on innovators is non negligible. The outcomes are markedly different: in the former case, which corresponds to the creation of a niche in the market, Innovators are able to drive and polarize the whole market. In the latter case the behavior of the market cannot be definitely predicted and become unstable. In both cases we highlight the emergence of collective phenomena and we show how the final outcome, in terms of the number of buyers, is affected by the concentration of innovators and by the interaction strengths among agents. △ Less","12 September, 2014",https://arxiv.org/pdf/1409.3837
Probabilistic Selection in AgentSpeak(L),Francisco Coelho;Vitor Nogueira,"Agent programming is mostly a symbolic discipline and, as such, draws little benefits from probabilistic areas as machine learning and graphical models. However, the greatest objective of agent research is the achievement of autonomy in dynamical and complex environments --- a goal that implies embracing uncertainty and therefore the entailed representations, algorithms and techniques. This paper proposes an innovative and conflict free two layer approach to agent programming that uses already established methods and tools from both symbolic and probabilistic artificial intelligence. Moreover, this framework is illustrated by means of a widely used agent programming example, GoldMiners. △ Less","12 September, 2014",https://arxiv.org/pdf/1409.3717
Time-domain multiscale shape identification in electro-sensing,Habib Ammari;Han Wang,This paper presents premier and innovative time-domain multi-scale method for shape identification in electro-sensing using pulse-type signals. The method is based on transform-invariant shape descriptors computed from filtered polarization tensors at multi-scales. The proposed algorithm enjoys a remarkable noise robustness even with far-field measurements at very limited angle of view. It opens a door for pulsed imaging using echolocation and induction data. △ Less,"12 September, 2014",https://arxiv.org/pdf/1409.3714
DeepID-Net: multi-stage and deformable deep convolutional neural networks for object detection,Wanli Ouyang;Ping Luo;Xingyu Zeng;Shi Qiu;Yonglong Tian;Hongsheng Li;Shuo Yang;Zhe Wang;Yuanjun Xiong;Chen Qian;Zhenyao Zhu;Ruohui Wang;Chen-Change Loy;Xiaogang Wang;Xiaoou Tang,"In this paper, we propose multi-stage and deformable deep convolutional neural networks for object detection. This new deep learning object detection diagram has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. With the proposed multi-stage training strategy, multiple classifiers are jointly optimized to process samples at different difficulty levels. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of modeling averaging. The proposed approach ranked \#2 in ILSVRC 2014. It improves the mean averaged precision obtained by RCNN, which is the state-of-the-art of object detection, from 31\% to 45\%. Detailed component-wise analysis is also provided through extensive experimental evaluation. △ Less","11 September, 2014",https://arxiv.org/pdf/1409.3505
Reconciliation of RDF* and Property Graphs,Olaf Hartig,"Both the notion of Property Graphs (PG) and the Resource Description Framework (RDF) are commonly used models for representing graph-shaped data. While there exist some system-specific solutions to convert data from one model to the other, these solutions are not entirely compatible with one another and none of them appears to be based on a formal foundation. In fact, for the PG model, there does not even exist a commonly agreed-upon formal definition. The aim of this document is to reconcile both models formally. To this end, the document proposes a formalization of the PG model and introduces well-defined transformations between PGs and RDF. As a result, the document provides a basis for the following two innovations: On one hand, by implementing the RDF-to-PG transformations defined in this document, PG-based systems can enable their users to load RDF data and make it accessible in a compatible, system-independent manner using, e.g., the graph traversal language Gremlin or the declarative graph query language Cypher. On the other hand, the PG-to-RDF transformation in this document enables RDF data management systems to support compatible, system-independent queries over the content of Property Graphs by using the standard RDF query language SPARQL. Additionally, this document represents a foundation for systematic research on relationships between the two models and between their query languages. △ Less","13 November, 2014",https://arxiv.org/pdf/1409.3288
Synergy cycles in the Norwegian innovation system: The relation between synergy and cycle values,Inga Ivanova;Oivind Strand;Loet Leydesdorff,"The knowledge base of an economy measured in terms of Triple Helix relations can be analyzed in terms of mutual information among geographical, sectorial, and size distributions of firms as dimensions of the probabilistic entropy. The resulting synergy values of a TH system provide static snapshots. In this study, we add the time dimension and analyze the synergy dynamics using the Norwegian innovation system as an example. The synergy among the three dimensions can be mapped as a set of partial time series and spectrally analyzed. The results suggest that the synergy at the level of both the country and its 19 counties shoe non-chaotic oscillatory behavior and resonates in a set of natural frequencies. That is, synergy surges and drops are non-random and can be analyzed and predicted. There is a proportional dependence between the amplitudes of oscillations and synergy values and an inverse proportional dependence between the oscillation frequencies' relative inputs and synergy values. This analysis of the data informs us that one can expect frequency-related synergy-volatility growth in relation to the synergy value and a shift in the synergy volatility towards the long-term fluctuations with the synergy growth. △ Less","8 September, 2014",https://arxiv.org/pdf/1409.2760
The Energy Navigator - A Web based Platform for functional Quality Mangement in Buildings,Stefan Plesser;M. Norbert Fisch;Claas Pinkernell;Thomas Kurpck;Bernhard Rumpe,"Energy efficient buildings require high quality standards for all their technical equipment to enable their efficient and successful operation and management. Building simulations enable engineers to design integrated HVAC systems with complex building automation systems to control all their technical functions. Numerous studies show that especially these supposedly innovative buildings often do not reach their energy efficiency targets when in operation. Key reasons for the suboptimal performance are imprecise functional descriptions and a lack of commissioning and monitoring of the technical systems that leave suboptimal operation undetected. In the research project ""Energy Navigator"" we create a web-based platform that enables engineers to create a comprehensive and precise functional description for the buildings services. The system reuses this functional description - written in an appropriate domain specific language - to control the building operation, to signal malfunctions or faults, and in particular to measure energy efficiency over time. The innovative approach of the platform is the combination of design and control within one artifact linking the phases of design and operation and improving the cost effectiveness for both services. The paper will describe the concept of the platform, the technical innovation and first application examples of the research project. △ Less","8 September, 2014",https://arxiv.org/pdf/1409.2364
Visualizing and Quantifying Impact and Effect in Twitter Narrative using Geometric Data Analysis,Fionn Murtagh;Monica Pianosi;Richard Bull,"We use geometric multivariate data analysis which has been termed a methodology for both the visualization and verbalization of data. The general objectives are data mining and knowledge discovery. In the first case study, we use the narrative surrounding very highly profiled tweets, and thus a Twitter event of significance and importance. In the second case study, we use eight carefully planned Twitter campaigns relating to environmental issues. The aim of these campaigns was to increase environmental awareness and behaviour. Unlike current marketing, political and other communication campaigns using Twitter, we develop an innovative approach to measuring bevavioural change. We show also how we can assess statistical significance of social media behaviour. △ Less","14 September, 2014",https://arxiv.org/pdf/1409.1039
Modelling Electrical Car Diffusion Based on Agents,Lei Yu;Tao Zhang;Peer-Olaf Siebers;Uwe Aickelin,"Replacing traditional fossil fuel vehicles with innovative zero-emission vehicles for the transport in ci ties is one of the major tactics to achieve the UK government 2020 target of cutting emission. We are developing an agent-based simulation model to study the possible impact of different governmental interventions on the diffusion of such vehicles. Options that could be studied with our what-if analysis to include things like car parking charges, price of electrical car, energy awareness and word of mouth. In this paper we present a first case study related to the introduction of a new car park charging scheme at the University of Nottingham. We have developed an agent based model to simulate theimpact of different car parking rates and other incentives on the uptake of electrical cars. The goal of this case study is to demonstrate the usefulness of agent-based modelling and simulation for such investigations. △ Less","2 September, 2014",https://arxiv.org/pdf/1409.0657
A Mobile Food Recommendation System Based on The Traffic Light Diet,Thienne Johnson;Jorge Vergara;Chelsea Doll;Madison Kramer;Gayathri Sundararaman;Harsha Rajendran;Alon Efrat;Melanie Hingle,"Innovative, real-time solutions are needed to address the mismatch between the demand for and supply of critical information to inform and motivate diet and health-related behavior change. Research suggests that interventions using mobile health technologies hold great promise for influencing knowledge, attitudes, and behaviors related to energy balance. The objective of this paper is to present insights related to the development and testing of a mobile food recommendation system targeting fast food restaurants. The system is designed to provide consumers with information about energy density of food options combined with tips for healthier choices when dining out, accessible through a mobile phone. △ Less","1 September, 2014",https://arxiv.org/pdf/1409.0296
Software-Defined and Virtualized Future Mobile and Wireless Networks: A Survey,Mao Yang;Yong Li;Depeng Jin;Lieguang Zeng;Xin Wu;Athanasios V. Vasilakos,"With the proliferation of mobile demands and increasingly multifarious services and applications, mobile Internet has been an irreversible trend. Unfortunately, the current mobile and wireless network (MWN) faces a series of pressing challenges caused by the inherent design. In this paper, we extend two latest and promising innovations of Internet, software-defined networking and network virtualization, to mobile and wireless scenarios. We first describe the challenges and expectations of MWN, and analyze the opportunities provided by the software-defined wireless network (SDWN) and wireless network virtualization (WNV). Then, this paper focuses on SDWN and WNV by presenting the main ideas, advantages, ongoing researches and key technologies, and open issues respectively. Moreover, we interpret that these two technologies highly complement each other, and further investigate efficient joint design between them. This paper confirms that SDWN and WNV may efficiently address the crucial challenges of MWN and significantly benefit the future mobile and wireless network. △ Less","30 August, 2014",https://arxiv.org/pdf/1409.0079
Voronoi Grid-Shell Structures,Nico Pietroni;Davide Tonelli;Enrico Puppo;Maurizio Froli;Roberto Scopigno;Paolo Cignoni,"We introduce a framework for the generation of grid-shell structures that is based on Voronoi diagrams and allows us to design tessellations that achieve excellent static performances. We start from an analysis of stress on the input surface and we use the resulting tensor field to induce an anisotropic non-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation under the same metric. The resulting mesh is hex-dominant and made of cells with a variable density, which depends on the amount of stress, and anisotropic shape, which depends on the direction of maximum stress. This mesh is further optimized taking into account symmetry and regularity of cells to improve aesthetics. We demonstrate that our grid-shells achieve better static performances with respect to quad-based grid shells, while offering an innovative and aesthetically pleasing look. △ Less","26 August, 2014",https://arxiv.org/pdf/1408.6591
A Multi-agent Based Digital Preservation Model,Jacopo Pellegrino,"Master's Degree Thesis: Department of Physics, University of Turin Supervisor: Prof. Marco Maggiora, Department of Physics, University of Turin; email: marco.maggiora@unito.it Co-Supervisor: Prof. Walter Allasia, Innovation Department, EURIX; email: allasia@eurix.it The thesis describes an agent-based model aimed to simulate those processes in which a digital object faces the risk of obsolescence, a migration process has to be performed and the most appropriate file format has to be adopted. Agents have been designed in order to monitor and control the local system where they reside and its environment. They are able to become aware of obsolescent formats based on global parameters such as their diffusion. They communicate as well with each other to find out the most suitable preservation action to be performed. Agents request suggestions that are evaluated and propagated according to a weighting based on the level of trust assigned to both the agents who identified the problem and proposed the solution. In the current research, the definition of the trust level has been chosen based on the cultural and geographical distances, the expertise of the involved agents and the file format numerosity. The level of trust between two agents is automatically updated after every interaction by the mean of a feedback mechanism profiting of an inter agent communication based on stigmergy. Summing up, the thesis demonstrates how a multi-agent system can either perform an autonomous preservation action or suggest a list of best candidate solutions to the user. It benefits the management of several kinds of digital archive, especially those with limited resources specifically dedicated to digital preservation, such as small personal collections and many public institutions. △ Less","28 August, 2014",https://arxiv.org/pdf/1408.6126
Diffusion of Innovations over Multiplex Social Networks,Rasoul Ramezanian;Mostafa Salehi;Matteo Magnani;Danilo Montesi,"The ways in which an innovation (e.g., new behaviour, idea, technology, product) diffuses among people can determine its success or failure. In this paper, we address the problem of diffusion of innovations over multiplex social networks where the neighbours of a person belong to one or multiple networks (or layers) such as friends, families, or colleagues. To this end, we generalise one of the basic game-theoretic diffusion models, called networked coordination game, for multiplex networks. We present analytical results for this extended model and validate them through a simulation study, finding among other properties a lower bound for the success of an innovation.While simple and leading to intuitively understandable results, to the best of our knowledge this is the first extension of a game-theoretic innovation diffusion model for multiplex networks and as such it provides a basic framework to study more sophisticated innovation dynamics. △ Less","25 August, 2014",https://arxiv.org/pdf/1408.5806
Temporal Evolution of Social Innovation,Varsha S. Kulkarni,"Acceptance of an innovation can occur through mutliple exposures to individuals who have already accepted it. Presented here is a model to trace the evolution of an innovation in a social network with a preference λ, amidst topological constraints specified mainly by connectivity, k and population size, N_k. With the interplay between properties of innovation and network structure, the model attempts to explain the variations in patterns of innovations across social networks. Time in which the propagation attains highest velocity depends on λ^{-2}k^{-2}N_{k}^{1/2}. Dynamics in random networks may lead or lag behind that in scale-free networks depending on the average connectivity. Hierarchical propagation is evident across connectivity classes within scale-free networks, as well as across random networks with distinct topological indices. For highly preferred innovations, the hierarchy observed within scale-free networks tends to be insignificant. The results have implications for administering innovations in finite size networks. △ Less","21 August, 2014",https://arxiv.org/pdf/1408.5079
The Hidden Cost of Accommodating Crowdfunder Privacy Preferences: A Randomized Field Experiment,Gordon Burtch;Anindya Ghose;Sunil Wattal,"Online crowdfunding has received a great deal of attention from entrepreneurs and policymakers as a promising avenue to fostering entrepreneurship and innovation. A notable aspect of this shift from an offline to an online setting is that it brings increased visibility and traceability of transactions. Many crowdfunding platforms therefore provide mechanisms that enable a campaign contributor to conceal his or her identity or contribution amount from peers. We study the impact of these information (privacy) control mechanisms on crowdfunder behavior. Employing a randomized experiment at one of the largest online crowdfunding platforms, we find evidence of both positive (e.g., comfort) and negative (e.g., privacy priming) causal effects. We find that reducing access to information controls induces a net increase in fundraising, yet this outcome results from two competing influences: treatment increases willingness to engage with the platform (a 4.9% increase in the probability of contribution) and simultaneously decreases the average contribution (a $5.81 decline). This decline derives from a publicity effect, wherein contributors respond to a lack of privacy by tempering extreme contributions. We unravel the causal mechanisms that drive the results and discuss the implications of our findings for the design of online platforms. △ Less","18 August, 2014",https://arxiv.org/pdf/1408.4194
Real-Time Impulse Noise Suppression from Images Using an Efficient Weighted-Average Filtering,Hossein Hosseini;Farzad Hessar;Farokh Marvasti,"In this paper, we propose a method for real-time high density impulse noise suppression from images. In our method, we first apply an impulse detector to identify the corrupted pixels and then employ an innovative weighted-average filter to restore them. The filter takes the nearest neighboring interpolated image as the initial image and computes the weights according to the relative positions of the corrupted and uncorrupted pixels. Experimental results show that the proposed method outperforms the best existing methods in both PSNR measure and visual quality and is quite suitable for real-time applications. △ Less","10 July, 2014",https://arxiv.org/pdf/1408.3139
Complex Contagions in Kleinberg's Small World Model,Roozbeh Ebrahimi;Jie Gao;Golnaz Ghasemiesfeh;Grant Schoenebeck,"Complex contagions describe diffusion of behaviors in a social network in settings where spreading requires the influence by two or more neighbors. In a k-complex contagion, a cluster of nodes are initially infected, and additional nodes become infected in the next round if they have at least k already infected neighbors. It has been argued that complex contagions better model behavioral changes such as adoption of new beliefs, fashion trends or expensive technology innovations. This has motivated rigorous understanding of spreading of complex contagions in social networks. Despite simple contagions (k=1) that spread fast in all small world graphs, how complex contagions spread is much less understood. Previous work~\cite{Ghasemiesfeh:2013:CCW} analyzes complex contagions in Kleinberg's small world model~\cite{kleinberg00small} where edges are randomly added according to a spatial distribution (with exponent γ) on top of a two dimensional grid structure. It has been shown in~\cite{Ghasemiesfeh:2013:CCW} that the speed of complex contagions differs exponentially when γ=0 compared to when γ=2. In this paper, we fully characterize the entire parameter space of γ except at one point, and provide upper and lower bounds for the speed of k-complex contagions. We study two subtly different variants of Kleinberg's small world model and show that, with respect to complex contagions, they behave differently. For each model and each k \geq 2, we show that there is an intermediate range of values, such that when γ takes any of these values, a k-complex contagion spreads quickly on the corresponding graph, in a polylogarithmic number of rounds. However, if γ is outside this range, then a k-complex contagion requires a polynomial number of rounds to spread to the entire network. △ Less","9 August, 2014",https://arxiv.org/pdf/1408.2159
A model of grassroots changes in linguistic systems,Janet B. Pierrehumbert;Forrest Stonedahl;Robert Daland,"Linguistic norms emerge in human communities because people imitate each other. A shared linguistic system provides people with the benefits of shared knowledge and coordinated planning. Once norms are in place, why would they ever change? This question, echoing broad questions in the theory of social dynamics, has particular force in relation to language. By definition, an innovator is in the minority when the innovation first occurs. In some areas of social dynamics, important minorities can strongly influence the majority through their power, fame, or use of broadcast media. But most linguistic changes are grassroots developments that originate with ordinary people. Here, we develop a novel model of communicative behavior in communities, and identify a mechanism for arbitrary innovations by ordinary people to have a good chance of being widely adopted. To imitate each other, people must form a mental representation of what other people do. Each time they speak, they must also decide which form to produce themselves. We introduce a new decision function that enables us to smoothly explore the space between two types of behavior: probability matching (matching the probabilities of incoming experience) and regularization (producing some forms disproportionately often). Using Monte Carlo methods, we explore the interactions amongst the degree of regularization, the distribution of biases in a network, and the network position of the innovator. We identify two regimes for the widespread adoption of arbritrary innovations, viewed as informational cascades in the network. With moderate regularization of experienced input, average people (not well-connected people) are the most likely source of successful innovations. Our results shed light on a major outstanding puzzle in the theory of language change. The framework also holds promise for understanding the dynamics of other social norms. △ Less","8 August, 2014",https://arxiv.org/pdf/1408.1985
The first SPIE software Hack Day,Sarah Kendrew;Casey Deen;Nicole Radziwill;Steve Crawford;James Gilbert;Michael Gully-Santiago;Petr Kubanek,"We report here on the software Hack Day organised at the 2014 SPIE conference on Astronomical Telescopes and Instrumentation in Montreal. The first ever Hack Day to take place at an SPIE event, the aim of the day was to bring together developers to collaborate on innovative solutions to problems of their choice. Such events have proliferated in the technology community, providing opportunities to showcase, share and learn skills. In academic environments, these events are often also instrumental in building community beyond the limits of national borders, institutions and projects. We show examples of projects the participants worked on, and provide some lessons learned for future events. △ Less","6 August, 2014",https://arxiv.org/pdf/1408.1278
Early Development of UVM based Verification Environment of Image Signal Processing Designs using TLM Reference Model of RTL,Abhishek Jain;Dr. Hima Gupta;Sandeep Jana;Krishna Kumar,"With semiconductor industry trend of smaller the better, from an idea to a final product, more innovation on product portfolio and yet remaining competitive and profitable are few criteria which are culminating into pressure and need for more and more innovation for CAD flow, process management and project execution cycle. Project schedules are very tight and to achieve first silicon success is key for projects. This necessitates quicker verification with better coverage matrix. Quicker Verification requires early development of the verification environment with wider test vectors without waiting for RTL to be available. In this paper, we are presenting a novel approach of early development of reusable multi-language verification flow, by addressing four major activities of verification like Early creation of Executable Specification, Early creation of Verification Environment, Early development of test vectors and Better and increased Re-use of blocks. Although this paper focuses on early development of UVM based Verification Environment of Image Signal Processing designs using TLM Reference Model of RTL, same concept can be extended for non-image signal processing designs. Main Keywords are SystemVerilog, SystemC, Transaction Level Modeling, Universal Verification Methodology (UVM), Processor model, Universal Verification Component (UVC), Reference Model. △ Less","5 August, 2014",https://arxiv.org/pdf/1408.1150
Big Data Dimensional Analysis,Vijay Gadepally;Jeremy Kepner,"The ability to collect and analyze large amounts of data is a growing problem within the scientific community. The growing gap between data and users calls for innovative tools that address the challenges faced by big data volume, velocity and variety. One of the main challenges associated with big data variety is automatically understanding the underlying structures and patterns of the data. Such an understanding is required as a pre-requisite to the application of advanced analytics to the data. Further, big data sets often contain anomalies and errors that are difficult to know a priori. Current approaches to understanding data structure are drawn from the traditional database ontology design. These approaches are effective, but often require too much human involvement to be effective for the volume, velocity and variety of data encountered by big data systems. Dimensional Data Analysis (DDA) is a proposed technique that allows big data analysts to quickly understand the overall structure of a big dataset, determine anomalies. DDA exploits structures that exist in a wide class of data to quickly determine the nature of the data and its statical anomalies. DDA leverages existing schemas that are employed in big data databases today. This paper presents DDA, applies it to a number of data sets, and measures its performance. The overhead of DDA is low and can be applied to existing big data systems without greatly impacting their computing requirements. △ Less","3 August, 2014",https://arxiv.org/pdf/1408.0517
Memetic Search in Differential Evolution Algorithm,Sandeep Kumar;Vivek Kumar Sharma;Rajani Kumari,"Differential Evolution (DE) is a renowned optimization stratagem that can easily solve nonlinear and comprehensive problems. DE is a well known and uncomplicated population based probabilistic approach for comprehensive optimization. It has apparently outperformed a number of Evolutionary Algorithms and further search heuristics in the vein of Particle Swarm Optimization at what time of testing over both yardstick and actual world problems. Nevertheless, DE, like other probabilistic optimization algorithms, from time to time exhibits precipitate convergence and stagnates at suboptimal position. In order to stay away from stagnation behavior while maintaining an excellent convergence speed, an innovative search strategy is introduced, named memetic search in DE. In the planned strategy, positions update equation customized as per a memetic search stratagem. In this strategy a better solution participates more times in the position modernize procedure. The position update equation is inspired from the memetic search in artificial bee colony algorithm. The proposed strategy is named as Memetic Search in Differential Evolution (MSDE). To prove efficiency and efficacy of MSDE, it is tested over 8 benchmark optimization problems and three real world optimization problems. A comparative analysis has also been carried out among proposed MSDE and original DE. Results show that the anticipated algorithm go one better than the basic DE and its recent deviations in a good number of the experiments. △ Less","1 August, 2014",https://arxiv.org/pdf/1408.0101
A Taxonomy and Survey on eScience as a Service in the Cloud,Amelie Chi Zhou;Bingsheng He;Shadi Ibrahim,"Cloud computing has recently evolved as a popular computing infrastructure for many applications. Scientific computing, which was mainly hosted in private clusters and grids, has started to migrate development and deployment to the public cloud environment. eScience as a service becomes an emerging and promising direction for science computing. We review recent efforts in developing and deploying scientific computing applications in the cloud. In particular, we introduce a taxonomy specifically designed for scientific computing in the cloud, and further review the taxonomy with four major kinds of science applications, including life sciences, physics sciences, social and humanities sciences, and climate and earth sciences. Our major finding is that, despite existing efforts in developing cloud-based eScience, eScience still has a long way to go to fully unlock the power of cloud computing paradigm. Therefore, we present the challenges and opportunities in the future development of cloud-based eScience services, and call for collaborations and innovations from both the scientific and computer system communities to address those challenges. △ Less","28 July, 2014",https://arxiv.org/pdf/1407.7360
Price of Anarchy of Innovation Diffusion in Social Networks,Xilun Chen;Chenxia Wu,"There have been great efforts in studying the cascading behavior in social networks such as the innovation diffusion, etc. Game theoretically, in a social network where individuals choose from two strategies: A (the innovation) and B (the status quo) and get payoff from their neighbors for coordination, it has long been known that the Price of Anarchy (PoA) of this game is not 1, since the Nash equilibrium (NE) where all players take B (B Nash) is inferior to the one all players taking A (A Nash). However, no quantitative analysis has been performed to give an accurate upper bound of PoA in this game. In this paper, we adopt a widely used networked coordination game setting [3] to study how bad a Nash equilibrium can be and give a tight upper bound of the PoA of such games. We show that there is an NE that is slightly worse than the B Nash. On the other hand, the PoA is bounded and the worst NE cannot be much worse than the B Nash. In addition, we discuss how the PoA upper bound would change when compatibility between A and B is introduced, and show an intuitive result that the upper bound strictly decreases as the compatibility is increased. △ Less","28 July, 2014",https://arxiv.org/pdf/1407.7319
Modulation Formats and Waveforms for the Physical Layer of 5G Wireless Networks: Who Will be the Heir of OFDM?,Paolo Banelli;Stefano Buzzi;Giulio Colavolpe;Andrea Modenini;Fredrik Rusek;Alessandro Ugolini,"5G cellular communications promise to deliver the gigabit experience to mobile users, with a capacity increase of up to three orders of magnitude with respect to current LTE systems. There is widespread agreement that such an ambitious goal will be realized through a combination of innovative techniques involving different network layers. At the physical layer, the OFDM modulation format, along with its multiple-access strategy OFDMA, is not taken for granted, and several alternatives promising larger values of spectral efficiency are being considered. This paper provides a review of some modulation formats suited for 5G, enriched by a comparative analysis of their performance in a cellular environment, and by a discussion on their interactions with specific 5G ingredients. The interaction with a massive MIMO system is also discussed by employing real channel measurements. △ Less","22 July, 2014",https://arxiv.org/pdf/1407.5947
D4D-Senegal: The Second Mobile Phone Data for Development Challenge,Yves-Alexandre de Montjoye;Zbigniew Smoreda;Romain Trinquart;Cezary Ziemlicki;Vincent D. Blondel,"The D4D-Senegal challenge is an open innovation data challenge on anonymous call patterns of Orange's mobile phone users in Senegal. The goal of the challenge is to help address society development questions in novel ways by contributing to the socio-economic development and well-being of the Senegalese population. Participants to the challenge are given access to three mobile phone datasets. This paper describes the three datasets. The datasets are based on Call Detail Records (CDR) of phone calls and text exchanges between more than 9 million of Orange's customers in Senegal between January 1, 2013 to December 31, 2013. The datasets are: (1) antenna-to-antenna traffic for 1666 antennas on an hourly basis, (2) fine-grained mobility data on a rolling 2-week basis for a year with bandicoot behavioral indicators at individual level for about 300,000 randomly sampled users, (3) one year of coarse-grained mobility data at arrondissement level with bandicoot behavioral indicators at individual level for about 150,000 randomly sampled users △ Less","30 July, 2014",https://arxiv.org/pdf/1407.4885
VANET Applications: Hot Use Cases,Marie-Ange Lèbre;Frédéric Le Mouël;Eric Ménard;Julien Dillschneider;Richard Denis,"Current challenges of car manufacturers are to make roads safe, to achieve free flowing traffic with few congestions, and to reduce pollution by an effective fuel use. To reach these goals, many improvements are performed in-car, but more and more approaches rely on connected cars with communication capabilities between cars, with an infrastructure, or with IoT devices. Monitoring and coordinating vehicles allow then to compute intelligent ways of transportation. Connected cars have introduced a new way of thinking cars - not only as a mean for a driver to go from A to B, but as smart cars - a user extension like the smartphone today. In this report, we introduce concepts and specific vocabulary in order to classify current innovations or ideas on the emerging topic of smart car. We present a graphical categorization showing this evolution in function of the societal evolution. Different perspectives are adopted: a vehicle-centric view, a vehicle-network view, and a user-centric view; described by simple and complex use-cases and illustrated by a list of emerging and current projects from the academic and industrial worlds. We identified an empty space in innovation between the user and his car: paradoxically even if they are both in interaction, they are separated through different application uses. Future challenge is to interlace social concerns of the user within an intelligent and efficient driving. △ Less","15 July, 2014",https://arxiv.org/pdf/1407.4088
Improving energy efficiency in MANET's for healthcare environments,Sohail Abid;Imran Shafi;Shahid Abid,"Now a day ad hoc mobile networks (MANETs) have lots of routing protocols, but no one can meet maximum performance. Some are good in a small network; some are suitable in large networks, and some give better performance in location or global networks. Today modern and innovative applications for health care environments based on a wireless network are being developed in the commercial sectors. The emerging wireless networks are rapidly becoming a fundamental part of every single field of life. Our proposed DEERP framework gives a better performance as compared to other routing protocol. △ Less","10 July, 2014",https://arxiv.org/pdf/1407.2747
Offline handwritten signature identification using adaptive window positioning techniques,Ghazali Sulong;Anwar Yahy Ebrahim;Muhammad Jehanzeb,"The paper presents to address this challenge, we have proposed the use of Adaptive Window Positioning technique which focuses on not just the meaning of the handwritten signature but also on the individuality of the writer. This innovative technique divides the handwritten signature into 13 small windows of size nxn(13x13).This size should be large enough to contain ample information about the style of the author and small enough to ensure a good identification performance.The process was tested with a GPDS data set containing 4870 signature samples from 90 different writers by comparing the robust features of the test signature with that of the user signature using an appropriate classifier. Experimental results reveal that adaptive window positioning technique proved to be the efficient and reliable method for accurate signature feature extraction for the identification of offline handwritten signatures.The contribution of this technique can be used to detect signatures signed under emotional duress. △ Less","10 July, 2014",https://arxiv.org/pdf/1407.2700
ImpNet: Programming Software-Defied Networks Using Imperative Techniques,Mohamed A. El-Zawawy;Adel I. AlSalem,"Software and hardware components are basic parts of modern networks. However the software compo- nent is typical sealed and function-oriented. Therefore it is very difficult to modify these components. This badly affected networking innovations. Moreover, this resulted in network policies having complex interfaces that are not user-friendly and hence resulted in huge and complicated flow tables on physical switches of networks. This greatly degrades the network performance in many cases. Software-Defined Networks (SDNs) is a modern architecture of networks to overcome issues mentioned above. The idea of SDN is to add to the network a controller device that manages all the other devices on the network including physical switches of the network. One of the main tasks of the managing process is switch learning; achieved via programming physical switches of the network by adding or removing rules for packet-processing to/from switches, more specifically to/from their flow tables. A high-level imperative network programming language, called ImpNet, is presented in this paper. ImpNet enables writing efficient, yet simple, and powerful programs to run on the controller to control all other network devices including switches. ImpNet is compositional, simply-structured, expressive, and more importantly imperative. The syntax of ImpNet together two types of operational semantics to contracts of ImpNet are presented in the paper. The proposed semantics are of the static and dynamic types. Two modern application programmed using ImpNet are shown in the paper as well. The semantics of the applications are shown in the paper also. △ Less","8 July, 2014",https://arxiv.org/pdf/1407.2041
Deterministic Near-Optimal P2P Streaming,Shaileshh Bojja Venkatakrishnan;Pramod Viswanath,"We consider streaming over a peer-to-peer network with homogeneous nodes in which a single source broadcasts a data stream to all the users in the system. Peers are allowed to enter or leave the system (adversarially) arbitrarily. Previous approaches for streaming in this setting have either used randomized distribution graphs or structured trees with randomized maintenance algorithms. Randomized graphs handle peer churn well but have poor connectivity guarantees, while structured trees have good connectivity but have proven hard to maintain under peer churn. We improve upon both approaches by presenting a novel distribution structure with a deterministic and distributed algorithm for maintenance under peer churn; our result is inspired by a recent work proposing deterministic algorithms for rumor spreading in graphs. A key innovation in our approach is in having redundant links in the distribution structure. While this leads to a reduction in the maximum streaming rate possible, we show that for the amount of redundancy used, the delay guarantee of the proposed algorithm is near optimal. We introduce a tolerance parameter that captures the worst-case transient streaming rate received by the peers during churn events and characterize the fundamental tradeoff between rate, delay and tolerance. A natural generalization of the deterministic algorithm achieves this tradeoff near optimally. Finally, the proposed deterministic algorithm is robust enough to handle various generalizations: ability to deal with heterogeneous node capacities of the peers and more complicated streaming patterns where multiple source transmissions are present. △ Less","7 July, 2014",https://arxiv.org/pdf/1407.1931
Future Influence Ranking of Scientific Literature,Senzhang Wang;Sihong Xie;Xiaoming Zhang;Zhoujun Li;Philip S. Yu;Xinyu Shu,"Researchers or students entering a emerging research area are particularly interested in what newly published papers will be most cited and which young researchers will become influential in the future, so that they can catch the most recent advances and find valuable research directions. However, predicting the future importance of scientific articles and authors is challenging due to the dynamic nature of literature networks and evolving research topics. Different from most previous studies aiming to rank the current importance of literatures and authors, we focus on \emph{ranking the future popularity of new publications and young researchers} by proposing a unified ranking model to combine various available information. Specifically, we first propose to extract two kinds of text features, words and words co-occurrence to characterize innovative papers and authors. Then, instead of using static and un-weighted graphs, we construct time-aware weighted graphs to distinguish the various importance of links established at different time. Finally, by leveraging both the constructed text features and graphs, we propose a mutual reinforcement ranking framework called \emph{MRFRank} to rank the future importance of papers and authors simultaneously. Experimental results on the ArnetMiner dataset show that the proposed approach significantly outperforms the baselines on the metric \emph{recommendation intensity}. △ Less","7 July, 2014",https://arxiv.org/pdf/1407.1772
Enumeration of Spanning Trees Using Edge Exchange with Minimal Partitioning,Nasr Mohamed,"In this thesis, Minimal Partitioning (MP) algorithm, an innovative algorithm for enumerating all the spanning trees in an undirected graph is presented. While MP algorithm uses a computational tree graph to traverse all possible spanning trees by the edge exchange technique, it has two unique properties compared to previous algorithms. In the first place, the algorithm maintains a state of minimal partition size in the spanning tree due to edge deletion. This is realized by swapping peripheral edges, more precisely leaf edges, in most of edge exchange operations. Consequently, the main structure of the spanning trees is preserved during the steps of the enumeration process. This extra constraint proves to be advantageous in many applications where the partition size is a factor in the solution cost. Secondly, we introduce, and utilize, the new concept of edge promotion: the exchanged edges always share one end. Practically, and as a result of this property, the interface between the two partitions of the spanning tree during edge exchange has to be maintained from one side only. For a graph G(V,E), MP algorithm requires O(log V+E/V) expected time and OV log V) worst case time for generating each spanning tree. MP algorithm requires a total expected space limit of O(E log V) with worst case limit of O(EV). Like all edge exchange algorithms, MP algorithm retains the advantage of compacted output of O(1) per spanning tree by listing the relative differences only. Three sample real-world applications of spanning trees enumeration are explored and the effects of using MP algorithm are studied. Namely: construction of nets of polyhedra, multi-robots spanning tree routing, and computing the electric current in edges of a network. We report that MP algorithm outperforms other algorithm by O(V) time complexity. △ Less","2 July, 2014",https://arxiv.org/pdf/1407.0699
Measuring Team Creativity Through Longitudinal Social Signals,Peter A. Gloor;Adam Almozlino;Orr Inbar;Wei Lo;Shannon Provost,"Research into human dynamical systems has long sought to identify robust signals for human behavior. We have discovered a series of social network-based indicators that are reliable predictors of team creativity and collaborative innovation. We extract these signals from electronic records of interpersonal interactions, including e-mail, and face-to-face interaction measured via sociometric badges. The first of these signals is Rotating Leadership, measuring the degree to which, over time, actors in a team vary in how central they are to team's communication network's structure. The second is Rotating Contribution, which measures the degree to which, over time, actors in a team vary in the ratio of communications they distribute versus receive. The third is Prompt Response Time, which measures, over time, the responsiveness of actors to one another's communications. Finally, we demonstrate the predictive utility of these signals in a variety of contexts, showing them to be robust to various methods of evaluating innovation. △ Less","1 July, 2014",https://arxiv.org/pdf/1407.0440
Sprinklers: A Randomized Variable-Size Striping Approach to Reordering-Free Load-Balanced Switching,Weijun Ding;Jim Xu;Jim Dai;Yang Song;Bill Lin,"Internet traffic continues to grow exponentially, calling for switches that can scale well in both size and speed. While load-balanced switches can achieve such scalability, they suffer from a fundamental packet reordering problem. Existing proposals either suffer from poor worst-case packet delays or require sophisticated matching mechanisms. In this paper, we propose a new family of stable load-balanced switches called ""Sprinklers"" that has comparable implementation cost and performance as the baseline load-balanced switch, but yet can guarantee packet ordering. The main idea is to force all packets within the same virtual output queue (VOQ) to traverse the same ""fat path"" through the switch, so that packet reordering cannot occur. At the core of Sprinklers are two key innovations: a randomized way to determine the ""fat path"" for each VOQ, and a way to determine its ""fatness"" roughly in proportion to the rate of the VOQ. These innovations enable Sprinklers to achieve near-perfect load-balancing under arbitrary admissible traffic. Proving this property rigorously using novel worst-case large deviation techniques is another key contribution of this work. △ Less","17 July, 2014",https://arxiv.org/pdf/1407.0006
Crowd-Sourcing Fuzzy and Faceted Classification for Concept Search,Richard Absalom;Marcus Luczak-Rosch;Dap Hartmann;Aske Plaat,"Searching for concepts in science and technology is often a difficult task. To facilitate concept search, different types of human-generated metadata have been created to define the content of scientific and technical disclosures. Classification schemes such as the International Patent Classification (IPC) and MEDLINE's MeSH are structured and controlled, but require trained experts and central management to restrict ambiguity (Mork, 2013). While unstructured tags of folksonomies can be processed to produce a degree of structure (Kalendar, 2010; Karampinas, 2012; Sarasua, 2012; Bragg, 2013) the freedom enjoyed by the crowd typically results in less precision (Stock 2007). Existing classification schemes suffer from inflexibility and ambiguity. Since humans understand language, inference, implication, abstraction and hence concepts better than computers, we propose to harness the collective wisdom of the crowd. To do so, we propose a novel classification scheme that is sufficiently intuitive for the crowd to use, yet powerful enough to facilitate search by analogy, and flexible enough to deal with ambiguity. The system will enhance existing classification information. Linking up with the semantic web and computer intelligence, a Citizen Science effort (Good, 2013) would support innovation by improving the quality of granted patents, reducing duplicitous research, and stimulating problem-oriented solution design. A prototype of our design is in preparation. A crowd-sourced fuzzy and faceted classification scheme will allow for better concept search and improved access to prior art in science and technology. △ Less","30 June, 2014",https://arxiv.org/pdf/1406.7749
Social Technologies for Developing Collective Intelligence in Networked Society,Aelita Skarzauskiene;Birute Pitrenaite-Zileniene;Edgaras Leichteris;Zaneta Paunksniene;Monika Maciuliene,"The scientific problem in our project is defined as a question: how social technologies could contribute to the development of smart and inclusive society? The subject of our research are networked projects (virtual CI systems) which include collective decision making tools and innovation mechanisms allowing and encouraging individual and team creativity, entrepreneurship, online collaboration, new forms of self-regulation and self-governance, self-configuration of communities by considering these projects as being catalyst for emergence of CI. The answers to these theoretical questions could have huge practical implications by influencing more reasonable and sophisticated application of social technologies in practice. △ Less","29 June, 2014",https://arxiv.org/pdf/1406.7549
Open Collaboration for Innovation: Principles and Performance,Sheen Levine;Michael Prietula,"The principles of open collaboration for innovation (and production), once distinctive to open source software, are now found in many other ventures. Some of these ventures are internet-based: Wikipedia, online forums and communities. Others are off-line: in medicine, science, and everyday life. Such ventures have been affecting traditional firms, and may represent a new organizational form. Despite the impact of such ventures, questions remain about their operating principles and performance. Here we define open collaboration (OC), the underlying set of principles, and propose that it is a robust engine for innovation and production. First, we review multiple OC ventures and identify four defining principles. In all instances, participants create goods and services of economic value, they exchange and reuse each other's work, they labor purposefully with just loose coordination, and they permit anyone to contribute and consume. These principles distinguish OC from other organizational forms, such as firms or cooperatives. Next, we turn to performance. To understand the performance of OC, we develop a computational model, combining innovation theory with recent evidence on human cooperation. We identify and investigate three elements that affect performance: the cooperativeness of participants, the diversity of their needs, and the degree to which the goods are rival (subtractable). Through computational experiments, we find that OC performs well even in seemingly harsh environments: when cooperators are a minority, free riders are present, diversity is lacking, or goods are rival. We conclude that OC is viable and likely to expand into new domains. The findings also inform the discussion on new organizational forms, collaborative and communal. △ Less","29 June, 2014",https://arxiv.org/pdf/1406.7541
The architecture of innovation: Tracking face-to-face interactions with ubicomp technologies,Chloë Brown;Christos Efstratiou;Ilias Leontiadis;Daniele Quercia;Cecilia Mascolo;James Scott;Peter Key,"The layouts of the buildings we live in shape our everyday lives. In office environments, building spaces affect employees' communication, which is crucial for productivity and innovation. However, accurate measurement of how spatial layouts affect interactions is a major challenge and traditional techniques may not give an objective view.We measure the impact of building spaces on social interactions using wearable sensing devices. We study a single organization that moved between two different buildings, affording a unique opportunity to examine how space alone can affect interactions. The analysis is based on two large scale deployments of wireless sensing technologies: short-range, lightweight RFID tags capable of detecting face-to-face interactions. We analyze the traces to study the impact of the building change on social behavior, which represents a first example of using ubiquitous sensing technology to study how the physical design of two workplaces combines with organizational structure to shape contact patterns. △ Less","26 June, 2014",https://arxiv.org/pdf/1406.6829
Computing on Masked Data: a High Performance Method for Improving Big Data Veracity,Jeremy Kepner;Vijay Gadepally;Pete Michaleas;Nabil Schear;Mayank Varia;Arkady Yerukhimovich;Robert K. Cunningham,"The growing gap between data and users calls for innovative tools that address the challenges faced by big data volume, velocity and variety. Along with these standard three V's of big data, an emerging fourth ""V"" is veracity, which addresses the confidentiality, integrity, and availability of the data. Traditional cryptographic techniques that ensure the veracity of data can have overheads that are too large to apply to big data. This work introduces a new technique called Computing on Masked Data (CMD), which improves data veracity by allowing computations to be performed directly on masked data and ensuring that only authorized recipients can unmask the data. Using the sparse linear algebra of associative arrays, CMD can be performed with significantly less overhead than other approaches while still supporting a wide range of linear algebraic operations on the masked data. Databases with strong support of sparse operations, such as SciDB or Apache Accumulo, are ideally suited to this technique. Examples are shown for the application of CMD to a complex DNA matching algorithm and to database operations over social media data. △ Less","22 June, 2014",https://arxiv.org/pdf/1406.5751
Performance Evaluation of Incremental K-means Clustering Algorithm,Sanjay Chakraborty;N. K. Nagwani,"The incremental K-means clustering algorithm has already been proposed and analysed in paper [Chakraborty and Nagwani, 2011]. It is a very innovative approach which is applicable in periodically incremental environment and dealing with a bulk of updates. In this paper the performance evaluation is done for this incremental K-means clustering algorithm using air pollution database. This paper also describes the comparison on the performance evaluations between existing K-means clustering and incremental K-means clustering using that particular database. It also evaluates that the particular point of change in the database upto which incremental K-means clustering performs much better than the existing K-means clustering. That particular point of change in the database is known as ""Threshold value"" or ""% delta change in the database"". This paper also defines the basic methodology for the incremental K-means clustering algorithm. △ Less","18 June, 2014",https://arxiv.org/pdf/1406.4737
Extracting information from S-curves of language change,Fakhteh Ghanbarnejad;Martin Gerlach;Jose M. Miotto;Eduardo G. Altmann,"It is well accepted that adoption of innovations are described by S-curves (slow start, accelerating period, and slow end). In this paper, we analyze how much information on the dynamics of innovation spreading can be obtained from a quantitative description of S-curves. We focus on the adoption of linguistic innovations for which detailed databases of written texts from the last 200 years allow for an unprecedented statistical precision. Combining data analysis with simulations of simple models (e.g., the Bass dynamics on complex networks) we identify signatures of endogenous and exogenous factors in the S-curves of adoption. We propose a measure to quantify the strength of these factors and three different methods to estimate it from S-curves. We obtain cases in which the exogenous factors are dominant (in the adoption of German orthographic reforms and of one irregular verb) and cases in which endogenous factors are dominant (in the adoption of conventions for romanization of Russian names and in the regularization of most studied verbs). These results show that the shape of S-curve is not universal and contains information on the adoption mechanism. (published at ""J. R. Soc. Interface, vol. 11, no. 101, (2014) 1044""; DOI: http://dx.doi.org/10.1098/rsif.2014.1044) △ Less","30 October, 2014",https://arxiv.org/pdf/1406.4498
Compressed Sensing Applied to Weather Radar,Kumar Vijay Mishra;Anton Kruger;Witold F. Krajewski,"We propose an innovative meteorological radar, which uses reduced number of spatiotemporal samples without compromising the accuracy of target information. Our approach extends recent research on compressed sensing (CS) for radar remote sensing of hard point scatterers to volumetric targets. The previously published CS-based radar techniques are not applicable for sampling weather since the precipitation echoes lack sparsity in both range-time and Doppler domains. We propose an alternative approach by adopting the latest advances in matrix completion algorithms to demonstrate the sparse sensing of weather echoes. We use Iowa X-band Polarimetric (XPOL) radar data to test and illustrate our algorithms. △ Less","13 June, 2014",https://arxiv.org/pdf/1406.3582
Toward a Local Perspective on Online Collaboration,Hani Safadi;Samer Faraj,"We study the structural properties of large scale collaboration in online communities of innovation and the role that position in the community plays in determining knowledge contribution. Contrary to previous research, we argue for a more local perspective when examining online collaboration. We demonstrate that a member's centrality and spanning within his/her local neighborhood is a better predictor of contribution than global centrality and spanning within the whole community. We contribute both theoretically and methodologically to research on large scale collaboration. On the theoretical front, a local view of position implies a more confined and local organization of work in online communities than previously thought. From a methodological perspective, evaluating the local structure of large networks involves radically different algorithms that have only recently become feasible with the increase of processing power. △ Less","11 June, 2014",https://arxiv.org/pdf/1406.2977
Out Performance Of Cuckoo Search Algorithm Among Nature Inspired Algorithms in Planar Antenna Arrays,A. Sai Charan;N. K. Manasa;Prof. N. V. S. N. Sarma,"In this modern era a great deal of metamorphism is observed around us which eventuate due to some minute modifications and innovations in the area of Science and Technology. This paper deals with the application of a meta heuristic optimization algorithm namely the Cuckoo Search Algorithm in the design of an optimized planar antenna array which ensures high gain,directivity, suppression of side lobes, increased efficiency and improves other antenna parameters as well. △ Less","11 June, 2014",https://arxiv.org/pdf/1406.2777
Subsidization Competition: Vitalizing the Neutral Internet,Richard T. B. Ma,"Unlike telephone operators, which pay termination fees to reach the users of another network, Internet Content Providers (CPs) do not pay the Internet Service Providers (ISPs) of users they reach. While the consequent cross subsidization to CPs has nurtured content innovations at the edge of the Internet, it reduces the investment incentives for the access ISPs to expand capacity. As potential charges for terminating CPs' traffic are criticized under the net neutrality debate, we propose to allow CPs to voluntarily subsidize the usagebased fees induced by their content traffic for end-users. We model the regulated subsidization competition among CPs under a neutral network and show how deregulation of subsidization could increase an access ISP's utilization and revenue, strengthening its investment incentives. Although the competition might harm certain CPs, we find that the main cause comes from high access prices rather than the existence of subsidization. Our results suggest that subsidization competition will increase the competitiveness and welfare of the Internet content market; however, regulators might need to regulate access prices if the access ISP market is not competitive enough. We envision that subsidization competition could become a viable model for the future Internet. △ Less","10 June, 2014",https://arxiv.org/pdf/1406.2516
Provenance and data differencing for workflow reproducibility analysis,Paolo Missier;Simon Woodman;Hugo Hiden;Paul Watson,"One of the foundations of science is that researchers must publish the methodology used to achieve their results so that others can attempt to reproduce them. This has the added benefit of allowing methods to be adopted and adapted for other purposes. In the field of e-Science, services -- often choreographed through workflow, process data to generate results. The reproduction of results is often not straightforward as the computational objects may not be made available or may have been updated since the results were generated. For example, services are often updated to fix bugs or improve algorithms. This paper addresses these problems in three ways. Firstly, it introduces a new framework to clarify the range of meanings of ""reproducibility"". Secondly, it describes a new algorithm, \PDIFF, that uses a comparison of workflow provenance traces to determine whether an experiment has been reproduced; the main innovation is that if this is not the case then the specific point(s) of divergence are identified through graph analysis, assisting any researcher wishing to understand those differences. One key feature is support for user-defined, semantic data comparison operators. Finally, the paper describes an implementation of \PDIFF that leverages the power of the e-Science Central platform which enacts workflows in the cloud. As well as automatically generating a provenance trace for consumption by \PDIFF, the platform supports the storage and re-use of old versions of workflows, data and services; the paper shows how this can be powerfully exploited in order to achieve reproduction and re-use. △ Less","3 June, 2014",https://arxiv.org/pdf/1406.0905
Improving Computer Assisted Speech Therapy Through Speech Based Emotion Recognition,Ovidiu Schipor,"Speech therapy consists in a wide range of services whose aim is to prevent, diagnose and treat different types of speech impairments. One of the most important conditions for obtaining favourable and steady results is the ""immersing"" of the subject as long as possible into therapeutic context: at home, at school/work, on the street. Since nowadays portable computers tend to become habitual accessories, it seems a good idea to create virtual versions of human SLTs and to integrate them into these devices. However one of the main distinctions between a Speech and Language Therapist (SLT) and a Computer Based Speech Therapy System (CBST) arise from the field of emotion intelligence. The inability of current CBSTs to detect emotional state of human subjects leads to inadequate behavioural responses. Furthermore, this ""unresponsive"" behaviour is perceived as a lack of empathy and, especially when subjects are children, leads to negative emotional state such as frustration. Thus in this article we propose an original emotions recognition framework - named PhonEM - to be integrated in our previous developed CBST - Logomon. The originality consists in both emotions representation (a fuzzy model) and detection (using only subjects' speech stream). These exceptional restrictions along with the fuzzy representation of emotions lie at the origin of our approach and make our task a difficult and, in the same time, an innovative one. As far as we know, this is the first attempt to combine these techniques in order to improve assisted speech therapy and the obtained results encourage as to further develop our CBST. △ Less","30 May, 2014",https://arxiv.org/pdf/1405.7796
Complex contagion process in spreading of online innovation,Márton Karsai;Gerardo Iñiguez;Kimmo Kaski;János Kertész,"Diffusion of innovation can be interpreted as a social spreading phenomena governed by the impact of media and social interactions. Although these mechanisms have been identified by quantitative theories, their role and relative importance are not entirely understood, since empirical verification has so far been hindered by the lack of appropriate data. Here we analyse a dataset recording the spreading dynamics of the world's largest Voice over Internet Protocol service to empirically support the assumptions behind models of social contagion. We show that the rate of spontaneous service adoption is constant, the probability of adoption via social influence is linearly proportional to the fraction of adopting neighbours, and the rate of service termination is time-invariant and independent of the behaviour of peers. By implementing the detected diffusion mechanisms into a dynamical agent-based model, we are able to emulate the adoption dynamics of the service in several countries worldwide. This approach enables us to make medium-term predictions of service adoption and disclose dependencies between the dynamics of innovation spreading and the socioeconomic development of a country. △ Less","23 October, 2014",https://arxiv.org/pdf/1405.6879
Proximal Reinforcement Learning: A New Theory of Sequential Decision Making in Primal-Dual Spaces,Sridhar Mahadevan;Bo Liu;Philip Thomas;Will Dabney;Steve Giguere;Nicholas Jacek;Ian Gemp;Ji Liu,"In this paper, we set forth a new vision of reinforcement learning developed by us over the past few years, one that yields mathematically rigorous solutions to longstanding important questions that have remained unresolved: (i) how to design reliable, convergent, and robust reinforcement learning algorithms (ii) how to guarantee that reinforcement learning satisfies pre-specified ""safety"" guarantees, and remains in a stable region of the parameter space (iii) how to design ""off-policy"" temporal difference learning algorithms in a reliable and stable manner, and finally (iv) how to integrate the study of reinforcement learning into the rich theory of stochastic optimization. In this paper, we provide detailed answers to all these questions using the powerful framework of proximal operators. The key idea that emerges is the use of primal dual spaces connected through the use of a Legendre transform. This allows temporal difference updates to occur in dual spaces, allowing a variety of important technical advantages. The Legendre transform elegantly generalizes past algorithms for solving reinforcement learning problems, such as natural gradient methods, which we show relate closely to the previously unconnected framework of mirror descent methods. Equally importantly, proximal operator theory enables the systematic development of operator splitting methods that show how to safely and reliably decompose complex products of gradients that occur in recent variants of gradient-based temporal difference learning. This key technical innovation makes it possible to finally design ""true"" stochastic gradient methods for reinforcement learning. Finally, Legendre transforms enable a variety of other benefits, including modeling sparsity and domain geometry. Our work builds extensively on recent work on the convergence of saddle-point algorithms, and on the theory of monotone operators. △ Less","26 May, 2014",https://arxiv.org/pdf/1405.6757
Changing minds about electric cars: An empirically grounded agent-based modeling approach,Ingo Wolf;Tobias Schroeder;Jochen Neumann;Gerhard de Haan,"The diffusion of electric vehicles (EVs) is considered an effective policy strategy to meet greenhouse gas reduction targets. For large-scale adoption, however, demand-side oriented policy measures are required, based on consumers transport needs, values and social norms. We introduce an empirically grounded, spatially explicit, agent-based model, InnoMind Innovation diffusion driven by changing Minds), to simulate the effects of policy interventions and social influence on consumers transport mode preferences. The agents in this model represent individual consumers. They are calibrated based on empirically derived attributes and characteristics of survey respondents. We model agent decision-making with artificial neural networks that account for the role of emotions in information processing. We present simulations of 4 scenarios for the diffusion of EVs in the city of Berlin, Germany (3 policy scenarios and 1 base case). The results illustrate the varying effectiveness of measures in different market segments and the need for appropriate policies tailored to the heterogeneous needs of different travelers. Moreover, the simulations suggest that introducing an exclusive zone for EVs in the city would accelerate the early-phase diffusion of EVs more effectively than financial incentives only. △ Less","26 August, 2014",https://arxiv.org/pdf/1405.6230
Self-referencing cellular automata: A model of the evolution of information control in biological systems,Theodore P. Pavlic;Alyssa M. Adams;Paul C. W. Davies;Sara Imari Walker,"Cellular automata have been useful artificial models for exploring how relatively simple rules combined with spatial memory can give rise to complex emergent patterns. Moreover, studying the dynamics of how rules emerge under artificial selection for function has recently become a powerful tool for understanding how evolution can innovate within its genetic rule space. However, conventional cellular automata lack the kind of state feedback that is surely present in natural evolving systems. Each new generation of a population leaves an indelible mark on its environment and thus affects the selective pressures that shape future generations of that population. To model this phenomenon, we have augmented traditional cellular automata with state-dependent feedback. Rather than generating automata executions from an initial condition and a static rule, we introduce mappings which generate iteration rules from the cellular automaton itself. We show that these new automata contain disconnected regions which locally act like conventional automata, thus encapsulating multiple functions into one structure. Consequently, we have provided a new model for processes like cell differentiation. Finally, by studying the size of these regions, we provide additional evidence that the dynamics of self-reference may be critical to understanding the evolution of natural language. In particular, the rules of elementary cellular automata appear to be distributed in the same way as words in the corpus of a natural language. △ Less","16 May, 2014",https://arxiv.org/pdf/1405.4070
"Can Online MBA Programs Allow Professional Working Mothers to Balance Work, Family, and Career Progression? A Case Study in China",Mboni Kibelloh;Yukun Bao,"Career progression is a general concern of professional working mothers in China. The purpose of this paper is to report a qualitative study of Chinese professional working mothers that explored the perceptions of online Master's of Business Administration (MBA) programmes as a tool for career progression for working mothers balancing work and family in China. The objective was to examine existing work-family and career progression conflicts, the perceived usefulness of online MBA in balancing work-family and career aspirations, and the perceived ease of use of e-learning. Using Davis's (1989) technology acceptance model (TAM), the research drew on in-depth interviews with 10 female part-time MBA students from a university in Wuhan. The data were analysed through coding and transcribing. The findings showed that conflicts arose where demanding work schedules competed with family obligations, studies, and caring for children and the elderly. Online MBA programmes were viewed as a viable tool for balancing work and family and studying, given its flexible time management capabilities. However, consideration must be given to address students' motivation issues, lack of networking, lack of face-to-face interaction, and quality. The research findings emphasise the pragmatic need to re-align higher education policy and practice to position higher education e-learning as a trustable education delivery channel in China. By shedding light on the prevailing work-family conflict experienced by women seeking career advancement, this study suggests developing better gender-supporting policies and innovative e-learning practices to champion online MBA programme for this target niche. △ Less","14 May, 2014",https://arxiv.org/pdf/1405.3381
The Past and the Future of Holocaust Research: From Disparate Sources to an Integrated European Holocaust Research Infrastructure,Reto Speck;Tobias Blanke;Cony Kristel;Michal Frankl;Kepa Rodriguez;Veerle Vanden Daelen,"The European Holocaust Research Infrastructure (EHRI) has been set up by the European Union to create a sustainable complex of services for researchers. EHRI will bring together information about dispersed collections, based on currently more than 20 partner organisations in 13 countries and many other archives. EHRI, which brings together historians, archivists and specialists in digital humanities, strives to develop innovative on-line tools for finding, researching and sharing knowledge about the Holocaust. While connecting information about Holocaust collections, it strives to create tools and approaches applicable to other digital archival projects. The paper describes its current progress and collaboration across the disciplines involved. △ Less","10 May, 2014",https://arxiv.org/pdf/1405.2407
Cooperative Beamforming for Cognitive Radio-Based Broadcasting Systems with Asynchronous Interferences,Mai H. Hassan;Md. J. Hossain,"In order to address the asynchronous interference issue for a generalized scenario with multiple primary and multiple secondary receivers, in this paper, we propose an innovative cooperative beamforming technique. In particular, the cooperative beamforming design is formulated as an optimization problem that maximizes the weighted sum achievable transmission rate of secondary destinations while it maintains the asynchronous interferences at the primary receivers below their target thresholds. In light of the intractability of the problem, we propose a two-phase suboptimal cooperative beamforming technique. First, it finds the beamforming directions corresponding to different secondary destinations. Second, it allocates the power among different beamforming directions. Due to the multiple interference constraints corresponding to multiple primary receivers, the power allocation scheme in the second phase is still complex. Therefore, we also propose a low complex power allocation algorithm. The proposed beamforming technique is extended for the cases, when cooperating CR nodes (CCRNs) have statistical or erroneous channel knowledge of the primary receivers. We also investigate the performance of joint CCRN selection and beamforming technique. The presented numerical results show that the proposed beamforming technique can significantly reduce the asynchronous interference signals at the primary receivers and increase the sum transmission rate of secondary destinations compared to the well known zero-forcing beamforming (ZFBF) technique. △ Less","8 May, 2014",https://arxiv.org/pdf/1405.1802
What Cost Knowledge Management? The Example of Infosys,Chris Kimble,"The term knowledge management (KM) first came to prominence in the late 1990s. Although initially dismissed as a fad, KM continues to be featured in articles concerning business productivity and innovation. And yet, clear-cut examples that demonstrate the success of KM are few and far between. A brief examination of the history of KM explores the reasons for this and looks at some of the assumptions about what KM can achieve. A subsequent analysis of the experiences of Infosys with KM shows that for KM to be successful, organizational leaders need to engage in a continuous process of modification and maintenance. Although KM initiatives can be made to yield worthwhile returns over an extended period, there are often substantial ongoing costs associated with them. △ Less","29 April, 2014",https://arxiv.org/pdf/1405.1679
Configuration in ERP SaaS Multi-Tenancy,Djamal Ziani,"Software as a Service (SaaS) becomes in this decade the focus of many enterprises and research. SaaS provides software application as Web based delivery to server many customers. This sharing of infrastructure and application provided by Saas has a great benefit to customers, since it reduces costs, minimizes risks, improves their competitive positioning, as well as seeks out innovative. SaaS application is generally developed with standardized software functionalities to serve as many customers as possible. However many customers ask to change the standardized provided functions according to their specific business needs, and this can be achieve through the configuration and customization provided by the SaaS vendor. Allowing many customers to change software configurations without impacting others customers and with preserving security and efficiency of the provided services, becomes a big challenge to SaaS vendors, who are oblige to design new strategies and architectures. Multi-tenancy (MT) architectures allow multiple customers to be consolidated into the same operational system without changing anything in the vendor source code. In this paper, we will present how the configuration can be done on an ERP web application in a Multi-Tenancy SaaS environment. △ Less","4 May, 2014",https://arxiv.org/pdf/1405.0650
Assessing the players'performance in the game of bridge: A fuzzy logic approach,Michael Gr. Voskoglou,"Contract bridge occupies nowadays a position of great prestige being, together with chess, the only mind games officially recognized by the International Olympic Committee. In the present paper an innovative method for assessing the total performance of bridge- players' belonging to groups of special interest(e.g. different bridge clubs during a tournament, men and women, new and old players, etc) is introduced, which is based on principles of fuzzy logic. For this, the cohorts under assessment are represented as fuzzy subsets of a set of linguistic labels characterizing their performance and the centroid defuzzification method is used to convert the fuzzy data collected from the game to a crisp number. This new method of assessment could be used informally as a complement of the official bridge-scoring methods for statistical and other obvious reasons. Two real applications related to simultaneous tournaments with pre-dealt boards, organized by the Hellenic Bridge Federation, are also presented, illustrating the importance of our results in practice. △ Less","29 April, 2014",https://arxiv.org/pdf/1404.7279
Inventions on dialog boxes used in GUI,Umakant Mishra,"The dialog boxes are useful in case of displaying warnings, errors, confirmations etc. in special situations. A typical dialog box is displayed in a small window with some text message along with a few options for the user to select. However, there are certain difficulties associated in programming and implementing a conventional dialog box, such as, severe programming effort, rigidity of the hard coded message, obscuring screen space and so on. There is a need to overcome these difficulties of the dialog box to make them more efficient and useful. The modality of the dialog boxes also creates some limitations. While modal dialog boxes needs to be closed explicitly by the user, modeless dialog boxes can grow in number and become difficult to control. Thus, an ideal dialog box should be deprived of all the above-mentioned drawbacks. The dialog box should not obscure the screen. The user should be able open multiple dialog boxes but without obscuring the screen. This article analyses 5 interesting inventions on dialog boxes selected from US Patent database. Each invention tries to overcome some limitations of a conventional dialog box and provides some innovative features. Each solution is also analyzed from a TRIZ perspective. △ Less","27 April, 2014",https://arxiv.org/pdf/1404.6754
SimpleTrack:Adaptive Trajectory Compression with Deterministic Projection Matrix for Mobile Sensor Networks,Rajib Rana;Mingrui Yang;Tim Wark;Chun Tung Chou;Wen Hu,"Some mobile sensor network applications require the sensor nodes to transfer their trajectories to a data sink. This paper proposes an adaptive trajectory (lossy) compression algorithm based on compressive sensing. The algorithm has two innovative elements. First, we propose a method to compute a deterministic projection matrix from a learnt dictionary. Second, we propose a method for the mobile nodes to adaptively predict the number of projections needed based on the speed of the mobile nodes. Extensive evaluation of the proposed algorithm using 6 datasets shows that our proposed algorithm can achieve sub-metre accuracy. In addition, our method of computing projection matrices outperforms two existing methods. Finally, comparison of our algorithm against a state-of-the-art trajectory compression algorithm show that our algorithm can reduce the error by 10-60 cm for the same compression ratio. △ Less","23 April, 2014",https://arxiv.org/pdf/1404.6151
Faculty Attitudes Towards Integrating Technology and Innovation,Colleen Marzilli;Julie Delello;Shelly Marmion;Rochell McWhorter;Paul Roberts;T. Scott Marzilli,"Technological innovation is an important aspect of teaching and learning in the 21st century. This article examines faculty attitudes toward technology use in the classroom at one regional public university in the United States. Building on a faculty-led initiative to develop a Community of Practice for improving education, this study used a mixed-method approach of a faculty-developed, electronic survey to assess this topic. Findings from 72 faculty members revealed an overall positive stance toward technology in the classroom and the average faculty member utilized about six technology tools in their courses. The opportunities, barriers and future uses for technologies in the higher education classroom emerged from the open-ended questions on the survey. One finding of particular concern is that faculty are fearful that technology causes a loss of the humanistic perspective in education. The university is redesigning ten of its most popular courses to increase flexibility, accessibility and student success. △ Less","15 April, 2014",https://arxiv.org/pdf/1404.4334
Big Data: Overview,Richa Gupta;Sunny Gupta;Anuradha Singhal,"Big data is data that exceeds the processing capacity of traditional databases. The data is too big to be processed by a single machine. New and innovative methods are required to process and store such large volumes of data. This paper provides an overview on big data, its importance in our live and some technologies to handle big data.","16 April, 2014",https://arxiv.org/pdf/1404.4136
Conditions for viral influence spreading through multiplex correlated social networks,Yanqing Hu;Shlomo Havlin;Hernán A. Makse,"A fundamental problem in network science is to predict how certain individuals are able to initiate new networks to spring up ""new ideas"". Frequently, these changes in trends are triggered by a few innovators who rapidly impose their ideas through ""viral"" influence spreading producing cascades of followers fragmenting an old network to create a new one. Typical examples include the raise of scientific ideas or abrupt changes in social media, like the raise of Facebook.com to the detriment of Myspace.com. How this process arises in practice has not been conclusively demonstrated. Here, we show that a condition for sustaining a viral spreading process is the existence of a multiplex correlated graph with hidden ""influence links"". Analytical solutions predict percolation phase transitions, either abrupt or continuous, where networks are disintegrated through viral cascades of followers as in empirical data. Our modeling predicts the strict conditions to sustain a large viral spreading via a scaling form of the local correlation function between multilayers, which we also confirm empirically. Ultimately, the theory predicts the conditions for viral cascading in a large class of multiplex networks ranging from social to financial systems and markets. △ Less","27 May, 2014",https://arxiv.org/pdf/1404.3114
User Centered Development of Agent-based Business Process Models and Notations,Robert Singer,"We discuss questions about user centric development of business process modeling notations. In the center of our research there is a fully featured multi-enterprise business process platform (ME-BPP) based on the concepts of agent-based business processes, which builds on the formal foundations of the subject-oriented business process management methodology (S-BPM). The platform is implemented based on cloud technology using commercial services. Additionally we developed a ""block modeling"" technique to find a semantically transparent modeling notation which can be used by novice users to model subject-oriented business process (S-BPM) models. As this is ongoing research there are still serious open questions. But, the presented approach breaks with some of the rules of typical process modeling notations and hopefully stimulates innovation. Additionally we want to continue our research towards the enhancement of our modeling approach towards a user centric ""syntax and semantic free"" modeling technique to develop user and domain specific modeling notations. △ Less","13 May, 2014",https://arxiv.org/pdf/1404.2737
TARDIS: Stably shifting traffic in space and time (extended version),Richard G. Clegg;Raul Landa;João Taveira Araújo;Eleni Mykoniati;David Griffin;Miguel Rio,"This paper describes TARDIS (Traffic Assignment and Retiming Dynamics with Inherent Stability) which is an algorithmic procedure designed to reallocate traffic within Internet Service Provider (ISP) networks. Recent work has investigated the idea of shifting traffic in time (from peak to off-peak) or in space (by using different links). This work gives a unified scheme for both time and space shifting to reduce costs. Particular attention is given to the commonly used 95th percentile pricing scheme. The work has three main innovations: firstly, introducing the Shapley Gradient, a way of comparing traffic pricing between different links at different times of day; secondly, a unified way of reallocating traffic in time and/or in space; thirdly, a continuous approximation to this system is proved to be stable. A trace-driven investigation using data from two service providers shows that the algorithm can create large savings in transit costs even when only small proportions of the traffic can be shifted. △ Less","8 April, 2014",https://arxiv.org/pdf/1404.2325
Collective Innovation in Open Source Hardware,Harris Kyriakou;Jeffrey V. Nickerson,"A growing community that shares digital 3D designs has created an opportunity to study, encourage and stimulate innovation. This remix community allows people not only to prototype at a minimal cost but also to work on projects they are genuinely interested in. Participants free of the limitations typically imposed by formal organizations develop products driven by their own interest. △ Less","7 April, 2014",https://arxiv.org/pdf/1404.1799
Nanowire Volatile RAM as an Alternative to SRAM,Mostafizur Rahman;Santosh Khasanvis;Csaba Andras Moritz,"Maintaining benefits of CMOS technology scaling is becoming challenging due to increased manufacturing complexities and unwanted passive power dissipations. This is particularly challenging in SRAM, where manufacturing precision and leakage power control are critical issues. To alleviate some of these challenges a novel non-volatile memory alternative to SRAM was proposed called nanowire volatile RAM (NWRAM). Due to NWRAMs regular grid based layout and innovative circuit style, manufacturing complexity is reduced and at the same time considerable benefits are attained in terms of performance and leakage power reduction. In this paper, we elaborate more on NWRAM circuit aspects and manufacturability, and quantify benefits at 16nm technology node through simulation against state-of-the-art 6T-SRAM and gridded 8T-SRAM designs. Our results show the 10T-NWRAM to be 2x faster and 35x better in terms of leakage when compared to high performance gridded 8T-SRAM design. △ Less","2 April, 2014",https://arxiv.org/pdf/1404.0615
The diffusion dynamics of choice: From durable goods markets to fashion first names,Baptiste Coulmont;Virginie Supervie;Romulus Breban,"Goods, styles, ideologies are adopted by society through various mechanisms. In particular, adoption driven by innovation is extensively studied by marketing economics. Mathematical models are currently used to forecast the sales of innovative goods. Inspired by the theory of diffusion processes developed for marketing economics, we propose, for the first time, a predictive framework for the mechanism of fashion, which we apply to first names. Analyses of French, Dutch and US national databases validate our modelling approach for thousands of first names, covering, on average, more than 50% of the yearly incidence in each database. In these cases, it is thus possible to forecast how popular the first names will become and when they will run out of fashion. Furthermore, we uncover a clear distinction between popularity and fashion: less popular names, typically not included in studies of fashion, may be driven by fashion, as well. △ Less","1 April, 2014",https://arxiv.org/pdf/1404.0267
Blind Recognition of Touched Keys: Attack and Countermeasures,Qinggang Yue;Zhen Ling;Benyuan Liu;Xinwen Fu;Wei Zhao,"In this paper, we introduce a novel computer vision based attack that discloses inputs on a touch enabled device, while the attacker cannot see any text or popups from a video of the victim tapping on the touch screen. In the attack, we use the optical flow algorithm to identify touching frames where the finger touches the screen surface. We innovatively use intersections of detected edges of the touch screen to derive the homography matrix mapping the touch screen surface in video frames to a reference image of the virtual keyboard. We analyze the shadow formation around the fingertip and use the k-means clustering algorithm to identify touched points. Homography can then map these touched points to keys of the virtual keyboard. Our work is substantially different from existing work. We target password input and are able to achieve a high success rate. We target scenarios like classrooms, conferences and similar gathering places and use a webcam or smartphone camera. In these scenes, single-lens reflex (SLR) cameras and high-end camcorders used in related work will appear suspicious. To defeat such computer vision based attacks, we design, implement and evaluate the Privacy Enhancing Keyboard (PEK) where a randomized virtual keyboard is used to input sensitive information. △ Less","19 March, 2014",https://arxiv.org/pdf/1403.4829
Improving Performance of a Group of Classification Algorithms Using Resampling and Feature Selection,Mehdi Naseriparsa;Amir-masoud Bidgoli;Touraj Varaee,"In recent years the importance of finding a meaningful pattern from huge datasets has become more challenging. Data miners try to adopt innovative methods to face this problem by applying feature selection methods. In this paper we propose a new hybrid method in which we use a combination of resampling, filtering the sample domain and wrapper subset evaluation method with genetic search to reduce dimensions of Lung-Cancer dataset that we received from UCI Repository of Machine Learning databases. Finally, we apply some well- known classification algorithms (Naïve Bayes, Logistic, Multilayer Perceptron, Best First Decision Tree and JRIP) to the resulting dataset and compare the results and prediction rates before and after the application of our feature selection method on that dataset. The results show a substantial progress in the average performance of five classification algorithms simultaneously and the classification error for these classifiers decreases considerably. The experiments also show that this method outperforms other feature selection methods with a lower cost. △ Less","8 March, 2014",https://arxiv.org/pdf/1403.1946
Automated Tracking and Estimation for Control of Non-rigid Cloth,Marc D. Killpack,"This report is a summary of research conducted on cloth tracking for automated textile manufacturing during a two semester long research course at Georgia Tech. This work was completed in 2009. Advances in current sensing technology such as the Microsoft Kinect would now allow me to relax certain assumptions and generally improve the tracking performance. This is because a major part of my approach described in this paper was to track features in a 2D image and use these to estimate the cloth deformation. Innovations such as the Kinect would improve estimation due to the automatic depth information obtained when tracking 2D pixel locations. Additionally, higher resolution camera images would probably give better quality feature tracking. However, although I would use different technology now to implement this tracker, the algorithm described and implemented in this paper is still a viable approach which is why I am publishing this as a tech report for reference. In addition, although the related work is a bit exhaustive, it will be useful to a reader who is new to methods for tracking and estimation as well as modeling of cloth. △ Less","6 March, 2014",https://arxiv.org/pdf/1403.1653
Real-time Topic-aware Influence Maximization Using Preprocessing,Wei Chen;Tian Lin;Cheng Yang,"Influence maximization is the task of finding a set of seed nodes in a social network such that the influence spread of these seed nodes based on certain influence diffusion model is maximized. Topic-aware influence diffusion models have been recently proposed to address the issue that influence between a pair of users are often topic-dependent and information, ideas, innovations etc. being propagated in networks (referred collectively as items in this paper) are typically mixtures of topics. In this paper, we focus on the topic-aware influence maximization task. In particular, we study preprocessing methods for these topics to avoid redoing influence maximization for each item from scratch. We explore two preprocessing algorithms with theoretical justifications. Our empirical results on data obtained in a couple of existing studies demonstrate that one of our algorithms stands out as a strong candidate providing microsecond online response time and competitive influence spread, with reasonable preprocessing effort. △ Less","21 November, 2014",https://arxiv.org/pdf/1403.0057
An Optimal Transmission Strategy for Kalman Filtering over Packet Dropping Links with Imperfect Acknowledgements,Mojtaba Nourian;Alex S. Leong;Subhrakanti Dey;Daniel E. Quevedo,"This paper presents a novel design methodology for optimal transmission policies at a smart sensor to remotely estimate the state of a stable linear stochastic dynamical system. The sensor makes measurements of the process and forms estimates of the state using a local Kalman filter. The sensor transmits quantized information over a packet dropping link to the remote receiver. The receiver sends packet receipt acknowledgments back to the sensor via an erroneous feedback communication channel which is itself packet dropping. The key novelty of this formulation is that the smart sensor decides, at each discrete time instant, whether to transmit a quantized version of either its local state estimate or its local innovation. The objective is to design optimal transmission policies in order to minimize a long term average cost function as a convex combination of the receiver's expected estimation error covariance and the energy needed to transmit the packets. The optimal transmission policy is obtained by the use of dynamic programming techniques. Using the concept of submodularity, the optimality of a threshold policy in the case of scalar systems with perfect packet receipt acknowledgments is proved. Suboptimal solutions and their structural results are also discussed. Numerical results are presented illustrating the performance of the optimal and suboptimal transmission policies. △ Less","26 February, 2014",https://arxiv.org/pdf/1402.6633
Dual Power Assignment via Second Hamiltonian Cycle,Karim Abu-Affash;Paz Carmi;Anat Parush Tzur,"A power assignment is an assignment of transmission power to each of the wireless nodes of a wireless network, so that the induced graph satisfies some desired properties. The cost of a power assignment is the sum of the assigned powers. In this paper, we consider the dual power assignment problem, in which each wireless node is assigned a high- or low-power level, so that the induced graph is strongly connected and the cost of the assignment is minimized. We improve the best known approximation ratio from \frac{π^2}{6}-\frac{1}{36}+ε\thickapprox 1.617 to \frac{11}{7}\thickapprox 1.571. Moreover, we show that the algorithm of Khuller et al. for the strongly connected spanning subgraph problem, which achieves an approximation ratio of 1.61, is 1.522-approximation algorithm for symmetric directed graphs. The innovation of this paper is in achieving these results via utilizing interesting properties for the existence of a second Hamiltonian cycle. △ Less","24 February, 2014",https://arxiv.org/pdf/1402.5783
Software Requirement Specification Using Reverse Speech Technology,Santhy Viswam;Sajeer Karattil,"Speech analysis had been taken to a new level with the discovery of Reverse Speech (RS). RS is the discovery of hidden messages, referred as reversals, in normal speech. Works are in progress for exploiting the relevance of RS in different real world applications such as investigation, medical field etc. In this paper we represent an innovative method for preparing a reliable Software Requirement Specification (SRS) document with the help of reverse speech. As SRS act as the backbone for the successful completion of any project, a reliable method is needed to overcome the inconsistencies. Using RS such a reliable method for SRS documentation was developed. △ Less","13 February, 2014",https://arxiv.org/pdf/1402.3080
Enhancing Human Aspect of Software Engineering using Bayesian Classifier,Sangita Gupta;Suma V,"IT industries in current scenario have to struggle effectively in terms of cost, quality, service or innovation for their subsistence in the global market. Due to the swift transformation of technology, software industries owe to manage a large set of data having precious information hidden. Data mining technique enables one to effectively cope with this hidden information where it can be applied to code optimization, fault prediction and other domains which modulates the success nature of software projects. Additionally, the efficiency of the product developed further depends upon the quality of the project personnel. The position of the paper therefore is to explore potentials of project personnel in terms of their competency and skill set and its influence on quality of project. The above mentioned objective is accomplished using a Bayesian classifier in order to capture the pattern of human performance. By this means, the hidden and valuable knowledge discovered in the related databases will be summarized in the statistical structure. This mode of predictive study enables the project managers to reduce the failure ratio to a significant level and improve the performance of the project using the right choice of project personnel. △ Less","11 February, 2014",https://arxiv.org/pdf/1402.2379
Surfaces Representation with Sharp Features Using Sqrt(3) and Loop Subdivision Schemes,Yasser M. Abd El-Latif,"This paper presents a hybrid algorithm that combines features form both Sqrt(3) and Loop Subdivision schemes. The algorithm aims at preserving sharp features and trim regions, during the surfaces subdivision, using a set of rules. The implementation is nontrivial due to the computational, topological, and smoothness constraints, which should be satisfied by the underlying surface. The fundamental innovation, in this research work, is the ability to preserve sharp features anywhere on a surface. In addition, the resulting representation remains within the multiresolution subdivision framework. Preserving the original representation has a core advantage that all the applicable operations to the multiresolution subdivision surfaces can subsequently be applied to the edited model. Experimental results, including surfaces coarsening and smoothing, were performed using the proposed algorithm for validation purposes, and the results revealed that the proposed algorithm outperforms the other recent state of the art algorithms. △ Less","10 February, 2014",https://arxiv.org/pdf/1402.2190
Analysis of Hashrate-Based Double Spending,Meni Rosenfeld,"Bitcoin is the world's first decentralized digital currency. Its main technical innovation is the use of a blockchain and hash-based proof of work to synchronize transactions and prevent double-spending the currency. While the qualitative nature of this system is well understood, there is widespread confusion about its quantitative aspects and how they relate to attack vectors and their countermeasures. In this paper we take a look at the stochastic processes underlying typical attacks and their resulting probabilities of success. △ Less","9 February, 2014",https://arxiv.org/pdf/1402.2009
Study of Cloud Computing in HealthCare Industry,G. Nikhita Reddy;G. J. Ugander Reddy,In Todays real world technology has become a domiant crucial component in every industry including healthcare industry. The benefits of storing electronically the records of patients have increased the productivity of patient care and easy accessibility and usage. The recent technological innovations in the health care is the invention of cloud based Technology. But many fears and security measures regarding patient records storing remotely is a concern for many in health care industry. One needs to understand the benefits and fears of implementation of cloud computing its advantages and disadvantages of this new technology. △ Less,"8 February, 2014",https://arxiv.org/pdf/1402.1841
Towards a Multi-criteria Development Distribution Model: An Analysis of Existing Task Distribution Approaches,Ansgar Lamersdorf;Jürgen Münch;Dieter Rombach,"Distributing development tasks in the context of global software development bears both many risks and many opportunities. Nowadays, distributed development is often driven by only a few factors or even just a single factor such as workforce costs. Risks and other relevant factors such as workforce capabilities, the innovation potential of different regions, or cultural factors are often not recognized sufficiently. This could be improved by using empirically-based multi-criteria distribution models. Currently, there is a lack of such decision models for distributing software development work. This article focuses on mechanisms for such decision support. First, requirements for a distribution model are formulated based on needs identified from practice. Then, distribution models from different domains are surveyed, compared, and analyzed in terms of suitability. Finally, research questions and directions for future work are given. △ Less","7 February, 2014",https://arxiv.org/pdf/1402.1563
Deployment of an Innovative Resource Choice Method for Process Planning,Alexandre Candlot;Nicolas Perry;Alain Bernard;Samar Ammar-Khodja,"Designers, process planners and manufacturers naturally consider different concepts for a same object. The stiffness of production means and the design specification requirements mark out process planners as responsible of the coherent integration of all constraints. First, this paper details an innovative solution of resource choice, applied for aircraft manufacturing parts. In a second part, key concepts are instanced for the considered industrial domain. Finally, a digital mock up validates the solution viability and demonstrates the possibility of an in-process knowledge capitalisation and use. Formalising the link between Design and Manufacturing allows to hope enhancements of simultaneous Product / Process developments. △ Less","5 February, 2014",https://arxiv.org/pdf/1402.1438
A survey on Human Computer Interaction Mechanism Using Finger Tracking,Kinjal N. Shah;Kirit R. Rathod;Shardul J. Agravat,Human Computer Interaction (HCI) is a field in which developer makes a user friendly system. User can interact with a computer system without using any conventional peripheral devices. Marker is used to recognize hand movement accurately & successfully. Researchers establish the mechanism to interact with computer system using computer vision. The interaction is better than normal static keyboard and mouse. This paper represents most of innovative mechanisms of the finger tracking used to interact with a computer system using computer vision. △ Less,"4 February, 2014",https://arxiv.org/pdf/1402.0693
Stochastic Event-triggered Sensor Schedule for Remote State Estimation,Duo Han;Yilin Mo;Junfeng Wu;Sean Weerakkody;Bruno Sinopoli;Ling Shi,"We propose an open-loop and a closed-loop stochastic event-triggered sensor schedule for remote state estimation. Both schedules overcome the essential difficulties of existing schedules in recent literature works where, through introducing a deterministic event-triggering mechanism, the Gaussian property of the innovation process is destroyed which produces a challenging nonlinear filtering problem that cannot be solved unless approximation techniques are adopted. The proposed stochastic event-triggered sensor schedules eliminate such approximations. Under these two schedules, the MMSE estimator and its estimation error covariance matrix at the remote estimator are given in a closed-form. Simulation studies demonstrate that the proposed schedules have better performance than periodic ones with the same sensor-to-estimator communication rate. △ Less","3 February, 2014",https://arxiv.org/pdf/1402.0599
Asymmetric Distributed Constraint Optimization Problems,Tal Grinshpoun;Alon Grubshtein;Roie Zivan;Arnon Netzer;Amnon Meisels,"Distributed Constraint Optimization (DCOP) is a powerful framework for representing and solving distributed combinatorial problems, where the variables of the problem are owned by different agents. Many multi-agent problems include constraints that produce different gains (or costs) for the participating agents. Asymmetric gains of constrained agents cannot be naturally represented by the standard DCOP model. The present paper proposes a general framework for Asymmetric DCOPs (ADCOPs). In ADCOPs different agents may have different valuations for constraints that they are involved in. The new framework bridges the gap between multi-agent problems which tend to have asymmetric structure and the standard symmetric DCOP model. The benefits of the proposed model over previous attempts to generalize the DCOP model are discussed and evaluated. Innovative algorithms that apply to the special properties of the proposed ADCOP model are presented in detail. These include complete algorithms that have a substantial advantage in terms of runtime and network load over existing algorithms (for standard DCOPs) which use alternative representations. Moreover, standard incomplete algorithms (i.e., local search algorithms) are inapplicable to the existing DCOP representations of asymmetric constraints and when they are applied to the new ADCOP framework they often fail to converge to a local optimum and yield poor results. The local search algorithms proposed in the present paper converge to high quality solutions. The experimental evidence that is presented reveals that the proposed local search algorithms for ADCOPs achieve high quality solutions while preserving a high level of privacy. △ Less","3 February, 2014",https://arxiv.org/pdf/1402.0587
Introducing E-maintenance 2.0,Abdessamad Mouzoune;Saoudi Taibi,"While research literature is still debating e-maintenance definition, a new reality is emerging in business world confirming the enterprise 2.0 model. Executives are more and more forced to stop running against current trend towards social media and instead envisage harnessing its power within the enterprise. Maintenance can't be an exception for long and has to take advantage of new opportunities created by social technological innovations. In this paper, a combination of pure E perspective and 2.0 perspective is proposed to avoid a lock-in and allow continous evolution of e-maintenance within the new context of business: A combination of data centric models and people oriented applications to form a collaborative environment in order to conceive and achieve global goals of maintenance.New challenges are also to be expected as to the effecient integration of enterprise 2.0 tools within current e-maintenance platforms and futher research work is still to be done in this area. △ Less","31 January, 2014",https://arxiv.org/pdf/1401.8252
"Mobile Services and ICT4D, To the Network Economy - Bridging the Digital Divide, Ethiopia's Case",Naod Duga Jebessa;Henok Getachew Alemayehu,"This paper presents a development paradigm for Ethiopia, based on appropriate services and innovative use of mobile communications technologies via applications tailored for sectors like business, finance, healthcare, governance, education and infotainment. The experience of other developing countries like India and Kenya is cited so as to adapt those to the Ethiopian context. Notable application areas in the aforementioned sectors have been outlined. The ETC 'next generation network' is taken into consideration, with an emphasis on mobile service offering by the Telco itself and/or third party service providers. In addition, enabling technologies like mobile internet, location-based systems, open interfaces to large telecom networks, specifically service-oriented architecture (SOA), Parlay/JAIN and the like are discussed. The paper points out possible endeavors by such stakeholders like: telecom agencies and network operators; businesses, government and NGOs; entrepreneurs and innovators; technology companies and professionals; as well as researchers and academic institutions. ICT4D through mobile services and their role in bridging the digital divide by building a virtual 'network economy' is presented. △ Less","29 January, 2014",https://arxiv.org/pdf/1401.7435
Intelligent Product: Mobile Agent Architecture Integrating the End of Life Cycle (EOL) For minimizing the lunch phase PLM,Abdelhak Boulaalam;El Habib Nfaoui;Omar El Beqqali,"To improve the increasingly demands products that are customized, all business activities performed along the product life cycle must be coordinated and efficiently managed along the extended enterprise. For this, enterprise had wanted to retain control over the whole product lifecycle especially when the product is in use/recycling (End Of Life phase). Although there have been many previous research works about product lifecycle management in the beginning of life (BOL) and middle of life (MOL) phases, few addressed the end of life (EOL) phase, in particular, when the product is at the customers. In this paper, based on product embedded device identification (PEID) and mobile agent technologies, and with the advent of the development of the ""intelligent products"", we will try to improve innovation: (a) by minimize the lunch phase, (b) and the involvement of the customer in product lifecycle. △ Less","21 January, 2014",https://arxiv.org/pdf/1401.5509
A Stable Fountain Code Mechanism for Peer-to-Peer Content Distribution,Cedric Westphal,"Most peer-to-peer content distribution systems require the peers to privilege the welfare of the overall system over greedily maximizing their own utility. When downloading a file broken up into multiple pieces, peers are often asked to pass on some possible download opportunities of common pieces in order to favor rare pieces. This is to avoid the missing piece syndrome, which throttles the download rate of the peer-to-peer system to that of downloading the file straight from the server. In other situations, peers are asked to stay in the system even though they have collected all the file's pieces and have an incentive to leave right away. We propose a mechanism which allows peers to act greedily and yet stabilizes the peer-to-peer content sharing system. Our mechanism combines a fountain code at the server to generate innovative new pieces, and a prioritization for the server to deliver pieces only to new peers. While by itself, neither the fountain code nor the prioritization of new peers alone stabilizes the system, we demonstrate that their combination does, through both analytical and numerical evaluation. △ Less","20 January, 2014",https://arxiv.org/pdf/1401.5099
Multiple Hybrid Phase Transition: Bootstrap Percolation on Complex Networks with Communities,Chong Wu;Shenggong Ji;Rui Zhang;Liujun Chen;Jiawei Chen;Xiaobin Li;Yanqing Hu,"Bootstrap percolation is a well-known model to study the spreading of rumors, new products or innovations on social networks. The empirical studies show that community structure is ubiquitous among various social networks. Thus, studying the bootstrap percolation on the complex networks with communities can bring us new and important insights of the spreading dynamics on social networks. It attracts a lot of scientists' attentions recently. In this letter, we study the bootstrap percolation on Erdős-Rényi networks with communities and observed second order, hybrid (both second and first order) and multiple hybrid phase transitions, which is rare in natural system. Moreover, we have analytically solved this system and obtained the phase diagram, which is further justified well by the corresponding simulations. △ Less","1 July, 2014",https://arxiv.org/pdf/1401.4680
Service-oriented Communities: Visions and Contributions towards Social Organizations,Vincenzo De Florio;Chris Blondia,"With the increase of the populations, resources are becoming scarcer, and a smarter way to make use of them becomes a vital necessity of our societies. On the other hand, resource management is traditionally carried out through well established organizations, policies, and regulations that are often considered as impossible to restructure. Our position is that merely expanding the traditional approaches might not be enough. Systems must be radically rethought in order to achieve a truly effective and rational use of the available resources. Classical concepts such as demand and supply need to be rethought as well, as they operate artificial classifications that limit the true potential of systems and organizations. In what follows we propose our vision to future, ""smarter"" systems able to overcome the limitations of the status quo. Such systems require what Boulding called ""gestalts,"" namely concepts able to ""directing research towards the gaps which they reveal"". In this paper we elaborate on this and show how such gestalts can pave the way towards novel reformulations of traditional services able to reach a better and more sensible management of the available resources and cope with their scarcity. Our vision of a Service-oriented Community is also introduced. We believe that such communities---in heterarchical coexistence with traditional systems---provide the necessary diversity and innovation orientation to prevent societal lock-ins such as the ones we are experiencing in assisting our elderly ones. △ Less","15 January, 2014",https://arxiv.org/pdf/1401.3621
"Patents as Instruments for Exploring Innovation Dynamics: Geographic and Technological Perspectives on ""Photovoltaic Cells""",Loet Leydesdorff;Floortje Alkemade;Gaston Heimeriks;Rinke Hoekstra,"The dynamics of innovation are nonlinear and complex: geographical, technological, and economic selection environments can be expected to interact. Can patents provide an analytical lens to this process in terms of different attributes such as inventor addresses, classification codes, backward and forward citations, etc.? Two recently developed patent maps with interactive overlay techniques--Google Maps and maps based on citation relations among International Patent Classifications (IPC)--are elaborated into dynamic versions that allow for online animations and comparisons by using split screens. Various forms of animation are explored. The recently developed Cooperative Patent Classifications (CPC) of the U.S. Patent and Trade Office (USPTO) and the European Patent Office (EPO) provide new options for a precise delineation of samples in both USPTO data and the Worldwide Patent Statistics Database (PatStat) of EPO. Among the ""technologies for the mitigation of climate change"" (class Y02), we zoom in on nine material technologies for photovoltaic cells; and focus on one of them (CuInSe2) as a lead case. The longitudinal development of Rao-Stirling diversity in the IPC-based maps provides a heuristics for studying technological generations during the period under study (1975-2012). The sequencing of generations prevails in USPTO data more than in PatStat data because PatStat aggregates patent information from countries in different stages of technological development, whereas one can expect USPTO patents to be competitive at the technological edge. △ Less","10 September, 2014",https://arxiv.org/pdf/1401.2778
A Survey of Volunteered Open Geo-Knowledge Bases in the Semantic Web,Andrea Ballatore;David C. Wilson;Michela Bertolotto,"Over the past decade, rapid advances in web technologies, coupled with innovative models of spatial data collection and consumption, have generated a robust growth in geo-referenced information, resulting in spatial information overload. Increasing 'geographic intelligence' in traditional text-based information retrieval has become a prominent approach to respond to this issue and to fulfill users' spatial information needs. Numerous efforts in the Semantic Geospatial Web, Volunteered Geographic Information (VGI), and the Linking Open Data initiative have converged in a constellation of open knowledge bases, freely available online. In this article, we survey these open knowledge bases, focusing on their geospatial dimension. Particular attention is devoted to the crucial issue of the quality of geo-knowledge bases, as well as of crowdsourced data. A new knowledge base, the OpenStreetMap Semantic Network, is outlined as our contribution to this area. Research directions in information integration and Geographic Information Retrieval (GIR) are then reviewed, with a critical discussion of their current limitations and future prospects. △ Less","12 January, 2014",https://arxiv.org/pdf/1401.2610
Can Synergy in Triple-Helix Relations be Quantified? A Review of the Development of the Triple-Helix Indicator,Loet Leydesdorff;Han Woo Park,"Triple-Helix arrangements of bi- and trilateral relations can be considered as adaptive eco-systems. During the last decade, we have further developed a Triple-Helix indicator of synergy as reduction of uncertainty in niches that can be shaped among three or more distributions. Reduction of uncertainty can be generated in correlations among distributions of relations, but this (next-order) effect can be counterbalanced by uncertainty generated in the relations. We first explain the indicator, and then review possible results when this indicator is applied to (i) co-author networks of academic, industrial, and governmental authors and (ii) synergies in the distributions of firms over geographical addresses, technological classes, and industrial-size classes for a number of nations. Co-variation is then considered as a measure of relationship. The balance between globalizing and localizing dynamics can be quantified. Too much synergy locally can also be considered as lock-in. Tendencies are different for the globalizing knowledge dynamics versus locally retaining wealth from knowledge in industrial innovations. △ Less","10 January, 2014",https://arxiv.org/pdf/1401.2342
Minimizing the Time of Detection of Large (Probably) Prime Numbers,Dragan Vidakovic;Dusko Parezanovic;Zoran Vucetic,"In this paper we present the experimental results that more clearly than any theory suggest an answer to the question: when in detection of large (probably) prime numbers to apply, a very resource demanding, Miller-Rabin algorithm. Or, to put it another way, when the dividing by first several tens of prime numbers should be replaced by primality testing? As an innovation, the procedure above will be supplemented by considering the use of the well-known Goldbach's conjecture in the solving of this and some other important questions about the RSA cryptosystem, always guided by the motto ""do not harm"" - neither the security nor the time spent. △ Less","9 January, 2014",https://arxiv.org/pdf/1401.2107
Fast nonparametric clustering of structured time-series,James Hensman;Magnus Rattray;Neil D. Lawrence,"In this publication, we combine two Bayesian non-parametric models: the Gaussian Process (GP) and the Dirichlet Process (DP). Our innovation in the GP model is to introduce a variation on the GP prior which enables us to model structured time-series data, i.e. data containing groups where we wish to model inter- and intra-group variability. Our innovation in the DP model is an implementation of a new fast collapsed variational inference procedure which enables us to optimize our variationala pproximation significantly faster than standard VB approaches. In a biological time series application we show how our model better captures salient features of the data, leading to better consistency with existing biological classifications, while the associated inference algorithm provides a twofold speed-up over EM-based variational inference. △ Less","14 April, 2014",https://arxiv.org/pdf/1401.1605
BigDataBench: a Big Data Benchmark Suite from Internet Services,Lei Wang;Jianfeng Zhan;Chunjie Luo;Yuqing Zhu;Qiang Yang;Yongqiang He;Wanling Gao;Zhen Jia;Yingjie Shi;Shujie Zhang;Chen Zheng;Gang Lu;Kent Zhan;Xiaona Li;Bizhu Qiu,"As architecture, systems, and data management communities pay greater attention to innovative big data systems and architectures, the pressure of benchmarking and evaluating these systems rises. Considering the broad use of big data systems, big data benchmarks must include diversity of data and workloads. Most of the state-of-the-art big data benchmarking efforts target evaluating specific types of applications or system software stacks, and hence they are not qualified for serving the purposes mentioned above. This paper presents our joint research efforts on this issue with several industrial partners. Our big data benchmark suite BigDataBench not only covers broad application scenarios, but also includes diverse and representative data sets. BigDataBench is publicly available from http://prof.ict.ac.cn/BigDataBench . Also, we comprehensively characterize 19 big data workloads included in BigDataBench with varying data inputs. On a typical state-of-practice processor, Intel Xeon E5645, we have the following observations: First, in comparison with the traditional benchmarks: including PARSEC, HPCC, and SPECCPU, big data applications have very low operation intensity; Second, the volume of data input has non-negligible impact on micro-architecture characteristics, which may impose challenges for simulation-based big data architecture research; Last but not least, corroborating the observations in CloudSuite and DCBench (which use smaller data inputs), we find that the numbers of L1 instruction cache misses per 1000 instructions of the big data applications are higher than in the traditional benchmarks; also, we find that L3 caches are effective for the big data applications, corroborating the observation in DCBench. △ Less","22 February, 2014",https://arxiv.org/pdf/1401.1406
Multi-Parameter Decision Support with Data Transmission over GSM/GPRS Network: a Case Study of Landslide Monitoring,Satyajit Rath;B. P. S. Sahoo;S. K. Pandey;D. P. Sandha,"The planet Earth has hundreds of impact events, with some occurrences causing both in terms of human casualty as well as economic losses. Such attitudes of earth pushed the frontiers to develop innovative monitoring strategies for the earth system. To make that real, although, will require coherent and real-time data by observing the earth behavior contiguously. Wireless Sensor Network (WSN) appears to be the best suitable infrastructure to sense environmental parameters of our interests. In this event of earth observation, another important issue is the monitoring system with high level of precision. There are different types of sensors to measure the behavioral aspects of earth. The sensors integrated with WSN, provide an accurate and contiguous data for analysis and interpretation. This paper briefly addresses earth observation and areas of critical importance to people and society. A case study has also been carried out for disaster like Landslide in the North Eastern region of India. Application software has been developed for the said study for online data acquisition and analysis with pre-disaster early warning system. The system monitors the changing geotechnical condition of this region using various geo-technical sensors like Rain gauge, In-place Inclinometer, Tilt-meter, Piezo-meter and Crack meter. This paper also touches upon the aspects of data transmission over Global System for Mobile Communication (GSM) / General Packet Radio Service (GPRS) to a remote data center. △ Less","1 August, 2014",https://arxiv.org/pdf/1312.4179
Choreography In Inter-Organizational Innovation Networks,Giovanna Ferraro;Antonio Iovanella,"This paper introduces the concept of choreography with respect to inter-organizational innovation networks, as they constitute an attractive environment to create innovation in different sectors. We argue that choreography governs behaviours by shaping the level of connectivity and cohesion among network members. It represents a valid organizational system able to sustain some activities and to reach effects generating innovation outcomes. This issue is tackled introducing a new framework in which we propose a network model as prerequisite for our hypothesis. The analysis is focused on inter-organizational innovation networks characterized by the presence of hubs, semi-peripheral and peripheral members lacking hierarchical authority. We sustain that the features of a network, bringing to synchronization phenomena, are extremely similar to those existing in innovation network characterized by the emergence of choreography. The effectiveness of our model is verified by providing a real case study that gives preliminary empirical hints on the network aptitude to perform choreography. Indeed, the innovation network analysed in the case study reveals characteristics causing synchronization and consequently the establishment of choreography. △ Less","20 June, 2014",https://arxiv.org/pdf/1311.6609
Network communities within and across borders,Federica Cerina;Alessandro Chessa;Fabio Pammolli;Massimo Riccaboni,"We investigate the impact of borders on the topology of spatially embedded networks. Indeed territorial subdivisions and geographical borders significantly hamper the geographical span of networks thus playing a key role in the formation of network communities. This is especially important in scientific and technological policy-making, highlighting the interplay between pressure for the internationalization to lead towards a global innovation system and the administrative borders imposed by the national and regional institutions. In this study we introduce an outreach index to quantify the impact of borders on the community structure and apply it to the case of the European and US patent co-inventors networks. We find that (a) the US connectivity decays as a power of distance, whereas we observe a faster exponential decay for Europe; (b) European network communities essentially correspond to nations and contiguous regions while US communities span multiple states across the whole country without any characteristic geographic scale. We confirm our findings by means of a set of simulations aimed at exploring the relationship between different patterns of cross-border community structures and the outreach index. △ Less","3 April, 2014",https://arxiv.org/pdf/1311.4211
"First Evaluation of the CPU, GPGPU and MIC Architectures for Real Time Particle Tracking based on Hough Transform at the LHC",V. Halyo;P. LeGresley;P. Lujan;V. Karpusenko;A. Vladimirov,"Recent innovations focused around {\em parallel} processing, either through systems containing multiple processors or processors containing multiple cores, hold great promise for enhancing the performance of the trigger at the LHC and extending its physics program. The flexibility of the CMS/ATLAS trigger system allows for easy integration of computational accelerators, such as NVIDIA's Tesla Graphics Processing Unit (GPU) or Intel's \xphi, in the High Level Trigger. These accelerators have the potential to provide faster or more energy efficient event selection, thus opening up possibilities for new complex triggers that were not previously feasible. At the same time, it is crucial to explore the performance limits achievable on the latest generation multicore CPUs with the use of the best software optimization methods. In this article, a new tracking algorithm based on the Hough transform will be evaluated for the first time on a multi-core Intel Xeon E5-2697v2 CPU, an NVIDIA Tesla K20c GPU, and an Intel \xphi\ 7120 coprocessor. Preliminary time performance will be presented. △ Less","3 February, 2014",https://arxiv.org/pdf/1310.7556
Competitive dynamics of lexical innovations in multi-layer networks,Marco Alberto Javarone,"We study the introduction of lexical innovations into a community of language users. Lexical innovations, i.e., new terms added to people's vocabulary, play an important role in the process of language evolution. Nowadays, information is spread through a variety of networks, including, among others, online and offline social networks and the World Wide Web. The entire system, comprising networks of different nature, can be represented as a multi-layer network. In this context, lexical innovations diffusion occurs in a peculiar fashion. In particular, a lexical innovation can undergo three different processes: its original meaning is accepted; its meaning can be changed or misunderstood (e.g., when not properly explained), hence more than one meaning can emerge in the population; lastly, in the case of a loan word, it can be translated into the population language (i.e., defining a new lexical innovation or using a synonym) or into a dialect spoken by part of the population. Therefore, lexical innovations cannot be considered simply as information. We develop a model for analyzing this scenario using a multi-layer network comprising a social network and a media network. The latter represents the set of all information systems of a society, e.g., television, the World Wide Web and radio. Furthermore, we identify temporal directed edges between the nodes of these two networks. In particular, at each time step, nodes of the media network can be connected to randomly chosen nodes of the social network and vice versa. In so doing, information spreads through the whole system and people can share a lexical innovation with their neighbors or, in the event they work as reporters, by using media nodes. Lastly, we use the concept of ""linguistic sign"" to model lexical innovations, showing its fundamental role in the study of these dynamics. Many numerical simulations have been performed. △ Less","14 August, 2014",https://arxiv.org/pdf/1310.4975
"Measuring Triple-Helix Synergy in the Russian Innovation Systems at Regional, Provincial, and National Levels",Loet Leydesdorff;Evgeniy Perevodchikov;Alexander Uvarov,"We measure synergy for the Russian national, provincial, and regional innovation systems as reduction of uncertainty using mutual information among the three distributions of firm sizes, technological knowledge-bases of firms, and geographical locations. Half a million data at firm level in 2011 were obtained from the Orbis database of Bureau Van Dijk. The firm level data were aggregated at the levels of eight Federal Districts, the regional level of 83 Federal Subjects, and the single level of the Russian Federation. Not surprisingly, the knowledge base of the economy is concentrated in the Moscow region (22.8%); St. Petersburg follows with 4.0%. Only 0.4% of the firms are classified as high-tech, and 2.7% as medium-tech manufacturing (NACE, Rev. 2). Except in Moscow itself, high-tech manufacturing does not add synergy to any other unit at any of the various levels of geographical granularity; instead it disturbs regional coordination even in the region surrounding Moscow (""Moscow Region""). In the case of medium-tech manufacturing, there is also synergy in St. Petersburg. Knowledge-intensive services (KIS; including laboratories) contribute 12.8% to the economy in terms of establishments and contribute to the synergy in all Federal Districts (except the North-Caucasian Federal District), but only in 30 of the 83 Federal Subjects. The synergy in KIS is concentrated in centers of administration. Unlike Western European countries, the knowledge-intensive services (which are often state-affiliated) thus provide backbone to an emerging knowledge-based economy at the level of Federal Districts, but the economy is otherwise not knowledge-based (except for the Moscow region). △ Less","31 January, 2014",https://arxiv.org/pdf/1310.3040
Building Programmable Wireless Networks: An Architectural Survey,Junaid Qadir;Nadeem Ahmed;Nauman Ahad,"In recent times, there have been a lot of efforts for improving the ossified Internet architecture in a bid to sustain unstinted growth and innovation. A major reason for the perceived architectural ossification is the lack of ability to program the network as a system. This situation has resulted partly from historical decisions in the original Internet design which emphasized decentralized network operations through co-located data and control planes on each network device. The situation for wireless networks is no different resulting in a lot of complexity and a plethora of largely incompatible wireless technologies. The emergence of ""programmable wireless networks"", that allow greater flexibility, ease of management and configurability, is a step in the right direction to overcome the aforementioned shortcomings of the wireless networks. In this paper, we provide a broad overview of the architectures proposed in literature for building programmable wireless networks focusing primarily on three popular techniques, i.e., software defined networks, cognitive radio networks, and virtualized networks. This survey is a self-contained tutorial on these techniques and its applications. We also discuss the opportunities and challenges in building next-generation programmable wireless networks and identify open research issues and future research directions. △ Less","28 January, 2014",https://arxiv.org/pdf/1310.0251
"Integrating GPS, GSM and Cellular Phone for Location Tracking and Monitoring",B. P. S. Sahoo;Satyajit Rath,"The wide spread of mobiles as handheld devices leads to various innovative applications that makes use of their ever increasing presence in our daily life. One such application is location tracking and monitoring. This paper proposes a prototype model for location tracking using Geographical Positioning System (GPS) and Global System for Mobile Communication (GSM) technology. The system displays the object moving path on the monitor and the same information can also be communicated to the user cell phone, on demand of the user by asking the specific information via SMS. This system is very useful for car theft situations, for adolescent drivers being watched and monitored by parents. The result shows that the object is being tracked with a minimal tracking error. △ Less","8 July, 2014",https://arxiv.org/pdf/1307.3147
An age structured demographic theory of technological change,J. -F. Mercure,"At the heart of technology transitions lie complex processes of social and industrial dynamics. The quantitative study of sustainability transitions requires modelling work, which necessitates a theory of technology substitution. Many, if not most, contemporary modelling approaches for future technology pathways overlook most aspects of transitions theory, for instance dimensions of heterogenous investor choices, dynamic rates of diffusion and the profile of transitions. A significant body of literature however exists that demonstrates how transitions follow S-shaped diffusion curves or Lotka-Volterra systems of equations. This framework is used ex-post since timescales can only be reliably obtained in cases where the transitions have already occurred, precluding its use for studying cases of interest where nascent innovations in protective niches await favourable conditions for their diffusion. In principle, scaling parameters of transitions can, however, be derived from knowledge of industrial dynamics, technology turnover rates and technology characteristics. In this context, this paper presents a theory framework for evaluating the parameterisation of S-shaped diffusion curves for use in simulation models of technology transitions without the involvement of historical data fitting, making use of standard demography theory applied to technology at the unit level. The classic Lotka-Volterra competition system emerges from first principles from demography theory, its timescales explained in terms of technology lifetimes and industrial dynamics. The theory is placed in the context of the multi-level perspective on technology transitions, where innovation and the diffusion of new socio-technical regimes take a prominent place, as well as discrete choice theory, the primary theoretical framework for introducing agent diversity. △ Less","26 November, 2014",https://arxiv.org/pdf/1304.3602
"Fast methods for denoising matrix completion formulations, with applications to robust seismic data interpolation",Aleksandr Y. Aravkin;Rajiv Kumar;Hassan Mansour;Ben Recht;Felix J. Herrmann,"Recent SVD-free matrix factorization formulations have enabled rank minimization for systems with millions of rows and columns, paving the way for matrix completion in extremely large-scale applications, such as seismic data interpolation. In this paper, we consider matrix completion formulations designed to hit a target data-fitting error level provided by the user, and propose an algorithm called LR-BPDN that is able to exploit factorized formulations to solve the corresponding optimization problem. Since practitioners typically have strong prior knowledge about target error level, this innovation makes it easy to apply the algorithm in practice, leaving only the factor rank to be determined. Within the established framework, we propose two extensions that are highly relevant to solving practical challenges of data interpolation. First, we propose a weighted extension that allows known subspace information to improve the results of matrix completion formulations. We show how this weighting can be used in the context of frequency continuation, an essential aspect to seismic data interpolation. Second, we propose matrix completion formulations that are robust to large measurement errors in the available data. We illustrate the advantages of LR-BPDN on the collaborative filtering problem using the MovieLens 1M, 10M, and Netflix 100M datasets. Then, we use the new method, along with its robust and subspace re-weighted extensions, to obtain high-quality reconstructions for large scale seismic interpolation problems with real data, even in the presence of data contamination. △ Less","5 March, 2014",https://arxiv.org/pdf/1302.4886
Asymptotically Efficient Distributed Estimation With Exponential Family Statistics,Soummya Kar;Jose Moura,"The paper studies the problem of distributed parameter estimation in multi-agent networks with exponential family observation statistics. A certainty-equivalence type distributed estimator of the consensus + innovations form is proposed in which, at each each observation sampling epoch agents update their local parameter estimates by appropriately combining the data received from their neighbors and the locally sensed new information (innovation). Under global observability of the networked sensing model, i.e., the ability to distinguish between different instances of the parameter value based on the joint observation statistics, and mean connectivity of the inter-agent communication network, the proposed estimator is shown to yield consistent parameter estimates at each network agent. Further, it is shown that the distributed estimator is asymptotically efficient, in that, the asymptotic covariances of the agent estimates coincide with that of the optimal centralized estimator, i.e., the inverse of the centralized Fisher information rate. From a technical viewpoint, the proposed distributed estimator leads to non-Markovian mixed timescale stochastic recursions and the analytical methods developed in the paper contribute to the general theory of distributed stochastic approximation. △ Less","1 February, 2014",https://arxiv.org/pdf/1301.5047
Fast community detection by SCORE,Jiashun Jin,"Consider a network where the nodes split into K different communities. The community labels for the nodes are unknown and it is of major interest to estimate them (i.e., community detection). Degree Corrected Block Model (DCBM) is a popular network model. How to detect communities with the DCBM is an interesting problem, where the main challenge lies in the degree heterogeneity. We propose a new approach to community detection which we call the Spectral Clustering On Ratios-of-Eigenvectors (SCORE). Compared to classical spectral methods, the main innovation is to use the entry-wise ratios between the first leading eigenvector and each of the other leading eigenvectors for clustering. Let A be the adjacency matrix of the network. We first obtain the K leading eigenvectors of A, say, \hatη_1,\ldots,\hatη_K, and let \hat{R} be the n\times (K-1) matrix such that \hat{R}(i,k)=\hatη_{k+1}(i)/\hatη_1(i), 1\leq i\leq n, 1\leq k\leq K-1. We then use \hat{R} for clustering by applying the k-means method. The central surprise is, the effect of degree heterogeneity is largely ancillary, and can be effectively removed by taking entry-wise ratios between \hatη_{k+1} and \hatη_1, 1\leq k\leq K-1. The method is successfully applied to the web blogs data and the karate club data, with error rates of 58/1222 and 1/34, respectively. These results are more satisfactory than those by the classical spectral methods. Additionally, compared to modularity methods, SCORE is easier to implement, computationally faster, and also has smaller error rates. We develop a theoretic framework where we show that under mild conditions, the SCORE stably yields consistent community detection. In the core of the analysis is the recent development on Random Matrix Theory (RMT), where the matrix-form Bernstein inequality is especially helpful. △ Less","27 November, 2014",https://arxiv.org/pdf/1211.5803
Professional diversity and the productivity of cities,Luís M. A. Bettencourt;Horacio Samaniego;HyeJin Youn,"The relationships between diversity, productivity and scale determine much of the structure and robustness of complex biological and social systems. While arguments for the link between specialization and productivity are common, diversity has often been invoked as a hedging strategy, allowing systems to evolve in response to environmental change. Despite their general appeal, these arguments have not typically produced quantitative predictions for optimal levels of functional diversity consistent with observations. One important reason why these relationships have resisted formalization is the idiosyncratic nature of diversity measures, which depend on given classification schemes. Here, we address these issues by analyzing the statistics of professions in cities and show how their probability distribution takes a universal scale-invariant form, common to all cities, obtained in the limit of infinite resolution of given taxonomies. We propose a model that generates the form and parameters of this distribution via the introduction of new occupations at a rate leading to individual specialization subject to the preservation of access to overall function via their ego social networks. This perspective unifies ideas about the importance of network structure in ecology and of innovation as a recombinatory process with economic concepts of productivity gains obtained through the division and coordination of labor, stimulated by scale. △ Less","23 June, 2014",https://arxiv.org/pdf/1210.7335
A two-layer team-assembly model for invention networks,Hiroyasu Inoue,"Companies are exposed to rigid competition, so they seek how best to improve the capabilities of their innovations. One strategy is to collaborate with other companies in order to speed up their own innovations. Such inter-company collaborations are conducted by inventors belonging to the companies. At the same time, the inventors also seem to be affected by past collaborations between companies. Therefore, interdependency of two networks, namely inventor and company networks, exists. This paper discusses a model that replicates two-layer networks extracted from patent data of Japan and the United States in terms of degree distributions. The model replicates two-layer networks with the interdependency. Moreover it is the only model that uses local information, while other models have to use overall information, which is unrealistic. In addition, the proposed model replicates empirical data better than other models. △ Less","3 July, 2014",https://arxiv.org/pdf/1206.4933
