title,authors,abstract,submitted_date,pdf_link
Survey on some optimization possibilities for data plane applications,Gereltsetseg Altangerel;Tejfel Máté,"By programming both the data plane and the control plane, network operators can customize their networks based on their needs, regardless of the hardware manufacturer. Control plane programming, a major component of the SDN (Software Defined Network) concept, has been developed for more than 10 years and successfully implemented in real networks. Efforts to develop reconfigurable data planes and high-level network programming languages make it truly possible to program data planes. Therefore, the programmable data planes and SDNs offer great flexibility in network customization, allowing many innovations to be introduced on the network. The general focus of research on the data plane is data-plane abstractions, languages and compilers, data plane algorithms, and applications. This paper outlines some emerging applications on the data plane and offers opportunities for further improvement and optimization. △ Less","29 November, 2021",https://arxiv.org/pdf/2201.11516
The link between countries' economic and scientific wealth has a complex dependence on technological activity and research policy,Alonso Rodriguez-Navarro;Ricardo Brito,"We studied the research performance of 69 countries by considering two different types of new knowledge: incremental (normal) and fundamental (radical). In principle, these two types of new knowledge should be assessed at two very different levels of citations, but we demonstrate that a simpler assessment can be performed based on the total number of papers (P) and the ratio of the number of papers in the global top 10% of most cited papers divided to the total number of papers (Ptop 10%/P). P represents the quantity, whereas the Ptop 10%/P ratio represents the efficiency. In ideal countries, P and the Ptop 10%/P ratio are linked to the gross domestic product (GDP) and GDP the per capita, respectively. Only countries with high Ptop 10%/P ratios participate actively in the creation of fundamental new knowledge and have Noble laureates. In real countries, the link between economic and scientific wealth can be modified by the technological activity and the research policy. We discuss how technological activity may decrease the Ptop 10%/P ratio while only slightly affecting the capacity to create fundamental new knowledge; in such countries, many papers may report incremental innovations that do not drive the advancement of knowledge. Japan is the clearest example of this, although there are many less extreme examples. Independently of technological activity, research policy has a strong influence on the Ptop 10%/P ratio, which may be higher or lower than expected from the GDP per capita depending on the success of the research policy. △ Less","29 December, 2021",https://arxiv.org/pdf/2112.14522
How Powerful are Interest Diffusion on Purchasing Prediction: A Case Study of Taocode,Xuanwen Huang;Yang Yang;Ziqiang Cheng;Shen Fan;Zhongyao Wang;Juren Li;Jun Zhang;Jingmin Chen,"A taocode is a kind of specially coded text-link on Taobao(the world's biggest online shopping website), through which users can share messages about products with each other. Analyzing taocodes can potentially facilitate understanding of the social relationships between users and, more excitingly, their online purchasing behaviors under the influence of taocode diffusion. This paper innovatively investigates the problem of online purchasing predictions from an information diffusion perspective, with taocode as a case study. Specifically, we conduct profound observational studies on a large-scale real-world dataset from Taobao, containing over 100M Taocode sharing records. Inspired by our observations, we propose InfNet, a dynamic GNN-based framework that models the information diffusion across Taocode. We then apply InfNet to item purchasing predictions. Extensive experiments on real-world datasets validate the effectiveness of InfNet compared with 8 state-of-the-art baselines. △ Less","30 December, 2021",https://arxiv.org/pdf/2112.14446
Heterogenous Networks: From small cells to 5G NR-U,Vanlin Sathya;Srikant Manas Kala;Kalpana Naidu,"With the exponential increase in mobile users, the mobile data demand has grown tremendously. To meet these demands, cellular operators are constantly innovating to enhance the capacity of cellular systems. Consequently, operators have been reusing the licensed spectrum spatially, by deploying 4G/LTE small cells (e.g., Femto Cells) in the past. However, despite the use of small cells, licensed spectrum will be unable to meet the consistently rising data traffic because of data-intensive applications such as augmented reality or virtual reality (AR/VR) and on-the-go high-definition video streaming. Applications such AR/VR and online gaming not only place extreme data demands on the network, but are also latency-critical. To meet the QoS guarantees, cellular operators have begun leveraging the unlicensed spectrum by coexisting with Wi-Fi in the 5 GHz band. The standardizing body 3GPP, has prescribed cellular standards for fair unlicensed coexistence with Wi-Fi, namely LTE Licensed Assisted Access (LAA), New Radio in unlicensed (NR-U), and NR in Millimeter. The rapid roll-out of LAA deployments in developed nations like the US, offers an opportunity to study and analyze the performance of unlicensed coexistence networks through real-world ground truth. Thus, this paper presents a high-level overview of past, present, and future of the research in small cell and unlicensed coexistence communication technologies. It outlines the vision for future research work in the recently allocated unlicensed spectrum: The 6 GHz band, where the latest Wi-Fi standard, IEEE 802.11ax, will coexist with the latest cellular technology, 5G New Radio (NR) in unlicensed. △ Less","28 December, 2021",https://arxiv.org/pdf/2112.14240
A Metamodel and Prototype for Fluid Document Formats,Ahmed A. O. Tayeh;Bruno Dumas;Beat Signer,"With the transformation of computing from personal computers to the Internet, document formats have also seen some changes over the years. Future document formats are likely going to adapt to the emerging needs of ubiquitous computing, where information processing is embedded in everyday activities and objects. While most existing document formats have originally been a digital emulation of paper documents, over the years they have been enriched with additional digital features. These features were mainly incorporated to take advantage of the new functionality offered by the devices on which the documents are accessed. With the advent of ubiquitous computing, document formats seem to be facing the next evolutionary step. They will have to adapt to novel mobile devices, innovative interaction modalities, the distribution over multiple devices as well as heterogeneous input sources. This adaptation to the age of ubiquitous computing asks for several new document features. We outline a roadmap towards future fluid document representations for ubiquitous information environments. Based on the resource-selector-link (RSL) hypermedia metamodel - a general hypermedia metamodel supporting distribution, user rights management and content adaptation - we developed a metamodel for fluid document formats and the corresponding online text editor for fluid documents. △ Less","28 December, 2021",https://arxiv.org/pdf/2112.14109
MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer,Tianyi Zhang;Yunlu Feng;Yu Zhao;Guangda Fan;Aiming Yang;Shangqin Lyu;Peng Zhang;Fan Song;Chenbin Ma;Yangyang Sun;Youdan Feng;Guanglei Zhang,"Pancreatic cancer is one of the most malignant cancers in the world, which deteriorates rapidly with very high mortality. The rapid on-site evaluation (ROSE) technique innovates the workflow by immediately analyzing the fast stained cytopathological images with on-site pathologists, which enables faster diagnosis in this time-pressured process. However, the wider expansion of ROSE diagnosis has been hindered by the lack of experienced pathologists. To overcome this problem, we propose a hybrid high-performance deep learning model to enable the automated workflow, thus freeing the occupation of the valuable time of pathologists. By firstly introducing the Transformer block into this field with our particular multi-stage hybrid design, the spatial features generated by the convolutional neural network (CNN) significantly enhance the Transformer global modeling. Turning multi-stage spatial features as global attention guidance, this design combines the robustness from the inductive bias of CNN with the sophisticated global modeling power of Transformer. A dataset of 4240 ROSE images is collected to evaluate the method in this unexplored field. The proposed multi-stage hybrid Transformer (MSHT) achieves 95.68% in classification accuracy, which is distinctively higher than the state-of-the-art models. Facing the need for interpretability, MSHT outperforms its counterparts with more accurate attention regions. The results demonstrate that the MSHT can distinguish cancer samples accurately at an unprecedented image scale, laying the foundation for deploying automatic decision systems and enabling the expansion of ROSE in clinical practice. The code and records are available at: https://github.com/sagizty/Multi-Stage-Hybrid-Transformer. △ Less","27 December, 2021",https://arxiv.org/pdf/2112.13513
Analyzing Scientific Publications using Domain-Specific Word Embedding and Topic Modelling,Trisha Singhal;Junhua Liu;Lucienne T. M. Blessing;Kwan Hui Lim,"The scientific world is changing at a rapid pace, with new technology being developed and new trends being set at an increasing frequency. This paper presents a framework for conducting scientific analyses of academic publications, which is crucial to monitor research trends and identify potential innovations. This framework adopts and combines various techniques of Natural Language Processing, such as word embedding and topic modelling. Word embedding is used to capture semantic meanings of domain-specific words. We propose two novel scientific publication embedding, i.e., PUB-G and PUB-W, which are capable of learning semantic meanings of general as well as domain-specific words in various research fields. Thereafter, topic modelling is used to identify clusters of research topics within these larger research fields. We curated a publication dataset consisting of two conferences and two journals from 1995 to 2020 from two research domains. Experimental results show that our PUB-G and PUB-W embeddings are superior in comparison to other baseline embeddings by a margin of ~0.18-1.03 based on topic coherence. △ Less","23 December, 2021",https://arxiv.org/pdf/2112.12940
Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification,Zhiyu Fang;Xiaobin Zhu;Chun Yang;Zheng Han;Jingyan Qin;Xu-Cheng Yin,"Learning a common latent embedding by aligning the latent spaces of cross-modal autoencoders is an effective strategy for Generalized Zero-Shot Classification (GZSC). However, due to the lack of fine-grained instance-wise annotations, it still easily suffer from the domain shift problem for the discrepancy between the visual representation of diversified images and the semantic representation of fixed attributes. In this paper, we propose an innovative autoencoder network by learning Aligned Cross-Modal Representations (dubbed ACMR) for GZSC. Specifically, we propose a novel Vision-Semantic Alignment (VSA) method to strengthen the alignment of cross-modal latent features on the latent subspaces guided by a learned classifier. In addition, we propose a novel Information Enhancement Module (IEM) to reduce the possibility of latent variables collapse meanwhile encouraging the discriminative ability of latent variables. Extensive experiments on publicly available datasets demonstrate the state-of-the-art performance of our method. △ Less","23 December, 2021",https://arxiv.org/pdf/2112.12927
Learning Hierarchical Attention for Weakly-supervised Chest X-Ray Abnormality Localization and Diagnosis,Xi Ouyang;Srikrishna Karanam;Ziyan Wu;Terrence Chen;Jiayu Huo;Xiang Sean Zhou;Qian Wang;Jie-Zhi Cheng,"We consider the problem of abnormality localization for clinical applications. While deep learning has driven much recent progress in medical imaging, many clinical challenges are not fully addressed, limiting its broader usage. While recent methods report high diagnostic accuracies, physicians have concerns trusting these algorithm results for diagnostic decision-making purposes because of a general lack of algorithm decision reasoning and interpretability. One potential way to address this problem is to further train these models to localize abnormalities in addition to just classifying them. However, doing this accurately will require a large amount of disease localization annotations by clinical experts, a task that is prohibitively expensive to accomplish for most applications. In this work, we take a step towards addressing these issues by means of a new attention-driven weakly supervised algorithm comprising a hierarchical attention mining framework that unifies activation- and gradient-based visual attention in a holistic manner. Our key algorithmic innovations include the design of explicit ordinal attention constraints, enabling principled model training in a weakly-supervised fashion, while also facilitating the generation of visual-attention-driven model explanations by means of localization cues. On two large-scale chest X-ray datasets (NIH ChestX-ray14 and CheXpert), we demonstrate significant localization performance improvements over the current state of the art while also achieving competitive classification performance. Our code is available on https://github.com/oyxhust/HAM. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.12349
Startup Ecosystem Rankings,Attila Lajos Makai,"The number, importance, and popularity of rankings measuring innovation performance and the strength and resources of ecosystems that provide its spatial framework are on an increasing trend globally. In addition to influencing the specific decisions taken by economic actors, these rankings significantly impact the development of innovation-related policies at regional, national, and international levels. The importance of startup ecosystems is proven by the growing scientific interest, which is demonstrated by the increasing number of related scientific articles. The concept of the startup ecosystem is a relatively new category, the application of which in everyday and scientific life has been gaining ground since the end of the 2000s. In parallel, of course, the demand for measurability and comparability has emerged among decision-makers and scholars. This demand is met by startup ecosystem rankings, which now measure and rank the performance of individual ecosystems on a continental and global scale. However, while the number of scientific publications examining rankings related to higher education, economic performance, or even innovation, can be measured in the order of thousands, scientific research has so far rarely or tangentially addressed the rankings of startup ecosystems. This study and the related research intend to fill this gap by presenting and analysing the characteristics of global rankings and identifying possible future research directions. △ Less","21 December, 2021",https://arxiv.org/pdf/2112.11931
Multimodal Analysis of memes for sentiment extraction,Nayan Varma Alluri;Neeli Dheeraj Krishna,"Memes are one of the most ubiquitous forms of social media communication. The study and processing of memes, which are intrinsically multimedia, is a popular topic right now. The study presented in this research is based on the Memotion dataset, which involves categorising memes based on irony, comedy, motivation, and overall-sentiment. Three separate innovative transformer-based techniques have been developed, and their outcomes have been thoroughly reviewed.The best algorithm achieved a macro F1 score of 0.633 for humour classification, 0.55 for motivation classification, 0.61 for sarcasm classification, and 0.575 for overall sentiment of the meme out of all our techniques. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.11850
Porting a benchmark with a classic workload to blockchain: TPC-C on Hyperledger Fabric,Attila Klenik;Imre Kocsis,"Many cross-organization cooperation applications of blockchain-based distributed ledger technologies (DLT) do not aim at innovation at the cooperation pattern level: essentially the same ''business'' is conducted by the parties, but this time without a central party to be trusted with bookkeeping. The migration to DLT is expected to have a negative performance impact, but some DLTs, such as Hyperledger Fabric, are accepted to be much better suited performance-wise to such use cases than others. However, with the somewhat surprising, but ongoing absence of application-level performance benchmarks for DLTs, cross-DLT comparison for ""classic"" workloads and the evaluation of the performance impact of ""blockchainification"" is still ill-supported. We present the design and Hyperledger Caliper-based open implementation of a full port of the classic TPC-C benchmark to Hyperledger Fabric, complete with a structured approach for transforming the original database schema to a smart contract data model. Initial measurements about the workload characteristics that will affect the design of large-scale performance evaluations are also included. △ Less","21 December, 2021",https://arxiv.org/pdf/2112.11277
Hybrid Learning Aided Technology-Rich Instructional Tools -- A Case Study: Community College of Qatar,Muhammad Jamal Shehab;Mosa Alokla;Mais Alkhateeb;Mohammad Alokla,"Educational Institutions have an essential role in promoting the teaching and learning process, within universities, colleges, and communities. Due to the recent coronavirus COVID 19 pandemic, many educational institutions adopted hybrid learning (HL), which is a combination of classic and online learning. It integrates the advantages of both, and it is a fundamental factor to ensure continued learning. Technological innovations such as HL are changing the teaching process, and how students, lecturers, and administrators interact. Based on this, the Community College of Qatar (CCQ) focused on researching the structures and elements related to the adoption of HL. Thus, the goal of this research paper is to reveal the impact of HL on the learning process in CCQ, and the effective instructional technology (INST) tools required for a successful HL program. Our research questions for the survey were designed to measure the opinions of the students, instructors, and administrators about the HL program. It is observed from the results that the majority of students, instructors, and administrators showed a positive attitude toward HL, but some had negative views and experienced challenges. The results were analyzed and discussed to better utilize HL to meet the growing demands of the community. △ Less","12 December, 2021",https://arxiv.org/pdf/2112.11197
There is an elephant in the room: Towards a critique on the use of fairness in biometrics,Ana Valdivia;Júlia Corbera-Serrajòrdia;Aneta Swianiewicz,"In 2019, the UK's Immigration and Asylum Chamber of the Upper Tribunal dismissed an asylum appeal basing the decision on the output of a biometric system, alongside other discrepancies. The fingerprints of the asylum seeker were found in a biometric database which contradicted the appellant's account. The Tribunal found this evidence unequivocal and denied the asylum claim. Nowadays, the proliferation of biometric systems is shaping public debates around its political, social and ethical implications. Yet whilst concerns towards the racialised use of this technology for migration control have been on the rise, investment in the biometrics industry and innovation is increasing considerably. Moreover, fairness has also been recently adopted by biometrics to mitigate bias and discrimination on biometrics. However, algorithmic fairness cannot distribute justice in scenarios which are broken or intended purpose is to discriminate, such as biometrics deployed at the border. In this paper, we offer a critical reading of recent debates about biometric fairness and show its limitations drawing on research in fairness in machine learning and critical border studies. Building on previous fairness demonstrations, we prove that biometric fairness criteria are mathematically mutually exclusive. Then, the paper moves on illustrating empirically that a fair biometric system is not possible by reproducing experiments from previous works. Finally, we discuss the politics of fairness in biometrics by situating the debate at the border. We claim that bias and error rates have different impact on citizens and asylum seekers. Fairness has overshadowed the elephant in the room of biometrics, focusing on the demographic biases and ethical discourses of algorithms rather than examine how these systems reproduce historical and political injustices. △ Less","24 December, 2021",https://arxiv.org/pdf/2112.11193
Counting Simplices in Hypergraph Streams,Amit Chakrabarti;Themistoklis Haris,"We consider the problem of space-efficiently estimating the number of simplices in a hypergraph stream. This is the most natural hypergraph generalization of the highly-studied problem of estimating the number of triangles in a graph stream. Our input is a k-uniform hypergraph H with n vertices and m hyperedges. A k-simplex in H is a subhypergraph on k+1 vertices X such that all k+1 possible hyperedges among X exist in H. The goal is to process a stream of hyperedges of H and compute a good estimate of T_k(H), the number of k-simplices in H. We design a suite of algorithms for this problem. Under a promise that T_k(H) \ge T, our algorithms use at most four passes and together imply a space bound of O( ε^{-2} \logδ^{-1} \text{polylog} n \cdot \min\{ m^{1+1/k}/T, m/T^{2/(k+1)} \} ) for each fixed k \ge 3, in order to guarantee an estimate within (1\pmε)T_k(H) with probability at least 1-δ. We also give a simpler 1-pass algorithm that achieves O(ε^{-2} \logδ^{-1} \log n\cdot (m/T) ( Δ_E + Δ_V^{1-1/k} )) space, where Δ_E (respectively, Δ_V) denotes the maximum number of k-simplices that share a hyperedge (respectively, a vertex). We complement these algorithmic results with space lower bounds of the form Ω(ε^{-2}), Ω(m^{1+1/k}/T), Ω(m/T^{1-1/k}) and Ω(mΔ_V^{1/k}/T) for multi-pass algorithms and Ω(mΔ_E/T) for 1-pass algorithms, which show that some of the dependencies on parameters in our upper bounds are nearly tight. Our techniques extend and generalize several different ideas previously developed for triangle counting in graphs, using appropriate innovations to handle the more complicated combinatorics of hypergraphs. △ Less","21 December, 2021",https://arxiv.org/pdf/2112.11016
What are Attackers after on IoT Devices? An approach based on a multi-phased multi-faceted IoT honeypot ecosystem and data clustering,Armin Ziaie Tabari;Xinming Ou;Anoop Singhal,"The growing number of Internet of Things (IoT) devices makes it imperative to be aware of the real-world threats they face in terms of cybersecurity. While honeypots have been historically used as decoy devices to help researchers/organizations gain a better understanding of the dynamic of threats on a network and their impact, IoT devices pose a unique challenge for this purpose due to the variety of devices and their physical connections. In this work, by observing real-world attackers' behavior in a low-interaction honeypot ecosystem, we (1) presented a new approach to creating a multi-phased, multi-faceted honeypot ecosystem, which gradually increases the sophistication of honeypots' interactions with adversaries, (2) designed and developed a low-interaction honeypot for cameras that allowed researchers to gain a deeper understanding of what attackers are targeting, and (3) devised an innovative data analytics method to identify the goals of adversaries. Our honeypots have been active for over three years. We were able to collect increasingly sophisticated attack data in each phase. Furthermore, our data analytics points to the fact that the vast majority of attack activities captured in the honeypots share significant similarity, and can be clustered and grouped to better understand the goals, patterns, and trends of IoT attacks in the wild. △ Less","20 December, 2021",https://arxiv.org/pdf/2112.10974
NFTGAN: Non-Fungible Token Art Generation Using Generative Adversarial Networks,Sakib Shahriar;Kadhim Hayawi,"Digital arts have gained an unprecedented level of popularity with the emergence of non-fungible tokens (NFTs). NFTs are cryptographic assets that are stored on blockchain networks and represent a digital certificate of ownership that cannot be forged. NFTs can be incorporated into a smart contract which allows the owner to benefit from a future sale percentage. While digital art producers can benefit immensely with NFTs, their production is time consuming. Therefore, this paper explores the possibility of using generative adversarial networks (GANs) for automatic generation of digital arts. GANs are deep learning architectures that are widely and effectively used for synthesis of audio, images, and video contents. However, their application to NFT arts have been limited. In this paper, a GAN-based architecture is implemented and evaluated for novel NFT-style digital arts generation. Results from the qualitative case study indicate that the generated artworks are comparable to the real samples in terms of being interesting and inspiring and they were judged to be more innovative than real samples. △ Less","27 December, 2021",https://arxiv.org/pdf/2112.10577
Low-Complexity Resource Allocation for Dense Cellular Vehicle-to-Everything (C-V2X) Communications,Mohammad Hossein Bahonar;Mohammad Javad Omidi;Halim Yanikomeroglu,"Vehicular communications are the key enabler of traffic reduction and road safety improvement referred to as cellular vehicle-to-everything (C-V2X) communications. Considering the numerous transmitting entities in next generation cellular networks, most existing resource allocation algorithms are impractical or non-effective to ensure reliable C-V2X communications which lead to safe intelligent transportation systems. We study a centralized framework to develop a low-complexity, scalable, and practical resource allocation scheme for dense C-V2X communications. The NP-hard sum-rate maximization resource allocation problem is formulated as a mixed-integer non-linear non-convex optimization problem considering both cellular vehicular links (CVLs) and non-cellular VLs (NCVLs) quality-of-service (QoS) constraints. By assuming that multiple NCVLs can simultaneously reuse a single cellular link (CL), we propose two low-complexity sub-optimal matching-based algorithms in four steps. The first two steps provide a channel gain-based CVL priority and CL assignment followed by an innovative scalable min-max channel-gain-based CVL-NCVL matching. We propose an analytically proven closed-form fast feasibility check theorem as the third step. The objective function is transformed into a difference of convex (DC) form and the power allocation problem is solved optimally using majorization-minimization (MaMi) method and interior point methods as the last step. Numerical results verify that our schemes are scalable and effective for dense C-V2X communications. The low-complexity and practicality of the proposed schemes for dense cellular networks is also shown. Furthermore, it is shown that the proposed schemes outperform other methods up to %6 in terms of overall sum-rate in dense scenarios and have a near optimal performance. △ Less","16 December, 2021",https://arxiv.org/pdf/2112.10499
Learning to Model the Relationship Between Brain Structural and Functional Connectomes,Yang Li;Gonzalo Mateos;Zhengwu Zhang,"Recent advances in neuroimaging along with algorithmic innovations in statistical learning from network data offer a unique pathway to integrate brain structure and function, and thus facilitate revealing some of the brain's organizing principles at the system level. In this direction, we develop a supervised graph representation learning framework to model the relationship between brain structural connectivity (SC) and functional connectivity (FC) via a graph encoder-decoder system, where the SC is used as input to predict empirical FC. A trainable graph convolutional encoder captures direct and indirect interactions between brain regions-of-interest that mimic actual neural communications, as well as to integrate information from both the structural network topology and nodal (i.e., region-specific) attributes. The encoder learns node-level SC embeddings which are combined to generate (whole brain) graph-level representations for reconstructing empirical FC networks. The proposed end-to-end model utilizes a multi-objective loss function to jointly reconstruct FC networks and learn discriminative graph representations of the SC-to-FC mapping for downstream subject (i.e., graph-level) classification. Comprehensive experiments demonstrate that the learnt representations of said relationship capture valuable information from the intrinsic properties of the subject's brain networks and lead to improved accuracy in classifying a large population of heavy drinkers and non-drinkers from the Human Connectome Project. Our work offers new insights on the relationship between brain networks that support the promising prospect of using graph representation learning to discover more about human brain activity and function. △ Less","18 December, 2021",https://arxiv.org/pdf/2112.09906
It's Time to Do Something: Mitigating the Negative Impacts of Computing Through a Change to the Peer Review Process,Brent Hecht;Lauren Wilcox;Jeffrey P. Bigham;Johannes Schöning;Ehsan Hoque;Jason Ernst;Yonatan Bisk;Luigi De Russis;Lana Yarosh;Bushra Anjum;Danish Contractor;Cathy Wu,"The computing research community needs to work much harder to address the downsides of our innovations. Between the erosion of privacy, threats to democracy, and automation's effect on employment (among many other issues), we can no longer simply assume that our research will have a net positive impact on the world. While bending the arc of computing innovation towards societal benefit may at first seem intractable, we believe we can achieve substantial progress with a straightforward step: making a small change to the peer review process. As we explain below, we hypothesize that our recommended change will force computing researchers to more deeply consider the negative impacts of their work. We also expect that this change will incentivize research and policy that alleviates computing's negative impacts. △ Less","17 December, 2021",https://arxiv.org/pdf/2112.09544
A Survey on the Applications of Blockchains in Security of IoT Systems,Zulfiqar Ali Khan;Akbar Siami Namin,"The Internet of Things (IoT) has already changed our daily lives by integrating smart devices together towards delivering high quality services to its clients. These devices when integrated together form a network through which massive amount of data can be produced, transferred, and shared. A critical concern is the security and integrity of such a complex platform to ensure the sustainability and reliability of these IoT-based systems. Blockchain is an emerging technology that has demonstrated its unique features and capabilities for different problems and application domains including IoT-based systems. This survey paper reviews the adaptation of Blockchain in the context of IoT to represent how this technology is capable of addressing the integration and security problems of devices connected to IoT systems. The innovation of this survey is that we present a survey based upon the integration approaches and security issues of IoT data and discuss the role of Blockchain in connection with these issues. △ Less","16 December, 2021",https://arxiv.org/pdf/2112.09296
Open-Source Software Radio Platform for Research on Cellular Networked UAVs -- It Works!,Aly Sabri Abdalla;Andrew Yingst;Keith Powell;Antoni Gelonch-Bosch;Vuk Marojevic,"Cellular network-connected unmanned aerial vehicles (UAVs) experience different radio propagation conditions than radio nodes on the ground. Therefore, it has become critical to investigate the performance of aerial radios, both theoretically and through field trials. In this paper, we consider low-altitude aerial nodes that are served by an experimental cellular network. We provide a detailed description of the hardware and software components needed for establishing a broadband wireless testbed for UAV communications research using software radios. Results show that a testbed for innovation in UAV communications and networking is feasible with commercial off-the-shelf hardware, open-source software, and low-power signaling. △ Less","11 December, 2021",https://arxiv.org/pdf/2112.09067
Use Image Clustering to Facilitate Technology Assisted Review,Haozhen Zhao;Fusheng Wei;Hilary Quatinetz;Han Qin;Adam Dabrowski,"During the past decade breakthroughs in GPU hardware and deep neural networks technologies have revolutionized the field of computer vision, making image analytical potentials accessible to a range of real-world applications. Technology Assisted Review (TAR) in electronic discovery though traditionally has dominantly dealt with textual content, is witnessing a rising need to incorporate multimedia content in the scope. We have developed innovative image analytics applications for TAR in the past years, such as image classification, image clustering, and object detection, etc. In this paper, we discuss the use of image clustering applications to facilitate TAR based on our experiences in serving clients. We describe our general workflow on leveraging image clustering in tasks and use statistics from real projects to showcase the effectiveness of using image clustering in TAR. We also summarize lessons learned and best practices on using image clustering in TAR. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.08604
RA V-Net: Deep learning network for automated liver segmentation,Zhiqi Lee;Sumin Qi;Chongchong Fan;Ziwei Xie,"Accurate segmentation of the liver is a prerequisite for the diagnosis of disease. Automated segmentation is an important application of computer-aided detection and diagnosis of liver disease. In recent years, automated processing of medical images has gained breakthroughs. However, the low contrast of abdominal scan CT images and the complexity of liver morphology make accurate automatic segmentation challenging. In this paper, we propose RA V-Net, which is an improved medical image automatic segmentation model based on U-Net. It has the following three main innovations. CofRes Module (Composite Original Feature Residual Module) is proposed. With more complex convolution layers and skip connections to make it obtain a higher level of image feature extraction capability and prevent gradient disappearance or explosion. AR Module (Attention Recovery Module) is proposed to reduce the computational effort of the model. In addition, the spatial features between the data pixels of the encoding and decoding modules are sensed by adjusting the channels and LSTM convolution. Finally, the image features are effectively retained. CA Module (Channel Attention Module) is introduced, which used to extract relevant channels with dependencies and strengthen them by matrix dot product, while weakening irrelevant channels without dependencies. The purpose of channel attention is achieved. The attention mechanism provided by LSTM convolution and CA Module are strong guarantees for the performance of the neural network. The accuracy of U-Net network: 0.9862, precision: 0.9118, DSC: 0.8547, JSC: 0.82. The evaluation metrics of RA V-Net, accuracy: 0.9968, precision: 0.9597, DSC: 0.9654, JSC: 0.9414. The most representative metric for the segmentation effect is DSC, which improves 0.1107 over U-Net, and JSC improves 0.1214. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.08232
Temporal Action Proposal Generation with Background Constraint,Haosen Yang;Wenhao Wu;Lining Wang;Sheng Jin;Boyang Xia;Hongxun Yao;Hujie Huang,"Temporal action proposal generation (TAPG) is a challenging task that aims to locate action instances in untrimmed videos with temporal boundaries. To evaluate the confidence of proposals, the existing works typically predict action score of proposals that are supervised by the temporal Intersection-over-Union (tIoU) between proposal and the ground-truth. In this paper, we innovatively propose a general auxiliary Background Constraint idea to further suppress low-quality proposals, by utilizing the background prediction score to restrict the confidence of proposals. In this way, the Background Constraint concept can be easily plug-and-played into existing TAPG methods (e.g., BMN, GTAD). From this perspective, we propose the Background Constraint Network (BCNet) to further take advantage of the rich information of action and background. Specifically, we introduce an Action-Background Interaction module for reliable confidence evaluation, which models the inconsistency between action and background by attention mechanisms at the frame and clip levels. Extensive experiments are conducted on two popular benchmarks, i.e., ActivityNet-1.3 and THUMOS14. The results demonstrate that our method outperforms state-of-the-art methods. Equipped with the existing action classifier, our method also achieves remarkable performance on the temporal action localization task. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.07984
DRaGon: Mining Latent Radio Channel Information from Geographical Data Leveraging Deep Learning,Benjamin Sliwa;Melina Geis;Caner Bektas;Melisa Lopéz;Preben Mogensen;Christian Wietfeld,"Radio channel modeling is one of the most fundamental aspects in the process of designing, optimizing, and simulating wireless communication networks. In this field, long-established approaches such as analytical channel models and ray tracing techniques represent the de-facto standard methodologies. However, as demonstrated by recent results, there remains an untapped potential to innovate this research field by enriching model-based approaches with machine learning techniques. In this paper, we present Deep RAdio channel modeling from GeOinformatioN (DRaGon) as a novel machine learning-enabled method for automatic generation of Radio Environmental Maps (REMs) from geographical data. For achieving accurate path loss prediction results, DRaGon combines determining features extracted from a three-dimensional model of the radio propagation environment with raw images of the receiver area within a deep learning model. In a comprehensive performance evaluation and validation campaign, we compare the accuracy of the proposed approach with real world measurements, ray tracing analyses, and well-known channel models. It is found that the combination of expert knowledge from the communications domain and the data analysis capabilities of deep learning allows to achieve a significantly higher prediction accuracy than the reference methods. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.07941
Blockchain Developments and Innovations,Mahdi Fahmideh;Anuradha Gunawardana;Shiping Chen;Jun Shen;Brian Yecies,"Blockchain has received expanding interest from various domains. Institutions, enterprises, governments, and agencies are interested in Blockchain potential to augment their software systems. The unique requirements and characteristics of Blockchain platforms raise new challenges involving extensive enhancement to conventional software development processes to meet the needs of these domains. Software engineering approaches supporting Blockchain-oriented developments have been slow to materialize, despite proposals in the literature, and they have yet to be objectively analyzed. A critical appraisal of these innovations is crucial to identify their respective strengths and weaknesses. We present an analytical evaluation of several prominent Blockchain-oriented methods through a comprehensive, criteria-based evaluation framework. The results can be used for comparing, adapting, and developing a new generation of Blockchain-oriented software development processes and innovations. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07179
"Matrix-free approaches for GPU acceleration of a high-order finite element hydrodynamics application using MFEM, Umpire, and RAJA",Arturo Vargas;Thomas M. Stitt;Kenneth Weiss;Vladimir Z. Tomov;Jean-Sylvain Camier;Tzanio Kolev;Robert N. Rieben,"With the introduction of advanced heterogeneous computing architectures based on GPU accelerators, large-scale production codes have had to rethink their numerical algorithms and incorporate new programming models and memory management strategies in order to run efficiently on the latest supercomputers. In this work we discuss our co-design strategy to address these challenges and achieve performance and portability with MARBL, a next-generation multi-physics code in development at Lawrence Livermore National Laboratory. We present a two-fold approach, wherein new hardware is used to motivate both new algorithms and new abstraction layers, resulting in a single source application code suitable for a variety of platforms. Focusing on MARBL's ALE hydrodynamics package, we demonstrate scalability on different platforms and highlight that many of our innovations have been contributed back to open-source software libraries, such as MFEM (finite element algorithms) and RAJA (kernel abstractions). △ Less","13 December, 2021",https://arxiv.org/pdf/2112.07075
Land use identification through social network interaction,Diana C. Pauca-Quispe;Cinthya Butron-Revilla;Ernesto Suarez-Lopez;Karla Aranibar-Tila;Jesus S. Aguilar-Ruiz,"The Internet generates large volumes of data at a high rate, in particular, posts on social networks. Although social network data has numerous semantic adulterations, and is not intended to be a source of geo-spatial information, in the text of posts we find pieces of important information about how people relate to their environment, which can be used to identify interesting aspects of how human beings interact with portions of land based on their activities. This research proposes a methodology for the identification of land uses using Natural Language Processing (NLP) from the contents of the popular social network Twitter. It will be approached by identifying keywords with linguistic patterns from the text, and the geographical coordinates associated with the publication. Context-specific innovations are introduced to deal with data across South America and, in particular, in the city of Arequipa, Peru. The objective is to identify the five main land uses: residential, commercial, institutional-governmental, industrial-offices and unbuilt land. Within the framework of urban planning and sustainable urban management, the methodology contributes to the optimization of the identification techniques applied for the updating of land use cadastres, since the results achieved an accuracy of about 90%, which motivates its application in the real context. In addition, it would allow the identification of land use categories at a more detailed level, in situations such as a complex/mixed distribution building based on the amount of data collected. Finally, the methodology makes land use information available in a more up-to-date fashion and, above all, avoids the high economic cost of the non-automatic production of land use maps for cities, mostly in developing countries. △ Less","5 December, 2021",https://arxiv.org/pdf/2112.06704
Using Machine Learning to Find New Density Functionals,Bhupalee Kalita;Kieron Burke,"Machine learning has now become an integral part of research and innovation. The field of machine learning density functional theory has continuously expanded over the years while making several noticeable advances. We briefly discuss the status of this field and point out some current and future challenges. We also talk about how state-of-the-art science and technology tools can help overcome these challenges. This draft is a part of the ""Roadmap on Machine Learning in Electronic Structure"" to be published in Electronic Structure (EST). △ Less","3 December, 2021",https://arxiv.org/pdf/2112.05554
Combining Design Thinking and Software Requirements Engineering to create Human-centered Software-intensive Systems,Jennifer Hehn;Daniel Mendez,"Effective Requirements Engineering is a crucial activity in softwareintensive development projects. The human-centric working mode of Design Thinking is considered a powerful way to complement such activities when designing innovative systems. Research has already made great strides to illustrate the benefits of using Design Thinking for Requirements Engineering. However, it has remained mostly unclear how to actually realize a combination of both. In this chapter, we contribute an artifact-based model that integrates Design Thinking and Requirements Engineering for innovative software-intensive systems. Drawing from our research and project experiences, we suggest three strategies for tailoring and integrating Design Thinking and Requirements Engineering with complementary synergies. △ Less","10 December, 2021",https://arxiv.org/pdf/2112.05549
Improving Productivity through Corporate Hackathons: A Multiple Case Study of Two Large-scale Agile Organizations,Nils Brede Moe;Rasmus Ulfsnes;Viktoria Stray;Darja Smite,"Software development companies organize hackathons to encourage innovation. Despite many benefits of hackathons, in large-scale agile organizations where many teams work together, stopping the ongoing work results in a significant decrease in the immediate output. Motivated by the need to understand whether and how to run hackathons, we investigated how the practice affects productivity on the individual and organizational levels. By mapping the benefits and challenges to an established productivity framework, we found that hackathons improve developers' satisfaction and well-being, strengthen the company culture, improve performance (as many ideas are tested), increase activity (as the ideas are developed quickly), and improve communication and collaboration (because the social network is strengthened). Addressing managerial concerns, we found that hackathons also increase efficiency and flow because people learn to complete work and make progress quickly, and they build new competence. Finally, with respect to virtual hackathons we found that developers work more in isolation because tasks are split between team members resulting in less collaboration. This means that some important, expected hackathon values in virtual contexts require extra effort and cannot be taken for granted. △ Less","10 December, 2021",https://arxiv.org/pdf/2112.05528
Illumination and Temperature-Aware Multispectral Networks for Edge-Computing-Enabled Pedestrian Detection,Yifan Zhuang;Ziyuan Pu;Jia Hu;Yinhai Wang,"Accurate and efficient pedestrian detection is crucial for the intelligent transportation system regarding pedestrian safety and mobility, e.g., Advanced Driver Assistance Systems, and smart pedestrian crosswalk systems. Among all pedestrian detection methods, vision-based detection method is demonstrated to be the most effective in previous studies. However, the existing vision-based pedestrian detection algorithms still have two limitations that restrict their implementations, those being real-time performance as well as the resistance to the impacts of environmental factors, e.g., low illumination conditions. To address these issues, this study proposes a lightweight Illumination and Temperature-aware Multispectral Network (IT-MN) for accurate and efficient pedestrian detection. The proposed IT-MN is an efficient one-stage detector. For accommodating the impacts of environmental factors and enhancing the sensing accuracy, thermal image data is fused by the proposed IT-MN with visual images to enrich useful information when visual image quality is limited. In addition, an innovative and effective late fusion strategy is also developed to optimize the image fusion performance. To make the proposed model implementable for edge computing, the model quantization is applied to reduce the model size by 75% while shortening the inference time significantly. The proposed algorithm is evaluated by comparing with the selected state-of-the-art algorithms using a public dataset collected by in-vehicle cameras. The results show that the proposed algorithm achieves a low miss rate and inference time at 14.19% and 0.03 seconds per image pair on GPU. Besides, the quantized IT-MN achieves an inference time of 0.21 seconds per image pair on the edge device, which also demonstrates the potentiality of deploying the proposed model on edge devices as a highly efficient pedestrian detection algorithm. △ Less","9 December, 2021",https://arxiv.org/pdf/2112.05053
Application of Artificial Intelligence and Machine Learning in Libraries: A Systematic Review,Rajesh Kumar Das;Mohammad Sharif Ul Islam,"As the concept and implementation of cutting-edge technologies like artificial intelligence and machine learning has become relevant, academics, researchers and information professionals involve research in this area. The objective of this systematic literature review is to provide a synthesis of empirical studies exploring application of artificial intelligence and machine learning in libraries. To achieve the objectives of the study, a systematic literature review was conducted based on the original guidelines proposed by Kitchenham et al. (2009). Data was collected from Web of Science, Scopus, LISA and LISTA databases. Following the rigorous/ established selection process, a total of thirty-two articles were finally selected, reviewed and analyzed to summarize on the application of AI and ML domain and techniques which are most often used in libraries. Findings show that the current state of the AI and ML research that is relevant with the LIS domain mainly focuses on theoretical works. However, some researchers also emphasized on implementation projects or case studies. This study will provide a panoramic view of AI and ML in libraries for researchers, practitioners and educators for furthering the more technology-oriented approaches, and anticipating future innovation pathways. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.04573
Binary Change Guided Hyperspectral Multiclass Change Detection,Meiqi Hu;Chen Wu;Bo Du;Liangpei Zhang,"Characterized by tremendous spectral information, hyperspectral image is able to detect subtle changes and discriminate various change classes for change detection. The recent research works dominated by hyperspectral binary change detection, however, cannot provide fine change classes information. And most methods incorporating spectral unmixing for hyperspectral multiclass change detection (HMCD), yet suffer from the neglection of temporal correlation and error accumulation. In this study, we proposed an unsupervised Binary Change Guided hyperspectral multiclass change detection Network (BCG-Net) for HMCD, which aims at boosting the multiclass change detection result and unmixing result with the mature binary change detection approaches. In BCG-Net, a novel partial-siamese united-unmixing module is designed for multi-temporal spectral unmixing, and a groundbreaking temporal correlation constraint directed by the pseudo-labels of binary change detection result is developed to guide the unmixing process from the perspective of change detection, encouraging the abundance of the unchanged pixels more coherent and that of the changed pixels more accurate. Moreover, an innovative binary change detection rule is put forward to deal with the problem that traditional rule is susceptible to numerical values. The iterative optimization of the spectral unmixing process and the change detection process is proposed to eliminate the accumulated errors and bias from unmixing result to change detection result. The experimental results demonstrate that our proposed BCG-Net could achieve comparative or even outstanding performance of multiclass change detection among the state-of-the-art approaches and gain better spectral unmixing results at the same time. △ Less","10 December, 2021",https://arxiv.org/pdf/2112.04493
Ethical and social risks of harm from Language Models,Laura Weidinger;John Mellor;Maribeth Rauh;Conor Griffin;Jonathan Uesato;Po-Sen Huang;Myra Cheng;Mia Glaese;Borja Balle;Atoosa Kasirzadeh;Zac Kenton;Sasha Brown;Will Hawkins;Tom Stepleton;Courtney Biles;Abeba Birhane;Julia Haas;Laura Rimell;Lisa Anne Hendricks;William Isaac;Sean Legassick;Geoffrey Irving;Iason Gabriel,"This paper aims to help structure the risk landscape associated with large-scale Language Models (LMs). In order to foster advances in responsible innovation, an in-depth understanding of the potential risks posed by these models is needed. A wide range of established and anticipated risks are analysed in detail, drawing on multidisciplinary expertise and literature from computer science, linguistics, and social sciences. We outline six specific risk areas: I. Discrimination, Exclusion and Toxicity, II. Information Hazards, III. Misinformation Harms, V. Malicious Uses, V. Human-Computer Interaction Harms, VI. Automation, Access, and Environmental Harms. The first area concerns the perpetuation of stereotypes, unfair discrimination, exclusionary norms, toxic language, and lower performance by social group for LMs. The second focuses on risks from private data leaks or LMs correctly inferring sensitive information. The third addresses risks arising from poor, false or misleading information including in sensitive domains, and knock-on risks such as the erosion of trust in shared information. The fourth considers risks from actors who try to use LMs to cause harm. The fifth focuses on risks specific to LLMs used to underpin conversational agents that interact with human users, including unsafe use, manipulation or deception. The sixth discusses the risk of environmental harm, job automation, and other challenges that may have a disparate effect on different social groups or communities. In total, we review 21 risks in-depth. We discuss the points of origin of different risks and point to potential mitigation approaches. Lastly, we discuss organisational responsibilities in implementing mitigations, and the role of collaboration and participation. We highlight directions for further research, particularly on expanding the toolkit for assessing and evaluating the outlined risks in LMs. △ Less","8 December, 2021",https://arxiv.org/pdf/2112.04359
Non parametric estimation of causal populations in a counterfactual scenario,Celine Beji;Florian Yger;Jamal Atif,"In causality, estimating the effect of a treatment without confounding inference remains a major issue because requires to assess the outcome in both case with and without treatment. Not being able to observe simultaneously both of them, the estimation of potential outcome remains a challenging task. We propose an innovative approach where the problem is reformulated as a missing data model. The aim is to estimate the hidden distribution of \emph{causal populations}, defined as a function of treatment and outcome. A Causal Auto-Encoder (CAE), enhanced by a prior dependent on treatment and outcome information, assimilates the latent space to the probability distribution of the target populations. The features are reconstructed after being reduced to a latent space and constrained by a mask introduced in the intermediate layer of the network, containing treatment and outcome information. △ Less","8 December, 2021",https://arxiv.org/pdf/2112.04288
DMRVisNet: Deep Multi-head Regression Network for Pixel-wise Visibility Estimation Under Foggy Weather,Jing You;Shaocheng Jia;Xin Pei;Danya Yao,"Scene perception is essential for driving decision-making and traffic safety. However, fog, as a kind of common weather, frequently appears in the real world, especially in the mountain areas, making it difficult to accurately observe the surrounding environments. Therefore, precisely estimating the visibility under foggy weather can significantly benefit traffic management and safety. To address this, most current methods use professional instruments outfitted at fixed locations on the roads to perform the visibility measurement; these methods are expensive and less flexible. In this paper, we propose an innovative end-to-end convolutional neural network framework to estimate the visibility leveraging Koschmieder's law exclusively using the image data. The proposed method estimates the visibility by integrating the physical model into the proposed framework, instead of directly predicting the visibility value via the convolutional neural work. Moreover, we estimate the visibility as a pixel-wise visibility map against those of previous visibility measurement methods which solely predict a single value for an entire image. Thus, the estimated result of our method is more informative, particularly in uneven fog scenarios, which can benefit to developing a more precise early warning system for foggy weather, thereby better protecting the intelligent transportation infrastructure systems and promoting its development. To validate the proposed framework, a virtual dataset, FACI, containing 3,000 foggy images in different concentrations, is collected using the AirSim platform. Detailed experiments show that the proposed method achieves performance competitive to those of state-of-the-art methods. △ Less","8 December, 2021",https://arxiv.org/pdf/2112.04278
SimulSLT: End-to-End Simultaneous Sign Language Translation,Aoxiong Yin;Zhou Zhao;Jinglin Liu;Weike Jin;Meng Zhang;Xingshan Zeng;Xiaofei He,"Sign language translation as a kind of technology with profound social significance has attracted growing researchers' interest in recent years. However, the existing sign language translation methods need to read all the videos before starting the translation, which leads to a high inference latency and also limits their application in real-life scenarios. To solve this problem, we propose SimulSLT, the first end-to-end simultaneous sign language translation model, which can translate sign language videos into target text concurrently. SimulSLT is composed of a text decoder, a boundary predictor, and a masked encoder. We 1) use the wait-k strategy for simultaneous translation. 2) design a novel boundary predictor based on the integrate-and-fire module to output the gloss boundary, which is used to model the correspondence between the sign language video and the gloss. 3) propose an innovative re-encode method to help the model obtain more abundant contextual information, which allows the existing video features to interact fully. The experimental results conducted on the RWTH-PHOENIX-Weather 2014T dataset show that SimulSLT achieves BLEU scores that exceed the latest end-to-end non-simultaneous sign language translation model while maintaining low latency, which proves the effectiveness of our method. △ Less","8 December, 2021",https://arxiv.org/pdf/2112.04228
Augment & Valuate : A Data Enhancement Pipeline for Data-Centric AI,Youngjune Lee;Oh Joon Kwon;Haeju Lee;Joonyoung Kim;Kangwook Lee;Kee-Eung Kim,"Data scarcity and noise are important issues in industrial applications of machine learning. However, it is often challenging to devise a scalable and generalized approach to address the fundamental distributional and semantic properties of dataset with black box models. For this reason, data-centric approaches are crucial for the automation of machine learning operation pipeline. In order to serve as the basis for this automation, we suggest a domain-agnostic pipeline for refining the quality of data in image classification problems. This pipeline contains data valuation, cleansing, and augmentation. With an appropriate combination of these methods, we could achieve 84.711% test accuracy (ranked #6, Honorable Mention in the Most Innovative) in the Data-Centric AI competition only with the provided dataset. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.03837
Federated Deep Reinforcement Learning for the Distributed Control of NextG Wireless Networks,Peyman Tehrani;Francesco Restuccia;Marco Levorato,"Next Generation (NextG) networks are expected to support demanding tactile internet applications such as augmented reality and connected autonomous vehicles. Whereas recent innovations bring the promise of larger link capacity, their sensitivity to the environment and erratic performance defy traditional model-based control rationales. Zero-touch data-driven approaches can improve the ability of the network to adapt to the current operating conditions. Tools such as reinforcement learning (RL) algorithms can build optimal control policy solely based on a history of observations. Specifically, deep RL (DRL), which uses a deep neural network (DNN) as a predictor, has been shown to achieve good performance even in complex environments and with high dimensional inputs. However, the training of DRL models require a large amount of data, which may limit its adaptability to ever-evolving statistics of the underlying environment. Moreover, wireless networks are inherently distributed systems, where centralized DRL approaches would require excessive data exchange, while fully distributed approaches may result in slower convergence rates and performance degradation. In this paper, to address these challenges, we propose a federated learning (FL) approach to DRL, which we refer to federated DRL (F-DRL), where base stations (BS) collaboratively train the embedded DNN by only sharing models' weights rather than training data. We evaluate two distinct versions of F-DRL, value and policy based, and show the superior performance they achieve compared to distributed and centralized DRL. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.03465
Approximating Nash Equilibrium in Random Graphical Games,Morris Yau,"Computing Nash equilibrium in multi-agent games is a longstanding challenge at the interface of game theory and computer science. It is well known that a general normal form game in N players and k strategies requires exponential space simply to write down. This Curse of Multi-Agents prompts the study of succinct games which can be written down efficiently. A canonical example of a succinct game is the graphical game which models players as nodes in a graph interacting with only their neighbors in direct analogy with markov random fields. Graphical games have found applications in wireless, financial, and social networks. However, computing the nash equilbrium of graphical games has proven challenging. Even for polymatrix games, a model where payoffs to an agent can be written as the sum of payoffs of interactions with the agent's neighbors, it has been shown that computing an epsilon approximate nash equilibrium is PPAD hard for epsilon smaller than a constant. The focus of this work is to circumvent this computational hardness by considering average case graph models i.e random graphs. We provide a quasipolynomial time approximation scheme (QPTAS) for computing an epsilon approximate nash equilibrium of polymatrix games on random graphs with edge density greater than poly(k, 1/epsilon, ln(N))$ with high probability. Furthermore, with the same runtime we can compute an epsilon-approximate Nash equilibrium that epsilon-approximates the maximum social welfare of any nash equilibrium of the game. Our primary technical innovation is an ""accelerated rounding"" of a novel hierarchical convex program for the nash equilibrium problem. Our accelerated rounding also yields faster algorithms for Max-2CSP on the same family of random graphs, which may be of independent interest. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.03442
SIMD-Optimized Search Over Sorted Data,Benjamin Mastripolito;Nicholas Koskelo;Dylan Weatherred;David A. Pimentel;Daniel Sheppard;Anna Pietarila Graham;Laura Monroe;Robert Robey,"Applications often require a fast, single-threaded search algorithm over sorted data, typical in table-lookup operations. We explore various search algorithms for a large number of search candidates over a relatively small array of logarithmically-distributed sorted data. These include an innovative hash-based search that takes advantage of floating point representation to bin data by the exponent. Algorithms that can be optimized to take advantage of SIMD vector instructions are of particular interest. We then conduct a case study applying our results and analyzing algorithmic performance with the EOSPAC package. EOSPAC is a table look-up library for manipulation and interpolation of SESAME equation-of-state data. Our investigation results in a couple of algorithms with better performance with a best case 8x speedup over the original EOSPAC Hunt-and-Locate implementation. Our techniques are generalizable to other instances of search algorithms seeking to get a performance boost from vectorization. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.03229
Market Microstructure of Non Fungible Tokens,Mayukh Mukhopadhyay;Kaushik Ghosh,"Non Fungible Token (NFT) Industry has been witnessing multi-million dollar trade in recent times. With rapid innovation of the NFT market environment by technology, innovation, and decentralization, it is becoming hard to distinguish between genuine NFT from fads and scams. This article discuss the NFT market microstructure, with a focus on price formation, market structure, transparency, and applications to other financial areas. Market manipulation in NFT market with the context of wash-sale patterns has also been surveyed. The article concludes by providing pointers on due-diligence activity that can be adopted by investors to mitigate NFT trading risk. △ Less","8 October, 2021",https://arxiv.org/pdf/2112.03172
Efficient Pressure: Improving efficiency for signalized intersections,Qiang Wu;Liang Zhang;Jun Shen;Linyuan Lü;Bo Du;Jianqing Wu,"Since conventional approaches could not adapt to dynamic traffic conditions, reinforcement learning (RL) has attracted more attention to help solve the traffic signal control (TSC) problem. However, existing RL-based methods are rarely deployed considering that they are neither cost-effective in terms of computing resources nor more robust than traditional approaches, which raises a critical research question: how to construct an adaptive controller for TSC with less training and reduced complexity based on RL-based approach? To address this question, in this paper, we (1) innovatively specify the traffic movement representation as a simple but efficient pressure of vehicle queues in a traffic network, namely efficient pressure (EP); (2) build a traffic signal settings protocol, including phase duration, signal phase number and EP for TSC; (3) design a TSC approach based on the traditional max pressure (MP) approach, namely efficient max pressure (Efficient-MP) using the EP to capture the traffic state; and (4) develop a general RL-based TSC algorithm template: efficient Xlight (Efficient-XLight) under EP. Through comprehensive experiments on multiple real-world datasets in our traffic signal settings' protocol for TSC, we demonstrate that efficient pressure is complementary to traditional and RL-based modeling to design better TSC methods. Our code is released on Github. △ Less","4 December, 2021",https://arxiv.org/pdf/2112.02336
International Conferences of Bibliometrics,Grischa Fraumann;Rogerio Mugnaini;Elias Sanz-Casado,"Conferences are deeply connected to research fields, in this case bibliometrics. As such, they are a venue to present and discuss current and innovative research, and play an important role for the scholarly community. In this article, we provide an overview on the history of conferences in bibliometrics. We conduct an analysis to list the most prominent conferences that were announced in the newsletter by ISSI, the International Society for Scientometrics and Informetrics. Furthermore, we describe how conferences are connected to learned societies and journals. Finally, we provide an outlook on how conferences might change in future. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.02183
Evaluating Two Approaches to Assessing Student Progress in Cybersecurity Exercises,Valdemar Švábenský;Richard Weiss;Jack Cook;Jan Vykopal;Pavel Čeleda;Jens Mache;Radoslav Chudovský;Ankur Chattopadhyay,"Cybersecurity students need to develop practical skills such as using command-line tools. Hands-on exercises are the most direct way to assess these skills, but assessing students' mastery is a challenging task for instructors. We aim to alleviate this issue by modeling and visualizing student progress automatically throughout the exercise. The progress is summarized by graph models based on the shell commands students typed to achieve discrete tasks within the exercise. We implemented two types of models and compared them using data from 46 students at two universities. To evaluate our models, we surveyed 22 experienced computing instructors and qualitatively analyzed their responses. The majority of instructors interpreted the graph models effectively and identified strengths, weaknesses, and assessment use cases for each model. Based on the evaluation, we provide recommendations to instructors and explain how our graph models innovate teaching and promote further research. The impact of this paper is threefold. First, it demonstrates how multiple institutions can collaborate to share approaches to modeling student progress in hands-on exercises. Second, our modeling techniques generalize to data from different environments to support student assessment, even outside the cybersecurity domain. Third, we share the acquired data and open-source software so that others can use the models in their classes or research. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.02053
Scale up to infinity: the UWB Indoor Global Positioning System,Luca Santoro;Matteo Nardello;Davide Brunelli;Daniele Fontanelli,"Determining assets position with high accuracy and scalability is one of the most investigated technology on the market. The accuracy provided by satellites-based positioning systems (i.e., GLONASS or Galileo) is not always sufficient when a decimeter-level accuracy is required or when there is the need of localising entities that operate inside indoor environments. Scalability is also a recurrent problem when dealing with indoor positioning systems. This paper presents an innovative UWB Indoor GPS-Like local positioning system able to tracks any number of assets without decreasing measurements update rate. To increase the system's accuracy the mathematical model and the sources of uncertainties are investigated. Results highlight how the proposed implementation provides positioning information with an absolute maximum error below 20 cm. Scalability is also resolved thanks to DTDoA transmission mechanisms not requiring an active role from the asset to be tracked. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.01950
Localized Feature Aggregation Module for Semantic Segmentation,Ryouichi Furukawa;Kazuhiro Hotta,"We propose a new information aggregation method which called Localized Feature Aggregation Module based on the similarity between the feature maps of an encoder and a decoder. The proposed method recovers positional information by emphasizing the similarity between decoder's feature maps with superior semantic information and encoder's feature maps with superior positional information. The proposed method can learn positional information more efficiently than conventional concatenation in the U-net and attention U-net. Additionally, the proposed method also uses localized attention range to reduce the computational cost. Two innovations contributed to improve the segmentation accuracy with lower computational cost. By experiments on the Drosophila cell image dataset and COVID-19 image dataset, we confirmed that our method outperformed conventional methods. △ Less","2 December, 2021",https://arxiv.org/pdf/2112.01702
Systematic Generalization with Edge Transformers,Leon Bergen;Timothy J. O'Donnell;Dzmitry Bahdanau,"Recent research suggests that systematic generalization in natural language understanding remains a challenge for state-of-the-art neural models such as Transformers and Graph Neural Networks. To tackle this challenge, we propose Edge Transformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The first key idea in Edge Transformers is to associate vector states with every edge, that is, with every pair of input nodes -- as opposed to just every node, as it is done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge representations in a way that is inspired by unification from logic programming. We evaluate Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing. In all three settings, the Edge Transformer outperforms Relation-aware, Universal and classical Transformer baselines. △ Less","1 December, 2021",https://arxiv.org/pdf/2112.00578
'Entanglement' -- A new dynamic metric to measure team flow,P. A. Gloor;M. P. Zylka;A. Fronzetti Colladon;M. Makai,"We introduce ""entanglement"", a novel metric to measure how synchronized communication between team members is. This measure calculates the Euclidean distance among team members' social network metrics timeseries. We validate the metric with four case studies. The first case study uses entanglement of 11 medical innovation teams to predict team performance and learning behavior. The second case looks at the e-mail communication of 113 senior executives of an international services firm, predicting employee turnover through lack of entanglement of an employee. The third case analyzes the individual employee performance of 81 managers. The fourth case study predicts performance of 13 customer-dedicated teams at a big international company by comparing entanglement in the e-mail interactions with satisfaction of their customers measured through Net Promoter Score (NPS). While we can only speculate about what is causing the entanglement effect, we find that it is a new and versatile indicator for the analysis of employees' communication, analyzing the hitherto underused temporal dimension of online social networks which could be used as a powerful predictor of employee and team performance, employee turnover, and customer satisfaction. △ Less","1 December, 2021",https://arxiv.org/pdf/2112.00538
Learning Oriented Remote Sensing Object Detection via Naive Geometric Computing,Yanjie Wang;Xu Zou;Zhijun Zhang;Wenhui Xu;Liqun Chen;Sheng Zhong;Luxin Yan;Guodong Wang,"Detecting oriented objects along with estimating their rotation information is one crucial step for analyzing remote sensing images. Despite that many methods proposed recently have achieved remarkable performance, most of them directly learn to predict object directions under the supervision of only one (e.g. the rotation angle) or a few (e.g. several coordinates) groundtruth values individually. Oriented object detection would be more accurate and robust if extra constraints, with respect to proposal and rotation information regression, are adopted for joint supervision during training. To this end, we innovatively propose a mechanism that simultaneously learns the regression of horizontal proposals, oriented proposals, and rotation angles of objects in a consistent manner, via naive geometric computing, as one additional steady constraint (see Figure 1). An oriented center prior guided label assignment strategy is proposed for further enhancing the quality of proposals, yielding better performance. Extensive experiments demonstrate the model equipped with our idea significantly outperforms the baseline by a large margin to achieve a new state-of-the-art result without any extra computational burden during inference. Our proposed idea is simple and intuitive that can be readily implemented. Source codes and trained models are involved in supplementary files. △ Less","1 December, 2021",https://arxiv.org/pdf/2112.00504
Harnessing expressive capacity of Machine Learning modeling to represent complex coupling of Earth's auroral space weather regimes,Jack Ziegler;Ryan M. Mcgranaghan,"We develop multiple Deep Learning (DL) models that advance the state-of-the-art predictions of the global auroral particle precipitation. We use observations from low Earth orbiting spacecraft of the electron energy flux to develop a model that improves global nowcasts (predictions at the time of observation) of the accelerated particles. Multiple Machine Learning (ML) modeling approaches are compared, including a novel multi-task model, models with tail- and distribution-based loss functions, and a spatio-temporally sparse 2D-convolutional model. We detail the data preparation process as well as the model development that will be illustrative for many similar time series global regression problems in space weather and across domains. Our ML improvements are three-fold: 1) loss function engineering; 2) multi-task learning; and 3) transforming the task from time series prediction to spatio-temporal prediction. Notably, the ML models improve prediction of the extreme events, historically obstinate to accurate specification and indicate that increased expressive capacity provided by ML innovation can address grand challenges in the science of space weather. △ Less","29 November, 2021",https://arxiv.org/pdf/2111.14998
An Overview of Healthcare Data Analytics With Applications to the COVID-19 Pandemic,Zhe Fei;Yevgen Ryeznik;Oleksandr Sverdlov;Chee Wei Tan;Weng Kee Wong,"In the era of big data, standard analysis tools may be inadequate for making inference and there is a growing need for more efficient and innovative ways to collect, process, analyze and interpret the massive and complex data. We provide an overview of challenges in big data problems and describe how innovative analytical methods, machine learning tools and metaheuristics can tackle general healthcare problems with a focus on the current pandemic. In particular, we give applications of modern digital technology, statistical methods, data platforms and data integration systems to improve diagnosis and treatment of diseases in clinical research and novel epidemiologic tools to tackle infection source problems, such as finding Patient Zero in the spread of epidemics. We make the case that analyzing and interpreting big data is a very challenging task that requires a multi-disciplinary effort to continuously create more effective methodologies and powerful tools to transfer data information into knowledge that enables informed decision making. △ Less","25 November, 2021",https://arxiv.org/pdf/2111.14623
CoNIC: Colon Nuclei Identification and Counting Challenge 2022,Simon Graham;Mostafa Jahanifar;Quoc Dang Vu;Giorgos Hadjigeorghiou;Thomas Leech;David Snead;Shan E Ahmed Raza;Fayyaz Minhas;Nasir Rajpoot,"Nuclear segmentation, classification and quantification within Haematoxylin & Eosin stained histology images enables the extraction of interpretable cell-based features that can be used in downstream explainable models in computational pathology (CPath). However, automatic recognition of different nuclei is faced with a major challenge in that there are several different types of nuclei, some of them exhibiting large intra-class variability. To help drive forward research and innovation for automatic nuclei recognition in CPath, we organise the Colon Nuclei Identification and Counting (CoNIC) Challenge. The challenge encourages researchers to develop algorithms that perform segmentation, classification and counting of nuclei within the current largest known publicly available nuclei-level dataset in CPath, containing around half a million labelled nuclei. Therefore, the CoNIC challenge utilises over 10 times the number of nuclei as the previous largest challenge dataset for nuclei recognition. It is important for algorithms to be robust to input variation if we wish to deploy them in a clinical setting. Therefore, as part of this challenge we will also test the sensitivity of each submitted algorithm to certain input variations. △ Less","29 November, 2021",https://arxiv.org/pdf/2111.14485
Automated Detection of Patients in Hospital Video Recordings,Siddharth Sharma;Florian Dubost;Christopher Lee-Messer;Daniel Rubin,"In a clinical setting, epilepsy patients are monitored via video electroencephalogram (EEG) tests. A video EEG records what the patient experiences on videotape while an EEG device records their brainwaves. Currently, there are no existing automated methods for tracking the patient's location during a seizure, and video recordings of hospital patients are substantially different from publicly available video benchmark datasets. For example, the camera angle can be unusual, and patients can be partially covered with bedding sheets and electrode sets. Being able to track a patient in real-time with video EEG would be a promising innovation towards improving the quality of healthcare. Specifically, an automated patient detection system could supplement clinical oversight and reduce the resource-intensive efforts of nurses and doctors who need to continuously monitor patients. We evaluate an ImageNet pre-trained Mask R-CNN, a standard deep learning model for object detection, on the task of patient detection using our own curated dataset of 45 videos of hospital patients. The dataset was aggregated and curated for this work. We show that without fine-tuning, ImageNet pre-trained Mask R-CNN models perform poorly on such data. By fine-tuning the models with a subset of our dataset, we observe a substantial improvement in patient detection performance, with a mean average precision of 0.64. We show that the results vary substantially depending on the video clip. △ Less","28 November, 2021",https://arxiv.org/pdf/2111.14270
How Can Applications of Blockchain and Artificial Intelligence Improve Performance of Internet of Things? -- A Survey,Priyanka Bothra;Raja Karmakar;Sanjukta Bhattacharya;Sayantani De,"In the era of the Internet of Things (IoT), massive computing devices surrounding us operate and interact with each other to provide several significant services in industries, medical as well as in daily life activities at home, office, education sectors, and so on. The participating devices in an IoT network usually have resource constraints and the devices are prone to different cyber attacks, leading to the loopholes in the security and authentication. As a revolutionized and innovated technology, blockchain, that is applied in cryptocurrency, market prediction, etc., uses a distributed ledger that records transactions securely and efficiently. To utilize the great potential of blockchain, both industries and academia have paid a significant attention to integrate it with the IoT, as reported by several existing literature. On the other hand, Artificial Intelligence (AI) is able to embed intelligence in a system, and thus the AI can be integrated with IoT devices in order to automatically cope with different environments according to the demands. Furthermore, both blockchain and AI can be integrated with the IoT to design an automated secure and robust IoT model, as mentioned by numerous existing works. In this survey, we present a discussion on the IoT, blockchain, and AI, along with the descriptions of several research works that apply blockchain and AI in the IoT. In this direction, we point out strengths and limitations of the related existing researches. We also discuss different open challenges to exploit the full capacities of blockchain and AI in designing an IoT-based model. Therefore, the highlighted challenging issues can open the door for the development of future IoT models which will be intelligent and secure based on the integration of blockchain and AI with the IoT. △ Less","27 November, 2021",https://arxiv.org/pdf/2111.14018
A Trust and Reputation System for IoT Exploiting Distributed Ledger Technology,Seyed Amid Moeinzadeh Mirhosseini;Ali Fanian;T. Aaron Gulliver,"The advent of Bitcoin, and consequently Blockchain, has ushered in a new era of decentralization. Blockchain enables mutually distrusting entities to work collaboratively to attain a common objective. However, current Blockchain technologies lack scalability, which limits their use in Internet of Things (IoT) applications. Many devices on the Internet have the computational and communication capabilities to facilitate decision-making. These devices will soon be a 50 billion node network. Furthermore, new IoT business models such as Sensor-as-a-Service (SaaS) require a robust Trust and Reputation System (TRS). In this paper, we introduce an innovative distributed ledger combining Tangle and Blockchain as a TRS framework for IoT. The combination of Tangle and Blockchain provides maintainability of the former and scalability of the latter. The proposed ledger can handle large numbers of IoT device transactions and facilitates low power nodes joining and contributing. Employing a distributed ledger mitigates many threats, such as whitewashing attacks. Along with combining payments and rating protocols, the proposed approach provides cleaner data to the upper layer reputation algorithm. △ Less","26 November, 2021",https://arxiv.org/pdf/2111.13500
Unbiased Pairwise Learning to Rank in Recommender Systems,Yi Ren;Hongyan Tang;Siwen Zhu,"Nowadays, recommender systems already impact almost every facet of peoples lives. To provide personalized high quality recommendation results, conventional systems usually train pointwise rankers to predict the absolute value of objectives and leverage a distinct shallow tower to estimate and alleviate the impact of position bias. However, with such a training paradigm, the optimization target differs a lot from the ranking metrics valuing the relative order of top ranked items rather than the prediction precision of each item. Moreover, as the existing system tends to recommend more relevant items at higher positions, it is difficult for the shallow tower based methods to precisely attribute the user feedback to the impact of position or relevance. Therefore, there exists an exciting opportunity for us to get enhanced performance if we manage to solve the aforementioned issues. Unbiased learning to rank algorithms, which are verified to model the relative relevance accurately based on noisy feedback, are appealing candidates and have already been applied in many applications with single categorical labels, such as user click signals. Nevertheless, the existing unbiased LTR methods cannot properly handle multiple feedback incorporating both categorical and continuous labels. Accordingly, we design a novel unbiased LTR algorithm to tackle the challenges, which innovatively models position bias in the pairwise fashion and introduces the pairwise trust bias to separate the position bias, trust bias, and user relevance explicitly. Experiment results on public benchmark datasets and internal live traffic show the superior results of the proposed method for both categorical and continuous labels. △ Less","29 November, 2021",https://arxiv.org/pdf/2111.12929
SM3D: Simultaneous Monocular Mapping and 3D Detection,Runfa Li;Truong Nguyen,"Mapping and 3D detection are two major issues in vision-based robotics, and self-driving. While previous works only focus on each task separately, we present an innovative and efficient multi-task deep learning framework (SM3D) for Simultaneous Mapping and 3D Detection by bridging the gap with robust depth estimation and ""Pseudo-LiDAR"" point cloud for the first time. The Mapping module takes consecutive monocular frames to generate depth and pose estimation. In 3D Detection module, the depth estimation is projected into 3D space to generate ""Pseudo-LiDAR"" point cloud, where LiDAR-based 3D detector can be leveraged on point cloud for vehicular 3D detection and localization. By end-to-end training of both modules, the proposed mapping and 3D detection method outperforms the state-of-the-art baseline by 10.0% and 13.2% in accuracy, respectively. While achieving better accuracy, our monocular multi-task SM3D is more than 2 times faster than pure stereo 3D detector, and 18.3% faster than using two modules separately. △ Less","24 November, 2021",https://arxiv.org/pdf/2111.12643
Deep metric learning improves lab of origin prediction of genetically engineered plasmids,Igor M. Soares;Fernando H. F. Camargo;Adriano Marques;Oliver M. Crook,"Genome engineering is undergoing unprecedented development and is now becoming widely available. To ensure responsible biotechnology innovation and to reduce misuse of engineered DNA sequences, it is vital to develop tools to identify the lab-of-origin of engineered plasmids. Genetic engineering attribution (GEA), the ability to make sequence-lab associations, would support forensic experts in this process. Here, we propose a method, based on metric learning, that ranks the most likely labs-of-origin whilst simultaneously generating embeddings for plasmid sequences and labs. These embeddings can be used to perform various downstream tasks, such as clustering DNA sequences and labs, as well as using them as features in machine learning models. Our approach employs a circular shift augmentation approach and is able to correctly rank the lab-of-origin 90\% of the time within its top 10 predictions - outperforming all current state-of-the-art approaches. We also demonstrate that we can perform few-shot-learning and obtain 76\% top-10 accuracy using only 10\% of the sequences. This means, we outperform the previous CNN approach using only one-tenth of the data. We also demonstrate that we are able to extract key signatures in plasmid sequences for particular labs, allowing for an interpretable examination of the model's outputs. △ Less","24 November, 2021",https://arxiv.org/pdf/2111.12606
CircuitFlow: A Domain Specific Language for Dataflow Programming (with appendices),Riley Evans;Samantha Frohlich;Meng Wang,"Dataflow applications, such as machine learning algorithms, can run for days, making it desirable to have assurances that they will work correctly. Current tools are not good enough: too often the interactions between tasks are not type-safe, leading to undesirable run-time errors. This paper presents a new declarative Haskell Embedded DSL (eDSL) for dataflow programming: CircuitFlow. Defined as a Symmetric Monoidal Preorder (SMP) on data that models dependencies in the workflow, it has a strong mathematical basis, refocusing on how data flows through an application, resulting in a more expressive solution that not only catches errors statically, but also achieves competitive run-time performance. In our preliminary evaluation, CircuitFlow outperforms the industry-leading Luigi library of Spotify by scaling better with the number of inputs. The innovative creation of CircuitFlow is also of note, exemplifying how to create a modular eDSL whose semantics necessitates effects, and where storing complex type information for program correctness is paramount. △ Less","24 November, 2021",https://arxiv.org/pdf/2111.12420
"Real-time intelligent big data processing: technology, platform, and applications",Tongya Zheng;Gang Chen;Xinyu Wang;Chun Chen;Xingen Wang;Sihui Luo,"Human beings keep exploring the physical space using information means. Only recently, with the rapid development of information technologies and the increasing accumulation of data, human beings can learn more about the unknown world with data-driven methods. Given data timeliness, there is a growing awareness of the importance of real-time data. There are two categories of technologies accounting for data processing: batching big data and streaming processing, which have not been integrated well. Thus, we propose an innovative incremental processing technology named after Stream Cube to process both big data and stream data. Also, we implement a real-time intelligent data processing system, which is based on real-time acquisition, real-time processing, real-time analysis, and real-time decision-making. The real-time intelligent data processing technology system is equipped with a batching big data platform, data analysis tools, and machine learning models. Based on our applications and analysis, the real-time intelligent data processing system is a crucial solution to the problems of the national society and economy. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.11872
The Ethics of Biosurveillance,S. K. Devitt;P. W. J. Baxter;G. Hamilton,"Governments must keep agricultural systems free of pests that threaten agricultural production and international trade. Biosecurity surveillance already makes use of a wide range of technologies, such as insect traps and lures, geographic information systems, and diagnostic biochemical tests. The rise of cheap and usable surveillance technologies such as remotely piloted aircraft systems (RPAS) presents value conflicts not addressed in international biosurveillance guidelines. The costs of keeping agriculture pest-free include privacy violations and reduced autonomy for farmers. We argue that physical and digital privacy in the age of ubiquitous aerial and ground surveillance is a natural right to allow people to function freely on their land. Surveillance methods must be co-created and justified through using ethically defensible processes such as discourse theory, value-centred design and responsible innovation to forge a cooperative social contract between diverse stakeholders. We propose an ethical framework for biosurveillance activities that balances the collective benefits for food security with individual privacy: (1) establish the boundaries of a biosurveillance social contract; (2) justify surveillance operations for the farmers, researchers, industry, the public and regulators; (3) give decision makers a reasonable measure of control over their personal and agricultural data; and (4) choose surveillance methodologies that give the appropriate information. The benefits of incorporating an ethical framework for responsible biosurveillance innovation include increased participation and accumulated trust over time. Long term trust and cooperation will support food security, producing higher quality data overall and mitigating against anticipated information gaps that may emerge due to disrespecting landholder rights △ Less","23 November, 2021",https://arxiv.org/pdf/2111.11712
Learning Explicit User Interest Boundary for Recommendation,Jianhuan Zhuo;Qiannan Zhu;Yinliang Yue;Yuhong Zhao,"The core objective of modelling recommender systems from implicit feedback is to maximize the positive sample score s_p and minimize the negative sample score s_n, which can usually be summarized into two paradigms: the pointwise and the pairwise. The pointwise approaches fit each sample with its label individually, which is flexible in weighting and sampling on instance-level but ignores the inherent ranking property. By qualitatively minimizing the relative score s_n - s_p, the pairwise approaches capture the ranking of samples naturally but suffer from training efficiency. Additionally, both approaches are hard to explicitly provide a personalized decision boundary to determine if users are interested in items unseen. To address those issues, we innovatively introduce an auxiliary score b_u for each user to represent the User Interest Boundary(UIB) and individually penalize samples that cross the boundary with pairwise paradigms, i.e., the positive samples whose score is lower than b_u and the negative samples whose score is higher than b_u. In this way, our approach successfully achieves a hybrid loss of the pointwise and the pairwise to combine the advantages of both. Analytically, we show that our approach can provide a personalized decision boundary and significantly improve the training efficiency without any special sampling strategy. Extensive results show that our approach achieves significant improvements on not only the classical pointwise or pairwise models but also state-of-the-art models with complex loss function and complicated feature encoding. △ Less","22 November, 2021",https://arxiv.org/pdf/2111.11026
One-shot Weakly-Supervised Segmentation in Medical Images,Wenhui Lei;Qi Su;Ran Gu;Na Wang;Xinglong Liu;Guotai Wang;Xiaofan Zhang;Shaoting Zhang,"Deep neural networks usually require accurate and a large number of annotations to achieve outstanding performance in medical image segmentation. One-shot segmentation and weakly-supervised learning are promising research directions that lower labeling effort by learning a new class from only one annotated image and utilizing coarse labels instead, respectively. Previous works usually fail to leverage the anatomical structure and suffer from class imbalance and low contrast problems. Hence, we present an innovative framework for 3D medical image segmentation with one-shot and weakly-supervised settings. Firstly a propagation-reconstruction network is proposed to project scribbles from annotated volume to unlabeled 3D images based on the assumption that anatomical patterns in different human bodies are similar. Then a dual-level feature denoising module is designed to refine the scribbles based on anatomical- and pixel-level features. After expanding the scribbles to pseudo masks, we could train a segmentation model for the new class with the noisy label training strategy. Experiments on one abdomen and one head-and-neck CT dataset show the proposed method obtains significant improvement over the state-of-the-art methods and performs robustly even under severe class imbalance and low contrast. △ Less","21 November, 2021",https://arxiv.org/pdf/2111.10773
Towards Scalable Unpaired Virtual Try-On via Patch-Routed Spatially-Adaptive GAN,Zhenyu Xie;Zaiyu Huang;Fuwei Zhao;Haoye Dong;Michael Kampffmeyer;Xiaodan Liang,"Image-based virtual try-on is one of the most promising applications of human-centric image generation due to its tremendous real-world potential. Yet, as most try-on approaches fit in-shop garments onto a target person, they require the laborious and restrictive construction of a paired training dataset, severely limiting their scalability. While a few recent works attempt to transfer garments directly from one person to another, alleviating the need to collect paired datasets, their performance is impacted by the lack of paired (supervised) information. In particular, disentangling style and spatial information of the garment becomes a challenge, which existing methods either address by requiring auxiliary data or extensive online optimization procedures, thereby still inhibiting their scalability. To achieve a \emph{scalable} virtual try-on system that can transfer arbitrary garments between a source and a target person in an unsupervised manner, we thus propose a texture-preserving end-to-end network, the PAtch-routed SpaTially-Adaptive GAN (PASTA-GAN), that facilitates real-world unpaired virtual try-on. Specifically, to disentangle the style and spatial information of each garment, PASTA-GAN consists of an innovative patch-routed disentanglement module for successfully retaining garment texture and shape characteristics. Guided by the source person keypoints, the patch-routed disentanglement module first decouples garments into normalized patches, thus eliminating the inherent spatial information of the garment, and then reconstructs the normalized patches to the warped garment complying with the target person pose. Given the warped garment, PASTA-GAN further introduces novel spatially-adaptive residual blocks that guide the generator to synthesize more realistic garment details. △ Less","20 November, 2021",https://arxiv.org/pdf/2111.10544
A privacy-aware zero interaction smart mobility system,Stefano Righini;Luca Calderoni;Dario Maio,"Smart cities often rely on technological innovation to improve citizens' safety and quality of life. This paper presents a novel smart mobility system that aims to facilitate people accessing public mobility while preserving their privacy. The system is based on a zero interaction approach whereby a person can use public transport services without any need to perform explicit actions. Operations related to ticket purchase and validation have been fully automated. The system is also designed with the privacy-by-design paradigm in mind, to preserve user privacy as much as possible. Throughout the paper several technical details are discussed as well to describe a prototype version of the system that was implemented. The prototype has been successfully tested in the city of Imola (Emilia Romagna, Italy) in order to prove the system validity on the field. △ Less","19 November, 2021",https://arxiv.org/pdf/2111.10307
Attention based end to end Speech Recognition for Voice Search in Hindi and English,Raviraj Joshi;Venkateshan Kannan,"We describe here our work with automatic speech recognition (ASR) in the context of voice search functionality on the Flipkart e-Commerce platform. Starting with the deep learning architecture of Listen-Attend-Spell (LAS), we build upon and expand the model design and attention mechanisms to incorporate innovative approaches including multi-objective training, multi-pass training, and external rescoring using language models and phoneme based losses. We report a relative WER improvement of 15.7% on top of state-of-the-art LAS models using these modifications. Overall, we report an improvement of 36.9% over the phoneme-CTC system. The paper also provides an overview of different components that can be tuned in a LAS-based system. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.10208
A big data intelligence marketplace and secure analytics experimentation platform for the aviation industry,Dimitrios Miltiadou;Stamatis Pitsios;Dimitrios Spyropoulos;Dimitrios Alexandrou;Fenareti Lampathaki;Domenico Messina;Konstantinos Perakis,"The unprecedented volume, diversity and richness of aviation data that can be acquired, generated, stored, and managed provides unique capabilities for the aviation-related industries and pertains value that remains to be unlocked with the adoption of the innovative Big Data Analytics technologies. Despite the large efforts and investments on research and innovation, the Big Data technologies introduce a number of challenges to its adopters. Besides the effective storage and access to the underlying big data, efficient data integration and data interoperability should be considered, while at the same time multiple data sources should be effectively combined by performing data exchange and data sharing between the different stakeholders. However, this reveals additional challenges for the crucial preservation of the information security of the collected data, the trusted and secure data exchange and data sharing, as well as the robust data access control. The current paper aims to introduce the ICARUS big data-enabled platform that aims provide a multi-sided platform that offers a novel aviation data and intelligence marketplace accompanied by a trusted and secure analytics workspace. It holistically handles the complete big data lifecycle from the data collection, data curation and data exploration to the data integration and data analysis of data originating from heterogeneous data sources with different velocity, variety and volume in a trusted and secure manner. △ Less","18 November, 2021",https://arxiv.org/pdf/2111.09872
A Secure Experimentation Sandbox for the design and execution of trusted and secure analytics in the aviation domain,Dimitrios Miltiadou;Stamatis Pitsios;Dimitrios Spyropoulos;Dimitrios Alexandrou;Fenareti Lampathaki;Domenico Messina;Konstantinos Perakis,"The aviation industry as well as the industries that benefit and are linked to it are ripe for innovation in the form of Big Data analytics. The number of available big data technologies is constantly growing, while at the same time the existing ones are rapidly evolving and empowered with new features. However, the Big Data era imposes the crucial challenge of how to effectively handle information security while managing massive and rapidly evolving data from heterogeneous data sources. While multiple technologies have emerged, there is a need to find a balance between multiple security requirements, privacy obligations, system performance and rapid dynamic analysis on large datasets. The current paper aims to introduce the ICARUS Secure Experimentation Sandbox of the ICARUS platform. The ICARUS platform aims to provide a big data-enabled platform that aspires to become an 'one-stop shop' for aviation data and intelligence marketplace that provides a trusted and secure 'sandboxed' analytics workspace, allowing the exploration, integration and deep analysis of original and derivative data in a trusted and fair manner. Towards this end, a Secure Experimentation Sandbox has been designed and integrated in the ICARUS platform offering, that enables the provisioning of a sophisticated environment that can completely guarantee the safety and confidentiality of data, allowing to any interested party to utilise the platform to conduct analytical experiments in closed-lab conditions. △ Less","18 November, 2021",https://arxiv.org/pdf/2111.09863
Estimation of the thermal properties of an historic building wall by combining Modal Identification Method and Optimal Experiment Design,Julien Berger;Benjamin Kadoch,"The estimation of wall thermal properties by \emph{in situ} measurement enables to increase the reliability of the model predictions for building energy efficiency. Nevertheless, retrieving the unknown parameters has an important computational cost. Indeed, several computations of the heat transfer problem are required to identify these thermal properties. To handle this drawback, an innovative approach is investigated. The first step is to search the optimal experiment design among the sequence of observation of several months. A reduced sequence of observations of three days is identified which guarantees to estimate the parameter with the maximum accuracy. Moreover, the inverse problem is only solved for this short sequence. To decrease further the computational efforts, a reduced order model based on the modal identification method is employed. This \emph{a posteriori} model reduction method approximates the solution with a lower degree of freedom. The whole methodology is illustrated to estimate the thermal diffusivity of an historical building that has been monitored with temperature sensors for several months. The computational efforts is cut by five. The estimated parameter improves the reliability of the predictions of the wall thermal efficiency. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09175
Average reduced model to simulate solutions for heat and mass transfer through porous material,Julien Berger;Madina Abdykarim,"The design of numerical tools to model the behavior of building materials is a challenging task. The crucial point is to save computational cost and maintain high accuracy of predictions. There are two main limitations on the time scale choice, which put an obstacle to solve the above issues. First one is the numerical restriction. A number of research is dedicated to overcome this limitation and it is shown that it can be relaxed with innovative numerical schemes. The second one is the physical restriction. It is imposed by properties of a material, phenomena itself and corresponding boundary conditions. This work is focused on the study of a methodology that enables to overcome the physical restriction on the time grid. So-called Average Reduced Model (ARM) is suggested. It is based on smoothing the time-dependent boundary conditions. Besides, the approximate solution is decomposed into average and fluctuating components. The primer is obtained by integrating the equations over time, whereas the latter is an user-defined empirical model. The methodology is investigated for both heat diffusion and coupled heat and mass transfer. It is demonstrated that the signal core of the boundary conditions is preserved and the physical restriction can be relaxed. The model proved to be reliable, accurate and efficient also in comparison with the experimental data of two years. The implementation of the scarce time-step of 1 \, \sf{h} is justified. It is shown, that by maintaining the tolerable error it is possible to cut computational effort up to almost four times in comparison with the complete model with the same time grid. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09041
Multi-Attribute Relation Extraction (MARE) -- Simplifying the Application of Relation Extraction,Lars Klöser;Philipp Kohl;Bodo Kraft;Albert Zündorf,"Natural language understanding's relation extraction makes innovative and encouraging novel business concepts possible and facilitates new digitilized decision-making processes. Current approaches allow the extraction of relations with a fixed number of entities as attributes. Extracting relations with an arbitrary amount of attributes requires complex systems and costly relation-trigger annotations to assist these systems. We introduce multi-attribute relation extraction (MARE) as an assumption-less problem formulation with two approaches, facilitating an explicit mapping from business use cases to the data annotations. Avoiding elaborated annotation constraints simplifies the application of relation extraction approaches. The evaluation compares our models to current state-of-the-art event extraction and binary relation extraction methods. Our approaches show improvement compared to these on the extraction of general multi-attribute relations. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09035
GAP Enhancing Semantic Interoperability of Genomic Datasets and Provenance Through Nanopublications,Matheus Feijoó;Rodrigo Jardim;Sergio Serra;Maria Luiza Campos,"While the publication of datasets in scientific repositories has become broadly recognised, the repositories tend to have increasing semantic-related problems. For instance, they present various data reuse obstacles for machine-actionable processes, especially in biological repositories, hampering the reproducibility of scientific experiments. An example of these shortcomings is the GenBank database. We propose GAP, an innovative data model to enhance the semantic data meaning to address these issues. The model focuses on converging related approaches like data provenance, semantic interoperability, FAIR principles, and nanopublications. Our experiments include a prototype to scrape genomic data and trace them to nanopublications as a proof of concept. For this, (meta)data are stored in a three-level nanopub data model. The first level is related to a target organism, specifying data in terms of biological taxonomy. The second level focuses on the biological strains of the target, the central part of our contribution. The strains express information related to deciphered (meta)data of the genetic variations of the genomic material. The third level stores related scientific papers (meta)data. We expect it will offer higher data storage flexibility and more extensive interoperability with other data sources by incorporating and adopting associated approaches to store genomic data in the proposed model. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.08739
Generative Pre-Trained Transformer for Design Concept Generation: An Exploration,Qihao Zhu;Jianxi Luo,"Novel concepts are essential for design innovation and can be generated with the aid of data stimuli and computers. However, current generative design algorithms focus on diagrammatic or spatial concepts that are either too abstract to understand or too detailed for early phase design exploration. This paper explores the uses of generative pre-trained transformers (GPT) for natural language design concept generation. Our experiments involve the use of GPT-2 and GPT-3 for different creative reasonings in design tasks. Both show reasonably good performance for verbal design concept generation. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08489
An Empirical Study of Finding Similar Exercises,Tongwen Huang;Xihua Li,"Education artificial intelligence aims to profit tasks in the education domain such as intelligent test paper generation and consolidation exercises where the main technique behind is how to match the exercises, known as the finding similar exercises(FSE) problem. Most of these approaches emphasized their model abilities to represent the exercise, unfortunately there are still many challenges such as the scarcity of data, insufficient understanding of exercises and high label noises. We release a Chinese education pre-trained language model BERT_{Edu} for the label-scarce dataset and introduce the exercise normalization to overcome the diversity of mathematical formulas and terms in exercise. We discover new auxiliary tasks in an innovative way depends on problem-solving ideas and propose a very effective MoE enhanced multi-task model for FSE task to attain better understanding of exercises. In addition, confidence learning was utilized to prune train-set and overcome high noises in labeling data. Experiments show that these methods proposed in this paper are very effective. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08322
Improving Learning from Demonstrations by Learning from Experience,Haofeng Liu;Yiwen Chen;Jiayi Tan;Marcelo H Ang Jr,"How to make imitation learning more general when demonstrations are relatively limited has been a persistent problem in reinforcement learning (RL). Poor demonstrations lead to narrow and biased date distribution, non-Markovian human expert demonstration makes it difficult for the agent to learn, and over-reliance on sub-optimal trajectories can make it hard for the agent to improve its performance. To solve these problems we propose a new algorithm named TD3fG that can smoothly transition from learning from experts to learning from experience. Our algorithm achieves good performance in the MUJOCO environment with limited and sub-optimal demonstrations. We use behavior cloning to train the network as a reference action generator and utilize it in terms of both loss function and exploration noise. This innovation can help agents extract a priori knowledge from demonstrations while reducing the detrimental effects of the poor Markovian properties of the demonstrations. It has a better performance compared to the BC+ fine-tuning and DDPGfD approach, especially when the demonstrations are relatively limited. We call our method TD3fG meaning TD3 from a generator. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.08156
The ubiquitous efficiency of going further: how street networks affect travel speed,Gabriel L. Maia;Caio Ponte;Carlos Caminha;Lara Furtado;Hygor P. M. Melo;Vasco Furtado,"As cities struggle to adapt to more ``people-centered'' urbanism, transportation planning and engineering must innovate to expand the street network strategically in order to ensure efficiency but also to deter sprawl. Here, we conducted a study of over 200 cities around the world to understand the impact that the patterns of deceleration points in streets due to traffic signs has in trajectories done from motorized vehicles. We demonstrate that there is a ubiquitous nonlinear relationship between time and distance in the optimal trajectories within each city. More precisely, given a specific period of time τ, without any traffic, one can move on average up to the distance \left \langle D \right \rangle \simτ^β. We found a super-linear relationship for almost all cities in which β>1.0. This points to an efficiency of scale when traveling large distances, meaning the average speed will be higher for longer trips when compared to shorter trips. We demonstrate that this efficiency is a consequence of the spatial distribution of large segments of streets without deceleration points, favoring access to routes in which a vehicle can cross large distances without stops. These findings show that cities must consider how their street morphology can affect travel speed. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.07801
Read-and-Run Constrained Coding for Modern Flash Devices,Ahmed Hareedy;Simeng Zheng;Paul Siegel;Robert Calderbank,"The pivotal storage density win achieved by solid-state devices over magnetic devices in 2015 is a result of multiple innovations in physics, architecture, and signal processing. One of the most important innovations in that regard is enabling the storage of more than one bit per cell in the Flash device, i.e., having more than two charge levels per cell. Constrained coding is used in Flash devices to increase reliability via mitigating inter-cell interference that stems from charge propagation among cells. Recently, capacity-achieving constrained codes were introduced to serve that purpose in modern Flash devices, which have more than two levels per cell. While these codes result in minimal redundancy via exploiting the underlying physics, they result in non-negligible complexity increase and access speed limitation since pages cannot be read separately. In this paper, we suggest new constrained coding schemes that have low-complexity and preserve the desirable high access speed in modern Flash devices. The idea is to eliminate error-prone patterns by coding data only on the left-most page while leaving data on all the remaining pages uncoded. Our coding schemes work for any number of levels per cell, offer systematic encoding and decoding, and are capacity-approaching. Since the proposed schemes enable the separation of pages, we refer to them as read-and-run (RR) constrained coding schemes as opposed to schemes adopting read-and-wait for other pages. We analyze the new RR coding schemes and discuss their impact on the probability of occurrence of different charge levels. We also demonstrate the performance improvement achieved via RR coding on a practical triple-level cell Flash device. △ Less","14 November, 2021",https://arxiv.org/pdf/2111.07415
Identification and Adaptive Control of Markov Jump Systems: Sample Complexity and Regret Bounds,Yahya Sattar;Zhe Du;Davoud Ataee Tarzanagh;Laura Balzano;Necmiye Ozay;Samet Oymak,"Learning how to effectively control unknown dynamical systems is crucial for intelligent autonomous systems. This task becomes a significant challenge when the underlying dynamics are changing with time. Motivated by this challenge, this paper considers the problem of controlling an unknown Markov jump linear system (MJS) to optimize a quadratic objective. By taking a model-based perspective, we consider identification-based adaptive control for MJSs. We first provide a system identification algorithm for MJS to learn the dynamics in each mode as well as the Markov transition matrix, underlying the evolution of the mode switches, from a single trajectory of the system states, inputs, and modes. Through mixing-time arguments, sample complexity of this algorithm is shown to be \mathcal{O}(1/\sqrt{T}). We then propose an adaptive control scheme that performs system identification together with certainty equivalent control to adapt the controllers in an episodic fashion. Combining our sample complexity results with recent perturbation results for certainty equivalent control, we prove that when the episode lengths are appropriately chosen, the proposed adaptive control scheme achieves \mathcal{O}(\sqrt{T}) regret, which can be improved to \mathcal{O}(polylog(T)) with partial knowledge of the system. Our proof strategy introduces innovations to handle Markovian jumps and a weaker notion of stability common in MJSs. Our analysis provides insights into system theoretic quantities that affect learning accuracy and control performance. Numerical simulations are presented to further reinforce these insights. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.07018
Adversarially Robust Learning for Security-Constrained Optimal Power Flow,Priya L. Donti;Aayushya Agarwal;Neeraj Vijay Bedmutha;Larry Pileggi;J. Zico Kolter,"In recent years, the ML community has seen surges of interest in both adversarially robust learning and implicit layers, but connections between these two areas have seldom been explored. In this work, we combine innovations from these areas to tackle the problem of N-k security-constrained optimal power flow (SCOPF). N-k SCOPF is a core problem for the operation of electrical grids, and aims to schedule power generation in a manner that is robust to potentially k simultaneous equipment outages. Inspired by methods in adversarially robust training, we frame N-k SCOPF as a minimax optimization problem - viewing power generation settings as adjustable parameters and equipment outages as (adversarial) attacks - and solve this problem via gradient-based techniques. The loss function of this minimax problem involves resolving implicit equations representing grid physics and operational decisions, which we differentiate through via the implicit function theorem. We demonstrate the efficacy of our framework in solving N-3 SCOPF, which has traditionally been considered as prohibitively expensive to solve given that the problem size depends combinatorially on the number of potential outages. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06961
NRC-GAMMA: Introducing a Novel Large Gas Meter Image Dataset,Ashkan Ebadi;Patrick Paul;Sofia Auer;Stéphane Tremblay,"Automatic meter reading technology is not yet widespread. Gas, electricity, or water accumulation meters reading is mostly done manually on-site either by an operator or by the homeowner. In some countries, the operator takes a picture as reading proof to confirm the reading by checking offline with another operator and/or using it as evidence in case of conflicts or complaints. The whole process is time-consuming, expensive, and prone to errors. Automation can optimize and facilitate such labor-intensive and human error-prone processes. With the recent advances in the fields of artificial intelligence and computer vision, automatic meter reading systems are becoming more viable than ever. Motivated by the recent advances in the field of artificial intelligence and inspired by open-source open-access initiatives in the research community, we introduce a novel large benchmark dataset of real-life gas meter images, named the NRC-GAMMA dataset. The data were collected from an Itron 400A diaphragm gas meter on January 20, 2020, between 00:05 am and 11:59 pm. We employed a systematic approach to label the images, validate the labellings, and assure the quality of the annotations. The dataset contains 28,883 images of the entire gas meter along with 57,766 cropped images of the left and the right dial displays. We hope the NRC-GAMMA dataset helps the research community to design and implement accurate, innovative, intelligent, and reproducible automatic gas meter reading solutions. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06827
Human-Centric Decision Support Tools: Insights from Real-World Design and Implementation,Narges Ahani;Andrew C. Trapp,"Decision support tools enable improved decision-making for challenging decision problems by empowering stakeholders to process, analyze, visualize, and otherwise make sense of a variety of key factors. Their intentional design is a critical component of the value they create. All decision-support tools share in common that there is a complex decision problem to be solved for which decision-support is useful, and moreover, that appropriate analytics expertise is available to produce solutions to the problem setting at hand. When well-designed, decision support tools reduce friction and increase efficiency in providing support for the decision-making process, thereby improving the ability of decision-makers to make quality decisions. On the other hand, the presence of overwhelming, superfluous, insufficient, or ill-fitting information and software features can have an adverse effect on the decision-making process and, consequently, outcomes. We advocate for an innovative, and perhaps overlooked, approach to designing effective decision support tools: genuinely listening to the project stakeholders, to ascertain and appreciate their real needs and perspectives. By prioritizing stakeholder needs, a foundation of mutual trust and understanding is established with the design team. We maintain this trust is critical to eventual tool acceptance and adoption, and its absence jeopardizes the future use of the tool, which would leave its analytical insights for naught. We discuss examples across multiple contexts to underscore our collective experience, highlight lessons learned, and present recommended practices to improve the design and eventual adoption of decision dupport tools. △ Less","11 November, 2021",https://arxiv.org/pdf/2111.05796
Design Theory to improve health evidence retrieval,Alvet Miranda;Shah Jahan Miah,"Objective: Our study objective is to design a feasible technology solution for health organizations to remove barriers to evidence-based clinical information retrieval, and improve Evidence-Based Practice. Methods: Literature from 2010 to 2020 was reviewed to define problems in evidence-based clinical information retrieval with recommendations from literature used to define solution objectives. Design Science Research is used to complete three projects in a research stream using cloud services such as Web-Scale Discovery, Content Management System, Federated Access, Global Knowledgebase, and Document Delivery. Design thinking, systems thinking, and user-oriented theory of information need are adopted to construct a design theory. Results: The research stream produced three novel and innovative artefacts: a contextual model, a unified architecture, and a context-aware unified architecture which we evaluate as part of academic reviews, scholarly publications, and conference proceedings in various research stream stages. A fourth artefact or design theory is presented to generalize results as mature knowledge. △ Less","10 November, 2021",https://arxiv.org/pdf/2111.05587
FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance,Zechu Li;Xiao-Yang Liu;Jiahao Zheng;Zhaoran Wang;Anwar Walid;Jian Guo,"Machine learning techniques are playing more and more important roles in finance market investment. However, finance quantitative modeling with conventional supervised learning approaches has a number of limitations. The development of deep reinforcement learning techniques is partially addressing these issues. Unfortunately, the steep learning curve and the difficulty in quick modeling and agile development are impeding finance researchers from using deep reinforcement learning in quantitative trading. In this paper, we propose an RLOps in finance paradigm and present a FinRL-Podracer framework to accelerate the development pipeline of deep reinforcement learning (DRL)-driven trading strategy and to improve both trading performance and training efficiency. FinRL-Podracer is a cloud solution that features high performance and high scalability and promises continuous training, continuous integration, and continuous delivery of DRL-driven trading strategies, facilitating a rapid transformation from algorithmic innovations into a profitable trading strategy. First, we propose a generational evolution mechanism with an ensemble strategy to improve the trading performance of a DRL agent, and schedule the training of a DRL algorithm onto a GPU cloud via multi-level mapping. Then, we carry out the training of DRL components with high-performance optimizations on GPUs. Finally, we evaluate the FinRL-Podracer framework for a stock trend prediction task on an NVIDIA DGX SuperPOD cloud. FinRL-Podracer outperforms three popular DRL libraries Ray RLlib, Stable Baseline 3 and FinRL, i.e., 12% \sim 35% improvements in annual return, 0.1 \sim 0.6 improvements in Sharpe ratio and 3 times \sim 7 times speed-up in training time. We show the high scalability by training a trading agent in 10 minutes with 80 A100 GPUs, on NASDAQ-100 constituent stocks with minute-level data over 10 years. △ Less","6 November, 2021",https://arxiv.org/pdf/2111.05188
A toolkit for data-driven discovery of governing equations in high-noise regimes,Charles B. Delahunt;J. Nathan Kutz,"We consider the data-driven discovery of governing equations from time-series data in the limit of high noise. The algorithms developed describe an extensive toolkit of methods for circumventing the deleterious effects of noise in the context of the sparse identification of nonlinear dynamics (SINDy) framework. We offer two primary contributions, both focused on noisy data acquired from a system x' = f(x). First, we propose, for use in high-noise settings, an extensive toolkit of critically enabling extensions for the SINDy regression method, to progressively cull functionals from an over-complete library and yield a set of sparse equations that regress to the derivate x'. These innovations can extract sparse governing equations and coefficients from high-noise time-series data (e.g. 300% added noise). For example, it discovers the correct sparse libraries in the Lorenz system, with median coefficient estimate errors equal to 1% - 3% (for 50% noise), 6% - 8% (for 100% noise); and 23% - 25% (for 300% noise). The enabling modules in the toolkit are combined into a single method, but the individual modules can be tactically applied in other equation discovery methods (SINDy or not) to improve results on high-noise data. Second, we propose a technique, applicable to any model discovery method based on x' = f(x), to assess the accuracy of a discovered model in the context of non-unique solutions due to noisy data. Currently, this non-uniqueness can obscure a discovered model's accuracy and thus a discovery method's effectiveness. We describe a technique that uses linear dependencies among functionals to transform a discovered model into an equivalent form that is closest to the true model, enabling more accurate assessment of a discovered model's accuracy. △ Less","29 December, 2021",https://arxiv.org/pdf/2111.04870
RF-Net: a Unified Meta-learning Framework for RF-enabled One-shot Human Activity Recognition,Shuya Ding;Zhe Chen;Tianyue Zheng;Jun Luo,"Radio-Frequency (RF) based device-free Human Activity Recognition (HAR) rises as a promising solution for many applications. However, device-free (or contactless) sensing is often more sensitive to environment changes than device-based (or wearable) sensing. Also, RF datasets strictly require on-line labeling during collection, starkly different from image and text data collections where human interpretations can be leveraged to perform off-line labeling. Therefore, existing solutions to RF-HAR entail a laborious data collection process for adapting to new environments. To this end, we propose RF-Net as a meta-learning based approach to one-shot RF-HAR; it reduces the labeling efforts for environment adaptation to the minimum level. In particular, we first examine three representative RF sensing techniques and two major meta-learning approaches. The results motivate us to innovate in two designs: i) a dual-path base HAR network, where both time and frequency domains are dedicated to learning powerful RF features including spatial and attention-based temporal ones, and ii) a metric-based meta-learning framework to enhance the fast adaption capability of the base network, including an RF-specific metric module along with a residual classification module. We conduct extensive experiments based on all three RF sensing techniques in multiple real-world indoor environments; all results strongly demonstrate the efficacy of RF-Net compared with state-of-the-art baselines. △ Less","28 October, 2021",https://arxiv.org/pdf/2111.04566
Feature Concepts for Data Federative Innovations,Yukio Ohsawa;Sae Kondo;Teruaki Hayashi,"A feature concept, the essence of the data-federative innovation process, is presented as a model of the concept to be acquired from data. A feature concept may be a simple feature, such as a single variable, but is more likely to be a conceptual illustration of the abstract information to be obtained from the data. For example, trees and clusters are feature concepts for decision tree learning and clustering, respectively. Useful feature concepts for satis-fying the requirements of users of data have been elicited so far via creative communication among stakeholders in the market of data. In this short paper, such a creative communication is reviewed, showing a couple of appli-cations, for example, change explanation in markets and earthquakes, and highlight the feature concepts elicited in these cases. △ Less","5 November, 2021",https://arxiv.org/pdf/2111.04505
Internet of Things Technologies for Managing COVID-19 Pandemic: Recommendations and Proposed Framework,Navod Neranjan Thilakrathne;Rohan Samarasinghe,"The Internet of Things, often known as IoT, is an innovative technology that connects digital devices all around us, allowing Machine to Machine (M2M) communication between digital devices all over the world. Due to the convenience, connectivity, and affordability, this IoT is being served in various domains including healthcare where it brings exceptional benefits to improve patient care, uplifting medical resources to the next level. Some of these examples include surveillance networks, healthcare delivery technologies, and smart thermal detection. As of now, the IoT is served in various aspects of healthcare making many of the medical processes much easier as opposed to the earlier times. One of the most important aspects that this IoT can be used is, managing various aspects of healthcare during global pandemics, as pandemics can bring an immense strain on healthcare resources, during the pandemic. As there is no proper study is done with regards to the proper use of IoT for managing pandemics, in this regard, through our study we aim to review various use cases of IoT towards managing pandemics especially in terms of COVID-19; owing to what we are currently going through. In this regard, we are proposing a conceptual framework synthesizing the current literature and resources, which can be adopted when managing global pandemics to accelerate the battle pace with these deadly pandemics and focusing on what the entire world is currently going through where almost more than four (04) million people are diminished of this COVID-19 pandemic. △ Less","19 October, 2021",https://arxiv.org/pdf/2111.04430
Exponential Bellman Equation and Improved Regret Bounds for Risk-Sensitive Reinforcement Learning,Yingjie Fei;Zhuoran Yang;Yudong Chen;Zhaoran Wang,"We study risk-sensitive reinforcement learning (RL) based on the entropic risk measure. Although existing works have established non-asymptotic regret guarantees for this problem, they leave open an exponential gap between the upper and lower bounds. We identify the deficiencies in existing algorithms and their analysis that result in such a gap. To remedy these deficiencies, we investigate a simple transformation of the risk-sensitive Bellman equations, which we call the exponential Bellman equation. The exponential Bellman equation inspires us to develop a novel analysis of Bellman backup procedures in risk-sensitive RL algorithms, and further motivates the design of a novel exploration mechanism. We show that these analytic and algorithmic innovations together lead to improved regret upper bounds over existing ones. △ Less","6 November, 2021",https://arxiv.org/pdf/2111.03947
FAIR Metadata: A Community-driven Vocabulary Application,Christopher B. Rauch;Mat Kelly;John A. Kunze;Jane Greenberg,"FAIR metadata is critical to supporting FAIR data overall. Transparency, community engagement, and flexibility are key aspects of FAIR that apply to metadata. This paper presents YAMZ (Yet Another Metadata Zoo), a community-driven vocabulary application that supports FAIR. The history ofYAMZ and its original features are reviewed, followed by a presentation of recent innovations and a discussion of how YAMZ supports FAIR principles. The conclusion identifies next steps and key outputs. △ Less","6 November, 2021",https://arxiv.org/pdf/2111.03910
Dialogue Inspectional Summarization with Factual Inconsistency Awareness,Leilei Gan;Yating Zhang;Kun Kuang;Lin Yuan;Shuo Li;Changlong Sun;Xiaozhong Liu;Fei Wu,"Dialogue summarization has been extensively studied and applied, where the prior works mainly focused on exploring superior model structures to align the input dialogue and the output summary. However, for professional dialogues (e.g., legal debate and medical diagnosis), semantic/statistical alignment can hardly fill the logical/factual gap between input dialogue discourse and summary output with external knowledge. In this paper, we mainly investigate the factual inconsistency problem for Dialogue Inspectional Summarization (DIS) under non-pretraining and pretraining settings. An innovative end-to-end dialogue summary generation framework is proposed with two auxiliary tasks: Expectant Factual Aspect Regularization (EFAR) and Missing Factual Entity Discrimination (MFED). Comprehensive experiments demonstrate that the proposed model can generate a more readable summary with accurate coverage of factual aspects as well as informing the user with potential missing facts detected from the input dialogue for further human intervention. △ Less","5 November, 2021",https://arxiv.org/pdf/2111.03284
Long-distance Deterministic Transmission among TSN Networks: Converging CQF and DIP,Weiqian Tan;Binwei Wu,"With the development of 5G, innovative applications requiring bounded transmission delays and zero packet loss emerge, e.g., AR, industrial automation, and smart grid. In this circumstance, time-sensitive networking (TSN) is proposed, which addresses the deterministic transmission in the local area networks. Nevertheless, TSN is essentially a Layer 2 technique, which cannot provide deterministic transmission on a large geographic area. To solve this problem, this paper proposes a hierarchical network for the end-to-end deterministic transmission. In the proposed network, we leverage CQF (i.e., one of the most efficient TSN mechanisms) in the access networks which aggregates the traffic from end-devices. Meanwhile, in the core network, we exploit the DIP (i.e., a well-known deterministic networking mechanism for backbone networks) for long-distance deterministic transmission. We design the cycle alignment mechanism to enable seamless and deterministic transmission among hierarchical networks. A joint schedule is also formulated, which introduces the traffic shaping at the network edge to maximize the network throughput. Experimental simulations show that the proposed network can achieve end-to-end deterministic transmission, even in the highly-load scenarios. △ Less","4 November, 2021",https://arxiv.org/pdf/2111.03246
Fast Camouflaged Object Detection via Edge-based Reversible Re-calibration Network,Ge-Peng Ji;Lei Zhu;Mingchen Zhuge;Keren Fu,"Camouflaged Object Detection (COD) aims to detect objects with similar patterns (e.g., texture, intensity, colour, etc) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet. Our model is characterized by two innovative designs, namely Selective Edge Aggregation (SEA) and Reversible Re-calibration Unit (RRU), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNet outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNet significantly improves the performance by \sim6% (mean E-measure) with notably high speed (79.3 FPS), showing that ERRNet could be a general and robust solution for the COD task. △ Less","4 November, 2021",https://arxiv.org/pdf/2111.03216
Human-Level Control without Server-Grade Hardware,Brett Daley;Christopher Amato,"Deep Q-Network (DQN) marked a major milestone for reinforcement learning, demonstrating for the first time that human-level control policies could be learned directly from raw visual inputs via reward maximization. Even years after its introduction, DQN remains highly relevant to the research community since many of its innovations have been adopted by successor methods. Nevertheless, despite significant hardware advances in the interim, DQN's original Atari 2600 experiments remain costly to replicate in full. This poses an immense barrier to researchers who cannot afford state-of-the-art hardware or lack access to large-scale cloud computing resources. To facilitate improved access to deep reinforcement learning research, we introduce a DQN implementation that leverages a novel concurrent and synchronized execution framework designed to maximally utilize a heterogeneous CPU-GPU desktop system. With just one NVIDIA GeForce GTX 1080 GPU, our implementation reduces the training time of a 200-million-frame Atari experiment from 25 hours to just 9 hours. The ideas introduced in our paper should be generalizable to a large number of off-policy deep reinforcement learning methods. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.01264
Safe Online Gain Optimization for Variable Impedance Control,Changhao Wang;Zhian Kuang;Xiang Zhang;Masayoshi Tomizuka,"Smooth behaviors are preferable for many contact-rich manipulation tasks. Impedance control arises as an effective way to regulate robot movements by mimicking a mass-spring-damping system. Consequently, the robot behavior can be determined by the impedance gains. However, tuning the impedance gains for different tasks is tricky, especially for unstructured environments. Moreover, online adapting the optimal gains to meet the time-varying performance index is even more challenging. In this paper, we present Safe Online Gain Optimization for Variable Impedance Control (Safe OnGO-VIC). By reformulating the dynamics of impedance control as a control-affine system, in which the impedance gains are the inputs, we provide a novel perspective to understand variable impedance control. Additionally, we innovatively formulate an optimization problem with online collected force information to obtain the optimal impedance gains in real-time. Safety constraints are also embedded in the proposed framework to avoid unwanted collisions. We experimentally validated the proposed algorithm on three manipulation tasks. Comparison results with a constant gain baseline and an adaptive control method prove that the proposed algorithm is effective and generalizable to different scenarios. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.01258
Neural Scene Flow Prior,Xueqian Li;Jhony Kaesemodel Pontes;Simon Lucey,"Before the deep learning revolution, many perception algorithms were based on runtime optimization in conjunction with a strong prior/regularization penalty. A prime example of this in computer vision is optical and scene flow. Supervised learning has largely displaced the need for explicit regularization. Instead, they rely on large amounts of labeled data to capture prior statistics, which are not always readily available for many problems. Although optimization is employed to learn the neural network, the weights of this network are frozen at runtime. As a result, these learning solutions are domain-specific and do not generalize well to other statistically different scenarios. This paper revisits the scene flow problem that relies predominantly on runtime optimization and strong regularization. A central innovation here is the inclusion of a neural scene flow prior, which uses the architecture of neural networks as a new type of implicit regularizer. Unlike learning-based scene flow methods, optimization occurs at runtime, and our approach needs no offline datasets -- making it ideal for deployment in new environments such as autonomous driving. We show that an architecture based exclusively on multilayer perceptrons (MLPs) can be used as a scene flow prior. Our method attains competitive -- if not better -- results on scene flow benchmarks. Also, our neural prior's implicit and continuous scene flow representation allows us to estimate dense long-term correspondences across a sequence of point clouds. The dense motion information is represented by scene flow fields where points can be propagated through time by integrating motion vectors. We demonstrate such a capability by accumulating a sequence of lidar point clouds. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.01253
Two Heads are Better than One: Geometric-Latent Attention for Point Cloud Classification and Segmentation,Hanz Cuevas-Velasquez;Antonio Javier Gallego;Robert B. Fisher,"We present an innovative two-headed attention layer that combines geometric and latent features to segment a 3D scene into semantically meaningful subsets. Each head combines local and global information, using either the geometric or latent features, of a neighborhood of points and uses this information to learn better local relationships. This Geometric-Latent attention layer (Ge-Latto) is combined with a sub-sampling strategy to capture global features. Our method is invariant to permutation thanks to the use of shared-MLP layers, and it can also be used with point clouds with varying densities because the local attention layer does not depend on the neighbor order. Our proposal is simple yet robust, which allows it to achieve competitive results in the ShapeNetPart and ModelNet40 datasets, and the state-of-the-art when segmenting the complex dataset S3DIS, with 69.2% IoU on Area 5, and 89.7% overall accuracy using K-fold cross-validation on the 6 areas. △ Less","30 October, 2021",https://arxiv.org/pdf/2111.00231
Assyrian merchants meet nuclear physicists: history of the early contributions from social sciences to computer science. The case of automatic pattern detection in graphs (1950s--1970s),Sébastien Plutniak,"Community detection is a major issue in network analysis. This paper combines a socio-historical approach with an experimental reconstruction of programs to investigate the early automation of clique detection algorithms, which remains one of the unsolved NP-complete problems today. The research led by the archaeologist Jean-Claude Gardin from the 1950s on non-numerical information and graph analysis is retraced to demonstrate the early contributions of social sciences and humanities. The limited recognition and reception of Gardin's innovative computer application to the humanities are addressed through two factors, in addition to the effects of historiography and bibliographies on the recording, discoverability, and reuse of scientific productions: 1) funding policies, evidenced by the transfer of research effort on graph applications from temporary interdisciplinary spaces to disciplinary organizations related to the then-emerging field of computer science; and 2) the erratic careers of algorithms, in which efficiency, flaws, corrections, and authors' status, were determining factors. △ Less","29 October, 2021",https://arxiv.org/pdf/2110.15567
"Masked LARk: Masked Learning, Aggregation and Reporting worKflow",Joseph J. Pfeiffer III;Denis Charles;Davis Gilton;Young Hun Jung;Mehul Parsana;Erik Anderson,"Today, many web advertising data flows involve passive cross-site tracking of users. Enabling such a mechanism through the usage of third party tracking cookies (3PC) exposes sensitive user data to a large number of parties, with little oversight on how that data can be used. Thus, most browsers are moving towards removal of 3PC in subsequent browser iterations. In order to substantially improve end-user privacy while allowing sites to continue to sustain their business through ad funding, new privacy-preserving primitives need to be introduced. In this paper, we discuss a new proposal, called Masked LARk, for aggregation of user engagement measurement and model training that prevents cross-site tracking, while remaining (a) flexible, for engineering development and maintenance, (b) secure, in the sense that cross-site tracking and tracing are blocked and (c) open for continued model development and training, allowing advertisers to serve relevant ads to interested users. We introduce a secure multi-party compute (MPC) protocol that utilizes ""helper"" parties to train models, so that once data leaves the browser, no downstream system can individually construct a complete picture of the user activity. For training, our key innovation is through the usage of masking, or the obfuscation of the true labels, while still allowing a gradient to be accurately computed in aggregate over a batch of data. Our protocol only utilizes light cryptography, at such a level that an interested yet inexperienced reader can understand the core algorithm. We develop helper endpoints that implement this system, and give example usage of training in PyTorch. △ Less","27 October, 2021",https://arxiv.org/pdf/2110.14794
The Importance of Open Data Policy to Tackle Pandemic in Latin America,Josimar Chire,"Open Data Policies can provide transparency, impulse innovation and citizenship participation. Access to the right data in right time can produce huge benefits to population. But, in Latin America there is not enough interest from governments to promote and use properly. By the other hand, global pandemic has caused many damages in different levels, i.e. Economy, Public Health, Education, etc. The paper opens a discussion about the importance of Open Data Policy to mitigate the impact of Covid-19 and overpass this problem. △ Less","27 October, 2021",https://arxiv.org/pdf/2110.14629
"Closing the ""Quantum Supremacy"" Gap: Achieving Real-Time Simulation of a Random Quantum Circuit Using a New Sunway Supercomputer",Yong;Liu;Xin;Liu;Fang;Li;Haohuan Fu;Yuling Yang;Jiawei Song;Pengpeng Zhao;Zhen Wang;Dajia Peng;Huarong Chen;Chu Guo;Heliang Huang;Wenzhao Wu;Dexun Chen,"We develop a high-performance tensor-based simulator for random quantum circuits(RQCs) on the new Sunway supercomputer. Our major innovations include: (1) a near-optimal slicing scheme, and a path-optimization strategy that considers both complexity and compute density; (2) a three-level parallelization scheme that scales to about 42 million cores; (3) a fused permutation and multiplication design that improves the compute efficiency for a wide range of tensor contraction scenarios; and (4) a mixed-precision scheme to further improve the performance. Our simulator effectively expands the scope of simulatable RQCs to include the 10*10(qubits)*(1+40+1)(depth) circuit, with a sustained performance of 1.2 Eflops (single-precision), or 4.4 Eflops (mixed-precision)as a new milestone for classical simulation of quantum circuits; and reduces the simulation sampling time of Google Sycamore to 304 seconds, from the previously claimed 10,000 years. △ Less","22 November, 2021",https://arxiv.org/pdf/2110.14502
SiWa: See into Walls via Deep UWB Radar,Tianyue Zheng;Zhe Chen;Jun Luo;Lin Ke;Chaoyang Zhao;Yaowen Yang,"Being able to see into walls is crucial for diagnostics of building health; it enables inspections of wall structure without undermining the structural integrity. However, existing sensing devices do not seem to offer a full capability in mapping the in-wall structure while identifying their status (e.g., seepage and corrosion). In this paper, we design and implement SiWa as a low-cost and portable system for wall inspections. Built upon a customized IR-UWB radar, SiWa scans a wall as a user swipes its probe along the wall surface; it then analyzes the reflected signals to synthesize an image and also to identify the material status. Although conventional schemes exist to handle these problems individually, they require troublesome calibrations that largely prevent them from practical adoptions. To this end, we equip SiWa with a deep learning pipeline to parse the rich sensory data. With an ingenious construction and innovative training, the deep learning modules perform structural imaging and the subsequent analysis on material status, without the need for parameter tuning and calibrations. We build SiWa as a prototype and evaluate its performance via extensive experiments and field studies; results confirm that SiWa accurately maps in-wall structures, identifies their materials, and detects possible failures, suggesting a promising solution for diagnosing building health with lower effort and cost. △ Less","27 October, 2021",https://arxiv.org/pdf/2110.14279
Sharding and HTTP/2 Connection Reuse Revisited: Why Are There Still Redundant Connections?,Constantin Sander;Leo Blöcher;Klaus Wehrle;Jan Rüth,"HTTP/2 and HTTP/3 avoid concurrent connections but instead multiplex requests over a single connection. Besides enabling new features, this reduces overhead and enables fair bandwidth sharing. Redundant connections should hence be a story of the past with HTTP/2. However, they still exist, potentially hindering innovation and performance. Thus, we measure their spread and analyze their causes in this paper. We find that 36% - 72% of the 6.24M HTTP Archive and 78% of the Alexa Top 100k websites cause Chromium-based webbrowsers to open superfluous connections. We mainly attribute these to domain sharding, despite HTTP/2 efforts to revert it, and DNS load balancing, but also the Fetch Standard. △ Less","27 October, 2021",https://arxiv.org/pdf/2110.14239
DP-SSL: Towards Robust Semi-supervised Learning with A Few Labeled Samples,Yi Xu;Jiandong Ding;Lu Zhang;Shuigeng Zhou,"The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning~(MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conflict and overlap among the noisy labels, and finally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classification performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82% annotation accuracy on unlabeled data and 93.46% classification accuracy on test data, which are higher than the SOTA results. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.13740
Bayesian Optimization and Deep Learning forsteering wheel angle prediction,Alessandro Riboni;Nicolò Ghioldi;Antonio Candelieri;Matteo Borrotti,"Automated driving systems (ADS) have undergone a significant improvement in the last years. ADS and more precisely self-driving cars technologies will change the way we perceive and know the world of transportation systems in terms of user experience, mode choices and business models. The emerging field of Deep Learning (DL) has been successfully applied for the development of innovative ADS solutions. However, the attempt to single out the best deep neural network architecture and tuning its hyperparameters are all expensive processes, both in terms of time and computational resources. In this work, Bayesian Optimization (BO) is used to optimize the hyperparameters of a Spatiotemporal-Long Short Term Memory (ST-LSTM) network with the aim to obtain an accurate model for the prediction of the steering angle in a ADS. BO was able to identify, within a limited number of trials, a model -- namely BOST-LSTM -- which resulted, on a public dataset, the most accurate when compared to classical end-to-end driving models. △ Less","22 October, 2021",https://arxiv.org/pdf/2110.13629
DeepHelp: Deep Learning for Shout Crisis Text Conversations,Daniel Cahn,"The Shout Crisis Text Line provides individuals undergoing mental health crises an opportunity to have an anonymous text message conversation with a trained Crisis Volunteer (CV). This project partners with Shout and its parent organisation, Mental Health Innovations, to explore the applications of Machine Learning in understanding Shout's conversations and improving its service. The overarching aim of this project is to develop a proof-of-concept model to demonstrate the potential of applying deep learning to crisis text messages. Specifically, this project aims to use deep learning to (1) predict an individual's risk of suicide or self-harm, (2) assess conversation success and CV skill using robust metrics, and (3) extrapolate demographic information from a texter survey to conversations where the texter did not complete the survey. To these ends, contributions to deep learning include a modified Transformer-over-BERT model; a framework for multitask learning to improve generalisation in the presence of sparse labels; and a mathematical model for using imperfect machine learning models to estimate population parameters from a biased training set. Key results include a deep learning model with likely better performance at predicting suicide risk than trained CVs and the ability to predict whether a texter is 21 or under with 88.4% accuracy. We produce three metrics for conversation success and evaluate the validity and usefulness for each. Finally, reversal of participation bias provides evidence that women, who make up 80.3% of conversations with an associated texter survey, make up closer to 73.5%- 74.8% of all conversations; and that if, after every conversation, the texter had shared whether they found their conversation helpful, affirmative answers would fall from 85.1% to 45.45% - 46.51%. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.13244
Gradient-based Quadratic Multiform Separation,Wen-Teng Chang,"Classification as a supervised learning concept is an important content in machine learning. It aims at categorizing a set of data into classes. There are several commonly-used classification methods nowadays such as k-nearest neighbors, random forest, and support vector machine. Each of them has its own pros and cons, and none of them is invincible for all kinds of problems. In this thesis, we focus on Quadratic Multiform Separation (QMS), a classification method recently proposed by Michael Fan et al. (2019). Its fresh concept, rich mathematical structure, and innovative definition of loss function set it apart from the existing classification methods. Inspired by QMS, we propose utilizing a gradient-based optimization method, Adam, to obtain a classifier that minimizes the QMS-specific loss function. In addition, we provide suggestions regarding model tuning through explorations of the relationships between hyperparameters and accuracies. Our empirical result shows that QMS performs as good as most classification methods in terms of accuracy. Its superior performance is almost comparable to those of gradient boosting algorithms that win massive machine learning competitions. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.13006
SSMF: Shifting Seasonal Matrix Factorization,Koki Kawabata;Siddharth Bhatia;Rui Liu;Mohit Wadhwa;Bryan Hooi,"Given taxi-ride counts information between departure and destination locations, how can we forecast their future demands? In general, given a data stream of events with seasonal patterns that innovate over time, how can we effectively and efficiently forecast future events? In this paper, we propose Shifting Seasonal Matrix Factorization approach, namely SSMF, that can adaptively learn multiple seasonal patterns (called regimes), as well as switching between them. Our proposed method has the following properties: (a) it accurately forecasts future events by detecting regime shifts in seasonal patterns as the data stream evolves; (b) it works in an online setting, i.e., processes each observation in constant time and memory; (c) it effectively realizes regime shifts without human intervention by using a lossless data compression scheme. We demonstrate that our algorithm outperforms state-of-the-art baseline methods by accurately forecasting upcoming events on three real-world data streams. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.12763
Changing Software Engineers' Self-Efficacy with Bootcamps:A Research Proposal,Danilo Monteiro Ribeiro;Alberto Souza;Victor Santiago;Danilo Lucena;Geraldo Gomes;Gustavo Pinto,"In several areas of knowledge, self-efficacy is related to the perfomance of individuals, including in Software Engineering. However,it is not clear how self-efficacy can be modified in training conducted by the industry. Furthermore, we still do not understand how self-efficacy can impact an individual's team and career in the industry. This lack of understanding can negatively impact how companies and individuals perceive the importance of self-efficacy in the field. Therefore, We present a research proposal that aims to understand the relationship between self-efficacy and training in Software Engineering. Moreover, we look to understand the role of self-efficacy at Software Development industry. We propose a longitudinal case study with software engineers at Zup Innovation that participating of our bootcamp training. We expect to collect data to support our assumptions that self-efficacy can be related to training in Software Engineering. The other assumption is that self-efficacy at the beginning of training is higher than the middle, and that self-efficacy at the end of training is higher than the middle. We expect that the study proposed in this article will motivate a discussion about self-efficacy and the importance of training employers in the industry of software development. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.12241
Attend and Guide (AG-Net): A Keypoints-driven Attention-based Deep Network for Image Recognition,Asish Bera;Zachary Wharton;Yonghuai Liu;Nik Bessis;Ardhendu Behera,"This paper presents a novel keypoints-based attention mechanism for visual recognition in still images. Deep Convolutional Neural Networks (CNNs) for recognizing images with distinctive classes have shown great success, but their performance in discriminating fine-grained changes is not at the same level. We address this by proposing an end-to-end CNN model, which learns meaningful features linking fine-grained changes using our novel attention mechanism. It captures the spatial structures in images by identifying semantic regions (SRs) and their spatial distributions, and is proved to be the key to modelling subtle changes in images. We automatically identify these SRs by grouping the detected keypoints in a given image. The ``usefulness'' of these SRs for image recognition is measured using our innovative attentional mechanism focusing on parts of the image that are most relevant to a given task. This framework applies to traditional and fine-grained image recognition tasks and does not require manually annotated regions (e.g. bounding-box of body parts, objects, etc.) for learning and prediction. Moreover, the proposed keypoints-driven attention mechanism can be easily integrated into the existing CNN models. The framework is evaluated on six diverse benchmark datasets. The model outperforms the state-of-the-art approaches by a considerable margin using Distracted Driver V1 (Acc: 3.39%), Distracted Driver V2 (Acc: 6.58%), Stanford-40 Actions (mAP: 2.15%), People Playing Musical Instruments (mAP: 16.05%), Food-101 (Acc: 6.30%) and Caltech-256 (Acc: 2.59%) datasets. △ Less","23 October, 2021",https://arxiv.org/pdf/2110.12183
An attention-driven hierarchical multi-scale representation for visual recognition,Zachary Wharton;Ardhendu Behera;Asish Bera,"Convolutional Neural Networks (CNNs) have revolutionized the understanding of visual content. This is mainly due to their ability to break down an image into smaller pieces, extract multi-scale localized features and compose them to construct highly expressive representations for decision making. However, the convolution operation is unable to capture long-range dependencies such as arbitrary relations between pixels since it operates on a fixed-size window. Therefore, it may not be suitable for discriminating subtle changes (e.g. fine-grained visual recognition). To this end, our proposed method captures the high-level long-range dependencies by exploring Graph Convolutional Networks (GCNs), which aggregate information by establishing relationships among multi-scale hierarchical regions. These regions consist of smaller (closer look) to larger (far look), and the dependency between regions is modeled by an innovative attention-driven message propagation, guided by the graph structure to emphasize the neighborhoods of a given region. Our approach is simple yet extremely effective in solving both the fine-grained and generic visual classification problems. It outperforms the state-of-the-arts with a significant margin on three and is very competitive on other two datasets. △ Less","23 October, 2021",https://arxiv.org/pdf/2110.12178
Circle Representation for Medical Object Detection,Ethan H. Nguyen;Haichun Yang;Ruining Deng;Yuzhe Lu;Zheyu Zhu;Joseph T. Roland;Le Lu;Bennett A. Landman;Agnes B. Fogo;Yuankai Huo,"Box representation has been extensively used for object detection in computer vision. Such representation is efficacious but not necessarily optimized for biomedical objects (e.g., glomeruli), which play an essential role in renal pathology. In this paper, we propose a simple circle representation for medical object detection and introduce CircleNet, an anchor-free detection framework. Compared with the conventional bounding box representation, the proposed bounding circle representation innovates in three-fold: (1) it is optimized for ball-shaped biomedical objects; (2) The circle representation reduced the degree of freedom compared with box representation; (3) It is naturally more rotation invariant. When detecting glomeruli and nuclei on pathological images, the proposed circle representation achieved superior detection performance and be more rotation-invariant, compared with the bounding box. The code has been made publicly available: https://github.com/hrlblab/CircleNet △ Less","22 October, 2021",https://arxiv.org/pdf/2110.12093
Polynomial-Time Sum-of-Squares Can Robustly Estimate Mean and Covariance of Gaussians Optimally,Pravesh K. Kothari;Peter Manohar;Brian Hu Zhang,"In this work, we revisit the problem of estimating the mean and covariance of an unknown d-dimensional Gaussian distribution in the presence of an \varepsilon-fraction of adversarial outliers. The pioneering work of [DKK+16] gave a polynomial time algorithm for this task with optimal \tilde{O}(\varepsilon) error using n = \textrm{poly}(d, 1/\varepsilon) samples. On the other hand, [KS17b] introduced a general framework for robust moment estimation via a canonical sum-of-squares relaxation that succeeds for the more general class of certifiably subgaussian and certifiably hypercontractive [BK20] distributions. When specialized to Gaussians, this algorithm obtains the same \tilde{O}(\varepsilon) error guarantee as [DKK+16] but incurs a super-polynomial sample complexity (n = d^{O(\log(1/\varepsilon)}) and running time (n^{O(\log(1/\varepsilon))}). This cost appears inherent to their analysis as it relies only on sum-of-squares certificates of upper bounds on directional moments while the analysis in [DKK+16] relies on lower bounds on directional moments inferred from algebraic relationships between moments of Gaussian distributions. We give a new, simple analysis of the same canonical sum-of-squares relaxation used in [KS17b, BK20] and show that for Gaussian distributions, their algorithm achieves the same error, sample complexity and running time guarantees as of the specialized algorithm in [DKK+16]. Our key innovation is a new argument that allows using moment lower bounds without having sum-of-squares certificates for them. We believe that our proof technique will likely be useful in developing further robust estimation algorithms. △ Less","22 October, 2021",https://arxiv.org/pdf/2110.11853
WebFed: Cross-platform Federated Learning Framework Based on Web Browser with Local Differential Privacy,Zhuotao Lian;Qinglin Yang;Qingkui Zeng;Chunhua Su,"For data isolated islands and privacy issues, federated learning has been extensively invoking much interest since it allows clients to collaborate on training a global model using their local data without sharing any with a third party. However, the existing federated learning frameworks always need sophisticated condition configurations (e.g., sophisticated driver configuration of standalone graphics card like NVIDIA, compile environment) that bring much inconvenience for large-scale development and deployment. To facilitate the deployment of federated learning and the implementation of related applications, we innovatively propose WebFed, a novel browser-based federated learning framework that takes advantage of the browser's features (e.g., Cross-platform, JavaScript Programming Features) and enhances the privacy protection via local differential privacy mechanism. Finally, We conduct experiments on heterogeneous devices to evaluate the performance of the proposed WebFed framework. △ Less","22 October, 2021",https://arxiv.org/pdf/2110.11646
Occlusion-Robust Object Pose Estimation with Holistic Representation,Bo Chen;Tat-Jun Chin;Marius Klimavicius,"Practical object pose estimation demands robustness against occlusions to the target object. State-of-the-art (SOTA) object pose estimators take a two-stage approach, where the first stage predicts 2D landmarks using a deep network and the second stage solves for 6DOF pose from 2D-3D correspondences. Albeit widely adopted, such two-stage approaches could suffer from novel occlusions when generalising and weak landmark coherence due to disrupted features. To address these issues, we develop a novel occlude-and-blackout batch augmentation technique to learn occlusion-robust deep features, and a multi-precision supervision architecture to encourage holistic pose representation learning for accurate and coherent landmark predictions. We perform careful ablation tests to verify the impact of our innovations and compare our method to SOTA pose estimators. Without the need of any post-processing or refinement, our method exhibits superior performance on the LINEMOD dataset. On the YCB-Video dataset our method outperforms all non-refinement methods in terms of the ADD(-S) metric. We also demonstrate the high data-efficiency of our method. Our code is available at http://github.com/BoChenYS/ROPE △ Less","22 October, 2021",https://arxiv.org/pdf/2110.11636
User Incentives for Blockchain-based Data Sharing Platforms,Vikas Jaiman;Leonard Pernice;Visara Urovi,"Data sharing is very important for accelerating scientific research, business innovations, and for informing individuals. Yet, concerns over data privacy, cost, and lack of secure data-sharing solutions have prevented data owners from sharing data. To overcome these issues, several research works have proposed blockchain-based data-sharing solutions for their ability to add transparency and control to the data-sharing process. Yet, while models for decentralized data sharing exist, how to incentivize these structures to enable data sharing at scale remains largely unexplored. In this paper, we propose incentive mechanisms for decentralized data-sharing platforms. We use smart contracts to automate different payment options between data owners and data requesters. We discuss multiple cost pricing scenarios for data owners to monetize their data. Moreover, we simulate the incentive mechanisms on a blockchain-based data-sharing platform. The evaluation of our simulation indicates that a cost compensation model for the data owner can rapidly cover the cost of data sharing and balance the overall incentives for all the actors in the platform. △ Less","20 October, 2021",https://arxiv.org/pdf/2110.11348
Unsupervised cross-user adaptation in taste sensation recognition based on surface electromyography with conformal prediction and domain regularized component analysis,Hengyang Wang;Xianghao Zhan;Li Liu;Asif Ullah;Huiyan Li;Han Gao;You Wang;Guang Li,"Human taste sensation can be qualitatively described with surface electromyography. However, the pattern recognition models trained on one subject (the source domain) do not generalize well on other subjects (the target domain). To improve the generalizability and transferability of taste sensation models developed with sEMG data, two methods were innovatively applied in this study: domain regularized component analysis (DRCA) and conformal prediction with shrunken centroids (CPSC). The effectiveness of these two methods was investigated independently in an unlabeled data augmentation process with the unlabeled data from the target domain, and the same cross-user adaptation pipeline were conducted on six subjects. The results show that DRCA improved the classification accuracy on six subjects (p < 0.05), compared with the baseline models trained only with the source domain data;, while CPSC did not guarantee the accuracy improvement. Furthermore, the combination of DRCA and CPSC presented statistically significant improvement (p < 0.05) in classification accuracy on six subjects. The proposed strategy combining DRCA and CPSC showed its effectiveness in addressing the cross-user data distribution drift in sEMG-based taste sensation recognition application. It also shows the potential in more cross-user adaptation applications. △ Less","11 December, 2021",https://arxiv.org/pdf/2110.11339
Analysis of the first Genetic Engineering Attribution Challenge,Oliver M. Crook;Kelsey Lane Warmbrod;Greg Lipstein;Christine Chung;Christopher W. Bakerlee;T. Greg McKelvey Jr.;Shelly R. Holland;Jacob L. Swett;Kevin M. Esvelt;Ethan C. Alley;William J. Bradshaw,"The ability to identify the designer of engineered biological sequences -- termed genetic engineering attribution (GEA) -- would help ensure due credit for biotechnological innovation, while holding designers accountable to the communities they affect. Here, we present the results of the first Genetic Engineering Attribution Challenge, a public data-science competition to advance GEA. Top-scoring teams dramatically outperformed previous models at identifying the true lab-of-origin of engineered sequences, including an increase in top-1 and top-10 accuracy of 10 percentage points. A simple ensemble of prizewinning models further increased performance. New metrics, designed to assess a model's ability to confidently exclude candidate labs, also showed major improvements, especially for the ensemble. Most winning teams adopted CNN-based machine-learning approaches; however, one team achieved very high accuracy with an extremely fast neural-network-free approach. Future work, including future competitions, should further explore a wide diversity of approaches for bringing GEA technology into practical use. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.11242
RefRec: Pseudo-labels Refinement via Shape Reconstruction for Unsupervised 3D Domain Adaptation,Adriano Cardace;Riccardo Spezialetti;Pierluigi Zama Ramirez;Samuele Salti;Luigi Di Stefano,"Unsupervised Domain Adaptation (UDA) for point cloud classification is an emerging research problem with relevant practical motivations. Reliance on multi-task learning to align features across domains has been the standard way to tackle it. In this paper, we take a different path and propose RefRec, the first approach to investigate pseudo-labels and self-training in UDA for point clouds. We present two main innovations to make self-training effective on 3D data: i) refinement of noisy pseudo-labels by matching shape descriptors that are learned by the unsupervised task of shape reconstruction on both domains; ii) a novel self-training protocol that learns domain-specific decision boundaries and reduces the negative impact of mislabelled target samples and in-domain intra-class variability. RefRec sets the new state of the art in both standard benchmarks used to test UDA for point cloud classification, showcasing the effectiveness of self-training for this important problem. △ Less","21 October, 2021",https://arxiv.org/pdf/2110.11036
PipAttack: Poisoning Federated Recommender Systems forManipulating Item Promotion,Shijie Zhang;Hongzhi Yin;Tong Chen;Zi Huang;Quoc Viet Hung Nguyen;Lizhen Cui,"Due to the growing privacy concerns, decentralization emerges rapidly in personalized services, especially recommendation. Also, recent studies have shown that centralized models are vulnerable to poisoning attacks, compromising their integrity. In the context of recommender systems, a typical goal of such poisoning attacks is to promote the adversary's target items by interfering with the training dataset and/or process. Hence, a common practice is to subsume recommender systems under the decentralized federated learning paradigm, which enables all user devices to collaboratively learn a global recommender while retaining all the sensitive data locally. Without exposing the full knowledge of the recommender and entire dataset to end-users, such federated recommendation is widely regarded `safe' towards poisoning attacks. In this paper, we present a systematic approach to backdooring federated recommender systems for targeted item promotion. The core tactic is to take advantage of the inherent popularity bias that commonly exists in data-driven recommenders. As popular items are more likely to appear in the recommendation list, our innovatively designed attack model enables the target item to have the characteristics of popular items in the embedding space. Then, by uploading carefully crafted gradients via a small number of malicious users during the model update, we can effectively increase the exposure rate of a target (unpopular) item in the resulted federated recommender. Evaluations on two real-world datasets show that 1) our attack model significantly boosts the exposure rate of the target item in a stealthy way, without harming the accuracy of the poisoned recommender; and 2) existing defenses are not effective enough, highlighting the need for new defenses against our local model poisoning attacks to federated recommender systems. △ Less","21 October, 2021",https://arxiv.org/pdf/2110.10926
DEEPAGÉ: Answering Questions in Portuguese about the Brazilian Environment,Flávio Nakasato Cação;Marcos Menon José;André Seidel Oliveira;Stefano Spindola;Anna Helena Reali Costa;Fábio Gagliardi Cozman,"The challenge of climate change and biome conservation is one of the most pressing issues of our time - particularly in Brazil, where key environmental reserves are located. Given the availability of large textual databases on ecological themes, it is natural to resort to question answering (QA) systems to increase social awareness and understanding about these topics. In this work, we introduce multiple QA systems that combine in novel ways the BM25 algorithm, a sparse retrieval technique, with PTT5, a pre-trained state-of-the-art language model. Our QA systems focus on the Portuguese language, thus offering resources not found elsewhere in the literature. As training data, we collected questions from open-domain datasets, as well as content from the Portuguese Wikipedia and news from the press. We thus contribute with innovative architectures and novel applications, attaining an F1-score of 36.2 with our best model. △ Less","19 October, 2021",https://arxiv.org/pdf/2110.10015
Towards responsible research in digital technology for health care,Pierre Jannin,"Digital technology is everywhere for the benefit of our daily and professional life. It strongly impacts our life and was crucial to maintain professional and social activities during the COVID19 crisis. Similarly, digital technologies are key within biomedical engineering research topics. Innovations have been generated and introduced over the last 40 years, demonstrating how computing and digital technologies have impacted health care. Although the benefits of digital technology are obvious now, we are at the convergence of several issues which makes us aware about social, societal and environmental challenges associated with this technology. In the social domain, digital technologies raise concern about exclusion (financial, geographical, educational, demographical, racial, gender, language, and disabled related exclusion) and physical and mental health. In the societal dimension, digital technologies raise concern about politics and democracy (sovereignty and governance, cognitive filters and citizen's engagement), privacy and security (data acquisition and usage transparency, level of personal approval, and level of anonymization), and economics. In the environmental dimension, digital technologies raise concern about energy consumption and hardware production. This paper introduces and defines these challenges for digital technology in general, as well as when applied to health care. The objective of this paper is to make the research community more aware about the challenges of digital technology and to promote more transparency for innovative and responsible research. △ Less","20 October, 2021",https://arxiv.org/pdf/2110.09255
Learning First-Order Rules with Relational Path Contrast for Inductive Relation Reasoning,Yudai Pan;Jun Liu;Lingling Zhang;Xin Hu;Tianzhe Zhao;Qika Lin,"Relation reasoning in knowledge graphs (KGs) aims at predicting missing relations in incomplete triples, whereas the dominant paradigm is learning the embeddings of relations and entities, which is limited to a transductive setting and has restriction on processing unseen entities in an inductive situation. Previous inductive methods are scalable and consume less resource. They utilize the structure of entities and triples in subgraphs to own inductive ability. However, in order to obtain better reasoning results, the model should acquire entity-independent relational semantics in latent rules and solve the deficient supervision caused by scarcity of rules in subgraphs. To address these issues, we propose a novel graph convolutional network (GCN)-based approach for interpretable inductive reasoning with relational path contrast, named RPC-IR. RPC-IR firstly extracts relational paths between two entities and learns representations of them, and then innovatively introduces a contrastive strategy by constructing positive and negative relational paths. A joint training strategy considering both supervised and contrastive information is also proposed. Comprehensive experiments on three inductive datasets show that RPC-IR achieves outstanding performance comparing with the latest inductive reasoning methods and could explicitly represent logical rules for interpretability. △ Less","17 October, 2021",https://arxiv.org/pdf/2110.08810
Hand Gesture Recognition Using Temporal Convolutions and Attention Mechanism,Elahe Rahimian;Soheil Zabihi;Amir Asif;Dario Farina;S. Farokh Atashzar;Arash Mohammadi,"Advances in biosignal signal processing and machine learning, in particular Deep Neural Networks (DNNs), have paved the way for the development of innovative Human-Machine Interfaces for decoding the human intent and controlling artificial limbs. DNN models have shown promising results with respect to other algorithms for decoding muscle electrical activity, especially for recognition of hand gestures. Such data-driven models, however, have been challenged by their need for a large number of trainable parameters and their structural complexity. Here we propose the novel Temporal Convolutions-based Hand Gesture Recognition architecture (TC-HGR) to reduce this computational burden. With this approach, we classified 17 hand gestures via surface Electromyogram (sEMG) signals by the adoption of attention mechanisms and temporal convolutions. The proposed method led to 81.65% and 80.72% classification accuracy for window sizes of 300ms and 200ms, respectively. The number of parameters to train the proposed TC-HGR architecture is 11.9 times less than that of its state-of-the-art counterpart. △ Less","17 October, 2021",https://arxiv.org/pdf/2110.08717
Pyramid Correlation based Deep Hough Voting for Visual Object Tracking,Ying Wang;Tingfa Xu;Jianan Li;Shenwang Jiang;Junjie Chen,"Most of the existing Siamese-based trackers treat tracking problem as a parallel task of classification and regression. However, some studies show that the sibling head structure could lead to suboptimal solutions during the network training. Through experiments we find that, without regression, the performance could be equally promising as long as we delicately design the network to suit the training objective. We introduce a novel voting-based classification-only tracking algorithm named Pyramid Correlation based Deep Hough Voting (short for PCDHV), to jointly locate the top-left and bottom-right corners of the target. Specifically we innovatively construct a Pyramid Correlation module to equip the embedded feature with fine-grained local structures and global spatial contexts; The elaborately designed Deep Hough Voting module further take over, integrating long-range dependencies of pixels to perceive corners; In addition, the prevalent discretization gap is simply yet effectively alleviated by increasing the spatial resolution of the feature maps while exploiting channel-space relationships. The algorithm is general, robust and simple. We demonstrate the effectiveness of the module through a series of ablation experiments. Without bells and whistles, our tracker achieves better or comparable performance to the SOTA algorithms on three challenging benchmarks (TrackingNet, GOT-10k and LaSOT) while running at a real-time speed of 80 FPS. Codes and models will be released. △ Less","15 October, 2021",https://arxiv.org/pdf/2110.07994
Anomaly Detection in Multi-Agent Trajectories for Automated Driving,Julian Wiederer;Arij Bouazizi;Marco Troina;Ulrich Kressel;Vasileios Belagiannis,"Human drivers can recognise fast abnormal driving situations to avoid accidents. Similar to humans, automated vehicles are supposed to perform anomaly detection. In this work, we propose the spatio-temporal graph auto-encoder for learning normal driving behaviours. Our innovation is the ability to jointly learn multiple trajectories of a dynamic number of agents. To perform anomaly detection, we first estimate a density function of the learned trajectory feature representation and then detect anomalies in low-density regions. Due to the lack of multi-agent trajectory datasets for anomaly detection in automated driving, we introduce our dataset using a driving simulator for normal and abnormal manoeuvres. Our evaluations show that our approach learns the relation between different agents and delivers promising results compared to the related works. The code, simulation and the dataset are publicly available on https://github.com/againerju/maad_highway. △ Less","28 October, 2021",https://arxiv.org/pdf/2110.07922
"Reconfigurable, Intelligent, and SustainableWireless Environments for 6G Smart Connectivity",Emilio Calvanese Strinati;George C. Alexandropoulos;Henk Wymeersch;Benoit Denis;Vincenzo Sciancalepore;Raffaele D'Errico;Antonio Clemente;Dinh-Thuy Phan-Huy;Elisabeth De Carvalho;Petar Popovski,"Various visions on the forthcoming sixth Generation (6G) networks point towards flexible connect-and-compute technologies to support future innovative services and the corresponding use cases. 6G should be capable to accommodate ever-evolving and heterogeneous applications, future regulations, and diverse user-, service-, and location-based requirements. A key element towards building smart and energy sustainable wireless systems beyond 5G is the Reconfigurable Intelligent Surface (RIS), which offers programmable control and shaping of the wireless propagation environment. Capitalizing on this technology potential, in this article we introduce two new concepts: i) wireless environment as a service, which leverages a novel RIS-empowered networking paradigm to trade off diverse, and usually conflicting, connectivity objectives; and ii) performance-boosted areas enabled by RIS-based connectivity, representing competing service provisioning areas that are highly spatially and temporally focused. We discuss the key technological enablers and research challenges with the proposed networking paradigm, and highlight the potential profound role of RISs in the recent Open Radio Access Network (O-RAN) architecture. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.07547
IB-GAN: A Unified Approach for Multivariate Time Series Classification under Class Imbalance,Grace Deng;Cuize Han;Tommaso Dreossi;Clarence Lee;David S. Matteson,"Classification of large multivariate time series with strong class imbalance is an important task in real-world applications. Standard methods of class weights, oversampling, or parametric data augmentation do not always yield significant improvements for predicting minority classes of interest. Non-parametric data augmentation with Generative Adversarial Networks (GANs) offers a promising solution. We propose Imputation Balanced GAN (IB-GAN), a novel method that joins data augmentation and classification in a one-step process via an imputation-balancing approach. IB-GAN uses imputation and resampling techniques to generate higher quality samples from randomly masked vectors than from white noise, and augments classification through a class-balanced set of real and synthetic samples. Imputation hyperparameter p_{miss} allows for regularization of classifier variability by tuning innovations introduced via generator imputation. IB-GAN is simple to train and model-agnostic, pairing any deep learning classifier with a generator-discriminator duo and resulting in higher accuracy for under-observed classes. Empirical experiments on open-source UCR data and proprietary 90K product dataset show significant performance gains against state-of-the-art parametric and GAN baselines. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.07460
Ethics lines and Machine learning: a design and simulation of an Association Rules Algorithm for exploiting the data,Patrici Calvo;Rebeca Egea-Moreno,"Data mining techniques offer great opportunities for developing ethics lines, tools for communication, participation and innovation whose main aim is to ensure improvements and compliance with the values, conduct and commitments making up the code of ethics. The aim of this study is to suggest a process for exploiting the data generated by the data generated and collected from an ethics line by extracting rules of association and applying the Apriori algorithm. This makes it possible to identify anomalies and behaviour patterns requiring action to review, correct, promote or expand them, as appropriate. Finally, I offer a simulated application of the Apriori algorithm, supplying it with synthetic data to find out its potential, strengths and limitations. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.07370
State of Security and Privacy Practices of Top Websites in the East African Community (EAC),Abdirahman Mohamed;Christopher Dare;Ayobami Esther Olanrewaju;Mercyleen Tanui;Fonyuy Boris Lami,"Growth in technology has resulted in the large-scale collection and processing of Personally Identifiable Information by organizations that run digital services such as websites, which led to the emergence of new legislation to regulate PII collection and processing by organizations. Subsequently, several African countries have recently started enacting new data protection regulations due to recent technological innovations. However, there is little information about the security and privacy practices of top websites serving content to EAC citizens. We, therefore, analyze the website operators' patterns in terms of third-party tracking, security of data transmission, cookie information, and privacy policies for 169 top EAC website operators using WebXray, OpenSSL, and Alexa top websites API. Our results show that only 75 percent of the analyzed websites have a privacy policy in place. Out of this, only 16 percent of the third-party tracking companies that track users on a particular website are disclosed in the site's privacy policy statements which means that users don not have a way of knowing which third parties collect data about them when they visit a website. Such privacy policies take time to read and are difficult to understand; on average, it takes a college graduate to comprehend the policy and a user spends 12 minutes to read the policy. Additionally, most third-party tracking on EAC websites is related to advertisement and belongs to companies outside the EAC. This means that EAC lawmakers need to enact suitable laws to ensure that people's privacy is protected as the rate of technology adoption continues to increase. △ Less","13 October, 2021",https://arxiv.org/pdf/2110.06654
Enabling Level-4 Autonomous Driving on a Single $1k Off-the-Shelf Card,Hsin-Hsuan Sung;Yuanchao Xu;Jiexiong Guan;Wei Niu;Shaoshan Liu;Bin Ren;Yanzhi Wang;Xipeng Shen,"Autonomous driving is of great interest in both research and industry. The high cost has been one of the major roadblocks that slow down the development and adoption of autonomous driving in practice. This paper, for the first-time, shows that it is possible to run level-4 (i.e., fully autonomous driving) software on a single off-the-shelf card (Jetson AGX Xavier) for less than $1k, an order of magnitude less than the state-of-the-art systems, while meeting all the requirements of latency. The success comes from the resolution of some important issues shared by existing practices through a series of measures and innovations. The study overturns the common perceptions of the computing resources required by level-4 autonomous driving, points out a promising path for the industry to lower the cost, and suggests a number of research opportunities for rethinking the architecture, software design, and optimizations of autonomous driving. △ Less","12 October, 2021",https://arxiv.org/pdf/2110.06373
A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA,Jani Antikainen;Mamia Agbese;Hanna-Kaisa Alanen;Erika Halme;Hannakaisa Isomäki;Marianna Jantunen;Kai-Kristian Kemell;Rebekah Rousi;Heidi Vainio-Pekka;Ville Vakkuri,"There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOLA method for creating ethically aligned AI -systems. ECCOLA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOLA with a deployment model to drive the adoption of ECCOLA, as any method, no matter how good, is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given lifecycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition. △ Less","12 October, 2021",https://arxiv.org/pdf/2110.05933
Rethinking the Spatial Route Prior in Vision-and-Language Navigation,Xinzhe Zhou;Wei Liu;Yadong Mu,"Vision-and-language navigation (VLN) is a trending topic which aims to navigate an intelligent agent to an expected position through natural language instructions. This work addresses the task of VLN from a previously-ignored aspect, namely the spatial route prior of the navigation scenes. A critically enabling innovation of this work is explicitly considering the spatial route prior under several different VLN settings. In a most information-rich case of knowing environment maps and admitting shortest-path prior, we observe that given an origin-destination node pair, the internal route can be uniquely determined. Thus, VLN can be effectively formulated as an ordinary classification problem over all possible destination nodes in the scenes. Furthermore, we relax it to other more general VLN settings, proposing a sequential-decision variant (by abandoning the shortest-path route prior) and an explore-and-exploit scheme (for addressing the case of not knowing the environment maps) that curates a compact and informative sub-graph to exploit. As reported by [34], the performance of VLN methods has been stuck at a plateau in past two years. Even with increased model complexity, the state-of-the-art success rate on R2R validation-unseen set has stayed around 62% for single-run and 73% for beam-search with model-ensemble. We have conducted comprehensive evaluations on both R2R and R4R, and surprisingly found that utilizing the spatial route priors may be the key of breaking above-mentioned performance ceiling. For example, on R2R validation-unseen set, when the number of discrete nodes explored is about 40, our single-model success rate reaches 73%, and increases to 78% if a Speaker model is ensembled, which significantly outstrips previous state-of-the-art VLN-BERT with 3 models ensembled. △ Less","11 October, 2021",https://arxiv.org/pdf/2110.05728
Training Computing Educators to Become Computing Education Researchers,Jeffrey C. Carver;Sarah Heckman;Mark Sherriff,"The computing education community endeavors to consistently move forward, improving the educational experience of our students. As new innovations in computing education practice are learned and shared, however, these papers may not exhibit the desired qualities that move simple experience reports to true Scholarship of Teaching and Learning (SoTL). We report on our six years of experience in running professional development for computing educators in empirical research methods for social and behavioral studies in the classroom. Our goal is to have a direct impact on instructors who are in the beginning stages of transitioning their educational innovations from anecdotal to empirical results that can be replicated by instructors at other institutions. To achieve this, we created a year-long mentoring experience, beginning with a multi-day workshop on empirical research methods during the summer, followed by regular mentoring sessions with participants, and culminating in a follow-up session at the following year's SIGCSE Technical Symposium. From survey results and as evidenced by eventual research results and publications from participants, we believe that our method of structuring empirical research professional development was successful and could be a model for similar programs in other areas. △ Less","11 October, 2021",https://arxiv.org/pdf/2110.05560
We Need to Talk About Data: The Importance of Data Readiness in Natural Language Processing,Fredrik Olsson;Magnus Sahlgren,"In this paper, we identify the state of data as being an important reason for failure in applied Natural Language Processing (NLP) projects. We argue that there is a gap between academic research in NLP and its application to problems outside academia, and that this gap is rooted in poor mutual understanding between academic researchers and their non-academic peers who seek to apply research results to their operations. To foster transfer of research results from academia to non-academic settings, and the corresponding influx of requirements back to academia, we propose a method for improving the communication between researchers and external stakeholders regarding the accessibility, validity, and utility of data based on Data Readiness Levels \cite{lawrence2017data}. While still in its infancy, the method has been iterated on and applied in multiple innovation and research projects carried out with stakeholders in both the private and public sectors. Finally, we invite researchers and practitioners to share their experiences, and thus contributing to a body of work aimed at raising awareness of the importance of data readiness for NLP. △ Less","11 October, 2021",https://arxiv.org/pdf/2110.05464
Dynamic Median Consensus Over Random Networks,Shuhua Yu;Yuan Chen;Soummya Kar,"This paper studies the problem of finding the median of N distinct numbers distributed across networked agents. Each agent updates its estimate for the median from noisy local observations of one of the N numbers and information from neighbors. We consider an undirected random network that is connected on average, and a noisy observation sequence that has finite variance and almost surely decaying bias. We present a consensus+innovations algorithm with clipped innovations. Under some regularity assumptions on the network and observation model, we show that each agent's local estimate converges to the set of median(s) almost surely at an asymptotic sublinear rate. Numerical experiments demonstrate the effectiveness of the presented algorithm. △ Less","11 October, 2021",https://arxiv.org/pdf/2110.05317
"Ethical Assurance: A practical approach to the responsible design, development, and deployment of data-driven technologies",Christopher Burr;David Leslie,"This article offers several contributions to the interdisciplinary project of responsible research and innovation in data science and AI. First, it provides a critical analysis of current efforts to establish practical mechanisms for algorithmic assessment, which are used to operationalise normative principles, such as sustainability, accountability, transparency, fairness, and explainability, in order to identify limitations and gaps with the current approaches. Second, it provides an accessible introduction to the methodology of argument-based assurance, and explores how it is currently being applied in the development of safety cases for autonomous and intelligent systems. Third, it generalises this method to incorporate wider ethical, social, and legal considerations, in turn establishing a novel version of argument-based assurance that we call 'ethical assurance'. Ethical assurance is presented as a structured means for unifying the myriad practical mechanisms that have been proposed, as it is built upon a process-based form of project governance that supports inclusive and participatory ethical deliberation while also remaining grounded in social and technical realities. Finally, it sets an agenda for ethical assurance, by detailing current challenges, open questions, and next steps, which serve as a springboard to build an active (and interdisciplinary) research programme as well as contribute to ongoing discussions in policy and governance. △ Less","11 October, 2021",https://arxiv.org/pdf/2110.05164
Topological Data Analysis (TDA) Techniques Enhance Hand Pose Classification from ECoG Neural Recordings,Simone Azeglio;Arianna Di Bernardo;Gabriele Penna;Fabrizio Pittatore;Simone Poetto;Johannes Gruenwald;Christoph Kapeller;Kyousuke Kamada;Christoph Guger,"Electrocorticogram (ECoG) well characterizes hand movement intentions and gestures. In the present work we aim to investigate the possibility to enhance hand pose classification, in a Rock-Paper-Scissor - and Rest - task, by introducing topological descriptors of time series data. We hypothesized that an innovative approach based on topological data analysis can extract hidden information that are not detectable with standard Brain Computer Interface (BCI)techniques. To investigate this hypothesis, we integrate topological features together with power band features and feed them to several standard classifiers, e.g. Random Forest,Gradient Boosting. Model selection is thus completed after a meticulous phase of bayesian hyperparameter optimization. With our method, we observed robust results in terms of ac-curacy for a four-labels classification problem, with limited available data. Through feature importance investigation, we conclude that topological descriptors are able to extract useful discriminative information and provide novel insights.Since our data are restricted to single-patient recordings, generalization might be limited. Nevertheless, our method can be extended and applied to a wide range of neurophysiological recordings and it might be an intriguing point of departure for future studies. △ Less","9 October, 2021",https://arxiv.org/pdf/2110.04653
Optimization of Reconfigurable Intelligent Surfaces Through Trace Maximization,Akbar Sayeed,"Reconfigurable Intelligent Surfaces (RIS) have received significant attention recently as an innovation for enhanced connectivity, capacity, and energy efficiency in future wireless networks. Recent works indicate that such RIS-augmented communications can significantly enhance performance by intelligently shaping the characteristics of the multipath propagation environment to focus the energy in a desired direction and to circumvent impediments such as blockage, especially for communication at millimeter-wave (mmW), Terahertz (THz) and higher frequencies. In this paper, we investigate optimized (amplitude and phase) RIS design in a point-to-point multipath MIMO link and study the impact on link capacity under the assumption of perfect channel state information at the transmitter (TX), receiver (RX) and RIS. Specifically, we propose RIS design based on the maximization of the trace of the composite TX-RIS-RX link matrix which is a measure of the average power at the RX. We propose two RIS designs: a diagonal RIS matrix, and a general RIS matrix representing a more advanced architecture. The optimum design, in both cases, corresponds to calculating the dominant eigenvector of certain Hermitian matrices induced by the component channel matrices. We illustrate the capacity performance of the optimized RIS designs and compare them to a baseline design (random amplitudes and phases) and a recently proposed low-complexity phase-only design. We present results for sparse and rich multipath, and also consider the impact of line-of-sight paths. Our results show that while all designs offer comparable capacity at high signal-to-noise ratios (SNRs), the proposed optimum designs offer substantial gains at lower SNRs. △ Less","13 September, 2021",https://arxiv.org/pdf/2110.04073
Ranking Cost: Building An Efficient and Scalable Circuit Routing Planner with Evolution-Based Optimization,Shiyu Huang;Bin Wang;Dong Li;Jianye Hao;Ting Chen;Jun Zhu,"Circuit routing has been a historically challenging problem in designing electronic systems such as very large-scale integration (VLSI) and printed circuit boards (PCBs). The main challenge is that connecting a large number of electronic components under specific design rules involves a very large search space. Early solutions are typically designed with hard-coded heuristics, which suffer from problems of non-optimal solutions and lack of flexibility for new design needs. Although a few learning-based methods have been proposed recently, they are typically cumbersome and hard to extend to large-scale applications. In this work, we propose a new algorithm for circuit routing, named Ranking Cost, which innovatively combines search-based methods (i.e., A* algorithm) and learning-based methods (i.e., Evolution Strategies) to form an efficient and trainable router. In our method, we introduce a new set of variables called cost maps, which can help the A* router to find out proper paths to achieve the global objective. We also train a ranking parameter, which can produce the ranking order and further improve the performance of our method. Our algorithm is trained in an end-to-end manner and does not use any artificial data or human demonstration. In the experiments, we compare with the sequential A* algorithm and a canonical reinforcement learning approach, and results show that our method outperforms these baselines with higher connectivity rates and better scalability. △ Less","8 October, 2021",https://arxiv.org/pdf/2110.03939
MPD: Moving Target Defense through Communication Protocol Dialects,Yongsheng Mei;Kailash Gogineni;Tian Lan;Guru Venkataramani,"Communication protocol security is among the most significant challenges of the Internet of Things (IoT) due to the wide variety of hardware and software technologies involved. Moving target defense (MTD) has been adopted as an innovative strategy to solve this problem by dynamically changing target system properties and configurations to obfuscate the attack surface. Nevertheless, the existing work of MTD primarily focuses on lower-level properties (e.g., IP addresses or port numbers), and only a limited number of variations can be generated based on these properties. In this paper, we propose a new approach of MTD through communication protocol dialects (MPD) - which dynamically customizes a communication protocol into various protocol dialects and leverages them to create a moving target defense. Specifically, MPD harnesses a dialect generating function to create protocol dialects and then a mapping function to select one specific dialect for each packet during communication. To keep different network entities in synchronization, we also design a self-synchronization mechanism utilizing a pseudo-random number generator with the input of a pre-shared secret key and previously sent packets. We implement a prototype of MPD and evaluate its feasibility on standard network protocol (i.e., File Transfer Protocol) and internet of things protocol (i.e., Message Queuing Telemetry Transport). The results indicate that MPD can create a moving target defense with protocol dialects to effectively address various attacks - including the denial of service attack and malicious packet modifications - with negligible overhead. △ Less","7 October, 2021",https://arxiv.org/pdf/2110.03798
Deployment of Polar Codes for Mission-Critical Machine-Type Communication Over Wireless Networks,Najib Ahmed Mohammed;Ali Mohammed Mansoor;Rodina Binti Ahmad;Saaidal Razalli Bin Azzuhri,"Mission critical Machine-type Communication, also referred to as Ultra-reliable Low Latency Communication is primarily characterized by communication that provides ultra-high reliability and very low latency to concurrently transmit short commands to a massive number of connected devices. While the reduction in PHY layer overhead and improvement in channel coding techniques are pivotal in reducing latency and improving reliability, the current wireless standards dedicated to support mcMTC rely heavily on adopting the bottom layers of general-purpose wireless standards and customizing only the upper layers. The mcMTC has a significant technical impact on the design of all layers of the communication protocol stack. In this paper, an innovative bottom-up approach has been proposed for mcMTC applications through PHY layer targeted at improving the transmission reliability by implementing ultra-reliable channel coding scheme in the PHY layer of IEEE 802.11a bearing in mind short packet transmission system. To achieve this aim, we analyzed and compared the channel coding performance of convolutional codes, LDPC codes, and polar codes in wireless network on the condition of short data packet transmission. The Viterbi decoding algorithm, logarithmic belief propagation algorithm, and cyclic redundancy check - successive cancellation list decoding algorithm were adopted to CC, LDPC codes, and polar codes, respectively. Consequently, a new PHY layer for mcMTC has been proposed. The reliability of the proposed approach has been validated by simulation in terms of Bit error rate vs. SNR. The simulation results demonstrate that the reliability of IEEE 802.11a standard has been significantly improved to be at PER less 10e-5 with the implementation of polar codes. The results also show that the general-purpose wireless networks are prominent in providing short packet mcMTC with the modification needed. △ Less","6 October, 2021",https://arxiv.org/pdf/2110.02938
Improving Generalization of Deep Reinforcement Learning-based TSP Solvers,Wenbin Ouyang;Yisen Wang;Shaochen Han;Zhejian Jin;Paul Weng,"Recent work applying deep reinforcement learning (DRL) to solve traveling salesman problems (TSP) has shown that DRL-based solvers can be fast and competitive with TSP heuristics for small instances, but do not generalize well to larger instances. In this work, we propose a novel approach named MAGIC that includes a deep learning architecture and a DRL training method. Our architecture, which integrates a multilayer perceptron, a graph neural network, and an attention model, defines a stochastic policy that sequentially generates a TSP solution. Our training method includes several innovations: (1) we interleave DRL policy gradient updates with local search (using a new local search technique), (2) we use a novel simple baseline, and (3) we apply curriculum learning. Finally, we empirically demonstrate that MAGIC is superior to other DRL-based methods on random TSP instances, both in terms of performance and generalizability. Moreover, our method compares favorably against TSP heuristics and other state-of-the-art approach in terms of performance and computational time. △ Less","6 October, 2021",https://arxiv.org/pdf/2110.02843
SSFL: Tackling Label Deficiency in Federated Learning via Personalized Self-Supervision,Chaoyang He;Zhengyu Yang;Erum Mushtaq;Sunwoo Lee;Mahdi Soltanolkotabi;Salman Avestimehr,"Federated Learning (FL) is transforming the ML training ecosystem from a centralized over-the-cloud setting to distributed training over edge devices in order to strengthen data privacy. An essential but rarely studied challenge in FL is label deficiency at the edge. This problem is even more pronounced in FL compared to centralized training due to the fact that FL users are often reluctant to label their private data. Furthermore, due to the heterogeneous nature of the data at edge devices, it is crucial to develop personalized models. In this paper we propose self-supervised federated learning (SSFL), a unified self-supervised and personalized federated learning framework, and a series of algorithms under this framework which work towards addressing these challenges. First, under the SSFL framework, we demonstrate that the standard FedAvg algorithm is compatible with recent breakthroughs in centralized self-supervised learning such as SimSiam networks. Moreover, to deal with data heterogeneity at the edge devices in this framework, we have innovated a series of algorithms that broaden existing supervised personalization algorithms into the setting of self-supervised learning. We further propose a novel personalized federated self-supervised learning algorithm, Per-SSFL, which balances personalization and consensus by carefully regulating the distance between the local and global representations of data. To provide a comprehensive comparative analysis of all proposed algorithms, we also develop a distributed training system and related evaluation protocol for SSFL. Our findings show that the gap of evaluation accuracy between supervised learning and unsupervised learning in FL is both small and reasonable. The performance comparison indicates the representation regularization-based personalization method is able to outperform other variants. △ Less","5 October, 2021",https://arxiv.org/pdf/2110.02470
Fast and Interpretable Consensus Clustering via Minipatch Learning,Luqin Gan;Genevera I. Allen,"Consensus clustering has been widely used in bioinformatics and other applications to improve the accuracy, stability and reliability of clustering results. This approach ensembles cluster co-occurrences from multiple clustering runs on subsampled observations. For application to large-scale bioinformatics data, such as to discover cell types from single-cell sequencing data, for example, consensus clustering has two significant drawbacks: (i) computational inefficiency due to repeatedly applying clustering algorithms, and (ii) lack of interpretability into the important features for differentiating clusters. In this paper, we address these two challenges by developing IMPACC: Interpretable MiniPatch Adaptive Consensus Clustering. Our approach adopts three major innovations. We ensemble cluster co-occurrences from tiny subsets of both observations and features, termed minipatches, thus dramatically reducing computation time. Additionally, we develop adaptive sampling schemes for observations, which result in both improved reliability and computational savings, as well as adaptive sampling schemes of features, which leads to interpretable solutions by quickly learning the most relevant features that differentiate clusters. We study our approach on synthetic data and a variety of real large-scale bioinformatics data sets; results show that our approach not only yields more accurate and interpretable cluster solutions, but it also substantially improves computational efficiency compared to standard consensus clustering approaches. △ Less","18 October, 2021",https://arxiv.org/pdf/2110.02388
OTTR: Off-Road Trajectory Tracking using Reinforcement Learning,Akhil Nagariya;Dileep Kalathil;Srikanth Saripalli,"In this work, we present a novel Reinforcement Learning (RL) algorithm for the off-road trajectory tracking problem. Off-road environments involve varying terrain types and elevations, and it is difficult to model the interaction dynamics of specific off-road vehicles with such a diverse and complex environment. Standard RL policies trained on a simulator will fail to operate in such challenging real-world settings. Instead of using a naive domain randomization approach, we propose an innovative supervised-learning based approach for overcoming the sim-to-real gap problem. Our approach efficiently exploits the limited real-world data available to adapt the baseline RL policy obtained using a simple kinematics simulator. This avoids the need for modeling the diverse and complex interaction of the vehicle with off-road environments. We evaluate the performance of the proposed algorithm using two different off-road vehicles, Warthog and Moose. Compared to the standard ILQR approach, our proposed approach achieves a 30% and 50% reduction in cross track error in Warthog and Moose, respectively, by utilizing only 30 minutes of real-world driving data. △ Less","5 October, 2021",https://arxiv.org/pdf/2110.02332
Geography of Science: Competitiveness and Inequality,Aurelio Patelli;Lorenzo Napolitano;Giulio Cimini;Andrea Gabrielli,"Using ideas and tools of complexity science we design a holistic measure of \textit{Scientific Fitness}, encompassing the scientific knowledge, capabilities and competitiveness of a research system. We characterize the temporal dynamics of Scientific Fitness and R\&D expenditures at the geographical scale of nations, highlighting patterns of similar research systems, and showing how developing nations (China in particular) are quickly catching up the developed ones. Down-scaling the aggregation level of the analysis, we find that even developed nations show a considerable level of inequality in the Scientific Fitness of their internal regions. Further, we assess comparatively how the competitiveness of each geographic region is distributed over the spectrum of research sectors. Overall, the Scientific Fitness represents the first high quality estimation of the scientific strength of nations and regions, opening new policy-making applications for better allocating resources, filling inequality gaps and ultimately promoting innovation. △ Less","3 October, 2021",https://arxiv.org/pdf/2110.01615
Feedback Loops in Open Data Ecosystems,Daniel Rudmark;Magnus Andersson,"Public agencies are increasingly publishing open data to increase transparency and fuel data-driven innovation. For these organizations, maintaining sufficient data quality is key to continuous re-use but also heavily dependent on feedback loops being initiated between data publishers and users. This paper reports from a longitudinal engagement with Scandinavian transportation agencies, where such feedback loops have been successfully established. Based on these experiences, we propose four distinct types of data feedback loops in which both data publishers and re-users play critical roles. △ Less","3 October, 2021",https://arxiv.org/pdf/2110.01023
Simple Recurrent Neural Networks is all we need for clinical events predictions using EHR data,Laila Rasmy;Jie Zhu;Zhiheng Li;Xin Hao;Hong Thoai Tran;Yujia Zhou;Firat Tiryaki;Yang Xiang;Hua Xu;Degui Zhi,"Recently, there is great interest to investigate the application of deep learning models for the prediction of clinical events using electronic health records (EHR) data. In EHR data, a patient's history is often represented as a sequence of visits, and each visit contains multiple events. As a result, deep learning models developed for sequence modeling, like recurrent neural networks (RNNs) are common architecture for EHR-based clinical events predictive models. While a large variety of RNN models were proposed in the literature, it is unclear if complex architecture innovations will offer superior predictive performance. In order to move this field forward, a rigorous evaluation of various methods is needed. In this study, we conducted a thorough benchmark of RNN architectures in modeling EHR data. We used two prediction tasks: the risk for developing heart failure and the risk of early readmission for inpatient hospitalization. We found that simple gated RNN models, including GRUs and LSTMs, often offer competitive results when properly tuned with Bayesian Optimization, which is in line with similar to findings in the natural language processing (NLP) domain. For reproducibility, Our codebase is shared at https://github.com/ZhiGroup/pytorch_ehr. △ Less","3 October, 2021",https://arxiv.org/pdf/2110.00998
Virtual Element based formulations for computational materials micro-mechanics and homogenization,Marco Lo Cascio,"In this thesis, a computational framework for microstructural modelling of transverse behaviour of heterogeneous materials is presented. The context of this research is part of the broad and active field of Computational Micromechanics, which has emerged as an effective tool both to understand the influence of complex microstructures on the macro-mechanical response of engineering materials and to tailor-design innovative materials for specific applications through a proper modification of their microstructure. The computational framework presented in this thesis is based on the Virtual Element Method (VEM), a recently developed numerical technique able to provide robust numerical results even with highly-distorted meshes. The peculiar features of VEM have been exploited to analyse two-dimensional representations of heterogeneous materials microstructures. Ad-hoc polygonal multi-domain meshing strategies have been developed and tested to exploit the discretisation freedom that VEM allows. To further simplify the preprocessing stage of the analysis and reduce the total computational cost, a novel hybrid formulation for analysing multi-domain problems has been developed by combining the Virtual Element Method with the well-known Boundary Element Method (BEM). The hybrid approach has been used to study both composite material transverse behaviour in presence of inclusions with complex geometries and damage and crack propagation in the matrix phase. Numerical results are presented that demonstrate the potential of the developed framework. △ Less","2 October, 2021",https://arxiv.org/pdf/2110.00830
A Theoretical Overview of Neural Contraction Metrics for Learning-based Control with Guaranteed Stability,Hiroyasu Tsukamoto;Soon-Jo Chung;Jean-Jacques Slotine;Chuchu Fan,"This paper presents a theoretical overview of a Neural Contraction Metric (NCM): a neural network model of an optimal contraction metric and corresponding differential Lyapunov function, the existence of which is a necessary and sufficient condition for incremental exponential stability of non-autonomous nonlinear system trajectories. Its innovation lies in providing formal robustness guarantees for learning-based control frameworks, utilizing contraction theory as an analytical tool to study the nonlinear stability of learned systems via convex optimization. In particular, we rigorously show in this paper that, by regarding modeling errors of the learning schemes as external disturbances, the NCM control is capable of obtaining an explicit bound on the distance between a time-varying target trajectory and perturbed solution trajectories, which exponentially decreases with time even under the presence of deterministic and stochastic perturbation. These useful features permit simultaneous synthesis of a contraction metric and associated control law by a neural network, thereby enabling real-time computable and probably robust learning-based control for general control-affine nonlinear systems. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00693
Survey and synthesis of state of the art in driver monitoring,Anaïs Halin;Jacques G. Verly;Marc Van Droogenbroeck,"Road-vehicle accidents are mostly due to human errors, and many such accidents could be avoided by continuously monitoring the driver. Driver monitoring (DM) is a topic of growing interest in the automotive industry, and it will remain relevant for all vehicles that are not fully autonomous, and thus for decades for the average vehicle owner. The present paper focuses on the first step of DM, which consists in characterizing the state of the driver. Since DM will be increasingly linked to driving automation (DA), this paper presents a clear view of the role of DM at each of the six SAE levels of DA. This paper surveys the state of the art of DM, and then synthesizes it, providing a unique, structured, polychotomous view of the many characterization techniques of DM. Informed by the survey, the paper characterizes the driver state along the five main dimensions--called here ""(sub)states""--of drowsiness, mental workload, distraction, emotions, and under the influence. The polychotomous view of DM is presented through a pair of interlocked tables that relate these states to their indicators (e.g., the eye-blink rate) and the sensors that can access each of these indicators (e.g., a camera). The tables factor in not only the effects linked directly to the driver, but also those linked to the (driven) vehicle and the (driving) environment. They show, at a glance, to concerned researchers, equipment providers, and vehicle manufacturers (1) most of the options they have to implement various forms of advanced DM systems, and (2) fruitful areas for further research and innovation. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00472
PhiNets: a scalable backbone for low-power AI at the edge,Francesco Paissan;Alberto Ancilotto;Elisabetta Farella,"In the Internet of Things era, where we see many interconnected and heterogeneous mobile and fixed smart devices, distributing the intelligence from the cloud to the edge has become a necessity. Due to limited computational and communication capabilities, low memory and limited energy budget, bringing artificial intelligence algorithms to peripheral devices, such as the end-nodes of a sensor network, is a challenging task and requires the design of innovative methods. In this work, we present PhiNets, a new scalable backbone optimized for deep-learning-based image processing on resource-constrained platforms. PhiNets are based on inverted residual blocks specifically designed to decouple the computational cost, working memory, and parameter memory, thus exploiting all the available resources. With a YoloV2 detection head and Simple Online and Realtime Tracking, the proposed architecture has achieved the state-of-the-art results in (i) detection on the COCO and VOC2012 benchmarks, and (ii) tracking on the MOT15 benchmark. PhiNets reduce the parameter count of 87% to 93% with respect to previous state-of-the-art models (EfficientNetv1, MobileNetv2) and achieve better performance with lower computational cost. Moreover, we demonstrate our approach on a prototype node based on a STM32H743 microcontroller (MCU) with 2MB of internal Flash and 1MB of RAM and achieve power requirements in the order of 10 mW. The code for the PhiNets is publicly available on GitHub. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00337
A formal model for ledger management systems based on contracts and temporal logic,Paolo Bottoni;Anna Labella;Remo Pareschi,"A key component of blockchain technology is the ledger, viz., a database that, unlike standard databases, keeps in memory the complete history of past transactions as in a notarial archive for the benefit of any future test. In second-generation blockchains such as Ethereum the ledger is coupled with smart contracts, which enable the automation of transactions associated with agreements between the parties of a financial or commercial nature. The coupling of smart contracts and ledgers provides the technological background for very innovative application areas, such as Decentralized Autonomous Organizations (DAOs), Initial Coin Offerings (ICOs) and Decentralized Finance (DeFi), which propelled blockchains beyond cryptocurrencies that were the only focus of first generation blockchains such as the Bitcoin. However, the currently used implementation of smart contracts as arbitrary programming constructs has made them susceptible to dangerous bugs that can be exploited maliciously and has moved their semantics away from that of legal contracts. We propose here to recompose the split and recover the reliability of databases by formalizing a notion of contract modelled as a finite-state automaton with well-defined computational characteristics derived from an encoding in terms of allocations of resources to actors, as an alternative to the approach based on programming. To complete the work, we use temporal logic as the basis for an abstract query language that is effectively suited to the historical nature of the information kept in the ledger. △ Less","30 September, 2021",https://arxiv.org/pdf/2109.15212
The anatomy of social dynamics in escape rooms,Rebeka O. Szabo;Sandeep Chowdhary;David Deritei;Federico Battiston,"From sport and science production to everyday life, higher-level pursuits demand collaboration. Despite an increase in the number of data-driven studies on human behavior, the social dynamics of collaborative problem solving are still largely unexplored with network science and other computational and quantitative tools. Here we introduce escape rooms as a non-interventional and minimally biased social laboratory, which allows us to capture at a high resolution real-time communications in small project teams. Our analysis portrays a nuanced picture of different dimensions of social dynamics. We reveal how socio-demographic characteristics impact problem solving and the importance of prior relationships for enhanced interactions. We extract key conversation rules from motif analysis, and discuss turn-usurping gendered behavior, a phenomenon particularly strong in male dominated teams. We investigate the temporal evolution of signed and group interactions, finding that a minimum level of tense communication might be beneficial for collective problem solving, and revealing differences in the behavior of successful and failed teams. Our work unveils the innovative potential of escape rooms to study teams in their complexity, contributing to a deeper understanding of the micro-dynamics of collaborative team processes. △ Less","30 September, 2021",https://arxiv.org/pdf/2109.15146
Boost-RS: Boosted Embeddings for Recommender Systems and its Application to Enzyme-Substrate Interaction Prediction,Xinmeng Li;Li-ping Liu;Soha Hassoun,"Despite experimental and curation efforts, the extent of enzyme promiscuity on substrates continues to be largely unexplored and under documented. Recommender systems (RS), which are currently unexplored for the enzyme-substrate interaction prediction problem, can be utilized to provide enzyme recommendations for substrates, and vice versa. The performance of Collaborative-Filtering (CF) recommender systems however hinges on the quality of embedding vectors of users and items (enzymes and substrates in our case). Importantly, enhancing CF embeddings with heterogeneous auxiliary data, specially relational data (e.g., hierarchical, pairwise, or groupings), remains a challenge. We propose an innovative general RS framework, termed Boost-RS, that enhances RS performance by ""boosting"" embedding vectors through auxiliary data. Specifically, Boost-RS is trained and dynamically tuned on multiple relevant auxiliary learning tasks Boost-RS utilizes contrastive learning tasks to exploit relational data. To show the efficacy of Boost-RS for the enzyme-substrate prediction interaction problem, we apply the Boost-RS framework to several baseline CF models. We show that each of our auxiliary tasks boosts learning of the embedding vectors, and that contrastive learning using Boost-RS outperforms attribute concatenation and multi-label learning. We also show that Boost-RS outperforms similarity-based models. Ablation studies and visualization of learned representations highlight the importance of using contrastive learning on some of the auxiliary data in boosting the embedding vectors. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.14766
An Energy Efficient Health Monitoring Approach with Wireless Body Area Networks,Seemandhar Jain;Prarthi Jain;Prabhat K. Upadhyay;Jules M. Moualeu;Abhishek Srivastava,"Wireless Body Area Networks (WBANs) comprise a network of sensors subcutaneously implanted or placed near the body surface and facilitate continuous monitoring of health parameters of a patient. Research endeavours involving WBAN are directed towards effective transmission of detected parameters to a Local Processing Unit (LPU, usually a mobile device) and analysis of the parameters at the LPU or a back-end cloud. An important concern in WBAN is the lightweight nature of WBAN nodes and the need to conserve their energy. This is especially true for subcutaneously implanted nodes that cannot be recharged or regularly replaced. Work in energy conservation is mostly aimed at optimising the routing of signals to minimise energy expended. In this paper, a simple yet innovative approach to energy conservation and detection of alarming health status is proposed. Energy conservation is ensured through a two-tier approach wherein the first tier eliminates `uninteresting' health parameter readings at the site of a sensing node and prevents these from being transmitted across the WBAN to the LPU. A reading is categorised as uninteresting if it deviates very slightly from its immediately preceding reading and does not provide new insight on the patient's well being. In addition to this, readings that are faulty and emanate from possible sensor malfunctions are also eliminated. These eliminations are done at the site of the sensor using algorithms that are light enough to effectively function in the extremely resource-constrained environments of the sensor nodes. We notice, through experiments, that this eliminates and thus reduces around 90% of the readings that need to be transmitted to the LPU leading to significant energy savings. Furthermore, the proper functioning of these algorithms in such constrained environments is confirmed and validated over a hardware simulation set up. The second tier of assessment includes a proposed anomaly detection model at the LPU that is capable of identifying anomalies from streaming health parameter readings and indicates an adverse medical condition. In addition to being able to handle streaming data, the model works within the resource-constrained environments of an LPU and eliminates the need of transmitting the data to a back-end cloud, ensuring further energy savings. The anomaly detection capability of the model is validated using data available from the critical care units of hospitals and is shown to be superior to other anomaly detection techniques. △ Less","27 September, 2021",https://arxiv.org/pdf/2109.14546
Secure Multi-Party Computation based Privacy Preserving Data Analysis in Healthcare IoT Systems,Kevser Şahinbaş;Ferhat Ozgur Catak,"Recently, many innovations have been experienced in healthcare by rapidly growing Internet-of-Things (IoT) technology that provides significant developments and facilities in the health sector and improves daily human life. The IoT bridges people, information technology and speed up shopping. For these reasons, IoT technology has started to be used on a large scale. Thanks to the use of IoT technology in health services, chronic disease monitoring, health monitoring, rapid intervention, early diagnosis and treatment, etc. facilitates the delivery of health services. However, the data transferred to the digital environment pose a threat of privacy leakage. Unauthorized persons have used them, and there have been malicious attacks on the health and privacy of individuals. In this study, it is aimed to propose a model to handle the privacy problems based on federated learning. Besides, we apply secure multi party computation. Our proposed model presents an extensive privacy and data analysis and achieve high performance. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14334
Majority Vote in Social Networks: Make Random Friends or Be Stubborn to Overpower Elites,Charlotte Out;Ahad N. Zehmakan,"Consider a graph G, representing a social network. Assume that initially each node is colored either black or white, which corresponds to a positive or negative opinion regarding a consumer product or a technological innovation. In the majority model, in each round all nodes simultaneously update their color to the most frequent color among their connections. Experiments on the graph data from the real world social networks (SNs) suggest that if all nodes in an extremely small set of high-degree nodes, often referred to as the elites, agree on a color, that color becomes the dominant color at the end of the process. We propose two countermeasures that can be adopted by individual nodes relatively easily and guarantee that the elites will not have this disproportionate power to engineer the dominant output color. The first countermeasure essentially requires each node to make some new connections at random while the second one demands the nodes to be more reluctant towards changing their color (opinion). We verify their effectiveness and correctness both theoretically and experimentally. We also investigate the majority model and a variant of it when the initial coloring is random on the real world SNs and several random graph models. In particular, our results on the Erdős-Rényi and regular random graphs confirm or support several theoretical findings or conjectures by the prior work regarding the threshold behavior of the process. Finally, we provide theoretical and experimental evidence for the existence of a poly-logarithmic bound on the expected stabilization time of the majority model. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14265
RelicVR: A Virtual Reality Game for Active Exploration of Archaeological Relics,Yilin Liu;Yiming Lin;Rongkai Shi;Yiming Luo;Hai-Ning Liang,"Digitalization is changing how people visit museums and explore the artifacts they house. Museums, as important educational venues outside classrooms, need to actively explore the application of digital interactive media, including games that can balance entertainment and knowledge acquisition. In this paper, we introduce RelicVR, a virtual reality (VR) game that encourages players to discover artifacts through physical interaction in a game-based approach. Players need to unearth artifacts hidden in a clod enclosure by using available tools and physical movements. The game relies on the dynamic voxel deformation technique to allow players to chip away earth covering the artifacts. We added uncertainty in the exploration process to bring it closer to how archaeological discovery happens in real life. Players do not know the shape or features of the hidden artifact and have to take away the earth gradually but strategically without hitting the artifact itself. From playtesting sessions with eight participants, we found that the uncertainty elements are conducive to their engagement and exploration experience. Overall, RelicVR is an innovative game that can improve players' learning motivation and outcomes of ancient artifacts. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14185
Learning and evaluation without access to schools during COVID-19,Gunnar Stefansson;Anna Helga Jonsdottir,"The tutor-web drilling system is designed for learning so there are typically no limits on the number of attempts at improving performance. This system is used at multiple schools and universities in Iceland and Kenya, mostly for mathematics and statistics. Students earn SmileyCoin, a cryptocurrency, while studying. In Iceland the system has typically been used by students who use their own devices to solve homework assignments during the semester, accessing the Internet-based tutor-web at http://tutor-web.net. These students typically take final exams on paper at the end of the semester. In Kenya the system is a part of a plan to enhance mathematics education using educational technology, organised by the Smiley Charity with the African Maths Initiative. This has been done by donating servers running the tutor-web to schools and tablets to students. Typically these schools do not have Internet access so the cryptocurrency can not be used. Innovative redesign was needed during COVID-19 in spring, 2020, since universities in Iceland were not able to host in-house finals and schools in Kenya were closed so tablets could not be donated directly to students. Remote finals were held in Iceland but the implementation was largely in the hands of the instructors. In Kenya, community libraries remained open and became a place for students to come in to study. Innovations included using the tutor-web as a remote drilling system in place of final exams in a large undergraduate course in statistics and donating tablets to libraries in Kenya. These libraries all have access to the Internet and the students have therefore been given the option to purchase the tablet using their SmileyCoin. This paper describes these implementations and how this unintended experiment will likely affect the future development and use of the tutor-web in both countries. △ Less","19 August, 2021",https://arxiv.org/pdf/2109.13658
Lithium-ion Battery State of Health Estimation based on Cycle Synchronization using Dynamic Time Warping,Kate Qi Zhou;Yan Qin;Billy Pik Lik Lau;Chau Yuen;Stefan Adams,"The state of health (SOH) estimation plays an essential role in battery-powered applications to avoid unexpected breakdowns due to battery capacity fading. However, few studies have paid attention to the problem of uneven length of degrading cycles, simply employing manual operation or leaving to the automatic processing mechanism of advanced machine learning models, like long short-term memory (LSTM). As a result, this causes information loss and caps the full capability of the data-driven SOH estimation models. To address this challenge, this paper proposes an innovative cycle synchronization way to change the existing coordinate system using dynamic time warping, not only enabling the equal length inputs of the estimation model but also preserving all information. By exploiting the time information of the time series, the proposed method embeds the time index and the original measurements into a novel indicator to reflect the battery degradation status, which could have the same length over cycles. Adopting the LSTM as the basic estimation model, the cycle synchronization-based SOH model could significantly improve the prediction accuracy by more than 30% compared to the traditional LSTM. △ Less","27 September, 2021",https://arxiv.org/pdf/2109.13448
Rabia: Simplifying State-Machine Replication Through Randomization,Haochen Pan;Jesse Tuglu;Neo Zhou;Tianshu Wang;Yicheng Shen;Xiong Zheng;Joseph Tassarotti;Lewis Tseng;Roberto Palmieri,"We introduce Rabia, a simple and high performance framework for implementing state-machine replication (SMR) within a datacenter. The main innovation of Rabia is in using randomization to simplify the design. Rabia provides the following two features: (i) It does not need any fail-over protocol and supports trivial auxiliary protocols like log compaction, snapshotting, and reconfiguration, components that are often considered the most challenging when developing SMR systems; and (ii) It provides high performance, up to 1.5x higher throughput than the closest competitor (i.e., EPaxos) in a favorable setup (same availability zone with three replicas) and is comparable with a larger number of replicas or when deployed in multiple availability zones. △ Less","26 September, 2021",https://arxiv.org/pdf/2109.12616
MIIDL: a Python package for microbial biomarkers identification powered by interpretable deep learning,Jian Jiang,"Detecting microbial biomarkers used to predict disease phenotypes and clinical outcomes is crucial for disease early-stage screening and diagnosis. Most methods for biomarker identification are linear-based, which is very limited as biological processes are rarely fully linear. The introduction of machine learning to this field tends to bring a promising solution. However, identifying microbial biomarkers in an interpretable, data-driven and robust manner remains challenging. We present MIIDL, a Python package for the identification of microbial biomarkers based on interpretable deep learning. MIIDL innovatively applies convolutional neural networks, a variety of interpretability algorithms and plenty of pre-processing methods to provide a one-stop and robust pipeline for microbial biomarkers identification from high-dimensional and sparse data sets. △ Less","24 September, 2021",https://arxiv.org/pdf/2109.12204
Universal Payment Channels: An Interoperability Platform for Digital Currencies,Mihai Christodorescu;Erin English;Wanyun Catherine Gu;David Kreissman;Ranjit Kumaresan;Mohsen Minaei;Srinivasan Raghuraman;Cuy Sheffield;Arjuna Wijeyekoon;Mahdi Zamani,"With the innovation of distributed ledger technology (DLT), often known as blockchain technology, there has been significant growth of digital tokens in the form of cryptocurrencies, stablecoins, and central bank digital currencies. As the number of DLT networks increases, each with varying design characteristics, the likelihood that transacting parties are on the same network decreases. Thus, it is crucial to facilitate payments that are universal across networks, scalable to massive loads, and highly available. We envision a future payment network that may be built on top of DLT networks without being subject to their limitations on interoperability, scalability, and availability faced by DLT payment solutions today. Specifically, we propose a hub-and-spoke payment route, referred to here as Universal Payment Channels (UPC), that can be used to support digital token transfers of funds across different networks through payment channels. We further discuss the potential use cases of the UPC technology to support, and not complicate, an already robust digital payment ecosystem. Finally, through the paper, we share some future directions of the UPC technology. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.12194
What Truly Matters? Using Linguistic Cues for Analyzing the #BlackLivesMatter Movement and its Counter Protests: 2013 to 2020,Jamell Dacon;Jiliang Tang,"Since the fatal shooting of 17-year old Black teenager Trayvon Martin in February 2012 by a White neighborhood watchman, George Zimmerman in Sanford, Florida, there has been a significant increase in digital activism addressing police-brutality related and racially-motivated incidents in the United States. In this work, we administer an innovative study of digital activism by exploiting social media as an authoritative tool to examine and analyze the linguistic cues and thematic relationships in these three mediums. We conduct a multi-level text analysis on 36,984,559 tweets to investigate users' behaviors to examine the language used and understand the impact of digital activism on social media within each social movement on a sentence-level, word-level, and topic-level. Our results show that excessive use of racially-related or prejudicial hashtags were used by the counter protests which portray potential discriminatory tendencies. Consequently, our findings highlight that social activism done by Black Lives Matter activists does not diverge from the social issues and topics involving police-brutality related and racially-motivated killings of Black individuals due to the shape of its topical graph that topics and conversations encircling the largest component directly relate to the topic of Black Lives Matter. Finally, we see that both Blue Lives Matter and All Lives Matter movements depict a different directive, as the topics of Blue Lives Matter or All Lives Matter do not reside in the center. These findings suggest that topics and conversations within each social movement are skewed, random or possessed racially-related undertones, and thus, deviating from the prominent social injustice issues. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.12192
From Bench to Bedside: The First Live Robotic Surgery on the dVRK to Enable Remote Telesurgery with Motion Scaling,Florian Richter;Emily K. Funk;Won Seo Park;Ryan K. Orosco;Michael C. Yip,"Innovations from surgical robotic research rarely translates to live surgery due to the significant difference between the lab and a live environment. Live environments require considerations that are often overlooked during early stages of research such as surgical staff, surgical procedure, and the challenges of working with live tissue. One such example is the da Vinci Research Kit (dVRK) which is used by over 40 robotics research groups and represents an open-sourced version of the da Vinci Surgical System. Despite dVRK being available for nearly a decade and the ideal candidate for translating research to practice on over 5,000 da Vinci Systems used in hospitals around the world, not one live surgery has been conducted with it. In this paper, we address the challenges, considerations, and solutions for translating surgical robotic research from bench-to-bedside. This is explained from the perspective of a remote telesurgery scenario where motion scaling solutions previously experimented in a lab setting are translated to a live pig surgery. This study presents results from the first ever use of a dVRK in a live animal and discusses how the surgical robotics community can approach translating their research to practice. △ Less","24 September, 2021",https://arxiv.org/pdf/2109.12177
Deep Neural Networks for Blind Image Quality Assessment: Addressing the Data Challenge,Shahrukh Athar;Zhongling Wang;Zhou Wang,"The enormous space and diversity of natural images is usually represented by a few small-scale human-rated image quality assessment (IQA) datasets. This casts great challenges to deep neural network (DNN) based blind IQA (BIQA), which requires large-scale training data that is representative of the natural image distribution. It is extremely difficult to create human-rated IQA datasets composed of millions of images due to constraints of subjective testing. While a number of efforts have focused on design innovations to enhance the performance of DNN based BIQA, attempts to address the scarcity of labeled IQA data remain surprisingly missing. To address this data challenge, we construct so far the largest IQA database, namely Waterloo Exploration-II, which contains 3,570 pristine reference and around 3.45 million singly and multiply distorted images. Since subjective testing for such a large dataset is nearly impossible, we develop a novel mechanism that synthetically assigns perceptual quality labels to the distorted images. We construct a DNN-based BIQA model called EONSS, train it on Waterloo Exploration-II, and test it on nine subject-rated IQA datasets, without any retraining or fine-tuning. The results show that with a straightforward DNN architecture, EONSS is able to outperform the very state-of-the-art in BIQA, both in terms of quality prediction performance and execution speed. This study strongly supports the view that the quantity and quality of meaningfully annotated training data, rather than a sophisticated network architecture or training strategy, is the dominating factor that determines the performance of DNN-based BIQA models. (Note: Since this is an ongoing project, the final versions of Waterloo Exploration-II database, quality annotations, and EONSS, will be made publicly available in the future when it culminates.) △ Less","24 September, 2021",https://arxiv.org/pdf/2109.12161
Towards a Governance Framework for Brain Data,Marcello Ienca;Joseph J. Fins;Ralf J. Jox;Fabrice Jotterand;Silja Voeneky;Roberto Andorno;Tonio Ball;Claude Castelluccia;Ricardo Chavarriaga;Hervé Chneiweiss;Agata Ferretti;Orsolya Friedrich;Samia Hurst;Grischa Merkel;Fruzsina Molnar-Gabor;Jean-Marc Rickli;James Scheibner;Effy Vayena;Rafael Yuste;Philipp Kellmeyer,"The increasing availability of brain data within and outside the biomedical field, combined with the application of artificial intelligence (AI) to brain data analysis, poses a challenge for ethics and governance. We identify distinctive ethical implications of brain data acquisition and processing, and outline a multi-level governance framework. This framework is aimed at maximizing the benefits of facilitated brain data collection and further processing for science and medicine whilst minimizing risks and preventing harmful use. The framework consists of four primary areas of regulatory intervention: binding regulation, ethics and soft law, responsible innovation, and human rights. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.11960
A Domain-Specific Language for Modeling and Analyzing Solution Spaces for Technology Roadmapping,Alexander Breckel;Jakob Pietron;Katharina Juhnke;Florian Sihler;Matthias Tichy,"The introduction of major innovations in industry requires a collaboration across the whole value chain. A common way to organize such a collaboration is the use of technology roadmaps, which act as an industry-wide long-term planning tool. Technology roadmaps are used to identify industry needs, estimate the availability of technological solutions, and identify the need for innovation in the future. Roadmaps are inherently both time-dependent and based on uncertain values, i.e., properties and structural components can change over time. Furthermore, roadmaps have to reason about alternative solutions as well as their key performance indicators. Current approaches for model-based engineering do not inherently support these aspects. We present a novel model-based approach treating those aspects as first-class citizens. To address the problem of missing support for time in the context of roadmap modeling, we introduce the concepts of a common global time, time-dependent properties, and time-dependent availability. This includes requirements, properties, and the structure of the model or its components as well. Furthermore, we support the specification and analysis of key performance indicators for alternative solutions. These concepts result in a continuous range of various valid models over time instead of a single valid model at a certain point of time. We present a graphical user interface to enable the user to efficiently create and analyze those models. We further show the semantics of the resulting model by a translation into a set of global constraints as well as how we solve the resulting constraint system. We report on the evaluation of these concepts and the Iris tool with domain experts from different companies in the automotive value chain based on the industrial case of a smart sensing electrical fuse. △ Less","24 September, 2021",https://arxiv.org/pdf/2109.11816
Predicting pigging operations in oil pipelines,Riccardo Angelo Giro;Giancarlo Bernasconi;Giuseppe Giunta;Simone Cesari,"This paper presents an innovative machine learning methodology that leverages on long-term vibroacoustic measurements to perform automated predictions of the needed pigging operations in crude oil trunklines. Historical pressure signals have been collected by Eni (e-vpms monitoring system) for two years on discrete points at a relative distance of 30-35 km along an oil pipeline (100 km length, 16 inch diameter pipes) located in Northern Italy. In order to speed up the activity and to check the operation logs, a tool has been implemented to automatically highlight the historical pig operations performed on the line. Such a tool is capable of detecting, in the observed pressure measurements, the acoustic noise generated by the travelling pig. All the data sets have been reanalyzed and exploited by using field data validations to guide a decision tree regressor (DTR). Several statistical indicators, computed from pressure head loss between line segments, are fed to the DTR, which automatically outputs probability values indicating the possible need for pigging the pipeline. The procedure is applied to the vibroacoustic signals of each pair of consecutive monitoring stations, such that the proposed predictive maintenance strategy is capable of tracking the conditions of individual pipeline sections, thus determining which portion of the conduit is subject to the highest occlusion levels in order to optimize the clean-up operations. Prediction accuracy is assessed by evaluating the typical metrics used in statistical analysis of regression problems, such as the Root Mean Squared Error (RMSE). △ Less","24 September, 2021",https://arxiv.org/pdf/2109.11812
WebFlow: Scalable and Decentralized Routing for Payment Channel Networks with High Resource Utilization,Xiaoxue Zhang;Shouqian Shi;Chen Qian,"Payment channel networks (PCNs) have been designed and utilized to address the scalability challenge and throughput limitation of blockchains. Routing is a core problem of PCNs. An ideal PCN routing method needs to achieve 1) high scalability that can maintain low per-node memory and communication cost for large PCNs, 2) high resource utilization of payment channels, and 3) the privacy of users. However, none of the existing PCN systems consider all these requirements. In this work, we propose WebFlow, a distributed routing solution for PCNs, which only requires each user to maintain localized information and can be used for massive-scale networks with high resource utilization. We make use of two distributed data structures: multi-hop Delaunay triangulation (MDT) originally proposed for wireless networks and our innovation called distributed Voronoi diagram. We propose new protocols to generate a virtual Euclidean space in order to apply MDT to PCNs and use the distributed Voronoi diagram to enhance routing privacy. We conduct extensive simulations and prototype implementation to further evaluate WebFlow. The results using real and synthetic PCN topologies and transaction traces show that WebFlow can achieve extremely low per-node overhead and a high success rate compared to existing methods. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11665
Learning Dynamics from Noisy Measurements using Deep Learning with a Runge-Kutta Constraint,Pawan Goyal;Peter Benner,"Measurement noise is an integral part while collecting data of a physical process. Thus, noise removal is a necessary step to draw conclusions from these data, and it often becomes quite essential to construct dynamical models using these data. We discuss a methodology to learn differential equation(s) using noisy and sparsely sampled measurements. In our methodology, the main innovation can be seen in of integration of deep neural networks with a classical numerical integration method. Precisely, we aim at learning a neural network that implicitly represents the data and an additional neural network that models the vector fields of the dependent variables. We combine these two networks by enforcing the constraint that the data at the next time-steps can be given by following a numerical integration scheme such as the fourth-order Runge-Kutta scheme. The proposed framework to learn a model predicting the vector field is highly effective under noisy measurements. The approach can handle scenarios where dependent variables are not available at the same temporal grid. We demonstrate the effectiveness of the proposed method to learning models using data obtained from various differential equations. The proposed approach provides a promising methodology to learn dynamic models, where the first-principle understanding remains opaque. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11446
Deep Learning for Ultrasound Beamforming,Ruud JG van Sloun;Jong Chul Ye;Yonina C Eldar,"Diagnostic imaging plays a critical role in healthcare, serving as a fundamental asset for timely diagnosis, disease staging and management as well as for treatment choice, planning, guidance, and follow-up. Among the diagnostic imaging options, ultrasound imaging is uniquely positioned, being a highly cost-effective modality that offers the clinician an unmatched and invaluable level of interaction, enabled by its real-time nature. Ultrasound probes are becoming increasingly compact and portable, with the market demand for low-cost pocket-sized and (in-body) miniaturized devices expanding. At the same time, there is a strong trend towards 3D imaging and the use of high-frame-rate imaging schemes; both accompanied by dramatically increasing data rates that pose a heavy burden on the probe-system communication and subsequent image reconstruction algorithms. With the demand for high-quality image reconstruction and signal extraction from less (e.g unfocused or parallel) transmissions that facilitate fast imaging, and a push towards compact probes, modern ultrasound imaging leans heavily on innovations in powerful digital receive channel processing. Beamforming, the process of mapping received ultrasound echoes to the spatial image domain, naturally lies at the heart of the ultrasound image formation chain. In this chapter on Deep Learning for Ultrasound Beamforming, we discuss why and when deep learning methods can play a compelling role in the digital beamforming pipeline, and then show how these data-driven systems can be leveraged for improved ultrasound image reconstruction. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11431
Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing,Kamal Raj Kanakarajan;Bhuvana Kundumani;Malaikannan Sankarasubbu,"Recent progress in the Natural Language Processing domain has given us several State-of-the-Art (SOTA) pretrained models which can be finetuned for specific tasks. These large models with billions of parameters trained on numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In this paper, we discuss the need for a benchmark for cost and time effective smaller models trained on a single GPU. This will enable researchers with resource constraints experiment with novel and innovative ideas on tokenization, pretraining tasks, architecture, fine tuning methods etc. We set up Small-Bench NLP, a benchmark for small efficient neural language models trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks on the publicly available GLUE datasets and a leaderboard to track the progress of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture achieves an average score of 81.53 which is comparable to that of BERT-Base's 82.20 (110M parameters). Our models, code and leaderboard are available at https://github.com/smallbenchnlp △ Less","23 September, 2021",https://arxiv.org/pdf/2109.10847
Privacy-preserving Credit Scoring via Functional Encryption,Lorenzo Andolfo;Luigi Coppolino;Salvatore D'Antonio;Giovanni Mazzeo;Luigi Romano;Matthew Ficke;Arne Hollum;Darshan Vaydia,"The majority of financial organizations managing confidential data are aware of security threats and leverage widely accepted solutions (e.g., storage encryption, transport-level encryption, intrusion detection systems) to prevent or detect attacks. Yet these hardening measures do little to face even worse threats posed on data-in-use. Solutions such as Homomorphic Encryption (HE) and hardware-assisted Trusted Execution Environment (TEE) are nowadays among the preferred approaches for mitigating this type of threat. However, given the high-performance overhead of HE, financial institutions -- whose processing rate requirements are stringent -- are more oriented towards TEE-based solutions. The X-Margin Inc. company, for example, offers secure financial computations by combining the Intel SGX TEE technology and HE-based Zero-Knowledge Proofs, which shield customers' data-in-use even against malicious insiders, i.e., users having privileged access to the system. Despite such a solution offers strong security guarantees, it is constrained by having to trust Intel and by the SGX hardware extension availability. In this paper, we evaluate a new frontier for X-Margin, i.e., performing privacy-preserving credit risk scoring via an emerging cryptographic scheme: Functional Encryption (FE), which allows a user to only learn a function of the encrypted data. We describe how the X-Margin application can benefit from this innovative approach and -- most importantly -- evaluate its performance impact. △ Less","22 September, 2021",https://arxiv.org/pdf/2109.10606
An Audio Synthesis Framework Derived from Industrial Process Control,Ashwin Pillay,"Since its conception, digital synthesis has significantly influenced the advancement of music, leading to new genres and production styles. Through existing synthesis techniques, one can recreate naturally occurring sounds as well as generate innovative artificial timbres. However, research in audio technology continues to pursue new methods of synthesizing sounds, keeping the transformation of music constant. This research attempts to formulate the framework of a new synthesis technique by redefining the popular Proportional-Integral-Derivative (PID) algorithm used in feedback-based process control. The framework is then implemented as a Python application to study the available control parameters and their effect on the synthesized output. Further, applications of this technique as an audio signal and LFO generator, including its potentiality as an alternative to FM and Wavetable synthesis techniques, are studied in detail. The research concludes by highlighting some of the imperfections in the current framework and the possible research directions to be considered to address them. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.10455
"Social, Environmental, and Technical: Factors at Play in the Current Use and Future Design of Small-Group Captioning",Emma J. McDonnell;Ping Liu;Steven M. Goodman;Raja Kushalnagar;Jon E. Froehlich;Leah Findlater,"Real-time captioning is a critical accessibility tool for many d/Deaf and hard of hearing (DHH) people. While the vast majority of captioning work has focused on formal settings and technical innovations, in contrast, we investigate captioning for informal, interactive small-group conversations, which have a high degree of spontaneity and foster dynamic social interactions. This paper reports on semi-structured interviews and design probe activities we conducted with 15 DHH participants to understand their use of existing real-time captioning services and future design preferences for both in-person and remote small-group communication. We found that our participants' experiences of captioned small-group conversations are shaped by social, environmental, and technical considerations (e.g., interlocutors' pre-established relationships, the type of captioning displays available, and how far captions lag behind speech). When considering future captioning tools, participants were interested in greater feedback on non-speech elements of conversation (e.g., speaker identity, speech rate, volume) both for their personal use and to guide hearing interlocutors toward more accessible communication. We contribute a qualitative account of DHH people's real-time captioning experiences during small-group conversation and future design considerations to better support the groups being captioned, both in person and online. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.10412
3-of-3 Multisignature Approach for Enabling Lightning Network Micro-payments on IoT Devices,Ahmet Kurt;Suat Mercan;Enes Erdin;Kemal Akkaya,"Bitcoin's success as a cryptocurrency enabled it to penetrate into many daily life transactions. Its problems regarding the transaction fees and long validation times are addressed through an innovative concept called the Lightning Network (LN) which works on top of Bitcoin by leveraging off-chain transactions. This made Bitcoin an attractive micro-payment solution that can also be used within certain IoT applications (e.g., toll payments) since it eliminates the need for traditional centralized payment systems. Nevertheless, it is not possible to run LN and Bitcoin on resource-constrained IoT devices due to their storage, memory, and processing requirements. Therefore, in this paper, we propose an efficient and secure protocol that enables an IoT device to use LN's functions through a gateway LN node even if it is not trusted. The idea is to involve the IoT device only in signing operations, which is possible by replacing LN's original 2-of-2 multisignature channels with 3-of-3 multisignature channels. Once the gateway is delegated to open a channel for the IoT device in a secure manner, our protocol enforces the gateway to request the IoT device's cryptographic signature for all further operations on the channel such as sending payments or closing the channel. LN's Bitcoin transactions are revised to incorporate the 3-of-3 multisignature channels. In addition, we propose other changes to protect the IoT device's funds from getting stolen in possible revoked state broadcast attempts. We evaluated the proposed protocol using a Raspberry Pi considering a toll payment scenario. Our results show that timely payments can be sent and the computational and communication delays associated with the protocol are negligible. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.09950
MESSFN : a Multi-level and Enhanced Spectral-Spatial Fusion Network for Pan-sharpening,Yuan Yuan;Yi Sun;Yuanlin Zhang,"Dominant pan-sharpening frameworks simply concatenate the MS stream and the PAN stream once at a specific level. This way of fusion neglects the multi-level spectral-spatial correlation between the two streams, which is vital to improving the fusion performance. In consideration of this, we propose a Multi-level and Enhanced Spectral-Spatial Fusion Network (MESSFN) with the following innovations: First, to fully exploit and strengthen the above correlation, a Hierarchical Multi-level Fusion Architecture (HMFA) is carefully designed. A novel Spectral-Spatial (SS) stream is established to hierarchically derive and fuse the multi-level prior spectral and spatial expertise from the MS stream and the PAN stream. This helps the SS stream master a joint spectral-spatial representation in the hierarchical network for better modeling the fusion relationship. Second, to provide superior expertise, consequently, based on the intrinsic characteristics of the MS image and the PAN image, two feature extraction blocks are specially developed. In the MS stream, a Residual Spectral Attention Block (RSAB) is proposed to mine the potential spectral correlations between different spectra of the MS image through adjacent cross-spectrum interaction. While in the PAN stream, a Residual Multi-scale Spatial Attention Block (RMSAB) is proposed to capture multi-scale information and reconstruct precise high-frequency details from the PAN image through an improved spatial attention-based inception structure. The spectral and spatial feature representations are enhanced. Extensive experiments on two datasets demonstrate that the proposed network is competitive with or better than state-of-the-art methods. Our code can be found in github. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09937
Survey: Transformer based Video-Language Pre-training,Ludan Ruan;Qin Jin,"Inspired by the success of transformer-based pre-training methods on natural language tasks and further computer vision tasks, researchers have begun to apply transformer to video processing. This survey aims to give a comprehensive overview on transformer-based pre-training methods for Video-Language learning. We first briefly introduce the transformer tructure as the background knowledge, including attention mechanism, position encoding etc. We then describe the typical paradigm of pre-training & fine-tuning on Video-Language processing in terms of proxy tasks, downstream tasks and commonly used video datasets. Next, we categorize transformer models into Single-Stream and Multi-Stream structures, highlight their innovations and compare their performances. Finally, we analyze and discuss the current challenges and possible future research directions for Video-Language pre-training. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09920
The Vision and the Perspective of Digital Tourism,Olga Kononova;Dmitry Prokudin;Julia Ryabysko,"The dynamics of the modern information society changes the usual areas of human activity, generates various innovations based on the widespread use of Information and Communication Technologies (ICTs). Virtually, every activity today is technology related. In these conditions, scientific activity is also changing. Digitalization processes act as integrative to various scientific directions, which form the base for interdisciplinary scientific research. The study of their formation is an important scientific task aimed at predicting the development of both science and society as a whole. In this study, based on the integrated use of ICTs, we consider methods of the terminology base analysis in various interdisciplinary research directions on the instance of tourism in the digital age. The development of scientific interest in the area of digital tourism in Russian and global scientific discourses is also compared. The purpose of this paper is to proof the relevance of scientific study in the field of tourism digitalization, to identify the generic directions and trends of digital tourism, and to specify technologies for the implementation of digital tourism using the case study of St. Petersburg. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09795
MeetDot: Videoconferencing with Live Translation Captions,Arkady Arkhangorodsky;Christopher Chu;Scot Fang;Yiqi Huang;Denglin Jiang;Ajay Nagesh;Boliang Zhang;Kevin Knight,"We present MeetDot, a videoconferencing system with live translation captions overlaid on screen. The system aims to facilitate conversation between people who speak different languages, thereby reducing communication barriers between multilingual participants. Currently, our system supports speech and captions in 4 languages and combines automatic speech recognition (ASR) and machine translation (MT) in a cascade. We use the re-translation strategy to translate the streamed speech, resulting in caption flicker. Additionally, our system has very strict latency requirements to have acceptable call quality. We implement several features to enhance user experience and reduce their cognitive load, such as smooth scrolling captions and reducing caption flicker. The modular architecture allows us to integrate different ASR and MT services in our backend. Our system provides an integrated evaluation suite to optimize key intrinsic evaluation metrics such as accuracy, latency and erasure. Finally, we present an innovative cross-lingual word-guessing game as an extrinsic evaluation metric to measure end-to-end system performance. We plan to make our system open-source for research purposes. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09577
A Deep Learning-based Penetration Testing Framework for Vulnerability Identification in Internet of Things Environments,Nickolaos Koroniotis;Nour Moustafa;Benjamin Turnbull;Francesco Schiliro;Praveen Gauravaram;Helge Janicke,"The Internet of Things (IoT) paradigm has displayed tremendous growth in recent years, resulting in innovations like Industry 4.0 and smart environments that provide improvements to efficiency, management of assets and facilitate intelligent decision making. However, these benefits are offset by considerable cybersecurity concerns that arise due to inherent vulnerabilities, which hinder IoT-based systems' Confidentiality, Integrity, and Availability. Security vulnerabilities can be detected through the application of penetration testing, and specifically, a subset of the information-gathering stage, known as vulnerability identification. Yet, existing penetration testing solutions can not discover zero-day vulnerabilities from IoT environments, due to the diversity of generated data, hardware constraints, and environmental complexity. Thus, it is imperative to develop effective penetration testing solutions for the detection of vulnerabilities in smart IoT environments. In this paper, we propose a deep learning-based penetration testing framework, namely Long Short-Term Memory Recurrent Neural Network-Enabled Vulnerability Identification (LSTM-EVI). We utilize this framework through a novel cybersecurity-oriented testbed, which is a smart airport-based testbed comprised of both physical and virtual elements. The framework was evaluated using this testbed and on real-time data sources. Our results revealed that the proposed framework achieves about 99% detection accuracy for scanning attacks, outperforming other four peer techniques. △ Less","19 September, 2021",https://arxiv.org/pdf/2109.09259
Proteome-informed machine learning studies of cocaine addiction,Kaifu Gao;Dong Chen;Alfred J Robison;Guo-Wei Wei,"Cocaine addiction accounts for a large portion of substance use disorders and threatens millions of lives worldwide. There is an urgent need to come up with efficient anti-cocaine addiction drugs. Unfortunately, no medications have been approved by the Food and Drug Administration (FDA), despite the extensive effort in the past few decades. The main challenge is the intricate molecular mechanisms of cocaine addiction, involving synergistic interactions among proteins upstream and downstream of dopamine transporter (DAT) functions impacted by cocaine. However, traditional in vivo or in vitro experiments can not address the roles of so many proteins, highlighting the need for innovative strategies in the field. We propose a proteome-informed machine learning/deep learning (ML/DL) platform to discover nearly optimal anti-cocaine addiction lead compounds. We construct and analyze proteomic protein-protein interaction (PPI) networks for cocaine dependence to identify 141 involved drug targets and represent over 60,000 associated drug candidates or experimental drugs in the latent space using an autoencoder (EA) model trained from over 104 million molecules. We build 32 ML models for cross-target analysis of these drug candidates for side effects and repurposing potential. We further screen the absorption, distribution, metabolism, excretion, and toxicity (ADMET) properties of these candidates. Our platform reveals that essentially all of the existing drug candidates, including dozens of experimental drugs, fail to pass our cross-target and ADMET screenings. Nonetheless, we have identified two nearly optimal leads for further optimization. △ Less","17 September, 2021",https://arxiv.org/pdf/2109.08718
Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification,Wei Huang;Chen Liu;Yihua Zhao;Xinyun Yang;Zhaoming Pan;Zhimin Zhang;Guiquan Liu,"Hierarchical Text Classification (HTC), which aims to predict text labels organized in hierarchical space, is a significant task lacking in investigation in natural language processing. Existing methods usually encode the entire hierarchical structure and fail to construct a robust label-dependent model, making it hard to make accurate predictions on sparse lower-level labels and achieving low Macro-F1. In this paper, we propose a novel PAMM-HiA-T5 model for HTC: a hierarchy-aware T5 model with path-adaptive mask mechanism that not only builds the knowledge of upper-level labels into low-level ones but also introduces path dependency information in label prediction. Specifically, we generate a multi-level sequential label structure to exploit hierarchical dependency across different levels with Breadth-First Search (BFS) and T5 model. To further improve label dependency prediction within each path, we then propose an original path-adaptive mask mechanism (PAMM) to identify the label's path information, eliminating sources of noises from other paths. Comprehensive experiments on three benchmark datasets show that our novel PAMM-HiA-T5 model greatly outperforms all state-of-the-art HTC approaches especially in Macro-F1. The ablation studies show that the improvements mainly come from our innovative approach instead of T5. △ Less","17 September, 2021",https://arxiv.org/pdf/2109.08585
GLASS: Towards Secure and Decentralized eGovernance Services using IPFS,Christos Chrysoulas;Amanda Thomson;Nikolaos Pitropakis;Pavlos Papadopoulos;Owen Lo;William J. Buchanan;George Domalis;Nikos Karacapilidis;Dimitris Tsakalidis;Dimitris Tsolis,"The continuously advancing digitization has provided answers to the bureaucratic problems faced by eGovernance services. This innovation led them to an era of automation it has broadened the attack surface and made them a popular target for cyber attacks. eGovernance services utilize internet, which is currently a location addressed system where whoever controls the location controls not only the content itself, but the integrity of that content, and the access to that content. We propose GLASS, a decentralised solution which combines the InterPlanetary File System (IPFS) with Distributed Ledger technology and Smart Contracts to secure EGovernance services. We also create a testbed environment where we measure the IPFS performance. △ Less","17 September, 2021",https://arxiv.org/pdf/2109.08566
Formalisation of Action with Durations in Answer Set Programming,Etienne Tignon,"In this paper, I will discuss the work I am currently doing as a Ph.D. student at the University of Potsdam, under the tutoring of T. Schaub. I'm currently looking into action description in ASP. More precisely, my goal is to explore how to represent actions with durations in ASP, in different contexts. Right now, I'm focused on Multi-Agent Path Finding (MAPF), looking at how to represent speeds for different agents and contexts. Before tackling duration, I wanted to explore and compare different representations of action taking in ASP. For this, I started comparing different simple encodings tackling the MAPF problem. Even in simple code, choices and assumptions have been made in their creations. The objective of my work is to present the consequences of those design decisions in terms of performance and knowledge representation. As far as I know, there is no current research on this topic. Besides that, I'm also exploring different ways to represent duration and to solve related problems. I planed to compare them the same way I described before. I also want this to help me find innovative and effective ways to solve problems with duration. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.08305
Neural Architecture Search in operational context: a remote sensing case-study,Anthony Cazasnoves;Pierre-Antoine Ganaye;Kévin Sanchis;Tugdual Ceillier,"Deep learning has become in recent years a cornerstone tool fueling key innovations in the industry, such as autonomous driving. To attain good performances, the neural network architecture used for a given application must be chosen with care. These architectures are often handcrafted and therefore prone to human biases and sub-optimal selection. Neural Architecture Search (NAS) is a framework introduced to mitigate such risks by jointly optimizing the network architectures and its weights. Albeit its novelty, it was applied on complex tasks with significant results - e.g. semantic image segmentation. In this technical paper, we aim to evaluate its ability to tackle a challenging operational task: semantic segmentation of objects of interest in satellite imagery. Designing a NAS framework is not trivial and has strong dependencies to hardware constraints. We therefore motivate our NAS approach selection and provide corresponding implementation details. We also present novel ideas to carry out other such use-case studies. △ Less","15 September, 2021",https://arxiv.org/pdf/2109.08028
"ObjectFolder: A Dataset of Objects with Implicit Visual, Auditory, and Tactile Representations",Ruohan Gao;Yen-Yu Chang;Shivani Mall;Li Fei-Fei;Jiajun Wu,"Multisensory object-centric perception, reasoning, and interaction have been a key research topic in recent years. However, the progress in these directions is limited by the small set of objects available -- synthetic objects are not realistic enough and are mostly centered around geometry, while real object datasets such as YCB are often practically challenging and unstable to acquire due to international shipping, inventory, and financial cost. We present ObjectFolder, a dataset of 100 virtualized objects that addresses both challenges with two key innovations. First, ObjectFolder encodes the visual, auditory, and tactile sensory data for all objects, enabling a number of multisensory object recognition tasks, beyond existing datasets that focus purely on object geometry. Second, ObjectFolder employs a uniform, object-centric, and implicit representation for each object's visual textures, acoustic simulations, and tactile readings, making the dataset flexible to use and easy to share. We demonstrate the usefulness of our dataset as a testbed for multisensory perception and control by evaluating it on a variety of benchmark tasks, including instance recognition, cross-sensory retrieval, 3D reconstruction, and robotic grasping. △ Less","7 November, 2021",https://arxiv.org/pdf/2109.07991
Alquist 4.0: Towards Social Intelligence Using Generative Models and Dialogue Personalization,Jakub Konrád;Jan Pichl;Petr Marek;Petr Lorenc;Van Duy Ta;Ondřej Kobza;Lenka Hýlová;Jan Šedivý,"The open domain-dialogue system Alquist has a goal to conduct a coherent and engaging conversation that can be considered as one of the benchmarks of social intelligence. The fourth version of the system, developed within the Alexa Prize Socialbot Grand Challenge 4, brings two main innovations. The first addresses coherence, and the second addresses the engagingness of the conversation. For innovations regarding coherence, we propose a novel hybrid approach combining hand-designed responses and a generative model. The proposed approach utilizes hand-designed dialogues, out-of-domain detection, and a neural response generator. Hand-designed dialogues walk the user through high-quality conversational flows. The out-of-domain detection recognizes that the user diverges from the predefined flow and prevents the system from producing a scripted response that might not make sense for unexpected user input. Finally, the neural response generator generates a response based on the context of the dialogue that correctly reacts to the unexpected user input and returns the dialogue to the boundaries of hand-designed dialogues. The innovations for engagement that we propose are mostly inspired by the famous exploration-exploitation dilemma. To conduct an engaging conversation with the dialogue partners, one has to learn their preferences and interests -- exploration. Moreover, to engage the partner, we have to utilize the knowledge we have already learned -- exploitation. In this work, we present the principles and inner workings of individual components of the open-domain dialogue system Alquist developed within the Alexa Prize Socialbot Grand Challenge 4 and the experiments we have conducted to evaluate them. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.07968
Measuring and improving information systems agility through the balanced scorecard approach,Yassine Rdiouat;Samir Bahsani;Mouhsine Lakhdissi;Alami Semma,"Facing an environment increasingly complex, uncertain and changing, even in crisis, organizations are driven to be agile in order to survive. Agility, at the core heart of business strategy, represents the ability to grow in a competitive environment of continuous and unpredictable changes with information systems perceived as one of its main enablers. In other words, to be agile, organizations must be able to rely on agile enterprise information systems/information technology (IT/IS). Since, the agility needs are not the same among stakeholders, the objective of this research is to develop a conceptual model for the achievement and assessment of IT/IS agility from balanced perspectives to support agile organizations. Several researches have indicated that the IT balanced scorecard (BSC) approach is an appropriate technique for evaluating IT performance. This paper provides a balanced-scorecard based framework to evaluate the IS agility through four perspectives: business contribution, user orientation, operation excellence and innovation and competitiveness. The proposed framework, called IS Agility BSC, propose a three layer structure for each of the four perspectives: mission, key success factors, and agility evaluation criteria. According to this conceptual model, enterprise information systems agility is measured according to 14 agility key success factors, over the four BSC Perspectives, using 42 agility evaluation criteria that are identified based on literature survey methodology. This paper explores agility in the broader context of the enterprise information systems. The findings will provide, for both researchers and practitioners, a practical approach for achieving and measuring IS agility performance to support organizations in attempt to become agile as a new condition of surviving in the new business world. △ Less","13 September, 2021",https://arxiv.org/pdf/2109.07281
Semantics of European poetry is shaped by conservative forces: The relationship between poetic meter and meaning in accentual-syllabic verse,Artjoms Šeļa;Petr Plecháč;Alie Lassche,"Recent advances in cultural analytics and large-scale computational studies of art, literature and film often show that long-term change in the features of artistic works happens gradually. These findings suggest that conservative forces that shape creative domains might be underestimated. To this end, we provide the first large-scale formal evidence of the persistent association between poetic meter and semantics in 18-19th European literatures, using Czech, German and Russian collections with additional data from English poetry and early modern Dutch songs. Our study traces this association through a series of clustering experiments using the abstracted semantic features of 150,000 poems. With the aid of topic modeling we infer semantic features for individual poems. Texts were also lexically simplified across collections to increase generalizability and decrease the sparseness of word frequency distributions. Topics alone enable recognition of the meters in each observed language, as may be seen from highly robust clustering of same-meter samples (median Adjusted Rand Index between 0.48 and 1). In addition, this study shows that the strength of the association between form and meaning tends to decrease over time. This may reflect a shift in aesthetic conventions between the 18th and 19th centuries as individual innovation was increasingly favored in literature. Despite this decline, it remains possible to recognize semantics of the meters from past or future, which suggests the continuity of semantic traditions while also revealing the historical variability of conditions across languages. This paper argues that distinct metrical forms, which are often copied in a language over centuries, also maintain long-term semantic inertia in poetry. Our findings, thus, highlight the role of the formal features of cultural items in influencing the pace and shape of cultural evolution. △ Less","15 September, 2021",https://arxiv.org/pdf/2109.07148
WaveCorr: Correlation-savvy Deep Reinforcement Learning for Portfolio Management,Saeed Marzban;Erick Delage;Jonathan Yumeng Li;Jeremie Desgagne-Bouchard;Carl Dussault,"The problem of portfolio management represents an important and challenging class of dynamic decision making problems, where rebalancing decisions need to be made over time with the consideration of many factors such as investors preferences, trading environments, and market conditions. In this paper, we present a new portfolio policy network architecture for deep reinforcement learning (DRL)that can exploit more effectively cross-asset dependency information and achieve better performance than state-of-the-art architectures. In particular, we introduce a new property, referred to as \textit{asset permutation invariance}, for portfolio policy networks that exploit multi-asset time series data, and design the first portfolio policy network, named WaveCorr, that preserves this invariance property when treating asset correlation information. At the core of our design is an innovative permutation invariant correlation processing layer. An extensive set of experiments are conducted using data from both Canadian (TSX) and American stock markets (S&P 500), and WaveCorr consistently outperforms other architectures with an impressive 3%-25% absolute improvement in terms of average annual return, and up to more than 200% relative improvement in average Sharpe ratio. We also measured an improvement of a factor of up to 5 in the stability of performance under random choices of initial asset ordering and weights. The stability of the network has been found as particularly valuable by our industrial partner. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.07005
Joining Forces: Applying Design Thinking Techniques in Scrum Meetings,Franziska Dobrigkeit;Christoph Matthies;Ralf Teusner;Michael Perscheid,"The most prominent Agile framework Scrum, is often criticized for its amount of meetings. These regular events are essential to the empirical inspect-and-adapt cycle proposed by Agile methods. Scrum meetings face several challenges, such as being perceived as boring, repetitive, or irrelevant, leading to decreased cooperation in teams and less successful projects. In an attempt to address these challenges, Agile practitioners have adopted teamwork, innovation, and design techniques geared towards improving collaboration. Additionally, they have developed their own activities to be used in Scrum meetings, most notably for conducting retrospective and planning events. Design thinking incorporates non-designers and designers in design and conceptualization activities, including user research, ideation, or testing. Accordingly, the design thinking approach provides a process with different phases and accompanying techniques for each step. These design thinking techniques can support shared understanding in teams and can improve collaboration, creativity, and product understanding. For these reasons, design thinking techniques represent a worthwhile addition to the Scrum meeting toolkit and can support Agile meetings in preventing or countering common meeting challenges and achieving meeting goals. This chapter explores how techniques from the design thinking toolkit can support Scrum meetings from a theoretical and practical viewpoint. We analyze Scrum meetings' requirements, goals, and challenges and link them to groups of techniques from the design thinking toolkit. In addition, we review interview and observational data from two previous studies with software development practitioners and derive concrete examples. As a result, we present initial guidelines on integrating design thinking techniques into Scrum meetings to make them more engaging, collaborative, and interactive. △ Less","14 September, 2021",https://arxiv.org/pdf/2109.06578
A Practical Adversarial Attack on Contingency Detection of Smart Energy Systems,Moein Sabounchi;Jin Wei-Kocsis,"Due to the advances in computing and sensing, deep learning (DL) has widely been applied in smart energy systems (SESs). These DL-based solutions have proved their potentials in improving the effectiveness and adaptiveness of the control systems. However, in recent years, increasing evidence shows that DL techniques can be manipulated by adversarial attacks with carefully-crafted perturbations. Adversarial attacks have been studied in computer vision and natural language processing. However, there is very limited work focusing on the adversarial attack deployment and mitigation in energy systems. In this regard, to better prepare the SESs against potential adversarial attacks, we propose an innovative adversarial attack model that can practically compromise dynamical controls of energy system. We also optimize the deployment of the proposed adversarial attack model by employing deep reinforcement learning (RL) techniques. In this paper, we present our first-stage work in this direction. In simulation section, we evaluate the performance of our proposed adversarial attack model using standard IEEE 9-bus system. △ Less","13 September, 2021",https://arxiv.org/pdf/2109.06358
Internet of Things in Space: A Review of Opportunities and Challenges from Satellite-Aided Computing to Digitally-Enhanced Space Living,Jonathan Kua;Chetan Arora;Seng W. Loke;Niroshinie Fernando;Chathurika Ranaweera,"Recent scientific and technological advancements driven by the Internet of Things (IoT), Machine Learning (ML) and Artificial Intelligence (AI), distributed computing and data communication technologies have opened up a vast range of opportunities in many scientific fields - spanning from fast, reliable and efficient data communication to large-scale cloud/edge computing and intelligent big data analytics. Technological innovations and developments in these areas have also enabled many opportunities in the space industry. The successful Mars landing of NASA's Perseverance rover on February 18, 2021 represents another giant leap for mankind in space exploration. Emerging research and developments of connectivity and computing technologies in IoT for space/non-terrestrial environments is expected to yield significant benefits in the near future. This survey paper presents a broad overview of the area and provides a look-ahead of the opportunities made possible by IoT and space-based technologies. We first survey the current developments of IoT and space industry, and identify key challenges and opportunities in these areas. We then review the state-of-the-art and discuss future opportunities for IoT developments, deployment and integration to support future endeavours in space exploration. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.05971
An Adaptive Boosting Technique to Mitigate Popularity Bias in Recommender System,Ajay Gangwar;Shweta Jain,"The observed ratings in most recommender systems are subjected to popularity bias and are thus not randomly missing. Due to this, only a few popular items are recommended, and a vast number of non-popular items are hardly recommended. Not suggesting the non-popular items lead to fewer products dominating the market and thus offering fewer opportunities for creativity and innovation. In the literature, several fair algorithms have been proposed which mainly focused on improving the accuracy of the recommendation system. However, a typical accuracy measure is biased towards popular items, i.e., it promotes better accuracy for popular items compared to non-popular items. This paper considers a metric that measures the popularity bias as the difference in error on popular items and non-popular items. Motivated by the fair boosting algorithm on classification, we propose an algorithm that reduces the popularity bias present in the data while maintaining accuracy within acceptable limits. The main idea of our algorithm is that it lifts the weights of the non-popular items, which are generally underrepresented in the data. With the help of comprehensive experiments on real-world datasets, we show that our proposed algorithm outperforms the existing algorithms on the proposed popularity bias metric. △ Less","12 September, 2021",https://arxiv.org/pdf/2109.05677
Source Inference Attacks in Federated Learning,Hongsheng Hu;Zoran Salcic;Lichao Sun;Gillian Dobbie;Xuyun Zhang,"Federated learning (FL) has emerged as a promising privacy-aware paradigm that allows multiple clients to jointly train a model without sharing their private data. Recently, many studies have shown that FL is vulnerable to membership inference attacks (MIAs) that can distinguish the training members of the given model from the non-members. However, existing MIAs ignore the source of a training member, i.e., the information of which client owns the training member, while it is essential to explore source privacy in FL beyond membership privacy of examples from all clients. The leakage of source information can lead to severe privacy issues. For example, identification of the hospital contributing to the training of an FL model for COVID-19 pandemic can render the owner of a data record from this hospital more prone to discrimination if the hospital is in a high risk region. In this paper, we propose a new inference attack called source inference attack (SIA), which can derive an optimal estimation of the source of a training member. Specifically, we innovatively adopt the Bayesian perspective to demonstrate that an honest-but-curious server can launch an SIA to steal non-trivial source information of the training members without violating the FL protocol. The server leverages the prediction loss of local models on the training members to achieve the attack effectively and non-intrusively. We conduct extensive experiments on one synthetic and five real datasets to evaluate the key factors in an SIA, and the results show the efficacy of the proposed source inference attack. △ Less","12 September, 2021",https://arxiv.org/pdf/2109.05659
AdViCE: Aggregated Visual Counterfactual Explanations for Machine Learning Model Validation,Oscar Gomez;Steffen Holter;Jun Yuan;Enrico Bertini,"Rapid improvements in the performance of machine learning models have pushed them to the forefront of data-driven decision-making. Meanwhile, the increased integration of these models into various application domains has further highlighted the need for greater interpretability and transparency. To identify problems such as bias, overfitting, and incorrect correlations, data scientists require tools that explain the mechanisms with which these model decisions are made. In this paper we introduce AdViCE, a visual analytics tool that aims to guide users in black-box model debugging and validation. The solution rests on two main visual user interface innovations: (1) an interactive visualization design that enables the comparison of decisions on user-defined data subsets; (2) an algorithm and visual design to compute and visualize counterfactual explanations - explanations that depict model outcomes when data features are perturbed from their original values. We provide a demonstration of the tool through a use case that showcases the capabilities and potential limitations of the proposed approach. △ Less","12 September, 2021",https://arxiv.org/pdf/2109.05629
HyP-ABC: A Novel Automated Hyper-Parameter Tuning Algorithm Using Evolutionary Optimization,Leila Zahedi;Farid Ghareh Mohammadi;M. Hadi Amini,"Machine learning techniques lend themselves as promising decision-making and analytic tools in a wide range of applications. Different ML algorithms have various hyper-parameters. In order to tailor an ML model towards a specific application, a large number of hyper-parameters should be tuned. Tuning the hyper-parameters directly affects the performance (accuracy and run-time). However, for large-scale search spaces, efficiently exploring the ample number of combinations of hyper-parameters is computationally challenging. Existing automated hyper-parameter tuning techniques suffer from high time complexity. In this paper, we propose HyP-ABC, an automatic innovative hybrid hyper-parameter optimization algorithm using the modified artificial bee colony approach, to measure the classification accuracy of three ML algorithms, namely random forest, extreme gradient boosting, and support vector machine. Compared to the state-of-the-art techniques, HyP-ABC is more efficient and has a limited number of parameters to be tuned, making it worthwhile for real-world hyper-parameter optimization problems. We further compare our proposed HyP-ABC algorithm with state-of-the-art techniques. In order to ensure the robustness of the proposed method, the algorithm takes a wide range of feasible hyper-parameter values, and is tested using a real-world educational dataset. △ Less","11 September, 2021",https://arxiv.org/pdf/2109.05319
A Fast PC Algorithm with Reversed-order Pruning and A Parallelization Strategy,Kai Zhang;Chao Tian;Kun Zhang;Todd Johnson;Xiaoqian Jiang,"The PC algorithm is the state-of-the-art algorithm for causal structure discovery on observational data. It can be computationally expensive in the worst case due to the conditional independence tests are performed in an exhaustive-searching manner. This makes the algorithm computationally intractable when the task contains several hundred or thousand nodes, particularly when the true underlying causal graph is dense. We propose a critical observation that the conditional set rendering two nodes independent is non-unique, and including certain redundant nodes do not sacrifice result accuracy. Based on this finding, the innovations of our work are two-folds. First, we innovate on a reserve order linkage pruning PC algorithm which significantly increases the algorithm's efficiency. Second, we propose a parallel computing strategy for statistical independence tests by leveraging tensor computation, which brings further speedup. We also prove the proposed algorithm does not induce statistical power loss under mild graph and data dimensionality assumptions. Experimental results show that the single-threaded version of the proposed algorithm can achieve a 6-fold speedup compared to the PC algorithm on a dense 95-node graph, and the parallel version can make a 825-fold speed-up. We also provide proof that the proposed algorithm is consistent under the same set of conditions with conventional PC algorithm. △ Less","9 September, 2021",https://arxiv.org/pdf/2109.04626
Modeling Massive Spatial Datasets Using a Conjugate Bayesian Linear Regression Framework,Sudipto Banerjee,"Geographic Information Systems (GIS) and related technologies have generated substantial interest among statisticians with regard to scalable methodologies for analyzing large spatial datasets. A variety of scalable spatial process models have been proposed that can be easily embedded within a hierarchical modeling framework to carry out Bayesian inference. While the focus of statistical research has mostly been directed toward innovative and more complex model development, relatively limited attention has been accorded to approaches for easily implementable scalable hierarchical models for the practicing scientist or spatial analyst. This article discusses how point-referenced spatial process models can be cast as a conjugate Bayesian linear regression that can rapidly deliver inference on spatial processes. The approach allows exact sampling directly (avoids iterative algorithms such as Markov chain Monte Carlo) from the joint posterior distribution of regression parameters, the latent process and the predictive random variables, and can be easily implemented on statistical programming environments such as R. △ Less","9 September, 2021",https://arxiv.org/pdf/2109.04447
Machine Learning-Enabled Data Rate Prediction for 5G NSA Vehicle-to-Cloud Communications,Benjamin Sliwa;Hendrik Schippers;Christian Wietfeld,"In order to satisfy the ever-growing Quality of Service (QoS) requirements of innovative services, cellular communication networks are constantly evolving. Recently, the 5G NonStandalone (NSA) mode has been deployed as an intermediate strategy to deliver high-speed connectivity to early adopters of 5G by incorporating Long Term Evolution (LTE) network infrastructure. In addition to the technological advancements, novel communication paradigms such as anticipatory mobile networking aim to achieve a more intelligent usage of the available network resources through exploitation of context knowledge. For this purpose, novel methods for proactive prediction of the end-to-end behavior are seen as key enablers. In this paper, we present a first empirical analysis of client-based end-to-end data rate prediction for 5G NSA vehicle-to-cloud communications. Although this operation mode is characterized by massive fluctuations of the observed data rate, the results show that conventional machine learning methods can utilize locally acquirable measurements for achieving comparably accurate estimations of the end-to-end behavior. △ Less","9 September, 2021",https://arxiv.org/pdf/2109.04117
Multiscale Laplacian Learning,Ekaterina Merkurjev;Duc DUy Nguyen;Guo-Wei Wei,"Machine learning methods have greatly changed science, engineering, finance, business, and other fields. Despite the tremendous accomplishments of machine learning and deep learning methods, many challenges still remain. In particular, the performance of machine learning methods is often severely affected in case of diverse data, usually associated with smaller data sets or data related to areas of study where the size of the data sets is constrained by the complexity and/or high cost of experiments. Moreover, data with limited labeled samples is a challenge to most learning approaches. In this paper, the aforementioned challenges are addressed by integrating graph-based frameworks, multiscale structure, modified and adapted optimization procedures and semi-supervised techniques. This results in two innovative multiscale Laplacian learning (MLL) approaches for machine learning tasks, such as data classification, and for tackling diverse data, data with limited samples and smaller data sets. The first approach, called multikernel manifold learning (MML), integrates manifold learning with multikernel information and solves a regularization problem consisting of a loss function and a warped kernel regularizer using multiscale graph Laplacians. The second approach, called the multiscale MBO (MMBO) method, introduces multiscale Laplacians to a modification of the famous classical Merriman-Bence-Osher (MBO) scheme, and makes use of fast solvers for finding the approximations to the extremal eigenvectors of the graph Laplacian. We demonstrate the performance of our methods experimentally on a variety of data sets, such as biological, text and image data, and compare them favorably to existing approaches. △ Less","8 September, 2021",https://arxiv.org/pdf/2109.03718
Self-supervised Tumor Segmentation through Layer Decomposition,Xiaoman Zhang;Weidi Xie;Chaoqin Huang;Yanfeng Wang;Ya Zhang;Xin Chen;Qi Tian,"In this paper, we target self-supervised representation learning for zero-shot tumor segmentation. We make the following contributions: First, we advocate a zero-shot setting, where models from pre-training should be directly applicable for the downstream task, without using any manual annotations. Second, we take inspiration from ""layer-decomposition"", and innovate on the training regime with simulated tumor data. Third, we conduct extensive ablation studies to analyse the critical components in data simulation, and validate the necessity of different proxy tasks. We demonstrate that, with sufficient texture randomization in simulation, model trained on synthetic data can effortlessly generalise to segment real tumor data. Forth, our approach achieves superior results for zero-shot tumor segmentation on different downstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for liver tumor segmentation. While evaluating the model transferability for tumor segmentation under a low-annotation regime, the proposed approach also outperforms all existing self-supervised approaches, opening up the usage of self-supervised learning in practical scenarios. △ Less","21 December, 2021",https://arxiv.org/pdf/2109.03230
GeneNet VR: Interactive visualization of large-scale biological networks using a standalone headset,Álvaro Martínez Fernández;Lars Ailo Bongo;Edvard Pedersen,"Visualizations are an essential part of biomedical analysis result interpretation. Often, interactive networks are used to visualize the data. However, the high interconnectivity, and high dimensionality of the data often results in information overload, making it hard to interpret the results. To address the information overload problem, existing solutions typically either use data reduction, reduced interactivity, or expensive hardware. We propose using the affordable Oculus Quest Virtual Reality (VR) headset for interactive visualization of large-scale biological networks. We present the design and implementation of our solution, GeneNet VR, and we evaluate its scalability and usability using large gene-to-gene interaction networks. We achieve the 72 FPS required by the Oculus performance guidelines for the largest of our networks (2693 genes) using both a GPU and the Oculus Quest standalone. We found from our interviews with biomedical researchers that GeneNet VR is innovative, interesting, and easy to use for novice VR users. We believe affordable hardware like the Oculus Quest has a big potential for biological data analysis. However, additional work is required to evaluate its benefits to improve knowledge discovery for real data analysis use cases. GeneNet VR is open-sourced: https://github.com/kolibrid/GeneNet-VR. A video demonstrating GeneNet VR used to explore large biological networks: https://youtu.be/N4QDZiZqVNY. △ Less","7 September, 2021",https://arxiv.org/pdf/2109.02937
Application of Monte Carlo Stochastic Optimization (MOST) to Deep Learning,Sin-ichi Inage;Hana Hebishima,"In this paper, we apply the Monte Carlo stochastic optimization (MOST) proposed by the authors to a deep learning of XOR gate and verify its effectiveness. Deep machine learning based on neural networks is one of the most important keywords driving innovation in today's highly advanced information society. Therefore, there has been active research on large-scale, high-speed, and high-precision systems. For the purpose of efficiently searching the optimum value of the objective function, the author divides the search region of a multivariable parameter constituting the objective function into two by each parameter, numerically finds the integration of the two regions by the Monte Carlo method, compares the magnitude of the integration value, and judges that there is an optimum point in a small region. In the previous paper, we examined the problem of the benchmark in the optimization method. This method is applied to neural networks of XOR gate, and compared with the results of weight factor optimization by Adam and genetic algorithm. As a result, it was confirmed that it converged faster than the existing method. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.02441
Method for making multi-attribute decisions in wargames by combining intuitionistic fuzzy numbers with reinforcement learning,Yuxiang Sun;Bo Yuan;Yufan Xue;Jiawei Zhou;Xiaoyu Zhang;Xianzhong Zhou,"Researchers are increasingly focusing on intelligent games as a hot research area.The article proposes an algorithm that combines the multi-attribute management and reinforcement learning methods, and that combined their effect on wargaming, it solves the problem of the agent's low rate of winning against specific rules and its inability to quickly converge during intelligent wargame training.At the same time, this paper studied a multi-attribute decision making and reinforcement learning algorithm in a wargame simulation environment, and obtained data on red and blue conflict.Calculate the weight of each attribute based on the intuitionistic fuzzy number weight calculations. Then determine the threat posed by each opponent's chess pieces.Using the red side reinforcement learning reward function, the AC framework is trained on the reward function, and an algorithm combining multi-attribute decision-making with reinforcement learning is obtained. A simulation experiment confirms that the algorithm of multi-attribute decision-making combined with reinforcement learning presented in this paper is significantly more intelligent than the pure reinforcement learning algorithm.By resolving the shortcomings of the agent's neural network, coupled with sparse rewards in large-map combat games, this robust algorithm effectively reduces the difficulties of convergence. It is also the first time in this field that an algorithm design for intelligent wargaming combines multi-attribute decision making with reinforcement learning.Attempt interdisciplinary cross-innovation in the academic field, like designing intelligent wargames and improving reinforcement learning algorithms. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.02354
Data science and Machine learning in the Clouds: A Perspective for the Future,Hrishav Bakul Barua,"As we are fast approaching the beginning of a paradigm shift in the field of science, Data driven science (the so called fourth science paradigm) is going to be the driving force in research and innovation. From medicine to biodiversity and astronomy to geology, all these terms are somehow going to be affected by this paradigm shift. The huge amount of data to be processed under this new paradigm will be a major concern in the future and one will strongly require cloud based services in all the aspects of these computations (from storage to compute and other services). Another aspect will be energy consumption and performance of prediction jobs and tasks within such a scientific paradigm which will change the way one sees computation. Data science has heavily impacted or rather triggered the emergence of Machine Learning, Signal/Image/Video processing related algorithms, Artificial intelligence, Robotics, health informatics, geoinformatics, and many more such areas of interest. Hence, we envisage an era where Data science can deliver its promises with the help of the existing cloud based platforms and services with the addition of new services. In this article, we discuss about data driven science and Machine learning and how they are going to be linked through cloud based services in the future. It also discusses the rise of paradigms like approximate computing, quantum computing and many more in recent times and their applicability in big data processing, data science, analytics, prediction and machine learning in the cloud environments. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.01661
Open Data Ecosystems -- an empirical investigation into an emerging industry collaboration concept,Per Runeson;Thomas Olsson;Johan Linåker,"Software systems are increasingly depending on data, particularly with the rising use of machine learning, and developers are looking for new sources of data. Open Data Ecosystems (ODE) is an emerging concept for data sharing under public licenses in software ecosystems, similar to Open Source Software (OSS). It has certain similarities to Open Government Data (OGD), where public agencies share data for innovation and transparency. We aimed to explore open data ecosystems involving commercial actors. Thus, we organized five focus groups with 27 practitioners from 22 companies, public organizations, and research institutes. Based on the outcomes, we surveyed three cases of emerging ODE practice to further understand the concepts and to validate the initial findings. The main outcome is an initial conceptual model of ODEs' value, intrinsics, governance, and evolution, and propositions for practice and further research. We found that ODE must be value driven. Regarding the intrinsics of data, we found their type, meta-data, and legal frameworks influential for their openness. We also found the characteristics of ecosystem initiation, organization, data acquisition and openness be differentiating, which we advise research and practice to take into consideration. △ Less","3 September, 2021",https://arxiv.org/pdf/2109.01378
Deep Learning for Fitness,Mahendran N,"We present Fitness tutor, an application for maintaining correct posture during workout exercises or doing yoga. Current work on fitness focuses on suggesting food supplements, accessing workouts, workout wearables does a great job in improving the fitness. Meanwhile, the current situation is making difficult to monitor workouts by trainee. Inspired by healthcare innovations like robotic surgery, we design a novel application Fitness tutor which can guide the workouts using pose estimation. Pose estimation can be deployed on the reference image for gathering data and guide the user with the data. This allow Fitness tutor to guide the workouts (both exercise and yoga) in remote conditions with a single reference posture as image. We use posenet model in tensorflow with p5js for developing skeleton. Fitness tutor is an application of pose estimation model in bringing a realtime teaching experience in fitness. Our experiments shows that it can leverage potential of pose estimation models by providing guidance in realtime. △ Less","3 September, 2021",https://arxiv.org/pdf/2109.01376
Joint Management and Analysis of Textual Documents and Tabular Data within the AUDAL Data Lake,Pegdwendé Sawadogo;Jérôme Darmont;Camille Noûs,"In 2010, the concept of data lake emerged as an alternative to data warehouses for big data management. Data lakes follow a schema-on-read approach to provide rich and flexible analyses. However, although trendy in both the industry and academia, the concept of data lake is still maturing, and there are still few methodological approaches to data lake design. Thus, we introduce a new approach to design a data lake and propose an extensive metadata system to activate richer features than those usually supported in data lake approaches. We implement our approach in the AUDAL data lake, where we jointly exploit both textual documents and tabular data, in contrast with structured and/or semi-structured data typically processed in data lakes from the literature. Furthermore, we also innovate by leveraging metadata to activate both data retrieval and content analysis, including Text-OLAP and SQL querying. Finally, we show the feasibility of our approach using a real-word use case on the one hand, and a benchmark on the other hand. △ Less","3 September, 2021",https://arxiv.org/pdf/2109.01374
Accurate shape and phase averaging of time series through Dynamic Time Warping,George Sioros;Kristian Nymoen,"We propose a novel time series averaging method based on Dynamic Time Warping (DTW). In contrast to previous methods, our algorithm preserves durational information and the distinctive durational features of the sequences due to a simple conversion of the output of DTW into a time sequence and an innovative iterative averaging process. We show that it accurately estimates the ground truth mean sequences and mean temporal location of landmarks in synthetic and real-world datasets and outperforms state-of-the-art methods. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.00978
Towards a Reference Architecture for Future Industrial Internet of Things Networks,Dominik Martin;Niklas Kühl;Marcel Schwenk,"With the continuing decrease of sensor technology prices as well as the increase of communication and analytical capabilities of modern internet of things devices, the continuously generated amount of data is constantly growing. Various use cases show the untapped potential of this data for new business models. However, conventional industrial IT networks of traditional manufacturing companies can hardly meet the modern requirements emerging with today's and future industrial internet of things applications. Outdated and rigid network infrastructures are one of the main reasons for hesitant innovation efforts and cross-organizational collaborations as well as the slow adoption of modern business models by traditional manufacturing companies. Following the design science research paradigm, our work contributes by elaborating on a comprehensive list of requirements for future industrial internet of things networks from a theoretical and practical perspective as well as a proposed reference architecture acting as a blueprint for future implementations. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.00833
Sense representations for Portuguese: experiments with sense embeddings and deep neural language models,Jessica Rodrigues da Silva;Helena de Medeiros Caseli,"Sense representations have gone beyond word representations like Word2Vec, GloVe and FastText and achieved innovative performance on a wide range of natural language processing tasks. Although very useful in many applications, the traditional approaches for generating word embeddings have a strict drawback: they produce a single vector representation for a given word ignoring the fact that ambiguous words can assume different meanings. In this paper, we explore unsupervised sense representations which, different from traditional word embeddings, are able to induce different senses of a word by analyzing its contextual semantics in a text. The unsupervised sense representations investigated in this paper are: sense embeddings and deep neural language models. We present the first experiments carried out for generating sense embeddings for Portuguese. Our experiments show that the sense embedding model (Sense2vec) outperformed traditional word embeddings in syntactic and semantic analogies task, proving that the language resource generated here can improve the performance of NLP tasks in Portuguese. We also evaluated the performance of pre-trained deep neural language models (ELMo and BERT) in two transfer learning approaches: feature based and fine-tuning, in the semantic textual similarity task. Our experiments indicate that the fine tuned Multilingual and Portuguese BERT language models were able to achieve better accuracy than the ELMo model and baselines. △ Less","31 August, 2021",https://arxiv.org/pdf/2109.00025
Arctic connectivity: A frugal approach to infrastructural development,Mette Simonsen Abildgaard;Carina Ren;Israel Leyva-Mayorga;Cedomir Stefanovic;Beatriz Soret;Petar Popovski,"As the Arctic is heating up, so are efforts to strengthen connectivity within the region, but also to enhance the connections from remote settlements to the global networks of trade as well as sociality. With global interest in the Arctic on the rise, it becomes increasingly relevant to ensure that investments in arctic infrastructure actually serve the people of the Arctic, while promoting industrial and commercial innovation in the region through widespread access to broadband and Internet of Things (IoT) services. This calls for interdisciplinary research strategies that are able to connect and integrate technological and societal approaches, which are commonly applied separately and in isolation from one another. In this article, we propose an interdisciplinary collaborative research agenda for Arctic connectivity. Drawing on examples from Greenland, we stress the need for localized knowledge to design valuable and cost-effective connectivity solutions that cover the needs for everyday life and may also provide a new set of collaborative connectivity tools for innovation at an international level. Such solutions, termed 'frugal connectivity', are vital for the development of connected Arctic communities. △ Less","30 August, 2021",https://arxiv.org/pdf/2108.13012
Font Completion and Manipulation by Cycling Between Multi-Modality Representations,Ye Yuan;Wuyang Chen;Zhaowen Wang;Matthew Fisher;Zhifei Zhang;Zhangyang Wang;Hailin Jin,"Generating font glyphs of consistent style from one or a few reference glyphs, i.e., font completion, is an important task in topographical design. As the problem is more well-defined than general image style transfer tasks, thus it has received interest from both vision and machine learning communities. Existing approaches address this problem as a direct image-to-image translation task. In this work, we innovate to explore the generation of font glyphs as 2D graphic objects with the graph as an intermediate representation, so that more intrinsic graphic properties of font styles can be captured. Specifically, we formulate a cross-modality cycled image-to-image model structure with a graph constructor between an image encoder and an image renderer. The novel graph constructor maps a glyph's latent code to its graph representation that matches expert knowledge, which is trained to help the translation task. Our model generates improved results than both image-to-image baseline and previous state-of-the-art methods for glyph completion. Furthermore, the graph representation output by our model also provides an intuitive interface for users to do local editing and manipulation. Our proposed cross-modality cycled representation learning has the potential to be applied to other domains with prior knowledge from different data modalities. Our code is available at https://github.com/VITA-Group/Font_Completion_Graph. △ Less","29 August, 2021",https://arxiv.org/pdf/2108.12965
KO codes: Inventing Nonlinear Encoding and Decoding for Reliable Wireless Communication via Deep-learning,Ashok Vardhan Makkuva;Xiyang Liu;Mohammad Vahid Jamali;Hessam Mahdavifar;Sewoong Oh;Pramod Viswanath,"Landmark codes underpin reliable physical layer communication, e.g., Reed-Muller, BCH, Convolution, Turbo, LDPC and Polar codes: each is a linear code and represents a mathematical breakthrough. The impact on humanity is huge: each of these codes has been used in global wireless communication standards (satellite, WiFi, cellular). Reliability of communication over the classical additive white Gaussian noise (AWGN) channel enables benchmarking and ranking of the different codes. In this paper, we construct KO codes, a computationaly efficient family of deep-learning driven (encoder, decoder) pairs that outperform the state-of-the-art reliability performance on the standardized AWGN channel. KO codes beat state-of-the-art Reed-Muller and Polar codes, under the low-complexity successive cancellation decoding, in the challenging short-to-medium block length regime on the AWGN channel. We show that the gains of KO codes are primarily due to the nonlinear mapping of information bits directly to transmit real symbols (bypassing modulation) and yet possess an efficient, high performance decoder. The key technical innovation that renders this possible is design of a novel family of neural architectures inspired by the computation tree of the {\bf K}ronecker {\bf O}peration (KO) central to Reed-Muller and Polar codes. These architectures pave way for the discovery of a much richer class of hitherto unexplored nonlinear algebraic structures. The code is available at \href{https://github.com/deepcomm/KOcodes}{https://github.com/deepcomm/KOcodes} △ Less","29 August, 2021",https://arxiv.org/pdf/2108.12920
Key Lessons Learned from Working During Covid-19 on a Project in the World's Biggest Refugee Camp,Faheem Hussain;Suzana Brown,"Using a case study structure, this research-in-progress paper elaborates the struggles of working on a humanitarian project during the Covid-19 period. The authors identify six specific challenges and propose innovations to address each of these challenges. The challenges are the following: supply chain, design of solutions, human resource development, connectivity, and user data collection. This unprecedented situation has been a testing ground for new innovative solutions for work in conflict zones. △ Less","22 August, 2021",https://arxiv.org/pdf/2108.12286
Segmentation of Shoulder Muscle MRI Using a New Region and Edge based Deep Auto-Encoder,Saddam Hussain Khan;Asifullah Khan;Yeon Soo Lee;Mehdi Hassan;Woong Kyo jeong,"Automatic segmentation of shoulder muscle MRI is challenging due to the high variation in muscle size, shape, texture, and spatial position of tears. Manual segmentation of tear and muscle portion is hard, time-consuming, and subjective to pathological expertise. This work proposes a new Region and Edge-based Deep Auto-Encoder (RE-DAE) for shoulder muscle MRI segmentation. The proposed RE-DAE harmoniously employs average and max-pooling operation in the encoder and decoder blocks of the Convolutional Neural Network (CNN). Region-based segmentation incorporated in the Deep Auto-Encoder (DAE) encourages the network to extract smooth and homogenous regions. In contrast, edge-based segmentation tries to learn the boundary and anatomical information. These two concepts, systematically combined in a DAE, generate a discriminative and sparse hybrid feature space (exploiting both region homogeneity and boundaries). Moreover, the concept of static attention is exploited in the proposed RE-DAE that helps in effectively learning the tear region. The performances of the proposed MRI segmentation based DAE architectures have been tested using a 3D MRI shoulder muscle dataset using the hold-out cross-validation technique. The MRI data has been collected from the Korea University Anam Hospital, Seoul, South Korea. Experimental comparisons have been conducted by employing innovative custom-made and existing pre-trained CNN architectures both using transfer learning and fine-tuning. Objective evaluation on the muscle datasets using the proposed SA-RE-DAE showed a dice similarity of 85.58% and 87.07%, an accuracy of 81.57% and 95.58% for tear and muscle regions, respectively. The high visual quality and the objective result suggest that the proposed SA-RE-DAE is able to correctly segment tear and muscle regions in shoulder muscle MRI for better clinical decisions. △ Less","26 August, 2021",https://arxiv.org/pdf/2108.11720
Evaluating Students Perspectives on ICT Readiness in Somali Higher Education towards Teaching -- Learning Acceptance,Yunis Ali Ahmed;Mohamed M. Mohamed;Abdifatah Farah Ali;Mohamud M. Alasso;Ahmed Dahir Siyad Mohammad Nazir Ahmad,"Along the rapid development of Information and communication technology (ICT) tools and growth of Internet access offer opportunities that facilitate teaching and learning activities in the context of higher education. However, the study of ICTs readiness and acceptance in Somalia higher education is meagre. This research aims to examine the current state of ICT readiness among university students and explores the factors that affect their readiness acceptance. It proposes an extended model, based on the Technology Acceptance Model (TAM), which explains how University students beliefs influence their readiness to accept ICT applications in their learning. Survey responses of 304 students from undergraduate and Graduate in Somalia higher education were collected and analyzed using structural equation modelling. The results of the data analysis demonstrated that the TAM explained university students readiness acceptance of ICT applications reasonably well. More specifically, perceived usefulness, Ease of Use, ICT Selfefficacy, Teaching-Learning autonomy, Students Optimism and Availability of ICT infrastructure are robust predictors of Students ICT readiness acceptance. Results also showed that internet affordability, network speed and quality, innovativeness, discomfort and insecurity do not have a meaningful effect on perceived usefulness and Ease of Use towards ICT readiness acceptance. Through the empirical results, this study helped us understand why students choose to engage in ICT applications for their learning context. Keywords: ICT readiness acceptance, Higher education,Teaching- Learning, Technology Acceptance Model △ Less","25 August, 2021",https://arxiv.org/pdf/2108.11455
Lightweight Self-Attentive Sequential Recommendation,Yang Li;Tong Chen;Peng-Fei Zhang;Hongzhi Yin,"Modern deep neural networks (DNNs) have greatly facilitated the development of sequential recommender systems by achieving state-of-the-art recommendation performance on various sequential recommendation tasks. Given a sequence of interacted items, existing DNN-based sequential recommenders commonly embed each item into a unique vector to support subsequent computations of the user interest. However, due to the potentially large number of items, the over-parameterised item embedding matrix of a sequential recommender has become a memory bottleneck for efficient deployment in resource-constrained environments, e.g., smartphones and other edge devices. Furthermore, we observe that the widely-used multi-head self-attention, though being effective in modelling sequential dependencies among items, heavily relies on redundant attention units to fully capture both global and local item-item transition patterns within a sequence. In this paper, we introduce a novel lightweight self-attentive network (LSAN) for sequential recommendation. To aggressively compress the original embedding matrix, LSAN leverages the notion of compositional embeddings, where each item embedding is composed by merging a group of selected base embedding vectors derived from substantially smaller embedding matrices. Meanwhile, to account for the intrinsic dynamics of each item, we further propose a temporal context-aware embedding composition scheme. Besides, we develop an innovative twin-attention network that alleviates the redundancy of the traditional multi-head self-attention while retaining full capacity for capturing long- and short-term (i.e., global and local) item dependencies. Comprehensive experiments demonstrate that LSAN significantly advances the accuracy and memory efficiency of existing sequential recommenders. △ Less","25 August, 2021",https://arxiv.org/pdf/2108.11333
Fully Non-Homogeneous Atmospheric Scattering Modeling with Convolutional Neural Networks for Single Image Dehazing,Cong Wang;Yan Huang;Yuexian Zou;Yong Xu,"In recent years, single image dehazing models (SIDM) based on atmospheric scattering model (ASM) have achieved remarkable results. However, it is noted that ASM-based SIDM degrades its performance in dehazing real world hazy images due to the limited modelling ability of ASM where the atmospheric light factor (ALF) and the angular scattering coefficient (ASC) are assumed as constants for one image. Obviously, the hazy images taken in real world cannot always satisfy this assumption. Such generating modelling mismatch between the real-world images and ASM sets up the upper bound of trained ASM-based SIDM for dehazing. Bearing this in mind, in this study, a new fully non-homogeneous atmospheric scattering model (FNH-ASM) is proposed for well modeling the hazy images under complex conditions where ALF and ASC are pixel dependent. However, FNH-ASM brings difficulty in practical application. In FNH-ASM based SIDM, the estimation bias of parameters at different positions lead to different distortion of dehazing result. Hence, in order to reduce the influence of parameter estimation bias on dehazing results, two new cost sensitive loss functions, beta-Loss and D-Loss, are innovatively developed for limiting the parameter bias of sensitive positions that have a greater impact on the dehazing result. In the end, based on FNH-ASM, an end-to-end CNN-based dehazing network, FNHD-Net, is developed, which applies beta-Loss and D-Loss. Experimental results demonstrate the effectiveness and superiority of our proposed FNHD-Net for dehazing on both synthetic and real-world images. And the performance improvement of our method increases more obviously in dense and heterogeneous haze scenes. △ Less","25 August, 2021",https://arxiv.org/pdf/2108.11292
Autoencoder-based Semantic Novelty Detection: Towards Dependable AI-based Systems,Andreas Rausch;Azarmidokht Motamedi Sedeh;Meng Zhang,"Many autonomous systems, such as driverless taxis, perform safety critical functions. Autonomous systems employ artificial intelligence (AI) techniques, specifically for the environment perception. Engineers cannot completely test or formally verify AI-based autonomous systems. The accuracy of AI-based systems depends on the quality of training data. Thus, novelty detection - identifying data that differ in some respect from the data used for training - becomes a safety measure for system development and operation. In this paper, we propose a new architecture for autoencoder-based semantic novelty detection with two innovations: architectural guidelines for a semantic autoencoder topology and a semantic error calculation as novelty criteria. We demonstrate that such a semantic novelty detection outperforms autoencoder-based novelty detection approaches known from literature by minimizing false negatives. △ Less","25 August, 2021",https://arxiv.org/pdf/2108.10851
"S&P 500 Stock Price Prediction Using Technical, Fundamental and Text Data",Shan Zhong;David B. Hitchcock,"We summarized both common and novel predictive models used for stock price prediction and combined them with technical indices, fundamental characteristics and text-based sentiment data to predict S&P stock prices. A 66.18% accuracy in S&P 500 index directional prediction and 62.09% accuracy in individual stock directional prediction was achieved by combining different machine learning models such as Random Forest and LSTM together into state-of-the-art ensemble models. The data we use contains weekly historical prices, finance reports, and text information from news items associated with 518 different common stocks issued by current and former S&P 500 large-cap companies, from January 1, 2000 to December 31, 2019. Our study's innovation includes utilizing deep language models to categorize and infer financial news item sentiment; fusing different models containing different combinations of variables and stocks to jointly make predictions; and overcoming the insufficient data problem for machine learning models in time series by using data across different stocks. △ Less","22 September, 2021",https://arxiv.org/pdf/2108.10826
Federated Learning for Privacy-Preserving Open Innovation Future on Digital Health,Guodong Long;Tao Shen;Yue Tan;Leah Gerrard;Allison Clarke;Jing Jiang,"Privacy protection is an ethical issue with broad concern in Artificial Intelligence (AI). Federated learning is a new machine learning paradigm to learn a shared model across users or organisations without direct access to the data. It has great potential to be the next-general AI model training framework that offers privacy protection and therefore has broad implications for the future of digital health and healthcare informatics. Implementing an open innovation framework in the healthcare industry, namely open health, is to enhance innovation and creative capability of health-related organisations by building a next-generation collaborative framework with partner organisations and the research community. In particular, this game-changing collaborative framework offers knowledge sharing from diverse data with a privacy-preserving. This chapter will discuss how federated learning can enable the development of an open health ecosystem with the support of AI. Existing challenges and solutions for federated learning will be discussed. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10761
Sharing Practices for Datasets Related to Accessibility and Aging,Rie Kamikubo;Utkarsh Dwivedi;Hernisa Kacorri,"Datasets sourced from people with disabilities and older adults play an important role in innovation, benchmarking, and mitigating bias for both assistive and inclusive AI-infused applications. However, they are scarce. We conduct a systematic review of 137 accessibility datasets manually located across different disciplines over the last 35 years. Our analysis highlights how researchers navigate tensions between benefits and risks in data collection and sharing. We uncover patterns in data collection purpose, terminology, sample size, data types, and data sharing practices across communities of focus. We conclude by critically reflecting on challenges and opportunities related to locating and sharing accessibility datasets calling for technical, legal, and institutional privacy frameworks that are more attuned to concerns from these communities. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10665
Improving Fake News Detection by Using an Entity-enhanced Framework to Fuse Diverse Multimodal Clues,Peng Qi;Juan Cao;Xirong Li;Huan Liu;Qiang Sheng;Xiaoyue Mi;Qin He;Yongbiao Lv;Chenyang Guo;Yingchao Yu,"Recently, fake news with text and images have achieved more effective diffusion than text-only fake news, raising a severe issue of multimodal fake news detection. Current studies on this issue have made significant contributions to developing multimodal models, but they are defective in modeling the multimodal content sufficiently. Most of them only preliminarily model the basic semantics of the images as a supplement to the text, which limits their performance on detection. In this paper, we find three valuable text-image correlations in multimodal fake news: entity inconsistency, mutual enhancement, and text complementation. To effectively capture these multimodal clues, we innovatively extract visual entities (such as celebrities and landmarks) to understand the news-related high-level semantics of images, and then model the multimodal entity inconsistency and mutual enhancement with the help of visual entities. Moreover, we extract the embedded text in images as the complementation of the original text. All things considered, we propose a novel entity-enhanced multimodal fusion framework, which simultaneously models three cross-modal correlations to detect diverse multimodal fake news. Extensive experiments demonstrate the superiority of our model compared to the state of the art. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.10509
ChiNet: Deep Recurrent Convolutional Learning for Multimodal Spacecraft Pose Estimation,Duarte Rondao;Nabil Aouf;Mark A. Richardson,"This paper presents an innovative deep learning pipeline which estimates the relative pose of a spacecraft by incorporating the temporal information from a rendezvous sequence. It leverages the performance of long short-term memory (LSTM) units in modelling sequences of data for the processing of features extracted by a convolutional neural network (CNN) backbone. Three distinct training strategies, which follow a coarse-to-fine funnelled approach, are combined to facilitate feature learning and improve end-to-end pose estimation by regression. The capability of CNNs to autonomously ascertain feature representations from images is exploited to fuse thermal infrared data with red-green-blue (RGB) inputs, thus mitigating the effects of artefacts from imaging space objects in the visible wavelength. Each contribution of the proposed framework, dubbed ChiNet, is demonstrated on a synthetic dataset, and the complete pipeline is validated on experimental data. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.10282
Development of A Fully Data-Driven Artificial Intelligence and Deep Learning for URLLC Application in 6G Wireless Systems: A Survey,Adeeb Salh;Lukman Audah;Qazwan Abdullah;Abdullah Noorsaliza;Nor Shahida Mohd Shah;Jameel Mukred;Shipun Hamzah,"The full future of the sixth generation will develop a fully data-driven that provide terabit rate per second, and adopt an average of 1000+ massive number of connections per person in 10 years 2030 virtually instantaneously. Data-driven for ultra-reliable and low latency communication is a new service paradigm provided by a new application of future sixth-generation wireless communication and network architecture, involving 100+ Gbps data rates with one millisecond latency. The key constraint is the amount of computing power available to spread massive data and well-designed artificial neural networks. Artificial Intelligence provides a new technique to design wireless networks by apply learning, predicting, and make decisions to manage the stream of big data training individuals, which provides more the capacity to transform that expert learning to develop the performance of wireless networks. We study the developing technologies that will be the driving force are artificial intelligence, communication systems to guarantee low latency. This paper aims to discuss the efficiency of the developing network and alleviate the great challenge for application scenarios and study Holographic radio, enhanced wireless channel coding, enormous Internet of Things integration, and haptic communication for virtual and augmented reality provide new services on the 6G network. Furthermore, improving a multi-level architecture for ultra-reliable and low latency in deep Learning allows for data-driven AI and 6G networks for device intelligence, as well as allowing innovations based on effective learning capabilities. These difficulties must be solved in order to meet the needs of future smart networks. Furthermore, this research categorizes various unexplored research gaps between machine learning and the sixth generation. △ Less","3 August, 2021",https://arxiv.org/pdf/2108.10076
The Moderating Effect of Gender on Adopting Digital Government Innovations in Ethiopia,Debas Senshaw;Hossana Twinomurinzi,"Digital government innovation is being recognised as a solution to many problems faced by governments in providing services to their citizens. It is especially important for low-income countries where there are resource constraints. This research was aimed at exploring the moderating effect of gender on the adoption of a digital government innovation in Ethiopia based on the UTAUT model (n=270) and using structural equation modeling (SEM). The results reveal that gender only moderates the relationship between facilitating conditions and usage behavior of government employees to adopt the digital government innovation which is inconsistent with other findings. Another key finding was that even though the innovation was regarded as not being easy to use, women identified that they would still use it because of the social influence from the peers and the bosses. This finding suggests that women government employees who obtain external support are more likely to use digital government innovations compared with men who are unlikely to use it even if they were facilitated. The paper recommends that governments of low-income countries like Ethiopia should design appropriate policies that encourage women in digital government. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.09960
Digital Resilience for What? Case Study of South Korea,Kyung Ryul Park;Sundeep Sahay;Jørn Braa;Pamod Amarakoon,"Resilience has become an emerging topic in various fields of academic research. In spite of its widespread use, there remains conceptual confusion over what resilience means particularly in multi-disciplinary studies including the field of ICT and Development. With the potential of digital technology, research is needed to critically question what key socio-institutional values related to resilience are being strengthened, for what and for whom through the different conceptualizations of resilience. In this study, we conduct an interpretive case study on South Korea's response to the pandemic and construct a chronological narrative to identify key aspects of digital resilience. We identify agility, diversity, and plurality - enabled by active roles of various stakeholders, including citizens, research communities, and private sector - as keys to digital resilience to the pandemic. Findings from the case of South Korea provide implications to ICT4D research while discussing how developing countries, where a national single window platform is typically implemented with greater level of homogeneity, achieve digital resilience with inclusive innovation with plurality of diverse platforms. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.09950
Role of Digital Platforms in Entrepreneurial Processes: A Resource Enabling Perspective of Startups in Pakistan,Hareem Nassar;Fareesa Malik,"This article aims to explore the role of digital platforms as external enablers in entrepreneurial processes. The recent infusion of digital platforms into different aspects of innovation and entrepreneurship has supported digital entrepreneurship; however, the altered entrepreneurial processes are yet to be explored. This study focuses on digital platform-based startups of Pakistan and draws on entrepreneurial bricolage theory to understand the enabling external resources. We followed multiple qualitative case studies approach and collected data through semi-structured interviews from two startups operating solely on digital platforms, 1) XYLEXA and 2) Toycycle. The findings show that entrepreneurial process is a continuous process. Digital platforms have made entrepreneurial processes less bounded i.e. the products and services keep on evolving even after they have been endorsed to the end user. Moreover, platform-based startups having limited resources can move through the entire entrepreneurial process by combining available resources efficiently and effectively. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.09943
Transcending Old Boundaries: Digital Afterlife in the Age of COVID-19,Mashiat Mostafa;Faheem Hussain,"The primary objective of our exploratory research is to contribute to the ongoing conversation on Digital Afterlife from the lenses of Global South during the COVID-19 period. Digital Afterlife is fast becoming a challenge for our increasingly connected society. Moreover, the situation got worse with the COVID-19 pandemic. The on-going research is to address the disparity in the Global South, specifically in countries like Indonesia, India and The Philippines compared to the Global North for Digital Afterlife services such as policies and digital mourning services. By addressing the research question, 'What services and policy frameworks are available for Digital Afterlife in the Global South during COVID-19?', we aim to find the multitude of ways people in the Global South are managing their digital footprints. Our preliminary findings show that some considerable research and death related digital services and innovation have taken place during the pandemic. However, overwhelming majority of these works are western-centric and mainly dealing with post-mortem personal asset management. Cultural nuances, socio-economic perspectives, religion, political climate, regional infrastructures are mostly sidelined. We found significant disparity in Digital Afterlife product and service designs, which got worse during the global pandemic. Our goal is to collect further in-depth data within the three big ICT powerhouses of global south (Indonesia, India and The Philippines), identify the challenges as well as the innovations around Digital Afterlife.We envision proposing a set of recommendations, based on our findings, for developing a more inclusive and equitable digital space in this pandemic-stricken world. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.09939
"Global Transfers: M-Pesa, Intellectual Property Rights and Digital Innovation",Christopher Foster,"In July 2020, in the midst of the COVID crisis, the Kenyan mobile operator Safaricom announced that the intellectual property rights (IPR) for mobile money service M-Pesa were ""moving back into African control"". This paper tracks how the IPR originally came to be held outside Kenya, and the implications for understanding M-Pesa as an inclusive innovation. Through reflection of this analysis of IPR and innovation, the paper contributes to discussions on structural aspects of digital innovation in the global south. By focussing on IPR, it unpacks some of the processes by which global intellectual property regimes and cross-border IPR practices shape uneven outcomes and power. △ Less","22 August, 2021",https://arxiv.org/pdf/2108.09781
A Climate Change Vulnerability Assessment Framework: A Spatial Approach,Claudia Cáceres;Yan Li;Brian Hilton,"Climate change is affecting every known society, especially for small farmers in Low-Income Countries because they depend heavily on rain, seasonality patterns, and known temperature ranges. To build climate change resilient communities among rural farmers, the first step is to understand the impact of climate change on the population. This paper proposes a Climate Change Vulnerability Assessment Framework (CCVAF) to assess climate change vulnerabilities among rural farmers. The CCVAF framework uses information and communication technology (ICT) to assess climate change vulnerabilities among rural farmers by integrating both community level and individual household level indicators. The CCVAF was instantiated into a GIS-based web application named THRIVE for different decision-makers to better assess how climate change is affecting rural farmers in Western Honduras. Qualitative evaluation of the THRIVE showed that it is an innovative and useful tool. The CCVAF contributes to not only the knowledge base of the climate change vulnerability assessment but also the design science literature by providing guidelines to design a class of climate change vulnerability assessment solutions. △ Less","22 August, 2021",https://arxiv.org/pdf/2108.09762
Smart Cities: Potentialities and Challenges in a Context of Sharing Economy,Ben Hur Monteiro Barizon;Renata Lèbre La Rovere,"The purpose of the present paper is to show how blockchain and IoT technologies can benefit smart city projects, which tend to spread in the context of the sharing economy. The article also aims to describe the challenges and potentialities of smart city projects. It was found that technology platforms can serve as a strategy to build the basis for product development (goods and services) and technology-based innovation. △ Less","22 August, 2021",https://arxiv.org/pdf/2108.09758
Digitalising the Water Sector: Implications for Water Service Management and Governance,Godfred Amankwaa;Richard Heeks;Alison L. Browne,"Digital technologies are becoming central to water governance and management, yet their impact and developmental implications are under-researched, particularly in the global South. This paper addresses this knowledge gap by examining the process of water service digitalisation and the resulting effects on service providers. Drawing on qualitative methods, we apply ideas on digitalisation, value, and power to investigate the implementation and impact of digital technologies in Ghana's state water utility company. We find digital water innovations to be recent, and delivering relatively limited impacts as yet, with value mainly accruing at the utility's operational rather than strategic level. The digital technologies present avenues for power shifts and struggles internally and externally as well as some changes in water management structures and responsibilities. We end with a brief discussion on the implications for water service governance and research. △ Less","22 August, 2021",https://arxiv.org/pdf/2108.09746
"Application of Executive Information System for COVID-19 Reporting System and Management: An Example from DKI Jakarta, Indonesia",Verry Adrian;Intan Rachmita Sari;Hardya Gustada Hikmahrachim,"SARS CoV-2 infection and transmission are problematic in developing countries such as Indonesia. Due to the lack of an information system, Provinces must be able to innovate in developing information systems related to surveillance of SARS CoV-2 infection. Jakarta Department of Health built a data management system called Executive Information System (EIS) of COVID-19 Reporting. EIS aimed to provide actual data so that current epidemiological analysis is accurate. The main idea of EIS is to provide valid and actual information to stakeholders, which can then be presented in the form of a dashboard. EIS is utilized to push data flow and management for rapid surveillance purposes. This could be the first time in Indonesia that a system reports near-actual data of nearly half a million people daily using an integrated system through a transparent system. The main data presented is important to monitor and evaluate COVID-19 transmission is the cumulative case dan daily case number. Data in EIS also can offer data geographically so that a more detailed analysis could be done. EIS's data and the dashboard help the government in pandemic control by presenting actual data on bed occupancy and availability across hospitals, especially isolation wards. Stakeholders, academic institutions should utilize EIS data and other elements to help Indonesia fight COVID-19. △ Less","22 August, 2021",https://arxiv.org/pdf/2108.09738
Temporally Nonstationary Component Analysis; Application to Noninvasive Fetal Electrocardiogram Extraction,Fahimeh Jamshidian-Tehrani;Reza Sameni;Christian Jutten,"Objective: Mixtures of temporally nonstationary signals are very common in biomedical applications. The nonstationarity of the source signals can be used as a discriminative property for signal separation. Herein, a semi-blind source separation algorithm is proposed for the extraction of temporally nonstationary components from linear multichannel mixtures of signals and noises. Methods: A hypothesis test is proposed for the detection and fusion of temporally nonstationary events, by using ad hoc indexes for monitoring the first and second order statistics of the innovation process. As proof of concept, the general framework is customized and tested over noninvasive fetal cardiac recordings acquired from the maternal abdomen, over publicly available datasets, using two types of nonstationarity detectors: 1) a local power variations detector, and 2) a model-deviations detector using the innovation process properties of an extended Kalman filter. Results: The performance of the proposed method is assessed in presence of white and colored noise, in different signal-to-noise ratios. Conclusion and Significance: The proposed scheme is general and it can be used for the extraction of nonstationary events and sample deviations from a presumed model in multivariate data, which is a recurrent problem in many machine learning applications. △ Less","20 August, 2021",https://arxiv.org/pdf/2108.09353
An Innovative Attack Modelling and Attack Detection Approach for a Waiting Time-based Adaptive Traffic Signal Controller,Sagar Dasgupta;Courtland Hollis;Mizanur Rahman;Travis Atkison,"An adaptive traffic signal controller (ATSC) combined with a connected vehicle (CV) concept uses real-time vehicle trajectory data to regulate green time and has the ability to reduce intersection waiting time significantly and thereby improve travel time in a signalized corridor. However, the CV-based ATSC increases the size of the surface vulnerable to potential cyber-attack, allowing an attacker to generate disastrous traffic congestion in a roadway network. An attacker can congest a route by generating fake vehicles by maintaining traffic and car-following rules at a slow rate so that the signal timing and phase change without having any abrupt changes in number of vehicles. Because of the adaptive nature of ATSC, it is a challenge to model this kind of attack and also to develop a strategy for detection. This paper introduces an innovative ""slow poisoning"" cyberattack for a waiting time based ATSC algorithm and a corresponding detection strategy. Thus, the objectives of this paper are to: (i) develop a ""slow poisoning"" attack generation strategy for an ATSC, and (ii) develop a prediction-based ""slow poisoning"" attack detection strategy using a recurrent neural network -- i.e., long short-term memory model. We have generated a ""slow poisoning"" attack modeling strategy using a microscopic traffic simulator -- Simulation of Urban Mobility (SUMO) -- and used generated data from the simulation to develop both the attack model and detection model. Our analyses revealed that the attack strategy is effective in creating a congestion in an approach and detection strategy is able to flag the attack. △ Less","19 August, 2021",https://arxiv.org/pdf/2108.08627
TB-ICT: A Trustworthy Blockchain-Enabled System for Indoor COVID-19 Contact Tracing,Mohammad Salimibeni;Zohreh Hajiakhondi-Meybodi;Arash Mohammadi;Yingxu Wang,"Recently, as a consequence of the COVID-19 pandemic, dependence on Contact Tracing (CT) models has significantly increased to prevent spread of this highly contagious virus and be prepared for the potential future ones. Since the spreading probability of the novel coronavirus in indoor environments is much higher than that of the outdoors, there is an urgent and unmet quest to develop/design efficient, autonomous, trustworthy, and secure indoor CT solutions. Despite such an urgency, this field is still in its infancy. The paper addresses this gap and proposes the Trustworthy Blockchain-enabled system for Indoor Contact Tracing (TB-ICT) framework. The TB-ICT framework is proposed to protect privacy and integrity of the underlying CT data from unauthorized access. More specifically, it is a fully distributed and innovative blockchain platform exploiting the proposed dynamic Proof of Work (dPoW) credit-based consensus algorithm coupled with Randomized Hash Window (W-Hash) and dynamic Proof of Credit (dPoC) mechanisms to differentiate between honest and dishonest nodes. The TB-ICT not only provides a decentralization in data replication but also quantifies the node's behavior based on its underlying credit-based mechanism. For achieving high localization performance, we capitalize on availability of Internet of Things (IoT) indoor localization infrastructures, and develop a data driven localization model based on Bluetooth Low Energy (BLE) sensor measurements. The simulation results show that the proposed TB-ICT prevents the COVID-19 from spreading by implementation of a highly accurate contact tracing model while improving the users' privacy and security. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.08275
Selectively-Amortized Resource Bounding (Extended Version),Tianhan Lu;Bor-Yuh Evan Chang;Ashutosh Trivedi,"We consider the problem of automatically proving resource bounds. That is, we study how to prove that an integer-valued resource variable is bounded by a given program expression. Automatic resource-bound analysis has recently received significant attention because of a number of important applications (e.g., detecting performance bugs, preventing algorithmic-complexity attacks, identifying side-channel vulnerabilities), where the focus has often been on developing precise amortized reasoning techniques to infer the most exact resource usage. While such innovations remain critical, we observe that fully precise amortization is not always necessary to prove a bound of interest. And in fact, by amortizing selectively, the needed supporting invariants can be simpler, making the invariant inference task more feasible and predictable. We present a framework for selectively-amortized analysis that mixes worst-case and amortized reasoning via a property decomposition and a program transformation. We show that proving bounds in any such decomposition yields a sound resource bound in the original program, and we give an algorithm for selecting a reasonable decomposition. △ Less","13 October, 2021",https://arxiv.org/pdf/2108.08263
X-modaler: A Versatile and High-performance Codebase for Cross-modal Analytics,Yehao Li;Yingwei Pan;Jingwen Chen;Ting Yao;Tao Mei,"With the rise and development of deep learning over the past decade, there has been a steady momentum of innovation and breakthroughs that convincingly push the state-of-the-art of cross-modal analytics between vision and language in multimedia field. Nevertheless, there has not been an open-source codebase in support of training and deploying numerous neural network models for cross-modal analytics in a unified and modular fashion. In this work, we propose X-modaler -- a versatile and high-performance codebase that encapsulates the state-of-the-art cross-modal analytics into several general-purpose stages (e.g., pre-processing, encoder, cross-modal interaction, decoder, and decode strategy). Each stage is empowered with the functionality that covers a series of modules widely adopted in state-of-the-arts and allows seamless switching in between. This way naturally enables a flexible implementation of state-of-the-art algorithms for image captioning, video captioning, and vision-language pre-training, aiming to facilitate the rapid development of research community. Meanwhile, since the effective modular designs in several stages (e.g., cross-modal interaction) are shared across different vision-language tasks, X-modaler can be simply extended to power startup prototypes for other tasks in cross-modal analytics, including visual question answering, visual commonsense reasoning, and cross-modal retrieval. X-modaler is an Apache-licensed codebase, and its source codes, sample projects and pre-trained models are available on-line: https://github.com/YehLi/xmodaler. △ Less","18 August, 2021",https://arxiv.org/pdf/2108.08217
Multi-Anchor Active Domain Adaptation for Semantic Segmentation,Munan Ning;Donghuan Lu;Dong Wei;Cheng Bian;Chenglang Yuan;Shuang Yu;Kai Ma;Yefeng Zheng,"Unsupervised domain adaption has proven to be an effective approach for alleviating the intensive workload of manual annotation by aligning the synthetic source-domain data and the real-world target-domain samples. Unfortunately, mapping the target-domain distribution to the source-domain unconditionally may distort the essential structural information of the target-domain data. To this end, we firstly propose to introduce a novel multi-anchor based active learning strategy to assist domain adaptation regarding the semantic segmentation task. By innovatively adopting multiple anchors instead of a single centroid, the source domain can be better characterized as a multimodal distribution, thus more representative and complimentary samples are selected from the target domain. With little workload to manually annotate these active samples, the distortion of the target-domain distribution can be effectively alleviated, resulting in a large performance gain. The multi-anchor strategy is additionally employed to model the target-distribution. By regularizing the latent representation of the target samples compact around multiple anchors through a novel soft alignment loss, more precise segmentation can be achieved. Extensive experiments are conducted on public datasets to demonstrate that the proposed approach outperforms state-of-the-art methods significantly, along with thorough ablation study to verify the effectiveness of each component. △ Less","18 August, 2021",https://arxiv.org/pdf/2108.08012
Using Guilds to Foster Internal Startups in Large Organizations: A case study,Tor Sporsem;Anastasiia Tkalich;Nils Brede Moe;Marius Mikalsen;Nina Rygh,"Software product innovation in large organizations is fundamentally chal-lenging because of restrained freedom and flexibility to conduct experi-ments. As a response, large agile companies form internal startups to initiate employ-driven innovation, inspired by Lean startup. This case study investi-gates how communities of practice support five internal startups in develop-ing new software products within a large organization. We observed six communities of practice meetings, two workshops and conducted ten semi-structured interviews over the course of a year. Our findings show that a community of practice, called the Innovation guild, allowed internal startups to help each other by collectively solving problems, creating shared practic-es, and sharing knowledge. This study confirms that benefits documented in earlier research into CoPs also hold true in the context of software product innovation in large organizations. Henceforth, we suggest that similar innova-tion guilds, as described in this paper, can support large companies in the in-novation race for new software products. △ Less","17 August, 2021",https://arxiv.org/pdf/2108.07618
O-HAS: Optical Hardware Accelerator Search for Boosting Both Acceleration Performance and Development Speed,Mengquan Li;Zhongzhi Yu;Yongan Zhang;Yonggan Fu;Yingyan Lin,"The recent breakthroughs and prohibitive complexities of Deep Neural Networks (DNNs) have excited extensive interest in domain-specific DNN accelerators, among which optical DNN accelerators are particularly promising thanks to their unprecedented potential of achieving superior performance-per-watt. However, the development of optical DNN accelerators is much slower than that of electrical DNN accelerators. One key challenge is that while many techniques have been developed to facilitate the development of electrical DNN accelerators, techniques that support or expedite optical DNN accelerator design remain much less explored, limiting both the achievable performance and the innovation development of optical DNN accelerators. To this end, we develop the first-of-its-kind framework dubbed O-HAS, which for the first time demonstrates automated Optical Hardware Accelerator Search for boosting both the acceleration efficiency and development speed of optical DNN accelerators. Specifically, our O-HAS consists of two integrated enablers: (1) an O-Cost Predictor, which can accurately yet efficiently predict an optical accelerator's energy and latency based on the DNN model parameters and the optical accelerator design; and (2) an O-Search Engine, which can automatically explore the large design space of optical DNN accelerators and identify the optimal accelerators (i.e., the micro-architectures and algorithm-to-accelerator mapping methods) in order to maximize the target acceleration efficiency. Extensive experiments and ablation studies consistently validate the effectiveness of both our O-Cost Predictor and O-Search Engine as well as the excellent efficiency of O-HAS generated optical accelerators. △ Less","17 August, 2021",https://arxiv.org/pdf/2108.07538
SPMoE: Generate Multiple Pattern-Aware Outputs with Sparse Pattern Mixture of Experts,Shaobo Cui;Xintong Bao;Xuming Lin;Zhongzhou Zhao;Ji Zhang;Wei Zhou;Haiqing Chen,"Many generation tasks follow a one-to-many mapping relationship: each input could be associated with multiple outputs. Existing methods like Conditional Variational AutoEncoder(CVAE) employ a latent variable to model this one-to-many relationship. However, this high-dimensional and dense latent variable lacks explainability and usually leads to poor and uncontrollable generations. In this paper, we innovatively introduce the linguistic concept of pattern to decompose the one-to-many mapping into multiple one-to-one mappings and further propose a model named Sparse Pattern Mixture of Experts(SPMoE). Each one-to-one mapping is associated with a conditional generation pattern and is modeled with an expert in SPMoE. To ensure each language pattern can be exclusively handled with an expert model for better explainability and diversity, a sparse mechanism is employed to coordinate all the expert models in SPMoE. We assess the performance of our SPMoE on the paraphrase generation task and the experiment results prove that SPMoE can achieve a good balance in terms of quality, pattern-level diversity, and corpus-level diversity. △ Less","17 August, 2021",https://arxiv.org/pdf/2108.07535
Provable Data Clustering via Innovation Search,Weiwei Li;Mostafa Rahmani;Ping Li,"This paper studies the subspace clustering problem in which data points collected from high-dimensional ambient space lie in a union of linear subspaces. Subspace clustering becomes challenging when the dimension of intersection between subspaces is large and most of the self-representation based methods are sensitive to the intersection between the span of clusters. In sharp contrast to the self-representation based methods, a recently proposed clustering method termed Innovation Pursuit, computed a set of optimal directions (directions of innovation) to build the adjacency matrix. This paper focuses on the Innovation Pursuit Algorithm to shed light on its impressive performance when the subspaces are heavily intersected. It is shown that in contrast to most of the existing methods which require the subspaces to be sufficiently incoherent with each other, Innovation Pursuit only requires the innovative components of the subspaces to be sufficiently incoherent with each other. These new sufficient conditions allow the clusters to be strongly close to each other. Motivated by the presented theoretical analysis, a simple yet effective projection based technique is proposed which we show with both numerical and theoretical results that it can boost the performance of Innovation Pursuit. △ Less","16 August, 2021",https://arxiv.org/pdf/2108.06888
Development of the InBan_CIDO Ontology by Reusing the Concepts along with Detecting Overlapping Information,Archana Patel;Narayan C Debnath,"The covid19 pandemic is a global emergency that badly impacted the economies of various countries. Covid19 hit India when the growth rate of the country was at the lowest in the last 10 years. To semantically analyze the impact of this pandemic on the economy, it is curial to have an ontology. CIDO ontology is a well standardized ontology that is specially designed to assess the impact of coronavirus disease and utilize its results for future decision forecasting for the government, industry experts, and professionals in the field of various domains like research, medical advancement, technical innovative adoptions, and so on. However, this ontology does not analyze the impact of the Covid19 pandemic on the Indian banking sector. On the other side, Covid19IBO ontology has been developed to analyze the impact of the Covid19 pandemic on the Indian banking sector but this ontology does not reflect complete information of Covid19 data. Resultantly, users cannot get all the relevant information about Covid19 and its impact on the Indian economy. This article aims to extend the CIDO ontology to show the impact of Covid19 on the Indian economy sector by reusing the concepts from other data sources. We also provide a simplified schema matching approach that detects the overlapping information among the ontologies. The experimental analysis proves that the proposed approach has reasonable results. △ Less","15 August, 2021",https://arxiv.org/pdf/2108.06742
Socio-Technological Challenges and Opportunities: Paths Forward,Carole-Jean Wu;Srilatha Manne;Parthasarathy Ranganathan;Sarah Bird;Shane Greenstein,"Advancements in digital technologies have a bootstrapping effect. The past fifty years of technological innovations from the computer architecture community have brought innovations and orders-of-magnitude efficiency improvements that engender use cases that were not previously possible -- stimulating novel application domains and increasing uses and deployments at an ever-faster pace. Consequently, computing technologies have fueled significant economic growth, creating education opportunities, enabling access to a wider and more diverse spectrum of information, and, at the same time, connecting people of differing needs in the world together. Technology must be offered that is inclusive of the world's physical, cultural, and economic diversity, and which is manufactured, used, and recycled with environmental sustainability at the forefront. For the next decades to come, we envision significant cross-disciplinary efforts to build a circular development cycle by placing pervasive connectivity, sustainability, and demographic inclusion at the design forefront in order to sustain and expand the benefits of a technologically rich society. We hope this work will inspire our computing community to take broader and more holistic approaches when developing technological solutions to serve people from different parts of the world. △ Less","15 August, 2021",https://arxiv.org/pdf/2108.06738
CPNet: Cycle Prototype Network for Weakly-supervised 3D Renal Compartments Segmentation on CT Images,Song Wang;Yuting He;Youyong Kong;Xiaomei Zhu;Shaobo Zhang;Pengfei Shao;Jean-Louis Dillenseger;Jean-Louis Coatrieux;Shuo Li;Guanyu Yang,"Renal compartment segmentation on CT images targets on extracting the 3D structure of renal compartments from abdominal CTA images and is of great significance to the diagnosis and treatment for kidney diseases. However, due to the unclear compartment boundary, thin compartment structure and large anatomy variation of 3D kidney CT images, deep-learning based renal compartment segmentation is a challenging task. We propose a novel weakly supervised learning framework, Cycle Prototype Network, for 3D renal compartment segmentation. It has three innovations: 1) A Cycle Prototype Learning (CPL) is proposed to learn consistency for generalization. It learns from pseudo labels through the forward process and learns consistency regularization through the reverse process. The two processes make the model robust to noise and label-efficient. 2) We propose a Bayes Weakly Supervised Module (BWSM) based on cross-period prior knowledge. It learns prior knowledge from cross-period unlabeled data and perform error correction automatically, thus generates accurate pseudo labels. 3) We present a Fine Decoding Feature Extractor (FDFE) for fine-grained feature extraction. It combines global morphology information and local detail information to obtain feature maps with sharp detail, so the model will achieve fine segmentation on thin structures. Our model achieves Dice of 79.1% and 78.7% with only four labeled images, achieving a significant improvement by about 20% than typical prototype model PANet. △ Less","15 August, 2021",https://arxiv.org/pdf/2108.06669
Active Assessment of Prediction Services as Accuracy Surface Over Attribute Combinations,Vihari Piratla;Soumen Chakrabarty;Sunita Sarawagi,"Our goal is to evaluate the accuracy of a black-box classification model, not as a single aggregate on a given test data distribution, but as a surface over a large number of combinations of attributes characterizing multiple test data distributions. Such attributed accuracy measures become important as machine learning models get deployed as a service, where the training data distribution is hidden from clients, and different clients may be interested in diverse regions of the data distribution. We present Attributed Accuracy Assay (AAA)--a Gaussian Process (GP)--based probabilistic estimator for such an accuracy surface. Each attribute combination, called an 'arm', is associated with a Beta density from which the service's accuracy is sampled. We expect the GP to smooth the parameters of the Beta density over related arms to mitigate sparsity. We show that obvious application of GPs cannot address the challenge of heteroscedastic uncertainty over a huge attribute space that is sparsely and unevenly populated. In response, we present two enhancements: pooling sparse observations, and regularizing the scale parameter of the Beta densities. After introducing these innovations, we establish the effectiveness of AAA in terms of both its estimation accuracy and exploration efficiency, through extensive experiments and analysis. △ Less","26 October, 2021",https://arxiv.org/pdf/2108.06514
Computing Research for the Climate Crisis,Nadya Bliss;Elizabeth Bradley;Claire Monteleoni,"Climate change is an existential threat to the United States and the world. Inevitably, computing will play a key role in mitigation, adaptation, and resilience in response to this threat. The needs span all areas of computing, from devices and architectures (e.g., low-power sensor systems for wildfire monitoring) to algorithms (e.g., predicting impacts and evaluating mitigation), and robotics (e.g., autonomous UAVs for monitoring and actuation) -- as well as every level of the software stack, from data management systems and energy-aware operating systems to hardware/software co-design. The goal of this white paper is to highlight the role of computing research in addressing climate change-induced challenges. To that end, we outline six key impact areas in which these challenges will arise -- energy, environmental justice, transportation, infrastructure, agriculture, and environmental monitoring and forecasting -- then identify specific ways in which computing research can help address the associated problems. These impact areas will create a driving force behind, and enable, cross-cutting, system-level innovation. We further break down this information into four broad areas of computing research: devices & architectures, software, algorithms/AI/robotics, and sociotechnical computing. Additional contributions by: Ilkay Altintas (San Diego Supercomputer Center), Kyri Baker (University of Colorado Boulder), Sujata Banerjee (VMware), Andrew A. Chien (University of Chicago), Thomas Dietterich (Oregon State University), Ian Foster (Argonne National Labs), Carla P. Gomes (Cornell University), Chandra Krintz (University of California, Santa Barbara), Jessica Seddon (World Resources Institute), and Regan Zane (Utah State University). △ Less","23 August, 2021",https://arxiv.org/pdf/2108.05926
Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion,Yikai Wang;Fuchun Sun;Ming Lu;Anbang Yao,"We propose a compact and effective framework to fuse multimodal features at multiple layers in a single network. The framework consists of two innovative fusion schemes. Firstly, unlike existing multimodal methods that necessitate individual encoders for different modalities, we verify that multimodal features can be learnt within a shared single network by merely maintaining modality-specific batch normalization layers in the encoder, which also enables implicit fusion via joint feature representation learning. Secondly, we propose a bidirectional multi-layer fusion scheme, where multimodal features can be exploited progressively. To take advantage of such scheme, we introduce two asymmetric fusion operations including channel shuffle and pixel shift, which learn different fused features with respect to different fusion directions. These two operations are parameter-free and strengthen the multimodal feature interactions across channels as well as enhance the spatial feature discrimination within channels. We conduct extensive experiments on semantic segmentation and image translation tasks, based on three publicly available datasets covering diverse modalities. Results indicate that our proposed framework is general, compact and is superior to state-of-the-art fusion frameworks. △ Less","10 August, 2021",https://arxiv.org/pdf/2108.05009
Using Information Theory to Measure Psychophysical Performance,James V Stone,"Most psychophysical experiments discard half the data collected. Specifically, experiments discard reaction time data, and use binary responses (e.g. yes/no) to measure performance. Here, Shannon's information theory is used to define Shannon competence s', which depends on the mutual information between stimulus strength (e.g. luminance) and a combination of reaction times and binary responses. Mutual information is the entropy of the joint distribution of responses minus the residual entropy after a model has been fitted to these responses. Here, this model is instantiated as a proportional rate diffusion model, with the additional innovation that the full covariance structure of responses is taken into account. Results suggest information associated with reaction times is independent of (i.e. additional to) information associated with binary responses, and that reaction time and binary responses together provide substantially more than the sum of their individual contributions (i.e. they act synergistically). Consequently, the additional information supplied by reaction times suggests that using combined reaction time and binary responses requires fewer stimulus presentations, without loss of precision in psychophysical parameters. Finally, because s' takes account of both reaction time and binary responses, (and in contrast to d') s' is immune to speed-accuracy trade-offs, which vary between observers and experimental designs. △ Less","11 December, 2021",https://arxiv.org/pdf/2108.04936
Industrial Digital Twins at the Nexus of NextG Wireless Networks and Computational Intelligence: A Survey,Shah Zeb;Aamir Mahmood;Syed Ali Hassan;MD. Jalil Piran;Mikael Gidlund;Mohsen Guizani,"By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry~4.0 promotes integrating cyber-physical worlds through cyber-physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT is an emerging but conceptually different construct than CPS. Like CPS, DT relies on communication to create a highly-consistent, synchronized digital mirror image of the objects or physical processes. DT, in addition, uses built-in models on this precise image to simulate, analyze, predict, and optimize their real-time operation using feedback. DT is rapidly diffusing in the industries with recent advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research in next-generation (NextG) wireless technologies (e.g., 5G and beyond networks), various tools (e.g., age of information, federated learning, data analytics), and other promising trends in networked computing (e.g., edge and cloud computing). Moreover, we discuss the DT deployment strategies at different industrial communication layers to meet the monitoring and control requirements of industrial applications. We also outline several key reflections and future research challenges and directions to facilitate industrial DT's adoption. △ Less","10 August, 2021",https://arxiv.org/pdf/2108.04465
Research on Third-Party Libraries in AndroidApps: A Taxonomy and Systematic LiteratureReview,Xian Zhan;Tianming Liu;Lingling Fan;Li Li;Sen Chen;Xiapu Luo;Yang Liu,"Third-party libraries (TPLs) have been widely used in mobile apps, which play an essential part in the entire Android ecosystem. However, TPL is a double-edged sword. On the one hand, it can ease the development of mobile apps. On the other hand, it also brings security risks such as privacy leaks or increased attack surfaces (e.g., by introducing over-privileged permissions) to mobile apps. Although there are already many studies for characterizing third-party libraries, including automated detection, security and privacy analysis of TPLs, TPL attributes analysis, etc., what strikes us odd is that there is no systematic study to summarize those studies' endeavors. To this end, we conduct the first systematic literature review on Android TPL-related research. Following a well-defined systematic literature review protocol, we collected 74 primary research papers closely related to the Android third-party library from 2012 to 2020. After carefully examining these studies, we designed a taxonomy of TPL-related research studies and conducted a systematic study to summarize current solutions, limitations, challenges and possible implications of new research directions related to third-party library analysis. We hope that these contributions can give readers a clear overview of existing TPL-related studies and inspire them to go beyond the current status quo by advancing the discipline with innovative approaches. △ Less","8 August, 2021",https://arxiv.org/pdf/2108.03787
Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers,Yifang Wang;Tai-Quan Peng;Huihua Lu;Haoren Wang;Xiao Xie;Huamin Qu;Yingcai Wu,"How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose ACSeeker, an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of ACSeeker, we report two case studies and interviews with a social scientist and general researchers. △ Less","13 August, 2021",https://arxiv.org/pdf/2108.03381
"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual, Interactive, and Ecological Environments",Sanjana Srivastava;Chengshu Li;Michael Lingelbach;Roberto Martín-Martín;Fei Xia;Kent Vainio;Zheng Lian;Cem Gokmen;Shyamal Buch;C. Karen Liu;Silvio Savarese;Hyowon Gweon;Jiajun Wu;Li Fei-Fei,"We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in simulation, spanning a range of everyday household chores such as cleaning, maintenance, and food preparation. These activities are designed to be realistic, diverse, and complex, aiming to reproduce the challenges that agents must face in the real world. Building such a benchmark poses three fundamental difficulties for each activity: definition (it can differ by time, place, or person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these with three innovations. First, we propose an object-centric, predicate logic-based description language for expressing an activity's initial and goal conditions, enabling generation of diverse instances for any activity. Second, we identify the simulator-agnostic features required by an underlying environment to support BEHAVIOR, and demonstrate its realization in one such simulator. Third, we introduce a set of metrics to measure task progress and efficiency, absolute and relative to human demonstrators. We include 500 human demonstrations in virtual reality (VR) to serve as the human ground truth. Our experiments demonstrate that even state of the art embodied AI solutions struggle with the level of realism, diversity, and complexity imposed by the activities in our benchmark. We make BEHAVIOR publicly available at behavior.stanford.edu to facilitate and calibrate the development of new embodied AI solutions. △ Less","6 August, 2021",https://arxiv.org/pdf/2108.03332
iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks,Chengshu Li;Fei Xia;Roberto Martín-Martín;Michael Lingelbach;Sanjana Srivastava;Bokui Shen;Kent Vainio;Cem Gokmen;Gokul Dharan;Tanish Jain;Andrey Kurenkov;C. Karen Liu;Hyowon Gweon;Jiajun Wu;Li Fei-Fei;Silvio Savarese,"Recent research in embodied AI has been boosted by the use of simulation environments to develop and train robot learning approaches. However, the use of simulation has skewed the attention to tasks that only require what robotics simulators can simulate: motion and physical contact. We present iGibson 2.0, an open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations. First, iGibson 2.0 supports object states, including temperature, wetness level, cleanliness level, and toggled and sliced states, necessary to cover a wider range of tasks. Second, iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked. Additionally, given a logic state, iGibson 2.0 can sample valid physical states that satisfy it. This functionality can generate potentially infinite instances of tasks with minimal effort from the users. The sampling mechanism allows our scenes to be more densely populated with small objects in semantically meaningful locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations. As a result, we can collect demonstrations from humans on these new types of tasks, and use them for imitation learning. We evaluate the new capabilities of iGibson 2.0 to enable robot learning of novel tasks, in the hope of demonstrating the potential of this new simulator to support new research in embodied AI. iGibson 2.0 and its new dataset are publicly available at http://svl.stanford.edu/igibson/. △ Less","3 November, 2021",https://arxiv.org/pdf/2108.03272
Two Basic Queueing Models of Service Platforms in Digital Sharing Economy,Heng-Li Liu;Quan-Lin Li;Xiaole Wu;Chi Zhang,"This paper describes two basic queueing models of service platforms in digital sharing economy by means of two different policies of platform matching information. We show that the two queueing models of service platforms can be expressed as the level-independent quasi birth-and-death (QBD) processes. Using the proposed QBD processes, we provide a detailed analysis for the two queueing models of service platforms, including the system stability, the average stationary numbers of seekers and of idle owners, the expected sojourn time of an arriving seeker, and the expected profits for both the service platform and each owner. Finally, numerical examples are employed to verify our theoretical results, and demonstrate how the performance measures of service platforms are influenced by some key system parameters. We believe that the methodology and results developed in this paper not only can be applied to develop a broad class of queuing models of service platforms, but also will open a series of promising innovative research on performance evaluation, optimal control and queueing-game of service platforms and digital sharing economy. △ Less","27 July, 2021",https://arxiv.org/pdf/2108.02852
Quantum Topological Data Analysis with Linear Depth and Exponential Speedup,Shashanka Ubaru;Ismail Yunus Akhalwaya;Mark S. Squillante;Kenneth L. Clarkson;Lior Horesh,"Quantum computing offers the potential of exponential speedups for certain classical computations. Over the last decade, many quantum machine learning (QML) algorithms have been proposed as candidates for such exponential improvements. However, two issues unravel the hope of exponential speedup for some of these QML algorithms: the data-loading problem and, more recently, the stunning dequantization results of Tang et al. A third issue, namely the fault-tolerance requirements of most QML algorithms, has further hindered their practical realization. The quantum topological data analysis (QTDA) algorithm of Lloyd, Garnerone and Zanardi was one of the first QML algorithms that convincingly offered an expected exponential speedup. From the outset, it did not suffer from the data-loading problem. A recent result has also shown that the generalized problem solved by this algorithm is likely classically intractable, and would therefore be immune to any dequantization efforts. However, the QTDA algorithm of Lloyd et~al. has a time complexity of O(n^4/(ε^2 δ)) (where n is the number of data points, ε is the error tolerance, and δ is the smallest nonzero eigenvalue of the restricted Laplacian) and requires fault-tolerant quantum computing, which has not yet been achieved. In this paper, we completely overhaul the QTDA algorithm to achieve an improved exponential speedup and depth complexity of O(n\log(1/(δε))). Our approach includes three key innovations: (a) an efficient realization of the combinatorial Laplacian as a sum of Pauli operators; (b) a quantum rejection sampling approach to restrict the superposition to the simplices in the complex; and (c) a stochastic rank estimation method to estimate the Betti numbers. We present a theoretical error analysis, and the circuit and computational time and depth complexities for Betti number estimation. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.02811
JITA4DS: Disaggregated execution of Data Science Pipelines between the Edge and the Data Centre,Genoveva Vargas-Solar;Ali Akoglu;Md Sahil Hassan,"This paper targets the execution of data science (DS) pipelines supported by data processing, transmission and sharing across several resources executing greedy processes. Current data science pipelines environments provide various infrastructure services with computing resources such as general-purpose processors (GPP), Graphics Processing Units (GPUs), Field Programmable Gate Arrays (FPGAs) and Tensor Processing Unit (TPU) coupled with platform and software services to design, run and maintain DS pipelines. These one-fits-all solutions impose the complete externalization of data pipeline tasks. However, some tasks can be executed in the edge, and the backend can provide just in time resources to ensure ad-hoc and elastic execution environments. This paper introduces an innovative composable ""Just in Time Architecture"" for configuring DCs for Data Science Pipelines (JITA-4DS) and associated resource management techniques. JITA-4DS is a cross-layer management system that is aware of both the application characteristics and the underlying infrastructures to break the barriers between applications, middleware/operating system, and hardware layers. Vertical integration of these layers is needed for building a customizable Virtual Data Center (VDC) to meet the dynamically changing data science pipelines' requirements such as performance, availability, and energy consumption. Accordingly, the paper shows an experimental simulation devoted to run data science workloads and determine the best strategies for scheduling the allocation of resources implemented by JITA-4DS. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.02558
HIPPODROME: Data Race Repair using Static Analysis Summaries,Andreea Costea;Abhishek Tiwari;Sigmund Chianasta;Kishore R;Abhik Roychoudhury;Ilya Sergey,"Implementing bug-free concurrent programs is a challenging task in modern software development. State-of-the-art static analyses find hundreds of concurrency bugs in production code, scaling to large codebases. Yet, fixing these bugs in constantly changing codebases represents a daunting effort for programmers, particularly because a fix in the concurrent code can introduce other bugs in a subtle way. In this work, we show how to harness compositional static analysis for concurrency bug detection, to enable a new Automated Program Repair (APR) technique for data races in large concurrent Java codebases. The key innovation of our work is an algorithm that translates procedure summaries inferred by the analysis tool for the purpose of bug reporting, into small local patches that fix concurrency bugs (without introducing new ones). This synergy makes it possible to extend the virtues of compositional static concurrency analysis to APR, making our approach effective (it can detect and fix many more bugs than existing tools for data race repair), scalable (it takes seconds to analyse and suggest fixes for sizeable codebases), and usable (generally, it does not require annotations from the users and can perform continuous automated repair). Our study conducted on popular open-source projects has confirmed that our tool automatically produces concurrency fixes similar to those proposed by the developers in the past. △ Less","6 August, 2021",https://arxiv.org/pdf/2108.02490
Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning,Ahmed Ibrahim;Ayman El-Refai;Sara Ahmed;Mariam Aboul-Ela;Hesham M. Eraqi;Mohamed Moustafa,"Due to the mass advancement in ubiquitous technologies nowadays, new pervasive methods have come into the practice to provide new innovative features and stimulate the research on new human-computer interactions. This paper presents a hand gesture recognition method that utilizes the smartphone's built-in speakers and microphones. The proposed system emits an ultrasonic sonar-based signal (inaudible sound) from the smartphone's stereo speakers, which is then received by the smartphone's microphone and processed via a Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data augmentation techniques are proposed to improve the detection accuracy and three dual-channel input fusion methods are compared. The first method merges the dual-channel audio as a single input spectrogram image. The second method adopts early fusion by concatenating the dual-channel spectrograms. The third method adopts late fusion by having two convectional input branches processing each of the dual-channel spectrograms and then the outputs are merged by the last layers. Our experimental results demonstrate a promising detection accuracy for the six gestures presented in our publicly available dataset with an accuracy of 93.58\% as a baseline. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.02148
The MIT Supercloud Dataset,Siddharth Samsi;Matthew L Weiss;David Bestor;Baolin Li;Michael Jones;Albert Reuther;Daniel Edelman;William Arcand;Chansup Byun;John Holodnack;Matthew Hubbell;Jeremy Kepner;Anna Klein;Joseph McDonald;Adam Michaleas;Peter Michaleas;Lauren Milechin;Julia Mullen;Charles Yee;Benjamin Price;Andrew Prout;Antonio Rosa;Allan Vanterpool;Lindsey McEvoy;Anson Cheng,"Artificial intelligence (AI) and Machine learning (ML) workloads are an increasingly larger share of the compute workloads in traditional High-Performance Computing (HPC) centers and commercial cloud systems. This has led to changes in deployment approaches of HPC clusters and the commercial cloud, as well as a new focus on approaches to optimized resource usage, allocations and deployment of new AI frame- works, and capabilities such as Jupyter notebooks to enable rapid prototyping and deployment. With these changes, there is a need to better understand cluster/datacenter operations with the goal of developing improved scheduling policies, identifying inefficiencies in resource utilization, energy/power consumption, failure prediction, and identifying policy violations. In this paper we introduce the MIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the analysis of large scale HPC and datacenter/cloud operations. We provide detailed monitoring logs from the MIT Supercloud system, which include CPU and GPU usage by jobs, memory usage, file system logs, and physical monitoring data. This paper discusses the details of the dataset, collection methodology, data availability, and discusses potential challenge problems being developed using this data. Datasets and future challenge announcements will be available via https://dcc.mit.edu. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.02037
High-Performance Level-1 and Level-2 BLAS,Amit Singh;Cem Bassoy,"The introduction of the Basic Linear Algebra Subroutine (BLAS) in the 1970s paved the way for different libraries to solve the same problem with an improved approach and hardware. The new BLAS implementation led to High-Performance Computing (HPC) innovation. All the love went to the level 3 BLAS due to its humongous application in different fields, not bounded by computer science. However, level 1 and level 2 got neglected; we tried to solve the problem by introducing the new algorithm for the Vector-Vector dot product, Vector-Vector outer product and Matrix-Vector product, which improves the performance of these operations in a significant way. We are not introducing any library but algorithms, which improves upon the current state of art algorithms. Also, we rely on the FMA instruction, OpenMP, and the compiler to optimize the code rather than implementing the algorithm in assembly. Therefore, our current implementation is machine oblivious and depends on the compilers ability to optimize the code. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.02025
Two Treatments of Definite Descriptions in Intuitionist Negative Free Logic,Nils Kürbis,"Sentences containing definite descriptions, expressions of the form `The F', can be formalised using a binary quantifier ι that forms a formula out of two predicates, where ιx[F, G] is read as `The F is G'. This is an innovation over the usual formalisation of definite descriptions with a term forming operator. The present paper compares the two approaches. After a brief overview of the system \mathbf{INF}^ι of intuitionist negative free logic extended by such a quantifier, which was presented in \citep{kurbisiotaI}, \mathbf{INF}^ι is first compared to a system of Tennant's and an axiomatic treatment of a term forming ι operator within intuitionist negative free logic. Both systems are shown to be equivalent to the subsystem of \mathbf{INF}^ι in which the G of ιx[F, G] is restricted to identity. \mathbf{INF}^ι is then compared to an intuitionist version of a system of Lambert's which in addition to the term forming operator has an operator for predicate abstraction for indicating scope distinctions. The two systems will be shown to be equivalent through a translation between their respective languages. Advantages of the present approach over the alternatives are indicated in the discussion. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.01977
How Can Datacenters Join the Smart Grid to Address the Climate Crisis? Using simulation to explore power and cost effects of direct participation in the energy market,Hongyu He,"Amidst the climate crisis, the massive introduction of renewable energy sources has brought tremendous challenges to both the power grid and its surrounding markets. As datacenters have become ever-larger and more powerful, they play an increasingly significant role in the energy arena. With their unique characteristics, datacenters have been proved to be well-suited for regulating the power grid yet currently provide little, if any, such active response. This problem is due to issues such as unsuitability of the market design, high complexity of the currently proposed solutions, as well as the potential risks thereof. This work aims to provide individual datacenters with insights on the feasibility and profitability of directly participating in the energy market. By modelling the power system of datacenters, and by conducting simulations on real-world datacenter traces, we demonstrate the substantial financial incentive for individual datacenters to directly participate in both the day-ahead and the balancing markets. In turn, we suggest a new short-term, direct scheme of market participation for individual datacenters in place of the current long-term, inactive participation. Furthermore, we develop a novel proactive DVFS scheduling algorithm that can both reduce energy consumption and save energy costs during the market participation of datacenters. Also, in developing this scheduler, we propose an innovative combination of machine learning methods and the DVFS technology that can provide the power grid with indirect demand response (DR). Our experimental results strongly support that individual datacenters can and should directly participate in the energy market both to save their energy costs and to curb their energy consumption, whilst providing the power grid with indirect DR. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.01776
Multi-Frequency GPR Microwave Imaging of Sparse Targets Through a Multi-Task Bayesian Compressive Sensing Approach,Marco Salucci;Nicola Anselmi,"An innovative inverse scattering (IS) method is proposed for the quantitative imaging of pixel-sparse scatterers buried within a lossy half-space. On the one hand, such an approach leverages on the wide-band nature of ground penetrating radar (GPR) data by jointly processing the multi-frequency (MF) spectral components of the collected radargrams. On the other hand, it enforces sparsity priors on the problem unknowns to yield regularized solutions of the fully non-linear scattering equations. Towards this end, a multi-task Bayesian Compressive Sensing (MT-BCS) methodology is adopted and suitably customized to take full advantage of the available frequency diversity and of the a-priori information on the class of imaged targets. Representative results are reported to assess the proposed MF-MT-BCS strategy also in comparison with competitive state-of-the-art alternatives. △ Less","3 August, 2021",https://arxiv.org/pdf/2108.01627
Adoption Factors of Enabling I4.0 Technologies and Benefits in the Supply Chain,José Carlos Franceli;Silvia Novaes Zilber Turri,"Industry 4.0 technologies is a new paradigm of integration of cyber-physical systems and information and communication solutions. This topic has been vaguely explored in social sciences; hence this study attempts to bridge this gap by investigating adoption process, based on I4.0 technologies. This paper aims to identify the barriers and benefits generated in the Supply Chain through a systematic literature review. The results present a framework that connects adoption factors, enabling technologies of I4.0, and benefits to the Supply Chain. innovations to be adopted. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.01207
Hybrid Quantum-Classical Neural Network for Incident Detection,Zadid Khan;Sakib Mahmud Khan;Jean Michel Tine;Ayse Turhan Comert;Diamon Rice;Gurcan Comert;Dimitra Michalaka;Judith Mwakalonge;Reek Majumdar;Mashrur Chowdhury,"The efficiency and reliability of real-time incident detection models directly impact the affected corridors' traffic safety and operational conditions. The recent emergence of cloud-based quantum computing infrastructure and innovations in noisy intermediate-scale quantum devices have revealed a new era of quantum-enhanced algorithms that can be leveraged to improve real-time incident detection accuracy. In this research, a hybrid machine learning model, which includes classical and quantum machine learning (ML) models, is developed to identify incidents using the connected vehicle (CV) data. The incident detection performance of the hybrid model is evaluated against baseline classical ML models. The framework is evaluated using data from a microsimulation tool for different incident scenarios. The results indicate that a hybrid neural network containing a 4-qubit quantum layer outperforms all other baseline models when there is a lack of training data. We have created three datasets; DS-1 with sufficient training data, and DS-2 and DS-3 with insufficient training data. The hybrid model achieves a recall of 98.9%, 98.3%, and 96.6% for DS-1, DS-2, and DS-3, respectively. For DS-2 and DS-3, the average improvement in F2-score (measures model's performance to correctly identify incidents) achieved by the hybrid model is 1.9% and 7.8%, respectively, compared to the classical models. It shows that with insufficient data, which may be common for CVs, the hybrid ML model will perform better than the classical models. With the continuing improvements of quantum computing infrastructure, the quantum ML models could be a promising alternative for CV-related applications when the available data is insufficient. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.01127
A Framework for Multi-View Classification of Features,Khalil Taheri;Hadi Moradi;Mostafa Tavassolipour,"One of the most important problems in the field of pattern recognition is data classification. Due to the increasing development of technologies introduced in the field of data classification, some of the solutions are still open and need more research. One of the challenging problems in this area is the curse of dimensionality of the feature set of the data classification problem. In solving the data classification problems, when the feature set is too large, typical approaches will not be able to solve the problem. In this case, an approach can be used to partition the feature set into multiple feature sub-sets so that the data classification problem is solved for each of the feature subsets and finally using the ensemble classification, the classification is applied to the entire feature set. In the above-mentioned approach, the partitioning of feature set into feature sub-sets is still an interesting area in the literature of this field. In this research, an innovative framework for multi-view ensemble classification, inspired by the problem of object recognition in the multiple views theory of humans, is proposed. In this method, at first, the collaboration values between the features is calculated using a criterion called the features collaboration criterion. Then, the collaboration graph is formed based on the calculated collaboration values. In the next step, using the community detection method, graph communities are found. The communities are considered as the problem views and the different base classifiers are trained for different views using the views corresponding training data. The multi-view ensemble classifier is then formed by a combination of base classifiers based on the AdaBoost algorithm. The simulation results of the proposed method on the real and synthetic datasets show that the proposed method increases the classification accuracy. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.01019
LDDMM-Face: Large Deformation Diffeomorphic Metric Learning for Flexible and Consistent Face Alignment,Huilin Yang;Junyan Lyu;Pujin Cheng;Xiaoying Tang,"We innovatively propose a flexible and consistent face alignment framework, LDDMM-Face, the key contribution of which is a deformation layer that naturally embeds facial geometry in a diffeomorphic way. Instead of predicting facial landmarks via heatmap or coordinate regression, we formulate this task in a diffeomorphic registration manner and predict momenta that uniquely parameterize the deformation between initial boundary and true boundary, and then perform large deformation diffeomorphic metric mapping (LDDMM) simultaneously for curve and landmark to localize the facial landmarks. Due to the embedding of LDDMM into a deep network, LDDMM-Face can consistently annotate facial landmarks without ambiguity and flexibly handle various annotation schemes, and can even predict dense annotations from sparse ones. Our method can be easily integrated into various face alignment networks. We extensively evaluate LDDMM-Face on four benchmark datasets: 300W, WFLW, HELEN and COFW-68. LDDMM-Face is comparable or superior to state-of-the-art methods for traditional within-dataset and same-annotation settings, but truly distinguishes itself with outstanding performance when dealing with weakly-supervised learning (partial-to-full), challenging cases (e.g., occluded faces), and different training and prediction datasets. In addition, LDDMM-Face shows promising results on the most challenging task of predicting across datasets with different annotation schemes. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.00690
Towards Making Deep Learning-based Vulnerability Detectors Robust,Zhen Li;Jing Tang;Deqing Zou;Qian Chen;Shouhuai Xu;Chao Zhang;Yichen Li;Hai Jin,"Automatically detecting software vulnerabilities in source code is an important problem that has attracted much attention. In particular, deep learning-based vulnerability detectors, or DL-based detectors, are attractive because they do not need human experts to define features or patterns of vulnerabilities. However, such detectors' robustness is unclear. In this paper, we initiate the study in this aspect by demonstrating that DL-based detectors are not robust against simple code transformations, dubbed attacks in this paper, as these transformations may be leveraged for malicious purposes. As a first step towards making DL-based detectors robust against such attacks, we propose an innovative framework, dubbed ZigZag, which is centered at (i) decoupling feature learning and classifier learning and (ii) using a ZigZag-style strategy to iteratively refine them until they converge to robust features and robust classifiers. Experimental results show that the ZigZag framework can substantially improve the robustness of DL-based detectors. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.00669
Recurrent Mask Refinement for Few-Shot Medical Image Segmentation,Hao Tang;Xingwei Liu;Shanlin Sun;Xiangyi Yan;Xiaohui Xie,"Although having achieved great success in medical image segmentation, deep convolutional neural networks usually require a large dataset with manual annotations for training and are difficult to generalize to unseen classes. Few-shot learning has the potential to address these challenges by learning new classes from only a few labeled examples. In this work, we propose a new framework for few-shot medical image segmentation based on prototypical networks. Our innovation lies in the design of two key modules: 1) a context relation encoder (CRE) that uses correlation to capture local relation features between foreground and background regions; and 2) a recurrent mask refinement module that repeatedly uses the CRE and a prototypical network to recapture the change of context relationship and refine the segmentation mask iteratively. Experiments on two abdomen CT datasets and an abdomen MRI dataset show the proposed method obtains substantial improvement over the state-of-the-art methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively. Code is publicly available. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.00622
A Survey on Software Engineering Practices in Brazilian Startups,Renata Souza;Orges Cico;Ivan Machado,"Today's significant technological advancement allows early-stage software startups to build and launch innovative products quickly on the market. However, many of them die in the early years of their path due to market conditions, ignorance of customer needs, lack of resources, or focus, such as the misuse of well-established practices. The study's motivation is to analyze software engineering practices in startups from a practitioner's perspective. Our objective was to identify practices and tools the startups employ in their daily routines. We carried out an expert survey study with 140 software developers involved in software startups from different domains. The results show that startups in the initial and validation phases select practices and tools on an ad-hoc basis and based on the development team's prior knowledge. When they move into the growth phase, they recognize that they could have adopted better practices beforehand to support product scaling with a more mature team. The results also indicated that support tools are selected based on their integration with other tools and their ability to automate operational activities. △ Less","31 July, 2021",https://arxiv.org/pdf/2108.00343
"Seeing poverty from space, how much can it be tuned?",Tomas Sako;Arturo Jr M. Martinez,"Since the United Nations launched the Sustainable Development Goals (SDG) in 2015, numerous universities, NGOs and other organizations have attempted to develop tools for monitoring worldwide progress in achieving them. Led by advancements in the fields of earth observation techniques, data sciences and the emergence of artificial intelligence, a number of research teams have developed innovative tools for highlighting areas of vulnerability and tracking the implementation of SDG targets. In this paper we demonstrate that individuals with no organizational affiliation and equipped only with common hardware, publicly available datasets and cloud-based computing services can participate in the improvement of predicting machine-learning-based approaches to predicting local poverty levels in a given agro-ecological environment. The approach builds upon several pioneering efforts over the last five years related to mapping poverty by deep learning to process satellite imagery and ""ground-truth"" data from the field to link features with incidence of poverty in a particular context. The approach employs new methods for object identification in order to optimize the modeled results and achieve significantly high accuracy. A key goal of the project was to intentionally keep costs as low as possible - by using freely available resources - so that citizen scientists, students and organizations could replicate the method in other areas of interest. Moreover, for simplicity, the input data used were derived from just a handful of sources (involving only earth observation and population headcounts). The results of the project could therefore certainly be strengthened further through the integration of proprietary data from social networks, mobile phone providers, and other sources. △ Less","30 July, 2021",https://arxiv.org/pdf/2107.14700
Sensing and Mapping for Better Roads: Initial Plan for Using Federated Learning and Implementing a Digital Twin to Identify the Road Conditions in a Developing Country -- Sri Lanka,Thilanka Munasinghe;HR Pasindu,"We propose how a developing country like Sri Lanka can benefit from privacy-enabled machine learning techniques such as Federated Learning to detect road conditions using crowd-sourced data collection and proposed the idea of implementing a Digital Twin for the national road system in Sri Lanka. Developing countries such as Sri Lanka are far behind in implementing smart road systems and smart cities compared to the developed countries. The proposed work discussed in this paper matches the UN Sustainable Development Goal (SDG) 9: ""Build Resilient Infrastructure, Promote Inclusive and Sustainable Industrialization and Foster Innovation"". Our proposed work discusses how the government and private sector vehicles that conduct routine trips to collect crowd-sourced data using smartphone devices to identify the road conditions and detect where the potholes, surface unevenness (roughness), and other major distresses are located on the roads. We explore Mobile Edge Computing (MEC) techniques that can bring machine learning intelligence closer to the edge devices where produced data is stored and show how the applications of Federated Learning can be made to detect and improve road conditions. During the second phase of this study, we plan to implement a Digital Twin for the road system in Sri Lanka. We intend to use data provided by both Dedicated and Non-Dedicated systems in the proposed Digital Twin for the road system. As of writing this paper, and best to our knowledge, there is no Digital Twin system implemented for roads and other infrastructure systems in Sri Lanka. The proposed Digital Twin will be one of the first implementations of such systems in Sri Lanka. Lessons learned from this pilot project will benefit other developing countries who wish to follow the same path and make data-driven decisions. △ Less","30 July, 2021",https://arxiv.org/pdf/2107.14551
Fine-Grained Classroom Activity Detection from Audio with Neural Networks,Eric Slyman;Chris Daw;Morgan Skrabut;Ana Usenko;Brian Hutchinson,"Instructors are increasingly incorporating student-centered learning techniques in their classrooms to improve learning outcomes. In addition to lecture, these class sessions involve forms of individual and group work, and greater rates of student-instructor interaction. Quantifying classroom activity is a key element of accelerating the evaluation and refinement of innovative teaching practices, but manual annotation does not scale. In this manuscript, we present advances to the young application area of automatic classroom activity detection from audio. Using a university classroom corpus with nine activity labels (e.g., ""lecture,"" ""group work,"" ""student question""), we propose and evaluate deep fully connected, convolutional, and recurrent neural network architectures, comparing the performance of mel-filterbank, OpenSmile, and self-supervised acoustic features. We compare 9-way classification performance with 5-way and 4-way simplifications of the task and assess two types of generalization: (1) new class sessions from previously seen instructors, and (2) previously unseen instructors. We obtain strong results on the new fine-grained task and state-of-the-art on the 4-way task: our best model obtains frame-level error rates of 6.2%, 7.7% and 28.0% when generalizing to unseen instructors for the 4-way, 5-way, and 9-way classification tasks, respectively (relative reductions of 35.4%, 48.3% and 21.6% over a strong baseline). When estimating the aggregate time spent on classroom activities, our average root mean squared error is 1.64 minutes per class session, a 54.9% relative reduction over the baseline. △ Less","9 November, 2021",https://arxiv.org/pdf/2107.14369
A Checklist for Explainable AI in the Insurance Domain,Olivier Koster;Ruud Kosman;Joost Visser,"Artificial intelligence (AI) is a powerful tool to accomplish a great many tasks. This exciting branch of technology is being adopted increasingly across varying sectors, including the insurance domain. With that power arise several complications. One of which is a lack of transparency and explainability of an algorithm for experts and non-experts alike. This brings into question both the usefulness as well as the accuracy of the algorithm, coupled with an added difficulty to assess potential biases within the data or the model. In this paper, we investigate the current usage of AI algorithms in the Dutch insurance industry and the adoption of explainable artificial intelligence (XAI) techniques. Armed with this knowledge we design a checklist for insurance companies that should help assure quality standards regarding XAI and a solid foundation for cooperation between organisations. This checklist extends an existing checklist of SIVI, the standardisation institute for digital cooperation and innovation in Dutch insurance. △ Less","18 July, 2021",https://arxiv.org/pdf/2107.14039
Closing the B-tree vs. LSM-tree Write Amplification Gap on Modern Storage Hardware with Built-in Transparent Compression,Yifan Qiao;Xubin Chen;Ning Zheng;Jiangpeng Li;Yang Liu;Tong Zhang,"This paper studies the design of B-tree that can take full advantage of modern storage hardware with built-in transparent compression. Recent years have witnessed significant interest in applying log-structured merge tree (LSM-tree) as an alternative to B-tree. The current consensus is that, compared with B-tree, LSM-tree has distinct advantages in terms of storage space efficiency and write amplification. This paper argues that one should revisit this belief upon the arrival of storage hardware with built-in transparent compression. Advanced storage appliances~(e.g., all-flash array) and emerging computational storage drives perform hardware-based lossless data compression, transparent to OS and user applications. Beyond straightforwardly reducing the physical storage cost difference between B-tree and LSM-tree, such modern storage hardware brings new opportunities to innovate B-tree implementation in order to largely reduce its write amplification. As the first step to explore the potential, this paper presents three simple design techniques (i.e., deterministic page shadowing, localized page modification logging, and sparse redo logging) that can leverage such modern storage hardware to significantly reduce the B-tree write amplification. We implemented these design techniques and carried out experiments on a commercial storage drive with built-in transparent compression. The results show that the proposed design techniques can reduce the B-tree write amplification by over 10x. Compared with RocksDB (a popular key-value store built upon LSM-tree), the implemented B-tree can achieve similar or even smaller write amplification and physical storage space usage. △ Less","26 July, 2021",https://arxiv.org/pdf/2107.13987
Underwater Acoustic Networks for Security Risk Assessment in Public Drinking Water Reservoirs,Jörg Stork;Philip Wenzel;Severin Landwein;Maria-Elena Algorri;Martin Zaefferer;Wolfgang Kusch;Martin Staubach;Thomas Bartz-Beielstein;Hartmut Köhn;Hermann Dejager;Christian Wolf,"We have built a novel system for the surveillance of drinking water reservoirs using underwater sensor networks. We implement an innovative AI-based approach to detect, classify and localize underwater events. In this paper, we describe the technology and cognitive AI architecture of the system based on one of the sensor networks, the hydrophone network. We discuss the challenges of installing and using the hydrophone network in a water reservoir where traffic, visitors, and variable water conditions create a complex, varying environment. Our AI solution uses an autoencoder for unsupervised learning of latent encodings for classification and anomaly detection, and time delay estimates for sound localization. Finally, we present the results of experiments carried out in a laboratory pool and the water reservoir and discuss the system's potential. △ Less","29 July, 2021",https://arxiv.org/pdf/2107.13977
Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection,Shi-Xue Zhang;Xiaobin Zhu;Chun Yang;Hongfa Wang;Xu-Cheng Yin,"Arbitrary shape text detection is a challenging task due to the high complexity and variety of scene texts. In this work, we propose a novel adaptive boundary proposal network for arbitrary shape text detection, which can learn to directly produce accurate boundary for arbitrary shape text without any post-processing. Our method mainly consists of a boundary proposal model and an innovative adaptive boundary deformation model. The boundary proposal model constructed by multi-layer dilated convolutions is adopted to produce prior information (including classification map, distance field, and direction field) and coarse boundary proposals. The adaptive boundary deformation model is an encoder-decoder network, in which the encoder mainly consists of a Graph Convolutional Network (GCN) and a Recurrent Neural Network (RNN). It aims to perform boundary deformation in an iterative way for obtaining text instance shape guided by prior information from the boundary proposal model. In this way, our method can directly and efficiently generate accurate text boundaries without complex post-processing. Extensive experiments on publicly available datasets demonstrate the state-of-the-art performance of our method. △ Less","13 August, 2021",https://arxiv.org/pdf/2107.12664
Employee-Driven Innovation to Fuel Internal Software Startups: Preliminary Findings,Anastasiia Tkalich;Nils Brede Moe;Tor Sporsem,"To keep up with the pace of innovation, established companies are increasingly relying on internal software startups. However, succeeding with such startups is a challenging task because internal startups need to find a balance between the interests of the company and the interest of the innovator. One approach that is argued to strengthen innovation in existing companies is employee-driven innovation (EDI). This study explores this argument by examining two internal software startups in companies aligned with the principles of EDI and with a strong focus on innovation. The preliminary findings indicate that startups with EDI are characterized by commitment towards innovation, cooperative orientation, and autonomy. The findings suggest that internal software startups may be strengthened when the parent companies practice EDI. △ Less","27 July, 2021",https://arxiv.org/pdf/2107.12659
How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review,Florian Tambon;Gabriel Laberge;Le An;Amin Nikanjam;Paulina Stevia Nouwou Mindom;Yann Pequignot;Foutse Khomh;Giulio Antoniol;Ettore Merlo;François Laviolette,"Context: Machine Learning (ML) has been at the heart of many innovations over the past years. However, including it in so-called 'safety-critical' systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches. Objective: This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question 'How to Certify Machine Learning Based Safety-critical Systems?'. Method: We conduct a Systematic Literature Review (SLR) of research papers published between 2015 to 2020, covering topics related to the certification of ML systems. In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted. Results: The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of models. It also emphasized the need to further develop connections between academia and industries to deepen the domain study. Finally, it also illustrated the necessity to build connections between the above mention main pillars that are for now mainly studied separately. Conclusion: We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions. △ Less","1 December, 2021",https://arxiv.org/pdf/2107.12045
CP-loss: Connectivity-preserving Loss for Road Curb Detection in Autonomous Driving with Aerial Images,Zhenhua Xu;Yuxiang Sun;Lujia Wang;Ming Liu,"Road curb detection is important for autonomous driving. It can be used to determine road boundaries to constrain vehicles on roads, so that potential accidents could be avoided. Most of the current methods detect road curbs online using vehicle-mounted sensors, such as cameras or 3-D Lidars. However, these methods usually suffer from severe occlusion issues. Especially in highly-dynamic traffic environments, most of the field of view is occupied by dynamic objects. To alleviate this issue, we detect road curbs offline using high-resolution aerial images in this paper. Moreover, the detected road curbs can be used to create high-definition (HD) maps for autonomous vehicles. Specifically, we first predict the pixel-wise segmentation map of road curbs, and then conduct a series of post-processing steps to extract the graph structure of road curbs. To tackle the disconnectivity issue in the segmentation maps, we propose an innovative connectivity-preserving loss (CP-loss) to improve the segmentation performance. The experimental results on a public dataset demonstrate the effectiveness of our proposed loss function. This paper is accompanied with a demonstration video and a supplementary document, which are available at \texttt{\url{https://sites.google.com/view/cp-loss}}. △ Less","25 July, 2021",https://arxiv.org/pdf/2107.11920
Spatio-Temporal Representation Factorization for Video-based Person Re-Identification,Abhishek Aich;Meng Zheng;Srikrishna Karanam;Terrence Chen;Amit K. Roy-Chowdhury;Ziyan Wu,"Despite much recent progress in video-based person re-identification (re-ID), the current state-of-the-art still suffers from common real-world challenges such as appearance similarity among various people, occlusions, and frame misalignment. To alleviate these problems, we propose Spatio-Temporal Representation Factorization (STRF), a flexible new computational unit that can be used in conjunction with most existing 3D convolutional neural network architectures for re-ID. The key innovations of STRF over prior work include explicit pathways for learning discriminative temporal and spatial features, with each component further factorized to capture complementary person-specific appearance and motion information. Specifically, temporal factorization comprises two branches, one each for static features (e.g., the color of clothes) that do not change much over time, and dynamic features (e.g., walking patterns) that change over time. Further, spatial factorization also comprises two branches to learn both global (coarse segments) as well as local (finer segments) appearance features, with the local features particularly useful in cases of occlusion or spatial misalignment. These two factorization operations taken together result in a modular architecture for our parameter-wise light STRF unit that can be plugged in between any two 3D convolutional layers, resulting in an end-to-end learning framework. We empirically show that STRF improves performance of various existing baseline architectures while demonstrating new state-of-the-art results using standard person re-ID evaluation protocols on three benchmarks. △ Less","14 August, 2021",https://arxiv.org/pdf/2107.11878
Quantifying machine learning-induced overdiagnosis in sepsis,Anna Fedyukova;Douglas Pires;Daniel Capurro,"The proliferation of early diagnostic technologies, including self-monitoring systems and wearables, coupled with the application of these technologies on large segments of healthy populations may significantly aggravate the problem of overdiagnosis. This can lead to unwanted consequences such as overloading health care systems and overtreatment, with potential harms to healthy individuals. The advent of machine-learning tools to assist diagnosis -- while promising rapid and more personalised patient management and screening -- might contribute to this issue. The identification of overdiagnosis is usually post hoc and demonstrated after long periods (from years to decades) and costly randomised control trials. In this paper, we present an innovative approach that allows us to preemptively detect potential cases of overdiagnosis during predictive model development. This approach is based on the combination of labels obtained from a prediction model and clustered medical trajectories, using sepsis in adults as a test case. This is one of the first attempts to quantify machine-learning induced overdiagnosis and we believe will serves as a platform for further development, leading to guidelines for safe deployment of computational diagnostic tools. △ Less","3 July, 2021",https://arxiv.org/pdf/2107.10399
Comparing OpenMP Implementations With Applications Across A64FX Platforms,Benjamin Michalowicz;Eric Raut;Yan Kang;Tony Curtis;Barbara Chapman;Dossay Oryspayev,"The development of the A64FX processor by Fujitsu has created a massive innovation in High-Performance Computing and the birth of Fugaku: the current world's fastest supercomputer. A variety of tools are used to analyze the run-times and performances of several applications, and in particular, how these applications scale on the A64FX processor. We examine the performance and behavior of applications through OpenMP scaling and how their performance differs across different compilers on the new Ookami cluster at Stony Brook University as well as the Fugaku supercomputer at RIKEN in Japan. △ Less","21 July, 2021",https://arxiv.org/pdf/2107.10346
TLA: Twitter Linguistic Analysis,Tushar Sarkar;Nishant Rajadhyaksha,"Linguistics has been instrumental in developing a deeper understanding of human nature. Words are indispensable to bequeath the thoughts, emotions, and purpose of any human interaction, and critically analyzing these words can elucidate the social and psychological behavior and characteristics of these social animals. Social media has become a platform for human interaction on a large scale and thus gives us scope for collecting and using that data for our study. However, this entire process of collecting, labeling, and analyzing this data iteratively makes the entire procedure cumbersome. To make this entire process easier and structured, we would like to introduce TLA(Twitter Linguistic Analysis). In this paper, we describe TLA and provide a basic understanding of the framework and discuss the process of collecting, labeling, and analyzing data from Twitter for a corpus of languages while providing detailed labeled datasets for all the languages and the models are trained on these datasets. The analysis provided by TLA will also go a long way in understanding the sentiments of different linguistic communities and come up with new and innovative solutions for their problems based on the analysis. △ Less","20 July, 2021",https://arxiv.org/pdf/2107.09710
Evolutionary Innovation Viewed as Novel Physical Phenomena and Hierarchical Systems Building,Tim Taylor,"In previous work I proposed a framework for thinking about open-ended evolution. The framework characterised the basic processes required for Darwinian evolution as: (1) the generation of a phenotype from a genetic description; (2) the evaluation of that phenotype; and (3) the reproduction with variation of successful genotype-phenotypes. My treatment emphasized the potential influence of the biotic and abiotic environment, and of the laws of physics/chemistry, on each of these processes. I demonstrated the conditions under which these processes can allow for ongoing exploration of a space of possible phenotypes (which I labelled exploratory open-endedness). However, these processes by themselves cannot expand the space of possible phenotypes and therefore cannot account for the more interesting and unexpected kinds of evolutionary innovation (such as those I labelled expansive and transformational open-endedness). In the previous work I looked at ways in which expansive and transformational innovations could arise. I proposed transdomain bridges and non-additive compositional systems as two mechanisms by which these kinds of innovations could arise. In the current paper I wish to generalise and expand upon these two concepts. I do this by adopting the Parameter Space-Organisation Space-Action Space (POA) perspective, as suggested at in my previous work, and proposing that all evolutionary innovations can be viewed as either capturing some novel physical phenomena that had previously been unused, or as the creation of new persistent systems within the environment. △ Less","20 July, 2021",https://arxiv.org/pdf/2107.09669
RansomClave: Ransomware Key Management using SGX,Alpesh Bhudia;Daniel O'Keeffe;Daniele Sgandurra;Darren Hurley-Smith,"Modern ransomware often generate and manage cryptographic keys on the victim's machine, giving defenders an opportunity to capture exposed keys and recover encrypted data without paying the ransom. However, recent work has raised the possibility of future enclave-enhanced malware that could avoid such mitigations using emerging support for hardware-enforced secure enclaves in commodity CPUs. Nonetheless, the practicality of such enclave-enhanced malware and its potential impact on all phases of the ransomware lifecyle remain unclear. Given the demonstrated capacity of ransomware authors to innovate in order to better extort their victims (e.g. through the adoption of untraceable virtual currencies and anonymity networks), it is important to better understand the risks involved and identify potential mitigations. As a basis for comprehensive security and performance analysis of enclave-enhanced ransomware, we present RansomClave, a family of ransomware that securely manage their cryptographic keys using an enclave. We use RansomClave to explore the implications of enclave-enhanced ransomware for the key generation, encryption and key release phases of the ransomware lifecycle, and to identify potential limitations and mitigations. We propose two plausible victim models and analyse, from an attacker's perspective, how RansomClave can protect cryptographic keys from each type of victim. We find that some existing mitigations are likely to be effective during the key generation and encryption phases, but that RansomClave enables new trustless key release schemes that could potentially improve attacker's profitability and, by extension, make enclaves an attractive target for future attackers. △ Less","20 July, 2021",https://arxiv.org/pdf/2107.09470
Assessment of Self-Attention on Learned Features For Sound Event Localization and Detection,Parthasaarathy Sudarsanam;Archontis Politis;Konstantinos Drossos,"Joint sound event localization and detection (SELD) is an emerging audio signal processing task adding spatial dimensions to acoustic scene analysis and sound event detection. A popular approach to modeling SELD jointly is using convolutional recurrent neural network (CRNN) models, where CNNs learn high-level features from multi-channel audio input and the RNNs learn temporal relationships from these high-level features. However, RNNs have some drawbacks, such as a limited capability to model long temporal dependencies and slow training and inference times due to their sequential processing nature. Recently, a few SELD studies used multi-head self-attention (MHSA), among other innovations in their models. MHSA and the related transformer networks have shown state-of-the-art performance in various domains. While they can model long temporal dependencies, they can also be parallelized efficiently. In this paper, we study in detail the effect of MHSA on the SELD task. Specifically, we examined the effects of replacing the RNN blocks with self-attention layers. We studied the influence of stacking multiple self-attention blocks, using multiple attention heads in each self-attention block, and the effect of position embeddings and layer normalization. Evaluation on the DCASE 2021 SELD (task 3) development data set shows a significant improvement in all employed metrics compared to the baseline CRNN accompanying the task. △ Less","27 September, 2021",https://arxiv.org/pdf/2107.09388
The approach with the Data Protection and Privacy Relationships Model (DAPPREMO),Nicola Fabiano,"We describe the Data Protection and Privacy Relationships Model (DAPPREMO), which is based on the set theory, considering that both the data protection and privacy regulation and Ethics principles in those domains belong to a set. DAPPREMO is a new and innovative solution to adopt a model in any data protection and privacy activities. We theorise that DAPPREMO is an innovative approach to have a broad overview of all the objects related to a specific case or more cases from data protection and privacy perspective. We describe DAPPREMO as a solution for a multidisciplinary approach to address any data protection and privacy issue. △ Less","19 July, 2021",https://arxiv.org/pdf/2107.09113
Secure Aerial Surveillance using Split Learning,Yoo Jeong Ha;Minjae Yoo;Soohyun Park;Soyi Jung;Joongheon Kim,"Personal monitoring devices such as cyclist helmet cameras to record accidents or dash cams to catch collisions have proliferated, with more companies producing smaller and compact recording gadgets. As these devices are becoming a part of citizens' everyday arsenal, concerns over the residents' privacy are progressing. Therefore, this paper presents SASSL, a secure aerial surveillance drone using split learning to classify whether there is a presence of a fire on the streets. This innovative split learning method transfers CCTV footage captured with a drone to a nearby server to run a deep neural network to detect a fire's presence in real-time without exposing the original data. We devise a scenario where surveillance UAVs roam around the suburb, recording any unnatural behavior. The UAV can process the recordings through its on-mobile deep neural network system or transfer the information to a server. Due to the resource limitations of mobile UAVs, the UAV does not have the capacity to run an entire deep neural network on its own. This is where the split learning method comes in handy. The UAV runs the deep neural network only up to the first hidden layer and sends only the feature map to the cloud server, where the rest of the deep neural network is processed. By ensuring that the learning process is divided between the UAV and the server, the privacy of raw data is secured while the UAV does not overexert its minimal resources. △ Less","19 July, 2021",https://arxiv.org/pdf/2107.08628
"Train on Small, Play the Large: Scaling Up Board Games with AlphaZero and GNN",Shai Ben-Assayag;Ran El-Yaniv,"Playing board games is considered a major challenge for both humans and AI researchers. Because some complicated board games are quite hard to learn, humans usually begin with playing on smaller boards and incrementally advance to master larger board strategies. Most neural network frameworks that are currently tasked with playing board games neither perform such incremental learning nor possess capabilities to automatically scale up. In this work, we look at the board as a graph and combine a graph neural network architecture inside the AlphaZero framework, along with some other innovative improvements. Our ScalableAlphaZero is capable of learning to play incrementally on small boards, and advancing to play on large ones. Our model can be trained quickly to play different challenging board games on multiple board sizes, without using any domain knowledge. We demonstrate the effectiveness of ScalableAlphaZero and show, for example, that by training it for only three days on small Othello boards, it can defeat the AlphaZero model on a large board, which was trained to play the large board for 30 days. △ Less","18 July, 2021",https://arxiv.org/pdf/2107.08387
Port-Hamiltonian Neural Networks for Learning Explicit Time-Dependent Dynamical Systems,Shaan Desai;Marios Mattheakis;David Sondak;Pavlos Protopapas;Stephen Roberts,"Accurately learning the temporal behavior of dynamical systems requires models with well-chosen learning biases. Recent innovations embed the Hamiltonian and Lagrangian formalisms into neural networks and demonstrate a significant improvement over other approaches in predicting trajectories of physical systems. These methods generally tackle autonomous systems that depend implicitly on time or systems for which a control signal is known apriori. Despite this success, many real world dynamical systems are non-autonomous, driven by time-dependent forces and experience energy dissipation. In this study, we address the challenge of learning from such non-autonomous systems by embedding the port-Hamiltonian formalism into neural networks, a versatile framework that can capture energy dissipation and time-dependent control forces. We show that the proposed \emph{port-Hamiltonian neural network} can efficiently learn the dynamics of nonlinear physical systems of practical interest and accurately recover the underlying stationary Hamiltonian, time-dependent force, and dissipative coefficient. A promising outcome of our network is its ability to learn and predict chaotic systems such as the Duffing equation, for which the trajectories are typically hard to learn. △ Less","16 July, 2021",https://arxiv.org/pdf/2107.08024
A Comparison of Modern General-Purpose Visual SLAM Approaches,Alexey Merzlyakov;Steve Macenski,"Advancing maturity in mobile and legged robotics technologies is changing the landscapes where robots are being deployed and found. This innovation calls for a transformation in simultaneous localization and mapping (SLAM) systems to support this new generation of service and consumer robots. No longer can traditionally robust 2D lidar systems dominate while robots are being deployed in multi-story indoor, outdoor unstructured, and urban domains with increasingly inexpensive stereo and RGB-D cameras. Visual SLAM (VSLAM) systems have been a topic of study for decades and a small number of openly available implementations have stood out: ORB-SLAM3, OpenVSLAM and RTABMap. This paper presents a comparison of these 3 modern, feature rich, and uniquely robust VSLAM techniques that have yet to be benchmarked against each other, using several different datasets spanning multiple domains negotiated by service robots. ORB-SLAM3 and OpenVSLAM each were not compared against at least one of these datasets previously in literature and we provide insight through this lens. This analysis is motivated to find general purpose, feature complete, and multi-domain VSLAM options to support a broad class of robot applications for integration into the new and improved ROS 2 Nav2 System as suitable alternatives to traditional 2D lidar solutions. △ Less","5 August, 2021",https://arxiv.org/pdf/2107.07589
Reuse of Semantic Models for Emerging Smart Grids Applications,Valentina Janev;Dušan Popadić;Dea Pujić;Maria Esther Vidal;Kemele Endris,"Data in the energy domain grows at unprecedented rates. Despite the great potential that IoT platforms and other big data-driven technologies have brought in the energy sector, data exchange and data integration are still not wholly achieved. As a result, fragmented applications are developed against energy data silos, and data exchange is limited to few applications. Therefore, this paper identifies semantic models that can be reused for building interoperable energy management services and applications. The ambition is to innovate the Institute Mihajlo Pupin proprietary SCADA system and to enable integration of the Institute Mihajlo Pupin services and applications in the European Union (EU) Energy Data Space. The selection of reusable models has been done based on a set of scenarios related to electricity balancing services, predictive maintenance services, and services for the residential, commercial and industrial sectors. △ Less","8 July, 2021",https://arxiv.org/pdf/2107.06999
From Show to Tell: A Survey on Deep Learning-based Image Captioning,Matteo Stefanini;Marcella Cornia;Lorenzo Baraldi;Silvia Cascianelli;Giuseppe Fiameni;Rita Cucchiara,"Connecting Vision and Language plays an essential role in Generative Intelligence. For this reason, large research efforts have been devoted to image captioning, i.e. describing images with syntactically and semantically meaningful sentences. Starting from 2015 the task has generally been addressed with pipelines composed of a visual encoder and a language model for text generation. During these years, both components have evolved considerably through the exploitation of object regions, attributes, the introduction of multi-modal connections, fully-attentive approaches, and BERT-like early-fusion strategies. However, regardless of the impressive results, research in image captioning has not reached a conclusive answer yet. This work aims at providing a comprehensive overview of image captioning approaches, from visual encoding and text generation to training strategies, datasets, and evaluation metrics. In this respect, we quantitatively compare many relevant state-of-the-art approaches to identify the most impactful technical innovations in architectures and training strategies. Moreover, many variants of the problem and its open challenges are discussed. The final goal of this work is to serve as a tool for understanding the existing literature and highlighting the future directions for a research area where Computer Vision and Natural Language Processing can find an optimal synergy. △ Less","30 November, 2021",https://arxiv.org/pdf/2107.06912
Artificial Intelligence in PET: an Industry Perspective,Arkadiusz Sitek;Sangtae Ahn;Evren Asma;Adam Chandler;Alvin Ihsani;Sven Prevrhal;Arman Rahmim;Babak Saboury;Kris Thielemans,"Artificial intelligence (AI) has significant potential to positively impact and advance medical imaging, including positron emission tomography (PET) imaging applications. AI has the ability to enhance and optimize all aspects of the PET imaging chain from patient scheduling, patient setup, protocoling, data acquisition, detector signal processing, reconstruction, image processing and interpretation. AI poses industry-specific challenges which will need to be addressed and overcome to maximize the future potentials of AI in PET. This paper provides an overview of these industry-specific challenges for the development, standardization, commercialization, and clinical adoption of AI, and explores the potential enhancements to PET imaging brought on by AI in the near future. In particular, the combination of on-demand image reconstruction, AI, and custom designed data processing workflows may open new possibilities for innovation which would positively impact the industry and ultimately patients. △ Less","14 July, 2021",https://arxiv.org/pdf/2107.06747
Opportunities and challenges of Blockchain-Oriented systems in the tourism industry,Fabio Caddeo;Andrea Pinna,"The tourism industry is increasingly influenced by the evolution of information and communication technologies (ICT), which are revolutionizing the way people travel. In this work we want to nvestigate the use of innovative IT technologies by DMOs (Destination Management Organizations), focusing on blockchain technology, both from the point of view of research in the field, and in the study of the most relevant software projects. In particular, we intend to verify the benefits offered by these IT tools in the management and monitoring of a destination, without forgetting the implications for the other stakeholders involved. These technologies, in fact, can offer a wide range of services that can be useful throughout the life cycle of the destination. △ Less","11 March, 2021",https://arxiv.org/pdf/2107.06732
Everybody Is Unique: Towards Unbiased Human Mesh Recovery,Ren Li;Meng Zheng;Srikrishna Karanam;Terrence Chen;Ziyan Wu,"We consider the problem of obese human mesh recovery, i.e., fitting a parametric human mesh to images of obese people. Despite obese person mesh fitting being an important problem with numerous applications (e.g., healthcare), much recent progress in mesh recovery has been restricted to images of non-obese people. In this work, we identify this crucial gap in the current literature by presenting and discussing limitations of existing algorithms. Next, we present a simple baseline to address this problem that is scalable and can be easily used in conjunction with existing algorithms to improve their performance. Finally, we present a generalized human mesh optimization algorithm that substantially improves the performance of existing methods on both obese person images as well as community-standard benchmark datasets. A key innovation of this technique is that it does not rely on supervision from expensive-to-create mesh parameters. Instead, starting from widely and cheaply available 2D keypoints annotations, our method automatically generates mesh parameters that can in turn be used to re-train and fine-tune any existing mesh estimation algorithm. This way, we show our method acts as a drop-in to improve the performance of a wide variety of contemporary mesh estimation methods. We conduct extensive experiments on multiple datasets comprising both standard and obese person images and demonstrate the efficacy of our proposed techniques. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06239
Attention-Guided Progressive Neural Texture Fusion for High Dynamic Range Image Restoration,Jie Chen;Zaifeng Yang;Tsz Nam Chan;Hui Li;Junhui Hou;Lap-Pui Chau,"High Dynamic Range (HDR) imaging via multi-exposure fusion is an important task for most modern imaging platforms. In spite of recent developments in both hardware and algorithm innovations, challenges remain over content association ambiguities caused by saturation, motion, and various artifacts introduced during multi-exposure fusion such as ghosting, noise, and blur. In this work, we propose an Attention-guided Progressive Neural Texture Fusion (APNT-Fusion) HDR restoration model which aims to address these issues within one framework. An efficient two-stream structure is proposed which separately focuses on texture feature transfer over saturated regions and multi-exposure tonal and texture feature fusion. A neural feature transfer mechanism is proposed which establishes spatial correspondence between different exposures based on multi-scale VGG features in the masked saturated HDR domain for discriminative contextual clues over the ambiguous image areas. A progressive texture blending module is designed to blend the encoded two-stream features in a multi-scale and progressive manner. In addition, we introduce several novel attention mechanisms, i.e., the motion attention module detects and suppresses the content discrepancies among the reference images; the saturation attention module facilitates differentiating the misalignment caused by saturation from those caused by motion; and the scale attention module ensures texture blending consistency between different coder/decoder scales. We carry out comprehensive qualitative and quantitative evaluations and ablation studies, which validate that these novel modules work coherently under the same framework and outperform state-of-the-art methods. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06211
Deep learning approaches to Earth Observation change detection,Antonio Di Pilato;Nicolò Taggio;Alexis Pompili;Michele Iacobellis;Adriano Di Florio;Davide Passarelli;Sergio Samarelli,"The interest for change detection in the field of remote sensing has increased in the last few years. Searching for changes in satellite images has many useful applications, ranging from land cover and land use analysis to anomaly detection. In particular, urban change detection provides an efficient tool to study urban spread and growth through several years of observation. At the same time, change detection is often a computationally challenging and time-consuming task, which requires innovative methods to guarantee optimal results with unquestionable value and within reasonable time. In this paper we present two different approaches to change detection (semantic segmentation and classification) that both exploit convolutional neural networks to achieve good results, which can be further refined and used in a post-processing workflow for a large variety of applications. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06132
Indian Legal NLP Benchmarks : A Survey,Prathamesh Kalamkar;Janani Venugopalan Ph. D.;Vivek Raghavan Ph. D,"Availability of challenging benchmarks is the key to advancement of AI in a specific field.Since Legal Text is significantly different than normal English text, there is a need to create separate Natural Language Processing benchmarks for Indian Legal Text which are challenging and focus on tasks specific to Legal Systems. This will spur innovation in applications of Natural language Processing for Indian Legal Text and will benefit AI community and Legal fraternity. We review the existing work in this area and propose ideas to create new benchmarks for Indian Legal Natural Language Processing. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06056
Argus: A Fully Transparent Incentive System for Anti-Piracy Campaigns (Extended Version),Xian Zhang;Xiaobing Guo;Zixuan Zeng;Wenyan Liu;Zhongxin Guo;Yang Chen;Shuo Chen;Qiufeng Yin;Mao Yang;Lidong Zhou,"Anti-piracy is fundamentally a procedure that relies on collecting data from the open anonymous population, so how to incentivize credible reporting is a question at the center of the problem. Industrial alliances and companies are running anti-piracy incentive campaigns, but their effectiveness is publicly questioned due to the lack of transparency. We believe that full transparency of a campaign is necessary to truly incentivize people. It means that every role, e.g., content owner, licensee of the content, or every person in the open population, can understand the mechanism and be assured about its execution without trusting any single role. We see this as a distributed system problem. In this paper, we present Argus, a fully transparent incentive system for anti-piracy campaigns. The groundwork of Argus is to formulate the objectives for fully transparent incentive mechanisms, which securely and comprehensively consolidate the different interests of all roles. These objectives form the core of the Argus design, highlighted by our innovations about a Sybil-proof incentive function, a commit-and-reveal scheme, and an oblivious transfer scheme. In the implementation, we overcome a set of unavoidable obstacles to ensure security despite full transparency. Moreover, we effectively optimize several cryptographic operations so that the cost for a piracy reporting is reduced to an equivalent cost of sending about 14 ETH-transfer transactions to run on the public Ethereum network, which would otherwise correspond to thousands of transactions. With the security and practicality of Argus, we hope real-world anti-piracy campaigns will be truly effective by shifting to a fully transparent incentive mechanism. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06049
An Orchestration Platform that Puts Radiologists in the Driver's Seat of AI Innovation: A Methodological Approach,Raphael Y. Cohen;Aaron D. Sodickson,"Current AI-driven research in radiology requires resources and expertise that are often inaccessible to small and resource-limited labs. The clinicians who are able to participate in AI research are frequently well-funded, well-staffed, and either have significant experience with AI and computing, or have access to colleagues or facilities that do. Current imaging data is clinician-oriented and is not easily amenable to machine learning initiatives, resulting in inefficient, time consuming, and costly efforts that rely upon a crew of data engineers and machine learning scientists, and all too often preclude radiologists from driving AI research and innovation. We present the system and methodology we have developed to address infrastructure and platform needs, while reducing the staffing and resource barriers to entry. We emphasize a data-first and modular approach that streamlines the AI development and deployment process while providing efficient and familiar interfaces for radiologists, such that they can be the drivers of new AI innovations. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.04409
Petri Net Modeling for Ising Model Formulation in Quantum Annealing,Morikazu Nakamura;Kohei Kaneshima;Takeo Yoshida,"Quantum annealing is an emerging new platform for combinatorial optimization, requiring an Ising model formulation for optimization problems. The formulation can be an essential obstacle to the permeation of this innovation into broad areas of everyday life. Our research is aimed at the proposal of a Petri net modeling approach for an Ising model formulation. Although the proposed method requires users to model their optimization problems with Petri nets, this process can be carried out in a relatively straightforward manner if we know the target problem and the simple Petri net modeling rules. With our method, the constraints and objective functions in the target optimization problems are represented as fundamental characteristics of Petri net models, extracted systematically from Petri net models, and then converted into binary quadratic nets, equivalent to Ising models. The proposed method can drastically reduce the difficulty of the Ising model formulation. △ Less","9 July, 2021",https://arxiv.org/pdf/2107.04304
Crowd Sensing and Living Lab Outdoor Experimentation Made Easy,Evangelos Pournaras;Atif Nabi Ghulam;Renato Kunz;Regula Hänggli,"Living lab outdoor experimentation using pervasive computing provides new opportunities: higher realism, external validity and socio-spatio-temporal observations in large scale. However, experimentation `in the wild' is complex and costly. Noise, biases, privacy concerns, compliance with standards of ethical review boards, remote moderation, control of experimental conditions and equipment perplex the collection of high-quality data for causal inference. This article introduces Smart Agora, a novel open-source software platform for rigorous systematic outdoor experimentation. Without writing a single line of code, highly complex experimental scenarios are visually designed and automatically deployed to smart phones. Novel geolocated survey and sensor data are collected subject of participants verifying desired experimental conditions, for instance, their localization at certain urban spots. This new approach drastically improves the quality and purposefulness of crowd sensing, tailored to conditions that confirm/reject hypotheses. The features that support this innovative functionality and the broad spectrum of its applicability are demonstrated. △ Less","15 October, 2021",https://arxiv.org/pdf/2107.04117
Staying Ahead in the MOOC-Era by Teaching Innovative AI Courses,Patrick Glauner,"As a result of the rapidly advancing digital transformation of teaching, universities have started to face major competition from Massive Open Online Courses (MOOCs). Universities thus have to set themselves apart from MOOCs in order to justify the added value of three to five-year degree programs to prospective students. In this paper, we show how we address this challenge at Deggendorf Institute of Technology in ML and AI. We first share our best practices and present two concrete courses including their unique selling propositions: Computer Vision and Innovation Management for AI. We then demonstrate how these courses contribute to Deggendorf Institute of Technology's ability to differentiate itself from MOOCs (and other universities). △ Less","13 August, 2021",https://arxiv.org/pdf/2107.04024
The Show Must Go On -- Examination During a Pandemic,Pamela Fleischmann;Mitja Kulczynski;Dirk Nowotka,"When unexpected incidents occur, new innovative and flexible solutions are required. If this event is something such radical and dramatic like the COVID-19 pandemic, these solutions must aim to guarantee as much normality as possible while protecting lives. After a moment of shock our university decided that the students have to be able to pursue their studies for guaranteeing a degree in the expected time since most of them faced immediate financial problems due to the loss of their student jobs. This implied, for us as teachers, that we had to reorganise not only the teaching methods from nearly one day to the next, but we also had to come up with an adjusted way of examinations which had to take place in person with pen and paper under strict hygiene rules. On the other hand the correction should avoid personal contacts. We developed a framework which allowed us to correct the digitalised exams safely at home while providing the high standards given by the general data protection regulation of our country. Moreover, the time spent in the offices could be reduced to a minimum thanks to automatically generated exam sheets, automatically re-digitalised and sorted worked-on exams. △ Less","25 June, 2021",https://arxiv.org/pdf/2107.04014
Malware Classification Using Deep Boosted Learning,Muhammad Asam;Saddam Hussain Khan;Tauseef Jamal;Umme Zahoora;Asifullah Khan,"Malicious activities in cyberspace have gone further than simply hacking machines and spreading viruses. It has become a challenge for a nations survival and hence has evolved to cyber warfare. Malware is a key component of cyber-crime, and its analysis is the first line of defence against attack. This work proposes a novel deep boosted hybrid learning-based malware classification framework and named as Deep boosted Feature Space-based Malware classification (DFS-MC). In the proposed framework, the discrimination power is enhanced by fusing the feature spaces of the best performing customized CNN architectures models and its discrimination by an SVM for classification. The discrimination capacity of the proposed classification framework is assessed by comparing it against the standard customized CNNs. The customized CNN models are implemented in two ways: softmax classifier and deep hybrid learning-based malware classification. In the hybrid learning, Deep features are extracted from customized CNN architectures and fed into the conventional machine learning classifier to improve the classification performance. We also introduced the concept of transfer learning in a customized CNN architecture based malware classification framework through fine-tuning. The performance of the proposed malware classification approaches are validated on the MalImg malware dataset using the hold-out cross-validation technique. Experimental comparisons were conducted by employing innovative, customized CNN, trained from scratch and fine-tuning the customized CNN using transfer learning. The proposed classification framework DFS-MC showed improved results, Accuracy: 98.61%, F-score: 0.96, Precision: 0.96, and Recall: 0.96. △ Less","8 July, 2021",https://arxiv.org/pdf/2107.04008
Human Resource Development and the Internet of Things,Robert M Yawson;Daniel Woldeab;Emmanuel Osafo,"The Internet of Things (IoT) is affecting national innovation ecosystems and the approach of organizations to innovation and how they create and capture value in everyday business activities. The Internet of Things (IoT), is disruptive, and it will change the manner in which human resources are developed and managed, calling for a new and adaptive human resource development approach. The Classical Internet communication form is human-human. The prospect of IoT is that every object will have a unique way of identification and can be addressed so that every object can be connected. The communication forms will expand from human-human to human-human, human-thing, and thing-thing. This will bring a new challenge to how Human Resource Development (HRD) is practiced. This paper provides an overview of the Internet of Things and conceptualizes the role of HRD in the age of the Internet of Things. Keywords: △ Less","21 June, 2021",https://arxiv.org/pdf/2107.04003
"Smart Healthcare in the Age of AI: Recent Advances, Challenges, and Future Prospects",Mahmoud Nasr;MD. Milon Islam;Shady Shehata;Fakhri Karray;Yuri Quintana,"The significant increase in the number of individuals with chronic ailments (including the elderly and disabled) has dictated an urgent need for an innovative model for healthcare systems. The evolved model will be more personalized and less reliant on traditional brick-and-mortar healthcare institutions such as hospitals, nursing homes, and long-term healthcare centers. The smart healthcare system is a topic of recently growing interest and has become increasingly required due to major developments in modern technologies, especially in artificial intelligence (AI) and machine learning (ML). This paper is aimed to discuss the current state-of-the-art smart healthcare systems highlighting major areas like wearable and smartphone devices for health monitoring, machine learning for disease diagnosis, and the assistive frameworks, including social robots developed for the ambient assisted living environment. Additionally, the paper demonstrates software integration architectures that are very significant to create smart healthcare systems, integrating seamlessly the benefit of data analytics and other tools of AI. The explained developed systems focus on several facets: the contribution of each developed framework, the detailed working procedure, the performance as outcomes, and the comparative merits and limitations. The current research challenges with potential future directions are addressed to highlight the drawbacks of existing systems and the possible methods to introduce novel frameworks, respectively. This review aims at providing comprehensive insights into the recent developments of smart healthcare systems to equip experts to contribute to the field. △ Less","24 June, 2021",https://arxiv.org/pdf/2107.03924
AI and the future of pharmaceutical research,Adam Zielinski,"This paper examines how pharmaceutical Artificial Intelligence advancements may affect the development of new drugs in the coming years. The question was answered by reviewing a rich body of source material, including industry literature, research journals, AI studies, market reports, market projections, discussion papers, press releases, and organizations' websites. The paper argues that continued innovation in pharmaceutical AI will enable rapid development of safe and effective therapies for previously untreatable diseases. A series of major points support this conclusion: The pharmaceutical industry is in a significant productivity crisis today, and AI-enabled research methods can be directly applied to reduce the time and cost of drug discovery projects. The industry already reported results such as a 10-fold reduction in drug molecule discovery times. Numerous AI alliances between industry, governments, and academia enabled utilizing proprietary data and led to outcomes such as the largest molecule toxicity database to date or more than 200 drug safety predictive models. The momentum was recently increased by the involvement of tech giants combined with record rounds of funding. The long-term effects will range from safer and more effective therapies, through the diminished role of pharmaceutical patents, to large-scale collaboration and new business strategies oriented around currently untreatable diseases. The paper notes that while many reviewed resources seem to have overly optimistic future expectations, even a fraction of these developments would alleviate the productivity crisis. Finally, the paper concludes that the focus on pharmaceutical AI put the industry on a trajectory towards another significant disruption: open data sharing and collaboration. △ Less","25 June, 2021",https://arxiv.org/pdf/2107.03896
Short-term Renewable Energy Forecasting in Greece using Prophet Decomposition and Tree-based Ensembles,Argyrios Vartholomaios;Stamatis Karlos;Eleftherios Kouloumpris;Grigorios Tsoumakas,"Energy production using renewable sources exhibits inherent uncertainties due to their intermittent nature. Nevertheless, the unified European energy market promotes the increasing penetration of renewable energy sources (RES) by the regional energy system operators. Consequently, RES forecasting can assist in the integration of these volatile energy sources, since it leads to higher reliability and reduced ancillary operational costs for power systems. This paper presents a new dataset for solar and wind energy generation forecast in Greece and introduces a feature engineering pipeline that enriches the dimensional space of the dataset. In addition, we propose a novel method that utilizes the innovative Prophet model, an end-to-end forecasting tool that considers several kinds of nonlinear trends in decomposing the energy time series before a tree-based ensemble provides short-term predictions. The performance of the system is measured through representative evaluation metrics, and by estimating the model's generalization under an industryprovided scheme of absolute error thresholds. The proposed hybrid model competes with baseline persistence models, tree-based regression ensembles, and the Prophet model, managing to outperform them, presenting both lower error rates and more favorable error distribution. △ Less","8 July, 2021",https://arxiv.org/pdf/2107.03825
Quantifying the rise and fall of scientific fields,Chakresh Singh;Emma Barme;Robert Ward;Liubov Tupikina;Marc Santolini,"Science advances by pushing the boundaries of the adjacent possible. While the global scientific enterprise grows at an exponential pace, at the mesoscopic level the exploration and exploitation of research ideas is reflected through the rise and fall of research fields. The empirical literature has largely studied such dynamics on a case-by-case basis, with a focus on explaining how and why communities of knowledge production evolve. Although fields rise and fall on different temporal and population scales, they are generally argued to pass through a common set of evolutionary stages. To understand the social processes that drive these stages beyond case studies, we need a way to quantify and compare different fields on the same terms. In this paper we develop techniques for identifying scale-invariant patterns in the evolution of scientific fields, and demonstrate their usefulness using 1.5 million preprints from the arXiv repository covering 175 research fields spanning Physics, Mathematics, Computer Science, Quantitative Biology and Quantitative Finance. We show that fields consistently follows a rise and fall pattern captured by a two parameters right-tailed Gumbel temporal distribution. We introduce a field-specific rescaled time and explore the generic properties shared by articles and authors at the creation, adoption, peak, and decay evolutionary phases. We find that the early phase of a field is characterized by the mixing of cognitively distant fields by small teams of interdisciplinary authors, while late phases exhibit the role of specialized, large teams building on the previous works in the field. This method provides foundations to quantitatively explore the generic patterns underlying the evolution of research fields in science, with general implications in innovation studies. △ Less","9 July, 2021",https://arxiv.org/pdf/2107.03749
A comparative study of various Deep Learning techniques for spatio-temporal Super-Resolution reconstruction of Forced Isotropic Turbulent flows,T. S. Sachin Venkatesh;Rajat Srivastava;Pratyush Bhatt;Prince Tyagi;Raj Kumar Singh,"Super-resolution is an innovative technique that upscales the resolution of an image or a video and thus enables us to reconstruct high-fidelity images from low-resolution data. This study performs super-resolution analysis on turbulent flow fields spatially and temporally using various state-of-the-art machine learning techniques like ESPCN, ESRGAN and TecoGAN to reconstruct high-resolution flow fields from low-resolution flow field data, especially keeping in mind the need for low resource consumption and rapid results production/verification. The dataset used for this study is extracted from the 'isotropic 1024 coarse' dataset which is a part of Johns Hopkins Turbulence Databases (JHTDB). We have utilized pre-trained models and fine tuned them to our needs, so as to minimize the computational resources and the time required for the implementation of the super-resolution models. The advantages presented by this method far exceed the expectations and the outcomes of regular single structure models. The results obtained through these models are then compared using MSE, PSNR, SAM, VIF and SCC metrics in order to evaluate the upscaled results, find the balance between computational power and output quality, and then identify the most accurate and efficient model for spatial and temporal super-resolution of turbulent flow fields. △ Less","7 July, 2021",https://arxiv.org/pdf/2107.03361
Telelife: The Future of Remote Living,Jason Orlosky;Misha Sra;Kenan Bektaş;Huaishu Peng;Jeeeun Kim;Nataliya Kos'myna;Tobias Hollerer;Anthony Steed;Kiyoshi Kiyokawa;Kaan Akşit,"In recent years, everyday activities such as work and socialization have steadily shifted to more remote and virtual settings. With the COVID-19 pandemic, the switch from physical to virtual has been accelerated, which has substantially affected various aspects of our lives, including business, education, commerce, healthcare, and personal life. This rapid and large-scale switch from in-person to remote interactions has revealed that our current technologies lack functionality and are limited in their ability to recreate interpersonal interactions. To help address these limitations in the future, we introduce ""Telelife,"" a vision for the near future that depicts the potential means to improve remote living better aligned with how we interact, live and work in the physical world. Telelife encompasses novel synergies of technologies and concepts such as digital twins, virtual prototyping, and attention and context-aware user interfaces with innovative hardware that can support ultrarealistic graphics, user state detection, and more. These ideas will guide the transformation of our daily lives and routines soon, targeting the year 2035. In addition, we identify opportunities across high-impact applications in domains related to this vision of Telelife. Along with a recent survey of relevant fields such as human-computer interaction, pervasive computing, and virtual reality, the directions outlined in this paper will guide future research on remote living. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.02965
Neural Computing,Ayushe Gangal;Peeyush Kumar;Sunita Kumari;Aditya Kumar,"This chapter aims to provide next-level understanding of the problems of the world and the solutions available to those problems, which lie very well within the domain of neural computing, and at the same time are intelligent in their approach, to invoke a sense of innovation among the educationalists, researchers, academic professionals, students and people concerned, by highlighting the work done by major researchers and innovators in this field and thus, encouraging the readers to develop newer and more advanced techniques for the same. By means of this chapter, the societal problems are discussed and various solutions are also given by means of the theories presented and researches done so far. Different types of neural networks discovered so far and applications of some of those neural networks are focused on, apart from their theoretical understanding, the working and core concepts involved in the applications. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.02744
Multi-Modal Mutual Information (MuMMI) Training for Robust Self-Supervised Deep Reinforcement Learning,Kaiqi Chen;Yong Lee;Harold Soh,"This work focuses on learning useful and robust deep world models using multiple, possibly unreliable, sensors. We find that current methods do not sufficiently encourage a shared representation between modalities; this can cause poor performance on downstream tasks and over-reliance on specific sensors. As a solution, we contribute a new multi-modal deep latent state-space model, trained using a mutual information lower-bound. The key innovation is a specially-designed density ratio estimator that encourages consistency between the latent codes of each modality. We tasked our method to learn policies (in a self-supervised manner) on multi-modal Natural MuJoCo benchmarks and a challenging Table Wiping task. Experiments show our method significantly outperforms state-of-the-art deep reinforcement learning methods, particularly in the presence of missing observations. △ Less","5 July, 2021",https://arxiv.org/pdf/2107.02339
Histogram of Cell Types: Deep Learning for Automated Bone Marrow Cytology,Rohollah Moosavi Tayebi;Youqing Mu;Taher Dehkharghanian;Catherine Ross;Monalisa Sur;Ronan Foley;Hamid R. Tizhoosh;Clinton JV Campbell,"Bone marrow cytology is required to make a hematological diagnosis, influencing critical clinical decision points in hematology. However, bone marrow cytology is tedious, limited to experienced reference centers and associated with high inter-observer variability. This may lead to a delayed or incorrect diagnosis, leaving an unmet need for innovative supporting technologies. We have developed the first ever end-to-end deep learning-based technology for automated bone marrow cytology. Starting with a bone marrow aspirate digital whole slide image, our technology rapidly and automatically detects suitable regions for cytology, and subsequently identifies and classifies all bone marrow cells in each region. This collective cytomorphological information is captured in a novel representation called Histogram of Cell Types (HCT) quantifying bone marrow cell class probability distribution and acting as a cytological ""patient fingerprint"". The approach achieves high accuracy in region detection (0.97 accuracy and 0.99 ROC AUC), and cell detection and cell classification (0.75 mAP, 0.78 F1-score, Log-average miss rate of 0.31). HCT has potential to revolutionize hematopathology diagnostic workflows, leading to more cost-effective, accurate diagnosis and opening the door to precision medicine. △ Less","8 July, 2021",https://arxiv.org/pdf/2107.02293
Blockchain-enabled Network Sharing for O-RAN in 5G and Beyond,Lorenza Giupponi;Francesc Wilhelmi,"The innovation provided by network virtualization in 5G, together with standardization and openness boosted by the Open Radio Access Network (O-RAN) Alliance, has paved the way to a collaborative future in cellular systems, driven by flexible network sharing. Such advents are expected to attract new players like content providers and verticals, increasing competitiveness in the telecom market. However, scalability and trust issues are expected to arise, given the criticality of ownership traceability and resource exchanging in a sharing ecosystem. To address that, we propose integrating blockchain technology for enabling mobile operators and other players to exchange RAN resources (e.g., infrastructure) in the form of virtual network functions (VNF) autonomously and dynamically. Blockchain will provide automation, robustness, trustworthiness, and reliability to mobile networks, thus bringing confidence to open RAN environments. In particular, we define a novel O-RAN-based blockchain-enabled architecture that allows automating RAN sharing procedures through either auction or marketplace-based mechanisms. The potential advantages of the proposed solution are demonstrated through simulation results. The used simulation platform is openly released. △ Less","10 December, 2021",https://arxiv.org/pdf/2107.02005
Arabic Code-Switching Speech Recognition using Monolingual Data,Ahmed Ali;Shammur Chowdhury;Amir Hussein;Yasser Hifny,"Code-switching in automatic speech recognition (ASR) is an important challenge due to globalization. Recent research in multilingual ASR shows potential improvement over monolingual systems. We study key issues related to multilingual modeling for ASR through a series of large-scale ASR experiments. Our innovative framework deploys a multi-graph approach in the weighted finite state transducers (WFST) framework. We compare our WFST decoding strategies with a transformer sequence to sequence system trained on the same data. Given a code-switching scenario between Arabic and English languages, our results show that the WFST decoding approaches were more suitable for the intersentential code-switching datasets. In addition, the transformer system performed better for intrasentential code-switching task. With this study, we release an artificially generated development and test sets, along with ecological code-switching test set, to benchmark the ASR performance. △ Less","4 July, 2021",https://arxiv.org/pdf/2107.01573
Case study of Innovative Teaching Practices and their Impact for Electrical Engineering Courses during COVID-19 Pandemic,Amith Khandakar;Muhammad E. H. Chowdhury;Md. Saifuddin Khalid;Nizar Zorba,"Due to the COVID-19 pandemic, there was an urgent need to move to online teaching and develop innovations to guarantee the Student Learning Outcomes (SLOs) are being fulfilled. The contributions of this paper are two-fold: the effects of an experimented teaching strategy, i.e. multi-course project-based learning (MPL) approach, are presented followed with online assessment techniques investigation for senior level electrical engineering (EE) courses at Qatar University. The course project of the senior course was designed in such a way that it helps in simultaneously attaining the objectives of the senior and capstone courses, that the students were taking at the same time. It is known that the MPL approach enhances the critical thinking capacity of students which is also a major outcome of Education for Sustainable Development (ESD). The developed project ensures the fulfillment of a series of SLOs, that are concentrated on soft engineering and project management skills. The difficulties of adopting the MPL method for the senior level courses are in aligning the project towards fulfilling the learning outcomes of every individual course. The study also provides the students feedback on online assessment techniques incorporated with the MPL, due to online teaching during COVID-19 pandemic. In order to provide a benchmark and to highlight the obtained results, the innovative teaching approaches were compared to conventional methods taught on the same senior course in a previous semester. Based on the feedback from teachers and students from previously conducted case study it was believed that the MPL approach would support the students. With the statistical analysis (Chi-square, two-tailed T statistics and hypothesis testing using z-test) it can be concluded that the MPL and online assessment actually help to achieve better attainment of the SLOs, even during a pandemic situation. △ Less","1 July, 2021",https://arxiv.org/pdf/2107.00746
GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph,Yunxin Li;Yu Zhao;Baotian Hu;Qingcai Chen;Yang Xiang;Xiaolong Wang;Yuxin Ding;Lin Ma,"Previous works indicate that the glyph of Chinese characters contains rich semantic information and has the potential to enhance the representation of Chinese characters. The typical method to utilize the glyph features is by incorporating them into the character embedding space. Inspired by previous methods, we innovatively propose a Chinese pre-trained representation model named as GlyphCRM, which abandons the ID-based character embedding method yet solely based on sequential character images. We render each character into a binary grayscale image and design two-channel position feature maps for it. Formally, we first design a two-layer residual convolutional neural network, namely HanGlyph to generate the initial glyph representation of Chinese characters, and subsequently adopt multiple bidirectional encoder Transformer blocks as the superstructure to capture the context-sensitive information. Meanwhile, we feed the glyph features extracted from each layer of the HanGlyph module into the underlying Transformer blocks by skip-connection method to fully exploit the glyph features of Chinese characters. As the HanGlyph module can obtain a sufficient glyph representation of any Chinese character, the long-standing out-of-vocabulary problem could be effectively solved. Extensive experimental results indicate that GlyphCRM substantially outperforms the previous BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has strong transferability and generalization on specialized fields and low-resource tasks. We hope this work could spark further research beyond the realms of well-established representation of Chinese texts. △ Less","1 July, 2021",https://arxiv.org/pdf/2107.00395
The Use of Bandit Algorithms in Intelligent Interactive Recommender Systems,Qing Wang,"In today's business marketplace, many high-tech Internet enterprises constantly explore innovative ways to provide optimal online user experiences for gaining competitive advantages. The great needs of developing intelligent interactive recommendation systems are indicated, which could sequentially suggest users the most proper items by accurately predicting their preferences, while receiving the up-to-date feedback to refine the recommendation results, continuously. Multi-armed bandit algorithms, which have been widely applied into various online systems, are quite capable of delivering such efficient recommendation services. However, few existing bandit models are able to adapt to new changes introduced by the modern recommender systems. △ Less","30 June, 2021",https://arxiv.org/pdf/2107.00161
An extended and more practical mwp flow analysis,Clément Aubert;Thomas Rubiano;Neea Rusch;Thomas Seiller,"We improve and refine a method for certifying that the values' sizes computed by an imperative program will be bounded by polynomials in the program's inputs' sizes. Our work ''tames'' the non-determinism of the original analysis, and offers an innovative way of completing the analysis when a non-polynomial growth is found. We furthermore enrich the analyzed language by adding function definitions and calls, allowing to compose the analysis of different libraries and offering generally more modularity. The implementation of our improved method, discussed in a tool paper (https://hal.archives-ouvertes.fr/hal-03269121), also required to reason about the efficiency of some of the needed operations on the matrices produced by the analysis. It is our hope that this work will enable and facilitate static analysis of source code to guarantee its correctness with respect to resource usages. △ Less","2 July, 2021",https://arxiv.org/pdf/2107.00086
Reservoir Based Edge Training on RF Data To Deliver Intelligent and Efficient IoT Spectrum Sensors,Silvija Kokalj-Filipovic;Paul Toliver;William Johnson;Rob Miller,"Current radio frequency (RF) sensors at the Edge lack the computational resources to support practical, in-situ training for intelligent spectrum monitoring, and sensor data classification in general. We propose a solution via Deep Delay Loop Reservoir Computing (DLR), a processing architecture that supports general machine learning algorithms on compact mobile devices by leveraging delay-loop reservoir computing in combination with innovative electrooptical hardware. With both digital and photonic realizations of our design of the loops, DLR delivers reductions in form factor, hardware complexity and latency, compared to the State-of-the-Art (SoA). The main impact of the reservoir is to project the input data into a higher dimensional space of reservoir state vectors in order to linearly separate the input classes. Once the classes are well separated, traditionally complex, power-hungry classification models are no longer needed for the learning process. Yet, even with simple classifiers based on Ridge regression (RR), the complexity grows at least quadratically with the input size. Hence, the hardware reduction required for training on compact devices is in contradiction with the large dimension of state vectors. DLR employs a RR-based classifier to exceed the SoA accuracy, while further reducing power consumption by leveraging the architecture of parallel (split) loops. We present DLR architectures composed of multiple smaller loops whose state vectors are linearly combined to create a lower dimensional input into Ridge regression. We demonstrate the advantages of using DLR for two distinct applications: RF Specific Emitter Identification (SEI) for IoT authentication, and wireless protocol recognition for IoT situational awareness. △ Less","1 April, 2021",https://arxiv.org/pdf/2106.16087
Multilayer Networks for Text Analysis with Multiple Data Types,Charles C. Hyland;Yuanming Tao;Lamiae Azizi;Martin Gerlach;Tiago P. Peixoto;Eduardo G. Altmann,"We are interested in the widespread problem of clustering documents and finding topics in large collections of written documents in the presence of metadata and hyperlinks. To tackle the challenge of accounting for these different types of datasets, we propose a novel framework based on Multilayer Networks and Stochastic Block Models. The main innovation of our approach over other techniques is that it applies the same non-parametric probabilistic framework to the different sources of datasets simultaneously. The key difference to other multilayer complex networks is the strong unbalance between the layers, with the average degree of different node types scaling differently with system size. We show that the latter observation is due to generic properties of text, such as Heaps' law, and strongly affects the inference of communities. We present and discuss the performance of our method in different datasets (hundreds of Wikipedia documents, thousands of scientific papers, and thousands of E-mails) showing that taking into account multiple types of information provides a more nuanced view on topic- and document-clusters and increases the ability to predict missing links. △ Less","30 June, 2021",https://arxiv.org/pdf/2106.15821
SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal Action Detection,Ranyu Ning;Can Zhang;Yuexian Zou,"Temporal action detection (TAD) is a challenging task which aims to temporally localize and recognize the human action in untrimmed videos. Current mainstream one-stage TAD approaches localize and classify action proposals relying on pre-defined anchors, where the location and scale for action instances are set by designers. Obviously, such an anchor-based TAD method limits its generalization capability and will lead to performance degradation when videos contain rich action variation. In this study, we explore to remove the requirement of pre-defined anchors for TAD methods. A novel TAD model termed as Selective Receptive Field Network (SRF-Net) is developed, in which the location offsets and classification scores at each temporal location can be directly estimated in the feature map and SRF-Net is trained in an end-to-end manner. Innovatively, a building block called Selective Receptive Field Convolution (SRFC) is dedicatedly designed which is able to adaptively adjust its receptive field size according to multiple scales of input information at each temporal location in the feature map. Extensive experiments are conducted on the THUMOS14 dataset, and superior results are reported comparing to state-of-the-art TAD approaches. △ Less","29 June, 2021",https://arxiv.org/pdf/2106.15258
Blockchain and AI-based Solutions to Combat Coronavirus (COVID-19)-like Epidemics: A Survey,Dinh C. Nguyen;Ming Ding;Pubudu N. Pathirana;Aruna Seneviratne,"The beginning of 2020 has seen the emergence of coronavirus outbreak caused by a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled worldwide spread of COVID-19 show the limitations of existing healthcare systems in timely handling public health emergencies. In such contexts, innovative technologies such as blockchain and Artificial Intelligence (AI) have emerged as promising solutions for fighting coronavirus epidemic. In particular, blockchain can combat pandemics by enabling early detection of outbreaks, ensuring the ordering of medical data, and ensuring reliable medical supply chain during the outbreak tracing. Moreover, AI provides intelligent solutions for identifying symptoms caused by coronavirus for treatments and supporting drug manufacturing. Therefore, we present an extensive survey on the use of blockchain and AI for combating COVID-19 epidemics. First, we introduce a new conceptual architecture which integrates blockchain and AI for fighting COVID-19. Then, we survey the latest research efforts on the use of blockchain and AI for fighting COVID-19 in various applications. The newly emerging projects and use cases enabled by these technologies to deal with coronavirus pandemic are also presented. A case study is also provided using federated AI for COVID-19 detection. Finally, we point out challenges and future directions that motivate more research efforts to deal with future coronavirus-like epidemics. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.14631
ICDAR 2021 Competition on Scientific Literature Parsing,Antonio Jimeno Yepes;Xu Zhong;Douglas Burdick,"Scientific literature contain important information related to cutting-edge innovations in diverse domains. Advances in natural language processing have been driving the fast development in automated information extraction from scientific literature. However, scientific literature is often available in unstructured PDF format. While PDF is great for preserving basic visual elements, such as characters, lines, shapes, etc., on a canvas for presentation to humans, automatic processing of the PDF format by machines presents many challenges. With over 2.5 trillion PDF documents in existence, these issues are prevalent in many other important application domains as well. Our ICDAR 2021 Scientific Literature Parsing Competition (ICDAR2021-SLP) aims to drive the advances specifically in document understanding. ICDAR2021-SLP leverages the PubLayNet and PubTabNet datasets, which provide hundreds of thousands of training and evaluation examples. In Task A, Document Layout Recognition, submissions with the highest performance combine object detection and specialised solutions for the different categories. In Task B, Table Recognition, top submissions rely on methods to identify table components and post-processing methods to generate the table structure and content. Results from both tasks show an impressive performance and opens the possibility for high performance practical applications. △ Less","8 June, 2021",https://arxiv.org/pdf/2106.14616
Detecting race and gender bias in visual representation of AI on web search engines,Mykola Makhortykh;Aleksandra Urman;Roberto Ulloa,"Web search engines influence perception of social reality by filtering and ranking information. However, their outputs are often subjected to bias that can lead to skewed representation of subjects such as professional occupations or gender. In our paper, we use a mixed-method approach to investigate presence of race and gender bias in representation of artificial intelligence (AI) in image search results coming from six different search engines. Our findings show that search engines prioritize anthropomorphic images of AI that portray it as white, whereas non-white images of AI are present only in non-Western search engines. By contrast, gender representation of AI is more diverse and less skewed towards a specific gender that can be attributed to higher awareness about gender bias in search outputs. Our observations indicate both the the need and the possibility for addressing bias in representation of societally relevant subjects, such as technological innovation, and emphasize the importance of designing new approaches for detecting bias in information retrieval systems. △ Less","26 June, 2021",https://arxiv.org/pdf/2106.14072
Ranger21: a synergistic deep learning optimizer,Less Wright;Nestor Demeure,"As optimizers are critical to the performances of neural networks, every year a large number of papers innovating on the subject are published. However, while most of these publications provide incremental improvements to existing algorithms, they tend to be presented as new optimizers rather than composable algorithms. Thus, many worthwhile improvements are rarely seen out of their initial publication. Taking advantage of this untapped potential, we introduce Ranger21, a new optimizer which combines AdamW with eight components, carefully selected after reviewing and testing ideas from the literature. We found that the resulting optimizer provides significantly improved validation accuracy and training speed, smoother training curves, and is even able to train a ResNet50 on ImageNet2012 without Batch Normalization layers. A problem on which AdamW stays systematically stuck in a bad initial state. △ Less","6 August, 2021",https://arxiv.org/pdf/2106.13731
Circumpapillary OCT-Focused Hybrid Learning for Glaucoma Grading Using Tailored Prototypical Neural Networks,Gabriel García;Rocío del Amor;Adrián Colomer;Rafael Verdú-Monedero;Juan Morales-Sánchez;Valery Naranjo,"Glaucoma is one of the leading causes of blindness worldwide and Optical Coherence Tomography (OCT) is the quintessential imaging technique for its detection. Unlike most of the state-of-the-art studies focused on glaucoma detection, in this paper, we propose, for the first time, a novel framework for glaucoma grading using raw circumpapillary B-scans. In particular, we set out a new OCT-based hybrid network which combines hand-driven and deep learning algorithms. An OCT-specific descriptor is proposed to extract hand-crafted features related to the retinal nerve fibre layer (RNFL). In parallel, an innovative CNN is developed using skip-connections to include tailored residual and attention modules to refine the automatic features of the latent space. The proposed architecture is used as a backbone to conduct a novel few-shot learning based on static and dynamic prototypical networks. The k-shot paradigm is redefined giving rise to a supervised end-to-end system which provides substantial improvements discriminating between healthy, early and advanced glaucoma samples. The training and evaluation processes of the dynamic prototypical network are addressed from two fused databases acquired via Heidelberg Spectralis system. Validation and testing results reach a categorical accuracy of 0.9459 and 0.8788 for glaucoma grading, respectively. Besides, the high performance reported by the proposed model for glaucoma detection deserves a special mention. The findings from the class activation maps are directly in line with the clinicians' opinion since the heatmaps pointed out the RNFL as the most relevant structure for glaucoma diagnosis. △ Less","25 June, 2021",https://arxiv.org/pdf/2106.13551
"Cost-efficient, QoS and Security aware Placement of Smart Farming IoT Applications in Cloud-Fog Infrastructure",Jagruti Sahoo,"Smart farming is a recent innovation in the agriculture sector that can improve the agricultural yield by using smarter, automated, and data driven farm processes that interact with IoT devices deployed on farms. A cloud-fog infrastructure provides an effective platform to execute IoT applications. While fog computing satisfies the real-time processing need of delay-sensitive IoT services by bringing virtualized services closer to the IoT devices, cloud computing allows execution of applications with higher computational requirements. The deployment of IoT applications is a critical challenge as cloud and fog nodes vary in terms of their resource availability and use different cost models. Moreover, diversity in resource, quality of service (QoS) and security requirements of IoT applications make the problem even more complex. In this paper, we model IoT application placement as an optimization problem that aims at minimizing the cost while satisfying the QoS and security constraints. The problem is formulated using Integer Linear Programming (ILP). The ILP model is evaluated for a small-scale scenario. The evaluation shows the impact of QoS and security requirement on the cost. We also study the impact of relaxing security constraint on the placement decision. △ Less","29 July, 2021",https://arxiv.org/pdf/2106.13524
Half-body Portrait Relighting with Overcomplete Lighting Representation,Guoxian Song;Tat-Jen Cham;Jianfei Cai;Jianmin Zheng,"We present a neural-based model for relighting a half-body portrait image by simply referring to another portrait image with the desired lighting condition. Rather than following classical inverse rendering methodology that involves estimating normals, albedo and environment maps, we implicitly encode the subject and lighting in a latent space, and use these latent codes to generate relighted images by neural rendering. A key technical innovation is the use of a novel overcomplete lighting representation, which facilitates lighting interpolation in the latent space, as well as helping regularize the self-organization of the lighting latent space during training. In addition, we propose a novel multiplicative neural render that more effectively combines the subject and lighting latent codes for rendering. We also created a large-scale photorealistic rendered relighting dataset for training, which allows our model to generalize well to real images. Extensive experiments demonstrate that our system not only outperforms existing methods for referral-based portrait relighting, but also has the capability generate sequences of relighted images via lighting rotations. △ Less","25 June, 2021",https://arxiv.org/pdf/2106.13425
A Deep Learning Approach to Private Data Sharing of Medical Images Using Conditional GANs,Hanxi Sun;Jason Plawinski;Sajanth Subramaniam;Amir Jamaludin;Timor Kadir;Aimee Readie;Gregory Ligozio;David Ohlssen;Mark Baillie;Thibaud Coroller,"Sharing data from clinical studies can facilitate innovative data-driven research and ultimately lead to better public health. However, sharing biomedical data can put sensitive personal information at risk. This is usually solved by anonymization, which is a slow and expensive process. An alternative to anonymization is sharing a synthetic dataset that bears a behaviour similar to the real data but preserves privacy. As part of the collaboration between Novartis and the Oxford Big Data Institute, we generate a synthetic dataset based on COSENTYX (secukinumab) Ankylosing Spondylitis clinical study. We apply an Auxiliary Classifier GAN to generate synthetic MRIs of vertebral units. The images are conditioned on the VU location (cervical, thoracic and lumbar). In this paper, we present a method for generating a synthetic dataset and conduct an in-depth analysis on its properties along three key metrics: image fidelity, sample diversity and dataset privacy. △ Less","19 August, 2021",https://arxiv.org/pdf/2106.13199
"Detection, Analysis, and Prediction of Research Topics with Scientific Knowledge Graphs",Angelo Salatino;Andrea Mannocci;Francesco Osborne,"Analysing research trends and predicting their impact on academia and industry is crucial to gain a deeper understanding of the advances in a research field and to inform critical decisions about research funding and technology adoption. In the last years, we saw the emergence of several publicly-available and large-scale Scientific Knowledge Graphs fostering the development of many data-driven approaches for performing quantitative analyses of research trends. This chapter presents an innovative framework for detecting, analysing, and forecasting research topics based on a large-scale knowledge graph characterising research articles according to the research topics from the Computer Science Ontology. We discuss the advantages of a solution based on a formal representation of topics and describe how it was applied to produce bibliometric studies and innovative tools for analysing and predicting research dynamics. △ Less","24 June, 2021",https://arxiv.org/pdf/2106.12875
Factors affecting the COVID-19 risk in the US counties: an innovative approach by combining unsupervised and supervised learning,Samira Ziyadidegan;Moein Razavi;Homa Pesarakli;Amir Hossein Javid;Madhav Erraguntla,"The COVID-19 disease spreads swiftly, and nearly three months after the first positive case was confirmed in China, Coronavirus started to spread all over the United States. Some states and counties reported high number of positive cases and deaths, while some reported lower COVID-19 related cases and mortality. In this paper, the factors that could affect the risk of COVID-19 infection and mortality were analyzed in county level. An innovative method by using K-means clustering and several classification models is utilized to determine the most critical factors. Results showed that mean temperature, percent of people below poverty, percent of adults with obesity, air pressure, population density, wind speed, longitude, and percent of uninsured people were the most significant attributes △ Less","5 December, 2021",https://arxiv.org/pdf/2106.12766
High-Throughput Precision Phenotyping of Left Ventricular Hypertrophy with Cardiovascular Deep Learning,Grant Duffy;Paul P Cheng;Neal Yuan;Bryan He;Alan C. Kwan;Matthew J. Shun-Shin;Kevin M. Alexander;Joseph Ebinger;Matthew P. Lungren;Florian Rader;David H. Liang;Ingela Schnittger;Euan A. Ashley;James Y. Zou;Jignesh Patel;Ronald Witteles;Susan Cheng;David Ouyang,"Left ventricular hypertrophy (LVH) results from chronic remodeling caused by a broad range of systemic and cardiovascular disease including hypertension, aortic stenosis, hypertrophic cardiomyopathy, and cardiac amyloidosis. Early detection and characterization of LVH can significantly impact patient care but is limited by under-recognition of hypertrophy, measurement error and variability, and difficulty differentiating etiologies of LVH. To overcome this challenge, we present EchoNet-LVH - a deep learning workflow that automatically quantifies ventricular hypertrophy with precision equal to human experts and predicts etiology of LVH. Trained on 28,201 echocardiogram videos, our model accurately measures intraventricular wall thickness (mean absolute error [MAE] 1.4mm, 95% CI 1.2-1.5mm), left ventricular diameter (MAE 2.4mm, 95% CI 2.2-2.6mm), and posterior wall thickness (MAE 1.2mm, 95% CI 1.1-1.3mm) and classifies cardiac amyloidosis (area under the curve of 0.83) and hypertrophic cardiomyopathy (AUC 0.98) from other etiologies of LVH. In external datasets from independent domestic and international healthcare systems, EchoNet-LVH accurately quantified ventricular parameters (R2 of 0.96 and 0.90 respectively) and detected cardiac amyloidosis (AUC 0.79) and hypertrophic cardiomyopathy (AUC 0.89) on the domestic external validation site. Leveraging measurements across multiple heart beats, our model can more accurately identify subtle changes in LV geometry and its causal etiologies. Compared to human experts, EchoNet-LVH is fully automated, allowing for reproducible, precise measurements, and lays the foundation for precision diagnosis of cardiac hypertrophy. As a resource to promote further innovation, we also make publicly available a large dataset of 23,212 annotated echocardiogram videos. △ Less","23 June, 2021",https://arxiv.org/pdf/2106.12511
Training Data Subset Selection for Regression with Controlled Generalization Error,Durga Sivasubramanian;Rishabh Iyer;Ganesh Ramakrishnan;Abir De,"Data subset selection from a large number of training instances has been a successful approach toward efficient and cost-effective machine learning. However, models trained on a smaller subset may show poor generalization ability. In this paper, our goal is to design an algorithm for selecting a subset of the training data, so that the model can be trained quickly, without significantly sacrificing on accuracy. More specifically, we focus on data subset selection for L2 regularized regression problems and provide a novel problem formulation which seeks to minimize the training loss with respect to both the trainable parameters and the subset of training data, subject to error bounds on the validation set. We tackle this problem using several technical innovations. First, we represent this problem with simplified constraints using the dual of the original training problem and show that the objective of this new representation is a monotone and alpha-submodular function, for a wide variety of modeling choices. Such properties lead us to develop SELCON, an efficient majorization-minimization algorithm for data subset selection, that admits an approximation guarantee even when the training provides an imperfect estimate of the trained model. Finally, our experiments on several datasets show that SELCON trades off accuracy and efficiency more effectively than the current state-of-the-art. △ Less","23 June, 2021",https://arxiv.org/pdf/2106.12491
From Canonical Correlation Analysis to Self-supervised Graph Neural Networks,Hengrui Zhang;Qitian Wu;Junchi Yan;David Wipf;Philip S. Yu,"We introduce a conceptually simple yet effective model for self-supervised representation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Compared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most importantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an understanding for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets. The code is available at: https://github.com/hengruizhang98/CCA-SSG. △ Less","27 October, 2021",https://arxiv.org/pdf/2106.12484
Innovations Autoencoder and its Application in One-class Anomalous Sequence Detection,Xinyi Wang;Lang Tong,"An innovations sequence of a time series is a sequence of independent and identically distributed random variables with which the original time series has a causal representation. The innovation at a time is statistically independent of the history of the time series. As such, it represents the new information contained at present but not in the past. Because of its simple probability structure, an innovations sequence is the most efficient signature of the original. Unlike the principle or independent component analysis representations, an innovations sequence preserves not only the complete statistical properties but also the temporal order of the original time series. An long-standing open problem is to find a computationally tractable way to extract an innovations sequence of non-Gaussian processes. This paper presents a deep learning approach, referred to as Innovations Autoencoder (IAE), that extracts innovations sequences using a causal convolutional neural network. An application of IAE to the one-class anomalous sequence detection problem with unknown anomaly and anomaly-free models is also presented. △ Less","15 July, 2021",https://arxiv.org/pdf/2106.12382
"Closed-Form, Provable, and Robust PCA via Leverage Statistics and Innovation Search",Mostafa Rahmani;Ping Li,"The idea of Innovation Search, which was initially proposed for data clustering, was recently used for outlier detection. In the application of Innovation Search for outlier detection, the directions of innovation were utilized to measure the innovation of the data points. We study the Innovation Values computed by the Innovation Search algorithm under a quadratic cost function and it is proved that Innovation Values with the new cost function are equivalent to Leverage Scores. This interesting connection is utilized to establish several theoretical guarantees for a Leverage Score based robust PCA method and to design a new robust PCA method. The theoretical results include performance guarantees with different models for the distribution of outliers and the distribution of inliers. In addition, we demonstrate the robustness of the algorithms against the presence of noise. The numerical and theoretical studies indicate that while the presented approach is fast and closed-form, it can outperform most of the existing algorithms. △ Less","23 June, 2021",https://arxiv.org/pdf/2106.12190
Agile Methods in Higher Education: Adapting and Using eduScrum with Real World Projects,Michael Neumann;Lars Baumann,"This Innovative Practice Full Paper presents our learnings of the process to perform a Master of Science class with eduScrum integrating real world problems as projects. We prepared, performed, and evaluated an agile educational concept for the new Master of Science program Digital Transformation organized and provided by the department of business computing at the University of Applied Sciences and Arts - Hochschule Hannover in Germany. The course deals with innovative methodologies of agile project management and is attended by 25 students. We performed the class due the summer term in 2019 and 2020 as a teaching pair. The eduScrum method has been used in different educational contexts, including higher education. During the approach preparation, we decided to use challenges, problems, or questions from the industry. Thus, we acquired four companies and prepared in coordination with them dedicated project descriptions. Each project description was refined in the form of a backlog (list of requirements). We divided the class into four eduScrum teams, one team for each project. The subdivision of the class was done randomly. Since we wanted to integrate realistic projects into industry partners' implementation, we decided to adapt the eduScrum approach. The eduScrum teams were challenged with different projects, e.g., analyzing a dedicated phenomenon in a real project or creating a theoretical model for a company's new project management approach. We present our experiences of the whole process to prepare, perform and evaluate an agile educational approach combined with projects from practice. We found, that the students value the agile method using real world problems. The paper contributes to the distribution of methods for higher education teaching in the classroom and distance learning. △ Less","23 June, 2021",https://arxiv.org/pdf/2106.12166
"PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database",Fangyuan Lei;Da Huang;Jianjian Jiang;Ruijun Ma;Senhong Wang;Jiangzhong Cao;Yusen Lin;Qingyun Dai,"In deep learning area, large-scale image datasets bring a breakthrough in the success of object recognition and retrieval. Nowadays, as the embodiment of innovation, the diversity of the industrial goods is significantly larger, in which the incomplete multiview, multimodal and multilabel are different from the traditional dataset. In this paper, we introduce an industrial goods dataset, namely PatentNet, with numerous highly diverse, accurate and detailed annotations of industrial goods images, and corresponding texts. In PatentNet, the images and texts are sourced from design patent. Within over 6M images and corresponding texts of industrial goods labeled manually checked by professionals, PatentNet is the first ongoing industrial goods image database whose varieties are wider than industrial goods datasets used previously for benchmarking. PatentNet organizes millions of images into 32 classes and 219 subclasses based on the Locarno Classification Agreement. Through extensive experiments on image classification, image retrieval and incomplete multiview clustering, we demonstrate that our PatentNet is much more diverse, complex, and challenging, enjoying higher potentials than existing industrial image datasets. Furthermore, the characteristics of incomplete multiview, multimodal and multilabel in PatentNet are able to offer unparalleled opportunities in the artificial intelligence community and beyond. △ Less","22 June, 2021",https://arxiv.org/pdf/2106.12139
A Survey on Human-aware Robot Navigation,Ronja Möller;Antonino Furnari;Sebastiano Battiato;Aki Härmä;Giovanni Maria Farinella,"Intelligent systems are increasingly part of our everyday lives and have been integrated seamlessly to the point where it is difficult to imagine a world without them. Physical manifestations of those systems on the other hand, in the form of embodied agents or robots, have so far been used only for specific applications and are often limited to functional roles (e.g. in the industry, entertainment and military fields). Given the current growth and innovation in the research communities concerned with the topics of robot navigation, human-robot-interaction and human activity recognition, it seems like this might soon change. Robots are increasingly easy to obtain and use and the acceptance of them in general is growing. However, the design of a socially compliant robot that can function as a companion needs to take various areas of research into account. This paper is concerned with the navigation aspect of a socially-compliant robot and provides a survey of existing solutions for the relevant areas of research as well as an outlook on possible future directions. △ Less","22 June, 2021",https://arxiv.org/pdf/2106.11650
Graph Neural Networks for Learning Real-Time Prices in Electricity Market,Shaohui Liu;Chengyang Wu;Hao Zhu,"Solving the optimal power flow (OPF) problem in real-time electricity market improves the efficiency and reliability in the integration of low-carbon energy resources into the power grids. To address the scalability and adaptivity issues of existing end-to-end OPF learning solutions, we propose a new graph neural network (GNN) framework for predicting the electricity market prices from solving OPFs. The proposed GNN-for-OPF framework innovatively exploits the locality property of prices and introduces physics-aware regularization, while attaining reduced model complexity and fast adaptivity to varying grid topology. Numerical tests have validated the learning efficiency and adaptivity improvements of our proposed method over existing approaches. △ Less","19 June, 2021",https://arxiv.org/pdf/2106.10529
Hybrid approach to detecting symptoms of depression in social media entries,Agnieszka Wołk;Karol Chlasta;Paweł Holas,"Sentiment and lexical analyses are widely used to detect depression or anxiety disorders. It has been documented that there are significant differences in the language used by a person with emotional disorders in comparison to a healthy individual. Still, the effectiveness of these lexical approaches could be improved further because the current analysis focuses on what the social media entries are about, and not how they are written. In this study, we focus on aspects in which these short texts are similar to each other, and how they were created. We present an innovative approach to the depression screening problem by applying Collgram analysis, which is a known effective method of obtaining linguistic information from texts. We compare these results with sentiment analysis based on the BERT architecture. Finally, we create a hybrid model achieving a diagnostic accuracy of 71%. △ Less","19 June, 2021",https://arxiv.org/pdf/2106.10485
Comparing the behavior of OpenMP Implementations with various Applications on two different Fujitsu A64FX platforms,Benjamin Michalowicz;Eric Raut;Yan Kang;Tony Curtis;Barbara Chapman;Dossay Oryspayev,"The development of the A64FX processor by Fujitsu has been a massive innovation in vectorized processors and led to Fugaku: the current world's fastest supercomputer. We use a variety of tools to analyze the behavior and performance of several OpenMP applications with different compilers, and how these applications scale on the different A64FX processors on clusters at Stony Brook University and RIKEN. △ Less","17 June, 2021",https://arxiv.org/pdf/2106.09787
Training like Playing: A Reinforcement Learning And Knowledge Graph-based framework for building Automatic Consultation System in Medical Field,Yining Huang;Meilian Chen;Keke Tang,"We introduce a framework for AI-based medical consultation system with knowledge graph embedding and reinforcement learning components and its implement. Our implement of this framework leverages knowledge organized as a graph to have diagnosis according to evidence collected from patients recurrently and dynamically. According to experiment we designed for evaluating its performance, it archives a good result. More importantly, for getting better performance, researchers can implement it on this framework based on their innovative ideas, well designed experiments and even clinical trials. △ Less","14 June, 2021",https://arxiv.org/pdf/2106.07502
"Information exchange, meaning and redundancy generation in anticipatory systems: self-organization of expectations -- the case of Covid-19",Inga A. Ivanova,"When studying the evolution of complex systems one refers to model representations comprising various descriptive parameters. There is hardly research where system evolution is described on the base of information flows in the system. The paper focuses on the link between the dynamics of information and system evolution. Information, exchanged between different system's parts, before being processed is first provided with meaning by the system. Meanings are generated from the perspective of hindsight, i.e. against the arrow of time. The same information can be differently interpreted by different system's parts (i,e,provided with different meanings) so that the number of options for possible system development is proliferated. Some options eventually turn into observable system states. So that system evolutionary dynamics can be considered as due to information processing within the system. This process is considered here in a model representation. The model under study is Triple Helix (TH) model, which was earlier used to describe interactions between university, industry and government to foster innovations. In TH model the system is comprised of three interacting parts where each part process information ina different way. The model is not limited to the sphere of innovation and can be used in a broader perspective. Here TH is conceptualized in the framework of three compertment model used to describe infectious disease. The paper demonstrates how the dynamics of information and meaning can be incorporated in the description of Covid-19 infectious propagation. The results show correspondence of model predictions with observable infection dynamics. △ Less","25 May, 2021",https://arxiv.org/pdf/2106.07432
Gridless Evolutionary Approach for Line Spectral Estimation with Unknown Model Order,Bai Yan;Qi Zhao;Jin Zhang;J. Andrew Zhang;Xin Yao,"Gridless methods show great superiority in line spectral estimation. These methods need to solve an atomic l_0 norm (i.e., the continuous analog of l_0 norm) minimization problem to estimate frequencies and model order. Since this problem is NP-hard to compute, relaxations of atomic l_0 norm, such as nuclear norm and reweighted atomic norm, have been employed for promoting sparsity. However, the relaxations give rise to a resolution limit, subsequently leading to biased model order and convergence error. To overcome the above shortcomings of relaxation, we propose a novel idea of simultaneously estimating the frequencies and model order by means of the atomic l_0 norm. To accomplish this idea, we build a multiobjective optimization model. The measurment error and the atomic l_0 norm are taken as the two optimization objectives. The proposed model directly exploits the model order via the atomic l_0 norm, thus breaking the resolution limit. We further design a variable-length evolutionary algorithm to solve the proposed model, which includes two innovations. One is a variable-length coding and search strategy. It flexibly codes and interactively searches diverse solutions with different model orders. These solutions act as steppingstones that help fully exploring the variable and open-ended frequency search space and provide extensive potentials towards the optima. Another innovation is a model order pruning mechanism, which heuristically prunes less contributive frequencies within the solutions, thus significantly enhancing convergence and diversity. Simulation results confirm the superiority of our approach in both frequency estimation and model order selection. △ Less","14 June, 2021",https://arxiv.org/pdf/2106.07323
Evolutionary Robust Clustering Over Time for Temporal Data,Qi Zhao;Bai Yan;Yuhui Shi,"In many clustering scenes, data samples' attribute values change over time. For such data, we are often interested in obtaining a partition for each time step and tracking the dynamic change of partitions. Normally, a smooth change is assumed for data to have a temporal smooth nature. Existing algorithms consider the temporal smoothness as an a priori preference and bias the search towards the preferred direction. This a priori manner leads to a risk of converging to an unexpected region because it is not always the case that a reasonable preference can be elicited given the little prior knowledge about the data. To address this issue, this paper proposes a new clustering framework called evolutionary robust clustering over time. One significant innovation of the proposed framework is processing the temporal smoothness in an a posteriori manner, which avoids unexpected convergence that occurs in existing algorithms. Furthermore, the proposed framework automatically tunes the weight of smoothness without data's affinity matrix and predefined parameters, which holds better applicability and scalability. The effectiveness and efficiency of the proposed framework are confirmed by comparing with state-of-the-art algorithms on both synthetic and real datasets. △ Less","14 June, 2021",https://arxiv.org/pdf/2106.07252
Certification of embedded systems based on Machine Learning: A survey,Guillaume Vidot;Christophe Gabreau;Ileana Ober;Iulian Ober,"Advances in machine learning (ML) open the way to innovating functions in the avionic domain, such as navigation/surveillance assistance (e.g. vision-based navigation, obstacle sensing, virtual sensing), speechto-text applications, autonomous flight, predictive maintenance or cockpit assistance. Current certification standards and practices, which were defined and refined decades over decades with classical programming in mind, do not however support this new development paradigm. This article provides an overview of the main challenges raised by the use ML in the demonstration of compliance with regulation requirements, and a survey of literature relevant to these challenges, with particular focus on the issues of robustness and explainability of ML results. △ Less","30 July, 2021",https://arxiv.org/pdf/2106.07221
Contingency-Aware Influence Maximization: A Reinforcement Learning Approach,Haipeng Chen;Wei Qiu;Han-Ching Ou;Bo An;Milind Tambe,"The influence maximization (IM) problem aims at finding a subset of seed nodes in a social network that maximize the spread of influence. In this study, we focus on a sub-class of IM problems, where whether the nodes are willing to be the seeds when being invited is uncertain, called contingency-aware IM. Such contingency aware IM is critical for applications for non-profit organizations in low resource communities (e.g., spreading awareness of disease prevention). Despite the initial success, a major practical obstacle in promoting the solutions to more communities is the tremendous runtime of the greedy algorithms and the lack of high performance computing (HPC) for the non-profits in the field -- whenever there is a new social network, the non-profits usually do not have the HPCs to recalculate the solutions. Motivated by this and inspired by the line of works that use reinforcement learning (RL) to address combinatorial optimization on graphs, we formalize the problem as a Markov Decision Process (MDP), and use RL to learn an IM policy over historically seen networks, and generalize to unseen networks with negligible runtime at test phase. To fully exploit the properties of our targeted problem, we propose two technical innovations that improve the existing methods, including state-abstraction and theoretically grounded reward shaping. Empirical results show that our method achieves influence as high as the state-of-the-art methods for contingency-aware IM, while having negligible runtime at test phase. △ Less","13 June, 2021",https://arxiv.org/pdf/2106.07039
How Crucial Is It for 6G Networks to Be Autonomous?,Nadia Adem;Ahmed Benfaid;Ramy Harib;Anas Alarabi,"The sixth generation (6G), unlike any of the previous generations, is envisioned by 2030 to connect everything. Moreover, in addition to the new use cases, 6G is expected to support, it will need to provide a superior performance over 5G. The global connectivity, large network dimensions, users heterogeneity, extremely low-power consumption, high throughput, ultrahigh reliability, efficient network operation and maintenance, and low-latency requirements to be met by future networks inevitably necessitate the autonomy of 6G. Intelligence, facilitated mainly by the advancement of artificial intelligence (AI) techniques, is a key to achieve autonomy. In this paper, we provide a bird's-eye view of 6G, its vision, progress, and objectives. Furthermore, we present some technologies that would be mainly enabling intelligent globally connected world. In addition to discussing the role of AI for future wireless communications, we, unlike any other review papers, provide our original results which give early evidence for the viability of achieving 6G networks autonomy through leveraging AI advances. Furthermore, we, very importantly, identify 6G implementation challenges and key innovative techniques that promise to solve them. This article serves as a starting point for learners to acquire more knowledge about 6G and also for researchers to promote more development to the field. △ Less","13 August, 2021",https://arxiv.org/pdf/2106.06949
A One-Shot Texture-Perceiving Generative Adversarial Network for Unsupervised Surface Inspection,Lingyun Gu;Lin Zhang;Zhaokui Wang,"Visual surface inspection is a challenging task owing to the highly diverse appearance of target surfaces and defective regions. Previous attempts heavily rely on vast quantities of training examples with manual annotation. However, in some practical cases, it is difficult to obtain a large number of samples for inspection. To combat it, we propose a hierarchical texture-perceiving generative adversarial network (HTP-GAN) that is learned from the one-shot normal image in an unsupervised scheme. Specifically, the HTP-GAN contains a pyramid of convolutional GANs that can capture the global structure and fine-grained representation of an image simultaneously. This innovation helps distinguishing defective surface regions from normal ones. In addition, in the discriminator, a texture-perceiving module is devised to capture the spatially invariant representation of normal image via directional convolutions, making it more sensitive to defective areas. Experiments on a variety of datasets consistently demonstrate the effectiveness of our method. △ Less","12 June, 2021",https://arxiv.org/pdf/2106.06792
Towards a Privacy-preserving Deep Learning-based Network Intrusion Detection in Data Distribution Services,Stanislav Abaimov,"Data Distribution Service (DDS) is an innovative approach towards communication in ICS/IoT infrastructure and robotics. Being based on the cross-platform and cross-language API to be applicable in any computerised device, it offers the benefits of modern programming languages and the opportunities to develop more complex and advanced systems. However, the DDS complexity equally increases its vulnerability, while the existing security measures are limited to plug-ins and static rules, with the rest of the security provided by third-party applications and operating system. Specifically, traditional intrusion detection systems (IDS) do not detect any anomalies in the publish/subscribe method. With the exponentially growing global communication exchange, securing DDS is of the utmost importance to futureproofing industrial, public, and even personal devices and systems. This report presents an experimental work on the simulation of several specific attacks against DDS, and the application of Deep Learning for their detection. The findings show that even though Deep Learning allows to detect all simulated attacks using only metadata analysis, their detection level varies, with some of the advanced attacks being harder to detect. The limitations imposed by the attempts to preserve privacy significantly decrease the detection rate. The report also reviews the drawbacks and limitations of the Deep Learning approach and proposes a set of selected solutions and configurations, that can further improve the DDS security. △ Less","12 June, 2021",https://arxiv.org/pdf/2106.06765
Explaining the Deep Natural Language Processing by Mining Textual Interpretable Features,Francesco Ventura;Salvatore Greco;Daniele Apiletti;Tania Cerquitelli,"Despite the high accuracy offered by state-of-the-art deep natural-language models (e.g. LSTM, BERT), their application in real-life settings is still widely limited, as they behave like a black-box to the end-user. Hence, explainability is rapidly becoming a fundamental requirement of future-generation data-driven systems based on deep-learning approaches. Several attempts to fulfill the existing gap between accuracy and interpretability have been done. However, robust and specialized xAI (Explainable Artificial Intelligence) solutions tailored to deep natural-language models are still missing. We propose a new framework, named T-EBAnO, which provides innovative prediction-local and class-based model-global explanation strategies tailored to black-box deep natural-language models. Given a deep NLP model and the textual input data, T-EBAnO provides an objective, human-readable, domain-specific assessment of the reasons behind the automatic decision-making process. Specifically, the framework extracts sets of interpretable features mining the inner knowledge of the model. Then, it quantifies the influence of each feature during the prediction process by exploiting the novel normalized Perturbation Influence Relation index at the local level and the novel Global Absolute Influence and Global Relative Influence indexes at the global level. The effectiveness and the quality of the local and global explanations obtained with T-EBAnO are proved on (i) a sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic comment classification task performed by an LSTM model. △ Less","12 June, 2021",https://arxiv.org/pdf/2106.06697
Stochastic modelling of blockchain consensus,Claudio J. Tessone;Paolo Tasca;Flavio Iannelli,"Blockchain and general purpose distributed ledgers are foundational technologies which bring significant innovation in the infrastructures and other underpinnings of our socio-economic systems. These P2P technologies are able to securely diffuse information within and across networks, without need for trustees or central authorities to enforce consensus. In this contribution, we propose a minimalistic stochastic model to understand the dynamics of blockchain-based consensus. By leveraging on random-walk theory, we model block propagation delay on different network topologies and provide a classification of blockchain systems in terms of two emergent properties. Firstly, we identify two performing regimes: a functional regime corresponding to an optimal system function; and a non-functional regime characterised by a congested or branched state of sub-optimal blockchains. Secondly, we discover a phase transition during the emergence of consensus and numerically investigate the corresponding critical point. Our results provide important insights into the consensus mechanism and sub-optimal states in decentralised systems. △ Less","11 June, 2021",https://arxiv.org/pdf/2106.06465
Security and Privacy for Healthcare Blockchains,Rui Zhang;Rui Xue;Ling Liu,"Healthcare blockchains provide an innovative way to store healthcare information, execute healthcare transactions, and build trust for healthcare data sharing and data integration in a decentralized open healthcare network environment. Although the healthcare blockchain technology has attracted broad interests and attention in industry, government and academia, the security and privacy concerns remain the focus of debate when deploying blockchains for information sharing in the healthcare sector from business operation to research collaboration. This paper focuses on the security and privacy requirements for medical data sharing using blockchain, and provides a comprehensive analysis of the security and privacy risks and requirements, accompanied by technical solution techniques and strategies. First, we discuss the security and privacy requirements and attributes required for electronic medical data sharing by deploying the healthcare blockchain. Second, we categorize existing efforts into three reference blockchain usage scenarios for electronic medical data sharing, and discuss the technologies for implementing these security and privacy properties in the three categories of usage scenarios for healthcare blockchain, such as anonymous signatures, attribute-based encryption, zero-knowledge proofs, verification techniques for smart contract security. Finally, we discuss other potential blockchain application scenarios in healthcare sector. We conjecture that this survey will help healthcare professionals, decision makers, and healthcare service developers to gain technical and intuitive insights into the security and privacy of healthcare blockchains in terms of concepts, risks, requirements, development and deployment technologies and systems. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.06136
Fastening the Initial Access in 5G NR Sidelink for 6G V2X Networks,Marouan Mizmizi;Francesco Linsalata;Mattia Brambilla;Filippo Morandi;Kai Dong;Maurizio Magarini;Monica Nicoli;Majid Nasiri Khormuji;Peng Wang;Renaud Alexandre Pitaval;Umberto Spagnolini,"The ever-increasing demand for intelligent, automated, and connected mobility solutions pushes for the development of an innovative sixth Generation (6G) of cellular networks. A radical transformation on the physical layer of vehicular communications is planned, with a paradigm shift towards beam-based millimeter Waves or sub-Terahertz communications, which require precise beam pointing for guaranteeing the communication link, especially in high mobility. A key design aspect is a fast and proactive Initial Access (IA) algorithm to select the optimal beam to be used. In this work, we investigate alternative IA techniques to fasten the current fifth-generation (5G) standard, targeting an efficient 6G design. First, we discuss cooperative position-based schemes that rely on the position information. Then, motivated by the intuition of a non-uniform distribution of the communication directions due to road topology constraints, we design two Probabilistic Codebook (PCB) techniques of prioritized beams. In the first one, the PCBs are built leveraging past collected traffic information, while in the second one, we use the Hough Transform over the digital map to extract dominant road directions. We also show that the information coming from the angular probability distribution allows designing non-uniform codebook quantization, reducing the degradation of the performances compared to uniform one. Numerical simulation on realistic scenarios shows that PCBs-based beam selection outperforms the 5G standard in terms of the number of IA trials, with a performance comparable to position-based methods, without requiring the signaling of sensitive information. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.05716
On the Use of Data from Multiple Mobile Network Operators in Europe to fight COVID-19,Michele Vespe;Stefano Maria Iacus;Carlos Santamaria;Francesco Sermi;Spyridon Spyratos,"The rapid spread of COVID-19 infections on a global level has highlighted the need for accurate, transparent and timely information regarding collective mobility patterns to inform de-escalation strategies as well as to provide forecasting capacity for re-escalation policies aiming at addressing further waves of the virus. Such information can be extracted using aggregate anonymised data from innovative sources such as mobile positioning data. This paper presents lessons learnt and results of a unique Business-to-Government (B2G) initiative between several Mobile Network Operators in Europe and the European Commission. Mobile positioning data have supported policy makers and practitioners with evidence and data-driven knowledge to understand and predict the spread of the disease, the effectiveness of the containment measures, their socio-economic impacts while feeding scenarios at EU scale and in a comparable way across countries. The challenges of this data sharing initiative are not limited to data quality, harmonisation, and comparability across countries, however important they are. Equally essential aspects that need to be addressed from the onset are related to data privacy, security, fundamental rights and commercial sensitivity. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.05647
FRI-TEM: Time Encoding Sampling of Finite-Rate-of-Innovation Signals,Hila Naaman;Satish Mulleti;Yonina C. Eldar,"Classical sampling is based on acquiring signal amplitudes at specific points in time, with the minimal sampling rate dictated by the degrees of freedom in the signal. The samplers in this framework are controlled by a global clock that operates at a rate greater than or equal to the minimal sampling rate. At high sampling rates, clocks are power-consuming and prone to electromagnetic interference. An integrate-and-fire time encoding machine (IF-TEM) is an alternative power-efficient sampling mechanism which does not require a global clock. Here, the samples are irregularly spaced threshold-based samples. In this paper, we investigate the problem of sampling finite-rate-of-innovation (FRI) signals using an IF-TEM. We provide theoretical recovery guarantees for an FRI signal with arbitrary pulse shape and without any constraint on the minimum separation between the pulses. In particular, we show how to design a sampling kernel, IF-TEM, and recovery method such that the FRI signals are perfectly reconstructed. We then propose a modification to the sampling kernel to improve noise robustness. Our results enable designing low-cost and energy-efficient analog-to-digital converters for FRI signals. △ Less","15 June, 2021",https://arxiv.org/pdf/2106.05564
Investigating Alternatives to the Root Mean Square for Adaptive Gradient Methods,Brett Daley;Christopher Amato,"Adam is an adaptive gradient method that has experienced widespread adoption due to its fast and reliable training performance. Recent approaches have not offered significant improvement over Adam, often because they do not innovate upon one of its core features: normalization by the root mean square (RMS) of recent gradients. However, as noted by Kingma and Ba (2015), any number of L^p normalizations are possible, with the RMS corresponding to the specific case of p=2. In our work, we theoretically and empirically characterize the influence of different L^p norms on adaptive gradient methods for the first time. We show mathematically how the choice of p influences the size of the steps taken, while leaving other desirable properties unaffected. We evaluate Adam with various L^p norms on a suite of deep learning benchmarks, and find that p > 2 consistently leads to improved learning speed and final performance. The choices of p=3 or p=6 also match or outperform state-of-the-art methods in all of our experiments. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.05449
On the Cover and Pombra Gaussian Feedback Capacity: Complete Sequential Characterizations via a Sufficient Statistic,Charalambos D. Charalambous;Christos Kourtellaris;Stelios Louka,"The main objective of this paper is to derive a new sequential characterization of the Cover and Pombra \cite{cover-pombra1989} characterization of the n-finite block or transmission feedback information (n-FTFI) capacity, which clarifies several issues of confusion and incorrect interpretation of results in literature. The optimal channel input processes of the new equivalent sequential characterizations are expressed as functionals of a sufficient statistic and a Gaussian orthogonal innovations process. From the new representations follows that the Cover and Pombra characterization of the n-FTFI capacity is expressed as a functional of two generalized matrix difference Riccati equations (DRE) of filtering theory of Gaussian systems. This contradicts results which are redundant in the literature, and illustrates the fundamental complexity of the feedback capacity formula. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.05075
Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding,Xin Sun;Tao Ge;Furu Wei;Houfeng Wang,"In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC). SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference. Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield the same predictions as greedy decoding but with a significant speedup for online inference. Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss. Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages. Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.04970
A Markov Decision Process Approach for Managing Medical Drone Deliveries,Amin Asadi;Sarah Nurre Pinkley;Martijn Mes,"We consider the problem of optimizing the distribution operations at a drone hub that dispatches drones to different geographic locations generating stochastic demands for medical supplies. Drone delivery is an innovative method that introduces many benefits, such as low-contact delivery, thereby reducing the spread of pandemic and vaccine-preventable diseases. While we focus on medical supply delivery for this work, drone delivery is suitable for many other items, including food, postal parcels, and e-commerce. In this paper, our goal is to address drone delivery challenges related to the stochastic demands of different geographic locations. We consider different classes of demand related to geographic locations that require different flight ranges, which is directly related to the amount of charge held in a drone battery. We classify the stochastic demands based on their distance from the drone hub, use a Markov decision process to model the problem, and perform computational tests using realistic data representing a prominent drone delivery company. We solve the problem using a reinforcement learning method and show its high performance compared with the exact solution found using dynamic programming. Finally, we analyze the results and provide insights for managing the drone hub operations. △ Less","29 November, 2021",https://arxiv.org/pdf/2106.04729
Defining definition: a Text mining Approach to Define Innovative Technological Fields,Vito Giordano;Filippo Chiarello;Elena Cervelli,"One of the first task of an innovative project is delineating the scope of the project itself or of the product/service to be developed. A wrong scope definition can determine (in the worst case) project failure. A good scope definition become even more relevant in technological intensive innovation projects, nowadays characterized by a highly dynamic multidisciplinary, turbulent and uncertain environment. In these cases, the boundaries of the project are not easily detectable and it is difficult to decide what it is in-scope and out-of-scope. The present work proposes a tool for the scope delineation process, that automatically define an innovative technological field or a new technology. The tool is based on Text Mining algorithm that exploits Elsevier's Scopus abstracts in order to the extract relevant data to define a technological scope. The automatic definition tool is then applied on four case studies: Artificial Intelligence and Data Science. The results show how the tool can provide many crucial information in the definition process of a technological field. In particular for the target technological field (or technology), it provides the definition and other elements related to the target. △ Less","8 June, 2021",https://arxiv.org/pdf/2106.04210
CloudChain: A Cloud Blockchain Using Shared Memory Consensus and RDMA,Minghui Xu;Shuo Liu;Dongxiao Yu;Xiuzhen Cheng;Shaoyong Guo;Jiguo Yu,"Blockchain technologies can enable secure computing environments among mistrusting parties. Permissioned blockchains are particularly enlightened by companies, enterprises, and government agencies due to their efficiency, customizability, and governance-friendly features. Obviously, seamlessly fusing blockchain and cloud computing can significantly benefit permissioned blockchains; nevertheless, most blockchains implemented on clouds are originally designed for loosely-coupled networks where nodes communicate asynchronously, failing to take advantages of the closely-coupled nature of cloud servers. In this paper, we propose an innovative cloud-oriented blockchain -- CloudChain, which is a modularized three-layer system composed of the network layer, consensus layer, and blockchain layer. CloudChain is based on a shared-memory model where nodes communicate synchronously by direct memory accesses. We realize the shared-memory model with the Remote Direct Memory Access technology, based on which we propose a shared-memory consensus algorithm to ensure presistence and liveness, the two crucial blockchain security properties countering Byzantine nodes. We also implement a CloudChain prototype based on a RoCEv2-based testbed to experimentally validate our design, and the results verify the feasibility and efficiency of CloudChain. △ Less","8 June, 2021",https://arxiv.org/pdf/2106.04122
Widening Access to Applied Machine Learning with TinyML,Vijay Janapa Reddi;Brian Plancher;Susan Kennedy;Laurence Moroney;Pete Warden;Anant Agarwal;Colby Banbury;Massimo Banzi;Matthew Bennett;Benjamin Brown;Sharad Chitlangia;Radhika Ghosal;Sarah Grafman;Rupert Jaeger;Srivatsan Krishnan;Maximilian Lam;Daniel Leiker;Cara Mann;Mark Mazumder;Dominic Pajak;Dhilan Ramaprasad;J. Evan Smith;Matthew Stewart;Dustin Tingley,"Broadening access to both computational and educational resources is critical to diffusing machine-learning (ML) innovation. However, today, most ML resources and experts are siloed in a few countries and organizations. In this paper, we describe our pedagogical approach to increasing access to applied ML through a massive open online course (MOOC) on Tiny Machine Learning (TinyML). We suggest that TinyML, ML on resource-constrained embedded devices, is an attractive means to widen access because TinyML both leverages low-cost and globally accessible hardware, and encourages the development of complete, self-contained applications, from data collection to deployment. To this end, a collaboration between academia (Harvard University) and industry (Google) produced a four-part MOOC that provides application-oriented instruction on how to develop solutions using TinyML. The series is openly available on the edX MOOC platform, has no prerequisites beyond basic programming, and is designed for learners from a global variety of backgrounds. It introduces pupils to real-world applications, ML algorithms, data-set engineering, and the ethical considerations of these technologies via hands-on programming and deployment of TinyML applications in both the cloud and their own microcontrollers. To facilitate continued learning, community building, and collaboration beyond the courses, we launched a standalone website, a forum, a chat, and an optional course-project competition. We also released the course materials publicly, hoping they will inspire the next generation of ML practitioners and educators and further broaden access to cutting-edge ML technologies. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.04008
Self-supervised Depth Estimation Leveraging Global Perception and Geometric Smoothness Using On-board Videos,Shaocheng Jia;Xin Pei;Wei Yao;S. C. Wong,"Self-supervised depth estimation has drawn much attention in recent years as it does not require labeled data but image sequences. Moreover, it can be conveniently used in various applications, such as autonomous driving, robotics, realistic navigation, and smart cities. However, extracting global contextual information from images and predicting a geometrically natural depth map remain challenging. In this paper, we present DLNet for pixel-wise depth estimation, which simultaneously extracts global and local features with the aid of our depth Linformer block. This block consists of the Linformer and innovative soft split multi-layer perceptron blocks. Moreover, a three-dimensional geometry smoothness loss is proposed to predict a geometrically natural depth map by imposing the second-order smoothness constraint on the predicted three-dimensional point clouds, thereby realizing improved performance as a byproduct. Finally, we explore the multi-scale prediction strategy and propose the maximum margin dual-scale prediction strategy for further performance improvement. In experiments on the KITTI and Make3D benchmarks, the proposed DLNet achieves performance competitive to those of the state-of-the-art methods, reducing time and space complexities by more than 62\% and 56\%, respectively. Extensive testing on various real-world situations further demonstrates the strong practicality and generalization capability of the proposed model. △ Less","7 June, 2021",https://arxiv.org/pdf/2106.03505
Self-Damaging Contrastive Learning,Ziyu Jiang;Tianlong Chen;Bobak Mortazavi;Zhangyang Wang,"The recent breakthrough achieved by contrastive learning accelerates the pace for deploying unsupervised training on real-world data applications. However, unlabeled data in reality is commonly imbalanced and shows a long-tail distribution, and it is unclear how robustly the latest contrastive learning methods could perform in the practical scenario. This paper proposes to explicitly tackle this challenge, via a principled framework called Self-Damaging Contrastive Learning (SDCLR), to automatically balance the representation learning without knowing the classes. Our main inspiration is drawn from the recent finding that deep models have difficult-to-memorize samples, and those may be exposed through network pruning. It is further natural to hypothesize that long-tail samples are also tougher for the model to learn well due to insufficient examples. Hence, the key innovation in SDCLR is to create a dynamic self-competitor model to contrast with the target model, which is a pruned version of the latter. During training, contrasting the two models will lead to adaptive online mining of the most easily forgotten samples for the current target model, and implicitly emphasize them more in the contrastive loss. Extensive experiments across multiple datasets and imbalance settings show that SDCLR significantly improves not only overall accuracies but also balancedness, in terms of linear evaluation on the full-shot and few-shot settings. Our code is available at: https://github.com/VITA-Group/SDCLR. △ Less","5 June, 2021",https://arxiv.org/pdf/2106.02990
Meta-research on COVID-19: An overview of the early trends,Giovanni Colavizza,"COVID-19 is having a dramatic impact on research and researchers. The pandemic has underlined the severity of known challenges in research and surfaced new ones, but also accelerated the adoption of innovations and manifested new opportunities. This review considers early trends emerging from meta-research on COVID-19. In particular, it focuses on the following topics: i) mapping COVID-19 research; ii) data and machine learning; iii) research practices including open access and open data, reviewing, publishing and funding; iv) communicating research to the public; v) the impact of COVID-19 on researchers, in particular with respect to gender and career trajectories. This overview finds that most early meta-research on COVID-19 has been reactive and focused on short-term questions, while more recently a shift to consider the long-term consequences of COVID-19 is taking place. Based on these findings, the author speculates that some aspects of doing research during COVID-19 are more likely to persist than others. These include: the shift to virtual for academic events such as conferences; the use of openly accessible pre-prints; the `datafication' of scholarly literature and consequent broader adoption of machine learning in science communication; the public visibility of research and researchers on social and online media. △ Less","5 June, 2021",https://arxiv.org/pdf/2106.02961
Constrained Generalized Additive 2 Model with Consideration of High-Order Interactions,Akihisa Watanabe;Michiya Kuramata;Kaito Majima;Haruka Kiyohara;Kensho Kondo;Kazuhide Nakata,"In recent years, machine learning and AI have been introduced in many industrial fields. In fields such as finance, medicine, and autonomous driving, where the inference results of a model may have serious consequences, high interpretability as well as prediction accuracy is required. In this study, we propose CGA2M+, which is based on the Generalized Additive 2 Model (GA2M) and differs from it in two major ways. The first is the introduction of monotonicity. Imposing monotonicity on some functions based on an analyst's knowledge is expected to improve not only interpretability but also generalization performance. The second is the introduction of a higher-order term: given that GA2M considers only second-order interactions, we aim to balance interpretability and prediction accuracy by introducing a higher-order term that can capture higher-order interactions. In this way, we can improve prediction performance without compromising interpretability by applying learning innovation. Numerical experiments showed that the proposed model has high predictive performance and interpretability. Furthermore, we confirmed that generalization performance is improved by introducing monotonicity. △ Less","22 November, 2021",https://arxiv.org/pdf/2106.02836
Coordination problems on networks revisited: statics and dynamics,Luca Dall'Asta,"Simple binary-state coordination models are widely used to study collective socio-economic phenomena such as the spread of innovations or the adoption of products on social networks. The common trait of these systems is the occurrence of large-scale coordination events taking place abruptly, in the form of a cascade process, as a consequence of small perturbations of an apparently stable state. The conditions for the occurrence of cascade instabilities have been largely analysed in the literature, however for the same coordination models no sufficient attention was given to the relation between structural properties of (Nash) equilibria and possible outcomes of dynamical equilibrium selection. Using methods from the statistical physics of disordered systems, the present work investigates both analytically and numerically, the statistical properties of such Nash equilibria on networks, focusing mostly on random graphs. We provide an accurate description of these properties, which is then exploited to shed light on the mechanisms behind the onset of coordination/miscoordination on large networks. This is done studying the most common processes of dynamical equilibrium selection, such as best response, bounded-rational dynamics and learning processes. In particular, we show that well beyond the instability region, full coordination is still globally stochastically stable, however equilibrium selection processes with low stochasticity (e.g. best response) or strong memory effects (e.g. reinforcement learning) can be prevented from achieving full coordination by being trapped into a large (exponentially in number of agents) set of locally stable Nash equilibria at low/medium coordination (inefficient equilibria). These results should be useful to allow a better understanding of general coordination problems on complex networks. △ Less","4 June, 2021",https://arxiv.org/pdf/2106.02548
Subdivision-Based Mesh Convolution Networks,Shi-Min Hu;Zheng-Ning Liu;Meng-Hao Guo;Jun-Xiong Cai;Jiahui Huang;Tai-Jiang Mu;Ralph R. Martin,"Convolutional neural networks (CNNs) have made great breakthroughs in 2D computer vision. However, their irregular structure makes it hard to harness the potential of CNNs directly on meshes. A subdivision surface provides a hierarchical multi-resolution structure, in which each face in a closed 2-manifold triangle mesh is exactly adjacent to three faces. Motivated by these two observations, this paper presents SubdivNet, an innovative and versatile CNN framework for 3D triangle meshes with Loop subdivision sequence connectivity. Making an analogy between mesh faces and pixels in a 2D image allows us to present a mesh convolution operator to aggregate local features from nearby faces. By exploiting face neighborhoods, this convolution can support standard 2D convolutional network concepts, e.g. variable kernel size, stride, and dilation. Based on the multi-resolution hierarchy, we make use of pooling layers which uniformly merge four faces into one and an upsampling method which splits one face into four. Thereby, many popular 2D CNN architectures can be easily adapted to process 3D meshes. Meshes with arbitrary connectivity can be remeshed to have Loop subdivision sequence connectivity via self-parameterization, making SubdivNet a general approach. Extensive evaluation and various applications demonstrate SubdivNet's effectiveness and efficiency. △ Less","29 December, 2021",https://arxiv.org/pdf/2106.02285
Tractable Regularization of Probabilistic Circuits,Anji Liu;Guy Van den Broeck,"Probabilistic Circuits (PCs) are a promising avenue for probabilistic modeling. They combine advantages of probabilistic graphical models (PGMs) with those of neural networks (NNs). Crucially, however, they are tractable probabilistic models, supporting efficient and exact computation of many probabilistic inference queries, such as marginals and MAP. Further, since PCs are structured computation graphs, they can take advantage of deep-learning-style parameter updates, which greatly improves their scalability. However, this innovation also makes PCs prone to overfitting, which has been observed in many standard benchmarks. Despite the existence of abundant regularization techniques for both PGMs and NNs, they are not effective enough when applied to PCs. Instead, we re-think regularization for PCs and propose two intuitive techniques, data softening and entropy regularization, that both take advantage of PCs' tractability and still have an efficient implementation as a computation graph. Specifically, data softening provides a principled way to add uncertainty in datasets in closed form, which implicitly regularizes PC parameters. To learn parameters from a softened dataset, PCs only need linear time by virtue of their tractability. In entropy regularization, the exact entropy of the distribution encoded by a PC can be regularized directly, which is again infeasible for most other density estimation models. We show that both methods consistently improve the generalization performance of a wide variety of PCs. Moreover, when paired with a simple PC structure, we achieved state-of-the-art results on 10 out of 20 standard discrete density estimation benchmarks. △ Less","4 June, 2021",https://arxiv.org/pdf/2106.02264
Learning Elastic Embeddings for Customizing On-Device Recommenders,Tong Chen;Hongzhi Yin;Yujia Zheng;Zi Huang;Yang Wang;Meng Wang,"In today's context, deploying data-driven services like recommendation on edge devices instead of cloud servers becomes increasingly attractive due to privacy and network latency concerns. A common practice in building compact on-device recommender systems is to compress their embeddings which are normally the cause of excessive parameterization. However, despite the vast variety of devices and their associated memory constraints, existing memory-efficient recommender systems are only specialized for a fixed memory budget in every design and training life cycle, where a new model has to be retrained to obtain the optimal performance while adapting to a smaller/larger memory budget. In this paper, we present a novel lightweight recommendation paradigm that allows a well-trained recommender to be customized for arbitrary device-specific memory constraints without retraining. The core idea is to compose elastic embeddings for each item, where an elastic embedding is the concatenation of a set of embedding blocks that are carefully chosen by an automated search function. Correspondingly, we propose an innovative approach, namely recommendation with universally learned elastic embeddings (RULE). To ensure the expressiveness of all candidate embedding blocks, RULE enforces a diversity-driven regularization when learning different embedding blocks. Then, a performance estimator-based evolutionary search function is designed, allowing for efficient specialization of elastic embeddings under any memory constraint for on-device recommendation. Extensive experiments on real-world datasets reveal the superior performance of RULE under tight memory budgets. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.02223
"Influence of cognitive, geographical, and collaborative proximity on knowledge production of Canadian nanotechnology",Elva Luz Crespo Neira;Ashkan Ebadi;Catherine Beaudry;Andrea Schiffauerova,"Incorporating existing knowledge is vital for innovating, discovering, and generating new ideas. Knowledge production through research and invention is the key to scientific and technological development. As an emerging technology, nanotechnology has already proved its great potential for the global economy, attracting considerable federal investments. Canada is reported as one of the major players in producing nanotechnology research. In this paper, we focused on the main drivers of knowledge production and diffusion by analyzing Canadian nanotechnology researchers. We hypothesized that knowledge production in Canadian nanotechnology is influenced by three key proximity factors, namely cognitive, geographical, and collaborative. Using statistical analysis, social network analysis, and machine learning techniques we comprehensively assessed the influence of the proximity factors on academic knowledge production. Our results not only prove a significant impact of the three key proximity factors but also their predictive potential. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.02110
Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits,Ruohan Zhan;Vitor Hadad;David A. Hirshberg;Susan Athey,"It has become increasingly common for data to be collected adaptively, for example using contextual bandits. Historical data of this type can be used to evaluate other treatment assignment policies to guide future innovation or experiments. However, policy evaluation is challenging if the target policy differs from the one used to collect data, and popular estimators, including doubly robust (DR) estimators, can be plagued by bias, excessive variance, or both. In particular, when the pattern of treatment assignment in the collected data looks little like the pattern generated by the policy to be evaluated, the importance weights used in DR estimators explode, leading to excessive variance. In this paper, we improve the DR estimator by adaptively weighting observations to control its variance. We show that a t-statistic based on our improved estimator is asymptotically normal under certain conditions, allowing us to form confidence intervals and test hypotheses. Using synthetic data and public benchmarks, we provide empirical evidence for our estimator's improved accuracy and inferential properties relative to existing alternatives. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.02029
Cloud-Enabled High-Altitude Platform Systems: Challenges and Opportunities,Khaleel Mershad;Hayssam Dahrouj;Hadi Sarieddeen;Basem Shihada;Tareq Al-Naffouri;Mohamed-Slim Alouini,"Augmenting ground-level communications with flying networks, such as the high-altitude platform system (HAPS), is among the major innovative initiatives of the next generation of wireless systems (6G). Given HAPS quasi-static positioning at the stratosphere, HAPS-to-ground and HAPS-to-air connectivity frameworks are expected to be prolific in terms of data acquisition and computing, especially given the mild weather and quasi-constant wind speed characteristics of the stratospheric layer. This paper explores the opportunities stemming from the realization of cloud-enabled HAPS in the context of telecommunications applications and services. The paper first advocates for the potential physical advantages of deploying HAPS as flying data-centers, also known as super-macro base stations. The paper then describes various cloud services that can be offered from the HAPS and the merits that can be achieved by this integration, such as enhancing the quality, speed, and range of the offered services. The proposed services span a wide range of fields, including satellites, Internet of Things (IoT), ad hoc networks (such as sensor; vehicular; and aerial networks), gaming, and social networks. For each service, the paper illustrates the methods that would be used by cloud providers to offload the service data to the HAPS and enable the cloud customers to consume the service. The paper further sheds light on the challenges that need to be addressed for realizing practical cloud-enabled HAPS, mainly, those related to high energy, processing power, quality of service (QoS), and security considerations. Finally, the paper discusses some open issues on the topic, namely, HAPS mobility and message routing, HAPS security via blockchain and machine learning, artificial intelligence-based resource allocation in cloud-enabled HAPS, and integration with vertical heterogeneous networks. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.02006
DEIS: Dependability Engineering Innovation for Industrial CPS,Erik Armengaud;Georg Macher;Alexander Massoner;Sebastian Frager;Rasmus Adler;Daniel Schneider;Simone Longo;Massimiliano Melis;Riccardo Groppo;Federica Villa;Padraig OLeary;Kevin Bambury;Finnegan Anita;Marc Zeller;Kai Hoefig;Yiannis Papadopoulos;Richard Hawkins;Tim Kelly,"The open and cooperative nature of Cyber-Physical Systems (CPS) poses new challenges in assuring dependability. The DEIS project (Dependability Engineering Innovation for automotive CPS. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 732242, see http://www.deis-project.eu) addresses these challenges by developing technologies that form a science of dependable system integration. In the core of these technologies lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable in the field facilitating (a) efficient synthesis of component and system dependability information over the supply chain and (b) effective evaluation of this information in-the-field for safe and secure composition of highly distributed and autonomous CPS. The paper outlines the DDI concept and opportunities for application in four industrial use cases. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.01729
Corporate core values and social responsibility: What really matters to whom,M. A. Barchiesi;A. Fronzetti Colladon,"This study uses an innovative measure, the Semantic Brand Score, to assess the interest of stakeholders in different company core values. Among others, we focus on corporate social responsibility (CSR) core value statements, and on the attention they receive from five categories of stakeholders (customers, company communication teams, employees, associations and media). Combining big data methods and tools of Social Network Analysis and Text Mining, we analyzed about 58,000 Italian tweets and found that different stakeholders have different prevailing interests. CSR gets much less attention than expected. Core values related to customers and employees are in the foreground. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.01644
Rate-Splitting Multiple Access in Cache-Aided Cloud-Radio Access Networks,Robert-Jeron Reifert;Alaa Alameer Ahmad;Yijie Mao;Aydin Sezgin;Bruno Clerckx,"Rate-splitting multiple access (RSMA) has been recognized as a promising physical layer strategy for 6G. Motivated by ever increasing popularity of cache-enabled content delivery in wireless communications, this paper proposes an innovative multigroup multicast transmission scheme based on RSMA for cache-aided cloud-radio access networks (C-RAN). Our proposed scheme not only exploits the properties of content-centric communications and local caching at the base stations (BSs), but also incorporates RSMA to better manage interference in multigroup multicast transmission with statistical channel state information (CSI) known at the central processor (CP) and the BSs. At the RSMA-enabled cloud CP, the message of each multicast group is split into a private and a common part with the former private part being decoded by all users in the respective group and the latter common part being decoded by multiple users from other multicast groups. Common message decoding is done for the purpose of mitigating the interference. In this work, we jointly optimize the clustering of BSs and the precoding with the aim of maximizing the minimum rate among all multicast groups to guarantee fairness serving all groups. The problem is a mixed-integer non-linear stochastic program (MINLSP), which is solved by a practical algorithm we proposed including a heuristic clustering algorithm for assigning a set of BSs to serve each user followed by an efficient iterative algorithm that combines the sample average approximation (SAA) and weighted minimum mean square error (WMMSE) to solve the stochastic non-convex sub-problem of precoder design. Numerical results show the explicit max-min rate gain of our proposed transmission scheme compared to the state-of-the-art trivial interference processing methods. Therefore, we conclude that RSMA is a promising technique for cache-aided C-RAN. △ Less","1 June, 2021",https://arxiv.org/pdf/2106.00369
Innovation in Large-scale agile -- Benefits and Challenges of Hackathons when Hacking from Home,Rasmus Ulfsnes;Viktoria Stray;Nils Brede Moe;Darja Šmite,"Hackathons are events in which diverse teams work together to explore, and develop solutions, software or even ideas. Hackathons have been recognized not only as public events for hacking, but also as a corporate mechanism for innovation. Hackathons are a way for established companies to achieve increased employee wellbeing as well as being a curator for innovation and developing new products. Sudden transition to the work-from-home mode caused by the COVID-19 pandemic first put many corporate events requiring collocation, such as hackathons, temporarily on hold and then motivated companies to find ways to hold these events virtually. In this paper, we report our findings from investigating hackathons in the context of a large agile company by first exploring the general benefits and challenges of hackathons and then trying to understand how they were affected by the virtual setup. We conducted nine interviews, surveyed 23 employees and analyzed a hackathon demo. We found that hackathons provide both individual and organizational benefits of innovation, personal interests, and acquiring new skills and competences. Several challenges such as added stress due to stopping the regular work, employees fearing not having enough contribution to deliver and potential mismatch between individual and organizational goals were also found. With respect to the virtual setup, we found that virtual hackathons are not diminishing the innovation benefits, however, some negative effect surfaced on the social and networking side. △ Less","1 June, 2021",https://arxiv.org/pdf/2106.00309
Improving Formality Style Transfer with Context-Aware Rule Injection,Zonghai Yao;Hong Yu,"Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text. Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST). CARI injects multiple rules into an end-to-end BERT-based encoder and decoder model. It learns to select optimal rules based on context. The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset. Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models' performance on several tweet sentiment analysis tasks. △ Less","31 May, 2021",https://arxiv.org/pdf/2106.00210
Towards a trustful digital world: exploring self-sovereign identity ecosystems,Gabriella Laatikainen;Taija Kolehmainen;Mengcheng Li;Markus Hautala;Antti Kettunen;Pekka Abrahamsson,"In the current global situation-burdened by, among others, a vast number of people without formal identification, digital leap, the need for health passports and contact tracking applications-providing private and secure digital identity for individuals, organizations and other entities is crucial. The emerging self-sovereign identity (SSI) solutions rely on distributed ledger technologies and verifiable credentials and have the potential to enable trustful digital interactions. In this human-centric paradigm, trust among actors can be established in a decentralized manner while the identity holders are able to own and control their confidential data. In this paper, we build on observations gathered in a field study to identify the building blocks, antecedents and possible outcomes of SSI ecosystems. We also showcase opportunities for researchers and practitioners to investigate this phenomenon from a wide range of domains and theories, such as the digital innovation ecosystems, value co-creation, surveillance theory, or entrepreneurship theories. △ Less","13 September, 2021",https://arxiv.org/pdf/2105.15131
Strengthening e-Education in India using Machine Learning,Naheed Khan;Darshan Bhanushali;Shreya Patel;Radhika Kotecha,"e-Education has developed as one of the most encouraging territories. The Indian Government is investing all amounts of energy to improve education among the residents of the nation. School and graduate understudies are focused on, however the stage is being created for all the residents seeking to learn. Without a doubt, the objective is to build the quantity of literates with advanced education. To accomplish the equivalent, propels in Data and Correspondence innovation are being utilized in the education division, which has cleared route for e-Training in India as well. To help educators in concentrating more on more current viewpoints, their excess work can be disposed of utilizing Machine Learning (ML). Difference to programming, ML deals with information and answers to create rules. In the event that Machine Learning is tackled effectively, it can setup the training division and contribute essentially to the development of the country. Hence, the work presented in this paper fortifies e-Education in India utilizing Machine Learning. For the most part, three concerns are focused to be tended to: Personalized recommendation of course and Customized teaching methodology. The work proposes utilizing developmental methodology of hereditary calculations for improving conventional procedures. Implementation and experiments presented in the paper verify the viability of proposed calculations. △ Less","24 May, 2021",https://arxiv.org/pdf/2105.15125
Adoption of Precision Medicine; Limitations and Considerations,Nasim Sadat Mosavi;Manuel Filipe Santos,"Research is ongoing all over the world for identifying the barriers and finding effective solutions to accelerate the projection of Precision Medicine (PM) in the healthcare industry. Yet there has not been a valid and practical model to tackle the several challenges that have slowed down the widespread of this clinical practice. This study aimed to highlight the major limitations and considerations for implementing Precision Medicine. The two theories Diffusion of Innovation and Socio-Technical are employed to discuss the success indicators of PM adoption. Throughout the theoretical assessment, two key theoretical gaps are identified and related findings are discussed. △ Less","22 May, 2021",https://arxiv.org/pdf/2105.15115
WAP: Digital Dependability Identities,Daniel Schneider;Mario Trapp;Yiannis Papadopoulos;Eric Armengaud;Marc Zeller;Kai Hoefig,"Cyber-Physical Systems (CPS) provide enormous potential for innovation but a precondition for this is that the issue of dependability has been addressed. This paper presents the concept of a Digital Dependability Identity (DDI) of a component or system as foundation for assuring the dependability of CPS. A DDI is an analyzable and potentially executable model of information about the dependability of a component or system. We argue that DDIs must fulfill a number of properties including being universally useful across supply chains, enabling off-line certification of systems where possible, and providing capabilities for in-field certification of safety of CPS. In this paper, we focus on system safety as one integral part of dependability and as a practical demonstration of the concept, we present an initial implementation of DDIs in the form of Conditional Safety Certificates (also known as ConSerts). We explain ConSerts and their practical operationalization based on an illustrative example. △ Less","31 May, 2021",https://arxiv.org/pdf/2105.14984
Power and Performance Efficient SDN-Enabled Fog Architecture,Adnan Akhunzada;Sherali Zeadally;Saif ul Islam,"Software Defined Networks (SDNs) have dramatically simplified network management. However, enabling pure SDNs to respond in real-time while handling massive amounts of data still remains a challenging task. In contrast, fog computing has strong potential to serve large surges of data in real-time. SDN control plane enables innovation, and greatly simplifies network operations and management thereby providing a promising solution to implement energy and performance aware SDN-enabled fog computing. Besides, power efficiency and performance evaluation in SDN-enabled fog computing is an area that has not yet been fully explored by the research community. We present a novel SDN-enabled fog architecture to improve power efficacy and performance by leveraging cooperative and non-cooperative policy-based computing. Preliminary results from extensive simulation demonstrate an improvement in the power utilization as well as the overall performance (i.e., processing time, response time). Finally, we discuss several open research issues that need further investigation in the future. △ Less","30 May, 2021",https://arxiv.org/pdf/2105.14607
BAAI-VANJEE Roadside Dataset: Towards the Connected Automated Vehicle Highway technologies in Challenging Environments of China,Deng Yongqiang;Wang Dengjiang;Cao Gang;Ma Bing;Guan Xijia;Wang Yajun;Liu Jianchao;Fang Yanming;Li Juanjuan,"As the roadside perception plays an increasingly significant role in the Connected Automated Vehicle Highway(CAVH) technologies, there are immediate needs of challenging real-world roadside datasets for bench marking and training various computer vision tasks such as 2D/3D object detection and multi-sensor fusion. In this paper, we firstly introduce a challenging BAAI-VANJEE roadside dataset which consist of LiDAR data and RGB images collected by VANJEE smart base station placed on the roadside about 4.5m high. This dataset contains 2500 frames of LiDAR data, 5000 frames of RGB images, including 20% collected at the same time. It also contains 12 classes of objects, 74K 3D object annotations and 105K 2D object annotations. By providing a real complex urban intersections and highway scenes, we expect the BAAI-VANJEE roadside dataset will actively assist the academic and industrial circles to accelerate the innovation research and achievement transformation in the field of intelligent transportation in big data era. △ Less","29 May, 2021",https://arxiv.org/pdf/2105.14370
Covid-19 diagnosis from x-ray using neural networks,Dinesh J;Mohammed Rhithick A,"Corona virus or COVID-19 is a pandemic illness, which has influenced more than million of causalities worldwide and infected a few large number of individuals .Innovative instrument empowering quick screening of the COVID-19 contamination with high precision can be critically useful to the medical care experts. The primary clinical device presently being used for the analysis of COVID-19 is the Reverse record polymerase chain response as known as RT-PCR, which is costly, less-delicate and requires specific clinical work force. X-Ray imaging is an effectively available apparatus that can be a great option in the COVID-19 conclusion. This exploration was taken to examine the utility of computerized reasoning in the quick and exact recognition of COVID-19 from chest X-Ray pictures. The point of this paper is to propose a procedure for programmed recognition of COVID-19 from advanced chest X-Ray images applying pre-prepared profound learning calculations while boosting the discovery exactness. The point is to give over-focused on clinical experts a second pair of eyes through a learning picture characterization models. We distinguish an appropriate Convolutional Neural Network-CNN model through beginning similar investigation of a few mainstream CNN models. △ Less","29 May, 2021",https://arxiv.org/pdf/2105.14333
A Review of Network Evolution Towards a Smart Connected World,Olivia Haring;Sylvia Worlali Azumah;Nelly Elsayed,"With the rapid innovations in technology, wireless internet-connected devices are more ubiquitous than ever and can be found in virtually every aspect of both our personal and professional lives. In this paper, we propose a comprehensive literature review that focuses on various network components that create connectivity among different devices, specifically Wireless Sensor Networks (WSNs), Radio-Frequency Identification (RFID) tags, Internet of Things (IoT) devices, and how these devices helped usher in the 4th Industrial Revolution, or Industry 4.0. This paper focuses on the protocols, architecture, uses, security concerns, and solutions used in these network technologies, as well as their differences and similarities. △ Less","28 May, 2021",https://arxiv.org/pdf/2105.13964
Using Convolutional Neural Networks for Relative Pose Estimation of a Non-Cooperative Spacecraft with Thermal Infrared Imagery,Maxwell Hogan;Duarte Rondao;Nabil Aouf;Olivier Dubois-Matra,"Recent interest in on-orbit servicing and Active Debris Removal (ADR) missions have driven the need for technologies to enable non-cooperative rendezvous manoeuvres. Such manoeuvres put heavy burden on the perception capabilities of a chaser spacecraft. This paper demonstrates Convolutional Neural Networks (CNNs) capable of providing an initial coarse pose estimation of a target from a passive thermal infrared camera feed. Thermal cameras offer a promising alternative to visible cameras, which struggle in low light conditions and are susceptible to overexposure. Often, thermal information on the target is not available a priori; this paper therefore proposes using visible images to train networks. The robustness of the models is demonstrated on two different targets, first on synthetic data, and then in a laboratory environment for a realistic scenario that might be faced during an ADR mission. Given that there is much concern over the use of CNN in critical applications due to their black box nature, we use innovative techniques to explain what is important to our network and fault conditions. △ Less","28 May, 2021",https://arxiv.org/pdf/2105.13789
Efficient and Accurate Gradients for Neural SDEs,Patrick Kidger;James Foster;Xuechen Li;Terry Lyons,"Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory efficient training, high-capacity function approximation, and strong priors on model space. This makes them a natural choice for modelling many types of temporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires backpropagating through an SDE solve. This may be done by solving a backwards-in-time SDE whose solution is the desired parameter gradients. However, this has previously suffered from severe speed and accuracy issues, due to high computational cost and numerical truncation errors. Here, we overcome these issues through several technical innovations. First, we introduce the \textit{reversible Heun method}. This is a new SDE solver that is \textit{algebraically reversible}: eliminating numerical gradient errors, and the first such solver of which we are aware. Moreover it requires half as many function evaluations as comparable solvers, giving up to a 1.98\times speedup. Second, we introduce the \textit{Brownian Interval}: a new, fast, memory efficient, and exact way of sampling \textit{and reconstructing} Brownian motion. With this we obtain up to a 10.6\times speed improvement over previous techniques, which in contrast are both approximate and relatively slow. Third, when specifically training Neural SDEs as GANs (Kidger et al. 2021), we demonstrate how SDE-GANs may be trained through careful weight clipping and choice of activation function. This reduces computational cost (giving up to a 1.87\times speedup) and removes the numerical truncation errors associated with gradient penalty. Altogether, we outperform the state-of-the-art by substantial margins, with respect to training speed, and with respect to classification, prediction, and MMD test metrics. We have contributed implementations of all of our techniques to the torchsde library to help facilitate their adoption. △ Less","19 October, 2021",https://arxiv.org/pdf/2105.13493
Estimating Fund-Raising Performance for Start-up Projects from a Market Graph Perspective,Likang Wu;Zhi Li;Hongke Zhao;Qi Liu;Enhong Chen,"In the online innovation market, the fund-raising performance of the start-up project is a concerning issue for creators, investors and platforms. Unfortunately, existing studies always focus on modeling the fund-raising process after the publishment of a project but the predicting of a project attraction in the market before setting up is largely unexploited. Usually, this prediction is always with great challenges to making a comprehensive understanding of both the start-up project and market environment. To that end, in this paper, we present a focused study on this important problem from a market graph perspective. Specifically, we propose a Graph-based Market Environment (GME) model for predicting the fund-raising performance of the unpublished project by exploiting the market environment. In addition, we discriminatively model the project competitiveness and market preferences by designing two graph-based neural network architectures and incorporating them into a joint optimization stage. Furthermore, to explore the information propagation problem with dynamic environment in a large-scale market graph, we extend the GME model with parallelizing competitiveness quantification and hierarchical propagation algorithm. Finally, we conduct extensive experiments on real-world data. The experimental results clearly demonstrate the effectiveness of our proposed model. △ Less","26 May, 2021",https://arxiv.org/pdf/2105.12918
Neural Enhanced Belief Propagation for Cooperative Localization,Mingchao Liang;Florian Meyer,"Location-aware networks will introduce innovative services and applications for modern convenience, applied ocean sciences, and public safety. In this paper, we establish a hybrid method for model-based and data-driven inference. We consider a cooperative localization (CL) scenario where the mobile agents in a wireless network aim to localize themselves by performing pairwise observations with other agents and by exchanging location information. A traditional method for distributed CL in large agent networks is belief propagation (BP) which is completely model-based and is known to suffer from providing inconsistent (overconfident) estimates. The proposed approach addresses these limitations by complementing BP with learned information provided by a graph neural network (GNN). We demonstrate numerically that our method can improve estimation accuracy and avoid overconfident beliefs, while its computational complexity remains comparable to BP. Notably, more consistent beliefs are obtained by not explicitly addressing overconfidence in the loss function used for training of the GNN. △ Less","26 May, 2021",https://arxiv.org/pdf/2105.12903
A Review on Applications and Challenges in Internet of Things,Subhash Bhagavan Kommina;Kiran Kumar Pulamoluy;Kumar Umesh Pentakota;S Sravya,"World has been introduced with a new way of living by inventing this advanced technology Internet of Things (IOT). By using this technology all the real-world objects can be made to interconnect and communicate with each other which can be done by diagnosing, sensing and networking over the internet to fulfill some objectives. Due to advancements that have taken place in sensors, wireless communication systems and cloud computing has brought steps to invent this innovative technology. This paper deals with the elements, applications and the challenges faced by Internet of Things. This paper mainly concentrates on the current scenario of IoT △ Less","9 May, 2021",https://arxiv.org/pdf/2105.12835
It is rotating leaders who build the swarm: social network determinants of growth for healthcare virtual communities of practice,G. Antonacci;A. Fronzetti Colladon;A. Stefanini;P. Gloor,"Purpose: The purpose of this paper is to identify the factors influencing the growth of healthcare virtual communities of practice (VCoPs) through a seven-year longitudinal study conducted using metrics from social-network and semantic analysis. By studying online communication along the three dimensions of social interactions (connectivity, interactivity and language use), the authors aim to provide VCoP managers with valuable insights to improve the success of their communities. Design/methodology/approach: Communications over a period of seven years (April 2008 to April 2015) and between 14,000 members of 16 different healthcare VCoPs coexisting on the same web platform were analysed. Multilevel regression models were used to reveal the main determinants of community growth over time. Independent variables were derived from social network and semantic analysis measures. Findings: Results show that structural and content-based variables predict the growth of the community. Progressively, more people will join a community if its structure is more centralised, leaders are more dynamic (they rotate more) and the language used in the posts is less complex. Research limitations/implications: The available data set included one Web platform and a limited number of control variables. To consolidate the findings of the present study, the experiment should be replicated on other healthcare VCoPs. Originality/value: The study provides useful recommendations for setting up and nurturing the growth of professional communities, considering, at the same time, the interaction patterns among the community members, the dynamic evolution of these interactions and the use of language. New analytical tools are presented, together with the use of innovative interaction metrics, that can significantly influence community growth, such as rotating leadership. △ Less","26 May, 2021",https://arxiv.org/pdf/2105.12659
Blockchain-Based Approach to Foster Student Engagement on Campus,Ritu Gala;Eshita Shukla;Nidhee Kamble;Revathi Vijayaraghavan;Dhiren Patel,"On-campus activities like positions of responsibility in campus amenities and participation in research, benefit the students as well as the university, while also making students financially self-sufficient to a certain extent. However, this student participation is stymied by lack of awareness and motivation. Significant impetus to innovation and student participation can be provided by incentivization of these activities. In this paper, we propose a system to create a blockchain-based economy, to incentivize students with empirical benefits or monetary awards calculated using objective algorithms. The incentivization algorithms have been designed for three promising use cases: research work, positions of responsibility in universities, and crowdfunding. The demonstrated implementation of this system utilises VJTI Chain, an already established Proof of Authority blockchain in VJTI Mumbai, India. This creates a circular economy within the university which encourages students to earn more rewards by reinforcing positive feedback. △ Less","26 May, 2021",https://arxiv.org/pdf/2105.12504
"The ""given data"" paradigm undermines both cultures",Tyler McCormick,"Breiman organizes ""Statistical modeling: The two cultures"" around a simple visual. Data, to the far right, are compelled into a ""black box"" with an arrow and then catapulted left by a second arrow, having been transformed into an output. Breiman then posits two interpretations of this visual as encapsulating a distinction between two cultures in statistics. The divide, he argues is about what happens in the ""black box."" In this comment, I argue for a broader perspective on statistics and, in doing so, elevate questions from ""before"" and ""after"" the box as fruitful areas for statistical innovation and practice. △ Less","26 May, 2021",https://arxiv.org/pdf/2105.12478
Finance 4.0: Design principles for a value-sensitive cryptoecnomic system to address sustainability,Mark C. Ballandies;Marcus M. Dapp;Benjamin A. Degenhart;Dirk Helbing,"Cryptoeconomic systems derive their power but can not be controlled by the underlying software systems and the rules they enshrine. This adds a level of complexity to the software design process. At the same time, such systems, when designed with human values in mind, offer new approaches to tackle sustainability challenges, that are plagued by commons dilemmas and negative external effects caused by a one-dimensional monetary system. This paper proposes a design science research methodology with value-sensitive design methods to derive design principles for a value-sensitive socio-ecological cryptoeconomic system that incentivizes actions toward sustainability via multi-dimensional token incentives. These design principles are implemented in a software that is validated in user studies that demonstrate its relevance, usability and impact. Our findings provide new insights on designing cryptoeconomic systems. Moreover, the identified design principles for a value-sensitive socio-ecological financial system indicate opportunities for new research directions and business innovations. △ Less","25 May, 2021",https://arxiv.org/pdf/2105.11955
Estimating Redundancy in Clinical Text,Thomas Searle;Zina Ibrahim;James Teo;Richard JB Dobson,"The current mode of use of Electronic Health Record (EHR) elicits text redundancy. Clinicians often populate new documents by duplicating existing notes, then updating accordingly. Data duplication can lead to a propagation of errors, inconsistencies and misreporting of care. Therefore, quantifying information redundancy can play an essential role in evaluating innovations that operate on clinical narratives. This work is a quantitative examination of information redundancy in EHR notes. We present and evaluate two strategies to measure redundancy: an information-theoretic approach and a lexicosyntactic and semantic model. We evaluate the measures by training large Transformer-based language models using clinical text from a large openly available US-based ICU dataset and a large multi-site UK based Trust. By comparing the information-theoretic content of the trained models with open-domain language models, the language models trained using clinical text have shown ~1.5x to ~3x less efficient than open-domain corpora. Manual evaluation shows a high correlation with lexicosyntactic and semantic redundancy, with averages ~43 to ~65%. △ Less","26 October, 2021",https://arxiv.org/pdf/2105.11832
Deep High-Resolution Representation Learning for Cross-Resolution Person Re-identification,Guoqing Zhang;Yu Ge;Zhicheng Dong;Hao Wang;Yuhui Zheng;Shengyong Chen,"Person re-identification (re-ID) tackles the problem of matching person images with the same identity from different cameras. In practical applications, due to the differences in camera performance and distance between cameras and persons of interest, captured person images usually have various resolutions. We name this problem as Cross-Resolution Person Re-identification which brings a great challenge for matching correctly. In this paper, we propose a Deep High-Resolution Pseudo-Siamese Framework (PS-HRNet) to solve the above problem. Specifically, in order to restore the resolution of low-resolution images and make reasonable use of different channel information of feature maps, we introduce and innovate VDSR module with channel attention (CA) mechanism, named as VDSR-CA. Then we reform the HRNet by designing a novel representation head to extract discriminating features, named as HRNet-ReID. In addition, a pseudo-siamese framework is constructed to reduce the difference of feature distributions between low-resolution images and high-resolution images. The experimental results on five cross-resolution person datasets verify the effectiveness of our proposed approach. Compared with the state-of-the-art methods, our proposed PS-HRNet improves 3.4\%, 6.2\%, 2.5\%,1.1\% and 4.2\% at Rank-1 on MLR-Market-1501, MLR-CUHK03, MLR-VIPeR, MLR-DukeMTMC-reID, and CAVIAR datasets, respectively. Our code is available at \url{https://github.com/zhguoqing}. △ Less","25 May, 2021",https://arxiv.org/pdf/2105.11722
The power of reciprocal knowledge sharing relationships for startup success,T. J. Allen;P. Gloor;A. Fronzetti Colladon;S. L. Woerner;O. Raz,"Purpose: The purpose of this paper is to examine the innovative capabilities of biotech start-ups in relation to geographic proximity and knowledge sharing interaction in the R&D network of a major high-tech cluster. Design-methodology-approach: This study compares longitudinal informal communication networks of researchers at biotech start-ups with company patent applications in subsequent years. For a year, senior R&D staff members from over 70 biotech firms located in the Boston biotech cluster were polled and communication information about interaction with peers, universities and big pharmaceutical companies was collected, as well as their geolocation tags. Findings: Location influences the amount of communication between firms, but not their innovation success. Rather, what matters is communication intensity and recollection by others. In particular, there is evidence that rotating leadership - changing between a more active and passive communication style - is a predictor of innovative performance. Practical implications: Expensive real-estate investments can be replaced by maintaining social ties. A more dynamic communication style and more diverse social ties are beneficial to innovation. Originality-value: Compared to earlier work that has shown a connection between location, network and firm performance, this paper offers a more differentiated view; including a novel measure of communication style, using a unique data set and providing new insights for firms who want to shape their communication patterns to improve innovation, independently of their location. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.11538
Editorial introduction: The power of words and networks,A. Fronzetti Colladon;P. Gloor;D. F. Iezzi,"According to Freud ""words were originally magic and to this day words have retained much of their ancient magical power"". By words, behaviors are transformed and problems are solved. The way we use words reveals our intentions, goals and values. Novel tools for text analysis help understand the magical power of words. This power is multiplied, if it is combined with the study of social networks, i.e. with the analysis of relationships among social units. This special issue of the International Journal of Information Management, entitled ""Combining Social Network Analysis and Text Mining: from Theory to Practice"", includes heterogeneous and innovative research at the nexus of text mining and social network analysis. It aims to enrich work at the intersection of these fields, which still lags behind in theoretical, empirical, and methodological foundations. The nine articles accepted for inclusion in this special issue all present methods and tools that have business applications. They are summarized in this editorial introduction. △ Less","24 May, 2021",https://arxiv.org/pdf/2105.11263
"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad Virtual Y Conocimiento Experto para generación de entornos de aprendizaje Hombre-Máquina",Santiago González;Alvaro García;Ana Núñez,"This work presents the results of project CONECT4, which addresses the research and development of new non-intrusive communication methods for the generation of a human-machine learning ecosystem oriented to predictive maintenance in the automotive industry. Through the use of innovative technologies such as Augmented Reality, Virtual Reality, Digital Twin and expert knowledge, CONECT4 implements methodologies that allow improving the efficiency of training techniques and knowledge management in industrial companies. The research has been supported by the development of content and systems with a low level of technological maturity that address solutions for the industrial sector applied in training and assistance to the operator. The results have been analyzed in companies in the automotive sector, however, they are exportable to any other type of industrial sector. -- -- En esta publicación se presentan los resultados del proyecto CONECT4, que aborda la investigación y desarrollo de nuevos métodos de comunicación no intrusivos para la generación de un ecosistema de aprendizaje hombre-máquina orientado al mantenimiento predictivo en la industria de automoción. A través del uso de tecnologías innovadoras como la Realidad Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto, CONECT4 implementa metodologías que permiten mejorar la eficiencia de las técnicas de formación y gestión de conocimiento en las empresas industriales. La investigación se ha apoyado en el desarrollo de contenidos y sistemas con un nivel de madurez tecnológico bajo que abordan soluciones para el sector industrial aplicadas en la formación y asistencia al operario. Los resultados han sido analizados en empresas del sector de automoción, no obstante, son exportables a cualquier otro tipo de sector industrial. △ Less","24 May, 2021",https://arxiv.org/pdf/2105.11216
Pre-trained Language Model based Ranking in Baidu Search,Lixin Zou;Shengqiang Zhang;Hengyi Cai;Dehong Ma;Suqi Cheng;Daiting Shi;Zhifan Zhu;Weiyue Su;Shuaiqiang Wang;Zhicong Cheng;Dawei Yin,"As the heart of a search engine, the ranking system plays a crucial role in satisfying users' information demands. More recently, neural rankers fine-tuned from pre-trained language models (PLMs) establish state-of-the-art ranking effectiveness. However, it is nontrivial to directly apply these PLM-based rankers to the large-scale web search system due to the following challenging issues:(1) the prohibitively expensive computations of massive neural PLMs, especially for long texts in the web-document, prohibit their deployments in an online ranking system that demands extremely low latency;(2) the discrepancy between existing ranking-agnostic pre-training objectives and the ad-hoc retrieval scenarios that demand comprehensive relevance modeling is another main barrier for improving the online ranking system;(3) a real-world search engine typically involves a committee of ranking components, and thus the compatibility of the individually fine-tuned ranking model is critical for a cooperative ranking system. In this work, we contribute a series of successfully applied techniques in tackling these exposed issues when deploying the state-of-the-art Chinese pre-trained language model, i.e., ERNIE, in the online search engine system. We first articulate a novel practice to cost-efficiently summarize the web document and contextualize the resultant summary content with the query using a cheap yet powerful Pyramid-ERNIE architecture. Then we endow an innovative paradigm to finely exploit the large-scale noisy and biased post-click behavioral data for relevance-oriented pre-training. We also propose a human-anchored fine-tuning strategy tailored for the online ranking system, aiming to stabilize the ranking signals across various online components. Extensive offline and online experimental results show that the proposed techniques significantly boost the search engine's performance. △ Less","25 June, 2021",https://arxiv.org/pdf/2105.11108
The digital footprint of innovators: Using email to detect the most creative people in your organization,P. A. Gloor;A. Fronzetti Colladon;F. Grippa,"We propose a novel method for finding the most innovative people in an organization, using email to analyze structure and dynamics of the organization's online communication. To illustrate our approach, we analyzed the email archive of 2000 members of the R&D department of a US multinational company. We use metrics of social network analysis extended with meta-data of interaction dynamics to calculate features for individual employees: their network positions, messages sent and received, pings to others and response times. We find a distinction between innovation group leaders and subject matter experts focused on publishing papers and patents. Innovation administrators have a higher number of direct contacts, are more committed in conversations and receive more messages than they send. We also found significant differences between innovators oriented towards internal awards and innovators more concerned with external recognition of their work. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.10247
RLIRank: Learning to Rank with Reinforcement Learning for Dynamic Search,Jianghong Zhou;Eugene Agichtein,"To support complex search tasks, where the initial information requirements are complex or may change during the search, a search engine must adapt the information delivery as the user's information requirements evolve. To support this dynamic ranking paradigm effectively, search result ranking must incorporate both the user feedback received, and the information displayed so far. To address this problem, we introduce a novel reinforcement learning-based approach, RLIrank. We first build an adapted reinforcement learning framework to integrate the key components of the dynamic search. Then, we implement a new Learning to Rank (LTR) model for each iteration of the dynamic search, using a recurrent Long Short Term Memory neural network (LSTM), which estimates the gain for each next result, learning from each previously ranked document. To incorporate the user's feedback, we develop a word-embedding variation of the classic Rocchio Algorithm, to help guide the ranking towards the high-value documents. Those innovations enable RLIrank to outperform the previously reported methods from the TREC Dynamic Domain Tracks 2017 and exceed all the methods in 2016 TREC Dynamic Domain after multiple search iterations, advancing the state of the art for dynamic search. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.10124
Dynamic enterprise architecture capabilities and organizational benefits: an empirical mediation study,Rogier van de Wetering,"In recent years the literature has put a greater emphasis on theory building in the context of Enterprise Architecture, EA, research. Specifically, scholars tend to focus on EA-based capabilities that organize and deploy organization-specific resources to align strategic objectives with the particular use of technology. Despite the growth in EA studies, substantial gaps remain in the literature. The most noteworthy gaps are that the conceptualization of EA-based capabilities still lacks a firm base in theory and that there is no conclusive evidence on how EA-based capabilities drive business transformation and deliver benefits to the firm. Therefore, this study focuses on EA-based capabilities, using the dynamic capabilities view as a theoretical foundation, develops and tests a new research model that explains how dynamic enterprise architecture capabilities lead to organizational benefits. Hypotheses associated with the research model are tested using a dataset that contains responses from 299 CIOs, IT managers, and lead architects. Results show that dynamic enterprise architecture capabilities positively influence the firm's process innovation and business/IT alignment. These mediating forces are both positively associated with organizational benefits. This study advances our understanding of how to efficaciously de-lineate dynamic enterprise architecture capabilities in delivering benefits to the organization. △ Less","18 May, 2021",https://arxiv.org/pdf/2105.10036
A practical introduction to the Rational Speech Act modeling framework,Gregory Scontras;Michael Henry Tessler;Michael Franke,"Recent advances in computational cognitive science (i.e., simulation-based probabilistic programs) have paved the way for significant progress in formal, implementable models of pragmatics. Rather than describing a pragmatic reasoning process in prose, these models formalize and implement one, deriving both qualitative and quantitative predictions of human behavior -- predictions that consistently prove correct, demonstrating the viability and value of the framework. The current paper provides a practical introduction to and critical assessment of the Bayesian Rational Speech Act modeling framework, unpacking theoretical foundations, exploring technological innovations, and drawing connections to issues beyond current applications. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09867
Congestion-Aware Routing in Dynamic IoT Networks: A Reinforcement Learning Approach,Hossam Farag;Cedomir Stefanovic,"The innovative services empowered by the Internet of Things (IoT) require a seamless and reliable wireless infrastructure that enables communications within heterogeneous and dynamic low-power and lossy networks (LLNs). The Routing Protocol for LLNs (RPL) was designed to meet the communication requirements of a wide range of IoT application domains. However, a load balancing problem exists in RPL under heavy traffic-load scenarios, degrading the network performance in terms of delay and packet delivery. In this paper, we tackle the problem of load-balancing in RPL networks using a reinforcement-learning framework. The proposed method adopts Q-learning at each node to learn an optimal parent selection policy based on the dynamic network conditions. Each node maintains the routing information of its neighbours as Q-values that represent a composite routing cost as a function of the congestion level, the link-quality and the hop-distance. The Q-values are updated continuously exploiting the existing RPL signalling mechanism. The performance of the proposed approach is evaluated through extensive simulations and compared with the existing work to demonstrate its effectiveness. The results show that the proposed method substantially improves network performance in terms of packet delivery and average delay with a marginal increase in the signalling frequency. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09678
Understanding the Perceived Relevance of Capability Measures: A Survey of Agile Software Development Practitioners,Sai Datta Vishnubhotla;Emilia Mendes;Lars Lundberg,"Context: In the light of the swift and iterative nature of Agile Software Development (ASD) practices, establishing deeper insights into capability measurement within the context of team formation is crucial, as the capability of individuals and teams can affect team performance and productivity. Although a former Systematic Literature Review (SLR) synthesized the state of the art in relation to capability measurement in ASD with a focus on selecting individuals to agile teams, and capabilities related to team performance and success, determining to what degree the SLR's results apply to practice can provide progressive insights to both research and practice. Objective: Our study investigates how agile practitioners perceive the relevance of individual and team level measures for characterizing the capability of an agile team and its members. Furthermore, to scrutinize variations in practitioners' perceptions, our study further analyzes perceptions across stratified demographic groups. Method: We undertook a Web-based survey using a questionnaire built based on the capability measures identified from a previously conducted SLR. Results: Our survey responses (60) indicate that 127 individual and 28 team capability measures were considered as relevant by the majority of practitioners. We also identified seven individual and one team capability measure that have not been previously characterized by our SLR. The surveyed practitioners suggested that an agile team member's responsibility and questioning skills significantly represent the member's capability. Conclusion: Results from our survey align with our SLR's findings. Measures associated with social aspects were observed to be dominant compared to technical and innovative aspects. Our results can support agile practitioners in their team composition decisions. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09523
Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder,Chuhong Lahlou;Ancil Crayton;Caroline Trier;Evan Willett,"In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to predict risk in value-based care for incorporation into CMS Innovation Center payment and service delivery models. Recently, modern language models have played key roles in a number of health related tasks. This paper presents, to the best of our knowledge, the first application of these models to patient readmission prediction. To facilitate this, we create a dataset of 1.2 million medical history samples derived from the Limited Dataset (LDS) issued by CMS. Moreover, we propose a comprehensive modeling solution centered on a deep learning framework for this data. To demonstrate the framework, we train an attention-based Transformer to learn Medicare semantics in support of performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91 recall on readmission classification. We also introduce a novel data pre-processing pipeline and discuss pertinent deployment considerations surrounding model explainability and bias. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09428
Procedural animations in interactive art experiences -- A state of the art review,C. Tollola,"The state of the art review broadly oversees the use of novel research utilized in the creation of virtual environments applied in interactive art experiences, with a specific focus on the application of procedural animation in spatially augmented reality (SAR) exhibitions. These art exhibitions frequently combine sensory displays that appeal, replace, and augment the visual, auditory and touch or haptic senses. We analyze and break down art-technology related innovations in the last three years, and thoroughly identify the most recent and vibrant applications of interactive art experiences in the review of numerous installation applications, studies, and events. Display mediums such as virtual reality, augmented reality, mixed reality, and robotics are overviewed in the context of art experiences such as visual art museums, park or historic site tours, live concerts, and theatre. We explore research and extrapolate how recent innovations can lead to different applications that will be seen in the future. △ Less","16 May, 2021",https://arxiv.org/pdf/2105.09153
LNGate: Powering IoT with Next Generation Lightning Micro-payments using Threshold Cryptography,Ahmet Kurt;Suat Mercan;Omer Shlomovits;Enes Erdin;Kemal Akkaya,"Bitcoin has emerged as a revolutionary payment system with its decentralized ledger concept however it has significant problems such as high transaction fees and long confirmation times. Lightning Network (LN), which was introduced much later, solves most of these problems with an innovative concept called off-chain payments. With this advancement, Bitcoin has become an attractive venue to perform micro-payments which can also be adopted in many IoT applications (e.g. toll payments). Nevertheless, it is not feasible to host LN and Bitcoin on IoT devices due to the storage, memory, and processing requirements. Therefore, in this paper, we propose an efficient and secure protocol that enables an IoT device to use LN through an untrusted gateway node. The gateway hosts LN and Bitcoin nodes and can open & close LN channels, send LN payments on behalf of the IoT device. This delegation approach is powered by a (2,2)-threshold scheme that requires the IoT device and the LN gateway to jointly perform all LN operations which in turn secures both parties' funds. Specifically, we propose to thresholdize LN's Bitcoin public and private keys as well as its commitment points. With these and several other protocol level changes, IoT device is protected against revoked state broadcast, collusion, and ransom attacks. We implemented the proposed protocol by changing LN's source code and thoroughly evaluated its performance using a Raspberry Pi. Our evaluation results show that computational and communication delays associated with the protocol are negligible. To the best of our knowledge, this is the first work that implemented threshold cryptography in LN. △ Less","27 May, 2021",https://arxiv.org/pdf/2105.08902
AI and Ethics -- Operationalising Responsible AI,Liming Zhu;Xiwei Xu;Qinghua Lu;Guido Governatori;Jon Whittle,"In the last few years, AI continues demonstrating its positive impact on society while sometimes with ethically questionable consequences. Building and maintaining public trust in AI has been identified as the key to successful and sustainable innovation. This chapter discusses the challenges related to operationalizing ethical AI principles and presents an integrated view that covers high-level ethical AI principles, the general notion of trust/trustworthiness, and product/process support in the context of responsible AI, which helps improve both trust and trustworthiness of AI for a wider set of stakeholders. △ Less","18 May, 2021",https://arxiv.org/pdf/2105.08867
"3D Displays: Their Evolution, Inherent Challenges & Future Perspectives",Xingyu Pan;Xuanhui Xu;Soumyabrata Dev;Abraham G Campbell,"The popularity of 3D displays has risen drastically over the past few decades but these displays are still merely a novelty compared to their true potential. The development has mostly focused on Head Mounted Displays (HMD) development for Virtual Reality and in general ignored non-HMD 3D displays. This is due to the inherent difficulty in the creation of these displays and their impracticability in general use due to cost, performance, and lack of meaningful use cases. In fairness to the hardware manufacturers who have made striking innovations in this field, there has been a dereliction of duty of software developers and researchers in terms of developing software to best utilize these displays. This paper will seek to identify what areas of future software development could mitigate this dereliction. To achieve this goal, the paper will first examine the current state of the art and perform a comparative analysis on different types of 3D displays, from this analysis a clear researcher gap exists in terms of software development for Light field displays which are the current state of the art of non-HMD-based 3D displays. The paper will then outline six distinct areas where the context-awareness concept will allow for non-HMD-based 3D displays in particular light field displays that can not only compete but surpass their HMD-based brethren for many specific use cases. △ Less","18 May, 2021",https://arxiv.org/pdf/2105.08390
EchoCP: An Echocardiography Dataset in Contrast Transthoracic Echocardiography for Patent Foramen Ovale Diagnosis,Tianchen Wang;Zhihe Li;Meiping Huang;Jian Zhuang;Shanshan Bi;Jiawei Zhang;Yiyu Shi;Hongwen Fei;Xiaowei Xu,"Patent foramen ovale (PFO) is a potential separation between the septum, primum and septum secundum located in the anterosuperior portion of the atrial septum. PFO is one of the main factors causing cryptogenic stroke which is the fifth leading cause of death in the United States. For PFO diagnosis, contrast transthoracic echocardiography (cTTE) is preferred as being a more robust method compared with others. However, the current PFO diagnosis through cTTE is extremely slow as it is proceeded manually by sonographers on echocardiography videos. Currently there is no publicly available dataset for this important topic in the community. In this paper, we present EchoCP, as the first echocardiography dataset in cTTE targeting PFO diagnosis. EchoCP consists of 30 patients with both rest and Valsalva maneuver videos which covers various PFO grades. We further establish an automated baseline method for PFO diagnosis based on the state-of-the-art cardiac chamber segmentation technique, which achieves 0.89 average mean Dice score, but only 0.60/0.67 mean accuracies for PFO diagnosis, leaving large room for improvement. We hope that the challenging EchoCP dataset can stimulate further research and lead to innovative and generic solutions that would have an impact in multiple domains. Our dataset is released. △ Less","15 September, 2021",https://arxiv.org/pdf/2105.08267
The Topological Mu-Calculus: completeness and decidability,Alexandru Baltag;Nick Bezhanishvili;David Fernández-Duque,"We study the topological μ-calculus, based on both Cantor derivative and closure modalities, proving completeness, decidability and FMP over general topological spaces, as well as over T_0 and T_D spaces. We also investigate relational μ-calculus, providing general completeness results for all natural fragments of μ-calculus over many different classes of relational frames. Unlike most other such proofs for μ-calculus, ours is model-theoretic, making an innovative use of a known Modal Logic method (--the 'final' submodel of the canonical model), that has the twin advantages of great generality and essential simplicity. △ Less","17 May, 2021",https://arxiv.org/pdf/2105.08231
Pay Attention to MLPs,Hanxiao Liu;Zihang Dai;David R. So;Quoc V. Le,"Transformers have become one of the most important architectural innovations in deep learning and have enabled many breakthroughs over the past few years. Here we propose a simple network architecture, gMLP, based on MLPs with gating, and show that it can perform as well as Transformers in key language and vision applications. Our comparisons show that self-attention is not critical for Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model achieves parity with Transformers on pretraining perplexity and is better on some downstream NLP tasks. On finetuning tasks where gMLP performs worse, making the gMLP model substantially larger can close the gap with Transformers. In general, our experiments show that gMLP can scale as well as Transformers over increased data and compute. △ Less","1 June, 2021",https://arxiv.org/pdf/2105.08050
Using Transformers to Provide Teachers with Personalized Feedback on their Classroom Discourse: The TalkMoves Application,Abhijit Suresh;Jennifer Jacobs;Vivian Lai;Chenhao Tan;Wayne Ward;James H. Martin;Tamara Sumner,"TalkMoves is an innovative application designed to support K-12 mathematics teachers to reflect on, and continuously improve their instructional practices. This application combines state-of-the-art natural language processing capabilities with automated speech recognition to automatically analyze classroom recordings and provide teachers with personalized feedback on their use of specific types of discourse aimed at broadening and deepening classroom conversations about mathematics. These specific discourse strategies are referred to as ""talk moves"" within the mathematics education community and prior research has documented the ways in which systematic use of these discourse strategies can positively impact student engagement and learning. In this article, we describe the TalkMoves application's cloud-based infrastructure for managing and processing classroom recordings, and its interface for providing teachers with feedback on their use of talk moves during individual teaching episodes. We present the series of model architectures we developed, and the studies we conducted, to develop our best-performing, transformer-based model (F1 = 79.3%). We also discuss several technical challenges that need to be addressed when working with real-world speech and language data from noisy K-12 classrooms. △ Less","29 April, 2021",https://arxiv.org/pdf/2105.07949
Confidence Assertions in Cyber-Security for an Information-Sharing Environment,Paul B. Kantor;Dennis E. Egan;Jonathan Bullinger;Katie McKeon;James Wojtowicz,"Information sharing is vital in resisting cyberattacks, and the volume and severity of these attacks is increasing very rapidly. Therefore responders must triage incoming warnings in deciding how to act. This study asked a very specific question: ""how can the addition of confidence information to alerts and warnings improve overall resistance to cyberattacks."" We sought, in particular, to identify current practices, and if possible, to identify some ""best practices."" The research involved literature review and interviews with subject matter experts at every level from system administrators to persons who develop broad principles of policy. An innovative Modified Online Delphi Panel technique was used to elicit judgments and recommendations from experts who were able to speak with each other and vote anonymously to rank proposed practices. △ Less","17 May, 2021",https://arxiv.org/pdf/2105.07937
"The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges",Beatrice Alex;Clare Llewellyn;Pawel Michal Orzechowski;Maria Boutchkova,"In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students' learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.07847
Advances in Multi-Variate Analysis Methods for New Physics Searches at the Large Hadron Collider,Anna Stakia;Tommaso Dorigo;Giovanni Banelli;Daniela Bortoletto;Alessandro Casa;Pablo de Castro;Christophe Delaere;Julien Donini;Livio Finos;Michele Gallinaro;Andrea Giammanco;Alexander Held;Fabricio Jiménez Morales;Grzegorz Kotkowski;Seng Pei Liew;Fabio Maltoni;Giovanna Menardi;Ioanna Papavergou;Alessia Saggio;Bruno Scarpa;Giles C. Strong;Cecilia Tosciri;João Varela;Pietro Vischia;Andreas Weiler,"Between the years 2015 and 2019, members of the Horizon 2020-funded Innovative Training Network named ""AMVA4NewPhysics"" studied the customization and application of advanced multivariate analysis methods and statistical learning tools to high-energy physics problems, as well as developed entirely new ones. Many of those methods were successfully used to improve the sensitivity of data analyses performed by the ATLAS and CMS experiments at the CERN Large Hadron Collider; several others, still in the testing phase, promise to further improve the precision of measurements of fundamental physics parameters and the reach of searches for new phenomena. In this paper, the most relevant new tools, among those studied and developed, are presented along with the evaluation of their performances. △ Less","22 November, 2021",https://arxiv.org/pdf/2105.07530
Que Bian: An Electronic Medical Record Management System on Blockchain,Hao Wang,"Medical Record Management System is an important information management system in healthcare centers and hospitals. Information kept in such systems need to be clean, correct and tamper-proof. In this paper, we take advantage of blockchains' tamper-proof and decentralization properties to develop a robust and secure electronic medical record management system. In particular we choose HyperLedger Fabric as our underlying technical architecture. HyperLedger Fabric yields higher throughput and lower latency compared with other blockchains, which is a perfect candidate for enterprise software development. Our system is a novel innovation that can serve as an ideal replacement for conventional Medical Record Management System. △ Less","7 August, 2021",https://arxiv.org/pdf/2105.07327
Circumferential Crack Modeling of Thin Cylindrical Shells in Modal Deformation,Ali Alijani;Olga Barrera;Stephane P. A. Bordas,"An innovative technique, called conversion, is introduced to model circumferential cracks in thin cylindrical shells. The semi-analytical finite element method is applied to investigate the modal deformation of the cylinder. An element including the crack is divided into three sub-elements with four nodes in which the stiffness matrix is enriched. The crack characteristics are included in the finite element method relations through conversion matrices and a rotational spring corresponding to the crack. Conversion matrices obtained by applying continuity conditions at the crack tip are used to transform displacements of the middle nodes to those of the main nodes. Moreover, another technique, called spring set, is represented based on a set of springs to model the crack as a separated element. Components of the stiffness matrix related to the separated element are incorporated while the geometric boundary conditions at the crack tip are satisfied. The effects of the circumferential mode number, the crack depth and the length of the cylinder on the critical buckling load are investigated. Experimental tests, ABAQUS modeling and results from literature are used to verify and validate the results and derived relations. In addition, the crack effect on the natural frequency is examined using the vibration analysis based on the conversion technique. △ Less","15 May, 2021",https://arxiv.org/pdf/2105.07290
Window-Level is a Strong Denoising Surrogate,Ayaan Haque;Adam Wang;Abdullah-Al-Zubaer Imran,"CT image quality is heavily reliant on radiation dose, which causes a trade-off between radiation dose and image quality that affects the subsequent image-based diagnostic performance. However, high radiation can be harmful to both patients and operators. Several (deep learning-based) approaches have been attempted to denoise low dose images. However, those approaches require access to large training sets, specifically the full dose CT images for reference, which can often be difficult to obtain. Self-supervised learning is an emerging alternative for lowering the reference data requirement facilitating unsupervised learning. Currently available self-supervised CT denoising works are either dependent on foreign domain or pretexts are not very task-relevant. To tackle the aforementioned challenges, we propose a novel self-supervised learning approach, namely Self-Supervised Window-Leveling for Image DeNoising (SSWL-IDN), leveraging an innovative, task-relevant, simple, yet effective surrogate -- prediction of the window-leveled equivalent. SSWL-IDN leverages residual learning and a hybrid loss combining perceptual loss and MSE, all incorporated in a VAE framework. Our extensive (in- and cross-domain) experimentation demonstrates the effectiveness of SSWL-IDN in aggressive denoising of CT (abdomen and chest) images acquired at 5\% dose level only. △ Less","15 May, 2021",https://arxiv.org/pdf/2105.07153
Methods Included: Standardizing Computational Reuse and Portability with the Common Workflow Language,Michael R. Crusoe;Sanne Abeln;Alexandru Iosup;Peter Amstutz;John Chilton;Nebojša Tijanić;Hervé Ménager;Stian Soiland-Reyes;Bogdan Gavrilovic;Carole Goble,"Computational Workflows are widely used in data analysis, enabling innovation and decision-making. In many domains (bioinformatics, image analysis, & radio astronomy) the analysis components are numerous and written in multiple different computer languages by third parties. However, many competing workflow systems exist, severely limiting portability of such workflows, thereby hindering the transfer of workflows between different systems, between different projects and different settings, leading to vendor lock-ins and limiting their generic re-usability. Here we present the Common Workflow Language (CWL) project which produces free and open standards for describing command-line tool based workflows. The CWL standards provide a common but reduced set of abstractions that are both used in practice and implemented in many popular workflow systems. The CWL language is declarative, which allows expressing computational workflows constructed from diverse software tools, executed each through their command-line interface. Being explicit about the runtime environment and any use of software containers enables portability and reuse. Workflows written according to the CWL standards are a reusable description of that analysis that are runnable on a diverse set of computing environments. These descriptions contain enough information for advanced optimization without additional input from workflow authors. The CWL standards support polylingual workflows, enabling portability and reuse of such workflows, easing for example scholarly publication, fulfilling regulatory requirements, collaboration in/between academic research and industry, while reducing implementation costs. CWL has been taken up by a wide variety of domains, and industries and support has been implemented in many major workflow systems. △ Less","4 August, 2021",https://arxiv.org/pdf/2105.07028
Exploiting Aliasing for Manga Restoration,Minshan Xie;Menghan Xia;Tien-Tsin Wong,"As a popular entertainment art form, manga enriches the line drawings details with bitonal screentones. However, manga resources over the Internet usually show screentone artifacts because of inappropriate scanning/rescaling resolution. In this paper, we propose an innovative two-stage method to restore quality bitonal manga from degraded ones. Our key observation is that the aliasing induced by downsampling bitonal screentones can be utilized as informative clues to infer the original resolution and screentones. First, we predict the target resolution from the degraded manga via the Scale Estimation Network (SE-Net) with spatial voting scheme. Then, at the target resolution, we restore the region-wise bitonal screentones via the Manga Restoration Network (MR-Net) discriminatively, depending on the degradation degree. Specifically, the original screentones are directly restored in pattern-identifiable regions, and visually plausible screentones are synthesized in pattern-agnostic regions. Quantitative evaluation on synthetic data and visual assessment on real-world cases illustrate the effectiveness of our method. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.06830
Efficient Parallel Self-Adjusting Computation,Daniel Anderson;Guy E. Blelloch;Anubhav Baweja;Umut A. Acar,"Self-adjusting computation is an approach for automatically producing dynamic algorithms from static ones. The approach works by tracking control and data dependencies, and propagating changes through the dependencies when making an update. Extensively studied in the sequential setting, some results on parallel self-adjusting computation exist, but are either only applicable to limited classes of computations, such as map-reduce, or are ad-hoc systems with no theoretical analysis of their performance. In this paper, we present the first system for parallel self-adjusting computation that applies to a wide class of nested parallel algorithms and provides theoretical bounds on the work and span of the resulting dynamic algorithms. As with bounds in the sequential setting, our bounds relate a ""distance"" measure between computations on different inputs to the cost of propagating an update. However, here we also consider parallelism in the propagation cost. The main innovation in the paper is in using Series-Parallel trees (SP trees) to track sequential and parallel control dependencies to allow propagation of changes to be applied safely in parallel. We show both theoretically and through experiments that our system allows algorithms to produce updated results over large datasets significantly faster than from-scratch execution. We demonstrate our system with several example applications, including algorithms for dynamic sequences and dynamic trees. In all cases studied, we show that parallel self-adjusting computation can provide a significant benefit in both work savings and parallel time. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.06712
Innovation Compression for Communication-efficient Distributed Optimization with Linear Convergence,Jiaqi Zhang;Keyou You;Lihua Xie,"Information compression is essential to reduce communication cost in distributed optimization over peer-to-peer networks. This paper proposes a communication-efficient linearly convergent distributed (COLD) algorithm to solve strongly convex optimization problems. By compressing innovation vectors, which are the differences between decision vectors and their estimates, COLD is able to achieve linear convergence for a class of δ-contracted compressors. We explicitly quantify how the compression affects the convergence rate and show that COLD matches the same rate of its uncompressed version. To accommodate a wider class of compressors that includes the binary quantizer, we further design a novel dynamical scaling mechanism and obtain the linearly convergent Dyna-COLD. Importantly, our results strictly improve existing results for the quantized consensus problem. Numerical experiments demonstrate the advantages of both algorithms under different compressors. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.06697
COVID-Net CXR-2: An Enhanced Deep Convolutional Neural Network Design for Detection of COVID-19 Cases from Chest X-ray Images,Maya Pavlova;Naomi Terhljan;Audrey G. Chung;Andy Zhao;Siddharth Surana;Hossein Aboutalebi;Hayden Gunraj;Ali Sabri;Amer Alaref;Alexander Wong,"As the COVID-19 pandemic continues to devastate globally, the use of chest X-ray (CXR) imaging as a complimentary screening strategy to RT-PCR testing continues to grow given its routine clinical use for respiratory complaint. As part of the COVID-Net open source initiative, we introduce COVID-Net CXR-2, an enhanced deep convolutional neural network design for COVID-19 detection from CXR images built using a greater quantity and diversity of patients than the original COVID-Net. To facilitate this, we also introduce a new benchmark dataset composed of 19,203 CXR images from a multinational cohort of 16,656 patients from at least 51 countries, making it the largest, most diverse COVID-19 CXR dataset in open access form. The COVID-Net CXR-2 network achieves sensitivity and positive predictive value of 95.5%/97.0%, respectively, and was audited in a transparent and responsible manner. Explainability-driven performance validation was used during auditing to gain deeper insights in its decision-making behaviour and to ensure clinically relevant factors are leveraged for improving trust in its usage. Radiologist validation was also conducted, where select cases were reviewed and reported on by two board-certified radiologists with over 10 and 19 years of experience, respectively, and showed that the critical factors leveraged by COVID-Net CXR-2 are consistent with radiologist interpretations. While not a production-ready solution, we hope the open-source, open-access release of COVID-Net CXR-2 and the respective CXR benchmark dataset will encourage researchers, clinical scientists, and citizen scientists to accelerate advancements and innovations in the fight against the pandemic. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.06640
An Interpretable Graph-based Mapping of Trustworthy Machine Learning Research,Noemi Derzsy;Subhabrata Majumdar;Rajat Malik,"There is an increasing interest in ensuring machine learning (ML) frameworks behave in a socially responsible manner and are deemed trustworthy. Although considerable progress has been made in the field of Trustworthy ML (TwML) in the recent past, much of the current characterization of this progress is qualitative. Consequently, decisions about how to address issues of trustworthiness and future research goals are often left to the interested researcher. In this paper, we present the first quantitative approach to characterize the comprehension of TwML research. We build a co-occurrence network of words using a web-scraped corpus of more than 7,000 peer-reviewed recent ML papers -- consisting of papers both related and unrelated to TwML. We use community detection to obtain semantic clusters of words in this network that can infer relative positions of TwML topics. We propose an innovative fingerprinting algorithm to obtain probabilistic similarity scores for individual words, then combine them to give a paper-level relevance score. The outcomes of our analysis inform a number of interesting insights on advancing the field of TwML research. △ Less","13 May, 2021",https://arxiv.org/pdf/2105.06591
A Pragmatic Approach to Regulating Artificial Intelligence: A Technology Regulator's Perspective,Joshua Ellul;Stephen McCarthy;Trevor Sammut;Juanita Brockdorff;Matthew Scerri;Gordon J. Pace,"Artificial Intelligence (AI) and the regulation thereof is a topic that is increasingly being discussed within various fora. Various proposals have been made in literature for defining regulatory bodies and/or related regulation. In this paper, we present a pragmatic approach for providing a technology assurance regulatory framework. To the best knowledge of the authors this work presents the first national AI technology assurance legal and regulatory framework that has been implemented by a national authority empowered through law to do so. In aim of both providing assurances where required and not stifling innovation yet supporting it, herein it is proposed that such regulation should not be mandated for all AI-based systems and that rather it should primarily provide a voluntary framework and only be mandated in sectors and activities where required and as deemed necessary by other authorities for regulated and critical areas. △ Less","15 April, 2021",https://arxiv.org/pdf/2105.06267
Playing Codenames with Language Graphs and Word Embeddings,Divya Koyyalagunta;Anna Sun;Rachel Lea Draelos;Cynthia Rudin,"Although board games and video games have been studied for decades in artificial intelligence research, challenging word games remain relatively unexplored. Word games are not as constrained as games like chess or poker. Instead, word game strategy is defined by the players' understanding of the way words relate to each other. The word game Codenames provides a unique opportunity to investigate common sense understanding of relationships between words, an important open challenge. We propose an algorithm that can generate Codenames clues from the language graph BabelNet or from any of several embedding methods - word2vec, GloVe, fastText or BERT. We introduce a new scoring function that measures the quality of clues, and we propose a weighting term called DETECT that incorporates dictionary-based word representations and document frequency to improve clue selection. We develop BabelNet-Word Selection Framework (BabelNet-WSF) to improve BabelNet clue quality and overcome the computational barriers that previously prevented leveraging language graphs for Codenames. Extensive experiments with human evaluators demonstrate that our proposed innovations yield state-of-the-art performance, with up to 102.8% improvement in precision@2 in some cases. Overall, this work advances the formal study of word games and approaches for common sense language understanding. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.05885
Building a Question and Answer System for News Domain,Sandipan Basu;Aravind Gaddala;Pooja Chetan;Garima Tiwari;Narayana Darapaneni;Sadwik Parvathaneni;Anwesh Reddy Paduri,"This project attempts to build a Question- Answering system in the News Domain, where Passages will be News articles, and anyone can ask a Question against it. We have built a span-based model using an Attention mechanism, where the model predicts the answer to a question as to the position of the start and end tokens in a paragraph. For training our model, we have used the Stanford Question and Answer (SQuAD 2.0) dataset[1]. To do well on SQuAD 2.0, systems must not only answer questions when possible but also determine when no answer is supported by the paragraph and abstain from answering. Our model architecture comprises three layers- Embedding Layer, RNN Layer, and the Attention Layer. For the Embedding layer, we used GloVe and the Universal Sentence Encoder. For the RNN Layer, we built variations of the RNN Layer including bi-LSTM and Stacked LSTM and we built an Attention Layer using a Context to Question Attention and also improvised on the innovative Bidirectional Attention Layer. Our best performing model which uses GloVe Embedding combined with Bi-LSTM and Context to Question Attention achieved an F1 Score and EM of 33.095 and 33.094 respectively. We also leveraged transfer learning and built a Transformer based model using BERT. The BERT-based model achieved an F1 Score and EM of 57.513 and 49.769 respectively. We concluded that the BERT model is superior in all aspects of answering various types of questions. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.05744
Self-citation Analysis using Sentence Embeddings,Athanasios Lagopoulos;Grigorios Tsoumakas,"The purpose of citation indexes and metrics is intended to be a measure for scientific innovation and quality for researchers, journals, and institutions. However, those metrics are often prone to abuse and manipulation by excessive and unethical self-citations induced by authors, reviewers, editors, or journals. Identifying whether there are or not legitimate reasons for self-citations is normally determined during the review process, where the participating parts may have intrinsic incentives, rendering the legitimacy of self-citations, after publication, questionable. In this paper, we conduct a large-scale analysis of journal self-citations while taking into consideration the similarity between a publication and its references. Specifically, we look into PubMed Central articles published since 1990 and compute similarities of article-reference pairs using sentence embeddings. We examine journal self-citations with an aim to distinguish between justifiable and unethical self-citations. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.05527
The Role of Intent-Based Networking in ICT Supply Chains,Mounir Bensalem;Jasenka Dizdarević;Francisco Carpio;Admela Jukan,"The evolution towards Industry 4.0 is driving the need for innovative solutions in the area of network management, considering the complex, dynamic and heterogeneous nature of ICT supply chains. To this end, Intent-Based networking (IBN) which is already proven to evolve how network management is driven today, can be implemented as a solution to facilitate the management of large ICT supply chains. In this paper, we first present a comparison of the main architectural components of typical IBN systems and, then, we study the key engineering requirements when integrating IBN with ICT supply chain network systems while considering AI methods. We also propose a general architecture design that enables intent translation of ICT supply chain specifications into lower level policies, to finally show an example of how the access control is performed in a modeled ICT supply chain system. △ Less","11 May, 2021",https://arxiv.org/pdf/2105.05179
DeepLight: Robust & Unobtrusive Real-time Screen-Camera Communication for Real-World Displays,Vu Tran;Gihan Jayatilaka;Ashwin Ashok;Archan Misra,"The paper introduces a novel, holistic approach for robust Screen-Camera Communication (SCC), where video content on a screen is visually encoded in a human-imperceptible fashion and decoded by a camera capturing images of such screen content. We first show that state-of-the-art SCC techniques have two key limitations for in-the-wild deployment: (a) the decoding accuracy drops rapidly under even modest screen extraction errors from the captured images, and (b) they generate perceptible flickers on common refresh rate screens even with minimal modulation of pixel intensity. To overcome these challenges, we introduce DeepLight, a system that incorporates machine learning (ML) models in the decoding pipeline to achieve humanly-imperceptible, moderately high SCC rates under diverse real-world conditions. Deep-Light's key innovation is the design of a Deep Neural Network (DNN) based decoder that collectively decodes all the bits spatially encoded in a display frame, without attempting to precisely isolate the pixels associated with each encoded bit. In addition, DeepLight supports imperceptible encoding by selectively modulating the intensity of only the Blue channel, and provides reasonably accurate screen extraction (IoU values >= 83%) by using state-of-the-art object detection DNN pipelines. We show that a fully functional DeepLight system is able to robustly achieve high decoding accuracy (frame error rate < 0.2) and moderately-high data goodput (>=0.95Kbps) using a human-held smartphone camera, even over larger screen-camera distances (approx =2m). △ Less","11 May, 2021",https://arxiv.org/pdf/2105.05092
Pruning of Deep Spiking Neural Networks through Gradient Rewiring,Yanqi Chen;Zhaofei Yu;Wei Fang;Tiejun Huang;Yonghong Tian,"Spiking Neural Networks (SNNs) have been attached great importance due to their biological plausibility and high energy-efficiency on neuromorphic chips. As these chips are usually resource-constrained, the compression of SNNs is thus crucial along the road of practical use of SNNs. Most existing methods directly apply pruning approaches in artificial neural networks (ANNs) to SNNs, which ignore the difference between ANNs and SNNs, thus limiting the performance of the pruned SNNs. Besides, these methods are only suitable for shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination in the neural system, we propose gradient rewiring (Grad R), a joint learning algorithm of connectivity and weight for SNNs, that enables us to seamlessly optimize network structure without retraining. Our key innovation is to redefine the gradient to a new synaptic parameter, allowing better exploration of network structures by taking full advantage of the competition between pruning and regrowth of connections. The experimental results show that the proposed method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset so far. Moreover, it reaches a \sim3.5% accuracy loss under unprecedented 0.73% connectivity, which reveals remarkable structure refining capability in SNNs. Our work suggests that there exists extremely high redundancy in deep SNNs. Our codes are available at https://github.com/Yanqi-Chen/Gradient-Rewiring. △ Less","22 June, 2021",https://arxiv.org/pdf/2105.04916
Recurrent Neural Networks to automate Quality assessment of Software Requirements,María Guadalupe Gramajo;Luciana Ballejos;Mariel Ale,"Many problems related to the quality of requirements arise during elicitation and specification activities since they are written in natural language. The flexibility and inherent nature of language make requirements prone to inconsistencies, redundancies, and ambiguities, and consequently, this influences negatively the later phases of the software life cycle. To address this problem, this paper proposes an innovative approach that combines natural language processing techniques and recurrent neural networks to automatically assess the quality of software requirements. Initially, the analysis of singular, complete, correct, and appropriate quality properties defined in the IEEE 29148: 2018 standard is addressed. The proposed neural models are trained with a data set composed of 1000 software requirements. The proposal provides an average accuracy of 75%. These promising results were a motivation to explore its application in the evaluation of other quality properties △ Less","13 May, 2021",https://arxiv.org/pdf/2105.04757
Validation of image systems simulation technology using a Cornell Box,Zheng Lyu;Krithin Kripakaran;Max Furth;Eric Tang;Brian Wandell;Joyce Farrell,"We describe and experimentally validate an end-to-end simulation of a digital camera. The simulation models the spectral radiance of 3D-scenes, formation of the spectral irradiance by multi-element optics, and conversion of the irradiance to digital values by the image sensor. We quantify the accuracy of the simulation by comparing real and simulated images of a precisely constructed, three-dimensional high dynamic range test scene. Validated end-to-end software simulation of a digital camera can accelerate innovation by reducing many of the time-consuming and expensive steps in designing, building and evaluating image systems. △ Less","10 May, 2021",https://arxiv.org/pdf/2105.04106
Photonic Network Coding and Partial Protection for Optical Core Networks: Two for a Tango,Dao Thanh Hai,"The digital transformation is creating basically a digital version of our physical world and the currency in that digital space is data. Massive amount of data has been generated ranging from wearable devices monitoring our physical health every single millisecond to autonomous vehicles generating roughly 5Tb hourly to even astronomical activities producing an order of Exabytes on daily basis and then ultra-broadband Internet comes into play, moving such data to the cloud. Internet traffic therefore has been experiencing explosive growth and in this context, optical transport networks forming the backbone of the Internet are pushed for transformation in system capacity. While the intuitive solution of deploying multiple fibers can address the pressing demand for increased capacity, doing so does not bring improvement in economic of scales in terms of cost, power consumption and spectral efficiency. This necessitates for a different approach so that the fiber capacity could be utilized in a more efficient manner. In this paper, we focus on innovative techniques, that is, photonic network coding and partial protection, to reduce the effective traffic load in order to achieve greater capacity efficiency for optical transport networks. Specifically, the application of network coding is examined by upgrading the functionalities of intermediate nodes with all-optical processing (i.e., encoding and decoding) capabilities. Besides, partial protection relying on the premise of providing just enough bandwidth in case of failure events is investigated for saving the redundant protection capacity. That it takes two to tango, combining photonic network coding and partial protection therefore bring to light new opportunities and challenges. In mining such new avenue, we present insights on how to derive compounding gains to maximize spectral efficiency via a case study. △ Less","18 November, 2021",https://arxiv.org/pdf/2105.03503
SERVAS! Secure Enclaves via RISC-V Authenticryption Shield,Stefan Steinegger;David Schrammel;Samuel Weiser;Pascal Nasahl;Stefan Mangard,"Isolation is a long-standing challenge of software security. Traditional privilege rings and virtual memory are more and more augmented with concepts such as capabilities, protection keys, and powerful enclaves. At the same time, we are evidencing an increased need for physical protection, shifting towards full memory encryption schemes. This results in a complex interplay of various security mechanisms, increasing the burden for system architects and security analysts. In this work, we tackle the isolation challenge with a new isolation primitive called authenticryption shield that unifies both traditional and advanced isolation policies while offering the potential for future extensibility. At the core, we build upon an authenticated memory encryption scheme that gives cryptographic isolation guarantees and, thus, streamlines the security reasoning. We showcase the versatility of our approach by designing and prototyping SERVAS -- an innovative enclave architecture for RISC-V. Unlike current enclave systems, SERVAS facilitates efficient and secure enclave memory sharing. While the memory encryption constitutes the main overhead, entering or exiting a SERVAS enclave requires only 3.5x of a simple syscall, instead of 71x for Intel SGX. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03395
Finding the unicorn: Predicting early stage startup success through a hybrid intelligence method,Dominik Dellermann;Nikolaus Lipusch;Philipp Ebel;Karl Michael Popp;Jan Marco Leimeister,"Artificial intelligence is an emerging topic and will soon be able to perform decisions better than humans. In more complex and creative contexts such as innovation, however, the question remains whether machines are superior to humans. Machines fail in two kinds of situations: processing and interpreting soft information (information that cannot be quantified) and making predictions in unknowable risk situations of extreme uncertainty. In such situations, the machine does not have representative information for a certain outcome. Thereby, humans are still the gold standard for assessing soft signals and make use of intuition. To predict the success of startups, we, thus, combine the complementary capabilities of humans and machines in a Hybrid Intelligence method. To reach our aim, we follow a design science research approach to develop a Hybrid Intelligence method that combines the strength of both machine and collective intelligence to demonstrate its utility for predictions under extreme uncertainty. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03360
An interdisciplinary conceptual study of Artificial Intelligence (AI) for helping benefit-risk assessment practices: Towards a comprehensive qualification matrix of AI programs and devices (pre-print 2020),Gauthier Chassang;Mogens Thomsen;Pierre Rumeau;Florence Sèdes;Alejandra Delfin,"This paper proposes a comprehensive analysis of existing concepts coming from different disciplines tackling the notion of intelligence, namely psychology and engineering, and from disciplines aiming to regulate AI innovations, namely AI ethics and law. The aim is to identify shared notions or discrepancies to consider for qualifying AI systems. Relevant concepts are integrated into a matrix intended to help defining more precisely when and how computing tools (programs or devices) may be qualified as AI while highlighting critical features to serve a specific technical, ethical and legal assessment of challenges in AI development. Some adaptations of existing notions of AI characteristics are proposed. The matrix is a risk-based conceptual model designed to allow an empirical, flexible and scalable qualification of AI technologies in the perspective of benefit-risk assessment practices, technological monitoring and regulatory compliance: it offers a structured reflection tool for stakeholders in AI development that are engaged in responsible research and innovation.Pre-print version (achieved on May 2020) △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03192
Impact of digital economic activity on regional economic growth: A Case study from northern Minas Gerais between 2009 To 2018,Cesar R Salas-Guerra,"At present, the economic measurement of the national statistical offices has not defined or captured the benefits of the digital economy activity due to the low quality or inexistence of methodologies. Currently, there is a relevant debate on the capacity of the digital economy activity to generate productivity, economic growth, and well-being through innovation and knowledge. For this reason, this research identified and studied specialized knowledge, human settlement, and digital economic activity as the factors that influence regional economic growth. As a result, the impact generated by a new business operating models based on information technology was measured. Furthermore, this research used an empirical measurement model that made it possible to identify certain phenomena such as regional poles of regional economic development (PRDE) that surround economically flourishing regions. In addition, it showed that municipalities with high degrees of economic growth were impacted by digital economic activity and specialized knowledge. This finding is consistent with economic growth theories that point to technological evolution as the main factor of modern economic growth. Consequently, this study contributed beneficial results to the local government to develop strategies framed in solving industrial cooperation of economically flourishing regions with their neighbors, facing the problem of agglomeration of resources and capital reflected in human settlement promote an imbalance in economic growth and social development. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02849
A probabilistic model for missing traffic volume reconstruction based on data fusion,Xintao Yan;Yan Zhao;Henry X. Liu,"Traffic volume information is critical for intelligent transportation systems. It serves as a key input to transportation planning, roadway design, and traffic signal control. However, the traffic volume data collected by fixed-location sensors, such as loop detectors, often suffer from the missing data problem and low coverage problem. The missing data problem could be caused by hardware malfunction. The low coverage problem is due to the limited coverage of fixed-location sensors in the transportation network, which restrains our understanding of the traffic at the network level. To tackle these problems, we propose a probabilistic model for traffic volume reconstruction by fusing fixed-location sensor data and probe vehicle data. We apply the probabilistic principal component analysis (PPCA) to capture the correlations in traffic volume data. An innovative contribution of this work is that we also integrate probe vehicle data into the framework, which allows the model to solve both of the above-mentioned two problems. Using a real-world traffic volume dataset, we show that the proposed method outperforms state-of-the-art methods for the extensively studied missing data problem. Moreover, for the low coverage problem, which cannot be handled by most existing methods, the proposed model can also achieve high accuracy. The experiments also show that even when the missing ratio reaches 80%, the proposed method can still give an accurate estimate of the unknown traffic volumes with only a 10% probe vehicle penetration rate. The results validate the effectiveness and robustness of the proposed model and demonstrate its potential for practical applications. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02777
Brain Multigraph Prediction using Topology-Aware Adversarial Graph Neural Network,Alaa Bessadok;Mohamed Ali Mahjoub;Islem Rekik,"Brain graphs (i.e, connectomes) constructed from medical scans such as magnetic resonance imaging (MRI) have become increasingly important tools to characterize the abnormal changes in the human brain. Due to the high acquisition cost and processing time of multimodal MRI, existing deep learning frameworks based on Generative Adversarial Network (GAN) focused on predicting the missing multimodal medical images from a few existing modalities. While brain graphs help better understand how a particular disorder can change the connectional facets of the brain, synthesizing a target brain multigraph (i.e, multiple brain graphs) from a single source brain graph is strikingly lacking. Additionally, existing graph generation works mainly learn one model for each target domain which limits their scalability in jointly predicting multiple target domains. Besides, while they consider the global topological scale of a graph (i.e., graph connectivity structure), they overlook the local topology at the node scale (e.g., how central a node is in the graph). To address these limitations, we introduce topology-aware graph GAN architecture (topoGAN), which jointly predicts multiple brain graphs from a single brain graph while preserving the topological structure of each target graph. Its three key innovations are: (i) designing a novel graph adversarial auto-encoder for predicting multiple brain graphs from a single one, (ii) clustering the encoded source graphs in order to handle the mode collapse issue of GAN and proposing a cluster-specific decoder, (iii) introducing a topological loss to force the prediction of topologically sound target brain graphs. The experimental results using five target domains demonstrated the outperformance of our method in brain multigraph prediction from a single graph in comparison with baseline approaches. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02565
Federated Face Recognition,Fan Bai;Jiaxiang Wu;Pengcheng Shen;Shaoxin Li;Shuigeng Zhou,"Face recognition has been extensively studied in computer vision and artificial intelligence communities in recent years. An important issue of face recognition is data privacy, which receives more and more public concerns. As a common privacy-preserving technique, Federated Learning is proposed to train a model cooperatively without sharing data between parties. However, as far as we know, it has not been successfully applied in face recognition. This paper proposes a framework named FedFace to innovate federated learning for face recognition. Specifically, FedFace relies on two major innovative algorithms, Partially Federated Momentum (PFM) and Federated Validation (FV). PFM locally applies an estimated equivalent global momentum to approximating the centralized momentum-SGD efficiently. FV repeatedly searches for better federated aggregating weightings via testing the aggregated models on some private validation datasets, which can improve the model's generalization ability. The ablation study and extensive experiments validate the effectiveness of the FedFace method and show that it is comparable to or even better than the centralized baseline in performance. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02501
Development of a Fast and Robust Gaze Tracking System for Game Applications,Manh Duong Phung;Cong Hoang Quach;Quang Vinh Tran,"In this study, a novel eye tracking system using a visual camera is developed to extract human's gaze, and it can be used in modern game machines to bring new and innovative interactive experience to players. Central to the components of the system, is a robust iris-center and eye-corner detection algorithm basing on it the gaze is continuously and adaptively extracted. Evaluation tests were applied to nine people to evaluate the accuracy of the system and the results were 2.50 degrees (view angle) in horizontal direction and 3.07 degrees in vertical direction. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02460
Spatio-Temporal Matching for Siamese Visual Tracking,Jinpu Zhang;Yuehuan Wang,"Similarity matching is a core operation in Siamese trackers. Most Siamese trackers carry out similarity learning via cross correlation that originates from the image matching field. However, unlike 2-D image matching, the matching network in object tracking requires 4-D information (height, width, channel and time). Cross correlation neglects the information from channel and time dimensions, and thus produces ambiguous matching. This paper proposes a spatio-temporal matching process to thoroughly explore the capability of 4-D matching in space (height, width and channel) and time. In spatial matching, we introduce a space-variant channel-guided correlation (SVC-Corr) to recalibrate channel-wise feature responses for each spatial location, which can guide the generation of the target-aware matching features. In temporal matching, we investigate the time-domain context relations of the target and the background and develop an aberrance repressed module (ARM). By restricting the abrupt alteration in the interframe response maps, our ARM can clearly suppress aberrances and thus enables more robust and accurate object tracking. Furthermore, a novel anchor-free tracking framework is presented to accommodate these innovations. Experiments on challenging benchmarks including OTB100, VOT2018, VOT2020, GOT-10k, and LaSOT demonstrate the state-of-the-art performance of the proposed method. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.02408
"A collaborative path to scientific discovery: Distribution of labor, productivity and innovation in collaborative science",Floriana Gargiulo;Maria Castaldo;Tommaso Venturini;Paolo Frasca,"In this work we dig into the process of scientific discovery by looking at a yet unexploited source of information: Polymath projects. Polymath projects are an original attempt to collectively solve mathematical problems in an online collaborative environment. To investigate the Polymath experiment, we analyze all the posts related to the projects that arrived to a peer reviewed publication with a particular attention to the organization of labor and the innovations originating from the author contributions. We observe that a significant presence of sporadic contributor boosts the productivity of the most active users and that productivity, in terms of number of posts, grows super-linearly with the number of contributors. When it comes to innovation in large scale collaborations, there is no exact rule determining, a priori, who the main innovators will be. Sometimes, serendipitous interactions by sporadic contributors can have a large impact on the discovery process and a single post by an occasional participant can steer the work into a new direction. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.02303
Managing Blockchain Systems and Applications: A Process Model for Blockchain Configurations,Olga Labazova;Erol Kazan;Tobias Dehling;Tuure Tuunanen;Ali Sunyaev,"Blockchain is a radical innovation with a unique value proposition that shifts trust from institutions to algorithms. Still, the potential of blockchains remains elusive due to knowledge gaps between computer science research and socio-economic research. Building on information technology governance literature and the theory of coevolution, this study develops a process model for blockchain configurations that captures blockchain capability dimensions and application areas. We demonstrate the applicability of the proposed blockchain configuration process model on four blockchain projects. The proposed blockchain configuration process model assists with the selection and configuration of blockchain systems based on a set of known requirements for a blockchain project. Our findings contribute to research by bridging knowledge gaps between computer science and socio-economic research on blockchain. Specifically, we explore existing blockchain concepts and integrate them in a process model for blockchain configurations. △ Less","16 April, 2021",https://arxiv.org/pdf/2105.02118
LEADOR: A Method for End-to-End Participatory Design of Autonomous Social Robots,Katie Winkle;Emmanuel Senft;Séverin Lemaignan,"Participatory Design (PD) in Human-Robot Interaction (HRI) typically remains limited to the early phases of development, with subsequent robot behaviours then being hardcoded by engineers or utilised in Wizard-of-Oz (WoZ) systems that rarely achieve autonomy. We present LEADOR (Led-by-Experts Automation and Design Of Robots) an end-to-end PD methodology for domain expert co-design, automation and evaluation of social robots. LEADOR starts with typical PD to co-design the interaction specifications and state and action space of the robot. It then replaces traditional offline programming or WoZ by an in-situ, online teaching phase where the domain expert can live-program or teach the robot how to behave while being embedded in the interaction context. We believe that this live teaching can be best achieved by adding a learning component to a WoZ setup, to capture experts' implicit knowledge, as they intuitively respond to the dynamics of the situation. The robot progressively learns an appropriate, expert-approved policy, ultimately leading to full autonomy, even in sensitive and/or ill-defined environments. However, LEADOR is agnostic to the exact technical approach used to facilitate this learning process. The extensive inclusion of the domain expert(s) in robot design represents established responsible innovation practice, lending credibility to the system both during the teaching phase and when operating autonomously. The combination of this expert inclusion with the focus on in-situ development also means LEADOR supports a mutual shaping approach to social robotics. We draw on two previously published, foundational works from which this (generalisable) methodology has been derived in order to demonstrate the feasibility and worth of this approach, provide concrete examples in its application and identify limitations and opportunities when applying this framework in new environments. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.01910
The EMPATHIC Project: Mid-term Achievements,M. I. Torres;J. M. Olaso;C. Montenegro;R. Santana;A. Vázquez;R. Justo;J. A. Lozano;S. Schlögl;G. Chollet;N. Dugan;M. Irvine;N. Glackin;C. Pickard;A. Esposito;G. Cordasco;A. Troncone;D. Petrovska-Delacretaz;A. Mtibaa;M. A. Hmani;M. S. Korsnes;L. J. Martinussen;S. Escalera;C. Palmero Cantariño;O. Deroo;O. Gordeeva,"The goal of active aging is to promote changes in the elderly community so as to maintain an active, independent and socially-engaged lifestyle. Technological advancements currently provide the necessary tools to foster and monitor such processes. This paper reports on mid-term achievements of the European H2020 EMPATHIC project, which aims to research, innovate, explore and validate new interaction paradigms and platforms for future generations of personalized virtual coaches to assist the elderly and their carers to reach the active aging goal, in the vicinity of their home. The project focuses on evidence-based, user-validated research and integration of intelligent technology, and context sensing methods through automatic voice, eye and facial analysis, integrated with visual and spoken dialogue system capabilities. In this paper, we describe the current status of the system, with a special emphasis on its components and their integration, the creation of a Wizard of Oz platform, and findings gained from user interaction studies conducted throughout the first 18 months of the project. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.01878
TransHash: Transformer-based Hamming Hashing for Efficient Image Retrieval,Yongbiao Chen;Sheng Zhang;Fangxin Liu;Zhigang Chang;Mang Ye;Zhengwei Qi,"Deep hamming hashing has gained growing popularity in approximate nearest neighbour search for large-scale image retrieval. Until now, the deep hashing for the image retrieval community has been dominated by convolutional neural network architectures, e.g. \texttt{Resnet}\cite{he2016deep}. In this paper, inspired by the recent advancements of vision transformers, we present \textbf{Transhash}, a pure transformer-based framework for deep hashing learning. Concretely, our framework is composed of two major modules: (1) Based on \textit{Vision Transformer} (ViT), we design a siamese vision transformer backbone for image feature extraction. To learn fine-grained features, we innovate a dual-stream feature learning on top of the transformer to learn discriminative global and local features. (2) Besides, we adopt a Bayesian learning scheme with a dynamically constructed similarity matrix to learn compact binary hash codes. The entire framework is jointly trained in an end-to-end manner.~To the best of our knowledge, this is the first work to tackle deep hashing learning problems without convolutional neural networks (\textit{CNNs}). We perform comprehensive experiments on three widely-studied datasets: \textbf{CIFAR-10}, \textbf{NUSWIDE} and \textbf{IMAGENET}. The experiments have evidenced our superiority against the existing state-of-the-art deep hashing methods. Specifically, we achieve 8.2\%, 2.6\%, 12.7\% performance gains in terms of average \textit{mAP} for different hash bit lengths on three public datasets, respectively. △ Less","4 May, 2021",https://arxiv.org/pdf/2105.01823
Drifting Features: Detection and evaluation in the context of automatic RRLs identification in VVV,J. B. Cabral;M. Lares;S. Gurovich;D. Minniti;P. M. Granitto,"As most of the modern astronomical sky surveys produce data faster than humans can analyze it, Machine Learning (ML) has become a central tool in Astronomy. Modern ML methods can be characterized as highly resistant to some experimental errors. However, small changes on the data over long distances or long periods of time, which cannot be easily detected by statistical methods, can be harmful to these methods. We develop a new strategy to cope with this problem, also using ML methods in an innovative way, to identify these potentially harmful features. We introduce and discuss the notion of Drifting Features, related with small changes in the properties as measured in the data features. We use the identification of RRLs in VVV based on an earlier work and introduce a method for detecting Drifting Features. Our method forces a classifier to learn the tile of origin of diverse sources (mostly stellar 'point sources'), and select the features more relevant to the task of finding candidates to Drifting Features. We show that this method can efficiently identify a reduced set of features that contains useful information about the tile of origin of the sources. For our particular example of detecting RRLs in VVV, we find that Drifting Features are mostly related to color indices. On the other hand, we show that, even if we have a clear set of Drifting Features in our problem, they are mostly insensitive to the identification of RRLs. Drifting Features can be efficiently identified using ML methods. However, in our example, removing Drifting Features does not improve the identification of RRLs. △ Less","22 May, 2021",https://arxiv.org/pdf/2105.01714
Event Camera Simulator Design for Modeling Attention-based Inference Architectures,Md Jubaer Hossain Pantho;Joel Mandebi Mbongue;Pankaj Bhowmik;Christophe Bobda,"In recent years, there has been a growing interest in realizing methodologies to integrate more and more computation at the level of the image sensor. The rising trend has seen an increased research interest in developing novel event cameras that can facilitate CNN computation directly in the sensor. However, event-based cameras are not generally available in the market, limiting performance exploration on high-level models and algorithms. This paper presents an event camera simulator that can be a potent tool for hardware design prototyping, parameter optimization, attention-based innovative algorithm development, and benchmarking. The proposed simulator implements a distributed computation model to identify relevant regions in an image frame. Our simulator's relevance computation model is realized as a collection of modules and performs computations in parallel. The distributed computation model is configurable, making it highly useful for design space exploration. The Rendering engine of the simulator samples frame-regions only when there is a new event. The simulator closely emulates an image processing pipeline similar to that of physical cameras. Our experimental results show that the simulator can effectively emulate event vision with low overheads. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.01203
Weighted Least Squares Twin Support Vector Machine with Fuzzy Rough Set Theory for Imbalanced Data Classification,Maysam Behmanesh;Peyman Adibi;Hossein Karshenas,"Support vector machines (SVMs) are powerful supervised learning tools developed to solve classification problems. However, SVMs are likely to perform poorly in the classification of imbalanced data. The rough set theory presents a mathematical tool for inference in nondeterministic cases that provides methods for removing irrelevant information from data. In this work, we propose an approach that efficiently used fuzzy rough set theory in weighted least squares twin support vector machine called FRLSTSVM for classification of imbalanced data. The first innovation is introducing a new fuzzy rough set-based under-sampling strategy to make the classifier robust in terms of the imbalanced data. For constructing the two proximal hyperplanes in FRLSTSVM, data points from the minority class remain unchanged while a subset of data points in the majority class are selected using a new method. In this model, we embed the weight biases in the LSTSVM formulations to overcome the bias phenomenon in the original twin SVM for the classification of imbalanced data. In order to determine these weights in this formulation, we introduce a new strategy that uses fuzzy rough set theory as the second innovation. Experimental results on the famous imbalanced datasets, compared to the related traditional SVM-based methods, demonstrate the superiority of the proposed FRLSTSVM model in the imbalanced data classification. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.01198
Neural Monocular 3D Human Motion Capture with Physical Awareness,Soshi Shimada;Vladislav Golyanik;Weipeng Xu;Patrick Pérez;Christian Theobalt,"We present a new trainable system for physically plausible markerless 3D human motion capture, which achieves state-of-the-art results in a broad range of challenging scenarios. Unlike most neural methods for human motion capture, our approach, which we dub physionical, is aware of physical and environmental constraints. It combines in a fully differentiable way several key innovations, i.e., 1. a proportional-derivative controller, with gains predicted by a neural network, that reduces delays even in the presence of fast motions, 2. an explicit rigid body dynamics model and 3. a novel optimisation layer that prevents physically implausible foot-floor penetration as a hard constraint. The inputs to our system are 2D joint keypoints, which are canonicalised in a novel way so as to reduce the dependency on intrinsic camera parameters -- both at train and test time. This enables more accurate global translation estimation without generalisability loss. Our model can be finetuned only with 2D annotations when the 3D annotations are not available. It produces smooth and physically principled 3D motions in an interactive frame rate in a wide variety of challenging scenes, including newly recorded ones. Its advantages are especially noticeable on in-the-wild sequences that significantly differ from common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP. Qualitative results are available at http://gvv.mpi-inf.mpg.de/projects/PhysAware/ △ Less","3 May, 2021",https://arxiv.org/pdf/2105.01057
Context-aware Ensemble of Multifaceted Factorization Models for Recommendation Prediction in Social Networks,Yunwen Chen;Zuotao Liu;Daqi Ji;Yingwei Xin;Wenguang Wang;Lu Yao;Yi Zou,"This paper describes the solution of Shanda Innovations team to Task 1 of KDD-Cup 2012. A novel approach called Multifaceted Factorization Models is proposed to incorporate a great variety of features in social networks. Social relationships and actions between users are integrated as implicit feedbacks to improve the recommendation accuracy. Keywords, tags, profiles, time and some other features are also utilized for modeling user interests. In addition, user behaviors are modeled from the durations of recommendation records. A context-aware ensemble framework is then applied to combine multiple predictors and produce final recommendation results. The proposed approach obtained 0.43959 (public score) / 0.41874 (private score) on the testing dataset, which achieved the 2nd place in the KDD-Cup competition. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.00991
Fast Multi-Step Critiquing for VAE-based Recommender Systems,Diego Antognini;Boi Faltings,"Recent studies have shown that providing personalized explanations alongside recommendations increases trust and perceived quality. Furthermore, it gives users an opportunity to refine the recommendations by critiquing parts of the explanations. On one hand, current recommender systems model the recommendation, explanation, and critiquing objectives jointly, but this creates an inherent trade-off between their respective performance. On the other hand, although recent latent linear critiquing approaches are built upon an existing recommender system, they suffer from computational inefficiency at inference due to the objective optimized at each conversation's turn. We address these deficiencies with M&Ms-VAE, a novel variational autoencoder for recommendation and explanation that is based on multimodal modeling assumptions. We train the model under a weak supervision scheme to simulate both fully and partially observed variables. Then, we leverage the generalization ability of a trained M&Ms-VAE model to embed the user preference and the critique separately. Our work's most important innovation is our critiquing module, which is built upon and trained in a self-supervised manner with a simple ranking objective. Experiments on four real-world datasets demonstrate that among state-of-the-art models, our system is the first to dominate or match the performance in terms of recommendation, explanation, and multi-step critiquing. Moreover, M&Ms-VAE processes the critiques up to 25.6x faster than the best baselines. Finally, we show that our model infers coherent joint and cross generation, even under weak supervision, thanks to our multimodal-based modeling and training scheme. △ Less","7 July, 2021",https://arxiv.org/pdf/2105.00774
Learning by Design: Structuring and Documenting the Human Choices in Machine Learning Development,Simon Enni;Ira Assent,"The influence of machine learning (ML) is quickly spreading, and a number of recent technological innovations have applied ML as a central technology. However, ML development still requires a substantial amount of human expertise to be successful. The deliberation and expert judgment applied during ML development cannot be revisited or scrutinized if not properly documented, and this hinders the further adoption of ML technologies--especially in safety critical situations. In this paper, we present a method consisting of eight design questions, that outline the deliberation and normative choices going into creating a ML model. Our method affords several benefits, such as supporting critical assessment through methodological transparency, aiding in model debugging, and anchoring model explanations by committing to a pre hoc expectation of the model's behavior. We believe that our method can help ML practitioners structure and justify their choices and assumptions when developing ML models, and that it can help bridge a gap between those inside and outside the ML field in understanding how and why ML models are designed and developed the way they are. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.00687
Adapting CRISP-DM for Idea Mining: A Data Mining Process for Generating Ideas Using a Textual Dataset,W. Y. Ayele,"Data mining project managers can benefit from using standard data mining process models. The benefits of using standard process models for data mining, such as the de facto and the most popular, Cross-Industry-Standard-Process model for Data Mining (CRISP-DM) are reduced cost and time. Also, standard models facilitate knowledge transfer, reuse of best practices, and minimize knowledge requirements. On the other hand, to unlock the potential of ever-growing textual data such as publications, patents, social media data, and documents of various forms, digital innovation is increasingly needed. Furthermore, the introduction of cutting-edge machine learning tools and techniques enable the elicitation of ideas. The processing of unstructured textual data to generate new and useful ideas is referred to as idea mining. Existing literature about idea mining merely overlooks the utilization of standard data mining process models. Therefore, the purpose of this paper is to propose a reusable model to generate ideas, CRISP-DM, for Idea Mining (CRISP-IM). The design and development of the CRISP-IM are done following the design science approach. The CRISP-IM facilitates idea generation, through the use of Dynamic Topic Modeling (DTM), unsupervised machine learning, and subsequent statistical analysis on a dataset of scholarly articles. The adapted CRISP-IM can be used to guide the process of identifying trends using scholarly literature datasets or temporally organized patent or any other textual dataset of any domain to elicit ideas. The ex-post evaluation of the CRISP-IM is left for future study. △ Less","2 May, 2021",https://arxiv.org/pdf/2105.00574
Blind microscopy image denoising with a deep residual and multiscale encoder/decoder network,Fabio Hernán Gil Zuluaga;Francesco Bardozzo;Jorge Iván Ríos Patiño;Roberto Tagliaferri,"In computer-aided diagnosis (CAD) focused on microscopy, denoising improves the quality of image analysis. In general, the accuracy of this process may depend both on the experience of the microscopist and on the equipment sensitivity and specificity. A medical image could be corrupted by both intrinsic noise, due to the device limitations, and, by extrinsic signal perturbations during image acquisition. Nowadays, CAD deep learning applications pre-process images with image denoising models to reinforce learning and prediction. In this work, an innovative and lightweight deep multiscale convolutional encoder-decoder neural network is proposed. Specifically, the encoder uses deterministic mapping to map features into a hidden representation. Then, the latent representation is rebuilt to generate the reconstructed denoised image. Residual learning strategies are used to improve and accelerate the training process using skip connections in bridging across convolutional and deconvolutional layers. The proposed model reaches on average 38.38 of PSNR and 0.98 of SSIM on a test set of 57458 images overcoming state-of-the-art models in the same application domain △ Less","1 May, 2021",https://arxiv.org/pdf/2105.00273
COVID-Net CXR-S: Deep Convolutional Neural Network for Severity Assessment of COVID-19 Cases from Chest X-ray Images,Hossein Aboutalebi;Maya Pavlova;Mohammad Javad Shafiee;Ali Sabri;Amer Alaref;Alexander Wong,"The world is still struggling in controlling and containing the spread of the COVID-19 pandemic caused by the SARS-CoV-2 virus. The medical conditions associated with SARS-CoV-2 infections have resulted in a surge in the number of patients at clinics and hospitals, leading to a significantly increased strain on healthcare resources. As such, an important part of managing and handling patients with SARS-CoV-2 infections within the clinical workflow is severity assessment, which is often conducted with the use of chest x-ray (CXR) images. In this work, we introduce COVID-Net CXR-S, a convolutional neural network for predicting the airspace severity of a SARS-CoV-2 positive patient based on a CXR image of the patient's chest. More specifically, we leveraged transfer learning to transfer representational knowledge gained from over 16,000 CXR images from a multinational cohort of over 15,000 patient cases into a custom network architecture for severity assessment. Experimental results with a multi-national patient cohort curated by the Radiological Society of North America (RSNA) RICORD initiative showed that the proposed COVID-Net CXR-S has potential to be a powerful tool for computer-aided severity assessment of CXR images of COVID-19 positive patients. Furthermore, radiologist validation on select cases by two board-certified radiologists with over 10 and 19 years of experience, respectively, showed consistency between radiologist interpretation and critical factors leveraged by COVID-Net CXR-S for severity assessment. While not a production-ready solution, the ultimate goal for the open source release of COVID-Net CXR-S is to act as a catalyst for clinical scientists, machine learning researchers, as well as citizen scientists to develop innovative new clinical decision support solutions for helping clinicians around the world manage the continuing pandemic. △ Less","1 May, 2021",https://arxiv.org/pdf/2105.00256
A systematic mapping study on security countermeasures of in-vehicle communication systems,Jinghua Yu;Stefan Wagner;Bowen Wang;Feng Luo,"The innovations of vehicle connectivity have been increasing dramatically to enhance the safety and user experience of driving, while the rising numbers of interfaces to the external world also bring security threats to vehicles. Many security countermeasures have been proposed and discussed to protect the systems and services against attacks. To provide an overview of the current states in this research field, we conducted a systematic mapping study on the topic area ""security countermeasures of in-vehicle communication systems"". 279 papers are identified based on the defined study identification strategy and criteria. We discussed four research questions related to the security countermeasures, validation methods, publication patterns, and research trends and gaps based on the extracted and classified data. Finally, we evaluated the validity threats, the study identification results, and the whole mapping process. We found that the studies in this topic area are increasing rapidly in recent years. However, there are still gaps in various subtopics like automotive Ethernet security, anomaly reaction, and so on. This study reviews the target field not only related to research findings but also research activities, which can help identify research gaps at a high level and inspire new ideas for future work. △ Less","1 May, 2021",https://arxiv.org/pdf/2105.00183
"The EMPATHIC Project: Building an Expressive, Advanced Virtual Coach to Improve Independent Healthy-Life-Years of the Elderly",Luisa Brinkschulte;Natascha Mariacher;Stephan Schlögl;María Inés Torres;Raquel Justo;Javier Mikel Olaso;Anna Esposito;Gennaro Cordasco;Gérard Chollet;Cornelius Glackin;Colin Pickard;Dijana Petrovska-Delacretaz;Mohamed Amine Hmani;Ayment Mtibaa;Anaïs Fernandez;Daria Kyslitska;Begoña Fernandez-Ruanova;Jofre Tenorio-Laranga;Mari Aksnes;Maria Stylianou Korsnes;Miriam Reiner;Fredrik Lindner;Olivier Deroo;Olga Gordeeva,"This paper outlines the EMPATHIC Research & Innovation project, which aims to research, innovate, explore and validate new interaction paradigms and plat-forms for future generations of Personalized Virtual Coaches to assist elderly people living independently at and around their home. Innovative multimodal face analytics, adaptive spoken dialogue systems, and natural language inter-faces are part of what the project investigates and innovates, aiming to help dependent aging persons and their carers. It will uses remote, non-intrusive technologies to extract physiological markers of emotional states and adapt respective coach responses. In doing so, it aims to develop causal models for emotionally believable coach-user interactions, which shall engage elders and thus keep off loneliness, sustain health, enhance quality of life, and simplify access to future telecare services. Through measurable end-user validations performed in Spain, Norway and France (and complementary user evaluations in Italy), the proposed methods and solutions will have to demonstrate useful-ness, reliability, flexibility and robustness. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.13836
ConTNet: Why not use convolution and transformer at the same time?,Haotian Yan;Zhe Li;Weijian Li;Changhu Wang;Ming Wu;Chuang Zhang,"Although convolutional networks (ConvNets) have enjoyed great success in computer vision (CV), it suffers from capturing global information crucial to dense prediction tasks such as object detection and segmentation. In this work, we innovatively propose ConTNet (ConvolutionTransformer Network), combining transformer with ConvNet architectures to provide large receptive fields. Unlike the recently-proposed transformer-based models (e.g., ViT, DeiT) that are sensitive to hyper-parameters and extremely dependent on a pile of data augmentations when trained from scratch on a midsize dataset (e.g., ImageNet1k), ConTNet can be optimized like normal ConvNets (e.g., ResNet) and preserve an outstanding robustness. It is also worth pointing that, given identical strong data augmentations, the performance improvement of ConTNet is more remarkable than that of ResNet. We present its superiority and effectiveness on image classification and downstream tasks. For example, our ConTNet achieves 81.8% top-1 accuracy on ImageNet which is the same as DeiT-B with less than 40% computational complexity. ConTNet-M also outperforms ResNet50 as the backbone of both Faster-RCNN (by 2.6%) and Mask-RCNN (by 3.2%) on COCO2017 dataset. We hope that ConTNet could serve as a useful backbone for CV tasks and bring new ideas for model design △ Less","10 May, 2021",https://arxiv.org/pdf/2104.13497
Pronto: Federated Task Scheduling,Andreas Grammenos;Evangelia Kalyvianaki;Peter Pietzuch,"We present a federated, asynchronous, memory-limited algorithm for online task scheduling across large-scale networks of hundreds of workers. This is achieved through recent advancements in federated edge computing that unlocks the ability to incrementally compute local model updates within each node separately. This local model is then used along with incoming data to generate a rejection signal which reflects the overall node responsiveness and if it is able to accept an incoming task without resulting in degraded performance. Through this innovation, we allow each node to execute scheduling decisions on whether to accept an incoming job independently based on the workload seen thus far. Further, using the aggregate of the iterates a global view of the system can be constructed, as needed, and could be used to produce a holistic perspective of the system. We complement our findings, by an empirical evaluation on a large-scale real-world dataset of traces from a virtualized production data center that shows, while using limited memory, that our algorithm exhibits state-of-the-art performance. Concretely, it is able to predict changes in the system responsiveness ahead of time based on the industry-standard CPU-Ready metric and, in turn, can lead to better scheduling decisions and overall utilization of the available resources. Finally, in the absence of communication latency, it exhibits attractive horizontal scalability. △ Less","27 April, 2021",https://arxiv.org/pdf/2104.13429
Trend Alert: How a Cross-Platform Organization Manipulated Twitter Trends in the Indian General Election,Maurice Jakesch;Kiran Garimella;Dean Eckles;Mor Naaman,"Political organizations worldwide keep innovating their use of social media technologies. In the 2019 Indian general election, organizers used a network of WhatsApp groups to manipulate Twitter trends through coordinated mass postings. We joined 600 WhatsApp groups that support the Bharatiya Janata Party, the right-wing party that won the general election, to investigate these campaigns. We found evidence of 75 hashtag manipulation campaigns in the form of mobilization messages with lists of pre-written tweets. Building on this evidence, we estimate the campaigns' size, describe their organization and determine whether they succeeded in creating controlled social media narratives. Our findings show that the campaigns produced hundreds of nationwide Twitter trends throughout the election. Centrally controlled but voluntary in participation, this hybrid configuration of technologies and organizational strategies shows how profoundly online tools transform campaign politics. Trend alerts complicate the debates over the legitimate use of digital tools for political participation and may have provided a blueprint for participatory media manipulation by a party with popular support. △ Less","2 September, 2021",https://arxiv.org/pdf/2104.13259
CPS Engineering: Gap Analysis and Perspectives,Emmanuel Ledinot,"Virtualization of computing and networking, IT-OT convergence, cybersecurity and AI-based enhancement of autonomy are significantly increasing the complexity of CPS and CPSoS. New challenges have emerged to demonstrate that these systems are safe and secure. We emphasize the role of control and emerging fields therein, like symbolic control or set-based fault-tolerant and decentralized control, to address safety. We have chosen three open verification problems we deem central in cost-effective development and certification of safety critical CPSoS. We review some promising threads of research that could lead in the long term to a scalable and powerful verification strategy. Its main components are set-based and invariant-based design, contracts, adversarial testing, algorithmic geometry of dynamics, and probabilistic estimation derived from compositional massive testing. To explore these orientations in collaborative projects, and to promote them in certification arenas, we propose to continue and upgrade an open innovation drone-based use case that originated from a collaborative research project in aeronautic certification reformation △ Less","26 April, 2021",https://arxiv.org/pdf/2104.13210
"C-Ports: a proposal for a comprehensive standardization and implementation plan of digital services offered by the ""Port of the Future""",P. Pagano;S. Antonelli;A. Tardo,"In this paper we address the topic of a possible path to standardize the ICT services expected to be delivered by the so-called ""Port of the Future"". How the most relevant technologies and Information Systems are used by the Port Communities for their businesses is discussed together with a detailed analysis of the on-going actions carried on by Standard Setting Organizations. Considering the examples given by the C-ITS Platform and the C-Roads programme at EU level, a proposal of contents to be considered in a comprehensive standardization action is given. The innovation services are therefore grouped into four bundles: (i) Vessel \& Marine Navigation, (ii) e-Freight \& (Intermodal) Logistics, (iii) Passenger Transport, (iv) Environmental sustainability. The standardized version of these applications will be finally labeled as C-Port services. Alongside the standardization plan, a proposal for ranking the ports on the basis of a specially-defined C- Port vector is discussed with the purpose of addressing the well-known lack of consensus around the mathematical definition of the Smart Port Index. Considering the good practice and the background offered by the Port of Livorno in terms of innovation actions, the prospected final user applications are then labeled as Day 1, Day 1.5, and Day 2 services in consideration of the technical and commercial gaps to be filled. As a case study about the evolution in the C-Port vector experienced by the Port of Livorno in the last years will also be discussed. △ Less","23 December, 2021",https://arxiv.org/pdf/2104.13175
Graph Neural Networks for Traffic Forecasting,João Rico;José Barateiro;Arlindo Oliveira,"The significant increase in world population and urbanisation has brought several important challenges, in particular regarding the sustainability, maintenance and planning of urban mobility. At the same time, the exponential increase of computing capability and of available sensor and location data have offered the potential for innovative solutions to these challenges. In this work, we focus on the challenge of traffic forecasting and review the recent development and application of graph neural networks (GNN) to this problem. GNNs are a class of deep learning methods that directly process the input as graph data. This leverages more directly the spatial dependencies of traffic data and makes use of the advantages of deep learning producing state-of-the-art results. We introduce and review the emerging topic of GNNs, including their most common variants, with a focus on its application to traffic forecasting. We address the different ways of modelling traffic forecasting as a (temporal) graph, the different approaches developed so far to combine the graph and temporal learning components, as well as current limitations and research opportunities. △ Less","27 April, 2021",https://arxiv.org/pdf/2104.13096
A Novel Interaction-based Methodology Towards Explainable AI with Better Understanding of Pneumonia Chest X-ray Images,Shaw-Hwa Lo;Yiqiao Yin,"In the field of eXplainable AI (XAI), robust ""blackbox"" algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. A number of methods existing in literature focus on visualization techniques but the concepts of explainability and interpretability still require rigorous definition. In view of the above needs, this paper proposes an interaction-based methodology -- Influence Score (I-score) -- to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real world application in Pneumonia Chest X-ray Image data set and produced state-of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explainability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems. △ Less","15 June, 2021",https://arxiv.org/pdf/2104.12672
Microservice Dynamic Architecture-Level Deployment Orchestration (Extended Version),Lorenzo Bacchiani;Mario Bravetti;Saverio Giallorenzo;Jacopo Mauro;Iacopo Talevi;Gianluigi Zavattaro,"In the context of the BI-REX (Big Data Innovation and Research Excellence) competence center SEAWALL (SEAmless loW lAtency cLoud pLatforms) project (scientific coordinator Prof. Maurizio Gabbrielli) we develop a novel approach for run-time global adaptation of microservice applications, based on synthesis of architecture-level reconfiguration orchestrations. More precisely, we devise an algorithm for automatic reconfiguration that reaches a target system Maximum Computational Load by performing optimal deployment orchestrations. To conceive and simulate our approach, we introduce a novel integrated timed architectural modeling/execution language based on an extension of the actor-based object-oriented Abstract Behavioral Specification (ABS) language. In particular, we realize a timed extension of SmartDeployer, whose ABS code annotations make it possible to express architectural properties. Our Timed SmartDeployer tool fully integrates time features of ABS and architectural annotations by generating timed deployment orchestrations. We evaluate the applicability of our approach on a realistic microservice application taken from the literature: an Email Pipeline Processing System. We prove its effectiveness by simulating such an application and by comparing architecture-level reconfiguration with traditional local scaling techniques (which detect scaling needs and enact replications at the level of single microservices). Our comparison results show that our approach avoids cascading slowdowns and consequent increased message loss and latency, which affect traditional local scaling. △ Less","4 June, 2021",https://arxiv.org/pdf/2104.12466
Diverse Image Inpainting with Bidirectional and Autoregressive Transformers,Yingchen Yu;Fangneng Zhan;Rongliang Wu;Jianxiong Pan;Kaiwen Cui;Shijian Lu;Feiying Ma;Xuansong Xie;Chunyan Miao,"Image inpainting is an underdetermined inverse problem, which naturally allows diverse contents to fill up the missing or corrupted regions realistically. Prevalent approaches using convolutional neural networks (CNNs) can synthesize visually pleasant contents, but CNNs suffer from limited perception fields for capturing global features. With image-level attention, transformers enable to model long-range dependencies and generate diverse contents with autoregressive modeling of pixel-sequence distributions. However, the unidirectional attention in autoregressive transformers is suboptimal as corrupted image regions may have arbitrary shapes with contexts from any direction. We propose BAT-Fill, an innovative image inpainting framework that introduces a novel bidirectional autoregressive transformer (BAT) for image inpainting. BAT utilizes the transformers to learn autoregressive distributions, which naturally allows the diverse generation of missing contents. In addition, it incorporates the masked language model like BERT, which enables bidirectionally modeling of contextual information of missing regions for better image completion. Extensive experiments over multiple datasets show that BAT-Fill achieves superior diversity and fidelity in image inpainting qualitatively and quantitatively. △ Less","1 June, 2021",https://arxiv.org/pdf/2104.12335
Explainable Artificial Intelligence Reveals Novel Insight into Tumor Microenvironment Conditions Linked with Better Prognosis in Patients with Breast Cancer,Debaditya Chakraborty;Cristina Ivan;Paola Amero;Maliha Khan;Cristian Rodriguez-Aguayo;Hakan Başağaoğlu;Gabriel Lopez-Berestein,"We investigated the data-driven relationship between features in the tumor microenvironment (TME) and the overall and 5-year survival in triple-negative breast cancer (TNBC) and non-TNBC (NTNBC) patients by using Explainable Artificial Intelligence (XAI) models. We used clinical information from patients with invasive breast carcinoma from The Cancer Genome Atlas and from two studies from the cbioPortal, the PanCanAtlas project and the GDAC Firehose study. In this study, we used a normalized RNA sequencing data-driven cohort from 1,015 breast cancer patients, alive or deceased, from the UCSC Xena data set and performed integrated deconvolution with the EPIC method to estimate the percentage of seven different immune and stromal cells from RNA sequencing data. Novel insights derived from our XAI model showed that CD4+ T cells and B cells are more critical than other TME features for enhanced prognosis for both TNBC and NTNBC patients. Our XAI model revealed the critical inflection points (i.e., threshold fractions) of CD4+ T cells and B cells above or below which 5-year survival rates improve. Subsequently, we ascertained the conditional probabilities of \geq 5-year survival in both TNBC and NTNBC patients under specific conditions inferred from the inflection points. In particular, the XAI models revealed that a B-cell fraction exceeding 0.018 in the TME could ensure 100% 5-year survival for NTNBC patients. The findings from this research could lead to more accurate clinical predictions and enhanced immunotherapies and to the design of innovative strategies to reprogram the TME of breast cancer patients. △ Less","24 April, 2021",https://arxiv.org/pdf/2104.12021
Supervised Anomaly Detection via Conditional Generative Adversarial Network and Ensemble Active Learning,Zhi Chen;Jiang Duan;Li Kang;Guoping Qiu,"Anomaly detection has wide applications in machine intelligence but is still a difficult unsolved problem. Major challenges include the rarity of labeled anomalies and it is a class highly imbalanced problem. Traditional unsupervised anomaly detectors are suboptimal while supervised models can easily make biased predictions towards normal data. In this paper, we present a new supervised anomaly detector through introducing the novel Ensemble Active Learning Generative Adversarial Network (EAL-GAN). EAL-GAN is a conditional GAN having a unique one generator vs. multiple discriminators architecture where anomaly detection is implemented by an auxiliary classifier of the discriminator. In addition to using the conditional GAN to generate class balanced supplementary training data, an innovative ensemble learning loss function ensuring each discriminator makes up for the deficiencies of the others is designed to overcome the class imbalanced problem, and an active learning algorithm is introduced to significantly reduce the cost of labeling real-world data. We present extensive experimental results to demonstrate that the new anomaly detector consistently outperforms a variety of SOTA methods by significant margins. The codes are available on Github. △ Less","24 April, 2021",https://arxiv.org/pdf/2104.11952
COMTEST Project: A Complete Modular Test Stand for Human and Humanoid Posture Control and Balance,Vittorio Lippi;Thomas Mergner;Thomas Seel;Christoph Maurer,"This work presents a system to benchmark humanoid posture control and balance performances under perturbed conditions. The specific benchmarking scenario consists, for example, of balancing upright stance while performing voluntary movements on moving surfaces. The system includes a motion platform used to provide the perturbation, an innovative body-tracking system suitable for robots, humans and exoskeletons, control software and a set of predefined perturbations, a humanoid robot used to test algorithms, and analysis software providing state of the art data analysis used to provide quantitative measures of performance. In order to provide versatility, the design of the system is oriented to modularity: all its components can be replaced or extended according to experimental needs, adding additional perturbation profiles, new evaluation principles, and alternative tracking systems. It will be possible to use the system with different kinds of robots and exoskeletons as well as for human experiments aimed at gaining insights into human balance capabilities. △ Less","24 April, 2021",https://arxiv.org/pdf/2104.11935
Do All MobileNets Quantize Poorly? Gaining Insights into the Effect of Quantization on Depthwise Separable Convolutional Networks Through the Eyes of Multi-scale Distributional Dynamics,Stone Yun;Alexander Wong,"As the ""Mobile AI"" revolution continues to grow, so does the need to understand the behaviour of edge-deployed deep neural networks. In particular, MobileNets are the go-to family of deep convolutional neural networks (CNN) for mobile. However, they often have significant accuracy degradation under post-training quantization. While studies have introduced quantization-aware training and other methods to tackle this challenge, there is limited understanding into why MobileNets (and potentially depthwise-separable CNNs (DWSCNN) in general) quantize so poorly compared to other CNN architectures. Motivated to gain deeper insights into this phenomenon, we take a different strategy and study the multi-scale distributional dynamics of MobileNet-V1, a set of smaller DWSCNNs, and regular CNNs. Specifically, we investigate the impact of quantization on the weight and activation distributional dynamics as information propagates from layer to layer, as well as overall changes in distributional dynamics at the network level. This fine-grained analysis revealed significant dynamic range fluctuations and a ""distributional mismatch"" between channelwise and layerwise distributions in DWSCNNs that lead to increasing quantized degradation and distributional shift during information propagation. Furthermore, analysis of the activation quantization errors show that there is greater quantization error accumulation in DWSCNN compared to regular CNNs. The hope is that such insights can lead to innovative strategies for reducing such distributional dynamics changes and improve post-training quantization for mobile. △ Less","23 April, 2021",https://arxiv.org/pdf/2104.11849
Digestive System Dynamics in Molecular Communication Perspectives,Dixon Vimalajeewa;Sasitharan Balasubramaniam,"Consumption of food in excess of the required optimal nutritional requirements has already resulted in a global crisis and this is from the perspective of human health, such as obesity, as well as food waste and sustainability. In order to minimize the impact of these issues, there is a need to develop novel innovative and effective solutions that can optimally match the food consumption to the demand. This requires accurate understanding of the food digestion dynamics and its impact on each individual's physiological characteristics. This study proposes a model to characterize digestive system dynamics by using concepts from the field of Molecular Communications (MC), and this includes integrating advection-diffusion and reaction mechanisms and its role in characterizing the digestion process as a communication system. The model is then used to explore starch digestion dynamics by using communication system metrics such as delay and path loss. Our simulations found that the long gastric emptying time increases the delay in starch digestion and in turn the glucose production and absorption into the blood stream. At the same time, the enzyme activity on the hydrolyzed starch directly impacts the path loss, as higher reaction rates and lower half saturation concentration of starch results in lower path loss. Our work can lead to provide insights formulated for each individuals by creating a digital twin digestion model △ Less","22 April, 2021",https://arxiv.org/pdf/2104.11082
How individuals change language,Richard A Blythe;William Croft,"Languages emerge and change over time at the population level though interactions between individual speakers. It is, however, hard to directly observe how a single speaker's linguistic innovation precipitates a population-wide change in the language, and many theoretical proposals exist. We introduce a very general mathematical model that encompasses a wide variety of individual-level linguistic behaviours and provides statistical predictions for the population-level changes that result from them. This model allows us to compare the likelihood of empirically-attested changes in definite and indefinite articles in multiple languages under different assumptions on the way in which individuals learn and use language. We find that accounts of language change that appeal primarily to errors in childhood language acquisition are very weakly supported by the historical data, whereas those that allow speakers to change incrementally across the lifespan are more plausible, particularly when combined with social network effects. △ Less","20 April, 2021",https://arxiv.org/pdf/2104.10210
CrossATNet - A Novel Cross-Attention Based Framework for Sketch-Based Image Retrieval,Ushasi Chaudhuri;Biplab Banerjee;Avik Bhattacharya;Mihai Datcu,"We propose a novel framework for cross-modal zero-shot learning (ZSL) in the context of sketch-based image retrieval (SBIR). Conventionally, the SBIR schema mainly considers simultaneous mappings among the two image views and the semantic side information. Therefore, it is desirable to consider fine-grained classes mainly in the sketch domain using highly discriminative and semantically rich feature space. However, the existing deep generative modeling-based SBIR approaches majorly focus on bridging the gaps between the seen and unseen classes by generating pseudo-unseen-class samples. Besides, violating the ZSL protocol by not utilizing any unseen-class information during training, such techniques do not pay explicit attention to modeling the discriminative nature of the shared space. Also, we note that learning a unified feature space for both the multi-view visual data is a tedious task considering the significant domain difference between sketches and color images. In this respect, as a remedy, we introduce a novel framework for zero-shot SBIR. While we define a cross-modal triplet loss to ensure the discriminative nature of the shared space, an innovative cross-modal attention learning strategy is also proposed to guide feature extraction from the image domain exploiting information from the respective sketch counterpart. In order to preserve the semantic consistency of the shared space, we consider a graph CNN-based module that propagates the semantic class topology to the shared space. To ensure an improved response time during inference, we further explore the possibility of representing the shared space in terms of hash codes. Experimental results obtained on the benchmark TU-Berlin and the Sketchy datasets confirm the superiority of CrossATNet in yielding state-of-the-art results. △ Less","20 April, 2021",https://arxiv.org/pdf/2104.09918
Domain adaptation based self-correction model for COVID-19 infection segmentation in CT images,Qiangguo Jin;Hui Cui;Changming Sun;Zhaopeng Meng;Leyi Wei;Ran Su,"The capability of generalization to unseen domains is crucial for deep learning models when considering real-world scenarios. However, current available medical image datasets, such as those for COVID-19 CT images, have large variations of infections and domain shift problems. To address this issue, we propose a prior knowledge driven domain adaptation and a dual-domain enhanced self-correction learning scheme. Based on the novel learning schemes, a domain adaptation based self-correction model (DASC-Net) is proposed for COVID-19 infection segmentation on CT images. DASC-Net consists of a novel attention and feature domain enhanced domain adaptation model (AFD-DA) to solve the domain shifts and a self-correction learning process to refine segmentation results. The innovations in AFD-DA include an image-level activation feature extractor with attention to lung abnormalities and a multi-level discrimination module for hierarchical feature domain alignment. The proposed self-correction learning process adaptively aggregates the learned model and corresponding pseudo labels for the propagation of aligned source and target domain information to alleviate the overfitting to noises caused by pseudo labels. Extensive experiments over three publicly available COVID-19 CT datasets demonstrate that DASC-Net consistently outperforms state-of-the-art segmentation, domain shift, and coronavirus infection segmentation methods. Ablation analysis further shows the effectiveness of the major components in our model. The DASC-Net enriches the theory of domain adaptation and self-correction learning in medical imaging and can be generalized to multi-site COVID-19 infection segmentation on CT images for clinical deployment. △ Less","19 April, 2021",https://arxiv.org/pdf/2104.09699
Science Mapping to study academic knowledge circulation,Julian D. Cortes;Zaida Chinchilla-Rodriguez;Katerina Bohle-Karbonell,"The application of mathematics and statistical methods to scholarly communication: scientometrics, has facilitated the systematic analysis of the modern digital tide of literature. This chapter reviews three of such applications: coauthorship, bibliographic coupling, and coword networks. It also presents an exploratory case of study for the knowledge circulation literature. It was found a diverse geographical production, mainly in the Global North and Asian institutions with significant intermediation of universities from USA, Colombia, and Japan. The research fronts identified were related to science and medicine's history and philosophy; education, health, policy studies; and a set of interdisciplinary topics. Finally, the knowledge pillars were comprised of urban planning policy, economic geography, and historical and theoretical perspectives in the Netherlands and Central Europe; globalization and science, technology, and innovation and historical and institutional frameworks in the UK; and cultural and learning studies in the XXI century. △ Less","25 May, 2021",https://arxiv.org/pdf/2104.09484
"One More Check: Making ""Fake Background"" Be Tracked Again",Chao Liang;Zhipeng Zhang;Xue Zhou;Bing Li;Weiming Hu,"The one-shot multi-object tracking, which integrates object detection and ID embedding extraction into a unified network, has achieved groundbreaking results in recent years. However, current one-shot trackers solely rely on single-frame detections to predict candidate bounding boxes, which may be unreliable when facing disastrous visual degradation, e.g., motion blur, occlusions. Once a target bounding box is mistakenly classified as background by the detector, the temporal consistency of its corresponding tracklet will be no longer maintained. In this paper, we set out to restore the bounding boxes misclassified as ``fake background'' by proposing a re-check network. The re-check network innovatively expands the role of ID embedding from data association to motion forecasting by effectively propagating previous tracklets to the current frame with a small overhead. Note that the propagation results are yielded by an independent and efficient embedding search, preventing the model from over-relying on detection results. Eventually, it helps to reload the ``fake background'' and repair the broken tracklets. Building on a strong baseline CSTrack, we construct a new one-shot tracker and achieve favorable gains by 70.7 \rightarrow 76.4, 70.6 \rightarrow 76.3 MOTA on MOT16 and MOT17, respectively. It also reaches a new state-of-the-art MOTA and IDF1 performance. Code is released at https://github.com/JudasDie/SOTS. △ Less","12 December, 2021",https://arxiv.org/pdf/2104.09441
SIGIR 2021 E-Commerce Workshop Data Challenge,Jacopo Tagliabue;Ciro Greco;Jean-Francis Roy;Bingqing Yu;Patrick John Chia;Federico Bianchi;Giovanni Cassani,"The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for ""In-session prediction for purchase intent and recommendations"". The challenge addresses the growing need for reliable predictions within the boundaries of a shopping session, as customer intentions can be different depending on the occasion. The need for efficient procedures for personalization is even clearer if we consider the e-commerce landscape more broadly: outside of giant digital retailers, the constraints of the problem are stricter, due to smaller user bases and the realization that most users are not frequently returning customers. We release a new session-based dataset including more than 30M fine-grained browsing events (product detail, add, purchase), enriched by linguistic behavior (queries made by shoppers, with items clicked and items not clicked after the query) and catalog meta-data (images, text, pricing information). On this dataset, we ask participants to showcase innovative solutions for two open problems: a recommendation task (where a model is shown some events at the start of a session, and it is asked to predict future product interactions); an intent prediction task, where a model is shown a session containing an add-to-cart event, and it is asked to predict whether the item will be bought before the end of the session. △ Less","16 July, 2021",https://arxiv.org/pdf/2104.09423
"Strategies for Democratization of Supercomputing: Availability, Accessibility and Usability of High Performance Computing for Education and Practice of Big Data Analytics",Jim Samuel;Margaret Brennan-Tonetta;Yana Samuel;Pradeep Subedi;Jack Smith,"There has been an increasing interest in and growing need for high performance computing (HPC), popularly known as supercomputing, in domains such as textual analytics, business domains analytics, forecasting and natural language processing (NLP), in addition to the relatively mature supercomputing domains of quantum physics and biology. HPC has been widely used in computer science (CS) and other traditionally computation intensive disciplines, but has remained largely siloed away from the vast array of social, behavioral, business and economics disciplines. However, with ubiquitous big data, there is a compelling need to make HPC technologically and economically accessible, easy to use, and operationally democratized. Therefore, this research focuses on making two key contributions, the first is the articulation of strategies based on availability, accessibility and usability for the demystification and democratization of HPC, based on an analytical review of Caliburn, a notable supercomputer at its inception. The second contribution is a set of principles for HPC adoption based on an experiential narrative of HPC usage for textual analytics and NLP of social media data from a first time user perspective. Both, the HPC usage process and the output of the early stage analytics are summarized. This research study synthesizes expert input on HPC democratization strategies, and chronicles the challenges and opportunities from a multidisciplinary perspective, of a case of rapid adoption of supercomputing for textual analytics and NLP. Deductive logic is used to identify strategies which can lead to efficacious engagement, adoption, production and sustained usage for research, teaching, application and innovation by researchers, faculty, professionals and students across a broad range of disciplines. △ Less","19 April, 2021",https://arxiv.org/pdf/2104.09091
Deep Learning in Beyond 5G Networks with Image-based Time-Series Representation,Lucas Fernando Alvarenga e Silva;Bruno Yuji Lino Kimura;Jurandy Almeida,"Towards the network innovation, the Beyond Five-Generation (B5G) networks envision the use of machine learning (ML) methods to predict the network conditions and performance indicators in order to best make decisions and allocate resources. In this paper, we propose a new ML approach to accomplish predictions in B5G networks. Instead of handling the time-series in the network domain of values, we transform them into image thus allowing to apply advanced ML methods of Computer Vision field to reach better predictions in B5G networks. Particularly, we analyze different techniques to transform time-series of network measures into image representation, e.g., Recurrence Plots, Markov Transition Fields, and Gramian Angular Fields. Then, we apply deep neural networks with convolutional layers to predict different 5G radio signal quality indicators. When comparing with other ML-based solutions, experimental results from 5G transmission datasets showed the feasibility and small prediction errors of the proposed approach. △ Less","17 April, 2021",https://arxiv.org/pdf/2104.08584
Cycle-free CycleGAN using Invertible Generator for Unsupervised Low-Dose CT Denoising,Taesung Kwon;Jong Chul Ye,"Recently, CycleGAN was shown to provide high-performance, ultra-fast denoising for low-dose X-ray computed tomography (CT) without the need for a paired training dataset. Although this was possible thanks to cycle consistency, CycleGAN requires two generators and two discriminators to enforce cycle consistency, demanding significant GPU resources and technical skills for training. A recent proposal of tunable CycleGAN with Adaptive Instance Normalization (AdaIN) alleviates the problem in part by using a single generator. However, two discriminators and an additional AdaIN code generator are still required for training. To solve this problem, here we present a novel cycle-free Cycle-GAN architecture, which consists of a single generator and a discriminator but still guarantees cycle consistency. The main innovation comes from the observation that the use of an invertible generator automatically fulfills the cycle consistency condition and eliminates the additional discriminator in the CycleGAN formulation. To make the invertible generator more effective, our network is implemented in the wavelet residual domain. Extensive experiments using various levels of low-dose CT images confirm that our method can significantly improve denoising performance using only 10% of learnable parameters and faster training time compared to the conventional CycleGAN. △ Less","17 April, 2021",https://arxiv.org/pdf/2104.08538
"Open data for Moroccan license plates for OCR applications : data collection, labeling, and model construction",Abdelkrim Alahyane;Mohamed El Fakir;Saad Benjelloun;Ikram Chairi,"Significant number of researches have been developed recently around intelligent system for traffic management, especially, OCR based license plate recognition, as it is considered as a main step for any automatic traffic management system. Good quality data sets are increasingly needed and produced by the research community to improve the performance of those algorithms. Furthermore, a special need of data is noted for countries having special characters on their licence plates, like Morocco, where Arabic Alphabet is used. In this work, we present a labeled open data set of circulation plates taken in Morocco, for different type of vehicles, namely cars, trucks and motorcycles. This data was collected manually and consists of 705 unique and different images. Furthermore this data was labeled for plate segmentation and for matriculation number OCR. Also, As we show in this paper, the data can be enriched using data augmentation techniques to create training sets with few thousands of images for different machine leaning and AI applications. We present and compare a set of models built on this data. Also, we publish this data as an open access data to encourage innovation and applications in the field of OCR and image processing for traffic control and other applications for transportation and heterogeneous vehicle management. △ Less","16 April, 2021",https://arxiv.org/pdf/2104.08244
Introduction to Big data Technology,Bilal Abu-Salih;Pornpit Wongthongtham;Dengya Zhu;Kit Yan Chan;Amit Rudra,"Big data is no more ""all just hype"" but widely applied in nearly all aspects of our business, governments, and organizations with the technology stack of AI. Its influences are far beyond a simple technique innovation but involves all rears in the world. This chapter will first have historical review of big data; followed by discussion of characteristics of big data, i.e. from the 3V's to up 10V's of big data. The chapter then introduces technology stacks for an or-ganization to build a big data application, from infrastruc-ture/platform/ecosystem to constructional units and components. Finally, we provide some big data online resources for reference. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.08062
Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research,Denis Newman-Griffis;Jill Fain Lehman;Carolyn Rosé;Harry Hochheiser,"Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.07874
ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning,Samyam Rajbhandari;Olatunji Ruwase;Jeff Rasley;Shaden Smith;Yuxiong He,"In the last three years, the largest dense deep learning models have grown over 1000x to reach hundreds of billions of parameters, while the GPU memory has only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has been supported primarily though system innovations that allow large models to fit in the aggregate GPU memory of multiple GPUs. However, we are getting close to the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion parameter model for training, and such clusters are simply out of reach for most data scientists. In addition, training models at that scale requires complex combinations of parallelism techniques that puts a big burden on the data scientists to refactor their model. In this paper we present ZeRO-Infinity, a novel heterogeneous system technology that leverages GPU, CPU, and NVMe memory to allow for unprecedented model scale on limited resources without requiring model code refactoring. At the same time it achieves excellent training throughput and scalability, unencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models with tens and even hundreds of trillions of parameters for training on current generation GPU clusters. It can be used to fine-tune trillion parameter models on a single NVIDIA DGX-2 node, making large models more accessible. In terms of training throughput and scalability, it sustains over 25 petaflops on 512 NVIDIA V100 GPUs(40% of peak), while also demonstrating super linear scalability. An open source implementation of ZeRO-Infinity is available through DeepSpeed, a deep learning optimization library that makes distributed training easy, efficient, and effective. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.07857
Tourist route optimization in the context of Covid-19 pandemic,Cristina Maria Pacurar;Ruxandra-Gabriela Albu;Victor-Dan Pacurar,"The paper presents an innovative method for tourist route planning inside a destination. The necessity of reorganizing the tourist routes within a destination comes as an immediate response to the Covid-19 crisis. The implementation of the method inside tourist destinations can be an important advantage in transforming a destination into a safer destination in times of Covid-19 and post-Covid-19. The existing trend of shortening the tourist stay length has been accelerated while the epidemic became a pandemic. Moreover, the wariness for future pandemics has brought to the spotlight the issue of overcrowded attractions inside a destination at certain moments. The method proposed in this paper proposes a backtracking algorithm, more precisely an adaptation of the travelling salesman problem. The method presented aims to facilitate the navigation inside a destination and to revive certain less-visited sightseeing spots inside a destination while facilitating the social distancing measures imposed by Covid-19. △ Less","5 May, 2021",https://arxiv.org/pdf/2104.07663
SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local Elements,Qianli Ma;Shunsuke Saito;Jinlong Yang;Siyu Tang;Michael J. Black,"Learning to model and reconstruct humans in clothing is challenging due to articulation, non-rigid deformation, and varying clothing types and topologies. To enable learning, the choice of representation is the key. Recent work uses neural networks to parameterize local surface elements. This approach captures locally coherent geometry and non-planar details, can deal with varying topology, and does not require registered training data. However, naively using such methods to model 3D clothed humans fails to capture fine-grained local deformations and generalizes poorly. To address this, we present three key innovations: First, we deform surface elements based on a human body model such that large-scale deformations caused by articulation are explicitly separated from topological changes and local clothing deformations. Second, we address the limitations of existing neural surface elements by regressing local geometry from local features, significantly improving the expressiveness. Third, we learn a pose embedding on a 2D parameterization space that encodes posed body geometry, improving generalization to unseen poses by reducing non-local spurious correlations. We demonstrate the efficacy of our surface representation by learning models of complex clothing from point clouds. The clothing can change topology and deviate from the topology of the body. Once learned, we can animate previously unseen motions, producing high-quality point clouds, from which we generate realistic images with neural rendering. We assess the importance of each technical contribution and show that our approach outperforms the state-of-the-art methods in terms of reconstruction accuracy and inference time. The code is available for research purposes at https://qianlim.github.io/SCALE . △ Less","15 April, 2021",https://arxiv.org/pdf/2104.07660
"Innovation for Sustainability in the Global South: Bibliometric findings from management & business and STEM (Science, Technology, Engineering and Mathematics) fields in developing countries",Julian D. Cortes;Mireia Guix;Katerina Bohle Carbonell,"Research on innovation and sustainability is prolific but fragmented. This study integrates the research on innovation in management and business and STEM fields for sustainability in a unified framework for the case of developing countries (i.e., the Global South). It presents and discusses the output, impact, and structure of such research based on a sample of 14,000+ articles and conference proceedings extracted from the bibliographic database Scopus. The findings reveal research output inflections after global announcements such as UN-Earth Summits. The study also reveals the indisputable leadership of China in overall output and research agenda-setting. Nonetheless, countries such as India, Mexico, and Nigeria are either more efficient or impactful. GS research published in highly reputable journals is still scarce but increasing modestly. Central topic clusters (e.g., knowledge management) remain peripheral to the global Sustainable Development Goals (SDGs) research landscape. Finally, academic-corporate collaboration is in its infancy and limited to particular economic sectors: energy, pharmaceuticals, and high-tech. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.07543
A National Discovery Cloud: Preparing the US for Global Competitiveness in the New Era of 21st Century Digital Transformation,Ian Foster;Daniel Lopresti;Bill Gropp;Mark D. Hill;Katie Schuman,"The nature of computation and its role in our lives have been transformed in the past two decades by three remarkable developments: the emergence of public cloud utilities as a new computing platform; the ability to extract information from enormous quantities of data via machine learning; and the emergence of computational simulation as a research method on par with experimental science. Each development has major implications for how societies function and compete; together, they represent a change in technological foundations of society as profound as the telegraph or electrification. Societies that embrace these changes will lead in the 21st Century; those that do not, will decline in prosperity and influence. Nowhere is this stark choice more evident than in research and education, the two sectors that produce the innovations that power the future and prepare a workforce able to exploit those innovations, respectively. In this article, we introduce these developments and suggest steps that the US government might take to prepare the research and education system for its implications. △ Less","19 April, 2021",https://arxiv.org/pdf/2104.06953
A Review of Anonymization for Healthcare Data,Iyiola E. Olatunji;Jens Rauch;Matthias Katzensteiner;Megha Khosla,"Mining health data can lead to faster medical decisions, improvement in the quality of treatment, disease prevention, reduced cost, and it drives innovative solutions within the healthcare sector. However, health data is highly sensitive and subject to regulations such as the General Data Protection Regulation (GDPR), which aims to ensure patient's privacy. Anonymization or removal of patient identifiable information, though the most conventional way, is the first important step to adhere to the regulations and incorporate privacy concerns. In this paper, we review the existing anonymization techniques and their applicability to various types (relational and graph-based) of health data. Besides, we provide an overview of possible attacks on anonymized data. We illustrate via a reconstruction attack that anonymization though necessary, is not sufficient to address patient privacy and discuss methods for protecting against such attacks. Finally, we discuss tools that can be used to achieve anonymization. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06523
Trust and Safety,S. K. Devitt;R. Horne;Z. Assaad;E. Broad;H. Kurniawati;B. Cardier;A. Scott;S. Lazar;M. Gould;C. Adamson;C. Karl;F. Schrever;S. Keay;K. Tranter;E. Shellshear;D. Hunter;M. Brady;T. Putland,"Robotics in Australia have a long history of conforming with safety standards and risk managed practices. This chapter articulates the current state of trust and safety in robotics including society's expectations, safety management systems and system safety as well as emerging issues and methods for ensuring safety in increasingly autonomous robotics. The future of trust and safety will combine standards with iterative, adaptive and responsive regulatory and assurance methods for diverse applications of robotics, autonomous systems and artificial intelligence (RAS-AI). Robotics will need novel technical and social approaches to achieve assurance, particularly for game-changing innovations. The ability for users to easily update algorithms and software, which alters the performance of a system, implies that traditional machine assurance performed prior to deployment or sale, will no longer be viable. Moreover, the high frequency of updates implies that traditional certification that requires substantial time will no longer be practical. To alleviate these difficulties, automation of assurance will likely be needed; something like 'ASsurance-as-a-Service' (ASaaS), where APIs constantly ping RAS-AI to ensure abidance with various rules, frameworks and behavioural expectations. There are exceptions to this, such as in contested or communications denied environments, or in underground or undersea mining; and these systems need their own risk assessments and limitations imposed. Indeed, self-monitors are already operating within some systems. To ensure safe operations of future robotics systems, Australia needs to invest in RAS-AI assurance research, stakeholder engagement and continued development and refinement of robust frameworks, methods, guidelines and policy in order to educate and prepare its technology developers, certifiers, and general population. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06512
"Research on Innovation in China and Latin America: bibliometric insights in business, management, accounting, and decision sciences",Julian D. Cortes,"This study aims to comprehend the structure of RIBM (research on innovation in business and management) in China and LAC (Latin America and the Caribbean) via co-word and institutional co-authorship networks using Scopus' bibliographic data (1998- 2018). Multiple Correspondence Analysis and Social Network Analysis were applied. Public institutions are interconnected and generate most of the advances in RIBM. RIBM boards regional and national STi policies permeated by sustainability-related factors. China is focused on IT and knowledge management for supply chain and engineering, while LAC focuses on institutional perspectives for economic development. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06440
"Research on innovation in business and management about China and Latin America: bibliometrics insights using Google Scholar, Dimensions and Microsoft Academic",Julian D. Cortes;Xiaolei Lin;Xiaolei Xun,"Trade and investment between developing regions such as China and Latin America (LATAM) are growing prominently. However, insights on crucial factors such as innovation in business and management (iBM) about both regions have not been scrutinized. This study presents the research output, impact, and structure of iBM research published about China and LATAM in a comparative framework using Google Scholar, Dimensions, and Microsoft Academic. Findings showed i) that iBM topics of both regions were framed within research and development management, and technological development topics, ii) significant differences in output and impact between regions, and iii) the same case for platforms. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06437
"Research on innovation in China and Latin America: Bibliometric insights in the field of business, management and decision sciences",Julian D. Cortes,"China and Latin America (LATAM) are now key players in global research production. This study presents a comparative study on research on innovation in management and decision sciences based on data from Scopus and Web of Knowledge (WoS) between China and LATAM. Findings showed significant differences between regions regarding journals citation dependent measures, and between the number of authors and journal reputation, public universities have been leading producers, and China showed a particular interest in research topics such as commerce and industry, while LATAM in sustainable development and bio-technology. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06432
Terrain assessment for precision agriculture using vehicle dynamic modelling,Giulio Reina;Annalisa Milella;Rocco Galati,"Advances in precision agriculture greatly rely on innovative control and sensing technologies that allow service units to increase their level of driving automation while ensuring at the same time high safety standards. This paper deals with automatic terrain estimation and classification that is performed simultaneously by an agricultural vehicle during normal operations. Vehicle mobility and safety, and the successful implementation of important agricultural tasks including seeding, ploughing, fertilising and controlled traffic depend or can be improved by a correct identification of the terrain that is traversed. The novelty of this research lies in that terrain estimation is performed by using not only traditional appearance-based features, that is colour and geometric properties, but also contact-based features, that is measuring physics-based dynamic effects that govern the vehicleeterrain interaction and that greatly affect its mobility. Experimental results obtained from an all-terrain vehicle operating on different surfaces are presented to validate the system in the field. It was shown that a terrain classifier trained with contact features was able to achieve a correct prediction rate of 85.1%, which is comparable or better than that obtained with approaches using traditional feature sets. To further improve the classification performance, all feature sets were merged in an augmented feature space, reaching, for these tests, 89.1% of correct predictions. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06326
Wireless Environment as a Service Enabled by Reconfigurable Intelligent Surfaces: The RISE-6G Perspective,Emilio Calvanese Strinati;George C. Alexandropoulos;Vincenzo Sciancalepore;Marco Di Renzo;Henk Wymeersch;Dinh-Thuy Phan-huy;Maurizio Crozzoli;Raffaele D'Errico;Elisabeth De Carvalho;Petar Popovski;Paolo Di Lorenzo;Luca Bastianelli;Mathieu Belouar;Julien Etienne Mascolo;Gabriele Gradoni;Sendy Phang;Geoffroy Lerosey;Benoît Denis,"The design of 6th Generation (6G) wireless networks points towards flexible connect-and-compute technologies capable to support innovative services and use cases. Targeting the 2030 horizon, 6G networks are poised to pave the way for sustainable human-centered smart societies and vertical industries, such that wireless networks will be transformed into a distributed smart connectivity infrastructure, where new terminal types are embedded in the daily environment. In this context, the RISE-6G project aims at investigating innovative solutions that capitalize on the latest advances in the emerging technology of Reconfigurable Intelligent Surfaces (RISs), which offers dynamic and goal-oriented radio wave propagation control, enabling the concept of the wireless environment as a service. The project will focus on: i) the realistic modeling of RIS-assisted signal propagation, ii) the investigation of the fundamental limits of RIS-empowered wireless communications and sensing, and iii) the design of efficient algorithms for orchestrating networking RISs, in order to implement intelligent, sustainable, and dynamically programmable wireless environments enabling diverse services that go well beyond the 5G capabilities. RISE-6G will offer two unprecedented proof-of-concepts for realizing controlled wireless environments in near-future use cases. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06265
Relevance-Aware Anomalous Users Detection in Social Network via Graph Neural Network,Yangyang Li;Yipeng Ji;Shaoning Li;Shulong He;Yinhao Cao;Xiong Li;Jun Shi;Yangchao Yang;Yifeng Liu,"Anomalous users detection in social network is an imperative task for security problems. Motivated by the great power of Graph Neural Networks(GNNs), many current researches adopt GNN-based detectors to reveal the anomalous users. However, the increasing scale of social activities, explosive growth of users and manifold technical disguise render the user detection a difficult task. In this paper, we propose an innovate Relevance-aware Anomalous Users Detection model (RAU-GNN) to obtain a fine-grained detection result. RAU-GNN first extracts multiple relations of all types of users in social network, including both benign and anomalous users, and accordingly constructs the multiple user relation graph. Secondly, we employ relevance-aware GNN framework to learn the hidden features of users, and discriminate the anomalous users after discriminating. Concretely, by integrating Graph Convolution Network(GCN) and Graph Attention Network(GAT), we design a GCN-based relation fusion layer to aggregate initial information from different relations, and a GAT-based embedding layer to obtain the high-level embeddings. Lastly, we feed the learned representations to the following GNN layer in order to consolidate the node embedding by aggregating the final users' embeddings. We conduct extensive experiment on real-world datasets. The experimental results show that our approach can achieve high accuracy for anomalous users detection. △ Less","24 April, 2021",https://arxiv.org/pdf/2104.06095
Hierarchical Adaptive Pooling by Capturing High-order Dependency for Graph Representation Learning,Ning Liu;Songlei Jian;Dongsheng Li;Yiming Zhang;Zhiquan Lai;Hongzuo Xu,"Graph neural networks (GNN) have been proven to be mature enough for handling graph-structured data on node-level graph representation learning tasks. However, the graph pooling technique for learning expressive graph-level representation is critical yet still challenging. Existing pooling methods either struggle to capture the local substructure or fail to effectively utilize high-order dependency, thus diminishing the expression capability. In this paper we propose HAP, a hierarchical graph-level representation learning framework, which is adaptively sensitive to graph structures, i.e., HAP clusters local substructures incorporating with high-order dependencies. HAP utilizes a novel cross-level attention mechanism MOA to naturally focus more on close neighborhood while effectively capture higher-order dependency that may contain crucial information. It also learns a global graph content GCont that extracts the graph pattern properties to make the pre- and post-coarsening graph content maintain stable, thus providing global guidance in graph coarsening. This novel innovation also facilitates generalization across graphs with the same form of features. Extensive experiments on fourteen datasets show that HAP significantly outperforms twelve popular graph pooling methods on graph classification task with an maximum accuracy improvement of 22.79%, and exceeds the performance of state-of-the-art graph matching and graph similarity learning algorithms by over 3.5% and 16.7%. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.05960
"Evidence-based Prescriptive Analytics, CAUSAL Digital Twin and a Learning Estimation Algorithm",PG Madhavan,"Evidence-based Prescriptive Analytics (EbPA) is necessary to determine optimal operational set-points that will improve business productivity. EbPA results from what-if analysis and counterfactual experimentation on CAUSAL Digital Twins (CDTs) that quantify cause-effect relationships in the DYNAMICS of a system of connected assets. We describe the basics of Causality and Causal Graphs and develop a Learning Causal Digital Twin (LCDT) solution; our algorithm uses a simple recurrent neural network with some innovative modifications incorporating Causal Graph simulation. Since LCDT is a learning digital twin where parameters are learned online in real-time with minimal pre-configuration, the work of deploying digital twins will be significantly simplified. A proof-of-principle of LCDT was conducted using real vibration data from a system of bearings; results of causal factor estimation, what-if analysis study and counterfactual experiment are very encouraging. △ Less","12 April, 2021",https://arxiv.org/pdf/2104.05828
"WHOSe Heritage: Classification of UNESCO World Heritage ""Outstanding Universal Value"" Documents with Soft Labels",Nan Bai;Renqian Luo;Pirouz Nourian;Ana Pereira Roders,"The UNESCO World Heritage List (WHL) includes the exceptionally valuable cultural and natural heritage to be preserved for mankind. Evaluating and justifying the Outstanding Universal Value (OUV) is essential for each site inscribed in the WHL, and yet a complex task, even for experts, since the selection criteria of OUV are not mutually exclusive. Furthermore, manual annotation of heritage values and attributes from multi-source textual data, which is currently dominant in heritage studies, is knowledge-demanding and time-consuming, impeding systematic analysis of such authoritative documents in terms of their implications on heritage management. This study applies state-of-the-art NLP models to build a classifier on a new dataset containing Statements of OUV, seeking an explainable and scalable automation tool to facilitate the nomination, evaluation, research, and monitoring processes of World Heritage sites. Label smoothing is innovatively adapted to improve the model performance by adding prior inter-class relationship knowledge to generate soft labels. The study shows that the best models fine-tuned from BERT and ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation on the model prediction shows that the models are sufficiently generalizable. The study is promising to be further developed and applied in heritage research and practice. △ Less","9 September, 2021",https://arxiv.org/pdf/2104.05547
An approach utilizing negation of extended-dimensional vector of disposing mass for ordinal evidences combination in a fuzzy environment,Yuanpeng He,"How to measure the degree of uncertainty of a given frame of discernment has been a hot topic for years. A lot of meaningful works have provided some effective methods to measure the degree properly. However, a crucial factor, sequence of propositions, is missing in the definition of traditional frame of discernment. In this paper, a detailed definition of ordinal frame of discernment has been provided. Besides, an innovative method utilizing a concept of computer vision to combine the order of propositions and the mass of them is proposed to better manifest relationships between the two important element of the frame of discernment. More than that, a specially designed method covering some powerful tools in indicating the degree of uncertainty of a traditional frame of discernment is also offered to give an indicator of level of uncertainty of an ordinal frame of discernment on the level of vector. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.05416
Implementing an expert system to evaluate technical solutions innovativeness,V. K. Ivanov;I. V. Obraztsov;B. V. Palyukh,"The paper presents a possible solution to the problem of algorithmization for quantifying inno-vativeness indicators of technical products, inventions and technologies. The concepts of technological nov-elty, relevance and implementability as components of product innovation criterion are introduced. Authors propose a model and algorithm to calculate every of these indicators of innovativeness under conditions of incompleteness and inaccuracy, and sometimes inconsistency of the initial information. The paper describes the developed specialized software that is a promising methodological tool for using interval estimations in accordance with the theory of evidence. These estimations are used in the analysis of complex multicomponent systems, aggregations of large volumes of fuzzy and incomplete data of various structures. Composition and structure of a multi-agent expert system are presented. The purpose of such system is to process groups of measurement results and to estimate indicators values of objects innovativeness. The paper defines active elements of the system, their functionality, roles, interaction order, input and output inter-faces, as well as the general software functioning algorithm. It describes implementation of software modules and gives an example of solving a specific problem to determine the level of technical products innovation. △ Less","26 March, 2021",https://arxiv.org/pdf/2104.05407
Capacity-Driven Low-Interference Fast Beam Synthesis for Next Generation Base Stations,G. Oliveri;G. Gottardi;N. Anselmi;A. Massa,"The problem of the real-time multiple-input multiple-output (MIMO) array control when requirements on capacity performance, out-of-cell interference, and computational efficiency are simultaneously enforced is addressed by means of an innovative hybrid beamforming technique. The synthesis of the excitations of the MIMO system is first re-formulated as that of matching an ideal ""hybrid"" pattern fitting capacity or low-interference constraints along the angular coordinates. Then, a non-iterative processing scheme is derived for each MIMO beam where numerically-efficient synthesis techniques are profitably combined. Representative results, from an extensive numerical validation, are discussed to show, also comparatively, the advantages and the current limitations of the proposed synthesis method when dealing with different propagation scenarios, number of transmitters/receivers, and noise levels. △ Less","12 April, 2021",https://arxiv.org/pdf/2104.05363
Consideration of resilience for digital farming systems,Sebastian Boekle;Leon Koenn;David Reiser;Dimitris S. Paraforos;Hans W. Griepentrog,"Latest and current innovations of agricultural tech industry are increasingly driven by digital technologies. These digital farming solutions provide attractive advantages for farmers. The trend is going to devices and sensors, which send the acquired data directly to the cloud. Also the number of scientific publications on cloud based solutions follows this development. Considering on the other hand the necessity of continuous agricultural production in any kind of crises, new cloud-based digital systems and applications need to be reliable, independent of internet supply. In this conceptual study the necessary resilience is defined, which is marginally taken into account by agtech industry innovations. Problems of development using web-based farming systems are identified and discussed. For digital farming systems the farmers individual needs of resilience are classified into five levels. Consequently, suggestions for soft- and hardware equipment are made. This includes the installation of a farm server, a local farm network, offline applications and consideration of edge computing, which can ensure a high level of resilience of new digital farming components. △ Less","12 April, 2021",https://arxiv.org/pdf/2104.05287
The structure of online social networks modulates the rate of lexical change,Jian Zhu;David Jurgens,"New words are regularly introduced to communities, yet not all of these words persist in a community's lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community's network structure plays a significant role in lexical change. Apart from overall size, properties including dense connections, the lack of local clusters and more external contacts promote lexical innovation and retention. Unlike offline communities, these topic-based communities do not experience strong lexical levelling despite increased contact but accommodate more niche words. Our work provides support for the sociolinguistic hypothesis that lexical change is partially shaped by the structure of the underlying network but also uncovers findings specific to online communities. △ Less","11 April, 2021",https://arxiv.org/pdf/2104.05010
Use of Metamorphic Relations as Knowledge Carriers to Train Deep Neural Networks,Tsong Yueh Chen;Pak-Lok Poon;Kun Qiu;Zheng Zheng;Jinyi Zhou,"Training multiple-layered deep neural networks (DNNs) is difficult. The standard practice of using a large number of samples for training often does not improve the performance of a DNN to a satisfactory level. Thus, a systematic training approach is needed. To address this need, we introduce an innovative approach of using metamorphic relations (MRs) as ""knowledge carriers"" to train DNNs. Based on the concept of metamorphic testing and MRs (which play the role of a test oracle in software testing), we make use of the notion of metamorphic group of inputs as concrete instances of MRs (which are abstractions of knowledge) to train a DNN in a systematic and effective manner. To verify the viability of our training approach, we have conducted a preliminary experiment to compare the performance of two DNNs: one trained with MRs and the other trained without MRs. We found that the DNN trained with MRs has delivered a better performance, thereby confirming that our approach of using MRs as knowledge carriers to train DNNs is promising. More work and studies, however, are needed to solidify and leverage this approach to generate widespread impact on effective DNN training. △ Less","11 May, 2021",https://arxiv.org/pdf/2104.04718
Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis,Xutan Peng;Guanyi Chen;Chenghua Lin;Mark Stevenson,"Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm. △ Less","17 April, 2021",https://arxiv.org/pdf/2104.04676
INODE: Building an End-to-End Data Exploration System in Practice [Extended Vision],Sihem Amer-Yahia;Georgia Koutrika;Frederic Bastian;Theofilos Belmpas;Martin Braschler;Ursin Brunner;Diego Calvanese;Maximilian Fabricius;Orest Gkini;Catherine Kosten;Davide Lanti;Antonis Litke;Hendrik Lücke-Tieke;Francesco Alessandro Massucci;Tarcisio Mendes de Farias;Alessandro Mosca;Francesco Multari;Nikolaos Papadakis;Dimitris Papadopoulos;Yogendra Patil;Aurélien Personnaz;Guillem Rull;Ana Sima;Ellery Smith;Dimitrios Skoutas,"A full-fledged data exploration system must combine different access modalities with a powerful concept of guiding the user in the exploration process, by being reactive and anticipative both for data discovery and for data linking. Such systems are a real opportunity for our community to cater to users with different domain and data science expertise. We introduce INODE -- an end-to-end data exploration system -- that leverages, on the one hand, Machine Learning and, on the other hand, semantics for the purpose of Data Management (DM). Our vision is to develop a classic unified, comprehensive platform that provides extensive access to open datasets, and we demonstrate it in three significant use cases in the fields of Cancer Biomarker Reearch, Research and Innovation Policy Making, and Astrophysics. INODE offers sustainable services in (a) data modeling and linking, (b) integrated query processing using natural language, (c) guidance, and (d) data exploration through visualization, thus facilitating the user in discovering new insights. We demonstrate that our system is uniquely accessible to a wide range of users from larger scientific communities to the public. Finally, we briefly illustrate how this work paves the way for new research opportunities in DM. △ Less","9 April, 2021",https://arxiv.org/pdf/2104.04194
"Artificial intelligence, human rights, democracy, and the rule of law: a primer",David Leslie;Christopher Burr;Mhairi Aitken;Josh Cowls;Michael Katell;Morgan Briggs,"In September 2019, the Council of Europe's Committee of Ministers adopted the terms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI). The CAHAI is charged with examining the feasibility and potential elements of a legal framework for the design, development, and deployment of AI systems that accord with Council of Europe standards across the interrelated areas of human rights, democracy, and the rule of law. As a first and necessary step in carrying out this responsibility, the CAHAI's Feasibility Study, adopted by its plenary in December 2020, has explored options for an international legal response that fills existing gaps in legislation and tailors the use of binding and non-binding legal instruments to the specific risks and opportunities presented by AI systems. The Study examines how the fundamental rights and freedoms that are already codified in international human rights law can be used as the basis for such a legal framework. The purpose of this primer is to introduce the main concepts and principles presented in the CAHAI's Feasibility Study for a general, non-technical audience. It also aims to provide some background information on the areas of AI innovation, human rights law, technology policy, and compliance mechanisms covered therein. In keeping with the Council of Europe's commitment to broad multi-stakeholder consultations, outreach, and engagement, this primer has been designed to help facilitate the meaningful and informed participation of an inclusive group of stakeholders as the CAHAI seeks feedback and guidance regarding the essential issues raised by the Feasibility Study. △ Less","2 April, 2021",https://arxiv.org/pdf/2104.04147
Progressive extension of reinforcement learning action dimension for asymmetric assembly tasks,Yuhang Gai;Jiuming Guo;Dan Wu;Ken Chen,"Reinforcement learning (RL) is always the preferred embodiment to construct the control strategy of complex tasks, like asymmetric assembly tasks. However, the convergence speed of reinforcement learning severely restricts its practical application. In this paper, the convergence is first accelerated by combining RL and compliance control. Then a completely innovative progressive extension of action dimension (PEAD) mechanism is proposed to optimize the convergence of RL algorithms. The PEAD method is verified in DDPG and PPO. The results demonstrate the PEAD method will enhance the data-efficiency and time-efficiency of RL algorithms as well as increase the stable reward, which provides more potential for the application of RL. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.04078
Transient Information Adaptation of Artificial Intelligence: Towards Sustainable Data Processes in Complex Projects,Nicholas Dacre;Fredrik Kockum;PK Senyo,"Large scale projects increasingly operate in complicated settings whilst drawing on an array of complex data-points, which require precise analysis for accurate control and interventions to mitigate possible project failure. Coupled with a growing tendency to rely on new information systems and processes in change projects, 90% of megaprojects globally fail to achieve their planned objectives. Renewed interest in the concept of Artificial Intelligence (AI) against a backdrop of disruptive technological innovations, seeks to enhance project managers cognitive capacity through the project lifecycle and enhance project excellence. However, despite growing interest there remains limited empirical insights on project managers ability to leverage AI for cognitive load enhancement in complex settings. As such this research adopts an exploratory sequential linear mixed methods approach to address unresolved empirical issues on transient adaptations of AI in complex projects, and the impact on cognitive load enhancement. Initial thematic findings from semi-structured interviews with domain experts, suggest that in order to leverage AI technologies and processes for sustainable cognitive load enhancement with complex data over time, project managers require improved knowledge and access to relevant technologies that mediate data processes in complex projects, but equally reflect application across different project phases. These initial findings support further hypothesis testing through a larger quantitative study incorporating structural equation modelling to examine the relationship between artificial intelligence and project managers cognitive load with project data in complex contexts. △ Less","18 April, 2021",https://arxiv.org/pdf/2104.04067
Quantitative Assessment of Solution Innovation in Engineering Education,V. K. Ivanov;A. G. Glebova;I. V. Obrazthov,The article discusses the quantitative assessment approach to the innovation of engineering system components. The validity of the approach is based on the expert appraisal of the university's electronic information educational environment components and the measurement of engineering solution innovation in engineering education. The implementation of batch processing of object innovation assessments is justified and described. △ Less,"26 March, 2021",https://arxiv.org/pdf/2104.04065
Co-Creating Educational Project Management Board Games to Enhance Student Engagement,Vasilis Gkogkidis;Nicholas Dacre,"Management education scholarship has long outlined the need to enhance student engagement and participation in business schools, using more innovative teaching practices. This is increasingly motivating scholars to strive for more collaborative pedagogic dynamics between teachers and students. At the same time, research into co-creation of Game Based Learning material such as board games has largely focused on the value added to games when educators involve students in the design process. However there has been scant research examining the qualities of co-creational game design exercises as teaching experiences themselves, thus overlooking the opportunity to conceptualise such activities as an innovative teaching tool that can help educators facilitate student engagement and participation. To address this research gap, this paper presents a case study where Project Management students participated in two co-creation workshops designing educational Project Management games. Data were collected conducting focus groups at the end of the two workshops. Throughout the paper we have sought to present some positive outcomes of such processes as well as some critical points that emerged through the data that were collected. Mentionable outcomes include a series of positive characteristics of co-creative Game Based Learning activities like enhanced engagement as well as a list of challenges when facilitating such activities. The main findings of this research have been organised in two frameworks, one outlining five positive characteristics of co-creative Game Based Learning activities: engagement with knowledge, knowledge assessment, creativity, communication and the second outlining challenges in facilitating such activities: lack of focus, lack of structure and the need for more practice-oriented games. △ Less","18 April, 2021",https://arxiv.org/pdf/2104.04063
Interdisciplinary Research Methodologies in Engineering Education Research,David Reynolds;Nicholas Dacre,"As Engineering Education Research (EER) develops as a discipline it is necessary for EER scholars to contribute to the development of learning theory rather than simply being informed by it. It has been suggested that to do this effectively will require partnerships between Engineering scholars and psychologists, education researchers, including other social scientists. The formation of such partnerships is particularly important when considering the introduction of business-related skills into engineering curriculum designed to prepare 21st Century Engineering Students for workplace challenges. In order to encourage scholars beyond Engineering to engage with EER, it is necessary to provide an introduction to the complexities of EER. With this aim in mind, this paper provides an outline review of what is considered rigorous research from an EER perspective as well as highlighting some of the core methodological traditions of EER. The paper aims to facilitate further discussion between EER scholars and researchers from other disciplines, ultimately leading to future collaboration on innovative and rigorous EER. △ Less","18 April, 2021",https://arxiv.org/pdf/2104.04062
ORBIT: A Real-World Few-Shot Dataset for Teachable Object Recognition,Daniela Massiceti;Luisa Zintgraf;John Bronskill;Lida Theodorou;Matthew Tobias Harris;Edward Cutrell;Cecily Morrison;Katja Hofmann;Simone Stumpf,"Object recognition has made great advances in the last decade, but predominately still relies on many high-quality training examples per object category. In contrast, learning new objects from only a few examples could enable many impactful applications from robotics to user personalization. Most few-shot learning research, however, has been driven by benchmark datasets that lack the high variation that these applications will face when deployed in the real-world. To close this gap, we present the ORBIT dataset and benchmark, grounded in the real-world application of teachable object recognizers for people who are blind/low-vision. The dataset contains 3,822 videos of 486 objects recorded by people who are blind/low-vision on their mobile phones. The benchmark reflects a realistic, highly challenging recognition problem, providing a rich playground to drive research in robustness to few-shot, high-variation conditions. We set the benchmark's first state-of-the-art and show there is massive scope for further innovation, holding the potential to impact a broad range of real-world vision applications including tools for the blind/low-vision community. We release the dataset at https://doi.org/10.25383/city.14294597 and benchmark code at https://github.com/microsoft/ORBIT-Dataset. △ Less","8 October, 2021",https://arxiv.org/pdf/2104.03841
Voluntary safety commitments provide an escape from over-regulation in AI development,The Anh Han;Tom Lenaerts;Francisco C. Santos;Luis Moniz Pereira,"With the introduction of Artificial Intelligence (AI) and related technologies in our daily lives, fear and anxiety about their misuse as well as the hidden biases in their creation have led to a demand for regulation to address such issues. Yet blindly regulating an innovation process that is not well understood, may stifle this process and reduce benefits that society may gain from the generated technology, even under the best intentions. In this paper, starting from a baseline model that captures the fundamental dynamics of a race for domain supremacy using AI technology, we demonstrate how socially unwanted outcomes may be produced when sanctioning is applied unconditionally to risk-taking, i.e. potentially unsafe, behaviours. As an alternative to resolve the detrimental effect of over-regulation, we propose a voluntary commitment approach wherein technologists have the freedom of choice between independently pursuing their course of actions or establishing binding agreements to act safely, with sanctioning of those that do not abide to what they pledged. Overall, this work reveals for the first time how voluntary commitments, with sanctions either by peers or an institution, leads to socially beneficial outcomes in all scenarios envisageable in a short-term race towards domain supremacy through AI technology. These results are directly relevant for the design of governance and regulatory policies that aim to ensure an ethical and responsible AI technology development process. △ Less","8 April, 2021",https://arxiv.org/pdf/2104.03741
Half-Duplex Attack: An Effectual Attack Modelling in D2D Communication,Misbah Shafi;Rakesh Kumar Jha,"The visualization of future generation Wireless Communication Network WCN redirects the presumption of onward innovations, the fulfillment of user demands in the form of high data rates, energy efficiency, low latency, and long-range services. To content these demands, various technologies such as massive MIMO Multiple Input Multiple Output, UDN Ultra Dense Network, spectrum sharing, D2D Device to Device communication were improvised in the next generation WCN. In comparison to previous technologies, these technologies exhibit flat architecture, the involvement of clouds in the network, centralized architecture incorporating small cells which creates vulnerable breaches initiating menaces to the security of the network. The half-duplex attack is another threat to the WCN, where the resource spoofing mechanism is attained in the downlink phase of D2D communication. Instead of triggering an attack on both uplink and downlink, solely downlink is targeted by the attacker. This scheme allows the reduced failed attempt rate of the attacker as compared to the conventional attacks. The analysis is determined on the basis of Poissons distribution to determine the probability of failed attempts of half duplex attack in contrast to a full duplex attack △ Less","8 April, 2021",https://arxiv.org/pdf/2104.03499
Co-optimising Robot Morphology and Controller in a Simulated Open-Ended Environment,Emma Hjellbrekke Stensby;Kai Olav Ellefsen;Kyrre Glette,"Designing robots by hand can be costly and time consuming, especially if the robots have to be created with novel materials, or be robust to internal or external changes. In order to create robots automatically, without the need for human intervention, it is necessary to optimise both the behaviour and the body design of the robot. However, when co-optimising the morphology and controller of a locomoting agent the morphology tends to converge prematurely, reaching a local optimum. Approaches such as explicit protection of morphological innovation have been used to reduce this problem, but it might also be possible to increase exploration of morphologies using a more indirect approach. We explore how changing the environment, where the agent locomotes, affects the convergence of morphologies. The agents' morphologies and controllers are co-optimised, while the environments the agents locomote in are evolved open-endedly with the Paired Open-Ended Trailblazer (POET). We compare the diversity, fitness and robustness of agents evolving in environments generated by POET to agents evolved in handcrafted curricula of environments. Our agents each contain of a population of individuals being evolved with a genetic algorithm. This population is called the agent-population. We show that agent-populations evolving in open-endedly evolving environments exhibit larger morphological diversity than agent-populations evolving in hand crafted curricula of environments. POET proved capable of creating a curriculum of environments which encouraged both diversity and quality in the populations. This suggests that POET may be capable of reducing premature convergence in co-optimisation of morphology and controllers. △ Less","7 April, 2021",https://arxiv.org/pdf/2104.03062
Information Bottleneck Attribution for Visual Explanations of Diagnosis and Prognosis,Ugur Demir;Ismail Irmakci;Elif Keles;Ahmet Topcu;Ziyue Xu;Concetto Spampinato;Sachin Jambawalikar;Evrim Turkbey;Baris Turkbey;Ulas Bagci,"Visual explanation methods have an important role in the prognosis of the patients where the annotated data is limited or unavailable. There have been several attempts to use gradient-based attribution methods to localize pathology from medical scans without using segmentation labels. This research direction has been impeded by the lack of robustness and reliability. These methods are highly sensitive to the network parameters. In this study, we introduce a robust visual explanation method to address this problem for medical applications. We provide an innovative visual explanation algorithm for general purpose and as an example application, we demonstrate its effectiveness for quantifying lesions in the lungs caused by the Covid-19 with high accuracy and robustness without using dense segmentation labels. This approach overcomes the drawbacks of commonly used Grad-CAM and its extended versions. The premise behind our proposed strategy is that the information flow is minimized while ensuring the classifier prediction stays similar. Our findings indicate that the bottleneck condition provides a more stable severity estimation than the similar attribution methods. △ Less","22 June, 2021",https://arxiv.org/pdf/2104.02869
Interpreting A Pre-trained Model Is A Key For Model Architecture Optimization: A Case Study On Wav2Vec 2.0,Liu Chen;Meysam Asgari,"A deep Transformer model with good evaluation score does not mean each subnetwork (a.k.a transformer block) learns reasonable representation. Diagnosing abnormal representation and avoiding it can contribute to achieving a better evaluation score. We propose an innovative perspective for analyzing attention patterns: summarize block-level patterns and assume abnormal patterns contribute negative influence. We leverage Wav2Vec 2.0 as a research target and analyze a pre-trained model's pattern. All experiments leverage Librispeech-100-clean as training data. Through avoiding diagnosed abnormal ones, our custom Wav2Vec 2.0 outperforms the original version about 4.8% absolute word error rate (WER) on test-clean with viterbi decoding. Our version is still 0.9% better when decoding with a 4-gram language model. Moreover, we identify that avoiding abnormal patterns is the main contributor for performance boosting. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.02851
Hardware-Oriented Krylov Methods for High-Performance Computing,Nils-Arne Dreier,"Krylov subspace methods are an essential building block in numerical simulation software. The efficient utilization of modern hardware is a challenging problem in the development of these methods. In this work, we develop Krylov subspace methods to solve linear systems with multiple right-hand sides, tailored to modern hardware in high-performance computing. To this end, we analyze an innovative block Krylov subspace framework that allows to balance the computational and data-transfer costs to the hardware. Based on the framework, we formulate commonly used Krylov methods. For the CG and BiCGStab methods, we introduce a novel stabilization approach as an alternative to a deflation strategy. This helps us to retain the block size, thus leading to a simpler and more efficient implementation. In addition, we optimize the methods further for distributed memory systems and the communication overhead. For the CG method, we analyze approaches to overlap the communication and computation and present multiple variants of the CG method, which differ in their communication properties. Furthermore, we present optimizations of the orthogonalization procedure in the GMRes method. Beside introducing a pipelined Gram-Schmidt variant that overlaps the global communication with the computation of inner products, we present a novel orthonormalization method based on the TSQR algorithm, which is communication-optimal and stable. For all optimized method, we present tests that show their superiority in a distributed setting. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.02494
SimMBM Channel Simulator for Media-Based Modulation Systems,Zehra Yigit;Ertugrul Basar;Ibrahim Altunbas,"Media-based modulation (MBM), exploiting rich scattering properties of transmission environments via different radiation patterns of a single reconfigurable antenna (RA), has brought new insights into future communication systems. In this study, considering this innovative transmission principle, we introduce the realistic, two-dimensional (2D), and open-source SimMBM channel simulator to support various applications of MBM systems at sub-6 GHz frequency bands in different environments. △ Less","16 November, 2021",https://arxiv.org/pdf/2104.02336
IoT Security: Botnet detection in IoT using Machine learning,Satish Pokhrel;Robert Abbas;Bhulok Aryal,"The acceptance of Internet of Things (IoT) applications and services has seen an enormous rise of interest in IoT. Organizations have begun to create various IoT based gadgets ranging from small personal devices such as a smart watch to a whole network of smart grid, smart mining, smart manufacturing, and autonomous driver-less vehicles. The overwhelming amount and ubiquitous presence have attracted potential hackers for cyber-attacks and data theft. Security is considered as one of the prominent challenges in IoT. The key scope of this research work is to propose an innovative model using machine learning algorithm to detect and mitigate botnet-based distributed denial of service (DDoS) attack in IoT network. Our proposed model tackles the security issue concerning the threats from bots. Different machine learning algorithms such as K- Nearest Neighbour (KNN), Naive Bayes model and Multi-layer Perception Artificial Neural Network (MLP ANN) were used to develop a model where data are trained by BoT-IoT dataset. The best algorithm was selected by a reference point based on accuracy percentage and area under the receiver operating characteristics curve (ROC AUC) score. Feature engineering and Synthetic minority oversampling technique (SMOTE) were combined with machine learning algorithms (MLAs). Performance comparison of three algorithms used was done in class imbalance dataset and on the class balanced dataset. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.02231
Designing Efficient and High-performance AI Accelerators with Customized STT-MRAM,Kaniz Mishty;Mehdi Sadi,"In this paper, we demonstrate the design of efficient and high-performance AI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable core. Based on model-driven detailed design space exploration, we present the design methodology of an innovative scratchpad-assisted on-chip STT-MRAM based buffer system for high-performance accelerators. Using analytically derived expression of memory occupancy time of AI model weights and activation maps, the volatility of STT-MRAM is adjusted with process and temperature variation aware scaling of thermal stability factor to optimize the retention time, energy, read/write latency, and area of STT-MRAM. From the analysis of modern AI workloads and accelerator implementation in 14nm technology, we verify the efficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an SRAM-based implementation, the STT-AI accelerator achieves 75% area and 3% power savings at iso-accuracy. Furthermore, with a relaxed bit error rate and negligible AI accuracy trade-off, the designed STT-AI Ultra accelerator achieves 75.4%, and 3.5% savings in area and power, respectively over regular SRAM-based accelerators. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.02199
"Uniting Heterogeneity, Inductiveness, and Efficiency for Graph Representation Learning",Tong Chen;Hongzhi Yin;Jie Ren;Zi Huang;Xiangliang Zhang;Hao Wang,"With the ubiquitous graph-structured data in various applications, models that can learn compact but expressive vector representations of nodes have become highly desirable. Recently, bearing the message passing paradigm, graph neural networks (GNNs) have greatly advanced the performance of node representation learning on graphs. However, a majority class of GNNs are only designed for homogeneous graphs, leading to inferior adaptivity to the more informative heterogeneous graphs with various types of nodes and edges. Also, despite the necessity of inductively producing representations for completely new nodes (e.g., in streaming scenarios), few heterogeneous GNNs can bypass the transductive learning scheme where all nodes must be known during training. Furthermore, the training efficiency of most heterogeneous GNNs has been hindered by their sophisticated designs for extracting the semantics associated with each meta path or relation. In this paper, we propose WIde and DEep message passing Network (WIDEN) to cope with the aforementioned problems about heterogeneity, inductiveness, and efficiency that are rarely investigated together in graph representation learning. In WIDEN, we propose a novel inductive, meta path-free message passing scheme that packs up heterogeneous node features with their associated edges from both low- and high-order neighbor nodes. To further improve the training efficiency, we innovatively present an active downsampling strategy that drops unimportant neighbor nodes to facilitate faster information propagation. Experiments on three real-world heterogeneous graphs have further validated the efficacy of WIDEN on both transductive and inductive node representation learning, as well as the superior training efficiency against state-of-the-art baselines. △ Less","11 April, 2021",https://arxiv.org/pdf/2104.01711
Curating China's Cultural Revolution (1966-1976): CR/10 as a Warburgian Memory Atlas and Digital Humanities Interface,Rongqian Ma,"CR/10 is a digital oral history platform that aims to collect and preserve the cultural memories of China's Cultural Revolution (1966-1976). This paper discusses how CR/10 functions as a Warburgian memory atlas and shapes multifaceted narratives of the historical incident. Through ethnographic research and semi-structured interviews with users within and outside academia, I examined the usability of CR/10 among various user groups and proposed design opportunities to further empower the interface. This paper offered a strong case on the datafication of cultural memories among cultural heritage institutions and contributed to digital archiving scholarship with an innovative methodological lens. △ Less","4 April, 2021",https://arxiv.org/pdf/2104.01663
Learning the CSI Recovery in FDD Systems,Wolfgang Utschick;Valentina Rizzello;Michael Joham;Zhengxiang Ma;Leonard Piazzi,"We propose an innovative machine learning-based technique to address the problem of channel acquisition at the base station in frequency division duplex systems. In this context, the base station reconstructs the full channel state information in the downlink frequency range based on limited downlink channel state information feedback from the mobile terminal. The channel state information recovery is based on a convolutional neural network which is trained exclusively on collected channel state samples acquired in the uplink frequency domain. No acquisition of training samples in the downlink frequency range is required at all. Finally, after a detailed presentation and analysis of the proposed technique and its performance, the ""transfer learning'' assumption of the convolutional neural network that is central to the proposed approach is validated with an analysis based on the maximum mean discrepancy metric. △ Less","3 October, 2021",https://arxiv.org/pdf/2104.01322
Topic Scaling: A Joint Document Scaling -- Topic Model Approach To Learn Time-Specific Topics,Sami Diaf;Ulrich Fritsche,"This paper proposes a new methodology to study sequential corpora by implementing a two-stage algorithm that learns time-based topics with respect to a scale of document positions and introduces the concept of Topic Scaling which ranks learned topics within the same document scale. The first stage ranks documents using Wordfish, a Poisson-based document scaling method, to estimate document positions that serve, in the second stage, as a dependent variable to learn relevant topics via a supervised Latent Dirichlet Allocation. This novelty brings two innovations in text mining as it explains document positions, whose scale is a latent variable, and ranks the inferred topics on the document scale to match their occurrences within the corpus and track their evolution. Tested on the U.S. State Of The Union two-party addresses, this inductive approach reveals that each party dominates one end of the learned scale with interchangeable transitions that follow the parties' term of office. Besides a demonstrated high accuracy in predicting in-sample documents' positions from topic scores, this method reveals further hidden topics that differentiate similar documents by increasing the number of learned topics to unfold potential nested hierarchical topic structures. Compared to other popular topic models, Topic Scaling learns topics with respect to document similarities without specifying a time frequency to learn topic evolution, thus capturing broader topic patterns than dynamic topic models and yielding more interpretable outputs than a plain latent Dirichlet allocation. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.01117
Drug Recommendation System based on Sentiment Analysis of Drug Reviews using Machine Learning,Satvik Garg,"Since coronavirus has shown up, inaccessibility of legitimate clinical resources is at its peak, like the shortage of specialists, healthcare workers, lack of proper equipment and medicines. The entire medical fraternity is in distress, which results in numerous individuals demise. Due to unavailability, people started taking medication independently without appropriate consultation, making the health condition worse than usual. As of late, machine learning has been valuable in numerous applications, and there is an increase in innovative work for automation. This paper intends to present a drug recommender system that can drastically reduce specialists heap. In this research, we build a medicine recommendation system that uses patient reviews to predict the sentiment using various vectorization processes like Bow, TFIDF, Word2Vec, and Manual Feature Analysis, which can help recommend the top drug for a given disease by different classification algorithms. The predicted sentiments were evaluated by precision, recall, f1score, accuracy, and AUC score. The results show that classifier LinearSVC using TFIDF vectorization outperforms all other models with 93% accuracy. △ Less","4 April, 2021",https://arxiv.org/pdf/2104.01113
Visualizing computation in large-scale cellular automata,Hugo Cisneros;Josef Sivic;Tomas Mikolov,"Emergent processes in complex systems such as cellular automata can perform computations of increasing complexity, and could possibly lead to artificial evolution. Such a feat would require scaling up current simulation sizes to allow for enough computational capacity. Understanding complex computations happening in cellular automata and other systems capable of emergence poses many challenges, especially in large-scale systems. We propose methods for coarse-graining cellular automata based on frequency analysis of cell states, clustering and autoencoders. These innovative techniques facilitate the discovery of large-scale structure formation and complexity analysis in those systems. They emphasize interesting behaviors in elementary cellular automata while filtering out background patterns. Moreover, our methods reduce large 2D automata to smaller sizes and enable identifying systems that behave interestingly at multiple scales. △ Less","1 April, 2021",https://arxiv.org/pdf/2104.01008
Fast-adapting and Privacy-preserving Federated Recommender System,Qinyong Wang;Hongzhi Yin;Tong Chen;Junliang Yu;Alexander Zhou;Xiangliang Zhang,"In the mobile Internet era, the recommender system has become an irreplaceable tool to help users discover useful items, and thus alleviating the information overload problem. Recent deep neural network (DNN)-based recommender system research have made significant progress in improving prediction accuracy, which is largely attributed to the access to a large amount of users' personal data collected from users' devices and then centrally stored in the cloud server. However, as there are rising concerns around the globe on user privacy leakage in the online platform, the public is becoming anxious by such abuse of user privacy. Therefore, it is urgent and beneficial to develop a recommender system that can achieve both high prediction accuracy and high degree of user privacy protection. To this end, we propose a DNN-based recommendation model called PrivRec running on the decentralized federated learning (FL) environment, which ensures that a user's data never leaves his/her during the course of model training. On the other hand, to better embrace the data heterogeneity commonly existing in FL, we innovatively introduce a first-order meta-learning method that enables fast in-device personalization with only few data points. Furthermore, to defense from potential malicious participant that poses serious security threat to other users, we develop a user-level differentially private DP-PrivRec model so that it is unable to determine whether a particular user is present or not solely based on the trained model. Finally, we conduct extensive experiments on two large-scale datasets in a simulated FL environment, and the results validate the superiority of our proposed PrivRec and DP-PrivRec. △ Less","11 September, 2021",https://arxiv.org/pdf/2104.00919
Reservoir-Based Distributed Machine Learning for Edge Operation,Silvija Kokalj-Filipovic;Paul Toliver;William Johnson;Rob Miller,"We introduce a novel design for in-situ training of machine learning algorithms built into smart sensors, and illustrate distributed training scenarios using radio frequency (RF) spectrum sensors. Current RF sensors at the Edge lack the computational resources to support practical, in-situ training for intelligent signal classification. We propose a solution using Deepdelay Loop Reservoir Computing (DLR), a processing architecture that supports machine learning algorithms on resource-constrained edge-devices by leveraging delayloop reservoir computing in combination with innovative hardware. DLR delivers reductions in form factor, hardware complexity and latency, compared to the State-ofthe- Art (SoA) neural nets. We demonstrate DLR for two applications: RF Specific Emitter Identification (SEI) and wireless protocol recognition. DLR enables mobile edge platforms to authenticate and then track emitters with fast SEI retraining. Once delay loops separate the data classes, traditionally complex, power-hungry classification models are no longer needed for the learning process. Yet, even with simple classifiers such as Ridge Regression (RR), the complexity grows at least quadratically with the input size. DLR with a RR classifier exceeds the SoA accuracy, while further reducing power consumption by leveraging the architecture of parallel (split) loops. To authenticate mobile devices across large regions, DLR can be trained in a distributed fashion with very little additional processing and a small communication cost, all while maintaining accuracy. We illustrate how to merge locally trained DLR classifiers in use cases of interest. △ Less","1 April, 2021",https://arxiv.org/pdf/2104.00751
Fusing RGBD Tracking and Segmentation Tree Sampling for Multi-Hypothesis Volumetric Segmentation,Andrew Price;Kun Huang;Dmitry Berenson,"Despite rapid progress in scene segmentation in recent years, 3D segmentation methods are still limited when there is severe occlusion. The key challenge is estimating the segment boundaries of (partially) occluded objects, which are inherently ambiguous when considering only a single frame. In this work, we propose Multihypothesis Segmentation Tracking (MST), a novel method for volumetric segmentation in changing scenes, which allows scene ambiguity to be tracked and our estimates to be adjusted over time as we interact with the scene. Two main innovations allow us to tackle this difficult problem: 1) A novel way to sample possible segmentations from a segmentation tree; and 2) A novel approach to fusing tracking results with multiple segmentation estimates. These methods allow MST to track the segmentation state over time and incorporate new information, such as new objects being revealed. We evaluate our method on several cluttered tabletop environments in simulation and reality. Our results show that MST outperforms baselines in all tested scenes. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00205
Trajectory Tracking of Underactuated Sea Vessels With Uncertain Dynamics: An Integral Reinforcement Learning Approach,Mohammed Abouheaf;Wail Gueaieb;Md. Suruz Miah;Davide Spinello,"Underactuated systems like sea vessels have degrees of motion that are insufficiently matched by a set of independent actuation forces. In addition, the underlying trajectory-tracking control problems grow in complexity in order to decide the optimal rudder and thrust control signals. This enforces several difficult-to-solve constraints that are associated with the error dynamical equations using classical optimal tracking and adaptive control approaches. An online machine learning mechanism based on integral reinforcement learning is proposed to find a solution for a class of nonlinear tracking problems with partial prior knowledge of the system dynamics. The actuation forces are decided using innovative forms of temporal difference equations relevant to the vessel's surge and angular velocities. The solution is implemented using an online value iteration process which is realized by employing means of the adaptive critics and gradient descent approaches. The adaptive learning mechanism exhibited well-functioning and interactive features in react to different desired reference-tracking scenarios. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00190
Taking Stock of the Present and Future of Smart Technologies for Older Adults and Caregivers,Christina N. Harrington;Ben Jelen;Amanda Lazar;Aqueasha Martin-Hammond;Alisha Pradhan;Blaine Reeder;Katie Siek,"Technology has the opportunity to assist older adults as they age in place, coordinate caregiving resources, and meet unmet needs through access to resources. Currently, older adults use consumer technologies to support everyday life, however these technologies are not always accessible or as useful as they can be. Indeed, industry has attempted to create smart home technologies with older adults as a target user group, however these solutions are often more focused on the technical aspects and are short lived. In this paper, we advocate for older adults being involved in the design process - from initial ideation to product development to deployment. We encourage federally funded researchers and industry to create compensated, diverse older adult advisory boards to address stereotypes about aging while ensuring their needs are considered. We envision artificial intelligence systems that augment resources instead of replacing them - especially in under-resourced communities. Older adults rely on their caregiver networks and community organizations for social, emotional, and physical support; thus, AI should be used to coordinate resources better and lower the burden of connecting with these resources. Although sociotechnical smart systems can help identify needs of older adults, the lack of affordable research infrastructure and translation of findings into consumer technology perpetuates inequities in designing for diverse older adults. In addition, there is a disconnect between the creation of smart sensing systems and creating understandable, actionable data for older adults and caregivers to utilize. We ultimately advocate for a well-coordinated research effort across the United States that connects older adults, caregivers, community organizations, and researchers together to catalyze innovative and practical research for all stakeholders. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00096
NetAdaptV2: Efficient Neural Architecture Search with Fast Super-Network Training and Architecture Optimization,Tien-Ju Yang;Yi-Lun Liao;Vivienne Sze,"Neural architecture search (NAS) typically consists of three main steps: training a super-network, training and evaluating sampled deep neural networks (DNNs), and training the discovered DNN. Most of the existing efforts speed up some steps at the cost of a significant slowdown of other steps or sacrificing the support of non-differentiable search metrics. The unbalanced reduction in the time spent per step limits the total search time reduction, and the inability to support non-differentiable search metrics limits the performance of discovered DNNs. In this paper, we present NetAdaptV2 with three innovations to better balance the time spent for each step while supporting non-differentiable search metrics. First, we propose channel-level bypass connections that merge network depth and layer width into a single search dimension to reduce the time for training and evaluating sampled DNNs. Second, ordered dropout is proposed to train multiple DNNs in a single forward-backward pass to decrease the time for training a super-network. Third, we propose the multi-layer coordinate descent optimizer that considers the interplay of multiple layers in each iteration of optimization to improve the performance of discovered DNNs while supporting non-differentiable search metrics. With these innovations, NetAdaptV2 reduces the total search time by up to 5.8\times on ImageNet and 2.4\times on NYU Depth V2, respectively, and discovers DNNs with better accuracy-latency/accuracy-MAC trade-offs than state-of-the-art NAS works. Moreover, the discovered DNN outperforms NAS-discovered MobileNetV3 by 1.8% higher top-1 accuracy with the same latency. The project website is http://netadapt.mit.edu. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00031
Co-Adaptation of Algorithmic and Implementational Innovations in Inference-based Deep Reinforcement Learning,Hiroki Furuta;Tadashi Kozuno;Tatsuya Matsushima;Yutaka Matsuo;Shixiang Shane Gu,"Recently many algorithms were devised for reinforcement learning (RL) with function approximation. While they have clear algorithmic distinctions, they also have many implementation differences that are algorithm-independent and sometimes under-emphasized. Such mixing of algorithmic novelty and implementation craftsmanship makes rigorous analyses of the sources of performance improvements across algorithms difficult. In this work, we focus on a series of off-policy inference-based actor-critic algorithms -- MPO, AWR, and SAC -- to decouple their algorithmic innovations and implementation decisions. We present unified derivations through a single control-as-inference objective, where we can categorize each algorithm as based on either Expectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence minimization and treat the rest of specifications as implementation details. We performed extensive ablation studies, and identified substantial performance drops whenever implementation details are mismatched for algorithmic choices. These results show which implementation or code details are co-adapted and co-evolved with algorithms, and which are transferable across algorithms: as examples, we identified that tanh Gaussian policy and network sizes are highly adapted to algorithmic types, while layer normalization and ELU are critical for MPO's performances but also transfer to noticeable gains in SAC. We hope our work can inspire future work to further demystify sources of performance improvements across multiple algorithms and allow researchers to build on one another's both algorithmic and implementational innovations. △ Less","25 October, 2021",https://arxiv.org/pdf/2103.17258
Enhancing human bodies with extra robotic arms and fingers: The Neural Resource Allocation Problem,Giulia Dominijanni;Solaiman Shokur;Gionata Salvietti;Sarah Buehler;Erica Palmerini;Simone Rossi;Frederique De Vignemont;Andrea D'Avella;Tamar R. Makin;Domenico Prattichizzo;Silvestro Micera,"The emergence of robot-based body augmentation promises exciting innovations that will inform robotics, human-machine interaction, and wearable electronics. Even though augmentative devices like extra robotic arms and fingers in many ways build on restorative technologies, they introduce unique challenges for bidirectional human-machine collaboration. Can humans adapt and learn to operate a new limb collaboratively with their biological limbs without sacrificing their physical abilities? To successfully achieve robotic body augmentation, we need to ensure that by giving a person an additional (artificial) limb, we are not in fact trading off an existing (biological) one. In this manuscript, we introduce the ""Neural Resource Allocation"" problem, which distinguishes body augmentation from existing robotics paradigms such as teleoperation and prosthetics. We discuss how to allow the effective and effortless voluntary control of augmentative devices without compromising the voluntary control of the biological body. In reviewing the relevant literature on extra robotic fingers and limbs we critically assess the range of potential solutions available for the ""Neural Resource Allocation"" problem. For this purpose, we combine multiple perspectives from engineering and neuroscience with considerations from human-machine interaction, sensory-motor integration, ethics and law. Altogether we aim to define common foundations and operating principles for the successful implementation of motor augmentation. △ Less","31 March, 2021",https://arxiv.org/pdf/2103.17252
iCurb: Imitation Learning-based Detection of Road Curbs using Aerial Images for Autonomous Driving,Zhenhua Xu;Yuxiang Sun;Ming Liu,"Detection of road curbs is an essential capability for autonomous driving. It can be used for autonomous vehicles to determine drivable areas on roads. Usually, road curbs are detected on-line using vehicle-mounted sensors, such as video cameras and 3-D Lidars. However, on-line detection using video cameras may suffer from challenging illumination conditions, and Lidar-based approaches may be difficult to detect far-away road curbs due to the sparsity issue of point clouds. In recent years, aerial images are becoming more and more worldwide available. We find that the visual appearances between road areas and off-road areas are usually different in aerial images, so we propose a novel solution to detect road curbs off-line using aerial images. The input to our method is an aerial image, and the output is directly a graph (i.e., vertices and edges) representing road curbs. To this end, we formulate the problem as an imitation learning problem, and design a novel network and an innovative training strategy to train an agent to iteratively find the road-curb graph. The experimental results on a public dataset confirm the effectiveness and superiority of our method. This work is accompanied with a demonstration video and a supplementary document at https://tonyxuqaq.github.io/iCurb/. △ Less","31 March, 2021",https://arxiv.org/pdf/2103.17118
PAUL: Procrustean Autoencoder for Unsupervised Lifting,Chaoyang Wang;Simon Lucey,"Recent success in casting Non-rigid Structure from Motion (NRSfM) as an unsupervised deep learning problem has raised fundamental questions about what novelty in NRSfM prior could the deep learning offer. In this paper we advocate for a 3D deep auto-encoder framework to be used explicitly as the NRSfM prior. The framework is unique as: (i) it learns the 3D auto-encoder weights solely from 2D projected measurements, and (ii) it is Procrustean in that it jointly resolves the unknown rigid pose for each shape instance. We refer to this architecture as a Procustean Autoencoder for Unsupervised Lifting (PAUL), and demonstrate state-of-the-art performance across a number of benchmarks in comparison to recent innovations such as Deep NRSfM and C3PDO. △ Less","30 March, 2021",https://arxiv.org/pdf/2103.16773
Computational Model to Quantify Object Innovativeness,V. K. Ivanov,"The article considers the quantitative assessment approach to the innovativeness of different objects. The proposed assessment model is based on the object data retrieval from various databases including the Internet. We present an object linguistic model, the processing technique for the measurement results including the results retrieved from the different search engines, and the evaluating technique of the source credibility. Empirical research of the computational model adequacy includes the acquisition and preprocessing of patent data from different databases and the computation of invention innovativeness values: their novelty and relevance. The experiment results, namely the comparative assessments of innovativeness values and major trends, show the models developed are sufficiently adequate and can be used in further research. △ Less","26 March, 2021",https://arxiv.org/pdf/2103.16504
Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn Chatbot Responding with Intention,Hsuan Su;Jiun-Hao Jhan;Fan-yun Sun;Saurav Sahay;Hung-yi Lee,"Most chatbot literature that focuses on improving the fluency and coherence of a chatbot, is dedicated to making chatbots more human-like. However, very little work delves into what really separates humans from chatbots -- humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative framework to train chatbots to possess human-like intentions. Our framework includes a guiding chatbot and an interlocutor model that plays the role of humans. The guiding chatbot is assigned an intention and learns to induce the interlocutor to reply with responses matching the intention, for example, long responses, joyful responses, responses with specific words, etc. We examined our framework using three experimental setups and evaluated the guiding chatbot with four different metrics to demonstrate flexibility and performance advantages. Additionally, we performed trials with human interlocutors to substantiate the guiding chatbot's effectiveness in influencing the responses of humans to a certain extent. Code will be made available to the public. △ Less","23 April, 2021",https://arxiv.org/pdf/2103.16429
Rethinking Spatial Dimensions of Vision Transformers,Byeongho Heo;Sangdoo Yun;Dongyoon Han;Sanghyuk Chun;Junsuk Choe;Seong Joon Oh,"Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit △ Less","17 August, 2021",https://arxiv.org/pdf/2103.16302
Stakeholders interdependencies and their role in sustainable business model innovation,Iqra Sadaf Khan;Jukka Majava,"Sustainable innovation requires in-time development, diversification and transformation of business models from one to another. Business model innovation, development and transformation for sustainability incorporates economic, environmental and social value by advancing the management of the stakeholders into the business model. Except a little research on business model inter-dependencies, scant research has been done on stakeholders inter-dependencies in order to understand their nature and relationship while developing or transforming a business model and creating an impact on environment, society and economy. Therefore, current research uses actor dependency model to analyze four different kind of inter-dependencies, namely, goal-dependency, task-dependency, resource-dependency and soft-goal dependency. The ecology of business model experimentation map is used as a tool for practical understanding of sustainable business modelling with a multi-actor approach in a workshop setting. The findings will help to understand how stakeholders depend on each other while developing a business model for sustainability and innovation. △ Less","30 March, 2021",https://arxiv.org/pdf/2103.16281
DeepWORD: A GCN-based Approach for Owner-Member Relationship Detection in Autonomous Driving,Zizhang Wu;Man Wang;Jason Wang;Wenkai Zhang;Muqing Fang;Tianhao Xu,"It's worth noting that the owner-member relationship between wheels and vehicles has an significant contribution to the 3D perception of vehicles, especially in the embedded environment. However, there are currently two main challenges about the above relationship prediction: i) The traditional heuristic methods based on IoU can hardly deal with the traffic jam scenarios for the occlusion. ii) It is difficult to establish an efficient applicable solution for the vehicle-mounted system. To address these issues, we propose an innovative relationship prediction method, namely DeepWORD, by designing a graph convolution network (GCN). Specifically, we utilize the feature maps with local correlation as the input of nodes to improve the information richness. Besides, we introduce the graph attention network (GAT) to dynamically amend the prior estimation deviation. Furthermore, we establish an annotated owner-member relationship dataset called WORD as a large-scale benchmark, which will be available soon. The experiments demonstrate that our solution achieves state-of-the-art accuracy and real-time in practice. △ Less","20 April, 2021",https://arxiv.org/pdf/2103.16099
"Cognitive networks identify the content of English and Italian popular posts about COVID-19 vaccines: Anticipation, logistics, conspiracy and loss of trust",Massimo Stella;Michael S. Vitevitch;Federico Botta,"Monitoring social discourse about COVID-19 vaccines is key to understanding how large populations perceive vaccination campaigns. We focus on 4765 unique popular tweets in English or Italian about COVID-19 vaccines between 12/2020 and 03/2021. One popular English tweet was liked up to 495,000 times, stressing how popular tweets affected cognitively massive populations. We investigate both text and multimedia in tweets, building a knowledge graph of syntactic/semantic associations in messages including visual features and indicating how online users framed social discourse mostly around the logistics of vaccine distribution. The English semantic frame of ""vaccine"" was highly polarised between trust/anticipation (towards the vaccine as a scientific asset saving lives) and anger/sadness (mentioning critical issues with dose administering). Semantic associations with ""vaccine,"" ""hoax"" and conspiratorial jargon indicated the persistence of conspiracy theories and vaccines in massively read English posts (absent in Italian messages). The image analysis found that popular tweets with images of people wearing face masks used language lacking the trust and joy found in tweets showing people with no masks, indicating a negative affect attributed to face covering in social discourse. A behavioural analysis revealed a tendency for users to share content eliciting joy, sadness and disgust and to like less sad messages, highlighting an interplay between emotions and content diffusion beyond sentiment. With the AstraZeneca vaccine being suspended in mid March 2021, ""Astrazeneca"" was associated with trustful language driven by experts, but popular Italian tweets framed ""vaccine"" by crucially replacing earlier levels of trust with deep sadness. Our results stress how cognitive networks and innovative multimedia processing open new ways for reconstructing online perceptions about vaccines and trust. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.15909
Some Results of Experimental Check of The Model of the Object Innovativeness Quantitative Evaluation,V. K. Ivanov,The paper presents the results of the experiments that were conducted to confirm the main ideas of the proposed approach to determining the objects innovativeness. This approach assumed that the product life cycle of whose descriptions are placed in different data warehouses is adequate. The proposed formal model allows us to calculate the quantitative value of the additive evaluation criterion of objects innovativeness. The obtained experimental data make it possible to evaluate the adopted approach correctness. △ Less,"27 March, 2021",https://arxiv.org/pdf/2103.15816
Experimental check of model of object innovation evaluation,V. K. Ivanov,"The article discusses the approach for evaluating the innovation index of the products and technologies. The evaluation results can be used to create a warehouse of the object descriptions with significant innovation potential. The model of innovation index computation is based on the concepts of novelty, relevance, and implementability of the object. Formal definitions of these indicators are given and a methodology for their calculation are described. The fuzzy methods to coprocess (incomplete) data from numerous sources and to obtain probabilistic innovation assessments are used. The experimental data of the model check including the calculations of local criteria and global additive evaluation criterion are presented. The cyclical nature of dynamic changes in indicators, their interdependence was established, some general features of the products promotion were found. The obtained experimental data are consistent with expert estimates of the products under study. The analysis of the local criteria used in the research gives grounds to assert the correct use of the additive n-dimensional utility function. The adequacy of assumptions and formal expressions that are used in computational algorithms for selection information for data warehouse is confirmed. △ Less","27 March, 2021",https://arxiv.org/pdf/2103.15815
SCNN: Swarm Characteristic Neural Network,Ha-Thanh Nguyen;Le-Minh Nguyen,"Deep learning is a powerful approach with good performance on many different tasks. However, these models often require massive computational resources. It is a worrying trend that we increasingly need models that work well on more complex problems. In this paper, we propose and verify the effectiveness and efficiency of SCNN, an innovative neural network inspired by the swarm concept. In addition to introducing the relevant theories, our detailed experiments suggest that fewer parameters may perform better than models with more parameters. Besides, our experiments show that SCNN needs less data than traditional models. That could be an essential hint for problems where there is not much data. △ Less","7 March, 2021",https://arxiv.org/pdf/2103.15550
"Exploring, browsing and interacting with multi-scale structures of knowledge",Quentin Lobbé;Alexandre Delanoë;David Chavalarias,"The ICT revolution has given birth to a world of digital traces. A wide number of knowledgedriven domains like science are daily fueled by unlimited flows of textual contents. In order to navigate across these growing constellations of words, interdisciplinary innovations are emerging at the crossroad between social and computational sciences. In particular, complex systems approaches make it now possible to reconstruct multi-level and multi-scale structures of knowledge by means of phylomemies: inheritance networks of elements of knowledge. In this article, we will introduce an endogenous way to visualize the outcomes of the phylomemy reconstruction process by combining both synchronic and diachronic approaches. Our aim is to translate high-dimensional phylomemetic networks into graphical projections and interactive visualizations. To that end, we will use seabed and kinship views to translate the multilevel and multi-scale properties of complex branches of knowledge. We will then define a generic macro-to-micro methodology of exploration implemented within an open source software called Memiescape and validate our approach by browsing through the reconstructed histories of thousands of scientific publications and clinical trials. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.15448
LayoutParser: A Unified Toolkit for Deep Learning Based Document Image Analysis,Zejiang Shen;Ruochen Zhang;Melissa Dell;Benjamin Charles Germain Lee;Jacob Carlson;Weining Li,"Recent advances in document image analysis (DIA) have been primarily driven by the application of neural networks. Ideally, research outcomes could be easily deployed in production and extended for further investigation. However, various factors like loosely organized codebases and sophisticated model configurations complicate the easy reuse of important innovations by a wide audience. Though there have been on-going efforts to improve reusability and simplify deep learning (DL) model development in disciplines like natural language processing and computer vision, none of them are optimized for challenges in the domain of DIA. This represents a major gap in the existing toolkit, as DIA is central to academic research across a wide range of disciplines in the social sciences and humanities. This paper introduces layoutparser, an open-source library for streamlining the usage of DL in DIA research and applications. The core layoutparser library comes with a set of simple and intuitive interfaces for applying and customizing DL models for layout detection, character recognition, and many other document processing tasks. To promote extensibility, layoutparser also incorporates a community platform for sharing both pre-trained models and full document digitization pipelines. We demonstrate that layoutparser is helpful for both lightweight and large-scale digitization pipelines in real-word use cases. The library is publicly available at https://layout-parser.github.io/. △ Less","21 June, 2021",https://arxiv.org/pdf/2103.15348
Holographic photonic neuron,Vincent R. Daria,"The promise of artificial intelligence (AI) to process complex datasets has brought about innovative computing paradigms. While recent developments in quantum-photonic computing have reached significant feats, mimicking our brain's ability to recognize images are poorly integrated in these ventures. Here, I incorporate orbital angular momentum (OAM) states in a classical Vander Lugt optical correlator to create the holographic photonic neuron. The photonic neuron can memorize an array of matched filters in a phase-hologram, which is derived by linking OAM states with elements in the array. Successful correlation is independent of intensity and yields photons with OAM states, which can be used as a transmission protocol or qudits for quantum computing. The unique OAM identifier establishes the photonic neuron as a fundamental AI device for pattern recognition that can be scaled and integrated with other computing platforms to build-up a neuromorphic quantum-photonic processor that mimics the brain. △ Less","28 March, 2021",https://arxiv.org/pdf/2103.15272
Game Theory Based Privacy Preserving Approach for Collaborative Deep Learning in IoT,Deepti Gupta;Smriti Bhatt;Paras Bhatt;Maanak Gupta;Ali Saman Tosun,"The exponential growth of Internet of Things (IoT) has become a transcending force in creating innovative smart devices and connected domains including smart homes, healthcare, transportation and manufacturing. With billions of IoT devices, there is a huge amount of data continuously being generated, transmitted, and stored at various points in the IoT architecture. Deep learning is widely being used in IoT applications to extract useful insights from IoT data. However, IoT users have security and privacy concerns and prefer not to share their personal data with third party applications or stakeholders. In order to address user privacy concerns, Collaborative Deep Learning (CDL) has been largely employed in data-driven applications which enables multiple IoT devices to train their models locally on edge gateways. In this chapter, we first discuss different types of deep learning approaches and how these approaches can be employed in the IoT domain. We present a privacy-preserving collaborative deep learning approach for IoT devices which can achieve benefits from other devices in the system. This learning approach is analyzed from the behavioral perspective of mobile edge devices using a game-theoretic model. We analyze the Nash Equilibrium in N-player static game model. We further present a novel fair collaboration strategy among edge IoT devices using cluster based approach to solve the CDL game, which enforces mobile edge devices for cooperation. We also present implementation details and evaluation analysis in a real-world smart home deployment. △ Less","3 April, 2021",https://arxiv.org/pdf/2103.15245
Peculiarities of organization of data storage based on intelligent search agent and evolutionary model selection the target information,V. K. Ivanov,"The article presents a systematic review of the results of the development of the theoretical basis and the pilot implementation of data storage technology with automatic replenishment of data from sources belonging to different thematic segments. It is expected that the repository will contain information about objects with significant innovative potential. The mechanism of selection of such information is based on the determination of its semantic relevance to the generated search queries. At the same time, a quantitative assessment of the innovation of objects, in particular their technological novelty and demand is given. The article describes the accepted indicators of innovation, discusses the application of the theory of evidence for the processing of incomplete and fuzzy information, identifies the main ideas of the method of processing the results of measurements for the calculation of the probabilistic value of the components of innovation, briefly describes the application of the evolutionary approach in the formation of the linguistic model of the archetype of the object, provides information about the experimental verification of the adequacy of the developed computational model. The research results that are described in the article can be used for business planning, forecasting of technological development, information support of investment projects expertise. △ Less","27 March, 2021",https://arxiv.org/pdf/2103.14837
CalibDNN: Multimodal Sensor Calibration for Perception Using Deep Neural Networks,Ganning Zhao;Jiesi Hu;Suya You;C. -C. Jay Kuo,"Current perception systems often carry multimodal imagers and sensors such as 2D cameras and 3D LiDAR sensors. To fuse and utilize the data for downstream perception tasks, robust and accurate calibration of the multimodal sensor data is essential. We propose a novel deep learning-driven technique (CalibDNN) for accurate calibration among multimodal sensor, specifically LiDAR-Camera pairs. The key innovation of the proposed work is that it does not require any specific calibration targets or hardware assistants, and the entire processing is fully automatic with a single model and single iteration. Results comparison among different methods and extensive experiments on different datasets demonstrates the state-of-the-art performance. △ Less","26 March, 2021",https://arxiv.org/pdf/2103.14793
Multi-Disease Detection in Retinal Imaging based on Ensembling Heterogeneous Deep Learning Models,Dominik Müller;Iñaki Soto-Rey;Frank Kramer,"Preventable or undiagnosed visual impairment and blindness affect billion of people worldwide. Automated multi-disease detection models offer great potential to address this problem via clinical decision support in diagnosis. In this work, we proposed an innovative multi-disease detection pipeline for retinal imaging which utilizes ensemble learning to combine the predictive capabilities of several heterogeneous deep convolutional neural network models. Our pipeline includes state-of-the-art strategies like transfer learning, class weighting, real-time image augmentation and Focal loss utilization. Furthermore, we integrated ensemble learning techniques like heterogeneous deep learning models, bagging via 5-fold cross-validation and stacked logistic regression models. Through internal and external evaluation, we were able to validate and demonstrate high accuracy and reliability of our pipeline, as well as the comparability with other state-of-the-art pipelines for retinal disease prediction. △ Less","26 March, 2021",https://arxiv.org/pdf/2103.14660
OTA: Optimal Transport Assignment for Object Detection,Zheng Ge;Songtao Liu;Zeming Li;Osamu Yoshie;Jian Sun,"Recent advances in label assignment in object detection mainly seek to independently define positive/negative training samples for each ground-truth (gt) object. In this paper, we innovatively revisit the label assignment from a global perspective and propose to formulate the assigning procedure as an Optimal Transport (OT) problem -- a well-studied topic in Optimization Theory. Concretely, we define the unit transportation cost between each demander (anchor) and supplier (gt) pair as the weighted summation of their classification and regression losses. After formulation, finding the best assignment solution is converted to solve the optimal transport plan at minimal transportation costs, which can be solved via Sinkhorn-Knopp Iteration. On COCO, a single FCOS-ResNet-50 detector equipped with Optimal Transport Assignment (OTA) can reach 40.7% mAP under 1X scheduler, outperforming all other existing assigning methods. Extensive experiments conducted on COCO and CrowdHuman further validate the effectiveness of our proposed OTA, especially its superiority in crowd scenarios. The code is available at https://github.com/Megvii-BaseDetection/OTA. △ Less","26 March, 2021",https://arxiv.org/pdf/2103.14259
Super-Resolving Compressed Video in Coding Chain,Dewang Hou;Yang Zhao;Yuyao Ye;Jiayu Yang;Jian Zhang;Ronggang Wang,"Scaling and lossy coding are widely used in video transmission and storage. Previous methods for enhancing the resolution of such videos often ignore the inherent interference between resolution loss and compression artifacts, which compromises perceptual video quality. To address this problem, we present a mixed-resolution coding framework, which cooperates with a reference-based DCNN. In this novel coding chain, the reference-based DCNN learns the direct mapping from low-resolution (LR) compressed video to their high-resolution (HR) clean version at the decoder side. We further improve reconstruction quality by devising an efficient deformable alignment module with receptive field block to handle various motion distances and introducing a disentangled loss that helps networks distinguish the artifact patterns from texture. Extensive experiments demonstrate the effectiveness of proposed innovations by comparing with state-of-the-art single image, video and reference-based restoration methods. △ Less","25 March, 2021",https://arxiv.org/pdf/2103.14247
Frequency and Impact of Technical Debt Characteristics in Companies Producing Mechatronic Products,Fandi Bi;Birgit Vogel-Heuser;Litong Xu,"Complexity of products, volatility in global markets, and the increasingly rapid pace of innovations may make it difficult to know how to approach challenging situations in mechatronic design and production. Technical Debt (TD) is a metaphor that describes the practical bargain of exchanging short-term benefits for long-term negative consequences. Oftentimes, the scope and impact of TD, as well as the cost of corrective measures, are underestimated. Especially for mechatronic teams in the mechanical, electrical, and software disciplines, the adverse interdisciplinary ripple effects of TD incidents are passed on throughout the life cycle. The analysis of the first comprehensive survey showed that not only do the TD types differ in cross-disciplinary comparisons, but different characteristics can also be observed depending on whether a discipline is studied in isolation or in combination with others. To validate the study results and to report on a general consciousness of TD in the disciplines, this follow-up study involves 15 of the 50 experts of the predecessor study and reflects the frequency and impact of technical debt in industrial experts' daily work using a questionnaire. These experts rate 14 TD types, 47 TD causes, and 33 TD symptoms in terms of their frequency and impact. Detailed analyses reveal consistent results for the most frequent TD types and causes, yet they show divergent characteristics in a profound exploration of discipline-specific phenomena. Thus, this study has the potential to set the foundations for future automated TD identification analyses in mechatronics. △ Less","22 March, 2021",https://arxiv.org/pdf/2103.13350
From Shadow Generation to Shadow Removal,Zhihao Liu;Hui Yin;Xinyi Wu;Zhenyao Wu;Yang Mi;Song Wang,"Shadow removal is a computer-vision task that aims to restore the image content in shadow regions. While almost all recent shadow-removal methods require shadow-free images for training, in ECCV 2020 Le and Samaras introduces an innovative approach without this requirement by cropping patches with and without shadows from shadow images as training samples. However, it is still laborious and time-consuming to construct a large amount of such unpaired patches. In this paper, we propose a new G2R-ShadowNet which leverages shadow generation for weakly-supervised shadow removal by only using a set of shadow images and their corresponding shadow masks for training. The proposed G2R-ShadowNet consists of three sub-networks for shadow generation, shadow removal and refinement, respectively and they are jointly trained in an end-to-end fashion. In particular, the shadow generation sub-net stylises non-shadow regions to be shadow ones, leading to paired data for training the shadow-removal sub-net. Extensive experiments on the ISTD dataset and the Video Shadow Removal dataset show that the proposed G2R-ShadowNet achieves competitive performances against the current state of the arts and outperforms Le and Samaras' patch-based shadow-removal method. △ Less","24 March, 2021",https://arxiv.org/pdf/2103.12997
Receding Horizon Motion Planning for Multi-Agent Systems: A Velocity Obstacle Based Probabilistic Method,Xiaoxue Zhang;Jun Ma;Zilong Cheng;Sunan Huang;Tong Heng Lee,"In this paper, a novel and innovative methodology for feasible motion planning in the multi-agent system is developed. On the basis of velocity obstacles characteristics, the chance constraints are formulated in the receding horizon control (RHC) problem, and geometric information of collision cones is used to generate the feasible regions of velocities for the host agent. By this approach, the motion planning is conducted at the velocity level instead of the position level. Thus, it guarantees a safer collision-free trajectory for the multi-agent system, especially for the systems with high-speed moving agents. Moreover, a probability threshold of potential collisions can be satisfied during the motion planning process. In order to validate the effectiveness of the methodology, different scenarios for multiple agents are investigated, and the simulation results clearly show that the proposed approach can effectively avoid potential collisions with a collision probability less than a specific threshold. △ Less","23 March, 2021",https://arxiv.org/pdf/2103.12968
Integrating Novelty Detection Capabilities with MSL Mastcam Operations to Enhance Data Analysis,Paul Horton;Hannah R. Kerner;Samantha Jacob;Ernest Cisneros;Kiri L. Wagstaff;James Bell,"While innovations in scientific instrumentation have pushed the boundaries of Mars rover mission capabilities, the increase in data complexity has pressured Mars Science Laboratory (MSL) and future Mars rover operations staff to quickly analyze complex data sets to meet progressively shorter tactical and strategic planning timelines. MSLWEB is an internal data tracking tool used by operations staff to perform first pass analysis on MSL image sequences, a series of products taken by the Mast camera, Mastcam. Mastcam's multiband multispectral image sequences require more complex analysis compared to standard 3-band RGB images. Typically, these are analyzed using traditional methods to identify unique features within the sequence. Given the short time frame of tactical planning in which downlinked images might need to be analyzed (within 5-10 hours before the next uplink), there exists a need to triage analysis time to focus on the most important sequences and parts of a sequence. We address this need by creating products for MSLWEB that use novelty detection to help operations staff identify unusual data that might be diagnostic of new or atypical compositions or mineralogies detected within an imaging scene. This was achieved in two ways: 1) by creating products for each sequence to identify novel regions in the image, and 2) by assigning multispectral sequences a sortable novelty score. These new products provide colorized heat maps of inferred novelty that operations staff can use to rapidly review downlinked data and focus their efforts on analyzing potentially new kinds of diagnostic multispectral signatures. This approach has the potential to guide scientists to new discoveries by quickly drawing their attention to often subtle variations not detectable with simple color composites. △ Less","23 March, 2021",https://arxiv.org/pdf/2103.12815
DA4Event: towards bridging the Sim-to-Real Gap for Event Cameras using Domain Adaptation,Mirco Planamente;Chiara Plizzari;Marco Cannici;Marco Ciccone;Francesco Strada;Andrea Bottino;Matteo Matteucci;Barbara Caputo,"Event cameras are novel bio-inspired sensors, which asynchronously capture pixel-level intensity changes in the form of ""events"". The innovative way they acquire data presents several advantages over standard devices, especially in poor lighting and high-speed motion conditions. However, the novelty of these sensors results in the lack of a large amount of training data capable of fully unlocking their potential. The most common approach implemented by researchers to address this issue is to leverage simulated event data. Yet, this approach comes with an open research question: how well simulated data generalize to real data? To answer this, we propose to exploit, in the event-based context, recent Domain Adaptation (DA) advances in traditional computer vision, showing that DA techniques applied to event data help reduce the sim-to-real gap. To this purpose, we propose a novel architecture, which we call Multi-View DA4E (MV-DA4E), that better exploits the peculiarities of frame-based event representations while also promoting domain invariant characteristics in features. Through extensive experiments, we prove the effectiveness of DA methods and MV-DA4E on N-Caltech101. Moreover, we validate their soundness in a real-world scenario through a cross-domain analysis on the popular RGB-D Object Dataset (ROD), which we extended to the event modality (RGB-E). △ Less","29 October, 2021",https://arxiv.org/pdf/2103.12768
PanGEA: The Panoramic Graph Environment Annotation Toolkit,Alexander Ku;Peter Anderson;Jordi Pont-Tuset;Jason Baldridge,"PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks -- collecting navigation instructions and navigation instruction following -- and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support. △ Less","23 March, 2021",https://arxiv.org/pdf/2103.12703
Evolving Learning Rate Optimizers for Deep Neural Networks,Pedro Carvalho;Nuno Lourenço;Penousal Machado,"Artificial Neural Networks (ANNs) became popular due to their successful application difficult problems such image and speech recognition. However, when practitioners want to design an ANN they need to undergo laborious process of selecting a set of parameters and topology. Currently, there are several state-of-the art methods that allow for the automatic selection of some of these aspects. Learning Rate optimizers are a set of such techniques that search for good values of learning rates. Whilst these techniques are effective and have yielded good results over the years, they are general solution i.e. they do not consider the characteristics of a specific network. We propose a framework called AutoLR to automatically design Learning Rate Optimizers. Two versions of the system are detailed. The first one, Dynamic AutoLR, evolves static and dynamic learning rate optimizers based on the current epoch and the previous learning rate. The second version, Adaptive AutoLR, evolves adaptive optimizers that can fine tune the learning rate for each network eeight which makes them generally more effective. The results are competitive with the best state of the art methods, even outperforming them in some scenarios. Furthermore, the system evolved a classifier, ADES, that appears to be novel and innovative since, to the best of our knowledge, it has a structure that differs from state of the art methods. △ Less","23 March, 2021",https://arxiv.org/pdf/2103.12623
Incrementally Zero-Shot Detection by an Extreme Value Analyzer,Sixiao Zheng;Yanwei Fu;Yanxi Hou,"Human beings not only have the ability to recognize novel unseen classes, but also can incrementally incorporate the new classes to existing knowledge preserved. However, zero-shot learning models assume that all seen classes should be known beforehand, while incremental learning models cannot recognize unseen classes. This paper introduces a novel and challenging task of Incrementally Zero-Shot Detection (IZSD), a practical strategy for both zero-shot learning and class-incremental learning in real-world object detection. An innovative end-to-end model -- IZSD-EVer was proposed to tackle this task that requires incrementally detecting new classes and detecting the classes that have never been seen. Specifically, we propose a novel extreme value analyzer to detect objects from old seen, new seen, and unseen classes, simultaneously. Additionally and technically, we propose two innovative losses, i.e., background-foreground mean squared error loss alleviating the extreme imbalance of the background and foreground of images, and projection distance loss aligning the visual space and semantic spaces of old seen classes. Experiments demonstrate the efficacy of our model in detecting objects from both the seen and unseen classes, outperforming the alternative models on Pascal VOC and MSCOCO datasets. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.12609
iMAP: Implicit Mapping and Positioning in Real-Time,Edgar Sucar;Shikun Liu;Joseph Ortiz;Andrew J. Davison,"We show for the first time that a multilayer perceptron (MLP) can serve as the only scene representation in a real-time SLAM system for a handheld RGB-D camera. Our network is trained in live operation without prior data, building a dense, scene-specific implicit 3D model of occupancy and colour which is also immediately used for tracking. Achieving real-time SLAM via continual training of a neural network against a live image stream requires significant innovation. Our iMAP algorithm uses a keyframe structure and multi-processing computation flow, with dynamic information-guided pixel sampling for speed, with tracking at 10 Hz and global map updating at 2 Hz. The advantages of an implicit MLP over standard dense SLAM techniques include efficient geometry representation with automatic detail control and smooth, plausible filling-in of unobserved regions such as the back surfaces of objects. △ Less","13 September, 2021",https://arxiv.org/pdf/2103.12352
Security of Healthcare Data Using Blockchains: A Survey,Mayank Pandey;Rachit Agarwal;Sandeep K. Shukla;Nishchal K. Verma,"The advancement in the healthcare sector is entering into a new era in the form of Health 4.0. The integration of innovative technologies like Cyber-Physical Systems (CPS), Big Data, Cloud Computing, Machine Learning, and Blockchain with Healthcare services has led to improved performance and efficiency through data-based learning and interconnection of systems. On the other hand, it has also increased complexities and has brought its own share of vulnerabilities due to the heavy influx, sharing, and storage of healthcare data. The protection of the same from cyber-attacks along with privacy preservation through authenticated access is one of the significant challenges for the healthcare sector. For this purpose, the use of blockchain-based networks can lead to a considerable reduction in the vulnerabilities of the healthcare systems and secure their data. This chapter explores blockchain's role in strengthening healthcare data security by answering the questions related to what data use, when we need, why we need, who needs, and how state-of-the-art techniques use blockchains to secure healthcare data. As a case study, we also explore and analyze the state-of-the-art implementations for blockchain in healthcare data security for the COVID-19 pandemic. In order to provide a path to future research directions, we identify and discuss the technical limitations and regulatory challenges associated with blockchain-based healthcare data security implementation. △ Less","23 March, 2021",https://arxiv.org/pdf/2103.12326
A Joint Reinforcement-Learning Enabled Caching and Cross-Layer Network Code for Sum-Rate Maximization in F-RAN with D2D Communications,Mohammed S. Al-Abiad;Md. Zoheb Hassan;Md. Jahangir Hossain,"In this paper, we leverage reinforcement learning (RL) and cross-layer network coding (CLNC) for efficiently pre-fetching users' contents to the local caches and delivering these contents to users in a downlink fog-radio access network (F-RAN) with device-to-device (D2D) communications. In the considered system, fog access points (F-APs) and cache-enabled D2D (CE-D2D) users are equipped with local caches for alleviating traffic burden at the fronthaul, while users' contents can be easily and quickly accommodated. In CLNC, the coding decisions take users' contents, their rates, and power levels of F-APs and CE-D2D users into account, and RL optimizes caching strategy. Towards this goal, a joint content placement and delivery problem is formulated as an optimization problem with a goal to maximize system sum-rate. For this NP-hard problem, we first develop an innovative decentralized CLNC coalition formation (CLNC-CF) algorithm to obtain a stable solution for the content delivery problem, where F-APs and CE-D2D users utilize CLNC resource allocation. By taking the behavior of F-APs and CE-D2D users into account, we then develop a multi-agent RL (MARL) algorithm for optimizing the content placements at both F-APs and CE-D2D users. Simulation results show that the proposed joint CLNC-CF and RL framework can effectively improve the sum-rate by up to 30\%, 60\%, and 150\%, respectively, compared to: 1) an optimal uncoded algorithm, 2) a standard rate-aware-NC algorithm, and 3) a benchmark classical NC with network-layer optimization. △ Less","22 March, 2021",https://arxiv.org/pdf/2103.12134
A Total-Variation Sparseness-Promoting Method for the Synthesis of Contiguously Clustered Linear Arrays,N. Anselmi;G. Gottardi;G. Oliveri;A. Massa,"By exploiting an innovative total-variation compressive sensing (TV-CS) formulation, a new method for the synthesis of physically contiguous clustered linear arrays is presented. The computation of the feed network excitations is recast as the maximization of the gradient sparsity of the excitation vector subject to matching a user-defined pattern. The arising TV-CS functional is then optimized by means of a deterministic alternating direction algorithm. A selected set of representative numerical results, drawn from a wide validation, is reported to illustrate the potentialities and the limitations of the proposed approach when clustering arrays of both ideal and realistic antenna elements. Comparisons with some competitive state-of-the-art subarraying techniques are performed as well. △ Less","2 February, 2021",https://arxiv.org/pdf/2103.11778
Geo-Spatiotemporal Features and Shape-Based Prior Knowledge for Fine-grained Imbalanced Data Classification,Charles A. Kantor;Marta Skreta;Brice Rauby;Léonard Boussioux;Emmanuel Jehanno;Alexandra Luccioni;David Rolnick;Hugues Talbot,"Fine-grained classification aims at distinguishing between items with similar global perception and patterns, but that differ by minute details. Our primary challenges come from both small inter-class variations and large intra-class variations. In this article, we propose to combine several innovations to improve fine-grained classification within the use-case of wildlife, which is of practical interest for experts. We utilize geo-spatiotemporal data to enrich the picture information and further improve the performance. We also investigate state-of-the-art methods for handling the imbalanced data issue. △ Less","20 March, 2021",https://arxiv.org/pdf/2103.11285
Controllable Generation from Pre-trained Language Models via Inverse Prompting,Xu Zou;Da Yin;Qingyang Zhong;Ming Ding;Hongxia Yang;Zhilin Yang;Jie Tang,"Large-scale pre-trained language models have demonstrated strong capabilities of generating realistic text. However, it remains challenging to control the generation results. Previous approaches such as prompting are far from sufficient, which limits the usage of language models. To tackle this challenge, we propose an innovative method, inverse prompting, to better control text generation. The core idea of inverse prompting is to use generated text to inversely predict the prompt during beam search, which enhances the relevance between the prompt and the generated text and provides better controllability. Empirically, we pre-train a large-scale Chinese language model to perform a systematic study using human evaluation on the tasks of open-domain poem generation and open-domain long-form question answering. Our results show that our proposed method substantially outperforms the baselines and that our generation quality is close to human performance on some of the tasks. Narrators can try our poem generation demo at https://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at https://pretrain.aminer.cn/app/qa. For researchers, the code is provided in https://github.com/THUDM/InversePrompting. △ Less","9 November, 2021",https://arxiv.org/pdf/2103.10685
USTC-NELSLIP System Description for DIHARD-III Challenge,Yuxuan Wang;Maokui He;Shutong Niu;Lei Sun;Tian Gao;Xin Fang;Jia Pan;Jun Du;Chin-Hui Lee,"This system description describes our submission system to the Third DIHARD Speech Diarization Challenge. Besides the traditional clustering based system, the innovation of our system lies in the combination of various front-end techniques to solve the diarization problem, including speech separation and target-speaker based voice activity detection (TS-VAD), combined with iterative data purification. We also adopted audio domain classification to design domain-dependent processing. Finally, we performed post processing to do system fusion and selection. Our best system achieved DERs of 11.30% in track 1 and 16.78% in track 2 on evaluation set, respectively. △ Less","19 March, 2021",https://arxiv.org/pdf/2103.10661
Towards an Understanding of Why and How ICT Projects Are Initiated: Analysis via Repertory Grid,Htike Htike Wut Yi;Stephen G. MacDonell,"Contemporary business innovation relies increasingly on information and communications technology (ICT) solutions. As ICT initiatives are generally implemented via projects the management of ICT projects has come under increasing scrutiny. ICT projects continue to fail; as a result, while research in ICT project management has indeed increased, many challenges for research and practice remain. Many studies have addressed the execution and management of ICT projects and the many factors that might relate to project outcomes. Very few, however, have considered ICT project initiation and the crucial decisions made at that very early, pre-life cycle stage. The primary intent of this research is therefore to investigate ICT projects with a particular focus on their initiation. In doing so we wished to understand why ICT projects are started, and how they are moved from idea or proposal to supported reality. A combination of semi-structured interviews and the repertory grid data collection and analysis method was employed to investigate and validate the motivating factors that influence individual IT Managers' project initiation decisions and the methods they use to transition from idea to enacted project. Our results showed that there are indeed multiple underlying reasons for the decisions made at this early stage and that there are some especially common decision drivers. Some were expected, in the sense that they mapped to recommended best practice. For instance, most projects are motivated by a desire to achieve efficiencies or cost savings, and their potential tends to be assessed using cost benefit analysis. Other results were more surprising - competitor pressure was not a common driver for ICT project initiation in our analysis. Unsurprisingly, formal evaluation methods are more frequently used to assess project proposals when those projects are larger and higher profile. (Abridged) △ Less","18 March, 2021",https://arxiv.org/pdf/2103.10570
Understanding Barriers to Internal Startups in Large Organizations: Evidence from a Globally Distributed Company,Tor Sporsem;Anastasiia Tkalich;Nils Brede Moe;Marius Mikalsen,"Large global companies need to speed up their innovation activities to increase competitive advantage. However, such companies' organizational structures impede their ability to capture trends they are well aware of due to bureaucracy, slow decision-making, distributed departments, and distributed processes. One way to strengthen the innovation capability is through fostering internal startups. We report findings from an embedded multiple-case study of five internal startups in a globally distributed company to identify barriers for software product innovation: late involvement of software developers, executive sponsor is missing or not clarified, yearly budgeting and planning, unclear decision-making authority, lack of digital infrastructure for experimentation and access to data from external actors. Drawing on the framework of continuous software engineering proposed by Fitzgerald and Stol, we discuss the role of BizDev in software product innovation. We suggest that lack of continuity, rather than the lack of speed, is an ultimate challenge for internal startups in large global companies. △ Less","17 March, 2021",https://arxiv.org/pdf/2103.09707
"A Novel Framework for the Analysis of Unknown Transactions in Bitcoin: Theory, Model, and Experimental Results",Maurantonio Caprolu;Matteo Pontecorvi;Matteo Signorini;Carlos Segarra;Roberto Di Pietro,"Bitcoin (BTC) is probably the most transparent payment network in the world, thanks to the full history of transactions available to the public. Though, Bitcoin is not a fully anonymous environment, rather a pseudonymous one, accounting for a number of attempts to beat its pseudonimity using clustering techniques. There is, however, a recurring assumption in all the cited deanonymization techniques: that each transaction output has an address attached to it. That assumption is false. An evidence is that, as of block height 591,872, there are several millions transactions with at least one output for which the Bitcoin Core client cannot infer an address. In this paper, we present a novel approach based on sound graph theory for identifying transaction inputs and outputs. Our solution implements two simple yet innovative features: it does not rely on BTC addresses and explores all the transactions stored in the blockchain. All the other existing solutions fail with respect to one or both of the cited features. In detail, we first introduce the concept of Unknown Transaction and provide a new framework to parse the Bitcoin blockchain by taking them into account. Then, we introduce a theoretical model to detect, study, and classify -- for the first time in the literature -- unknown transaction patterns in the user network. Further, in an extensive experimental campaign, we apply our model to the Bitcoin network to uncover hidden transaction patterns within the Bitcoin user network. Results are striking: we discovered more than 30,000 unknown transaction DAGs, with a few of them exhibiting a complex yet ordered topology and potentially connected to automated payment services. To the best of our knowledge, the proposed framework is the only one that enables a complete study of the unknown transaction patterns, hence enabling further research in the fields -- for which we provide some directions. △ Less","17 March, 2021",https://arxiv.org/pdf/2103.09459
OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,Weihua Hu;Matthias Fey;Hongyu Ren;Maho Nakata;Yuxiao Dong;Jure Leskovec,"Enabling effective and efficient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientific applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks -- link prediction, graph regression, and node classification. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which significant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ . △ Less","20 October, 2021",https://arxiv.org/pdf/2103.09430
Contrastive Learning of Musical Representations,Janne Spijkervet;John Ashley Burgoyne,"While deep learning has enabled great advances in many areas of music, labeled music datasets remain especially hard, expensive, and time-consuming to create. In this work, we introduce SimCLR to the music domain and contribute a large chain of audio data augmentations to form a simple framework for self-supervised, contrastive learning of musical representations: CLMR. This approach works on raw time-domain music data and requires no labels to learn useful representations. We evaluate CLMR in the downstream task of music classification on the MagnaTagATune and Million Song datasets and present an ablation study to test which of our music-related innovations over SimCLR are most effective. A linear classifier trained on the proposed representations achieves a higher average precision than supervised models on the MagnaTagATune dataset, and performs comparably on the Million Song dataset. Moreover, we show that CLMR's representations are transferable using out-of-domain datasets, indicating that our method has strong generalisability in music classification. Lastly, we show that the proposed method allows data-efficient learning on smaller labeled datasets: we achieve an average precision of 33.1% despite using only 259 labeled songs in the MagnaTagATune dataset (1% of the full dataset) during linear evaluation. To foster reproducibility and future research on self-supervised learning in music, we publicly release the pre-trained models and the source code of all experiments of this paper. △ Less","24 September, 2021",https://arxiv.org/pdf/2103.09410
A survey of deep neural network watermarking techniques,Yue Li;Hongxia Wang;Mauro Barni,"Protecting the Intellectual Property Rights (IPR) associated to Deep Neural Networks (DNNs) is a pressing need pushed by the high costs required to train such networks and the importance that DNNs are gaining in our society. Following its use for Multimedia (MM) IPR protection, digital watermarking has recently been considered as a mean to protect the IPR of DNNs. While DNN watermarking inherits some basic concepts and methods from MM watermarking, there are significant differences between the two application areas, calling for the adaptation of media watermarking techniques to the DNN scenario and the development of completely new methods. In this paper, we overview the most recent advances in DNN watermarking, by paying attention to cast it into the bulk of watermarking theory developed during the last two decades, while at the same time highlighting the new challenges and opportunities characterizing DNN watermarking. Rather than trying to present a comprehensive description of all the methods proposed so far, we introduce a new taxonomy of DNN watermarking and present a few exemplary methods belonging to each class. We hope that this paper will inspire new research in this exciting area and will help researchers to focus on the most innovative and challenging problems in the field. △ Less","16 March, 2021",https://arxiv.org/pdf/2103.09274
Implementation of Technology Acceptance Model (TAM) and Importance Performance Analysis (IPA) in Testing the Ease and Usability of E-wallet Applications,Dedi Saputra;Burcu Gürbüz,"Digital payment innovation is currently increasingly needed by the community, especially in making non-cash payment transactions. The purpose of this research is to know and measure the ease and usefulness of e-wallet digital wallet services, especially in the GoPay application. The population in this study are users of the Go-Pay service on the GO-JEK platform. The sample of this study consisted of 124 respondents from distributing questionnaires in Depok, West Java using a modified Technology Acceptance Model (TAM) based on existing references. The data processing in this research uses Importance Performance Analysis (IPA) analysis. The results show that based on the gap analysis, it is found that in general Go-Pay users are not satisfied with the current service quality. Based on the IPA analysis, the priority scale of E-Wallet Go-Pay quality improvement can be mapped, where quadrant I is the highest priority scale according to the user's perspective: [1], [4], [5], and [6]. These three items must be upgraded immediately by the manager to meet user expectations. Areas that become the achievements or advantages of the GoPay E-Wallet that must be maintained are in quadrant II, namely: [2] and [3]. From this explanation, it can be concluded that in general the E-Wallet GoPay Service must be improved to improve its service performance. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.09049
From Innovations to Prospects: What Is Hidden Behind Cryptocurrencies?,Ang Jia;Ming Fan;Xi Xu;Di Cui;Wenying Wei;Zijiang Yang;Kai Ye;Ting Liu,"The great influence of Bitcoin has promoted the rapid development of blockchain-based digital currencies, especially the altcoins, since 2013. However, most altcoins share similar source codes, resulting in concerns about code innovations. In this paper, an empirical study on existing altcoins is carried out to offer a thorough understanding of various aspects associated with altcoin innovations. Firstly, we construct the dataset of altcoins, including source code repositories, GitHub fork relations, and market capitalizations (cap). Then, we analyze the altcoin innovations from the perspective of source code similarities. The results demonstrate that more than 85% of altcoin repositories present high code similarities. Next, a temporal clustering algorithm is proposed to mine the inheritance relationship among various altcoins. The family pedigrees of altcoin are constructed, in which the altcoin presents similar evolution features as biology, such as power-law in family size, variety in family evolution, etc. Finally, we investigate the correlation between code innovations and market capitalization. Although we fail to predict the price of altcoins based on their code similarities, the results show that altcoins with higher innovations reflect better market prospects. △ Less","16 March, 2021",https://arxiv.org/pdf/2103.08924
"Classroom Technology Deployment Matrix: A Planning, Monitoring, Evaluating and Reporting Tool",Philip Heslop;Ahmed Kharrufa;Madeline Balaam;David Leat,"We present the Classroom Technology Deployment Matrix (CTDM), a tool for high-level Planning, Monitoring, Evaluating and Reporting of classroom deployments of educational technologies, enabling researchers, teachers and schools to work together for successful deployments. The tool is de-rived from a review of literature on technology adaptation (at the individual, process and organisation level), concluding that Normalization Process Theory, which seeks to explain the social processes that lead to the routine embedding of innovative technology in an existing system, would a suitable foundation for developing this matrix. This can be leveraged in the specific context of the classroom, specifically including the Normal Desired State of teachers. We explore this classroom context, and the developed CTDM, through look-ing at two separate deployments (different schools and teachers) of the same technology (Collocated Collaborative Writing), observing how lessons learned from the first changed our approach to the second. The descriptive and an-alytical value of the tool is then demonstrated through map-ping these observation to the matrix and can be applied to future deployments. △ Less","15 March, 2021",https://arxiv.org/pdf/2103.08324
The entrepreneurial logic of startup software development: A study of 40 software startups,Anh Nguyen-Duc;Kai-Kristian Kemell;Pekka Abrahamsson,"Context: Software startups are an essential source of innovation and software-intensive products. The need to understand product development in startups and to provide relevant support are highlighted in software research. While state-of-the-art literature reveals how startups develop their software, the reasons why they adopt these activities are underexplored. Objective: This study investigates the tactics behind software engineering (SE) activities by analyzing key engineering events during startup journeys. We explore how entrepreneurial mindsets may be associated with SE knowledge areas and with each startup case. Method: Our theoretical foundation is based on causation and effectuation models. We conducted semi-structured interviews with 40 software startups. We used two-round open coding and thematic analysis to describe and identify entrepreneurial software development patterns. Additionally, we calculated an effectuation index for each startup case. Results: We identified 621 events merged into 32 codes of entrepreneurial logic in SE from the sample. We found a systemic occurrence of the logic in all areas of SE activities. Minimum Viable Product (MVP), Technical Debt (TD), and Customer Involvement (CI) tend to be associated with effectual logic, while testing activities at different levels are associated with causal logic. The effectuation index revealed that startups are either effectuation-driven or mixed-logics-driven. Conclusions: Software startups fall into two types that differentiate between how traditional SE approaches may apply to them. Effectuation seems the most relevant and essential model for explaining and developing suitable SE practices for software startups. △ Less","14 March, 2021",https://arxiv.org/pdf/2103.07999
Radar Camera Fusion via Representation Learning in Autonomous Driving,Xu Dong;Binnan Zhuang;Yunxiang Mao;Langechuan Liu,"Radars and cameras are mature, cost-effective, and robust sensors and have been widely used in the perception stack of mass-produced autonomous driving systems. Due to their complementary properties, outputs from radar detection (radar pins) and camera perception (2D bounding boxes) are usually fused to generate the best perception results. The key to successful radar-camera fusion is the accurate data association. The challenges in the radar-camera association can be attributed to the complexity of driving scenes, the noisy and sparse nature of radar measurements, and the depth ambiguity from 2D bounding boxes. Traditional rule-based association methods are susceptible to performance degradation in challenging scenarios and failure in corner cases. In this study, we propose to address radar-camera association via deep representation learning, to explore feature-level interaction and global reasoning. Additionally, we design a loss sampling mechanism and an innovative ordinal loss to overcome the difficulty of imperfect labeling and to enforce critical human-like reasoning. Despite being trained with noisy labels generated by a rule-based algorithm, our proposed method achieves a performance of 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover, this data-driven method also lends itself to continuous improvement via corner case mining. △ Less","18 June, 2021",https://arxiv.org/pdf/2103.07825
Diffusion of Innovation In Competitive Markets-A Study on the Global Smartphone Diffusion,Semra Gunduc,"In this work, the aim is to study the diffusion of innovation of two competing products. The main focus has been to understand the effects of the competitive dynamic market on the diffusion of innovation. The global smartphone operating system sales are chosen as an example. The availability of the sales and the number of users data, as well as the predictions for the future number of users, make the smartphone diffusion a new laboratory to test the innovation of diffusion models for the competitive markets. In this work, the Bass model and its extensions which incorporate the competition between the brands are used. The diffusion of smartphones can be considered on two levels: the product level and the brand level. The diffusion of the smartphone as a category is studied by using the Bass equation (category-level diffusion). The diffusion of each competing operating system (iOS and Android) are considered as the competition of the brands, and it is studied in the context of competitive market models (product-level diffusion). It is shown that the effects of personal interactions play the dominant role in the diffusion process. Moreover, the volume of near future sales can be predicted by introducing appropriate dynamic market potential which helps to extrapolate the model results for the future. △ Less","13 March, 2021",https://arxiv.org/pdf/2103.07707
A review of machine learning in processing remote sensing data for mineral exploration,Hojat Shirmard;Ehsan Farahbakhsh;R. Dietmar Muller;Rohitash Chandra,"The decline of the number of newly discovered mineral deposits and increase in demand for different minerals in recent years has led exploration geologists to look for more efficient and innovative methods for processing different data types at each stage of mineral exploration. As a primary step, various features, such as lithological units, alteration types, structures, and indicator minerals, are mapped to aid decision-making in targeting ore deposits. Different types of remote sensing datasets, such as satellite and airborne data, make it possible to overcome common problems associated with mapping geological features. The rapid increase in the volume of remote sensing data obtained from different platforms has encouraged scientists to develop advanced, innovative, and robust data processing methodologies. Machine learning methods can help process a wide range of remote sensing datasets and determine the relationship between components such as the reflectance continuum and features of interest. These methods are robust in processing spectral and ground truth measurements against noise and uncertainties. In recent years, many studies have been carried out by supplementing geological surveys with remote sensing datasets, which is now prominent in geoscience research. This paper provides a comprehensive review of the implementation and adaptation of some popular and recently established machine learning methods for processing different types of remote sensing data and investigates their applications for detecting various ore deposit types. We demonstrate the high capability of combining remote sensing data and machine learning methods for mapping different geological features that are critical for providing potential maps. Moreover, we find there is scope for advanced methods to process the new generation of remote sensing data for creating improved mineral prospectivity maps. △ Less","4 December, 2021",https://arxiv.org/pdf/2103.07678
Abolitionist Networks: Modeling Language Change in Nineteenth-Century Activist Newspapers,Sandeep Soni;Lauren Klein;Jacob Eisenstein,"The abolitionist movement of the nineteenth-century United States remains among the most significant social and political movements in US history. Abolitionist newspapers played a crucial role in spreading information and shaping public opinion around a range of issues relating to the abolition of slavery. These newspapers also serve as a primary source of information about the movement for scholars today, resulting in powerful new accounts of the movement and its leaders. This paper supplements recent qualitative work on the role of women in abolition's vanguard, as well as the role of the Black press, with a quantitative text modeling approach. Using diachronic word embeddings, we identify which newspapers tended to lead lexical semantic innovations -- the introduction of new usages of specific words -- and which newspapers tended to follow. We then aggregate the evidence across hundreds of changes into a weighted network with the newspapers as nodes; directed edge weights represent the frequency with which each newspaper led the other in the adoption of a lexical semantic change. Analysis of this network reveals pathways of lexical semantic influence, distinguishing leaders from followers, as well as others who stood apart from the semantic changes that swept through this period. More specifically, we find that two newspapers edited by women -- THE PROVINCIAL FREEMAN and THE LILY -- led a large number of semantic changes in our corpus, lending additional credence to the argument that a multiracial coalition of women led the abolitionist movement in terms of both thought and action. It also contributes additional complexity to the scholarship that has sought to tease apart the relation of the abolitionist movement to the women's suffrage movement, and the vexed racial politics that characterized their relation. △ Less","12 March, 2021",https://arxiv.org/pdf/2103.07538
Private Cross-Silo Federated Learning for Extracting Vaccine Adverse Event Mentions,Pallika Kanani;Virendra J. Marathe;Daniel Peterson;Rave Harpaz;Steve Bright,"Federated Learning (FL) is quickly becoming a goto distributed training paradigm for users to jointly train a global model without physically sharing their data. Users can indirectly contribute to, and directly benefit from a much larger aggregate data corpus used to train the global model. However, literature on successful application of FL in real-world problem settings is somewhat sparse. In this paper, we describe our experience applying a FL based solution to the Named Entity Recognition (NER) task for an adverse event detection application in the context of mass scale vaccination programs. We present a comprehensive empirical analysis of various dimensions of benefits gained with FL based training. Furthermore, we investigate effects of tighter Differential Privacy (DP) constraints in highly sensitive settings where federation users must enforce Local DP to ensure strict privacy guarantees. We show that local DP can severely cripple the global model's prediction accuracy, thus dis-incentivizing users from participating in the federation. In response, we demonstrate how recent innovation on personalization methods can help significantly recover the lost accuracy. We focus our analysis on the Federated Fine-Tuning algorithm, FedFT, and prove that it is not PAC Identifiable, thus making it even more attractive for FL-based training. △ Less","12 March, 2021",https://arxiv.org/pdf/2103.07491
Wireframe-Based UI Design Search Through Image Autoencoder,Jieshan Chen;Chunyang Chen;Zhenchang Xing;Xin Xia;Liming Zhu;John Grundy;Jinshui Wang,"UI design is an integral part of software development. For many developers who do not have much UI design experience, exposing them to a large database of real-application UI designs can help them quickly build up a realistic understanding of the design space for a software feature and get design inspirations from existing applications. However, existing keyword-based, image-similarity-based, and component-matching-based methods cannot reliably find relevant high-fidelity UI designs in a large database alike to the UI wireframe that the developers sketch, in face of the great variations in UI designs. In this article, we propose a deep-learning-based UI design search engine to fill in the gap. The key innovation of our search engine is to train a wireframe image autoencoder using a large database of real-application UI designs, without the need for labeling relevant UI designs. We implement our approach for Android UI design search, and conduct extensive experiments with artificially created relevant UI designs and human evaluation of UI design search results. Our experiments confirm the superior performance of our search engine over existing image-similarity or component-matching-based methods and demonstrate the usefulness of our search engine in real-world UI design tasks. △ Less","11 March, 2021",https://arxiv.org/pdf/2103.07085
Development of recommendation systems for software engineering: the CROSSMINER experience,Juri Di Rocco;Davide Di Ruscio;Claudio Di Sipio;Phuong T. Nguyen;Riccardo Rubei,"To perform their daily tasks, developers intensively make use of existing resources by consulting open-source software (OSS) repositories. Such platforms contain rich data sources, e.g., code snippets, documentation, and user discussions, that can be useful for supporting development activities. Over the last decades, several techniques and tools have been promoted to provide developers with innovative features, aiming to bring in improvements in terms of development effort, cost savings, and productivity. In the context of the EU H2020 CROSSMINER project, a set of recommendation systems has been conceived to assist software programmers in different phases of the development process. The systems provide developers with various artifacts, such as third-party libraries, documentation about how to use the APIs being adopted, or relevant API function calls. To develop such recommendations, various technical choices have been made to overcome issues related to several aspects including the lack of baselines, limited data availability, decisions about the performance measures, and evaluation approaches. This paper is an experience report to present the knowledge pertinent to the set of recommendation systems developed through the CROSSMINER project. We explain in detail the challenges we had to deal with, together with the related lessons learned when developing and evaluating these systems. Our aim is to provide the research community with concrete takeaway messages that are expected to be useful for those who want to develop or customize their own recommendation systems. The reported experiences can facilitate interesting discussions and research work, which in the end contribute to the advancement of recommendation systems applied to solve different issues in Software Engineering. △ Less","11 March, 2021",https://arxiv.org/pdf/2103.06987
Firms' Challenges and Social Responsibilities during Covid-19: a Twitter Analysis,Alessia Patuelli;Guido Caldarelli;Nicola Lattanzi;Fabio Saracco,"The Covid-19 pandemic caused disruptive effects for individuals, firms, and societies. In this paper, we offer insights on the major issues and challenges firms are facing in the Covid-19 pandemic, as well as their concerns for Corporate Social Responsibility (CSR) themes. To do so, we investigate large Italian firms' discussion on Twitter in the first nine months of the pandemic. We downloaded all Twitter posts from 1st of March, 2020, to 17th of November, 2020 by the accounts of the largest Italian firms, i.e. those with 250 or more employees. We then built the bipartite network of accounts and hashtags and, using an entropy-based null model as a benchmark, we projected the information contained in the network into the accounts layers, identifying a network of accounts in which a link indicates a non trivial similarity in terms of their usage of hashtags. We find that the conversation is focused around 13 communities, 10 of which include Covid-19 themes. The core of the network is formed of 5 communities, which deal with environmental sustainability, digital innovation and safety. Firms' ownership type does not seem to influence the conversation. 10 communities out of 13 mention hashtags related to CSR, with the environmental and social dimensions as the prevalent ones. Interestingly enough, the social dimension seems more relevant in the communities dealing with digital innovation and safety. However, the relevance of CSR hashtags is very small at the single message level, but with some peculiarities arising in specific communities. Overall, our paper highlights the role of network methods on Twitter data as a tool which can support managers and policy makers to design their strategies and decision making, capturing firms' emerging issues and relevant themes. △ Less","11 March, 2021",https://arxiv.org/pdf/2103.06705
"Designing and Implementing e-justice Systems: An Information Systems Approach to Regional Trial Court Case Docket Management in Northern Mindanao, Philippines",Benzar Glen Grepon,"Computer-based information systems for case management are still at an early stage of adoption in many trial courts in the Philippines. In most cases, information system implemented is the case docket using the official record book on which cases are written and inventory of cases and reports are generated. This is a standalone system that often face data processing, data security and case management challenges. However, there are examples of Information systems in overcoming these pitfalls and producing innovative solutions that surpass data management practices in in many trial courts in the country. One such case is the Regional Trial Court Branch 23 of Cagayan de Oro City in Northern Mindanao, Philippines. A project named Web-based Case Docket Information System (WCDIS) has been designed and developed for the court branch. This system uses a framework known as System Development Life Cycle (SDLC) which is a guide for the design and development. This paper also discusses the key system functionalities and the implementation methodology, including both the benefits and shortcomings of this approach, with the goal of applying lessons learned for future installations. Foremost among the successes of this project is its ability to increase efficiency and reliability in completing court transactions. △ Less","10 March, 2021",https://arxiv.org/pdf/2103.06412
CreativeGAN: Editing Generative Adversarial Networks for Creative Design Synthesis,Amin Heyrani Nobari;Muhammad Fathy Rashad;Faez Ahmed,"Modern machine learning techniques, such as deep neural networks, are transforming many disciplines ranging from image recognition to language understanding, by uncovering patterns in big data and making accurate predictions. They have also shown promising results for synthesizing new designs, which is crucial for creating products and enabling innovation. Generative models, including generative adversarial networks (GANs), have proven to be effective for design synthesis with applications ranging from product design to metamaterial design. These automated computational design methods can support human designers, who typically create designs by a time-consuming process of iteratively exploring ideas using experience and heuristics. However, there are still challenges remaining in automatically synthesizing `creative' designs. GAN models, however, are not capable of generating unique designs, a key to innovation and a major gap in AI-based design automation applications. This paper proposes an automated method, named CreativeGAN, for generating novel designs. It does so by identifying components that make a design unique and modifying a GAN model such that it becomes more likely to generate designs with identified unique components. The method combines state-of-art novelty detection, segmentation, novelty localization, rewriting, and generative models for creative design synthesis. Using a dataset of bicycle designs, we demonstrate that the method can create new bicycle designs with unique frames and handles, and generalize rare novelties to a broad set of designs. Our automated method requires no human intervention and demonstrates a way to rethink creative design synthesis and exploration. △ Less","10 March, 2021",https://arxiv.org/pdf/2103.06242
Privacy-Preserving and Sustainable Contact Tracing Using Batteryless Bluetooth Low-Energy Beacons,Pietro Tedeschi;Kang Eun Jeon;James She;Simon Wong;Spiridon Bakiras;Roberto Di Pietro,"Contact tracing is the techno-choice of reference to address the COVID-19 pandemic. Many of the current approaches have severe privacy and security issues and fail to offer a sustainable contact tracing infrastructure. We address these issues introducing an innovative, privacy-preserving, sustainable, and experimentally tested architecture that leverages batteryless BLE beacons. △ Less","21 December, 2021",https://arxiv.org/pdf/2103.06221
Regressive Domain Adaptation for Unsupervised Keypoint Detection,Junguang Jiang;Yifei Ji;Ximei Wang;Yufeng Liu;Jianmin Wang;Mingsheng Long,"Domain adaptation (DA) aims at transferring knowledge from a labeled source domain to an unlabeled target domain. Though many DA theories and algorithms have been proposed, most of them are tailored into classification settings and may fail in regression tasks, especially in the practical keypoint detection task. To tackle this difficult but significant task, we present a method of regressive domain adaptation (RegDA) for unsupervised keypoint detection. Inspired by the latest theoretical work, we first utilize an adversarial regressor to maximize the disparity on the target domain and train a feature generator to minimize this disparity. However, due to the high dimension of the output space, this regressor fails to detect samples that deviate from the support of the source. To overcome this problem, we propose two important ideas. First, based on our observation that the probability density of the output space is sparse, we introduce a spatial probability distribution to describe this sparsity and then use it to guide the learning of the adversarial regressor. Second, to alleviate the optimization difficulty in the high-dimensional space, we innovatively convert the minimax game in the adversarial training to the minimization of two opposite goals. Extensive experiments show that our method brings large improvement by 8% to 11% in terms of PCK on different datasets. △ Less","4 June, 2021",https://arxiv.org/pdf/2103.06175
Notebook articles: towards a transformative publishing experience in nonlinear science,Cristel Chandre;Jonathan Dubois,"Open Science, Reproducible Research, Findable, Accessible, Interoperable and Reusable (FAIR) data principles are long term goals for scientific dissemination. However, the implementation of these principles calls for a reinspection of our means of dissemination. In our viewpoint, we discuss and advocate, in the context of nonlinear science, how a notebook article represents an essential step toward this objective by fully embracing cloud computing solutions. Notebook articles as scholar articles offer an alternative, efficient and more ethical way to disseminate research through their versatile environment. This format invites the readers to delve deeper into the reported research. Through the interactivity of the notebook articles, research results such as for instance equations and figures are reproducible even for non-expert readers. The codes and methods are available, in a transparent manner, to interested readers. The methods can be reused and adapted to answer additional questions in related topics. The codes run on cloud computing services, which provide easy access, even to low-income countries and research groups. The versatility of this environment provides the stakeholders - from the researchers to the publishers - with opportunities to disseminate the research results in innovative ways. △ Less","17 February, 2021",https://arxiv.org/pdf/2103.05770
The Physics of Financial Networks,Marco Bardoscia;Paolo Barucca;Stefano Battiston;Fabio Caccioli;Giulio Cimini;Diego Garlaschelli;Fabio Saracco;Tiziano Squartini;Guido Caldarelli,"The field of Financial Networks is a paramount example of the novel applications of Statistical Physics that have made possible by the present data revolution. As the total value of the global financial market has vastly outgrown the value of the real economy, financial institutions on this planet have created a web of interactions whose size and topology calls for a quantitative analysis by means of Complex Networks. Financial Networks are not only a playground for the use of basic tools of statistical physics as ensemble representation and entropy maximization; rather, their particular dynamics and evolution triggered theoretical advancements as the definition of DebtRank to measure the impact and diffusion of shocks in the whole systems. In this review we present the state of the art in this field, starting from the different definitions of financial networks (based either on loans, on assets ownership, on contracts involving several parties -- such as credit default swaps, to multiplex representation when firms are introduced in the game and a link with real economy is drawn) and then discussing the various dynamics of financial contagion as well as applications in financial network inference and validation. We believe that this analysis is particularly timely since financial stability as well as recent innovations in climate finance, once properly analysed and understood in terms of complex network theory, can play a pivotal role in the transformation of our society towards a more sustainable world. △ Less","9 March, 2021",https://arxiv.org/pdf/2103.05623
Development of a VR tool to study pedestrian route and exit choice behaviour in a multi-story building,Yan Feng;Dorine Duives;Serge Hoogendoorn,"Although route and exit choice in complex buildings are important aspects of pedestrian behaviour, studies predominantly investigated pedestrian movement in a single level. This paper presents an innovative VR tool that was designed to investigate pedestrian route and exit choice in a multi-story building. This tool supports free navigation and collects pedestrian walking trajectories, head movements and gaze points automatically. An experiment was conducted to evaluate the VR tool from objective standpoints (i.e., pedestrian behaviour) and subjective standpoints (i.e., the feeling of presence, system usability, simulation sickness). The results show that the VR tool allows for accurate collection of pedestrian behavioural data in the complex building. Moreover, the results of the questionnaire report high realism of the virtual environment, high immersive feeling, high usability, and low simulator sickness. This paper contributes by showcasing an innovative approach of applying VR technologies to study pedestrian behaviour in complex and realistic environments. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.05560
Understanding the Robustness of Skeleton-based Action Recognition under Adversarial Attack,He Wang;Feixiang He;Zhexi Peng;Tianjia Shao;Yong-Liang Yang;Kun Zhou;David Hogg,"Action recognition has been heavily employed in many applications such as autonomous vehicles, surveillance, etc, where its robustness is a primary concern. In this paper, we examine the robustness of state-of-the-art action recognizers against adversarial attack, which has been rarely investigated so far. To this end, we propose a new method to attack action recognizers that rely on 3D skeletal motion. Our method involves an innovative perceptual loss that ensures the imperceptibility of the attack. Empirical studies demonstrate that our method is effective in both white-box and black-box scenarios. Its generalizability is evidenced on a variety of action recognizers and datasets. Its versatility is shown in different attacking strategies. Its deceitfulness is proven in extensive perceptual studies. Our method shows that adversarial attack on 3D skeletal motions, one type of time-series data, is significantly different from traditional adversarial attack problems. Its success raises serious concern on the robustness of action recognizers and provides insights on potential improvements. △ Less","18 March, 2021",https://arxiv.org/pdf/2103.05347
Dynamic Pricing and Learning under the Bass Model,Shipra Agrawal;Steven Yin;Assaf Zeevi,"We consider a novel formulation of the dynamic pricing and demand learning problem, where the evolution of demand in response to posted prices is governed by a stochastic variant of the popular Bass model with parameters α, β that are linked to the so-called ""innovation"" and ""imitation"" effects. Unlike the more commonly used i.i.d. and contextual demand models, in this model the posted price not only affects the demand and the revenue in the current round but also the future evolution of demand, and hence the fraction of potential market size m that can be ultimately captured. In this paper, we consider the more challenging incomplete information problem where dynamic pricing is applied in conjunction with learning the unknown parameters, with the objective of optimizing the cumulative revenues over a given selling horizon of length T. Equivalently, the goal is to minimize the regret which measures the revenue loss of the algorithm relative to the optimal expected revenue achievable under the stochastic Bass model with market size m and time horizon T. Our main contribution is the development of an algorithm that satisfies a high probability regret guarantee of order \tilde O(m^{2/3}); where the market size m is known a priori. Moreover, we show that no algorithm can incur smaller order of loss by deriving a matching lower bound. Unlike most regret analysis results, in the present problem the market size m is the fundamental driver of the complexity; our lower bound in fact, indicates that for any fixed α, β, most non-trivial instances of the problem have constant T and large m. We believe that this insight sets the problem of dynamic pricing under the Bass model apart from the typical i.i.d. setting and multi-armed bandit based models for dynamic pricing, which typically focus only on the asymptotics with respect to time horizon T. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.05199
Socio-Technical Root Cause Analysis of Cyber-enabled Theft of the U.S. Intellectual Property -- The Case of APT41,Mazaher Kianpour,"Increased connectivity has made us all more vulnerable. Cyberspace, besides all its benefits, spawned more devices to hack and more opportunities to commit cybercrime. Criminals have found it lucrative to target both individuals and businesses, by holding or stealing their assets via different types of cyber attacks. The cyber-enabled theft of Intellectual Property (IP), as one of the most important and critical intangible assets of nations, organizations and individuals, by foreign countries has been a devastating challenge of the United States (U.S.) in the past decades. In this study, we conduct a socio-technical root cause analysis to investigate one of the recent cases of IP theft by employing a holistic approach. It concludes with a list of root causes and some corrective actions to stop the impact and prevent the recurrence of the problem in the future. Building upon the findings of this study, the U.S. requires a detailed revision of IP strategies bringing the whole socio-technical regulatory system into focus and strengthen IP rights protection considering China's indigenous innovation policies. It is critical that businesses and other organizations take steps to reduce their exposure to cyber attacks. It is particularly important to train employees on how to spot potential threats, and to institute policies that encourage workers to report potential security failures so that action can be taken quickly. Finally, we discuss how cyber ranges can provide an efficient and safe platform for dealing with such challenges. The results of this study can be expanded to other countries in order to protect their IP rights and deter or prevent and respond to future incidents. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.04901
A Framework for Enabling Safe and Resilient Food Factories for the Public Feeding Programs,Nataraj Kuntagod;Sanjay Podder;Satya Sai Srinivas Abbabathula;Venkatesh Subramanian;Giju Mathew;Suresh Kumar Mani,"Public feeding programs continue to be a major source of nutrition to a large part of the population across the world. Any disruption to these activities, like the one during the Covid-19 pandemic, can lead to adverse health outcomes, especially among children. Policymakers and other stakeholders must balance the need for continuing the feeding programs while ensuring the health and safety of workers engaged in the operations. This has led to several innovations that leverage advanced technologies like AI and IOT to monitor the health and safety of workers and ensure hygienic operations. However, there are practical challenges in its implementation on a large scale. This paper presents an implementation framework to build resilient public feeding programs using a combination of intelligent technologies. The framework is a result of piloting the technology solution at a facility run as part of a large mid-day meal feeding program in India. Using existing resources like CCTV cameras and new technologies like AI and IOT, hygiene and safety compliance anomalies can be detected and reported in a resource-efficient manner. It will guide stakeholders running public feeding programs as they seek to restart suspended operations and build systems that better adapt to future crises. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.04811
Innovation adoption: Broadcasting vs. Virality,Yujia Zhai;Ying Ding;Hezhao Zhang,"Diffusion channels are critical to determining the adoption scale which leads to the ultimate impact of an innovation. The aim of this study is to develop an integrative understanding of the impact of two diffusion channels (i.e., broadcasting vs virality) on innovation adoption. Using citations of a series of classic algorithms and the time series of co-authorship as the footprints of their diffusion trajectories, we propose a novel method to analyze the intertwining relationships between broadcasting and virality in the innovation diffusion process. Our findings show that broadcasting and virality have similar diffusion power, but play different roles across diffusion stages. Broadcasting is more powerful in the early stages but may be gradually caught up or even surpassed by virality in the later period. Meanwhile, diffusion speed in virality is significantly faster than broadcasting and members from virality channels tend to adopt the same innovation repetitively. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.04663
FastFlowNet: A Lightweight Network for Fast Optical Flow Estimation,Lingtong Kong;Chunhua Shen;Jie Yang,"Dense optical flow estimation plays a key role in many robotic vision tasks. In the past few years, with the advent of deep learning, we have witnessed great progress in optical flow estimation. However, current networks often consist of a large number of parameters and require heavy computation costs, largely hindering its application on low power-consumption devices such as mobile phones. In this paper, we tackle this challenge and design a lightweight model for fast and accurate optical flow prediction. Our proposed FastFlowNet follows the widely-used coarse-to-fine paradigm with following innovations. First, a new head enhanced pooling pyramid (HEPP) feature extractor is employed to intensify high-resolution pyramid features while reducing parameters. Second, we introduce a new center dense dilated correlation (CDDC) layer for constructing compact cost volume that can keep large search radius with reduced computation burden. Third, an efficient shuffle block decoder (SBD) is implanted into each pyramid level to accelerate flow estimation with marginal drops in accuracy. Experiments on both synthetic Sintel data and real-world KITTI datasets demonstrate the effectiveness of the proposed approach, which needs only 1/10 computation of comparable networks to achieve on par accuracy. In particular, FastFlowNet only contains 1.37M parameters; and can execute at 90 FPS (with a single GTX 1080Ti) or 5.7 FPS (embedded Jetson TX2 GPU) on a pair of Sintel images of resolution 1024x436. △ Less","21 March, 2021",https://arxiv.org/pdf/2103.04524
RFN-Nest: An end-to-end residual fusion network for infrared and visible images,Hui Li;Xiao-Jun Wu;Josef Kittler,"In the image fusion field, the design of deep learning-based fusion methods is far from routine. It is invariably fusion-task specific and requires a careful consideration. The most difficult part of the design is to choose an appropriate strategy to generate the fused image for a specific task in hand. Thus, devising learnable fusion strategy is a very challenging problem in the community of image fusion. To address this problem, a novel end-to-end fusion network architecture (RFN-Nest) is developed for infrared and visible image fusion. We propose a residual fusion network (RFN) which is based on a residual architecture to replace the traditional fusion approach. A novel detail-preserving loss function, and a feature enhancing loss function are proposed to train RFN. The fusion model learning is accomplished by a novel two-stage training strategy. In the first stage, we train an auto-encoder based on an innovative nest connection (Nest) concept. Next, the RFN is trained using the proposed loss functions. The experimental results on public domain data sets show that, compared with the existing methods, our end-to-end fusion network delivers a better performance than the state-of-the-art methods in both subjective and objective evaluation. The code of our fusion method is available at https://github.com/hli1221/imagefusion-rfn-nest △ Less","14 March, 2021",https://arxiv.org/pdf/2103.04286
"Smart Speakers, the Next Frontier in Computational Health",Jacob Sunshine,"The rapid dissemination and adoption of smart speakers has enabled substantial opportunities to improve human health. Just as the introduction of the mobile phone led to considerable health innovation, smart speaker computing systems carry several unique advantages that have the potential to catalyze new fields of health research, particularly in out-of-hospital environments. The recent rise and ubiquity of these smart computing systems hold significant potential for enhancing chronic disease management, enabling passive identification of unwitnessed medical emergencies, detecting subtle changes in human behavior and cognition, limiting isolation, and potentially allowing widespread, passive, remote monitoring of respiratory diseases that impact the public health. There are 3 broad mechanisms for how a smart speaker can interact with a person to improve health. These include (i) as an intelligent conversational agent, (ii) a passive identifier of medically relevant diagnostic sounds and (iii) active sensing using the device's internal hardware to measure physiologic parameters, such as with active sonar, radar or computer vision. Each of these different modalities have specific clinical use cases, all of which need to be balanced against potential privacy concerns, equity related to system access and regulatory frameworks which have not yet been developed for this unique type of passive data collection. △ Less","6 March, 2021",https://arxiv.org/pdf/2103.04209
Using Dual-Network Analyser for extracting communities from Dual Networks,Pietro Hiram Guzzi;Giuseppe Tradigo;Pierangelo Veltri,"The representation of data and its relationships using networks is prevalent in many research fields such as computational biology, medical informatics and social networks. Recently, complex networks models have been introduced to better capture the insights of the modelled scenarios. Among others, dual networks -based models have been introduced, which consist in mapping information as pair of networks containing the same nodes but different edges. We focus on the use of a novel approach to visualise and analyse dual networks. The method uses two algorithms for community discovery, and it is provided as a Python-based tool with a graphical user interface. The tool is able to load dual networks and to extract both the densest connected subgraph as well as the common modular communities. The latter is obtained by using an adapted implementation of the Louvain algorithm. The proposed algorithm and graphical tool have been tested by using social, biological, and co-authorship networks. Results demonstrate that the proposed approach is efficient and is able to extract meaningful information from dual networks. Finally, as contribution, the proposed graphical user interface can be considered a valuable innovation to the context. △ Less","5 March, 2021",https://arxiv.org/pdf/2103.03728
Self-supervised Mean Teacher for Semi-supervised Chest X-ray Classification,Fengbei Liu;Yu Tian;Filipe R. Cordeiro;Vasileios Belagiannis;Ian Reid;Gustavo Carneiro,"The training of deep learning models generally requires a large amount of annotated data for effective convergence and generalisation. However, obtaining high-quality annotations is a laboursome and expensive process due to the need of expert radiologists for the labelling task. The study of semi-supervised learning in medical image analysis is then of crucial importance given that it is much less expensive to obtain unlabelled images than to acquire images labelled by expert radiologists. Essentially, semi-supervised methods leverage large sets of unlabelled data to enable better training convergence and generalisation than using only the small set of labelled images. In this paper, we propose Self-supervised Mean Teacher for Semi-supervised (S^2MTS^2) learning that combines self-supervised mean-teacher pre-training with semi-supervised fine-tuning. The main innovation of S^2MTS^2 is the self-supervised mean-teacher pre-training based on the joint contrastive learning, which uses an infinite number of pairs of positive query and key features to improve the mean-teacher representation. The model is then fine-tuned using the exponential moving average teacher framework trained with semi-supervised learning. We validate S^2MTS^2 on the multi-label classification problems from Chest X-ray14 and CheXpert, and the multi-class classification from ISIC2018, where we show that it outperforms the previous SOTA semi-supervised learning methods by a large margin. △ Less","4 November, 2021",https://arxiv.org/pdf/2103.03629
Graph-Based Tri-Attention Network for Answer Ranking in CQA,Wei Zhang;Zeyuan Chen;Chao Dong;Wen Wang;Hongyuan Zha;Jianyong Wang,"In community-based question answering (CQA) platforms, automatic answer ranking for a given question is critical for finding potentially popular answers in early times. The mainstream approaches learn to generate answer ranking scores based on the matching degree between question and answer representations as well as the influence of respondents. However, they encounter two main limitations: (1) Correlations between answers in the same question are often overlooked. (2) Question and respondent representations are built independently of specific answers before affecting answer representations. To address the limitations, we devise a novel graph-based tri-attention network, namely GTAN, which has two innovations. First, GTAN proposes to construct a graph for each question and learn answer correlations from each graph through graph neural networks (GNNs). Second, based on the representations learned from GNNs, an alternating tri-attention method is developed to alternatively build target-aware respondent representations, answer-specific question representations, and context-aware answer representations by attention computation. GTAN finally integrates the above representations to generate answer ranking scores. Experiments on three real-world CQA datasets demonstrate GTAN significantly outperforms state-of-the-art answer ranking methods, validating the rationality of the network architecture. △ Less","19 March, 2021",https://arxiv.org/pdf/2103.03583
Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food,Quin Thames;Arjun Karpur;Wade Norris;Fangting Xia;Liviu Panait;Tobias Weyand;Jack Sim,"Understanding the nutritional content of food from visual data is a challenging computer vision problem, with the potential to have a positive and widespread impact on public health. Studies in this area are limited to existing datasets in the field that lack sufficient diversity or labels required for training models with nutritional understanding capability. We introduce Nutrition5k, a novel dataset of 5k diverse, real world food dishes with corresponding video streams, depth images, component weights, and high accuracy nutritional content annotation. We demonstrate the potential of this dataset by training a computer vision algorithm capable of predicting the caloric and macronutrient values of a complex, real world dish at an accuracy that outperforms professional nutritionists. Further we present a baseline for incorporating depth sensor data to improve nutrition predictions. We will publicly release Nutrition5k in the hope that it will accelerate innovation in the space of nutritional understanding. △ Less","22 June, 2021",https://arxiv.org/pdf/2103.03375
Translating declarative control elements to imperative using 'l-value redefinition graphs',Anthony Savidis,"We focus on control constructs that allow programmers define actions to be performed when respective conditions are met without requiring the explicit evaluation and testing of conditions as part of an imperative algorithm. Such elements are commonly referred as declarative, not theoretically related to declarative languages. We introduce declarative constructs in the C++ language, presenting the translation method to standard C++. The innovative feature of our method is the accommodation of l-values involving arbitrary pointer / array expressions and objects, supporting immediate runtime evaluation upon content update even if such l-values bind to variant storage locations at runtime. To accomplish this we define 'l-value redefinition graphs', capturing storage binding dependencies among variables, being the floor-plan of our code generation and runtime management approach. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.03309
The RECIPE Approach to Challenges in Deeply Heterogeneous High Performance Systems,Giovanni Agosta;William Fornaciari;David Atienza;Ramon Canal;Alessandro Cilardo;José Flich Cardo;Carles Hernandez Luz;Michal Kulczewski;Giuseppe Massari;Rafael Tornero Gavilá;Marina Zapater,"RECIPE (REliable power and time-ConstraInts-aware Predictive management of heterogeneous Exascale systems) is a recently started project funded within the H2020 FETHPC programme, which is expressly targeted at exploring new High-Performance Computing (HPC) technologies. RECIPE aims at introducing a hierarchical runtime resource management infrastructure to optimize energy efficiency and minimize the occurrence of thermal hotspots, while enforcing the time constraints imposed by the applications and ensuring reliability for both time-critical and throughput-oriented computation that run on deeply heterogeneous accelerator-based systems. This paper presents a detailed overview of RECIPE, identifying the fundamental challenges as well as the key innovations addressed by the project. In particular, the need for predictive reliability approaches to maximize hardware lifetime and guarantee application performance is identified as the key concern for RECIPE, and is addressed via hierarchical resource management of the heterogeneous architectural components of the system, driven by estimates of the application latency and hardware reliability obtained respectively through timing analysis and modelling thermal properties, mean-time-to-failure of subsystems. We show the impact of prediction accuracy on the overheads imposed by the checkpointing policy, as well as a possible application to a weather forecasting use case. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.03044
An Internet of Things Service Roadmap,Athman Bouguettaya;Quan Z. Sheng;Boualem Benatallah;Azadeh Ghari Neiat;Sajib Mistry;Aditya Ghose;Surya Nepal;Lina Yao,"We propose a roadmap for leveraging the tremendous opportunities the Internet of Things (IoT) has to offer. We argue that the combination of the recent advances in service computing and IoT technology provide a unique framework for innovations not yet envisaged, as well as the emergence of yet-to-be-developed IoT applications. This roadmap covers: emerging novel IoT services, articulation of major research directions, and suggestion of a roadmap to guide the IoT and service computing community to address key IoT service challenges. △ Less","1 February, 2021",https://arxiv.org/pdf/2103.03043
Pandemic Drugs at Pandemic Speed: Infrastructure for Accelerating COVID-19 Drug Discovery with Hybrid Machine Learning- and Physics-based Simulations on High Performance Computers,Agastya P. Bhati;Shunzhou Wan;Dario Alfè;Austin R. Clyde;Mathis Bode;Li Tan;Mikhail Titov;Andre Merzky;Matteo Turilli;Shantenu Jha;Roger R. Highfield;Walter Rocchia;Nicola Scafuri;Sauro Succi;Dieter Kranzlmüller;Gerald Mathias;David Wifling;Yann Donon;Alberto Di Meglio;Sofia Vallecorsa;Heng Ma;Anda Trifan;Arvind Ramanathan;Tom Brettin;Alexander Partin,"The race to meet the challenges of the global pandemic has served as a reminder that the existing drug discovery process is expensive, inefficient and slow. There is a major bottleneck screening the vast number of potential small molecules to shortlist lead compounds for antiviral drug development. New opportunities to accelerate drug discovery lie at the interface between machine learning methods, in this case developed for linear accelerators, and physics-based methods. The two in silico methods, each have their own advantages and limitations which, interestingly, complement each other. Here, we present an innovative infrastructural development that combines both approaches to accelerate drug discovery. The scale of the potential resulting workflow is such that it is dependent on supercomputing to achieve extremely high throughput. We have demonstrated the viability of this workflow for the study of inhibitors for four COVID-19 target proteins and our ability to perform the required large-scale calculations to identify lead antiviral compounds through repurposing on a variety of supercomputers. △ Less","4 September, 2021",https://arxiv.org/pdf/2103.02843
A Novel Context-Aware Multimodal Framework for Persian Sentiment Analysis,Kia Dashtipour;Mandar Gogate;Erik Cambria;Amir Hussain,"Most recent works on sentiment analysis have exploited the text modality. However, millions of hours of video recordings posted on social media platforms everyday hold vital unstructured information that can be exploited to more effectively gauge public perception. Multimodal sentiment analysis offers an innovative solution to computationally understand and harvest sentiments from videos by contextually exploiting audio, visual and textual cues. In this paper, we, firstly, present a first of its kind Persian multimodal dataset comprising more than 800 utterances, as a benchmark resource for researchers to evaluate multimodal sentiment analysis approaches in Persian language. Secondly, we present a novel context-aware multimodal sentiment analysis framework, that simultaneously exploits acoustic, visual and textual cues to more accurately determine the expressed sentiment. We employ both decision-level (late) and feature-level (early) fusion methods to integrate affective cross-modal information. Experimental results demonstrate that the contextual integration of multimodal features such as textual, acoustic and visual features deliver better performance (91.39%) compared to unimodal features (89.24%). △ Less","3 March, 2021",https://arxiv.org/pdf/2103.02636
The Ultimate Weapon for Ultra-Broadband 6G: Digital Beamforming and Doubly Massive mmWave MIMO,Bengt Lindoff;Carmen D'Andrea;Stefano Buzzi;Markus Tormanen;Per-Olof Brandt,"The use of millimeter waves for wireless communications is one of the main technological innovations of 5G systems with respect to previous generations of cellular systems. Their consideration, however, has been up to now mainly restricted to the case in which analog or, at most, hybrid analog-digital beamforming structures were used, thus posing a limitation on the multiplexing capabilities and peak data rates that could be theoretically achieved at these frequencies. Recent progress in the field of electronics, however, has made the energy consumption of digital beamforming structures at least on par with that of analog beamforming, thus redeeming them from the ghetto they had been placed in over the last year. Digital beamforming, coupled with the use of large antenna arrays at both sides of the communication link, promises thus to be one of the secret weapons of future 6G networks, capable of unleashing unprecedented values of spectral and energy efficiency for ultra-broadband connectivity. △ Less","3 March, 2021",https://arxiv.org/pdf/2103.02286
Determinants of ICT Adoption Among Small Scale Agribusiness Enterprises In Somalia,Husein Osman Abdullahi;Abdikarim Abi Hassan;Murni Mahmud;Abdifatah Farah Ali,"The use of Information and Communication Technology (ICT) can advance the Agricultural Business sector, particularly in a country seeking opportunities to explore the sector. There is evidence that ICT has made significant contributions to agribusiness because it allows enterprises to manage their operations, and it has major impacts on the business. However, the critical factors that motivate the adoption of new innovative technology by agribusiness enterprises are underexplored. The literature has indicated ICT adoption among small-scale agribusiness enterprises in Somalia is not fully understood. Nevertheless, this study addresses this gap by investigating the adoption of ICT among small-scale agribusiness enterprises in Somalia. The paper reports the use of the Technology, Organization, Environment (TOE) framework. An online survey has been conducted with random sampling for data collection, with 107 respondents. The respondents are from agribusiness staff and farmers from various agricultural companies in Somalia. After quantitative data analysis, the results indicated that relative advantage, complexity, top management support, and competitive pressure factors are significant contributors to ICT adoption in Somalian agribusiness enterprises, while ICT costs and vendor support are not significantly related to the adoption of ICT in agricultural business. This study concludes that ICT adoption in Somalia is inspired by insight and motivation rather than financial and external support. Understanding these factors leads to a better understanding of ICT adoption in Somalia. Additionally, it enriches the literature about the agriculture business on the African continent Keywords: Determinants, ICT Adoption, Agribusiness, Small Scale, TOE framework △ Less","24 February, 2021",https://arxiv.org/pdf/2103.01769
The Difficulty in Scaling Blockchains: A Simple Explanation,Maarten van Steen;Andrew Chien;Patrick Eugster,"Blockchains have become immensely popular and are high on the list of national and international research and innovation agenda's. This is partly caused by the numerous interesting applications, combined with the promise of full decentralization and high scalability (among others). Keeping that promise, however, is technically extremely demanding and has already required substantial scientific efforts, which so far have not been overwhelmingly successful. In this paper, we provide a laymen's description of what may turn out to be a fundamental hurdle in attaining blockchains that combine scalability, high transaction processing capabilities, and are indeed fully decentralized. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.01487
Deep Unfolded Recovery of Sub-Nyquist Sampled Ultrasound Image,Alon Mamistvalov;Yonina C. Eldar,"The most common technique for generating B-mode ultrasound (US) images is delay and sum (DAS) beamforming, where the signals received at the transducer array are sampled before an appropriate delay is applied. This necessitates sampling rates exceeding the Nyquist rate and the use of a large number of antenna elements to ensure sufficient image quality. Recently we proposed methods to reduce the sampling rate and the array size relying on image recovery using iterative algorithms, based on compressed sensing (CS) and the finite rate of innovation (FRI) frameworks. Iterative algorithms typically require a large number of iterations, making them difficult to use in real-time. Here, we propose a reconstruction method from sub-Nyquist samples in the time and spatial domain, that is based on unfolding the ISTA algorithm, resulting in an efficient and interpretable deep network. The inputs to our network are the subsampled beamformed signals after summation and delay in the frequency domain, requiring only a subset of the US signal to be stored for recovery. Our method allows reducing the number of array elements, sampling rate, and computational time while ensuring high quality imaging performance. Using \emph{in vivo} data we demonstrate that the proposed method yields high-quality images while reducing the data volume traditionally used up to 36 times. In terms of image resolution and contrast, our technique outperforms previously suggested methods as well as DAS and minimum-variance (MV) beamforming, paving the way to real-time applicable recovery methods. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01263
Enhancing the role of academic librarians in conducting scoping reviews,Peter Kokol;Jernej Završnik;Marko Turčin;Helena Blažun Vošner,"Information exposing, in conjunction with technological innovations and the emergence of social media, altered the traditional roles of academic libraries and enabled librarians to become necessary partners in research. The role of academic librarians in conducting systematic reviews is well recognised, however, their role in conducting scoping reviews is not yet well established. Nevertheless, we propose that, in more and more frequent situations when it is not feasible to read and analyse all relevant literature to be scoped manually, librarians employ bibliometric analysis and mapping to visualise and chart literature content. Our study demonstrated that science landscapes induced automatically by bibliometric mapping software could serve as a tool to visualise and chart the content of relevant literature when conducting the fourth step of scoping reviews. Additionaly science landscapes can help also serve to help improve the decision strategies when conducting scoping reviews. Keywords: Scoping △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01099
Um Estudo sobre Atividades Participativas para Soluções IoT para o Home care de Pessoas Idosas,Renata de Podestá Gaspar;Rodrigo Bonacin;Vinícius Gonçalves,"Population aging in Brazil and in the world occurs at the same time of advances and evolutions in technology. Thus, opportunities for new solutions arise for the elderly, such as innovations in Home Care. With the Internet of Things, it is possible to improve the elderly autonomy, safety and quality of life. However, the design of IoT solutions for elderly Home Care poses new challenges. In this context, this technical report aims to detail activities developed as a case study to evaluate the IoT-PMHCS Method, which was developed in the context of the Master's program in Computer Science at UNIFACCAMP, Brazil. This report includes the planning and results of interviews, participatory workshops, validations, simulation of solutions, among other activities. This document reports the practical experience of applying the IoT-PMHCS Method. -- O envelhecimento populacional no Brasil e no mundo ocorre ao mesmo tempo que os avanços e evoluções na tecnologia. Desta forma, surgem oportunidades de novas soluções para o público idoso, tais como inovações em Home Care. Com a Internet das Coisas é possível promover maior autonomia, segurança e qualidade de vida aos idosos. Entretanto, o design de soluções de IoT para Home Care de pessoas idosas traz novos desafios. Diante disto, este relatório técnico tem o objetivo de detalhar atividades desenvolvidas como estudo de caso para avaliação do Método IoT-PMHCS, desenvolvido no contexto do programa de Mestrado em Ciência da Computação da UNIFACCAMP, Brasil. O relatório inclui o planejamento e resultados de entrevistas, workshops participativos, pesquisas de validação, simulação de soluções, dentre outras atividades. Este documento relata a experiência prática da aplicação do Método IoT-PMHCS. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01078
Anticipation Next -- System-sensitive technology development and integration in work contexts,Sarah Janboecke;Susanne Zajitschek,"When discussing future concerns within socio-technical systems in work contexts, we often find descriptions of missed technology development and integration. The experience of technology that fails whilst being integrated is often rooted in dysfunctional epistemological approaches within the research and development process. Thus, ultimately leading to sustainable technology-distrust in work contexts. This is true for organizations that integrate new technologies and for organizations that invent them. Organizations in which we find failed technology development and integrations are, in their very nature, social systems. Nowadays, those complex social systems act within an even more complex environment. This urges the development of new anticipation methods for technology development and integration. Gathering of and dealing with complex information in the described context is what we call Anticipation Next. This explorative work uses existing literature from the adjoining research fields of system theory, organizational theory, and socio-technical research to combine various concepts. We deliberately aim at a networked way of thinking in scientific contexts and thus combine multidisciplinary subject areas in one paper to present an innovative way to deal with multi-faceted problems in a human-centred way. We end with suggesting a conceptual framework that should be used in the very early stages of technology development and integration in work contexts. △ Less","8 July, 2021",https://arxiv.org/pdf/2103.00923
Rethinking complexity for software code structures: A pioneering study on Linux kernel code repository,Wenhe Zhang;Jin He;Kevin Song,"The recent progress of artificial intelligence(AI) has shown great potentials for alleviating human burden in various complex tasks. From the view of software engineering, AI techniques can be seen in many fundamental aspects of development, such as source code comprehension, in which state-of-the-art models are implemented to extract and express the meaning of code snippets automatically. However, such technologies are still struggling to tackle and comprehend the complex structures within industrial code, thus far from real-world applications. In the present work, we built an innovative and systematical framework, emphasizing the problem of complexity in code comprehension and further software engineering. Upon automatic data collection from the latest Linux kernel source code, we modeled code structures as complex networks through token extraction and relation parsing. Comprehensive analysis of complexity further revealed the density and scale of network-based code representations. Our work constructed the first large-scale dataset from industrial-strength software code for downstream software engineering tasks including code comprehension, and incorporated complex network theory into code-level investigations of software development for the first time. In the longer term, the proposed methodology could play significant roles in the entire software engineering process, powering software design, coding, debugging, testing, and sustaining by redefining and embracing complexity. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.00821
Topic Modelling Meets Deep Neural Networks: A Survey,He Zhao;Dinh Phung;Viet Huynh;Yuan Jin;Lan Du;Wray Buntine,"Topic modelling has been a successful technique for text analysis for almost twenty years. When topic modelling met deep neural networks, there emerged a new and increasingly popular research area, neural topic models, with over a hundred models developed and a wide range of applications in neural language understanding such as text generation, summarisation and language models. There is a need to summarise research developments and discuss open problems and future directions. In this paper, we provide a focused yet comprehensive overview of neural topic models for interested researchers in the AI community, so as to facilitate them to navigate and innovate in this fast-growing research area. To the best of our knowledge, ours is the first review focusing on this specific topic. △ Less","28 February, 2021",https://arxiv.org/pdf/2103.00498
Seamless Variability Management With the Virtual Platform,Wardah Mahmood;Daniel Strüber;Thorsten Berger;Ralf Lämmel;Mukelabai Mukelabai,"Customization is a general trend in software engineering, demanding systems that support variable stakeholder requirements. Two opposing strategies are commonly used to create variants: software clone & own and software configuration with an integrated platform. Organizations often start with the former, which is cheap, agile, and supports quick innovation, but does not scale. The latter scales by establishing an integrated platform that shares software assets between variants, but requires high up-front investments or risky migration processes. So, could we have a method that allows an easy transition or even combine the benefits of both strategies? We propose a method and tool that supports a truly incremental development of variant-rich systems, exploiting a spectrum between both opposing strategies. We design, formalize, and prototype the variability-management framework virtual platform. It bridges clone & own and platform-oriented development. Relying on programming-language-independent conceptual structures representing software assets, it offers operators for engineering and evolving a system, comprising: traditional, asset-oriented operators and novel, feature-oriented operators for incrementally adopting concepts of an integrated platform. The operators record meta-data that is exploited by other operators to support the transition. Among others, they eliminate expensive feature-location effort or the need to trace clones. Our evaluation simulates the evolution of a real-world, clone-based system, measuring its costs and benefits. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.00437
UAV-Enabled Wireless Power Transfer: A Tutorial Overview,Lifeng Xie;Xiaowen Cao;Jie Xu;Rui Zhang,"Unmanned aerial vehicle (UAV)-enabled wireless power transfer (WPT) has recently emerged as a promising technique to provide sustainable energy supply for widely distributed low-power ground devices (GDs) in large-scale wireless networks. Compared with the energy transmitters (ETs) in conventional WPT systems which are deployed at fixed locations, UAV-mounted aerial ETs can fly flexibly in the three-dimensional (3D) space to charge nearby GDs more efficiently. This paper provides a tutorial overview on UAV-enabled WPT and its appealing applications, in particular focusing on how to exploit UAVs' controllable mobility via their 3D trajectory design to maximize the amounts of energy transferred to all GDs in a wireless network with fairness. First, we consider the single-UAV-enabled WPT scenario with one UAV wirelessly charging multiple GDs at known locations. To solve the energy maximization problem in this case, we present a general trajectory design framework consisting of three innovative approaches to optimize the UAV trajectory, which are multi-location hovering, successive-hover-and-fly, and time-quantization-based optimization, respectively. Next, we consider the multi-UAV-enabled WPT scenario where multiple UAVs cooperatively charge many GDs in a large area. Building upon the single-UAV trajectory design, we propose two efficient schemes to jointly optimize multiple UAVs' trajectories, based on the principles of UAV swarming and GD clustering, respectively. Furthermore, we consider two important extensions of UAV-enabled WPT, namely UAV-enabled wireless powered communication networks (WPCN) and UAV-enabled wireless powered mobile edge computing (MEC). △ Less","27 February, 2021",https://arxiv.org/pdf/2103.00207
A Soft Method for Outliers Detection at the Edge of the Network,Kostas Kolomvatsos;Christos Anagnostopoulos,"The combination of the Internet of Things and the Edge Computing gives many opportunities to support innovative applications close to end users. Numerous devices present in both infrastructures can collect data upon which various processing activities can be performed. However, the quality of the outcomes may be jeopardized by the presence of outliers. In this paper, we argue on a novel model for outliers detection by elaborating on a `soft' approach. Our mechanism is built upon the concepts of candidate and confirmed outliers. Any data object that deviates from the population is confirmed as an outlier only after the study of its sequence of magnitude values as new data are incorporated into our decision making model. We adopt the combination of a sliding with a landmark window model when a candidate outlier is detected to expand the sequence of data objects taken into consideration. The proposed model is fast and efficient as exposed by our experimental evaluation while a comparative assessment reveals its pros and cons. △ Less","27 February, 2021",https://arxiv.org/pdf/2103.00179
Novelty and Primacy: A Long-Term Estimator for Online Experiments,Soheil Sadeghi;Somit Gupta;Stefan Gramatovici;Jiannan Lu;Hao Ai;Ruhan Zhang,"Online experiments are the gold standard for evaluating impact on user experience and accelerating innovation in software. However, since experiments are typically limited in duration, observed treatment effects are not always permanently stable, sometimes revealing increasing or decreasing patterns over time. There are multiple causes for a treatment effect to change over time. In this paper, we focus on a particular cause, user-learning, which is primarily associated with novelty or primacy. Novelty describes the desire to use new technology that tends to diminish over time. Primacy describes the growing engagement with technology as a result of adoption of the innovation. User-learning estimation is critical because it holds experimentation responsible for trustworthiness, empowers organizations to make better decisions by providing a long-term view of expected impact, and prevents user dissatisfaction. In this paper, we propose an observational approach, based on difference-in-differences technique to estimate user-learning at scale. We use this approach to test and estimate user-learning in many experiments at Microsoft. We compare our approach with the existing experimental method to show its benefits in terms of ease of use and higher statistical power, and to discuss its limitation in presence of other forms of treatment interaction with time. △ Less","17 February, 2021",https://arxiv.org/pdf/2102.12893
Modelling SARS-CoV-2 coevolution with genetic algorithms,Aymeric Vie,"At the end of 2020, policy responses to the SARS-CoV-2 outbreak have been shaken by the emergence of virus variants, impacting public health and policy measures worldwide. The emergence of these strains suspected to be more contagious, more severe, or even resistant to antibodies and vaccines, seem to have taken by surprise health services and policymakers, struggling to adapt to the new variants constraints. Anticipating the emergence of these mutations to plan ahead adequate policies, and understanding how human behaviors may affect the evolution of viruses by coevolution, are key challenges. In this article, we propose coevolution with genetic algorithms (GAs) as a credible approach to model this relationship, highlighting its implications, potential and challenges. Because of their qualities of exploration of large spaces of possible solutions, capacity to generate novelty, and natural genetic focus, GAs are relevant for this issue. We present a dual GA model in which both viruses aiming for survival and policy measures aiming at minimising infection rates in the population, competitively evolve. This artificial coevolution system may offer us a laboratory to ""debug"" our current policy measures, identify the weaknesses of our current strategies, and anticipate the evolution of the virus to plan ahead relevant policies. It also constitutes a decisive opportunity to develop new genetic algorithms capable of simulating much more complex objects. We highlight some structural innovations for GAs for that virus evolution context that may carry promising developments in evolutionary computation, artificial life and AI. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12365
Uniform Elgot Iteration in Foundations,Sergey Goncharov,"Category theory is famous for its innovative way of thinking of concepts by their descriptions, in particular by establishing universal properties. Concepts that can be characterized in a universal way receive a certain quality seal, which makes them easily transferable across application domains. The notion of partiality is however notoriously difficult to characterize in this way, although the importance of it is certain, especially for computer science where entire research areas, such as synthetic and axiomatic domain theory revolve around notions of partiality. More recently, this issue resurfaced in the context of (constructive) intensional type theory. Here, we provide a generic categorical iteration-based notion of partiality, which is arguably the most basic one. We show that the emerging free structures, which we dub uniform-iteration algebras enjoy various desirable properties, in particular, yield an equational lifting monad. We then study the impact of classicality assumptions and choice principles on this monad, in particular, we establish a suitable categorial formulation of the axiom of countable choice entailing that the monad is an Elgot monad. △ Less","3 July, 2021",https://arxiv.org/pdf/2102.11828
Data Engineering for Everyone,Vijay Janapa Reddi;Greg Diamos;Pete Warden;Peter Mattson;David Kanter,"Data engineering is one of the fastest-growing fields within machine learning (ML). As ML becomes more common, the appetite for data grows more ravenous. But ML requires more data than individual teams of data engineers can readily produce, which presents a severe challenge to ML deployment at scale. Much like the software-engineering revolution, where mass adoption of open-source software replaced the closed, in-house development model for infrastructure code, there is a growing need to enable rapid development and open contribution to massive machine learning data sets. This article shows that open-source data sets are the rocket fuel for research and innovation at even some of the largest AI organizations. Our analysis of nearly 2000 research publications from Facebook, Google and Microsoft over the past five years shows the widespread use and adoption of open data sets. Open data sets that are easily accessible to the public are vital to accelerating ML innovation for everyone. But such open resources are scarce in the wild. So, what if we are able to accelerate data-set creation via automatic data set generation tools? △ Less","22 February, 2021",https://arxiv.org/pdf/2102.11447
Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications using Emotional Intelligence,Falguni Patel;NirmalKumar Patel;Santosh Kumar Bharti,"Veracity is an essential key in research and development of innovative products. Live Emotion analysis and verification nullify deceit made to complainers on live chat, corroborate messages of both ends in messaging apps and promote an honest conversation between users. The main concept behind this emotion artificial intelligent verifier is to license or decline message accountability by comparing variegated emotions of chat app users recognized through facial expressions and text prediction. In this paper, a proposed emotion intelligent live detector acts as an honest arbiter who distributes facial emotions into labels namely, Happiness, Sadness, Surprise, and Hate. Further, it separately predicts a label of messages through text classification. Finally, it compares both labels and declares the message as a fraud or a bonafide. For emotion detection, we deployed Convolutional Neural Network (CNN) using a miniXception model and for text prediction, we selected Support Vector Machine (SVM) natural language processing probability classifier due to receiving the best accuracy on training dataset after applying Support Vector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and Logistic regression. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.11318
You Only Compress Once: Optimal Data Compression for Estimating Linear Models,Jeffrey Wong;Eskil Forsell;Randall Lewis;Tobias Mao;Matthew Wardrop,"Linear models are used in online decision making, such as in machine learning, policy algorithms, and experimentation platforms. Many engineering systems that use linear models achieve computational efficiency through distributed systems and expert configuration. While there are strengths to this approach, it is still difficult to have an environment that enables researchers to interactively iterate and explore data and models, as well as leverage analytics solutions from the open source community. Consequently, innovation can be blocked. Conditionally sufficient statistics is a unified data compression and estimation strategy that is useful for the model development process, as well as the engineering deployment process. The strategy estimates linear models from compressed data without loss on the estimated parameters and their covariances, even when errors are autocorrelated within clusters of observations. Additionally, the compression preserves almost all interactions with the the original data, unlocking better productivity for both researchers and engineering systems. △ Less","3 March, 2021",https://arxiv.org/pdf/2102.11297
Sign-regularized Multi-task Learning,Johnny Torres;Guangji Bai;Junxiang Wang;Liang Zhao;Carmen Vaca;Cristina Abad,"Multi-task learning is a framework that enforces different learning tasks to share their knowledge to improve their generalization performance. It is a hot and active domain that strives to handle several core issues; particularly, which tasks are correlated and similar, and how to share the knowledge among correlated tasks. Existing works usually do not distinguish the polarity and magnitude of feature weights and commonly rely on linear correlation, due to three major technical challenges in: 1) optimizing the models that regularize feature weight polarity, 2) deciding whether to regularize sign or magnitude, 3) identifying which tasks should share their sign and/or magnitude patterns. To address them, this paper proposes a new multi-task learning framework that can regularize feature weight signs across tasks. We innovatively formulate it as a biconvex inequality constrained optimization with slacks and propose a new efficient algorithm for the optimization with theoretical guarantees on generalization performance and convergence. Extensive experiments on multiple datasets demonstrate the proposed methods' effectiveness, efficiency, and reasonableness of the regularized feature weighted patterns. △ Less","22 February, 2021",https://arxiv.org/pdf/2102.11191
Belief-Propagation Decoding of LDPC Codes with Variable Node-Centric Dynamic Schedules,Tofar C. -Y. Chang;Pin-Han Wang;Jian-Jia Weng;I-Hsiang Lee;Yu T. Su,"Belief propagation (BP) decoding of low-density parity-check (LDPC) codes with various dynamic decoding schedules have been proposed to improve the efficiency of the conventional flooding schedule. As the ultimate goal of an ideal LDPC code decoder is to have correct bit decisions, a dynamic decoding schedule should be variable node (VN)-centric and be able to find the VNs with probable incorrect decisions and having a good chance to be corrected if chosen for update. We propose a novel and effective metric called conditional innovation (CI) which serves this design goal well. To make the most of dynamic scheduling which produces high-reliability bit decisions, we limit our search for the candidate VNs to those related to the latest updated nodes only. Based on the CI metric and the new search guideline separately or in combination, we develop several highly efficient decoding schedules. To reduce decoding latency, we introduce multi-edge updating versions which offer extra latency-performance tradeoffs. Numerical results show that both single-edge and multi-edge algorithms provide better decoding performance against most dynamic schedules and the CI-based algorithms are particularly impressive at the first few decoding iterations. △ Less","22 February, 2021",https://arxiv.org/pdf/2102.11089
"An open access NLP dataset for Arabic dialects : Data collection, labeling, and model construction",ElMehdi Boujou;Hamza Chataoui;Abdellah El Mekki;Saad Benjelloun;Ikram Chairi;Ismail Berrada,"Natural Language Processing (NLP) is today a very active field of research and innovation. Many applications need however big sets of data for supervised learning, suitably labelled for the training purpose. This includes applications for the Arabic language and its national dialects. However, such open access labeled data sets in Arabic and its dialects are lacking in the Data Science ecosystem and this lack can be a burden to innovation and research in this field. In this work, we present an open data set of social data content in several Arabic dialects. This data was collected from the Twitter social network and consists on +50K twits in five (5) national dialects. Furthermore, this data was labeled for several applications, namely dialect detection, topic detection and sentiment analysis. We publish this data as an open access data to encourage innovation and encourage other works in the field of NLP for Arabic dialects and social media. A selection of models were built using this data set and are presented in this paper along with their performances. △ Less","6 February, 2021",https://arxiv.org/pdf/2102.11000
Combining Spiking Neural Network and Artificial Neural Network for Enhanced Image Classification,Naoya Muramatsu;Hai-Tao Yu,"With the continued innovations of deep neural networks, spiking neural networks (SNNs) that more closely resemble biological brain synapses have attracted attention owing to their low power consumption.However, for continuous data values, they must employ a coding process to convert the values to spike trains.Thus, they have not yet exceeded the performance of artificial neural networks (ANNs), which handle such values directly.To this end, we combine an ANN and an SNN to build versatile hybrid neural networks (HNNs) that improve the concerned performance.To qualify this performance, MNIST and CIFAR-10 image datasets are used for various classification tasks in which the training and coding methods changes.In addition, we present simultaneous and separate methods to train the artificial and spiking layers, considering the coding methods of each.We find that increasing the number of artificial layers at the expense of spiking layers improves the HNN performance.For straightforward datasets such as MNIST, it is easy to achieve the same performance as ANNs by using duplicate coding and separate learning.However, for more complex tasks, the use of Gaussian coding and simultaneous learning is found to improve the accuracy of HNNs while utilizing a smaller number of artificial layers. △ Less","28 February, 2021",https://arxiv.org/pdf/2102.10592
Dynamical Analysis of the EIP-1559 Ethereum Fee Market,Stefanos Leonardos;Barnabé Monnot;Daniël Reijsbergen;Stratis Skoulakis;Georgios Piliouras,"Participation in permissionless blockchains results in competition over system resources, which needs to be controlled with fees. Ethereum's current fee mechanism is implemented via a first-price auction that results in unpredictable fees as well as other inefficiencies. EIP-1559 is a recent, improved proposal that introduces a number of innovative features such as a dynamically adaptive base fee that is burned, instead of being paid to the miners. Despite intense interest in understanding its properties, several basic questions such as whether and under what conditions does this protocol self-stabilize have remained elusive thus far. We perform a thorough analysis of the resulting fee market dynamic mechanism via a combination of tools from game theory and dynamical systems. We start by providing bounds on the step-size of the base fee update rule that suffice for global convergence to equilibrium via Lyapunov arguments. In the negative direction, we show that for larger step-sizes instability and even formally chaotic behavior are possible under a wide range of settings. We complement these qualitative results with quantitative bounds on the resulting range of base fees. We conclude our analysis with a thorough experimental case study that corroborates our theoretical findings. △ Less","5 June, 2021",https://arxiv.org/pdf/2102.10567
Unavailable Transit Feed Specification: Making it Available with Recurrent Neural Networks,Ludovico Iovino;Phuong T. Nguyen;Amleto Di Salle;Francesco Gallo;Michele Flammini,"Studies on public transportation in Europe suggest that European inhabitants use buses in ca. 56% of all public transport travels. One of the critical factors affecting such a percentage and more, in general, the demand for public transport services, with an increasing reluctance to use them, is their quality. End-users can perceive quality from various perspectives, including the availability of information, i.e., the access to details about the transit and the provided services. The approach proposed in this paper, using innovative methodologies resorting on data mining and machine learning techniques, aims to make available the unavailable data about public transport. In particular, by mining GPS traces, we manage to reconstruct the complete transit graph of public transport. The approach has been successfully validated on a real dataset collected from the local bus system of the city of L'Aquila (Italy). The experimental results demonstrate that the proposed approach and implemented framework are both effective and efficient, thus being ready for deployment. △ Less","20 February, 2021",https://arxiv.org/pdf/2102.10323
ReSonAte: A Runtime Risk Assessment Framework for Autonomous Systems,Charles Hartsell;Shreyas Ramakrishna;Abhishek Dubey;Daniel Stojcsics;Nagabhushan Mahadevan;Gabor Karsai,"Autonomous CPSs are often required to handle uncertainties and self-manage the system operation in response to problems and increasing risk in the operating paradigm. This risk may arise due to distribution shifts, environmental context, or failure of software or hardware components. Traditional techniques for risk assessment focus on design-time techniques such as hazard analysis, risk reduction, and assurance cases among others. However, these static, design-time techniques do not consider the dynamic contexts and failures the systems face at runtime. We hypothesize that this requires a dynamic assurance approach that computes the likelihood of unsafe conditions or system failures considering the safety requirements, assumptions made at design time, past failures in a given operating context, and the likelihood of system component failures. We introduce the ReSonAte dynamic risk estimation framework for autonomous systems. ReSonAte reasons over Bow-Tie Diagrams (BTDs) which capture information about hazard propagation paths and control strategies. Our innovation is the extension of the BTD formalism with attributes for modeling the conditional relationships with the state of the system and environment. We also describe a technique for estimating these conditional relationships and equations for estimating risk based on the state of the system and environment. To help with this process, we provide a scenario modeling procedure that can use the prior distributions of the scenes and threat conditions to generate the data required for estimating the conditional relationships. To improve scalability and reduce the amount of data required, this process considers each control strategy in isolation and composes several single-variate distributions into one complete multi-variate distribution for the control strategy in question. △ Less","24 March, 2021",https://arxiv.org/pdf/2102.09419
"SonicChain: A Wait-free, Pseudo-Static Approach Toward Concurrency in Blockchains",Kian Paimani,"Blockchains have a two-sided reputation: they are praised for disrupting some of our institutions through innovative technology for good, yet notorious for being slow and expensive to use. In this work, we tackle this issue with concurrency, yet we aim to take a radically different approach by valuing simplicity. We embrace the simplicity through two steps: first, we formulate a simple runtime mechanism to deal with conflicts called concurrency delegation. This method is much simpler and has less overhead, particularly in scenarios where conflicting transactions are relatively rare. Moreover, to further reduce the number of conflicting transactions, we propose using static annotations attached to each transaction, provided by the programmer. These annotations are pseudo-static: they are static with respect to the lifetime of the transaction, and therefore are free to use information such as the origin and parameters of the transaction. We propose a distributor component that can use the output of this pseudo-static annotations and use them to effectively distribute transactions between threads in the least-conflicting way. We name the outcome of a system combining concurrency delegation and pseudo-static annotations as SonicChain. We evaluate SonicChain for both validation and authoring tasks against a common workload in blockchains, namely, balance transfers, and observe that it performs expectedly well while introducing very little overhead and additional complexity to the system. △ Less","6 February, 2021",https://arxiv.org/pdf/2102.09073
DICODerma: A practical approach for metadata management of images in dermatology,Bell Raj Eapen;Feroze Kaliyadan;Ashique Karalikkattil T,"Clinical images are vital for diagnosing and monitoring skin diseases, and their importance has increased with the growing popularity of machine learning. Lack of standards has stifled innovation in dermatological imaging, unlike other image-intensive specialties such as radiology. We investigate the meta-requirements for utilizing the popular DICOM standard for metadata management of images in dermatology. We propose practical design solutions and provide open-source tools to integrate dermatologists' workflow with enterprise imaging systems. Using the tool, dermatologists can tag, search, organize and convert clinical images to the DICOM format. We believe that our less disruptive approach will improve the adoption of standards in the specialty. △ Less","17 February, 2021",https://arxiv.org/pdf/2102.08673
Darknet Traffic Big-Data Analysis and Network Management to Real-Time Automating the Malicious Intent Detection Process by a Weight Agnostic Neural Networks Framework,Konstantinos Demertzis;Konstantinos Tsiknas;Dimitrios Takezis;Charalabos Skianis;Lazaros Iliadis,"Attackers are perpetually modifying their tactics to avoid detection and frequently leverage legitimate credentials with trusted tools already deployed in a network environment, making it difficult for organizations to proactively identify critical security risks. Network traffic analysis products have emerged in response to attackers relentless innovation, offering organizations a realistic path forward for combatting creative attackers. Additionally, thanks to the widespread adoption of cloud computing, Device Operators processes, and the Internet of Things, maintaining effective network visibility has become a highly complex and overwhelming process. What makes network traffic analysis technology particularly meaningful is its ability to combine its core capabilities to deliver malicious intent detection. In this paper, we propose a novel darknet traffic analysis and network management framework to real-time automating the malicious intent detection process, using a weight agnostic neural networks architecture. It is an effective and accurate computational intelligent forensics tool for network traffic analysis, the demystification of malware traffic, and encrypted traffic identification in real-time. Based on Weight Agnostic Neural Networks methodology, we propose an automated searching neural net architectures strategy that can perform various tasks such as identify zero-day attacks. By automating the malicious intent detection process from the darknet, the advanced proposed solution is reducing the skills and effort barrier that prevents many organizations from effectively protecting their most critical assets. △ Less","16 February, 2021",https://arxiv.org/pdf/2102.08411
GradInit: Learning to Initialize Neural Networks for Stable and Efficient Training,Chen Zhu;Renkun Ni;Zheng Xu;Kezhi Kong;W. Ronny Huang;Tom Goldstein,"Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit. △ Less","24 November, 2021",https://arxiv.org/pdf/2102.08098
An Overview of Agent-based Traffic Simulators,Johannes Nguyen;Simon T. Powers;Neil Urquhart;Thomas Farrenkopf;Michael Guckert,"Individual traffic significantly contributes to climate change and environmental degradation. Therefore, innovation in sustainable mobility is gaining importance as it helps to reduce environmental pollution. However, effects of new ideas in mobility are difficult to estimate in advance and strongly depend on the individual traffic participants. The application of agent technology is particularly promising as it focuses on modelling heterogeneous individual preferences and behaviours. In this paper, we show how agent-based models are particularly suitable to address three pressing research topics in mobility: 1. Social dilemmas in resource utilisation; 2. Digital connectivity; and 3. New forms of mobility. We then explain how the features of several agent-based simulators are suitable for addressing these topics. We assess the capability of simulators to model individual travel behaviour, discussing implemented features and identifying gaps in functionality that we consider important. △ Less","13 November, 2021",https://arxiv.org/pdf/2102.07505
A Further Note on an Innovations Approach to Viterbi Decoding of Convolutional Codes,Masato Tajima,"In this paper, we show that the soft-decision input to the main decoder in an SST Viterbi decoder is regarded as the innovation as well from the viewpoint of mutual information and mean-square error. It is assumed that a code sequence is transmitted symbol by symbol over an AWGN channel using BPSK modulation. Then we can consider the signal model, where the signal is composed of the signal-to-noise ratio (SNR) and the equiprobable binary input. By assuming that the soft-decision input to the main decoder is the innovation, we show that the minimum mean-square error (MMSE) in estimating the binary input is expressed in terms of the distribution of the encoded block for the main decoder. It is shown that the obtained MMSE satisfies indirectly the known relation between the mutual information and the MMSE in Gaussian channels. Thus the derived MMSE is justified, which in turn implies that the soft-decision input to the main decoder can be regarded as the innovation. Moreover, we see that the input-output mutual information is connected with the distribution of the encoded block for the main decoder. △ Less","14 February, 2021",https://arxiv.org/pdf/2102.07315
"New methods for metastimuli: architecture, embeddings, and neural network optimization",Rico A. R. Picone;Dane Webb;Finbarr Obierefu;Jotham Lentz,"Six significant new methodological developments of the previously-presented ""metastimuli architecture"" for human learning through machine learning of spatially correlated structural position within a user's personal information management system (PIMS), providing the basis for haptic metastimuli, are presented. These include architectural innovation, recurrent (RNN) artificial neural network (ANN) application, a variety of atom embedding techniques (including a novel technique we call ""nabla"" embedding inspired by linguistics), ANN hyper-parameter (one that affects the network but is not trained, e.g. the learning rate) optimization, and meta-parameter (one that determines the system performance but is not trained and not a hyper-parameter, e.g. the atom embedding technique) optimization for exploring the large design space. A technique for using the system for automatic atom categorization in a user's PIMS is outlined. ANN training and hyper- and meta-parameter optimization results are presented and discussed in service of methodological recommendations. △ Less","14 February, 2021",https://arxiv.org/pdf/2102.07090
Blind stain separation using model-aware generative learning and its applications on fluorescence microscopy images,Xingyu Li,"Multiple stains are usually used to highlight biological substances in biomedical image analysis. To decompose multiple stains for co-localization quantification, blind source separation is usually performed. Prior model-based stain separation methods usually rely on stains' spatial distributions over an image and may fail to solve the co-localization problem. With the advantage of machine learning, deep generative models are used for this purpose. Since prior knowledge of imaging models is ignored in purely data-driven solutions, these methods may be sub-optimal. In this study, a novel learning-based blind source separation framework is proposed, where the physical model of biomedical imaging is incorporated to regularize the learning process. The introduced model-relevant adversarial loss couples all generators in the framework and limits the capacities of the generative models. Further more, a training algorithm is innovated for the proposed framework to avoid inter-generator confusion during learning. This paper particularly takes fluorescence unmixing in fluorescence microscopy images as an application example of the proposed framework. Qualitative and quantitative experimentation on a public fluorescence microscopy image set demonstrates the superiority of the proposed method over both prior model-based approaches and learning-based methods. △ Less","12 February, 2021",https://arxiv.org/pdf/2102.06802
VSync: Push-Button Verification and Optimization for Synchronization Primitives on Weak Memory Models (Technical Report),Jonas Oberhauser;Rafael Lourenco de Lima Chehab;Diogo Behrens;Ming Fu;Antonio Paolillo;Lilith Oberhauser;Koustubha Bhat;Yuzhong Wen;Haibo Chen;Jaeho Kim;Viktor Vafeiadis,"This technical report contains material accompanying our work with same title published at ASPLOS'21. We start in Sec. 1 with a detailed presentation of the core innovation of this work, Await Model Checking (AMC). The correctness proofs of AMC can be found in Sec. 2. Next, we discuss three study cases in Sec. 3, presenting bugs found and challenges encountered when applying VSync to existing code bases. Finally, in Sec. 4 we describe the setup details of our evaluation and report further experimental results. △ Less","12 February, 2021",https://arxiv.org/pdf/2102.06590
Editorial: Introduction to the Issue on Deep Learning for Image/Video Restoration and Compression,A. Murat Tekalp;Michele Covell;Radu Timofte;Chao Dong,"Recent works have shown that learned models can achieve significant performance gains, especially in terms of perceptual quality measures, over traditional methods. Hence, the state of the art in image restoration and compression is getting redefined. This special issue covers the state of the art in learned image/video restoration and compression to promote further progress in innovative architectures and training methods for effective and efficient networks for image/video restoration and compression. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.06531
Transparent FPGA Acceleration with TensorFlow,Simon Pfenning;Philipp Holzinger;Marc Reichenbach,"Today, artificial neural networks are one of the major innovators pushing the progress of machine learning. This has particularly affected the development of neural network accelerating hardware. However, since most of these architectures require specialized toolchains, there is a certain amount of additional effort for developers each time they want to make use of a new deep learning accelerator. Furthermore the flexibility of the device is bound to the architecture itself, as well as to the functionality of the runtime environment. In this paper we propose a toolflow using TensorFlow as frontend, thus offering developers the opportunity of using a familiar environment. On the backend we use an FPGA, which is addressable via an HSA runtime environment. In this way we are able to hide the complexity of controlling new hardware from the user, while at the same time maintaining a high amount of flexibility. This can be achieved by our HSA toolflow, since the hardware is not statically configured with the structure of the network. Instead, it can be dynamically reconfigured during runtime with the respective kernels executed by the network and simultaneously from other sources e.g. OpenCL/OpenMP. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.06018
SteaLTE: Private 5G Cellular Connectivity as a Service with Full-stack Wireless Steganography,Leonardo Bonati;Salvatore D'Oro;Francesco Restuccia;Stefano Basagni;Tommaso Melodia,"Fifth-generation (5G) systems will extensively employ radio access network (RAN) softwarization. This key innovation enables the instantiation of ""virtual cellular networks"" running on different slices of the shared physical infrastructure. In this paper, we propose the concept of Private Cellular Connectivity as a Service (PCCaaS), where infrastructure providers deploy covert network slices known only to a subset of users. We then present SteaLTE as the first realization of a PCCaaS-enabling system for cellular networks. At its core, SteaLTE utilizes wireless steganography to disguise data as noise to adversarial receivers. Differently from previous work, however, it takes a full-stack approach to steganography, contributing an LTE-compliant steganographic protocol stack for PCCaaS-based communications, and packet schedulers and operations to embed covert data streams on top of traditional cellular traffic (primary traffic). SteaLTE balances undetectability and performance by mimicking channel impairments so that covert data waveforms are almost indistinguishable from noise. We evaluate the performance of SteaLTE on an indoor LTE-compliant testbed under different traffic profiles, distance and mobility patterns. We further test it on the outdoor PAWR POWDER platform over long-range cellular links. Results show that in most experiments SteaLTE imposes little loss of primary traffic throughput in presence of covert data transmissions (< 6%), making it suitable for undetectable PCCaaS networking. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.05606
Biomedical Question Answering: A Survey of Approaches and Challenges,Qiao Jin;Zheng Yuan;Guangzhi Xiong;Qianlan Yu;Huaiyuan Ying;Chuanqi Tan;Mosha Chen;Songfang Huang;Xiaozhong Liu;Sheng Yu,"Automatic Question Answering (QA) has been successfully applied in various domains such as search engines and chatbots. Biomedical QA (BQA), as an emerging QA task, enables innovative applications to effectively perceive, access and understand complex biomedical knowledge. There have been tremendous developments of BQA in the past two decades, which we classify into 5 distinctive approaches: classic, information retrieval, machine reading comprehension, knowledge base and question entailment approaches. In this survey, we introduce available datasets and representative methods of each BQA approach in detail. Despite the developments, BQA systems are still immature and rarely used in real-life settings. We identify and characterize several key challenges in BQA that might lead to this issue, and discuss some potential future directions to explore. △ Less","8 September, 2021",https://arxiv.org/pdf/2102.05281
Characterizing the Online Learning Landscape: What and How People Learn Online,Sean Kross;Eszter Hargittai;Elissa M. Redmiles,"Hundreds of millions of people learn something new online every day. Simultaneously, the study of online education has blossomed within the human computer interaction community, with new systems, experiments, and observations creating and exploring previously undiscovered online learning environments. In this study we endeavor to characterize this entire landscape of online learning experiences using a national survey of 2260 US adults who are balanced to match the demographics of the U.S. We examine the online learning resources that they consult, and we analyze the subjects that they pursue using those resources. Furthermore, we compare both formal and informal online learning experiences on a larger scale than has ever been done before, to our knowledge, to better understand which subjects people are seeking for intensive study. We find that there is a core set of online learning experiences that are central to other experiences and these are shared among the majority of people who learn online. We conclude by showing how looking outside of these core online learning experiences can reveal opportunities for innovation in online education. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.05268
"Co-evolution of platform architecture, platform services, and platform governance: Expanding the platform value of industrial digital platforms",Marin Jovanovic;David Sjodin;Vinit Parida,"Industrial manufacturers increasingly develop digital platforms in the business-to-business (B2B) context. This emergent form of digital platforms requires a profound yet little understood holistic perspective that encompasses the co-evolution of platform architecture, platform services, and platform governance. To address this research gap, our study examines multiple platform sponsors from an industrial manufacturing context. The study demarcates three platform archetypes: product platform, supply chain platform, and platform ecosystem. We argue that each platform archetype involves a gradual development of platform architecture, platform services, and platform governance, which mirror each other. We also find that each platform archetype is characterized by a specific innovation mechanism that contributes to the platform service discovery and expands the platform value. Our study extends the co-evolution perspective of platform ecosystem literature and digital servitization literature. △ Less","24 January, 2021",https://arxiv.org/pdf/2102.04862
Fashion Focus: Multi-modal Retrieval System for Video Commodity Localization in E-commerce,Yanhao Zhang;Qiang Wang;Pan Pan;Yun Zheng;Cheng Da;Siyang Sun;Yinghui Xu,"Nowadays, live-stream and short video shopping in E-commerce have grown exponentially. However, the sellers are required to manually match images of the selling products to the timestamp of exhibition in the untrimmed video, resulting in a complicated process. To solve the problem, we present an innovative demonstration of multi-modal retrieval system called ""Fashion Focus"", which enables to exactly localize the product images in the online video as the focuses. Different modality contributes to the community localization, including visual content, linguistic features and interaction context are jointly investigated via presented multi-modal learning. Our system employs two procedures for analysis, including video content structuring and multi-modal retrieval, to automatically achieve accurate video-to-shop matching. Fashion Focus presents a unified framework that can orientate the consumers towards relevant product exhibitions during watching videos and help the sellers to effectively deliver the products over search and recommendation. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.04727
Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap,Haike Xu;Tengyu Ma;Simon S. Du,"This paper presents a new model-free algorithm for episodic finite-horizon Markov Decision Processes (MDP), Adaptive Multi-step Bootstrap (AMB), which enjoys a stronger gap-dependent regret bound. The first innovation is to estimate the optimal Q-function by combining an optimistic bootstrap with an adaptive multi-step Monte Carlo rollout. The second innovation is to select the action with the largest confidence interval length among admissible actions that are not dominated by any other actions. We show when each state has a unique optimal action, AMB achieves a gap-dependent regret bound that only scales with the sum of the inverse of the sub-optimality gaps. In contrast, Simchowitz and Jamieson (2019) showed all upper-confidence-bound (UCB) algorithms suffer an additional Ω\left(\frac{S}{Δ_{min}}\right) regret due to over-exploration where Δ_{min} is the minimum sub-optimality gap and S is the number of states. We further show that for general MDPs, AMB suffers an additional \frac{|Z_{mul}|}{Δ_{min}} regret, where Z_{mul} is the set of state-action pairs (s,a)'s satisfying a is a non-unique optimal action for s. We complement our upper bound with a lower bound showing the dependency on \frac{|Z_{mul}|}{Δ_{min}} is unavoidable for any consistent algorithm. This lower bound also implies a separation between reinforcement learning and contextual bandits. △ Less","2 July, 2021",https://arxiv.org/pdf/2102.04692
Adversarially Guided Actor-Critic,Yannis Flet-Berliac;Johan Ferret;Olivier Pietquin;Philippe Preux;Matthieu Geist,"Despite definite success in deep reinforcement learning problems, actor-critic algorithms are still confronted with sample inefficiency in complex environments, particularly in tasks where efficient exploration is a bottleneck. These methods consider a policy (the actor) and a value function (the critic) whose respective losses are built using different motivations and approaches. This paper introduces a third protagonist: the adversary. While the adversary mimics the actor by minimizing the KL-divergence between their respective action distributions, the actor, in addition to learning to solve the task, tries to differentiate itself from the adversary predictions. This novel objective stimulates the actor to follow strategies that could not have been correctly predicted from previous trajectories, making its behavior innovative in tasks where the reward is extremely rare. Our experimental analysis shows that the resulting Adversarially Guided Actor-Critic (AGAC) algorithm leads to more exhaustive exploration. Notably, AGAC outperforms current state-of-the-art methods on a set of various hard-exploration and procedurally-generated tasks. △ Less","8 February, 2021",https://arxiv.org/pdf/2102.04376
"The Rise of Technology in Crime Prevention: Opportunities, Challenges and Practitioners Perspectives",Dario Ortega Anderez;Eiman Kanjo;Amna Amnwar;Shane Johnson;David Lucy,"Criminal activity is a prevalent issue in contemporary culture and society, with most nations facing unacceptable levels of crime. Technological innovation has been one of the main driving forces leading to the continuous improvement of crime control and crime prevention strategies (e.g. GPS tracking and tagging, video surveillance, etc.). Given this, it is a moral obligation for the research community to consider how the contemporary technological developments (i.e. Internet of Things (IoT), Machine Learning, Edge Computing)might help reduce crime worldwide. In line with this, this paper provides a discussion of how a sample of contemporary hardware and software-based technologies might help further reduce criminal actions. After a thorough analysis of a wide array of technologies and a number of workshops with organisations of interest, we believe that the adoption of novel technologies by vulnerable individuals, victim support organisations and law enforcement can help reduce the occurrence of criminal activity. △ Less","26 January, 2021",https://arxiv.org/pdf/2102.04204
Dynamic Performance Management: An Approach for Managing the Common Goods,A. Sardi;E. Sorano,"Public organizations need innovative approaches for managing common goods and to explain the dynamics linking the (re)generation of common goods and organizational performance. Although system dynamics is recognised as a useful approach for managing common goods, public organizations rarely adopt the system dynamics for this goal. The paper aims to review the literature on the system dynamics and its recent application, known as dynamic performance management, to highlight the state of the art and future opportunities on the management of common goods. The authors analyzed 144 documents using a systematic literature review. The results obtained outline a fair number of documents, countries and journals involving the study of system dynamics, but do not cover sufficient research on the linking between the (re)generation of common goods and organizational performance. This paper outlines academic and practical contributions. Firstly, it contributes to the theory of common goods. It provides insight for linking the management of common goods and organizational performance through the use of dynamic performance management approach. Furthermore, it shows scholars the main research opportunities. Secondly, it indicates to practitioners the documents providing useful ideas on the adoption of system dynamics for managing common goods. △ Less","8 February, 2021",https://arxiv.org/pdf/2102.04090
Improving Accuracy and Diversity in Matching of Recommendation with Diversified Preference Network,Ruobing Xie;Qi Liu;Shukai Liu;Ziwei Zhang;Peng Cui;Bo Zhang;Leyu Lin,"Recently, real-world recommendation systems need to deal with millions of candidates. It is extremely challenging to conduct sophisticated end-to-end algorithms on the entire corpus due to the tremendous computation costs. Therefore, conventional recommendation systems usually contain two modules. The matching module focuses on the coverage, which aims to efficiently retrieve hundreds of items from large corpora, while the ranking module generates specific ranks for these items. Recommendation diversity is an essential factor that impacts user experience. Most efforts have explored recommendation diversity in ranking, while the matching module should take more responsibility for diversity. In this paper, we propose a novel Heterogeneous graph neural network framework for diversified recommendation (GraphDR) in matching to improve both recommendation accuracy and diversity. Specifically, GraphDR builds a huge heterogeneous preference network to record different types of user preferences, and conduct a field-level heterogeneous graph attention network for node aggregation. We also innovatively conduct a neighbor-similarity based loss to balance both recommendation accuracy and diversity for the diversified matching task. In experiments, we conduct extensive online and offline evaluations on a real-world recommendation system with various accuracy and diversity metrics and achieve significant improvements. We also conduct model analyses and case study for a better understanding of our model. Moreover, GraphDR has been deployed on a well-known recommendation system, which affects millions of users. The source code will be released. △ Less","7 February, 2021",https://arxiv.org/pdf/2102.03787
Convolutional Neural Network-based Intrusion Detection System for AVTP Streams in Automotive Ethernet-based Networks,Seonghoon Jeong;Boosun Jeon;Boheung Chung;Huy Kang Kim,"Connected and autonomous vehicles (CAVs) are an innovative form of traditional vehicles. Automotive Ethernet replaces the controller area network and FlexRay to support the large throughput required by high-definition applications. As CAVs have numerous functions, they exhibit a large attack surface and an increased vulnerability to attacks. However, no previous studies have focused on intrusion detection in automotive Ethernet-based networks. In this paper, we present an intrusion detection method for detecting audio-video transport protocol (AVTP) stream injection attacks in automotive Ethernet-based networks. To the best of our knowledge, this is the first such method developed for automotive Ethernet. The proposed intrusion detection model is based on feature generation and a convolutional neural network (CNN). To evaluate our intrusion detection system, we built a physical BroadR-Reach-based testbed and captured real AVTP packets. The experimental results show that the model exhibits outstanding performance: the F1-score and recall are greater than 0.9704 and 0.9949, respectively. In terms of the inference time per input and the generation intervals of AVTP traffic, our CNN model can readily be employed for real-time detection. △ Less","6 February, 2021",https://arxiv.org/pdf/2102.03546
Template-Free Try-on Image Synthesis via Semantic-guided Optimization,Chien-Lung Chou;Chieh-Yun Chen;Chia-Wei Hsieh;Hong-Han Shuai;Jiaying Liu;Wen-Huang Cheng,"The virtual try-on task is so attractive that it has drawn considerable attention in the field of computer vision. However, presenting the three-dimensional (3D) physical characteristic (e.g., pleat and shadow) based on a 2D image is very challenging. Although there have been several previous studies on 2D-based virtual try-on work, most 1) required user-specified target poses that are not user-friendly and may not be the best for the target clothing, and 2) failed to address some problematic cases, including facial details, clothing wrinkles and body occlusions. To address these two challenges, in this paper, we propose an innovative template-free try-on image synthesis (TF-TIS) network. The TF-TIS first synthesizes the target pose according to the user-specified in-shop clothing. Afterward, given an in-shop clothing image, a user image, and a synthesized pose, we propose a novel model for synthesizing a human try-on image with the target clothing in the best fitting pose. The qualitative and quantitative experiments both indicate that the proposed TF-TIS outperforms the state-of-the-art methods, especially for difficult cases. △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03503
Throughput Maximization of Network-Coded and Multi-Level Cache-Enabled Heterogeneous Network,Mohammed S. Al-Abiad;Md. Zoheb Hassan;Md. Jahangir Hossain,"One of the paramount advantages of multi-level cache-enabled (MLCE) networks is pushing contents proximity to the network edge and proactively caching them at multiple transmitters (i.e., small base-stations (SBSs), unmanned aerial vehicles (UAVs), and cache-enabled device-to-device (CE-D2D) users). As such, the fronthaul congestion between a core network and a large number of transmitters is alleviated. For this objective, we exploit network coding (NC) to schedule a set of users to the same transmitter. Focusing on this, we consider the throughput maximization problem that optimizes jointly the network-coded user scheduling and power allocation, subject to fronthaul capacity, transmit power, and NC constraints. Given the intractability of the problem, we decouple it into two separate subproblems. In the first subproblem, we consider the network-coded user scheduling problem for the given power allocation, while in the second subproblem, we use the NC resulting user schedule to optimize the power levels. We design an innovative \textit{two-layered rate-aware NC (RA-IDNC)} graph to solve the first subproblem and evaluate the second subproblem using an iterative function evaluation (IFE) approach. Simulation results are presented to depict the throughput gain of the proposed approach over the existing solutions. △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03405
Extending System Performance Past the Boundaries of Technical Maturity: Human-Agent Teamwork Perspective for Industrial Inspection,Garrick Cabour;Élise Ledoux;Samuel Bassetto,"Cyber-Physical-Social Systems (CPSS) performance for industry 4.0 is highly context-dependent, where three design areas arise: the artifact itself, the human-agent collaboration, and the organizational settings. Current HF&E tools are limited to conceptualize and anticipate future human-agent work situations with a fine-grained perspective. This paper explores how rich in-sights from work analysis can be translated into formative design patterns that provide finer guidance in conceptualizing the human-agent collaboration and the organizational settings. The current manual work content elicited is disaggregated into functional requirements. Each function is then scrutinized by a multidisciplinary design team that decides its feasibility and nature (autonomy function, human function, or hybrid function). By doing so, we uncover the technical capabilities of the CPSS in comparison with subject-matter experts` work activity. We called this concept technological coverage. The framework thereof allows close collaboration with design stakeholders to define detailed HAT configurations. We then imagined joint activity scenarios based on end-users work activity, the technological capabilities, and the interaction requirements to perform the work. We use a study on technological innovation in the aircraft maintenance domain to illustrate the framework`s early phases △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03241
BlockNet Report: Curriculum Guidance Document,Boris Düdder;Haiqin Wu;Michael Henke;Natalia Straub;Tan Gürpinar;Philipp Asterios Ioannidis;Vladislav Fomin;Raimundas Matulevičius;Mubashar Iqbal,"Blockchain is a challenging topic since it is novel and fosters potential innovation. The blockchain is attractive for various disciplines, and, because of its cross-cutting nature, needs knowledge stemming from various disciplines. The devised curriculum can be instantiated specifically to meet the needs of students' groups from various disciplines. The pedagogical innovation of the project is the inclusion of interdisciplinary project groups with participant's interaction via online platforms for project-based learning activities. MOOCs and SNOCs allow blended-learning for interdisciplinary and geographically distributed student groups. △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03226
Connecting flying backhauls of UAVs to enhance vehicular networks with fixed 5G NR infrastructure,Dalia Popescu;Philippe Jacquet;Bernard Mans,"This paper investigates moving networks of Unmanned Aerial Vehicles (UAVs), such as drones, as one of the innovative opportunities brought by the 5G. With a main purpose to extend connectivity and guarantee data rates, the drones require hovering locations due to limitations such as flight time and coverage surface. We provide analytic bounds on the requirements in terms of connectivity extension for vehicular networks served by fixed Enhanced Mobile BroadBand (eMBB) infrastructure, where both vehicular networks and infrastructures are modeled using stochastic and fractal geometry as a model for urban environment. We prove that assuming n mobile nodes (distributed according to a hyperfractal distribution of dimension d_F) and an average of ρ Next Generation NodeB (gNBs), distributed like an hyperfractal of dimension d_r if ρ=n^θ with θ>d_r/4 and letting n tending to infinity (to reflect megalopolis cities), then the average fraction of mobile nodes not covered by a gNB tends to zero like O\left(n^{-\frac{(d_F-2)}{d_r}(2θ-\frac{d_r}{2})}\right). Interestingly, we then prove that the average number of drones, needed to connect each mobile node not covered by gNBs is comparable to the number of isolated mobile nodes. We complete the characterisation by proving that when θ<d_r/4 the proportion of covered mobile nodes tends to zero. We provide insights on the intelligent placement of the ""garage of drones"", the home location of these nomadic infrastructure nodes, such as to minimize what we call the ""flight-to-coverage time"". We provide a fast procedure to select the relays that will be garages (and store drones) in order to minimize the number of garages and minimize the delay. Finally we confirm our analytical results using simulations carried out in Matlab. △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03040
An Empirical Analysis of Implementing Enterprise Blockchain Protocols in Supply Chain Anti-Counterfeiting and Traceability,Neo C. K. Yiu,"A variety of innovative software solutions, addressing product anti-counterfeiting and record provenance of the wider supply chain industry, have been implemented. However, these solutions have been developed with centralized system architecture which could be susceptible to malicious modifications on states of product records and various potential security attacks leading to system failure and downtime. Blockchain technology has been enabling decentralized trust with a network of distributed peer nodes to maintain consistent shared states via a decentralized consensus reached, with which an idea of developing decentralized and reliable solutions has been basing on. A Decentralized NFC-Enabled Anti-Counterfeiting System (dNAS) was therefore proposed and developed, decentralizing a legacy anti-counterfeiting system of supply chain industry utilizing enterprise blockchain protocols and enterprise consortium, to facilitate trustworthy data provenance retrieval, verification and management, as well as strengthening capability of product anti-counterfeiting and traceability in supply chain industry. The adoption of enterprise blockchain protocols and implementations has been surging in supply chain industry given its advantages in scalability, governance and compatibility with existing supply chain systems and networks, but development and adoption of decentralized solutions could also impose additional implications to supply chain integrity, in terms of security, privacy and confidentiality. In this research, an empirical analysis performed against decentralized solutions, including dNAS, summarizes the effectiveness, limitations and future opportunities of developing decentralized solutions built around existing enterprise blockchain protocols and implementations for supply chain anti-counterfeiting and traceability. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.02601
An Analysis of International Use of Robots for COVID-19,Robin R. Murphy;Vignesh B. M. Gandudi;Trisha Amin;Angela Clendenin;Jason Moats,"This article analyses data collected on 338 instances of robots used explicitly in response to COVID-19 from 24 Jan, 2020, to 23 Jan, 2021, in 48 countries. The analysis was guided by four overarching questions: 1) What were robots used for in the COVID-19 response? 2) When were they used? 3) How did different countries innovate? and 4) Did having a national policy on robotics influence a country's innovation and insertion of robotics for COVID-19? The findings indicate that robots were used for six different sociotechnical work domains and 29 discrete use cases. When robots were used varied greatly on the country; although many countries did report an increase at the beginning of their first surge. To understand the findings of how innovation occurred, the data was examined through the lens of the technology's maturity according to NASA's Technical Readiness Assessment metrics. Through this lens, findings note that existing robots were used for more than 78% of the instances; slightly modified robots made up 10%; and truly novel robots or novel use cases constituted 12% of the instances. The findings clearly indicate that countries with a national robotics initiative were more likely to use robotics more often and for broader purposes. Finally, the dataset and analysis produces a broad set of implications that warrant further study and investigation. The results from this analysis are expected to be of value to the robotics and robotics policy community in preparing robots for rapid insertion into future disasters. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.02509
BeFair: Addressing Fairness in the Banking Sector,Alessandro Castelnovo;Riccardo Crupi;Giulia Del Gamba;Greta Greco;Aisha Naseer;Daniele Regoli;Beatriz San Miguel Gonzalez,"Algorithmic bias mitigation has been one of the most difficult conundrums for the data science community and Machine Learning (ML) experts. Over several years, there have appeared enormous efforts in the field of fairness in ML. Despite the progress toward identifying biases and designing fair algorithms, translating them into the industry remains a major challenge. In this paper, we present the initial results of an industrial open innovation project in the banking sector: we propose a general roadmap for fairness in ML and the implementation of a toolkit called BeFair that helps to identify and mitigate bias. Results show that training a model without explicit constraints may lead to bias exacerbation in the predictions. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.02137
Decentralizing Supply Chain Anti-Counterfeiting Systems Using Blockchain Technology,Neo C. K. Yiu,"An interesting research problem in supply chain industry is evaluating and determining provenance of physical goods - demonstrating authenticity of luxury goods. Yet, there have been a few innovative software solutions addressing product anti-counterfeiting and record provenance of today's goods that are produced and transported in complex and internationally-spanning supply chain networks. However, these supply chain systems have been implemented with centralized system architecture, relying on centralized authorities or any form of intermediaries, and leading to issues such as single-point processing, storage and failure, which could be susceptible to malicious modifications of product records or various potential attacks to system components by dishonest participant nodes traversing along the supply chain. Blockchain technology has evolved from being merely a decentralized, distributed and immutable ledger of cryptocurrency transactions to a programmable interactive environment for building decentralized and reliable applications addressing different use cases and existing problems in the world. In this research, the Decentralized NFC-Enabled Anti-Counterfeiting System (dNAS) is proposed and developed, decentralizing a legacy anti-counterfeiting system of supply chain industry using Blockchain technology, to facilitate trustworthy data provenance retrieval, verification and management, as well as strengthening capability of product anti-counterfeiting in supply chain industry. The proposed dNAS utilizes decentralized blockchain network on a consensus protocol compatible with the concept of enterprise consortium, programmable smart contracts and a distributed file storage system to develop a secure and immutable scientific data provenance tracking and management platform on which provenance records, providing compelling properties on data integrity, are validated automatically. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.01456
The impact of external innovation on new drug approvals: A retrospective analysis,Xiong Liu;Craig E. Thomas;Christian C. Felder,"Pharmaceutical companies are relying more often on external sources of innovation to boost their discovery research productivity. However, more in-depth knowledge about how external innovation may translate to successful product launches is still required in order to better understand how to best leverage the innovation ecosystem. We analyzed the pre-approval publication histories for FDA-approved new molecular entities (NMEs) and new biologic entities (NBEs) launched by 13 top research pharma companies during the last decade (2006-2016). We found that academic institutions contributed the majority of pre-approval publications and that publication subject matter is closely aligned with the strengths of the respective innovator. We found this to also be true for candidate drugs terminated in Phase 3, but the volume of literature on these molecules is substantially less than for approved drugs. This may suggest that approved drugs are often associated with a more robust dataset provided by a large number of institutes. Collectively, the results of our analysis support the hypothesis that a collaborative research innovation environment spanning across academia, industry and government is highly conducive to successful drug approvals. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.01260
Characterizing the Energy Trade-Offs of End-to-End Vehicular Communications using an Hyperfractal Urban Modelling,Dalia Popescu;Philippe Jacquet;Bernard Mans;Bartomiej Blaszczyszyn,"We characterize trade-offs between the end-to-end communication delay and the energy in urban vehicular communications with infrastructure assistance. Our study exploits the self-similarity of the location of communication entities in cities by modeling them with an innovative model called ""hyperfractal"". We show that the hyperfractal model can be extended to incorporate road-side infrastructure and provide stochastic geometry tools to allow a rigorous analysis. We compute theoretical bounds for the end-to-end communication hop count considering two different energy-minimizing goals: either total accumulated energy or maximum energy per node. We prove that the hop count for an end-to-end transmission is bounded by O(n^{1-α/(d_F-1)}) where α<1 and d_F>2 is the fractal dimension of the mobile nodes process. This proves that for both constraints the energy decreases as we allow choosing routing paths of higher length. The asymptotic limit of the energy becomes significantly small when the number of nodes becomes asymptotically large. A lower bound on the network throughput capacity with constraints on path energy is also given. We show that our model fits real deployments where open data sets are available. The results are confirmed through simulations using different fractal dimensions in a Matlab simulator. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.01241
QoS-aware Link Scheduling Strategy for Data Transmission in SDVN,Yong Zhang;Mao Ye;Lin Guan,"The vehicular ad-hoc network (VANET) based on dedicated short-range communication (DSRC) is a distributed communication system, in which all the nodes share the wireless channel with carrier sense multiple access/collision avoid (CSMA/CA) protocol. However, the backoff mechanism of CSMA/CA in the channel contention might cause uncertain transmission delay and impede a certain quality of service (QoS) of applications. Moreover, there still exists a possibility of parlous data-packets collision, especially for broadcast or non-acknowledgement (NACK) transmissions. The original contributions of this paper are summarized as follows: (1) Model the packets collision probability of broadcast or NACK transmission in VANET with the combination theory and investigate the potential influence of miss my packets (MMP) problem. (2) Based on the software define vehicular network (SDVN) framework and QoS requirement, a novel link-level scheduling strategy, which determines the start-sending time for each connection, is proposed to maximize packets delivery ratio (PDR). Alternatively, maximizing PDR has been converted to the overlap minimization among transmission durations. (3) Meanwhile, an innovative transmission scheduling greedy search (TSGS) algorithm is originally proposed to mitigate computational complexity. Extensive simulations have been done in a unified platform Veins combining SUMO and OMNET++. And numerous results show that the proposed algorithm can effectively improve the PDR by at least 15%, enhance the collision-avoidance performance by almost 40%, and reduce the MMP ratio by about 3% compared with the random transmitting, meanwhile meet the QoS requirement. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.00953
"An Exhaustive Survey on P4 Programmable Data Plane Switches: Taxonomy, Applications, Challenges, and Future Trends",Elie F. Kfoury;Jorge Crichigno;Elias Bou-Harb,"Traditionally, the data plane has been designed with fixed functions to forward packets using a small set of protocols. This closed-design paradigm has limited the capability of the switches to proprietary implementations which are hardcoded by vendors, inducing a lengthy, costly, and inflexible process. Recently, data plane programmability has attracted significant attention from both the research community and the industry, permitting operators and programmers in general to run customized packet processing function. This open-design paradigm is paving the way for an unprecedented wave of innovation and experimentation by reducing the time of designing, testing, and adopting new protocols; enabling a customized, top-down approach to develop network applications; providing granular visibility of packet events defined by the programmer; reducing complexity and enhancing resource utilization of the programmable switches; and drastically improving the performance of applications that are offloaded to the data plane. Despite the impressive advantages of programmable data plane switches and their importance in modern networks, the literature has been missing a comprehensive survey. To this end, this paper provides a background encompassing an overview of the evolution of networks from legacy to programmable, describing the essentials of programmable switches, and summarizing their advantages over Software-defined Networking (SDN) and legacy devices. The paper then presents a unique, comprehensive taxonomy of applications developed with P4 language; surveying, classifying, and analyzing more than 150 articles; discussing challenges and considerations; and presenting future perspectives and open research issues. △ Less","7 June, 2021",https://arxiv.org/pdf/2102.00643
Box Re-Ranking: Unsupervised False Positive Suppression for Domain Adaptive Pedestrian Detection,Weijie Chen;Yilu Guo;Shicai Yang;Zhaoyang Li;Zhenxin Ma;Binbin Chen;Long Zhao;Di Xie;Shiliang Pu;Yueting Zhuang,"False positive is one of the most serious problems brought by agnostic domain shift in domain adaptive pedestrian detection. However, it is impossible to label each box in countless target domains. Therefore, it yields our attention to suppress false positive in each target domain in an unsupervised way. In this paper, we model an object detection task into a ranking task among positive and negative boxes innovatively, and thus transform a false positive suppression problem into a box re-ranking problem elegantly, which makes it feasible to solve without manual annotation. An attached problem during box re-ranking appears that no labeled validation data is available for cherrypicking. Considering we aim to keep the detection of true positive unchanged, we propose box number alignment, a self-supervised evaluation metric, to prevent the optimized model from capacity degeneration. Extensive experiments conducted on cross-domain pedestrian detection datasets have demonstrated the effectiveness of our proposed framework. Furthermore, the extension to two general unsupervised domain adaptive object detection benchmarks also supports our superiority to other state-of-the-arts. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00595
A SDN/OpenFlow Framework for Dynamic Resource Allocation based on Bandwidth Allocation Model,Eliseu Silva Torres;Rafael F. Reale;Leobino N. Sampaio;Joberto S. B. Martins,"The communication network context in actual systems like 5G, cloud and IoT (Internet of Things), presents an ever-increasing number of users, applications, and services that are highly distributed with distinct and heterogeneous communications requirements. Resource allocation in this context requires dynamic, efficient, and customized solutions and Bandwidth Allocation Models (BAMs) are an alternative to support this new trend. This paper proposes the BAMSDN (Bandwidth Allocation Model through Software-Defined Networking) framework that dynamically allocates resources (bandwidth) for a MPLS (MultiProtocol Label Switching) network using a SDN (Software-Defined Networking)/OpenFlow strategy with BAM. The framework adopts an innovative implementation approach for BAM systems by controlling the MPLS network using SDN with OpenFlow. Experimental results suggest that using SDN/OpenFlow with BAM for bandwidth allocation does have effective advantages for MPLS networks requiring flexible resource sharing among applications and facilitates the migration path to a SDN/OpenFlow network. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00460
Toward Blockchain-Enabled Supply Chain Anti-Counterfeiting and Traceability,Neo C. K. Yiu,"Innovative solutions addressing product anti-counterfeiting and record provenance have been deployed across today's internationally spanning supply chain networks. These product anti-counterfeiting solutions are developed and implemented with centralized system architecture relying on centralized authorities or any form of intermediaries. Vulnerabilities of centralized product anti-counterfeiting solutions could possibly lead to system failure or susceptibility of malicious modifications performed on product records or various potential attacks to the system components by dishonest participant nodes traversing along the supply chain. Blockchain technology has progressed from merely with a use case of immutable ledger for cryptocurrency transactions to a programmable interactive environment of developing decentralized and reliable applications addressing different use cases globally. In this research, so as to facilitate trustworthy data provenance retrieval, verification and management, as well as strengthening capability of product anti-counterfeiting, key areas of decentralization and feasible mechanisms of developing decentralized and distributed product anti-counterfeiting and traceability ecosystems utilizing blockchain technology, are identified via a series of security and threat analyses performed mainly against NFC-Enabled Anti-Counterfeiting System (NAS) which is one of the solutions currently implemented in the industry with centralized architecture. A set of fundamental system requirements are set out for developing a blockchain-enabled autonomous and decentralized solution for supply chain anti-counterfeiting and traceability, as a secure and immutable scientific data provenance tracking and management platform in which provenance records, providing compelling properties on data integrity of luxurious goods, are recorded and verified automatically, for supply chain industry. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00459
Model-Based Testing of Networked Applications,Yishuai Li;Benjamin C. Pierce;Steve Zdancewic,"We present a principled automatic testing framework for application-layer protocols. The key innovation is a domain-specific embedded language for writing nondeterministic models of the behavior of networked servers. These models are defined within the Coq interactive theorem prover, supporting a smooth transition from testing to formal verification. Given a server model, we show how to automatically derive a tester that probes the server for unexpected behaviors. We address the uncertainties caused by both the server's internal choices and the network delaying messages nondeterministically. The derived tester accepts server implementations whose possible behaviors are a subset of those allowed by the nondeterministic model. We demonstrate the effectiveness of this framework by using it to specify and test a fragment of the HTTP/1.1 protocol, showing that the automatically derived tester can capture RFC violations in buggy server implementations, including the latest versions of Apache and Nginx. △ Less","2 July, 2021",https://arxiv.org/pdf/2102.00378
A Competitive Edge: Can FPGAs Beat GPUs at DCNN Inference Acceleration in Resource-Limited Edge Computing Applications?,Ian Colbert;Jake Daly;Ken Kreutz-Delgado;Srinjoy Das,"When trained as generative models, Deep Learning algorithms have shown exceptional performance on tasks involving high dimensional data such as image denoising and super-resolution. In an increasingly connected world dominated by mobile and edge devices, there is surging demand for these algorithms to run locally on embedded platforms. FPGAs, by virtue of their reprogrammability and low-power characteristics, are ideal candidates for these edge computing applications. As such, we design a spatio-temporally parallelized hardware architecture capable of accelerating a deconvolution algorithm optimized for power-efficient inference on a resource-limited FPGA. We propose this FPGA-based accelerator to be used for Deconvolutional Neural Network (DCNN) inference in low-power edge computing applications. To this end, we develop methods that systematically exploit micro-architectural innovations, design space exploration, and statistical analysis. Using a Xilinx PYNQ-Z2 FPGA, we leverage our architecture to accelerate inference for two DCNNs trained on the MNIST and CelebA datasets using the Wasserstein GAN framework. On these networks, our FPGA design achieves a higher throughput to power ratio with lower run-to-run variation when compared to the NVIDIA Jetson TX1 edge computing GPU. △ Less","9 March, 2021",https://arxiv.org/pdf/2102.00294
An evolutionary view on the emergence of Artificial Intelligence,Matheus E. Leusin;Bjoern Jindra;Daniel S. Hain,"This paper draws upon the evolutionary concepts of technological relatedness and knowledge complexity to enhance our understanding of the long-term evolution of Artificial Intelligence (AI). We reveal corresponding patterns in the emergence of AI - globally and in the context of specific geographies of the US, Japan, South Korea, and China. We argue that AI emergence is associated with increasing related variety due to knowledge commonalities as well as increasing complexity. We use patent-based indicators for the period between 1974-2018 to analyse the evolution of AI's global technological space, to identify its technological core as well as changes to its overall relatedness and knowledge complexity. At the national level, we also measure countries' overall specialisations against AI-specific ones. At the global level, we find increasing overall relatedness and complexity of AI. However, for the technological core of AI, which has been stable over time, we find decreasing related variety and increasing complexity. This evidence points out that AI innovations related to core technologies are becoming increasingly distinct from each other. At the country level, we find that the US and Japan have been increasing the overall relatedness of their innovations. The opposite is the case for China and South Korea, which we associate with the fact that these countries are overall less technologically developed than the US and Japan. Finally, we observe a stable increasing overall complexity for all countries apart from China, which we explain by the focus of this country in technologies not strongly linked to AI. △ Less","30 January, 2021",https://arxiv.org/pdf/2102.00233
Rapid detection of fast innovation under the pressure of COVID-19,Nicola Melluso;Andrea Bonaccorsi;Filippo Chiarello;Gualtiero Fantoni,"Covid-19 has rapidly redefined the agenda of technological research and development both for academics and practitioners. If the medical scientific publication system has promptly reacted to this new situation, other domains, particularly in new technologies, struggle to map what is happening in their contexts. The pandemic has created the need for a rapid detection of technological convergence phenomena, but at the same time it has made clear that this task is impossible on the basis of traditional patent and publication indicators. This paper presents a novel methodology to perform a rapid detection of the fast technological convergence phenomenon that is occurring under the pressure of the Covid-19 pandemic. The fast detection has been performed thanks to the use of a novel source: the online blogging platform Medium. We demonstrate that the hybrid structure of this social journalism platform allows a rapid detection of innovation phenomena, unlike other traditional sources. The technological convergence phenomenon has been modelled through a network-based approach, analysing the differences of networks computed during two time periods (pre and post COVID-19). The results led us to discuss the repurposing of technologies regarding ""Remote Control"", ""Remote Working"", ""Health"" and ""Remote Learning"". △ Less","30 January, 2021",https://arxiv.org/pdf/2102.00197
Graph Embedding for Recommendation against Attribute Inference Attacks,Shijie Zhang;Hongzhi Yin;Tong Chen;Zi Huang;Lizhen Cui;Xiangliang Zhang,"In recent years, recommender systems play a pivotal role in helping users identify the most suitable items that satisfy personal preferences. As user-item interactions can be naturally modelled as graph-structured data, variants of graph convolutional networks (GCNs) have become a well-established building block in the latest recommenders. Due to the wide utilization of sensitive user profile data, existing recommendation paradigms are likely to expose users to the threat of privacy breach, and GCN-based recommenders are no exception. Apart from the leakage of raw user data, the fragility of current recommenders under inference attacks offers malicious attackers a backdoor to estimate users' private attributes via their behavioral footprints and the recommendation results. However, little attention has been paid to developing recommender systems that can defend such attribute inference attacks, and existing works achieve attack resistance by either sacrificing considerable recommendation accuracy or only covering specific attack models or protected information. In our paper, we propose GERAI, a novel differentially private graph convolutional network to address such limitations. Specifically, in GERAI, we bind the information perturbation mechanism in differential privacy with the recommendation capability of graph convolutional networks. Furthermore, based on local differential privacy and functional mechanism, we innovatively devise a dual-stage encryption paradigm to simultaneously enforce privacy guarantee on users' sensitive features and the model optimization process. Extensive experiments show the superiority of GERAI in terms of its resistance to attribute inference attacks and recommendation effectiveness. △ Less","29 January, 2021",https://arxiv.org/pdf/2101.12549
Open problems in cross-chain protocols,Thomas Eizinger;Philipp Hoenisch;Lucas Soriano del Pino,"Blockchain interoperability is a prominent research field which aims to build bridges between otherwise isolated blockchains. With advances in cryptography, novel protocols are published by academia and applied in different applications and products in the industry. In theory, these innovative protocols provide strong privacy and security guarantees by including formal proofs. However, pure theoretical work often lacks the perspective of real world applications. In this work, we describe a number of hardly researched problems which developers encounter when building cross-chain products. △ Less","29 January, 2021",https://arxiv.org/pdf/2101.12412
A Pub-Sub Architecture to Promote Blockchain Interoperability,Sara Ghaemi;Sara Rouhani;Rafael Belchior;Rui S. Cruz;Hamzeh Khazaei;Petr Musilek,"The maturing of blockchain technology leads to heterogeneity, where multiple solutions specialize in a particular use case. While the development of different blockchain networks shows great potential for blockchains, the isolated networks have led to data and asset silos, limiting the applications of this technology. Blockchain interoperability solutions are essential to enable distributed ledgers to reach their full potential. Such solutions allow blockchains to support asset and data transfer, resulting in the development of innovative applications. This paper proposes a novel blockchain interoperability solution for permissioned blockchains based on the publish/subscribe architecture. We implemented a prototype of this platform to show the feasibility of our design. We evaluate our solution by implementing examples of the different publisher and subscriber networks, such as Hyperledger Besu, which is an Ethereum client, and two different versions of Hyperledger Fabric. We present a performance analysis of the whole network that indicates its limits and bottlenecks. Finally, we discuss the extensibility and scalability of the platform in different scenarios. Our evaluation shows that our system can handle a throughput in the order of the hundreds of transactions per second. △ Less","28 January, 2021",https://arxiv.org/pdf/2101.12331
Discriminative Appearance Modeling with Multi-track Pooling for Real-time Multi-object Tracking,Chanho Kim;Li Fuxin;Mazen Alotaibi;James M. Rehg,"In multi-object tracking, the tracker maintains in its memory the appearance and motion information for each object in the scene. This memory is utilized for finding matches between tracks and detections and is updated based on the matching result. Many approaches model each target in isolation and lack the ability to use all the targets in the scene to jointly update the memory. This can be problematic when there are similar looking objects in the scene. In this paper, we solve the problem of simultaneously considering all tracks during memory updating, with only a small spatial overhead, via a novel multi-track pooling module. We additionally propose a training strategy adapted to multi-track pooling which generates hard tracking episodes online. We show that the combination of these innovations results in a strong discriminative appearance model, enabling the use of greedy data association to achieve online tracking performance. Our experiments demonstrate real-time, state-of-the-art performance on public multi-object tracking (MOT) datasets. △ Less","28 January, 2021",https://arxiv.org/pdf/2101.12159
BERTaú: Itaú BERT for digital customer service,Paulo Finardi;José Dié Viegas;Gustavo T. Ferreira;Alex F. Mansano;Vinicius F. Caridá,"In the last few years, three major topics received increased interest: deep learning, NLP and conversational agents. Bringing these three topics together to create an amazing digital customer experience and indeed deploy in production and solve real-world problems is something innovative and disruptive. We introduce a new Portuguese financial domain language representation model called BERTaú. BERTaú is an uncased BERT-base trained from scratch with data from the Itaú virtual assistant chatbot solution. Our novel contribution is that BERTaú pretrained language model requires less data, reached state-of-the-art performance in three NLP tasks, and generates a smaller and lighter model that makes the deployment feasible. We developed three tasks to validate our model: information retrieval with Frequently Asked Questions (FAQ) from Itaú bank, sentiment analysis from our virtual assistant data, and a NER solution. All proposed tasks are real-world solutions in production on our environment and the usage of a specialist model proved to be effective when compared to Google BERT multilingual and the DPRQuestionEncoder from Facebook, available at Hugging Face. The BERTaú improves the performance in 22% of FAQ Retrieval MRR metric, 2.1% in Sentiment Analysis F1 score, 4.4% in NER F1 score and can also represent the same sequence in up to 66% fewer tokens when compared to ""shelf models"". △ Less","25 July, 2021",https://arxiv.org/pdf/2101.12015
How the Design of YouTube Influences User Sense of Agency,Kai Lukoff;Ulrik Lyngs;Himanshu Zade;J. Vera Liao;James Choi;Kaiyue Fan;Sean A. Munson;Alexis Hiniker,"In the attention economy, video apps employ design mechanisms like autoplay that exploit psychological vulnerabilities to maximize watch time. Consequently, many people feel a lack of agency over their app use, which is linked to negative life effects such as loss of sleep. Prior design research has innovated external mechanisms that police multiple apps, such as lockout timers. In this work, we shift the focus to how the internal mechanisms of an app can support user agency, taking the popular YouTube mobile app as a test case. From a survey of 120 U.S. users, we find that autoplay and recommendations primarily undermine sense of agency, while search and playlists support it. From 13 co-design sessions, we find that when users have a specific intention for how they want to use YouTube they prefer interfaces that support greater agency. We discuss implications for how designers can help users reclaim a sense of agency over their media use. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11778
TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models,Chunxing Yin;Bilge Acun;Xing Liu;Carole-Jean Wu,"The memory capacity of embedding tables in deep learning recommendation models (DLRMs) is increasing dramatically from tens of GBs to TBs across the industry. Given the fast growth in DLRMs, novel solutions are urgently needed, in order to enable fast and efficient DLRM innovations. At the same time, this must be done without having to exponentially increase infrastructure capacity demands. In this paper, we demonstrate the promising potential of Tensor Train decomposition for DLRMs (TT-Rec), an important yet under-investigated context. We design and implement optimized kernels (TT-EmbeddingBag) to evaluate the proposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT implementation. The performance of TT-Rec is further optimized with the batched matrix multiplication and caching strategies for embedding vector lookup operations. In addition, we present mathematically and empirically the effect of weight initialization distribution on DLRM accuracy and propose to initialize the tensor cores of TT-Rec following the sampled Gaussian distribution. We evaluate TT-Rec across three important design space dimensions -- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM with Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112 times model size compression, for Kaggle and Terabyte, respectively. This impressive model size reduction can come with no accuracy nor training time overhead as compared to the uncompressed baseline. △ Less","25 January, 2021",https://arxiv.org/pdf/2101.11714
SkillNER: Mining and Mapping Soft Skills from any Text,Silvia Fareri;Nicola Melluso;Filippo Chiarello;Gualtiero Fantoni,"In today's digital world, there is an increasing focus on soft skills. On the one hand, they facilitate innovation at companies, but on the other, they are unlikely to be automated soon. Researchers struggle with accurately approaching quantitatively the study of soft skills due to the lack of data-driven methods to retrieve them. This limits the possibility for psychologists and HR managers to understand the relation between humans and digitalisation. This paper presents SkillNER, a novel data-driven method for automatically extracting soft skills from text. It is a named entity recognition (NER) system trained with a support vector machine (SVM) on a corpus of more than 5000 scientific papers. We developed this system by measuring the performance of our approach against different training models and validating the results together with a team of psychologists. Finally, SkillNER was tested in a real-world case study using the job descriptions of ESCO (European Skill/Competence Qualification and Occupation) as textual source. The system enabled the detection of communities of job profiles based on their shared soft skills and communities of soft skills based on their shared job profiles. This case study demonstrates that the tool can automatically retrieve soft skills from a large corpus in an efficient way, proving useful for firms, institutions, and workers. The tool is open and available online to foster quantitative methods for the study of soft skills. △ Less","12 July, 2021",https://arxiv.org/pdf/2101.11431
Modeling opinion leader's role in the diffusion of innovation,Natasa Vodopivec;Carole Adam;Jean-Pierre Chanteau,"The diffusion of innovations is an important topic for the consumer markets. Early research focused on how innovations spread on the level of the whole society. To get closer to the real world scenarios agent based models (ABM) started focusing on individual-level agents. In our work we will translate an existing ABM that investigates the role of opinion leaders in the process of diffusion of innovations to a new, more expressive platform designed for agent based modeling, GAMA. We will do it to show that taking advantage of new features of the chosen platform should be encouraged when making models in the field of social sciences in the future, because it can be beneficial for the explanatory power of simulation results. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11260
A Game Theory Based Ramp Merging Strategy for Connected and Automated Vehicles in the Mixed Traffic: A Unity-SUMO Integrated Platform,Xishun Liao;Xuanpeng Zhao;Guoyuan Wu;Matthew Barth;Ziran Wang;Kyungtae Han;Prashant Tiwari,"Ramp merging is considered as one of the major causes of traffic congestion and accidents because of its chaotic nature. With the development of connected and automated vehicle (CAV) technology, cooperative ramp merging has become one of the popular solutions to this problem. In a mixed traffic situation, CAVs will not only interact with each other, but also handle complicated situations with human-driven vehicles involved. In this paper, a game theory-based ramp merging strategy has been developed for the optimal merging coordination of CAVs in the mixed traffic, which determines dynamic merging sequence and corresponding longitudinal/lateral control. This strategy improves the safety and efficiency of the merging process by ensuring a safe inter-vehicle distance among the involved vehicles and harmonizing the speed of CAVs in the traffic stream. To verify the proposed strategy, mixed traffic simulations under different penetration rates and different congestion levels have been carried out on an innovative Unity-SUMO integrated platform, which connects a game engine-based driving simulator with a traffic simulator. This platform allows the human driver to participate in the simulation, and also equip CAVs with more realistic sensing systems. In the traffic flow level simulation test, Unity takes over the sensing and control of all CAVs in the simulation, while SUMO handles the behavior of all legacy vehicles. The results show that the average speed of traffic flow can be increased up to 110%, and the fuel consumption can be reduced up to 77%, respectively. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11237
ImageCHD: A 3D Computed Tomography Image Dataset for Classification of Congenital Heart Disease,Xiaowei Xu;Tianchen Wang;Jian Zhuang;Haiyun Yuan;Meiping Huang;Jianzheng Cen;Qianjun Jia;Yuhao Dong;Yiyu Shi,"Congenital heart disease (CHD) is the most common type of birth defect, which occurs 1 in every 110 births in the United States. CHD usually comes with severe variations in heart structure and great artery connections that can be classified into many types. Thus highly specialized domain knowledge and the time-consuming human process is needed to analyze the associated medical images. On the other hand, due to the complexity of CHD and the lack of dataset, little has been explored on the automatic diagnosis (classification) of CHDs. In this paper, we present ImageCHD, the first medical image dataset for CHD classification. ImageCHD contains 110 3D Computed Tomography (CT) images covering most types of CHD, which is of decent size Classification of CHDs requires the identification of large structural changes without any local tissue changes, with limited data. It is an example of a larger class of problems that are quite difficult for current machine-learning-based vision methods to solve. To demonstrate this, we further present a baseline framework for the automatic classification of CHD, based on a state-of-the-art CHD segmentation method. Experimental results show that the baseline framework can only achieve a classification accuracy of 82.0\% under a selective prediction scheme with 88.4\% coverage, leaving big room for further improvement. We hope that ImageCHD can stimulate further research and lead to innovative and generic solutions that would have an impact in multiple domains. Our dataset is released to the public compared with existing medical imaging datasets. △ Less","11 May, 2021",https://arxiv.org/pdf/2101.10799
Short-term prediction of Time Series based on bounding techniques,Pedro Cadahía;Jose Manuel Bravo Caro,"In this paper it is reconsidered the prediction problem in time series framework by using a new non-parametric approach. Through this reconsideration, the prediction is obtained by a weighted sum of past observed data. These weights are obtained by solving a constrained linear optimization problem that minimizes an outer bound of the prediction error. The innovation is to consider both deterministic and stochastic assumptions in order to obtain the upper bound of the prediction error, a tuning parameter is used to balance these deterministic-stochastic assumptions in order to improve the predictor performance. A benchmark is included to illustrate that the proposed predictor can obtain suitable results in a prediction scheme, and can be an interesting alternative method to the classical non-parametric methods. Besides, it is shown how this model can outperform the preexisting ones in a short term forecast. △ Less","26 January, 2021",https://arxiv.org/pdf/2101.10719
Modern Machine and Deep Learning Systems as a way to achieve Man-Computer Symbiosis,Chirag Gupta,"Man-Computer Symbiosis (MCS) was originally envisioned by the famous computer pioneer J.C.R. Licklider in 1960, as a logical evolution of the then inchoate relationship between computer and humans. In his paper, Licklider provided a set of criteria by which to judge if a Man-Computer System is a symbiotic one, and also provided some predictions about such systems in the near and far future. Since then, innovations in computer networks and the invention of the Internet were major developments towards that end. However, with most systems based on conventional logical algorithms, many aspects of Licklider's MCS remained unfulfilled. This paper explores the extent to which modern machine learning systems in general, and deep learning ones in particular best exemplify MCS systems, and why they are the prime contenders to achieve a true Man-Computer Symbiosis as described by Licklider in his original paper in the future. The case for deep learning is built by illustrating each point of the original criteria as well as the criteria laid by subsequent research into MCS systems, with specific examples and applications provided to strengthen the arguments. The efficacy of deep neural networks in achieving Artificial General Intelligence, which would be the perfect version of an MCS system is also explored. △ Less","24 January, 2021",https://arxiv.org/pdf/2101.10534
Fast Non-line-of-sight Imaging with Two-step Deep Remapping,Dayu Zhu;Wenshan Cai,"Conventional imaging only records photons directly sent from the object to the detector, while non-line-of-sight (NLOS) imaging takes the indirect light into account. Most NLOS solutions employ a transient scanning process, followed by a physical based algorithm to reconstruct the NLOS scenes. However, the transient detection requires sophisticated apparatus, with long scanning time and low robustness to ambient environment, and the reconstruction algorithms are typically time-consuming and computationally expensive. Here we propose a new NLOS solution to address the above defects, with innovations on both equipment and algorithm. We apply inexpensive commercial Lidar for detection, with much higher scanning speed and better compatibility to real-world imaging. Our reconstruction framework is deep learning based, with a generative two-step remapping strategy to guarantee high reconstruction fidelity. The overall detection and reconstruction process allows for millisecond responses, with reconstruction precision of millimeter level. We have experimentally tested the proposed solution on both synthetic and real objects, and further demonstrated our method to be applicable to full-color NLOS imaging. △ Less","25 March, 2021",https://arxiv.org/pdf/2101.10492
Dairy Cow rumination detection: A deep learning approach,Safa Ayadi;Ahmed ben said;Rateb Jabbar;Chafik Aloulou;Achraf Chabbouh;Ahmed Ben Achballah,"Cattle activity is an essential index for monitoring health and welfare of the ruminants. Thus, changes in the livestock behavior are a critical indicator for early detection and prevention of several diseases. Rumination behavior is a significant variable for tracking the development and yield of animal husbandry. Therefore, various monitoring methods and measurement equipment have been used to assess cattle behavior. However, these modern attached devices are invasive, stressful and uncomfortable for the cattle and can influence negatively the welfare and diurnal behavior of the animal. Multiple research efforts addressed the problem of rumination detection by adopting new methods by relying on visual features. However, they only use few postures of the dairy cow to recognize the rumination or feeding behavior. In this study, we introduce an innovative monitoring method using Convolution Neural Network (CNN)-based deep learning models. The classification process is conducted under two main labels: ruminating and other, using all cow postures captured by the monitoring camera. Our proposed system is simple and easy-to-use which is able to capture long-term dynamics using a compacted representation of a video in a single 2D image. This method proved efficiency in recognizing the rumination behavior with 95%, 98% and 98% of average accuracy, recall and precision, respectively. △ Less","7 January, 2021",https://arxiv.org/pdf/2101.10445
Reconfigurable Intelligent Surface for Massive Connectivity,Shuhao Xia;Yuanming Shi;Yong Zhou;Xiaojun Yuan,"With the rapid development of Internet of Things (IoT), massive machine-type communication has become a promising application scenario, where a large number of devices transmit sporadically to a base station (BS). Reconfigurable intelligent surface (RIS) has been recently proposed as an innovative new technology to achieve energy efficiency and coverage enhancement by establishing favorable signal propagation environments, thereby improving data transmission in massive connectivity. Nevertheless, the BS needs to detect active devices and estimate channels to support data transmission in RIS-assisted massive access systems, which yields unique challenges. This paper shall consider an RIS-assisted uplink IoT network and aims to solve the RIS-related activity detection and channel estimation problem, where the BS detects the active devices and estimates the separated channels of the RIS-to-device link and the RIS-to-BS link. Due to limited scattering between the RIS and the BS, we model the RIS-to-BS channel as a sparse channel. As a result, by simultaneously exploiting both the sparsity of sporadic transmission in massive connectivity and the RIS-to-BS channels, we formulate the RIS-related activity detection and channel estimation problem as a sparse matrix factorization problem. Furthermore, we develop an approximate message passing (AMP) based algorithm to solve the problem based on Bayesian inference framework and reduce the computational complexity by approximating the algorithm with the central limit theorem and Taylor series arguments. Finally, extensive numerical experiments are conducted to verify the effectiveness and improvements of the proposed algorithm. △ Less","13 January, 2021",https://arxiv.org/pdf/2101.10322
Assessing the Impact: Does an Improvement to a Revenue Management System Lead to an Improved Revenue?,Greta Laage;Emma Frejinger;Andrea Lodi;Guillaume Rabusseau,"Airlines and other industries have been making use of sophisticated Revenue Management Systems to maximize revenue for decades. While improving the different components of these systems has been the focus of numerous studies, estimating the impact of such improvements on the revenue has been overlooked in the literature despite its practical importance. Indeed, quantifying the benefit of a change in a system serves as support for investment decisions. This is a challenging problem as it corresponds to the difference between the generated value and the value that would have been generated keeping the system as before. The latter is not observable. Moreover, the expected impact can be small in relative value. In this paper, we cast the problem as counterfactual prediction of unobserved revenue. The impact on revenue is then the difference between the observed and the estimated revenue. The originality of this work lies in the innovative application of econometric methods proposed for macroeconomic applications to a new problem setting. Broadly applicable, the approach benefits from only requiring revenue data observed for origin-destination pairs in the network of the airline at each day, before and after a change in the system is applied. We report results using real large-scale data from Air Canada. We compare a deep neural network counterfactual predictions model with econometric models. They achieve respectively 1% and 1.1% of error on the counterfactual revenue predictions, and allow to accurately estimate small impacts (in the order of 2%). △ Less","16 June, 2021",https://arxiv.org/pdf/2101.10249
GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks,Tianming Zhao;Chunyang Chen;Yuanning Liu;Xiaodong Zhu,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our GUIGAN is based on SeqGAN by modeling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Frechet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs. △ Less","26 January, 2021",https://arxiv.org/pdf/2101.09978
Fighting deepfakes by detecting GAN DCT anomalies,Oliver Giudice;Luca Guarnera;Sebastiano Battiato,"To properly contrast the Deepfake phenomenon the need to design new Deepfake detection algorithms arises; the misuse of this formidable A.I. technology brings serious consequences in the private life of every involved person. State-of-the-art proliferates with solutions using deep neural networks to detect a fake multimedia content but unfortunately these algorithms appear to be neither generalizable nor explainable. However, traces left by Generative Adversarial Network (GAN) engines during the creation of the Deepfakes can be detected by analyzing ad-hoc frequencies. For this reason, in this paper we propose a new pipeline able to detect the so-called GAN Specific Frequencies (GSF) representing a unique fingerprint of the different generative architectures. By employing Discrete Cosine Transform (DCT), anomalous frequencies were detected. The \BETA statistics inferred by the AC coefficients distribution have been the key to recognize GAN-engine generated data. Robustness tests were also carried out in order to demonstrate the effectiveness of the technique using different attacks on images such as JPEG Compression, mirroring, rotation, scaling, addition of random sized rectangles. Experiments demonstrated that the method is innovative, exceeds the state of the art and also give many insights in terms of explainability. △ Less","11 August, 2021",https://arxiv.org/pdf/2101.09781
Effects of Pre- and Post-Processing on type-based Embeddings in Lexical Semantic Change Detection,Jens Kaiser;Sinan Kurtyigit;Serge Kotchourko;Dominik Schlechtweg,"Lexical semantic change detection is a new and innovative research field. The optimal fine-tuning of models including pre- and post-processing is largely unclear. We optimize existing models by (i) pre-training on large corpora and refining on diachronic target corpora tackling the notorious small data problem, and (ii) applying post-processing transformations that have been shown to improve performance on synchronic tasks. Our results provide a guide for the application and optimization of lexical semantic change detection models across various learning scenarios. △ Less","26 January, 2021",https://arxiv.org/pdf/2101.09368
"Benchmarking, Analysis, and Optimization of Serverless Function Snapshots",Dmitrii Ustiugov;Plamen Petrov;Marios Kogias;Edouard Bugnion;Boris Grot,"Serverless computing has seen rapid adoption due to its high scalability and flexible, pay-as-you-go billing model. In serverless, developers structure their services as a collection of functions, sporadically invoked by various events like clicks. High inter-arrival time variability of function invocations motivates the providers to start new function instances upon each invocation, leading to significant cold-start delays that degrade user experience. To reduce cold-start latency, the industry has turned to snapshotting, whereby an image of a fully-booted function is stored on disk, enabling a faster invocation compared to booting a function from scratch. This work introduces vHive, an open-source framework for serverless experimentation with the goal of enabling researchers to study and innovate across the entire serverless stack. Using vHive, we characterize a state-of-the-art snapshot-based serverless infrastructure, based on industry-leading Containerd orchestration framework and Firecracker hypervisor technologies. We find that the execution time of a function started from a snapshot is 95% higher, on average, than when the same function is memory-resident. We show that the high latency is attributable to frequent page faults as the function's state is brought from disk into guest memory one page at a time. Our analysis further reveals that functions access the same stable working set of pages across different invocations of the same function. By leveraging this insight, we build REAP, a light-weight software mechanism for serverless hosts that records functions' stable working set of guest memory pages and proactively prefetches it from disk into memory. Compared to baseline snapshotting, REAP slashes the cold-start delays by 3.7x, on average. △ Less","5 February, 2021",https://arxiv.org/pdf/2101.09355
Decision process for blockchain architectures based on requirements,Nicolas Six,"In recent years, blockchain has grown in popularity due to its singular attributes, enabling the development of new innovative decentralized applications. But when companies consider leveraging blockchain for their applications, the plethora of possible choices and the difficulty of integrating blockchain into architectures can hinder its adoption. Our research project aims to ease the adoption of blockchain into companies, notably with the construction of an automated decision process to solve this issue in which requirements are first-class citizens, a knowledge base containing architectural patterns and blockchains refined over time, and an architecture generator able to process outputs into architectural stubs. This paper will also present our current progression on this decision process, by introducing the preliminary version that is able to choose the most suitable blockchain between multiple choices and our process-driven benchmarking tool. △ Less","22 January, 2021",https://arxiv.org/pdf/2101.08998
Iterative Optimisation with an Innovation CNN for Pose Refinement,Gerard Kennedy;Zheyu Zhuang;Xin Yu;Robert Mahony,"Object pose estimation from a single RGB image is a challenging problem due to variable lighting conditions and viewpoint changes. The most accurate pose estimation networks implement pose refinement via reprojection of a known, textured 3D model, however, such methods cannot be applied without high quality 3D models of the observed objects. In this work we propose an approach, namely an Innovation CNN, to object pose estimation refinement that overcomes the requirement for reprojecting a textured 3D model. Our approach improves initial pose estimation progressively by applying the Innovation CNN iteratively in a stochastic gradient descent (SGD) framework. We evaluate our method on the popular LINEMOD and Occlusion LINEMOD datasets and obtain state-of-the-art performance on both datasets. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.08895
The Internet of Things in Ports: Six Key Security and Governance Challenges for the UK (Policy Brief),Feja Lesniewska;Uchenna D Ani;Jeremy M Watson;Madeline Carr,"In January 2019, the UK Government published its Maritime 2050 on Navigating the Future strategy. In the strategy, the government highlighted the importance of digitalization (with well-designed regulatory support) to achieve its goal of ensuring that the UK plays a global leadership role in the maritime sector. Ports, the gateways for 95% of UK trade movements, were identified as key sites for investment in technological innovation. The government identified the potential of the Internet of Things (IoT), in conjunction with other information-sharing technologies, such as shared data platforms, and Artificial Intelligence applications (AI), to synchronize processes within the port ecosystem leading to improved efficiency, safety, and environmental benefits, including improved air quality and lower greenhouse gas emissions. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.08812
Fidelity and Privacy of Synthetic Medical Data,Ofer Mendelevitch;Michael D. Lesh,"The digitization of medical records ushered in a new era of big data to clinical science, and with it the possibility that data could be shared, to multiply insights beyond what investigators could abstract from paper records. The need to share individual-level medical data to accelerate innovation in precision medicine continues to grow, and has never been more urgent, as scientists grapple with the COVID-19 pandemic. However, enthusiasm for the use of big data has been tempered by a fully appropriate concern for patient autonomy and privacy. That is, the ability to extract private or confidential information about an individual, in practice, renders it difficult to share data, since significant infrastructure and data governance must be established before data can be shared. Although HIPAA provided de-identification as an approved mechanism for data sharing, linkage attacks were identified as a major vulnerability. A variety of mechanisms have been established to avoid leaking private information, such as field suppression or abstraction, strictly limiting the amount of information that can be shared, or employing mathematical techniques such as differential privacy. Another approach, which we focus on here, is creating synthetic data that mimics the underlying data. For synthetic data to be a useful mechanism in support of medical innovation and a proxy for real-world evidence, one must demonstrate two properties of the synthetic dataset: (1) any analysis on the real data must be matched by analysis of the synthetic data (statistical fidelity) and (2) the synthetic data must preserve privacy, with minimal risk of re-identification (privacy guarantee). In this paper we propose a framework for quantifying the statistical fidelity and privacy preservation properties of synthetic datasets and demonstrate these metrics for synthetic data generated by Syntegra technology. △ Less","2 June, 2021",https://arxiv.org/pdf/2101.08658
FWB-Net:Front White Balance Network for Color Shift Correction in Single Image Dehazing via Atmospheric Light Estimation,Cong Wang;Yan Huang;Yuexian Zou;Yong Xu,"In recent years, single image dehazing deep models based on Atmospheric Scattering Model (ASM) have achieved remarkable results. But the dehazing outputs of those models suffer from color shift. Analyzing the ASM model shows that the atmospheric light factor (ALF) is set as a scalar which indicates ALF is constant for whole image. However, for images taken in real-world, the illumination is not uniformly distributed over whole image which brings model mismatch and possibly results in color shift of the deep models using ASM. Bearing this in mind, in this study, first, a new non-homogeneous atmospheric scattering model (NH-ASM) is proposed for improving image modeling of hazy images taken under complex illumination conditions. Second, a new U-Net based front white balance module (FWB-Module) is dedicatedly designed to correct color shift before generating dehazing result via atmospheric light estimation. Third, a new FWB loss is innovatively developed for training FWB-Module, which imposes penalty on color shift. In the end, based on NH-ASM and front white balance technology, an end-to-end CNN-based color-shift-restraining dehazing network is developed, termed as FWB-Net. Experimental results demonstrate the effectiveness and superiority of our proposed FWB-Net for dehazing on both synthetic and real-world images. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.08465
5G D2D Transmission Mode Selection Performance & Cluster Limits Evaluation of Distributed Artificial Intelligence and Machine Learning Techniques,Iacovos Ioannou;Christophoros Christophorou;Vasos Vassiliou;Andreas Pitsillides,"5G D2D Communication promises improvements in energy and spectral efficiency, overall system capacity, and higher data rates. However, to achieve optimum results it is important to select wisely the Transmission mode of the D2D Device to form clusters in the most fruitful positions in terms of Sum Rate and Power Consumption. Towards this end, this paper investigates the use of Distributed Artificial Intelligence (DAI) and innovative to D2D, Machine Learning (ML) approaches to achieve satisfactory results in terms of Spectral Efficiency (SE), Power Consumption (PC) and execution time, with the creation of clusters and backhauling D2D network under existing Base Station/Small Cell. Additionally, one of the major factors that affect the creation of high-quality clusters under a D2D network is the number of the Devices. Therefore, this paper focuses on a small (<=200) number of Devices, with the purpose to identify the limits of each approach in terms of number of devices. Specifically, to identify where it is beneficial to form a cluster, investigate the critical point that gains increases rapidly and at the end examine the applicability of 5G requirements. Additionally, prior work presented a Distributed Artificial Intelligence (DAI) Solution/Framework in D2D and a DAIS Transmission Mode Selection (TMS) plan was proposed. In this paper DAIS is further examined, improved in terms of thresholds evaluation, evaluated, and compared with other approaches (AI/ML). The results obtained demonstrate the exceptional performance of DAIS, compared to all other related approaches in terms of SE, PC, execution time and cluster formation efficiency. Also, results show that the investigated AI/ML approaches are also beneficial for Transmission Mode Selection (TMS) in 5G D2D communication, even with a smaller (i.e., >=5 D2D Relay,>=50 D2D Multi Hop Relay) numbers of devices as a lower limits. △ Less","28 April, 2021",https://arxiv.org/pdf/2101.08014
Macroscopic Control of Text Generation for Image Captioning,Zhangzi Zhu;Tianlei Wang;Hong Qu,"Despite the fact that image captioning models have been able to generate impressive descriptions for a given image, challenges remain: (1) the controllability and diversity of existing models are still far from satisfactory; (2) models sometimes may produce extremely poor-quality captions. In this paper, two novel methods are introduced to solve the problems respectively. Specifically, for the former problem, we introduce a control signal which can control the macroscopic sentence attributes, such as sentence quality, sentence length, sentence tense and number of nouns etc. With such a control signal, the controllability and diversity of existing captioning models are enhanced. For the latter problem, we innovatively propose a strategy that an image-text matching model is trained to measure the quality of sentences generated in both forward and backward directions and finally choose the better one. As a result, this strategy can effectively reduce the proportion of poorquality sentences. Our proposed methods can be easily applie on most image captioning models to improve their overall performance. Based on the Up-Down model, the experimental results show that our methods achieve BLEU- 4/CIDEr/SPICE scores of 37.5/120.3/21.5 on MSCOCO Karpathy test split with cross-entropy training, which surpass the results of other state-of-the-art methods trained by cross-entropy loss. △ Less","20 January, 2021",https://arxiv.org/pdf/2101.08000
Network Pruning using Adaptive Exemplar Filters,Mingbao Lin;Rongrong Ji;Shaojie Li;Yan Wang;Yongjian Wu;Feiyue Huang;Qixiang Ye,"Popular network pruning algorithms reduce redundant information by optimizing hand-crafted models, and may cause suboptimal performance and long time in selecting filters. We innovatively introduce adaptive exemplar filters to simplify the algorithm design, resulting in an automatic and efficient pruning approach called EPruner. Inspired by the face recognition community, we use a message passing algorithm Affinity Propagation on the weight matrices to obtain an adaptive number of exemplars, which then act as the preserved filters. EPruner breaks the dependency on the training data in determining the ""important"" filters and allows the CPU implementation in seconds, an order of magnitude faster than GPU based SOTAs. Moreover, we show that the weights of exemplars provide a better initialization for the fine-tuning. On VGGNet-16, EPruner achieves a 76.34%-FLOPs reduction by removing 88.80% parameters, with 0.06% accuracy improvement on CIFAR-10. In ResNet-152, EPruner achieves a 65.12%-FLOPs reduction by removing 64.18% parameters, with only 0.71% top-5 accuracy loss on ILSVRC-2012. Our code can be available at https://github.com/lmbxmu/EPruner. △ Less","26 May, 2021",https://arxiv.org/pdf/2101.07985
DeepTrust: A Deep Learning Approach for Measuring Social Media Users Trustworthiness,Majed Alrubaian;Muhammad Al-Qurishi;Sherif Omar;Mohamed A. Mostafa,"Veracity of data posted on the microblog platforms has in recent years been a subject of intensive study by professionals specializing in various fields of informatics as well as sociology, particularly in the light of increasing importance of online tools for news spreading. On Twitter and similar sites, it is possible to report on ongoing situations globally with minimal delay, while the cost of such reporting remains negligible. One of the most important features of this social network is that content delivery can be customized to allow users to focus only on news items covering subject matters they find interesting. With this in mind, it becomes necessary to create verification mechanisms that can ascertain whether the claims made on Twitter can be taken seriously and prevent false content from spreading too far. This study demonstrates an innovative System for verification of information that can fulfill the role described above. The System is comprised of four mutually connected modules: a legacy module, a trustworthiness classifier; a module managing user authority, and a ranking procedure. All of the modules function within an integrated framework and jointly contribute to an accurate classification of messages and authors. Effectiveness of the solution was evaluated empirically on a sample of Twitter users, with a strict 10-fold evaluation procedure applied for each module. The findings indicate that the solution successfully meets the primary objectives of the study and performs its function as expected. △ Less","19 January, 2021",https://arxiv.org/pdf/2101.07725
COTORRA: COntext-aware Testbed fOR Robotic Applications,Milan Groshev;Jorge Martín-Pérez;Kiril Antevski;Antonio de la Oliva;Carlos J. Bernardos,"Edge & Fog computing have received considerable attention as promising candidates for the evolution of robotic systems. In this letter, we propose COTORRA, an Edge & Fog driven robotic testbed that combines context information with robot sensor data to validate innovative concepts for robotic systems prior to being applied in a production environment. In lab/university, we established COTORRA as an easy applicable and modular testbed on top of heterogeneous network infrastructure. COTORRA is open for pluggable robotic applications. To verify its feasibility and assess its performance, we ran set of experiments that show how autonomous navigation applications can achieve target latencies bellow 15ms or perform an inter-domain (DLT) federation within 19 seconds. △ Less","19 January, 2021",https://arxiv.org/pdf/2101.07676
COVID-19 and Digital Transformation -- Developing an Open Experimental Testbed for Sustainable and Innovative Environments (ETSIE) using Fuzzy Cognitive Maps,Wolfgang Höhl,"This paper sketches a new approach using Fuzzy Cognitive Maps (FCMs) to operably map and simulate digital transformation in architecture and urban planning. Today these processes are poorly understood. Many current studies on digital transformation are only treating questions of economic efficiency. Sustainability and social impact only play a minor role. Decisive definitions, concepts and terms stay unclear. Therefore this paper develops an open experimental testbed for sustainable and innovative environments (ETSIE) for three different digital transformation scenarios using FCMs. A traditional growth-oriented scenario, a COVID-19 scenario and an innovative and sustainable COVID-19 scenario are modeled and tested. All three scenarios have the same number of components, connections and the same driver components. Only the initial state vectors are different and the internal correlations are weighted differently. This allows for comparing all three scenarios on an equal basis. The mental modeler software is used (Gray et al. 2013). This paper presents one of the first applications of FCMs in the context of digital transformation. It is shown, that the traditional growth-oriented scenario is structurally very similar to the current COVID-19 scenario. The current pandemic is able to accelerate digital transformation to a certain extent. But the pandemic does not guarantee for a distinct sustainable and innovative future development. Only by changing the initial state vectors and the weights of the connections an innovative and sustainable turnaround in a third scenario becomes possible. △ Less","20 January, 2021",https://arxiv.org/pdf/2101.07509
Cross-Layer Network Codes for Content Delivery in Cache-Enabled D2D Networks,Mohammed S. Al-Abiad;Md. Zoheb Hassan;Md. Jahangir Hossain,"In this paper, we consider the use of cross-layer network coding (CLNC), caching, and device-to-device (D2D) communications to jointly optimize the delivery of a set of popular contents to a set of user devices (UDs). In the considered D2D network, a group of near-by UDs cooperate with each other and use NC to combine their cached files, so as the completion time required for delivering all requested contents to all UDs is minimized. Unlike the previous work that considers only one transmitting UD at a time, our work allows multiple UDs to transmit simultaneously given the interference among the active links is small. Such configuration brings a new trade-off among scheduling UDs to transmitting UDs, selecting the coding decisions and the transmission rate/power. Therefore, we consider the completion time minimization problem that involves scheduling multiple transmitting UDs, determining their transmission rates/powers and file combinations. The problem is shown to be intractable because it involves all future coding decisions. To tackle the problem at each transmission slot, we first design a graph called herein the D2D Rate-Aware IDNC graph where its vertices have weights that judiciously balance between the rates/powers of the transmitting UDs and the number of their scheduled UDs. Then, we propose an innovative and efficient CLNC solution that iteratively selects a set of transmitting UDs only if the interference caused by the transmissions of the newly selected UDs does not significantly impact the overall completion time. Simulation results show that the proposed solution offers significant completion time reduction compared with the existing algorithms. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.07291
Leveraging AI to optimize website structure discovery during Penetration Testing,Diego Antonelli;Roberta Cascella;Gaetano Perrone;Simon Pietro Romano;Antonio Schiano,"Dirbusting is a technique used to brute force directories and file names on web servers while monitoring HTTP responses, in order to enumerate server contents. Such a technique uses lists of common words to discover the hidden structure of the target website. Dirbusting typically relies on response codes as discovery conditions to find new pages. It is widely used in web application penetration testing, an activity that allows companies to detect websites vulnerabilities. Dirbusting techniques are both time and resource consuming and innovative approaches have never been explored in this field. We hence propose an advanced technique to optimize the dirbusting process by leveraging Artificial Intelligence. More specifically, we use semantic clustering techniques in order to organize wordlist items in different groups according to their semantic meaning. The created clusters are used in an ad-hoc implemented next-word intelligent strategy. This paper demonstrates that the usage of clustering techniques outperforms the commonly used brute force methods. Performance is evaluated by testing eight different web applications. Results show a performance increase that is up to 50% for each of the conducted experiments. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.07223
"Impact of COVID-19 on IoT Adoption in Healthcare, Smart Homes, Smart Buildings, Smart Cities, Transportation and Industrial IoT",Muhammad Umair;Muhammad Aamir Cheema;Omer Cheema;Huan Li;Hua Lu,"COVID-19 has disrupted normal life and has enforced a substantial change in the policies, priorities and activities of individuals, organisations and governments. These changes are proving to be a catalyst for technology and innovation. In this paper, we discuss the pandemic's potential impact on the adoption of the Internet of Things (IoT) in various broad sectors namely healthcare, smart homes, smart buildings, smart cities, transportation and industrial IoT. Our perspective and forecast of this impact on IoT adoption is based on a thorough research literature review, a careful examination of reports from leading consulting firms and interactions with several industry experts. For each of these sectors, we also provide the details of notable IoT initiatives taken in wake of COVID-19. We also highlight the challenges that need to be addressed and important research directions that will facilitate accelerated IoT adoption. △ Less","8 June, 2021",https://arxiv.org/pdf/2101.07196
CaEGCN: Cross-Attention Fusion based Enhanced Graph Convolutional Network for Clustering,Guangyu Huo;Yong Zhang;Junbin Gao;Boyue Wang;Yongli Hu;Baocai Yin,"With the powerful learning ability of deep convolutional networks, deep clustering methods can extract the most discriminative information from individual data and produce more satisfactory clustering results. However, existing deep clustering methods usually ignore the relationship between the data. Fortunately, the graph convolutional network can handle such relationship, opening up a new research direction for deep clustering. In this paper, we propose a cross-attention based deep clustering framework, named Cross-Attention Fusion based Enhanced Graph Convolutional Network (CaEGCN), which contains four main modules: the cross-attention fusion module which innovatively concatenates the Content Auto-encoder module (CAE) relating to the individual data and Graph Convolutional Auto-encoder module (GAE) relating to the relationship between the data in a layer-by-layer manner, and the self-supervised model that highlights the discriminative information for clustering tasks. While the cross-attention fusion module fuses two kinds of heterogeneous representation, the CAE module supplements the content information for the GAE module, which avoids the over-smoothing problem of GCN. In the GAE module, two novel loss functions are proposed that reconstruct the content and relationship between the data, respectively. Finally, the self-supervised module constrains the distributions of the middle layer representations of CAE and GAE to be consistent. Experimental results on different types of datasets prove the superiority and robustness of the proposed CaEGCN. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.06883
The BIVEE Project: an overview of methodology and tools,M. Missikoff;P. Assogna,"EU needs an effective exit strategy from the crisis, with a special attention to SMEs that represent the 99% of the enterprises active in the European production system. To this end, innovation appears to be a key factor to relaunch the EU industrial system. The BIVEE project proceeded for almost 4 years to develop a rich framework, i.e., a methodology and a cloud-based software environment, that includes business principles, models, and best practices, plus a number of advanced software services, to support and promote production improvement and business innovation in virtual enterprise environments (essentially, enterprise networks.) △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06736
Separable Batch Normalization for Robust Facial Landmark Localization with Cross-protocol Network Training,Shuangping Jin;Zhenhua Feng;Wankou Yang;Josef Kittler,"A big, diverse and balanced training data is the key to the success of deep neural network training. However, existing publicly available datasets used in facial landmark localization are usually much smaller than those for other computer vision tasks. A small dataset without diverse and balanced training samples cannot support the training of a deep network effectively. To address the above issues, this paper presents a novel Separable Batch Normalization (SepBN) module with a Cross-protocol Network Training (CNT) strategy for robust facial landmark localization. Different from the standard BN layer that uses all the training data to calculate a single set of parameters, SepBN considers that the samples of a training dataset may belong to different sub-domains. Accordingly, the proposed SepBN module uses multiple sets of parameters, each corresponding to a specific sub-domain. However, the selection of an appropriate branch in the inference stage remains a challenging task because the sub-domain of a test sample is unknown. To mitigate this difficulty, we propose a novel attention mechanism that assigns different weights to each branch for automatic selection in an effective style. As a further innovation, the proposed CNT strategy trains a network using multiple datasets having different facial landmark annotation systems, boosting the performance and enhancing the generalization capacity of the trained network. The experimental results obtained on several well-known datasets demonstrate the effectiveness of the proposed method. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06663
Trilevel Neural Architecture Search for Efficient Single Image Super-Resolution,Yan Wu;Zhiwu Huang;Suryansh Kumar;Rhea Sanjay Sukthanker;Radu Timofte;Luc Van Gool,"Modern solutions to the single image super-resolution (SISR) problem using deep neural networks aim not only at better performance accuracy but also at a lighter and computationally efficient model. To that end, recently, neural architecture search (NAS) approaches have shown some tremendous potential. Following the same underlying, in this paper, we suggest a novel trilevel NAS method that provides a better balance between different efficiency metrics and performance to solve SISR. Unlike available NAS, our search is more complete, and therefore it leads to an efficient, optimized, and compressed architecture. We innovatively introduce a trilevel search space modeling, i.e., hierarchical modeling on network-, cell-, and kernel-level structures. To make the search on trilevel spaces differentiable and efficient, we exploit a new sparsestmax technique that is excellent at generating sparse distributions of individual neural architecture candidates so that they can be better disentangled for the final selection from the enlarged search space. We further introduce the sorting technique to the sparsestmax relaxation for better network-level compression. The proposed NAS optimization additionally facilitates simultaneous search and training in a single phase, reducing search time and train time. Comprehensive evaluations on the benchmark datasets show our method's clear superiority over the state-of-the-art NAS in terms of a good trade-off between model size, performance, and efficiency. △ Less","23 April, 2021",https://arxiv.org/pdf/2101.06658
Coarse Temporal Attention Network (CTA-Net) for Driver's Activity Recognition,Zachary Wharton;Ardhendu Behera;Yonghuai Liu;Nik Bessis,"There is significant progress in recognizing traditional human activities from videos focusing on highly distinctive actions involving discriminative body movements, body-object and/or human-human interactions. Driver's activities are different since they are executed by the same subject with similar body parts movements, resulting in subtle changes. To address this, we propose a novel framework by exploiting the spatiotemporal attention to model the subtle changes. Our model is named Coarse Temporal Attention Network (CTA-Net), in which coarse temporal branches are introduced in a trainable glimpse network. The goal is to allow the glimpse to capture high-level temporal relationships, such as 'during', 'before' and 'after' by focusing on a specific part of a video. These branches also respect the topology of the temporal dynamics in the video, ensuring that different branches learn meaningful spatial and temporal changes. The model then uses an innovative attention mechanism to generate high-level action specific contextual information for activity recognition by exploring the hidden states of an LSTM. The attention mechanism helps in learning to decide the importance of each hidden state for the recognition task by weighing them when constructing the representation of the video. Our approach is evaluated on four publicly accessible datasets and significantly outperforms the state-of-the-art by a considerable margin with only RGB video as input. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06636
Task-driven Self-supervised Bi-channel Networks for Diagnosis of Breast Cancers with Mammography,Ronglin Gong;Jun Wang;Jun Shi,"Deep learning can promote the mammography-based computer-aided diagnosis (CAD) for breast cancers, but it generally suffers from the small sample size problem. Self-supervised learning (SSL) has shown its effectiveness in medical image analysis with limited training samples. However, the network model sometimes cannot be well pre-trained in the conventional SSL framework due to the limitation of the pretext task and fine-tuning mechanism. In this work, a Task-driven Self-supervised Bi-channel Networks (TSBN) framework is proposed to improve the performance of classification model the mammography-based CAD. In particular, a new gray-scale image mapping (GSIM) is designed as the pretext task, which embeds the class label information of mammograms into the image restoration task to improve discriminative feature representation. The proposed TSBN then innovatively integrates different network architecture, including the image restoration network and the classification network, into a unified SSL framework. It jointly trains the bi-channel network models and collaboratively transfers the knowledge from the pretext task network to the downstream task network with improved diagnostic accuracy. The proposed TSBN is evaluated on a public INbreast mammogram dataset. The experimental results indicate that it outperforms the conventional SSL and multi-task learning algorithms for diagnosis of breast cancers with limited samples. △ Less","30 August, 2021",https://arxiv.org/pdf/2101.06228
Needmining: Designing Digital Support to Elicit Needs from Social Media,Niklas Kühl;Gerhard Satzger,"Today's businesses face a high pressure to innovate in order to succeed in highly competitive markets. Successful innovations, though, typically require the identification and analysis of customer needs. While traditional, established need elicitation methods are time-proven and have demonstrated their capabilities to deliver valuable insights, they lack automation and scalability and, thus, are expensive and time-consuming. In this article, we propose an approach to automatically identify and quantify customer needs by utilizing a novel data source: Users voluntarily and publicly expose information about themselves via social media, as for instance Facebook or Twitter. These posts may contain valuable information about the needs, wants, and demands of their authors. We apply a Design Science Research (DSR) methodology to add design knowledge and artifacts for the digitalization of innovation processes, in particular to provide digital support for the elicitation of customer needs. We want to investigate whether automated, speedy, and scalable need elicitation from social media is feasible. We concentrate on Twitter as a data source and on e-mobility as an application domain. In a first design cycle we conceive, implement and evaluate a method to demonstrate the feasibility of identifying those social media posts that actually express customer needs. In a second cycle, we build on this artifact to additionally quantify the need information elicited, and prove its feasibility. Third, we integrate both developed methods into an end-user software artifact and test usability in an industrial use case. Thus, we add new methods for need elicitation to the body of knowledge, and introduce concrete tooling for innovation management in practice. △ Less","14 January, 2021",https://arxiv.org/pdf/2101.06146
Data Science for Engineers: A Teaching Ecosystem,Felipe Tobar;Felipe Bravo-Marquez;Jocelyn Dunstan;Joaquin Fontbona;Alejandro Maass;Daniel Remenik;Jorge F. Silva,"We describe an ecosystem for teaching data science (DS) to engineers which blends theory, methods, and applications, developed at the Faculty of Physical and Mathematical Sciences, Universidad de Chile, over the last three years. This initiative has been motivated by the increasing demand for DS qualifications both from academic and professional environments. The ecosystem is distributed in a collaborative fashion across three departments in the above Faculty and includes postgraduate programmes, courses, professional diplomas, data repositories, laboratories, trainee programmes, and internships. By sharing our teaching principles and the innovative components of our approach to teaching DS, we hope our experience can be useful to those developing their own DS programmes and ecosystems. The open challenges and future plans for our ecosystem are also discussed at the end of the article. △ Less","14 January, 2021",https://arxiv.org/pdf/2101.06119
"Big Data Generated by Connected and Automated Vehicles for Safety Monitoring, Assessment and Improvement, Final Report (Year 3)",Asad J. Khattak;Iman Mahdinia;Sevin Mohammadi;Amin Mohammadnazar;Behram Wali,"This report focuses on safety aspects of connected and automated vehicles (CAVs). The fundamental question to be answered is how can CAVs improve road users' safety? Using advanced data mining and thematic text analytics tools, the goal is to systematically synthesize studies related to Big Data for safety monitoring and improvement. Within this domain, the report systematically compares Big Data initiatives related to transportation initiatives nationally and internationally and provides insights regarding the evolution of Big Data science applications related to CAVs and new challenges. The objectives addressed are: 1-Creating a database of Big Data efforts by acquiring reports, white papers, and journal publications; 2-Applying text analytics tools to extract key concepts, and spot patterns and trends in Big Data initiatives; 3-Understanding the evolution of CAV Big Data in the context of safety by quantifying granular taxonomies and modeling entity relations among contents in CAV Big Data research initiatives, and 4-Developing a foundation for exploring new approaches to tracking and analyzing CAV Big Data and related innovations. The study synthesizes and derives high-quality information from innovative research activities undertaken by various research entities through Big Data initiatives. The results can provide a conceptual foundation for developing new approaches for guiding and tracking the safety implications of Big Data and related innovations. △ Less","9 January, 2021",https://arxiv.org/pdf/2101.06106
Finding faults: A scoping study of fault diagnostics for Industrial Cyber-Physical Systems,Barry Dowdeswell;Roopak Sinha;Stephen G. MacDonell,"Context: As Industrial Cyber-Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems.(Abridged) △ Less","13 January, 2021",https://arxiv.org/pdf/2101.05451
Understanding Diffusion of Recurrent Innovations,Fuqi Lin,"The diffusion of innovations theory has been studied for years. Previous research efforts mainly focus on key elements, adopter categories, and the process of innovation diffusion. However, most of them only consider single innovations. With the development of modern technology, recurrent innovations gradually come into vogue. In order to reveal the characteristics of recurrent innovations, we present the first large-scale analysis of the adoption of recurrent innovations in the context of mobile app updates. Our analysis reveals the adoption behavior and new adopter categories of recurrent innovations as well as the features that have impact on the process of adoption. △ Less","13 January, 2021",https://arxiv.org/pdf/2101.05094
Effective Low-Cost Time-Domain Audio Separation Using Globally Attentive Locally Recurrent Networks,Max W. Y. Lam;Jun Wang;Dan Su;Dong Yu,"Recent research on the time-domain audio separation networks (TasNets) has brought great success to speech separation. Nevertheless, conventional TasNets struggle to satisfy the memory and latency constraints in industrial applications. In this regard, we design a low-cost high-performance architecture, namely, globally attentive locally recurrent (GALR) network. Alike the dual-path RNN (DPRNN), we first split a feature sequence into 2D segments and then process the sequence along both the intra- and inter-segment dimensions. Our main innovation lies in that, on top of features recurrently processed along the inter-segment dimensions, GALR applies a self-attention mechanism to the sequence along the inter-segment dimension, which aggregates context-aware information and also enables parallelization. Our experiments suggest that GALR is a notably more effective network than the prior work. On one hand, with only 1.5M parameters, it has achieved comparable separation performance at a much lower cost with 36.1% less runtime memory and 49.4% fewer computational operations, relative to the DPRNN. On the other hand, in a comparable model size with DPRNN, GALR has consistently outperformed DPRNN in three datasets, in particular, with a substantial margin of 2.4dB absolute improvement of SI-SNRi in the benchmark WSJ0-2mix task. △ Less","13 January, 2021",https://arxiv.org/pdf/2101.05014
GPS Spoofing Mitigation and Timing Risk Analysis in Networked PMUs via Stochastic Reachability,Sriramya Bhamidipati;Grace Xingxin Gao,"To address PMU vulnerability against spoofing, we propose a set-valued state estimation technique known as Stochastic Reachability-based Distributed Kalman Filter (SR-DKF) that computes secure GPS timing across a network of receivers. Utilizing stochastic reachability, we estimate not only GPS time but also its stochastic reachable set, which is parameterized via probabilistic zonotope (p-Zonotope). While requiring known measurement error bounds in only non-spoofed conditions, we design a two-tier approach: We first perform measurement-level spoofing mitigation via deviation of measurement innovation from its expected p-Zonotope and second perform state-level timing risk analysis via intersection probability of estimated pZonotope with an unsafe set that violates IEEE C37.118.1a-2014 standards. We validate the proposed SR-DKF by subjecting a simulated receiver network to coordinated signal-level spoofing. We demonstrate improved GPS timing accuracy and successful spoofing mitigation via our SR-DKF. We validate the robustness of the estimated timing risk as the number of receivers is varied. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.04835
Challenges and approaches to time-series forecasting in data center telemetry: A Survey,Shruti Jadon;Jan Kanty Milczek;Ajit Patankar,"Time-series forecasting has been an important research domain for so many years. Its applications include ECG predictions, sales forecasting, weather conditions, even COVID-19 spread predictions. These applications have motivated many researchers to figure out an optimal forecasting approach, but the modeling approach also changes as the application domain changes. This work has focused on reviewing different forecasting approaches for telemetry data predictions collected at data centers. Forecasting of telemetry data is a critical feature of network and data center management products. However, there are multiple options of forecasting approaches that range from a simple linear statistical model to high capacity deep learning architectures. In this paper, we attempted to summarize and evaluate the performance of well known time series forecasting techniques. We hope that this evaluation provides a comprehensive summary to innovate in forecasting approaches for telemetry data. △ Less","11 February, 2021",https://arxiv.org/pdf/2101.04224
Cybersecurity of Industrial Cyber-Physical Systems: A Review,Hakan Kayan;Matthew Nunes;Omer Rana;Pete Burnap;Charith Perera,"Industrial cyber-physical systems (ICPSs) manage critical infrastructures by controlling the processes based on the ""physics"" data gathered by edge sensor networks. Recent innovations in ubiquitous computing and communication technologies have prompted the rapid integration of highly interconnected systems to ICPSs. Hence, the ""security by obscurity"" principle provided by air-gapping is no longer followed. As the interconnectivity in ICPSs increases, so does the attack surface. Industrial vulnerability assessment reports have shown that a variety of new vulnerabilities have occurred due to this transition while the most common ones are related to weak boundary protection. Although there are existing surveys in this context, very little is mentioned regarding these reports. This paper bridges this gap by defining and reviewing ICPSs from a cybersecurity perspective. In particular, multi-dimensional adaptive attack taxonomy is presented and utilized for evaluating real-life ICPS cyber incidents. We also identify the general shortcomings and highlight the points that cause a gap in existing literature while defining future research directions. △ Less","10 January, 2021",https://arxiv.org/pdf/2101.03564
Artificial Intelligence enabled Smart Learning,Faisal Khan;Debdeep Bose,"Artificial Intelligence (AI) is a discipline of computer science that deals with machine intelligence. It is essential to bring AI into the context of learning because it helps in analysing the enormous amounts of data that is collected from individual students, teachers and academic staff. The major priorities of implementing AI in education are making innovative use of existing digital technologies for learning, and teaching practices that significantly improve traditional educational methods. The main problem with traditional learning is that it cannot be suited to every student in class. Some students may grasp the concepts well, while some may have difficulties in understanding them and some may be more auditory or visual learners. The World Bank report on education has indicated that the learning gap created by this problem causes many students to drop out (World Development Report, 2018). Personalised learning has been able to solve this grave problem. △ Less","8 January, 2021",https://arxiv.org/pdf/2101.02991
When does the Physarum Solver Distinguish the Shortest Path from other Paths: the Transition Point and its Applications,Yusheng Huang;Dong Chu;Joel Weijia Lai;Yong Deng;Kang Hao Cheong,"Physarum solver, also called the physarum polycephalum inspired algorithm (PPA), is a newly developed bio-inspired algorithm that has an inherent ability to find the shortest path in a given graph. Recent research has proposed methods to develop this algorithm further by accelerating the original PPA (OPPA)'s path-finding process. However, when does the PPA ascertain that the shortest path has been found? Is there a point after which the PPA could distinguish the shortest path from other paths? By innovatively proposing the concept of the dominant path (D-Path), the exact moment, named the transition point (T-Point), when the PPA finds the shortest path can be identified. Based on the D-Path and T-Point, a newly accelerated PPA named OPPA-D using the proposed termination criterion is developed which is superior to all other baseline algorithms according to the experiments conducted in this paper. The validity and the superiority of the proposed termination criterion is also demonstrated. Furthermore, an evaluation method is proposed to provide new insights for the comparison of different accelerated OPPAs. The breakthrough of this paper lies in using D-path and T-point to terminate the OPPA. The novel termination criterion reveals the actual performance of this OPPA. This OPPA is the fastest algorithm, outperforming some so-called accelerated OPPAs. Furthermore, we explain why some existing works inappropriately claim to be accelerated algorithms is in fact a product of inappropriate termination criterion, thus giving rise to the illusion that the method is accelerated. △ Less","8 January, 2021",https://arxiv.org/pdf/2101.02913
Active Screening for Recurrent Diseases: A Reinforcement Learning Approach,Han-Ching Ou;Haipeng Chen;Shahin Jabbari;Milind Tambe,"Active screening is a common approach in controlling the spread of recurring infectious diseases such as tuberculosis and influenza. In this approach, health workers periodically select a subset of population for screening. However, given the limited number of health workers, only a small subset of the population can be visited in any given time period. Given the recurrent nature of the disease and rapid spreading, the goal is to minimize the number of infections over a long time horizon. Active screening can be formalized as a sequential combinatorial optimization over the network of people and their connections. The main computational challenges in this formalization arise from i) the combinatorial nature of the problem, ii) the need of sequential planning and iii) the uncertainties in the infectiousness states of the population. Previous works on active screening fail to scale to large time horizon while fully considering the future effect of current interventions. In this paper, we propose a novel reinforcement learning (RL) approach based on Deep Q-Networks (DQN), with several innovative adaptations that are designed to address the above challenges. First, we use graph convolutional networks (GCNs) to represent the Q-function that exploit the node correlations of the underlying contact network. Second, to avoid solving a combinatorial optimization problem in each time period, we decompose the node set selection as a sub-sequence of decisions, and further design a two-level RL framework that solves the problem in a hierarchical way. Finally, to speed-up the slow convergence of RL which arises from reward sparseness, we incorporate ideas from curriculum learning into our hierarchical RL approach. We evaluate our RL algorithm on several real-world networks. △ Less","19 April, 2021",https://arxiv.org/pdf/2101.02766
Does Crowdfunding Really Foster Innovation? Evidence from the Board Game Industry,Johannes Wachs;Balazs Vedres,"Crowdfunding offers inventors and entrepreneurs alternative access to resources with which they can develop and realize their ideas. Besides helping to secure capital, crowdfunding also connects creators with engaged early supporters who provide public feedback. But does this process foster truly innovative outcomes? Does the proliferation of crowdfunding in an industry make it more innovative overall? Prior studies investigating the link between crowdfunding and innovation do not compare traditional and crowdfunded products and so while claims that crowdfunding supports innovation are theoretically sound, they lack empirical backing. We address this gap using a unique dataset of board games, an industry with significant crowdfunding activity in recent years. Each game is described by how it combines fundamental mechanisms such as dice-rolling, negotiation, and resource-management, from which we develop quantitative measures of innovation in game design. Using these measures to compare games, we find that crowdfunded games tend to be more distinctive from previous games than their traditionally published counterparts. They are also significantly more likely to implement novel combinations of mechanisms. Crowdfunded games are not just transient experiments: subsequent games imitate their novel ideas. These results hold in regression models controlling for game and designer-level confounders. Our findings demonstrate that the innovative potential of crowdfunding goes beyond individual products to entire industries, as new ideas spill over to traditionally funded products. △ Less","10 May, 2021",https://arxiv.org/pdf/2101.02683
RANK: AI-assisted End-to-End Architecture for Detecting Persistent Attacks in Enterprise Networks,Hazem M. Soliman;Geoff Salmon;Dušan Sovilj;Mohan Rao,"Advanced Persistent Threats (APTs) are sophisticated multi-step attacks, planned and executed by skilled adversaries targeting modern government and enterprise networks. Intrusion Detection Systems (IDSs) and User and Entity Behavior Analytics (UEBA) are commonly employed to aid a security analyst in the detection of APTs. The prolonged nature of APTs, combined with the granular focus of UEBA and IDS, results in overwhelming the analyst with an increasingly impractical number of alerts. Consequent to this abundance of data, and together with the crucial importance of the problem as well as the high cost of the skilled personnel involved, the problem of APT detection becomes a perfect candidate for automation through Artificial Intelligence (AI). In this paper, we provide, up to our knowledge, the first study and implementation of an end-to-end AI-assisted architecture for detecting APTs -- RANK. The goal of the system is not to replace the analyst, rather, it is to automate the complete pipeline from data sources to a final set of incidents for analyst review. The architecture is composed of four consecutive steps: 1) alert templating and merging, 2) alert graph construction, 3) alert graph partitioning into incidents, and 4) incident scoring and ordering. We evaluate our architecture against the 2000 DARPA Intrusion Detection dataset, as well as a read-world private dataset from a medium-scale enterprise. Extensive results are provided showing a three order of magnitude reduction in the amount of data to be reviewed by the analyst, innovative extraction of incidents and security-wise scoring of extracted incidents. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02573
VHS to HDTV Video Translation using Multi-task Adversarial Learning,Hongming Luo;Guangsen Liao;Xianxu Hou;Bozhi Liu;Fei Zhou;Guoping Qiu,"There are large amount of valuable video archives in Video Home System (VHS) format. However, due to the analog nature, their quality is often poor. Compared to High-definition television (HDTV), VHS video not only has a dull color appearance but also has a lower resolution and often appears blurry. In this paper, we focus on the problem of translating VHS video to HDTV video and have developed a solution based on a novel unsupervised multi-task adversarial learning model. Inspired by the success of generative adversarial network (GAN) and CycleGAN, we employ cycle consistency loss, adversarial loss and perceptual loss together to learn a translation model. An important innovation of our work is the incorporation of super-resolution model and color transfer model that can solve unsupervised multi-task problem. To our knowledge, this is the first work that dedicated to the study of the relation between VHS and HDTV and the first computational solution to translate VHS to HDTV. We present experimental results to demonstrate the effectiveness of our solution qualitatively and quantitatively. △ Less","7 January, 2021",https://arxiv.org/pdf/2101.02384
COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information Fusion Framework for Stock Price Movement Prediction,Farnoush Ronaghi;Mohammad Salimibeni;Farnoosh Naderkhani;Arash Mohammadi,"The novel of coronavirus (COVID-19) has suddenly and abruptly changed the world as we knew at the start of the 3rd decade of the 21st century. Particularly, COVID-19 pandemic has negatively affected financial econometrics and stock markets across the globe. Artificial Intelligence (AI) and Machine Learning (ML)-based prediction models, especially Deep Neural Network (DNN) architectures, have the potential to act as a key enabling factor to reduce the adverse effects of the COVID-19 pandemic and future possible ones on financial markets. In this regard, first, a unique COVID-19 related PRIce MOvement prediction (COVID19 PRIMO) dataset is introduced in this paper, which incorporates effects of social media trends related to COVID-19 on stock market price movements. Afterwards, a novel hybrid and parallel DNN-based framework is proposed that integrates different and diversified learning architectures. Referred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework for Stock price Movement Prediction (COVID19-HPSMP), innovative fusion strategies are used to combine scattered social media news related to COVID-19 with historical mark data. The proposed COVID19-HPSMP consists of two parallel paths (hence hybrid), one based on Convolutional Neural Network (CNN) with Local/Global Attention modules, and one integrated CNN and Bi-directional Long Short term Memory (BLSTM) path. The two parallel paths are followed by a multilayer fusion layer acting as a fusion centre that combines localized features. Performance evaluations are performed based on the introduced COVID19 PRIMO dataset illustrating superior performance of the proposed framework. △ Less","8 July, 2021",https://arxiv.org/pdf/2101.02287
Fast Parallel Newton-Raphson Power Flow Solver for Large Number of System Calculations with CPU and GPU,Zhenqi Wang;Sebastian Wende-von Berg;Martin Braun,"To analyze large sets of grid states, e.g. when evaluating the impact from the uncertainties of the renewable generation with probabilistic Monte Carlo simulation or in stationary time series simulation, large number of power flow calculations have to be performed. For the application in real-time grid operation, grid planning and in further cases when computational time is critical, a novel approach on simultaneous parallelization of many Newton-Raphson power flow calculations on CPU and with GPU-acceleration is proposed. The result shows a speed-up of over x100 comparing to the open-source tool pandapower, when performing repetitive power flows of system with admittance matrix of the same sparsity pattern on both CPU and GPU. The speed-up relies on the algorithm improvement and highly optimized parallelization strategy, which can reduce the repetitive work and saturate the high hardware computational capability of modern CPUs and GPUs well. This is achieved with the proposed batched sparse matrix operation and batched linear solver based on LU-refactorization. The batched linear solver shows a large performance improvement comparing to the state-of-the-art linear system solver KLU library and a better saturation of the GPU performance with small problem scale. Finally, the method of integrating the proposed solver into pandapower is presented, thus the parallel power flow solver with outstanding performance can be easily applied in challenging real-life grid operation and innovative researches e.g. data-driven machine learning studies. △ Less","28 April, 2021",https://arxiv.org/pdf/2101.02270
Playing with Food: Learning Food Item Representations through Interactive Exploration,Amrita Sawhney;Steven Lee;Kevin Zhang;Manuela Veloso;Oliver Kroemer,"A key challenge in robotic food manipulation is modeling the material properties of diverse and deformable food items. We propose using a multimodal sensory approach to interact and play with food that facilitates the ability to distinguish these properties across food items. First, we use a robotic arm and an array of sensors, which are synchronized using ROS, to collect a diverse dataset consisting of 21 unique food items with varying slices and properties. Afterwards, we learn visual embedding networks that utilize a combination of proprioceptive, audio, and visual data to encode similarities among food items using a triplet loss formulation. Our evaluations show that embeddings learned through interactions can successfully increase performance in a wide range of material and shape classification tasks. We envision that these learned embeddings can be utilized as a basis for planning and selecting optimal parameters for more material-aware robotic food manipulation skills. Furthermore, we hope to stimulate further innovations in the field of food robotics by sharing this food playing dataset with the research community. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02252
Interspeech 2021 Deep Noise Suppression Challenge,Chandan K A Reddy;Harishchandra Dubey;Kazuhito Koishida;Arun Nair;Vishak Gopal;Ross Cutler;Sebastian Braun;Hannes Gamper;Robert Aichner;Sriram Srinivasan,"The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality. We recently organized a DNS challenge special session at INTERSPEECH and ICASSP 2020. We open-sourced training and test datasets for the wideband scenario. We also open-sourced a subjective evaluation framework based on ITU-T standard P.808, which was also used to evaluate participants of the challenge. Many researchers from academia and industry made significant contributions to push the field forward, yet even the best noise suppressor was far from achieving superior speech quality in challenging scenarios. In this version of the challenge organized at INTERSPEECH 2021, we are expanding both our training and test datasets to accommodate full band scenarios. The two tracks in this challenge will focus on real-time denoising for (i) wide band, and(ii) full band scenarios. We are also making available a reliable non-intrusive objective speech quality metric called DNSMOS for the participants to use during their development phase. △ Less","4 April, 2021",https://arxiv.org/pdf/2101.01902
Connecting The Dots To Combat Collective Fraud,Mingxi Wu;Xi Chen,"Modern fraudsters write malicious programs to coordinate a group of accounts to commit collective fraud for illegal profits in online platforms. These programs have access to a set of finite resources - a set of IPs, devices, and accounts etc. and sometime manipulate fake accounts to collaboratively attack the target system. Inspired by these observations, we share our experience in building two real-time risk control systems to detect collective fraud. We show that with TigerGraph, a powerful graph database, and its innovative query language - GSQL, data scientists and fraud experts can conveniently implement and deploy an end-to-end risk control system as a graph database application. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.01898
Taxonomy Completion via Triplet Matching Network,Jieyu Zhang;Xiangchen Song;Ying Zeng;Jiaze Chen;Jiaming Shen;Yuning Mao;Lei Li,"Automatically constructing taxonomy finds many applications in e-commerce and web search. One critical challenge is as data and business scope grow in real applications, new concepts are emerging and needed to be added to the existing taxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an appropriate hypernym concept from the taxonomy for a new query concept. In this paper, we formulate a new task, ""taxonomy completion"", by discovering both the hypernym and hyponym concepts for a query. We propose Triplet Matching Network (TMN), to find the appropriate <hypernym, hyponym> pairs for a given query concept. TMN consists of one primal scorer and multiple auxiliary scorers. These auxiliary scorers capture various fine-grained signals (e.g., query to hypernym or query to hyponym semantics), and the primal scorer makes a holistic prediction on <query, hypernym, hyponym> triplet based on the internal feature representations of all auxiliary scorers. Also, an innovative channel-wise gating mechanism that retains task-specific information in concept representations is introduced to further boost model performance. Experiments on four real-world large-scale datasets show that TMN achieves the best performance on both taxonomy completion task and the previous taxonomy expansion task, outperforming existing methods. △ Less","4 March, 2021",https://arxiv.org/pdf/2101.01896
Perceptions of Smartphone Users Acceptance and Adoption of Mobile Commerce (MC) The Case of Jordan,Ahmad Nabot;Firas Omar;Mohammed Almousa,"This study investigates smartphone users perceptions of adopting and accepting Mobile Commerce (MC) based on users perceived adoption under the extended Technology Acceptance Model (TAM2) and Innovation Diffusion Theory (IDT) by providing research constructs for the domain of MC. Also, testing them with reliability and validity and demonstrating their distinctiveness with hypothesis testing. The results show that consumer intention to adopt MC on a smartphone was primarily influenced by Uncertainty Avoidance (UA), User Experience (UX), Perceived Ease Of Use (PEOU), Perceived Usefulness (PU) and Compatibility (CMP) as well as other constructs that positively determine attitude toward using a smartphone. For researchers, this study shows the benefits of adapting TAM constructs into MC acceptance on a smartphone. The perceptions of MC adoption on a smartphone in this study investigated based on a survey of specific people. For more reliability, a comprehensive study is needed to show the attitudes of people from different environments. △ Less","5 January, 2021",https://arxiv.org/pdf/2101.01401
SmartDeal: Re-Modeling Deep Network Weights for Efficient Inference and Training,Xiaohan Chen;Yang Zhao;Yue Wang;Pengfei Xu;Haoran You;Chaojian Li;Yonggan Fu;Yingyan Lin;Zhangyang Wang,"The record-breaking performance of deep neural networks (DNNs) comes with heavy parameterization, leading to external dynamic random-access memory (DRAM) for storage. The prohibitive energy of DRAM accesses makes it non-trivial to deploy DNN on resource-constrained devices, calling for minimizing the weight and data movements to improve the energy efficiency. We present SmartDeal (SD), an algorithm framework to trade higher-cost memory storage/access for lower-cost computation, in order to aggressively boost the storage and energy efficiency, for both inference and training. The core of SD is a novel weight decomposition with structural constraints, carefully crafted to unleash the hardware efficiency potential. Specifically, we decompose each weight tensor as the product of a small basis matrix and a large structurally sparse coefficient matrix whose non-zeros are quantized to power-of-2. The resulting sparse and quantized DNNs enjoy greatly reduced energy for data movement and weight storage, incurring minimal overhead to recover the original weights thanks to the sparse bit-operations and cost-favorable computations. Beyond inference, we take another leap to embrace energy-efficient training, introducing innovative techniques to address the unique roadblocks arising in training while preserving the SD structures. We also design a dedicated hardware accelerator to fully utilize the SD structure to improve the real energy efficiency and latency. We conduct experiments on both multiple tasks, models and datasets in different settings. Results show that: 1) applied to inference, SD achieves up to 2.44x energy efficiency as evaluated via real hardware implementations; 2) applied to training, SD leads to 10.56x and 4.48x reduction in the storage and training energy, with negligible accuracy loss compared to state-of-the-art training baselines. Our source codes are available online. △ Less","21 December, 2021",https://arxiv.org/pdf/2101.01163
"To Share, or not to Share Online Event Trend Aggregation Over Bursty Event Streams",Olga Poppe;Chuan Lei;Lei Ma;Allison Rozet;Elke A. Rundensteiner,"Complex event processing (CEP) systems continuously evaluate large workloads of pattern queries under tight time constraints. Event trend aggregation queries with Kleene patterns are commonly used to retrieve summarized insights about the recent trends in event streams. State-of-art methods are limited either due to repetitive computations or unnecessary trend construction. Existing shared approaches are guided by statically selected and hence rigid sharing plans that are often sub-optimal under stream fluctuations. In this work, we propose a novel framework Hamlet that is the first to overcome these limitations. Hamlet introduces two key innovations. First, Hamlet adaptively decides whether to share or not to share computations depending on the current stream properties at run time to harvest the maximum sharing benefit. Second, Hamlet is equipped with a highly efficient shared trend aggregation strategy that avoids trend construction. Our experimental study on both real and synthetic data sets demonstrates that Hamlet consistently reduces query latency by up to five orders of magnitude compared to the state-of-the-art approaches. △ Less","3 March, 2021",https://arxiv.org/pdf/2101.00361
Robotic Grasping of Fully-Occluded Objects using RF Perception,Tara Boroushaki;Junshan Leng;Ian Clester;Alberto Rodriguez;Fadel Adib,"We present the design, implementation, and evaluation of RF-Grasp, a robotic system that can grasp fully-occluded objects in unknown and unstructured environments. Unlike prior systems that are constrained by the line-of-sight perception of vision and infrared sensors, RF-Grasp employs RF (Radio Frequency) perception to identify and locate target objects through occlusions, and perform efficient exploration and complex manipulation tasks in non-line-of-sight settings. RF-Grasp relies on an eye-in-hand camera and batteryless RFID tags attached to objects of interest. It introduces two main innovations: (1) an RF-visual servoing controller that uses the RFID's location to selectively explore the environment and plan an efficient trajectory toward an occluded target, and (2) an RF-visual deep reinforcement learning network that can learn and execute efficient, complex policies for decluttering and grasping. We implemented and evaluated an end-to-end physical prototype of RF-Grasp. We demonstrate it improves success rate and efficiency by up to 40-50% over a state-of-the-art baseline. We also demonstrate RF-Grasp in novel tasks such mechanical search of fully-occluded objects behind obstacles, opening up new possibilities for robotic manipulation. Qualitative results (videos) available at rfgrasp.media.mit.edu △ Less","2 May, 2021",https://arxiv.org/pdf/2012.15436
StudyU: a platform for designing and conducting innovative digital N-of-1 trials,Stefan Konigorski;Sarah Wernicke;Tamara Slosarek;Alexander M. Zenner;Nils Strelow;Ferenc D. Ruether;Florian Henschel;Manisha Manaswini;Fabian Pottbäcker;Jonathan A. Edelman;Babajide Owoyele;Matteo Danieletto;Eddye Golden;Micol Zweig;Girish Nadkarni;Erwin Böttinger,"N-of-1 trials are the gold standard study design to evaluate individual treatment effects and derive personalized treatment strategies. Digital tools have the potential to initiate a new era of N-of-1 trials in terms of scale and scope, but fully-functional platforms are not yet available. Here, we present the open source StudyU platform which includes the StudyU designer and StudyU app. With the StudyU designer, scientists are given a collaborative web application to digitally specify, publish, and conduct N-of-1 trials. The StudyU app is a smartphone application with innovative user-centric elements for participants to partake in the published trials and assess the effects of different interventions on their health. Thereby, the StudyU platform allows clinicians and researchers worldwide to easily design and conduct digital N-of-1 trials in a safe manner. We envision that StudyU can change the landscape of personalized treatments both for patients and healthy individuals, democratize and personalize evidence generation for self-optimization and medicine, and can be integrated in clinical practice. △ Less","12 July, 2021",https://arxiv.org/pdf/2012.14201
Federated Multi-Agent Actor-Critic Learning for Age Sensitive Mobile Edge Computing,Zheqi Zhu;Shuo Wan;Pingyi Fan;Khaled B. Letaief,"As an emerging technique, mobile edge computing (MEC) introduces a new processing scheme for various distributed communication-computing systems such as industrial Internet of Things (IoT), vehicular communication, smart city, etc. In this work, we mainly focus on the timeliness of the MEC systems where the freshness of the data and computation tasks is significant. Firstly, we formulate a kind of age-sensitive MEC models and define the average age of information (AoI) minimization problems of interests. Then, a novel policy based multi-agent deep reinforcement learning (RL) framework, called heterogeneous multi-agent actor critic (H-MAAC), is proposed as a paradigm for joint collaboration in the investigated MEC systems, where edge devices and center controller learn the interactive strategies through their own observations. To improves the system performance, we develop the corresponding online algorithm by introducing an edge federated learning mode into the multi-agent cooperation whose advantages on learning convergence can be guaranteed theoretically. To the best of our knowledge, it's the first joint MEC collaboration algorithm that combines the edge federated mode with the multi-agent actor-critic reinforcement learning. Furthermore, we evaluate the proposed approach and compare it with classical RL based methods. As a result, the proposed framework not only outperforms the baseline on average system age, but also promotes the stability of training process. Besides, the simulation results provide some innovative perspectives for the system design under the edge federated collaboration. △ Less","11 May, 2021",https://arxiv.org/pdf/2012.14137
Deep Semantic Dictionary Learning for Multi-label Image Classification,Fengtao Zhou;Sheng Huang;Yun Xing,"Compared with single-label image classification, multi-label image classification is more practical and challenging. Some recent studies attempted to leverage the semantic information of categories for improving multi-label image classification performance. However, these semantic-based methods only take semantic information as type of complements for visual representation without further exploitation. In this paper, we present an innovative path towards the solution of the multi-label image classification which considers it as a dictionary learning task. A novel end-to-end model named Deep Semantic Dictionary Learning (DSDL) is designed. In DSDL, an auto-encoder is applied to generate the semantic dictionary from class-level semantics and then such dictionary is utilized for representing the visual features extracted by Convolutional Neural Network (CNN) with label embeddings. The DSDL provides a simple but elegant way to exploit and reconcile the label, semantic and visual spaces simultaneously via conducting the dictionary learning among them. Moreover, inspired by iterative optimization of traditional dictionary learning, we further devise a novel training strategy named Alternately Parameters Update Strategy (APUS) for optimizing DSDL, which alternately optimizes the representation coefficients and the semantic dictionary in forward and backward propagation. Extensive experimental results on three popular benchmarks demonstrate that our method achieves promising performances in comparison with the state-of-the-arts. Our codes and models have been released at {https://github.com/ZFT-CQU/DSDL}. △ Less","2 April, 2021",https://arxiv.org/pdf/2012.12509
Improving Unsupervised Image Clustering With Robust Learning,Sungwon Park;Sungwon Han;Sundong Kim;Danu Kim;Sungkyu Park;Seunghoon Hong;Meeyoung Cha,"Unsupervised image clustering methods often introduce alternative objectives to indirectly train the model and are subject to faulty predictions and overconfident results. To overcome these challenges, the current research proposes an innovative model RUC that is inspired by robust learning. RUC's novelty is at utilizing pseudo-labels of existing image clustering models as a noisy dataset that may include misclassified samples. Its retraining process can revise misaligned knowledge and alleviate the overconfidence problem in predictions. The model's flexible structure makes it possible to be used as an add-on module to other clustering methods and helps them achieve better performance on multiple datasets. Extensive experiments show that the proposed model can adjust the model confidence with better calibration and gain additional robustness against adversarial noise. △ Less","29 March, 2021",https://arxiv.org/pdf/2012.11150
Recent Developments in Detection of Central Serous Retinopathy through Imaging and Artificial Intelligence Techniques A Review,Syed Ale Hassan;Shahzad Akbar;Amjad Rehman;Tanzila Saba;Hoshang Kolivand;Saeed Ali Bahaj,"Central Serous Retinopathy (CSR) or Central Serous Chorioretinopathy (CSC) is a significant disease that causes blindness and vision loss among millions of people worldwide. It transpires as a result of accumulation of watery fluids behind the retina. Therefore, detection of CSR at early stages allows preventive measures to avert any impairment to the human eye. Traditionally, several manual methods for detecting CSR have been developed in the past; however, they have shown to be imprecise and unreliable. Consequently, Artificial Intelligence (AI) services in the medical field, including automated CSR detection, are now possible to detect and cure this disease. This review assessed a variety of innovative technologies and researches that contribute to the automatic detection of CSR. In this review, various CSR disease detection techniques, broadly classified into two categories: a) CSR detection based on classical imaging technologies, and b) CSR detection based on Machine/Deep Learning methods, have been reviewed after an elaborated evaluation of 29 different relevant articles. Additionally, it also goes over the advantages, drawbacks and limitations of a variety of traditional imaging techniques, such as Optical Coherence Tomography Angiography (OCTA), Fundus Imaging and more recent approaches that utilize Artificial Intelligence techniques. Finally, it is concluded that the most recent Deep Learning (DL) classifiers deliver accurate, fast, and reliable CSR detection. However, more research needs to be conducted on publicly available datasets to improve computation complexity for the reliable detection and diagnosis of CSR disease. △ Less","25 August, 2021",https://arxiv.org/pdf/2012.10961
Scientific Prizes and the Extraordinary Growth of Scientific Topics,Ching Jin;Yifang Ma;Brian Uzzi,"Fast growing scientific topics have famously been key harbingers of the new frontiers of science, yet, large-scale analyses of their genesis and impact are rare. We investigate one possible factor connected with a topic's extraordinary growth: scientific prizes. Our longitudinal analysis of nearly all recognized prizes worldwide and over 11,000 scientific topics from 19 disciplines indicates that topics associated with a scientific prize experience extraordinary growth in productivity, impact, and new entrants. Relative to matched non-prizewinning topics, prizewinning topics produce 40% more papers and 33% more citations, retain 55% more scientists, and gain 37% and 47% more new entrants and star scientists, respectively, in the first five-to-ten years after the prize. Funding do not account for a prizewinning topic's growth. Rather, growth is positively related to the degree to which the prize is discipline-specific, conferred for recent research, or has prize money. These findings reveal new dynamics behind scientific innovation and investment. △ Less","17 August, 2021",https://arxiv.org/pdf/2012.09269
Hierarchical Graph Capsule Network,Jinyu Yang;Peilin Zhao;Yu Rong;Chaochao Yan;Chunyuan Li;Hehuan Ma;Junzhou Huang,"Graph Neural Networks (GNNs) draw their strength from explicitly modeling the topological information of structured data. However, existing GNNs suffer from limited capability in capturing the hierarchical graph representation which plays an important role in graph classification. In this paper, we innovatively propose hierarchical graph capsule network (HGCN) that can jointly learn node embeddings and extract graph hierarchies. Specifically, disentangled graph capsules are established by identifying heterogeneous factors underlying each node, such that their instantiation parameters represent different properties of the same entity. To learn the hierarchical representation, HGCN characterizes the part-whole relationship between lower-level capsules (part) and higher-level capsules (whole) by explicitly considering the structure information among the parts. Experimental studies demonstrate the effectiveness of HGCN and the contribution of each component. △ Less","27 March, 2021",https://arxiv.org/pdf/2012.08734
Confidential Machine Learning on Untrusted Platforms: A Survey,Sagar Sharma;Keke Chen,"With the ever-growing data and the need for developing powerful machine learning models, data owners increasingly depend on various untrusted platforms (e.g., public clouds, edges, and machine learning service providers) for scalable processing or collaborative learning. Thus, sensitive data and models are in danger of unauthorized access, misuse, and privacy compromises. A relatively new body of research confidentially trains machine learning models on protected data to address these concerns. In this survey, we summarize notable studies in this emerging area of research. With a unified framework, we highlight the critical challenges and innovations in outsourcing machine learning confidentially. We focus on the cryptographic approaches for confidential machine learning (CML), primarily on model training, while also covering other directions such as perturbation-based approaches and CML in the hardware-assisted computing environment. The discussion will take a holistic way to consider a rich context of the related threat models, security assumptions, design principles, and associated trade-offs amongst data utility, cost, and confidentiality. △ Less","12 June, 2021",https://arxiv.org/pdf/2012.08156
Phase Retrieval with Holography and Untrained Priors: Tackling the Challenges of Low-Photon Nanoscale Imaging,Hannah Lawrence;David A. Barmherzig;Henry Li;Michael Eickenberg;Marylou Gabrié,"Phase retrieval is the inverse problem of recovering a signal from magnitude-only Fourier measurements, and underlies numerous imaging modalities, such as Coherent Diffraction Imaging (CDI). A variant of this setup, known as holography, includes a reference object that is placed adjacent to the specimen of interest before measurements are collected. The resulting inverse problem, known as holographic phase retrieval, is well-known to have improved problem conditioning relative to the original. This innovation, i.e. Holographic CDI, becomes crucial at the nanoscale, where imaging specimens such as viruses, proteins, and crystals require low-photon measurements. This data is highly corrupted by Poisson shot noise, and often lacks low-frequency content as well. In this work, we introduce a dataset-free deep learning framework for holographic phase retrieval adapted to these challenges. The key ingredients of our approach are the explicit and flexible incorporation of the physical forward model into an automatic differentiation procedure, the Poisson log-likelihood objective function, and an optional untrained deep image prior. We perform extensive evaluation under realistic conditions. Compared to competing classical methods, our method recovers signal from higher noise levels and is more resilient to suboptimal reference design, as well as to large missing regions of low frequencies in the observations. Finally, we show that these properties carry over to experimental data acquired on optical wavelengths. To the best of our knowledge, this is the first work to consider a dataset-free machine learning approach for holographic phase retrieval. △ Less","20 April, 2021",https://arxiv.org/pdf/2012.07386
Unsupervised Neural Domain Adaptation for Document Image Binarization,Francisco J. Castellanos;Antonio-Javier Gallego;Jorge Calvo-Zaragoza,"Binarization is a well-known image processing task, whose objective is to separate the foreground of an image from the background. One of the many tasks for which it is useful is that of preprocessing document images in order to identify relevant information, such as text or symbols. The wide variety of document types, alphabets, and formats makes binarization challenging. There are multiple proposals with which to solve this problem, from classical manually-adjusted methods, to more recent approaches based on machine learning. The latter techniques require a large amount of training data in order to obtain good results; however, labeling a portion of each existing collection of documents is not feasible in practice. This is a common problem in supervised learning, which can be addressed by using the so-called Domain Adaptation (DA) techniques. These techniques take advantage of the knowledge learned in one domain, for which labeled data are available, to apply it to other domains for which there are no labeled data. This paper proposes a method that combines neural networks and DA in order to carry out unsupervised document binarization. However, when both the source and target domains are very similar, this adaptation could be detrimental. Our methodology, therefore, first measures the similarity between domains in an innovative manner in order to determine whether or not it is appropriate to apply the adaptation process. The results reported in the experimentation, when evaluating up to 20 possible combinations among five different domains, show that our proposal successfully deals with the binarization of new document domains without the need for labeled data. △ Less","1 July, 2021",https://arxiv.org/pdf/2012.01204
RAFT-3D: Scene Flow using Rigid-Motion Embeddings,Zachary Teed;Jia Deng,"We address the problem of scene flow: given a pair of stereo or RGB-D video frames, estimate pixelwise 3D motion. We introduce RAFT-3D, a new deep architecture for scene flow. RAFT-3D is based on the RAFT model developed for optical flow but iteratively updates a dense field of pixelwise SE3 motion instead of 2D motion. A key innovation of RAFT-3D is rigid-motion embeddings, which represent a soft grouping of pixels into rigid objects. Integral to rigid-motion embeddings is Dense-SE3, a differentiable layer that enforces geometric consistency of the embeddings. Experiments show that RAFT-3D achieves state-of-the-art performance. On FlyingThings3D, under the two-view evaluation, we improved the best published accuracy (d < 0.05) from 34.3% to 83.7%. On KITTI, we achieve an error of 5.77, outperforming the best published method (6.31), despite using no object instance supervision. Code is available at https://github.com/princeton-vl/RAFT-3D. △ Less","6 April, 2021",https://arxiv.org/pdf/2012.00726
TStarBot-X: An Open-Sourced and Comprehensive Study for Efficient League Training in StarCraft II Full Game,Lei Han;Jiechao Xiong;Peng Sun;Xinghai Sun;Meng Fang;Qingwei Guo;Qiaobo Chen;Tengfei Shi;Hongsheng Yu;Xipeng Wu;Zhengyou Zhang,"StarCraft, one of the most difficult esport games with long-standing history of professional tournaments, has attracted generations of players and fans, and also, intense attentions in artificial intelligence research. Recently, Google's DeepMind announced AlphaStar, a grandmaster level AI in StarCraft II that can play with humans using comparable action space and operations. In this paper, we introduce a new AI agent, named TStarBot-X, that is trained under orders of less computations and can play competitively with expert human players. TStarBot-X takes advantage of important techniques introduced in AlphaStar, and also benefits from substantial innovations including new league training methods, novel multi-agent roles, rule-guided policy search, stabilized policy improvement, lightweight neural network architecture, and importance sampling in imitation learning, etc. We show that with orders of less computation scale, a faithful reimplementation of AlphaStar's methods can not succeed and the proposed techniques are necessary to ensure TStarBot-X's competitive performance. We reveal all technical details that are complementary to those mentioned in AlphaStar, showing the most sensitive parts in league training, reinforcement learning and imitation learning that affect the performance of the agents. Most importantly, this is an open-sourced study that all codes and resources (including the trained model parameters) are publicly accessible via \url{https://github.com/tencent-ailab/tleague_projpage}. We expect this study could be beneficial for both academic and industrial future research in solving complex problems like StarCraft, and also, might provide a sparring partner for all StarCraft II players and other AI agents. △ Less","30 April, 2021",https://arxiv.org/pdf/2011.13729
Learnable Gabor modulated complex-valued networks for orientation robustness,Felix Richards;Adeline Paiement;Xianghua Xie;Elisabeth Sola;Pierre-Alain Duc,"Robustness to transformation is desirable in many computer vision tasks, given that input data often exhibits pose variance. While translation invariance and equivariance is a documented phenomenon of CNNs, sensitivity to other transformations is typically encouraged through data augmentation. We investigate the modulation of complex valued convolutional weights with learned Gabor filters to enable orientation robustness. The resulting network can generate orientation dependent features free of interpolation with a single set of learnable rotation-governing parameters. By choosing to either retain or pool orientation channels, the choice of equivariance versus invariance can be directly controlled. Moreover, we introduce rotational weight-tying through a proposed cyclic Gabor convolution, further enabling generalisation over rotations. We combine these innovations into Learnable Gabor Convolutional Networks (LGCNs), that are parameter-efficient and offer increased model complexity. We demonstrate their rotation invariance and equivariance on MNIST, BSD and a dataset of simulated and real astronomical images of Galactic cirri. △ Less","5 October, 2021",https://arxiv.org/pdf/2011.11734
Learning Class Unique Features in Fine-Grained Visual Classification,Runkai Zheng;Zhijia Yu;Yinqi Zhang;Chris Ding;Hei Victor Cheng;Li Liu,"A major challenge in Fine-Grained Visual Classification (FGVC) is distinguishing various categories with high inter-class similarity by learning the feature that differentiate the details. Conventional cross entropy trained Convolutional Neural Network (CNN) fails this challenge as it may suffer from producing inter-class invariant features in FGVC. In this work, we innovatively propose to regularize the training of CNN by enforcing the uniqueness of the features to each category from an information theoretic perspective. To achieve this goal, we formulate a minimax loss based on a game theoretic framework, where a Nash equilibria is proved to be consistent with this regularization objective. Besides, to prevent from a feasible solution of minimax loss that may produce redundant features, we present a Feature Redundancy Loss (FRL) based on normalized inner product between each selected feature map pair to complement the proposed minimax loss. Superior experimental results on several influential benchmarks along with visualization show that our method gives full play to the performance of the baseline model without additional computation and achieves comparable results with state-of-the-art models. △ Less","16 March, 2021",https://arxiv.org/pdf/2011.10951
Design Space for Graph Neural Networks,Jiaxuan You;Rex Ying;Jure Leskovec,"The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management. △ Less","23 July, 2021",https://arxiv.org/pdf/2011.08843
Power System Event Identification based on Deep Neural Network with Information Loading,Jie Shi;Brandon Foggo;Nanpeng Yu,"Online power system event identification and classification is crucial to enhancing the reliability of transmission systems. In this paper, we develop a deep neural network (DNN) based approach to identify and classify power system events by leveraging real-world measurements from hundreds of phasor measurement units (PMUs) and labels from thousands of events. Two innovative designs are embedded into the baseline model built on convolutional neural networks (CNNs) to improve the event classification accuracy. First, we propose a graph signal processing based PMU sorting algorithm to improve the learning efficiency of CNNs. Second, we deploy information loading based regularization to strike the right balance between memorization and generalization for the DNN. Numerical studies results based on real-world dataset from the Eastern Interconnection of the U.S power transmission grid show that the combination of PMU based sorting and the information loading based regularization techniques help the proposed DNN approach achieve highly accurate event identification and classification results. △ Less","28 April, 2021",https://arxiv.org/pdf/2011.06718
"Deep Sound Change: Deep and Iterative Learning, Convolutional Neural Networks, and Language Change",Gašper Beguš,"This paper proposes a framework for modeling sound change that combines deep learning and iterative learning. Acquisition and transmission of speech is modeled by training generations of Generative Adversarial Networks (GANs) on unannotated raw speech data. The paper argues that several properties of sound change emerge from the proposed architecture. GANs (Goodfellow et al. 2014 arXiv:1406.2661, Donahue et al. 2019 arXiv:1705.07904) are uniquely appropriate for modeling language change because the networks are trained on raw unsupervised acoustic data, contain no language-specific features and, as argued in Beguš (2020 arXiv:2006.03965), encode phonetic and phonological representations in their latent space and generate linguistically informative innovative data. The first generation of networks is trained on the relevant sequences in human speech from TIMIT. The subsequent generations are not trained on TIMIT, but on generated outputs from the previous generation and thus start learning from each other in an iterative learning task. The initial allophonic distribution is progressively being lost with each generation, likely due to pressures from the global distribution of aspiration in the training data. The networks show signs of a gradual shift in phonetic targets characteristic of a gradual phonetic sound change. At endpoints, the outputs superficially resemble a phonological change -- rule loss. △ Less","22 September, 2021",https://arxiv.org/pdf/2011.05463
Deoscillated Graph Collaborative Filtering,Zhiwei Liu;Lin Meng;Fei Jiang;Jiawei Zhang;Philip S. Yu,"Collaborative Filtering (CF) signals are crucial for a Recommender System~(RS) model to learn user and item embeddings. High-order information can alleviate the cold-start issue of CF-based methods, which is modelled through propagating the information over the user-item bipartite graph. Recent Graph Neural Networks~(GNNs) propose to stack multiple aggregation layers to propagate high-order signals. However, the oscillation problem, varying locality of bipartite graph, and the fix propagation pattern spoil the ability of multi-layer structure to propagate information. The oscillation problem results from the bipartite structure, as the information from users only propagates to items. Besides oscillation problem, varying locality suggests the density of nodes should be considered in the propagation process. Moreover, the layer-fixed propagation pattern introduces redundant information between layers. In order to tackle these problems, we propose a new RS model, named as \textbf{D}eoscillated \textbf{G}raph \textbf{C}ollaborative \textbf{F}iltering~(DGCF). We introduce cross-hop propagation layers in it to break the bipartite propagating structure, thus resolving the oscillation problem. Additionally, we design innovative locality-adaptive layers which adaptively propagate information. Stacking multiple cross-hop propagation layers and locality layers constitutes the DGCF model, which models high-order CF signals adaptively to the locality of nodes and layers. Extensive experiments on real-world datasets show the effectiveness of DGCF. Detailed analyses indicate that DGCF solves oscillation problem, adaptively learns local factor, and has layer-wise propagation pattern. Our code is available online at https://github.com/JimLiu96/DeosciRec. △ Less","28 May, 2021",https://arxiv.org/pdf/2011.02100
Collective Knowledge: organizing research projects as a database of reusable components and portable workflows with common APIs,Grigori Fursin,"This article provides the motivation and overview of the Collective Knowledge framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. Inspired by the USB ""plug and play"" approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the universal CK program pipeline with software detection plugins and the automatic installation of missing packages. This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning (ML) and artificial intelligence (AI) in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at https://cKnowledge.io . △ Less","30 January, 2021",https://arxiv.org/pdf/2011.01149
A Critical Correspondence on Humpty Dumpty's Funding for European Journalism,Jukka Ruohonen,"This short critical correspondence discusses the Digital News Innovation (DNI) fund orchestrated by Humpty Dumpty -- a.k.a. Google -- for helping European journalism to innovate and renew itself. Based on topic modeling and critical discourse analysis, the results indicate that the innovative projects mostly mimic the old business model of Humpty Dumpty. With these results and the accompanying critical discussion, this correspondence contributes to the ongoing battle between platforms and media. △ Less","14 June, 2021",https://arxiv.org/pdf/2011.00751
Improved Hierarchical ADMM for Nonconvex Cooperative Distributed Model Predictive Control,Xiaoxue Zhang;Jun Ma;Zilong Cheng;Sunan Huang;Clarence W. de Silva;Tong Heng Lee,"Distributed optimization is often widely attempted and innovated as an attractive and preferred methodology to solve large-scale problems effectively in a localized and coordinated manner. Thus, it is noteworthy that the methodology of distributed model predictive control (DMPC) has become a promising approach to achieve effective outcomes, e.g., in decision-making tasks for multi-agent systems. However, the typical deployment of such distributed MPC frameworks would lead to the involvement of nonlinear processes with a large number of nonconvex constraints. To address this important problem, the development and innovation of a hierarchical three-block alternating direction method of multipliers (ADMM) approach is presented in this work to solve this nonconvex cooperative DMPC problem in multi-agent systems. Here firstly, an additional slack variable is introduced to transform the original large-scale nonconvex optimization problem. Then, a hierarchical ADMM approach, which contains outer loop iteration by the augmented Lagrangian method (ALM) and inner loop iteration by three-block semi-proximal ADMM, is utilized to solve the resulting transformed nonconvex optimization problem. Additionally, it is analytically shown and established that the requisite desired stationary point exists for convergence in the algorithm. Finally, an approximate optimization stage with a barrier method is then applied to further significantly improve the computational efficiency, yielding the final improved hierarchical ADMM. The effectiveness of the proposed method in terms of attained performance and computational efficiency is demonstrated on a cooperative DMPC problem of decision-making process for multiple unmanned aerial vehicles (UAVs). △ Less","27 August, 2021",https://arxiv.org/pdf/2011.00463
ACeD: Scalable Data Availability Oracle,Peiyao Sheng;Bowen Xue;Sreeram Kannan;Pramod Viswanath,"A popular method in practice offloads computation and storage in blockchains by relying on committing only hashes of off-chain data into the blockchain. This mechanism is acknowledged to be vulnerable to a stalling attack: the blocks corresponding to the committed hashes may be unavailable at any honest node. The straightforward solution of broadcasting all blocks to the entire network sidesteps this data availability attack, but it is not scalable. In this paper, we propose ACeD, a scalable solution to this data availability problem with O(1) communication efficiency, the first to the best of our knowledge. The key innovation is a new protocol that requires each of the N nodes to receive only O(1/N) of the block, such that the data is guaranteed to be available in a distributed manner in the network. Our solution creatively integrates coding-theoretic designs inside of Merkle tree commitments to guarantee efficient and tamper-proof reconstruction; this solution is distinct from Asynchronous Verifiable Information Dispersal (in guaranteeing efficient proofs of malformed coding) and Coded Merkle Tree (which only provides guarantees for random corruption as opposed to our guarantees for worst-case corruption). We implement ACeD with full functionality in 6000 lines of Rust code, integrate the functionality as a smart contract into Ethereum via a high-performance implementation demonstrating up to 10,000 transactions per second in throughput and 6000x reduction in gas cost on the Ethereum testnet Kovan. △ Less","3 March, 2021",https://arxiv.org/pdf/2011.00102
Interpretable Data-Driven Demand Modelling for On-Demand Transit Services,Nael Alsaleh;Bilal Farooq,"In recent years, with the advancements in information and communication technology, different emerging on-demand shared mobility services have been introduced as innovative solutions in the low-density areas, including on-demand transit (ODT), mobility on-demand (MOD) transit, and crowdsourced mobility services. However, due to their infancy, there is a strong need to understand and model the demand for these services. In this study, we developed trip production and distribution models for ODT services at Dissemination areas (DA) level using four machine learning algorithms: Random Forest (RF), Bagging, Artificial Neural Network (ANN) and Deep Neural Network (DNN). The data used in the modelling process were acquired from Belleville's ODT operational data and 2016 census data. Bayesian optimalization approach was used to find the optimal architecture of the adopted algorithms. Moreover, post-hoc model was employed to interpret the predictions and examine the importance of the explanatory variables. The results showed that the land-use type was the most important variable in the trip production model. On the other hand, the demographic characteristics of the trip destination were the most important variables in the trip distribution model. Moreover, the results revealed that higher trip distribution levels are expected between dissemination areas with commercial/industrial land-use type and dissemination areas with high-density residential land-use. Our findings suggest that the performance of ODT services can be further enhanced by (a) locating idle vehicles in the neighbourhoods with commercial/industrial land-use and (b) using the spatio-temporal demand models obtained in this work to continuously update the operating fleet size. △ Less","1 October, 2021",https://arxiv.org/pdf/2010.15673
Receptor Saturation Modeling for Synaptic DMC,Sebastian Lotter;Maximilian Schäfer;Johannes Zeitler;Robert Schober,"Synaptic communication is a natural Molecular Communication (MC) system which may serve as a blueprint for the design of synthetic MC systems. In particular, it features highly specialized mechanisms to enable inter-symbol interference (ISI)-free and energy efficient communication. The understanding of synaptic MC is furthermore critical for disruptive innovations in the context of brain-machine interfaces. However, the physical modeling of synaptic MC is complicated by the possible saturation of the molecular receiver arising from the competition of postsynaptic receptors for neurotransmitters. Saturation renders the system behavior nonlinear and is commonly neglected in existing analytical models. In this work, we propose a novel model for receptor saturation in terms of a nonlinear, state-dependent boundary condition for Fick's diffusion equation. We solve the resulting boundary-value problem using an eigenfunction expansion of the Laplace operator and the incorporation of the receiver memory as feedback system into the corresponding state-space description. The presented solution is numerically stable and computationally efficient. Furthermore, the proposed model is validated with particle-based stochastic computer simulations. △ Less","30 September, 2021",https://arxiv.org/pdf/2010.14828
Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores,Aozhi Liu;Lipei Zhang;Yaqi Mei;Baoqiang Han;Zifeng Cai;Zhaohua Zhu;Jing Xiao,"One of the challenges of the Optical Music Recognition task is to transcript the symbols of the camera-captured images into digital music notations. Previous end-to-end model which was developed as a Convolutional Recurrent Neural Network does not explore sufficient contextual information from full scales and there is still a large room for improvement. We propose an innovative framework that combines a block of Residual Recurrent Convolutional Neural Network with a recurrent Encoder-Decoder network to map a sequence of monophonic music symbols corresponding to the notations present in the image. The Residual Recurrent Convolutional block can improve the ability of the model to enrich the context information. The experiment results are benchmarked against a publicly available dataset called CAMERA-PRIMUS, which demonstrates that our approach surpass the state-of-the-art end-to-end method using Convolutional Recurrent Neural Network. △ Less","4 August, 2021",https://arxiv.org/pdf/2010.13418
Geometrically Matched Multi-source Microscopic Image Synthesis Using Bidirectional Adversarial Networks,Jun Zhuang;Dali Wang,"Microscopic images from multiple modalities can produce plentiful experimental information. In practice, biological or physical constraints under a given observation period may prevent researchers from acquiring enough microscopic scanning. Recent studies demonstrate that image synthesis is one of the popular approaches to release such constraints. Nonetheless, most existing synthesis approaches only translate images from the source domain to the target domain without solid geometric associations. To embrace this challenge, we propose an innovative model architecture, BANIS, to synthesize diversified microscopic images from multi-source domains with distinct geometric features. The experimental outcomes indicate that BANIS successfully synthesizes favorable image pairs on C. elegans microscopy embryonic images. To the best of our knowledge, BANIS is the first application to synthesize microscopic images that associate distinct spatial geometric features from multi-source domains. △ Less","27 March, 2021",https://arxiv.org/pdf/2010.13308
"LoopReg: Self-supervised Learning of Implicit Surface Correspondences, Pose and Shape for 3D Human Mesh Registration",Bharat Lal Bhatnagar;Cristian Sminchisescu;Christian Theobalt;Gerard Pons-Moll,"We address the problem of fitting 3D human models to 3D scans of dressed humans. Classical methods optimize both the data-to-model correspondences and the human model parameters (pose and shape), but are reliable only when initialized close to the solution. Some methods initialize the optimization based on fully supervised correspondence predictors, which is not differentiable end-to-end, and can only process a single scan at a time. Our main contribution is LoopReg, an end-to-end learning framework to register a corpus of scans to a common 3D human model. The key idea is to create a self-supervised loop. A backward map, parameterized by a Neural Network, predicts the correspondence from every scan point to the surface of the human model. A forward map, parameterized by a human model, transforms the corresponding points back to the scan based on the model parameters (pose and shape), thus closing the loop. Formulating this closed loop is not straightforward because it is not trivial to force the output of the NN to be on the surface of the human model - outside this surface the human model is not even defined. To this end, we propose two key innovations. First, we define the canonical surface implicitly as the zero level set of a distance field in R3, which in contrast to morecommon UV parameterizations, does not require cutting the surface, does not have discontinuities, and does not induce distortion. Second, we diffuse the human model to the 3D domain R3. This allows to map the NN predictions forward,even when they slightly deviate from the zero level set. Results demonstrate that we can train LoopRegmainly self-supervised - following a supervised warm-start, the model becomes increasingly more accurate as additional unlabelled raw scans are processed. Our code and pre-trained models can be downloaded for research. △ Less","26 November, 2021",https://arxiv.org/pdf/2010.12447
PlenoptiCam v1.0: A light-field imaging framework,Christopher Hahne;Amar Aggoun,"Light-field cameras play a vital role for rich 3-D information retrieval in narrow range depth sensing applications. The key obstacle in composing light-fields from exposures taken by a plenoptic camera is to computationally calibrate, align and rearrange four-dimensional image data. Several attempts have been proposed to enhance the overall image quality by tailoring pipelines dedicated to particular plenoptic cameras and improving the consistency across viewpoints at the expense of high computational loads. The framework presented herein advances prior outcomes thanks to its novel micro image scale-space analysis for generic camera calibration independent of the lens specifications and its parallax-invariant, cost-effective viewpoint color equalization from optimal transport theory. Artifacts from the sensor and micro lens grid are compensated in an innovative way to enable superior quality in sub-aperture image extraction, computational refocusing and Scheimpflug rendering with sub-sampling capabilities. Benchmark comparisons using established image metrics suggest that our proposed pipeline outperforms state-of-the-art tool chains in the majority of cases. Results from a Wasserstein distance further show that our color transfer outdoes the existing transport methods. Our algorithms are released under an open-source license, offer cross-platform compatibility with few dependencies and different user interfaces. This makes the reproduction of results and experimentation with plenoptic camera technology convenient for peer researchers, developers, photographers, data scientists and others working in this field. △ Less","25 July, 2021",https://arxiv.org/pdf/2010.11687
Cross-Modal Information Maximization for Medical Imaging: CMIM,Tristan Sylvain;Francis Dutil;Tess Berthier;Lisa Di Jorio;Margaux Luck;Devon Hjelm;Yoshua Bengio,"In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time. In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities. △ Less","1 February, 2021",https://arxiv.org/pdf/2010.10593
Towards an Automatic Analysis of CHO-K1 Suspension Growth in Microfluidic Single-cell Cultivation,Dominik Stallmann;Jan P. Göpfert;Julian Schmitz;Alexander Grünberger;Barbara Hammer,"Motivation: Innovative microfluidic systems carry the promise to greatly facilitate spatio-temporal analysis of single cells under well-defined environmental conditions, allowing novel insights into population heterogeneity and opening new opportunities for fundamental and applied biotechnology. Microfluidics experiments, however, are accompanied by vast amounts of data, such as time series of microscopic images, for which manual evaluation is infeasible due to the sheer number of samples. While classical image processing technologies do not lead to satisfactory results in this domain, modern deep learning technologies such as convolutional networks can be sufficiently versatile for diverse tasks, including automatic cell tracking and counting as well as the extraction of critical parameters, such as growth rate. However, for successful training, current supervised deep learning requires label information, such as the number or positions of cells for each image in a series; obtaining these annotations is very costly in this setting. Results: We propose a novel Machine Learning architecture together with a specialized training procedure, which allows us to infuse a deep neural network with human-powered abstraction on the level of data, leading to a high-performing regression model that requires only a very small amount of labeled data. Specifically, we train a generative model simultaneously on natural and synthetic data, so that it learns a shared representation, from which a target variable, such as the cell count, can be reliably estimated. △ Less","17 May, 2021",https://arxiv.org/pdf/2010.10124
Decoding Methods for Neural Narrative Generation,Alexandra DeLucia;Aaron Mueller;Xiang Lisa Li;João Sedoc,"Narrative generation is an open-ended NLP task in which a model generates a story given a prompt. The task is similar to neural response generation for chatbots; however, innovations in response generation are often not applied to narrative generation, despite the similarity between these tasks. We aim to bridge this gap by applying and evaluating advances in decoding methods for neural response generation to neural narrative generation. In particular, we employ GPT-2 and perform ablations across nucleus sampling thresholds and diverse decoding hyperparameters -- specifically, maximum mutual information -- analyzing results over multiple criteria with automatic and human evaluation. We find that (1) nucleus sampling is generally best with thresholds between 0.7 and 0.9; (2) a maximum mutual information objective can improve the quality of generated stories; and (3) established automatic metrics do not correlate well with human judgments of narrative quality on any qualitative metric. △ Less","8 July, 2021",https://arxiv.org/pdf/2010.07375
"Transportation Internet: Concepts, Models, and Architectures",Hui Li,"Disruptive changes in vehicles and transportation have been triggered by automated, connected, electrified and shared mobility. Autonomous vehicles, like Internet data packets, are transported from one address to another through the road network. The Internet has become a general network transmission paradigm, and the Energy Internet is a successful application of this paradigm to the field of energy. By introducing the Internet paradigm to the field of transportation, this paper is the first to propose the Transportation Internet. Based on the concept of the Transportation Internet, fundamental models, such as the switching, routing, and hierarchical models, are established to form basic theories; new architectures, such as transportation routers and software defined transportation, are proposed to make transportation interconnected and open; system verifications, such as prototype and simulation, are also carried out to prove feasibility and advancement. The Transportation Internet, which is of far-reaching significance in science and industry, has brought systematic breakthroughs in theory, architecture, and technology, explored innovative research directions, and provided an Internet-like solution for the new generation of transportation. △ Less","29 September, 2021",https://arxiv.org/pdf/2010.06880
Pretrained Transformers for Text Ranking: BERT and Beyond,Jimmy Lin;Rodrigo Nogueira;Andrew Yates,"The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query. Although the most common formulation of text ranking is search, instances of the task can also be found in many natural language processing applications. This survey provides an overview of text ranking with neural network architectures known as transformers, of which BERT is the best-known example. The combination of transformers and self-supervised pretraining has been responsible for a paradigm shift in natural language processing (NLP), information retrieval (IR), and beyond. In this survey, we provide a synthesis of existing work as a single point of entry for practitioners who wish to gain a better understanding of how to apply transformers to text ranking problems and researchers who wish to pursue work in this area. We cover a wide range of modern techniques, grouped into two high-level categories: transformer models that perform reranking in multi-stage architectures and dense retrieval techniques that perform ranking directly. There are two themes that pervade our survey: techniques for handling long documents, beyond typical sentence-by-sentence processing in NLP, and techniques for addressing the tradeoff between effectiveness (i.e., result quality) and efficiency (e.g., query latency, model and index size). Although transformer architectures and pretraining techniques are recent innovations, many aspects of how they are applied to text ranking are relatively well understood and represent mature techniques. However, there remain many open research questions, and thus in addition to laying out the foundations of pretrained transformers for text ranking, this survey also attempts to prognosticate where the field is heading. △ Less","19 August, 2021",https://arxiv.org/pdf/2010.06467
TurboTransformers: An Efficient GPU Serving System For Transformer Models,Jiarui Fang;Yang Yu;Chengduo Zhao;Jie Zhou,"The transformer is the most critical algorithm innovation of the Nature Language Processing (NLP) field in recent years. Unlike the Recurrent Neural Network (RNN) models, Transformers can process on dimensions of sequence lengths in parallel, therefore leading to better accuracy on long sequences. However, efficient deployments of them for online services in data centers equipped with GPUs are not easy. First, more computation introduced by transformer structures makes it more challenging to meet the latency and throughput constraints of serving. Second, NLP tasks take in sentences of variable length. The variability of input dimensions brings a severe problem to efficient memory management and serving optimization. This paper designed a transformer serving system called TurboTransformers, which consists of a computing runtime and a serving framework to solve the above challenges. Three innovative features make it stand out from other similar works. An efficient parallel algorithm is proposed for GPU-based batch reduction operations, like Softmax and LayerNorm, major hot spots besides BLAS routines. A memory allocation algorithm, which better balances the memory footprint and allocation/free efficiency, is designed for variable-length input situations. A serving framework equipped with a new batch scheduler using dynamic programming achieves the optimal throughput on variable-length requests. The system can achieve the state-of-the-art transformer model serving performance on GPU platforms and can be seamlessly integrated into your PyTorch code with a few lines of code. △ Less","20 February, 2021",https://arxiv.org/pdf/2010.05680
Learning Risk Preferences from Investment Portfolios Using Inverse Optimization,Shi Yu;Haoran Wang;Chaosheng Dong,"The fundamental principle in Modern Portfolio Theory (MPT) is based on the quantification of the portfolio's risk related to performance. Although MPT has made huge impacts on the investment world and prompted the success and prevalence of passive investing, it still has shortcomings in real-world applications. One of the main challenges is that the level of risk an investor can endure, known as \emph{risk-preference}, is a subjective choice that is tightly related to psychology and behavioral science in decision making. This paper presents a novel approach of measuring risk preference from existing portfolios using inverse optimization on the mean-variance portfolio allocation framework. Our approach allows the learner to continuously estimate real-time risk preferences using concurrent observed portfolios and market price data. We demonstrate our methods on real market data that consists of 20 years of asset pricing and 10 years of mutual fund portfolio holdings. Moreover, the quantified risk preference parameters are validated with two well-known risk measurements currently applied in the field. The proposed methods could lead to practical and fruitful innovations in automated/personalized portfolio management, such as Robo-advising, to augment financial advisors' decision intelligence in a long-term investment horizon. △ Less","12 February, 2021",https://arxiv.org/pdf/2010.01687
A Chirplet Transform-based Mode Retrieval Method for Multicomponent Signals with Crossover Instantaneous Frequencies,Lin Li;Ningning Han;Qingtang Jiang;Charles K. Chui,"In nature and engineering world, the acquired signals are usually affected by multiple complicated factors and appear as multicomponent nonstationary modes. In such and many other situations, it is necessary to separate these signals into a finite number of monocomponents to represent the intrinsic modes and underlying dynamics implicated in the source signals. In this paper, we consider the mode retrieval of a multicomponent signal which has crossing instantaneous frequencies (IFs), meaning that some of the components of the signal overlap in the time-frequency domain. We use the chirplet transform (CT) to represent a multicomponent signal in the three-dimensional space of time, frequency and chirp rate and introduce a CT-based signal separation scheme (CT3S) to retrieve modes. In addition, we analyze the error bounds for IF estimation and component recovery with this scheme. We also propose a matched-filter along certain specific time-frequency lines with respect to the chirp rate to make nonstationary signals be further separated and more concentrated in the three-dimensional space of CT. Furthermore, based on the approximation of source signals with linear chirps at any local time, we propose an innovative signal reconstruction algorithm, called the group filter-matched CT3S (GFCT3S), which also takes a group of components into consideration simultaneously. GFCT3S is suitable for signals with crossing IFs. It also decreases component recovery errors when the IFs curves of different components are not crossover, but fast-varying and close to one and other. Numerical experiments on synthetic and real signals show our method is more accurate and consistent in signal separation than the empirical mode decomposition, synchrosqueezing transform, and other approaches △ Less","13 October, 2021",https://arxiv.org/pdf/2010.01498
GCNNMatch: Graph Convolutional Neural Networks for Multi-Object Tracking via Sinkhorn Normalization,Ioannis Papakis;Abhijit Sarkar;Anuj Karpatne,"This paper proposes a novel method for online Multi-Object Tracking (MOT) using Graph Convolutional Neural Network (GCNN) based feature extraction and end-to-end feature matching for object association. The Graph based approach incorporates both appearance and geometry of objects at past frames as well as the current frame into the task of feature learning. This new paradigm enables the network to leverage the ""context"" information of the geometry of objects and allows us to model the interactions among the features of multiple objects. Another central innovation of our proposed framework is the use of the Sinkhorn algorithm for end-to-end learning of the associations among objects during model training. The network is trained to predict object associations by taking into account constraints specific to the MOT task. Experimental results demonstrate the efficacy of the proposed approach in achieving top performance on the MOT '15, '16, '17 and '20 Challenges among state-of-the-art online approaches. The code is available at https://github.com/IPapakis/GCNNMatch. △ Less","16 April, 2021",https://arxiv.org/pdf/2010.00067
Bandwidth-Agile Image Transmission with Deep Joint Source-Channel Coding,David Burth Kurka;Deniz Gündüz,"We propose deep learning based communication methods for adaptive-bandwidth transmission of images over wireless channels. We consider the scenario in which images are transmitted progressively in layers over time or frequency, and such layers can be aggregated by receivers in order to increase the quality of their reconstructions. We investigate two scenarios, one in which the layers are sent sequentially, and incrementally contribute to the refinement of a reconstruction, and another in which the layers are independent and can be retrieved in any order. Those scenarios correspond to the well known problems of \textit{successive refinement} and \textit{multiple descriptions}, respectively, in the context of joint source-channel coding (JSCC). We propose DeepJSCC-l, an innovative solution that uses convolutional autoencoders, and present three architectures with different complexity trade-offs. To the best of our knowledge, this is the first practical multiple-description JSCC scheme developed and tested for practical information sources and channels. Numerical results show that DeepJSCC-l can learn to transmit the source progressively with negligible losses in the end-to-end performance compared with a single transmission. Moreover, DeepJSCC-l has comparable performance with state of the art digital progressive transmission schemes in the challenging low signal-to-noise ratio (SNR) and small bandwidth regimes, with the additional advantage of graceful degradation with channel SNR. △ Less","13 June, 2021",https://arxiv.org/pdf/2009.12480
Local Post-Hoc Explanations for Predictive Process Monitoring in Manufacturing,Nijat Mehdiyev;Peter Fettke,"This study proposes an innovative explainable predictive quality analytics solution to facilitate data-driven decision-making for process planning in manufacturing by combining process mining, machine learning, and explainable artificial intelligence (XAI) methods. For this purpose, after integrating the top-floor and shop-floor data obtained from various enterprise information systems, a deep learning model was applied to predict the process outcomes. Since this study aims to operationalize the delivered predictive insights by embedding them into decision-making processes, it is essential to generate relevant explanations for domain experts. To this end, two complementary local post-hoc explanation approaches, Shapley values and Individual Conditional Expectation (ICE) plots are adopted, which are expected to enhance the decision-making capabilities by enabling experts to examine explanations from different perspectives. After assessing the predictive strength of the applied deep neural network with relevant binary classification evaluation measures, a discussion of the generated explanations is provided. △ Less","10 June, 2021",https://arxiv.org/pdf/2009.10513
Incentive-compatible mechanisms for online resource allocation in mobility-as-a-service systems,Haoning Xi;Wei Liu;David Rey;S. Travis Waller;Philip Kilby,"In the context of `Everything-as-a-Service', the transportation sector has been evolving towards user-centric business models in which customized services and mode-agnostic mobility resources are priced in a unified framework. Yet, in the vast majority of studies on Mobility as a Service (MaaS) systems, mobility resource pricing is based on segmented travel modes, e.g. private vehicle, public transit and shared mobility services. This study attempts to address this research gap by introducing innovative auction-based online MaaS mechanisms where users can bid for any amount of mode-agnostic mobility resources based on their willingness to pay and preferences. We take the perspective of a MaaS regulator which aims to maximize social welfare by allocating mobility resources to users. We propose two mechanisms which allow users to either pay for the immediate use of mobility service (pay-as-you-go), or to subscribe to mobility service packages (pay-as-a-package). We cast the proposed auction-based mechanisms as online resource allocation problems where users compete for MaaS resources and bid for travel time per trip. We propose (integer-) linear programming formulations to accommodate user bids based on available mobility resources in an online optimization approach. We show that the proposed MaaS mechanisms are incentive-compatible, develop customized online algorithms and derive performance bounds based on competitive analysis. Extensive numerical simulations are conducted on large scale instances generated from realistic mobility data, which highlight the benefits of the proposed MaaS mechanisms and the effectiveness of the proposed online optimization approaches. △ Less","28 March, 2021",https://arxiv.org/pdf/2009.06806
Analysing Twitter Semantic Networks: the case of 2018 Italian Elections,Tommaso Radicioni;Fabio Saracco;Elena Pavan;Tiziano Squartini,"Social media play a key role in shaping citizens' political opinion. According to the Eurobarometer, the percentage of EU citizens employing online social networks on a daily basis has increased from 18% in 2010 to 48% in 2019. The entwinement between social media and the unfolding of political dynamics has motivated the interest of researchers for the analysis of users online behavior - with particular emphasis on group polarization during debates and echo-chambers formation. In this context, attention has been predominantly directed towards the study of online relations between users while semantic aspects have remained under-explored. In the present paper, we aim at filling this gap by adopting a two-steps approach. First, we identify the discursive communities animating the political debate in the run up of the 2018 Italian Elections as groups of users with a significantly-similar retweeting behavior. Second, we study the semantic mechanisms that shape their internal discussions by monitoring, on a daily basis, the structural evolution of the semantic networks they induce. Above and beyond specifying the semantic peculiarities of the Italian electoral competition, our approach innovates studies of online political discussions in two main ways. On the one hand, it grounds semantic analysis within users' behaviors by implementing a method, rooted in statistical theory, that guarantees that our inference of socio-semantic structures is not biased by any unsupported assumption about missing information; on the other, it is completely automated as it does not rest upon any manual labelling (either based on the users' features or on their sharing patterns). These elements make our method applicable to any Twitter discussion regardless of the language or the topic addressed. △ Less","24 June, 2021",https://arxiv.org/pdf/2009.02960
Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations,Nghi D. Q. Bui;Yijun Yu;Lingxiao Jiang,"We propose Corder, a self-supervised contrastive learning framework for source code model. Corder is designed to alleviate the need of labeled data for code retrieval and code summarization tasks. The pre-trained model of Corder can be used in two ways: (1) it can produce vector representation of code which can be applied to code retrieval tasks that do not have labeled data; (2) it can be used in a fine-tuning process for tasks that might still require label data such as code summarization. The key innovation is that we train the source code model by asking it to recognize similar and dissimilar code snippets through a contrastive learning objective. To do so, we use a set of semantic-preserving transformation operators to generate code snippets that are syntactically diverse but semantically equivalent. Through extensive experiments, we have shown that the code models pretrained by Corder substantially outperform the other baselines for code-to-code retrieval, text-to-code retrieval, and code-to-text summarization tasks. △ Less","23 May, 2021",https://arxiv.org/pdf/2009.02731
PointNetLK Revisited,Xueqian Li;Jhony Kaesemodel Pontes;Simon Lucey,"We address the generalization ability of recent learning-based point cloud registration methods. Despite their success, these approaches tend to have poor performance when applied to mismatched conditions that are not well-represented in the training set, such as unseen object categories, different complex scenes, or unknown depth sensors. In these circumstances, it has often been better to rely on classical non-learning methods (e.g., Iterative Closest Point), which have better generalization ability. Hybrid learning methods, that use learning for predicting point correspondences and then a deterministic step for alignment, have offered some respite, but are still limited in their generalization abilities. We revisit a recent innovation -- PointNetLK -- and show that the inclusion of an analytical Jacobian can exhibit remarkable generalization properties while reaping the inherent fidelity benefits of a learning framework. Our approach not only outperforms the state-of-the-art in mismatched conditions but also produces results competitive with current learning methods when operating on real-world test data close to the training set. △ Less","29 March, 2021",https://arxiv.org/pdf/2008.09527
Understanding and Improving Artifact Sharing in Software Engineering Research,Christopher S. Timperley;Lauren Herckis;Claire Le Goues;Michael Hilton,"In recent years, many software engineering researchers have begun to include artifacts alongside their research papers. Ideally, artifacts, including tools, benchmarks, and data, support the dissemination of ideas, provide evidence for research claims, and serve as a starting point for future research. However, in practice, artifacts suffer from a variety of issues that prevent the realization of their full potential. To help the software engineering community realize the full potential of artifacts, we seek to understand the challenges involved in the creation, sharing, and use of artifacts. To that end, we perform a mixed-methods study including a survey of artifacts in software engineering publications, and an online survey of 153 software engineering researchers. By analyzing the perspectives of artifact creators, users, and reviewers, we identify several high-level challenges that affect the quality of artifacts including mismatched expectations between these groups, and a lack of sufficient reward for both creators and reviewers. Using Diffusion of Innovations as an analytical framework, we examine how these challenges relate to one another, and build an understanding of the factors that affect the sharing and success of artifacts. Finally, we make recommendations to improve the quality of artifacts based on our results and existing best practices. △ Less","4 May, 2021",https://arxiv.org/pdf/2008.01046
SeCo: Exploring Sequence Supervision for Unsupervised Representation Learning,Ting Yao;Yiheng Zhang;Zhaofan Qiu;Yingwei Pan;Tao Mei,"A steady momentum of innovations and breakthroughs has convincingly pushed the limits of unsupervised image representation learning. Compared to static 2D images, video has one more dimension (time). The inherent supervision existing in such sequential structure offers a fertile ground for building unsupervised learning models. In this paper, we compose a trilogy of exploring the basic and generic supervision in the sequence from spatial, spatiotemporal and sequential perspectives. We materialize the supervisory signals through determining whether a pair of samples is from one frame or from one video, and whether a triplet of samples is in the correct temporal order. We uniquely regard the signals as the foundation in contrastive learning and derive a particular form named Sequence Contrastive Learning (SeCo). SeCo shows superior results under the linear protocol on action recognition (Kinetics), untrimmed activity recognition (ActivityNet) and object tracking (OTB-100). More remarkably, SeCo demonstrates considerable improvements over recent unsupervised pre-training techniques, and leads the accuracy by 2.96% and 6.47% against fully-supervised ImageNet pre-training in action recognition task on UCF101 and HMDB51, respectively. Source code is available at \url{https://github.com/YihengZhang-CV/SeCo-Sequence-Contrastive-Learning}. △ Less","27 January, 2021",https://arxiv.org/pdf/2008.00975
Joint Object Contour Points and Semantics for Instance Segmentation,Wenchao Zhang;Chong Fu;Mai Zhu,"The attributes of object contours has great significance for instance segmentation task. However, most of the current popular deep neural networks do not pay much attention to the object edge information. Inspired by the human annotation process when making instance segmentation datasets, in this paper, we propose Mask Point R-CNN aiming at promoting the neural network's attention to the object boundary. Specifically, we innovatively extend the original human keypoint detection task to the contour point detection of any object. Based on this analogy, we present an contour point detection auxiliary task to Mask R-CNN, which can boost the gradient flow between different tasks by effectively using feature fusion strategies and multi-task joint training. As a consequence, the model will be more sensitive to the edges of the object and can capture more geometric features. Quantitatively, the experimental results show that our approach outperforms vanilla Mask R-CNN by 3.8\% on Cityscapes dataset and 0.8\% on COCO dataset. △ Less","3 July, 2021",https://arxiv.org/pdf/2008.00460
A Vision and Framework for the High Altitude Platform Station (HAPS) Networks of the Future,Gunes Kurt;Mohammad G. Khoshkholgh;Safwan Alfattani;Ahmed Ibrahim;Tasneem S. J. Darwish;Md Sahabul Alam;Halim Yanikomeroglu;Abbas Yongacoglu,"A High Altitude Platform Station (HAPS) is a network node that operates in the stratosphere at an of altitude around 20 km and is instrumental for providing communication services. Precipitated by technological innovations in the areas of autonomous avionics, array antennas, solar panel efficiency levels, and battery energy densities, and fueled by flourishing industry ecosystems, the HAPS has emerged as an indispensable component of next-generations of wireless networks. In this article, we provide a vision and framework for the HAPS networks of the future supported by a comprehensive and state-of-the-art literature review. We highlight the unrealized potential of HAPS systems and elaborate on their unique ability to serve metropolitan areas. The latest advancements and promising technologies in the HAPS energy and payload systems are discussed. The integration of the emerging Reconfigurable Smart Surface (RSS) technology in the communications payload of HAPS systems for providing a cost-effective deployment is proposed. A detailed overview of the radio resource management in HAPS systems is presented along with synergistic physical layer techniques, including Faster-Than-Nyquist (FTN) signaling. Numerous aspects of handoff management in HAPS systems are described. The notable contributions of Artificial Intelligence (AI) in HAPS, including machine learning in the design, topology management, handoff, and resource allocation aspects are emphasized. The extensive overview of the literature we provide is crucial for substantiating our vision that depicts the expected deployment opportunities and challenges in the next 10 years (next-generation networks), as well as in the subsequent 10 years (next-next-generation networks). △ Less","17 March, 2021",https://arxiv.org/pdf/2007.15088
Learning Differentiable Programs with Admissible Neural Heuristics,Ameesh Shah;Eric Zhan;Jennifer J. Sun;Abhinav Verma;Yisong Yue;Swarat Chaudhuri,"We study the problem of learning differentiable functions expressed as programs in a domain-specific language. Such programmatic models can offer benefits such as composability and interpretability; however, learning them requires optimizing over a combinatorial space of program ""architectures"". We frame this optimization problem as a search in a weighted graph whose paths encode top-down derivations of program syntax. Our key innovation is to view various classes of neural networks as continuous relaxations over the space of programs, which can then be used to complete any partial program. This relaxed program is differentiable and can be trained end-to-end, and the resulting training loss is an approximately admissible heuristic that can guide the combinatorial search. We instantiate our approach on top of the A-star algorithm and an iteratively deepened branch-and-bound search, and use these algorithms to learn programmatic classifiers in three sequence classification tasks. Our experiments show that the algorithms outperform state-of-the-art methods for program learning, and that they discover programmatic classifiers that yield natural interpretations and achieve competitive accuracy. △ Less","27 March, 2021",https://arxiv.org/pdf/2007.12101
Towards Practical Lipreading with Distilled and Efficient Models,Pingchuan Ma;Brais Martinez;Stavros Petridis;Maja Pantic,"Lipreading has witnessed a lot of progress due to the resurgence of neural networks. Recent works have placed emphasis on aspects such as improving performance by finding the optimal architecture or improving generalization. However, there is still a significant gap between the current methodologies and the requirements for an effective deployment of lipreading in practical scenarios. In this work, we propose a series of innovations that significantly bridge that gap: first, we raise the state-of-the-art performance by a wide margin on LRW and LRW-1000 to 88.5% and 46.6%, respectively using self-distillation. Secondly, we propose a series of architectural changes, including a novel Depthwise Separable Temporal Convolutional Network (DS-TCN) head, that slashes the computational cost to a fraction of the (already quite efficient) original model. Thirdly, we show that knowledge distillation is a very effective tool for recovering performance of the lightweight models. This results in a range of models with different accuracy-efficiency trade-offs. However, our most promising lightweight models are on par with the current state-of-the-art while showing a reduction of 8.2x and 3.9x in terms of computational cost and number of parameters, respectively, which we hope will enable the deployment of lipreading models in practical applications. △ Less","2 June, 2021",https://arxiv.org/pdf/2007.06504
IOCA: High-Speed I/O-Aware LLC Management for Network-Centric Multi-Tenant Platform,Yifan Yuan;Mohammad Alian;Yipeng Wang;Ilia Kurakin;Ren Wang;Charlie Tai;Nam Sung Kim,"In modern server CPUs, last-level cache (LLC) is a critical hardware resource that exerts significant influence on the performance of the workloads, and how to manage LLC is a key to the performance isolation and QoS in the cloud with multi-tenancy. In this paper, we argue that besides CPU cores, high-speed network I/O is also important for LLC management. This is because of an Intel architectural innovation -- Data Direct I/O (DDIO) -- that directly injects the inbound I/O traffic to (part of) the LLC instead of the main memory. We summarize two problems caused by DDIO and show that (1) the default DDIO configuration may not always achieve optimal performance, (2) DDIO can decrease the performance of non-I/O workloads which share LLC with it by as high as 32%. We then present IOCA, the first LLC management mechanism for network-centric platforms that treats the I/O as the first-class citizen. IOCA monitors and analyzes the performance of the cores, LLC, and DDIO using CPU's hardware performance counters, and adaptively adjusts the number of LLC ways for DDIO or the tenants that demand more LLC capacity. In addition, IOCA dynamically chooses the tenants that share its LLC resource with DDIO, to minimize the performance interference by both the tenants and the I/O. Our experiments with multiple microbenchmarks and real-world applications in two major end-host network models demonstrate that IOCA can effectively reduce the performance degradation caused by DDIO, with minimal overhead. △ Less","4 March, 2021",https://arxiv.org/pdf/2007.04552
Coded Computing for Federated Learning at the Edge,Saurav Prakash;Sagar Dhakal;Mustafa Akdeniz;A. Salman Avestimehr;Nageen Himayat,"Federated Learning (FL) is an exciting new paradigm that enables training a global model from data generated locally at the client nodes, without moving client data to a centralized server. Performance of FL in a multi-access edge computing (MEC) network suffers from slow convergence due to heterogeneity and stochastic fluctuations in compute power and communication link qualities across clients. A recent work, Coded Federated Learning (CFL), proposes to mitigate stragglers and speed up training for linear regression tasks by assigning redundant computations at the MEC server. Coding redundancy in CFL is computed by exploiting statistical properties of compute and communication delays. We develop CodedFedL that addresses the difficult task of extending CFL to distributed non-linear regression and classification problems with multioutput labels. The key innovation of our work is to exploit distributed kernel embedding using random Fourier features that transforms the training task into distributed linear regression. We provide an analytical solution for load allocation, and demonstrate significant performance gains for CodedFedL through experiments over benchmark datasets using practical network parameters. △ Less","9 May, 2021",https://arxiv.org/pdf/2007.03273
Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation,Liwei Wang;Jing Huang;Yin Li;Kun Xu;Zhengyuan Yang;Dong Yu,"Weakly supervised phrase grounding aims at learning region-phrase correspondences using only image-sentence pairs. A major challenge thus lies in the missing links between image regions and sentence phrases during training. To address this challenge, we leverage a generic object detector at training time, and propose a contrastive learning framework that accounts for both region-phrase and image-sentence matching. Our core innovation is the learning of a region-phrase score function, based on which an image-sentence score function is further constructed. Importantly, our region-phrase score function is learned by distilling from soft matching scores between the detected object names and candidate phrases within an image-sentence pair, while the image-sentence score function is supervised by ground-truth image-sentence pairs. The design of such score functions removes the need of object detection at test time, thereby significantly reducing the inference cost. Without bells and whistles, our approach achieves state-of-the-art results on visual phrase grounding, surpassing previous methods that require expensive object detectors at test time. △ Less","25 April, 2021",https://arxiv.org/pdf/2007.01951
Reinforcement Learning and its Connections with Neuroscience and Psychology,Ajay Subramanian;Sharad Chitlangia;Veeky Baths,"Reinforcement learning methods have recently been very successful at performing complex sequential tasks like playing Atari games, Go and Poker. These algorithms have outperformed humans in several tasks by learning from scratch, using only scalar rewards obtained through interaction with their environment. While there certainly has been considerable independent innovation to produce such results, many core ideas in reinforcement learning are inspired by phenomena in animal learning, psychology and neuroscience. In this paper, we comprehensively review a large number of findings in both neuroscience and psychology that evidence reinforcement learning as a promising candidate for modeling learning and decision making in the brain. In doing so, we construct a mapping between various classes of modern RL algorithms and specific findings in both neurophysiological and behavioral literature. We then discuss the implications of this observed relationship between RL, neuroscience and psychology and its role in advancing research in both AI and brain science. △ Less","26 September, 2021",https://arxiv.org/pdf/2007.01099
Adversarial Example Games,Avishek Joey Bose;Gauthier Gidel;Hugo Berard;Andre Cianflone;Pascal Vincent;Simon Lacoste-Julien;William L. Hamilton,"The existence of adversarial examples capable of fooling trained neural network classifiers calls for a much better understanding of possible attacks to guide the development of safeguards against them. This includes attack methods in the challenging non-interactive blackbox setting, where adversarial attacks are generated without any access, including queries, to the target model. Prior attacks in this setting have relied mainly on algorithmic innovations derived from empirical observations (e.g., that momentum helps), lacking principled transferability guarantees. In this work, we provide a theoretical foundation for crafting transferable adversarial examples to entire hypothesis classes. We introduce Adversarial Example Games (AEG), a framework that models the crafting of adversarial examples as a min-max game between a generator of attacks and a classifier. AEG provides a new way to design adversarial examples by adversarially training a generator and a classifier from a given hypothesis class (e.g., architecture). We prove that this game has an equilibrium, and that the optimal generator is able to craft adversarial examples that can attack any classifier from the corresponding hypothesis class. We demonstrate the efficacy of AEG on the MNIST and CIFAR-10 datasets, outperforming prior state-of-the-art approaches with an average relative improvement of 29.9\% and 47.2\% against undefended and robust models (Table 2 & 3) respectively. △ Less","8 January, 2021",https://arxiv.org/pdf/2007.00720
Coconut: sortable summarizations for scalable indexes over static and streaming data series,Haridimos Kondylakis;Niv Dayan;Kostas Zoumpatianos;Themis Palpanas,"Many modern applications produce massive streams of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. To address this problem, we present Coconut, the first data series index based on sortable summarizations and the first efficient solution for indexing and querying streaming series. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk loading and updating techniques that rely on sorting to quickly build and maintain a contiguous index using large sequential disk I/Os. We then explore prefix-based and median-based splitting policies for bottom-up bulk loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Finally, we explore the impact of sortable summarizations on variable-sized window queries, showing that they can be supported in the presence of updates through efficient merging of temporal partitions. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs. △ Less","16 April, 2021",https://arxiv.org/pdf/2006.11474
Independent Innovation Analysis for Nonlinear Vector Autoregressive Process,Hiroshi Morioka;Hermanni Hälvä;Aapo Hyvärinen,"The nonlinear vector autoregressive (NVAR) model provides an appealing framework to analyze multivariate time series obtained from a nonlinear dynamical system. However, the innovation (or error), which plays a key role by driving the dynamics, is almost always assumed to be additive. Additivity greatly limits the generality of the model, hindering analysis of general NVAR processes which have nonlinear interactions between the innovations. Here, we propose a new general framework called independent innovation analysis (IIA), which estimates the innovations from completely general NVAR. We assume mutual independence of the innovations as well as their modulation by an auxiliary variable (which is often taken as the time index and simply interpreted as nonstationarity). We show that IIA guarantees the identifiability of the innovations with arbitrary nonlinearities, up to a permutation and component-wise invertible nonlinearities. We also propose three estimation frameworks depending on the type of the auxiliary variable. We thus provide the first rigorous identifiability result for general NVAR, as well as very general tools for learning such models. △ Less","25 February, 2021",https://arxiv.org/pdf/2006.10944
Reinforcement Learning Control of Robotic Knee with Human in the Loop by Flexible Policy Iteration,Xiang Gao;Jennie Si;Yue Wen;Minhan Li;He;Huang,"We are motivated by the real challenges presented in a human-robot system to develop new designs that are efficient at data level and with performance guarantees such as stability and optimality at systems level. Existing approximate/adaptive dynamic programming (ADP) results that consider system performance theoretically are not readily providing practically useful learning control algorithms for this problem; and reinforcement learning (RL) algorithms that address the issue of data efficiency usually do not have performance guarantees for the controlled system. This study fills these important voids by introducing innovative features to the policy iteration algorithm. We introduce flexible policy iteration (FPI), which can flexibly and organically integrate experience replay and supplemental values from prior experience into the RL controller. We show system level performances including convergence of the approximate value function, (sub)optimality of the solution, and stability of the system. We demonstrate the effectiveness of the FPI via realistic simulations of the human-robot system. It is noted that the problem we face in this study may be difficult to address by design methods based on classical control theory as it is nearly impossible to obtain a customized mathematical model of a human-robot system either online or offline. The results we have obtained also indicate the great potential of RL control to solving realistic and challenging problems with high dimensional control inputs. △ Less","17 January, 2021",https://arxiv.org/pdf/2006.09008
Boosting Black-Box Attack with Partially Transferred Conditional Adversarial Distribution,Yan Feng;Baoyuan Wu;Yanbo Fan;Li Liu;Zhifeng Li;Shutao Xia,"This work studies black-box adversarial attacks against deep neural networks (DNNs), where the attacker can only access the query feedback returned by the attacked DNN model, while other information such as model parameters or the training datasets are unknown. One promising approach to improve attack performance is utilizing the adversarial transferability between some white-box surrogate models and the target model (i.e., the attacked model). However, due to the possible differences on model architectures and training datasets between surrogate and target models, dubbed ""surrogate biases"", the contribution of adversarial transferability to improving the attack performance may be weakened. To tackle this issue, we innovatively propose a black-box attack method by developing a novel mechanism of adversarial transferability, which is robust to the surrogate biases. The general idea is transferring partial parameters of the conditional adversarial distribution (CAD) of surrogate models, while learning the untransferred parameters based on queries to the target model, to keep the flexibility to adjust the CAD of the target model on any new benign sample. Extensive experiments on benchmark datasets and attacking against real-world API demonstrate the superior attack performance of the proposed method. △ Less","18 March, 2021",https://arxiv.org/pdf/2006.08538
Human and Multi-Agent collaboration in a human-MARL teaming framework,Neda Navidi;Francoi Chabo;Saga Kurandwa;Iv Lutigma;Vincent Robt;Gregry Szrftgr;Andea Schuh,"Reinforcement learning provides effective results with agents learning from their observations, received rewards, and internal interactions between agents. This study proposes a new open-source MARL framework, called COGMENT, to efficiently leverage human and agent interactions as a source of learning. We demonstrate these innovations by using a designed real-time environment with unmanned aerial vehicles driven by RL agents, collaborating with a human. The results of this study show that the proposed collaborative paradigm and the open-source framework leads to significant reductions in both human effort and exploration costs. △ Less","1 March, 2021",https://arxiv.org/pdf/2006.07301
Map3D: Registration Based Multi-Object Tracking on 3D Serial Whole Slide Images,Ruining Deng;Haichun Yang;Aadarsh Jha;Yuzhe Lu;Peng Chu;Agnes B. Fogo;Yuankai Huo,"There has been a long pursuit for precise and reproducible glomerular quantification on renal pathology to leverage both research and practice. When digitizing the biopsy tissue samples using whole slide imaging (WSI), a set of serial sections from the same tissue can be acquired as a stack of images, similar to frames in a video. In radiology, the stack of images (e.g., computed tomography) are naturally used to provide 3D context for organs, tissues, and tumors. In pathology, it is appealing to do a similar 3D assessment. However, the 3D identification and association of large-scale glomeruli on renal pathology is challenging due to large tissue deformation, missing tissues, and artifacts from WSI. In this paper, we propose a novel Multi-object Association for Pathology in 3D (Map3D) method for automatically identifying and associating large-scale cross-sections of 3D objects from routine serial sectioning and WSI. The innovations of the Map3D method are three-fold: (1) the large-scale glomerular association is formed as a new multi-object tracking (MOT) perspective; (2) the quality-aware whole series registration is proposed to not only provide affinity estimation but also offer automatic kidney-wise quality assurance (QA) for registration; (3) a dual-path association method is proposed to tackle the large deformation, missing tissues, and artifacts during tracking. To the best of our knowledge, the Map3D method is the first approach that enables automatic and large-scale glomerular association across 3D serial sectioning using WSI. Our proposed method Map3D achieved MOTA= 44.6, which is 12.1% higher than the non deep learning benchmarks. △ Less","25 March, 2021",https://arxiv.org/pdf/2006.06038
CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks,Gašper Beguš,"How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN), that combine a Deep Convolutional GAN architecture for audio data (WaveGAN; arXiv:1705.07904) with an information theoretic extension of GAN -- InfoGAN (arXiv:1606.03657), and propose a new latent space structure that can model featural learning simultaneously with a higher level classification and allows for a very low-dimension vector representation of lexical items. Lexical learning is modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from TIMIT learn to encode unique information corresponding to lexical items in the form of categorical variables in their latent space. By manipulating these variables, the network outputs specific lexical items. The network occasionally outputs innovative lexical items that violate training data, but are linguistically interpretable and highly informative for cognitive modeling and neural network interpretability. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on `suit' and `dark' outputs innovative `start', even though it never saw `start' or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. △ Less","28 July, 2021",https://arxiv.org/pdf/2006.02951
Health Risks Associated with 5G Exposure: A View from the Communications Engineering Perspective,Luca Chiaraviglio;Ahmed Elzanaty;Mohamed-Slim Alouini,"The deployment of 5G wireless communication services requires the installation of 5G next-generation Node-B Base Stations (gNBs) over the territory and the wide adoption of 5G User Equipment (UE). In this context, the population is concerned about the potential health risks associated with the Radio Frequency (RF) emissions from 5G equipment, with several communities actively working toward stopping the 5G deployment. To face these concerns, in this work, we analyze the health risks associated with 5G exposure by adopting a new and comprehensive viewpoint, based on the communications engineering perspective. By exploiting our background, we analyze the alleged health effects of 5G exposure and critically review the latest works that are often referenced to support the health concerns from 5G. We then precisely examine the up-to-date metrics, regulations, and assessment of compliance procedures for 5G exposure, by evaluating the latest guidelines from IEEE, ICNIRP, ITU, IEC, and FCC, as well as the national regulations in more than 220 countries. We also thoroughly analyze the main health risks that are frequently associated with specific 5G features (e.g., MIMO, beamforming, cell densification, adoption of millimeter waves, and connection of millions of devices). Finally, we examine the risk mitigation techniques based on communications engineering that can be implemented to reduce the exposure from 5G gNB and UE. Overall, we argue that the widely perceived health risks that are attributed to 5G are not supported by scientific evidence from communications engineering. In addition, we explain how the solutions to minimize the health risks from 5G are already mature and ready to be implemented. Finally, future works, e.g., aimed at evaluating long-term impacts of 5G exposure, as well as innovative solutions to further reduce the RF emissions, are suggested. △ Less","4 August, 2021",https://arxiv.org/pdf/2006.00944
Modular Politics: Toward a Governance Layer for Online Communities,Nathan Schneider;Primavera De Filippi;Seth Frey;Joshua Z. Tan;Amy X. Zhang,"Governance in online communities is an increasingly high-stakes challenge, and yet many basic features of offline governance legacies--juries, political parties, term limits, and formal debates, to name a few--are not in the feature-sets of the software most community platforms use. Drawing on the paradigm of Institutional Analysis and Development, this paper proposes a strategy for addressing this lapse by specifying basic features of a generalizable paradigm for online governance called Modular Politics. Whereas classical governance typologies tend to present a choice among wholesale ideologies, such as democracy or oligarchy, Modular Politics would enable platform operators and their users to build bottom-up governance processes from computational components that are modular and composable, highly versatile in their expressiveness, portable from one context to another, and interoperable across platforms. This kind of approach could implement pre-digital governance systems as well as accelerate innovation in uniquely digital techniques. As diverse communities share and connect their components and data, governance could occur through a ubiquitous network layer. To that end, this paper proposes the development of an open standard for networked governance. △ Less","12 March, 2021",https://arxiv.org/pdf/2005.13701
Better Late than Never; Scaling Computation in Blockchains by Delaying Execution,Sourav Das;Nitin Awathare;Ling Ren;Vinay Joseph Ribeiro;Umesh Bellur,"Proof-of-Work~(PoW) based blockchains typically allocate only a tiny fraction (e.g., less than 1% for Ethereum) of the average interarrival time~(\mathbb{I}) between blocks for validating transactions. A trivial increase in validation time~(τ) introduces the popularly known Verifier's Dilemma, and as we demonstrate, causes more forking and increases unfairness. Large τ also reduces the tolerance for safety against a Byzantine adversary. Solutions that offload validation to a set of non-chain nodes (a.k.a. off-chain approaches) suffer from trust issues that are non-trivial to resolve. In this paper, we present Tuxedo, the first on-chain protocol to theoretically scale τ/\mathbb{I} \approx 1 in PoW blockchains. The key innovation in Tuxedo is to separate the consensus on the ordering of transactions from their execution. We achieve this by allowing miners to delay validation of transactions in a block by up to ζ blocks, where ζ is a system parameter. We perform security analysis of Tuxedo considering all possible adversarial strategies in a synchronous network with end-to-end delay Δ and demonstrate that Tuxedo achieves security equivalent to known results for longest chain PoW Nakamoto consensus. Additionally, we also suggest a principled approach for practical choices of parameter ζ as per the application requirement. Our prototype implementation of Tuxedo atop Ethereum demonstrates that it can scale τ without suffering the harmful effects of naive scaling in existing blockchains. △ Less","2 June, 2021",https://arxiv.org/pdf/2005.11791
PeopleTraffic: a common framework for harmonizing privacy and epidemic risks,Ruggero Caravita,"PeopleTraffic is a proposed initiative to develop a real-time, open-data population density mapping tool open to public institutions, private companies and the civil society, providing a common framework for infection spreading prevention. The system is based on a real-time people' locations gathering and mapping system from available 2G, 3G and 4G mobile networks operators, enforcing privacy-by-design through the adoption of an innovative data anonymizing algorithm inspired by quantum information de-localizing processes. Besides being originally targeted to help balancing social distancing regulations during the Phase-2 of the COVID-19 pandemics, PeopleTraffic would be beneficial for any infection spreading prevention event, e.g. supporting policy-makers in strategic decision-making. △ Less","13 December, 2021",https://arxiv.org/pdf/2005.10061
"Open, Programmable, and Virtualized 5G Networks: State-of-the-Art and the Road Ahead",Leonardo Bonati;Michele Polese;Salvatore D'Oro;Stefano Basagni;Tommaso Melodia,"Fifth generation (5G) cellular networks will serve a wide variety of heterogeneous use cases, including mobile broadband users, ultra-low latency services and massively dense connectivity scenarios. The resulting diverse communication requirements will demand networking with unprecedented flexibility, not currently provided by the monolithic black-box approach of 4G cellular networks. The research community and an increasing number of standardization bodies and industry coalitions have recognized softwarization, virtualization, and disaggregation of networking functionalities as the key enablers of the needed shift to flexibility. Particularly, software-defined cellular networks are heralded as the prime technology to satisfy the new application-driven traffic requirements and to support the highly time-varying topology and interference dynamics, because of their openness through well-defined interfaces, and programmability, for swift and responsive network optimization. Leading the technological innovation in this direction, several 5G software-based projects and alliances have embraced the open source approach, making new libraries and frameworks available to the wireless community. This race to open source softwarization, however, has led to a deluge of solutions whose interoperability and interactions are often unclear. This article provides the first cohesive and exhaustive compendium of recent open source software and frameworks for 5G cellular networks, with a full stack and end-to-end perspective. We detail their capabilities and functionalities focusing on how their constituting elements fit the 5G ecosystem, and unravel the interactions among the surveyed solutions. Finally, we review hardware and testbeds on which these frameworks can run, and discuss the limitations of the state-of-the-art, as well as feasible directions toward fully open source, programmable 5G networks. △ Less","10 February, 2021",https://arxiv.org/pdf/2005.10027
List Decodable Mean Estimation in Nearly Linear Time,Yeshwanth Cherapanamjeri;Sidhanth Mohanty;Morris Yau,"Learning from data in the presence of outliers is a fundamental problem in statistics. Until recently, no computationally efficient algorithms were known to compute the mean of a high dimensional distribution under natural assumptions in the presence of even a small fraction of outliers. In this paper, we consider robust statistics in the presence of overwhelming outliers where the majority of the dataset is introduced adversarially. With only an α< 1/2 fraction of ""inliers"" (clean data) the mean of a distribution is unidentifiable. However, in their influential work, [CSV17] introduces a polynomial time algorithm recovering the mean of distributions with bounded covariance by outputting a succinct list of O(1/α) candidate solutions, one of which is guaranteed to be close to the true distributional mean; a direct analog of 'List Decoding' in the theory of error correcting codes. In this work, we develop an algorithm for list decodable mean estimation in the same setting achieving up to constants the information theoretically optimal recovery, optimal sample complexity, and in nearly linear time up to polylogarithmic factors in dimension. Our conceptual innovation is to design a descent style algorithm on a nonconvex landscape, iteratively removing minima to generate a succinct list of solutions. Our runtime bottleneck is a saddle-point optimization for which we design custom primal dual solvers for generalized packing and covering SDP's under Ky-Fan norms, which may be of independent interest. △ Less","21 January, 2021",https://arxiv.org/pdf/2005.09796
Diversity in News Recommendations,Abraham Bernstein;Claes de Vreese;Natali Helberger;Wolfgang Schulz;Katharina Zweig;Christian Baden;Michael A. Beam;Marc P. Hauer;Lucien Heitz;Pascal Jürgens;Christian Katzenbach;Benjamin Kille;Beate Klimkiewicz;Wiebke Loosen;Judith Moeller;Goran Radanovic;Guy Shani;Nava Tintarev;Suzanne Tolmeijer;Wouter van Atteveldt;Sanne Vrijenhoek;Theresa Zueger,"News diversity in the media has for a long time been a foundational and uncontested basis for ensuring that the communicative needs of individuals and society at large are met. Today, people increasingly rely on online content and recommender systems to consume information challenging the traditional concept of news diversity. In addition, the very concept of diversity, which differs between disciplines, will need to be re-evaluated requiring a interdisciplinary investigation, which requires a new level of mutual cooperation between computer scientists, social scientists, and legal scholars. Based on the outcome of a multidisciplinary workshop, we have the following recommendations, directed at researchers, funders, legislators, regulators, and the media industry: 1. Do more research on news recommenders and diversity. 2. Create a safe harbor for academic research with industry data. 3. Optimize the role of public values in news recommenders. 4. Create a meaningful governance framework. 5. Fund a joint lab to spearhead the needed interdisciplinary research, boost practical innovation, develop. reference solutions, and transfer insights into practice. △ Less","25 May, 2021",https://arxiv.org/pdf/2005.09495
Sham: A DSL for Fast DSLs,Rajan Walia;Chung-chieh Shan;Sam Tobin-Hochstadt,"Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and for transparently replacing DSL support code with faster versions. Sham provides seamless inter-operation with its host language without requiring any additional effort from its users. Sham also provides a framework for defining language syntax which implements Sham's own language interface as well. We validate Sham's design on a series of case studies, ranging from Krishnamurthi's classic automata DSL to a sound synthesis DSL and a probabilistic programming language. All of these are existing DSLs where we replaced the backend using Sham, resulting in major performance gains. We present an example-driven description of how Sham can smoothly enhance an existing DSL into a high-performance one. When compared to existing approaches for implementing high-performance DSLs, Sham's design aims for both simplicity and programmer control. This makes it easier to port our techniques to other languages and frameworks, or borrow Sham's innovations ""à la carte"" without adopting the whole approach. Sham builds a sophisticated and powerful DSL construction toolkit atop fundamental language features including higher-order functions, data structures, and a foreign-function interface (FFI), all readily available in other languages. Furthermore, Sham's approach allows DSL developers to simply write functions, either using Sham or generating Sham, without needing to work through complex staging or partial evaluation systems. △ Less","15 July, 2021",https://arxiv.org/pdf/2005.09028
Complex social contagion induces bistability on multiplex networks,Longzhao Liu;Xin Wang;Shaoting Tang;Hongwei Zheng;Zhiming Zheng,"Social reinforcement mechanism, which characterizes the promoting effects when exposing to multiple sources in social contagion process, is ubiquitous in information-technology ecosystem and has aroused great attention in recent years. While the impacts of social reinforcement on single-layer networks are well-documented, extension to multilayer networks is needed to study how reinforcement from different social circles influences the spreading dynamics. To this end, we incorporate multilayer social reinforcement into ignorant-spreader-ignorant (SIS) model on multiplex networks. Our theoretical analysis combines pairwise method and mean-field theory and agrees well with large-scale simulations. Surprisingly, we find this complex social contagion mechanism triggers the emergence of bistability phenomena, where extinction and outbreak states coexist. In particular, the hysteresis loop of stationary prevalence occurs in this bistable region, explaining why the fight towards the spread of rumors is protracted and difficult in modern society. Further, we show that the final state of bistable regions depends on the initial density of adopters, the critical value of which decreases as the contagion transmissibility or the multilayer reinforcement increases. In particular, we highlight two possible conditions for the outbreak of social contagion: to possess large contagion transmissibility, or to possess large initial density of adopters with strong multilayer reinforcement. Our results unveil the non-negligible power of social reinforcement on multiplex networks, which sheds lights on designing efficient strategies in spreading behaviors such as marketing and promoting innovations. △ Less","24 May, 2021",https://arxiv.org/pdf/2005.00664
Evolution of Semantic Similarity -- A Survey,Dhivya Chandrasekaran;Vijay Mago,"Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. In order to address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network-based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place, for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity. △ Less","30 January, 2021",https://arxiv.org/pdf/2004.13820
Variational Integrator Graph Networks for Learning Energy Conserving Dynamical Systems,Shaan Desai;Marios Mattheakis;Stephen Roberts,"Recent advances show that neural networks embedded with physics-informed priors significantly outperform vanilla neural networks in learning and predicting the long term dynamics of complex physical systems from noisy data. Despite this success, there has only been a limited study on how to optimally combine physics priors to improve predictive performance. To tackle this problem we unpack and generalize recent innovations into individual inductive bias segments. As such, we are able to systematically investigate all possible combinations of inductive biases of which existing methods are a natural subset. Using this framework we introduce Variational Integrator Graph Networks - a novel method that unifies the strengths of existing approaches by combining an energy constraint, high-order symplectic variational integrators, and graph neural networks. We demonstrate, across an extensive ablation, that the proposed unifying framework outperforms existing methods, for data-efficient learning and in predictive accuracy, across both single and many-body problems studied in recent literature. We empirically show that the improvements arise because high order variational integrators combined with a potential energy constraint induce coupled learning of generalized position and momentum updates which can be formalized via the Partitioned Runge-Kutta method. △ Less","16 July, 2021",https://arxiv.org/pdf/2004.13688
DFUC2020: Analysis Towards Diabetic Foot Ulcer Detection,Bill Cassidy;Neil D. Reeves;Pappachan Joseph;David Gillespie;Claire O'Shea;Satyan Rajbhandari;Arun G. Maiya;Eibe Frank;Andrew Boulton;David Armstrong;Bijan Najafi;Justina Wu;Moi Hoon Yap,"Every 20 seconds, a limb is amputated somewhere in the world due to diabetes. This is a global health problem that requires a global solution. The MICCAI challenge discussed in this paper, which concerns the automated detection of diabetic foot ulcers using machine learning techniques, will accelerate the development of innovative healthcare technology to address this unmet medical need. In an effort to improve patient care and reduce the strain on healthcare systems, recent research has focused on the creation of cloud-based detection algorithms. These can be consumed as a service by a mobile app that patients (or a carer, partner or family member) could use themselves at home to monitor their condition and to detect the appearance of a diabetic foot ulcer (DFU). Collaborative work between Manchester Metropolitan University, Lancashire Teaching Hospital and the Manchester University NHS Foundation Trust has created a repository of 4,000 DFU images for the purpose of supporting research toward more advanced methods of DFU detection. Based on a joint effort involving the lead scientists of the UK, US, India and New Zealand, this challenge will solicit original work, and promote interactions between researchers and interdisciplinary collaborations. This paper presents a dataset description and analysis, assessment methods, benchmark algorithms and initial evaluation results. It facilitates the challenge by providing useful insights into state-of-the-art and ongoing research. This grand challenge takes on even greater urgency in a peri and post-pandemic period, where stresses on resource utilization will increase the need for technology that allows people to remain active, healthy and intact in their home. △ Less","24 May, 2021",https://arxiv.org/pdf/2004.11853
Assurance 2.0: A Manifesto,Robin Bloomfield;John Rushby,"System assurance is confronted by significant challenges. Some of these are new, for example, autonomous systems with major functions driven by machine learning and AI, and ultra-rapid system development, while others are the familiar, persistent issues of the need for efficient, effective and timely assurance. Traditional assurance is seen as a brake on innovation and often costly and time consuming. We therefore propose a modernized framework, Assurance 2.0, as an enabler that supports innovation and continuous incremental assurance. Perhaps unexpectedly, it does so by making assurance more rigorous, with increased focus on the reasoning and evidence employed, and explicit identification of defeaters and counterevidence. △ Less","14 January, 2021",https://arxiv.org/pdf/2004.10474
Distributed Inference with Sparse and Quantized Communication,Aritra Mitra;John A. Richards;Saurabh Bagchi;Shreyas Sundaram,"We consider the problem of distributed inference where agents in a network observe a stream of private signals generated by an unknown state, and aim to uniquely identify this state from a finite set of hypotheses. We focus on scenarios where communication between agents is costly, and takes place over channels with finite bandwidth. To reduce the frequency of communication, we develop a novel event-triggered distributed learning rule that is based on the principle of diffusing low beliefs on each false hypothesis. Building on this principle, we design a trigger condition under which an agent broadcasts only those components of its belief vector that have adequate innovation, to only those neighbors that require such information. We prove that our rule guarantees convergence to the true state exponentially fast almost surely despite sparse communication, and that it has the potential to significantly reduce information flow from uninformative agents to informative agents. Next, to deal with finite-precision communication channels, we propose a distributed learning rule that leverages the idea of adaptive quantization. We show that by sequentially refining the range of the quantizers, every agent can learn the truth exponentially fast almost surely, while using just 1 bit to encode its belief on each hypothesis. For both our proposed algorithms, we rigorously characterize the trade-offs between communication-efficiency and the learning rate. △ Less","7 June, 2021",https://arxiv.org/pdf/2004.01302
Bayesian Models Applied to Cyber Security Anomaly Detection Problems,José A. Perusquía;Jim E. Griffin;Cristiano Villa,"Cyber security is an important concern for all individuals, organisations and governments globally. Cyber attacks have become more sophisticated, frequent and dangerous than ever, and traditional anomaly detection methods have been proved to be less effective when dealing with these new classes of cyber threats. In order to address this, both classical and Bayesian models offer a valid and innovative alternative to the traditional signature-based methods, motivating the increasing interest in statistical research that it has been observed in recent years. In this review we provide a description of some typical cyber security challenges, typical types of data and statistical methods, paying special attention to Bayesian approaches for these problems. △ Less","3 June, 2021",https://arxiv.org/pdf/2003.10360
A Trusted and Privacy-preserving Internet of Mobile Energy,Raja Jurdak;Ali Dorri;Mahinda Vilathgamuwa,"The rapid growth in distributed energy sources on power grids leads to increasingly decentralised energy management systems for the prediction of power supply and demand and the dynamic setting of an energy price signal. Within this emerging smart grid paradigm, electric vehicles can serve as consumers, transporters, and providers of energy through two-way charging stations, which highlights a critical feedback loop between the movement patterns of these vehicles and the state of the energy grid. This paper proposes a vision for an Internet of Mobile Energy (IoME), where energy and information flow seamlessly across the power and transport sectors to enhance the grid stability and end user welfare. We identify the key challenges of trust, scalability, and privacy, particularly location and energy linking privacy for EV owners, for realising the IoME vision. We propose an information architecture for IoME that uses scalable blockchain to provide energy data integrity and authenticity, and introduces one-time keys for public EV transactions and a verifiable anonymous trip extraction method for EV users to share their trip data while protecting their location privacy. We present an example scenario that details the seamless and closed loop information flow across the energy and transport sectors, along with a blockchain design and transaction vocabulary for trusted decentralised transactions. We finally discuss the open challenges presented by IoME that can unlock significant benefits to grid stability, innovation, and end user welfare. △ Less","26 January, 2021",https://arxiv.org/pdf/2003.10085
Evolution of diversity and dominance of companies in online activity,Paul X. McCarthy;Xian Gong;Sina Eghbal;Daniel S. Falster;Marian-Andrei Rizoiu,"Ever since the web began, the number of websites has been growing exponentially. These websites cover an ever-increasing range of online services that fill a variety of social and economic functions across a growing range of industries. Yet the networked nature of the web, combined with the economics of preferential attachment, increasing returns and global trade, suggest that over the long run a small number of competitive giants are likely to dominate each functional market segment, such as search, retail and social media. Here we perform a large scale longitudinal study to quantify the distribution of attention given in the online environment to competing organisations. In two large online social media datasets, containing more than 10 billion posts and spanning more than a decade, we tally the volume of external links posted towards the organisations' main domain name as a proxy for the online attention they receive. We also use the Common Crawl dataset -- which contains the linkage patterns between more than a billion different websites -- to study the patterns of link concentration over the past three years across the entire web. Lastly, we showcase the linking between economic, financial and market data by exploring the relationships between online attention on social media and the growth in enterprise value in the electric carmaker Tesla. Our analysis shows that despite the fact that we observe consistent growth in all the macro indicators -- the total amount of online attention, in the number of organisations with an online presence, and in the functions they perform -- we also observe that a smaller number of organisations account for an ever-increasing proportion of total user attention, usually with one large player dominating each function. These results highlight how evolution of the online economy involves innovation, diversity, and then competitive dominance. △ Less","1 April, 2021",https://arxiv.org/pdf/2003.07049
Multivariate time-series modeling with generative neural networks,Marius Hofert;Avinash Prasad;Mu Zhu,"Generative moment matching networks (GMMNs) are introduced as dependence models for the joint innovation distribution of multivariate time series (MTS). Following the popular copula-GARCH approach for modeling dependent MTS data, a framework based on a GMMN-GARCH approach is presented. First, ARMA-GARCH models are utilized to capture the serial dependence within each univariate marginal time series. Second, if the number of marginal time series is large, principal component analysis (PCA) is used as a dimension-reduction step. Last, the remaining cross-sectional dependence is modeled via a GMMN, the main contribution of this work. GMMNs are highly flexible and easy to simulate from, which is a major advantage over the copula-GARCH approach. Applications involving yield curve modeling and the analysis of foreign exchange-rate returns demonstrate the utility of the GMMN-GARCH approach, especially in terms of producing better empirical predictive distributions and making better probabilistic forecasts. △ Less","1 October, 2021",https://arxiv.org/pdf/2002.10645
Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization,Vien V. Mai;Mikael Johansson,"Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries. However, their sample complexities have not been obtained for problems beyond those that are convex or smooth. This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non-smooth, non-convex, and constrained optimization problems. Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tuning of the momentum parameter. For smooth problems, we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state-of-the-art. Numerical results confirm our theoretical developments. △ Less","11 February, 2021",https://arxiv.org/pdf/2002.05466
Road Traffic Poisoning of Navigation Apps: Threats and Countermeasures,Simone Raponi;Savio Sciancalepore;Gabriele Oligeri;Roberto Di Pietro,"Assisted-navigation applications have a relevant impact on our daily life. However, technological progress in virtualization technologies and Software-Defined Radios recently enabled new attack vectors, namely, road traffic poisoning. These attacks open up several dreadful scenarios, which are addressed in this contribution by identifying the associated challenges and proposing innovative countermeasures. △ Less","5 May, 2021",https://arxiv.org/pdf/2002.05051
Chinese E-Romance: Analyzing and Visualizing 7.92 Million Alibaba Valentine's Day Purchases,Yongzhen Wang;Xiaozhong Liu;Yingnan Ju;Katy Börner;Jun Lin;Changlong Sun;Luo Si,"The days that precede Valentine's Day are characterized by extensive gift shopping activities all across the globe. In China, where much shopping takes place online, there has been an explosive growth in e-commerce sales during Valentine's Day over the recent years. This exploratory study investigates the extent to which each product category and each shopper group can exhibit romantic love within China's e-market throughout the 2 weeks leading up to 2019 Valentine's Day. Massive data from Alibaba, the biggest e-commerce retailer worldwide, are utilized to formulate an innovative romance index (RI) to quantitatively measure e-romantic values for products and shoppers. On this basis, millions of shoppers, along with their millions of products purchased around Valentine's Day, are analyzed as a case study to demonstrate their love consumption and romantic gift-giving. The results of the analysis are then illustrated to help understand Chinese e-romance based on the perspectives of different product categories and shopper groups. This empirical information visualization also contributes to improving the segmentation, targeting, and positioning of China's e-market for Valentine's Day. △ Less","20 August, 2021",https://arxiv.org/pdf/2002.03060
A Comprehensive Feature Comparison Study of Open-Source Container Orchestration Frameworks,Eddy Truyen;Dimitri Van Landuyt;Davy Preuveneers;Bert Lagaisse;Wouter Joosen,"(1) Background: Container orchestration frameworks provide support for management of complex distributed applications. Different frameworks have emerged only recently, and they have been in constant evolution as new features are being introduced. This reality makes it difficult for practitioners and researchers to maintain a clear view of the technology space. (2) Methods: we present a descriptive feature comparison study of the three most prominent orchestration frameworks: Docker Swarm, Kubernetes, and Mesos, which can be combined with Marathon, Aurora or DC/OS. This study aims at (i) identifying the common and unique features of all frameworks, (ii) comparing these frameworks qualitatively and quantitatively with respect to genericity in terms of supported features, and (iii) investigating the maturity and stability of the frameworks as well as the pioneering nature of each framework by studying the historical evolution of the frameworks on GitHub. (3) Results: (i) we have identified 124 common features and 54 unique features that we divided into a taxonomy of 9 functional aspects and 27 functional sub-aspects. (ii) Kubernetes supports the highest number of accumulated common and unique features for all 9 functional aspects; however, no evidence has been found for significant differences in genericity with Docker Swarm and DC/OS. (iii) Very little feature deprecations have been found and 15 out of 27 sub-aspects have been identified as mature and stable. These are pioneered in descending order by Kubernetes, Mesos, and Marathon. (4) Conclusion: there is a broad and mature foundation that underpins all container orchestration frameworks. Likely areas for further evolution and innovation include system support for improved cluster security and container security, performance isolation of GPU, disk and network resources, and network plugin architectures. △ Less","5 March, 2021",https://arxiv.org/pdf/2002.02806
Optimal Causal Rate-Constrained Sampling for a Class of Continuous Markov Processes,Nian Guo;Victoria Kostina,"Consider the following communication scenario. An encoder observes a stochastic process and causally decides when and what to transmit about it, under a constraint on the expected number of bits transmitted per second. A decoder uses the received codewords to causally estimate the process in real time. The encoder and the decoder are synchronized in time. For a class of continuous Markov processes satisfying regularity conditions, we find the optimal encoding and decoding policies that minimize the end-to-end estimation mean-square error under the rate constraint. We show that the optimal encoding policy transmits a 1-bit codeword once the process innovation passes one of two thresholds. The optimal decoder noiselessly recovers the last sample from the 1-bit codewords and codeword-generating time stamps, and uses it to decide the running estimate of the current process, until the next codeword arrives. In particular, we show the optimal causal code for the Ornstein-Uhlenbeck process and calculate its distortion-rate function. Furthermore, we show that the optimal causal code also minimizes the mean-square cost of a continuous-time control system driven by a continuous Markov process and controlled by an additive control signal. △ Less","20 September, 2021",https://arxiv.org/pdf/2002.01581
Central Moment Analysis for Cost Accumulators in Probabilistic Programs,Di Wang;Jan Hoffmann;Thomas Reps,"For probabilistic programs, it is usually not possible to automatically derive exact information about their properties, such as the distribution of states at a given program point. Instead, one can attempt to derive approximations, such as upper bounds on tail probabilities. Such bounds can be obtained via concentration inequalities, which rely on the moments of a distribution, such as the expectation (the first raw moment) or the variance (the second central moment). Tail bounds obtained using central moments are often tighter than the ones obtained using raw moments, but automatically analyzing central moments is more challenging. This paper presents an analysis for probabilistic programs that automatically derives symbolic upper and lower bounds on variances, as well as higher central moments, of cost accumulators. To overcome the challenges of higher-moment analysis, it generalizes analyses for expectations with an algebraic abstraction that simultaneously analyzes different moments, utilizing relations between them. A key innovation is the notion of moment-polymorphic recursion, and a practical derivation system that handles recursive functions. The analysis has been implemented using a template-based technique that reduces the inference of polynomial bounds to linear programming. Experiments with our prototype central-moment analyzer show that, despite the analyzer's upper/lower bounds on various quantities, it obtains tighter tail bounds than an existing system that uses only raw moments, such as expectations. △ Less","8 April, 2021",https://arxiv.org/pdf/2001.10150
"Fundamental Limits of Prediction, Generalization, and Recursion: An Entropic-Innovations Perspective",Song Fang;Quanyan Zhu,"In this paper, we examine the fundamental performance limits of prediction, with or without side information. More specifically, we derive generic lower bounds on the \mathcal{L}_p norms of the prediction errors that are valid for any prediction algorithms and for any data distributions. Meanwhile, we combine the entropic analysis from information theory and the innovations approach from prediction/estimation theory to characterize the conditions (in terms of, e.g., directed information or mutual information) to achieve the bounds. We also investigate the implications of the results in analyzing the fundamental limits of generalization in fitting (learning) problems from the perspective of prediction with side information, as well as the fundamental limits of recursive algorithms by viewing them as generalized prediction problems. △ Less","3 June, 2021",https://arxiv.org/pdf/2001.03813
VulDeeLocator: A Deep Learning-based Fine-grained Vulnerability Detector,Zhen Li;Deqing Zou;Shouhuai Xu;Zhaoxuan Chen;Yawei Zhu;Hai Jin,"Automatically detecting software vulnerabilities is an important problem that has attracted much attention from the academic research community. However, existing vulnerability detectors still cannot achieve the vulnerability detection capability and the locating precision that would warrant their adoption for real-world use. In this paper, we present a vulnerability detector that can simultaneously achieve a high detection capability and a high locating precision, dubbed Vulnerability Deep learning-based Locator (VulDeeLocator). In the course of designing VulDeeLocator, we encounter difficulties including how to accommodate semantic relations between the definitions of types as well as macros and their uses across files, how to accommodate accurate control flows and variable define-use relations, and how to achieve high locating precision. We solve these difficulties by using two innovative ideas: (i) leveraging intermediate code to accommodate extra semantic information, and (ii) using the notion of granularity refinement to pin down locations of vulnerabilities. When applied to 200 files randomly selected from three real-world software products, VulDeeLocator detects 18 confirmed vulnerabilities (i.e., true-positives). Among them, 16 vulnerabilities correspond to known vulnerabilities; the other two are not reported in the National Vulnerability Database (NVD) but have been ""silently"" patched by the vendor of Libav when releasing newer versions. △ Less","1 May, 2021",https://arxiv.org/pdf/2001.02350
Deep Innovation Protection: Confronting the Credit Assignment Problem in Training Heterogeneous Neural Architectures,Sebastian Risi;Kenneth O. Stanley,"Deep reinforcement learning approaches have shown impressive results in a variety of different domains, however, more complex heterogeneous architectures such as world models require the different neural components to be trained separately instead of end-to-end. While a simple genetic algorithm recently showed end-to-end training is possible, it failed to solve a more complex 3D task. This paper presents a method called Deep Innovation Protection (DIP) that addresses the credit assignment problem in training complex heterogenous neural network models end-to-end for such environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in multi-component network, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn to predict properties important for the survival of the agent, without the need for a specific forward-prediction loss. △ Less","23 February, 2021",https://arxiv.org/pdf/2001.01683
Normalization of breast MRIs using Cycle-Consistent Generative Adversarial Networks,Gourav Modanwal;Adithya Vellal;Maciej A. Mazurowski,"Dynamic Contrast Enhanced-Magnetic Resonance Imaging (DCE-MRI) is widely used to complement ultrasound examinations and x-ray mammography during the early detection and diagnosis of breast cancer. However, images generated by various MRI scanners (e.g. GE Healthcare vs Siemens) differ both in intensity and noise distribution, preventing algorithms trained on MRIs from one scanner to generalize to data from other scanners successfully. We propose a method for image normalization to solve this problem. MRI normalization is challenging because it requires both normalizing intensity values and mapping between the noise distributions of different scanners. We utilize a cycle-consistent generative adversarial network to learn a bidirectional mapping between MRIs produced by GE Healthcare and Siemens scanners. This allows us learning the mapping between two different scanner types without matched data, which is not commonly available. To ensure the preservation of breast shape and structures within the breast, we propose two technical innovations. First, we incorporate a mutual information loss with the CycleGAN architecture to ensure that the structure of the breast is maintained. Second, we propose a modified discriminator architecture which utilizes a smaller field-of-view to ensure the preservation of finer details in the breast tissue. Quantitative and qualitative evaluations show that the second proposed method was able to consistently preserve a high level of detail in the breast structure while also performing the proper intensity normalization and noise mapping. Our results demonstrate that the proposed model can successfully learn a bidirectional mapping between MRIs produced by different vendors, potentially enabling improved accuracy of downstream computational algorithms for diagnosis and detection of breast cancer. All the data used in this study are publicly available. △ Less","17 June, 2021",https://arxiv.org/pdf/1912.08061
"Fundamental Limitations in Sequential Prediction and Recursive Algorithms: \mathcal{L}_{p}
Bounds via an Entropic Analysis",Song Fang;Quanyan Zhu,"In this paper, we obtain fundamental \mathcal{L}_{p} bounds in sequential prediction and recursive algorithms via an entropic analysis. Both classes of problems are examined by investigating the underlying entropic relationships of the data and/or noises involved, and the derived lower bounds may all be quantified in a conditional entropy characterization. We also study the conditions to achieve the generic bounds from an innovations' viewpoint. △ Less","11 May, 2021",https://arxiv.org/pdf/1912.02628
Driver perceptions of advanced driver assistance systems and safety,Sophie Le Page;Jason Millar;Kelly Bronson;Shalaleh Rismani;AJung Moon,"Advanced driver assistance systems (ADAS) are often used in the automotive industry to highlight innovative improvements in vehicle safety. However, today it is unclear whether certain automation (e.g., adaptive cruise control, lane keeping, parking assist) increases safety of our roads. In this paper, we investigate driver awareness, use, perceived safety, knowledge, training, and attitudes toward ADAS with different automation systems/features. Results of our online survey (n=1018) reveal that there is a significant difference in frequency of use and perceived safety for different ADAS features. Furthermore, we find that at least 70% of drivers activate an ADAS feature ""most or all of the time"" when driving, yet we find that at least 40% of drivers report feeling that ADAS often compromises their safety when activated. We also find that most respondents learn how to use ADAS in their vehicles by trying it out on the road by themselves, rather than through any formal driver education and training. These results may mirror how certain ADAS features are often activated by default resulting in high usage rates. These results also suggest a lack of driver training and education for safely interacting with, and operating, ADAS, such as turning off systems/features. These findings contribute to a critical discussion about the overall safety implications of current ADAS, especially as they enable higher-level automation features to creep into personal vehicles without a lockstep response in training, regulation, and policy. △ Less","23 September, 2021",https://arxiv.org/pdf/1911.10920
"Can We Benchmark Code Review Studies? A Systematic Mapping Study of Methodology, Dataset, and Metric",Dong Wang;Yuki Ueda;Raula Gaikovina Kula;Takashi Ishio;Kenichi Matsumoto,"Code Review (CR) is the cornerstone for software quality assurance and a crucial practice for software development. As CR research matures, it can be difficult to keep track of the best practices and state-of-the-art in methodology, dataset, and metric. This paper investigates the potential of benchmarking by collecting methodology, dataset, and metric of CR studies. A systematic mapping study was conducted. A total of 112 studies from 19,847 papers published in high-impact venues between the years 2011 and 2019 were selected and analyzed. First, we find that empirical evaluation is the most common methodology (65% of papers), with solution and experience being the least common methodology. Second, we highlight 50% of papers that use the quantitative method or mixed-method have the potential for replicability. Third, we identify 457 metrics that are grouped into sixteen core metric sets, applied to nine Software Engineering topics, showing different research topics tend to use specific metric sets. We conclude that at this stage, we cannot benchmark CR studies. Nevertheless, a common benchmark will facilitate new researchers, including experts from other fields, to innovate new techniques and build on top of already established methodologies. A full replication is available at https://naist-se.github.io/code-review/. △ Less","13 April, 2021",https://arxiv.org/pdf/1911.08816
Rotation Invariant Point Cloud Classification: Where Local Geometry Meets Global Topology,Chen Zhao;Jiaqi Yang;Xin Xiong;Angfan Zhu;Zhiguo Cao;Xin Li,"Point cloud analysis is a fundamental task in 3D computer vision. Most previous works have conducted experiments on synthetic datasets with well-aligned data; while real-world point clouds are often not pre-aligned. How to achieve rotation invariance remains an open problem in point cloud analysis. To meet this challenge, we propose a novel approach toward achieving rotation-invariant (RI) representations by combining local geometry with global topology. In our local-global-representation (LGR)-Net, we have designed a two-branch network where one stream encodes local geometric RI features and the other encodes global topology-preserving RI features. Motivated by the observation that local geometry and global topology have different yet complementary RI responses in varying regions, two-branch RI features are fused by an innovative multi-layer perceptron (MLP) based attention module. To the best of our knowledge, this work is the first principled approach toward adaptively combining global and local information under the context of RI point cloud analysis. Extensive experiments have demonstrated that our LGR-Net achieves the state-of-the-art performance on various rotation-augmented versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS. △ Less","1 June, 2021",https://arxiv.org/pdf/1911.00195
A perspective on Microscopy Metadata: data provenance and quality control,Maximiliaan Huisman;Mathias Hammer;Alex Rigano;Ulrike Boehm;James J. Chambers;Nathalie Gaudreault;Alison J. North;Jaime A. Pimentel;Damir Sudar;Peter Bajcsy;Claire M. Brown;Alexander D. Corbett;Orestis Faklaris;Judith Lacoste;Alex Laude;Glyn Nelson;Roland Nitschke;David Grunwald;Caterina Strambio-De-Castillia,"The application of microscopy in biomedical research has come a long way since Antonie van Leeuwenhoek discovered unicellular organisms. Countless innovations have positioned light microscopy as a cornerstone of modern biology and a method of choice for connecting omics datasets to their biological and clinical correlates. Still, regardless of how convincing published imaging data looks, it does not always convey meaningful information about the conditions in which it was acquired, processed, and analyzed. Adequate record-keeping, reporting, and quality control are therefore essential to ensure experimental rigor and data fidelity, allow experiments to be reproducibly repeated, and promote the proper evaluation, interpretation, comparison, and re-use. To this end, microscopy images should be accompanied by complete descriptions detailing experimental procedures, biological samples, microscope hardware specifications, image acquisition parameters, and image analysis procedures, as well as metrics accounting for instrument performance and calibration. However, universal, community-accepted Microscopy Metadata standards and reporting specifications that would result in Findable Accessible Interoperable and Reproducible (FAIR) microscopy data have not yet been established. To understand this shortcoming and to propose a way forward, here we provide an overview of the nature of microscopy metadata and its importance for fostering data quality, reproducibility, scientific rigor, and sharing value in light microscopy. The proposal for tiered Microscopy Metadata Specifications that extend the OME Data Model put forth by the 4D Nucleome Initiative and by Bioimaging North America [1-3] as well as a suite of three complementary and interoperable tools are being developed to facilitate the process of image data documentation and are presented in related manuscripts [4-6]. △ Less","31 May, 2021",https://arxiv.org/pdf/1910.11370
Efficient Stochastic Programming in Julia,Martin Biel;Mikael Johansson,"We present StochasticPrograms.jl, a user-friendly and powerful open-source framework for stochastic programming written in the Julia language. The framework includes both modeling tools and structure-exploiting optimization algorithms. Stochastic programming models can be efficiently formulated using expressive syntax and models can be instantiated, inspected, and analyzed interactively. The framework scales seamlessly to distributed environments. Small instances of a model can be run locally to ensure correctness, while larger instances are automatically distributed in a memory-efficient way onto supercomputers or clouds and solved using parallel optimization algorithms. These structure-exploiting solvers are based on variations of the classical L-shaped and progressive-hedging algorithms. We provide a concise mathematical background for the various tools and constructs available in the framework, along with code listings exemplifying their usage. Both software innovations related to the implementation of the framework and algorithmic innovations related to the structured solvers are highlighted. We conclude by demonstrating strong scaling properties of the distributed algorithms on numerical benchmarks in a multi-node setup. △ Less","12 January, 2021",https://arxiv.org/pdf/1909.10451
Voting with Random Classifiers (VORACE): Theoretical and Experimental Analysis,Cristina Cornelio;Michele Donini;Andrea Loreggia;Maria Silvia Pini;Francesca Rossi,"In many machine learning scenarios, looking for the best classifier that fits a particular dataset can be very costly in terms of time and resources. Moreover, it can require deep knowledge of the specific domain. We propose a new technique which does not require profound expertise in the domain and avoids the commonly used strategy of hyper-parameter tuning and model selection. Our method is an innovative ensemble technique that uses voting rules over a set of randomly-generated classifiers. Given a new input sample, we interpret the output of each classifier as a ranking over the set of possible classes. We then aggregate these output rankings using a voting rule, which treats them as preferences over the classes. We show that our approach obtains good results compared to the state-of-the-art, both providing a theoretical analysis and an empirical evaluation of the approach on several datasets. △ Less","24 May, 2021",https://arxiv.org/pdf/1909.08996
Is culture related to strong science? An empirical investigation,Mahmood Khosrowjerdi;Lutz Bornmann,"National culture is among those societal factors which could influence research and innovation activities. In this study, we investigated the associations of two national culture models with citation impact of nations (measured by the proportion of papers belonging to the 10% and 1% most cited papers in the corresponding fields, PPtop 10% and PPtop 1%). Bivariate statistical analyses showed that of six Hofstede's national culture dimensions (HNCD), uncertainty avoidance and power distance had a statistically significant negative associa-tion, while individualism and indulgence had a statistically significant positive associationwith both citation impact indicators. The study also revealed that of two Inglehart-Welzel cultural values (IWCV), the value survival versus self-expression is statistically significantly related to citation impact indicators. We additionally calculated multiple regression analyses controlling for the possible effects of confounding factors including national self-citations, international co-authorships, invest-ments in research and development, international migrant stock, number of researchers ofeach nation, language, and productivity. The results revealed that the statistically significant associations of HNCD with citation impact indicators disappeared. But the statistically significant relationship between survivals versus self-expression values and citation impact indicators remained stable even after controlling for the confounding variables. Thus, the freedom of expression and trust in society might contribute to better scholarly communication systems, higher level of international collaborations, and further quality research. △ Less","9 April, 2021",https://arxiv.org/pdf/1909.04521
Position-Aware Self-Attention based Neural Sequence Labeling,Wei Wei;Zanbo Wang;Xianling Mao;Guangyou Zhou;Pan Zhou;Sheng Jiang,"Sequence labeling is a fundamental task in natural language processing and has been widely studied. Recently, RNN-based sequence labeling models have increasingly gained attentions. Despite superior performance achieved by learning the long short-term (i.e., successive) dependencies, the way of sequentially processing inputs might limit the ability to capture the non-continuous relations over tokens within a sentence. To tackle the problem, we focus on how to effectively model successive and discrete dependencies of each token for enhancing the sequence labeling performance. Specifically, we propose an innovative attention-based model (called position-aware selfattention, i.e., PSA) as well as a well-designed self-attentional context fusion layer within a neural network architecture, to explore the positional information of an input sequence for capturing the latent relations among tokens. Extensive experiments on three classical tasks in sequence labeling domain, i.e., partof-speech (POS) tagging, named entity recognition (NER) and phrase chunking, demonstrate our proposed model outperforms the state-of-the-arts without any external knowledge, in terms of various metrics. △ Less","16 October, 2021",https://arxiv.org/pdf/1908.09128
DynaNet: Neural Kalman Dynamical Model for Motion Estimation and Prediction,Changhao Chen;Chris Xiaoxuan Lu;Bing Wang;Niki Trigoni;Andrew Markham,"Dynamical models estimate and predict the temporal evolution of physical systems. State Space Models (SSMs) in particular represent the system dynamics with many desirable properties, such as being able to model uncertainty in both the model and measurements, and optimal (in the Bayesian sense) recursive formulations e.g. the Kalman Filter. However, they require significant domain knowledge to derive the parametric form and considerable hand-tuning to correctly set all the parameters. Data driven techniques e.g. Recurrent Neural Networks have emerged as compelling alternatives to SSMs with wide success across a number of challenging tasks, in part due to their ability to extract relevant features from rich inputs. They however lack interpretability and robustness to unseen conditions. In this work, we present DynaNet, a hybrid deep learning and time-varying state-space model which can be trained end-to-end. Our neural Kalman dynamical model allows us to exploit the relative merits of each approach. We demonstrate state-of-the-art estimation and prediction on a number of physically challenging tasks, including visual odometry, sensor fusion for visual-inertial navigation and pendulum control. In addition we show how DynaNet can indicate failures through investigation of properties such as the rate of innovation (Kalman Gain). △ Less","11 September, 2021",https://arxiv.org/pdf/1908.03918
GraphBLAST: A High-Performance Linear Algebra-based Graph Framework on the GPU,Carl Yang;Aydin Buluc;John D. Owens,"High-performance implementations of graph algorithms are challenging to implement on new parallel hardware such as GPUs because of three challenges: (1) the difficulty of coming up with graph building blocks, (2) load imbalance on parallel hardware, and (3) graph problems having low arithmetic intensity. To address some of these challenges, GraphBLAS is an innovative, on-going effort by the graph analytics community to propose building blocks based on sparse linear algebra, which will allow graph algorithms to be expressed in a performant, succinct, composable and portable manner. In this paper, we examine the performance challenges of a linear-algebra-based approach to building graph frameworks and describe new design principles for overcoming these bottlenecks. Among the new design principles is exploiting input sparsity, which allows users to write graph algorithms without specifying push and pull direction. Exploiting output sparsity allows users to tell the backend which values of the output in a single vectorized computation they do not want computed. Load-balancing is an important feature for balancing work amongst parallel workers. We describe the important load-balancing features for handling graphs with different characteristics. The design principles described in this paper have been implemented in ""GraphBLAST"", the first high-performance linear algebra-based graph framework on NVIDIA GPUs that is open-source. The results show that on a single GPU, GraphBLAST has on average at least an order of magnitude speedup over previous GraphBLAS implementations SuiteSparse and GBTL, comparable performance to the fastest GPU hardwired primitives and shared-memory graph frameworks Ligra and Gunrock, and better performance than any other GPU graph framework, while offering a simpler and more concise programming model. △ Less","14 June, 2021",https://arxiv.org/pdf/1908.01407
EnlightenGAN: Deep Light Enhancement without Paired Supervision,Yifan Jiang;Xinyu Gong;Ding Liu;Yu Cheng;Chen Fang;Xiaohui Shen;Jianchao Yang;Pan Zhou;Zhangyang Wang,"Deep learning-based methods have achieved remarkable success in image restoration and enhancement, but are they still competitive when there is a lack of paired training data? As one such example, this paper explores the low-light image enhancement problem, where in practice it is extremely challenging to simultaneously take a low-light and a normal-light photo of the same visual scene. We propose a highly effective unsupervised generative adversarial network, dubbed EnlightenGAN, that can be trained without low/normal-light image pairs, yet proves to generalize very well on various real-world test images. Instead of supervising the learning using ground truth data, we propose to regularize the unpaired training using the information extracted from the input itself, and benchmark a series of innovations for the low-light image enhancement problem, including a global-local discriminator structure, a self-regularized perceptual loss fusion, and attention mechanism. Through extensive experiments, our proposed approach outperforms recent methods under a variety of metrics in terms of visual quality and subjective user study. Thanks to the great flexibility brought by unpaired training, EnlightenGAN is demonstrated to be easily adaptable to enhancing real-world images from various domains. The code is available at \url{https://github.com/yueruchen/EnlightenGAN} △ Less","24 January, 2021",https://arxiv.org/pdf/1906.06972
Privacy-Preserving Deep Action Recognition: An Adversarial Learning Framework and A New Dataset,Zhenyu Wu;Haotao Wang;Zhaowen Wang;Hailin Jin;Zhangyang Wang,"We investigate privacy-preserving, video-based action recognition in deep learning, a problem with growing importance in smart camera applications. A novel adversarial training framework is formulated to learn an anonymization transform for input videos such that the trade-off between target utility task performance and the associated privacy budgets is explicitly optimized on the anonymized videos. Notably, the privacy budget, often defined and measured in task-driven contexts, cannot be reliably indicated using any single model performance because strong protection of privacy should sustain against any malicious model that tries to steal private information. To tackle this problem, we propose two new optimization strategies of model restarting and model ensemble to achieve stronger universal privacy protection against any attacker models. Extensive experiments have been carried out and analyzed. On the other hand, given few public datasets available with both utility and privacy labels, the data-driven (supervised) learning cannot exert its full power on this task. We first discuss an innovative heuristic of cross-dataset training and evaluation, enabling the use of multiple single-task datasets (one with target task labels and the other with privacy labels) in our problem. To further address this dataset challenge, we have constructed a new dataset, termed PA-HMDB51, with both target task labels (action) and selected privacy attributes (skin color, face, gender, nudity, and relationship) annotated on a per-frame basis. This first-of-its-kind video dataset and evaluation protocol can greatly facilitate visual privacy research and open up other opportunities. Our codes, models, and the PA-HMDB51 dataset are available at https://github.com/VITA-Group/PA-HMDB51. △ Less","21 March, 2021",https://arxiv.org/pdf/1906.05675
Heterogeneous causal effects with imperfect compliance: a Bayesian machine learning approach,Falco J. Bargagli-Stoffi;Kristof De-Witte;Giorgio Gnecco,"This paper introduces an innovative Bayesian machine learning algorithm to draw interpretable inference on heterogeneous causal effects in the presence of imperfect compliance (e.g., under an irregular assignment mechanism). We show, through Monte Carlo simulations, that the proposed Bayesian Causal Forest with Instrumental Variable (BCF-IV) methodology outperforms other machine learning techniques tailored for causal inference in discovering and estimating the heterogeneous causal effects while controlling for the familywise error rate (or - less stringently - for the false discovery rate) at leaves' level. BCF-IV sheds a light on the heterogeneity of causal effects in instrumental variable scenarios and, in turn, provides the policy-makers with a relevant tool for targeted policies. Its empirical application evaluates the effects of additional funding on students' performances. The results indicate that BCF-IV could be used to enhance the effectiveness of school funding on students' performance. △ Less","30 November, 2021",https://arxiv.org/pdf/1905.12707
ROI Maximization in Stochastic Online Decision-Making,Nicolò Cesa-Bianchi;Tommaso Cesari;Yishay Mansour;Vianney Perchet,"We introduce a novel theoretical framework for Return On Investment (ROI) maximization in repeated decision-making. Our setting is motivated by the use case of companies that regularly receive proposals for technological innovations and want to quickly decide whether they are worth implementing. We design an algorithm for learning ROI-maximizing decision-making policies over a sequence of innovation proposals. Our algorithm provably converges to an optimal policy in class Π at a rate of order \min\big\{1/(NΔ^2),N^{-1/3}\}, where N is the number of innovations and Δ is the suboptimality gap in Π. A significant hurdle of our formulation, which sets it aside from other online learning problems such as bandits, is that running a policy does not provide an unbiased estimate of its performance. △ Less","22 December, 2021",https://arxiv.org/pdf/1905.11797
Generic Variance Bounds on Estimation and Prediction Errors in Time Series Analysis: An Entropy Perspective,Song Fang;Mikael Skoglund;Karl Henrik Johansson;Hideaki Ishii;Quanyan Zhu,"In this paper, we obtain generic bounds on the variances of estimation and prediction errors in time series analysis via an information-theoretic approach. It is seen in general that the error bounds are determined by the conditional entropy of the data point to be estimated or predicted given the side information or past observations. Additionally, we discover that in order to achieve the prediction error bounds asymptotically, the necessary and sufficient condition is that the ""innovation"" is asymptotically white Gaussian. When restricted to Gaussian processes and 1-step prediction, our bounds are shown to reduce to the Kolmogorov-Szegö formula and Wiener-Masani formula known from linear prediction theory. △ Less","11 May, 2021",https://arxiv.org/pdf/1904.04765
TaDA Live: Compositional Reasoning for Termination of Fine-grained Concurrent Programs,Emanuele D'Osualdo;Azadeh Farzan;Philippa Gardner;Julian Sutherland,"We present TaDA Live, a concurrent separation logic for reasoning compositionally about the termination of blocking fine-grained concurrent programs. The crucial challenge is how to deal with abstract atomic blocking: that is, abstract atomic operations that have blocking behaviour arising from busy-waiting patterns as found in, for example, fine-grained spin locks. Our fundamental innovation is with the design of abstract specifications that capture this blocking behaviour as liveness assumptions on the environment. We design a logic that can reason about the termination of clients which use such operations without breaking their abstraction boundaries, and the correctness of the implementations of the operations with respect to their abstract specifications. We introduce a novel semantic model using layered subjective obligations to express liveness invariants, and a proof system that is sound with respect to the model. The subtlety of our specifications and reasoning is illustrated using several case studies. △ Less","29 November, 2021",https://arxiv.org/pdf/1901.05750
On a Sparse Shortcut Topology of Artificial Neural Networks,Fenglei Fan;Dayang Wang;Hengtao Guo;Qikui Zhu;Pingkun Yan;Ge Wang;Hengyong Yu,"In established network architectures, shortcut connections are often used to take the outputs of earlier layers as additional inputs to later layers. Despite the extraordinary effectiveness of shortcuts, there remain open questions on the mechanism and characteristics. For example, why are shortcuts powerful? Why do shortcuts generalize well? In this paper, we investigate the expressivity and generalizability of a novel sparse shortcut topology. First, we demonstrate that this topology can empower a one-neuron-wide deep network to approximate any univariate continuous function. Then, we present a novel width-bounded universal approximator in contrast to depth-bounded universal approximators and extend the approximation result to a family of equally competent networks. Furthermore, with generalization bound theory, we show that the proposed shortcut topology enjoys excellent generalizability. Finally, we corroborate our theoretical analyses by comparing the proposed topology with popular architectures, including ResNet and DenseNet, on well-known benchmarks and perform a saliency map analysis to interpret the proposed topology. Our work helps enhance the understanding of the role of shortcuts and suggests further opportunities to innovate neural architectures. △ Less","11 November, 2021",https://arxiv.org/pdf/1811.09003
Towards an understanding of CNNs: analysing the recovery of activation pathways via Deep Convolutional Sparse Coding,Michael Murray;Jared Tanner,"Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep convolutional neural networks (DCNNs), but by omitting the learning of the dictionaries one can more transparently analyse the role of the activation function and its ability to recover activation paths through the layers. Papyan, Romano, and Elad conducted an analysis of such an architecture, demonstrated the relationship with DCNNs and proved conditions under which the D-CSC is guaranteed to recover specific activation paths. A technical innovation of their work highlights that one can view the efficacy of the ReLU nonlinear activation function of a DCNN through a new variant of the tensor's sparsity, referred to as stripe-sparsity. Using this they proved that representations with an activation density proportional to the ambient dimension of the data are recoverable. We extend their uniform guarantees to a modified model and prove that with high probability the true activation is typically possible to recover for a greater density of activations per layer. Our extension follows from incorporating the prior work on one step thresholding by Schnass and Vandergheynst. △ Less","1 June, 2021",https://arxiv.org/pdf/1806.09888
Learning 3D Object Categories by Looking Around Them,David Novotny;Diane Larlus;Andrea Vedaldi,"Traditional approaches for learning 3D object categories use either synthetic data or manual supervision. In this paper, we propose a method which does not require manual annotations and is instead cued by observing objects from a moving vantage point. Our system builds on two innovations: a Siamese viewpoint factorization network that robustly aligns different videos together without explicitly comparing 3D shapes; and a 3D shape completion network that can extract the full shape of an object from partial observations. We also demonstrate the benefits of configuring networks to perform probabilistic predictions as well as of geometry-aware data augmentation schemes. We obtain state-of-the-art results on publicly-available benchmarks. △ Less","2 December, 2021",https://arxiv.org/pdf/1705.03951
An NFC-Enabled Anti-Counterfeiting System for Wine Industry,Neo C. K. Yiu,"Wine counterfeiting has been posing significant challenges to wine industry, and has undermined the international wine trading market and the global economy hugely. The situation of counterfeiting has even been exacerbating in wine industry and global supply chain. There has been a number of anti-counterfeiting approaches which have been proposed and adopted utilizing different authentication technologies, in response to growing threats of counterfeiting to wine industry. The proposed NFC-Enabled Anti-Counterfeiting System (NAS) is developed for luxury-good industry such as wine industry, aiming at upholding provenance and authenticity of wine products from counterfeits via the product pedigree, transaction records and supply chain integrity maintained along the supply chain. Consumers can therefore safeguard their stake by authenticating a specific wine product with their NFC-enabled smartphones before purchasing at the retail points. NAS utilizes Near-field Communication (NFC), which has emerged as a promising technology and communication protocol for developing innovative alternatives, to facilitate the wine record processing of wine products and in turn combat wine and spirit counterfeiting. The integrated NAS is consisted of a wide range of hardware and software components, and the best combination of settings, parameters and deployments will therefore be identified. Other possible implementation issues, such as tag selection, tag programming and encryption, setup of back-end database servers and the design of NFC mobile application will also be discussed in this project. The critical design of NAS is vital not only to the key of product anti-counterfeiting of wine industry, but also to the strong foundation for other innovative supply chain solutions, such as the NFC-enabled purchasing system, developed on top of NAS with improved and integrated anti-counterfeiting functionalities. △ Less","28 January, 2021",https://arxiv.org/pdf/1601.06372
FS^3: A Sampling based method for top-k Frequent Subgraph Mining,Tanay Kumar Saha;Mohammad Al Hasan,"Mining labeled subgraph is a popular research task in data mining because of its potential application in many different scientific domains. All the existing methods for this task explicitly or implicitly solve the subgraph isomorphism task which is computationally expensive, so they suffer from the lack of scalability problem when the graphs in the input database are large. In this work, we propose FS^3, which is a sampling based method. It mines a small collection of subgraphs that are most frequent in the probabilistic sense. FS^3 performs a Markov Chain Monte Carlo (MCMC) sampling over the space of a fixed-size subgraphs such that the potentially frequent subgraphs are sampled more often. Besides, FS^3 is equipped with an innovative queue manager. It stores the sampled subgraph in a finite queue over the course of mining in such a manner that the top-k positions in the queue contain the most frequent subgraphs. Our experiments on database of large graphs show that FS^3 is efficient, and it obtains subgraphs that are the most frequent amongst the subgraphs of a given size. △ Less","3 May, 2021",https://arxiv.org/pdf/1409.1152
Innovation éducative en sciences de l'information,Enrique Wulff,"Concerning its development in the virtual classroom, the web 2.0 educational innovation means the use and the production of textbooks and the personalisation of the classnotes. The controversy, that is a precondition of awareness, organized around the assignment of knowledge to a central authority vs its grant to individuals who need it to share their plans with others, would meet the present dynamism of e-learning. To introduce these training strategies with scientific information in marine sciences, an online course was transformed into an opportunity for evaluating and living open access. △ Less","22 March, 2021",https://arxiv.org/pdf/1311.0804
