title,authors,abstract,submitted_date,pdf_link
Autonomous discovery of battery electrolytes with robotic experimentation and machine-learning,Adarsh Dave;Jared Mitchell;Kirthevasan Kandasamy;Sven Burke;Biswajit Paria;Barnabas Poczos;Jay Whitacre;Venkatasubramanian Viswanathan,"Innovations in batteries take years to formulate and commercialize, requiring extensive experimentation during the design and optimization phases. We approached the design and selection of a battery electrolyte through a black-box optimization algorithm directly integrated into a robotic test-stand. We report here the discovery of a novel battery electrolyte by this experiment completely guided by the machine-learning software without human intervention. Motivated by the recent trend toward super-concentrated aqueous electrolytes for high-performance batteries, we utilize Dragonfly - a Bayesian machine-learning software package - to search mixtures of commonly used lithium and sodium salts for super-concentrated aqueous electrolytes with wide electrochemical stability windows. Dragonfly autonomously managed the robotic test-stand, recommending electrolyte designs to test and receiving experimental feedback in real time. In 40 hours of continuous experimentation over a four-dimensional design space with millions of potential candidates, Dragonfly discovered a novel, mixed-anion aqueous sodium electrolyte with a wider electrochemical stability window than state-of-the-art sodium electrolyte. A human-guided design process may have missed this optimal electrolyte. This result demonstrates the possibility of integrating robotics with machine-learning to rapidly and autonomously discover novel battery materials. △ Less","22 October, 2019",https://arxiv.org/pdf/2001.09938
Resilient Cyberphysical Systems and their Application Drivers: A Technology Roadmap,Somali Chaterji;Parinaz Naghizadeh;Muhammad Ashraful Alam;Saurabh Bagchi;Mung Chiang;David Corman;Brian Henz;Suman Jana;Na Li;Shaoshuai Mou;Meeko Oishi;Chunyi Peng;Tiark Rompf;Ashutosh Sabharwal;Shreyas Sundaram;James Weimer;Jennifer Weller,"Cyberphysical systems (CPS) are ubiquitous in our personal and professional lives, and they promise to dramatically improve micro-communities (e.g., urban farms, hospitals), macro-communities (e.g., cities and metropolises), urban structures (e.g., smart homes and cars), and living structures (e.g., human bodies, synthetic genomes). The question that we address in this article pertains to designing these CPS systems to be resilient-from-the-ground-up, and through progressive learning, resilient-by-reaction. An optimally designed system is resilient to both unique attacks and recurrent attacks, the latter with a lower overhead. Overall, the notion of resilience can be thought of in the light of three main sources of lack of resilience, as follows: exogenous factors, such as natural variations and attack scenarios; mismatch between engineered designs and exogenous factors ranging from DDoS (distributed denial-of-service) attacks or other cybersecurity nightmares, so called ""black swan"" events, disabling critical services of the municipal electrical grids and other connected infrastructures, data breaches, and network failures; and the fragility of engineered designs themselves encompassing bugs, human-computer interactions (HCI), and the overall complexity of real-world systems. In the paper, our focus is on design and deployment innovations that are broadly applicable across a range of CPS application areas. △ Less","19 December, 2019",https://arxiv.org/pdf/2001.00090
Outlier Detection and Data Clustering via Innovation Search,Mostafa Rahmani;Ping Li,"The idea of Innovation Search was proposed as a data clustering method in which the directions of innovation were utilized to compute the adjacency matrix and it was shown that Innovation Pursuit can notably outperform the self representation based subspace clustering methods. In this paper, we present a new discovery that the directions of innovation can be used to design a provable and strong robust (to outlier) PCA method. The proposed approach, dubbed iSearch, uses the direction search optimization problem to compute an optimal direction corresponding to each data point. iSearch utilizes the directions of innovation to measure the innovation of the data points and it identifies the outliers as the most innovative data points. Analytical performance guarantees are derived for the proposed robust PCA method under different models for the distribution of the outliers including randomly distributed outliers, clustered outliers, and linearly dependent outliers. In addition, we study the problem of outlier detection in a union of subspaces and it is shown that iSearch provably recovers the span of the inliers when the inliers lie in a union of subspaces. Moreover, we present theoretical studies which show that the proposed measure of innovation remains stable in the presence of noise and the performance of iSearch is robust to noisy data. In the challenging scenarios in which the outliers are close to each other or they are close to the span of the inliers, iSearch is shown to remarkably outperform most of the existing methods. The presented method shows that the directions of innovation are useful representation of the data which can be used to perform both data clustering and outlier detection. △ Less","30 December, 2019",https://arxiv.org/pdf/1912.12988
Boldly Going Where No Prover Has Gone Before,Giles Reger,"I argue that the most interesting goal facing researchers in automated reasoning is being able to solve problems that cannot currently be solved by existing tools and methods. This may appear obvious, and is clearly not an original thought, but focusing on this as a primary goal allows us to examine other goals in a new light. Many successful theorem provers employ a portfolio of different methods for solving problems. This changes the landscape on which we perform our research: solving problems that can already be solved may not improve the state of the art and a method that can solve a handful of problems unsolvable by current methods, but generally performs poorly on most problems, can be very useful. We acknowledge that forcing new methods to compete against portfolio solvers can stifle innovation. However, this is only the case when comparisons are made at the level of total problems solved. We propose a movement towards focussing on unique solutions in evaluation and competitions i.e. measuring the potential contribution to a portfolio solver. This state of affairs is particularly prominent in first-order logic, which is undecidable. When reasoning in a decidable logic there can be a focus on optimising a decision procedure and measuring average solving times. But in a setting where solutions are difficult to find, average solving times lose meaning, and whilst improving the efficiency of a technique can move potential solutions within acceptable time limits, in general, complementary strategies may be more successful. △ Less","30 December, 2019",https://arxiv.org/pdf/1912.12958
Very Long Natural Scenery Image Prediction by Outpainting,Zongxin Yang;Jian Dong;Ping Liu;Yi Yang;Shuicheng Yan,"Comparing to image inpainting, image outpainting receives less attention due to two challenges in it. The first challenge is how to keep the spatial and content consistency between generated images and original input. The second challenge is how to maintain high quality in generated results, especially for multi-step generations in which generated regions are spatially far away from the initial input. To solve the two problems, we devise some innovative modules, named Skip Horizontal Connection and Recurrent Content Transfer, and integrate them into our designed encoder-decoder structure. By this design, our network can generate highly realistic outpainting prediction effectively and efficiently. Other than that, our method can generate new images with very long sizes while keeping the same style and semantic content as the given input. To test the effectiveness of the proposed architecture, we collect a new scenery dataset with diverse, complicated natural scenes. The experimental results on this dataset have demonstrated the efficacy of our proposed network. The code and dataset are available from https://github.com/z-x-yang/NS-Outpainting. △ Less","29 December, 2019",https://arxiv.org/pdf/1912.12688
Tha3aroon at NSURL-2019 Task 8: Semantic Question Similarity in Arabic,Ali Fadel;Ibraheem Tuffaha;Mahmoud Al-Ayyoub,"In this paper, we describe our team's effort on the semantic text question similarity task of NSURL 2019. Our top performing system utilizes several innovative data augmentation techniques to enlarge the training data. Then, it takes ELMo pre-trained contextual embeddings of the data and feeds them into an ON-LSTM network with self-attention. This results in sequence representation vectors that are used to predict the relation between the question pairs. The model is ranked in the 1st place with 96.499 F1-score (same as the second place F1-score) and the 2nd place with 94.848 F1-score (differs by 1.076 F1-score from the first place) on the public and private leaderboards, respectively. △ Less","28 December, 2019",https://arxiv.org/pdf/1912.12514
TASE: Reducing latency of symbolic execution with transactional memory,Adam Humphries;Kartik Cating-Subramanian;Michael K. Reiter,"We present the design and implementation of a tool called TASE that uses transactional memory to reduce the latency of symbolic-execution applications with small amounts of symbolic state. Execution paths are executed natively while operating on concrete values, and only when execution encounters symbolic values (or modeled functions) is native execution suspended and interpretation begun. Execution then returns to its native mode when symbolic values are no longer encountered. The key innovations in the design of TASE are a technique for amortizing the cost of checking whether values are symbolic over few instructions, and the use of hardware-supported transactional memory (TSX) to implement native execution that rolls back with no effect when use of a symbolic value is detected (perhaps belatedly). We show that TASE has the potential to dramatically improve some latency-sensitive applications of symbolic execution, such as methods to verify the behavior of a client in a client-server application. △ Less","27 December, 2019",https://arxiv.org/pdf/1912.12363
Fair Matching in Dynamic Kidney Exchange,Irena Gao,"Kidney transplants are sharply overdemanded in the United States. A recent innovation to address organ shortages is a kidney exchange, in which willing but medically incompatible patient-donor pairs swap donors so that two successful transplants occur. Proposed rules for matching such pairs include static fair matching rules, which improve matching for a particular group, such as highly-sensitized patients. However, in dynamic environments, it seems intuitively fair to prioritize time-critical pairs. We consider the tradeoff between established sensitization fairness and time fairness in dynamic environments. We design two algorithms, SENS and TIME, and study their patient loss. We show that the there is a theoretical advantage to prioritizing time-critical patients (around 9.18% tradeoff on U.S. data) rather than sensitized patients. Our results suggest that time fairness needs to be considered in kidney exchange. We then propose a batching algorithm for current branch-and-price solvers that balances both fairness needs. △ Less","22 December, 2019",https://arxiv.org/pdf/1912.10563
"An Overview on Smart Contracts: Challenges, Advances and Platforms",Zibin Zheng;Shaoan Xie;Hong-Ning Dai;Weili Chen;Xiangping Chen;Jian Weng;Muhammad Imran,"Smart contract technology is reshaping conventional industry and business processes. Being embedded in blockchains, smart contracts enable the contractual terms of an agreement to be enforced automatically without the intervention of a trusted third party. As a result, smart contracts can cut down administration and save services costs, improve the efficiency of business processes and reduce the risks. Although smart contracts are promising to drive the new wave of innovation in business processes, there are a number of challenges to be tackled.This paper presents a survey on smart contracts. We first introduce blockchains and smart contracts. We then present the challenges in smart contracts as well as recent technical advances. We also compare typical smart contract platforms and give a categorization of smart contract applications along with some representative examples. △ Less","21 December, 2019",https://arxiv.org/pdf/1912.10370
From Patches to Pictures (PaQ-2-PiQ): Mapping the Perceptual Space of Picture Quality,Zhenqiang Ying;Haoran Niu;Praful Gupta;Dhruv Mahajan;Deepti Ghadiyaram;Alan Bovik,"Blind or no-reference (NR) perceptual picture quality prediction is a difficult, unsolved problem of great consequence to the social and streaming media industries that impacts billions of viewers daily. Unfortunately, popular NR prediction models perform poorly on real-world distorted pictures. To advance progress on this problem, we introduce the largest (by far) subjective picture quality database, containing about 40000 real-world distorted pictures and 120000 patches, on which we collected about 4M human judgments of picture quality. Using these picture and patch quality labels, we built deep region-based architectures that learn to produce state-of-the-art global picture quality predictions as well as useful local picture quality maps. Our innovations include picture quality prediction architectures that produce global-to-local inferences as well as local-to-global inferences (via feedback). △ Less","20 December, 2019",https://arxiv.org/pdf/1912.10088
Exploring AI Futures Through Role Play,Shahar Avin;Ross Gruetzemacher;James Fox,"We present an innovative methodology for studying and teaching the impacts of AI through a role play game. The game serves two primary purposes: 1) training AI developers and AI policy professionals to reflect on and prepare for future social and ethical challenges related to AI and 2) exploring possible futures involving AI technology development, deployment, social impacts, and governance. While the game currently focuses on the inter relations between short --, mid and long term impacts of AI, it has potential to be adapted for a broad range of scenarios, exploring in greater depths issues of AI policy research and affording training within organizations. The game presented here has undergone two years of development and has been tested through over 30 events involving between 3 and 70 participants. The game is under active development, but preliminary findings suggest that role play is a promising methodology for both exploring AI futures and training individuals and organizations in thinking about, and reflecting on, the impacts of AI and strategic mistakes that can be avoided today. △ Less","18 December, 2019",https://arxiv.org/pdf/1912.08964
Kalman Filter Tuning with Bayesian Optimization,Zhaozhong Chen;Nisar Ahmed;Simon Julier;Christoffer Heckman,"Many state estimation algorithms must be tuned given the state space process and observation models, the process and observation noise parameters must be chosen. Conventional tuning approaches rely on heuristic hand-tuning or gradient-based optimization techniques to minimize a performance cost function. However, the relationship between tuned noise values and estimator performance is highly nonlinear and stochastic. Therefore, the tuning solutions can easily get trapped in local minima, which can lead to poor choices of noise parameters and suboptimal estimator performance. This paper describes how Bayesian Optimization (BO) can overcome these issues. BO poses optimization as a Bayesian search problem for a stochastic ``black box'' cost function, where the goal is to search the solution space to maximize the probability of improving the current best solution. As such, BO offers a principled approach to optimization-based estimator tuning in the presence of local minima and performance stochasticity. While extended Kalman filters (EKFs) are the main focus of this work, BO can be similarly used to tune other related state space filters. The method presented here uses performance metrics derived from normalized innovation squared (NIS) filter residuals obtained via sensor data, which renders knowledge of ground-truth states unnecessary. The robustness, accuracy, and reliability of BO-based tuning is illustrated on practical nonlinear state estimation problems,losed-loop aero-robotic control. △ Less","17 December, 2019",https://arxiv.org/pdf/1912.08601
Conversational Agents for Insurance Companies: From Theory to Practice,Falko Koetter;Matthias Blohm;Jens Drawehn;Monika Kochanowski;Joscha Goetzer;Daniel Graziotin;Stefan Wagner,"Advances in artificial intelligence have renewed interest in conversational agents. Additionally to software developers, today all kinds of employees show interest in new technologies and their possible applications for customers. German insurance companies generally are interested in improving their customer service and digitizing their business processes. In this work we investigate the potential use of conversational agents in insurance companies theoretically by determining which classes of agents exist which are of interest to insurance companies, finding relevant use cases and requirements. We add two practical parts: First we develop a showcase prototype for an exemplary insurance scenario in claim management. Additionally in a second step, we create a prototype focusing on customer service in a chatbot hackathon, fostering innovation in interdisciplinary teams. In this work, we describe the results of both prototypes in detail. We evaluate both chatbots defining criteria for both settings in detail and compare the results and draw conclusions for the maturity of chatbot technology for practical use, describing the opportunities and challenges companies, especially small and medium enterprises, face. △ Less","18 December, 2019",https://arxiv.org/pdf/1912.08473
Geographical and Topology Control based Opportunistic Routing for Ad Hoc Networks,Ning Li;Zhaoxin Zhang;Jose-Fernan Martinez-Ortega;Xin Yuan,"The opportunistic routing has great advantages on improving packet delivery probability between the source node and candidate forwarding set. For improving and reducing energy consumption and network interference, in this paper, we propose an efficient and reliable transmission power control based opportunistic routing (ERTO) for wireless ad hoc networks. In ERTO, the network interference and fading, which are critical to routing performances but not taken into account in previous works, are considered during the prediction of. The, the expected energy consumption, and the relationship between transmission power and node degree are considered to optimize the optimal transmission power and forwarding node degree jointly. For improving routing effectiveness and reducing computational complexity, we introduce the multi-objective optimization and Pareto optimal into ERTO. During the routing process, nodes calculate the optimal transmission power and forwarding node degree according to the properties of the Pareto optimal solution set. Based on these innovations, the energy consumption, the transmission delay, and the throughout have been improved greatly. △ Less","16 December, 2019",https://arxiv.org/pdf/1912.08100
The Network-based Candidate Forwarding Set Optimization Approach for Opportunistic Routing in Wireless Sensor Network,Ning Li;Alex X. Liu;Jose-Fernan Martinez-Ortega;Vicente Hernandez Diaz;Xin Yuan,"In wireless sensor networks (WSNs), the opportunistic routing has better performances on packet delivery probability than the deterministic routing. For reducing the transmission delay and duplicate transmission in opportunistic routing, the candidate forwarding set optimization technology is proposed. This technology is crucial to opportunistic routing. The prior arts have limitations on improving the routing performances. Therefore, in this paper, we propose the concept of the network-based approach to address the disadvantages of prior arts. In the network-based approach, the nodes in the candidate forwarding set are divided into different fully connected relay networks. For the forwarding nodes in CFS, more than one relay network can be constructed and only one relay network can be chosen as the final relay network. So, first, we propose the fully connected relay network judgment algorithm to judge whether the chosen network is fully connected or not. Next, the properties of these networks are investigated. In this algorithm, the relay network selection takes the packet delivery probability between the sender and the relay networks, the transmission delay, and the forwarding priorities of nodes in relay networks into account to choose the most effective relay network. The nodes in this relay network will be chosen as final forwarding nodes. By these, the transmission delay and duplicate transmission are reduced while the efficiency of opportunistic routing is kept. Based on these innovations, the proposed algorithm can improve network performances greater than that of ExOR and SOAR. △ Less","16 December, 2019",https://arxiv.org/pdf/1912.08098
Following the footsteps of giants: Modeling the mobility of historically notable individuals using Wikipedia,Lorenzo Lucchini;Sara Tonelli;Bruno Lepri,"The steady growth of digitized historical information is continuously stimulating new different approaches to the fields of Digital Humanities and Computational Social Science. In this work, we use Natural Language Processing techniques to retrieve large amounts of historical information from Wikipedia. In particular, the pages of a set of historically notable individuals are processed to catch the locations and the date of people's movements. This information is then structured in a geographical network of mobility patterns. We analyze the mobility of historically notable individuals from different perspectives to better understand the role of migrations and international collaborations in the context of innovation and cultural development. In this work, we first present some general characteristics of the dataset from a social and geographical perspective. Then, we build a spatial network of cities, and we model and quantify the tendency to explore by a set of people that can be considered historically and culturally notable. In this framework, we show that by using a multilevel radiation model for human mobility, we are able to catch important features of migration's behavior. Results show that the choice of the target migration place for historically and culturally relevant people is limited to a small number of locations and that it depends on the discipline a notable is interested in and on the number of opportunities she/he can find there. △ Less","16 December, 2019",https://arxiv.org/pdf/1912.07551
Recruiting Hay to Find Needles: Recursive Incentives and Innovation in Social Networks,Erik P. Duhaime;Brittany M. Bond;Qi Yang;Patrick de Boer;Thomas W. Malone,"Finding innovative solutions to complex problems is often about finding people who have access to novel information and alternative viewpoints. Research has found that most people are connected to each other through just a few degrees of separation, but successful social search is often difficult because it depends on people using their weak ties to make connections to distant social networks. Recursive incentive schemes have shown promise for social search by motivating people to use their weak ties to find distant targets, such as specific people or even weather balloons placed at undisclosed locations. Here, we report on a case study of a similar recursive incentive scheme for finding innovative ideas. Specifically, we implemented a competition to reward individuals(s) who helped refer Grand Prize winner(s) in MIT's Climate CoLab, an open innovation platform for addressing global climate change. Using data on over 78,000 CoLab members and over 36,000 people from over 100 countries who engaged with the referral contest, we find that people who are referred using this method are more likely than others to submit proposals and to submit high quality proposals. Furthermore, we find suggestive evidence that among the contributors referred via the contest, those who had more than one degree of separation from a pre-existing CoLab member were more likely to submit high quality proposals. Thus, the results from this case study are consistent the theory that people from distant networks are more likely to provide innovative solutions to complex problems. More broadly, the results suggest that rewarding indirect intermediaries in addition to final finders may promote effective social network recruitment. △ Less","14 December, 2019",https://arxiv.org/pdf/1912.06922
Graph Theory and Metro Traffic Modelling,Bruno Scalzo Dees;Anthony G. Constantinides;Danilo P. Mandic,"In this article we demonstrate how graph theory can be used to identify those stations in the London underground network which have the greatest influence on the functionality of the traffic, and proceed, in an innovative way, to assess the impact of a station closure on service levels across the city. Such underground network vulnerability analysis offers the opportunity to analyse, optimize and enhance the connectivity of the London underground network in a mathematically tractable and physically meaningful manner. △ Less","12 December, 2019",https://arxiv.org/pdf/1912.05964
Reliable Wide-Area Backscatter via Channel Polarization,Guochao Song;Hang Yang;Wei Wang;Tao Jiang,"A long-standing vision of backscatter communications is to provide long-range connectivity and high-speed transmissions for batteryless Internet-of-Things (IoT). Recent years have seen major innovations in designing backscatters toward this goal. Yet, they either operate at a very short range, or experience extremely low throughput. This paper takes one step further toward breaking this stalemate, by presenting PolarScatter that exploits channel polarization in long-range backscatter links. We transform backscatter channels into nearly noiseless virtual channels through channel polarization, and convey bits with extremely low error probability. Specifically, we propose a new polar code scheme that automatically adapts itself to different channel quality, and design a low-cost encoder to accommodate polar codes on resource-constrained backscatter tags. We build a prototype PCB tag and test it in various outdoor and indoor environments. Our experiments show that our prototype achieves up to 10\times throughput gain, or extends the range limit by 1.8\times compared with the state-of-the-art long-range backscatter solution. We also simulate an IC design in TSMC 65 nm LP CMOS process. Compared with traditional encoders, our encoder reduces storage overhead by three orders of magnitude, and lowers the power consumption to tens of microwatts. △ Less","12 December, 2019",https://arxiv.org/pdf/1912.05829
Envisioning Device-to-Device Communications in 6G,Shangwei Zhang;Jiajia Liu;Hongzhi Guo;Mingping Qi;Nei Kato,"To fulfill the requirements of various emerging applications, the future sixth generation (6G) mobile network is expected to be an innately intelligent, highly dynamic, ultradense heterogeneous network that interconnects all things with extremely low-latency and high speed data transmission. It is believed that artificial intelligence (AI) will be the most innovative technique that can achieve intelligent automated network operations, management and maintenance in future complex 6G networks. Driven by AI techniques, device-to-device (D2D) communication will be one of the pieces of the 6G jigsaw puzzle. To construct an efficient implementation of intelligent D2D in future 6G, we outline a number of potential D2D solutions associating with 6G in terms of mobile edge computing, network slicing, and Non-orthogonal multiple access (NOMA) cognitive Networking. △ Less","11 December, 2019",https://arxiv.org/pdf/1912.05771
Online Deep Reinforcement Learning for Autonomous UAV Navigation and Exploration of Outdoor Environments,Bruna G. Maciel-Pearson;Letizia Marchegiani;Samet Akcay;Amir Atapour-Abarghouei;James Garforth;Toby P. Breckon,"With the rapidly growing expansion in the use of UAVs, the ability to autonomously navigate in varying environments and weather conditions remains a highly desirable but as-of-yet unsolved challenge. In this work, we use Deep Reinforcement Learning to continuously improve the learning and understanding of a UAV agent while exploring a partially observable environment, which simulates the challenges faced in a real-life scenario. Our innovative approach uses a double state-input strategy that combines the acquired knowledge from the raw image and a map containing positional information. This positional data aids the network understanding of where the UAV has been and how far it is from the target position, while the feature map from the current scene highlights cluttered areas that are to be avoided. Our approach is extensively tested using variants of Deep Q-Network adapted to cope with double state input data. Further, we demonstrate that by altering the reward and the Q-value function, the agent is capable of consistently outperforming the adapted Deep Q-Network, Double Deep Q- Network and Deep Recurrent Q-Network. Our results demonstrate that our proposed Extended Double Deep Q-Network (EDDQN) approach is capable of navigating through multiple unseen environments and under severe weather conditions. △ Less","11 December, 2019",https://arxiv.org/pdf/1912.05684
A recipe for creating ideal hybrid memristive-CMOS neuromorphic computing systems,Elisabetta Chicca;Giacomo Indiveri,"The development of memristive device technologies has reached a level of maturity to enable the design of complex and large-scale hybrid memristive-CMOS neural processing systems. These systems offer promising solutions for implementing novel in-memory computing architectures for machine learning and data analysis problems. We argue that they are also ideal building blocks for the integration in neuromorphic electronic circuits suitable for ultra-low power brain-inspired sensory processing systems, therefore leading to the innovative solutions for always-on edge-computing and Internet-of-Things (IoT) applications. Here we present a recipe for creating such systems based on design strategies and computing principles inspired by those used in mammalian brains. We enumerate the specifications and properties of memristive devices required to support always-on learning in neuromorphic computing systems and to minimize their power consumption. Finally, we discuss in what cases such neuromorphic systems can complement conventional processing ones and highlight the importance of exploiting the physics of both the memristive devices and of the CMOS circuits interfaced to them. △ Less","11 December, 2019",https://arxiv.org/pdf/1912.05637
Object Recognition with Human in the Loop Intelligent Frameworks,Orod Razeghi;Guoping Qiu,"Classifiers embedded within human in the loop visual object recognition frameworks commonly utilise two sources of information: one derived directly from the imagery data of an object, and the other obtained interactively from user interactions. These computer vision frameworks exploit human high-level cognitive power to tackle particularly difficult visual object recognition tasks. In this paper, we present innovative techniques to combine the two sources of information intelligently for the purpose of improving recognition accuracy. We firstly employ standard algorithms to build two classifiers for the two sources independently, and subsequently fuse the outputs from these classifiers to make a conclusive decision. The two fusion techniques proposed are: i) a modified naive Bayes algorithm that adaptively selects an individual classifier's output or combines both to produce a definite answer, and ii) a neural network based algorithm which feeds the outputs of the two classifiers to a 4-layer feedforward network to generate a final output. We present extensive experimental results on 4 challenging visual recognition tasks to illustrate that the new intelligent techniques consistently outperform traditional approaches to fusing the two sources of information. △ Less","11 December, 2019",https://arxiv.org/pdf/1912.05575
High Accuracy Low Precision QR Factorization and Least Square Solver on GPU with TensorCore,Shaoshuai Zhang;Panruo Wu,"Driven by the insatiable needs to process ever larger amount of data with more complex models, modern computer processors and accelerators are beginning to offer half precision floating point arithmetic support, and extremely optimized special units such as NVIDIA TensorCore on GPU and Google Tensor Processing Unit (TPU) that does half precision matrix-matrix multiplication exceptionally efficiently. In this paper we present a large scale mixed precision linear least square solver that achieves high accuracy using the low precision TensorCore GPU. The mixed precision system consists of both innovative algorithms and implementations, and is shown to be up to 14x faster than single precision cuSOLVER at QR matrix factorization at large scale with slightly lower accuracy, and up to 10x faster than double precision direct QR least square solver with comparable accuracy. △ Less","11 December, 2019",https://arxiv.org/pdf/1912.05508
Multiple criteria hierarchy process for sorting problems under uncertainty applied to the evaluation of the operational maturity of research institutions,Renata Pelissari;Alvaro José Abackerli;Sarah Ben Amor;Maria Célia Oliveira;Kleber Manoel Infante,"Despite the availability of qualified research personnel, up-to-date research facilities and experience in developing applied research and innovation, many worldwide research institutions face difficulties when managing contracted Research and Development (R&D) projects due to expectations from Industry (private sector). Such difficulties have motivated funding agents to create evaluation processes to check whether the operational procedures of funded research institutions are sufficient to provide timely answers to demand for innovation from industry and also to identify aspects that require quality improvement in research development. For this purpose, several multiple criteria decision-making approaches can be applied. Among the available multiple criteria approaches, sorting methods are one prominent tool to evaluate the operational capacity. However, the first difficulty in applying multiple criteria sorting methods is the need to hierarchically structure multiple criteria in order to represent the intended decision process. Additional challenges include the elicitation of the preference information and the definition of criteria evaluation, since these are frequently affected by some imprecision. In this paper, a new sorting method is proposed to deal with all of those critical points simultaneously. To consider multiple levels for the decision criteria, the FlowSort method is extended to account for hierarchical criteria. To deal with imprecise data, the FlowSort is integrated with fuzzy approaches. To yield solutions that consider fluctuations from imprecise weights, the Stochastic Multicriteria Acceptability Analysis is used. Finally, the proposed method is applied to the evaluation of research institutions, classifying them according to their operational maturity for development of applied research. △ Less","6 December, 2019",https://arxiv.org/pdf/1912.05324
Blockchain for 5G and Beyond Networks: A State of the Art Survey,Dinh C Nguyen;Pubudu N Pathirana;Ming Ding;Aruna Seneviratne,"The fifth generation (5G) wireless networks are on the way to be deployed around the world. The 5G technologies target to support diverse vertical applications by connecting heterogeneous devices and machines with drastic improvements in terms of high quality of service, increased network capacity and enhanced system throughput. Despite all these advantages that 5G will bring about, there are still major challenges to be addressed, including decentralization, transparency, risks of data interoperability, network privacy and security vulnerabilities. Blockchain can offer innovative solutions to effectively solve the challenges in 5G networks. Driven by the dramatically increased capacities of the 5G networks and the recent breakthroughs in the blockchain technology, blockchain-based 5G services are expected to witness a rapid development and bring substantial benefits to future society. In this paper, we provide a state-of-art survey on the integration of blockchain with 5G networks and beyond. Our key focus is on the discussions on the potential of blockchain for enabling key 5G technologies, including cloud/edge computing, Software Defined Networks, Network Function Virtualization, Network Slicing, and D2D communications. We then explore the opportunities of blockchain to important 5G services, ranging from spectrum management, network virtualization, resource management to interference management, federated learning, privacy and security provision. The recent advances in the applications of blockchain in 5G Internet of Things are also surveyed in various domains, i.e. smart healthcare, smart city, smart transportation, smart grid and UAVs. The main findings derived from the survey are then summarized, and possible research challenges with open issues are also identified. Lastly, we complete this survey by shedding new light on future directions of research on this newly emerging area. △ Less","10 December, 2019",https://arxiv.org/pdf/1912.05062
MDFN: Multi-Scale Deep Feature Learning Network for Object Detection,Wenchi Ma;Yuanwei Wu;Feng Cen;Guanghui Wang,"This paper proposes an innovative object detector by leveraging deep features learned in high-level layers. Compared with features produced in earlier layers, the deep features are better at expressing semantic and contextual information. The proposed deep feature learning scheme shifts the focus from concrete features with details to abstract ones with semantic information. It considers not only individual objects and local contexts but also their relationships by building a multi-scale deep feature learning network (MDFN). MDFN efficiently detects the objects by introducing information square and cubic inception modules into the high-level layers, which employs parameter-sharing to enhance the computational efficiency. MDFN provides a multi-scale object detector by integrating multi-box, multi-scale and multi-level technologies. Although MDFN employs a simple framework with a relatively small base network (VGG-16), it achieves better or competitive detection results than those with a macro hierarchical structure that is either very deep or very wide for stronger ability of feature extraction. The proposed technique is evaluated extensively on KITTI, PASCAL VOC, and COCO datasets, which achieves the best results on KITTI and leading performance on PASCAL VOC and COCO. This study reveals that deep features provide prominent semantic information and a variety of contextual contents, which contribute to its superior performance in detecting small or occluded objects. In addition, the MDFN model is computationally efficient, making a good trade-off between the accuracy and speed. △ Less","10 December, 2019",https://arxiv.org/pdf/1912.04514
Delegated Proof of Reputation: a novel Blockchain consensus,Thuat Do;Thao Nguyen;Hung Pham,"Consensus mechanism is the heart of any blockchain network. Many projects have proposed alternative protocols to improve restricted scalability of Proof of Work originated since Bitcoin. As an improvement of Delegated Proof of Stake, in this paper, we introduce a novel consensus, namely, Delegated Proof of Reputation, which is scalable, secure with an acceptable decentralization. Our innovative idea is replacing pure coinstaking by a reputation ranking system essentially based on ranking theories (PageRank, NCDawareRank and HodgeRank). △ Less","6 December, 2019",https://arxiv.org/pdf/1912.04065
Realizing the Frugal 5G Network,Meghna Khaturia;Pranav Jha;Abhay Karandikar,"In order to make effective use of the Internet, broadband connectivity is a pre-requisite. However, in the majority of rural areas in developing countries, high-speed connectivity is unavailable. The Frugal 5G network architecture presented in this paper aims at enabling broadband in rural areas by addressing the challenges associated with it. The work presented in this paper is a development over our previous work, in which we proposed abstract network architecture for Frugal 5G. In this paper, we provide an innovative solution to realize the Frugal 5G network. We identify the key system requirements and show that the proposed solution enables an uncomplicated and flexible realization of the Frugal 5G network. We are currently building a testbed to implement the proposed changes. △ Less","9 December, 2019",https://arxiv.org/pdf/1912.03965
A Real-time Global Inference Network for One-stage Referring Expression Comprehension,Yiyi Zhou;Rongrong Ji;Gen Luo;Xiaoshuai Sun;Jinsong Su;Xinghao Ding;Chia-wen Lin;Qi Tian,"Referring Expression Comprehension (REC) is an emerging research spot in computer vision, which refers to detecting the target region in an image given an text description. Most existing REC methods follow a multi-stage pipeline, which are computationally expensive and greatly limit the application of REC. In this paper, we propose a one-stage model towards real-time REC, termed Real-time Global Inference Network (RealGIN). RealGIN addresses the diversity and complexity issues in REC with two innovative designs: the Adaptive Feature Selection (AFS) and the Global Attentive ReAsoNing unit (GARAN). AFS adaptively fuses features at different semantic levels to handle the varying content of expressions. GARAN uses the textual feature as a pivot to collect expression-related visual information from all regions, and thenselectively diffuse such information back to all regions, which provides sufficient context for modeling the complex linguistic conditions in expressions. On five benchmark datasets, i.e., RefCOCO, RefCOCO+, RefCOCOg, ReferIt and Flickr30k, the proposed RealGIN outperforms most prior works and achieves very competitive performances against the most advanced method, i.e., MAttNet. Most importantly, under the same hardware, RealGIN can boost the processing speed by about 10 times over the existing methods. △ Less","7 December, 2019",https://arxiv.org/pdf/1912.03478
Transitivity and degree assortativity explained: The bipartite structure of social networks,Demival Vasques Filho;Dion R. J. O'Neale,"Dynamical processes, such as the diffusion of knowledge, opinions, pathogens, ""fake news"", innovation, and others, are highly dependent on the structure of the social network on which they occur. However, questions on why most social networks present some particular structural features, namely high levels of transitivity and degree assortativity, when compared to other types of networks remain open. First, we argue that every one-mode network can be regarded as a projection of a bipartite network, and show that this is the case using two simple examples solved with the generating functions formalism. Second, using synthetic and empirical data, we reveal how the combination of the degree distribution of both sets of nodes of the bipartite network --- together with the presence of cycles of length four and six --- explains the observed levels of transitivity and degree assortativity in the one-mode projected network. Bipartite networks with top node degrees that display a more right-skewed distribution than the bottom nodes result in highly transitive and degree assortative projections, especially if a large number of small cycles are present in the bipartite structure. △ Less","6 December, 2019",https://arxiv.org/pdf/1912.03211
An Accelerated Correlation Filter Tracker,Tianyang Xu;Zhen-Hua Feng;Xiao-Jun Wu;Josef Kittler,"Recent visual object tracking methods have witnessed a continuous improvement in the state-of-the-art with the development of efficient discriminative correlation filters (DCF) and robust deep neural network features. Despite the outstanding performance achieved by the above combination, existing advanced trackers suffer from the burden of high computational complexity of the deep feature extraction and online model learning. We propose an accelerated ADMM optimisation method obtained by adding a momentum to the optimisation sequence iterates, and by relaxing the impact of the error between DCF parameters and their norm. The proposed optimisation method is applied to an innovative formulation of the DCF design, which seeks the most discriminative spatially regularised feature channels. A further speed up is achieved by an adaptive initialisation of the filter optimisation process. The significantly increased convergence of the DCF filter is demonstrated by establishing the optimisation process equivalence with a continuous dynamical system for which the convergence properties can readily be derived. The experimental results obtained on several well-known benchmarking datasets demonstrate the efficiency and robustness of the proposed ACFT method, with a tracking accuracy comparable to the start-of-the-art trackers. △ Less","5 December, 2019",https://arxiv.org/pdf/1912.02854
Blockchain Applications in Power Systems: A Bibliometric Analysis,Hossein Mohammadi Rouzbahani;Hadis Karimipour;Ali Dehghantanha;Reza M. Parizi,"Power systems are growing rapidly, due to the ever-increasing demand for electrical power. These systems require novel methodologies and modern tools and technologies, to better perform, particularly for communication among different parts. Therefore, power systems are facing new challenges such as energy trading and marketing and cyber threats. Using blockchain in power systems, as a solution, is one of the newest methods. Most studies aim to investigate innovative approach-es of blockchain application in power systems. Even though, many articles published to support the research activities, there has not been any bibliometric analysis which specifies the research trends. This paper aims to present a bibliographic analysis of the blockchain application in power systems related literature, in the Web of Science (WoS) database between January 2009 and July 2019. This paper discusses the research activities and performed a detailed analysis by looking at the number of articles published, citations, institutions, research areas, and authors. From the analysis, it was concluded that there are several significant impacts of research activities in China and the USA, in comparison to other countries. △ Less","4 December, 2019",https://arxiv.org/pdf/1912.02611
FMPC: Secure Multiparty Computation from Fourier Series and Parseval's Identity,Alberto Sonnino,"FMPC is a novel multiparty computation protocol of arithmetic circuits based on secret-sharing, capable of computing multiplication of secrets with no online communication; it thus enjoys constant online communication latency in the size of the circuit. FMPC is based on the application of Fourier series to Parseval's identity, and introduces the first generalization of Parseval's identity for Fourier series applicable to an arbitrary number of inputs. FMPC operates in a setting where users wish to compute a function over some secret inputs by submitting the computation to a set of nodes, but is only suitable for the evaluation of low-depth arithmetic circuits. FMPC relies on an offline phase consisting of traditional preprocessing as introduced by established protocols like SPDZ, and innovates on the online phase that mainly consists of each node locally evaluating specific functions. FMPC paves the way for a new kind of multiparty computation protocols capable of computing multiplication of secrets as an alternative to circuit garbling and the traditional algebra introduced by Donald Beaver in 1991. △ Less","5 December, 2019",https://arxiv.org/pdf/1912.02583
MORPHOLO C++ Library for glasses-free multi-view stereo vision and streaming of live 3D video,Enrique Canessa;Livio Tenze,"The MORPHOLO C++ extended Library allows to convert a specific stereoscopic snapshot into a Native multi-view image through morphing algorithms taking into account display calibration data for specific slanted lenticular 3D monitors. MORPHOLO can also be implemented for glasses-free live applicatons of 3D video streaming, and for diverse innovative scientific, engineering and 3D video game applications -see http://www.morpholo.it △ Less","4 December, 2019",https://arxiv.org/pdf/1912.02202
Optimization in Software Engineering -- A Pragmatic Approach,Guenther Ruhe,"Empirical software engineering is concerned with the design and analysis of empirical studies that include software products, processes, and resources. Optimization is a form of data analytics in support of human decision-making. Optimization methods are aimed to find the best decision alternatives. Empirical studies serve both as a model and as data input for optimization. In addition, the complexity of the models used for optimization trigger further studies on explaining and validating the results in real-world scenarios. The goal of this chapter is to give an overview of the as-is and of the to-be usage of optimization in software engineering. The emphasis is on pragmatic use of optimization, and not so much on describing the most recent algorithmic innovations and tool developments. The usage of optimization covers a wide range of questions from different types of software engineering problems along the whole life-cycle. To facilitate its more comprehensive and more effective usage, a checklist for a guided process is described. The chapter uses a running example Asymmetric Release Planning to illustrate the whole process. A Return-on-Investment analysis is proposed as part of the problem scoping. This helps to decide on the depth and breadth of analysis in relation to the effort needed to run the analysis and the projected value of the solution. △ Less","4 December, 2019",https://arxiv.org/pdf/1912.02071
Discovering Opioid Use Patterns from Social Media for Relapse Prevention,Zhou Yang;Spencer Bradshaw;Rattikorn Hewett;Fang Jin,"The United States is currently experiencing an unprecedented opioid crisis, and opioid overdose has become a leading cause of injury and death. Effective opioid addiction recovery calls for not only medical treatments, but also behavioral interventions for impacted individuals. In this paper, we study communication and behavior patterns of patients with opioid use disorder (OUD) from social media, intending to demonstrate how existing information from common activities, such as online social networking, might lead to better prediction, evaluation, and ultimately prevention of relapses. Through a multi-disciplinary and advanced novel analytic perspective, we characterize opioid addiction behavior patterns by analyzing opioid groups from Reddit.com - including modeling online discussion topics, analyzing text co-occurrence and correlations, and identifying emotional states of people with OUD. These quantitative analyses are of practical importance and demonstrate innovative ways to use information from online social media, to create technology that can assist in relapse prevention. △ Less","2 December, 2019",https://arxiv.org/pdf/1912.01122
ICD Coding from Clinical Text Using Multi-Filter Residual Convolutional Neural Network,Fei Li;Hong Yu,"Automated ICD coding, which assigns the International Classification of Disease codes to patient visits, has attracted much research attention since it can save time and labor for billing. The previous state-of-the-art model utilized one convolutional layer to build document representations for predicting ICD codes. However, the lengths and grammar of text fragments, which are closely related to ICD coding, vary a lot in different documents. Therefore, a flat and fixed-length convolutional architecture may not be capable of learning good document representations. In this paper, we proposed a Multi-Filter Residual Convolutional Neural Network (MultiResCNN) for ICD coding. The innovations of our model are two-folds: it utilizes a multi-filter convolutional layer to capture various text patterns with different lengths and a residual convolutional layer to enlarge the receptive field. We evaluated the effectiveness of our model on the widely-used MIMIC dataset. On the full code set of MIMIC-III, our model outperformed the state-of-the-art model in 4 out of 6 evaluation metrics. On the top-50 code set of MIMIC-III and the full code set of MIMIC-II, our model outperformed all the existing and state-of-the-art models in all evaluation metrics. The code is available at https://github.com/foxlf823/Multi-Filter-Residual-Convolutional-Neural-Network. △ Less","25 November, 2019",https://arxiv.org/pdf/1912.00862
On the Equilibria and Efficiency of Electricity Markets with Renewable Power Producers and Congestion Constraints,Hossein Khazaei;X. Andy Sun;Yue Zhao,"With increasing renewable penetration in power systems, a prominent challenge in efficient and reliable power system operation is handling the uncertainties inherent in the renewable generation. In this paper, we propose a simple two-settlement market mechanism in which renewable power producers (RPPs) participate, so that a) the independent system operator (ISO) does not need to consider the uncertainties of the renewables in its economic dispatch, and yet b) the market equilibrium is shown to approach social efficiency as if the ISO solves a stochastic optimization taking into account all the uncertainties. In showing this result, a key innovation is a new approach of efficiently computing the Nash equilibrium (NE) among the strategic RPPs in congestion-constrained power networks. In particular, the proposed approach decouples finding an NE into searching over congestion patterns and computing an NE candidate assuming a congestion pattern. As such, the computational complexity of finding an NE grows only cubically with the number of RPPs in the market. We demonstrate our results in the IEEE 14-bus system and show that the NE approaches social efficiency as the number of RPPs grows. △ Less","29 November, 2019",https://arxiv.org/pdf/1912.00733
Active Search for Nearest Neighbors,Hayoung Um;Heeyoul Choi,"In pattern recognition or machine learning, it is a very fundamental task to find nearest neighbors of a given point. All the methods for the task work basically by comparing the given point to all the points in the data set. That is why the computational cost increases with the number of data points. However, the human visual system seems to work in a different way. When the human visual system tries to find the neighbors of one point on a map, it directly focuses on the area around the point and actively searches the neighbors by looking or zooming in and out around the point. In this paper, we propose an innovative search method for nearest neighbors, which seems very similar to how human visual system works on the task. △ Less","8 December, 2019",https://arxiv.org/pdf/1912.00386
Practical Modeling and Analysis of Blockchain Radio Access Network,Xintong Ling;Yuwei Le;Jiaheng Wang;Zhi Ding;Xiqi Gao,"The continually rising demand for wireless services and applications in the era of Internet of things (IoT) and artificial intelligence (AI) presents a significant number of unprecedented challenges to existing network structures. To meet the rapid growth need of mobile data services, blockchain radio access network (B-RAN) has emerged as a decentralized, trustworthy radio access paradigm spurred by blockchain technologies. However, many characteristics of B-RAN remain unclear and hard to characterize. In this study, we develop an analytical framework to model B-RAN and provide some basic fundamental analysis. Starting from block generation, we establish a queuing model based on a time-homogeneous Markov chain. From the queuing model, we evaluate the performance of B-RAN with respect to latency and security considerations. By connecting latency and security, we uncover a more comprehensive picture of the achievable performance of B-RAN. Further, we present experimental results via an innovative prototype and validate the proposed model. △ Less","28 November, 2019",https://arxiv.org/pdf/1911.12537
Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers,Ya Zhao;Rui Xu;Xinchao Wang;Peng Hou;Haihong Tang;Mingli Song,"Lip reading has witnessed unparalleled development in recent years thanks to deep learning and the availability of large-scale datasets. Despite the encouraging results achieved, the performance of lip reading, unfortunately, remains inferior to the one of its counterpart speech recognition, due to the ambiguous nature of its actuations that makes it challenging to extract discriminant features from the lip movement videos. In this paper, we propose a new method, termed as Lip by Speech (LIBS), of which the goal is to strengthen lip reading by learning from speech recognizers. The rationale behind our approach is that the features extracted from speech recognizers may provide complementary and discriminant clues, which are formidable to be obtained from the subtle movements of the lips, and consequently facilitate the training of lip readers. This is achieved, specifically, by distilling multi-granularity knowledge from speech recognizers to lip readers. To conduct this cross-modal knowledge distillation, we utilize an efficacious alignment scheme to handle the inconsistent lengths of the audios and videos, as well as an innovative filtering strategy to refine the speech recognizer's prediction. The proposed method achieves the new state-of-the-art performance on the CMLR and LRS2 datasets, outperforming the baseline by a margin of 7.66% and 2.75% in character error rate, respectively. △ Less","26 November, 2019",https://arxiv.org/pdf/1911.11502
Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body Robotics using NVIDIA CUDA,Jacob Austin;Rafael Corrales-Fatou;Sofia Wyetzner;Hod Lipson,"While most robotics simulation libraries are built for low-dimensional and intrinsically serial tasks, soft-body and multi-agent robotics have created a demand for simulation environments that can model many interacting bodies in parallel. Despite the increasing interest in these fields, no existing simulation library addresses the challenge of providing a unified, highly-parallelized, GPU-accelerated interface for simulating large robotic systems. Titan is a versatile CUDA-based C++ robotics simulation library that employs a novel asynchronous computing model for GPU-accelerated simulations of robotics primitives. The innovative GPU architecture design permits simultaneous optimization and control on the CPU while the GPU runs asynchronously, enabling rapid topology optimization and reinforcement learning iterations. Kinematics are solved with a massively parallel integration scheme that incorporates constraints and environmental forces. We report dramatically improved performance over CPU-based baselines, simulating as many as 300 million primitive updates per second, while allowing flexibility for a wide range of research applications. We present several applications of Titan to high-performance simulations of soft-body and multi-agent robots. △ Less","22 November, 2019",https://arxiv.org/pdf/1911.10274
Learnable Pooling in Graph Convolution Networks for Brain Surface Analysis,Karthik Gopinath;Christian Desrosiers;Herve Lombaert,"Brain surface analysis is essential to neuroscience, however, the complex geometry of the brain cortex hinders computational methods for this task. The difficulty arises from a discrepancy between 3D imaging data, which is represented in Euclidean space, and the non-Euclidean geometry of the highly-convoluted brain surface. Recent advances in machine learning have enabled the use of neural networks for non-Euclidean spaces. These facilitate the learning of surface data, yet pooling strategies often remain constrained to a single fixed-graph. This paper proposes a new learnable graph pooling method for processing multiple surface-valued data to output subject-based information. The proposed method innovates by learning an intrinsic aggregation of graph nodes based on graph spectral embedding. We illustrate the advantages of our approach with in-depth experiments on two large-scale benchmark datasets. The flexibility of the pooling strategy is evaluated on four different prediction tasks, namely, subject-sex classification, regression of cortical region sizes, classification of Alzheimer's disease stages, and brain age regression. Our experiments demonstrate the superiority of our learnable pooling approach compared to other pooling techniques for graph convolution networks, with results improving the state-of-the-art in brain surface analysis. △ Less","22 November, 2019",https://arxiv.org/pdf/1911.10129
Knowledge Network and a Knowledge Network Example,Hilmi Bahadır Temur;Ahmet Serdar Yılmaz;Mehmet Tekerek,"Knowledge networks can be defined as social networks that enable the transfer of the knowledge, which is defined as the intellectual product formed as a result of the work of human intelligence, to be transferred to any other means of communication. A knowledge network represents a large number of people, resources and relationships between them, to create the highest value, primarily to accumulate and use knowledge through the process of generating and transmitting knowledge. General structure of knowledge networks; it consists of three basic stages: gathering, organizing and disseminating knowledge. The first step, knowledge collection, institutions and organizations to enter the network structure of the knowledge that is present. The organizing phase is the structuring of irregular and unstructured knowledge in the network structure according to certain standards and recording them regularly in the structure. Knowledge dissemination can be expressed as the transfer of organized knowledge in accordance with user knowledge and needs. The purpose of the training knowledge network is to ensure communication between the student, the teacher and the guide, and to store the knowledge that is formed in a course, to enable the system to be recorded in a systematic way, to disseminate it according to the needs of the users and to update the knowledge. A dynamic website and content management system have been developed for the training knowledge network. The training knowledge network has a flexible structure to support web technologies. By making production, management and publication of knowledge within the framework of specific needs, it provides knowledge network for different problems, producing different and tailored solutions. With the example of education knowledge network, a knowledge network that can be developed according to the innovative knowledge network structure is designed. △ Less","22 November, 2019",https://arxiv.org/pdf/1911.09899
MIMAMO Net: Integrating Micro- and Macro-motion for Video Emotion Recognition,Didan Deng;Zhaokang Chen;Yuqian Zhou;Bertram Shi,"Spatial-temporal feature learning is of vital importance for video emotion recognition. Previous deep network structures often focused on macro-motion which extends over long time scales, e.g., on the order of seconds. We believe integrating structures capturing information about both micro- and macro-motion will benefit emotion prediction, because human perceive both micro- and macro-expressions. In this paper, we propose to combine micro- and macro-motion features to improve video emotion recognition with a two-stream recurrent network, named MIMAMO (Micro-Macro-Motion) Net. Specifically, smaller and shorter micro-motions are analyzed by a two-stream network, while larger and more sustained macro-motions can be well captured by a subsequent recurrent network. Assigning specific interpretations to the roles of different parts of the network enables us to make choice of parameters based on prior knowledge: choices that turn out to be optimal. One of the important innovations in our model is the use of interframe phase differences rather than optical flow as input to the temporal stream. Compared with the optical flow, phase differences require less computation and are more robust to illumination changes. Our proposed network achieves state of the art performance on two video emotion datasets, the OMG emotion dataset and the Aff-Wild dataset. The most significant gains are for arousal prediction, for which motion information is intuitively more informative. Source code is available at https://github.com/wtomin/MIMAMO-Net. △ Less","21 November, 2019",https://arxiv.org/pdf/1911.09784
Kooplex: collaborative data analytics portal for advancing sciences,Dávid Visontai;József Stéger;János Márk Szalai-Gindl;László Dobos;László Oroszlány;István Ervin Csabai,"Research collaborations are continuously emerging catalyzed by online platforms, where people can share their codes, calculations, data and results. These virtual research platforms are innovative, community oriented, flexible and secure as required by modern scientific approaches. A wide range of open source and commercial solutions are available in this field emphasizing the relevant aspects of such a platform differently. In this paper we present our open source and modular platform, KOOPLEX, which combines the key concepts of dynamic collaboration, customizable research environment, data sharing, access to datahubs, reproducible research and reporting. It is easily deployable and scalable to serve more users or access large computational resources. △ Less","21 November, 2019",https://arxiv.org/pdf/1911.09553
Automatic Differentiable Monte Carlo: Theory and Application,Shi-Xin Zhang;Zhou-Quan Wan;Hong Yao,"Differentiable programming has emerged as a key programming paradigm empowering rapid developments of deep learning while its applications to important computational methods such as Monte Carlo remain largely unexplored. Here we present the general theory enabling infinite-order automatic differentiation on expectations computed by Monte Carlo with unnormalized probability distributions, which we call ""automatic differentiable Monte Carlo"" (ADMC). By implementing ADMC algorithms on computational graphs, one can also leverage state-of-the-art machine learning frameworks and techniques to traditional Monte Carlo applications in statistics and physics. We illustrate the versatility of ADMC by showing some applications: fast search of phase transitions and accurately finding ground states of interacting many-body models in two dimensions. ADMC paves a promising way to innovate Monte Carlo in various aspects to achieve higher accuracy and efficiency, e.g. easing or solving the sign problem of quantum many-body models through ADMC. △ Less","20 November, 2019",https://arxiv.org/pdf/1911.09117
Gradient Method for Continuous Influence Maximization with Budget-Saving Considerations,Wei Chen;Weizhong Zhang;Haoyu Zhao,"Continuous influence maximization (CIM) generalizes the original influence maximization by incorporating general marketing strategies: a marketing strategy mix is a vector \boldsymbol x = (x_1,\dots,x_d) such that for each node v in a social network, v could be activated as a seed of diffusion with probability h_v(\boldsymbol x), where h_v is a strategy activation function satisfying DR-submodularity. CIM is the task of selecting a strategy mix \boldsymbol x with constraint \sum_i x_i \le k where k is a budget constraint, such that the total number of activated nodes after the diffusion process, called influence spread and denoted as g(\boldsymbol x), is maximized. In this paper, we extend CIM to consider budget saving, that is, each strategy mix \boldsymbol x has a cost c(\boldsymbol x) where c is a convex cost function, we want to maximize the balanced sum g(\boldsymbol x) + λ(k - c(\boldsymbol x)) where λ is a balance parameter, subject to the constraint of c(\boldsymbol x) \le k. We denote this problem as CIM-BS. The objective function of CIM-BS is neither monotone, nor DR-submodular or concave, and thus neither the greedy algorithm nor the standard result on gradient method could be directly applied. Our key innovation is the combination of the gradient method with reverse influence sampling to design algorithms that solve CIM-BS: For the general case, we give an algorithm that achieves \left(\frac{1}{2}-\varepsilon\right)-approximation, and for the case of independent strategy activations, we present an algorithm that achieves \left(1-\frac{1}{e}-\varepsilon\right) approximation. △ Less","20 November, 2019",https://arxiv.org/pdf/1911.09100
Product Innovation through Internal Startup in Large Software Companies: a Case Study,Henry Edison;Xiaofeng Wang;Pekka Abrahamsson,"Product innovation is a risky activity, but when successful, it enables large software companies accrue high profits and leapfrog the competition. Internal startups have been promoted as one way to foster product innovation in large companies, which allows them to innovate as startups do. However, internal startups in large companies are challenging endeavours despite of the promised benefits. How large software companies can leverage internal startups in software product innovation is not fully understood due to the scarcity of the relevant studies. Based on a conceptual framework that combines the elements from the Lean startup approach and an internal corporate venturing model, we conducted a case study of a large software company to examine how a new product was developed through the internal startup effort and struggled to achieve the desired outcomes set by the management. As a result, the conceptual framework was further developed into a Lean startup-enabled new product development model for large software companies. △ Less","20 November, 2019",https://arxiv.org/pdf/1911.08973
"The Future Time Traveller Project: Career Guidance on Future Skills, Jobs and Career Prospects of Generation Z through a Game-Based Virtual World Environment",Michalis Xenos;Catherine Christodoulopoulou;Andreas Mallas;John Garofalakis,"Future Time Traveller is a European project that aims at transforming career guidance of generation Z through an innovative, games-based scenario approach and to prepare the next generation for the jobs of the future. The pro-ject objective is to foster innovative thinking and future-oriented mindset of young people, through an innovative game-based virtual world environment. This environment helps them explore the future world, understand the trends that shape the future world of work, the emerging jobs, and the skills they will require. The Future Time Traveller project is implemented by a team of experts in 7 European countries (Bulgaria, Germany, Greece, Italy, Poland, Portugal, and United Kingdom). The project target groups include young people (genera-tion Z), career guidance practitioners and experts, and policymakers. This paper presents, in brief, the Future Time Traveller project and introduces the reader to the main features and functionalities of the 3-dimensional virtual world and the games developed in this environment. △ Less","19 November, 2019",https://arxiv.org/pdf/1911.08480
Volenti non fit injuria: Ransomware and its Victims,Amir Atapour-Abarghouei;Stephen Bonner;Andrew Stephen McGough,"With the recent growth in the number of malicious activities on the internet, cybersecurity research has seen a boost in the past few years. However, as certain variants of malware can provide highly lucrative opportunities for bad actors, significant resources are dedicated to innovations and improvements by vast criminal organisations. Among these forms of malware, ransomware has experienced a significant recent rise as it offers the perpetrators great financial incentive. Ransomware variants operate by removing system access from the user by either locking the system or encrypting some or all of the data, and subsequently demanding payment or ransom in exchange for returning system access or providing a decryption key to the victim. Due to the ubiquity of sensitive data in many aspects of modern life, many victims of such attacks, be they an individual home user or operators of a business, are forced to pay the ransom to regain access to their data, which in many cases does not happen as renormalisation of system operations is never guaranteed. As the problem of ransomware does not seem to be subsiding, it is very important to investigate the underlying forces driving and facilitating such attacks in order to create preventative measures. As such, in this paper, we discuss and provide further insight into variants of ransomware and their victims in order to understand how and why they have been targeted and what can be done to prevent or mitigate the effects of such attacks. △ Less","19 November, 2019",https://arxiv.org/pdf/1911.08364
Smoke Sky -- Exploring New Frontiers of Unmanned Aerial Systems for Wildland Fire Science and Applications,E. Natasha Stavros;Ali Agha;Allen Sirota;Marco Quadrelli;Kamak Ebadi;Kyongsik Yun,"Wildfire has had increasing impacts on society as the climate changes and the wildland urban interface grows. As such, there is a demand for innovative solutions to help manage fire. Managing wildfire can include proactive fire management such as prescribed burning within constrained areas or advancements for reactive fire management (e.g., fire suppression). Because of the growing societal impact, the JPL BlueSky program sought to assess the current state of fire management and technology and determine areas with high return on investment. To accomplish this, we met with the national interagency Unmanned Aerial System (UAS) Advisory Group (UASAG) and with leading technology transfer experts for fire science and management applications. We provide an overview of the current state as well as an analysis of the impact, maturity and feasibility of integrating different technologies that can be developed by JPL. Based on the findings, the highest return on investment technologies for fire management are first to develop single micro-aerial vehicle (MAV) autonomy, autonomous sensing over fire, and the associated data and information system for active fire local environment mapping. Once this is completed for a single MAV, expanding the work to include many in a swarm would require further investment of distributed MAV autonomy and MAV swarm mechanics, but could greatly expand the breadth of application over large fires. Important to investing in these technologies will be in developing collaborations with the key influencers and champions for using UAS technology in fire management. △ Less","12 November, 2019",https://arxiv.org/pdf/1911.08288
Exploring the added value of blockchain technology for the healthcare domain,Bas R. J. Bolmer;Monique Taverne;Marco Scherer,"In this report, the University Medical Center Groningen (UMCG) has written down lessons learned on how blockchain technology can have an impact on the healthcare domain. By looking at two use-cases, the hospital challenged several teams, participating in an open innovation program and blockchain hackathon, to find a solution that showed the added value of the technology for patient care and scientific research. Besides this practical perspective, the report also considers literature discussing the current state of blockchain technology in regard to developments in the healthcare domain (touching on patient empowerment, data management, regulations, and interoperability between healthcare systems). △ Less","15 November, 2019",https://arxiv.org/pdf/1911.08277
The Design and Implementation of a Scalable DL Benchmarking Platform,Cheng Li;Abdul Dakkak;Jinjun Xiong;Wen-mei Hwu,"The current Deep Learning (DL) landscape is fast-paced and is rife with non-uniform models, hardware/software (HW/SW) stacks, but lacks a DL benchmarking platform to facilitate evaluation and comparison of DL innovations, be it models, frameworks, libraries, or hardware. Due to the lack of a benchmarking platform, the current practice of evaluating the benefits of proposed DL innovations is both arduous and error-prone - stifling the adoption of the innovations. In this work, we first identify 10 design features which are desirable within a DL benchmarking platform. These features include: performing the evaluation in a consistent, reproducible, and scalable manner, being framework and hardware agnostic, supporting real-world benchmarking workloads, providing in-depth model execution inspection across the HW/SW stack levels, etc. We then propose MLModelScope, a DL benchmarking platform design that realizes the 10 objectives. MLModelScope proposes a specification to define DL model evaluations and techniques to provision the evaluation workflow using the user-specified HW/SW stack. MLModelScope defines abstractions for frameworks and supports board range of DL models and evaluation scenarios. We implement MLModelScope as an open-source project with support for all major frameworks and hardware architectures. Through MLModelScope's evaluation and automated analysis workflows, we performed case-study analyses of 37 models across 4 systems and show how model, hardware, and framework selection affects model accuracy and performance under different benchmarking scenarios. We further demonstrated how MLModelScope's tracing capability gives a holistic view of model execution and helps pinpoint bottlenecks. △ Less","18 November, 2019",https://arxiv.org/pdf/1911.08031
AIM 2019 Challenge on Real-World Image Super-Resolution: Methods and Results,Andreas Lugmayr;Martin Danelljan;Radu Timofte;Manuel Fritsche;Shuhang Gu;Kuldeep Purohit;Praveen Kandula;Maitreya Suin;A N Rajagopalan;Nam Hyung Joon;Yu Seung Won;Guisik Kim;Dokyeong Kwon;Chih-Chung Hsu;Chia-Hsiang Lin;Yuanfei Huang;Xiaopeng Sun;Wen Lu;Jie Li;Xinbo Gao;Sefi Bell-Kligler,"This paper reviews the AIM 2019 challenge on real world super-resolution. It focuses on the participating methods and final results. The challenge addresses the real world setting, where paired true high and low-resolution images are unavailable. For training, only one set of source input images is therefore provided in the challenge. In Track 1: Source Domain the aim is to super-resolve such images while preserving the low level image characteristics of the source input domain. In Track 2: Target Domain a set of high-quality images is also provided for training, that defines the output domain and desired quality of the super-resolved images. To allow for quantitative evaluation, the source input images in both tracks are constructed using artificial, but realistic, image degradations. The challenge is the first of its kind, aiming to advance the state-of-the-art and provide a standard benchmark for this newly emerging task. In total 7 teams competed in the final testing phase, demonstrating new and innovative solutions to the problem. △ Less","19 November, 2019",https://arxiv.org/pdf/1911.07783
A Spark ML driven preprocessing approach for deep learning based scholarly data applications,Samiya Khan;Xiufeng Liu;Mansaf Alam,"Big data has found applications in multiple domains. One of the largest sources of textual big data is scientific documents and papers. Big scholarly data have been used in numerous ways to create innovative applications such as collaborator discovery, expert finding and research management systems. With the advent of advanced machine and deep learning techniques, the accuracy and novelty of such applications have risen manifold. However, the biggest challenge in the development of deep learning models for scholarly applications in cloud based environment is the underutilization of resources because of the excessive time taken by textual preprocessing. This paper presents a preprocessing pipeline that makes use of Spark for data ingestion and Spark ML for pipelining preprocessing tasks. The evaluation of the proposed work is done using a case study, which uses LSTM based text summarization for generating title or summary from abstract of any research. The ingestion, preprocessing and cumulative time for the proposed approach (P3SAPP) is much lower than the conventional approach (CA), which manifests in reduction of costs as well. △ Less","4 November, 2019",https://arxiv.org/pdf/1911.07763
SOGNet: Scene Overlap Graph Network for Panoptic Segmentation,Yibo Yang;Hongyang Li;Xia Li;Qijie Zhao;Jianlong Wu;Zhouchen Lin,"The panoptic segmentation task requires a unified result from semantic and instance segmentation outputs that may contain overlaps. However, current studies widely ignore modeling overlaps. In this study, we aim to model overlap relations among instances and resolve them for panoptic segmentation. Inspired by scene graph representation, we formulate the overlapping problem as a simplified case, named scene overlap graph. We leverage each object's category, geometry and appearance features to perform relational embedding, and output a relation matrix that encodes overlap relations. In order to overcome the lack of supervision, we introduce a differentiable module to resolve the overlap between any pair of instances. The mask logits after removing overlaps are fed into per-pixel instance \verb|id| classification, which leverages the panoptic supervision to assist in the modeling of overlap relations. Besides, we generate an approximate ground truth of overlap relations as the weak supervision, to quantify the accuracy of overlap relations predicted by our method. Experiments on COCO and Cityscapes demonstrate that our method is able to accurately predict overlap relations, and outperform the state-of-the-art performance for panoptic segmentation. Our method also won the Innovation Award in COCO 2019 challenge. △ Less","18 November, 2019",https://arxiv.org/pdf/1911.07527
Towards Robust RGB-D Human Mesh Recovery,Ren Li;Changjiang Cai;Georgios Georgakis;Srikrishna Karanam;Terrence Chen;Ziyan Wu,"We consider the problem of human pose estimation. While much recent work has focused on the RGB domain, these techniques are inherently under-constrained since there can be many 3D configurations that explain the same 2D projection. To this end, we propose a new method that uses RGB-D data to estimate a parametric human mesh model. Our key innovations include (a) the design of a new dynamic data fusion module that facilitates learning with a combination of RGB-only and RGB-D datasets, (b) a new constraint generator module that provides SMPL supervisory signals when explicit SMPL annotations are not available, and (c) the design of a new depth ranking learning objective, all of which enable principled model training with RGB-D data. We conduct extensive experiments on a variety of RGB-D datasets to demonstrate efficacy. △ Less","17 November, 2019",https://arxiv.org/pdf/1911.07383
Autonomics: In Search of a Foundation for Next Generation Autonomous Systems,David Harel;Assaf Marron;Joseph Sifakis,"The potential benefits of autonomous systems have been driving intensive development of such systems, and of supporting tools and methodologies. However, there are still major issues to be dealt with before such development becomes commonplace engineering practice, with accepted and trustworthy deliverables. We argue that a solid, evolving, publicly available, community-controlled foundation for developing next generation autonomous systems is a must. We discuss what is needed for such a foundation, identify a central aspect thereof, namely, decision-making, and focus on three main challenges: (i) how to specify autonomous system behavior and the associated decisions in the face of unpredictability of future events and conditions and the inadequacy of current languages for describing these; (ii) how to carry out faithful simulation and analysis of system behavior with respect to rich environments that include humans, physical artifacts, and other systems,; and (iii) how to engineer systems that combine executable model-driven techniques and data-driven machine learning techniques. We argue that autonomics, i.e., the study of unique challenges presented by next generation autonomous systems, and research towards resolving them, can introduce substantial contributions and innovations in system engineering and computer science. △ Less","16 November, 2019",https://arxiv.org/pdf/1911.07133
HealthFog: An Ensemble Deep Learning based Smart Healthcare System for Automatic Diagnosis of Heart Diseases in Integrated IoT and Fog Computing Environments,Shreshth Tuli;Nipam Basumatary;Sukhpal Singh Gill;Mohsen Kahani;Rajesh Chand Arya;Gurpreet Singh Wander;Rajkumar Buyya,"Cloud computing provides resources over the Internet and allows a plethora of applications to be deployed to provide services for different industries. The major bottleneck being faced currently in these cloud frameworks is their limited scalability and hence inability to cater to the requirements of centralized Internet of Things (IoT) based compute environments. The main reason for this is that latency-sensitive applications like health monitoring and surveillance systems now require computation over large amounts of data (Big Data) transferred to centralized database and from database to cloud data centers which leads to drop in performance of such systems. The new paradigms of fog and edge computing provide innovative solutions by bringing resources closer to the user and provide low latency and energy-efficient solutions for data processing compared to cloud domains. Still, the current fog models have many limitations and focus from a limited perspective on either accuracy of results or reduced response time but not both. We proposed a novel framework called HealthFog for integrating ensemble deep learning in Edge computing devices and deployed it for a real-life application of automatic Heart Disease analysis. HealthFog delivers healthcare as a fog service using IoT devices and efficiently manages the data of heart patients, which comes as user requests. Fog-enabled cloud framework, FogBus is used to deploy and test the performance of the proposed model in terms of power consumption, network bandwidth, latency, jitter, accuracy and execution time. HealthFog is configurable to various operation modes that provide the best Quality of Service or prediction accuracy, as required, in diverse fog computation scenarios and for different user requirements. △ Less","15 November, 2019",https://arxiv.org/pdf/1911.06633
Learning To Characterize Adversarial Subspaces,Xiaofeng Mao;Yuefeng Chen;Yuhong Li;Yuan He;Hui Xue,"Deep Neural Networks (DNNs) are known to be vulnerable to the maliciously generated adversarial examples. To detect these adversarial examples, previous methods use artificially designed metrics to characterize the properties of \textit{adversarial subspaces} where adversarial examples lie. However, we find these methods are not working in practical attack detection scenarios. Because the artificially defined features are lack of robustness and show limitation in discriminative power to detect strong attacks. To solve this problem, we propose a novel adversarial detection method which identifies adversaries by adaptively learning reasonable metrics to characterize adversarial subspaces. As auxiliary context information, \textit{k} nearest neighbors are used to represent the surrounded subspace of the detected sample. We propose an innovative model called Neighbor Context Encoder (NCE) to learn from \textit{k} neighbors context and infer if the detected sample is normal or adversarial. We conduct thorough experiment on CIFAR-10, CIFAR-100 and ImageNet dataset. The results demonstrate that our approach surpasses all existing methods under three settings: \textit{attack-aware black-box detection}, \textit{attack-unaware black-box detection} and \textit{white-box detection}. △ Less","15 November, 2019",https://arxiv.org/pdf/1911.06587
"Response to NITRD, NCO, NSF Request for Information on ""Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan""",J. Amundson;J. Annis;C. Avestruz;D. Bowring;J. Caldeira;G. Cerati;C. Chang;S. Dodelson;D. Elvira;A. Farahi;K. Genser;L. Gray;O. Gutsche;P. Harris;J. Kinney;J. B. Kowalkowski;R. Kutschke;S. Mrenna;B. Nord;A. Para;K. Pedro;G. N. Perdue;A. Scheinker;P. Spentzouris;J. St. John,"We present a response to the 2018 Request for Information (RFI) from the NITRD, NCO, NSF regarding the ""Update to the 2016 National Artificial Intelligence Research and Development Strategic Plan."" Through this document, we provide a response to the question of whether and how the National Artificial Intelligence Research and Development Strategic Plan (NAIRDSP) should be updated from the perspective of Fermilab, America's premier national laboratory for High Energy Physics (HEP). We believe the NAIRDSP should be extended in light of the rapid pace of development and innovation in the field of Artificial Intelligence (AI) since 2016, and present our recommendations below. AI has profoundly impacted many areas of human life, promising to dramatically reshape society --- e.g., economy, education, science --- in the coming years. We are still early in this process. It is critical to invest now in this technology to ensure it is safe and deployed ethically. Science and society both have a strong need for accuracy, efficiency, transparency, and accountability in algorithms, making investments in scientific AI particularly valuable. Thus far the US has been a leader in AI technologies, and we believe as a national Laboratory it is crucial to help maintain and extend this leadership. Moreover, investments in AI will be important for maintaining US leadership in the physical sciences. △ Less","4 November, 2019",https://arxiv.org/pdf/1911.05796
Atomic Services: sustainable ecosystem of smart city services through pan-European collaboration,Flavio Cirillo;Detlef Straeten;David Gomez;Jose Gato;Luis Diez;Ignacio Elicegui Maestro;Reza Akhavan,"In a world with an ever increasing urbanization, governance is investigating innovative solutions to sustain the society evolution. Internet-of-Things promises huge benefits for cities and the proliferation of smart city deployments demonstrates the common acceptance of IoT as basis for many solutions. The city pilots developments occurred in parallel and with different designs thus creating fragmentation of IoT. The European project SynchroniCity aims to synchronize 8 smart cities to establish a shared environment fostering a self-sustained business growth. In this article we present the collaborative methodology and shared efforts spent towards the creation of a common ecosystem for the development of smart city services. Our design evolves around the concept of ""atomic services"" that implements a single functional block to be composed for full-fledged smart city services. This creates opportunities for diverse stakeholders to participate to a global smart cities market. The methodology and outcome of our efforts will be followed by 10 new cities globally, thus expanding the market range for IoT stakeholders △ Less","8 November, 2019",https://arxiv.org/pdf/1911.05719
Going Negative Online? -- A Study of Negative Advertising on Social Media,Hongtao Liu,"A growing number of empirical studies suggest that negative advertising is effective in campaigning, while the mechanisms are rarely mentioned. With the scandal of Cambridge Analytica and Russian intervention behind the Brexit and the 2016 presidential election, people have become aware of the political ads on social media and have pressured congress to restrict political advertising on social media. Following the related legislation, social media companies began disclosing their political ads archive for transparency during the summer of 2018 when the midterm election campaign was just beginning. This research collects the data of the related political ads in the context of the U.S. midterm elections since August to study the overall pattern of political ads on social media and uses sets of machine learning methods to conduct sentiment analysis on these ads to classify the negative ads. A novel approach is applied that uses AI image recognition to study the image data. Through data visualization, this research shows that negative advertising is still the minority, Republican advertisers and third party organizations are more likely to engage in negative advertising than their counterparts. Based on ordinal regressions, this study finds that anger evoked information-seeking is one of the main mechanisms causing negative ads to be more engaging and effective rather than the negative bias theory. Overall, this study provides a unique understanding of political advertising on social media by applying innovative data science methods. Further studies can extend the findings, methods, and datasets in this study, and several suggestions are given for future research. △ Less","14 October, 2019",https://arxiv.org/pdf/1911.05497
PHASEN: A Phase-and-Harmonics-Aware Speech Enhancement Network,Dacheng Yin;Chong Luo;Zhiwei Xiong;Wenjun Zeng,"Time-frequency (T-F) domain masking is a mainstream approach for single-channel speech enhancement. Recently, focuses have been put to phase prediction in addition to amplitude prediction. In this paper, we propose a phase-and-harmonics-aware deep neural network (DNN), named PHASEN, for this task. Unlike previous methods that directly use a complex ideal ratio mask to supervise the DNN learning, we design a two-stream network, where amplitude stream and phase stream are dedicated to amplitude and phase prediction. We discover that the two streams should communicate with each other, and this is crucial to phase prediction. In addition, we propose frequency transformation blocks to catch long-range correlations along the frequency axis. The visualization shows that the learned transformation matrix spontaneously captures the harmonic correlation, which has been proven to be helpful for T-F spectrogram reconstruction. With these two innovations, PHASEN acquires the ability to handle detailed phase patterns and to utilize harmonic patterns, getting 1.76dB SDR improvement on AVSpeech + AudioSet dataset. It also achieves significant gains over Google's network on this dataset. On Voice Bank + DEMAND dataset, PHASEN outperforms previous methods by a large margin on four metrics. △ Less","12 November, 2019",https://arxiv.org/pdf/1911.04697
Shorter Distances between Papers over Time are Due to More Cross-Field References and Increased Citation Rate to Higher Impact Papers,Attila Varga,"The exponential increase in the number of scientific publications raises the question of whether the sciences are expanding into a fractured structure, making cross-field communication difficult. On the other hand, scientists may be motivated to learn extensively across fields to enhance their innovative capacity, and this may offset the negative effects of fragmentation. Through an investigation of the distances within and clustering of cross-sectional citation networks, this study presents evidence that fields of science become more integrated over time. The average citation distance between papers published in the same year decreased from approximately 5.33 to 3.18 steps between 1950 and 2018. This observation is attributed to the growth of cross-field communication throughout the entire period as well as the growing importance of high impact papers to bridge networks in the same year. Three empirical findings support this conclusion. First, distances decreased between almost all disciplines throughout the time period. Second, inequality in the number of citations received by papers increased, and as a consequence the shortest paths in the network depend more on high impact papers later in the period. Third, the dispersion of connections between fields increased continually. Moreover, these changes did not entail a lower level of clustering of citations. Both within- and cross-field citations show a similar rate of slowly growing clustering values in all years. The latter findings suggest that domain spanning scholarly communication is partly enabled by new fields that connect disciplines. △ Less","11 November, 2019",https://arxiv.org/pdf/1911.04548
Multidataset Independent Subspace Analysis with Application to Multimodal Fusion,Rogers F. Silva;Sergey M. Plis;Tulay Adali;Marios S. Pattichis;Vince D. Calhoun,"In the last two decades, unsupervised latent variable models---blind source separation (BSS) especially---have enjoyed a strong reputation for the interpretable features they produce. Seldom do these models combine the rich diversity of information available in multiple datasets. Multidatasets, on the other hand, yield joint solutions otherwise unavailable in isolation, with a potential for pivotal insights into complex systems. To take advantage of the complex multidimensional subspace structures that capture underlying modes of shared and unique variability across and within datasets, we present a direct, principled approach to multidataset combination. We design a new method called multidataset independent subspace analysis (MISA) that leverages joint information from multiple heterogeneous datasets in a flexible and synergistic fashion. Methodological innovations exploiting the Kotz distribution for subspace modeling in conjunction with a novel combinatorial optimization for evasion of local minima enable MISA to produce a robust generalization of independent component analysis (ICA), independent vector analysis (IVA), and independent subspace analysis (ISA) in a single unified model. We highlight the utility of MISA for multimodal information fusion, including sample-poor regimes and low signal-to-noise ratio scenarios, promoting novel applications in both unimodal and multimodal brain imaging data. △ Less","10 November, 2019",https://arxiv.org/pdf/1911.04048
XceptionTime: A Novel Deep Architecture based on Depthwise Separable Convolutions for Hand Gesture Classification,Elahe Rahimian;Soheil Zabihi;Seyed Farokh Atashzar;Amir Asif;Arash Mohammadi,"Capitalizing on the need for addressing the existing challenges associated with gesture recognition via sparse multichannel surface Electromyography (sEMG) signals, the paper proposes a novel deep learning model, referred to as the XceptionTime architecture. The proposed innovative XceptionTime is designed by integration of depthwise separable convolutions, adaptive average pooling, and a novel non-linear normalization technique. At the heart of the proposed architecture is several XceptionTime modules concatenated in series fashion designed to capture both temporal and spatial information-bearing contents of the sparse multichannel sEMG signals without the need for data augmentation and/or manual design of feature extraction. In addition, through integration of adaptive average pooling, Conv1D, and the non-linear normalization approach, XceptionTime is less prone to overfitting, more robust to temporal translation of the input, and more importantly is independent from the input window size. Finally, by utilizing the depthwise separable convolutions, the XceptionTime network has far fewer parameters resulting in a less complex network. The performance of XceptionTime is tested on a sub Ninapro dataset, DB1, and the results showed a superior performance in comparison to any existing counterparts. In this regard, 5:71% accuracy improvement, on a window size 200ms, is reported in this paper, for the first time. △ Less","9 November, 2019",https://arxiv.org/pdf/1911.03803
Sequence-Aware Factorization Machines for Temporal Predictive Analytics,Tong Chen;Hongzhi Yin;Quoc Viet Hung Nguyen;Wen-Chih Peng;Xue Li;Xiaofang Zhou,"In various web applications like targeted advertising and recommender systems, the available categorical features (e.g., product type) are often of great importance but sparse. As a widely adopted solution, models based on Factorization Machines (FMs) are capable of modelling high-order interactions among features for effective sparse predictive analytics. As the volume of web-scale data grows exponentially over time, sparse predictive analytics inevitably involves dynamic and sequential features. However, existing FM-based models assume no temporal orders in the data, and are unable to capture the sequential dependencies or patterns within the dynamic features, impeding the performance and adaptivity of these methods. Hence, in this paper, we propose a novel Sequence-Aware Factorization Machine (SeqFM) for temporal predictive analytics, which models feature interactions by fully investigating the effect of sequential dependencies. As static features (e.g., user gender) and dynamic features (e.g., user interacted items) express different semantics, we innovatively devise a multi-view self-attention scheme that separately models the effect of static features, dynamic features and the mutual interactions between static and dynamic features in three different views. In SeqFM, we further map the learned representations of feature interactions to the desired output with a shared residual network. To showcase the versatility and generalizability of SeqFM, we test SeqFM in three popular application scenarios for FM-based models, namely ranking, classification and regression tasks. Extensive experimental results on six large-scale datasets demonstrate the superior effectiveness and efficiency of SeqFM. △ Less","17 November, 2019",https://arxiv.org/pdf/1911.02752
Adversarial dictionary learning for a robust analysis and modelling of spontaneous neuronal activity,Eirini Troullinou;Grigorios Tsagkatakis;Ganna Palagina;Maria Papadopouli;Stelios Manolis Smirnakis;Panagiotis Tsakalides,"The field of neuroscience is experiencing rapid growth in the complexity and quantity of the recorded neural activity allowing us unprecedented access to its dynamics in different brain areas. The objective of this work is to discover directly from the experimental data rich and comprehensible models for brain function that will be concurrently robust to noise. Considering this task from the perspective of dimensionality reduction, we develop an innovative, robust to noise dictionary learning framework based on adversarial training methods for the identification of patterns of synchronous firing activity as well as within a time lag. We employ real-world binary datasets describing the spontaneous neuronal activity of laboratory mice over time, and we aim to their efficient low-dimensional representation. The results on the classification accuracy for the discrimination between the clean and the adversarial-noisy activation patterns obtained by an SVM classifier highlight the efficacy of the proposed scheme compared to other methods, and the visualization of the dictionary's distribution demonstrates the multifarious information that we obtain from it. △ Less","24 December, 2019",https://arxiv.org/pdf/1911.01721
Deep Heterogeneous Hashing for Face Video Retrieval,Shishi Qiao;Ruiping Wang;Shiguang Shan;Xilin Chen,"Retrieving videos of a particular person with face image as a query via hashing technique has many important applications. While face images are typically represented as vectors in Euclidean space, characterizing face videos with some robust set modeling techniques (e.g. covariance matrices as exploited in this study, which reside on Riemannian manifold), has recently shown appealing advantages. This hence results in a thorny heterogeneous spaces matching problem. Moreover, hashing with handcrafted features as done in many existing works is clearly inadequate to achieve desirable performance for this task. To address such problems, we present an end-to-end Deep Heterogeneous Hashing (DHH) method that integrates three stages including image feature learning, video modeling, and heterogeneous hashing in a single framework, to learn unified binary codes for both face images and videos. To tackle the key challenge of hashing on the manifold, a well-studied Riemannian kernel mapping is employed to project data (i.e. covariance matrices) into Euclidean space and thus enables to embed the two heterogeneous representations into a common Hamming space, where both intra-space discriminability and inter-space compatibility are considered. To perform network optimization, the gradient of the kernel mapping is innovatively derived via structured matrix backpropagation in a theoretically principled way. Experiments on three challenging datasets show that our method achieves quite competitive performance compared with existing hashing methods. △ Less","4 November, 2019",https://arxiv.org/pdf/1911.01048
"Precision Medicine Informatics: Principles, Prospects, and Challenges",Muhammad Afzal;S. M. Riazul Islam;Maqbool Hussain;Sungyoung Lee,"Precision Medicine (PM) is an emerging approach that appears with the impression of changing the existing paradigm of medical practice. Recent advances in technological innovations and genetics, and the growing availability of health data have set a new pace of the research and imposes a set of new requirements on different stakeholders. To date, some studies are available that discuss about different aspects of PM. Nevertheless, a holistic representation of those aspects deemed to confer the technological perspective, in relation to applications and challenges, is mostly ignored. In this context, this paper surveys advances in PM from informatics viewpoint and reviews the enabling tools and techniques in a categorized manner. In addition, the study discusses how other technological paradigms including big data, artificial intelligence, and internet of things can be exploited to advance the potentials of PM. Furthermore, the paper provides some guidelines for future research for seamless implementation and wide-scale deployment of PM based on identified open issues and associated challenges. To this end, the paper proposes an integrated holistic framework for PM motivating informatics researchers to design their relevant research works in an appropriate context. △ Less","3 November, 2019",https://arxiv.org/pdf/1911.01014
eBrainII: A 3 kW Realtime Custom 3D DRAM integrated ASIC implementation of a Biologically Plausible Model of a Human Scale Cortex,Dimitrios Stathis;Chirag Sudarshan;Yu Yang;Matthias Jung;Syed Asad Mohamad Hasan Jafri;Christian Weis;Ahmed Hemani;Anders Lansner;Norbert Wehn,"The Artificial Neural Networks (ANNs) like CNN/DNN and LSTM are not biologically plausible and in spite of their initial success, they cannot attain the cognitive capabilities enabled by the dynamic hierarchical associative memory systems of biological brains. The biologically plausible spiking brain models, for e.g. cortex, basal ganglia and amygdala have a greater potential to achieve biological brain like cognitive capabilities. Bayesian Confidence Propagation Neural Network (BCPNN) is a biologically plausible spiking model of cortex. A human scale model of BCPNN in real time requires 162 TFlops/s, 50 TBs of synaptic weight storage to be accessed with a bandwidth of 200 TBs. The spiking bandwidth is relatively modest at 250 GBs/s. A hand optimized implementation of rodent scale BCPNN has been implemented on Tesla K80 GPUs require 3 kW, we extrapolate from that a human scale network will require 3 MW. These power numbers rule out such implementations for field deployment as advanced cognition engines in embedded systems. The key innovation that this paper reports is that it is feasible and affordable to implement real time BCPNN as a custom tiled ASIC in 28 nm technology with custom 3D DRAM - eBrain II - that consumes 3 kWs for human scale and 12 W for rodent scale cortex model. Such implementations eminently fulfill the demands for field deployment. △ Less","3 November, 2019",https://arxiv.org/pdf/1911.00889
A neural document language modeling framework for spoken document retrieval,Li-Phen Yen;Zhen-Yu Wu;Kuan-Yu Chen,"Recent developments in deep learning have led to a significant innovation in various classic and practical subjects, including speech recognition, computer vision, question answering, information retrieval and so on. In the context of natural language processing (NLP), language representations have shown giant successes in many downstream tasks, so the school of studies have become a major stream of research recently. Because the immenseness of multimedia data along with speech have spread around the world in our daily life, spoken document retrieval (SDR) has become an important research subject in the past decades. Targeting on enhancing the SDR performance, the paper concentrates on proposing a neural retrieval framework, which assembles the merits of using language modeling (LM) mechanism in SDR and leveraging the abstractive information learned by the language representation models. Consequently, to our knowledge, this is a pioneer study on supervised training of a neural LM-based SDR framework, especially combined with the pretrained language representation methods. △ Less","31 October, 2019",https://arxiv.org/pdf/1910.14286
S4G: Amodal Single-view Single-Shot SE(3) Grasp Detection in Cluttered Scenes,Yuzhe Qin;Rui Chen;Hao Zhu;Meng Song;Jing Xu;Hao Su,"Grasping is among the most fundamental and long-lasting problems in robotics study. This paper studies the problem of 6-DoF(degree of freedom) grasping by a parallel gripper in a cluttered scene captured using a commodity depth sensor from a single viewpoint. We address the problem in a learning-based framework. At the high level, we rely on a single-shot grasp proposal network, trained with synthetic data and tested in real-world scenarios. Our single-shot neural network architecture can predict amodal grasp proposal efficiently and effectively. Our training data synthesis pipeline can generate scenes of complex object configuration and leverage an innovative gripper contact model to create dense and high-quality grasp annotations. Experiments in synthetic and real environments have demonstrated that the proposed approach can outperform state-of-the-arts by a large margin. △ Less","30 October, 2019",https://arxiv.org/pdf/1910.14218
Balancing Multi-level Interactions for Session-based Recommendation,Yujia Zheng;Siyi Liu;Zailei Zhou,"Predicting user actions based on anonymous sessions is a challenge to general recommendation systems because the lack of user profiles heavily limits data-driven models. Recently, session-based recommendation methods have achieved remarkable results in dealing with this task. However, the upper bound of performance can still be boosted through the innovative exploration of limited data. In this paper, we propose a novel method, namely Intra-and Inter-session Interaction-aware Graph-enhanced Network, to take inter-session item-level interactions into account. Different from existing intra-session item-level interactions and session-level collaborative information, our introduced data represents complex item-level interactions between different sessions. For mining the new data without breaking the equilibrium of the model between different interactions, we construct an intra-session graph and an inter-session graph for the current session. The former focuses on item-level interactions within a single session and the latter models those between items among neighborhood sessions. Then different approaches are employed to encode the information of two graphs according to different structures, and the generated latent vectors are combined to balance the model across different scopes. Experiments on real-world datasets verify that our method outperforms other state-of-the-art methods. △ Less","29 October, 2019",https://arxiv.org/pdf/1910.13527
"A Semi-Automated Approach for Information Extraction, Classification and Analysis of Unstructured Data",Alberto Purpura;Marco Calaresu,"In this paper, we show how Quantitative Narrative Analysis and simple Natural Language Processing techniques apply to the extraction and categorization of data in a sample case study of the Diary of the former President of the Italian Republic (PoR), Giorgio Napolitano. The Diary contains a record of all his institutional meetings. This information, if properly handled, allows for an analysis of how the PoR used his so-called soft-powers to influence the Italian political system during his first mandate. In this paper, we propose a way to use simple, yet very effective, Natural Language Processing techniques - such as Regular Expressions and Named Entity Recognition - to extract information from the Diary. Then, we propose an innovative way to organize the extracted data relying on the methodological framework of Quantitative Narrative Analysis. Finally, we show how to analyze the structured data under different levels of detail using PC-ACE (Program for Computer-Assisted Coding of Events), a software developed specifically for this task and for data visualization. △ Less","20 October, 2019",https://arxiv.org/pdf/1910.12734
Generation of digital patients for the simulation of tuberculosis with UISS-TB,Marzio Pennisi;Miguel A. Juarez;Giulia Russo;Marco Viceconti;Francesco Pappalardo,"EC funded STriTuVaD project aims to test, through a phase IIb clinical trial, two of the most advanced therapeutic vaccines against tuberculosis. In parallel, we have extended the Universal Immune System Simulator to include all relevant determinants of such clinical trial, to establish its predictive accuracy against the individual patients recruited in the trial, to use it to generate digital patients and predict their response to the HRT being tested, and to combine them to the observations made on physical patients using a new in silico-augmented clinical trial approach that uses a Bayesian adaptive design. This approach, where found effective could drastically reduce the cost of innovation in this critical sector of public healthcare. One of the most challenging task is to develop a methodology to reproduce biological diversity of the subjects that have to be simulated, i.e., provide an appropriate strategy for the generation of libraries of digital patients. This has been achieved through the the creation of the initial immune system repertoire in a stochastic way, and though the identification of a ""vector of features"" that combines both biological and pathophysiological parameters that personalize the digital patient to reproduce the physiology and the pathophysiology of the subject. △ Less","27 October, 2019",https://arxiv.org/pdf/1910.12293
Kuksa: A Cloud-Native Architecture for Enabling Continuous Delivery in the Automotive Domain,Ahmad Banijamali;Pooyan Jamshidi;Pasi Kuvaja;Markku Oivo,"Connecting vehicles to cloud platforms has enabled innovative business scenarios while raising new quality concerns, such as reliability and scalability, which must be addressed by research. Cloud-native architectures based on microservices are a recent approach to enable continuous delivery and to improve service reliability and scalability. We propose an approach for restructuring cloud platform architectures in the automotive domain into a microservices architecture. To this end, we adopted and implemented microservices patterns from literature to design the cloud-native automotive architecture and conducted a laboratory experiment to evaluate the reliability and scalability of microservices in the context of a real-world project in the automotive domain called Eclipse Kuksa. Findings indicated that the proposed architecture could handle the continuous software delivery over-the-air by sending automatic control messages to a vehicular setting. Different patterns enabled us to make changes or interrupt services without extending the impact to others. The results of this study provide evidences that microservices are a potential design solution when dealing with service failures and high payload on cloud-based services in the automotive domain. △ Less","22 October, 2019",https://arxiv.org/pdf/1910.10190
Artificial Intelligence and the Future of Psychiatry: Qualitative Findings from a Global Physician Survey,Charlotte Blease;Cosima Locher;Marisa Leon-Carlyle;P. Murali Doraiswamy,"The potential for machine learning to disrupt the medical profession is the subject of ongoing debate within biomedical informatics. This study aimed to explore psychiatrists' opinions about the potential impact of innovations in artificial intelligence and machine learning on psychiatric practice. In Spring 2019, we conducted a web-based survey of 791 psychiatrists from 22 countries worldwide. The survey measured opinions about the likelihood future technology would fully replace physicians in performing ten key psychiatric tasks. This study involved qualitative descriptive analysis of written response to three open-ended questions in the survey. Comments were classified into four major categories in relation to the impact of future technology on patient-psychiatric interactions, the quality of patient medical care, the profession of psychiatry, and health systems. Overwhelmingly, psychiatrists were skeptical that technology could fully replace human empathy. Many predicted that 'man and machine' would increasingly collaborate in undertaking clinical decisions, with mixed opinions about the benefits and harms of such an arrangement. Participants were optimistic that technology might improve efficiencies and access to care, and reduce costs. Ethical and regulatory considerations received limited attention. This study presents timely information of psychiatrists' view about the scope of artificial intelligence and machine learning on psychiatric practice. Psychiatrists expressed divergent views about the value and impact of future technology with worrying omissions about practice guidelines, and ethical and regulatory issues. △ Less","22 October, 2019",https://arxiv.org/pdf/1910.09956
Scalable Neural Dialogue State Tracking,Vevake Balaraman;Bernardo Magnini,"A Dialogue State Tracker (DST) is a key component in a dialogue system aiming at estimating the beliefs of possible user goals at each dialogue turn. Most of the current DST trackers make use of recurrent neural networks and are based on complex architectures that manage several aspects of a dialogue, including the user utterance, the system actions, and the slot-value pairs defined in a domain ontology. However, the complexity of such neural architectures incurs into a considerable latency in the dialogue state prediction, which limits the deployments of the models in real-world applications, particularly when task scalability (i.e. amount of slots) is a crucial factor. In this paper, we propose an innovative neural model for dialogue state tracking, named Global encoder and Slot-Attentive decoders (G-SAT), which can predict the dialogue state with a very low latency time, while maintaining high-level performance. We report experiments on three different languages (English, Italian, and German) of the WoZ2.0 dataset, and show that the proposed approach provides competitive advantages over state-of-art DST systems, both in terms of accuracy and in terms of time complexity for predictions, being over 15 times faster than the other systems. △ Less","22 October, 2019",https://arxiv.org/pdf/1910.09942
Fixed Pattern Noise Reduction for Infrared Images Based on Cascade Residual Attention CNN,Juntao Guan;Rui Lai;Ai Xiong;Zesheng Liu;Lin Gu,"Existing fixed pattern noise reduction (FPNR) methods are easily affected by the motion state of the scene and working condition of the image sensor, which leads to over smooth effects, ghosting artifacts as well as slow convergence rate. To address these issues, we design an innovative cascade convolution neural network (CNN) model with residual skip connections to realize single frame blind FPNR operation without any parameter tuning. Moreover, a coarse-fine convolution (CF-Conv) unit is introduced to extract complementary features in various scales and fuse them to pick more spatial information. Inspired by the success of the visual attention mechanism, we further propose a particular spatial-channel noise attention unit (SCNAU) to separate the scene details from fixed pattern noise more thoroughly and recover the real scene more accurately. Experimental results on test data demonstrate that the proposed cascade CNN-FPNR method outperforms the existing FPNR methods in both of visual effect and quantitative assessment. △ Less","22 October, 2019",https://arxiv.org/pdf/1910.09858
Approximate Sampling using an Accelerated Metropolis-Hastings based on Bayesian Optimization and Gaussian Processes,Asif J. Chowdhury;Gabriel Terejanu,"Markov Chain Monte Carlo (MCMC) methods have a drawback when working with a target distribution or likelihood function that is computationally expensive to evaluate, specially when working with big data. This paper focuses on Metropolis-Hastings (MH) algorithm for unimodal distributions. Here, an enhanced MH algorithm is proposed that requires less number of expensive function evaluations, has shorter burn-in period, and uses a better proposal distribution. The main innovations include the use of Bayesian optimization to reach the high probability region quickly, emulating the target distribution using Gaussian processes (GP), and using Laplace approximation of the GP to build a proposal distribution that captures the underlying correlation better. The experiments show significant improvement over the regular MH. Statistical comparison between the results from two algorithms is presented. △ Less","21 October, 2019",https://arxiv.org/pdf/1910.09347
Resilient Distributed Recovery of Large Fields,Yuan Chen;Soummya Kar;José M. F. Moura,"This paper studies the resilient distributed recovery of large fields under measurement attacks, by a team of agents, where each measures a small subset of the components of a large spatially distributed field. An adversary corrupts some of the measurements. The agents collaborate to process their measurements, and each is interested in recovering only a fraction of the field. We present a field recovery consensus+innovations type distributed algorithm that is resilient to measurement attacks, where an agent maintains and updates a local state based on its neighbors states and its own measurement. Under sufficient conditions on the attacker and the connectivity of the communication network, each agent's state, even those with compromised measurements, converges to the true value of the field components that it is interested in recovering. Finally, we illustrate the performance of our algorithm through numerical examples. △ Less","19 October, 2019",https://arxiv.org/pdf/1910.08841
ELSA: A Throughput-Optimized Design of an LSTM Accelerator for Energy-Constrained Devices,Elham Azari;Sarma Vrudhula,"The next significant step in the evolution and proliferation of artificial intelligence technology will be the integration of neural network (NN) models within embedded and mobile systems. This calls for the design of compact, energy efficient NN models in silicon. In this paper, we present a scalable ASIC design of an LSTM accelerator named ELSA, that is suitable for energy-constrained devices. It includes several architectural innovations to achieve small area and high energy efficiency. To reduce the area and power consumption of the overall design, the compute-intensive units of ELSA employ approximate multiplications and still achieve high performance and accuracy. The performance is further improved through efficient synchronization of the elastic pipeline stages to maximize the utilization. The paper also includes a performance model of ELSA, as a function of the hidden nodes and time steps, permitting its use for the evaluation of any LSTM application. ELSA was implemented in RTL and was synthesized and placed and routed in 65nm technology. Its functionality is demonstrated for language modeling-a common application of LSTM. ELSA is compared against a baseline implementation of an LSTM accelerator with standard functional units and without any of the architectural innovations of ELSA. The paper demonstrates that ELSA can achieve significant improvements in power, area and energy-efficiency when compared to the baseline design and several ASIC implementations reported in the literature, making it suitable for use in embedded systems and real-time applications. △ Less","18 October, 2019",https://arxiv.org/pdf/1910.08683
Understanding Deep Networks via Extremal Perturbations and Smooth Masks,Ruth Fong;Mandela Patrick;Andrea Vedaldi,"The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch. △ Less","18 October, 2019",https://arxiv.org/pdf/1910.08485
Indoor Information Retrieval using Lifelog Data,Deepanwita Datta,"Studying human behaviour through lifelogging has seen an increase in attention from researchers over the past decade. The opportunities that lifelogging offers are based on the fact that a lifelog, as a ""black box"" of our lives, offers rich contextual information, which has been an Achilles heel of information discovery. While lifelog data has been put to use in various contexts, its application to indoor environment scenario remains unexplored. In this proposal, I plan to design a method that enables us to capture and record indoor lifelog data of a person's life in order to facilitate healthcare systems, emergency response, item tracking etc. To this end, we aim to build an Indoor Information Retrieval system that can be queried with natural language queries over lifelog data. Judicious use of the lifelog data for the indoor application may enable us to solve very fundamental but non-avoidable problems of our daily life. Analysis of lifelog data coupled with Information Retrieval is not only a promising research topic, but the possibility of its indoor application especially for healthcare, lost-item tracking would be an innovative research idea to the best of our knowledge. △ Less","17 October, 2019",https://arxiv.org/pdf/1910.07784
Throughput and Delay Analysis of Slotted Aloha with Batch Service,Huanhuan Huang;Tong Ye;Tony T. Lee,"In this paper, we study the throughput and delay performances of the slotted Aloha with batch service, which has wide applications in random access networks. Different from the classical slotted Aloha, each node in the slotted Aloha with batch service can transmit up to M packets once it succeeds in channel competition. The throughput is substantially improved because up to M packets jointly undertake the overhead due to contention. In an innovative vacation model developed in this paper, we consider each batch of data transmission as a busy period of each node, and the process between two successive busy periods as a vacation period. We then formulate the number of arrivals during a vacation period in a renewal-type equation, which characterizes the dependency between busy periods and vacation periods. Based on this formulation, we derive the mean waiting time of a packet and the bounded delay region for the slotted Aloha with batch service. Our results indicate the throughput and delay performances are substantially improved with the increase of batch sizeM, and the bounded delay region is enlarged accordingly. As M goes to infinity, we find the saturated throughput can approach 100% of channel capacity, and the system remains stable irrespective of the population size and transmission probability. △ Less","16 October, 2019",https://arxiv.org/pdf/1910.07312
A Deep Learning Based Chatbot for Campus Psychological Therapy,Junjie Yin;Zixun Chen;Kelai Zhou;Chongyuan Yu,"In this paper, we propose Evebot, an innovative, sequence to sequence (Seq2seq) based, fully generative conversational system for the diagnosis of negative emotions and prevention of depression through positively suggestive responses. The system consists of an assembly of deep-learning based models, including Bi-LSTM based model for detecting negative emotions of users and obtaining psychological counselling related corpus for training the chatbot, anti-language sequence to sequence neural network, and maximum mutual information (MMI) model. As adolescents are reluctant to show their negative emotions in physical interaction, traditional methods of emotion analysis and comforting methods may not work. Therefore, this system puts emphasis on using virtual platform to detect signs of depression or anxiety, channel adolescents' stress and mood, and thus prevent the emergence of mental illness. We launched the integrated chatbot system onto an online platform for real-world campus applications. Through a one-month user study, we observe better results in the increase in positivity than other public chatbots in the control group. △ Less","9 October, 2019",https://arxiv.org/pdf/1910.06707
Blockchain 3.0 Smart Contracts in E-Government 3.0 Applications,Sofia Terzi;Konstantinos Votis;Dimitrios Tzovaras;Ioannis Stamelos;Kelly Cooper,"The adoption of Information Communication Technologies (ICT) and Web 3.0 contributes to the e-government sector by transforming how public administrations provide advanced and innovative services to interact with citizens. Blockchain (BC) and Artificial Intelligence (AI) disruptive technologies will reshape how we live, work, and interact with government sectors and industries. This paper presents how Blockchain 3.0 and Artificial Intelligence enhance robust, secure, scalable, and authenticity provenance solutions. Two validation scenarios are analyzed to present how blockchain smart contracts and AI agents support energy and health-oriented e-government services. △ Less","11 October, 2019",https://arxiv.org/pdf/1910.06092
Open Source and Sustainability: the Role of Universities,Giorgio F. Signorini,"One important goal in sustainability is making technologies available to the maximum possible number of individuals, and especially to those living in less developed areas (Goal 9 of SDG). However, the diffusion of technical knowledge is hindered by a number of factors, among which the Intellectual Property Rights (IPR) system plays a primary role. While opinions about the real effect of IPRs in stimulating and disseminating innovation differ, there is a growing number of authors arguing that a different approach may be more effective in promoting global development. The success of the Open Source (OS) model in the field of software has led analysts to speculate whether this paradigm can be extended to other fields. Key to this model are both free access to knowledge and the right to use other people's results. Abstract After reviewing the main features of the OS model, we explore different areas where it can be profitably applied, such as hardware design and production; we finally discuss how academical institutions can (and should) help diffusing the OS philosophy and practice. Widespread use of OS software, fostering of research projects aimed to use and develop OS software and hardware, the use of open education tools, and a strong commitment to open access publishing are some of the discussed examples. △ Less","8 October, 2019",https://arxiv.org/pdf/1910.06073
The Open Porous Media Flow Reservoir Simulator,Atgeirr Flø Rasmussen;Tor Harald Sandve;Kai Bao;Andreas Lauser;Joakim Hove;Bård Skaflestad;Robert Klöfkorn;Markus Blatt;Alf Birger Rustad;Ove Sævareid;Knut-Andreas Lie;Andreas Thune,"The Open Porous Media (OPM) initiative is a community effort that encourages open innovation and reproducible research for simulation of porous media processes. OPM coordinates collaborative software development, maintains and distributes open-source software and open data sets, and seeks to ensure that these are available under a free license in a long-term perspective. In this paper, we present OPM Flow, which is a reservoir simulator developed for industrial use, as well as some of the individual components used to make OPM Flow. The descriptions apply to the 2019.10 release of OPM. △ Less","4 October, 2019",https://arxiv.org/pdf/1910.06059
Communications and Networking Technologies for Intelligent Drone Cruisers,Li-Chun Wang;Chuan-Chi Lai;Hong-Han Shuai;Hsin-Piao Lin;Chi-Yu Li;Teng-Hu Cheng;Chiun-Hsun Chen,"Future mobile communication networks require an Aerial Base Station (ABS) with fast mobility and long-term hovering capabilities. At present, unmanned aerial vehicles (UAV) or drones do not have long flight times and are mainly used for monitoring, surveillance, and image post-processing. On the other hand, the traditional airship is too large and not easy to take off and land. Therefore, we propose to develop an ""Artificial Intelligence (AI) Drone-Cruiser"" base station that can help 5G mobile communication systems and beyond quickly recover the network after a disaster and handle the instant communications by the flash crowd. The drone-cruiser base station can overcome the communications problem for three types of flash crowds, such as in stadiums, parades, and large plaza so that an appropriate number of aerial base stations can be accurately deployed to meet large and dynamic traffic demands. Artificial intelligence can solve these problems by analyzing the collected data, and then adjust the system parameters in the framework of Self-Organizing Network (SON) to achieve the goals of self-configuration, self-optimization, and self-healing. With the help of AI technologies, 5G networks can become more intelligent. This paper aims to provide a new type of service, On-Demand Aerial Base Station as a Service. This work needs to overcome the following five technical challenges: innovative design of drone-cruisers for the long-time hovering, crowd estimation and prediction, rapid 3D wireless channel learning and modeling, 3D placement of aerial base stations and the integration of WiFi front-haul and millimeter wave/WiGig back-haul networks. △ Less","25 September, 2019",https://arxiv.org/pdf/1910.05309
Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base,Tao Shen;Xiubo Geng;Tao Qin;Daya Guo;Duyu Tang;Nan Duan;Guodong Long;Daxin Jiang,"We consider the problem of conversational question answering over a large-scale knowledge base. To handle huge entity vocabulary of a large-scale knowledge base, recent neural semantic parsing based approaches usually decompose the task into several subtasks and then solve them sequentially, which leads to following issues: 1) errors in earlier subtasks will be propagated and negatively affect downstream ones; and 2) each subtask cannot naturally share supervision signals with others. To tackle these issues, we propose an innovative multi-task learning framework where a pointer-equipped semantic parsing model is designed to resolve coreference in conversations, and naturally empower joint learning with a novel type-aware entity detection model. The proposed framework thus enables shared supervisions and alleviates the effect of error propagation. Experiments on a large-scale conversational question answering dataset containing 1.6M question answering pairs over 12.8M entities show that the proposed framework improves overall F1 score from 67% to 79% compared with previous state-of-the-art work. △ Less","11 October, 2019",https://arxiv.org/pdf/1910.05069
"Green Deep Reinforcement Learning for Radio Resource Management: Architecture, Algorithm Compression and Challenge",Zhiyong Du;Yansha Deng;Weisi Guo;Arumugam Nallanathan;Qihui Wu,"AI heralds a step-change in the performance and capability of wireless networks and other critical infrastructures. However, it may also cause irreversible environmental damage due to their high energy consumption. Here, we address this challenge in the context of 5G and beyond, where there is a complexity explosion in radio resource management (RRM). On the one hand, deep reinforcement learning (DRL) provides a powerful tool for scalable optimization for high dimensional RRM problems in a dynamic environment. On the other hand, DRL algorithms consume a high amount of energy over time and risk compromising progress made in green radio research. This paper reviews and analyzes how to achieve green DRL for RRM via both architecture and algorithm innovations. Architecturally, a cloud based training and distributed decision-making DRL scheme is proposed, where RRM entities can make lightweight deep local decisions whilst assisted by on-cloud training and updating. On the algorithm level, compression approaches are introduced for both deep neural networks and the underlying Markov Decision Processes, enabling accurate low-dimensional representations of challenges. To scale learning across geographic areas, a spatial transfer learning scheme is proposed to further promote the learning efficiency of distributed DRL entities by exploiting the traffic demand correlations. Together, our proposed architecture and algorithms provide a vision for green and on-demand DRL capability. △ Less","11 October, 2019",https://arxiv.org/pdf/1910.05054
"Design, Modelling and Validation of a Novel Extra Slender Continuum Robot for In-situ Inspection and Repair in Aeroengine",Mingfeng Wang;Xin Dong;Weiming Ba;Abdelkhalick Mohammad;Dragos Axinte;Andy Norton,"In-situ aeroengine maintenance works are highly beneficial as it can significantly reduce the current maintenance cycle which is extensive and costly due to the disassembly requirement of engines from aircrafts. However, navigating in/out via inspection ports and performing multi-axis movements with end-effectors in constrained environments (e.g. combustion chamber) are fairly challenging. A novel extra-slender (diameter-to-length ratio <0.02) dual-stage continuum robot (16 degree-of-freedom) is proposed to navigate in/out confined environments and perform required configuration shapes for further repair operations. Firstly, the robot design presents several innovative mechatronic solutions: (i) dual-stage tendon-driven structure with bevelled disks to perform required shapes and to provide selective stiffness for carrying high payloads; (ii) various rigid-compliant combined joints to enable different flexibility and stiffness in each stage; (iii) three commanding cables for each 2-DoF section to minimise the number of actuators with precise actuations. Secondly, a segment-scaled piecewise-constant-curvature-theory based kinematic model and a Kirchhoff-elastic-rod-theory based static model are established by considering the applied forces/moments (friction, actuation, gravity and external load), where the friction coefficient is modelled as a function of bending angle. Finally, experiments were carried out to validate the proposed static modelling and to evaluate the robot capabilities of performing the predefined shape and stiffness. △ Less","9 October, 2019",https://arxiv.org/pdf/1910.04572
Engineering for a Science-Centric Experimentation Platform,Nikos Diamantopoulos;Jeffrey Wong;David Issa Mattos;Ilias Gerostathopoulos;Matthew Wardrop;Tobias Mao;Colin McFarland,"Netflix is an internet entertainment service that routinely employs experimentation to guide strategy around product innovations. As Netflix grew, it had the opportunity to explore increasingly specialized improvements to its service, which generated demand for deeper analyses supported by richer metrics and powered by more diverse statistical methodologies. To facilitate this, and more fully harness the skill sets of both engineering and data science, Netflix engineers created a science-centric experimentation platform that leverages the expertise of data scientists from a wide range of backgrounds by allowing them to make direct code contributions in the languages used by scientists (Python and R). Moreover, the same code that runs in production is able to be run locally, making it straightforward to explore and graduate both metrics and causal inference methodologies directly into production services. In this paper, we utilize a case-study research method to provide two main contributions. Firstly, we report on the architecture of this platform, with a special emphasis on its novel aspects: how it supports science-centric end-to-end workflows without compromising engineering requirements. Secondly, we describe its approach to causal inference, which leverages the potential outcomes conceptual framework to provide a unified abstraction layer for arbitrary statistical models and methodologies. △ Less","9 October, 2019",https://arxiv.org/pdf/1910.03878
Gunrock: A Social Bot for Complex and Engaging Long Conversations,Dian Yu;Michelle Cohn;Yi Mang Yang;Chun-Yen Chen;Weiming Wen;Jiaping Zhang;Mingyang Zhou;Kevin Jesse;Austin Chau;Antara Bhowmick;Shreenath Iyer;Giritheja Sreenivasulu;Sam Davidson;Ashwin Bhandare;Zhou Yu,"Gunrock is the winner of the 2018 Amazon Alexa Prize, as evaluated by coherence and engagement from both real users and Amazon-selected expert conversationalists. We focus on understanding complex sentences and having in-depth conversations in open domains. In this paper, we introduce some innovative system designs and related validation analysis. Overall, we found that users produce longer sentences to Gunrock, which are directly related to users' engagement (e.g., ratings, number of turns). Additionally, users' backstory queries about Gunrock are positively correlated to user satisfaction. Finally, we found dialog flows that interleave facts and personal opinions and stories lead to better user satisfaction. △ Less","7 October, 2019",https://arxiv.org/pdf/1910.03042
Placement and Routing Optimization Problem for Service Function Chain: State of Art and Future Opportunities,Weihan Chen;Xia Yin;Zhiliang Wang;Xingang Shi,"Network Functions Virtualization (NFV) allows implantation of network functions to be independent of dedicated hardware devices. Any series of services can be represented by a service function chain which contains a set of virtualized network functions in a specified order. From the perspective of network performance optimization, the challenges of deploying service chain in network is twofold: 1) the location of placing virtualized network functions and resources allocation scheme; and 2) routing policy for traffic flow among different instances of network function. This article introduces service function chain related optimization problems, summarizes the optimization motivation and mainstream algorithm of virtualized network functions deployment and traffic routing. We hope it can help readers to learn about the current research progress and make further innovation in this field. △ Less","7 October, 2019",https://arxiv.org/pdf/1910.02613
Incremental learning for the detection and classification of GAN-generated images,Francesco Marra;Cristiano Saltori;Giulia Boato;Luisa Verdoliva,"Current developments in computer vision and deep learning allow to automatically generate hyper-realistic images, hardly distinguishable from real ones. In particular, human face generation achieved a stunning level of realism, opening new opportunities for the creative industry but, at the same time, new scary scenarios where such content can be maliciously misused. Therefore, it is essential to develop innovative methodologies to automatically tell apart real from computer generated multimedia, possibly able to follow the evolution and continuous improvement of data in terms of quality and realism. In the last few years, several deep learning-based solutions have been proposed for this problem, mostly based on Convolutional Neural Networks (CNNs). Although results are good in controlled conditions, it is not clear how such proposals can adapt to real-world scenarios, where learning needs to continuously evolve as new types of generated data appear. In this work, we tackle this problem by proposing an approach based on incremental learning for the detection and classification of GAN-generated images. Experiments on a dataset comprising images generated by several GAN-based architectures show that the proposed method is able to correctly perform discrimination when new GANs are presented to the network △ Less","6 October, 2019",https://arxiv.org/pdf/1910.01568
Can Sentiment Analysis Reveal Structure in a Plotless Novel?,Katherine Elkins;Jon Chun,"Modernist novels are thought to break with traditional plot structure. In this paper, we test this theory by applying Sentiment Analysis to one of the most famous modernist novels, To the Lighthouse by Virginia Woolf. We first assess Sentiment Analysis in light of the critique that it cannot adequately account for literary language: we use a unique statistical comparison to demonstrate that even simple lexical approaches to Sentiment Analysis are surprisingly effective. We then use the Syuzhet.R package to explore similarities and differences across modeling methods. This comparative approach, when paired with literary close reading, can offer interpretive clues. To our knowledge, we are the first to undertake a hybrid model that fully leverages the strengths of both computational analysis and close reading. This hybrid model raises new questions for the literary critic, such as how to interpret relative versus absolute emotional valence and how to take into account subjective identification. Our finding is that while To the Lighthouse does not replicate a plot centered around a traditional hero, it does reveal an underlying emotional structure distributed between characters - what we term a distributed heroine model. This finding is innovative in the field of modernist and narrative studies and demonstrates that a hybrid method can yield significant discoveries. △ Less","31 August, 2019",https://arxiv.org/pdf/1910.01441
Adaptive Generation of Phantom Limbs Using Visible Hierarchical Autoencoders,Dakila Ledesma;Yu Liang;Dalei Wu,"This paper proposed a hierarchical visible autoencoder in the adaptive phantom limbs generation according to the kinetic behavior of functional body-parts, which are measured by heterogeneous kinetic sensors. The proposed visible hierarchical autoencoder consists of interpretable and multi-correlated autoencoder pipelines, which is directly derived from the hierarchical network described in forest data-structure. According to specified kinetic script (e.g., dancing, running, etc.) and users' physical conditions, hierarchical network is extracted from human musculoskeletal network, which is fabricated by multiple body components (e.g., muscle, bone, and joints, etc.) that are bio-mechanically, functionally, or nervously correlated with each other and exhibit mostly non-divergent kinetic behaviors. Multi-layer perceptron (MLP) regressor models, as well as several variations of autoencoder models, are investigated for the sequential generation of missing or dysfunctional limbs. The resulting kinematic behavior of phantom limbs will be constructed using virtual reality and augmented reality (VR/AR), actuators, and potentially controller for a prosthesis (an artificial device that replaces a missing body part). The addressed work aims to develop practical innovative exercise methods that (1) engage individuals at all ages, including those with a chronic health condition(s) and/or disability, in regular physical activities, (2) accelerate the rehabilitation of patients, and (3) release users' phantom limb pain. The physiological and psychological impact of the addressed work will critically be assessed in future work. △ Less","2 October, 2019",https://arxiv.org/pdf/1910.01191
A Survey of Big Data Machine Learning Applications Optimization in Cloud Data Centers and Networks,Sanaa Hamid Mohamed;Taisir E. H. El-Gorashi;Jaafar M. H. Elmirghani,"This survey article reviews the challenges associated with deploying and optimizing big data applications and machine learning algorithms in cloud data centers and networks. The MapReduce programming model and its widely-used open-source platform; Hadoop, are enabling the development of a large number of cloud-based services and big data applications. MapReduce and Hadoop thus introduce innovative, efficient, and accelerated intensive computations and analytics. These services usually utilize commodity clusters within geographically-distributed data centers and provide cost-effective and elastic solutions. However, the increasing traffic between and within the data centers that migrate, store, and process big data, is becoming a bottleneck that calls for enhanced infrastructures capable of reducing the congestion and power consumption. Moreover, enterprises with multiple tenants requesting various big data services are challenged by the need to optimize leasing their resources at reduced running costs and power consumption while avoiding under or over utilization. In this survey, we present a summary of the characteristics of various big data programming models and applications and provide a review of cloud computing infrastructures, and related technologies such as virtualization, and software-defined networking that increasingly support big data systems. Moreover, we provide a brief review of data centers topologies, routing protocols, and traffic characteristics, and emphasize the implications of big data on such cloud data centers and their supporting networks. Wide ranging efforts were devoted to optimize systems that handle big data in terms of various applications performance metrics and/or infrastructure energy efficiency. Finally, some insights and future research directions are provided. △ Less","1 October, 2019",https://arxiv.org/pdf/1910.00731
VPN0: A Privacy-Preserving Decentralized Virtual Private Network,Matteo Varvello;Iñigo Querejeta Azurmendi;Antonio Nappa;Panagiotis Papadopoulos;Goncalo Pestana;Ben Livshits,"Distributed Virtual Private Networks (dVPNs) are new VPN solutions aiming to solve the trust-privacy concern of a VPN's central authority by leveraging a distributed architecture. In this paper, we first review the existing dVPN ecosystem and debate on its privacy requirements. Then, we present VPN0, a dVPN with strong privacy guarantees and minimal performance impact on its users. VPN0 guarantees that a dVPN node only carries traffic it has ""whitelisted"", without revealing its whitelist or knowing the traffic it tunnels. This is achieved via three main innovations. First, an attestation mechanism which leverages TLS to certify a user visit to a specific domain. Second, a zero knowledge proof to certify that some incoming traffic is authorized, e.g., falls in a node's whitelist, without disclosing the target domain. Third, a dynamic chain of VPN tunnels to both increase privacy and guarantee service continuation while traffic certification is in place. The paper demonstrates VPN0 functioning when integrated with several production systems, namely BitTorrent DHT and ProtonVPN. △ Less","30 September, 2019",https://arxiv.org/pdf/1910.00159
MIOpen: An Open Source Library For Deep Learning Primitives,Jehandad Khan;Paul Fultz;Artem Tamazov;Daniel Lowell;Chao Liu;Michael Melesse;Murali Nandhimandalam;Kamil Nasyrov;Ilya Perminov;Tejash Shah;Vasilii Filippov;Jing Zhang;Jing Zhou;Bragadeesh Natarajan;Mayank Daga,"Deep Learning has established itself to be a common occurrence in the business lexicon. The unprecedented success of deep learning in recent years can be attributed to: abundance of data, availability of gargantuan compute capabilities offered by GPUs, and adoption of open-source philosophy by the researchers and industry. Deep neural networks can be decomposed into a series of different operators. MIOpen, AMD's open-source deep learning primitives library for GPUs, provides highly optimized implementations of such operators, shielding researchers from internal implementation details and hence, accelerating the time to discovery. This paper introduces MIOpen and provides details about the internal workings of the library and supported features. MIOpen innovates on several fronts, such as implementing fusion to optimize for memory bandwidth and GPU launch overheads, providing an auto-tuning infrastructure to overcome the large design space of problem configurations, and implementing different algorithms to optimize convolutions for different filter and input sizes. MIOpen is one of the first libraries to publicly support the bfloat16 data-type for convolutions, allowing efficient training at lower precision without the loss of accuracy. △ Less","30 September, 2019",https://arxiv.org/pdf/1910.00078
A Price-Based Iterative Double Auction for Charger Sharing Markets,Jie Gao;Terrence Wong;Chun Wang;Jia Yuan Yu,"The unprecedented growth of demand for charging electric vehicles (EVs) calls for novel expansion solutions to today's charging networks. Riding on the wave of the proliferation of sharing economy, Airbnb-like charger sharing markets opens the opportunity to expand the existing charging networks without requiring costly and time-consuming infrastructure investments, yet the successful design of such markets relies on innovations at the interface between game theory, mechanism design, and large scale optimization. In this paper, we propose a price-based iterative double auction for charger sharing markets where charger owners rent out their under-utilized chargers to the charge-needing EV drivers. Charger owners and EV drivers form a two-sided market which is cleared by a price-based double auction. Chargers' locations, availability, and time unit costs as well as the EV drivers' time, distance constraints, and preferences are considered in the allocation and scheduling process. The goal is to compute social welfare maximizing allocations which benefits both charger owners and EV drivers and, in turn, ensure the continuous growth of the market. We prove that the proposed double auction is budget balanced, individually rational, and that it is a weakly dominant strategy for EV drivers and charger owners to truthfully report their charging time constraints. In addition, results from our computation study show that the double auction achieves on average 94% efficiency compared with the optimal solutions and scales well to larger problem instances. △ Less","30 September, 2019",https://arxiv.org/pdf/1910.00053
GACNN: Training Deep Convolutional Neural Networks with Genetic Algorithm,Parsa Esfahanian;Mohammad Akhavan,"Convolutional Neural Networks (CNNs) have gained a significant attraction in the recent years due to their increasing real-world applications. Their performance is highly dependent to the network structure and the selected optimization method for tuning the network parameters. In this paper, we propose novel yet efficient methods for training convolutional neural networks. The most of current state of the art learning method for CNNs are based on Gradient decent. In contrary to the traditional CNN training methods, we propose to optimize the CNNs using methods based on Genetic Algorithms (GAs). These methods are carried out using three individual GA schemes, Steady-State, Generational, and Elitism. We present new genetic operators for crossover, mutation and also an innovative encoding paradigm of CNNs to chromosomes aiming to reduce the resulting chromosome's size by a large factor. We compare the effectiveness and scalability of our encoding with the traditional encoding. Furthermore, the performance of individual GA schemes used for training the networks were compared with each other in means of convergence rate and overall accuracy. Finally, our new encoding alongside the superior GA-based training scheme is compared to Backpropagation training with Adam optimization. △ Less","29 September, 2019",https://arxiv.org/pdf/1909.13354
When to Intervene: Detecting Abnormal Mood using Everyday Smartphone Conversations,John Gideon;Katie Matton;Steve Anderau;Melvin G McInnis;Emily Mower Provost,"Bipolar disorder (BPD) is a chronic mental illness characterized by extreme mood and energy changes from mania to depression. These changes drive behaviors that often lead to devastating personal or social consequences. BPD is managed clinically with regular interactions with care providers, who assess mood, energy levels, and the form and content of speech. Recent work has proposed smartphones for monitoring mood using speech. However, these works do not predict when to intervene. Predicting when to intervene is challenging because there is not a single measure that is relevant for every person: different individuals may have different levels of symptom severity considered typical. Additionally, this typical mood, or baseline, may change over time, making a single symptom threshold insufficient. This work presents an innovative approach that expands clinical mood monitoring to predict when interventions are necessary using an anomaly detection framework, which we call Temporal Normalization. We first validate the model using a dataset annotated for clinical interventions and then incorporate this method in a deep learning framework to predict mood anomalies from natural, unstructured, telephone speech data. The combination of these approaches provides a framework to enable real-world speech-focused mood monitoring. △ Less","2 October, 2019",https://arxiv.org/pdf/1909.11248
Code-switching Language Modeling With Bilingual Word Embeddings: A Case Study for Egyptian Arabic-English,Injy Hamed;Moritz Zhu;Mohamed Elmahdy;Slim Abdennadher;Ngoc Thang Vu,"Code-switching (CS) is a widespread phenomenon among bilingual and multilingual societies. The lack of CS resources hinders the performance of many NLP tasks. In this work, we explore the potential use of bilingual word embeddings for code-switching (CS) language modeling (LM) in the low resource Egyptian Arabic-English language. We evaluate different state-of-the-art bilingual word embeddings approaches that require cross-lingual resources at different levels and propose an innovative but simple approach that jointly learns bilingual word representations without the use of any parallel data, relying only on monolingual and a small amount of CS data. While all representations improve CS LM, ours performs the best and improves perplexity 33.5% relative over the baseline. △ Less","24 September, 2019",https://arxiv.org/pdf/1909.10892
"Research Directions in Democratizing Innovation through Design Automation, One-Click Manufacturing Services and Intelligent Machines",Binil Starly;Atin Angrish;Paul Cohen,"The digitalization of manufacturing has created opportunities for consumers to customize products that fit their individualized needs which in turn would drive demand for manufacturing services. However, this pull-based manufacturing system production of extremely low quantity and limitless variety for products is expensive to implement. New emerging technology in design automation driven by data-driven computational design, manufacturing-as-a-service marketplaces and digitally enabled micro-factories holds promise towards democratization of innovation. In this paper, scientific, technology and infrastructure challenges are identified and if solved, the impact of these emerging technologies on product innovation and future factory organization is discussed. △ Less","23 September, 2019",https://arxiv.org/pdf/1909.10476
Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings,Gregor Wiedemann;Steffen Remus;Avi Chawla;Chris Biemann,"Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al., 2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a major recent innovation in NLP. CWEs provide semantic vector representations of words depending on their respective context. Their advantage over static word embeddings has been shown for a number of tasks, such as text classification, sequence tagging, or machine translation. Since vectors of the same word type can vary depending on the respective context, they implicitly provide a model for word sense disambiguation (WSD). We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs. We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets. We further show that the pre-trained BERT model is able to place polysemic words into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability. △ Less","1 October, 2019",https://arxiv.org/pdf/1909.10430
A True AR Authoring Tool for Interactive Virtual Museums,Efstratios Geronikolakis;Paul Zikas;Steve Kateros;Nick Lydatakis;Stelios Georgiou;Mike Kentros;George Papagiannakis,"In this work, a new and innovative way of spatial computing that appeared recently in the bibliography called True Augmented Reality (AR), is employed in cultural heritage preservation. This innovation could be adapted by the Virtual Museums of the future to enhance the quality of experience. It emphasises, the fact that a visitor will not be able to tell, at a first glance, if the artefact that he/she is looking at is real or not and it is expected to draw the visitors' interest. True AR is not limited to artefacts but extends even to buildings or life-sized character simulations of statues. It provides the best visual quality possible so that the users will not be able to tell the real objects from the augmented ones. Such applications can be beneficial for future museums, as with True AR, 3D models of various exhibits, monuments, statues, characters and buildings can be reconstructed and presented to the visitors in a realistic and innovative way. We also propose our Virtual Reality Sample application, a True AR playground featuring basic components and tools for generating interactive Virtual Museum applications, alongside a 3D reconstructed character (the priest of Asinou church) facilitating the storyteller of the augmented experience. △ Less","21 October, 2019",https://arxiv.org/pdf/1909.09429
EATEN: Entity-aware Attention for Single Shot Visual Text Extraction,He guo;Xiameng Qin;Jiaming Liu;Junyu Han;Jingtuo Liu;Errui Ding,"Extracting entity from images is a crucial part of many OCR applications, such as entity recognition of cards, invoices, and receipts. Most of the existing works employ classical detection and recognition paradigm. This paper proposes an Entity-aware Attention Text Extraction Network called EATEN, which is an end-to-end trainable system to extract the entities without any post-processing. In the proposed framework, each entity is parsed by its corresponding entity-aware decoder, respectively. Moreover, we innovatively introduce a state transition mechanism which further improves the robustness of entity extraction. In consideration of the absence of public benchmarks, we construct a dataset of almost 0.6 million images in three real-world scenarios (train ticket, passport and business card), which is publicly available at https://github.com/beacandler/EATEN. To the best of our knowledge, EATEN is the first single shot method to extract entities from images. Extensive experiments on these benchmarks demonstrate the state-of-the-art performance of EATEN. △ Less","20 September, 2019",https://arxiv.org/pdf/1909.09380
A Two-Stage Stochastic Programming Model for Car-Sharing Problem using Kernel Density Estimation,Xiaoming Li;Chun Wang;Xiao Huang,"Car-sharing problem is a popular research field in sharing economy. In this paper, we investigate the car-sharing re-balancing problem under uncertain demands. An innovative framework that integrates a non-parametric approach - kernel density estimation (KDE) and a two-stage stochastic programming (SP) model are proposed. Specifically, the probability distributions are derived from New York taxi trip data sets by KDE, which is used as the input uncertain parameters for SP. Additionally, the car-sharing problem is formulated as a two-stage SP model which aims to maximize the overall profit. Meanwhile, a Monte Carlo method called sample average approximation (SAA) and Benders decomposition algorithm is introduced to solve the large-scale optimization model. Finally, the experimental validations show that the proposed framework outperforms the existing works in terms of outcomes. △ Less","19 September, 2019",https://arxiv.org/pdf/1909.09293
Co-citations in context: disciplinary heterogeneity is relevant,James Bradley;Sitaram Devarakonda;Avon Davey;Dmitriy Korobskiy;Siyu Liu;Djamil Lakhdar-Hamina;Tandy Warnow;George Chacko,"Citation analysis of the scientific literature has been used to study and define disciplinary boundaries, to trace the dissemination of knowledge, and to estimate impact. Co-citation, the frequency with which pairs of publications are cited, provides insight into how documents relate to each other and across fields. Co-citation analysis has been used to characterize combinations of prior work as conventional or innovative and to derive features of highly cited publications. Given the organization of science into disciplines, a key question is the sensitivity of such analyses to frame of reference. Our study examines this question using semantically-themed citation networks. We observe that trends reported to be true across the scientific literature do not hold for focused citation networks, and we conclude that inferring novelty using co-citation analysis and random graph models benefits from disciplinary context. △ Less","18 September, 2019",https://arxiv.org/pdf/1909.08738
Introduction to the Tezos Blockchain,Victor Allombert;Mathias Bourgoin;Julien Tesson,Tezos is an innovative blockchain that improves on several aspects compared to more established blockchains. It offers an original proof-of-stake consensus algorithm and can be used as a decentralized smart contract platform. It has the capacity to amend its own economic protocol through a voting mechanism and focuses on formal methods to improve safety.,"18 September, 2019",https://arxiv.org/pdf/1909.08458
An efficient numerical method for a long-term simulation of heat and mass transfer: the case of an insulated rammed earth wall,Madina Abdykarim;Julien Berger;Denys Dutykh;Amen Agbossou,"Innovative numerical scheme studied in this work enables to overcome two main limitations of Building Performance Simulation (BPS) programs as high computational cost and the choice of a very fine numerical grid. The method, called Super-Time-Stepping (STS), is novel to the state-of-the-art of building simulations, but has already proved to be sufficiently efficient in recent studies from anisotropic heat conduction in astrophysics (Meyer et al. 2014). The given research is focused on employment of this adopted numerical method to model drying of a rammed earth wall with an additional insulation layer. The results show considerable advantage of the STS method compared to standard Euler explicit scheme. It is possible to choose at least 100 times bigger time-steps to maintain high accuracy and to cut computational cost by more than 92% in the same time. △ Less","17 September, 2019",https://arxiv.org/pdf/1909.08416
"Design research, eHealth, and the convergence revolution",Valeria Pannunzio;Maaike Kleinsmann;Dirk Snelders,"The Quadruple Aim is a framework which prioritizes four aims, or dimensions of performance, for innovating in the healthcare domain, respectively: 1) enhancing the individual experience of care; 2) improving the work life of health care clinicians and staff; 3) improving the health of populations; and 4) reducing the per capita cost of care. In this contribution, recent literature providing examples of design research in the eHealth domain is reviewed to answer the research question: in which measure has design research contributed to each of the four aims of eHealth innovation in the past five years?. The results of the review are presented and employed to draw three main observations: 1) design researchers in eHealth seem to be largely focused on improving experiences of care, either patients' or health professionals; 2) design researchers' contribution on reducing per capita costs of care appears to be less pronounced, which is outlined as a point for improvement; and 3) in a considerable amount of reviewed contributions, design researchers appear to be contributing to multiple aims at once. In this sub-group of reviewed contributions, several disciplinary areas and types of stakeholders interact and integrate through design research activities. The latter observation leads to a reflection on the strategic role of design research in the contexts of the convergence revolution and of the non-communicable disease crisis. Implications of this reflection for design researchers are recognized in the opportunity and timeliness to develop eHealth-specific ways to orchestrate design integration. A direction for further research in this sense is identified in the use of sensory and self-monitored data as a boundary object for eHealth innovation. The prospective value of this direction is finally exemplified through the case of blood pressure. △ Less","13 September, 2019",https://arxiv.org/pdf/1909.08398
Advances in Big Data Bio Analytics,Nicos Angelopoulos;Jan Wielemaker,"Delivering effective data analytics is of crucial importance to the interpretation of the multitude of biological datasets currently generated by an ever increasing number of high throughput techniques. Logic programming has much to offer in this area. Here, we detail advances that highlight two of the strengths of logical formalisms in developing data analytic solutions in biological settings: access to large relational databases and building analytical pipelines collecting graph information from multiple sources. We present significant advances on the bio_db package which serves biological databases as Prolog facts that can be served either by in-memory loading or via database backends. These advances include modularising the underlying architecture and the incorporation of datasets from a second organism (mouse). In addition, we introduce a number of data analytics tools that operate on these datasets and are bundled in the analysis package: bio_analytics. Emphasis in both packages is on ease of installation and use. We highlight the general architecture of our components based approach. An experimental graphical user interface via SWISH for local installation is also available. Finally, we advocate that biological data analytics is a fertile area which can drive further innovation in applied logic programming. △ Less","18 September, 2019",https://arxiv.org/pdf/1909.08254
Ludwig: a type-based declarative deep learning toolbox,Piero Molino;Yaroslav Dudin;Sai Sumanth Miryala,"In this work we present Ludwig, a flexible, extensible and easy to use toolbox which allows users to train deep learning models and use them for obtaining predictions without writing code. Ludwig implements a novel approach to deep learning model building based on two main abstractions: data types and declarative configuration files. The data type abstraction allows for easier code and sub-model reuse, and the standardized interfaces imposed by this abstraction allow for encapsulation and make the code easy to extend. Declarative model definition configuration files enable inexperienced users to obtain effective models and increase the productivity of expert users. Alongside these two innovations, Ludwig introduces a general modularized deep learning architecture called Encoder-Combiner-Decoder that can be instantiated to perform a vast amount of machine learning tasks. These innovations make it possible for engineers, scientists from other fields and, in general, a much broader audience to adopt deep learning models for their tasks, concretely helping in its democratization. △ Less","17 September, 2019",https://arxiv.org/pdf/1909.07930
Improving the Learning of Multi-column Convolutional Neural Network for Crowd Counting,Zhi-Qi Cheng;Jun-Xiu Li;Qi Dai;Xiao Wu;Jun-Yan He;Alexander Hauptmann,"Tremendous variation in the scale of people/head size is a critical problem for crowd counting. To improve the scale invariance of feature representation, recent works extensively employ Convolutional Neural Networks with multi-column structures to handle different scales and resolutions. However, due to the substantial redundant parameters in columns, existing multi-column networks invariably exhibit almost the same scale features in different columns, which severely affects counting accuracy and leads to overfitting. In this paper, we attack this problem by proposing a novel Multi-column Mutual Learning (McML) strategy. It has two main innovations: 1) A statistical network is incorporated into the multi-column framework to estimate the mutual information between columns, which can approximately indicate the scale correlation between features from different columns. By minimizing the mutual information, each column is guided to learn features with different image scales. 2) We devise a mutual learning scheme that can alternately optimize each column while keeping the other columns fixed on each mini-batch training data. With such asynchronous parameter update process, each column is inclined to learn different feature representation from others, which can efficiently reduce the parameter redundancy and improve generalization ability. More remarkably, McML can be applied to all existing multi-column networks and is end-to-end trainable. Extensive experiments on four challenging benchmarks show that McML can significantly improve the original multi-column networks and outperform the other state-of-the-art approaches. △ Less","17 September, 2019",https://arxiv.org/pdf/1909.07608
On the design of an innovative solution for increasing hazardous materials transportation safety,Emil Pricop,"Transportation of hazardous materials represent a high risk operation all over the world. Flammable substances such as oil, kerosene, hydrocarbons, ammonium nitrate or toxic products are shipped every day on busy roads by trucks. An innovative solution for increasing hazardous materials transportation safety is presented in this paper. The solution integrates three systems: one mounted on the truck that can alert authorities in case of an accident, one portable system for quick identification of the carried substances and intervention method and a component for real-time road monitoring. The proposed solution is based on RFID card with a special memory structure presented in this paper △ Less","10 September, 2019",https://arxiv.org/pdf/1909.07438
Learning an Effective Equivariant 3D Descriptor Without Supervision,Riccardo Spezialetti;Samuele Salti;Luigi Di Stefano,"Establishing correspondences between 3D shapes is a fundamental task in 3D Computer Vision, typically addressed by matching local descriptors. Recently, a few attempts at applying the deep learning paradigm to the task have shown promising results. Yet, the only explored way to learn rotation invariant descriptors has been to feed neural networks with highly engineered and invariant representations provided by existing hand-crafted descriptors, a path that goes in the opposite direction of end-to-end learning from raw data so successfully deployed for 2D images. In this paper, we explore the benefits of taking a step back in the direction of end-to-end learning of 3D descriptors by disentangling the creation of a robust and distinctive rotation equivariant representation, which can be learned from unoriented input data, and the definition of a good canonical orientation, required only at test time to obtain an invariant descriptor. To this end, we leverage two recent innovations: spherical convolutional neural networks to learn an equivariant descriptor and plane folding decoders to learn without supervision. The effectiveness of the proposed approach is experimentally validated by outperforming hand-crafted and learned descriptors on a standard benchmark. △ Less","15 September, 2019",https://arxiv.org/pdf/1909.06887
BAGH -- Comparative study,B. Kamala,"Process mining is a new emerging research trend over the last decade which focuses on analyzing the processes using event log and data. The raising integration of information systems for the operation of business processes provides the basis for innovative data analysis approaches. Process mining has the strong relationship between with data mining so that it enables the bond between business intelligence approach and business process management. It focuses on end to end processes and is possible because of the growing availability of event data and new process discovery and conformance checking techniques. Process mining aims to discover, monitor and improve real processes by extracting knowledge from event logs readily available in todays information systems. The discovered process models can be used for a variety of analysis purposes. Many companies have adopted Process aware Information Systems for supporting their business processes in some form. These systems typically have their log events related to the actual business process executions. Proper analysis of Process Aware Information Systems execution logs can yield important knowledge and help organizations improve the quality of their services. This paper reviews and compares various process mining algorithms based on their input parameters, the techniques used and the output generated by them. △ Less","4 September, 2019",https://arxiv.org/pdf/1909.06159
Challenges in the Decentralised Web: The Mastodon Case,Aravindh Raman;Sagar Joglekar;Emiliano De Cristofaro;Nishanth Sastry;Gareth Tyson,"The Decentralised Web (DW) has recently seen a renewed momentum, with a number of DW platforms like Mastodon, Peer-Tube, and Hubzilla gaining increasing traction. These offer alternatives to traditional social networks like Twitter, YouTube, and Facebook, by enabling the operation of web infrastructure and services without centralised ownership or control. Although their services differ greatly, modern DW platforms mostly rely on two key innovations: first, their open source software allows anybody to setup independent servers (""instances"") that people can sign-up to and use within a local community; and second, they build on top of federation protocols so that instances can mesh together, in a peer-to-peer fashion, to offer a globally integrated platform. In this paper, we present a measurement-driven exploration of these two innovations, using a popular DW microblogging platform (Mastodon) as a case study. We focus on identifying key challenges that might disrupt continuing efforts to decentralise the web, and empirically highlight a number of properties that are creating natural pressures towards recentralisation. Finally, our measurements shed light on the behaviour of both administrators (i.e., people setting up instances) and regular users who sign-up to the platforms, also discussing a few techniques that may address some of the issues observed. △ Less","12 September, 2019",https://arxiv.org/pdf/1909.05801
Raiders of the Lost Art,Anthony Bourached;George Cann,"Neural style transfer, first proposed by Gatys et al. (2015), can be used to create novel artistic work through rendering a content image in the form of a style image. We present a novel method of reconstructing lost artwork, by applying neural style transfer to x-radiographs of artwork with secondary interior artwork beneath a primary exterior, so as to reconstruct lost artwork. Finally we reflect on AI art exhibitions and discuss the social, cultural, ethical, and philosophical impact of these technical innovations. △ Less","10 September, 2019",https://arxiv.org/pdf/1909.05677
Validating Weak-form Market Efficiency in United States Stock Markets with Trend Deterministic Price Data and Machine Learning,Samuel Showalter;Jeffrey Gropp,"The Efficient Market Hypothesis has been a staple of economics research for decades. In particular, weak-form market efficiency -- the notion that past prices cannot predict future performance -- is strongly supported by econometric evidence. In contrast, machine learning algorithms implemented to predict stock price have been touted, to varying degrees, as successful. Moreover, some data scientists boast the ability to garner above-market returns using price data alone. This study endeavors to connect existing econometric research on weak-form efficient markets with data science innovations in algorithmic trading. First, a traditional exploration of stationarity in stock index prices over the past decade is conducted with Augmented Dickey-Fuller and Variance Ratio tests. Then, an algorithmic trading platform is implemented with the use of five machine learning algorithms. Econometric findings identify potential stationarity, hinting technical evaluation may be possible, though algorithmic trading results find little predictive power in any machine learning model, even when using trend-specific metrics. Accounting for transaction costs and risk, no system achieved above-market returns consistently. Our findings reinforce the validity of weak-form market efficiency. △ Less","11 September, 2019",https://arxiv.org/pdf/1909.05151
User-Controlled Privacy-Preserving User Profile Data Sharing based on Blockchain,Ajay Kumar Shrestha;Ralph Deters;Julita Vassileva,"The tremendous technological advancement in the last few decades has brought many enterprises to collaborate in a better way while making intelligent decisions. The use of Information Technology tools in obtaining data of people's everyday life from various autonomous data sources allowing unrestricted access to user data has emerged as an important practical issue and has given rise to legal implications. Various innovative models for data sharing and management have privacy and centrality issues. To alleviate these limitations, we have incorporated blockchain in user modeling. In this paper, we constructed a decentralized data sharing architecture with MultiChain blockchain in the travel domain, which is also applicable to other similar domains including education, health, and sports. Businesses that operate in the tourism industries including travel and tour agencies, hotels and resorts, shopping malls are connected to the MultiChain and they share their user profile data via stream in the MultiChain. The paper presents the hotel booking service for an imaginary hotel as one of the enterprise nodes, which collects user profile data with proper validation and will allow users to decide which of their data to be shared thus ensuring user control over their data and the preservation of privacy. The data from the repository is converted into an open data format while sharing via stream in the blockchain so that other enterprise nodes, after receiving the data, can easily convert them and store into their own repositories. The paper presents an evaluation of the performance of the model by measuring the latency and memory consumption with three test scenarios that mostly affect the user experience. The node responded quickly in all of these cases. △ Less","10 September, 2019",https://arxiv.org/pdf/1909.05028
QuTiBench: Benchmarking Neural Networks on Heterogeneous Hardware,Michaela Blott;Lisa Halder;Miriam Leeser;Linda Doyle,"Neural Networks have become one of the most successful universal machine learning algorithms. They play a key role in enabling machine vision and speech recognition for example. Their computational complexity is enormous and comes along with equally challenging memory requirements, which limits deployment in particular within energy constrained, embedded environments. In order to address these implementation challenges, a broad spectrum of new customized and heterogeneous hardware architectures have emerged, often accompanied with co-designed algorithms to extract maximum benefit out of the hardware. Furthermore, numerous optimization techniques are being explored for neural networks to reduce compute and memory requirements while maintaining accuracy. This results in an abundance of algorithmic and architectural choices, some of which fit specific use cases better than others. For system level designers, there is currently no good way to compare the variety of hardware, algorithm and optimization options. While there are many benchmarking efforts in this field, they cover only subsections of the embedded design space. None of the existing benchmarks support essential algorithmic optimizations such as quantization, an important technique to stay on chip, or specialized heterogeneous hardware architectures. We propose a novel benchmark suite, QuTiBench, that addresses this need. QuTiBench is a novel multi-tiered benchmarking methodology that supports algorithmic optimizations such as quantization and helps system developers understand the benefits and limitations of these novel compute architectures in regard to specific neural networks and will help drive future innovation. We invite the community to contribute to QuTiBench in order to support the full spectrum of choices in implementing machine learning systems. △ Less","17 November, 2019",https://arxiv.org/pdf/1909.05009
"A Survey of Techniques All Classifiers Can Learn from Deep Networks: Models, Optimizations, and Regularization",Alireza Ghods;Diane J Cook,"Deep neural networks have introduced novel and useful tools to the machine learning community. Other types of classifiers can potentially make use of these tools as well to improve their performance and generality. This paper reviews the current state of the art for deep learning classifier technologies that are being used outside of deep neural networks. Non-network classifiers can employ many components found in deep neural network architectures. In this paper, we review the feature learning, optimization, and regularization methods that form a core of deep network technologies. We then survey non-neural network learning algorithms that make innovative use of these methods to improve classification. Because many opportunities and challenges still exist, we discuss directions that can be pursued to expand the area of deep learning for a variety of classification algorithms. △ Less","27 September, 2019",https://arxiv.org/pdf/1909.04791
"Pretrained AI Models: Performativity, Mobility, and Change",Lav R. Varshney;Nitish Shirish Keskar;Richard Socher,"The paradigm of pretrained deep learning models has recently emerged in artificial intelligence practice, allowing deployment in numerous societal settings with limited computational resources, but also embedding biases and enabling unintended negative uses. In this paper, we treat pretrained models as objects of study and discuss the ethical impacts of their sociological position. We discuss how pretrained models are developed and compared under the common task framework, but that this may make self-regulation inadequate. Further how pretrained models may have a performative effect on society that exacerbates biases. We then discuss how pretrained models move through actor networks as a kind of computationally immutable mobile, but that users also act as agents of technological change by reinterpreting them via fine-tuning and transfer. We further discuss how users may use pretrained models in malicious ways, drawing a novel connection between the responsible innovation and user-centered innovation literatures. We close by discussing how this sociological understanding of pretrained models can inform AI governance frameworks for fairness, accountability, and transparency. △ Less","7 September, 2019",https://arxiv.org/pdf/1909.03290
One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques,Vijay Arya;Rachel K. E. Bellamy;Pin-Yu Chen;Amit Dhurandhar;Michael Hind;Samuel C. Hoffman;Stephanie Houde;Q. Vera Liao;Ronny Luss;Aleksandra Mojsilović;Sami Mourad;Pablo Pedemonte;Ramya Raghavendra;John Richards;Prasanna Sattigeri;Karthikeyan Shanmugam;Moninder Singh;Kush R. Varshney;Dennis Wei;Yunfeng Zhang,"As artificial intelligence and machine learning algorithms make further inroads into society, calls are increasing from multiple stakeholders for these algorithms to explain their outputs. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, present different requirements for explanations. Toward addressing these needs, we introduce AI Explainability 360 (http://aix360.mybluemix.net/), an open-source software toolkit featuring eight diverse and state-of-the-art explainability methods and two evaluation metrics. Equally important, we provide a taxonomy to help entities requiring explanations to navigate the space of explanation methods, not only those in the toolkit but also in the broader literature on explainability. For data scientists and other users of the toolkit, we have implemented an extensible software architecture that organizes methods according to their place in the AI modeling pipeline. We also discuss enhancements to bring research innovations closer to consumers of explanations, ranging from simplified, more accessible versions of algorithms, to tutorials and an interactive web demo to introduce AI explainability to different audiences and application domains. Together, our toolkit and taxonomy can help identify gaps where more explainability methods are needed and provide a platform to incorporate them as they are developed. △ Less","14 September, 2019",https://arxiv.org/pdf/1909.03012
Security Requirements of Commercial Drones for Public Authorities by Vulnerability Analysis of Applications,Daegeon Kim;Huy Kang Kim,"Due to the ability to overcome the geospatial limitations and to the possibility to converge the various information communication technologies, the application domains and the market size of drones are increasing internationally. Public authorities in South Korean are investing for the domestic drone industry and the technological advancement as a power of innovation and growth of the country. They are also increasing the utilization of drones for various purposes. The South Korean government ensures the security of IT equipment introduced to the public authorities by enforcing policies such as security compatibility verification and CCTV security certification. Considering the increase of the needs of drones and the possible security effects to the organization operating them, the government needs to develop the security requirements during introducing drones, but there are no such requirements yet. In this paper, we inspect the vulnerabilities of drones by analyzing the applications of commercial drones made by 4 manufacturers. We also propose the minimum security requirements to resolve the vulnerabilities. We expect our work contributes to the security improvements of drones operated in public authorities. △ Less","6 September, 2019",https://arxiv.org/pdf/1909.02786
Stack-VS: Stacked Visual-Semantic Attention for Image Caption Generation,Wei Wei;Ling Cheng;Xianling Mao;Guangyou Zhou;Feida Zhu,"Recently, automatic image caption generation has been an important focus of the work on multimodal translation task. Existing approaches can be roughly categorized into two classes, i.e., top-down and bottom-up, the former transfers the image information (called as visual-level feature) directly into a caption, and the later uses the extracted words (called as semanticlevel attribute) to generate a description. However, previous methods either are typically based one-stage decoder or partially utilize part of visual-level or semantic-level information for image caption generation. In this paper, we address the problem and propose an innovative multi-stage architecture (called as Stack-VS) for rich fine-gained image caption generation, via combining bottom-up and top-down attention models to effectively handle both visual-level and semantic-level information of an input image. Specifically, we also propose a novel well-designed stack decoder model, which is constituted by a sequence of decoder cells, each of which contains two LSTM-layers work interactively to re-optimize attention weights on both visual-level feature vectors and semantic-level attribute embeddings for generating a fine-gained image caption. Extensive experiments on the popular benchmark dataset MSCOCO show the significant improvements on different evaluation metrics, i.e., the improvements on BLEU-4/CIDEr/SPICE scores are 0.372, 1.226 and 0.216, respectively, as compared to the state-of-the-arts. △ Less","5 September, 2019",https://arxiv.org/pdf/1909.02489
Towards Models for Availability and Security Evaluation of Cloud Computing with Moving Target Defense,Matheus Torquato;Marco Vieira,"Security is one of the most relevant concerns in cloud computing. With the evolution of cyber-security threats, developing innovative techniques to thwart attacks is of utmost importance. One recent method to improve cloud computing security is Moving Target Defense (MTD). MTD makes use of dynamic reconfiguration in virtualized environments to ""confuse"" attackers or to nullify their knowledge about the system state. However, there is still no consolidated mechanism to evaluate the trade-offs between availability and security when using MTD on cloud computing. The evaluation through measurements is complex as one needs to deal with unexpected events as failures and attacks. To overcome this challenge, we intend to propose a set of models to evaluate the availability and security of MTD in cloud computing environments. The expected results include the quantification of availability and security levels under different conditions (e.g., different software aging rates, varying workloads, different attack intensities). △ Less","3 September, 2019",https://arxiv.org/pdf/1909.01392
Transferring Adaptive Theory of Mind to social robots: insights from developmental psychology to robotics,Francesca Bianco;Dimitri Ognibene,"Despite the recent advancement in the social robotic field, important limitations restrain its progress and delay the application of robots in everyday scenarios. In the present paper, we propose to develop computational models inspired by our knowledge of human infants' social adaptive abilities. We believe this may provide solutions at an architectural level to overcome the limits of current systems. Specifically, we present the functional advantages that adaptive Theory of Mind (ToM) systems would support in robotics (i.e., mentalizing for belief understanding, proactivity and preparation, active perception and learning) and contextualize them in practical applications. We review current computational models mainly based on the simulation and teleological theories, and robotic implementations to identify the limitations of ToM functions in current robotic architectures and suggest a possible future developmental pathway. Finally, we propose future studies to create innovative computational models integrating the properties of the simulation and teleological approaches for an improved adaptive ToM ability in robots with the aim of enhancing human-robot interactions and permitting the application of robots in unexplored environments, such as disasters and construction sites. To achieve this goal, we suggest directing future research towards the modern cross-talk between the fields of robotics and developmental psychology. △ Less","31 August, 2019",https://arxiv.org/pdf/1909.00197
Rethinking travel behavior modeling representations through embeddings,Francisco C. Pereira,"This paper introduces the concept of travel behavior embeddings, a method for re-representing discrete variables that are typically used in travel demand modeling, such as mode, trip purpose, education level, family type or occupation. This re-representation process essentially maps those variables into a latent space called the \emph{embedding space}. The benefit of this is that such spaces allow for richer nuances than the typical transformations used in categorical variables (e.g. dummy encoding, contrasted encoding, principal components analysis). While the usage of latent variable representations is not new per se in travel demand modeling, the idea presented here brings several innovations: it is an entirely data driven algorithm; it is informative and consistent, since the latent space can be visualized and interpreted based on distances between different categories; it preserves interpretability of coefficients, despite being based on Neural Network principles; and it is transferrable, in that embeddings learned from one dataset can be reused for other ones, as long as travel behavior keeps consistent between the datasets. The idea is strongly inspired on natural language processing techniques, namely the word2vec algorithm. Such algorithm is behind recent developments such as in automatic translation or next word prediction. Our method is demonstrated using a model choice model, and shows improvements of up to 60\% with respect to initial likelihood, and up to 20% with respect to likelihood of the corresponding traditional model (i.e. using dummy variables) in out-of-sample evaluation. We provide a new Python package, called PyTre (PYthon TRavel Embeddings), that others can straightforwardly use to replicate our results or improve their own models. Our experiments are themselves based on an open dataset (swissmetro). △ Less","31 August, 2019",https://arxiv.org/pdf/1909.00154
HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking,Shen Yan;Biyi Fang;Faen Zhang;Yu Zheng;Xiao Zeng;Hui Xu;Mi Zhang,"The use of automatic methods, often referred to as Neural Architecture Search (NAS), in designing neural network architectures has recently drawn considerable attention. In this work, we present an efficient NAS approach, named HM- NAS, that generalizes existing weight sharing based NAS approaches. Existing weight sharing based NAS approaches still adopt hand-designed heuristics to generate architecture candidates. As a consequence, the space of architecture candidates is constrained in a subset of all possible architectures, making the architecture search results sub-optimal. HM-NAS addresses this limitation via two innovations. First, HM-NAS incorporates a multi-level architecture encoding scheme to enable searching for more flexible network architectures. Second, it discards the hand-designed heuristics and incorporates a hierarchical masking scheme that automatically learns and determines the optimal architecture. Compared to state-of-the-art weight sharing based approaches, HM-NAS is able to achieve better architecture search performance and competitive model evaluation accuracy. Without the constraint imposed by the hand-designed heuristics, our searched networks contain more flexible and meaningful architectures that existing weight sharing based NAS approaches are not able to discover. △ Less","7 September, 2019",https://arxiv.org/pdf/1909.00122
Learning Rich Representations For Structured Visual Prediction Tasks,Mohammadreza Mostajabi,"We describe an approach to learning rich representations for images, that enables simple and effective predictors in a range of vision tasks involving spatially structured maps. Our key idea is to map small image elements to feature representations extracted from a sequence of nested regions of increasing spatial extent. These regions are obtained by ""zooming out"" from the pixel/superpixel all the way to scene-level resolution, and hence we call these zoom-out features. Applied to semantic segmentation and other structured prediction tasks, our approach exploits statistical structure in the image and in the label space without setting up explicit structured prediction mechanisms, and thus avoids complex and expensive inference. Instead image elements are classified by a feedforward multilayer network with skip-layer connections spanning the zoom-out levels. When used in conjunction with modern neural architectures such as ResNet, DenseNet and NASNet (to which it is complementary) our approach achieves competitive accuracy on segmentation benchmarks. In addition, we propose an approach for learning category-level semantic segmentation purely from image-level classification tag. It exploits localization cues that emerge from training a modified zoom-out architecture tailored for classification tasks, to drive a weakly supervised process that automatically labels a sparse, diverse training set of points likely to belong to classes of interest. Finally, we introduce data-driven regularization functions for the supervised training of CNNs. Our innovation takes the form of a regularizer derived by learning an autoencoder over the set of annotations. This approach leverages an improved representation of label space to inform extraction of features from images △ Less","30 August, 2019",https://arxiv.org/pdf/1908.11820
Cross-domain Aspect Category Transfer and Detection via Traceable Heterogeneous Graph Representation Learning,Zhuoren Jiang;Jian Wang;Lujun Zhao;Changlong Sun;Yao Lu;Xiaozhong Liu,"Aspect category detection is an essential task for sentiment analysis and opinion mining. However, the cost of categorical data labeling, e.g., label the review aspect information for a large number of product domains, can be inevitable but unaffordable. In this study, we propose a novel problem, cross-domain aspect category transfer and detection, which faces three challenges: various feature spaces, different data distributions, and diverse output spaces. To address these problems, we propose an innovative solution, Traceable Heterogeneous Graph Representation Learning (THGRL). Unlike prior text-based aspect detection works, THGRL explores latent domain aspect category connections via massive user behavior information on a heterogeneous graph. Moreover, an innovative latent variable ""Walker Tracer"" is introduced to characterize the global semantic/aspect dependencies and capture the informative vertexes on the random walk paths. By using THGRL, we project different domains' feature spaces into a common one, while allowing data distributions and output spaces stay differently. Experiment results show that the proposed method outperforms a series of state-of-the-art baseline models. △ Less","30 August, 2019",https://arxiv.org/pdf/1908.11610
Temporal Consistency Objectives Regularize the Learning of Disentangled Representations,Gabriele Valvano;Agisilaos Chartsias;Andrea Leo;Sotirios A. Tsaftaris,"There has been an increasing focus in learning interpretable feature representations, particularly in applications such as medical image analysis that require explainability, whilst relying less on annotated data (since annotations can be tedious and costly). Here we build on recent innovations in style-content representations to learn anatomy, imaging characteristics (appearance) and temporal correlations. By introducing a self-supervised objective of predicting future cardiac phases we improve disentanglement. We propose a temporal transformer architecture that given an image conditioned on phase difference, it predicts a future frame. This forces the anatomical decomposition to be consistent with the temporal cardiac contraction in cine MRI and to have semantic meaning with less need for annotations. We demonstrate that using this regularization, we achieve competitive results and improve semi-supervised segmentation, especially when very few labelled data are available. Specifically, we show Dice increase of up to 19\% and 7\% compared to supervised and semi-supervised approaches respectively on the ACDC dataset. Code is available at: https://github.com/gvalvano/sdtnet . △ Less","29 August, 2019",https://arxiv.org/pdf/1908.11330
PULP-NN: Accelerating Quantized Neural Networks on Parallel Ultra-Low-Power RISC-V Processors,Angelo Garofalo;Manuele Rusci;Francesco Conti;Davide Rossi;Luca Benini,"We present PULP-NN, an optimized computing library for a parallel ultra-low-power tightly coupled cluster of RISC-V processors. The key innovation in PULP-NN is a set of kernels for Quantized Neural Network (QNN) inference, targeting byte and sub-byte data types, down to INT-1, tuned for the recent trend toward aggressive quantization in deep neural network inference. The proposed library exploits both the digital signal processing (DSP) extensions available in the PULP RISC-V processors and the cluster's parallelism, achieving up to 15.5 MACs/cycle on INT-8 and improving performance by up to 63x with respect to a sequential implementation on a single RISC-V core implementing the baseline RV32IMC ISA. Using PULP-NN, a CIFAR-10 network on an octa-core cluster runs in 30x and 19.6x less clock cycles than the current state-of-the-art ARM CMSIS-NN library, running on STM32L4 and STM32H7 MCUs, respectively. The proposed library, when running on GAP-8 processor, outperforms by 36.8x and by 7.45x the execution on energy efficient MCUs such as STM32L4 and high-end MCUs such as STM32H7 respectively, when operating at the maximum frequency. The energy efficiency on GAP-8 is 14.1x higher than STM32L4 and 39.5x higher than STM32H7, at the maximum efficiency operating point. △ Less","29 August, 2019",https://arxiv.org/pdf/1908.11263
Evaluating resilience in urban transportation systems for sustainability: A systems-based Bayesian network model,Junqing Tang;Hans Heinimann;Ke Han;Hanbin Luo;Botao Zhong,"This paper proposes a hierarchical Bayesian network model (BNM) to quantitatively evaluate the resilience of urban transportation infrastructure. Based on systemic thinkings and sustainability perspectives, we investigate the long-term resilience of the road transportation systems in four cities of China from 1998 to 2017, namely Beijing, Tianjin, Shanghai, and Chongqing, respectively. The model takes into account the factors involved in stages of design, construction, operation, management, and innovation of urban road transportation, which collected from multi-source data platforms. We test the model with the forward inference, sensitivity analysis, and backward inference. The result shows that the overall resilience of all four cities' transportation infrastructure is within a moderate range with values between 50% to 60%. Although they all have an ever-increasing economic level, Beijing and Tianjin demonstrate a clear ""V"" shape in the long-term transportation resilience, which indicates a strong multi-dimensional, dynamic, and non-linear characteristic in resilience-economic coupling effect. Additionally, the results obtained from the sensitivity analysis and backward inference suggest that urban decision-makers should pay more attention to the capabilities of quick rebuilding and making changes to cope with future disturbance. As an exploratory study, this study clarifies the concepts of long-term multi-dimensional resilience and specific hazard-related resilience and provides an effective decision-support tool for stakeholders when building sustainable infrastructure. △ Less","26 August, 2019",https://arxiv.org/pdf/1908.09774
HyperService: Interoperability and Programmability Across Heterogeneous Blockchains,Zhuotao Liu;Yangxi Xiang;Jian Shi;Peng Gao;Haoyu Wang;Xusheng Xiao;Bihan Wen;Yih-Chun Hu,"Blockchain interoperability, which allows state transitions across different blockchain networks, is critical functionality to facilitate major blockchain adoption. Existing interoperability protocols mostly focus on atomic token exchange between blockchains. However, as blockchains have been upgraded from passive distributed ledgers into programmable state machines (thanks to smart contracts), the scope of blockchain interoperability goes beyond just token exchange. In this paper, we present HyperService, the first platform that delivers interoperability and programmability across heterogeneous blockchains. HyperService is powered by two innovative designs: (i) a developer-facing programming framework that allows developers to build cross-chain applications in a unified programming model; and (ii) a secure blockchain-facing cryptography protocol that provably realizes those applications on blockchains. We implement a prototype of HyperService in about 35,000 lines of code to demonstrate its practicality. Our experiment results show that HyperService imposes reasonable latency, in order of seconds, on the end-to-end execution of cross-chain applications △ Less","4 September, 2019",https://arxiv.org/pdf/1908.09343
AIBench: An Industry Standard Internet Service AI Benchmark Suite,Wanling Gao;Fei Tang;Lei Wang;Jianfeng Zhan;Chunxin Lan;Chunjie Luo;Yunyou Huang;Chen Zheng;Jiahui Dai;Zheng Cao;Daoyi Zheng;Haoning Tang;Kunlin Zhan;Biao Wang;Defei Kong;Tong Wu;Minghe Yu;Chongkang Tan;Huan Li;Xinhui Tian;Yatao Li;Junchao Shao;Zhenyu Wang;Xiaoyu Wang;Hainan Ye,"Today's Internet Services are undergoing fundamental changes and shifting to an intelligent computing era where AI is widely employed to augment services. In this context, many innovative AI algorithms, systems, and architectures are proposed, and thus the importance of benchmarking and evaluating them rises. However, modern Internet services adopt a microservice-based architecture and consist of various modules. The diversity of these modules and complexity of execution paths, the massive scale and complex hierarchy of datacenter infrastructure, the confidential issues of data sets and workloads pose great challenges to benchmarking. In this paper, we present the first industry-standard Internet service AI benchmark suite---AIBench with seventeen industry partners, including several top Internet service providers. AIBench provides a highly extensible, configurable, and flexible benchmark framework that contains loosely coupled modules. We identify sixteen prominent AI problem domains like learning to rank, each of which forms an AI component benchmark, from three most important Internet service domains: search engine, social network, and e-commerce, which is by far the most comprehensive AI benchmarking effort. On the basis of the AIBench framework, abstracting the real-world data sets and workloads from one of the top e-commerce providers, we design and implement the first end-to-end Internet service AI benchmark, which contains the primary modules in the critical paths of an industry scale application and is scalable to deploy on different cluster scales. The specifications, source code, and performance numbers are publicly available from the benchmark council web site http://www.benchcouncil.org/AIBench/index.html. △ Less","23 October, 2019",https://arxiv.org/pdf/1908.08998
Understanding Conditional Compilation Through Integrated Representation of Variability and Source Code,David Baum;Christina Sixtus;Lisa Vogelsberg;Ulrich Eisenecker,"The C preprocessor (CPP) is a standard tool for introducing variability into source programs and is often applied either implicitly or explicitly for implementing a Software Product Line (SPL). Despite its practical relevance, CPP has many drawbacks. Because of that it is very difficult to understand the variability implemented using CPP. To facilitate this task we provide an innovative analytics tool which bridges the gap between feature models as more abstract representations of variability and its concrete implementation with the means of CPP. It allows to interactively explore the entities of a source program with respect to the variability realized by conditional compilation. Thus, it simplifies tracing and understanding the effect of enabling or disabling feature flags. △ Less","20 August, 2019",https://arxiv.org/pdf/1908.08375
Enabling hyperparameter optimization in sequential autoencoders for spiking neural data,Mohammad Reza Keshtkaran;Chethan Pandarinath,"Continuing advances in neural interfaces have enabled simultaneous monitoring of spiking activity from hundreds to thousands of neurons. To interpret these large-scale data, several methods have been proposed to infer latent dynamic structure from high-dimensional datasets. One recent line of work uses recurrent neural networks in a sequential autoencoder (SAE) framework to uncover dynamics. SAEs are an appealing option for modeling nonlinear dynamical systems, and enable a precise link between neural activity and behavior on a single-trial basis. However, the very large parameter count and complexity of SAEs relative to other models has caused concern that SAEs may only perform well on very large training sets. We hypothesized that with a method to systematically optimize hyperparameters (HPs), SAEs might perform well even in cases of limited training data. Such a breakthrough would greatly extend their applicability. However, we find that SAEs applied to spiking neural data are prone to a particular form of overfitting that cannot be detected using standard validation metrics, which prevents standard HP searches. We develop and test two potential solutions: an alternate validation method (""sample validation"") and a novel regularization method (""coordinated dropout""). These innovations prevent overfitting quite effectively, and allow us to test whether SAEs can achieve good performance on limited data through large-scale HP optimization. When applied to data from motor cortex recorded while monkeys made reaches in various directions, large-scale HP optimization allowed SAEs to better maintain performance for small dataset sizes. Our results should greatly extend the applicability of SAEs in extracting latent dynamics from sparse, multidimensional data, such as neural population spiking activity. △ Less","22 August, 2019",https://arxiv.org/pdf/1908.07896
Leveraging creativity in requirements elicitation within agile software development: a systematic literature review,Ainhoa Aldave;Juan M. Vara;David Granada;Esperanza Marcos,"Agile approaches tend to focus solely on scoping and simplicity rather than on problem solving and discovery. This hampers the development of innovative solutions. Additionally, little has been said about how to capture and represent the real user needs. To fill this gap, some authors argue in favor of the application of Creative thinking for requirements elicitation within agile software development. This synergy between creativeness and agility has arisen as a new means of bringing innovation and flexibility to increasingly demanding software. The aim of the present study is therefore to employ a systematic review to investigate the state-of-the-art of those approaches that leverage creativity in requirements elicitation within Agile Software Development, as well as the benefits, limitations and strength of evidence of these approaches. The search strategy identified 1451 studies. The selected studies contained 13 different and unique proposals. These approaches provide evidence that enhanced creativity in requirements elicitation can be successfully implemented in real software projects. We specifically observed that projects related to user interface development, such as those for mobile or web applications, are good candidates for the use of these approaches. We have also found that agile methodologies are preferred when introducing creativity into requirements elicitation. Despite this being a new research field, there is a mixture of techniques, tools and processes that have already been and are currently being successfully tested in industry. Finally, we have found that, although creativity is an important ingredient with which to bring about innovation, it is not always sufficient to generate new requirements because this needs to be followed by user engagement and a specific context in which proper conditions, such as flexibility, time or resources, have to be met. △ Less","21 August, 2019",https://arxiv.org/pdf/1908.07783
Survey on Deep Neural Networks in Speech and Vision Systems,Mahbubul Alam;Manar D. Samad;Lasitha Vidyaratne;Alexander Glandon;Khan M. Iftekharuddin,"This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems. △ Less","30 November, 2019",https://arxiv.org/pdf/1908.07656
AI for Earth: Rainforest Conservation by Acoustic Surveillance,Yuan Liu;Zhongwei Cheng;Jie Liu;Bourhan Yassin;Zhe Nan;Jiebo Luo,"Saving rainforests is a key to halting adverse climate changes. In this paper, we introduce an innovative solution built on acoustic surveillance and machine learning technologies to help rainforest conservation. In particular, We propose new convolutional neural network (CNN) models for environmental sound classification and achieved promising preliminary results on two datasets, including a public audio dataset and our real rainforest sound dataset. The proposed audio classification models can be easily extended in an automated machine learning paradigm and integrated in cloud-based services for real world deployment. △ Less","19 August, 2019",https://arxiv.org/pdf/1908.07517
Weakly-supervised Action Localization with Background Modeling,Phuc Xuan Nguyen;Deva Ramanan;Charless C. Fowlkes,"We describe a latent approach that learns to detect actions in long sequences given training videos with only whole-video class labels. Our approach makes use of two innovations to attention-modeling in weakly-supervised learning. First, and most notably, our framework uses an attention model to extract both foreground and background frames whose appearance is explicitly modeled. Most prior works ignore the background, but we show that modeling it allows our system to learn a richer notion of actions and their temporal extents. Second, we combine bottom-up, class-agnostic attention modules with top-down, class-specific activation maps, using the latter as form of self-supervision for the former. Doing so allows our model to learn a more accurate model of attention without explicit temporal supervision. These modifications lead to 10% AP@IoU=0.5 improvement over existing systems on THUMOS14. Our proposed weaklysupervised system outperforms recent state-of-the-arts by at least 4.3% AP@IoU=0.5. Finally, we demonstrate that weakly-supervised learning can be used to aggressively scale-up learning to in-the-wild, uncurated Instagram videos. The addition of these videos significantly improves localization performance of our weakly-supervised model △ Less","18 August, 2019",https://arxiv.org/pdf/1908.06552
Occlusion Robust Face Recognition Based on Mask Learning with PairwiseDifferential Siamese Network,Lingxue Song;Dihong Gong;Zhifeng Li;Changsong Liu;Wei Liu,"Deep Convolutional Neural Networks (CNNs) have been pushing the frontier of the face recognition research in the past years. However, existing general CNN face models generalize poorly to the scenario of occlusions on variable facial areas. Inspired by the fact that a human visual system explicitly ignores occlusions and only focuses on non-occluded facial areas, we propose a mask learning strategy to find and discard the corrupted feature elements for face recognition. A mask dictionary is firstly established by exploiting the differences between the top convoluted features of occluded and occlusion-free face pairs using an innovatively designed Pairwise Differential Siamese Network (PDSN). Each item of this dictionary captures the correspondence between occluded facial areas and corrupted feature elements, which is named Feature Discarding Mask (FDM). When dealing with a face image with random partial occlusions, we generate its FDM by combining relevant dictionary items and then multiply it with the original features to eliminate those corrupted feature elements. Comprehensive experiments on both synthesized and realistic occluded face datasets show that the proposed approach significantly outperforms the state-of-the-arts. △ Less","17 August, 2019",https://arxiv.org/pdf/1908.06290
D-UNet: a dimension-fusion U shape network for chronic stroke lesion segmentation,Yongjin Zhou;Weijian Huang;Pei Dong;Yong Xia;Shanshan Wang,"Assessing the location and extent of lesions caused by chronic stroke is critical for medical diagnosis, surgical planning, and prognosis. In recent years, with the rapid development of 2D and 3D convolutional neural networks (CNN), the encoder-decoder structure has shown great potential in the field of medical image segmentation. However, the 2D CNN ignores the 3D information of medical images, while the 3D CNN suffers from high computational resource demands. This paper proposes a new architecture called dimension-fusion-UNet (D-UNet), which combines 2D and 3D convolution innovatively in the encoding stage. The proposed architecture achieves a better segmentation performance than 2D networks, while requiring significantly less computation time in comparison to 3D networks. Furthermore, to alleviate the data imbalance issue between positive and negative samples for the network training, we propose a new loss function called Enhance Mixing Loss (EML). This function adds a weighted focal coefficient and combines two traditional loss functions. The proposed method has been tested on the ATLAS dataset and compared to three state-of-the-art methods. The results demonstrate that the proposed method achieves the best quality performance in terms of DSC = 0.5349+0.2763 and precision = 0.6331+0.295). △ Less","14 August, 2019",https://arxiv.org/pdf/1908.05104
To Beta or Not To Beta: Information Bottleneck for DigitaL Image Forensics,Aurobrata Ghosh;Zheng Zhong;Steve Cruz;Subbu Veeravasarapu;Terrance E Boult;Maneesh Singh,"We consider an information theoretic approach to address the problem of identifying fake digital images. We propose an innovative method to formulate the issue of localizing manipulated regions in an image as a deep representation learning problem using the Information Bottleneck (IB), which has recently gained popularity as a framework for interpreting deep neural networks. Tampered images pose a serious predicament since digitized media is a ubiquitous part of our lives. These are facilitated by the easy availability of image editing software and aggravated by recent advances in deep generative models such as GANs. We propose InfoPrint, a computationally efficient solution to the IB formulation using approximate variational inference and compare it to a numerical solution that is computationally expensive. Testing on a number of standard datasets, we demonstrate that InfoPrint outperforms the state-of-the-art and the numerical solution. Additionally, it also has the ability to detect alterations made by inpainting GANs. △ Less","11 August, 2019",https://arxiv.org/pdf/1908.03864
Color-Coded Fiber-Optic Tactile Sensor for an Elastomeric Robot Skin,Zhanat Kappassov;Daulet Baimukashev;Zharaskhan Kuanyshuly;Yerzhan Massalin;Arshat Urazbayev;Huseyin Atakan Varol,"The sense of touch is essential for reliable mapping between the environment and a robot which interacts physically with objects. Presumably, an artificial tactile skin would facilitate safe interaction of the robots with the environment. In this work, we present our color-coded tactile sensor, incorporating plastic optical fibers (POF), transparent silicone rubber and an off-the-shelf color camera. Processing electronics are placed away from the sensing surface to make the sensor robust to harsh environments. Contact localization is possible thanks to the lower number of light sources compared to the number of camera POFs. Classical machine learning techniques and a hierarchical classification scheme were used for contact localization. Specifically, we generated the mapping from stimulation to sensation of a robotic perception system using our sensor. We achieved a force sensing range up to 18 N with the force resolution of around 3.6~N and the spatial resolution of 8~mm. The color-coded tactile sensor is suitable for tactile exploration and might enable further innovations in robust tactile sensing. △ Less","10 August, 2019",https://arxiv.org/pdf/1908.03687
LSTM-based Flow Prediction,Hongzhi Wang;Yang Song;Shihan Tang,"In this paper, a method of prediction on continuous time series variables from the production or flow -- an LSTM algorithm based on multivariate tuning -- is proposed. The algorithm improves the traditional LSTM algorithm and converts the time series data into supervised learning sequences regarding industrial data's features. The main innovation of this paper consists in introducing the concepts of periodic measurement and time window in the industrial prediction problem, especially considering industrial data with time series characteristics. Experiments using real-world datasets show that the prediction accuracy is improved, 54.05% higher than that of traditional LSTM algorithm. △ Less","9 August, 2019",https://arxiv.org/pdf/1908.03571
No Need of Data Pre-processing: A General Framework for Radio-Based Device-Free Context Awareness,Bo Wei;Kai Li;Chengwen Luo;Weitao Xu;Jin Zhang,"Device-free context awareness is important to many applications. There are two broadly used approaches for device-free context awareness, i.e. video-based and radio-based. Video-based applications can deliver good performance, but privacy is a serious concern. Radio-based context awareness has drawn researchers attention instead because it does not violate privacy and radio signal can penetrate obstacles. Recently, deep learning has been introduced into radio-based device-free context awareness and helps boost the recognition accuracy. The present works design explicit methods for each radio based application. They also use one additional step to extract features before conducting classification and exploit deep learning as a classification tool. The additional initial data processing step introduces unnecessary noise and information loss. Without initial data processing, it is, however, challenging to explore patterns of raw signals. In this paper, we are the first to propose an innovative deep learning based general framework for both signal processing and classification. The key novelty of this paper is that the framework can be generalised for all the radio-based context awareness applications. We also eliminate the additional effort to extract features from raw radio signals. We conduct extensive evaluations to show the superior performance of our proposed method and its generalisation. △ Less","9 August, 2019",https://arxiv.org/pdf/1908.03398
Local Differential Privacy for Deep Learning,M. A. P. Chamikara;P. Bertok;I. Khalil;D. Liu;S. Camtepe;M. Atiquzzaman,"The internet of things (IoT) is transforming major industries including but not limited to healthcare, agriculture, finance, energy, and transportation. IoT platforms are continually improving with innovations such as the amalgamation of software-defined networks (SDN) and network function virtualization (NFV) in the edge-cloud interplay. Deep learning (DL) is becoming popular due to its remarkable accuracy when trained with a massive amount of data, such as generated by IoT. However, DL algorithms tend to leak privacy when trained on highly sensitive crowd-sourced data such as medical data. Existing privacy-preserving DL algorithms rely on the traditional server-centric approaches requiring high processing powers. We propose a new local differentially private (LDP) algorithm named LATENT that redesigns the training process. LATENT enables a data owner to add a randomization layer before data leave the data owners' devices and reach a potentially untrusted machine learning service. This feature is achieved by splitting the architecture of a convolutional neural network (CNN) into three layers: (1) convolutional module, (2) randomization module, and (3) fully connected module. Hence, the randomization module can operate as an NFV privacy preservation service in an SDN-controlled NFV, making LATENT more practical for IoT-driven cloud-based environments compared to existing approaches. The randomization module employs a newly proposed LDP protocol named utility enhancing randomization, which allows LATENT to maintain high utility compared to existing LDP protocols. Our experimental evaluation of LATENT on convolutional deep neural networks demonstrates excellent accuracy (e.g. 91%- 96%) with high model quality even under low privacy budgets (e.g. \varepsilon=0.5). △ Less","9 November, 2019",https://arxiv.org/pdf/1908.02997
AlphaStock: A Buying-Winners-and-Selling-Losers Investment Strategy using Interpretable Deep Reinforcement Attention Networks,Jingyuan Wang;Yang Zhang;Ke Tang;Junjie Wu;Zhang Xiong,"Recent years have witnessed the successful marriage of finance innovations and AI techniques in various finance applications including quantitative trading (QT). Despite great research efforts devoted to leveraging deep learning (DL) methods for building better QT strategies, existing studies still face serious challenges especially from the side of finance, such as the balance of risk and return, the resistance to extreme loss, and the interpretability of strategies, which limit the application of DL-based strategies in real-life financial markets. In this work, we propose AlphaStock, a novel reinforcement learning (RL) based investment strategy enhanced by interpretable deep attention networks, to address the above challenges. Our main contributions are summarized as follows: i) We integrate deep attention networks with a Sharpe ratio-oriented reinforcement learning framework to achieve a risk-return balanced investment strategy; ii) We suggest modeling interrelationships among assets to avoid selection bias and develop a cross-asset attention mechanism; iii) To our best knowledge, this work is among the first to offer an interpretable investment strategy using deep reinforcement learning models. The experiments on long-periodic U.S. and Chinese markets demonstrate the effectiveness and robustness of AlphaStock over diverse market states. It turns out that AlphaStock tends to select the stocks as winners with high long-term growth, low volatility, high intrinsic value, and being undervalued recently. △ Less","24 July, 2019",https://arxiv.org/pdf/1908.02646
Etat de l'art des systèmes robotisés en vue d'une application pour la chirurgie otologique,M. Dreux;L Ginzburg;P. Bordure;D. Chablat;G. Michel,"This article deals with a patents state-of-the-art linked to our robotic system which holds an endoscope. In a first part, we analyze how the endoscope operates as well as we discover the environment where we wish to use it: the middle ear. In a second step, we establish a state-of-the-art on patents existing in the robotic and surgical fields. The approach is based on a specific search methodology using keywords and classifications. Thanks to all the patents found, we gather them according their country and date of publication, and we are able to analyze the results. This study brings to light technical solutions invented in recent years and allows us to find an innovative research axis. △ Less","7 August, 2019",https://arxiv.org/pdf/1908.02561
Multi-Robot Path Deconfliction through Prioritization by Path Prospects,Wenying Wu;Subhrajit Bhattacharya;Amanda Prorok,"This work deals with the problem of planning conflict-free paths for mobile robots in cluttered environments. Since centralized, coupled planning algorithms are computationally intractable for large numbers of robots, we consider decoupled planning, in which robots plan their paths sequentially in order of priority. Choosing how to prioritize the robots is a key consideration. State-of-the-art prioritization heuristics, however, do not model the coupling between a robot's mobility and its environment. In this paper, we propose a prioritization rule that can be computed online by each robot independently, and that provides consistent, conflict-free path plans. Our innovation is to formalize a robot's path prospects to reach its goal from its current location. To this end, we consider the number of homology classes of trajectories, and use this as a prioritization rule in our decentralized path planning algorithm, whenever any robots enter negotiation to deconflict path plans. This prioritization rule guarantees a partial ordering over the robot set. We perform simulations that compare our method to five benchmarks, and show that it reaches the highest success rate (w.r.t. completeness), and that it strikes the best balance between makespan and flowtime objectives. △ Less","6 August, 2019",https://arxiv.org/pdf/1908.02361
Proof of All: Verifiable Computation in a Nutshell,Mario Alessandro Barbara,"Recent advances in the cryptographic field of ""Zero-Knowledge Proofs"" have sparked a new wave of research, giving birth to many exciting theoretical approaches in the last few years. Such research has often overlapped with the need for private and scalable solutions of Blockchain-based communities, resulting in the first practical implementations of such systems. Many of these innovative constructions have developed in parallel, using different terminologies and evolving into a fragmented ecosystem, calling for their consolidation into the more stable domain of ""Verifiable Computation"". In this master thesis I propose a unifying Verifiable Computation model for the simplification and efficient comparison of all cryptographic proof systems. I take advantage of this model to analyse innovative technologies (Homomorphic Authenticators, Verifiable Delay Functions) which developed into their own specialised domains, and I attempt to make them more accessible for newcomers to the field. Furthermore, I expand on the future of Verifiable Computation, Universal proof compilers and ""Proofs of All"", by approaching the state-of-the-art zk-STARK construction from a more accessible and informal design perspective. △ Less","31 August, 2019",https://arxiv.org/pdf/1908.02327
Concury: A Fast and Light-weighted Software Load Balancer,Shouqian Shi;Chen Qian;Ye Yu;Xin Li;Ying Zhang;Xiaozhou Li,"A load balancer (LB) is a vital network function for cloud services to balance the load amongst resources. Stateful software LBs that run on commodity servers provides flexibility, cost-efficiency, and packet consistency. However current designs have two main limitations: 1) states are stored as digests which may cause packet inconsistency due to digest collisions; 2) the data plane needs to update for every new connection, and frequent updates hurt throughput and packet consistency. In this work, we present a new software stateful LB called Concury, which is the first solution to solve these problems. The key innovation of Concury is an algorithmic approach to store and look up large network states with frequent connection arrivals, which is succinct in memory cost, consistent under network changes, and incurs infrequent data plane updates. The evaluation results show that the Concury algorithm provides 4x throughput and consumes less memory compared to other LB algorithms, while providing weighted load balancing and false-hit freedom, for both real and synthetic data center traffic. We implement Concury as a prototype system deployed in CloudLab and show that the throughput of Concury on a single thread can reach 62.5% of the maximum capacity of two 10GbE NICs and that on two threads can reach the maximum capacity. △ Less","5 August, 2019",https://arxiv.org/pdf/1908.01889
uBaaS: A Unified Blockchain as a Service Platform,Qinghua Lu;Xiwei Xu;Yue Liu;Ingo Weber;Liming Zhu;Weishan Zhang,"Blockchain is an innovative distributed ledger technology which has attracted a wide range of interests for building the next generation of applications to address lack-of-trust issues in business. Blockchain as a service (BaaS) is a promising solution to improve the productivity of blockchain application development. However, existing BaaS deployment solutions are mostly vendor-locked: they are either bound to a cloud provider or a blockchain platform. In addition to deployment, design and implementation of blockchain-based applications is a hard task requiring deep expertise. Therefore, this paper presents a unified blockchain as a service platform (uBaaS) to support both design and deployment of blockchain-based applications. The services in uBaaS include deployment as a service, design pattern as a service and auxiliary services. In uBaaS, deployment as a service is platform agnostic, which can avoid lock-in to specific cloud platforms, while design pattern as a service applies design patterns for data management and smart contract design to address the scalability and security issues of blockchain. The proposed solutions are evaluated using a real-world quality tracing use case in terms of feasibility and scalability. △ Less","30 July, 2019",https://arxiv.org/pdf/1907.13293
Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking,Tianyang Xu;Zhen-Hua Feng;Xiao-Jun Wu;Josef Kittler,"We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system. In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters. To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at https://github.com/XU-TIANYANG/GFS-DCF. △ Less","2 August, 2019",https://arxiv.org/pdf/1907.13242
Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems,Vasimuddin Md;Sanchit Misra;Heng Li;Srinivas Aluru,"Innovations in Next-Generation Sequencing are enabling generation of DNA sequence data at ever faster rates and at very low cost. Large sequencing centers typically employ hundreds of such systems. Such high-throughput and low-cost generation of data underscores the need for commensurate acceleration in downstream computational analysis of the sequencing data. A fundamental step in downstream analysis is mapping of the reads to a long reference DNA sequence, such as a reference human genome. Sequence mapping is a compute-intensive step that accounts for more than 30% of the overall time of the GATK workflow. BWA-MEM is one of the most widely used tools for sequence mapping and has tens of thousands of users. In this work, we focus on accelerating BWA-MEM through an efficient architecture aware implementation, while maintaining identical output. The volume of data requires distributed computing environment, usually deploying multicore processors. Since the application can be easily parallelized for distributed memory systems, we focus on performance improvements on a single socket multicore processor. BWA-MEM run time is dominated by three kernels, collectively responsible for more than 85% of the overall compute time. We improved the performance of these kernels by 1) improving cache reuse, 2) simplifying the algorithms, 3) replacing small fragmented memory allocations with a few large contiguous ones, 4) software prefetching, and 5) SIMD utilization wherever applicable - and massive reorganization of the source code enabling these improvements. As a result, we achieved nearly 2x, 183x, and 8x speedups on the three kernels, respectively, resulting in up to 3.5x and 2.4x speedups on end-to-end compute time over the original BWA-MEM on single thread and single socket of Intel Xeon Skylake processor. To the best of our knowledge, this is the highest reported speedup over BWA-MEM. △ Less","27 July, 2019",https://arxiv.org/pdf/1907.12931
A Multi-Scale Mapping Approach Based on a Deep Learning CNN Model for Reconstructing High-Resolution Urban DEMs,Ling Jiang;Yang Hu;Xilin Xia;Qiuhua Liang;Andrea Soltoggio,"The shortage of high-resolution urban digital elevation model (DEM) datasets has been a challenge for modelling urban flood and managing its risk. A solution is to develop effective approaches to reconstruct high-resolution DEMs from their low-resolution equivalents that are more widely available. However, the current high-resolution DEM reconstruction approaches mainly focus on natural topography. Few attempts have been made for urban topography which is typically an integration of complex man-made and natural features. This study proposes a novel multi-scale mapping approach based on convolutional neural network (CNN) to deal with the complex characteristics of urban topography and reconstruct high-resolution urban DEMs. The proposed multi-scale CNN model is firstly trained using urban DEMs that contain topographic features at different resolutions, and then used to reconstruct the urban DEM at a specified (high) resolution from a low-resolution equivalent. A two-level accuracy assessment approach is also designed to evaluate the performance of the proposed urban DEM reconstruction method, in terms of numerical accuracy and morphological accuracy. The proposed DEM reconstruction approach is applied to a 121 km2 urbanized area in London, UK. Compared with other commonly used methods, the current CNN based approach produces superior results, providing a cost-effective innovative method to acquire high-resolution DEMs in other data-scarce environments. △ Less","8 October, 2019",https://arxiv.org/pdf/1907.12898
Optimal Dynamic Multi-Resource Management in Earth Observation Oriented Space Information Networks,Yu Wang;Min Sheng;Qiang Ye;Shan Zhang;Weihua Zhuang;Jiandong Li,"Space information network (SIN) is an innovative networking architecture to achieve near-real-time mass data observation, processing and transmission over the globe. In the SIN environment, it is essential to coordinate multi-dimensional heterogeneous resources (i.e., observation resource, computation resource and transmission resource) to improve network performance. However, the time varying property of both the observation resource and transmission resource is not fully exploited in existing studies. Dynamic resource management according to instantaneous channel conditions has a potential to enhance network performance. To this end, in this paper, we study the multi-resource dynamic management problem, considering stochastic observation and transmission channel conditions in SINs. Specifically, we develop an aggregate optimization framework for observation scheduling, compression ratio selection and transmission scheduling, and formulate a flow optimization problem based on extended time expanded graph (ETEG) to maximize the sum network utility. Then, we equivalently transform the flow optimization problem on ETEG as a queue stability-related stochastic optimization problem. An online algorithm is proposed to solve the problem in a slot-by-slot manner by exploiting the Lyapunov optimization technique. Performance analysis shows that the proposed algorithm achieves close-to-optimal network utility while guaranteeing bounded queue occupancy. Extensive simulation results further validate the efficiency of the proposed algorithm and evaluate the impacts of various network parameters on the algorithm performance. △ Less","29 July, 2019",https://arxiv.org/pdf/1907.12717
The Operational Cost of Ethereum Airdrops,Michael Fröwis;Rainer Böhme,"Efficient transfers to many recipients present a host of issues on Ethereum. First, accounts are identified by long and incompressible constants. Second, these constants have to be stored and communicated for each payment. Third, the standard interface for token transfers does not support lists of recipients, adding repeated communication to the overhead. Since Ethereum charges resource usage, even small optimizations translate to cost savings. Airdrops, a popular marketing tool used to boost coin uptake, present a relevant example for the value of optimizing bulk transfers. Therefore, we review technical solutions for airdrops of Ethereum-based tokens, discuss features and prerequisites, and compare the operational costs by simulating 35 scenarios. We find that cost savings of factor two are possible, but require specific provisions in the smart contract implementing the token system. Pull-based approaches, which use on-chain interaction with the recipients, promise moderate savings for the distributor while imposing a disproportional cost on each recipient. Total costs are broadly linear in the number of recipients independent of the technical approach. We publish the code of the simulation framework for reproducibility, to support future airdrop decisions, and to benchmark innovative bulk payment solutions. △ Less","29 July, 2019",https://arxiv.org/pdf/1907.12383
Capture Aware Sequential Waterfilling for LoraWAN Adaptive Data Rate,Giuseppe Bianchi;Francesca Cuomo;Domenico Garlisi;Ilenia Tinnirello,"LoRaWAN (Long Range Wide Area Network) is emerging as an attractive network infrastructure for ultra low power Internet of Things devices. Even if the technology itself is quite mature and specified, the currently deployed wireless resource allocation strategies are still coarse and based on rough heuristics. This paper proposes an innovative ""sequential waterfilling"" strategy for assigning Spreading Factors (SF) to End-Devices (ED). Our design relies on three complementary approaches: i) equalize the Time-on-Air of the packets transmitted by the system's EDs in each spreading factor's group; ii) balance the spreading factors across multiple access gateways, and iii) keep into account the channel capture, which our experimental results show to be very substantial in LoRa. While retaining an extremely simple and scalable implementation, this strategy yields a significant improvement (up to 38%) in the network capacity over the legacy Adaptive Data Rate (ADR), and appears to be extremely robust to different operating/load conditions and network topology configurations. △ Less","14 November, 2019",https://arxiv.org/pdf/1907.12360
Genetic Deep Learning for Lung Cancer Screening,Hunter Park;Connor Monahan,"Convolutional neural networks (CNNs) have shown great promise in improving computer aided detection (CADe). From classifying tumors found via mammography as benign or malignant to automated detection of colorectal polyps in CT colonography, these advances have helped reduce the need for further evaluation with invasive testing and prevent errors from missed diagnoses by acting as a second observer in today's fast paced and high volume clinical environment. CADe methods have become faster and more precise thanks to innovations in deep learning over the past several years. With advancements such as the inception module and utilization of residual connections, the approach to designing CNN architectures has become an art. It is customary to use proven models and fine tune them for particular tasks given a dataset, often requiring tedious work. We investigated using a genetic algorithm (GA) to conduct a neural architectural search (NAS) to generate a novel CNN architecture to find early stage lung cancer in chest x-rays (CXR). Using a dataset of over twelve thousand biopsy proven cases of lung cancer, the trained classification model achieved an accuracy of 97.15% with a PPV of 99.88% and a NPV of 94.81%, beating models such as Inception-V3 and ResNet-152 while simultaneously reducing the number of parameters a factor of 4 and 14, respectively. △ Less","27 July, 2019",https://arxiv.org/pdf/1907.11849
Mining Twitter to Assess the Determinants of Health Behavior towards Human Papillomavirus Vaccination in the United States,Hansi Zhang;Christopher Wheldon;Adam G. Dunn;Cui Tao;Jinhai Huo;Rui Zhang;Mattia Prosperi;Yi Guo;Jiang Bian,"Objectives To test the feasibility of using Twitter data to assess determinants of consumers' health behavior towards Human papillomavirus (HPV) vaccination informed by the Integrated Behavior Model (IBM). Methods We used three Twitter datasets spanning from 2014 to 2018. We preprocessed and geocoded the tweets, and then built a rule-based model that classified each tweet into either promotional information or consumers' discussions. We applied topic modeling to discover major themes, and subsequently explored the associations between the topics learned from consumers' discussions and the responses of HPV-related questions in the Health Information National Trends Survey (HINTS). Results We collected 2,846,495 tweets and analyzed 335,681 geocoded tweets. Through topic modeling, we identified 122 high-quality topics. The most discussed consumer topic is ""cervical cancer screening""; while in promotional tweets, the most popular topic is to increase awareness of ""HPV causes cancer"". 87 out of the 122 topics are correlated between promotional information and consumers' discussions. Guided by IBM, we examined the alignment between our Twitter findings and the results obtained from HINTS. 35 topics can be mapped to HINTS questions by keywords, 112 topics can be mapped to IBM constructs, and 45 topics have statistically significant correlations with HINTS responses in terms of geographic distributions. Conclusion Not only mining Twitter to assess consumers' health behaviors can obtain results comparable to surveys but can yield additional insights via a theory-driven approach. Limitations exist, nevertheless, these encouraging results impel us to develop innovative ways of leveraging social media in the changing health communication landscape. △ Less","6 July, 2019",https://arxiv.org/pdf/1907.11624
Measuring the Expertise of Workers for Crowdsourcing Applications,Jean-Christophe Dubois;Laetitia Gros;Mouloud Kharoune;Yolande Le Gall;Arnaud Martin;Zoltán Miklós;Hosna Ouni,"Crowdsourcing platforms enable companies to propose tasks to a large crowd of users. The workers receive a compensation for their work according to the serious of the tasks they managed to accomplish. The evaluation of the quality of responses obtained from the crowd remains one of the most important problems in this context. Several methods have been proposed to estimate the expertise level of crowd workers. We propose an innovative measure of expertise assuming that we possess a dataset with an objective comparison of the items concerned. Our method is based on the definition of four factors with the theory of belief functions. We compare our method to the Fagin distance on a dataset from a real experiment, where users have to assess the quality of some audio recordings. Then, we propose to fuse both the Fagin distance and our expertise measure. △ Less","24 June, 2019",https://arxiv.org/pdf/1907.10588
A system of different layers of abstraction for artificial intelligence,Alexander Serb;Themistoklis Prodromakis,"The field of artificial intelligence (AI) represents an enormous endeavour of humankind that is currently transforming our societies down to their very foundations. Its task, building truly intelligent systems, is underpinned by a vast array of subfields ranging from the development of new electronic components to mathematical formulations of highly abstract and complex reasoning. This breadth of subfields renders it often difficult to understand how they all fit together into a bigger picture and hides the multi-faceted, multi-layered conceptual structure that in a sense can be said to be what AI truly is. In this perspective we propose a system of five levels/layers of abstraction that underpin many AI implementations. We further posit that each layer is subject to a complexity-performance trade-off whilst different layers are interlocked with one another in a control-complexity trade-off. This overview provides a conceptual map that can help to identify how and where innovation should be targeted in order to achieve different levels of functionality, assure them for safety, optimise performance under various operating constraints and map the opportunity space for social and economic exploitation. △ Less","22 July, 2019",https://arxiv.org/pdf/1907.10508
Live Forensics for Distributed Storage Systems,Saurabh Jha;Shengkun Cui;Tianyin Xu;Jeremy Enos;Mike Showerman;Mark Dalton;Zbigniew T. Kalbarczyk;William T. Kramer;Ravishankar K. Iyer,"We present Kaleidoscope an innovative system that supports live forensics for application performance problems caused by either individual component failures or resource contention issues in large-scale distributed storage systems. The design of Kaleidoscope is driven by our study of I/O failures observed in a peta-scale storage system anonymized as PetaStore. Kaleidoscope is built on three key features: 1) using temporal and spatial differential observability for end-to-end performance monitoring of I/O requests, 2) modeling the health of storage components as a stochastic process using domain-guided functions that accounts for path redundancy and uncertainty in measurements, and, 3) observing differences in reliability and performance metrics between similar types of healthy and unhealthy components to attribute the most likely root causes. We deployed Kaleidoscope on PetaStore and our evaluation shows that Kaleidoscope can run live forensics at 5-minute intervals and pinpoint the root causes of 95.8% of real-world performance issues, with negligible monitoring overhead. △ Less","23 July, 2019",https://arxiv.org/pdf/1907.10203
Convolutional Dictionary Learning in Hierarchical Networks,Javier Zazo;Bahareh Tolooshams;Demba Ba,"Filter banks are a popular tool for the analysis of piecewise smooth signals such as natural images. Motivated by the empirically observed properties of scale and detail coefficients of images in the wavelet domain, we propose a hierarchical deep generative model of piecewise smooth signals that is a recursion across scales: the low pass scale coefficients at one layer are obtained by filtering the scale coefficients at the next layer, and adding a high pass detail innovation obtained by filtering a sparse vector. This recursion describes a linear dynamic system that is a non-Gaussian Markov process across scales and is closely related to multilayer-convolutional sparse coding (ML-CSC) generative model for deep networks, except that our model allows for deeper architectures, and combines sparse and non-sparse signal representations. We propose an alternating minimization algorithm for learning the filters in this hierarchical model given observations at layer zero, e.g., natural images. The algorithm alternates between a coefficient-estimation step and a filter update step. The coefficient update step performs sparse (detail) and smooth (scale) coding and, when unfolded, leads to a deep neural network. We use MNIST to demonstrate the representation capabilities of the model, and its derived features (coefficients) for classification. △ Less","23 July, 2019",https://arxiv.org/pdf/1907.09881
GA-DAN: Geometry-Aware Domain Adaptation Network for Scene Text Detection and Recognition,Fangneng Zhan;Chuhui Xue;Shijian Lu,"Recent adversarial learning research has achieved very impressive progress for modelling cross-domain data shifts in appearance space but its counterpart in modelling cross-domain shifts in geometry space lags far behind. This paper presents an innovative Geometry-Aware Domain Adaptation Network (GA-DAN) that is capable of modelling cross-domain shifts concurrently in both geometry space and appearance space and realistically converting images across domains with very different characteristics. In the proposed GA-DAN, a novel multi-modal spatial learning technique is designed which converts a source-domain image into multiple images of different spatial views as in the target domain. A new disentangled cycle-consistency loss is introduced which balances the cycle consistency in appearance and geometry spaces and improves the learning of the whole network greatly. The proposed GA-DAN has been evaluated for the classic scene text detection and recognition tasks, and experiments show that the domain-adapted images achieve superior scene text detection and recognition performance while applied to network training. △ Less","22 July, 2019",https://arxiv.org/pdf/1907.09653
Distributed Global Optimization by Annealing,Brian Swenson;Soummya Kar;H. Vincent Poor;José M. F. Moura,"The paper considers a distributed algorithm for global minimization of a nonconvex function. The algorithm is a first-order consensus + innovations type algorithm that incorporates decaying additive Gaussian noise for annealing, converging to the set of global minima under certain technical assumptions. The paper presents simple methods for verifying that the required technical assumptions hold and illustrates it with a distributed target-localization application. △ Less","20 July, 2019",https://arxiv.org/pdf/1907.08802
Risks and Assets: A Qualitative Study of a Software Ecosystem in the Mining Industry,Thomas Olsson;Ulrik Franke,"Digitalization and servitization are impacting many domains, including the mining industry. As the equipment becomes connected and technical infrastructure evolves, business models and risk management need to adapt. In this paper, we present a study on how changes in asset and risk distribution are evolving for the actors in a software ecosystem (SECO) and system-of-systems (SoS) around a mining operation. We have performed a survey to understand how Service Level Agreements (SLAs) -- a common mechanism for managing risk -- are used in other domains. Furthermore, we have performed a focus group study with companies. There is an overall trend in the mining industry to move the investment cost (CAPEX) from the mining operator to the vendors. Hence, the mining operator instead leases the equipment (as operational expense, OPEX) or even acquires a service. This change in business model impacts operation, as knowledge is moved from the mining operator to the suppliers. Furthermore, as the infrastructure becomes more complex, this implies that the mining operator is more and more reliant on the suppliers for the operation and maintenance. As this change is still in an early stage, there is no formalized risk management, e.g. through SLAs, in place. Rather, at present, the companies in the ecosystem rely more on trust and the incentives created by the promise of mutual future benefits of innovation activities. We believe there is a need to better understand how to manage risk in SECO as it is established and evolves. At the same time, in a SECO, the focus is on cooperation and innovation, the companies do not have incentives to address this unless there is an incident. Therefore, industry need, we believe, help in systematically understanding risk and defining quality aspects such as reliability and performance in the new business environment. △ Less","19 July, 2019",https://arxiv.org/pdf/1907.08412
Responsibility Analysis by Abstract Interpretation,Chaoqiang Deng;Patrick Cousot,"Given a behavior of interest in the program, statically determining the corresponding responsible entity is a task of critical importance, especially in program security. Classical static analysis techniques (e.g. dependency analysis, taint analysis, slicing, etc.) assist programmers in narrowing down the scope of responsibility, but none of them can explicitly identify the responsible entity. Meanwhile, the causality analysis is generally not pertinent for analyzing programs, and the structural equations model (SEM) of actual causality misses some information inherent in programs, making its analysis on programs imprecise. In this paper, a novel definition of responsibility based on the abstraction of event trace semantics is proposed, which can be applied in program security and other scientific fields. Briefly speaking, an entity ER is responsible for behavior B, if and only if ER is free to choose its input value, and such a choice is the first one that ensures the occurrence of B in the forthcoming execution. Compared to current analysis methods, the responsibility analysis is more precise. In addition, our definition of responsibility takes into account the cognizance of the observer, which, to the best of our knowledge, is a new innovative idea in program analysis. △ Less","18 July, 2019",https://arxiv.org/pdf/1907.08251
A Computer Vision Application for Assessing Facial Acne Severity from Selfie Images,Tingting Zhao;Hang Zhang;Jacob Spoelstra,"We worked with Nestle SHIELD (Skin Health, Innovation, Education, and Longevity Development, NSH) to develop a deep learning model that is able to assess acne severity from selfie images as accurate as dermatologists. The model was deployed as a mobile application, providing patients an easy way to assess and track the progress of their acne treatment. NSH acquired 4,700 selfie images for this study and recruited 11 internal dermatologists to label them in five categories: 1-Clear, 2- Almost Clear, 3-Mild, 4-Moderate, 5-Severe. Using OpenCV to detect facial landmarks we cut specific skin patches from the selfie images in order to minimize irrelevant background. We then applied a transfer learning approach by extracting features from the patches using a ResNet 152 pre-trained model, followed by a fully connected layer trained to approximate the desired severity rating. To address the problem of spatial sensitivity of CNN models, we introduce a new image rolling data augmentation approach, effectively causing acne lesions appeared in more locations in the training images. Our results demonstrate that this approach improved the generalization of the CNN model, outperforming more than half of the panel of human dermatologists on test images. To our knowledge, this is the first deep learning-based solution for acne assessment using selfie images. △ Less","31 July, 2019",https://arxiv.org/pdf/1907.07901
Towards a Multi-Chain Future of Proof-of-Space,Shuyang Tang;Jilai Zheng;Yao Deng;Ziyu Wang;Zhiqiang Liu;Dawu Gu,"Proof-of-Space provides an intriguing alternative for consensus protocol of permissionless blockchains due to its recyclable nature and the potential to support multiple chains simultaneously. However, a direct shared proof of the same storage, which was adopted in the existing multi-chain schemes based on Proof-of-Space, could give rise to newborn attack on new chain launching. To fix this gap, we propose an innovative framework of single-chain Proof-of-Space and further present a novel multi-chain scheme which can resist newborn attack effectively by elaborately combining shared proof and chain-specific proof of storage. Moreover, we analyze the security of the multi-chain scheme and prove that it is incentive-compatible. This means that participants in such multi-chain system can achieve their greatest utility with our proposed strategy of storage resource partition. △ Less","18 July, 2019",https://arxiv.org/pdf/1907.07896
"Towards Blockchain-based Multi-Agent Robotic Systems: Analysis, Classification and Applications",Ilya Afanasyev;Alexander Kolotov;Ruslan Rezin;Konstantin Danilov;Manuel Mazzara;Subham Chakraborty;Alexey Kashevnik;Andrey Chechulin;Aleksandr Kapitonov;Vladimir Jotsov;Andon Topalov;Nikola Shakev;Sevil Ahmed,"Decentralization, immutability and transparency make of Blockchain one of the most innovative technology of recent years. This paper presents an overview of solutions based on Blockchain technology for multi-agent robotic systems, and provide an analysis and classification of this emerging field. The reasons for implementing Blockchain in a multi-robot network may be to increase the interaction efficiency between agents by providing more trusted information exchange, reaching a consensus in trustless conditions, assessing robot productivity or detecting performance problems, identifying intruders, allocating plans and tasks, deploying distributed solutions and joint missions. Blockchain-based applications are discussed to demonstrate how distributed ledger can be used to extend the number of research platforms and libraries for multi-agent robotic systems. △ Less","17 July, 2019",https://arxiv.org/pdf/1907.07433
Single-bit-per-weight deep convolutional neural networks without batch-normalization layers for embedded systems,Mark D. McDonnell;Hesham Mostafa;Runchun Wang;Andre van Schaik,"Batch-normalization (BN) layers are thought to be an integrally important layer type in today's state-of-the-art deep convolutional neural networks for computer vision tasks such as classification and detection. However, BN layers introduce complexity and computational overheads that are highly undesirable for training and/or inference on low-power custom hardware implementations of real-time embedded vision systems such as UAVs, robots and Internet of Things (IoT) devices. They are also problematic when batch sizes need to be very small during training, and innovations such as residual connections introduced more recently than BN layers could potentially have lessened their impact. In this paper we aim to quantify the benefits BN layers offer in image classification networks, in comparison with alternative choices. In particular, we study networks that use shifted-ReLU layers instead of BN layers. We found, following experiments with wide residual networks applied to the ImageNet, CIFAR 10 and CIFAR 100 image classification datasets, that BN layers do not consistently offer a significant advantage. We found that the accuracy margin offered by BN layers depends on the data set, the network size, and the bit-depth of weights. We conclude that in situations where BN layers are undesirable due to speed, memory or complexity costs, that using shifted-ReLU layers instead should be considered; we found they can offer advantages in all these areas, and often do not impose a significant accuracy cost. △ Less","22 July, 2019",https://arxiv.org/pdf/1907.06916
Modeling User Selection in Quality Diversity,Alexander Hagg;Alexander Asteroth;Thomas Bäck,"The initial phase in real world engineering optimization and design is a process of discovery in which not all requirements can be made in advance, or are hard to formalize. Quality diversity algorithms, which produce a variety of high performing solutions, provide a unique chance to support engineers and designers in the search for what is possible and high performing. In this work we begin to answer the question how a user can interact with quality diversity and turn it into an interactive innovation aid. By modeling a user's selection it can be determined whether the optimization is drifting away from the user's preferences. The optimization is then constrained by adding a penalty to the objective function. We present an interactive quality diversity algorithm that can take into account the user's selection. The approach is evaluated in a new multimodal optimization benchmark that allows various optimization tasks to be performed. The user selection drift of the approach is compared to a state of the art alternative on both a planning and a neuroevolution control task, thereby showing its limits and possibilities. △ Less","16 July, 2019",https://arxiv.org/pdf/1907.06912
DOD-ETL: Distributed On-Demand ETL for Near Real-Time Business Intelligence,Gustavo V. Machado;Ítalo Cunha;Adriano C. M. Pereira;Leonardo B. Oliveira,"The competitive dynamics of the globalized market demand information on the internal and external reality of corporations. Information is a precious asset and is responsible for establishing key advantages to enable companies to maintain their leadership. However, reliable, rich information is no longer the only goal. The time frame to extract information from data determines its usefulness. This work proposes DOD-ETL, a tool that addresses, in an innovative manner, the main bottleneck in Business Intelligence solutions, the Extract Transform Load process (ETL), providing it in near real-time. DODETL achieves this by combining an on-demand data stream pipeline with a distributed, parallel and technology-independent architecture with in-memory caching and efficient data partitioning. We compared DOD-ETL with other Stream Processing frameworks used to perform near real-time ETL and found DOD-ETL executes workloads up to 10 times faster. We have deployed it in a large steelworks as a replacement for its previous ETL solution, enabling near real-time reports previously unavailable. △ Less","15 July, 2019",https://arxiv.org/pdf/1907.06723
Multi-scale Graph-based Grading for Alzheimer's Disease Prediction,Kilian Hett;Vinh-Thong Ta;José V. Manjón;Pierrick Coupé,"The prediction of subjects with mild cognitive impairment (MCI) who will progress to Alzheimer's disease (AD) is clinically relevant, and may above all have a significant impact on accelerate the development of new treatments. In this paper, we present a new MRI-based biomarker that enables us to predict conversion of MCI subjects to AD accurately. In order to better capture the AD signature, we introduce two main contributions. First, we present a new graph-based grading framework to combine inter-subject similarity features and intra-subject variability features. This framework involves patch-based grading of anatomical structures and graph-based modeling of structure alteration relationships. Second, we propose an innovative multiscale brain analysis to capture alterations caused by AD at different anatomical levels. Based on a cascade of classifiers, this multiscale approach enables the analysis of alterations of whole brain structures and hippocampus subfields at the same time. During our experiments using the ADNI-1 dataset, the proposed multiscale graph-based grading method obtained an area under the curve (AUC) of 81% to predict conversion of MCI subjects to AD within three years. Moreover, when combined with cognitive scores, the proposed method obtained 85% of AUC. These results are competitive in comparison to state-of-the-art methods evaluated on the same dataset. △ Less","15 July, 2019",https://arxiv.org/pdf/1907.06625
Out-of-core singular value decomposition,Vadim Demchik;Miroslav Bačák;Stefan Bordag,"Singular value decomposition (SVD) is a standard matrix factorization technique that produces optimal low-rank approximations of matrices. It has diverse applications, including machine learning, data science and signal processing. However, many common problems involve very large matrices that cannot fit in the main memory of commodity computers, making it impractical to use standard SVD algorithms that assume fast random access or large amounts of space for intermediate calculations. To address this issue, we have implemented an out-of-core (external memory) randomized SVD solution that is fully scalable and efficiently parallelizable. This solution factors both dense and sparse matrices of arbitrarily large size within arbitrarily small memory limits, efficiently using out-of-core storage as needed. It uses an innovative technique for partitioning matrices that lends itself to out-of-core and parallel processing, as well as memory and I/O use planning, automatic load balancing, performance tuning, and makes possible a number of other practical enhancements to the current state-of-the-art. Furthermore, by using persistent external storage (generally HDDs or SSDs), users can resume interrupted operations without having to recalculate previously performed steps, solving a major practical problem in factoring very large matrices. △ Less","15 July, 2019",https://arxiv.org/pdf/1907.06470
"Learning better generative models for dexterous, single-view grasping of novel objects",Marek Kopicki;Dominik Belter;Jeremy L. Wyatt,"This paper concerns the problem of how to learn to grasp dexterously, so as to be able to then grasp novel objects seen only from a single view-point. Recently, progress has been made in data-efficient learning of generative grasp models which transfer well to novel objects. These generative grasp models are learned from demonstration (LfD). One weakness is that, as this paper shall show, grasp transfer under challenging single view conditions is unreliable. Second, the number of generative model elements rises linearly in the number of training examples. This, in turn, limits the potential of these generative models for generalisation and continual improvement. In this paper, it is shown how to address these problems. Several technical contributions are made: (i) a view-based model of a grasp; (ii) a method for combining and compressing multiple grasp models; (iii) a new way of evaluating contacts that is used both to generate and to score grasps. These, together, improve both grasp performance and reduce the number of models learned for grasp transfer. These advances, in turn, also allow the introduction of autonomous training, in which the robot learns from self-generated grasps. Evaluation on a challenging test set shows that, with innovations (i)-(iii) deployed, grasp transfer success rises from 55.1% to 81.6%. By adding autonomous training this rises to 87.8%. These differences are statistically significant. In total, across all experiments, 539 test grasps were executed on real objects. △ Less","13 July, 2019",https://arxiv.org/pdf/1907.06053
Multi-Hop Wireless Optical Backhauling for LiFi Attocell Networks: Bandwidth Scheduling and Power Control,Hossein Kazemi;Majid Safari;Harald Haas,"The backhaul of hundreds of light fidelity (LiFi) base stations (BSs) constitutes a major challenge. Indoor wireless optical backhauling is a novel approach whereby the interconnections between adjacent LiFi BSs are provided by way of directed line-of-sight (LOS) wireless infrared (IR) links. Building on the aforesaid approach, this paper presents the top-down design of a multi-hop wireless backhaul configuration for multi-tier optical attocell networks by proposing the novel idea of super cells. Such cells incorporate multiple clusters of attocells that are connected to the core network via a single gateway based on multi-hop decode-and-forward (DF) relaying. Consequently, new challenges arise for managing the bandwidth and power resources of the bottleneck backhaul. By putting forward user-based bandwidth scheduling (UBS) and cell-based bandwidth scheduling (CBS) policies, the system-level modeling and analysis of the end-to-end multi-user sum rate is elaborated. In addition, optimal bandwidth scheduling under both UBS and CBS policies are formulated as constrained convex optimization problems, which are solved by using the projected subgradient method. Furthermore, the transmission power of the backhaul system is opportunistically reduced by way of an innovative fixed power control (FPC) strategy. The notion of backhaul bottleneck occurrence (BBO) is introduced. An accurate approximate expression of the probability of BBO is derived, and then verified using Monte Carlo simulations. Several insights are provided into the offered gains of the proposed schemes through extensive computer simulations, by studying different aspects of the performance of super cells including the average sum rate, the BBO probability and the backhaul power efficiency (PE). △ Less","12 July, 2019",https://arxiv.org/pdf/1907.05967
Coupled-Projection Residual Network for MRI Super-Resolution,Chun-Mei Feng;Kai Wang;Shijian Lu;Yong Xu;Heng Kong;Ling Shao,"Magnetic Resonance Imaging(MRI) has been widely used in clinical application and pathology research by helping doctors make more accurate diagnoses. On the other hand, accurate diagnosis by MRI remains a great challenge as images obtained via present MRI techniques usually have low resolutions. Improving MRI image quality and resolution thus becomes a critically important task. This paper presents an innovative Coupled-Projection Residual Network (CPRN) for MRI super-resolution. The CPRN consists of two complementary sub-networks: a shallow network and a deep network that keep the content consistency while learning high frequency differences between low-resolution and high-resolution images. The shallow sub-network employs coupled-projection for better retaining the MRI image details, where a novel feedback mechanism is introduced to guide the reconstruction of high-resolution images. The deep sub-network learns from the residuals of the high-frequency image information, where multiple residual blocks are cascaded to magnify the MRI images at the last network layer. Finally, the features from the shallow and deep sub-networks are fused for the reconstruction of high-resolution MRI images. For effective fusion of features from the deep and shallow sub-networks, a step-wise connection (CPRN S) is designed as inspired by the human cognitive processes (from simple to complex). Experiments over three public MRI datasets show that our proposed CPRN achieves superior MRI super-resolution performance as compared with the state-of-the-art. Our source code will be publicly available at http://www.yongxu.org/lunwen.html. △ Less","12 July, 2019",https://arxiv.org/pdf/1907.05598
Image Super-Resolution Using Attention Based DenseNet with Residual Deconvolution,Zhuangzi Li,"Image super-resolution is a challenging task and has attracted increasing attention in research and industrial communities. In this paper, we propose a novel end-to-end Attention-based DenseNet with Residual Deconvolution named as ADRD. In our ADRD, a weighted dense block, in which the current layer receives weighted features from all previous levels, is proposed to capture valuable features rely in dense layers adaptively. And a novel spatial attention module is presented to generate a group of attentive maps for emphasizing informative regions. In addition, we design an innovative strategy to upsample residual information via the deconvolution layer, so that the high-frequency details can be accurately upsampled. Extensive experiments conducted on publicly available datasets demonstrate the promising performance of the proposed ADRD against the state-of-the-arts, both quantitatively and qualitatively. △ Less","3 July, 2019",https://arxiv.org/pdf/1907.05282
Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences,Mathias Kraus;Stefan Feuerriegel,"Predicting the remaining useful life of machinery, infrastructure, or other equipment can facilitate preemptive maintenance decisions, whereby a failure is prevented through timely repair or replacement. This allows for a better decision support by considering the anticipated time-to-failure and thus promises to reduce costs. Here a common baseline may be derived by fitting a probability density function to past lifetimes and then utilizing the (conditional) expected remaining useful life as a prognostic. This approach finds widespread use in practice because of its high explanatory power. A more accurate alternative is promised by machine learning, where forecasts incorporate deterioration processes and environmental variables through sensor data. However, machine learning largely functions as a black-box method and its forecasts thus forfeit most of the desired interpretability. As our primary contribution, we propose a structured-effect neural network for predicting the remaining useful life which combines the favorable properties of both approaches: its key innovation is that it offers both a high accountability and the flexibility of deep learning. The parameters are estimated via variational Bayesian inferences. The different approaches are compared based on the actual time-to-failure for aircraft engines. This demonstrates the performance and superior interpretability of our method, while we finally discuss implications for decision support. △ Less","19 July, 2019",https://arxiv.org/pdf/1907.05146
Terahertz Band: The Last Piece of RF Spectrum Puzzle for Communication Systems,Hadeel Elayan;Osama Amin;Basem Shihada;Raed M. Shubair;Mohamed-Slim Alouini,"Ultra-high bandwidth, negligible latency and seamless communication for devices and applications are envisioned as major milestones that will revolutionize the way by which societies create, distribute and consume information. The remarkable expansion of wireless data traffic that we are witnessing recently has advocated the investigation of suitable regimes in the radio spectrum to satisfy users' escalating requirements and allow the development and exploitation of both massive capacity and massive connectivity of heterogeneous infrastructures. To this end, the Terahertz (THz) frequency band (0.1-10 THz) has received noticeable attention in the research community as an ideal choice for scenarios involving high-speed transmission. Particularly, with the evolution of technologies and devices, advancements in THz communication is bridging the gap between the millimeter wave (mmW) and optical frequency ranges. Moreover, the IEEE 802.15 suite of standards has been issued to shape regulatory frameworks that will enable innovation and provide a complete solution that crosses between wired and wireless boundaries at 100 Gbps. Nonetheless, despite the expediting progress witnessed in THz wireless research, the THz band is still considered one of the least probed frequency bands. As such, in this work, we present an up-to-date review paper to analyze the fundamental elements and mechanisms associated with the THz system architecture. △ Less","11 July, 2019",https://arxiv.org/pdf/1907.05043
"A Theoretical Model For Artificial Learning, Memory Management And Decision Making System",Ravin Kumar,"Human beings are considered as the most intelligent species on Earth. The ability to think, to create, to innovate, are the key elements which make humans superior over other existing species on Earth. Machines lack all those elements, although machines are faster than human in aspects like computing, equating etc. But humans are still more valuable than machines, due to all those previously discussed elements. Various models have been developed in last few years to create models that can think like human beings, but are not completely successful. This paper presents a new theoretical system for learning, memory management and decision making that can be used to develop highly complex systems, and shows the potential to be used for development of systems that can be used to provide the essential features to the machines to act like human beings. △ Less","1 July, 2019",https://arxiv.org/pdf/1907.04698
Let's measure run time! Extending the IR replicability infrastructure to include performance aspects,Sebastian Hofstätter;Allan Hanbury,"Establishing a docker-based replicability infrastructure offers the community a great opportunity: measuring the run time of information retrieval systems. The time required to present query results to a user is paramount to the users satisfaction. Recent advances in neural IR re-ranking models put the issue of query latency at the forefront. They bring a complex trade-off between performance and effectiveness based on a myriad of factors: the choice of encoding model, network architecture, hardware acceleration and many others. The best performing models (currently using the BERT transformer model) run orders of magnitude more slowly than simpler architectures. We aim to broaden the focus of the neural IR community to include performance considerations -- to sustain the practical applicability of our innovations. In this position paper we supply our argument with a case study exploring the performance of different neural re-ranking models. Finally, we propose to extend the OSIRRC docker-based replicability infrastructure with two performance focused benchmark scenarios. △ Less","10 July, 2019",https://arxiv.org/pdf/1907.04614
Learning from History: Recreating and Repurposing Sister Harriet Padberg's Computer Composed Canon and Free Fugue,Richard Savery;Benjamin Genchel;Jason Smith;Anthony Caulkins;Molly Jones;Anna Savery,"Harriet Padberg wrote Computer-Composed Canon and Free Fugue as part of her 1964 dissertation in Mathematics and Music at Saint Louis University. This program is one of the earliest examples of text-to-music software and algorithmic composition, which are areas of great interest in the present-day field of music technology. This paper aims to analyze the technological innovation, aesthetic design process, and impact of Harriet Padberg's original 1964 thesis as well as the design of a modern recreation and utilization, in order to gain insight to the nature of revisiting older works. Here, we present our open source recreation of Padberg's program with a modern interface and, through its use as an artistic tool by three composers, show how historical works can be effectively used for new creative purposes in contemporary contexts. Not Even One by Molly Jones draws on the historical and social significance of Harriet Padberg through using her program in a piece about the lack of representation of women judges in composition competitions. Brevity by Anna Savery utilizes the original software design as a composition tool, and The Padberg Piano by Anthony Caulkins uses the melodic generation of the original to create a software instrument. △ Less","9 July, 2019",https://arxiv.org/pdf/1907.04470
Melody Generation using an Interactive Evolutionary Algorithm,Majid Farzaneh;Rahil Mahdian Toroghi,"Music generation with the aid of computers has been recently grabbed the attention of many scientists in the area of artificial intelligence. Deep learning techniques have evolved sequence production methods for this purpose. Yet, a challenging problem is how to evaluate generated music by a machine. In this paper, a methodology has been developed based upon an interactive evolutionary optimization method, with which the scoring of the generated melodies is primarily performed by human expertise, during the training. This music quality scoring is modeled using a Bi-LSTM recurrent neural network. Moreover, the innovative generated melody through a Genetic algorithm will then be evaluated using this Bi-LSTM network. The results of this mechanism clearly show that the proposed method is able to create pleasurable melodies with desired styles and pieces. This method is also quite fast, compared to the state-of-the-art data-oriented evolutionary systems. △ Less","6 July, 2019",https://arxiv.org/pdf/1907.04258
Enabling Microsoft OneDrive Integration with HTCondor,Derek Weitzel,"Accessing data from distributed computing is essential in many workflows, but can be complicated for users of cyberinfrastructure. They must perform multiple steps to make data available to distributed computing using unfamiliar tools. Further, most research on data distribution has focused on the efficiency of providing data to computing resources rather than considering the ease of use for distributing data. Creating an easy to use data distribution method can reduce the time researchers spend learning cyberinfrastructure and increase its usefulness. Microsoft OneDrive is a online storage solution providing both file storage and sharing. OneDrive provides many different clients to access data stored in the service. It provides many features that users of cyberinfrastructure could find useful such as automatic synchronization with desktop clients. A barrier to using services such as OneDrive is the credential management necessary to access the service. Recent innovations in HTCondor have allowed the management of OAuth credentials to be handled by the scheduler on the user's behalf. The user no longer has to copy credentials along with the job, HTCondor will handle the acquisition, renewal, and secure transfer of credentials on the user's behalf. In this paper, I will focus on providing an easy to use data distribution method utilizing Microsoft OneDrive. Measuring ease of use is difficult, therefore I will will describe the features and advantages of using OneDrive. Additionally, I will compare it to measurements of data distribution methods currently used on a national cyberinfastructure, the Open Science Grid. △ Less","8 July, 2019",https://arxiv.org/pdf/1907.03688
Ensuring Responsible Outcomes from Technology,Aaditeshwar Seth,"We attempt to make two arguments in this essay. First, through a case study of a mobile phone based voice-media service we have been running in rural central India for more than six years, we describe several implementation complexities we had to navigate towards realizing our intended vision of bringing social development through technology. Most of these complexities arose in the interface of our technology with society, and we argue that even other technology providers can create similar processes to manage this socio-technological interface and ensure intended outcomes from their technology use. We then build our second argument about how to ensure that the organizations behind both market driven technologies and those technologies that are adopted by the state, pay due attention towards responsibly managing the socio-technological interface of their innovations. We advocate for the technology engineers and researchers who work within these organizations, to take up the responsibility and ensure that their labour leads to making the world a better place especially for the poor and marginalized. We outline possible governance structures that can give more voice to the technology developers to push their organizations towards ensuring that responsible outcomes emerge from their technology. We note that the examples we use to build our arguments are limited to contemporary information and communication technology (ICT) platforms used directly by end-users to share content with one another, and hence our argument may not generalize to other ICTs in a straightforward manner. △ Less","7 July, 2019",https://arxiv.org/pdf/1907.03263
Applying a Pre-trained Language Model to Spanish Twitter Humor Prediction,Bobak Farzin;Piotr Czapla;Jeremy Howard,"Our entry into the HAHA 2019 Challenge placed 3^{rd} in the classification task and 2^{nd} in the regression task. We describe our system and innovations, as well as comparing our results to a Naive Bayes baseline. A large Twitter based corpus allowed us to train a language model from scratch focused on Spanish and transfer that knowledge to our competition model. To overcome the inherent errors in some labels we reduce our class confidence with label smoothing in the loss function. All the code for our project is included in a GitHub repository for easy reference and to enable replication by others. △ Less","6 July, 2019",https://arxiv.org/pdf/1907.03187
AutoCompress: An Automatic DNN Structured Pruning Framework for Ultra-High Compression Rates,Ning Liu;Xiaolong Ma;Zhiyuan Xu;Yanzhi Wang;Jian Tang;Jieping Ye,"Structured weight pruning is a representative model compression technique of DNNs to reduce the storage and computation requirements and accelerate inference. An automatic hyperparameter determination process is necessary due to the large number of flexible hyperparameters. This work proposes AutoCompress, an automatic structured pruning framework with the following key performance improvements: (i) effectively incorporate the combination of structured pruning schemes in the automatic process; (ii) adopt the state-of-art ADMM-based structured weight pruning as the core algorithm, and propose an innovative additional purification step for further weight reduction without accuracy loss; and (iii) develop effective heuristic search method enhanced by experience-based guided search, replacing the prior deep reinforcement learning technique which has underlying incompatibility with the target pruning problem. Extensive experiments on CIFAR-10 and ImageNet datasets demonstrate that AutoCompress is the key to achieve ultra-high pruning rates on the number of weights and FLOPs that cannot be achieved before. As an example, AutoCompress outperforms the prior work on automatic model compression by up to 33x in pruning rate (120x reduction in the actual parameter count) under the same accuracy. Significant inference speedup has been observed from the AutoCompress framework on actual measurements on smartphone. We release all models of this work at anonymous link: http://bit.ly/2VZ63dS. △ Less","11 September, 2019",https://arxiv.org/pdf/1907.03141
TrustSAS: A Trustworthy Spectrum Access System for the 3.5 GHz CBRS Band,Mohamed Grissa;Attila A. Yavuz;Bechir Hamdaoui,"As part of its ongoing efforts to meet the increased spectrum demand, the Federal Communications Commission (FCC) has recently opened up 150 MHz in the 3.5 GHz band for shared wireless broadband use. Access and operations in this band, aka Citizens Broadband Radio Service (CBRS), will be managed by a dynamic spectrum access system (SAS) to enable seamless spectrum sharing between secondary users (SUs) and incumbent users. Despite its benefits, SAS's design requirements, as set by FCC, present privacy risks to SUs, merely because SUs are required to share sensitive operational information (e.g., location, identity, spectrum usage) with SAS to be able to learn about spectrum availability in their vicinity. In this paper, we propose TrustSAS , a trustworthy framework for SAS that synergizes state-of-the-art cryptographic techniques with blockchain technology in an innovative way to address these privacy issues while complying with FCC's regulatory design requirements. We analyze the security of our framework and evaluate its performance through analysis, simulation and experimentation. We show that TrustSAS can offer high security guarantees with reasonable overhead, making it an ideal solution for addressing SUs' privacy issues in an operational SAS environment. △ Less","6 July, 2019",https://arxiv.org/pdf/1907.03136
A Survey on Spatial Modulation in Emerging Wireless Systems: Research Progresses and Applications,Miaowen Wen;Beixiong Zheng;Kyeong Jin Kim;Marco Di Renzo;Theodoros A. Tsiftsis;Kwang-Cheng Chen;Naofal Al-Dhahir,"Spatial modulation (SM) is an innovative and promising digital modulation technology that strikes an appealing trade-off between spectral efficiency and energy efficiency with a simple design philosophy. SM enjoys plenty of benefits and shows great potential to fulfill the requirements of future wireless communications. The key idea behind SM is to convey additional information typically through the ON/OFF states of transmit antennas and simultaneously save the implementation cost by reducing the number of radio frequency chains. As a result, the SM concept can have widespread effects on diverse applications and can be applied in other signal domains such as frequency/time/code/angle domain or even across multiple domains. This survey provides a comprehensive overview of the latest results and progresses in SM research. Specifically, the fundamental principles, variants of system design, and enhancements of SM are described in detail. Furthermore, the integration of the SM family with other promising techniques, applications to emerging communication systems, and extensions to new signal domains are also extensively studied. △ Less","3 July, 2019",https://arxiv.org/pdf/1907.02941
Spine-Inspired Continuum Soft Exoskeleton for Stoop Lifting Assistance,Xiaolong Yang;Tzu-Hao Huang;Hang Hu;Shuangyue Yu;Sainan Zhang;Xianlian Zhou;Alessandra Carriero;Guang Yue;Hao Su,"Back injuries are the most prevalent work-related musculoskeletal disorders and represent a major cause of disability. Although innovations in wearable robots aim to alleviate this hazard, the majority of existing exoskeletons are obtrusive because the rigid linkage design limits natural movement, thus causing ergonomic risk. Moreover, these existing systems are typically only suitable for one type of movement assistance, not ubiquitous for a wide variety of activities. To fill in this gap, this paper presents a new wearable robot design approach continuum soft exoskeleton. This spine-inspired wearable robot is unobtrusive and assists both squat and stoops while not impeding walking motion. To tackle the challenge of the unique anatomy of spine that is inappropriate to be simplified as a single degree of freedom joint, our robot is conformal to human anatomy and it can reduce multiple types of forces along the human spine such as the spinae muscle force, shear, and compression force of the lumbar vertebrae. We derived kinematics and kinetics models of this mechanism and established an analytical biomechanics model of human-robot interaction. Quantitative analysis of disc compression force, disc shear force and muscle force was performed in simulation. We further developed a virtual impedance control strategy to deliver force control and compensate hysteresis of Bowden cable transmission. The feasibility of the prototype was experimentally tested on three healthy subjects. The root mean square error of force tracking is 6.63 N (3.3 % of the 200N peak force) and it demonstrated that it can actively control the stiffness to the desired value. This continuum soft exoskeleton represents a feasible solution with the potential to reduce back pain for multiple activities and multiple forces along the human spine. △ Less","4 July, 2019",https://arxiv.org/pdf/1907.02562
Generative Models for Automatic Chemical Design,Daniel Schwalbe-Koda;Rafael Gómez-Bombarelli,"Materials discovery is decisive for tackling urgent challenges related to energy, the environment, health care and many others. In chemistry, conventional methodologies for innovation usually rely on expensive and incremental strategies to optimize properties from molecular structures. On the other hand, inverse approaches map properties to structures, thus expediting the design of novel useful compounds. In this chapter, we examine the way in which current deep generative models are addressing the inverse chemical discovery paradigm. We begin by revisiting early inverse design algorithms. Then, we introduce generative models for molecular systems and categorize them according to their architecture and molecular representation. Using this classification, we review the evolution and performance of important molecular generation schemes reported in the literature. Finally, we conclude highlighting the prospects and challenges of generative models as cutting edge tools in materials discovery. △ Less","2 July, 2019",https://arxiv.org/pdf/1907.01632
An innovative adaptive kriging approach for efficient binary classification of mechanical problems,Jan N. Fuhg;Amelie Fau,"Kriging is an efficient machine-learning tool, which allows to obtain an approximate response of an investigated phenomenon on the whole parametric space. Adaptive schemes provide a the ability to guide the experiment yielding new sample point positions to enrich the metamodel. Herein a novel adaptive scheme called Monte Carlo-intersite Voronoi (MiVor) is proposed to efficiently identify binary decision regions on the basis of a regression surrogate model. The performance of the innovative approach is tested for analytical functions as well as some mechanical problems and is furthermore compared to two regression-based adaptive schemes. For smooth problems, all three methods have comparable performances. For highly fluctuating response surface as encountered e.g. for dynamics or damage problems, the innovative MiVor algorithm performs very well and provides accurate binary classification with only a few observation points. △ Less","2 July, 2019",https://arxiv.org/pdf/1907.01490
Improved Circuit Design of Analog Joint Source Channel Coding for Low-power and Low-complexity Wireless Sensors,Xueyuan Zhao;Vidyasagar Sadhu;Anthony Yang;Dario Pompili,"To enable low-power and low-complexity wireless monitoring, an improved circuit design of Analog Joint Source Channel Coding (AJSCC) is proposed for wireless sensor nodes. This innovative design is based on Analog Divider Blocks (ADB) with tunable spacing between AJSCC levels. The ADB controls the switching between two types of Voltage Controlled Voltage Sources (VCVS). LTSpice simulations were performed to evaluate the performance of the circuit, and the power consumption and circuit complexity of this new ADB-based design were compared with our previous parallel-VCVS design. It is found that this improved circuit design based on ADB outperforms the design based on parallel VCVS for a large number of AJSCC levels (>= 16), both in terms of power consumption as well as circuit complexity, thus enabling persistent and higher temporal/spatial resolution environmental sensing. △ Less","29 June, 2019",https://arxiv.org/pdf/1907.01442
Enumeration of Preferred Extensions in Almost Oriented Digraphs,Serge Gaspers;Ray Li,"In this paper, we present enumeration algorithms to list all preferred extensions of an argumentation framework. This task is equivalent to enumerating all maximal semikernels of a directed graph. For directed graphs on n vertices, all preferred extensions can be enumerated in O^*(3^{n/3}) time and there are directed graphs with Ω(3^{n/3}) preferred extensions. We give faster enumeration algorithms for directed graphs with at most 0.8004\cdot n vertices occurring in 2-cycles. In particular, for oriented graphs (digraphs with no 2-cycles) one of our algorithms runs in time O(1.2321^n), and we show that there are oriented graphs with Ω(3^{n/6}) > Ω(1.2009^n) preferred extensions. A combination of three algorithms leads to the fastest enumeration times for various proportions of the number of vertices in 2-cycles. The most innovative one is a new 2-stage sampling algorithm, combined with a new parameterized enumeration algorithm, analyzed with a combination of the recent monotone local search technique (STOC 2016) and an extension thereof (ICALP 2017). △ Less","1 July, 2019",https://arxiv.org/pdf/1907.01006
Topic Modeling the Reading and Writing Behavior of Information Foragers,Jaimie Murdock,"The general problem of ""information foraging"" in an environment about which agents have incomplete information has been explored in many fields, including cognitive psychology, neuroscience, economics, finance, ecology, and computer science. In all of these areas, the searcher aims to enhance future performance by surveying enough of existing knowledge to orient themselves in the information space. Individuals can be viewed as conducting a cognitive search in which they must balance exploration of ideas that are novel to them against exploitation of knowledge in domains in which they are already expert. In this dissertation, I present several case studies that demonstrate how reading and writing behaviors interact to construct personal knowledge bases. These studies use LDA topic modeling to represent the information environment of the texts each author read and wrote. Three studies revolve around Charles Darwin. Darwin left detailed records of every book he read for 23 years, from disembarking from the H.M.S. Beagle to just after publication of The Origin of Species. Additionally, he left copies of his drafts before publication. I characterize his reading behavior, then show how that reading behavior interacted with the drafts and subsequent revisions of The Origin of Species, and expand the dataset to include later readings and writings. Then, through a study of Thomas Jefferson's correspondence, I expand the study to non-book data. Finally, through an examination of neuroscience citation data, I move from individual behavior to collective behavior in constructing an information environment. Together, these studies reveal ""the interplay between individual and collective phenomena where innovation takes place"" (Tria et al. 2014). △ Less","30 June, 2019",https://arxiv.org/pdf/1907.00488
"Filter Early, Match Late: Improving Network-Based Visual Place Recognition",Stephen Hausler;Adam Jacobson;Michael Milford,"CNNs have excelled at performing place recognition over time, particularly when the neural network is optimized for localization in the current environmental conditions. In this paper we investigate the concept of feature map filtering, where, rather than using all the activations within a convolutional tensor, only the most useful activations are used. Since specific feature maps encode different visual features, the objective is to remove feature maps that are detract from the ability to recognize a location across appearance changes. Our key innovation is to filter the feature maps in an early convolutional layer, but then continue to run the network and extract a feature vector using a later layer in the same network. By filtering early visual features and extracting a feature vector from a higher, more viewpoint invariant later layer, we demonstrate improved condition and viewpoint invariance. Our approach requires image pairs for training from the deployment environment, but we show that state-of-the-art performance can regularly be achieved with as little as a single training image pair. An exhaustive experimental analysis is performed to determine the full scope of causality between early layer filtering and late layer extraction. For validity, we use three datasets: Oxford RobotCar, Nordland, and Gardens Point, achieving overall superior performance to NetVLAD. The work provides a number of new avenues for exploring CNN optimizations, without full re-training. △ Less","21 June, 2019",https://arxiv.org/pdf/1906.12176
SeF: A Secure Fountain Architecture for Slashing Storage Costs in Blockchains,Swanand Kadhe;Jichan Chung;Kannan Ramchandran,"Full nodes, which synchronize the entire blockchain history and independently validate all the blocks, form the backbone of any blockchain network by playing a vital role in ensuring security properties. On the other hand, a user running a full node needs to pay a heavy price in terms of storage costs. E.g., the Bitcoin blockchain size has grown over 215GB, in spite of its low throughput. The ledger size for a high throughput blockchain Ripple has already reached 9TB, and it is growing at an astonishing rate of 12GB per day! In this paper, we propose an architecture based on 'fountain codes', a class of erasure codes, that enables any full node to 'encode' validated blocks into a small number of 'coded blocks', thereby reducing its storage costs by orders of magnitude. In particular, our proposed ""Secure Fountain (SeF)"" architecture can achieve a near-optimal trade-off between the storage savings per node and the 'bootstrap cost' in terms of the number of (honest) storage-constrained nodes a new node needs to contact to recover the blockchain. A key technical innovation in SeF codes is to make fountain codes secure against adversarial nodes that can provide maliciously formed coded blocks. Our idea is to use the header-chain as a 'side-information' to check whether a coded block is maliciously formed while it is getting decoded. Further, the 'rateless property' of fountain codes helps in achieving high decentralization and scalability. Our experiments demonstrate that SeF codes tuned to achieve 1000x storage savings enable full nodes to encode the 191GB Bitcoin blockchain into 195MB on average. A new node can recover the blockchain from an arbitrary set of storage-constrained nodes as long as the set contains ~1100 honest nodes on average. Note that for a 1000x storage savings, the fundamental bound on the number of honest nodes to contact is 1000: we need about 10% more in practice. △ Less","28 June, 2019",https://arxiv.org/pdf/1906.12140
Bayesian Inference of Spacecraft Pose using Particle Filtering,Maxim Bazik;Brien Flewelling;Manoranjan Majji;Joseph Mundy,"Automated 3D pose estimation of satellites and other known space objects is a critical component of space situational awareness. Ground-based imagery offers a convenient data source for satellite characterization; however, analysis algorithms must contend with atmospheric distortion, variable lighting, and unknown reflectance properties. Traditional feature-based pose estimation approaches are unable to discover an accurate correlation between a known 3D model and imagery given this challenging image environment. This paper presents an innovative method for automated 3D pose estimation of known space objects in the absence of satisfactory texture. The proposed approach fits the silhouette of a known satellite model to ground-based imagery via particle filtering. Each particle contains enough information (orientation, position, scale, model articulation) to generate an accurate object silhouette. The silhouette of individual particles is compared to an observed image. Comparison is done probabilistically by calculating the joint probability that pixels inside the silhouette belong to the foreground distribution and that pixels outside the silhouette belong to the background distribution. Both foreground and background distributions are computed by observing empty space. The population of particles are resampled at each new image observation, with the probability of a particle being resampled proportional to how the particle's silhouette matches the observation image. The resampling process maintains multiple pose estimates which is beneficial in preventing and escaping local minimums. Experiments were conducted on both commercial imagery and on LEO satellite imagery. Imagery from the commercial experiments are shown in this paper. △ Less","26 June, 2019",https://arxiv.org/pdf/1906.11182
Blocking Mechanism of Porn Website in India: Claim and Truth,Saurabh Pandey;Harish Sharma,"In last few years, the addiction of internet is apparently recognized as the serious threat to the health of society. This internet addiction gives an impetus to pornographic addiction because most of the pornographic content is accessible through internet. There have been ethical concerns on blocking the contents over internet. In India Uttarakhand High court has taken initiative for the blocking of pornographic content over internet. Technocrats are coming up with various innovative mechanisms to block the content over internet with various techniques, although long ago in 2015. The Supreme Court of India has already asked to block some of the websites but it could not be materialized. The focus of this research paper is to review the effectiveness of blocking existing web content blocking mechanism of pornographic websites in Indian context. △ Less","25 June, 2019",https://arxiv.org/pdf/1906.10379
Peril v. Promise: IoT and the Ethical Imaginaries,Funda Ustek-Spilda;Alison Powell;Irina Shklovski;Sebastian Lehuede,"The future scenarios often associated with Internet of Things (IoT) oscillate between the peril of IoT for the future of humanity and the promises for an ever-connected and efficient future. Such a dichotomous positioning creates problems not only for expanding the field of application of the technology, but also ensuring ethical and responsible design and production. As part of VirtEU (Values and Ethics in Innovation for Responsible Technology in Europe) (EU Horizon 2020 FP7), we have conducted ethnographic research into the main hubs of IoT in Europe, such as London, Amsterdam, Barcelona and Belgrade, with developers and designers of IoT to identify the challenges they face in their day-to-day work. In this paper, we focus on the IoT and the ethical imaginaries explore the practical challenges IoT developers face when they are designing, producing and marketing IoT technologies. We argue that top-down ethical frameworks that overlook the situated capabilities of developers or the solutionist approaches that treat ethical issues as technical problems are unlikely to provide an alternative to the dichotomous imaginary for the future. △ Less","25 June, 2019",https://arxiv.org/pdf/1906.10378
"Internet of Autonomous Vehicles: Architecture, Features, and Socio-Technological Challenges",Furqan Jameel;Zheng Chang;Jun Huang;Tapani Ristaniemi,"Mobility is the backbone of urban life and a vital economic factor in the development of the world. Rapid urbanization and the growth of mega-cities is bringing dramatic changes in the capabilities of vehicles. Innovative solutions like autonomy, electrification, and connectivity are on the horizon. How, then, we can provide ubiquitous connectivity to the legacy and autonomous vehicles? This paper seeks to answer this question by combining recent leaps of innovation in network virtualization with remarkable feats of wireless communications. To do so, this paper proposes a novel paradigm called the Internet of autonomous vehicles (IoAV). We begin painting the picture of IoAV by discussing the salient features, and applications of IoAV which is followed by a detailed discussion on the key enabling technologies. Next, we describe the proposed layered architecture of IoAV and uncover some critical functions of each layer. This is followed by the performance evaluation of IoAV which shows the significant advantage of the proposed architecture in terms of transmission time and energy consumption. Finally, to best capture the benefits of IoAV, we enumerate some social and technological challenges and explain how some unresolved issues can disrupt the widespread use of autonomous vehicles in the future. △ Less","24 June, 2019",https://arxiv.org/pdf/1906.09918
Business Taxonomy Construction Using Concept-Level Hierarchical Clustering,Haodong Bai;Frank Z. Xing;Erik Cambria;Win-Bin Huang,"Business taxonomies are indispensable tools for investors to do equity research and make professional decisions. However, to identify the structure of industry sectors in an emerging market is challenging for two reasons. First, existing taxonomies are designed for mature markets, which may not be the appropriate classification for small companies with innovative business models. Second, emerging markets are fast-developing, thus the static business taxonomies cannot promptly reflect the new features. In this article, we propose a new method to construct business taxonomies automatically from the content of corporate annual reports. Extracted concepts are hierarchically clustered using greedy affinity propagation. Our method requires less supervision and is able to discover new terms. Experiments and evaluation on the Chinese National Equities Exchange and Quotations (NEEQ) market show several advantages of the business taxonomy we build. Our results provide an effective tool for understanding and investing in the new growth companies. △ Less","23 June, 2019",https://arxiv.org/pdf/1906.09694
Quality of Uncertainty Quantification for Bayesian Neural Network Inference,Jiayu Yao;Weiwei Pan;Soumya Ghosh;Finale Doshi-Velez,"Bayesian Neural Networks (BNNs) place priors over the parameters in a neural network. Inference in BNNs, however, is difficult; all inference methods for BNNs are approximate. In this work, we empirically compare the quality of predictive uncertainty estimates for 10 common inference methods on both regression and classification tasks. Our experiments demonstrate that commonly used metrics (e.g. test log-likelihood) can be misleading. Our experiments also indicate that inference innovations designed to capture structure in the posterior do not necessarily produce high quality posterior approximations. △ Less","23 June, 2019",https://arxiv.org/pdf/1906.09686
DAL: Dual Adversarial Learning for Dialogue Generation,Shaobo Cui;Rongzhong Lian;Di Jiang;Yuanfeng Song;Siqi Bao;Yong Jiang,"In open-domain dialogue systems, generative approaches have attracted much attention for response generation. However, existing methods are heavily plagued by generating safe responses and unnatural responses. To alleviate these two problems, we propose a novel framework named Dual Adversarial Learning (DAL) for high-quality response generation. DAL is the first work to innovatively utilizes the duality between query generation and response generation to avoid safe responses and increase the diversity of the generated responses. Additionally, DAL uses adversarial learning to mimic human judges and guides the system to generate natural responses. Experimental results demonstrate that DAL effectively improves both diversity and overall quality of the generated responses. DAL outperforms the state-of-the-art methods regarding automatic metrics and human evaluations. △ Less","23 June, 2019",https://arxiv.org/pdf/1906.09556
Retrieving Sequential Information for Non-Autoregressive Neural Machine Translation,Chenze Shao;Yang Feng;Jinchao Zhang;Fandong Meng;Xilin Chen;Jie Zhou,"Non-Autoregressive Transformer (NAT) aims to accelerate the Transformer model through discarding the autoregressive mechanism and generating target words independently, which fails to exploit the target sequential information. Over-translation and under-translation errors often occur for the above reason, especially in the long sentence translation scenario. In this paper, we propose two approaches to retrieve the target sequential information for NAT to enhance its translation ability while preserving the fast-decoding property. Firstly, we propose a sequence-level training method based on a novel reinforcement algorithm for NAT (Reinforce-NAT) to reduce the variance and stabilize the training procedure. Secondly, we propose an innovative Transformer decoder named FS-decoder to fuse the target sequential information into the top layer of the decoder. Experimental results on three translation tasks show that the Reinforce-NAT surpasses the baseline NAT system by a significant margin on BLEU without decelerating the decoding speed and the FS-decoder achieves comparable translation performance to the autoregressive Transformer with considerable speedup. △ Less","22 June, 2019",https://arxiv.org/pdf/1906.09444
Deep Learning in the Automotive Industry: Recent Advances and Application Examples,Kanwar Bharat Singh;Mustafa Ali Arat,"One of the most exciting technology breakthroughs in the last few years has been the rise of deep learning. State-of-the-art deep learning models are being widely deployed in academia and industry, across a variety of areas, from image analysis to natural language processing. These models have grown from fledgling research subjects to mature techniques in real-world use. The increasing scale of data, computational power and the associated algorithmic innovations are the main drivers for the progress we see in this field. These developments also have a huge potential for the automotive industry and therefore the interest in deep learning-based technology is growing. A lot of the product innovations, such as self-driving cars, parking and lane-change assist or safety functions, such as autonomous emergency braking, are powered by deep learning algorithms. Deep learning is poised to offer gains in performance and functionality for most ADAS (Advanced Driver Assistance System) solutions. Virtual sensing for vehicle dynamics application, vehicle inspection/heath monitoring, automated driving and data-driven product development are key areas that are expected to get the most attention. This article provides an overview of the recent advances and some associated challenges in deep learning techniques in the context of automotive applications. △ Less","24 June, 2019",https://arxiv.org/pdf/1906.08834
Reversible Privacy Preservation using Multi-level Encryption and Compressive Sensing,Mehmet Yamac;Mete Ahishali;Nikolaos Passalis;Jenni Raitoharju;Bulent Sankur;Moncef Gabbouj,"Security monitoring via ubiquitous cameras and their more extended in intelligent buildings stand to gain from advances in signal processing and machine learning. While these innovative and ground-breaking applications can be considered as a boon, at the same time they raise significant privacy concerns. In fact, recent GDPR (General Data Protection Regulation) legislation has highlighted and become an incentive for privacy-preserving solutions. Typical privacy-preserving video monitoring schemes address these concerns by either anonymizing the sensitive data. However, these approaches suffer from some limitations, since they are usually non-reversible, do not provide multiple levels of decryption and computationally costly. In this paper, we provide a novel privacy-preserving method, which is reversible, supports de-identification at multiple privacy levels, and can efficiently perform data acquisition, encryption and data hiding by combining multi-level encryption with compressive sensing. The effectiveness of the proposed approach in protecting the identity of the users has been validated using the goodness of reconstruction quality and strong anonymization of the faces. △ Less","20 June, 2019",https://arxiv.org/pdf/1906.08713
Holistic evaluation of XML queries with structural preferences on an annotated strong dataguide,Maurice Tchoupé Tchendji;Adolphe Gaius Nkuefone;Thomas Tébougang Tchendji,"With the emergence of XML as de facto format for storing and exchanging information over the Internet, the search for ever more innovative and effective techniques for their querying is a major and current concern of the XML database community. Several studies carried out to help solve this problem are mostly oriented towards the evaluation of so-called exact queries which, unfortunately, are likely (especially in the case of semi-structured documents) to yield abundant results (in the case of vague queries) or empty results (in the case of very precise queries). From the observation that users who make requests are not necessarily interested in all possible solutions, but rather in those that are closest to their needs, an important field of research has been opened on the evaluation of preferences queries. In this paper, we propose an approach for the evaluation of such queries, in case the preferences concern the structure of the document. The solution investigated revolves around the proposal of an evaluation plan in three phases: rewriting-evaluation-merge. The rewriting phase makes it possible to obtain, from a partitioning -transformation operation of the initial query, a hierarchical set of preferences path queries which are holistically evaluated in the second phase by an instrumented version of the algorithm TwigStack. The merge phase is the synthesis of the best results. △ Less","7 June, 2019",https://arxiv.org/pdf/1906.08231
Evaluating Protein Transfer Learning with TAPE,Roshan Rao;Nicholas Bhattacharya;Neil Thomas;Yan Duan;Xi Chen;John Canny;Pieter Abbeel;Yun S. Song,"Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape. △ Less","19 June, 2019",https://arxiv.org/pdf/1906.08230
Why and How zk-SNARK Works,Maksym Petkus,"Despite the existence of multiple great resources on zk-SNARK construction, from original papers to explainers, due to the sheer number of moving parts the subject remains a black box for many. While some pieces of the puzzle are given one can not see the full picture without the missing ones. Hence the focus of this work is to shed light onto the topic with a straightforward and clean approach based on examples and answering many whys along the way so that more individuals can appreciate the state of the art technology, its innovators and ultimately the beauty of math. Paper's contribution is a simplistic exposition with a sufficient and gradually increasing level of complexity, necessary to understand zk-SNARK without any prerequisite knowledge of the subject, cryptography or advanced math. The primary goal is not only to explain how it works but why it works and how it came to be this way. △ Less","17 June, 2019",https://arxiv.org/pdf/1906.07221
Deep Learning of Preconditioners for Conjugate Gradient Solvers in Urban Water Related Problems,Johannes Sappl;Laurent Seiler;Matthias Harders;Wolfgang Rauch,"Solving systems of linear equations is a problem occuring frequently in water engineering applications. Usually the size of the problem is too large to be solved via direct factorization. One can resort to iterative approaches, in particular the conjugate gradients method if the matrix is symmetric positive definite. Preconditioners further enhance the rate of convergence but hitherto only handcrafted ones requiring expert knowledge have been used. We propose an innovative approach employing Machine Learning, in particular a Convolutional Neural Network, to unassistedly design preconditioning matrices specifically for the problem at hand. Based on an in-depth case study in fluid simulation we are able to show that our learned preconditioner is able to improve the convergence rate even beyond well established methods like incomplete Cholesky factorization or Algebraic MultiGrid. △ Less","17 June, 2019",https://arxiv.org/pdf/1906.06925
ParNet: Position-aware Aggregated Relation Network for Image-Text matching,Yaxian Xia;Lun Huang;Wenmin Wang;Xiaoyong Wei;Wenmin Wang,"Exploring fine-grained relationship between entities(e.g. objects in image or words in sentence) has great contribution to understand multimedia content precisely. Previous attention mechanism employed in image-text matching either takes multiple self attention steps to gather correspondences or uses image objects (or words) as context to infer image-text similarity. However, they only take advantage of semantic information without considering that objects' relative position also contributes to image understanding. To this end, we introduce a novel position-aware relation module to model both the semantic and spatial relationship simultaneously for image-text matching in this paper. Given an image, our method utilizes the location of different objects to capture spatial relationship innovatively. With the combination of semantic and spatial relationship, it's easier to understand the content of different modalities (images and sentences) and capture fine-grained latent correspondences of image-text pairs. Besides, we employ a two-step aggregated relation module to capture interpretable alignment of image-text pairs. The first step, we call it intra-modal relation mechanism, in which we computes responses between different objects in an image or different words in a sentence separately; The second step, we call it inter-modal relation mechanism, in which the query plays a role of textual context to refine the relationship among object proposals in an image. In this way, our position-aware aggregated relation network (ParNet) not only knows which entities are relevant by attending on different objects (words) adaptively, but also adjust the inter-modal correspondence according to the latent alignments according to query's content. Our approach achieves the state-of-the-art results on MS-COCO dataset. △ Less","17 June, 2019",https://arxiv.org/pdf/1906.06892
Neural Decipherment via Minimum-Cost Flow: from Ugaritic to Linear B,Jiaming Luo;Yuan Cao;Regina Barzilay,"In this paper we propose a novel neural approach for automatic decipherment of lost languages. To compensate for the lack of strong supervision signal, our model design is informed by patterns in language change documented in historical linguistics. The model utilizes an expressive sequence-to-sequence model to capture character-level correspondences between cognates. To effectively train the model in an unsupervised manner, we innovate the training procedure by formalizing it as a minimum-cost flow problem. When applied to the decipherment of Ugaritic, we achieve a 5.5% absolute improvement over state-of-the-art results. We also report the first automatic results in deciphering Linear B, a syllabic language related to ancient Greek, where our model correctly translates 67.3% of cognates. △ Less","16 June, 2019",https://arxiv.org/pdf/1906.06718
Assessment of Urban Ecological Service value used in Urban Rail Transit Project,Yijie Li;Jing Chen,"Ecosystem services refer to the ones human beings often obtain from the natural environment ecosystem. In order to solve the problem of environmental degradation, based on the Integrated Valuation of Ecosystem Services and Trade-offs (InVEST model), this paper makes innovation by adding the urban module that was not in the previous models, which can better deal with the evaluation of ecosystem services in urban scenarios. △ Less","15 June, 2019",https://arxiv.org/pdf/1906.06572
Fusion vectors: Embedding Graph Fusions for Efficient Unsupervised Rank Aggregation,Icaro Cavalcante Dourado;Ricardo da Silva Torres,"The vast increase in amount and complexity of digital content led to a wide interest in ad-hoc retrieval systems in recent years. Complementary, the existence of heterogeneous data sources and retrieval models stimulated the proliferation of increasingly ingenious and effective rank aggregation functions. Although recently proposed rank aggregation functions are promising with respect to effectiveness, existing proposals in the area usually overlook efficiency aspects. We propose an innovative rank aggregation function that is unsupervised, intrinsically multimodal, and targeted for fast retrieval and top effectiveness performance. We introduce the concepts of embedding and indexing of graph-based rank-aggregation representation models, and their application for search tasks. Embedding formulations are also proposed for graph-based rank representations. We introduce the concept of fusion vectors, a late-fusion representation of objects based on ranks, from which an intrinsically rank-aggregation retrieval model is defined. Next, we present an approach for fast retrieval based on fusion vectors, thus promoting an efficient rank aggregation system. Our method presents top effectiveness performance among state-of-the-art related work, while bringing novel aspects of multimodality and effectiveness. Consistent speedups are achieved against the recent baselines in all datasets considered. △ Less","1 July, 2019",https://arxiv.org/pdf/1906.06011
Topological Data Analysis for Arrhythmia Detection through Modular Neural Networks,Meryll Dindin;Yuhei Umeda;Frederic Chazal,"This paper presents an innovative and generic deep learning approach to monitor heart conditions from ECG signals.We focus our attention on both the detection and classification of abnormal heartbeats, known as arrhythmia. We strongly insist on generalization throughout the construction of a deep-learning model that turns out to be effective for new unseen patient. The novelty of our approach relies on the use of topological data analysis as basis of our multichannel architecture, to diminish the bias due to individual differences. We show that our structure reaches the performances of the state-of-the-art methods regarding arrhythmia detection and classification. △ Less","13 June, 2019",https://arxiv.org/pdf/1906.05795
Understanding artificial intelligence ethics and safety,David Leslie,"A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur. This guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability. △ Less","11 June, 2019",https://arxiv.org/pdf/1906.05684
Reinforcement Knowledge Graph Reasoning for Explainable Recommendation,Yikun Xian;Zuohui Fu;S. Muthukrishnan;Gerard de Melo;Yongfeng Zhang,"Recent advances in personalized recommendation have sparked great interest in the exploitation of rich structured information provided by knowledge graphs. Unlike most existing approaches that only focus on leveraging knowledge graphs for more accurate recommendation, we perform explicit reasoning with knowledge for decision making so that the recommendations are generated and supported by an interpretable causal inference procedure. To this end, we propose a method called Policy-Guided Path Reasoning (PGPR), which couples recommendation and interpretability by providing actual paths in a knowledge graph. Our contributions include four aspects. We first highlight the significance of incorporating knowledge graphs into recommendation to formally define and interpret the reasoning process. Second, we propose a reinforcement learning (RL) approach featuring an innovative soft reward strategy, user-conditional action pruning and a multi-hop scoring function. Third, we design a policy-guided graph search algorithm to efficiently and effectively sample reasoning paths for recommendation. Finally, we extensively evaluate our method on several large-scale real-world benchmark datasets, obtaining favorable results compared with state-of-the-art methods. △ Less","12 June, 2019",https://arxiv.org/pdf/1906.05237
Adaptive Optimal Control for Reference Tracking Independent of Exo-System Dynamics,Florian Köpf;Johannes Westermann;Michael Flad;Sören Hohmann,"Model-free control based on the idea of Reinforcement Learning is a promising approach that has recently gained extensive attention. However, Reinforcement-Learning-based control methods solely focus on the regulation problem or learn to track a reference that is generated by a time-invariant exo-system. In the latter case, controllers are only able to track the time-invariant reference dynamics which they have been trained on and need to be re-trained each time the reference dynamics change. Consequently, these methods fail in a number of applications which obviously rely on a trajectory not being generated by an exo-system. One prominent example is autonomous driving. This paper provides for the first time an adaptive optimal control method capable to track reference trajectories not being generated by a time-invariant exo-system. The main innovation is a novel Q-function that directly incorporates a given reference trajectory on a moving horizon. This new Q-function exhibits a particular structure which allows the design of an efficient, iterative, provably convergent Reinforcement Learning algorithm that enables optimal tracking. Two real-world examples demonstrate the effectiveness of our new method. △ Less","28 November, 2019",https://arxiv.org/pdf/1906.05085
Adaptive Neural Signal Detection for Massive MIMO,Mehrdad Khani;Mohammad Alizadeh;Jakob Hoydis;Phil Fleming,"Symbol detection for Massive Multiple-Input Multiple-Output (MIMO) is a challenging problem for which traditional algorithms are either impractical or suffer from performance limitations. Several recently proposed learning-based approaches achieve promising results on simple channel models (e.g., i.i.d. Gaussian). However, their performance degrades significantly on real-world channels with spatial correlation. We propose MMNet, a deep learning MIMO detection scheme that significantly outperforms existing approaches on realistic channels with the same or lower computational complexity. MMNet's design builds on the theory of iterative soft-thresholding algorithms and uses a novel training algorithm that leverages temporal and spectral correlation to accelerate training. Together, these innovations allow MMNet to train online for every realization of the channel. On i.i.d. Gaussian channels, MMNet requires two orders of magnitude fewer operations than existing deep learning schemes but achieves near-optimal performance. On spatially-correlated channels, it achieves the same error rate as the next-best learning scheme (OAMPNet) at 2.5dB lower SNR and with at least 10x less computational complexity. MMNet is also 4--8dB better overall than a classic linear scheme like the minimum mean square error (MMSE) detector. △ Less","11 June, 2019",https://arxiv.org/pdf/1906.04610
Qualifying threshold of take off stage for successfully disseminated creative ideas,Guoqiang Liang;Xiaodan Lou;Haiyan Hou;Zhigang Hu,"The creative process is essentially Darwinian and only a small proportion of creative ideas are selected for further development. However, the threshold that identifies this small fraction of successfully disseminated creative ideas at their early stage has not been thoroughly analyzed through the lens of Rogers innovation diffusion theory. Here, we take highly cited (top 1%) research papers as an example of the most successfully disseminated creative ideas and explore the time it takes and citations it receives at their take off stage, which play a crucial role in the dissemination of creativity. Results show the majority of highly cited papers will reach 10% and 25% of their total citations within two years and four years, respectively. Interestingly, our results also present a minimal number of articles that attract their first citation before publication. As for the discipline, number of references, and Price index, we find a significant difference exists: Clinical, Pre-Clinical & Health and Life Sciences are the first two disciplines to reach the C10% and C25% in a shorter amount of time. Highly cited papers with limited references usually take more time to reach 10% and 25% of their total citations. In addition, highly cited papers will attract citations rapidly when they cite more recent references. These results provide insights into the timespan and citations for a research paper to become highly cited at the take off stage in its diffusion process, as well as the factors that may influence it. △ Less","10 June, 2019",https://arxiv.org/pdf/1906.04206
Time-Series Anomaly Detection Service at Microsoft,Hansheng Ren;Bixiong Xu;Yujing Wang;Chao Yi;Congrui Huang;Xiaoyu Kou;Tony Xing;Mao Yang;Jie Tong;Qi Zhang,"Large companies need to monitor various metrics (for example, Page Views and Revenue) of their applications and services in real time. At Microsoft, we develop a time-series anomaly detection service which helps customers to monitor the time-series continuously and alert for potential incidents on time. In this paper, we introduce the pipeline and algorithm of our anomaly detection service, which is designed to be accurate, efficient and general. The pipeline consists of three major modules, including data ingestion, experimentation platform and online compute. To tackle the problem of time-series anomaly detection, we propose a novel algorithm based on Spectral Residual (SR) and Convolutional Neural Network (CNN). Our work is the first attempt to borrow the SR model from visual saliency detection domain to time-series anomaly detection. Moreover, we innovatively combine SR and CNN together to improve the performance of SR model. Our approach achieves superior experimental results compared with state-of-the-art baselines on both public datasets and Microsoft production data. △ Less","10 June, 2019",https://arxiv.org/pdf/1906.03821
An Artificial Intelligence-Based System for Nutrient Intake Assessment of Hospitalised Patients,Ya Lu;Thomai Stathopoulou;Maria F. Vasiloglou;Stergios Christodoulidis;Beat Blum;Thomas Walser;Vinzenz Meier;Zeno Stanga;Stavroula G. Mougiakakou,"Regular nutrient intake monitoring in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition (DRM). Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve the data accuracy and reduce both the participant burden and the health costs. In this paper, we propose a novel system based on artificial intelligence to accurately estimate nutrient intake, by simply processing RGB depth image pairs captured before and after a meal consumption. For the development and evaluation of the system, a dedicated and new database of images and recipes of 322 meals was assembled, coupled to data annotation using innovative strategies. With this database, a system was developed that employed a novel multi-task neural network and an algorithm for 3D surface construction. This allowed sequential semantic food segmentation and estimation of the volume of the consumed food, and permitted fully automatic estimation of nutrient intake for each food type with a 15% estimation error. △ Less","12 June, 2019",https://arxiv.org/pdf/1906.02990
Towards navigation without precise localization: Weakly supervised learning of goal-directed navigation cost map,Huifang Ma;Yue Wang;Li Tang;Sarath Kodagoda;Rong Xiong,"Autonomous navigation based on precise localization has been widely developed in both academic research and practical applications. The high demand for localization accuracy has been essential for safe robot planing and navigation while it makes the current geometric solutions less robust to environmental changes. Recent research on end-to-end methods handle raw sensory data with forms of navigation instructions and directly output the command for robot control. However, the lack of intermediate semantics makes the system more rigid and unstable for practical use. To explore these issues, this paper proposes an innovate navigation framework based on the GPS-level localization, which takes the raw perception data with publicly accessible navigation maps to produce an intermediate navigation cost map that allows subsequent flexible motion planning. A deterministic conditional adversarial network is adopted in our method to generate visual goal-directed paths under diverse navigation conditions. The adversarial loss avoids the pixel-level annotation and enables a weakly supervised training strategy to implicitly learn both of the traffic semantics in image perceptions and the planning intentions in navigation instructions. The navigation cost map is then rendered from the goal-directed path and the concurrently collected laser data, indicating the way towards the destination. Comprehensive experiments have been conducted with a real vehicle running in our campus and the results have verified the robustness to localization error of the proposed navigation system. △ Less","6 June, 2019",https://arxiv.org/pdf/1906.02468
SparseSense: Human Activity Recognition from Highly Sparse Sensor Data-streams Using Set-based Neural Networks,Alireza Abedin;S. Hamid Rezatofighi;Qinfeng Shi;Damith C. Ranasinghe,"Batteryless or so called passive wearables are providing new and innovative methods for human activity recognition (HAR), especially in healthcare applications for older people. Passive sensors are low cost, lightweight, unobtrusive and desirably disposable; attractive attributes for healthcare applications in hospitals and nursing homes. Despite the compelling propositions for sensing applications, the data streams from these sensors are characterised by high sparsity---the time intervals between sensor readings are irregular while the number of readings per unit time are often limited. In this paper, we rigorously explore the problem of learning activity recognition models from temporally sparse data. We describe how to learn directly from sparse data using a deep learning paradigm in an end-to-end manner. We demonstrate significant classification performance improvements on real-world passive sensor datasets from older people over the state-of-the-art deep learning human activity recognition models. Further, we provide insights into the model's behaviour through complementary experiments on a benchmark dataset and visualisation of the learned activity feature spaces. △ Less","5 June, 2019",https://arxiv.org/pdf/1906.02399
Finding Friend and Foe in Multi-Agent Games,Jack Serrino;Max Kleiman-Weiner;David C. Parkes;Joshua B. Tenenbaum,"Recent breakthroughs in AI for multi-agent games like Go, Poker, and Dota, have seen great strides in recent years. Yet none of these games address the real-life challenge of cooperation in the presence of unknown and uncertain teammates. This challenge is a key game mechanism in hidden role games. Here we develop the DeepRole algorithm, a multi-agent reinforcement learning agent that we test on The Resistance: Avalon, the most popular hidden role game. DeepRole combines counterfactual regret minimization (CFR) with deep value networks trained through self-play. Our algorithm integrates deductive reasoning into vector-form CFR to reason about joint beliefs and deduce partially observable actions. We augment deep value networks with constraints that yield interpretable representations of win probabilities. These innovations enable DeepRole to scale to the full Avalon game. Empirical game-theoretic methods show that DeepRole outperforms other hand-crafted and learned agents in five-player Avalon. DeepRole played with and against human players on the web in hybrid human-agent teams. We find that DeepRole outperforms human players as both a cooperator and a competitor. △ Less","5 June, 2019",https://arxiv.org/pdf/1906.02330
A GLCM Embedded CNN Strategy for Computer-aided Diagnosis in Intracerebral Hemorrhage,Yifan Hu;Yefeng Zheng,"Computer-aided diagnosis (CADx) systems have been shown to assist radiologists by providing classifications of all kinds of medical images like Computed tomography (CT) and Magnetic resonance (MR). Currently, convolutional neural networks play an important role in CADx. However, since CNN model should have a square-like input, it is usually difficult to directly apply the CNN algorithms on the irregular segmentation region of interests (ROIs) where the radiologists are interested in. In this paper, we propose a new approach to construct the model by extracting and converting the information of the irregular region into a fixed-size Gray-Level Co-Occurrence Matrix (GLCM) and then utilize the GLCM as one input of our CNN model. In this way, as an useful implementary to the original CNN, a couple of GLCM-based features are also extracted by CNN. Meanwhile, the network will pay more attention to the important lesion area and achieve a higher accuracy in classification. Experiments are performed on three classification databases: Hemorrhage, BraTS18 and Cervix to validate the universality of our innovative model. In conclusion, the proposed framework outperforms the corresponding state-of-art algorithms on each database with both test losses and classification accuracy as the evaluation criteria. △ Less","5 June, 2019",https://arxiv.org/pdf/1906.02040
TechNet: Technology Semantic Network Based on Patent Data,Serhad Sarica;Jianxi Luo;Kristin L. Wood,"The growing developments in general semantic networks, knowledge graphs and ontology databases have motivated us to build a large-scale comprehensive semantic network of technology-related data for engineering knowledge discovery, technology search and retrieval, and artificial intelligence for engineering design and innovation. Specially, we constructed a technology semantic network (TechNet) that covers the elemental concepts in all domains of technology and their semantic associations by mining the complete U.S. patent database from 1976. To derive the TechNet, natural language processing techniques were utilized to extract terms from massive patent texts and recent word embedding algorithms were employed to vectorize such terms and establish their semantic relationships. We report and evaluate the TechNet for retrieving terms and their pairwise relevance that is meaningful from a technology and engineering design perspective. The TechNet may serve as an infrastructure to support a wide range of applications, e.g., technical text summaries, search query predictions, relational knowledge discovery, and design ideation support, in the context of engineering and technology, and complement or enrich existing semantic databases. To enable such applications, the TechNet is made public via an online interface and APIs for public users to retrieve technology-related terms and their relevancies. △ Less","4 October, 2019",https://arxiv.org/pdf/1906.00411
Applying Generative Adversarial Networks to Intelligent Subsurface Imaging and Identification,William Rice,"To augment training data for machine learning models in Ground Penetrating Radar (GPR) data classification and identification, this thesis focuses on the generation of realistic GPR data using Generative Adversarial Networks. An innovative GAN architecture is proposed for generating GPR B-scans, which is, to the author's knowledge, the first successful application of GAN to GPR B-scans. As one of the major contributions, a novel loss function is formulated by merging frequency domain with time domain features. To test the efficacy of generated B-scans, a real time object classifier is proposed to measure the performance gain derived from augmented B-Scan images. The numerical experiment illustrated that, based on the augmented training data, the proposed GAN architecture demonstrated a significant increase (from 82% to 98%) in the accuracy of the object classifier. △ Less","30 May, 2019",https://arxiv.org/pdf/1905.13321
Reinforcement Learning and Adaptive Sampling for Optimized DNN Compilation,Byung Hoon Ahn;Prannoy Pilligundla;Hadi Esmaeilzadeh,"Achieving faster execution with shorter compilation time can enable further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently, simulated annealing and genetic algorithms. Our work takes a unique approach by formulating compiler optimizations for neural networks as a reinforcement learning problem, whose solution takes fewer steps to converge. This solution, dubbed ReLeASE, comes with a sampling algorithm that leverages clustering to focus the costly samples (real hardware measurements) on representative points, subsuming an entire subspace. Our adaptive sampling not only reduces the number of samples, but also improves the quality of samples for better exploration in shorter time. As such, experimentation with real hardware shows that reinforcement learning with adaptive sampling provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%. Further experiments also confirm that our adaptive sampling can even improve AutoTVM's simulated annealing by 4.00x. △ Less","29 May, 2019",https://arxiv.org/pdf/1905.12799
Predicting Sparse Clients' Actions with CPOPT-Net in the Banking Environment,Jeremy Charlier;Radu State;Jean Hilger,"The digital revolution of the banking system with evolving European regulations have pushed the major banking actors to innovate by a newly use of their clients' digital information. Given highly sparse client activities, we propose CPOPT-Net, an algorithm that combines the CP canonical tensor decomposition, a multidimensional matrix decomposition that factorizes a tensor as the sum of rank-one tensors, and neural networks. CPOPT-Net removes efficiently sparse information with a gradient-based resolution while relying on neural networks for time series predictions. Our experiments show that CPOPT-Net is capable to perform accurate predictions of the clients' actions in the context of personalized recommendation. CPOPT-Net is the first algorithm to use non-linear conjugate gradient tensor resolution with neural networks to propose predictions of financial activities on a public data set. △ Less","23 May, 2019",https://arxiv.org/pdf/1905.12568
Disentangling Monocular 3D Object Detection,Andrea Simonelli;Samuel Rota Rota Bulò;Lorenzo Porzi;Manuel López-Antequera;Peter Kontschieder,"In this paper we propose an approach for monocular 3D object detection from a single RGB image, which leverages a novel disentangling transformation for 2D and 3D detection losses and a novel, self-supervised confidence score for 3D bounding boxes. Our proposed loss disentanglement has the twofold advantage of simplifying the training dynamics in the presence of losses with complex interactions of parameters, and sidestepping the issue of balancing independent regression terms. Our solution overcomes these issues by isolating the contribution made by groups of parameters to a given loss, without changing its nature. We further apply loss disentanglement to another novel, signed Intersection-over-Union criterion-driven loss for improving 2D detection results. Besides our methodological innovations, we critically review the AP metric used in KITTI3D, which emerged as the most important dataset for comparing 3D detection results. We identify and resolve a flaw in the 11-point interpolated AP metric, affecting all previously published detection results and particularly biases the results of monocular 3D detection. We provide extensive experimental evaluations and ablation studies on the KITTI3D and nuScenes datasets, setting new state-of-the-art results on object category car by large margins. △ Less","29 May, 2019",https://arxiv.org/pdf/1905.12365
MolSSI and BioExcel Workflow Workshop 2018 Report,Levi N. Naden;Sam Ellis;Shantenu Jha,"Workflows in biomolecular science are very important as they are intricately intertwined with the scientific outcomes, as well as algorithmic and methodological innovations. The use and effectiveness of workflow tools to meet the needs of the biomolecular science community is varied. MolSSI co-organized a biomolecular workflows workshop in December 2018 with the goal of identifying specific software gaps and opportunities for improved workflow practices. This report captures presentations and discussion from that workshop. The workshop participants were primary tools developers, along with ""neutral observers"" and some biomolecular domain scientists. After contextualizing and motivating the workshop, the report covers the existing roles and emerging trends in how workflow systems are utilized. A few recurring observations are presented as recommendations for improving the use and effectiveness of workflow tools. The tools presented are discussed in Appendix B. △ Less","28 May, 2019",https://arxiv.org/pdf/1905.11863
Supervised Discriminative Sparse PCA for Com-Characteristic Gene Selection and Tumor Classification on Multiview Biological Data,Chun-Mei Feng;Yong Xu;Jin-Xing Liu;Ying-Lian Gao;Chun-Hou Zheng,"Principal Component Analysis (PCA) has been used to study the pathogenesis of diseases. To enhance the interpretability of classical PCA, various improved PCA methods have been proposed to date. Among these, a typical method is the so-called sparse PCA, which focuses on seeking sparse loadings. However, the performance of these methods is still far from satisfactory due to their limitation of using unsupervised learning methods; moreover, the class ambiguity within the sample is high. To overcome this problem, this study developed a new PCA method, which is named the Supervised Discriminative Sparse PCA (SDSPCA). The main innovation of this method is the incorporation of discriminative information and sparsity into the PCA model. Specifically, in contrast to the traditional sparse PCA, which imposes sparsity on the loadings, here, sparse components are obtained to represent the data. Furthermore, via linear transformation, the sparse components approximate the given label information. On the one hand, sparse components improve interpretability over traditional PCA, while on the other hand, they are have discriminative abilities suitable for classification purposes. A simple algorithm is developed and its convergence proof is provided. The SDSPCA has been applied to common characteristic gene selection (com-characteristic gene) and tumor classification on multi-view biological data. The sparsity and classification performance of the SDSPCA are empirically verified via abundant, reasonable, and effective experiments, and the obtained results demonstrate that SDSPCA outperforms other state-of-the-art methods. △ Less","28 May, 2019",https://arxiv.org/pdf/1905.11837
Fast human motion prediction for human-robot collaboration with wearable interfaces,Stefano Tortora;Stefano Michieletto;Francesca Stival;Emanuele Menegatti,"In this paper, we aim at improving human motion prediction during human-robot collaboration in industrial facilities by exploiting contributions from both physical and physiological signals. Improved human-machine collaboration could prove useful in several areas, while it is crucial for interacting robots to understand human movement as soon as possible to avoid accidents and injuries. In this perspective, we propose a novel human-robot interface capable to anticipate the user intention while performing reaching movements on a working bench in order to plan the action of a collaborative robot. The proposed interface can find many applications in the Industry 4.0 framework, where autonomous and collaborative robots will be an essential part of innovative facilities. A motion intention prediction and a motion direction prediction levels have been developed to improve detection speed and accuracy. A Gaussian Mixture Model (GMM) has been trained with IMU and EMG data following an evidence accumulation approach to predict reaching direction. Novel dynamic stopping criteria have been proposed to flexibly adjust the trade-off between early anticipation and accuracy according to the application. The output of the two predictors has been used as external inputs to a Finite State Machine (FSM) to control the behaviour of a physical robot according to user's action or inaction. Results show that our system outperforms previous methods, achieving a real-time classification accuracy of 94.3\pm2.9\% after 160.0msec\pm80.0msec from movement onset. △ Less","28 May, 2019",https://arxiv.org/pdf/1905.11734
An Immersive Virtual Reality Serious Game to Enhance Earthquake Behavioral Responses and Post-earthquake Evacuation Preparedness in Buildings,Zhenan Feng;Vicente A. González;Robert Amor;Michael Spearpoint;Jared Thomas;Rafael Sacks;Ruggiero Lovreglio;Guillermo Cabrera-Guerrero,"Enhancing the earthquake behavioral responses and post-earthquake evacuation preparedness of building occupants is beneficial to increasing their chances of survival and reducing casualties after the main shock of an earthquake. Traditionally, training approaches such as seminars, posters, videos or drills are applied to enhance preparedness. However, they are not highly engaging and have limited sensory capabilities to mimic life-threatening scenarios for the purpose of training potential participants. Immersive Virtual Reality (IVR) and Serious Games (SG) as innovative digital technologies can be used to create training tools to overcome these limitations. In this study, we propose an IVR SG-based training system to improve earthquake behavioral responses and post-earthquake evacuation preparedness. Auckland City Hospital was chosen as a case study to test our IVR SG training system. A set of learning outcomes based on best evacuation practice has been identified and embedded into several training scenarios of the IVR SG. Hospital staff (healthcare and administrative professionals) and visitors were recruited as participants to be exposed to these training scenarios. Participants' preparedness has been measured along two dimensions: 1) Knowledge about best evacuation practice; 2) Self-efficacy in dealing with earthquake emergencies. Assessment results showed that there was a significant knowledge and self-efficacy increase after the training. And participants acknowledged that it was easy and engaging to learn best evacuation practice knowledge through the IVR SG training system. △ Less","27 May, 2019",https://arxiv.org/pdf/1905.11082
The Impact of Augmented-Reality Head-Mounted Displays on Users' Movement Behavior: An Exploratory Study,Yunlong Wang;Harald Reiterer,"The augmented-reality head-mounted display (e.g., Microsoft HoloLens) is one of the most innovative technologies in multimedia and human-computer interaction in recent years. Despite the emerging research of its applications on engineering, education, medicines, to name a few, its impact on users' movement behavior is still underexplored. The movement behavior, especially for office workers with sedentary lifestyles, is related to many chronic conditions. Unlike the traditional screens, the augmented-reality head-mounted display (AR-HMD) could enable mobile virtual screens, which might impact on users' movement behavior. In this paper, we present our initial study to explore the impact of AR-HMDs on users' movement behavior. We compared the differences of macro-movements (e.g., sit-stand transitions) and micro-movements (e.g., moving the head) between two experimental modes (i.e., spatial-mapping and tag-along) with a dedicated trivial quiz task using HoloLens. The study reveals interesting findings: strong evidence supports that participants had more head-movements in the tag-along mode where higher simplicity and freedom of moving the virtual screen were given; body position/direction changes show the same effect with moderate evidence, while sit-stand transitions show no difference between the two modes with weak evidence. Our results imply several design considerations and research opportunities for future work on the ergonomics of AR-HMDs in the perspective of health. △ Less","3 June, 2019",https://arxiv.org/pdf/1905.10315
An Integrated Model for User Innovation Knowledge Based on Super-network,Xiao Liao;Zhihong Li;Yunjiang Xi;Haibo Wang;Kenneth Zantow,"Online user innovation communities are becoming a promising source of user innovation knowledge and creative users. With the purpose of identifying valuable innovation knowledge and users, this study constructs an integrated super-network model, i.e., User Innovation Knowledge Super-Network (UIKSN), to integrate fragmented knowledge, knowledge fields, users and posts in an online community knowledge system. Based on the UIKSN, the core innovation knowledge, core innovation knowledge fields, core creative users, and the knowledge structure of individual users were identified specifically. The findings help capture the innovation trends of products, popular innovations and creative users, and makes contributions on mining, and integrating and analyzing innovation knowledge in community based innovation theory. △ Less","23 May, 2019",https://arxiv.org/pdf/1905.09923
Non-Negative PARATUCK2 Tensor Decomposition Combined to LSTM Network For Smart Contracts Profiling,Jeremy Charlier;Radu State;Jean Hilger,"Smart contracts are programs stored and executed on a blockchain. The Ethereum platform, an open-source blockchain-based platform, has been designed to use these programs offering secured protocols and transaction costs reduction. The Ethereum Virtual Machine performs smart contracts runs, where the execution of each contract is limited to the amount of gas required to execute the operations described in the code. Each gas unit must be paid using Ether, the crypto-currency of the platform. Due to smart contracts interactions evolving over time, analyzing the behavior of smart contracts is very challenging. We address this challenge in our paper. We develop for this purpose an innovative approach based on the non-negative tensor decomposition PARATUCK2 combined with long short-term memory (LSTM) to assess if predictive analysis can forecast smart contracts interactions over time. To validate our methodology, we report results for two use cases. The main use case is related to analyzing smart contracts and allows shedding some light into the complex interactions among smart contracts. In order to show the generality of our method on other use cases, we also report its performance on video on demand recommendation. △ Less","23 May, 2019",https://arxiv.org/pdf/1905.09869
Modeling Smart Contracts Activities: A Tensor Based Approach,Jeremy Charlier;Radu Statem;Jean Hilger,"Smart contracts are autonomous software executing predefined conditions. Two of the biggest advantages of the smart contracts are secured protocols and transaction costs reduction. On the Ethereum platform, an open-source blockchain-based platform, smart contracts implement a distributed virtual machine on the distributed ledger. To avoid denial of service attacks and monetize the services, payment transactions are executed whenever code is being executed between contracts. It is thus natural to investigate if predictive analysis is capable to forecast these interactions. We have addressed this issue and propose an innovative application of the tensor decomposition CANDECOMP/PARAFAC to the temporal link prediction of smart contracts. We introduce a new approach leveraging stochastic processes for series predictions based on the tensor decomposition that can be used for smart contracts predictive analytics. △ Less","23 May, 2019",https://arxiv.org/pdf/1905.09868
Population-based Global Optimisation Methods for Learning Long-term Dependencies with RNNs,Bryan Lim;Stefan Zohren;Stephen Roberts,"Despite recent innovations in network architectures and loss functions, training RNNs to learn long-term dependencies remains difficult due to challenges with gradient-based optimisation methods. Inspired by the success of Deep Neuroevolution in reinforcement learning (Such et al. 2017), we explore the use of gradient-free population-based global optimisation (PBO) techniques -- training RNNs to capture long-term dependencies in time-series data. Testing evolution strategies (ES) and particle swarm optimisation (PSO) on an application in volatility forecasting, we demonstrate that PBO methods lead to performance improvements in general, with ES exhibiting the most consistent results across a variety of architectures. △ Less","23 May, 2019",https://arxiv.org/pdf/1905.09691
From web crawled text to project descriptions: automatic summarizing of social innovation projects,Nikola Milosevic;Dimitar Marinov;Abdullah Gok;Goran Nenadic,"In the past decade, social innovation projects have gained the attention of policy makers, as they address important social issues in an innovative manner. A database of social innovation is an important source of information that can expand collaboration between social innovators, drive policy and serve as an important resource for research. Such a database needs to have projects described and summarized. In this paper, we propose and compare several methods (e.g. SVM-based, recurrent neural network based, ensambled) for describing projects based on the text that is available on project websites. We also address and propose a new metric for automated evaluation of summaries based on topic modelling. △ Less","22 May, 2019",https://arxiv.org/pdf/1905.09086
A realistic and robust model for Chinese word segmentation,Chu-Ren Huang;Ting-Shuo Yo;Petr Simon;Shu-Kai Hsieh,"A realistic Chinese word segmentation tool must adapt to textual variations with minimal training input and yet robust enough to yield reliable segmentation result for all variants. Various lexicon-driven approaches to Chinese segmentation, e.g. [1,16], achieve high f-scores yet require massive training for any variation. Text-driven approach, e.g. [12], can be easily adapted for domain and genre changes yet has difficulty matching the high f-scores of the lexicon-driven approaches. In this paper, we refine and implement an innovative text-driven word boundary decision (WBD) segmentation model proposed in [15]. The WBD model treats word segmentation simply and efficiently as a binary decision on whether to realize the natural textual break between two adjacent characters as a word boundary. The WBD model allows simple and quick training data preparation converting characters as contextual vectors for learning the word boundary decision. Machine learning experiments with four different classifiers show that training with 1,000 vectors and 1 million vectors achieve comparable and reliable results. In addition, when applied to SigHAN Bakeoff 3 competition data, the WBD model produces OOV recall rates that are higher than all published results. Unlike all previous work, our OOV recall rate is comparable to our own F-score. Both experiments support the claim that the WBD model is a realistic model for Chinese word segmentation as it can be easily adapted for new variants with the robust result. In conclusion, we will discuss linguistic ramifications as well as future implications for the WBD approach. △ Less","21 May, 2019",https://arxiv.org/pdf/1905.08732
"Quantifying Novelty and Influence, and the Patterns of Paradigm Shifts",Doheum Park;Juhan Nam;Juyong Park,"Recent advances in the quantitative, computational methodology for the modeling and analysis of heterogeneous large-scale data are leading to new opportunities for understanding of human behaviors and faculties, including the manifestation of creativity that drives creative enterprises such as science. While innovation is crucial for novel and influential achievements, quantifying these qualities in creative works remains a challenge. Here we present an information-theoretic framework for computing the novelty and influence of creative works based on their generation probabilities reflecting the degree of uniqueness of their elements in comparison with other works. Applying the formalism to the data set of a high-quality, large-scale classical piano compositions represented as symbolic progressions of chords-works of significant scientific and intellectual value-spanning several centuries of musical history, we find that the enterprise's developmental history can be characterized as a dynamic process of the emergence of dominant, paradigmatic creative styles that define distinct historical periods. These findings can lead to a deeper understanding of innovation, human creativity, and the advancement of creative enterprises. △ Less","21 May, 2019",https://arxiv.org/pdf/1905.08665
Citizen Science: An Information Quality Research Frontier,Roman Lukyanenko;Andrea Wiggins;Holly K. Rosser,"The rapid proliferation of online content producing and sharing technologies resulted in an explosion of user-generated content (UGC), which now extends to scientific data. Citizen science, in which ordinary people contribute information for scientific research, epitomizes UGC. Citizen science projects are typically open to everyone, engage diverse audiences, and challenge ordinary people to produce data of highest quality to be usable in science. This also makes citizen science a very exciting area to study both traditional and innovative approaches to information quality management. With this paper we position citizen science as a leading information quality research frontier. We also show how citizen science opens a unique opportunity for the information systems community to contribute to a broad range of disciplines in natural and social sciences and humanities. △ Less","20 May, 2019",https://arxiv.org/pdf/1905.08289
Stochastic Variance Reduction for Deep Q-learning,Wei-Ye Zhao;Xi-Ya Guan;Yang Liu;Xiaoming Zhao;Jian Peng,"Recent advances in deep reinforcement learning have achieved human-level performance on a variety of real-world applications. However, the current algorithms still suffer from poor gradient estimation with excessive variance, resulting in unstable training and poor sample efficiency. In our paper, we proposed an innovative optimization strategy by utilizing stochastic variance reduced gradient (SVRG) techniques. With extensive experiments on Atari domain, our method outperforms the deep q-learning baselines on 18 out of 20 games. △ Less","20 May, 2019",https://arxiv.org/pdf/1905.08152
Arena: A General Evaluation Platform and Building Toolkit for Multi-Agent Intelligence,Yuhang Song;Andrzej Wojcicki;Thomas Lukasiewicz;Jianyi Wang;Abi Aryan;Zhenghua Xu;Mai Xu;Zihan Ding;Lianlong Wu,"Learning agents that are not only capable of taking tests, but also innovating is becoming a hot topic in AI. One of the most promising paths towards this vision is multi-agent learning, where agents act as the environment for each other, and improving each agent means proposing new problems for others. However, existing evaluation platforms are either not compatible with multi-agent settings, or limited to a specific game. That is, there is not yet a general evaluation platform for research on multi-agent intelligence. To this end, we introduce Arena, a general evaluation platform for multi-agent intelligence with 35 games of diverse logics and representations. Furthermore, multi-agent intelligence is still at the stage where many problems remain unexplored. Therefore, we provide a building toolkit for researchers to easily invent and build novel multi-agent problems from the provided game set based on a GUI-configurable social tree and five basic multi-agent reward schemes. Finally, we provide Python implementations of five state-of-the-art deep multi-agent reinforcement learning baselines. Along with the baseline implementations, we release a set of 100 best agents/teams that we can train with different training schemes for each game, as the base for evaluating agents with population performance. As such, the research community can perform comparisons under a stable and uniform standard. All the implementations and accompanied tutorials have been open-sourced for the community at https://sites.google.com/view/arena-unity/. △ Less","27 November, 2019",https://arxiv.org/pdf/1905.08085
Exact-K Recommendation via Maximal Clique Optimization,Yu Gong;Yu Zhu;Lu Duan;Qingwen Liu;Ziyu Guan;Fei Sun;Wenwu Ou;Kenny Q. Zhu,"This paper targets to a novel but practical recommendation problem named exact-K recommendation. It is different from traditional top-K recommendation, as it focuses more on (constrained) combinatorial optimization which will optimize to recommend a whole set of K items called card, rather than ranking optimization which assumes that ""better"" items should be put into top positions. Thus we take the first step to give a formal problem definition, and innovatively reduce it to Maximum Clique Optimization based on graph. To tackle this specific combinatorial optimization problem which is NP-hard, we propose Graph Attention Networks (GAttN) with a Multi-head Self-attention encoder and a decoder with attention mechanism. It can end-to-end learn the joint distribution of the K items and generate an optimal card rather than rank individual items by prediction scores. Then we propose Reinforcement Learning from Demonstrations (RLfD) which combines the advantages in behavior cloning and reinforcement learning, making it sufficient- and-efficient to train the model. Extensive experiments on three datasets demonstrate the effectiveness of our proposed GAttN with RLfD method, it outperforms several strong baselines with a relative improvement of 7.7% and 4.7% on average in Precision and Hit Ratio respectively, and achieves state-of-the-art (SOTA) performance for the exact-K recommendation problem. △ Less","16 May, 2019",https://arxiv.org/pdf/1905.07089
Making ethical decisions for the immersive web,Diane Hosfelt,"Mixed reality (MR) ethics occupies a space that intersects with web ethics, emerging tech ethics, healthcare ethics and product ethics (among others). This paper focuses on how we can build an immersive web that encourages ethical development and usage. The technology is beyond emerging (footnote: generally, the ethics of emerging technologies are focused on ethical assessments of research and innovation), but not quite entrenched. We're still in a position to intervene in the development process, instead of attempting to retrofit ethical decisions into an established design. While we have a wider range of data to analyze than most emerging technologies, we're still in a much more speculative state than entrenched technologies. This space is a challenge and an opportunity. △ Less","14 May, 2019",https://arxiv.org/pdf/1905.06995
Multi Web Audio Sequencer: Collaborative Music Making,Xavier Favory;Xavier Serra,"Recent advancements in web-based audio systems have enabled sufficiently accurate timing control and real-time sound processing capabilities. Numerous specialized music tools, as well as digital audio workstations, are now accessible from browsers. Features such as the large accessibility of data and real-time communication between clients make the web attractive for collaborative data manipulation. However, this innovative field has yet to produce effective tools for multiple-user coordination on specialized music creation tasks. The Multi Web Audio Sequencer is a prototype of an application for segment-based sequencing of Freesound sound clips, with an emphasis on seamless remote collaboration. In this work we consider a fixed-grid step sequencer as a probe for understanding the necessary features of crowd-shared music creation sessions. This manuscript describes the sequencer and the functionalities and types of interactions required for effective and attractive collaboration of remote people during creative music creation activities. △ Less","16 May, 2019",https://arxiv.org/pdf/1905.06717
Multi-Cap Optimization for Wireless Data Plans with Time Flexibility,Zhiyuan Wang;Lin Gao;Jianwei Huang,"An effective way for a Mobile network operator (MNO) to improve its revenue is price discrimination, i.e., providing different combinations of data caps and subscription fees. Rollover data plan (allowing the unused data in the current month to be used in the next month) is an innovative data mechanism with time flexibility. In this paper, we study the MNO's optimal multi-cap data plans with time flexibility in a realistic asymmetric information scenario. Specifically, users are associated with multi-dimensional private information, and the MNO designs a contract (with different data caps and subscription fees) to induce users to truthfully reveal their private information. This problem is quite challenging due to the multi-dimensional private information. We address the challenge in two aspects. First, we find that a feasible contract (satisfying incentive compatibility and individual rationality) should allocate the data caps according to users' willingness-to-pay (captured by the slopes of users' indifference curves). Second, for the non-convex data cap allocation problem, we propose a Dynamic Quota Allocation Algorithm, which has a low complexity and guarantees the global optimality. Numerical results show that the time-flexible data mechanisms increase both the MNO's profit (25% on average) and users' payoffs (8.2% on average) under price discrimination. △ Less","14 May, 2019",https://arxiv.org/pdf/1905.05922
Using Delay Tolerant Networks as a Backbone for Low-cost Smart Cities,Oluwashina Madamori;Esther Max-Onakpoya;Christan Grant;Corey E. Baker,"Rapid urbanization burdens city infrastructure and creates the need for local governments to maximize the usage of resources to serve its citizens. Smart city projects aim to alleviate the urbanization problem by deploying a vast amount of Internet-of-things (IoT) devices to monitor and manage environmental conditions and infrastructure. However, smart city projects can be extremely expensive to deploy and manage. A significant portion of the expense is a result of providing Internet connectivity via 5G or WiFi to IoT devices. This paper proposes the use of delay tolerant networks (DTNs) as a backbone for smart city communication; enabling developing communities to become smart cities at a fraction of the cost. A model is introduced to aid policy makers in designing and evaluating the expected performance of such networks. Preliminary results are presented based on a public transit network data-set from Chapel Hill, North Carolina. Finally, innovative ways of improving network performance in a low-cost smart city is discussed. △ Less","14 May, 2019",https://arxiv.org/pdf/1905.05633
Adaptive surrogate models for parametric studies,Jan N. Fuhg,"The computational effort for the evaluation of numerical simulations based on e.g. the finite-element method is high. Metamodels can be utilized to create a low-cost alternative. However the number of required samples for the creation of a sufficient metamodel should be kept low, which can be achieved by using adaptive sampling techniques. In this Master thesis adaptive sampling techniques are investigated for their use in creating metamodels with the Kriging technique, which interpolates values by a Gaussian process governed by prior covariances. The Kriging framework with extension to multifidelity problems is presented and utilized to compare adaptive sampling techniques found in the literature for benchmark problems as well as applications for contact mechanics. This thesis offers the first comprehensive comparison of a large spectrum of adaptive techniques for the Kriging framework. Furthermore a multitude of adaptive techniques is introduced to multifidelity Kriging as well as well as to a Kriging model with reduced hyperparameter dimension called partial least squares Kriging. In addition, an innovative adaptive scheme for binary classification is presented and tested for identifying chaotic motion of a Duffing's type oscillator. △ Less","12 May, 2019",https://arxiv.org/pdf/1905.05345
Enabling Mobility in LTE-Compatible Mobile-edge Computing with Programmable Switches,Ashkan Aghdai;Yang Xu;Mark Huang;David H. Dai;H. Jonathan Chao,"Network softwarization triggered a new wave of innovation in modern network design. The next generation of mobile networks embraces this trend. Mobile-edge computing (MEC) is a key part of emerging mobile networks that enables ultra-low latency mission-critical application such as vehicle-to vehicle communication. MEC aims at bringing delay-sensitive applications closer to the radio access network to enable ultra-low latency for users and decrease the back-haul pressure on mobile service providers. However, there are no practical solutions to enable mobility at MEC where connections are no longer anchored to the core network and serving applications are supposed to move as their users move. We propose the mobile-edge gateway (MEGW) to address this gap. MEGW enables mobility for MEC applications transparently and without requiring any modifications to existing protocols and applications. MEGW supports mobility by reconstructing mobile users' location via listening to LTE control plane in addition to using two-stage location-dependent traffic steering for edge connections. Networks can incrementally upgrade to support MEC by upgrading some IP router to programmable switches that run MEGW. We have implemented MEGW using P4 language and verified its compatibility with existing LTE networks in a testbed running reference LTE protocol stack. Furthermore, using packet-level simulations we show that the two-stage traffic steering algorithm reduces the number of application migrations and simplifies service provisioning. △ Less","13 May, 2019",https://arxiv.org/pdf/1905.05258
Few-Shot Viewpoint Estimation,Hung-Yu Tseng;Shalini De Mello;Jonathan Tremblay;Sifei Liu;Stan Birchfield;Ming-Hsuan Yang;Jan Kautz,"Viewpoint estimation for known categories of objects has been improved significantly thanks to deep networks and large datasets, but generalization to unknown categories is still very challenging. With an aim towards improving performance on unknown categories, we introduce the problem of category-level few-shot viewpoint estimation. We design a novel framework to successfully train viewpoint networks for new categories with few examples (10 or less). We formulate the problem as one of learning to estimate category-specific 3D canonical shapes, their associated depth estimates, and semantic 2D keypoints. We apply meta-learning to learn weights for our network that are amenable to category-specific few-shot fine-tuning. Furthermore, we design a flexible meta-Siamese network that maximizes information sharing during meta-learning. Through extensive experimentation on the ObjectNet3D and Pascal3D+ benchmark datasets, we demonstrate that our framework, which we call MetaView, significantly outperforms fine-tuning the state-of-the-art models with few examples, and that the specific architectural innovations of our method are crucial to achieving good performance. △ Less","31 July, 2019",https://arxiv.org/pdf/1905.04957
Seele's New Anti-ASIC Consensus Algorithm with Emphasis on Matrix Computation,Luke Zeng;Shawn Xin;Avadesian Xu;Thomas Pang;Tim Yang;Maolin Zheng,"In this paper, we will present a new PoW consensus algorithm used in Seele's main-net, MPoW (Matrix-Proof-of-Work). Compared to Bitcoin's PoW consensus algorithm, MPoW requires miners to compute the determinants of submatrices from a matrix constructed with n hashes other than brute-force-hashing using a hash function to find the target. This paper will evaluate this algorithm's compatibility with difficulty adjustment. Then we will discuss its efficiency in countering machines with hashrate advantage, and its feasibility to personal computers. We believe more innovative consensus protocols can be developed based on this algorithm. △ Less","11 May, 2019",https://arxiv.org/pdf/1905.04565
"Growth, degrowth, and the challenge of artificial superintelligence",Salvador Pueyo,"The implications of technological innovation for sustainability are becoming increasingly complex with information technology moving machines from being mere tools for production or objects of consumption to playing a role in economic decision making. This emerging role will acquire overwhelming importance if, as a growing body of literature suggests, artificial intelligence is underway to outperform human intelligence in most of its dimensions, thus becoming ""superintelligence"". Hitherto, the risks posed by this technology have been framed as a technical rather than a political challenge. With the help of a thought experiment, this paper explores the environmental and social implications of superintelligence emerging in an economy shaped by neoliberal policies. It is argued that such policies exacerbate the risk of extremely adverse impacts. The experiment also serves to highlight some serious flaws in the pursuit of economic efficiency and growth per se, and suggests that the challenge of superintelligence cannot be separated from the other major environmental and social challenges, demanding a fundamental transformation along the lines of degrowth. Crucially, with machines outperforming them in their functions, there is little reason to expect economic elites to be exempt from the threats that superintelligence would pose in a neoliberal context, which opens a door to overcoming vested interests that stand in the way of social change toward sustainability and equity. △ Less","3 May, 2019",https://arxiv.org/pdf/1905.04288
DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud Registration,Weixin Lu;Guowei Wan;Yao Zhou;Xiangyu Fu;Pengfei Yuan;Shiyu Song,"We present DeepICP - a novel end-to-end learning-based 3D point cloud registration framework that achieves comparable registration accuracy to prior state-of-the-art geometric methods. Different from other keypoint based methods where a RANSAC procedure is usually needed, we implement the use of various deep neural network structures to establish an end-to-end trainable network. Our keypoint detector is trained through this end-to-end structure and enables the system to avoid the inference of dynamic objects, leverages the help of sufficiently salient features on stationary objects, and as a result, achieves high robustness. Rather than searching the corresponding points among existing points, the key contribution is that we innovatively generate them based on learned matching probabilities among a group of candidates, which can boost the registration accuracy. Our loss function incorporates both the local similarity and the global geometric constraints to ensure all above network designs can converge towards the right direction. We comprehensively validate the effectiveness of our approach using both the KITTI dataset and the Apollo-SouthBay dataset. Results demonstrate that our method achieves comparable or better performance than the state-of-the-art geometry-based methods. Detailed ablation and visualization analysis are included to further illustrate the behavior and insights of our network. The low registration error and high robustness of our method makes it attractive for substantial applications relying on the point cloud registration task. △ Less","16 September, 2019",https://arxiv.org/pdf/1905.04153
Compositional Coding for Collaborative Filtering,Chenghao Liu;Tao Lu;Xin Wang;Zhiyong Cheng;Jianling Sun;Steven C. H. Hoi,"Efficiency is crucial to the online recommender systems. Representing users and items as binary vectors for Collaborative Filtering (CF) can achieve fast user-item affinity computation in the Hamming space, in recent years, we have witnessed an emerging research effort in exploiting binary hashing techniques for CF methods. However, CF with binary codes naturally suffers from low accuracy due to limited representation capability in each bit, which impedes it from modeling complex structure of the data. In this work, we attempt to improve the efficiency without hurting the model performance by utilizing both the accuracy of real-valued vectors and the efficiency of binary codes to represent users/items. In particular, we propose the Compositional Coding for Collaborative Filtering (CCCF) framework, which not only gains better recommendation efficiency than the state-of-the-art binarized CF approaches but also achieves even higher accuracy than the real-valued CF method. Specifically, CCCF innovatively represents each user/item with a set of binary vectors, which are associated with a sparse real-value weight vector. Each value of the weight vector encodes the importance of the corresponding binary vector to the user/item. The continuous weight vectors greatly enhances the representation capability of binary codes, and its sparsity guarantees the processing speed. Furthermore, an integer weight approximation scheme is proposed to further accelerate the speed. Based on the CCCF framework, we design an efficient discrete optimization algorithm to learn its parameters. Extensive experiments on three real-world datasets show that our method outperforms the state-of-the-art binarized CF methods (even achieves better performance than the real-valued CF method) by a large margin in terms of both recommendation accuracy and efficiency. △ Less","9 May, 2019",https://arxiv.org/pdf/1905.03752
Mappa Mundi: An Interactive Artistic Mind Map Generator with Artificial Imagination,Ruixue Liu;Baoyang Chen;Meng Chen;Youzheng Wu;Zhijie Qiu;Xiaodong He,"We present a novel real-time, collaborative, and interactive AI painting system, Mappa Mundi, for artistic Mind Map creation. The system consists of a voice-based input interface, an automatic topic expansion module, and an image projection module. The key innovation is to inject Artificial Imagination into painting creation by considering lexical and phonological similarities of language, learning and inheriting artist's original painting style, and applying the principles of Dadaism and impossibility of improvisation. Our system indicates that AI and artist can collaborate seamlessly to create imaginative artistic painting and Mappa Mundi has been applied in art exhibition in UCCA, Beijing △ Less","21 June, 2019",https://arxiv.org/pdf/1905.03638
"Solo citations, duet citations, and prelude citations: New measures of the disruption of academic papers",Qiang Wu;Zhaoyang Yan,"It is important to measure the disruption of academic papers. According to the characteristics of three different kinds of citations, this paper borrows musical vocabulary and names them solo citations (SC), duet citations (DC), and prelude citations (PC) respectively. Studying how to measure the disruption of a published work effectively, this study analyzes nine indicators and suggests a general evaluation formula. Seven of the nine indicators are innovations introduced by this paper: SC, SC-DC, SC-PC, SC-DC-PC, (SC-DC)/(SC+DC), (SC-PC)/(SC+DC), and (SC-DC-PC)/(SC+DC), as is the general formula. These indices are discussed considering two cases: One case concerns the Citation Indexes for Science and the other concerns Co-citations. The results show that, compared with other indicators, four indicators (SC, SC-DC, SC/(SC+DC), and (SC-DC)/(SC+DC)) are logically and empirically reasonable. Future research may consider combining these indices, for example, using SC multiplied by SC/(SC+DC) or SC-DC multiplied by (SC-DC)/(SC+DC), to get final evaluation results that contain desirable characteristics of two types of indicators. Confirming which of the evaluation results from these indicators can best reflect the innovation of research papers requires much empirical analysis. △ Less","10 May, 2019",https://arxiv.org/pdf/1905.03461
The Implications of Pricing on Social Learning,Itai Arieli;Moran Koren;Rann Smorodinsky,"We study the implications of endogenous pricing for learning and welfare in the classic herding model . When prices are determined exogenously, it is known that learning occurs if and only if signals are unbounded. By contrast, we show that learning can occur when signals are bounded as long as non-conformism among consumers is scarce. More formally, learning happens if and only if signals exhibit the vanishing likelihood property introduced bellow. We discuss the implications of our results for potential market failure in the context of Schumpeterian growth with uncertainty over the value of innovations. △ Less","9 May, 2019",https://arxiv.org/pdf/1905.03452
Does Environmental Economics lead to patentable research?,Xiaojun Hu;Ronald Rousseau;Sandra Rousseau,"In this feasibility study, the impact of academic research from social sciences and humanities on technological innovation is explored through a study of citations patterns of journal articles in patents. Specifically we focus on citations of journals from the field of environmental economics in patents included in an American patent database (USPTO). Three decades of patents have led to a small set of journal articles (85) that are being cited from the field of environmental economics. While this route of measuring how academic research is validated through its role in stimulating technological progress may be rather limited (based on this first exploration), it may still point to a valuable and interesting topic for further research. △ Less","7 May, 2019",https://arxiv.org/pdf/1905.02875
PocketCare: Tracking the Flu with Mobile Phones using Partial Observations of Proximity and Symptoms,Wen Dong;Tong Guan;Bruno Lepri;Chunming Qiao,"Mobile phones provide a powerful sensing platform that researchers may adopt to understand proximity interactions among people and the diffusion, through these interactions, of diseases, behaviors, and opinions. However, it remains a challenge to track the proximity-based interactions of a whole community and then model the social diffusion of diseases and behaviors starting from the observations of a small fraction of the volunteer population. In this paper, we propose a novel approach that tries to connect together these sparse observations using a model of how individuals interact with each other and how social interactions happen in terms of a sequence of proximity interactions. We apply our approach to track the spreading of flu in the spatial-proximity network of a 3000-people university campus by mobilizing 300 volunteers from this population to monitor nearby mobile phones through Bluetooth scanning and to daily report flu symptoms about and around them. Our aim is to predict the likelihood for an individual to get flu based on how often her/his daily routine intersects with those of the volunteers. Thus, we use the daily routines of the volunteers to build a model of the volunteers as well as of the non-volunteers. Our results show that we can predict flu infection two weeks ahead of time with an average precision from 0.24 to 0.35 depending on the amount of information. This precision is six to nine times higher than with a random guess model. At the population level, we can predict infectious population in a two-week window with an r-squared value of 0.95 (a random-guess model obtains an r-squared value of 0.2). These results point to an innovative approach for tracking individuals who have interacted with people showing symptoms, allowing us to warn those in danger of infection and to inform health researchers about the progression of contact-induced diseases. △ Less","7 May, 2019",https://arxiv.org/pdf/1905.02607
"Bee^+
: A 95-mg Four-Winged Insect-Scale Flying Robot Driven by Twinned Unimorph Actuators",Xiufeng Yang;Ying Chen;Longlong Chang;Ariel A. Calderón;Néstor O. Pérez-Arancibia,"We introduce Bee^+, a 95-mg four-winged microrobot with improved controllability and open-loop-response characteristics with respect to those exhibited by state-of-the-art two-winged microrobots with the same size and similar weight (i.e., the 75-mg Harvard RoboBee). The key innovation that made possible the development of Bee^+ is the introduction of an extremely light (28-mg) pair of twinned unimorph actuators, which enabled the design of a new microrobotic mechanism that flaps four wings independently. A first main advantage of the proposed design, compared to those of two-winged flyers, is that by increasing the number of actuators from two to four, the number of direct control inputs increases from three to four when simple sinusoidal excitations are employed. A second advantage of Bee^+ is that its four-wing configuration and flapping mode naturally damp the rotational disturbances that commonly affect the yaw degree of freedom of two-winged microrobots. In addition, the proposed design greatly reduces the complexity of the associated fabrication process compared to those of other microrobots, as the unimorph actuators are fairly easy to build. Lastly, we hypothesize that given the relatively low wing-loading affecting their flapping mechanisms, the life expectancy of Bee^+s must be considerably higher than those of the two-winged counterparts. The functionality and basic capabilities of the robot are demonstrated through a set of simple control experiments. △ Less","23 July, 2019",https://arxiv.org/pdf/1905.02253
"Impact of Artificial Intelligence on Businesses: from Research, Innovation, Market Deployment to Future Shifts in Business Models",Neha Soni;Enakshi Khular Sharma;Narotam Singh;Amita Kapoor,"The fast pace of artificial intelligence (AI) and automation is propelling strategists to reshape their business models. This is fostering the integration of AI in the business processes but the consequences of this adoption are underexplored and need attention. This paper focuses on the overall impact of AI on businesses - from research, innovation, market deployment to future shifts in business models. To access this overall impact, we design a three-dimensional research model, based upon the Neo-Schumpeterian economics and its three forces viz. innovation, knowledge, and entrepreneurship. The first dimension deals with research and innovation in AI. In the second dimension, we explore the influence of AI on the global market and the strategic objectives of the businesses and finally, the third dimension examines how AI is shaping business contexts. Additionally, the paper explores AI implications on actors and its dark sides. △ Less","3 May, 2019",https://arxiv.org/pdf/1905.02092
A Self-Attentive Emotion Recognition Network,Harris Partaourides;Kostantinos Papadamou;Nicolas Kourtellis;Ilias Leontiadis;Sotirios Chatzis,"Modern deep learning approaches have achieved groundbreaking performance in modeling and classifying sequential data. Specifically, attention networks constitute the state-of-the-art paradigm for capturing long temporal dynamics. This paper examines the efficacy of this paradigm in the challenging task of emotion recognition in dyadic conversations. In contrast to existing approaches, our work introduces a novel attention mechanism capable of inferring the immensity of the effect of each past utterance on the current speaker emotional state. The proposed attention mechanism performs this inference procedure without the need of a decoder network; this is achieved by means of innovative self-attention arguments. Our self-attention networks capture the correlation patterns among consecutive encoder network states, thus allowing to robustly and effectively model temporal dynamics over arbitrary long temporal horizons. Thus, we enable capturing strong affective patterns over the course of long discussions. We exhibit the effectiveness of our approach considering the challenging IEMOCAP benchmark. As we show, our devised methodology outperforms state-of-the-art alternatives and commonly used approaches, giving rise to promising new research directions in the context of Online Social Network (OSN) analysis tasks. △ Less","24 April, 2019",https://arxiv.org/pdf/1905.01972
"Internet, Social Media and Conflict Studies Can Greater Interdisciplinarity Solve the Analytical Deadlocks in Cybersecurity Research?",H. Akin Unver,"In recent years, computational research methods, digital trace data and online human interactions have contributed to the emergence of new technology-oriented sub-fields within International Relations (IR). Although the cybersecurity scholarship had an initial promise to be the primus inter pares among these emerging fields, the main thrust of this new methodological innovation came through the digital conflict studies sub-field. By integrating Internet and social media research tools and questions into its core topics of sub-national violence, terrorism and radical mobilization, digital conflict studies has recently succeeded in addressing some of the data validity and methodology problems faced by the cybersecurity scholarship. This article begins by briefly reviewing some of the persistent data and method-oriented hurdles faced by the cybersecurity scholarship. Then, it moves onto a more detailed account of how digital conflict studies have been addressing some of these deadlocks by focusing individually on the literature on onset, mobilization, targeting, intensity/duration and termination phases of conflicts. Ultimately, the article concludes with the suggestion that the cybersecurity scholarship could move past its own deadlocks by building more granular and dedicated research datasets and establishing mechanisms to share event data with the scientific community. △ Less","5 May, 2019",https://arxiv.org/pdf/1905.01777
Generating Classification Weights with GNN Denoising Autoencoders for Few-Shot Learning,Spyros Gidaris;Nikos Komodakis,"Given an initial recognition model already trained on a set of base classes, the goal of this work is to develop a meta-model for few-shot learning. The meta-model, given as input some novel classes with few training examples per class, must properly adapt the existing recognition model into a new model that can correctly classify in a unified way both the novel and the base classes. To accomplish this goal it must learn to output the appropriate classification weight vectors for those two types of classes. To build our meta-model we make use of two main innovations: we propose the use of a Denoising Autoencoder network (DAE) that (during training) takes as input a set of classification weights corrupted with Gaussian noise and learns to reconstruct the target-discriminative classification weights. In this case, the injected noise on the classification weights serves the role of regularizing the weight generating meta-model. Furthermore, in order to capture the co-dependencies between different classes in a given task instance of our meta-model, we propose to implement the DAE model as a Graph Neural Network (GNN). In order to verify the efficacy of our approach, we extensively evaluate it on ImageNet based few-shot benchmarks and we report strong results that surpass prior approaches. The code and models of our paper will be published on: https://github.com/gidariss/wDAE_GNN_FewShot △ Less","3 May, 2019",https://arxiv.org/pdf/1905.01102
Facial Expressions Analysis Under Occlusions Based on Specificities of Facial Motion Propagation,Delphine Poux;Benjamin Allaert;Jose Mennesson;Nacim Ihaddadene;Ioan Marius Bilasco;Chaabane Djeraba,"Although much progress has been made in the facial expression analysis field, facial occlusions are still challenging. The main innovation brought by this contribution consists in exploiting the specificities of facial movement propagation for recognizing expressions in presence of important occlusions. The movement induced by an expression extends beyond the movement epicenter. Thus, the movement occurring in an occluded region propagates towards neighboring visible regions. In presence of occlusions, per expression, we compute the importance of each unoccluded facial region and we construct adapted facial frameworks that boost the performance of per expression binary classifier. The output of each expression-dependant binary classifier is then aggregated and fed into a fusion process that aims constructing, per occlusion, a unique model that recognizes all the facial expressions considered. The evaluations highlight the robustness of this approach in presence of significant facial occlusions. △ Less","30 April, 2019",https://arxiv.org/pdf/1904.13154
Challenges and Pitfalls of Machine Learning Evaluation and Benchmarking,Cheng Li;Abdul Dakkak;Jinjun Xiong;Wen-mei Hwu,"An increasingly complex and diverse collection of Machine Learning (ML) models as well as hardware/software stacks, collectively referred to as ""ML artifacts"", are being proposed - leading to a diverse landscape of ML. These ML innovations proposed have outpaced researchers' ability to analyze, study and adapt them. This is exacerbated by the complicated and sometimes non-reproducible procedures for ML evaluation. A common practice of sharing ML artifacts is through repositories where artifact authors post ad-hoc code and some documentation, but often fail to reveal critical information for others to reproduce their results. This results in users' inability to compare with artifact authors' claims or adapt the model to his/her own use. This paper discusses common challenges and pitfalls of ML evaluation and benchmarking, which can be used as a guideline for ML model authors when sharing ML artifacts, and for system developers when benchmarking or designing ML systems. △ Less","25 June, 2019",https://arxiv.org/pdf/1904.12437
A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy and Efficiency,Jinkun Cao;Jinhao Zhu;Liwei Lin;Zhengui Xue;Ruhui Ma;Haibing Guan,"As cloud computing becomes prevalent in recent years, more and more enterprises and individuals outsource their data to cloud servers. To avoid privacy leaks, outsourced data usually is encrypted before being sent to cloud servers, which disables traditional search schemes for plain text. To meet both end of security and searchability, search-supported encryption is proposed. However, many previous schemes suffer severe vulnerability when typos and semantic diversity exist in query requests. To overcome such flaw, higher error-tolerance is always expected for search-supported encryption design, sometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of multi-keyword fuzzy search over encrypted and outsourced data. Our approach introduces a new mechanism to map a natural language expression into a word-vector space. Compared with previous approaches, our design shows higher robustness when multiple kinds of typos are involved. Besides, our approach is enhanced with novel data structures to improve search efficiency. These two innovations can work well for both accuracy and efficiency. Moreover, these designs will not hurt the fundamental security. Experiments on a real-world dataset demonstrate the effectiveness of our proposed approach, which outperforms currently popular approaches focusing on similar tasks. △ Less","21 June, 2019",https://arxiv.org/pdf/1904.12111
Smart Laptop Bag with Machine Learning for Activity Recognition,Dwij Sukeshkumar Sheth;Shantanu Singh;Prakhar S Mathur;Vydeki D,"In todays world of smart living, the smart laptop bag, presented in this paper, provides a better solution to keep track of our precious possessions and monitoring them in real time. As the world moves towards a much tech-savvy direction, the novel laptop bag discussed here facilitates the user to perform location tracking, ambiance monitoring, user-state monitoring etc. in one device. The innovative design uses cloud computing and machine learning algorithms to monitor the health of the user and many parameters of the bag. The emergency alert system in this bag could be trained to send appropriate notifications to emergency contacts of the user, in case of abnormal health conditions or theft of the bag. The experimental smart laptop bag uses deep neural network, which was trained and tested over the various parameters from the bag and produces above 95% accurate results. △ Less","14 April, 2019",https://arxiv.org/pdf/1904.11882
XR: Enabling training mode in the human brain XR: Enabling training mode in the human brain,Philippe Lépinard;Sébastien Lozé,"The face of simulation-based training has greatly evolved, with the most recent tools giving the ability to create virtual environments that rival realism. At first glance, it might appear that what the training sector needs is the most realistic simulators possible, but traditional simulators are not necessarily the most efficient or practical training tools. With all that these new technologies have to offer; the challenge is to go back to the core of training needs and identify the right vector of sensory cues that will most effectively enable training mode in the human brain. Bigger and Pricier doesn't necessarily mean better. Simulation with cross-reality content (XR), which by definition encompasses virtual reality (VR), mixed reality (MR), and augmented reality (AR), is the most practical solution for deploying any kind of simulation-based training. The authors of this paper (a teacher and a technology expert) share their experiences and expose XR-specific best practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze : Starting his career in the modeling and simulation community more than 15 years ago, S{é}bastien has focused on learning about the latest simulation innovations and sharing information on how experts have solved their challenges. He worked on the COTS integration at CAE and the Presagis focusing on Simulation and Visualization products. More recently, Sebastien put together simulation and training teams and strategies for emerging companies like CM Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games, focusing on helping companies develop real-time solutions for simulation-based training. Philippe Lepinard: Former military helicopter pilot and simulation officer, Philippe L{é}pinard is now an associate professor at the University of Paris-Est Cr{é}teil (UPEC). His research is focusing on playful learning and training through simulation. He is one of the founding members of the French simulation association. △ Less","26 April, 2019",https://arxiv.org/pdf/1904.11704
IRC: Cross-layer design exploration of Intermittent Robust Computation units for IoTs,Arman Roohi;Ronald F DeMara,"Energy-harvesting-powered computing offers intriguing and vast opportunities to dramatically transform the landscape of the Internet of Things (IoT) devices by utilizing ambient sources of energy to achieve battery-free computing. In order to operate within the restricted energy capacity and intermittency profile, it is proposed to innovate Intermittent Robust Computation (IRC) Unit as a new duty-cycle-variable computing approach leveraging the non-volatility inherent in spin-based switching devices. The foundations of IRC will be advanced from the device-level upwards, by extending a Spin Hall Effect Magnetic Tunnel Junction (SHE-MTJ) device. The device will then be used to realize SHE-MTJ Majority/Polymorphic Gate (MG/PG) logic approaches and libraries. Then a Logic-Embedded Flip-Flop (LE-FF) is developed to realize rudimentary Boolean logic functions along with an inherent state-holding capability within a compact footprint. Finally, the NV-Clustering synthesis procedure and corresponding tool module are proposed to instantiate the LE-FF library cells within conventional Register Transfer Language (RTL) specifications. This selectively clusters together logic and NV state-holding functionality, based on energy and area minimization criteria. It also realizes middleware-coherent, intermittent computation without checkpointing, micro-tasking, or software bloat and energy overheads vital to IoT. Simulation results for various benchmark circuits including ISCAS-89 validate functionality and power dissipation, area, and delay benefits. △ Less","23 April, 2019",https://arxiv.org/pdf/1904.10564
The Theorem Prover Museum -- Conserving the System Heritage of Automated Reasoning,Michael Kohlhase,"We present the Theorem Prover Museum, and initiative to conserve -- and make publicly available -- the sources and source-related artefacts of automated reasoning systems. Theorem provers have been at the forefront of Artificial Intelligence, stretching the limits of computation, and incubating many innovations we take for granted today. Without the systems themselves as preserved cultural artefacts, future historians will have difficulties to study the history of science and engineering in our discipline. △ Less","23 April, 2019",https://arxiv.org/pdf/1904.10414
A survey on Big Data and Machine Learning for Chemistry,Jose F Rodrigues Jr;Larisa Florea;Maria C F de Oliveira;Dermot Diamond;Osvaldo N Oliveira Jr,"Herein we review aspects of leading-edge research and innovation in chemistry which exploits big data and machine learning (ML), two computer science fields that combine to yield machine intelligence. ML can accelerate the solution of intricate chemical problems and even solve problems that otherwise would not be tractable. But the potential benefits of ML come at the cost of big data production; that is, the algorithms, in order to learn, demand large volumes of data of various natures and from different sources, from materials properties to sensor data. In the survey, we propose a roadmap for future developments, with emphasis on materials discovery and chemical sensing, and within the context of the Internet of Things (IoT), both prominent research fields for ML in the context of big data. In addition to providing an overview of recent advances, we elaborate upon the conceptual and practical limitations of big data and ML applied to chemistry, outlining processes, discussing pitfalls, and reviewing cases of success and failure. △ Less","23 April, 2019",https://arxiv.org/pdf/1904.10370
"Student Becoming the Master: Knowledge Amalgamation for Joint Scene Parsing, Depth Estimation, and More",Jingwen Ye;Yixin Ji;Xinchao Wang;Kairi Ou;Dapeng Tao;Mingli Song,"In this paper, we investigate a novel deep-model reusing task. Our goal is to train a lightweight and versatile student model, without human-labelled annotations, that amalgamates the knowledge and masters the expertise of two pretrained teacher models working on heterogeneous problems, one on scene parsing and the other on depth estimation. To this end, we propose an innovative training strategy that learns the parameters of the student intertwined with the teachers, achieved by 'projecting' its amalgamated features onto each teacher's domain and computing the loss. We also introduce two options to generalize the proposed training strategy to handle three or more tasks simultaneously. The proposed scheme yields very encouraging results. As demonstrated on several benchmarks, the trained student model achieves results even superior to those of the teachers in their own expertise domains and on par with the state-of-the-art fully supervised models relying on human-labelled annotations. △ Less","23 April, 2019",https://arxiv.org/pdf/1904.10167
Multiview Hessian Regularization for Image Annotation,Weifeng Liu;Dacheng Tao,"The rapid development of computer hardware and Internet technology makes large scale data dependent models computationally tractable, and opens a bright avenue for annotating images through innovative machine learning algorithms. Semi-supervised learning (SSL) has consequently received intensive attention in recent years and has been successfully deployed in image annotation. One representative work in SSL is Laplacian regularization (LR), which smoothes the conditional distribution for classification along the manifold encoded in the graph Laplacian, however, it has been observed that LR biases the classification function towards a constant function which possibly results in poor generalization. In addition, LR is developed to handle uniformly distributed data (or single view data), although instances or objects, such as images and videos, are usually represented by multiview features, such as color, shape and texture. In this paper, we present multiview Hessian regularization (mHR) to address the above two problems in LR-based image annotation. In particular, mHR optimally combines multiple Hessian regularizations, each of which is obtained from a particular view of instances, and steers the classification function which varies linearly along the data manifold. We apply mHR to kernel least squares and support vector machines as two examples for image annotation. Extensive experiments on the PASCAL VOC'07 dataset validate the effectiveness of mHR by comparing it with baseline algorithms, including LR and HR. △ Less","22 April, 2019",https://arxiv.org/pdf/1904.10100
"LATTE: Accelerating LiDAR Point Cloud Annotation via Sensor Fusion, One-Click Annotation, and Tracking",Bernie Wang;Virginia Wu;Bichen Wu;Kurt Keutzer,"LiDAR (Light Detection And Ranging) is an essential and widely adopted sensor for autonomous vehicles, particularly for those vehicles operating at higher levels (L4-L5) of autonomy. Recent work has demonstrated the promise of deep-learning approaches for LiDAR-based detection. However, deep-learning algorithms are extremely data hungry, requiring large amounts of labeled point-cloud data for training and evaluation. Annotating LiDAR point cloud data is challenging due to the following issues: 1) A LiDAR point cloud is usually sparse and has low resolution, making it difficult for human annotators to recognize objects. 2) Compared to annotation on 2D images, the operation of drawing 3D bounding boxes or even point-wise labels on LiDAR point clouds is more complex and time-consuming. 3) LiDAR data are usually collected in sequences, so consecutive frames are highly correlated, leading to repeated annotations. To tackle these challenges, we propose LATTE, an open-sourced annotation tool for LiDAR point clouds. LATTE features the following innovations: 1) Sensor fusion: We utilize image-based detection algorithms to automatically pre-label a calibrated image, and transfer the labels to the point cloud. 2) One-click annotation: Instead of drawing 3D bounding boxes or point-wise labels, we simplify the annotation to just one click on the target object, and automatically generate the bounding box for the target. 3) Tracking: we integrate tracking into sequence annotation such that we can transfer labels from one frame to subsequent ones and therefore significantly reduce repeated labeling. Experiments show the proposed features accelerate the annotation speed by 6.2x and significantly improve label quality with 23.6% and 2.2% higher instance-level precision and recall, and 2.0% higher bounding box IoU. LATTE is open-sourced at https://github.com/bernwang/latte. △ Less","19 April, 2019",https://arxiv.org/pdf/1904.09085
On the Impact of Perceived Vulnerability in the Adoption of Information Systems Security Innovations,Mumtaz Abdul Hameed;Nalin Asanka Gamagedara Arachchilage,"A number of determinants predict the adoption of Information Systems (IS) security innovations. Amongst, perceived vulnerability of IS security threats has been examined in a number of past explorations. In this research, we examined the processes pursued in analysing the relationship between perceived vulnerability of IS security threats and the adoption of IS security innovations. The study uses Systematic Literature Review (SLR) method to evaluate the practice involved in examining perceived vulnerability on IS security innovation adoption. The SLR findings revealed the appropriateness of the existing empirical investigations of the relationship between perceived vulnerability of IS security threats on IS security innovation adoption. Furthermore, the SLR results confirmed that individuals who perceives vulnerable to an IS security threat are more likely to engage in the adoption an IS security innovation. In addition, the study validates the past studies on the relationship between perceived vulnerability and IS security innovation adoption. △ Less","16 April, 2019",https://arxiv.org/pdf/1904.08229
Economic Analysis of Rollover and Shared Data Plans,Xuehe Wang;Lingjie Duan,"In today's growing data market, wireless service providers (WSPs) compete severely to attract users by announcing innovative data plans. Two of the most popular innovative data plans are rollover and shared data plans, where the former plan allows a user to keep his unused data quota to next month and the latter plan allows users in a family to share unused data. As a pioneer to provide such data plans, a WSP faces immediate revenue loss from existing users who pay less overage charges due to less data over-usage, but his market share increases gradually by attracting new users and those under the other WSPs. In some countries, WSPs have asymmetric timing for providing such innovative data plans, while some other markets' WSPs have symmetric timing or no planning. This raises the question of why and when the competitive WSPs should offer the new data plans. This paper provides game theoretic modelling and analysis of the WSPs' timing of offering innovative data plans, by considering new user arrival and dynamic user churn between WSPs. Our equilibrium analysis shows that the WSP with small market share prefers to announce the innovative data plan first to attract more users, while the WSP with large market share prefers to announce later to avoid the immediate revenue loss. In a market with many new users, WSPs with similar market shares will offer the data plans simultaneously, but these WSPs facing few new users may not offer any new plan. Perhaps surprisingly, WSPs' profits can decrease with new user number and they may not benefit from the option of innovative data plans. Finally, unlike rollover data plan, we show that the timing of shared data plan further depends on the composition of users. △ Less","29 March, 2019",https://arxiv.org/pdf/1904.07970
Advanced Customer Activity Prediction based on Deep Hierarchic Encoder-Decoders,Andrei Damian;Laurentiu Piciu;Sergiu Turlea;Nicolae Tapus,"Product recommender systems and customer profiling techniques have always been a priority in online retail. Recent machine learning research advances and also wide availability of massive parallel numerical computing has enabled various approaches and directions of recommender systems advancement. Worth to mention is the fact that in past years multiple traditional ""offline"" retail business are gearing more and more towards employing inferential and even predictive analytics both to stock-related problems such as predictive replenishment but also to enrich customer interaction experience. One of the most important areas of recommender systems research and development is that of Deep Learning based models which employ representational learning to model consumer behavioral patterns. Current state of the art in Deep Learning based recommender systems uses multiple approaches ranging from already classical methods such as the ones based on learning product representation vector, to recurrent analysis of customer transactional time-series and up to generative models based on adversarial training. Each of these methods has multiple advantages and inherent weaknesses such as inability of understanding the actual user-journey, ability to propose only single product recommendation or top-k product recommendations without prediction of actual next-best-offer. In our work we will present a new and innovative architectural approach of applying state-of-the-art hierarchical multi-module encoder-decoder architecture in order to solve several of current state-of-the-art recommender systems issues. Our approach will also produce by-products such as product need-based segmentation and customer behavioral segmentation - all in an end-to-end trainable approach. Finally, we will present a couple methods that solve known retail & distribution pain-points based on the proposed architecture. △ Less","21 June, 2019",https://arxiv.org/pdf/1904.07687
Painting on Placement: Forecasting Routing Congestion using Conditional Generative Adversarial Nets,Cunxi Yu;Zhiru Zhang,"Physical design process commonly consumes hours to days for large designs, and routing is known as the most critical step. Demands for accurate routing quality prediction raise to a new level to accelerate hardware innovation with advanced technology nodes. This work presents an approach that forecasts the density of all routing channels over the entire floorplan, with features collected up to placement, using conditional GANs. Specifically, forecasting the routing congestion is constructed as an image translation (colorization) problem. The proposed approach is applied to a) placement exploration for minimum congestion, b) constrained placement exploration and c) forecasting congestion in real-time during incremental placement, using eight designs targeting a fixed FPGA architecture. △ Less","15 April, 2019",https://arxiv.org/pdf/1904.07077
Animo: Sharing Biosignals on a Smartwatch for Lightweight Social Connection,Fannie Liu;Mario Esparza;Maria Pavlovskaia;Geoff Kaufman;Laura Dabbish;Andrés Monroy-Hernández,"We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch-based biosignals social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N=34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches. △ Less","12 April, 2019",https://arxiv.org/pdf/1904.06427
Explore with caution: mapping the evolution of scientific interest in Physics,Alberto Aleta;Sandro Meloni;Nicola Perra;Yamir Moreno,"In the book The Essential Tension Thomas Kuhn described the conflict between tradition and innovation in scientific research --i.e., the desire to explore new promising areas, counterposed to the need to capitalize on the work done in the past. While it is true that along their careers many scientists probably felt this tension, only few works have tried to quantify it. Here, we address this question by analyzing a large-scale dataset, containing all the papers published by the American Physical Society (APS) in more than 25 years, which allows for a better understanding of scientists' careers evolution in Physics. We employ the Physics and Astronomy Classification Scheme (PACS) present in each paper to map the scientific interests of 181,397 authors and their evolution along the years. Our results indeed confirm the existence of the `essential tension' with scientists balancing between exploring the boundaries of their area and exploiting previous work. In particular, we found that although the majority of physicists change the topics of their research, they stay within the same broader area thus exploring with caution new scientific endeavors. Furthermore, we quantify the flows of authors moving between different subfields and pinpoint which areas are more likely to attract or donate researchers to the other ones. Overall, our results depict a very distinctive portrait of the evolution of research interests in Physics and can help in designing specific policies for the future. △ Less","12 April, 2019",https://arxiv.org/pdf/1904.06306
Expressive haptics for enhanced usability of mobile interfaces in situations of impairments,Tigmanshu Bhatnagar;Youngjun Cho;Nicolai Marquardt;Catherine Holloway,"Designing for situational awareness could lead to better solutions for disabled people, likewise, exploring the needs of disabled people could lead to innovations that can address situational impairments. This in turn can create non-stigmatising assistive technology for disabled people from which eventually everyone could benefit. In this paper, we investigate the potential for advanced haptics to compliment the graphical user interface of mobile devices, thereby enhancing user experiences of all people in some situations (e.g. sunlight interfering with interaction) and visually impaired people. We explore technical solutions to this problem space and demonstrate our justification for a focus on the creation of kinaesthetic force feedback. We propose initial design concepts and studies, with a view to co-create delightful and expressive haptic interactions with potential users motivated by scenarios of situational and permanent impairments. △ Less","12 April, 2019",https://arxiv.org/pdf/1904.06119
Uplink Grant-Free Random Access Solutions for URLLC services in 5G New Radio,Nurul Huda Mahmood;Renato Abreu;Ronald Böhnke;Martin Schubert;Gilberto Berardinelli;Thomas H. Jacobsen,"The newly introduced ultra-reliable low latency communication service class in 5G New Radio depends on innovative low latency radio resource management solutions that can guarantee high reliability. Grant-free random access, where channel resources are accessed without undergoing assignment through a handshake process, is proposed in 5G New Radio as an important latency reducing solution. However, this comes at an increased likelihood of collisions resulting from uncontrolled channel access, when the same resources are preallocated to a group of users. Novel reliability enhancement techniques are therefore needed. This article provides an overview of grant-free random access in 5G New Radio focusing on the ultra-reliable low latency communication service class, and presents two reliability-enhancing solutions. The first proposes retransmissions over shared resources, whereas the second proposal incorporates grant-free transmission with non-orthogonal multiple access with overlapping transmissions being resolved through the use of advanced receivers. Both proposed solutions result in significant performance gains, in terms of reliability as well as resource efficiency. For example, the proposed non-orthogonal multiple access scheme can support a normalized load of more than 1.5 users/slot at packet loss rates of ~10^{-5} - a significant improvement over the maximum supported load with conventional grant-free schemes like slotted-ALOHA. △ Less","11 April, 2019",https://arxiv.org/pdf/1904.05593
Generalizing Monocular 3D Human Pose Estimation in the Wild,Luyang Wang;Yan Chen;Zhenhua Guo;Keyuan Qian;Mude Lin;Hongsheng Li;Jimmy S. Ren,"The availability of the large-scale labeled 3D poses in the Human3.6M dataset plays an important role in advancing the algorithms for 3D human pose estimation from a still image. We observe that recent innovation in this area mainly focuses on new techniques that explicitly address the generalization issue when using this dataset, because this database is constructed in a highly controlled environment with limited human subjects and background variations. Despite such efforts, we can show that the results of the current methods are still error-prone especially when tested against the images taken in-the-wild. In this paper, we aim to tackle this problem from a different perspective. We propose a principled approach to generate high quality 3D pose ground truth given any in-the-wild image with a person inside. We achieve this by first devising a novel stereo inspired neural network to directly map any 2D pose to high quality 3D counterpart. We then perform a carefully designed geometric searching scheme to further refine the joints. Based on this scheme, we build a large-scale dataset with 400,000 in-the-wild images and their corresponding 3D pose ground truth. This enables the training of a high quality neural network model, without specialized training scheme and auxiliary loss function, which performs favorably against the state-of-the-art 3D pose estimation methods. We also evaluate the generalization ability of our model both quantitatively and qualitatively. Results show that our approach convincingly outperforms the previous methods. We make our dataset and code publicly available. △ Less","10 April, 2019",https://arxiv.org/pdf/1904.05512
Application performance on a Cluster-Booster system,Anke Kreuzer;Jorge Amaya;Norbert Eicker;Estela Suarez,"The DEEP projects have developed a variety of hardware and software technologies aiming at improving the efficiency and usability of next generation high-performance computers. They evolve around an innovative concept for heterogeneous systems: the Cluster-Booster architecture. In it, a general purpose cluster is tightly coupled to a many-core system (the Booster). This modular way of integrating heterogeneous components enables applications to freely choose the kind of computing resources on which it runs most efficiently. Codes might even be partitioned to map specific requirements of code-parts onto the best suited hardware. This paper presents for the first time measurements done by a real world scientific application demonstrating the performance gain achieved with this kind of code-partition approach. △ Less","10 April, 2019",https://arxiv.org/pdf/1904.05275
SPM-Tracker: Series-Parallel Matching for Real-Time Visual Object Tracking,Guangting Wang;Chong Luo;Zhiwei Xiong;Wenjun Zeng,"The greatest challenge facing visual object tracking is the simultaneous requirements on robustness and discrimination power. In this paper, we propose a SiamFC-based tracker, named SPM-Tracker, to tackle this challenge. The basic idea is to address the two requirements in two separate matching stages. Robustness is strengthened in the coarse matching (CM) stage through generalized training while discrimination power is enhanced in the fine matching (FM) stage through a distance learning network. The two stages are connected in series as the input proposals of the FM stage are generated by the CM stage. They are also connected in parallel as the matching scores and box location refinements are fused to generate the final results. This innovative series-parallel structure takes advantage of both stages and results in superior performance. The proposed SPM-Tracker, running at 120fps on GPU, achieves an AUC of 0.687 on OTB-100 and an EAO of 0.434 on VOT-16, exceeding other real-time trackers by a notable margin. △ Less","8 April, 2019",https://arxiv.org/pdf/1904.04452
Giving Attention to the Unexpected: Using Prosody Innovations in Disfluency Detection,Vicky Zayats;Mari Ostendorf,"Disfluencies in spontaneous speech are known to be associated with prosodic disruptions. However, most algorithms for disfluency detection use only word transcripts. Integrating prosodic cues has proved difficult because of the many sources of variability affecting the acoustic correlates. This paper introduces a new approach to extracting acoustic-prosodic cues using text-based distributional prediction of acoustic cues to derive vector z-score features (innovations). We explore both early and late fusion techniques for integrating text and prosody, showing gains over a high-accuracy text-only model. △ Less","8 April, 2019",https://arxiv.org/pdf/1904.04388
Achievement of Minimized Combinatorial Test Suite for Configuration-Aware Software Functional Testing Using the Cuckoo Search Algorithm,Bestoun S. Ahmed;Taib Sh. Abdulsamad;Moayad Y. Potrus,"Context: Software has become an innovative solution nowadays for many applications and methods in science and engineering. Ensuring the quality and correctness of software is challenging because each program has different configurations and input domains. To ensure the quality of software, all possible configurations and input combinations need to be evaluated against their expected outputs. However, this exhaustive test is impractical because of time and resource constraints due to the large domain of input and configurations. Thus, different sampling techniques have been used to sample these input domains and configurations. Objective: Combinatorial testing can be used to effectively detect faults in software-under-test. This technique uses combinatorial optimization concepts to systematically minimize the number of test cases by considering the combinations of inputs. This paper proposes a new strategy to generate combinatorial test suite by using cuckoo search concepts. Method: Cuckoo Search is used in the design and implementation of a strategy to construct optimized combinatorial sets. The strategy consists of different algorithms for construction. These algorithms are combined to serve the Cuckoo Search. Results: The efficiency and performance of the new technique were proven through different experiment sets. The effectiveness of the strategy is assessed by applying the generated test suites on a real-world case study for the purpose of functional testing. Conclusion: Results show that the generated test suites can detect faults effectively. In addition, the strategy also opens a new direction for the application of Cuckoo Search in the context of software engineering. △ Less","25 March, 2019",https://arxiv.org/pdf/1904.04348
AI Meets Austen: Towards Human-Robot Discussions of Literary Metaphor,Natalie Parde;Rodney D. Nielsen,"Artificial intelligence is revolutionizing formal education, fueled by innovations in learning assessment, content generation, and instructional delivery. Informal, lifelong learning settings have been the subject of less attention. We provide a proof-of-concept for an embodied book discussion companion, designed to stimulate conversations with readers about particularly creative metaphors in fiction literature. We collect ratings from 26 participants, each of whom discuss Jane Austen's ""Pride and Prejudice"" with the robot across one or more sessions, and find that participants rate their interactions highly. This suggests that companion robots could be an interesting entryway for the promotion of lifelong learning and cognitive exercise in future applications. △ Less","7 April, 2019",https://arxiv.org/pdf/1904.03713
A Logic Simplification Approach for Very Large Scale Crosstalk Circuit Designs,Md Arif Iqbal;Naveen Kumar Macha;Bhavana Tejaswini Repalle;Mostafizur Rahman,"Crosstalk computing, involving engineered interference between nanoscale metal lines, offers a fresh perspective to scaling through co-existence with CMOS. Through capacitive manipulations and innovative circuit style, not only primitive gates can be implemented, but custom logic cells such as an Adder, Subtractor can be implemented with huge gains. Our simulations show over 5x density and 2x power benefits over CMOS custom designs at 16nm [1]. This paper introduces the Crosstalk circuit style and a key method for large-scale circuit synthesis utilizing existing EDA tool flow. We propose to manipulate the CMOS synthesis flow by adding two extra steps: conversion of the gate-level netlist to Crosstalk implementation friendly netlist through logic simplification and Crosstalk gate mapping, and the inclusion of custom cell libraries for automated placement and layout. Our logic simplification approach first converts Cadence generated structured netlist to Boolean expressions and then uses the majority synthesis tool to obtain majority functions, which is further used to simplify functions for Crosstalk friendly implementations. We compare our approach of logic simplification to that of CMOS and majority logic-based approaches. Crosstalk circuits share some similarities to majority synthesis that are typically applied to Quantum Cellular Automata technology. However, our investigation shows that by closely following Crosstalk's core circuit styles, most benefits can be achieved. In the best case, our approach shows 36% density improvements over majority synthesis for MCNC benchmark. △ Less","5 April, 2019",https://arxiv.org/pdf/1904.03294
RADICAL-Cybertools: Middleware Building Blocks for Scalable Science,Vivek Balasubramanian;Shantenu Jha;Andre Merzky;Matteo Turilli,"RADICAL-Cybertools (RCT) are a set of software systems that serve as middleware to develop efficient and effective tools for scientific computing. Specifically, RCT enable executing many-task applications at extreme scale and on a variety of computing infrastructures. RCT are building blocks, designed to work as stand-alone systems, integrated among themselves or integrated with third-party systems. RCT enables innovative science in multiple domains, including but not limited to biophysics, climate science and particle physics, consuming hundreds of millions of core hours. This paper provides an overview of RCT systems, their impact, and the architectural principles and software engineering underlying RCT △ Less","5 April, 2019",https://arxiv.org/pdf/1904.03085
DDM: Fast Near-Optimal Multi-Robot Path Planning using Diversified-Path and Optimal Sub-Problem Solution Database Heuristics,Shuai D. Han;Jingjin Yu,"We propose a novel centralized and decoupled algorithm, DDM, for solving multi-robot path planning problems in grid graphs, targeting on-demand and automated warehouse-like settings. Two settings are studied: a traditional one whose objective is to move a set of robots from their respective initial vertices to the goal vertices as quickly as possible, and a dynamic one which requires frequent re-planning to accommodate for goal configuration adjustments. Among other techniques, DDM is mainly enabled through exploiting two innovative heuristics: path diversification and optimal sub-problem solution databases. The two heuristics attack two distinct phases of a decoupling-based planner: while path diversification allows the more effective use of the entire workspace for robot travel, optimal sub-problem solution databases facilitate the fast resolution of local path conflicts. Extensive evaluation demonstrates that DDM achieves high levels of scalability and high levels of solution optimality. △ Less","15 December, 2019",https://arxiv.org/pdf/1904.02598
Generic Multiview Visual Tracking,Minye Wu;Haibin Ling;Ning Bi;Shenghua Gao;Hao Sheng;Jingyi Yu,"Recent progresses in visual tracking have greatly improved the tracking performance. However, challenges such as occlusion and view change remain obstacles in real world deployment. A natural solution to these challenges is to use multiple cameras with multiview inputs, though existing systems are mostly limited to specific targets (e.g. human), static cameras, and/or camera calibration. To break through these limitations, we propose a generic multiview tracking (GMT) framework that allows camera movement, while requiring neither specific object model nor camera calibration. A key innovation in our framework is a cross-camera trajectory prediction network (TPN), which implicitly and dynamically encodes camera geometric relations, and hence addresses missing target issues such as occlusion. Moreover, during tracking, we assemble information across different cameras to dynamically update a novel collaborative correlation filter (CCF), which is shared among cameras to achieve robustness against view change. The two components are integrated into a correlation filter tracking framework, where the features are trained offline using existing single view tracking datasets. For evaluation, we first contribute a new generic multiview tracking dataset (GMTD) with careful annotations, and then run experiments on GMTD and the PETS2009 datasets. On both datasets, the proposed GMT algorithm shows clear advantages over state-of-the-art ones. △ Less","4 April, 2019",https://arxiv.org/pdf/1904.02553
Graph clustering in industrial networks,V. Bouet;A. Y. Klimenko,"The present work investigates clustering of a graph-based representation of industrial connections derived from international trade data by Hidalgo et al (2007) and confirms existence of around ten industrial clusters that are reasonably consistent with expected historical patterns of diffusion of innovation and technology. This supports the notion that technological development occurs in sequential innovation waves. The clustering method developed in this work follows conceptual ideas of Lambiotte and Barahona (2009), who suggested to use random walk to assess a hierarchical structure of network communities where different levels of the hierarchy correspond to different diffusion times. We, however, implement these ideas differently to match physics of the problem under consideration and introduce a hierarchal clustering procedure that is combined with convenient resorting of the elements. An equivalent spectral interpretation of the clustering is also given and discussed in the paper. △ Less","3 April, 2019",https://arxiv.org/pdf/1904.02536
"Group Maturity and Agility, Are They Connected? - A Survey Study",Lucas Gren;Richard Torkar;Robert Feldt,"The focus on psychology has increased within software engineering due to the project management innovation ""agile development processes"". The agile methods do not explicitly consider group development aspects; they simply assume what is described in group psychology as mature groups. This study was conducted with 45 employees and their twelve managers (N=57) from two SAP customers in the US that were working with agile methods, and the data were collected via an online survey. The selected Agility measurement was correlated to a Group Development measurement and showed significant convergent validity, i.e., a more mature team is also a more agile team. This means that the agile methods probably would benefit from taking group development into account when its practices are being introduced. △ Less","4 April, 2019",https://arxiv.org/pdf/1904.02451
Temporal similarity metrics for latent network reconstruction: The role of time-lag decay,Hao Liao;Ming-Kai Liu;Manuel Sebastian Mariani;Mingyang Zhou;Xingtong Wu,"When investigating the spreading of a piece of information or the diffusion of an innovation, we often lack information on the underlying propagation network. Reconstructing the hidden propagation paths based on the observed diffusion process is a challenging problem which has recently attracted attention from diverse research fields. To address this reconstruction problem, based on static similarity metrics commonly used in the link prediction literature, we introduce new node-node temporal similarity metrics. The new metrics take as input the time-series of multiple independent spreading processes, based on the hypothesis that two nodes are more likely to be connected if they were often infected at similar points in time. This hypothesis is implemented by introducing a time-lag function which penalizes distant infection times. We find that the choice of this time-lag strongly affects the metrics' reconstruction accuracy, depending on the network's clustering coefficient and we provide an extensive comparative analysis of static and temporal similarity metrics for network reconstruction. Our findings shed new light on the notion of similarity between pairs of nodes in complex networks. △ Less","4 April, 2019",https://arxiv.org/pdf/1904.02413
MMED: A Multi-domain and Multi-modality Event Dataset,Zhenguo Yang;Zehang Lin;Min Cheng;Qing Li;Wenyin Liu,"In this work, we construct and release a multi-domain and multi-modality event dataset (MMED), containing 25,165 textual news articles collected from hundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and 76,516 image posts shared on Flickr social media, which are annotated according to 412 real-world events. The dataset is collected to explore the problem of organizing heterogeneous data contributed by professionals and amateurs in different data domains, and the problem of transferring event knowledge obtained from one data domain to heterogeneous data domain, thus summarizing the data with different contributors. We hope that the release of the MMED dataset can stimulate innovate research on related challenging problems, such as event discovery, cross-modal (event) retrieval, and visual question answering, etc. △ Less","9 April, 2019",https://arxiv.org/pdf/1904.02354
Processing Tweets for Cybersecurity Threat Awareness,Fernando Alves;Aurélien Bettini;Pedro M. Ferreira;Alysson Bessani,"Receiving timely and relevant security information is crucial for maintaining a high-security level on an IT infrastructure. This information can be extracted from Open Source Intelligence published daily by users, security organisations, and researchers. In particular, Twitter has become an information hub for obtaining cutting-edge information about many subjects, including cybersecurity. This work proposes SYNAPSE, a Twitter-based streaming threat monitor that generates a continuously updated summary of the threat landscape related to a monitored infrastructure. Its tweet-processing pipeline is composed of filtering, feature extraction, binary classification, an innovative clustering strategy, and generation of Indicators of Compromise (IoCs). A quantitative evaluation considering all tweets from 80 accounts over more than 8 months (over 195.000 tweets), shows that our approach timely and successfully finds the majority of security-related tweets concerning an example IT infrastructure (true positive rate above 90%), incorrectly selects a small number of tweets as relevant (false positive rate under 10%), and summarises the results to very few IoCs per day. A qualitative evaluation of the IoCs generated by SYNAPSE demonstrates their relevance (based on the CVSS score and the availability of patches or exploits), and timeliness (based on threat disclosure dates from NVD). △ Less","3 April, 2019",https://arxiv.org/pdf/1904.02072
Rinascimento: Optimising Statistical Forward Planning Agents for Playing Splendor,Ivan Bravi;Simon Lucas;Diego Perez-Liebana;Jialin Liu,"Game-based benchmarks have been playing an essential role in the development of Artificial Intelligence (AI) techniques. Providing diverse challenges is crucial to push research toward innovation and understanding in modern techniques. Rinascimento provides a parameterised partially-observable multiplayer card-based board game, these parameters can easily modify the rules, objectives and items in the game. We describe the framework in all its features and the game-playing challenge providing baseline game-playing AIs and analysis of their skills. We reserve to agents' hyper-parameter tuning a central role in the experiments highlighting how it can heavily influence the performance. The base-line agents contain several additional contribution to Statistical Forward Planning algorithms. △ Less","3 April, 2019",https://arxiv.org/pdf/1904.01883
Multi-Modal Generative Adversarial Network for Short Product Title Generation in Mobile E-Commerce,Jian-Guo Zhang;Pengcheng Zou;Zhao Li;Yao Wan;Xiuming Pan;Yu Gong;Philip S. Yu,"Nowadays, more and more customers browse and purchase products in favor of using mobile E-Commerce Apps such as Taobao and Amazon. Since merchants are usually inclined to describe redundant and over-informative product titles to attract attentions from customers, it is important to concisely display short product titles on limited screen of mobile phones. To address this discrepancy, previous studies mainly consider textual information of long product titles and lacks of human-like view during training and evaluation process. In this paper, we propose a Multi-Modal Generative Adversarial Network (MM-GAN) for short product title generation in E-Commerce, which innovatively incorporates image information and attribute tags from product, as well as textual information from original long titles. MM-GAN poses short title generation as a reinforcement learning process, where the generated titles are evaluated by the discriminator in a human-like view. Extensive experiments on a large-scale E-Commerce dataset demonstrate that our algorithm outperforms other state-of-the-art methods. Moreover, we deploy our model into a real-world online E-Commerce environment and effectively boost the performance of click through rate and click conversion rate by 1.66% and 1.87%, respectively. △ Less","2 April, 2019",https://arxiv.org/pdf/1904.01735
Out of Site: Empowering a New Approach to Online Boycotts,H. Li;B. Alarcon;S. M. Espinosa;B;Hecht,"GrabYourWallet, #boycottNRA and other online boycott campaigns have attracted substantial public interest in recent months. However, a number of significant challenges are preventing online boycotts from reaching their potential. In particular, complex webs of brands and subsidiaries can make it difficult for participants to conform to the goals of a boycott. Similarly, participants and organizers have limited visibility into a boycott's progress. This affects their ability to use sociotechnical innovations from social computing to incentivize participation. To address these challenges, this paper makes a system contribution: a new boycott tool called Out of Site. Out of Site uses lightweight automation to remove obstacles to successful online boycotts. We describe the design challenges associated with Out of Site and report results from two phases of deployment with the GrabYourWallet and Stop Animal Testing boycott communities. Our findings highlight the potential of boycott-assisting technologies and inform the design of this new class of technologies. Finally, like is the case for many systems in social computing, while we designed Out of Site for pro-social uses, there are a number of easily predictable ways in which the system can be leveraged for anti-social purposes (e.g. exacerbating filter bubble issues, empowering boycotts of businesses owned by racial, ethnic, and religious minorities). As such, we developed for this project a new, very straightforward design approach that treats preventing these anti-social uses as a top-tier design concern. This approach stands in contrast to the status quo of ignoring potential anti-social uses and/or considering them to be a secondary design priority. We discuss how our simple approach may help other research projects reduce their potential negative impacts with minimal burden. △ Less","2 April, 2019",https://arxiv.org/pdf/1904.01688
Lab Hackathons to Overcome Laboratory Equipment Shortages in Africa: Opportunities and Challenges,Helena Webb;Jason R. C. Nurse;Louise Bezuidenhout;Marina Jirotka,"Equipment shortages in Africa undermine Science, Technology, Engineering and Mathematics (STEM) Education. We have pioneered the LabHackathon (LabHack): a novel initiative that adapts the conventional hackathon and draws on insights from the Open Hardware movement and Responsible Research and Innovation (RRI). LabHacks are fun, educational events that challenge student participants to build frugal and reproducible pieces of laboratory equipment. Completed designs are then made available to others. LabHacks can therefore facilitate the open and sustainable design of laboratory equipment, in situ, in Africa. In this case study we describe the LabHackathon model, discuss its application in a pilot event held in Zimbabwe and outline the opportunities and challenges it presents. △ Less","2 April, 2019",https://arxiv.org/pdf/1904.01687
Exploring Randomly Wired Neural Networks for Image Recognition,Saining Xie;Alexander Kirillov;Ross Girshick;Kaiming He,"Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets and DenseNets is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design. △ Less","8 April, 2019",https://arxiv.org/pdf/1904.01569
Context-Aware Misbehavior Detection Scheme for Vehicular Ad Hoc Networks using Sequential Analysis of the Temporal and Spatial Correlation of the Cooperative Awareness Messages,Fuad A. Ghaleb,"Vehicular ad hoc Networks (VANETs) are emerged mainly to improve road safety, traffic efficiency, and passenger comfort. The performance of most VANET applications relies on the availability of accurate and recent mobility-information, shared by neighboring vehicles. However, sharing false mobility information can disrupt any potential VANET application. Misbehavior detection is an important security component. However, existing misbehavior detection solutions lack considering the high dynamicity of vehicular context which leads to low detection accuracy and high false alarms. The use of predefined and static security thresholds are the main drawbacks of the existing solutions. In this paper, a context-aware misbehavior detection scheme (CAMDS) is proposed using sequential analysis of temporal and spatial properties of mobility information. A dynamic context reference is constructed online and timely updated using statistical techniques. Firstly, the Kalman filter algorithm is used to track the mobility information received from neighboring vehicles. Then, the innovation errors of the Kalman filter are utilized to construct a temporal consistency assessment model for each vehicle using Box-plot. Then, the Hampel filter is used to construct a spatial consistency assessment model that represents the context reference model. Similarly, plausibility assessment reference models are built online and timely updated using the Hampel filter and by utilizing the consistency assessment reference model of neighboring information. Finally, a message is classified as suspicious if its consistency and plausibility scores deviate much from the context reference model. The proposed context-aware scheme achieved a 73% reduction in false Alarm rate while it achieves a 37% improvement in the detection rate. This proves the effectiveness of the proposed scheme compared with the existing static solutions. △ Less","2 April, 2019",https://arxiv.org/pdf/1904.01392
"DeepCloud. The Application of a Data-driven, Generative Model in Design",Ardavan Bidgoli;Pedro Veloso,"Generative systems have a significant potential to synthesize innovative design alternatives. Still, most of the common systems that have been adopted in design require the designer to explicitly define the specifications of the procedures and in some cases the design space. In contrast, a generative system could potentially learn both aspects through processing a database of existing solutions without the supervision of the designer. To explore this possibility, we review recent advancements of generative models in machine learning and current applications of learning techniques in design. Then, we describe the development of a data-driven generative system titled DeepCloud. It combines an autoencoder architecture for point clouds with a web-based interface and analog input devices to provide an intuitive experience for data-driven generation of design alternatives. We delineate the implementation of two prototypes of DeepCloud, their contributions, and potentials for generative design. △ Less","1 April, 2019",https://arxiv.org/pdf/1904.01083
Some variations on Lyndon words,Francesco Dolce;Antonio Restivo;Christophe Reutenauer,"In this paper we compare two finite words u and v by the lexicographical order of the infinite words u^ω and v^ω. Informally, we say that we compare u and v by the infinite order. We show several properties of Lyndon words expressed using this infinite order. The innovative aspect of this approach is that it allows to take into account also non trivial conditions on the prefixes of a word, instead that only on the suffixes. In particular, we derive a result of Ufnarovskij [V. Ufnarovskij, ""Combinatorial and asymptotic methods in algebra"", 1995] that characterizes a Lyndon word as a word which is greater, with respect to the infinite order, than all its prefixes. Motivated by this result, we introduce the prefix standard permutation of a Lyndon word and the corresponding (left) Cartesian tree. We prove that the left Cartesian tree is equal to the left Lyndon tree, defined by the left standard factorization of Viennot [G. Viennot, ""Algèbres de Lie libres et monoïdes libres"", 1978]. This result is dual with respect to a theorem of Hohlweg and Reutenauer [C. Hohlweg and C. Reutenauer, ""Lyndon words, permutations and trees"", 2003]. △ Less","1 April, 2019",https://arxiv.org/pdf/1904.00954
A Convolutional Neural Network for Language-Agnostic Source Code Summarization,Jessica Moore;Ben Gelman;David Slater,"Descriptive comments play a crucial role in the software engineering process. They decrease development time, enable better bug detection, and facilitate the reuse of previously written code. However, comments are commonly the last of a software developer's priorities and are thus either insufficient or missing entirely. Automatic source code summarization may therefore have the ability to significantly improve the software development process. We introduce a novel encoder-decoder model that summarizes source code, effectively writing a comment to describe the code's functionality. We make two primary innovations beyond current source code summarization models. First, our encoder is fully language-agnostic and requires no complex input preprocessing. Second, our decoder has an open vocabulary, enabling it to predict any word, even ones not seen in training. We demonstrate results comparable to state-of-the-art methods on a single-language data set and provide the first results on a data set consisting of multiple programming languages. △ Less","29 March, 2019",https://arxiv.org/pdf/1904.00805
Adaptive sampling of time-space signals in a reproducing kernel subspace of mixed Lebesgue space,Yingchun Jiang;Wenchang Sun,"The Mixed Lebesgue space is a suitable tool for modelling and measuring signals living in time-space domains. And sampling in such spaces plays an important role for processing high-dimensional time-varying signals. In this paper, we first define reproducing kernel subspaces of mixed Lebesgue spaces. Then, we study the frame properties and show that the reproducing kernel subspace has finite rate of innovation. Finally, we propose a semi-adaptive sampling scheme for time-space signals in a reproducing kernel subspace, where the sampling in time domain is conducted by a time encoding machine. Two kinds of timing sampling methods are considered and the corresponding iterative approximation algorithms with exponential convergence are given. △ Less","1 April, 2019",https://arxiv.org/pdf/1904.00727
Linguistic generalization and compositionality in modern artificial neural networks,Marco Baroni,"In the last decade, deep artificial neural networks have achieved astounding performance in many natural language processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: Are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language. △ Less","26 June, 2019",https://arxiv.org/pdf/1904.00157
Enabling decentral collaborative innovation processes - a web based real time collaboration platform,Matthias Joiko;Florian Kohnen;Kevin Lapinski;Houda Moudrik;Irawan Nurhas;Florian Paproth;Jan M. Pawlowski,The main goal of this paper is to define a collaborative innovation process as well as a supporting tool. It is motivated through the increasing competition on global markets and the resultant propagation of decentralized projects with a high demand of innovative collaboration in global contexts. It bases on a project accomplished by the author group. A detailed literature review and the action design research methodology of the project led to an enhanced process model for decentral collaborative innovation processes and a basic realization of a browser based real time tool to enable these processes. The initial evaluation in a practical distributed setting has shown that the created tool is a useful way to support collaborative innovation processes. △ Less,"29 March, 2019",https://arxiv.org/pdf/1904.00112
Towards Brain-inspired System: Deep Recurrent Reinforcement Learning for Simulated Self-driving Agent,Jieneng Chen;Jingye Chen;Ruiming Zhang;Xiaobin Hu,"An effective way to achieve intelligence is to simulate various intelligent behaviors in the human brain. In recent years, bio-inspired learning methods have emerged, and they are different from the classical mathematical programming principle. In the perspective of brain inspiration, reinforcement learning has gained additional interest in solving decision-making tasks as increasing neuroscientific research demonstrates that significant links exist between reinforcement learning and specific neural substrates. Because of the tremendous research that focuses on human brains and reinforcement learning, scientists have investigated how robots can autonomously tackle complex tasks in the form of a self-driving agent control in a human-like way. In this study, we propose an end-to-end architecture using novel deep-Q-network architecture in conjunction with a recurrence to resolve the problem in the field of simulated self-driving. The main contribution of this study is that we trained the driving agent using a brain-inspired trial-and-error technique, which was in line with the real world situation. Besides, there are three innovations in the proposed learning network: raw screen outputs are the only information which the driving agent can rely on, a weighted layer that enhances the differences of the lengthy episode, and a modified replay mechanism that overcomes the problem of sparsity and accelerates learning. The proposed network was trained and tested under a third-partied OpenAI Gym environment. After training for several episodes, the resulting driving agent performed advanced behaviors in the given scene. We hope that in the future, the proposed brain-inspired learning system would inspire practicable self-driving control solutions. △ Less","29 March, 2019",https://arxiv.org/pdf/1903.12517
"In Search of Meaning: Lessons, Resources and Next Steps for Computational Analysis of Financial Discourse",Mahmoud El-Haj;Paul Rayson;Martin Walker;Steven Young;Vasiliki Simaki,"We critically assess mainstream accounting and finance research applying methods from computational linguistics (CL) to study financial discourse. We also review common themes and innovations in the literature and assess the incremental contributions of work applying CL methods over manual content analysis. Key conclusions emerging from our analysis are: (a) accounting and finance research is behind the curve in terms of CL methods generally and word sense disambiguation in particular; (b) implementation issues mean the proposed benefits of CL are often less pronounced than proponents suggest; (c) structural issues limit practical relevance; and (d) CL methods and high quality manual analysis represent complementary approaches to analyzing financial discourse. We describe four CL tools that have yet to gain traction in mainstream AF research but which we believe offer promising ways to enhance the study of meaning in financial discourse. The four tools are named entity recognition (NER), summarization, semantics and corpus linguistics. △ Less","28 March, 2019",https://arxiv.org/pdf/1903.12271
Differential Geometric Foundations for Power Flow Computations,Franz-Erich Wolter;Benjamin Berger,"This paper aims to systematically and comprehensively initiate a foundation for using concepts from computational differential geometry as instruments for power flow computing and research. At this point we focus our discussion on the static case, with power flow equations given by quadratic functions defined on voltage space with values in power space; both spaces have real Euclidean coordinates. Central issue is a differential geometric analysis of the power flow solution space boundary (SSB) both in voltage and in power space. We present different methods for computing tangent vectors, tangent planes and normals of the SSB and the normals' derivatives. Using the latter we compute normal and principal curvatures. All this is needed for tracing the orthogonal projection of curves in voltage and power space onto the SSB for points on the SSB cosest to given points on the curves, thus obtaining estimates for the distance to the SSB. Furthermore, we present a new high precision continuation method for power flow solutions. We also compute geodesics on the SSB or an implicitly defined submanfold thereof and, used to define geodesic coordinates together with their Jacobians on the manifolds. These computations might be the most innovative and most significant contribution of this paper, because this concept provides a comprehensive coordinate system for sub many folds defined by implicit equations. Therefore while moving on geodesics described by the geodesic coordinates of the sub manifold at hand we get, via systematic navigation guided by geodesic coordinates, access to all feasible operation points of the system. We propose some applications and show some properties of the Jacobian of the power flow map. △ Less","25 June, 2019",https://arxiv.org/pdf/1903.11131
Blockchain Solutions for Multi-Agent Robotic Systems: Related Work and Open Questions,Ilya Afanasyev;Alexander Kolotov;Ruslan Rezin;Konstantin Danilov;Alexey Kashevnik;Vladimir Jotsov,"The possibilities of decentralization and immutability make blockchain probably one of the most breakthrough and promising technological innovations in recent years. This paper presents an overview, analysis, and classification of possible blockchain solutions for practical tasks facing multi-agent robotic systems. The paper discusses blockchain-based applications that demonstrate how distributed ledger can be used to extend the existing number of research platforms and libraries for multi-agent robotic systems. △ Less","26 March, 2019",https://arxiv.org/pdf/1903.11041
Apache Hive: From MapReduce to Enterprise-grade Big Data Warehousing,Jesús Camacho-Rodríguez;Ashutosh Chauhan;Alan Gates;Eugene Koifman;Owen O'Malley;Vineet Garg;Zoltan Haindrich;Sergey Shelukhin;Prasanth Jayachandran;Siddharth Seth;Deepak Jaiswal;Slim Bouguerra;Nishant Bangarwa;Sankar Hariappan;Anishek Agarwal;Jason Dere;Daniel Dai;Thejas Nair;Nita Dembla;Gopal Vijayaraghavan;Günther Hagleitner,"Apache Hive is an open-source relational database system for analytic big-data workloads. In this paper we describe the key innovations on the journey from batch tool to fully fledged enterprise data warehousing system. We present a hybrid architecture that combines traditional MPP techniques with more recent big data and cloud concepts to achieve the scale and performance required by today's analytic applications. We explore the system by detailing enhancements along four main axis: Transactions, optimizer, runtime, and federation. We then provide experimental results to demonstrate the performance of the system for typical workloads and conclude with a look at the community roadmap. △ Less","26 March, 2019",https://arxiv.org/pdf/1903.10970
Data Protection by Design for Cybersecurity Systems in a Smart Home Environment,Olga Gkotsopoulou;Elisavet Charalambous;Konstantinos Limniotis;Paul Quinn;Dimitris Kavallieros;Gohar Sargsyan;Stavros Shiaeles;Nicholas Kolokotronis,"The present paper deals with the elucidation and implementation of the Data Protection by Design (DPbD) principle as recently introduced in the European Union data protection law, specifically with regards to cybersecurity systems in a Smart Home environment, both from a legal and a technical perspective. Starting point constitutes the research conducted in the Cyber-Trust project, which endeavours the development of an innovative and customisable cybersecurity platform for cyber-threat intelligence gathering, detection and mitigation within the Internet of Things ecosystem. During the course of the paper, the requirements of DPbD with regards to the conceptualisation, design and actual development of the system are presented as prescribed in law. These requirements are then translated into technical solutions, as envisaged in the Cyber-Trust system. For trade-offs are not foreign to the DPbD context, technical limitations and legal challenges are also discussed in this interdisciplinary dialogue. △ Less","26 March, 2019",https://arxiv.org/pdf/1903.10778
Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud,Xinshuo Weng;Kris Kitani,"Monocular 3D scene understanding tasks, such as object size estimation, heading angle estimation and 3D localization, is challenging. Successful modern day methods for 3D scene understanding require the use of a 3D sensor. On the other hand, single image based methods have significantly worse performance. In this work, we aim at bridging the performance gap between 3D sensing and 2D sensing for 3D object detection by enhancing LiDAR-based algorithms to work with single image input. Specifically, we perform monocular depth estimation and lift the input image to a point cloud representation, which we call pseudo-LiDAR point cloud. Then we can train a LiDAR-based 3D detection network with our pseudo-LiDAR end-to-end. Following the pipeline of two-stage 3D detection algorithms, we detect 2D object proposals in the input image and extract a point cloud frustum from the pseudo-LiDAR for each proposal. Then an oriented 3D bounding box is detected for each frustum. To handle the large amount of noise in the pseudo-LiDAR, we propose two innovations: (1) use a 2D-3D bounding box consistency constraint, adjusting the predicted 3D bounding box to have a high overlap with its corresponding 2D proposal after projecting onto the image; (2) use the instance mask instead of the bounding box as the representation of 2D proposals, in order to reduce the number of points not belonging to the object in the point cloud frustum. Through our evaluation on the KITTI benchmark, we achieve the top-ranked performance on both bird's eye view and 3D object detection among all monocular methods, effectively quadrupling the performance over previous state-of-the-art. Our code is available at https://github.com/xinshuoweng/Mono3D_PLiDAR. △ Less","30 August, 2019",https://arxiv.org/pdf/1903.09847
Rods and Rings: Soft Subdivision Planner for R^3 x S^2,Ching-Hsiang Hsu;Yi-Jen Chiang;Chee Yap,"We consider path planning for a rigid spatial robot moving amidst polyhedral obstacles. Our robot is either a rod or a ring. Being axially-symmetric, their configuration space is R^3 x S^2 with 5 degrees of freedom (DOF). Correct, complete and practical path planning for such robots is a long standing challenge in robotics. While the rod is one of the most widely studied spatial robots in path planning, the ring seems to be new, and a rare example of a non-simply-connected robot. This work provides rigorous and complete algorithms for these robots with theoretical guarantees. We implemented the algorithms in our open-source Core Library. Experiments show that they are practical, achieving near real-time performance. We compared our planner to state-of-the-art sampling planners in OMPL. Our subdivision path planner is based on the twin foundations of ε-exactness and soft predicates. Correct implementation is relatively easy. The technical innovations include subdivision atlases for S^2, introduction of Σ_2 representations for footprints, and extensions of our feature-based technique for ""opening up the blackbox of collision detection"". △ Less","6 June, 2019",https://arxiv.org/pdf/1903.09416
Individualized Multilayer Tensor Learning with An Application in Imaging Analysis,Xiwei Tang;Xuan Bi;Annie Qu,"This work is motivated by multimodality breast cancer imaging data, which is quite challenging in that the signals of discrete tumor-associated microvesicles (TMVs) are randomly distributed with heterogeneous patterns. This imposes a significant challenge for conventional imaging regression and dimension reduction models assuming a homogeneous feature structure. We develop an innovative multilayer tensor learning method to incorporate heterogeneity to a higher-order tensor decomposition and predict disease status effectively through utilizing subject-wise imaging features and multimodality information. Specifically, we construct a multilayer decomposition which leverages an individualized imaging layer in addition to a modality-specific tensor structure. One major advantage of our approach is that we are able to efficiently capture the heterogeneous spatial features of signals that are not characterized by a population structure as well as integrating multimodality information simultaneously. To achieve scalable computing, we develop a new bi-level block improvement algorithm. In theory, we investigate both the algorithm convergence property, tensor signal recovery error bound and asymptotic consistency for prediction model estimation. We also apply the proposed method for simulated and human breast cancer imaging data. Numerical results demonstrate that the proposed method outperforms other existing competing methods. △ Less","21 March, 2019",https://arxiv.org/pdf/1903.08871
Fault-Tolerant Nanosatellite Computing on a Budget,Christian M. Fuchs;Nadia Murillo;Aske Plaat;Erik Van der Kouwe;Daniel Harsono;Todor Stefanov,"Micro- and nanosatellites have become popular platforms for a variety of commercial and scientific applications, but today are considered suitable mainly for short and low-priority space missions due to their low reliability. In part, this can be attributed to their reliance upon cheap, low-feature size, COTS components originally designed for embedded and mobile-market applications, for which traditional hardware-voting concepts are ineffective. Software-fault-tolerance concepts have been shown effective for such systems, but have largely been ignored by the space industry due to low maturity, as most have only been researched in theory. In practice, designers of payload instruments and miniaturized satellites are usually forced to sacrifice reliability in favor deliver the level of performance necessary for cutting-edge science and innovative commercial applications. Thus, we developed a software-fault-tolerance-approach based upon thread-level coarse-grain lockstep, which was validated using fault-injection. To offer strong long-term fault coverage, our architecture is implemented as tiled MPSoC on an FPGA, utilizing partial reconfiguration, as well as mixed criticality. This architecture can satisfy the high performance requirements of current and future scientific and commercial space missions at very low cost, while offering the strong fault-coverage guarantees necessary for platform control even for missions with a long duration. This architecture was developed for a 4-year ESA project. Together with two industrial partners, we are developing a prototype to then undergo radiation testing. △ Less","20 March, 2019",https://arxiv.org/pdf/1903.08781
When redundancy is useful: A Bayesian approach to 'overinformative' referring expressions,Judith Degen;Robert D. Hawkins;Caroline Graf;Elisa Kreiss;Noah D. Goodman,"Referring is one of the most basic and prevalent uses of language. How do speakers choose from the wealth of referring expressions at their disposal? Rational theories of language use have come under attack for decades for not being able to account for the seemingly irrational overinformativeness ubiquitous in referring expressions. Here we present a novel production model of referring expressions within the Rational Speech Act framework that treats speakers as agents that rationally trade off cost and informativeness of utterances. Crucially, we relax the assumption that informativeness is computed with respect to a deterministic Boolean semantics, in favor of a non-deterministic continuous semantics. This innovation allows us to capture a large number of seemingly disparate phenomena within one unified framework: the basic asymmetry in speakers' propensity to overmodify with color rather than size; the increase in overmodification in complex scenes; the increase in overmodification with atypical features; and the increase in specificity in nominal reference as a function of typicality. These findings cast a new light on the production of referring expressions: rather than being wastefully overinformative, reference is usefully redundant. △ Less","10 December, 2019",https://arxiv.org/pdf/1903.08237
How to Make Swarms Open-Ended? Evolving Collective Intelligence Through a Constricted Exploration of Adjacent Possibles,Olaf Witkowski;Takashi Ikegami,"We propose an approach of open-ended evolution via the simulation of swarm dynamics. In nature, swarms possess remarkable properties, which allow many organisms, from swarming bacteria to ants and flocking birds, to form higher-order structures that enhance their behavior as a group. Swarm simulations highlight three important factors to create novelty and diversity: (a) communication generates combinatorial cooperative dynamics, (b) concurrency allows for separation of timescales, and (c) complexity and size increases push the system towards transitions in innovation. We illustrate these three components in a model computing the continuous evolution of a swarm of agents. The results, divided in three distinct applications, show how emergent structures are capable of filtering information through the bottleneck of their memory, to produce meaningful novelty and diversity within their simulated environment. △ Less","19 March, 2019",https://arxiv.org/pdf/1903.08228
Dynamic Deep Networks for Retinal Vessel Segmentation,Aashis Khanal;Rolando Estrada,"Segmenting the retinal vasculature entails a trade-off between how much of the overall vascular structure we identify vs. how precisely we segment individual vessels. In particular, state-of-the-art methods tend to under-segment faint vessels, as well as pixels that lie on the edges of thicker vessels. Thus, they underestimate the width of individual vessels, as well as the ratio of large to small vessels. More generally, many crucial bio-markers---including the artery-vein (AV) ratio, branching angles, number of bifurcation, fractal dimension, tortuosity, vascular length-to-diameter ratio and wall-to-lumen length---require precise measurements of individual vessels. To address this limitation, we propose a novel, stochastic training scheme for deep neural networks that better classifies the faint, ambiguous regions of the image. Our approach relies on two key innovations. First, we train our deep networks with dynamic weights that fluctuate during each training iteration. This stochastic approach forces the network to learn a mapping that robustly balances precision and recall. Second, we decouple the segmentation process into two steps. In the first half of our pipeline, we estimate the likelihood of every pixel and then use these likelihoods to segment pixels that are clearly vessel or background. In the latter part of our pipeline, we use a second network to classify the ambiguous regions in the image. Our proposed method obtained state-of-the-art results on five retinal datasets---DRIVE, STARE, CHASE-DB, AV-WIDE, and VEVIO---by learning a robust balance between false positive and false negative rates. In addition, we are the first to report segmentation results on the AV-WIDE dataset, and we have made the ground-truth annotations for this dataset publicly available. △ Less","27 March, 2019",https://arxiv.org/pdf/1903.07803
Between Promise and Performance: Science and Technology Policy Implementation through Network Governance,Travis A. Whetsell;Michael J. Leiblein;Caroline S. Wagner,"This research analyzes the effects of U.S. science and technology policy on the technological performance of organizations in a global strategic alliance network. During the mid-1980s the U.S. semiconductor industry appeared to be collapsing. Industry leaders and policymakers moved to support and protect U.S. firms by creating a program called Sematech. While many scholars regard Sematech as a success, how the program succeeded remains unclear. This study re-contextualizes Sematech as a network administrative organization which lowered cooperation costs and enhanced resource combination for innovation at the cutting edge. This study combines network analysis and longitudinal regression techniques to test the effects of public policy on organizational network position and technological performance in an unbalanced panel of semiconductor firms between 1986 and 2001. This research suggests governments might achieve policy through inter-organizational innovations aimed at the development and administration of robust governance networks. △ Less","13 August, 2019",https://arxiv.org/pdf/1903.07730
Security and Privacy on Blockchain,Rui Zhang;Rui Xue;Ling Liu,"Blockchain offers an innovative approach to storing information, executing transactions, performing functions, and establishing trust in an open environment. Many consider blockchain as a technology breakthrough for cryptography and cybersecurity, with use cases ranging from globally deployed cryptocurrency systems like Bitcoin, to smart contracts, smart grids over the Internet of Things, and so forth. Although blockchain has received growing interests in both academia and industry in the recent years, the security and privacy of blockchains continue to be at the center of the debate when deploying blockchain in different applications. This paper presents a comprehensive overview of the security and privacy of blockchain. To facilitate the discussion, we first introduce the notion of blockchains and its utility in the context of Bitcoin like online transactions. Then we describe the basic security properties that are supported as the essential requirements and building blocks for Bitcoin like cryptocurrency systems, followed by presenting the additional security and privacy properties that are desired in many blockchain applications. Finally, we review the security and privacy techniques for achieving these security properties in blockchain-based systems, including representative consensus algorithms, hash chained storage, mixing protocols, anonymous signatures, non-interactive zero-knowledge proof, and so forth. We conjecture that this survey can help readers to gain an in-depth understanding of the security and privacy of blockchain with respect to concept, attributes, techniques and systems. △ Less","16 August, 2019",https://arxiv.org/pdf/1903.07602
"Quantifying dynamics of failure across science, startups, and security",Yian Yin;Yang Wang;James A. Evans;Dashun Wang,"Human achievements are often preceded by repeated attempts that initially fail, yet little is known about the mechanisms governing the dynamics of failure. Here, building on the rich literature on innovation, human dynamics and learning, we develop a simple one-parameter model that mimics how successful future attempts build on those past. Analytically solving this model reveals a phase transition that separates dynamics of failure into regions of stagnation or progression, predicting that near the critical threshold, agents who share similar characteristics and learning strategies may experience fundamentally different outcomes following failures. Below the critical point, we see those who explore disjoint opportunities without a pattern of improvement, and above it, those who exploit incremental refinements to systematically advance toward success. The model makes several empirically testable predictions, demonstrating that those who eventually succeed and those who do not may be initially similar, yet are characterized by fundamentally distinct failure dynamics in terms of the efficiency and quality of each subsequent attempt. We collected large-scale data from three disparate domains, tracing repeated attempts by (i) NIH investigators to fund their research, (ii) innovators to successfully exit their startup ventures, and (iii) terrorist organizations to post casualties in violent attacks, finding broadly consistent empirical support across all three domains. Together, our findings unveil identifiable yet previously unknown early signals that allow us to identify failure dynamics that will lead to ultimate victory or defeat. Given the ubiquitous nature of failures and the paucity of quantitative approaches to understand them, these results represent a crucial step toward deeper understanding of the complex dynamics beneath failures, the essential prerequisites for success. △ Less","18 March, 2019",https://arxiv.org/pdf/1903.07562
Short Datathon for the Interdisciplinary Development of Data Analysis and Visualization Skills,Myrian Noguera Salinas;Maria Claudia Figueiredo Pereira Emer;Adolfo Gustavo Serra Seca Neto,"Understanding the major fraud problems in the world and interpreting the data available for analysis is a current challenge that requires interdisciplinary knowledge to complement the knowledge of computer professionals. Collaborative events (called Hackathons, Datathons, Codefests, Hack Days, etc.) have become relevant in several fields. Examples of fields which are explored in these events include startup development, open civic innovation, corporate innovation, and social issues. These events have features that favor knowledge exchange to solve challenges. In this paper, we present an event format called Short Datathon, a Hackathon for the development of exploratory data analysis and visualization skills. Our goal is to evaluate if participating in a Short Datathon can help participants learn basic data analysis and visualization concepts. We evaluated the Short Datathon in two case studies, with a total of 20 participants, carried out at the Federal University of Technology - Paraná. In both case studies we addressed the issue of tax evasion using real world data. We describe, as a result of this work, the qualitative aspects of the case studies and the perception of the participants obtained through questionnaires. Participants stated that the event helped them understand more about data analysis and visualization and that the experience with people from other areas during the event made data analysis more efficient. Further studies are necessary to evolve the format of the event and to evaluate its effectiveness. △ Less","18 March, 2019",https://arxiv.org/pdf/1903.07539
Complexity-entropy analysis at different levels of organization in written language,E. Estevez-Rams;A. Mesa Rodriguez;D. Estevez-Moya,"Written language is complex. A written text can be considered an attempt to convey a meaningful message which ends up being constrained by language rules, context dependence and highly redundant in its use of resources. Despite all these constraints, unpredictability is an essential element of natural language. Here we present the use of entropic measures to assert the balance between predictability and surprise in written text. In short, it is possible to measure innovation and context preservation in a document. It is shown that this can also be done at the different levels of organization of a text. The type of analysis presented is reasonably general, and can also be used to analyze the same balance in other complex messages such as DNA, where a hierarchy of organizational levels are known to exist. △ Less","13 March, 2019",https://arxiv.org/pdf/1903.07416
swCaffe: a Parallel Framework for Accelerating Deep Learning Applications on Sunway TaihuLight,Jiarui Fang;Liandeng Li;Haohuan Fu;Jinlei Jiang;Wenlai Zhao;Conghui He;Xin You;Guangwen Yang,"This paper reports our efforts on swCaffe, a highly efficient parallel framework for accelerating deep neural networks (DNNs) training on Sunway TaihuLight, the current fastest supercomputer in the world that adopts a unique many-core heterogeneous architecture, with 40,960 SW26010 processors connected through a customized communication network. First, we point out some insightful principles to fully exploit the performance of the innovative many-core architecture. Second, we propose a set of optimization strategies for redesigning a variety of neural network layers based on Caffe. Third, we put forward a topology-aware parameter synchronization scheme to scale the synchronous Stochastic Gradient Descent (SGD) method to multiple processors efficiently. We evaluate our framework by training a variety of widely used neural networks with the ImageNet dataset. On a single node, swCaffe can achieve 23\%\~{}119\% overall performance compared with Caffe running on K40m GPU. As compared with the Caffe on CPU, swCaffe runs 3.04\~{}7.84x faster on all the networks. Finally, we present the scalability of swCaffe for the training of ResNet-50 and AlexNet on the scale of 1024 nodes. △ Less","16 March, 2019",https://arxiv.org/pdf/1903.06934
Low Power Inference for On-Device Visual Recognition with a Quantization-Friendly Solution,Chen Feng;Tao Sheng;Zhiyu Liang;Shaojie Zhuo;Xiaopeng Zhang;Liang Shen;Matthew Ardi;Alexander C. Berg;Yiran Chen;Bo Chen;Kent Gauen;Yung-Hsiang Lu,"The IEEE Low-Power Image Recognition Challenge (LPIRC) is an annual competition started in 2015 that encourages joint hardware and software solutions for computer vision systems with low latency and power. Track 1 of the competition in 2018 focused on the innovation of software solutions with fixed inference engine and hardware. This decision allows participants to submit models online and not worry about building and bringing custom hardware on-site, which attracted a historically large number of submissions. Among the diverse solutions, the winning solution proposed a quantization-friendly framework for MobileNets that achieves an accuracy of 72.67% on the holdout dataset with an average latency of 27ms on a single CPU core of Google Pixel2 phone, which is superior to the best real-time MobileNet models at the time. △ Less","12 March, 2019",https://arxiv.org/pdf/1903.06791
PointNetLK: Robust & Efficient Point Cloud Registration using PointNet,Yasuhiro Aoki;Hunter Goforth;Rangaprasad Arun Srivatsan;Simon Lucey,"PointNet has revolutionized how we think about representing point clouds. For classification and segmentation tasks, the approach and its subsequent extensions are state-of-the-art. To date, the successful application of PointNet to point cloud registration has remained elusive. In this paper we argue that PointNet itself can be thought of as a learnable ""imaging"" function. As a consequence, classical vision algorithms for image alignment can be applied on the problem - namely the Lucas & Kanade (LK) algorithm. Our central innovations stem from: (i) how to modify the LK algorithm to accommodate the PointNet imaging function, and (ii) unrolling PointNet and the LK algorithm into a single trainable recurrent deep neural network. We describe the architecture, and compare its performance against state-of-the-art in common registration scenarios. The architecture offers some remarkable properties including: generalization across shape categories and computational efficiency - opening up new paths of exploration for the application of deep learning to point cloud registration. Code and videos are available at https://github.com/hmgoforth/PointNetLK. △ Less","4 April, 2019",https://arxiv.org/pdf/1903.05711
Discriminative Principal Component Analysis: A REVERSE THINKING,Hanli Qiao,"In this paper, we propose a novel approach named by Discriminative Principal Component Analysis which is abbreviated as Discriminative PCA in order to enhance separability of PCA by Linear Discriminant Analysis (LDA). The proposed method performs feature extraction by determining a linear projection that captures the most scattered discriminative information. The most innovation of Discriminative PCA is performing PCA on discriminative matrix rather than original sample matrix. For calculating the required discriminative matrix under low complexity, we exploit LDA on a converted matrix to obtain within-class matrix and between-class matrix thereof. During the computation process, we utilise direct linear discriminant analysis (DLDA) to solve the encountered SSS problem. For evaluating the performances of Discriminative PCA in face recognition, we analytically compare it with DLAD and PCA on four well known facial databases, they are PIE, FERET, YALE and ORL respectively. Results in accuracy and running time obtained by nearest neighbour classifier are compared when different number of training images per person used. Not only the superiority and outstanding performance of Discriminative PCA showed in recognition rate, but also the comparable results of running time. △ Less","12 March, 2019",https://arxiv.org/pdf/1903.04963
Gathering Insights from Teenagers' Hacking Experience with Authentic Cybersecurity Tools,Valdemar Švábenský;Jan Vykopal,"This Work-In-Progress Paper for the Innovative Practice Category presents a novel experiment in active learning of cybersecurity. We introduced a new workshop on hacking for an existing science-popularizing program at our university. The workshop participants, 28 teenagers, played a cybersecurity game designed for training undergraduates and professionals in penetration testing. Unlike in learning environments that are simplified for young learners, the game features a realistic virtual network infrastructure. This allows exploring security tools in an authentic scenario, which is complemented by a background story. Our research aim is to examine how young players approach using cybersecurity tools by interacting with the professional game. A preliminary analysis of the game session showed several challenges that the workshop participants faced. Nevertheless, they reported learning about security tools and exploits, and 61% of them reported wanting to learn more about cybersecurity after the workshop. Our results support the notion that young learners should be allowed more hands-on experience with security topics, both in formal education and informal extracurricular events. △ Less","11 March, 2019",https://arxiv.org/pdf/1903.04174
An innovative method to determine optimum insulation thickness based on non-uniform adaptive moving grid,Suelen Gasparin;Julien Berger;Denys Dutykh;Nathan Mendes,"It is well known that thermal insulation is a leading strategy for reducing energy consumption associated to heating or cooling processes in buildings. Nevertheless, building insulation can generate high expenditures so that the selection of an optimum insulation thickness requires a detailed energy simulation as well as an economic analysis. In this way, the present study proposes an innovative non-uniform adaptive method to determine the optimal insulation thickness of external walls. First, the method is compared with a reference solution to properly understand the features of the method, which can provide high accuracy with less spatial nodes. Then, the adaptive method is used to simulate the transient heat conduction through the building envelope of buildings located in Brazil, where there is a large potential of energy reduction. Simulations have been efficiently carried out for different wall and roof configurations, showing that the innovative method efficiently provides a gain of 25% on the computer run time. △ Less","26 February, 2019",https://arxiv.org/pdf/1903.03587
ABC: A Cryptocurrency-Focused Threat Modeling Framework,Ghada Almashaqbeh;Allison Bishop;Justin Cappos,"Cryptocurrencies are an emerging economic force, but there are concerns about their security. This is due, in part, to complex collusion cases and new threat vectors that could be missed by conventional security assessment strategies. To address these issues, we propose ABC, an Asset-Based Cryptocurrency-focused threat modeling framework capable of identifying such risks. ABC's key innovation is the use of collusion matrices. A collusion matrix forces a threat model to cover a large space of threat cases while simultaneously manages this process to prevent it from being overly complex. Moreover, ABC derives a system-specific threat categories that account for the financial aspects and the new asset types that cryptocurrencies introduce. We demonstrate that ABC is effective by conducting a user study and by presenting real-world use cases. The user study showed that around 71\% of those who used ABC were able to identify financial security threats, as compared to only 13\% of participants who used the popular framework STRIDE. The use cases further attest to the usefulness of ABC's tools for both cryptocurrency-based systems, as well as a cloud native security technology. This shows the potential of ABC as an effective security assessment technique for various types of large-scale distributed systems. △ Less","23 August, 2019",https://arxiv.org/pdf/1903.03422
"An Introduction to hpxMP: A Modern OpenMP Implementation Leveraging HPX, An Asynchronous Many-Task System",Tianyi Zhang;Shahrzad Shirzad;Patrick Diehl;R. Tohid;Weile Wei;Hartmut Kaiser,"Asynchronous Many-task (AMT) runtime systems have gained increasing acceptance in the HPC community due to the performance improvements offered by fine-grained tasking runtime systems. At the same time, C++ standardization efforts are focused on creating higher-level interfaces able to replace OpenMP or OpenACC in modern C++ codes. These higher level functions have been adopted in standards conforming runtime systems such as HPX, giving users the ability to simply utilize fork-join parallelism in their own codes. Despite innovations in runtime systems and standardization efforts users face enormous challenges porting legacy applications. Not only must users port their own codes, but often users rely on highly optimized libraries such as BLAS and LAPACK which use OpenMP for parallization. Current efforts to create smooth migration paths have struggled with these challenges, especially as the threading systems of AMT libraries often compete with the treading system of OpenMP. To overcome these issues, our team has developed hpxMP, an implementation of the OpenMP standard, which utilizes the underlying AMT system to schedule and manage tasks. This approach leverages the C++ interfaces exposed by HPX and allows users to execute their applications on an AMT system without changing their code. In this work, we compare hpxMP with Clang's OpenMP library with four linear algebra benchmarks of the Blaze C++ library. While hpxMP is often not able to reach the same performance, we demonstrate viability for providing a smooth migration for applications but have to be extended to benefit from a more general task based programming model. △ Less","5 July, 2019",https://arxiv.org/pdf/1903.03023
Multiple configurations for puncturing robot positioning,Omar Abdelaziz;Minzhou Luo;Guanwu Jiang;Saixuan Chen,"The paper presents the Inverse Kinematics (IK) close form derivation steps using combination of analytical and geometric techniques for the UR robot. The innovative application of this work is used in the precise positioning of puncture robotics system. The end effector is a puncture needle guide tube, which needs precise positioning over the puncture insertion point. The IK closed form solutions bring out maximum 8 solutions represents 8 different robot joints configurations. These multiple solutions are helpful in the puncture robotics system, it allow doctors to choose the most suitable configuration during the operation. Therefore the workspace becomes more adequate for the coexistence of human and robot. Moreover IK closed form solutions are more precise in positioning for medical puncture surgery compared to other numerical methods. We include a performance evaluation for both of the IK obtained by the closed form solution and by a numerical method. △ Less","6 March, 2019",https://arxiv.org/pdf/1903.02281
Breaching the Future: Understanding Human Challenges of Autonomous Systems for the Home,Tommy Nilsson;Andy Crabtree;Joel Fischer;Boriana Koleva,"The domestic environment is a key area for the design and deployment of autonomous systems. Yet research indicates their adoption is already being hampered by a variety of critical issues including trust, privacy and security. This paper explores how potential users relate to the concept of autonomous systems in the home and elaborates further points of friction. It makes two contributions. One methodological, focusing on the use of provocative utopian and dystopian scenarios of future autonomous systems in the home. These are used to drive an innovative workshop-based approach to breaching experiments, which surfaces the usually tacit and unspoken background expectancies implicated in the organisation of everyday life that have a powerful impact on the acceptability of future and emerging technologies. The other contribution is substantive, produced through participants efforts to repair the incongruity or ""reality disjuncture"" created by utopian and dystopian visions, and highlights the need to build social as well as computational accountability into autonomous systems, and to enable coordination and control. △ Less","4 March, 2019",https://arxiv.org/pdf/1903.01831
Improving Cross-Domain Chinese Word Segmentation with Word Embeddings,Yuxiao Ye;Yue Zhang;Weikang Li;Likun Qiu;Jian Sun,"Cross-domain Chinese Word Segmentation (CWS) remains a challenge despite recent progress in neural-based CWS. The limited amount of annotated data in the target domain has been the key obstacle to a satisfactory performance. In this paper, we propose a semi-supervised word-based approach to improving cross-domain CWS given a baseline segmenter. Particularly, our model only deploys word embeddings trained on raw text in the target domain, discarding complex hand-crafted features and domain-specific dictionaries. Innovative subsampling and negative sampling methods are proposed to derive word embeddings optimized for CWS. We conduct experiments on five datasets in special domains, covering domains in novels, medicine, and patent. Results show that our model can obviously improve cross-domain CWS, especially in the segmentation of domain-specific noun entities. The word F-measure increases by over 3.0% on four datasets, outperforming state-of-the-art semi-supervised and unsupervised cross-domain CWS approaches with a large margin. We make our code and data available on Github. △ Less","28 March, 2019",https://arxiv.org/pdf/1903.01698
A Reliabel and an efficient web testing system,Kamran Ali;Xia Xiaoling,"To improve the reliability and efficiency of Web Software, the Testing Team should be creative and innovative. The experience and intuition of Tester also matters a lot and most often the destructive nature of Tester brings reliable software to the user. Actually, Testing is the responsibility of everybody who is involved in the Project.","8 February, 2019",https://arxiv.org/pdf/1903.01221
Automatic microscopic cell counting by use of deeply-supervised density regression model,Shenghua He;Kyaw Thu Minn;Lilianna Solnica-Krezel;Mark Anastasio;Hua Li,"Accurately counting cells in microscopic images is important for medical diagnoses and biological studies, but manual cell counting is very tedious, time-consuming, and prone to subjective errors, and automatic counting can be less accurate than desired. To improve the accuracy of automatic cell counting, we propose here a novel method that employs deeply-supervised density regression. A fully convolutional neural network (FCNN) serves as the primary FCNN for density map regression. Innovatively, a set of auxiliary FCNNs are employed to provide additional supervision for learning the intermediate layers of the primary CNN to improve network performance. In addition, the primary CNN is designed as a concatenating framework to integrate multi-scale features through shortcut connections in the network, which improves the granularity of the features extracted from the intermediate CNN layers and further supports the final density map estimation. △ Less","22 March, 2019",https://arxiv.org/pdf/1903.01084
Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research,Joel Z. Leibo;Edward Hughes;Marc Lanctot;Thore Graepel,"Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations. △ Less","11 March, 2019",https://arxiv.org/pdf/1903.00742
A Cooperative Multi-Agent Reinforcement Learning Framework for Resource Balancing in Complex Logistics Network,Xihan Li;Jia Zhang;Jiang Bian;Yunhai Tong;Tie-Yan Liu,"Resource balancing within complex transportation networks is one of the most important problems in real logistics domain. Traditional solutions on these problems leverage combinatorial optimization with demand and supply forecasting. However, the high complexity of transportation routes, severe uncertainty of future demand and supply, together with non-convex business constraints make it extremely challenging in the traditional resource management field. In this paper, we propose a novel sophisticated multi-agent reinforcement learning approach to address these challenges. In particular, inspired by the externalities especially the interactions among resource agents, we introduce an innovative cooperative mechanism for state and reward design resulting in more effective and efficient transportation. Extensive experiments on a simulated ocean transportation service demonstrate that our new approach can stimulate cooperation among agents and lead to much better performance. Compared with traditional solutions based on combinatorial optimization, our approach can give rise to a significant improvement in terms of both performance and stability. △ Less","2 March, 2019",https://arxiv.org/pdf/1903.00714
Design and Validation of a Bluetooth 5 Fog Computing Based Industrial CPS Architecture for Intelligent Industry 4.0 Shipyard Workshops,Paula Fraga-Lamas;Peio Lopez-Iturri;Mikel Celaya-Echarri;Oscar Blanco-Novoa;Leyre Azpilicueta;José Varela-Barbeito;Francisco Falcone;Tiago M. Fernández-Caramés,"Navantia, one of Europe's largest shipbuilders, is creating a fog computing-based Industrial Cyber-Physical System (ICPS) for remote monitoring in real-time of their pipe workshops. Such a monitoring process, which involves pipe traceability and tracking, is a unique industrial challenge, given their metallic content, massive quantity and heterogeneous typology, as well as to the number of complex processes involved. Pipe improved location, from production and through their lifetime, can provide significant productivity and safety benefits to shipyards and foster innovative applications in process planning. Bluetooth 5 represents a cost-effective opportunity to cope with this harsh environment, since it has been significantly enhanced in terms of low power consumption, range, speed and broadcasting capacity. Thus, this article proposes a Bluetooth 5 fog computing-based ICPS architecture that is designed to support physically-distributed and low-latency Industry 4.0 applications that off-load network traffic and computational resource consumption from the cloud. In order to validate the proposed ICPS design, one of Navantia's pipe workshops has been modeled through an in-house-developed 3D ray launching radio planning simulator that considers three main intrinsic characteristics: the number of pipes, the main working areas with their corresponding machines, and the daily workforce. The radio propagation results obtained by the simulation tool are validated through empirical measurements. These results aim to provide guidelines for ICPS developers, network operators and planners to investigate further complex industrial deployments based on Bluetooth 5. △ Less","2 March, 2019",https://arxiv.org/pdf/1903.00713
Machine learning in policy evaluation: new tools for causal inference,Noemi Kreif;Karla DiazOrdaz,"While machine learning (ML) methods have received a lot of attention in recent years, these methods are primarily for prediction. Empirical researchers conducting policy evaluations are, on the other hand, pre-occupied with causal problems, trying to answer counterfactual questions: what would have happened in the absence of a policy? Because these counterfactuals can never be directly observed (described as the ""fundamental problem of causal inference"") prediction tools from the ML literature cannot be readily used for causal inference. In the last decade, major innovations have taken place incorporating supervised ML tools into estimators for causal parameters such as the average treatment effect (ATE). This holds the promise of attenuating model misspecification issues, and increasing of transparency in model selection. One particularly mature strand of the literature include approaches that incorporate supervised ML approaches in the estimation of the ATE of a binary treatment, under the \textit{unconfoundedness} and positivity assumptions (also known as exchangeability and overlap assumptions). This article reviews popular supervised machine learning algorithms, including the Super Learner. Then, some specific uses of machine learning for treatment effect estimation are introduced and illustrated, namely (1) to create balance among treated and control groups, (2) to estimate so-called nuisance models (e.g. the propensity score, or conditional expectations of the outcome) in semi-parametric estimators that target causal parameters (e.g. targeted maximum likelihood estimation or the double ML estimator), and (3) the use of machine learning for variable selection in situations with a high number of covariates. △ Less","1 March, 2019",https://arxiv.org/pdf/1903.00402
Actions Generation from Captions,Xuan Liang;Yida Xu,"Sequence transduction models have been widely explored in many natural language processing tasks. However, the target sequence usually consists of discrete tokens which represent word indices in a given vocabulary. We barely see the case where target sequence is composed of continuous vectors, where each vector is an element of a time series taken successively in a temporal domain. In this work, we introduce a new data set, named Action Generation Data Set (AGDS) which is specifically designed to carry out the task of caption-to-action generation. This data set contains caption-action pairs. The caption is comprised of a sequence of words describing the interactive movement between two people, and the action is a captured sequence of poses representing the movement. This data set is introduced to study the ability of generating continuous sequences through sequence transduction models. We also propose a model to innovatively combine Multi-Head Attention (MHA) and Generative Adversarial Network (GAN) together. In our model, we have one generator to generate actions from captions and three discriminators where each of them is designed to carry out a unique functionality: caption-action consistency discriminator, pose discriminator and pose transition discriminator. This novel design allowed us to achieve plausible generation performance which is demonstrated in the experiments. △ Less","13 February, 2019",https://arxiv.org/pdf/1902.11109
Induction Networks for Few-Shot Text Classification,Ruiying Geng;Binhua Li;Yongbin Li;Xiaodan Zhu;Ping Jian;Jian Sun,"Text classification tends to struggle when data is deficient or when it needs to adapt to unseen classes. In such challenging scenarios, recent studies have used meta-learning to simulate the few-shot task, in which new queries are compared to a small support set at the sample-wise level. However, this sample-wise comparison may be severely disturbed by the various expressions in the same class. Therefore, we should be able to learn a general representation of each class in the support set and then compare it to new queries. In this paper, we propose a novel Induction Network to learn such a generalized class-wise representation, by innovatively leveraging the dynamic routing algorithm in meta-learning. In this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification. △ Less","29 September, 2019",https://arxiv.org/pdf/1902.10482
Coloring Big Graphs with AlphaGoZero,Jiayi Huang;Mostofa Patwary;Gregory Diamos,"We show that recent innovations in deep reinforcement learning can effectively color very large graphs -- a well-known NP-hard problem with clear commercial applications. Because the Monte Carlo Tree Search with Upper Confidence Bound algorithm used in AlphaGoZero can improve the performance of a given heuristic, our approach allows deep neural networks trained using high performance computing (HPC) technologies to transform computation into improved heuristics with zero prior knowledge. Key to our approach is the introduction of a novel deep neural network architecture (FastColorNet) that has access to the full graph context and requires O(V) time and space to color a graph with V vertices, which enables scaling to very large graphs that arise in real applications like parallel computing, compilers, numerical solvers, and design automation, among others. As a result, we are able to learn new state of the art heuristics for graph coloring. △ Less","8 November, 2019",https://arxiv.org/pdf/1902.10162
Community structure in co-inventor networks affects time to first citation for patents,W. Doonan;K. W. Higham;M. Governale;U. Zülicke,"We have investigated community structure in the co-inventor network of a given cohort of patents and related this structure to the dynamics of how these patents acquire their first citation. A statistically significant difference in the time lag until first citation is linked to whether or not this citation comes from a patent whose listed inventors share membership in the same communities as the inventors of the cited patent. Although the inventor-community structures identified by different community-detection algorithms differ in several aspects, including the community-size distribution, the magnitude of the difference in time to first citation is robustly exhibited. Our work is able to quantify the expected acceleration of knowledge flow within inventor communities and thereby further establishes the utility of network-analysis tools for studying innovation dynamics. △ Less","30 March, 2019",https://arxiv.org/pdf/1902.09679
"Slotted ALOHA on LoRaWAN - Design, Analysis, and Deployment",Tommaso Polonelli;Davide Brunelli;Achille Marzocchi;Luca Benini,"LoRaWAN is one of the most promising standards for long-range sensing applications. However, the high number of end devices expected in at-scale deployment, combined with the absence of an effective synchronization scheme, challenge the scalability of this standard. In this paper, we present an approach to increase network throughput through a Slotted-ALOHA overlay on LoRaWAN networks. To increase the single channel capacity, we propose to regulate the communication of LoRaWAN networks using a Slotted-ALOHA variant on the top of the Pure-ALOHA approach used by the standard; thus, no modification in pre-existing libraries is necessary. Our method is based on an innovative synchronization service that is suitable for low-cost wireless sensor nodes. We modelled the LoRaWAN channel with extensive measurement on hardware platforms, and we quantified the impact of tuning parameters on physical and medium access control layers, as well as the packet collision rate. Results show that Slotted-ALOHA supported by our synchronization service significantly improves the performance of traditional LoRaWAN networks regarding packet loss rate and network throughput. △ Less","18 February, 2019",https://arxiv.org/pdf/1902.09468
"Incentive Compatibility, Scalability and Privacy in real time Demand Response",Georgios Tsaousoglou;Konstantinos Steriotis;Nikolaos Efthymiopoulos;Prodrommos Makris;Emmanouel Varvarigos,"The high penetration of Renewable Energy Sources in modern smart grids necessitated the development of Demand Response (DR) mechanisms as well as corresponding innovative services for the emerging flexibility markets. From a game theoretic perspective, the basic key performance indicators (KPIs) for such DR mechanisms are efficiency in terms of social welfare, practical applicability, and incentive guarantees, in the sense of making it a dominant strategy for each user to act truthfully according to his/her preferences, leaving no room for cheating. In this paper, we propose a DR architecture, including a mechanism based on Ausubel clinching auction and a communication protocol, that provably guarantee both efficiency and truthful user participation. Practicality/easiness of participation is enhanced via simple queries, while user privacy issues are addressed via a distributed implementation. Simulation results confirm the desired properties, while also showing that the truthfulness property becomes even more important in markets where participants are not particularly flexible △ Less","25 February, 2019",https://arxiv.org/pdf/1902.09251
Bi-Skip: A Motion Deblurring Network Using Self-paced Learning,Yiwei Zhang;Chunbiao Zhu;Ge Li;Yuan Zhao;Haifeng Shen,"A fast and effective motion deblurring method has great application values in real life. This work presents an innovative approach in which a self-paced learning is combined with GAN to deblur image. First, We explain that a proper generator can be used as deep priors and point out that the solution for pixel-based loss is not same with the one for perception-based loss. By using these ideas as starting points, a Bi-Skip network is proposed to improve the generating ability and a bi-level loss is adopted to solve the problem that common conditions are non-identical. Second, considering that the complex motion blur will perturb the network in the training process, a self-paced mechanism is adopted to enhance the robustness of the network. Through extensive evaluations on both qualitative and quantitative criteria, it is demonstrated that our approach has a competitive advantage over state-of-the-art methods. △ Less","24 February, 2019",https://arxiv.org/pdf/1902.08915
Hybrid Beamforming for Millimeter Wave Systems Using the MMSE Criterion,Tian Lin;Jiaqi Cong;Yu Zhu;Jun Zhang;Khaled B. Letaief,"Hybrid analog and digital beamforming (HBF) has recently emerged as an attractive technique for millimeter-wave (mmWave) communication systems. It well balances the demand for sufficient beamforming gains to overcome the propagation loss and the desire to reduce the hardware cost and power consumption. In this paper, the mean square error (MSE) is chosen as the performance metric to characterize the transmission reliability. Using the minimum sum-MSE criterion, we investigate the HBF design for broadband mmWave transmissions. To overcome the difficulty of solving the multi-variable design problem, the alternating minimization method is adopted to optimize the hybrid transmit and receive beamformers alternatively. Specifically, a manifold optimization based HBF algorithm is firstly proposed, which directly handles the constant modulus constraint of the analog component. Its convergence is then proved. To reduce the computational complexity, we then propose a low-complexity general eigenvalue decomposition based HBF algorithm in the narrowband scenario and three algorithms via the eigenvalue decomposition and orthogonal matching pursuit methods in the broadband scenario. A particular innovation in our proposed alternating minimization algorithms is a carefully designed initialization method, which leads to faster convergence. {Furthermore, we extend the sum-MSE based design to that with weighted sum-MSE, which is then connected to the spectral efficiency based design.} Simulation results show that the proposed HBF algorithms achieve significant performance improvement over existing ones, and perform close to full-digital beamforming. △ Less","21 February, 2019",https://arxiv.org/pdf/1902.08343
Design and Control of a Quasi-Direct Drive Soft Exoskeleton for Knee Injury Prevention during Squatting,Shuangyue Yu;Tzu-Hao Huang;Dianpeng Wang;Brian Lynn;Dina Sayd;Viktor Silivanov;Young Soo Park;Yingli Tian;Hao Su,"This paper presents design and control innovations of wearable robots that tackle two barriers to widespread adoption of powered exoskeletons, namely restriction of human movement and versatile control of wearable co-robot systems. First, the proposed quasi-direct drive actuation comprising of our customized high torque density motors and low ratio transmission mechanism significantly reduces the mass of the robot and produces high backdrivability. Second, we derive a biomechanics model-based control that generates biological torque profile for versatile control of both squat and stoop lifting assistance. The control algorithm detects lifting postures using compact inertial measurement unit (IMU) sensors to generate an assistive profile that is proportional to the biological torque produced from our model. Experimental results demonstrate that the robot exhibits low mechanical impedance (1.5 Nm resistive torque) when it is unpowered and 0.5 Nm resistive torque with zero-torque tracking control. Root mean square (RMS) error of torque tracking is less than 0.29 Nm (1.21% error of 24 Nm peak torque). Compared with squatting without the exoskeleton, the controller reduces 87.5%, 80% and 75% of the of three knee extensor muscles (average peak EMG of 3 healthy subjects) during squat with 50% of biological torque assistance. △ Less","4 July, 2019",https://arxiv.org/pdf/1902.07106
Topics of Concern: Identifying User Issues in Reviews of IoT Apps and Devices,Andrew Truelove;Farah Naz Chowdhury;Omprakash Gnawali;Mohammad Amin Alipour,"Internet of Things (IoT) systems are bundles of networked sensors and actuators that are deployed in an environment and act upon the sensory data that they receive. These systems, especially consumer electronics, have two main cooperating components: a device and a mobile app. The unique combination of hardware and software in IoT systems presents challenges that are lesser known to mainstream software developers. They might require innovative solutions to support the development and integration of such systems. In this paper, we analyze more than 90,000 reviews of ten IoT devices and their corresponding apps and extract the issues that users encountered while using these systems. Our results indicate that issues with connectivity, timing, and updates are particularly prevalent in the reviews. Our results call for a new software-hardware development framework to assist the development of reliable IoT systems. △ Less","29 March, 2019",https://arxiv.org/pdf/1902.06384
The AtLarge Vision on the Design of Distributed Systems and Ecosystems,Alexandru Iosup;Laurens Versluis;Animesh Trivedi;Erwin van Eyk;Lucian Toader;Vincent van Beek;Giulia Frascaria;Ahmed Musaafir;Sacheendra Talluri,"High-quality designs of distributed systems and services are essential for our digital economy and society. Threatening to slow down the stream of working designs, we identify the mounting pressure of scale and complexity of \mbox{(eco-)systems}, of ill-defined and wicked problems, and of unclear processes, methods, and tools. We envision design itself as a core research topic in distributed systems, to understand and improve the science and practice of distributed (eco-)system design. Toward this vision, we propose the AtLarge design framework, accompanied by a set of 8 core design principles. We also propose 10 key challenges, which we hope the community can address in the following 5 years. In our experience so far, the proposed framework and principles are practical, and lead to pragmatic and innovative designs for large-scale distributed systems. △ Less","14 February, 2019",https://arxiv.org/pdf/1902.05416
Yelp Food Identification via Image Feature Extraction and Classification,Fanbo Sun;Zhixiang Gu;Bo Feng,"Yelp has been one of the most popular local service search engine in US since 2004. It is powered by crowd-sourced text reviews and photo reviews. Restaurant customers and business owners upload photo images to Yelp, including reviewing or advertising either food, drinks, or inside and outside decorations. It is obviously not so effective that labels for food photos rely on human editors, which is an issue should be addressed by innovative machine learning approaches. In this paper, we present a simple but effective approach which can identify up to ten kinds of food via raw photos from the challenge dataset. We use 1) image pre-processing techniques, including filtering and image augmentation, 2) feature extraction via convolutional neural networks (CNN), and 3) three ways of classification algorithms. Then, we illustrate the classification accuracy by tuning parameters for augmentations, CNN, and classification. Our experimental results show this simple but effective approach to identify up to 10 food types from images. △ Less","11 February, 2019",https://arxiv.org/pdf/1902.05413
A Cross-Repository Model for Predicting Popularity in GitHub,Neda Hajiakhoond Bidoki;Gita Sukthankar;Heather Keathley;Ivan Garibay,"Social coding platforms, such as GitHub, can serve as natural laboratories for studying the diffusion of innovation through tracking the pattern of code adoption by programmers. This paper focuses on the problem of predicting the popularity of software repositories over time; our aim is to forecast the time series of popularity-related events (code forks and watches). In particular, we are interested in cross-repository patterns-how do events on one repository affect other repositories? Our proposed LSTM (Long Short-Term Memory) recurrent neural network integrates events across multiple active repositories, outperforming a standard ARIMA (Auto-Regressive Integrated Moving Average) time series prediction based on the single repository. The ability of the LSTM to leverage cross-repository information gives it a significant edge over standard time series forecasting. △ Less","13 February, 2019",https://arxiv.org/pdf/1902.05216
Unsupervised 3D End-to-End Medical Image Registration with Volume Tweening Network,Shengyu Zhao;Tingfung Lau;Ji Luo;Eric I-Chao Chang;Yan Xu,"3D medical image registration is of great clinical importance. However, supervised learning methods require a large amount of accurately annotated corresponding control points (or morphing), which are very difficult to obtain. Unsupervised learning methods ease the burden of manual annotation by exploiting unlabeled data without supervision. In this paper, we propose a new unsupervised learning method using convolutional neural networks under an end-to-end framework, Volume Tweening Network (VTN), for 3D medical image registration. We propose three innovative technical components: (1) An end-to-end cascading scheme that resolves large displacement; (2) An efficient integration of affine registration network; and (3) An additional invertibility loss that encourages backward consistency. Experiments demonstrate that our algorithm is 880x faster (or 3.3x faster without GPU acceleration) than traditional optimization-based methods and achieves state-of-theart performance in medical image registration. △ Less","30 October, 2019",https://arxiv.org/pdf/1902.05020
3D Face Modeling From Diverse Raw Scan Data,Feng Liu;Luan Tran;Xiaoming Liu,"Traditional 3D face models learn a latent representation of faces using linear subspaces from limited scans of a single database. The main roadblock of building a large-scale face model from diverse 3D databases lies in the lack of dense correspondence among raw scans. To address these problems, this paper proposes an innovative framework to jointly learn a nonlinear face model from a diverse set of raw 3D scan databases and establish dense point-to-point correspondence among their scans. Specifically, by treating input scans as unorganized point clouds, we explore the use of PointNet architectures for converting point clouds to identity and expression feature representations, from which the decoder networks recover their 3D face shapes. Further, we propose a weakly supervised learning approach that does not require correspondence label for the scans. We demonstrate the superior dense correspondence and representation power of our proposed method, and its contribution to single-image 3D face reconstruction. △ Less","13 August, 2019",https://arxiv.org/pdf/1902.04943
IEEE 802.11be Extremely High Throughput: The Next Generation of Wi-Fi Technology Beyond 802.11ax,David López-Pérez;Adrian Garcia-Rodriguez;Lorenzo Galati-Giordano;Mika Kasslin;Klaus Doppler,"Wi-Fi technology is continuously innovating to cater to the growing customer demands, driven by the digitalisation of everything, both in the home as well as the enterprise and hotspot spaces. In this article, we introduce to the wireless community the next generation Wi-Fi-based on IEEE 802.11be Extremely High Throughput (EHT)-, present the main objectives and timelines of this new 802.11be amendment, thoroughly describe its main candidate features and enhancements, and cover the important issue of coexistence with other wireless technologies. We also provide simulation results to assess the potential throughput gains brought by 802.11be with respect to 802.11ax. △ Less","27 September, 2019",https://arxiv.org/pdf/1902.04320
Cloud Futurology,Blesson Varghese;Philipp Leitner;Suprio Ray;Kyle Chard;Adam Barker;Yehia Elkhatib;Herry Herry;Cheol-Ho Hong;Jeremy Singer;Fung Po Tso;Eiko Yoneki;Mohamed-Faten Zhani,"The Cloud has become integral to most Internet-based applications and user gadgets. This article provides a brief history of the Cloud and presents a researcher's view of the prospects for innovating at the infrastructure, middleware, and application and delivery levels of the already crowded Cloud computing stack. △ Less","10 February, 2019",https://arxiv.org/pdf/1902.03656
A Novel Secure Authentication Scheme for Heterogeneous Internet of Thing,Jingwei Liu;Ailian Ren;Lihuan Zhang;Rong Sun;Xiaojiang Du;Mohsen Guizani,"Today, Internet of Things (IoT) technology is being increasingly popular which is applied in a wide range of industry sectors such as healthcare, transportation and some critical infrastructures. With the widespread applications of IoT technology, people's lives have changed dramatically. Due to its capabilities of sensitive data-aware, information collection, communication and processing, it raises security and privacy concerns. Moreover, a malicious attacker may impersonate a legitimate user, which may cause security threat and violation privacy. In allusion to the above problems, we propose a novel and lightweight anonymous authentication and key agreement scheme for heterogeneous IoT, which is innovatively designed to shift between the public key infrastructure (PKI) and certificateless cryptography (CLC) environment. The proposed scheme not only achieves secure communication among the legal authorized users, but also possesses more attributes with user anonymity, non-repudiation and key agreement fairness. Through the security analysis, it is proved that the proposed scheme can resist replay attacks and denial of service (DOS) attacks. Finally, the performance evaluation demonstrates that our scheme is more lightweight and innovative. △ Less","10 February, 2019",https://arxiv.org/pdf/1902.03562
Novelty Search for Deep Reinforcement Learning Policy Network Weights by Action Sequence Edit Metric Distance,Ethan C. Jackson;Mark Daley,"Reinforcement learning (RL) problems often feature deceptive local optima, and learning methods that optimize purely for reward signal often fail to learn strategies for overcoming them. Deep neuroevolution and novelty search have been proposed as effective alternatives to gradient-based methods for learning RL policies directly from pixels. In this paper, we introduce and evaluate the use of novelty search over agent action sequences by string edit metric distance as a means for promoting innovation. We also introduce a method for stagnation detection and population resampling inspired by recent developments in the RL community that uses the same mechanisms as novelty search to promote and develop innovative policies. Our methods extend a state-of-the-art method for deep neuroevolution using a simple-yet-effective genetic algorithm (GA) designed to efficiently learn deep RL policy network weights. Experiments using four games from the Atari 2600 benchmark were conducted. Results provide further evidence that GAs are competitive with gradient-based algorithms for deep RL. Results also demonstrate that novelty search over action sequences is an effective source of selection pressure that can be integrated into existing evolutionary algorithms for deep RL. △ Less","8 February, 2019",https://arxiv.org/pdf/1902.03142
Size Independent Neural Transfer for RDDL Planning,Sankalp Garg;Aniket Bajpai;Mausam,"Neural planners for RDDL MDPs produce deep reactive policies in an offline fashion. These scale well with large domains, but are sample inefficient and time-consuming to train from scratch for each new problem. To mitigate this, recent work has studied neural transfer learning, so that a generic planner trained on other problems of the same domain can rapidly transfer to a new problem. However, this approach only transfers across problems of the same size. We present the first method for neural transfer of RDDL MDPs that can transfer across problems of different sizes. Our architecture has two key innovations to achieve size independence: (1) a state encoder, which outputs a fixed length state embedding by max pooling over varying number of object embeddings, (2) a single parameter-tied action decoder that projects object embeddings into action probabilities for the final policy. On the two challenging RDDL domains of SysAdmin and Game Of Life, our approach powerfully transfers across problem sizes and has superior learning curves over training from scratch. △ Less","4 April, 2019",https://arxiv.org/pdf/1902.03081
Community Animation: Exploring a design space that leverages geosocial networking to increase community engagement,Jomara Sandbulte;Jessica Kropczynski;John M. Carroll,"This paper explores a design study of a smartphone enabled meet-up app meant to inspire engagement in community innovation. Community hubs such as co-working spaces, incubators, and maker spaces attract community members with diverse interests. This paper presents these spaces as a design opportunity for an application that helps host community-centered meet-ups in smart and connected communities. Our design study explores three scenarios of use, inspired by previous literature, for organizing meet-ups and compares them by surveying potential users. Based on the results of our survey, we propose several design implications and implement them in the Community Animator geosocial networking application, which identifies nearby individuals that are willing to chat or perform community-centered activities. We present the results of both our survey and our prototype, discuss our design goals, and provide design implications for civic-minded, geosocial networking applications. Our contribution in this work is the development process, proposed design of a mobile application to support community-centered meet-ups, and insights for future work. △ Less","7 February, 2019",https://arxiv.org/pdf/1902.02842
Assessing the Usages of LMS at KAU and Proposing FORCE Strategy for the Diffusion,Rayed AlGhamdi;Adel Bahadad,"Since the beginning of the Saudi Arabian academic year 1435 (Sept 2014), the web-based learning management system Blackboard has been introduced and made available to all instructors and students for all courses at King Abdulaziz University (KAU). The current study takes place to assess the current usages of the Blackboard usages at KAU. The data collected from the 923 students of the foundation year which represent about one-third of the total number of the male students for the academic 2016-2017. Based on statistical evidence gained from the students responses to the survey questions, 78% of the students are inactive users of the Blackboard. The study follows up with interviewing five instructors who teach first-year students in order to seek explanations of the Blackboard low usages by the students. The outcomes point significant processes at an individual level and as well as an organizational level. The Diffusion of Innovation (DOI) was used to study the case because it is believed to be the best explain such adoption of innovation at individual and organizational levels. Based on the current outcomes and the author's experience in teaching a computer course using Blackboard, a strategy called 'FORCE' is proposed for the diffusion process. △ Less","3 February, 2019",https://arxiv.org/pdf/1902.00953
Partition Pruning: Parallelization-Aware Pruning for Deep Neural Networks,Sina Shahhosseini;Ahmad Albaqsami;Masoomeh Jasemi;Nader Bagherzadeh,"Parameters of recent neural networks require a huge amount of memory. These parameters are used by neural networks to perform machine learning tasks when processing inputs. To speed up inference, we develop Partition Pruning, an innovative scheme to reduce the parameters used while taking into consideration parallelization. We evaluated the performance and energy consumption of parallel inference of partitioned models, which showed a 7.72x speed up of performance and a 2.73x reduction in the energy used for computing pruned layers of TinyVGG16 in comparison to running the unpruned model on a single accelerator. In addition, our method showed a limited reduction some numbers in accuracy while partitioning fully connected layers. △ Less","27 February, 2019",https://arxiv.org/pdf/1901.11391
Second order hierarchical partial least squares regression-polynomial chaos expansion for global sensitivity and reliability analyses of high-dimensional models,Ling-Ze Bu;Wei Zhao;Wei Wang,"To tackle the curse of dimensionality and multicollinearity problems of polynomial chaos expansion for analyzing global sensitivity and reliability of models with high stochastic dimensions, this paper proposes a novel non-intrusive algorithm called second order hierarchical partial least squares regression-polynomial chaos expansion. The first step of the innovative algorithm is to divide the polynomials into several groups according to their interaction degrees and nonlinearity degrees, which avoids large data sets and reflects the relationship between polynomial chaos expansion and high dimensional model representation. Then a hierarchical regression algorithm based on partial least squares regression is devised for extracting latent variables from each group at different variable levels. The optimal interaction degree and the corresponding nonlinearity degrees are automatically estimated with an improved cross validation scheme. Based on the relationship between variables at two adjacent levels, Sobol' sensitivity indices can be obtained by a simple post-processing of expansion coefficients. Thus, the expansion is greatly simplified through retaining the important inputs, leading to accurate reliability analysis without requirements of additional model evaluations. Finally, finite element models with three different types of structures verified that the proposed method can greatly improve the computational efficiency compared with the ordinary least squares regression-based method. △ Less","7 March, 2019",https://arxiv.org/pdf/1901.11295
Open Research Knowledge Graph: Next Generation Infrastructure for Semantic Scholarly Knowledge,Mohamad Yaser Jaradeh;Allard Oelen;Kheir Eddine Farfar;Manuel Prinz;Jennifer D'Souza;Gábor Kismihók;Markus Stocker;Sören Auer,"Despite improved digital access to scholarly knowledge in recent decades, scholarly communication remains exclusively document-based. In this form, scholarly knowledge is hard to process automatically. In this paper, we present the first steps towards a knowledge graph based infrastructure that acquires scholarly knowledge in machine actionable form thus enabling new possibilities for scholarly knowledge curation, publication and processing. The primary contribution is to present, evaluate and discuss multi-modal scholarly knowledge acquisition, combining crowdsourced and automated techniques. We present the results of the first user evaluation of the infrastructure with the participants of a recent international conference. Results suggest that users were intrigued by the novelty of the proposed infrastructure and by the possibilities for innovative scholarly knowledge processing it could enable. △ Less","1 August, 2019",https://arxiv.org/pdf/1901.10816
Using Score Distributions to Compare Statistical Significance Tests for Information Retrieval Evaluation,Javier Parapar;David E. Losada;Manuel A. Presedo-Quindimil;Alvaro Barreiro,"Statistical significance tests can provide evidence that the observed difference in performance between two methods is not due to chance. In Information Retrieval, some studies have examined the validity and suitability of such tests for comparing search systems. We argue here that current methods for assessing the reliability of statistical tests suffer from some methodological weaknesses, and we propose a novel way to study significance tests for retrieval evaluation. Using Score Distributions, we model the output of multiple search systems, produce simulated search results from such models, and compare them using various significance tests. A key strength of this approach is that we assess statistical tests under perfect knowledge about the truth or falseness of the null hypothesis. This new method for studying the power of significance tests in Information Retrieval evaluation is formal and innovative. Following this type of analysis, we found that both the sign test and Wilcoxon signed test have more power than the permutation test and the t-test. The sign test and Wilcoxon signed test also have a good behavior in terms of type I errors. The bootstrap test shows few type I errors, but it has less power than the other methods tested. △ Less","30 January, 2019",https://arxiv.org/pdf/1901.10696
Big Data Platform Architecture Under The Background of Financial Technology,Yi Liu;Jiawen Peng;Zhihao Yu,"With the rise of the concept of financial technology, financial and technology gradually in-depth integration, scientific and technological means to become financial product innovation, improve financial efficiency and reduce financial transaction costs an important driving force. In this context, the new technology platform is from the business philosophy, business model, technical means, sales, internal management, and other dimensions to re-shape the financial industry. In this paper, the existing big data platform architecture technology innovation, adding space-time data elements, combined with the insurance industry for practical analysis, put forward a meaningful product circle and customer circle. △ Less","19 January, 2019",https://arxiv.org/pdf/1901.10527
A Robot for Nondestructive Assay of Holdup Deposits in Gaseous Diffusion Piping,Heather Jones;Siri Maley;Mohammadreza Mousaei;David Kohanbash;Warren Whittaker;James Teza;Andrew Zhang;Nikhil Jog;William Whittaker,"Miles of contaminated pipe must be measured, foot by foot, as part of the decommissioning effort at deactivated gaseous diffusion enrichment facilities. The current method requires cutting away asbestos-lined thermal enclosures and performing repeated, elevated operations to manually measure pipe from the outside. The RadPiper robot, part of the Pipe Crawling Activity Measurement System (PCAMS) developed by Carnegie Mellon University and commissioned for use at the DOE Portsmouth Gaseous Diffusion Enrichment Facility, automatically measures U-235 in pipes from the inside. This improves certainty, increases safety, and greatly reduces measurement time. The heart of the RadPiper robot is a sodium iodide scintillation detector in an innovative disc-collimated assembly. By measuring from inside pipes, the robot significantly increases its count rate relative to external through-pipe measurements. The robot also provides imagery, models interior pipe geometry, and precisely measures distance in order to localize radiation measurements. Data collected by this system provides insight into pipe interiors that is simply not possible from exterior measurements, all while keeping operators safer. This paper describes the technical details of the PCAMS RadPiper robot. Key features for this robot include precision distance measurement, in-pipe obstacle detection, ability to transform for two pipe sizes, and robustness in autonomous operation. Test results demonstrating the robot's functionality are presented, including deployment tolerance tests, safeguarding tests, and localization tests. Integrated robot tests are also shown. △ Less","29 January, 2019",https://arxiv.org/pdf/1901.10341
Who's Afraid of Adversarial Queries? The Impact of Image Modifications on Content-based Image Retrieval,Zhuoran Liu;Zhengyu Zhao;Martha Larson,"An adversarial query is an image that has been modified to disrupt content-based image retrieval (CBIR) while appearing nearly untouched to the human eye. This paper presents an analysis of adversarial queries for CBIR based on neural, local, and global features. We introduce an innovative neural image perturbation approach, called Perturbations for Image Retrieval Error (PIRE), that is capable of blocking neural-feature-based CBIR. PIRE differs significantly from existing approaches that create images adversarial with respect to CNN classifiers because it is unsupervised, i.e., it needs no labelled data from the data set to which it is applied. Our experimental analysis demonstrates the surprising effectiveness of PIRE in blocking CBIR, and also covers aspects of PIRE that must be taken into account in practical settings, including saving images, image quality and leaking adversarial queries into the background collection. Our experiments also compare PIRE (a neural approach) with existing keypoint removal and injection approaches (which modify local features). Finally, we discuss the challenges that face multimedia researchers in the future study of adversarial queries. △ Less","2 May, 2019",https://arxiv.org/pdf/1901.10332
Generative Adversarial Networks for geometric surfaces prediction in injection molding,Pierre Nagorny;Thomas Lacombe;Hugues Favreliere;Maurice Pillet;Eric Pairel;Ronan Le Goff;Marlene Wali;Jerome Loureaux;Patrice Kiener,"Geometrical and appearance quality requirements set the limits of the current industrial performance in injection molding. To guarantee the product's quality, it is necessary to adjust the process settings in a closed loop. Those adjustments cannot rely on the final quality because a part takes days to be geometrically stable. Thus, the final part geometry must be predicted from measurements on hot parts. In this paper, we use recent success of Generative Adversarial Networks (GAN) with the pix2pix network architecture to predict the final part geometry, using only hot parts thermographic images, measured right after production. Our dataset is really small, and the GAN learns to translate thermography to geometry. We firstly study prediction performances using different image similarity comparison algorithms. Moreover, we introduce the innovative use of Discrete Modal Decomposition (DMD) to analyze network predictions. The DMD is a geometrical parameterization technique using a modal space projection to geometrically describe surfaces. We study GAN performances to retrieve geometrical parameterization of surfaces. △ Less","29 January, 2019",https://arxiv.org/pdf/1901.10178
Blockchain Trilemma Solver Algorand has Dilemma over Undecidable Messages,Mauro Conti;Ankit Gangwal;Michele Todero,"Recently, an ingenious protocol called Algorand has been proposed to overcome these limitations. Algorand uses an innovative process - called cryptographic sortition - to securely and unpredictably elect a set of voters from the network periodically. These voters are responsible for reaching consensus through a Byzantine Agreement (BA) protocol on one block per time, guaranteeing an overwhelming probability of linearity of the blockchain. In this paper, we present a security analysis of Algorand. To the best of our knowledge, it is the first security analysis as well as the first formal study on Algorand. We designed an attack scenario in which a group of malicious users tries to break the protocol, or at least limiting it to a reduced partition of network users, by exploiting a possible security flaw in the messages validation process of the BA. Since the source code or an official simulator for Algorand was not available at the time of our study, we created a simulator (which is available on request) to implement the protocol and assess the feasibility of our attack scenario. Our attack requires the attacker to have a trivial capability of establishing multiple connections with targeted nodes and costs practically nothing to the attacker. Our results show that it is possible to slow down the message validation process on honest nodes, which eventually forces them to choose default values on the consensus; leaving the targeted nodes behind in the chain as compared to the non-attacked nodes. Even though our results are subject to the real implementation assumption, the core concept of our attack remains valid. △ Less","28 January, 2019",https://arxiv.org/pdf/1901.10019
A new evaluation framework for topic modeling algorithms based on synthetic corpora,Hanyu Shi;Martin Gerlach;Isabel Diersen;Doug Downey;Luis A. N. Amaral,"Topic models are in widespread use in natural language processing and beyond. Here, we propose a new framework for the evaluation of probabilistic topic modeling algorithms based on synthetic corpora containing an unambiguously defined ground truth topic structure. The major innovation of our approach is the ability to quantify the agreement between the planted and inferred topic structures by comparing the assigned topic labels at the level of the tokens. In experiments, our approach yields novel insights about the relative strengths of topic models as corpus characteristics vary, and the first evidence of an ""undetectable phase"" for topic models when the planted structure is weak. We also establish the practical relevance of the insights gained for synthetic corpora by predicting the performance of topic modeling algorithms in classification tasks in real-world corpora. △ Less","28 January, 2019",https://arxiv.org/pdf/1901.09848
Tangled String for Multi-Scale Explanation of Contextual Shifts in Stock Market,Yukio Ohsawa;Teruaki Hayashi;Takaaki Yoshino,"The original research question here is given by marketers in general, i.e., how to explain the changes in the desired timescale of the market. Tangled String, a sequence visualization tool based on the metaphor where contexts in a sequence are compared to tangled pills in a string, is here extended and diverted to detecting stocks that trigger changes in the market and to explaining the scenario of contextual shifts in the market. Here, the sequential data on the stocks of top 10 weekly increase rates in the First Section of the Tokyo Stock Exchange for 12 years are visualized by Tangled String. The changing in the prices of stocks is a mixture of various timescales and can be explained in the time-scale set as desired by using TS. Also, it is found that the change points found by TS coincided by high precision with the real changes in each stock price. As TS has been created from the data-driven innovation platform called Innovators Marketplace on Data Jackets and is extended to satisfy data users, this paper is as evidence of the contribution of the market of data to data-driven innovations. △ Less","27 January, 2019",https://arxiv.org/pdf/1901.09469
Towards Approximate Mobile Computing,Veljko Pejovic,"Mobile computing is one of the main drivers of innovation, yet the future growth of mobile computing capabilities remains critically threatened by hardware constraints, such as the already extremely dense transistor packing and limited battery capacity. The breakdown of Dennard scaling and stagnating energy storage improvements further amplify these threats. However, the computational burden we put on our mobile devices is not always justified. In a myriad of situations the result of a computation is further manipulated, interpreted, and finally acted upon. This allows for the computation to be relaxed, so that the result is calculated with ""good enough"", not perfect accuracy. For example, results of a Web search may be perfectly acceptable even if the order of the last few listed items is shuffled, as an end user decides which of the available links to follow. Similarly, the quality of a voice-over-IP call may be acceptable, despite being imperfect, as long as the two involved parties can clearly understand each other. This novel way of thinking about computation is termed Approximate Computing (AC) and promises to reduce resource usage, while ensuring that satisfactory performance is delivered to end-users. AC is already experimented with on various levels of desktop computer architecture, from the hardware level where incorrect adders have been designed to sacrifice result correctness for reduced energy consumption, to compiler-level optimisations that omit certain lines of code to speed up video encoding. AC is yet to be attempted on mobile devices and in this article we examine the potential benefits of mobile AC and present an overview of AC techniques applicable in the mobile domain. △ Less","8 January, 2019",https://arxiv.org/pdf/1901.08972
Joint Service Placement and Request Routing in Multi-cell Mobile Edge Computing Networks,Konstantinos Poularakis;Jaime Llorca;Antonia M. Tulino;Ian Taylor;Leandros Tassiulas,"The proliferation of innovative mobile services such as augmented reality, networked gaming, and autonomous driving has spurred a growing need for low-latency access to computing resources that cannot be met solely by existing centralized cloud systems. Mobile Edge Computing (MEC) is expected to be an effective solution to meet the demand for low-latency services by enabling the execution of computing tasks at the network-periphery, in proximity to end-users. While a number of recent studies have addressed the problem of determining the execution of service tasks and the routing of user requests to corresponding edge servers, the focus has primarily been on the efficient utilization of computing resources, neglecting the fact that non-trivial amounts of data need to be stored to enable service execution, and that many emerging services exhibit asymmetric bandwidth requirements. To fill this gap, we study the joint optimization of service placement and request routing in MEC-enabled multi-cell networks with multidimensional (storage-computation-communication) constraints. We show that this problem generalizes several problems in literature and propose an algorithm that achieves close-to-optimal performance using randomized rounding. Evaluation results demonstrate that our approach can effectively utilize the available resources to maximize the number of requests served by low-latency edge cloud servers. △ Less","25 January, 2019",https://arxiv.org/pdf/1901.08946
Curvature-Exploiting Acceleration of Elastic Net Computations,Vien V. Mai;Mikael Johansson,"This paper introduces an efficient second-order method for solving the elastic net problem. Its key innovation is a computationally efficient technique for injecting curvature information in the optimization process which admits a strong theoretical performance guarantee. In particular, we show improved run time over popular first-order methods and quantify the speed-up in terms of statistical measures of the data matrix. The improved time complexity is the result of an extensive exploitation of the problem structure and a careful combination of second-order information, variance reduction techniques, and momentum acceleration. Beside theoretical speed-up, experimental results demonstrate great practical performance benefits of curvature information, especially for ill-conditioned data sets. △ Less","24 January, 2019",https://arxiv.org/pdf/1901.08523
Deep learning and sub-word-unit approach in written art generation,Krzysztof Wołk;Emilia Zawadzka-Gosk;Wojciech Czarnowski,"Automatic poetry generation is novel and interesting application of natural language processing research. It became more popular during the last few years due to the rapid development of technology and neural computing power. This line of research can be applied to the study of linguistics and literature, for social science experiments, or simply for entertainment. The most effective known method of artificial poem generation uses recurrent neural networks (RNN). We also used RNNs to generate poems in the style of Adam Mickiewicz. Our network was trained on the Sir Thaddeus poem. For data pre-processing, we used a specialized stemming tool, which is one of the major innovations and contributions of this work. Our experiment was conducted on the source text, divided into sub-word units (at a level of resolution close to syllables). This approach is novel and is not often employed in the published literature. The subwords units seem to be a natural choice for analysis of the Polish language, as the language is morphologically rich due to cases, gender forms and a large vocabulary. Moreover, Sir Thaddeus contains rhymes, so the analysis of syllables can be meaningful. We verified our model with different settings for the temperature parameter, which controls the randomness of the generated text. We also compared our results with similar models trained on the same text but divided into characters (which is the most common approach alongside the use of full word units). The differences were tremendous. Our solution generated much better poems that were able to follow the metre and vocabulary of the source data text. △ Less","22 January, 2019",https://arxiv.org/pdf/1901.07426
"Dual Graph-Laplacian PCA: A Closed-Form Solution for Bi-clustering to Find ""Checkerboard"" Structures on Gene Expression Data",Jin-Xing Liu;Chun-Mei Feng;Xiang-Zhen Kong;Yong Xu,"In the context of cancer, internal ""checkerboard"" structures are normally found in the matrices of gene expression data, which correspond to genes that are significantly up- or down-regulated in patients with specific types of tumors. In this paper, we propose a novel method, called dual graph-regularization principal component analysis (DGPCA). The main innovation of this method is that it simultaneously considers the internal geometric structures of the condition manifold and the gene manifold. Specifically, we obtain principal components (PCs) to represent the data and approximate the cluster membership indicators through Laplacian embedding. This new method is endowed with internal geometric structures, such as the condition manifold and gene manifold, which are both suitable for bi-clustering. A closed-form solution is provided for DGPCA. We apply this new method to simultaneously cluster genes and conditions (e.g., different samples) with the aim of finding internal ""checkerboard"" structures on gene expression data, if they exist. Then, we use this new method to identify regulatory genes under the particular conditions and to compare the results with those of other state-of-the-art PCA-based methods. Promising results on gene expression data have been verified by extensive experiments △ Less","21 January, 2019",https://arxiv.org/pdf/1901.06794
Neuroflight: Next Generation Flight Control Firmware,William Koch;Renato Mancuso;Azer Bestavros,"Little innovation has been made to low-level attitude flight control used by uncrewed aerial vehicles (UAVs), which still predominantly uses the classical PID controller. In this work we introduce Neuroflight, the first open source neuro-flight controller firmware. We present our toolchain for training a neural network in simulation and compiling it to run on embedded hardware. Challenges faced jumping from simulation to reality are discussed along with our solutions. Our evaluation shows the neural network can execute at over 2.67kHz on an Arm Cortex-M7 processor and flight tests demonstrate a quadcopter running Neuroflight can achieve stable flight and execute aerobatic maneuvers. △ Less","16 September, 2019",https://arxiv.org/pdf/1901.06553
"Analysis of a site's integrity by 3D models and Integrated database, case study : the pic-du-midi high-mountain observatory (France)",Michel Cotte;Florent Laroche;Matthieu Quantin;Loic Jeanson;Nicolas Bourgeois,"The concept of ""integrity"", as currently used in the analysis of World Heritage sites or cultural landscapes mainly consists of 1) the composition of a given site, regarding its origins and its current state of conservation; 2) the visual and functional relationships between its components (attributes). One of the major questions is ""what defines the origin period?"". The integrity analysis has to clearly understand and evaluate which tangible components exist, in order to correctly identify their origin period, estimate how much of the original structure and function remain. An additional difficulty rises in case of scientific and/or technical heritage assessment: the very historical necessity of updating and implementing technical/scientific innovations. These follow advances in science and/or technique(s), and lead to frequent successive changes, impacting the site's structure. Hence, for living sites, one cannot see the origin of the project as the only reference state, as it could be for classical heritage; it is instead requisite to enlighten a chronological series of major reference states. The Pic-du-Midi Observatory (France) is a remarkable example as high-mountain scientific station (2860 m). Its various and successive scientific uses led to a series of reshaping and structural evolutions over around 150 years, till nowadays. We have there a series of reference states very conform to an active scientific station and observatory. The project aims to combine historical data and measurements on the current site's state, to build a succession of 3D models for a series of 5 or 6 'reference state' of the observatory within associated database, and to document the integrity analysis more accurately than usual. The final uses of the digital achievement will also serve for the management of the site and for further development of △ Less","15 January, 2019",https://arxiv.org/pdf/1901.06265
Cognitive Analysis of 360 degree Surround Photos,Madhawa Vidanapathirana;Lakmal Meegahapola;Indika Perera,"360 degrees surround photography or photospheres have taken the world by storm as the new media for content creation providing viewers rich, immersive experience compared to conventional photography. With the emergence of Virtual Reality as a mainstream trend, the 360 degrees photography is increasingly important to offer a practical approach to the general public to capture virtual reality ready content from their mobile phones without explicit tool support or knowledge. Even though the amount of 360-degree surround content being uploaded to the Internet continues to grow, there is no proper way to index them or to process them for further information. This is because of the difficulty in image processing the photospheres due to the distorted nature of objects embedded. This challenge lies in the way 360-degree panoramic photospheres are saved. This paper presents a unique, and innovative technique named Photosphere to Cognition Engine (P2CE), which allows cognitive analysis on 360-degree surround photos using existing image cognitive analysis algorithms and APIs designed for conventional photos. We have optimized the system using a wide variety of indoor and outdoor samples and extensive evaluation approaches. On average, P2CE provides up-to 100% growth in accuracy on image cognitive analysis of Photospheres over direct use of conventional non-photosphere based Image Cognition Systems. △ Less","17 January, 2019",https://arxiv.org/pdf/1901.05634
A System Dynamics Analysis of National R&D Performance Measurement System in Korea,Taekho You;Woo-Sung Jung,"Peer review is one of useful and powerful performance measurement process. In Korea, it needs to increase quality of R&D performance, but bibliometric evaluation and lack of peers have opposite effect. We used system dynamics to describe Korean R&D performance measurement system and ways to increase performance quality. To meet a desired R&D performance quality, increasing fairness and quality of evaluation is needed. Size of peer pool decreased because of the specialization of R&D projects and the Sangpi process both, and it is critical to acquire both fairness and quality. Also, shortening evaluation period affect to R&D performance quality, by causing workloads increase, limiting long-term and innovative R&D projects, and decreasing evaluation quality. Previous evaluation policies do a role like micro-controlling the R&D's activities, but increasing the size of peer pool and changing evaluation period would make a change to quality and fairness of evaluation. △ Less","16 January, 2019",https://arxiv.org/pdf/1901.05447
Learning Direct and Inverse Transmission Matrices,Daniele Ancora;Luca Leuzzi,"Linear problems appear in a variety of disciplines and their application for the transmission matrix recovery is one of the most stimulating challenges in biomedical imaging. Its knowledge turns any random media into an optical tool that can focus or transmit an image through disorder. Here, converting an input-output problem into a statistical mechanical formulation, we investigate how inference protocols can learn the transmission couplings by pseudolikelihood maximization. Bridging linear regression and thermodynamics let us propose an innovative framework to pursue the solution of the scattering-riddle. △ Less","2 February, 2019",https://arxiv.org/pdf/1901.04816
Learning Accurate Extended-Horizon Predictions of High Dimensional Trajectories,Brian Gaudet;Richard Linares;Roberto Furfaro,"We present a novel predictive model architecture based on the principles of predictive coding that enables open loop prediction of future observations over extended horizons. There are two key innovations. First, whereas current methods typically learn to make long-horizon open-loop predictions using a multi-step cost function, we instead run the model open loop in the forward pass during training. Second, current predictive coding models initialize the representation layer's hidden state to a constant value at the start of an episode, and consequently typically require multiple steps of interaction with the environment before the model begins to produce accurate predictions. Instead, we learn a mapping from the first observation in an episode to the hidden state, allowing the trained model to immediately produce accurate predictions. We compare the performance of our architecture to a standard predictive coding model and demonstrate the ability of the model to make accurate long horizon open-loop predictions of simulated Doppler radar altimeter readings during a six degree of freedom Mars landing. Finally, we demonstrate a 2X reduction in sample complexity by using the model to implement a Dyna style algorithm to accelerate policy learning with proximal policy optimization. △ Less","12 January, 2019",https://arxiv.org/pdf/1901.03895
UPSNet: A Unified Panoptic Segmentation Network,Yuwen Xiong;Renjie Liao;Hengshuang Zhao;Rui Hu;Min Bai;Ersin Yumer;Raquel Urtasun,"In this paper, we propose a unified panoptic segmentation network (UPSNet) for tackling the newly proposed panoptic segmentation task. On top of a single backbone residual network, we first design a deformable convolution based semantic segmentation head and a Mask R-CNN style instance segmentation head which solve these two subtasks simultaneously. More importantly, we introduce a parameter-free panoptic head which solves the panoptic segmentation via pixel-wise classification. It first leverages the logits from the previous two heads and then innovatively expands the representation for enabling prediction of an extra unknown class which helps better resolve the conflicts between semantic and instance segmentation. Additionally, it handles the challenge caused by the varying number of instances and permits back propagation to the bottom modules in an end-to-end manner. Extensive experimental results on Cityscapes, COCO and our internal dataset demonstrate that our UPSNet achieves state-of-the-art performance with much faster inference. Code has been made available at: https://github.com/uber-research/UPSNet △ Less","3 April, 2019",https://arxiv.org/pdf/1901.03784
A Secure Connectivity Model for Internet of Things Analytics Service Delivery,Hussain Al-Aqrabi;Richard Hill,"Wide scale interest and adoption of Internet of Things (IoT) technologies is fuelling innovation in the way individuals and even machines can interact to exchange knowledge. One area of particular interest is that of analytics. Ever decreasing form factor hardware is enabling computation and data storage to be embedded into many different devices. The combination of network connectivity and emerging distributed models of service orchestration is allowing the creation of new ways of measuring, monitoring and analysing performance. Using an approach inspired by the NIST seven layer model of cloud computing, we propose a model of connectivity that enables analytics services to be consumed across individual system components that are distributed, such as those found in the IoT and Industrial IoT (IIoT) domains. △ Less","10 January, 2019",https://arxiv.org/pdf/1901.03052
The 2nd Workshop on Hacking and Making at Time-Bounded Events,Ei Pa Pa Pe-Than;Alexander Nolte,"In hackathons, small teams work together over a specified period of time to complete a project of interest. Such time-bounded hackathon-style events have become increasingly popular across different domains in recent years. Collegiate hackathons, just one of the many variants of hackathons, that are supported by the largest hackathon league (https://mlh.io/) alone attract over 65,000 participants among more than 200 events each year. Variously known as data dives, codefests, hack-days, sprints, edit-a-thons, mapathons, and so on, such events vary depending on different audiences and with divergent aims: for example, whether teams know each other beforehand, whether the event is structured as a competition with prizes, whether the event is open or requires membership or invitations, and whether the desired outcome is primarily a product innovation, learning a new skill, forming a community around a cause, solving a technical problem that requires intensive focus by a group, or just having fun. Taken together, hackathons offer new opportunities and challenges for collaboration by affording explicit, predictable, time-bounded spaces for collaborative work and engaging with new audiences. With the goal of discussing opportunities and challenges surrounding hackathons of different kinds, this one-day workshop brought together researchers, experienced event organizers, and practitioners to share and discuss their practical experiences. Empirical insights from studying these events may help position the CHI community to better study, plan and design hackathon-style events as socio-technical systems that support new modes of production and collaboration. △ Less","3 January, 2019",https://arxiv.org/pdf/1901.02710
D3TW: Discriminative Differentiable Dynamic Time Warping for Weakly Supervised Action Alignment and Segmentation,Chien-Yi Chang;De-An Huang;Yanan Sui;Li Fei-Fei;Juan Carlos Niebles,"We address weakly supervised action alignment and segmentation in videos, where only the order of occurring actions is available during training. We propose Discriminative Differentiable Dynamic Time Warping (D3TW), the first discriminative model using weak ordering supervision. The key technical challenge for discriminative modeling with weak supervision is that the loss function of the ordering supervision is usually formulated using dynamic programming and is thus not differentiable. We address this challenge with a continuous relaxation of the min-operator in dynamic programming and extend the alignment loss to be differentiable. The proposed D3TW innovatively solves sequence alignment with discriminative modeling and end-to-end training, which substantially improves the performance in weakly supervised action alignment and segmentation tasks. We show that our model is able to bypass the degenerated sequence problem usually encountered in previous work and outperform the current state-of-the-art across three evaluation metrics in two challenging datasets. △ Less","11 April, 2019",https://arxiv.org/pdf/1901.02598
Paired Open-Ended Trailblazer (POET): Endlessly Generating Increasingly Complex and Diverse Learning Environments and Their Solutions,Rui Wang;Joel Lehman;Jeff Clune;Kenneth O. Stanley,"While the history of machine learning so far largely encompasses a series of problems posed by researchers and algorithms that learn their solutions, an important question is whether the problems themselves can be generated by the algorithm at the same time as they are being solved. Such a process would in effect build its own diverse and expanding curricula, and the solutions to problems at various stages would become stepping stones towards solving even more challenging problems later in the process. The Paired Open-Ended Trailblazer (POET) algorithm introduced in this paper does just that: it pairs the generation of environmental challenges and the optimization of agents to solve those challenges. It simultaneously explores many different paths through the space of possible problems and solutions and, critically, allows these stepping-stone solutions to transfer between problems if better, catalyzing innovation. The term open-ended signifies the intriguing potential for algorithms like POET to continue to create novel and increasingly complex capabilities without bound. Our results show that POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved by direct optimization alone, or even through a direct-path curriculum-building control algorithm introduced to highlight the critical role of open-endedness in solving ambitious challenges. The ability to transfer solutions from one environment to another proves essential to unlocking the full potential of the system as a whole, demonstrating the unpredictable nature of fortuitous stepping stones. We hope that POET will inspire a new push towards open-ended discovery across many domains, where algorithms like POET can blaze a trail through their interesting possible manifestations and solutions. △ Less","20 February, 2019",https://arxiv.org/pdf/1901.01753
Should I stay or should I go: Analysis of the impact of application QoS on user engagement in YouTube,Maria Plakia;Evripides Tzamousis;Thomais Asvestopoulou;Giorgos Pantermakis;Nick Filippakis;Henning Schulzrinne;Yana Kane-Esrig;Maria Papadopouli,"To improve the quality of experience (QoE), especially under moderate to high traffic demand, it is important to understand the impact of the network and application QoS on user experience. This paper comparatively evaluates the impact of impairments, their intensity and temporal dynamics, on user engagement in the context of video streaming. The analysis employed two large YouTube datasets. To characterize the user engagement and the impact of impairments, several new metrics were defined. We assessed whether or not there is a statistically significant relationship between different types of impairments and QoE and user engagement metrics, taking into account not only the characteristics of the impairments but also the covariates of the session (e.g., video duration, mean datarate). After observing the relationships across the entire dataset, we tested whether these relationships also persist under specific conditions with respect to the covariates. The introduction of several new metrics and of various covariates in the analysis are two innovative aspects of this work. We found that the number of negative bitrate changes (BR-) is a stronger predictor of abandonment than rebufferrings (RB). Even positive bitrate changes (BR+) are associated with increases in abandonment. Specifically, BR+ in low resolution sessions is not well received. Temporal dynamics of the impairments have also an impact: a BR- that follows much later a RB appears to be perceived as a worse impairment than a BR- that occurs immediately after a RB. These results can be used to guide the design of the video streaming adaptation as well as suggest which parameters should be varied in controlled field studies. △ Less","15 March, 2019",https://arxiv.org/pdf/1901.01603
Dynamic Visualization and Fast Computation for Convex Clustering via Algorithmic Regularization,Michael Weylandt;John Nagorski;Genevera I. Allen,"Convex clustering is a promising new approach to the classical problem of clustering, combining strong performance in empirical studies with rigorous theoretical foundations. Despite these advantages, convex clustering has not been widely adopted, due to its computationally intensive nature and its lack of compelling visualizations. To address these impediments, we introduce Algorithmic Regularization, an innovative technique for obtaining high-quality estimates of regularization paths using an iterative one-step approximation scheme. We justify our approach with a novel theoretical result, guaranteeing global convergence of the approximate path to the exact solution under easily-checked non-data-dependent assumptions. The application of algorithmic regularization to convex clustering yields the Convex Clustering via Algorithmic Regularization Paths (CARP) algorithm for computing the clustering solution path. On example data sets from genomics and text analysis, CARP delivers over a 100-fold speed-up over existing methods, while attaining a finer approximation grid than standard methods. Furthermore, CARP enables improved visualization of clustering solutions: the fine solution grid returned by CARP can be used to construct a convex clustering-based dendrogram, as well as forming the basis of a dynamic path-wise visualization based on modern web technologies. Our methods are implemented in the open-source R package clustRviz, available at https://github.com/DataSlingers/clustRviz. △ Less","8 July, 2019",https://arxiv.org/pdf/1901.01477
HG-Caffe: Mobile and Embedded Neural Network GPU (OpenCL) Inference Engine with FP16 Supporting,Zhuoran Ji,"Breakthroughs in the fields of deep learning and mobile system-on-chips are radically changing the way we use our smartphones. However, deep neural networks inference is still a challenging task for edge AI devices due to the computational overhead on mobile CPUs and a severe drain on the batteries. In this paper, we present a deep neural network inference engine named HG-Caffe, which supports GPUs with half precision. HG-Caffe provides up to 20 times speedup with GPUs compared to the original implementations. In addition to the speedup, the peak memory usage is also reduced to about 80%. With HG-Caffe, more innovative and fascinating mobile applications will be turned into reality. △ Less","2 January, 2019",https://arxiv.org/pdf/1901.00858
An adaptive stigmergy-based system for evaluating technological indicator dynamics in the context of smart specialization,A. L. Alfeo;F. P. Appio;M. G. C. A. Cimino;A. Lazzeri;A. Martini;G. Vaglini,"Regional innovation is more and more considered an important enabler of welfare. It is no coincidence that the European Commission has started looking at regional peculiarities and dynamics, in order to focus Research and Innovation Strategies for Smart Specialization towards effective investment policies. In this context, this work aims to support policy makers in the analysis of innovation-relevant trends. We exploit a European database of the regional patent application to determine the dynamics of a set of technological innovation indicators. For this purpose, we design and develop a software system for assessing unfolding trends in such indicators. In contrast with conventional knowledge-based design, our approach is biologically-inspired and based on self-organization of information. This means that a functional structure, called track, appears and stays spontaneous at runtime when local dynamism in data occurs. A further prototyping of tracks allows a better distinction of the critical phenomena during unfolding events, with a better assessment of the progressing levels. The proposed mechanism works if structural parameters are correctly tuned for the given historical context. Determining such correct parameters is not a simple task since different indicators may have different dynamics. For this purpose, we adopt an adaptation mechanism based on differential evolution. The study includes the problem statement and its characterization in the literature, as well as the proposed solving approach, experimental setting and results. △ Less","2 January, 2019",https://arxiv.org/pdf/1901.00553
Measuring Physical Activity of Older Adults via Smartwatch and Stigmergic Receptive Fields,A. L. Alfeo;M. G. C. A. Cimino;G. Vaglini,"Physical activity level (PAL) in older adults can enhance healthy aging, improve functional capacity, and prevent diseases. It is known that human annotations of PAL can be affected by subjectivity and inaccuracy. Recently developed smart devices can allow a non-invasive, analytic, and continuous gathering of physiological signals. We present an innovative computational system fed by signals of heartbeat rate, wrist motion and pedometer sensed by a smartwatch. More specifically, samples of each signal are aggregated by functional structures called trails. The trailing process is inspired by stigmergy, an insects' coordination mechanism, and is managed by computational units called stigmergic receptive fields (SRFs). SRFs, which compute the similarity between trails, are arranged in a stigmergic perceptron to detect a collection of micro-behaviours of the raw signal, called archetypes. A SRF is adaptive to subjects: its structural parameters are tuned by a differential evolution algorithm. SRFs are used in a multilayer architecture, providing further levels of processing to realize macro analyses in the application domain. As a result, the architecture provides a daily PAL, useful to detect behavioural shift indicating initial signs of disease or deviations in performance. As a proof of concept, the approach has been experimented on three subjects. △ Less","2 January, 2019",https://arxiv.org/pdf/1901.00552
Agile Development at Scale: The Next Frontier,Torgeir Dingsøyr;Davide Falessi;Ken Power,"Agile methods have transformed the way software is developed, emphasizing active end-user involvement, tolerance to change, and evolutionary delivery of products. The first special issue on agile development described the methods as focusing on ""feedback and change"". These methods have led to major changes in how software is developed. Scrum is now the most common framework for development in most countries, and other methods like extreme programming (XP) and elements of lean software development and Kanban are widely used. What started as a bottom-up movement amongst software practitioners and consultants has been taken up by major international consulting companies who prescribe agile development, particularly for contexts where learning and innovation are key. Agile development methods have attracted interest primarily in software engineering, but also in a number of other disciplines including information systems and project management. The agile software development methods were originally targeted towards small, co-located development teams, but are increasingly applied in other contexts. They were initially used to develop Web systems and internal IT systems, but are now used in a range of domains, including mission-critical systems. Methods that were designed for single teams of 5-9 developers have been adapted for use in projects with tens of teams, hundreds of developers, which can involve integration with hundreds of existing systems and affect hundreds of thousands of users. △ Less","2 January, 2019",https://arxiv.org/pdf/1901.00324
Multitask Learning Deep Neural Networks to Combine Revealed and Stated Preference Data,Shenhao Wang;Qingyi Wang;Jinhua Zhao,"It is an enduring question how to combine revealed preference (RP) and stated preference (SP) data to analyze travel behavior. This study presents a framework of multitask learning deep neural networks (MTLDNNs) for this question, and demonstrates that MTLDNNs are more generic than the traditional nested logit (NL) method, due to its capacity of automatic feature learning and soft constraints. About 1,500 MTLDNN models are designed and applied to the survey data that was collected in Singapore and focused on the RP of four current travel modes and the SP with autonomous vehicles (AV) as the one new travel mode in addition to those in RP. We found that MTLDNNs consistently outperform six benchmark models and particularly the classical NL models by about 5% prediction accuracy in both RP and SP datasets. This performance improvement can be mainly attributed to the soft constraints specific to MTLDNNs, including its innovative architectural design and regularization methods, but not much to the generic capacity of automatic feature learning endowed by a standard feedforward DNN architecture. Besides prediction, MTLDNNs are also interpretable. The empirical results show that AV is mainly the substitute of driving and AV alternative-specific variables are more important than the socio-economic variables in determining AV adoption. Overall, this study introduces a new MTLDNN framework to combine RP and SP, and demonstrates its theoretical flexibility and empirical power for prediction and interpretation. Future studies can design new MTLDNN architectures to reflect the speciality of RP and SP and extend this work to other behavioral analysis. △ Less","22 August, 2019",https://arxiv.org/pdf/1901.00227
"Steerable e
PCA: Rotationally Invariant Exponential Family PCA",Zhizhen Zhao;Lydia T. Liu;Amit Singer,"In photon-limited imaging, the pixel intensities are affected by photon count noise. Many applications, such as 3-D reconstruction using correlation analysis in X-ray free electron laser (XFEL) single molecule imaging, require an accurate estimation of the covariance of the underlying 2-D clean images. Accurate estimation of the covariance from low-photon count images must take into account that pixel intensities are Poisson distributed, hence the classical sample covariance estimator is sub-optimal. Moreover, in single molecule imaging, including in-plane rotated copies of all images could further improve the accuracy of covariance estimation. In this paper we introduce an efficient and accurate algorithm for covariance matrix estimation of count noise 2-D images, including their uniform planar rotations and possibly reflections. Our procedure, steerable ePCA, combines in a novel way two recently introduced innovations. The first is a methodology for principal component analysis (PCA) for Poisson distributions, and more generally, exponential family distributions, called ePCA. The second is steerable PCA, a fast and accurate procedure for including all planar rotations for PCA. The resulting principal components are invariant to the rotation and reflection of the input images. We demonstrate the efficiency and accuracy of steerable ePCA in numerical experiments involving simulated XFEL datasets and rotated Yale B face data. △ Less","17 December, 2019",https://arxiv.org/pdf/1812.08789
Malthusian Reinforcement Learning,Joel Z. Leibo;Julien Perolat;Edward Hughes;Steven Wheelwright;Adam H. Marblestone;Edgar Duéñez-Guzmán;Peter Sunehag;Iain Dunning;Thore Graepel,"Here we explore a new algorithmic framework for multi-agent reinforcement learning, called Malthusian reinforcement learning, which extends self-play to include fitness-linked population size dynamics that drive ongoing innovation. In Malthusian RL, increases in a subpopulation's average return drive subsequent increases in its size, just as Thomas Malthus argued in 1798 was the relationship between preindustrial income levels and population growth. Malthusian reinforcement learning harnesses the competitive pressures arising from growing and shrinking population size to drive agents to explore regions of state and policy spaces that they could not otherwise reach. Furthermore, in environments where there are potential gains from specialization and division of labor, we show that Malthusian reinforcement learning is better positioned to take advantage of such synergies than algorithms based on self-play. △ Less","3 March, 2019",https://arxiv.org/pdf/1812.07019
Spatial Fusion GAN for Image Synthesis,Fangneng Zhan;Hongyuan Zhu;Shijian Lu,"Recent advances in generative adversarial networks (GANs) have shown great potentials in realistic image synthesis whereas most existing works address synthesis realism in either appearance space or geometry space but few in both. This paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a geometry synthesizer and an appearance synthesizer to achieve synthesis realism in both geometry and appearance spaces. The geometry synthesizer learns contextual geometries of background images and transforms and places foreground objects into the background images unanimously. The appearance synthesizer adjusts the color, brightness and styles of the foreground objects and embeds them into background images harmoniously, where a guided filter is introduced for detail preserving. The two synthesizers are inter-connected as mutual references which can be trained end-to-end without supervision. The SF-GAN has been evaluated in two tasks: (1) realistic scene text image synthesis for training better recognition models; (2) glass and hat wearing for realistic matching glasses and hats with real portraits. Qualitative and quantitative comparisons with the state-of-the-art demonstrate the superiority of the proposed SF-GAN. △ Less","2 April, 2019",https://arxiv.org/pdf/1812.05840
ESIR: End-to-end Scene Text Recognition via Iterative Image Rectification,Fangneng Zhan;Shijian Lu,"Automated recognition of texts in scenes has been a research challenge for years, largely due to the arbitrary variation of text appearances in perspective distortion, text line curvature, text styles and different types of imaging artifacts. The recent deep networks are capable of learning robust representations with respect to imaging artifacts and text style changes, but still face various problems while dealing with scene texts with perspective and curvature distortions. This paper presents an end-to-end trainable scene text recognition system (ESIR) that iteratively removes perspective distortion and text line curvature as driven by better scene text recognition performance. An innovative rectification network is developed which employs a novel line-fitting transformation to estimate the pose of text lines in scenes. In addition, an iterative rectification pipeline is developed where scene text distortions are corrected iteratively towards a fronto-parallel view. The ESIR is also robust to parameter initialization and the training needs only scene text images and word-level annotations as required by most scene text recognition systems. Extensive experiments over a number of public datasets show that the proposed ESIR is capable of rectifying scene text distortions accurately, achieving superior recognition performance for both normal scene text images and those suffering from perspective and curvature distortions. △ Less","2 April, 2019",https://arxiv.org/pdf/1812.05824
Von Mises-Fisher Loss for Training Sequence to Sequence Models with Continuous Outputs,Sachin Kumar;Yulia Tsvetkov,"The Softmax function is used in the final layer of nearly all existing sequence-to-sequence models for language generation. However, it is usually the slowest layer to compute which limits the vocabulary size to a subset of most frequent types; and it has a large memory footprint. We propose a general technique for replacing the softmax layer with a continuous embedding layer. Our primary innovations are a novel probabilistic loss, and a training and inference procedure in which we generate a probability distribution over pre-trained word embeddings, instead of a multinomial distribution over the vocabulary obtained via softmax. We evaluate this new class of sequence-to-sequence models with continuous outputs on the task of neural machine translation. We show that our models obtain upto 2.5x speed-up in training time while performing on par with the state-of-the-art models in terms of translation quality. These models are capable of handling very large vocabularies without compromising on translation quality. They also produce more meaningful errors than in the softmax-based models, as these errors typically lie in a subspace of the vector space of the reference translations. △ Less","21 March, 2019",https://arxiv.org/pdf/1812.04616
DRONE: a Distributed Subgraph-Centric Framework for Processing Large Scale Power-law Graphs,Xiaole Wen;Shuai Zhang;Haihang You,"Nowadays, in the big data era, social networks, graph databases, knowledge graphs, electronic commerce etc. demand efficient and scalable capability to process an ever increasing volume of graph-structured data. To meet the challenge, two mainstream distributed programming models, vertex-centric (VC) and subgraph-centric (SC) were proposed. Compared to the VC model, the SC model converges faster with less communication overhead on well-partitioned graphs, and is easy to program due to the ""think like a graph"" philosophy. The edge-cut method is considered as a natural choice of subgraph-centric model for graph partitioning, and has been adopted by Giraph++, Blogel and GRAPE. However, the edge-cut method causes significant performance bottleneck for processing large scale power-law graphs. Thus, the SC model is less competitive in practice. In this paper, we present an innovative distributed graph computing framework, DRONE (Distributed gRaph cOmputiNg Engine). It combines the subgraph-centric model and the vertex-cut graph partitioning strategy. Experiments show that DRONE outperforms the state-of-art distributed graph computing engines on real-world graphs and synthetic power-law graphs. DRONE is capable of scaling up to process one-trillion-edge synthetic power-law graphs, which is orders of magnitude larger than previously reported by existing SC-based frameworks. △ Less","9 January, 2019",https://arxiv.org/pdf/1812.04380
SDNet: Contextualized Attention-based Deep Network for Conversational Question Answering,Chenguang Zhu;Michael Zeng;Xuedong Huang,"Conversational question answering (CQA) is a novel QA task that requires understanding of dialogue context. Different from traditional single-turn machine reading comprehension (MRC) tasks, CQA includes passage comprehension, coreference resolution, and contextual understanding. In this paper, we propose an innovated contextualized attention-based deep neural network, SDNet, to fuse context into traditional MRC models. Our model leverages both inter-attention and self-attention to comprehend conversation context and extract relevant information from passage. Furthermore, we demonstrated a novel method to integrate the latest BERT contextual model. Empirical results show the effectiveness of our model, which sets the new state of the art result in CoQA leaderboard, outperforming the previous best model by 1.6% F1. Our ensemble model further improves the result by 2.7% F1. △ Less","2 January, 2019",https://arxiv.org/pdf/1812.03593
Zoom-In-to-Check: Boosting Video Interpolation via Instance-level Discrimination,Liangzhe Yuan;Yibo Chen;Hantian Liu;Tao Kong;Jianbo Shi,"We propose a light-weight video frame interpolation algorithm. Our key innovation is an instance-level supervision that allows information to be learned from the high-resolution version of similar objects. Our experiment shows that the proposed method can generate state-of-the-art results across different datasets, with fractional computation resources (time and memory) of competing methods. Given two image frames, a cascade network creates an intermediate frame with 1) a flow-warping module that computes coarse bi-directional optical flow and creates an interpolated image via flow-based warping, followed by 2) an image synthesis module to make fine-scale corrections. In the learning stage, object detection proposals are generated on the interpolated image.Lower resolution objects are zoomed into, and the learning algorithms using an adversarial loss trained on high-resolution objects to guide the system towards the instance-level refinement corrects details of object shape and boundaries. △ Less","27 April, 2019",https://arxiv.org/pdf/1812.01210
"The Art, Science, and Engineering of Fuzzing: A Survey",Valentin J. M. Manes;HyungSeok Han;Choongwoo Han;Sang Kil Cha;Manuel Egele;Edward J. Schwartz;Maverick Woo,"Among the many software vulnerability discovery techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective. △ Less","7 April, 2019",https://arxiv.org/pdf/1812.00140
A Tutorial on Formulating and Using QUBO Models,Fred Glover;Gary Kochenberger;Yu Du,"The Quadratic Unconstrained Binary Optimization (QUBO) model has gained prominence in recent years with the discovery that it unifies a rich variety of combinatorial optimization problems. By its association with the Ising problem in physics, the QUBO model has emerged as an underpinning of the quantum computing area known as quantum annealing and has become a subject of study in neuromorphic computing. Through these connections, QUBO models lie at the heart of experimentation carried out with quantum computers developed by D-Wave Systems and neuromorphic computers developed by IBM. Computational experience is being amassed by both the classical and the quantum computing communities that highlights not only the potential of the QUBO model but also its effectiveness as an alternative to traditional modeling and solution methodologies. This tutorial discloses the basic features of the QUBO model that give it the power and flexibility to encompass the range of applications that have thrust it onto center stage of the optimization field. We show how many different types of constraining relationships arising in practice can be embodied within the ""unconstrained"" QUBO formulation in a very natural manner using penalty functions, yielding exact model representations in contrast to the approximate representations produced by customary uses of penalty functions. Each step of generating such models is illustrated in detail by simple numerical examples, to highlight the convenience of using QUBO models in numerous settings. We also describe recent innovations for solving QUBO models that offer a fertile avenue for integrating classical and quantum computing and for applying these models in machine learning. △ Less","4 November, 2019",https://arxiv.org/pdf/1811.11538
Frustrated with Replicating Claims of a Shared Model? A Solution,Abdul Dakkak;Cheng Li;Jinjun Xiong;Wen-Mei Hwu,"Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that model owners and evaluators are hard-pressed analyzing and studying them. This is exacerbated by the complicated procedures for evaluation. The lack of standard systems and efficient techniques for specifying and provisioning ML/DL evaluation is the main cause of this ""pain point"". This work discusses common pitfalls for replicating DL model evaluation, and shows that these subtle pitfalls can affect both accuracy and performance. It then proposes a solution to remedy these pitfalls called MLModelScope, a specification for repeatable model evaluation and a runtime to provision and measure experiments. We show that by easing the model specification and evaluation process, MLModelScope facilitates rapid adoption of ML/DL innovations. △ Less","25 June, 2019",https://arxiv.org/pdf/1811.09737
Re-Identification with Consistent Attentive Siamese Networks,Meng Zheng;Srikrishna Karanam;Ziyan Wu;Richard J. Radke,"We propose a new deep architecture for person re-identification (re-id). While re-id has seen much recent progress, spatial localization and view-invariant representation learning for robust cross-view matching remain key, unsolved problems. We address these questions by means of a new attention-driven Siamese learning architecture, called the Consistent Attentive Siamese Network. Our key innovations compared to existing, competing methods include (a) a flexible framework design that produces attention with only identity labels as supervision, (b) explicit mechanisms to enforce attention consistency among images of the same person, and (c) a new Siamese framework that integrates attention and attention consistency, producing principled supervisory signals as well as the first mechanism that can explain the reasoning behind the Siamese framework's predictions. We conduct extensive evaluations on the CUHK03-NP, DukeMTMC-ReID, and Market-1501 datasets and report competitive performance. △ Less","11 April, 2019",https://arxiv.org/pdf/1811.07487
Sharpen Focus: Learning with Attention Separability and Consistency,Lezi Wang;Ziyan Wu;Srikrishna Karanam;Kuan-Chuan Peng;Rajat Vikram Singh;Bo Liu;Dimitris N. Metaxas,"Recent developments in gradient-based attention modeling have seen attention maps emerge as a powerful tool for interpreting convolutional neural networks. Despite good localization for an individual class of interest, these techniques produce attention maps with substantially overlapping responses among different classes, leading to the problem of visual confusion and the need for discriminative attention. In this paper, we address this problem by means of a new framework that makes class-discriminative attention a principled part of the learning process. Our key innovations include new learning objectives for attention separability and cross-layer consistency, which result in improved attention discriminability and reduced visual confusion. Extensive experiments on image classification benchmarks show the effectiveness of our approach in terms of improved classification accuracy, including CIFAR-100 (+3.33%), Caltech-256 (+1.64%), ILSVRC2012 (+0.92%), CUB-200-2011 (+4.8%) and PASCAL VOC2012 (+5.73%). △ Less","7 August, 2019",https://arxiv.org/pdf/1811.07484
RePr: Improved Training of Convolutional Filters,Aaditya Prakash;James Storer;Dinei Florencio;Cha Zhang,"A well-trained Convolutional Neural Network can easily be pruned without significant loss of performance. This is because of unnecessary overlap in the features captured by the network's filters. Innovations in network architecture such as skip/dense connections and Inception units have mitigated this problem to some extent, but these improvements come with increased computation and memory requirements at run-time. We attempt to address this problem from another angle - not by changing the network structure but by altering the training method. We show that by temporarily pruning and then restoring a subset of the model's filters, and repeating this process cyclically, overlap in the learned features is reduced, producing improved generalization. We show that the existing model-pruning criteria are not optimal for selecting filters to prune in this context and introduce inter-filter orthogonality as the ranking criteria to determine under-expressive filters. Our method is applicable both to vanilla convolutional networks and more complex modern architectures, and improves the performance across a variety of tasks, especially when applied to smaller networks. △ Less","25 February, 2019",https://arxiv.org/pdf/1811.07275
Learning Local RGB-to-CAD Correspondences for Object Pose Estimation,Georgios Georgakis;Srikrishna Karanam;Ziyan Wu;Jana Kosecka,"We consider the problem of 3D object pose estimation. While much recent work has focused on the RGB domain, the reliance on accurately annotated images limits their generalizability and scalability. On the other hand, the easily available CAD models of objects are rich sources of data, providing a large number of synthetically rendered images. In this paper, we solve this key problem of existing methods requiring expensive 3D pose annotations by proposing a new method that matches RGB images to CAD models for object pose estimation. Our key innovations compared to existing work include removing the need for either real-world textures for CAD models or explicit 3D pose annotations for RGB images. We achieve this through a series of objectives that learn how to select keypoints and enforce viewpoint and modality invariance across RGB images and CAD model renderings. We conduct extensive experiments to demonstrate that the proposed method can reliably estimate object pose in RGB images, as well as generalize to object instances not seen during training. △ Less","31 July, 2019",https://arxiv.org/pdf/1811.07249
Evolving intrinsic motivations for altruistic behavior,Jane X. Wang;Edward Hughes;Chrisantha Fernando;Wojciech M. Czarnecki;Edgar A. Duenez-Guzman;Joel Z. Leibo,"Multi-agent cooperation is an important feature of the natural world. Many tasks involve individual incentives that are misaligned with the common good, yet a wide range of organisms from bacteria to insects and humans are able to overcome their differences and collaborate. Therefore, the emergence of cooperative behavior amongst self-interested individuals is an important question for the fields of multi-agent reinforcement learning (MARL) and evolutionary theory. Here, we study a particular class of multi-agent problems called intertemporal social dilemmas (ISDs), where the conflict between the individual and the group is particularly sharp. By combining MARL with appropriately structured natural selection, we demonstrate that individual inductive biases for cooperation can be learned in a model-free way. To achieve this, we introduce an innovative modular architecture for deep reinforcement learning agents which supports multi-level selection. We present results in two challenging environments, and interpret these in the context of cultural and ecological evolution. △ Less","11 March, 2019",https://arxiv.org/pdf/1811.05931
A Ginga-enabled Digital Radio Mondiale Broadcasting chain: Signaling and Definitions,Rafael Diniz;Alan L. V. Guedes;Sergio Colcher,"ISDB-T International standard is currently adopted by most Latin America countries and is already installed in most TV sets sold in recent years in the region. To support interactive applications in Digital TV receivers, ISDB-T defines the middleware Ginga. Similar to Digital TV, Digital Radio standards also provide the means to carry interactive applications; however, their specifications for interactive applications are usually more restricted than the ones used in Digital TV. Also, interactive applications for Digital TV and Digital Radio are usually incompatible. Motivated by such observations, this report considers the importance of interactive applications for both TV and Radio Broadcasting and the advantages of using the same middleware and languages specification for Digital TV and Radio. More specifically, it establishes the signaling and definitions on how to transport and execute Ginga-NCL and Ginga-HTML5 applications over DRM (Digital Radio Mondiale) transmission. Ministry of Science, Technology, Innovation and Communication of Brazil is carrying trials with Digital Radio Mondiale standard in order to define the reference model of the Brazilian Digital Radio System (Portuguese: Sistema Brasileiro de Rádio Digital - SBRD). △ Less","12 June, 2019",https://arxiv.org/pdf/1811.04193
Credit Card Fraud Detection in e-Commerce: An Outlier Detection Approach,Utkarsh Porwal;Smruthi Mukund,"Often the challenge associated with tasks like fraud and spam detection is the lack of all likely patterns needed to train suitable supervised learning models. This problem accentuates when the fraudulent patterns are not only scarce, they also change over time. Change in fraudulent pattern is because fraudsters continue to innovate novel ways to circumvent measures put in place to prevent fraud. Limited data and continuously changing patterns makes learning significantly difficult. We hypothesize that good behavior does not change with time and data points representing good behavior have consistent spatial signature under different groupings. Based on this hypothesis we are proposing an approach that detects outliers in large data sets by assigning a consistency score to each data point using an ensemble of clustering methods. Our main contribution is proposing a novel method that can detect outliers in large datasets and is robust to changing patterns. We also argue that area under the ROC curve, although a commonly used metric to evaluate outlier detection methods is not the right metric. Since outlier detection problems have a skewed distribution of classes, precision-recall curves are better suited because precision compares false positives to true positives (outliers) rather than true negatives (inliers) and therefore is not affected by the problem of class imbalance. We show empirically that area under the precision-recall curve is a better than ROC as an evaluation metric. The proposed approach is tested on the modified version of the Landsat satellite dataset, the modified version of the ann-thyroid dataset and a large real world credit card fraud detection dataset available through Kaggle where we show significant improvement over the baseline methods. △ Less","6 May, 2019",https://arxiv.org/pdf/1811.02196
Spreading of Memes on Multiplex Networks,Joseph D. O'Brien;Ioannis K. Dassios;James P. Gleeson,"A model for the spreading of online information or ""memes"" on multiplex networks is introduced and analyzed using branching-process methods. The model generalizes that of [Gleeson et al., Phys.Rev. X., 2016] in two ways. First, even for a monoplex (single-layer) network, the model is defined for any specific network defined by its adjacency matrix, instead of being restricted to an ensemble of random networks. Second, a multiplex version of the model is introduced to capture the behaviour of users who post information from one social media platform to another. In both cases the branching process analysis demonstrates that the dynamical system is, in the limit of low innovation, poised near a critical point, which is known to lead to heavy-tailed distributions of meme popularity similar to those observed in empirical data. △ Less","28 February, 2019",https://arxiv.org/pdf/1810.12630
Chain of Antichains: An Efficient and Secure Distributed Ledger Technology and Its Applications,Jinwook Lee;Paul Moon Sub Choi,"Since the inception of blockchain and Bitcoin (Nakamoto (2008)), a decentralized-distributed ledger system and its associated cryptocurrency, respectively, the world has witnessed a slew of newer adaptations and applications. Although the original distributed ledger technology (DLT) of blockchain is deemed secure and decentralized, the confirmation of transactions is inefficient by design. Recently adopted, directed acyclic graph (DAG)-based distributed ledgers validate transactions efficiently without the physically and environmentally costly building process of blocks (Lerner (2015)). However, centrally-controlled confirmation against the odds of multiple validation disqualifies the DAG as a decentralized-distributed ledger. In this regard, we introduce an innovative DLT by reconstructing a chain of antichains based on a given DAG-pool of transactions. Each antichain (box) contains distinct nodes whose approved transactions are recursively validated by subsequently augmenting nodes. The boxer node closes the box and keeps the hash of all transactions confirmed by the box-genesis node. Designation of boxers and box-geneses is conditionally randomized for decentralization. The boxes are serially concatenated with recursive confirmation (boxchain) without incurring the cost of box generation. Rewards (boxcoin) are paid to the contributing nodes of the ecosystem whose trust is built on the doubly-secure protocol of confirmation. A value-preserving medium of payment (boxdollar) is among numerous practical applications discussed herein. △ Less","14 January, 2019",https://arxiv.org/pdf/1810.11871
Clustering Time Series with Nonlinear Dynamics: A Bayesian Non-Parametric and Particle-Based Approach,Alexander Lin;Yingzhuo Zhang;Jeremy Heng;Stephen A. Allsop;Kay M. Tye;Pierre E. Jacob;Demba Ba,"We propose a general statistical framework for clustering multiple time series that exhibit nonlinear dynamics into an a-priori-unknown number of sub-groups. Our motivation comes from neuroscience, where an important problem is to identify, within a large assembly of neurons, subsets that respond similarly to a stimulus or contingency. Upon modeling the multiple time series as the output of a Dirichlet process mixture of nonlinear state-space models, we derive a Metropolis-within-Gibbs algorithm for full Bayesian inference that alternates between sampling cluster assignments and sampling parameter values that form the basis of the clustering. The Metropolis step employs recent innovations in particle-based methods. We apply the framework to clustering time series acquired from the prefrontal cortex of mice in an experiment designed to characterize the neural underpinnings of fear. △ Less","4 March, 2019",https://arxiv.org/pdf/1810.09920
OCAPIS: R package for Ordinal Classification And Preprocessing In Scala,M. Cristina Heredia-Gómez;Salvador García;Pedro Antonio Gutiérrez;Francisco Herrera,"Ordinal Data are those where a natural order exist between the labels. The classification and pre-processing of this type of data is attracting more and more interest in the area of machine learning, due to its presence in many common problems. Traditionally, ordinal classification problems have been approached as nominal problems. However, that implies not taking into account their natural order constraints. In this paper, an innovative R package named ocapis (Ordinal Classification and Preprocessing In Scala) is introduced. Implemented mainly in Scala and available through Github, this library includes four learners and two pre-processing algorithms for ordinal and monotonic data. Main features of the package and examples of installation and use are explained throughout this manuscript. △ Less","17 March, 2019",https://arxiv.org/pdf/1810.09733
Urban Swarms: A new approach for autonomous waste management,Antonio Luca Alfeo;Eduardo Castelló Ferrer;Yago Lizarribar Carrillo;Arnaud Grignard;Luis Alonso Pastor;Dylan T. Sleeper;Mario G. C. A. Cimino;Bruno Lepri;Gigliola Vaglini;Kent Larson;Marco Dorigo;Alex `Sandy' Pentland,"Modern cities are growing ecosystems that face new challenges due to the increasing population demands. One of the many problems they face nowadays is waste management, which has become a pressing issue requiring new solutions. Swarm robotics systems have been attracting an increasing amount of attention in the past years and they are expected to become one of the main driving factors for innovation in the field of robotics. The research presented in this paper explores the feasibility of a swarm robotics system in an urban environment. By using bio-inspired foraging methods such as multi-place foraging and stigmergy-based navigation, a swarm of robots is able to improve the efficiency and autonomy of the urban waste management system in a realistic scenario. To achieve this, a diverse set of simulation experiments was conducted using real-world GIS data and implementing different garbage collection scenarios driven by robot swarms. Results presented in this research show that the proposed system outperforms current approaches. Moreover, results not only show the efficiency of our solution, but also give insights about how to design and customize these systems. △ Less","1 March, 2019",https://arxiv.org/pdf/1810.07910
Deterministic Variational Inference for Robust Bayesian Neural Networks,Anqi Wu;Sebastian Nowozin;Edward Meeds;Richard E. Turner;José Miguel Hernández-Lobato;Alexander L. Gaunt,"Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches. △ Less","7 March, 2019",https://arxiv.org/pdf/1810.03958
A scalable parallel finite element framework for growing geometries. Application to metal additive manufacturing,Eric Neiva;Santiago Badia;Alberto F. Martín;Michele Chiumenti,"This work introduces an innovative parallel, fully-distributed finite element framework for growing geometries and its application to metal additive manufacturing. It is well-known that virtual part design and qualification in additive manufacturing requires highly-accurate multiscale and multiphysics analyses. Only high performance computing tools are able to handle such complexity in time frames compatible with time-to-market. However, efficiency, without loss of accuracy, has rarely held the centre stage in the numerical community. Here, in contrast, the framework is designed to adequately exploit the resources of high-end distributed-memory machines. It is grounded on three building blocks: (1) Hierarchical adaptive mesh refinement with octree-based meshes; (2) a parallel strategy to model the growth of the geometry; (3) state-of-the-art parallel iterative linear solvers. Computational experiments consider the heat transfer analysis at the part scale of the printing process by powder-bed technologies. After verification against a 3D benchmark, a strong-scaling analysis assesses performance and identifies major sources of parallel overhead. A third numerical example examines the efficiency and robustness of (2) in a curved 3D shape. Unprecedented parallelism and scalability were achieved in this work. Hence, this framework contributes to take on higher complexity and/or accuracy, not only of part-scale simulations of metal or polymer additive manufacturing, but also in welding, sedimentation, atherosclerosis, or any other physical problem where the physical domain of interest grows in time. △ Less","27 March, 2019",https://arxiv.org/pdf/1810.03506
VeilGraph: Streaming Graph Approximations,Miguel E. Coimbra;Sérgio Esteves;Alexandre P. Francisco;Luís Veiga,"Graphs are found in a plethora of domains, including online social networks, the World Wide Web and the study of epidemics, to name a few. With the advent of greater volumes of information and the need for continuously updated results under temporal constraints, it is necessary to explore novel approaches that further enable performance improvements. In the scope of stream processing over graphs, we research the trade-offs between result accuracy and the speedup of approximate computation techniques. We see this as a natural path towards these performance improvements. Herein we present \name, through which we conducted our research. We showcase an innovative model for approximate graph processing, implemented in \texttt{Apache Flink}. We analyze our model and evaluate it with the case study of the PageRank algorithm \cite{pageRank}, perhaps the most famous measure of vertex centrality used to rank websites in search engine results. %In light of our model, we discuss the challenges driven by relations between result accuracy and potential performance gains. Our experiments, even when set up for favoring \texttt{Flink} for comparability, show that \name can improve performance up to 3X speedups, while achieving result quality above 95\% when compared to results of the traditional version of PageRank without any summarization or approximation techniques. △ Less","17 December, 2019",https://arxiv.org/pdf/1810.02781
Privado: Practical and Secure DNN Inference with Enclaves,Karan Grover;Shruti Tople;Shweta Shinde;Ranjita Bhagwan;Ramachandran Ramjee,"Cloud providers are extending support for trusted hardware primitives such as Intel SGX. Simultaneously, the field of deep learning is seeing enormous innovation as well as an increase in adoption. In this paper, we ask a timely question: ""Can third-party cloud services use Intel SGX enclaves to provide practical, yet secure DNN Inference-as-a-service?"" We first demonstrate that DNN models executing inside enclaves are vulnerable to access pattern based attacks. We show that by simply observing access patterns, an attacker can classify encrypted inputs with 97% and 71% attack accuracy for MNIST and CIFAR10 datasets on models trained to achieve 99% and 79% original accuracy respectively. This motivates the need for PRIVADO, a system we have designed for secure, easy-to-use, and performance efficient inference-as-a-service. PRIVADO is input-oblivious: it transforms any deep learning framework that is written in C/C++ to be free of input-dependent access patterns thus eliminating the leakage. PRIVADO is fully-automated and has a low TCB: with zero developer effort, given an ONNX description of a model, it generates compact and enclave-compatible code which can be deployed on an SGX cloud platform. PRIVADO incurs low performance overhead: we use PRIVADO with Torch framework and show its overhead to be 17.18% on average on 11 different contemporary neural networks. △ Less","5 September, 2019",https://arxiv.org/pdf/1810.00602
Parameter-free Sentence Embedding via Orthogonal Basis,Ziyi Yang;Chenguang Zhu;Weizhu Chen,"We propose a simple and robust non-parameterized approach for building sentence representations. Inspired by the Gram-Schmidt Process in geometric theory, we build an orthogonal basis of the subspace spanned by a word and its surrounding context in a sentence. We model the semantic meaning of a word in a sentence based on two aspects. One is its relatedness to the word vector subspace already spanned by its contextual words. The other is the word's novel semantic meaning which shall be introduced as a new basis vector perpendicular to this existing subspace. Following this motivation, we develop an innovative method based on orthogonal basis to combine pre-trained word embeddings into sentence representations. This approach requires zero parameters, along with efficient inference performance. We evaluate our approach on 11 downstream NLP tasks. Our model shows superior performance compared with non-parameterized alternatives and it is competitive to other approaches relying on either large amounts of labelled data or prolonged training time. △ Less","6 December, 2019",https://arxiv.org/pdf/1810.00438
Learning Pose Estimation for High-Precision Robotic Assembly Using Simulated Depth Images,Yuval Litvak;Armin Biess;Aharon Bar-Hillel,"Most of industrial robotic assembly tasks today require fixed initial conditions for successful assembly. These constraints induce high production costs and low adaptability to new tasks. In this work we aim towards flexible and adaptable robotic assembly by using 3D CAD models for all parts to be assembled. We focus on a generic assembly task - the Siemens Innovation Challenge - in which a robot needs to assemble a gear-like mechanism with high precision into an operating system. To obtain the millimeter-accuracy required for this task and industrial settings alike, we use a depth camera mounted near the robot end-effector. We present a high-accuracy two-stage pose estimation procedure based on deep convolutional neural networks, which includes detection, pose estimation, refinement, and handling of near- and full symmetries of parts. The networks are trained on simulated depth images with means to ensure successful transfer to the real robot. We obtain an average pose estimation error of 2.16 millimeters and 0.64 degree leading to 91% success rate for robotic assembly of randomly distributed parts. To the best of our knowledge, this is the first time that the Siemens Innovation Challenge is fully addressed, with all the parts assembled with high success rates. △ Less","23 March, 2019",https://arxiv.org/pdf/1809.10699
Online Object and Task Learning via Human Robot Interaction,Masood Dehghan;Zichen Zhang;Mennatullah Siam;Jun Jin;Laura Petrich;Martin Jagersand,"This work describes the development of a robotic system that acquires knowledge incrementally through human interaction where new tools and motions are taught on the fly. The robotic system developed was one of the five finalists in the KUKA Innovation Award competition and demonstrated during the Hanover Messe 2018 in Germany. The main contributions of the system are a) a novel incremental object learning module - a deep learning based localization and recognition system - that allows a human to teach new objects to the robot, b) an intuitive user interface for specifying 3D motion task associated with the new object, c) a hybrid force-vision control module for performing compliant motion on an unstructured surface. This paper describes the implementation and integration of the main modules of the system and summarizes the lessons learned from the competition. △ Less","27 February, 2019",https://arxiv.org/pdf/1809.08722
Improving Optimization Bounds using Machine Learning: Decision Diagrams meet Deep Reinforcement Learning,Quentin Cappart;Emmanuel Goutierre;David Bergman;Louis-Martin Rousseau,"Finding tight bounds on the optimal solution is a critical element of practical solution methods for discrete optimization problems. In the last decade, decision diagrams (DDs) have brought a new perspective on obtaining upper and lower bounds that can be significantly better than classical bounding mechanisms, such as linear relaxations. It is well known that the quality of the bounds achieved through this flexible bounding method is highly reliant on the ordering of variables chosen for building the diagram, and finding an ordering that optimizes standard metrics is an NP-hard problem. In this paper, we propose an innovative and generic approach based on deep reinforcement learning for obtaining an ordering for tightening the bounds obtained with relaxed and restricted DDs. We apply the approach to both the Maximum Independent Set Problem and the Maximum Cut Problem. Experimental results on synthetic instances show that the deep reinforcement learning approach, by achieving tighter objective function bounds, generally outperforms ordering methods commonly used in the literature when the distribution of instances is known. To the best knowledge of the authors, this is the first paper to apply machine learning to directly improve relaxation bounds obtained by general-purpose bounding mechanisms for combinatorial optimization problems. △ Less","27 February, 2019",https://arxiv.org/pdf/1809.03359
A Neural Temporal Model for Human Motion Prediction,Anand Gopalakrishnan;Ankur Mali;Dan Kifer;C. Lee Giles;Alexander G. Ororbia,"We propose novel neural temporal models for predicting and synthesizing human motion, achieving state-of-the-art in modeling long-term motion trajectories while being competitive with prior work in short-term prediction and requiring significantly less computation. Key aspects of our proposed system include: 1) a novel, two-level processing architecture that aids in generating planned trajectories, 2) a simple set of easily computable features that integrate derivative information, and 3) a novel multi-objective loss function that helps the model to slowly progress from simple next-step prediction to the harder task of multi-step, closed-loop prediction. Our results demonstrate that these innovations improve the modeling of long-term motion trajectories. Finally, we propose a novel metric, called Normalized Power Spectrum Similarity (NPSS), to evaluate the long-term predictive ability of motion synthesis models, complementing the popular mean-squared error (MSE) measure of Euler joint angles over time. We conduct a user study to determine if the proposed NPSS correlates with human evaluation of long-term motion more strongly than MSE and find that it indeed does. We release code and additional results (visualizations) for this paper at: https://github.com/cr7anand/neural_temporal_models △ Less","22 November, 2019",https://arxiv.org/pdf/1809.03036
On Learning 3D Face Morphable Model from In-the-wild Images,Luan Tran;Xiaoming Liu,"As a classic statistical model of 3D facial shape and albedo, 3D Morphable Model (3DMM) is widely used in facial analysis, e.g., model fitting, image synthesis. Conventional 3DMM is learned from a set of 3D face scans with associated well-controlled 2D face images, and represented by two sets of PCA basis functions. Due to the type and amount of training data, as well as, the linear bases, the representation power of 3DMM can be limited. To address these problems, this paper proposes an innovative framework to learn a nonlinear 3DMM model from a large set of in-the-wild face images, without collecting 3D face scans. Specifically, given a face image as input, a network encoder estimates the projection, lighting, shape and albedo parameters. Two decoders serve as the nonlinear 3DMM to map from the shape and albedo parameters to the 3D shape and albedo, respectively. With the projection parameter, lighting, 3D shape, and albedo, a novel analytically-differentiable rendering layer is designed to reconstruct the original input face. The entire network is end-to-end trainable with only weak supervision. We demonstrate the superior representation power of our nonlinear 3DMM over its linear counterpart, and its contribution to face alignment, 3D reconstruction, and face editing. △ Less","14 July, 2019",https://arxiv.org/pdf/1808.09560
Rethinking Monocular Depth Estimation with Adversarial Training,Richard Chen;Faisal Mahmood;Alan Yuille;Nicholas J. Durr,"Monocular depth estimation is an extensively studied computer vision problem with a vast variety of applications. Deep learning-based methods have demonstrated promise for both supervised and unsupervised depth estimation from monocular images. Most existing approaches treat depth estimation as a regression problem with a local pixel-wise loss function. In this work, we innovate beyond existing approaches by using adversarial training to learn a context-aware, non-local loss function. Such an approach penalizes the joint configuration of predicted depth values at the patch-level instead of the pixel-level, which allows networks to incorporate more global information. In this framework, the generator learns a mapping between RGB images and its corresponding depth map, while the discriminator learns to distinguish depth map and RGB pairs from ground truth. This conditional GAN depth estimation framework is stabilized using spectral normalization to prevent mode collapse when learning from diverse datasets. We test this approach using a diverse set of generators that include U-Net and joint CNN-CRF. We benchmark this approach on the NYUv2, Make3D and KITTI datasets, and observe that adversarial training reduces relative error by several fold, achieving state-of-the-art performance. △ Less","15 June, 2019",https://arxiv.org/pdf/1808.07528
A Survey on Context-based Co-presence Detection Techniques,Mauro Conti;Chhagan Lal,"In this paper, we present a systematic survey on the contextual information based proximity detection techniques. These techniques are heavily used for improving security and usability in Zero-Interaction based Co-presence Detection and Authentication (ZICDA) systems. In particular, the survey includes a discussion on the possible adversary and communication models along with the existing security attacks on ZICDA systems, and it reviews the state-of-the-art proximity detection techniques that make use of contextual information. These proximity detection techniques are commonly referred to as Contextual Co-presence (COCO) protocols, which dynamically collect and use contextual information to improve the security of ZICDA systems during the proximity verification process. Finally, we summarize the significant challenges and suggest possible innovative and efficient future solutions for securely detecting co-presence between devices in the presence of adversaries. The proximity verification techniques presented in the literature usually involve trade-offs between metrics such as efficiency, security, deployment cost, and usability. At present, there is no ideal solution which adequately addresses the trade-off between these metrics. Therefore, we trust that this review gives an insight into the strengths and shortcomings of the known research methodologies and pave the way for the design of future practical, secure, and efficient solutions. △ Less","12 April, 2019",https://arxiv.org/pdf/1808.03320
Devito (v3.1.0): an embedded domain-specific language for finite differences and geophysical exploration,Mathias Louboutin;Michael Lange;Fabio Luporini;Navjot Kukreja;Philipp A. Witte;Felix J. Herrmann;Paulius Velesko;Gerard J. Gorman,"We introduce Devito, a new domain-specific language for implementing high-performance finite difference partial differential equation solvers. The motivating application is exploration seismology where methods such as Full-Waveform Inversion and Reverse-Time Migration are used to invert terabytes of seismic data to create images of the earth's subsurface. Even using modern supercomputers, it can take weeks to process a single seismic survey and create a useful subsurface image. The computational cost is dominated by the numerical solution of wave equations and their corresponding adjoints. Therefore, a great deal of effort is invested in aggressively optimizing the performance of these wave-equation propagators for different computer architectures. Additionally, the actual set of partial differential equations being solved and their numerical discretization is under constant innovation as increasingly realistic representations of the physics are developed, further ratcheting up the cost of practical solvers. By embedding a domain-specific language within Python and making heavy use of SymPy, a symbolic mathematics library, we make it possible to develop finite difference simulators quickly using a syntax that strongly resembles the mathematics. The Devito compiler reads this code and applies a wide range of analysis to generate highly optimized and parallel code. This approach can reduce the development time of a verified and optimized solver from months to days. △ Less","9 August, 2019",https://arxiv.org/pdf/1808.01995
sCompile: Critical Path Identification and Analysis for Smart Contracts,Jialiang Chang;Bo Gao;Hao Xiao;Jun Sun;Yan Cai;Zijiang Yang,"Ethereum smart contracts are an innovation built on top of the blockchain technology, which provides a platform for automatically executing contracts in an anonymous, distributed, and trusted way. The problem is magnified by the fact that smart contracts, unlike ordinary programs, cannot be patched easily once deployed. It is important for smart contracts to be checked against potential vulnerabilities. In this work, we propose an alternative approach to automatically identify critical program paths (with multiple function calls including inter-contract function calls) in a smart contract, rank the paths according to their criticalness, discard them if they are infeasible or otherwise present them with user friendly warnings for user inspection. We identify paths which involve monetary transaction as critical paths, and prioritize those which potentially violate important properties. For scalability, symbolic execution techniques are only applied to top ranked critical paths. Our approach has been implemented in a tool called sCompile, which has been applied to 36,099 smart contracts. The experiment results show that sCompile is efficient, i.e., 5 seconds on average for one smart contract. Furthermore, we show that many known vulnerabilities can be captured if user inspects as few as 10 program paths generated by sCompile. Lastly, sCompile discovered 224 unknown vulnerabilities with a false positive rate of 15.4% before user inspection. △ Less","6 November, 2019",https://arxiv.org/pdf/1808.00624
Learning Adaptive Discriminative Correlation Filters via Temporal Consistency Preserving Spatial Feature Selection for Robust Visual Tracking,Tianyang Xu;Zhen-Hua Feng;Xiao-Jun Wu;Josef Kittler,"With efficient appearance learning models, Discriminative Correlation Filter (DCF) has been proven to be very successful in recent video object tracking benchmarks and competitions. However, the existing DCF paradigm suffers from two major issues, i.e., spatial boundary effect and temporal filter degradation. To mitigate these challenges, we propose a new DCF-based tracking method. The key innovations of the proposed method include adaptive spatial feature selection and temporal consistent constraints, with which the new tracker enables joint spatial-temporal filter learning in a lower dimensional discriminative manifold. More specifically, we apply structured spatial sparsity constraints to multi-channel filers. Consequently, the process of learning spatial filters can be approximated by the lasso regularisation. To encourage temporal consistency, the filter model is restricted to lie around its historical value and updated locally to preserve the global structure in the manifold. Last, a unified optimisation framework is proposed to jointly select temporal consistency preserving spatial features and learn discriminative filters with the augmented Lagrangian method. Qualitative and quantitative evaluations have been conducted on a number of well-known benchmarking datasets such as OTB2013, OTB50, OTB100, Temple-Colour, UAV123 and VOT2018. The experimental results demonstrate the superiority of the proposed method over the state-of-the-art approaches. △ Less","19 June, 2019",https://arxiv.org/pdf/1807.11348
Optimal Continuous State POMDP Planning with Semantic Observations: A Variational Approach,Luke Burks;Ian Loefgren;Nisar Ahmed,"This work develops novel strategies for optimal planning with semantic observations using continuous state partially observable markov decision processes (CPOMDPs). Two major innovations are presented in relation to Gaussian mixture (GM) CPOMDP policy approximation methods. While existing methods have many desirable theoretical properties, they are unable to efficiently represent and reason over hybrid continuous-discrete probabilistic models. The first major innovation is the derivation of closed-form variational Bayes GM approximations of Point-Based Value Iteration Bellman policy backups, using softmax models of continuous-discrete semantic observation probabilities. A key benefit of this approach is that dynamic decision-making tasks can be performed with complex non-Gaussian uncertainties, while also exploiting continuous dynamic state space models (thus avoiding cumbersome and costly discretization). The second major innovation is a new clustering-based technique for mixture condensation that scales well to very large GM policy functions and belief functions. Simulation results for a target search and interception task with semantic observations show that the GM policies resulting from these innovations are more effective than those produced by other state of the art policy approximations, but require significantly less modeling overhead and online runtime cost. Additional results show the robustness of this approach to model errors and scaling to higher dimensions. △ Less","7 August, 2019",https://arxiv.org/pdf/1807.08229
Beyond Surveys: Analyzing Software Development Artifacts to Assess Teaching Efforts,Christoph Matthies;Ralf Teusner;Guenter Hesse,"This Innovative Practice Full Paper presents an approach of using software development artifacts to gauge student behavior and the effectiveness of changes to curriculum design. There is an ongoing need to adapt university courses to changing requirements and shifts in industry. As an educator it is therefore vital to have access to methods, with which to ascertain the effects of curriculum design changes. In this paper, we present our approach of analyzing software repositories in order to gauge student behavior during project work. We evaluate this approach in a case study of a university undergraduate software development course teaching agile development methodologies. Surveys revealed positive attitudes towards the course and the change of employed development methodology from Scrum to Kanban. However, surveys were not usable to ascertain the degree to which students had adapted their workflows and whether they had done so in accordance with course goals. Therefore, we analyzed students' software repository data, which represents information that can be collected by educators to reveal insights into learning successes and detailed student behavior. We analyze the software repositories created during the last five courses, and evaluate differences in workflows between Kanban and Scrum usage. △ Less","20 March, 2019",https://arxiv.org/pdf/1807.02400
The Implementation of the Colored Abstract Simplicial Complex and its Application to Mesh Generation,C. T. Lee;J. B. Moody;R. E. Amaro;J. A. McCammon;M. Holst,"We introduce CASC: a new, modern, and header-only C++ library which provides a data structure to represent arbitrary dimension abstract simplicial complexes (ASC) with user-defined classes stored directly on the simplices at each dimension. This is accomplished by using the latest C++ language features including variadic template parameters introduced in C++11 and automatic function return type deduction from C++14. Effectively CASC decouples the representation of the topology from the interactions of user data. We present the innovations and design principles of the data structure and related algorithms. This includes a metadata aware decimation algorithm which is general for collapsing simplices of any dimension. We also present an example application of this library to represent an orientable surface mesh. △ Less","27 March, 2019",https://arxiv.org/pdf/1807.01417
Learning to Update for Object Tracking with Recurrent Meta-learner,Bi Li;Wenxuan Xie;Wenjun Zeng;Wenyu Liu,"Model update lies at the heart of object tracking. Generally, model update is formulated as an online learning problem where a target model is learned over the online training set. Our key innovation is to \emph{formulate the model update problem in the meta-learning framework and learn the online learning algorithm itself using large numbers of offline videos}, i.e., \emph{learning to update}. The learned updater takes as input the online training set and outputs an updated target model. As a first attempt, we design the learned updater based on recurrent neural networks (RNNs) and demonstrate its application in a template-based tracker and a correlation filter-based tracker. Our learned updater consistently improves the base trackers and runs faster than realtime on GPU while requiring small memory footprint during testing. Experiments on standard benchmarks demonstrate that our learned updater outperforms commonly used update baselines including the efficient exponential moving average (EMA)-based update and the well-designed stochastic gradient descent (SGD)-based update. Equipped with our learned updater, the template-based tracker achieves state-of-the-art performance among realtime trackers on GPU. △ Less","4 March, 2019",https://arxiv.org/pdf/1806.07078
CryptoGuard: High Precision Detection of Cryptographic Vulnerabilities in Massive-sized Java Projects,Sazzadur Rahaman;Ya Xiao;Sharmin Afrose;Fahad Shaon;Ke Tian;Miles Frantz;Danfeng;Yao;Murat Kantarcioglu,"Cryptographic API misuses, such as exposed secrets, predictable random numbers, and vulnerable certificate verification, seriously threaten software security. The vision of automatically screening cryptographic API calls in massive-sized (e.g., millions of LoC) Java programs is not new. However, hindered by the practical difficulty of reducing false positives without compromising analysis quality, this goal has not been accomplished. State-of-the-art crypto API screening solutions are not designed to operate on a large scale. Our technical innovation is a set of fast and highly accurate slicing algorithms. Our algorithms refine program slices by identifying language-specific irrelevant elements. The refinements reduce false alerts by 76% to 80% in our experiments. Running our tool, CrytoGuard, on 46 high-impact large-scale Apache projects and 6,181 Android apps generate many security insights. Our findings helped multiple popular Apache projects to harden their code, including Spark, Ranger, and Ofbiz. We also have made substantial progress towards the science of analysis in this space, including: i) manually analyzing 1,295 Apache alerts and confirming 1,277 true positives (98.61% precision), ii) creating a benchmark with 38-unit basic cases and 74-unit advanced cases, iii) performing an in-depth comparison with leading solutions including CrySL, SpotBugs, and Coverity. We are in the process of integrating CryptoGuard with the Software Assurance Marketplace (SWAMP). △ Less","27 March, 2019",https://arxiv.org/pdf/1806.06881
Rethinking Blockchain Security: Position Paper,Vincent Chia;Pieter Hartel;Qingze Hum;Sebastian Ma;Georgios Piliouras;Daniel Reijsbergen;Mark van Staalduinen;Pawel Szalachowski,"Blockchain technology has become almost as famous for incidents involving security breaches as for its innovative potential. We shed light on the prevalence and nature of these incidents through a database structured using the STIX format. Apart from OPSEC-related incidents, we find that the nature of many incidents is specific to blockchain technology. Two categories stand out: smart contracts, and techno-economic protocol incentives. For smart contracts, we propose to use recent advances in software testing to find flaws before deployment. For protocols, we propose the PRESTO framework that allows us to compare different protocols within a five-dimensional framework. △ Less","24 April, 2019",https://arxiv.org/pdf/1806.04358
Quantifying the dynamics of topical fluctuations in language,Andres Karjus;Richard A. Blythe;Simon Kirby;Kenny Smith,"The availability of large diachronic corpora has provided the impetus for a growing body of quantitative research on language evolution and meaning change. The central quantities in this research are token frequencies of linguistic elements in texts, with changes in frequency taken to reflect the popularity or selective fitness of an element. However, corpus frequencies may change for a wide variety of reasons, including purely random sampling effects, or because corpora are composed of contemporary media and fiction texts within which the underlying topics ebb and flow with cultural and socio-political trends. In this work, we introduce a simple model for controlling for topical fluctuations in corpora - the topical-cultural advection model - and demonstrate how it provides a robust baseline of variability in word frequency changes over time. We validate the model on a diachronic corpus spanning two centuries, and a carefully-controlled artificial language change scenario, and then use it to correct for topical fluctuations in historical time series. Finally, we use the model to show that the emergence of new words typically corresponds with the rise of a trending topic. This suggests that some lexical innovations occur due to growing communicative need in a subspace of the lexicon, and that the topical-cultural advection model can be used to quantify this. △ Less","21 June, 2019",https://arxiv.org/pdf/1806.00699
Designing for Democratization: Introducing Novices to Artificial Intelligence Via Maker Kits,Victor Dibia;Aaron Cox;Justin Weisz,"Existing research highlight the myriad of benefits realized when technology is sufficiently democratized and made accessible to non-technical or novice users. However, democratizing complex technologies such as artificial intelligence (AI) remains hard. In this work, we draw on theoretical underpinnings from the democratization of innovation, in exploring the design of maker kits that help introduce novice users to complex technologies. We report on our work designing TJBot: an open source cardboard robot that can be programmed using pre-built AI services. We highlight principles we adopted in this process (approachable design, simplicity, extensibility and accessibility), insights we learned from showing the kit at workshops (66 participants) and how users interacted with the project on GitHub over a 12-month period (Nov 2016 - Nov 2017). We find that the project succeeds in attracting novice users (40% of users who forked the project are new to GitHub) and a variety of demographics are interested in prototyping use cases such as home automation, task delegation, teaching and learning. △ Less","5 January, 2019",https://arxiv.org/pdf/1805.10723
Nonparametric Bayesian Deep Networks with Local Competition,Konstantinos P. Panousis;Sotirios Chatzis;Sergios Theodoridis,"The aim of this work is to enable inference of deep networks that retain high accuracy for the least possible model complexity, with the latter deduced from the data during inference. To this end, we revisit deep networks that comprise competing linear units, as opposed to nonlinear units that do not entail any form of (local) competition. In this context, our main technical innovation consists in an inferential setup that leverages solid arguments from Bayesian nonparametrics. We infer both the needed set of connections or locally competing sets of units, as well as the required floating-point precision for storing the network parameters. Specifically, we introduce auxiliary discrete latent variables representing which initial network components are actually needed for modeling the data at hand, and perform Bayesian inference over them by imposing appropriate stick-breaking priors. As we experimentally show using benchmark datasets, our approach yields networks with less computational footprint than the state-of-the-art, and with no compromises in predictive accuracy. △ Less","5 May, 2019",https://arxiv.org/pdf/1805.07624
Interdisciplinary collaboration in research networks: Empirical analysis of energy-related research in Greece,Georgios A. Tritsaris;Afreen Siddiqi,"Technological innovation is intimately related to knowledge creation and recombination. In this work we introduce a combined statistical and network-based approach to study collaboration in scientific authorship. We apply it to characterize recent research efforts in renewable energy technology and its intersections with the domains of nanoscience and nanotechnology with focus on materials, and electrical engineering and computer science in Greece and its broader European and international environment as a case study. Using our methods we attempt to illuminate the processes which underlie knowledge creation and diversification in these research networks: a (positive) relationship between expenditure on research and development and the extent and diversity of team-based research at the intersections of the three domains is established. Our specific findings collectively provide insights into the collaboration structure and evolution of energy-related research activity in Greece, while our methodology can be used for evidence-based design, monitoring, and evaluation of interdisciplinary research programs. △ Less","31 March, 2019",https://arxiv.org/pdf/1805.04882
Nearly Optimal Planar k Nearest Neighbors Queries under General Distance Functions,Chih-Hung Liu,"We study the k nearest neighbors problem in the plane for general, convex, pairwise disjoint sites of constant description complexity such as line segments, disks, and quadrilaterals and with respect to a general family of distance functions including the L_p-norms and additively weighted Euclidean distances. For point sites in the Euclidean metric, after four decades of effort, an optimal data structure has recently been developed with O( n ) space, O( log n + k ) query time, and O( n log n ) preprocessing time. We develop a static data structure for the general setting with nearly optimal O( n log log n ) space, the optimal O( log n + k ) query time, and expected O( n polylog n ) preprocessing time. The O( n log log n ) space approaches the linear space, whose achievability is still unknown with the optimal query time, and improves the so far best O( n ( log^2 n )( log log n )^2 ) space of Bohler et al.'s work. Our dynamic version (that allows insertions and deletions of sites) also reduces the space of Kaplan et al.'s work from O( n log^3 n ) to O( n log n ). To obtain these progresses, we devise shallow cuttings of linear size for general distance functions. Shallow cuttings are a key technique to deal with the k nearest neighbors problem for point sites in the Euclidean metric. Agarwal et al. already designed linear-size shallow cuttings for general distance functions, but their shallow cuttings could not be applied to the k nearest neighbors problem. Recently, Kaplan et al. constructed shallow cuttings that are feasible for the k nearest neighbors problem, while the size of their shallow cuttings has an extra double logarithmic factor. Our innovation is a new random sampling technique for the analysis of geometric structures. Since our new technique provides a new way to develop and analyze geometric algorithms, we believe it is of independent interest. △ Less","27 October, 2019",https://arxiv.org/pdf/1805.02066
An explicit two-source extractor with min-entropy rate near 4/9,Mark Lewko,"In 2005 Bourgain gave the first explicit construction of a two-source extractor family with min-entropy rate less than 1/2. His approach combined Fourier analysis with innovative but inefficient tools from arithmetic combinatorics and yielded an unspecified min-entropy rate which was greater than .499. This remained essentially the state of the art until a 2015 breakthrough of Chattopadhyay and Zuckerman in which they gave an alternative approach which produced extractors with arbitrarily small min-entropy rate. In the current work, we revisit the Fourier analytic approach. We give an improved analysis of one of Bourgain's extractors which shows that it in fact extracts from sources with min-entropy rate near \frac{21}{44} =.477\ldots, moreover we construct a variant of this extractor which we show extracts from sources with min-entropy rate near 4/9 = .444\ldots. While this min-entropy rate is inferior to Chattopadhyay and Zuckerman's construction, our extractors have the advantage of exponential small error which is important in some applications. The key ingredient in these arguments is recent progress connected to the restriction theory of the finite field paraboloid by Rudnev and Shkredov. This in turn relies on a Rudnev's point-plane incidence estimate, which in turn relies on Kollár's generalization of the Guth-Katz incidence theorem. △ Less","17 March, 2019",https://arxiv.org/pdf/1804.05451
Merging supply chain and blockchain technologies,M. M. Eljazzar;M. A. Amr;S. S. Kassem;M. Ezzat,"Technology has been playing a major role in our lives. One definition for technology is all the knowledge, products, processes, tools,methods and systems employed in the creation of goods or in providing services.This makes technological innovations raise the competitiveness between organizations that depend on supply chain and logistics in the global market. With increasing competitiveness, new challenges arise due to lack of information and assets tractability. This paper introduces three scenarios for solving these challenges using the Blockchain technology. In this work, Blockchain technology targets two main issues within the supply chain, namely, data transparency and resource sharing. These issues are reflected into the organizations strategies and plans. △ Less","23 January, 2019",https://arxiv.org/pdf/1804.04149
Sparse non-negative super-resolution -- simplified and stabilised,Armin Eftekhari;Jared Tanner;Andrew Thompson;Bogdan Toader;Hemant Tyagi,"The convolution of a discrete measure, x=\sum_{i=1}^ka_iδ_{t_i}, with a local window function, φ(s-t), is a common model for a measurement device whose resolution is substantially lower than that of the objects being observed. Super-resolution concerns localising the point sources \{a_i,t_i\}_{i=1}^k with an accuracy beyond the essential support of φ(s-t), typically from m samples y(s_j)=\sum_{i=1}^k a_iφ(s_j-t_i)+η_j, where η_j indicates an inexactness in the sample value. We consider the setting of x being non-negative and seek to characterise all non-negative measures approximately consistent with the samples. We first show that x is the unique non-negative measure consistent with the samples provided the samples are exact, i.e. η_j=0, m\ge 2k+1 samples are available, and φ(s-t) generates a Chebyshev system. This is independent of how close the sample locations are and {\em does not rely on any regulariser beyond non-negativity}; as such, it extends and clarifies the work by Schiebinger et al. and De Castro et al., who achieve the same results but require a total variation regulariser, which we show is unnecessary. Moreover, we characterise non-negative solutions \hat{x} consistent with the samples within the bound \sum_{j=1}^mη_j^2\le δ^2. Any such non-negative measure is within {\mathcal O}(δ^{1/7}) of the discrete measure x generating the samples in the generalised Wasserstein distance, converging to one another as δ approaches zero. We also show how to make these general results, for windows that form a Chebyshev system, precise for the case of φ(s-t) being a Gaussian window. The main innovation of these results is that non-negativity alone is sufficient to localise point sources beyond the essential sensor resolution. △ Less","26 November, 2019",https://arxiv.org/pdf/1804.01490
3D Interpreter Networks for Viewer-Centered Wireframe Modeling,Jiajun Wu;Tianfan Xue;Joseph J. Lim;Yuandong Tian;Joshua B. Tenenbaum;Antonio Torralba;William T. Freeman,"Understanding 3D object structure from a single image is an important but challenging task in computer vision, mostly due to the lack of 3D object annotations to real images. Previous research tackled this problem by either searching for a 3D shape that best explains 2D annotations, or training purely on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Networks (3D-INN), an end-to-end trainable framework that sequentially estimates 2D keypoint heatmaps and 3D object skeletons and poses. Our system learns from both 2D-annotated real images and synthetic 3D data. This is made possible mainly by two technical innovations. First, heatmaps of 2D keypoints serve as an intermediate representation to connect real and synthetic data. 3D-INN is trained on real images to estimate 2D keypoint heatmaps from an input image; it then predicts 3D object structure from heatmaps using knowledge learned from synthetic 3D shapes. By doing so, 3D-INN benefits from the variation and abundance of synthetic 3D objects, without suffering from the domain difference between real and synthesized images, often due to imperfect rendering. Second, we propose a Projection Layer, mapping estimated 3D structure back to 2D. During training, it ensures 3D-INN to predict 3D structure whose projection is consistent with the 2D annotations to real images. Experiments show that the proposed system performs well on both 2D keypoint estimation and 3D structure recovery. We also demonstrate that the recovered 3D information has wide vision applications, such as image retrieval. △ Less","9 August, 2019",https://arxiv.org/pdf/1804.00782
Sub-Nyquist Radar: Principles and Prototypes,Kumar Vijay Mishra;Yonina C. Eldar,"In the past few years, new approaches to radar signal processing have been introduced which allow the radar to perform signal detection and parameter estimation from much fewer measurements than that required by Nyquist sampling. These systems - referred to as sub-Nyquist radars - model the received signal as having finite rate of innovation and employ the Xampling framework to obtain low-rate samples of the signal. Sub-Nyquist radars exploit the fact that the target scene is sparse facilitating the use of compressed sensing (CS) methods in signal recovery. In this chapter, we review several pulse-Doppler radar systems based on these principles. Contrary to other CS-based designs, our formulations directly address the reduced-rate analog sampling in space and time, avoid a prohibitive dictionary size, and are robust to noise and clutter. We begin by introducing temporal sub-Nyquist processing for estimating the target locations using less bandwidth than conventional systems. This paves the way to cognitive radars which share their transmit spectrum with other communication services, thereby providing a robust solution for coexistence in spectrally crowded environments. Next, without impairing Doppler resolution, we reduce the dwell time by transmitting interleaved radar pulses in a scarce manner within a coherent processing interval or ""slow time"". Then, we consider multiple-input-multiple-output array radars and demonstrate spatial sub-Nyquist processing which allows the use of few antenna elements without degradation in angular resolution. Finally, we demonstrate application of sub-Nyquist and cognitive radars to imaging systems such as synthetic aperture radar. For each setting, we present a state-of-the-art hardware prototype designed to demonstrate the real-time feasibility of sub-Nyquist radars. △ Less","4 February, 2019",https://arxiv.org/pdf/1803.01819
Data Curation with Deep Learning [Vision],Saravanan Thirumuruganathan;Nan Tang;Mourad Ouzzani;AnHai Doan,"Data curation - the process of discovering, integrating, and cleaning data - is one of the oldest, hardest, yet inevitable data management problems. Despite decades of efforts from both researchers and practitioners, it is still one of the most time consuming and least enjoyable work of data scientists. In most organizations, data curation plays an important role so as to fully unlock the value of big data. Unfortunately, the current solutions are not keeping up with the ever-changing data ecosystem, because they often require substantially high human cost. Meanwhile, deep learning is making strides in achieving remarkable successes in multiple areas, such as image recognition, natural language processing, and speech recognition. In this vision paper, we explore how some of the fundamental innovations in deep learning could be leveraged to improve existing data curation solutions and to help build new ones. In particular, we provide a thorough overview of the current deep learning landscape, and identify interesting research opportunities and dispel common myths. We hope that the synthesis of these important domains will unleash a series of research activities that will lead to significantly improved solutions for many data curation tasks. △ Less","24 March, 2019",https://arxiv.org/pdf/1803.01384
FFT Multichannel Interpolation and Application to Image Super-resolution,Dong Cheng;Kit Ian Kou,"This paper presents an innovative set of tools to support a methodology for the multichannel interpolation (MCI) of a discrete signal. It is shown that a bandlimited signal f can be exactly reconstructed from finite samples of g_k (1\leq k\leq M) which are the responses of M linear systems with input f. The proposed interpolation can also be applied to approximate non-bandlimited signals. Quantitative error is analyzed to ensure its effectiveness in approximating non-bandlimited signals and its Hilbert transform. Based on the FFT technique, a fast algorithm which brings high computational efficiency and reliability for MCI is presented. The standout performance of MCI is illustrated by several simulations. Additionally, the proposed interpolation is applied to the single image super-resolution (SISR). Its superior performance in accuracy and speed of SISR is demonstrated by the experimental studies. Our results are compared qualitatively and quantitatively with the state-of-the-art methods in image upsampling and reconstruction by using the standard measurement criteria. △ Less","10 April, 2019",https://arxiv.org/pdf/1802.10291
Learning Integral Representations of Gaussian Processes,Zilong Tan;Sayan Mukherjee,"We propose a representation of Gaussian processes (GPs) based on powers of the integral operator defined by a kernel function, we call these stochastic processes integral Gaussian processes (IGPs). Sample paths from IGPs are functions contained within the reproducing kernel Hilbert space (RKHS) defined by the kernel function, in contrast sample paths from the standard GP are not functions within the RKHS. We develop computationally efficient non-parametric regression models based on IGPs. The main innovation in our regression algorithm is the construction of a low dimensional subspace that captures the information most relevant to explaining variation in the response. We use ideas from supervised dimension reduction to compute this subspace. The result of using the construction we propose involves significant improvements in the computational complexity of estimating kernel hyper-parameters as well as reducing the prediction variance. △ Less","4 March, 2019",https://arxiv.org/pdf/1802.07528
Nonparametric Estimation of Low Rank Matrix Valued Function,Fan Zhou,"Let A:[0,1]\rightarrow\mathbb{H}_m (the space of Hermitian matrices) be a matrix valued function which is low rank with entries in Hölder class Σ(β,L). The goal of this paper is to study statistical estimation of A based on the regression model \mathbb{E}(Y_j|τ_j,X_j) = \langle A(τ_j), X_j \rangle, where τ_j are i.i.d. uniformly distributed in [0,1], X_j are i.i.d. matrix completion sampling matrices, Y_j are independent bounded responses. We propose an innovative nuclear norm penalized local polynomial estimator and establish an upper bound on its point-wise risk measured by Frobenius norm. Then we extend this estimator globally and prove an upper bound on its integrated risk measured by L_2-norm. We also propose another new estimator based on bias-reducing kernels to study the case when A is not necessarily low rank and establish an upper bound on its risk measured by L_{\infty}-norm. We show that the obtained rates are all optimal up to some logarithmic factor in minimax sense. Finally, we propose an adaptive estimation procedure based on Lepskii's method and model selection with data splitting which is computationally efficient and can be easily implemented and parallelized. △ Less","14 April, 2019",https://arxiv.org/pdf/1802.06292
NeVAE: A Deep Generative Model for Molecular Graphs,Bidisha Samanta;Abir De;Gourhari Jana;Pratim Kumar Chattaraj;Niloy Ganguly;Manuel Gomez-Rodriguez,"Deep generative models have been praised for their ability to learn smooth latent representation of images, text, and audio, which can then be used to generate new, plausible data. However, current generative models are unable to work with molecular graphs due to their unique characteristics-their underlying structure is not Euclidean or grid-like, they remain isomorphic under permutation of the nodes labels, and they come with a different number of nodes and edges. In this paper, we first propose a novel variational autoencoder for molecular graphs, whose encoder and decoder are specially designed to account for the above properties by means of several technical innovations. Moreover, in contrast with the state of the art, our decoder is able to provide the spatial coordinates of the atoms of the molecules it generates. Then, we develop a gradient-based algorithm to optimize the decoder of our model so that it learns to generate molecules that maximize the value of certain property of interest and, given a molecule of interest, it is able to optimize the spatial configuration of its atoms for greater stability. Experiments reveal that our variational autoencoder can discover plausible, diverse and novel molecules more effectively than several state of the art models. Moreover, for several properties of interest, our optimized decoder is able to identify molecules with property values 121% higher than those identified by several state of the art methods based on Bayesian optimization and reinforcement learning △ Less","6 September, 2019",https://arxiv.org/pdf/1802.05283
Blockchain moderated by empty blocks to reduce the energetic impact of crypto-moneys,Philippe Jacquet;Bernard Mans,"While cryptocurrencies and blockchain applications continue to gain popularity, their energy cost is evidently becoming unsustainable. In most instances, the main cost comes from the required amount of energy for the Proof-of-Work, and this cost is inherent to the design. In addition, useless costs from discarded work (e.g., the so-called Forks) and lack of scalability (in number of users and in rapid transactions) limit their practical effectiveness. In this paper, we present an innovative scheme which eliminates the nonce and thus the burden of the Proof-of-Work which is the main cause of the energy waste in cryptocurrencies such as Bitcoin. We prove that our scheme guarantees a tunable and bounded average number of simultaneous mining whatever the size of the population in competition, thus by making the use of nonce-based techniques unnecessary, achieves scalability without the cost of consuming a large volume of energy. The technique used in the proof of our scheme is based on the analogy of the analysis of a green leader election. The additional difference with Proof-of-Work schemes (beyond the suppression of the nonce field that is triggering most of the waste), is the introduction of (what we denote as) ""empty blocks"" which aim are to call regular blocks following a staircase set of values. Our scheme reduces the risk of Forks and provides tunable scalability for the number of users and the speed of block generation. We also prove using game theoretical analysis that our scheme is resilient to unfair competitive investments (e.g., ""51 percent"" attack) and block nursing. △ Less","18 August, 2019",https://arxiv.org/pdf/1801.07814
Information Dissemination Speed in Delay Tolerant Urban Vehicular Networks in a Hyperfractal Setting,Dalia Popescu;Philippe Jacquet;Bernard Mans;Robert Dumitru;Andra Pastrav;Emanuel Puschita,"This paper studies the fundamental communication properties of urban vehicle networks by exploiting the self-similarity and hierarchical organization of modern cities. We use an innovative model called ""hyperfractal"" that captures the self-similarities of both the traffic and vehicle locations but avoids the extremes of regularity and randomness. We use analytical tools to derive theoretical upper and lower bounds for the information propagation speed in an urban delay tolerant network (i.e., a network that is disconnected at all time, and thus uses a store-carry-and-forward routing model). We prove that the average broadcast time behaves as n^{1-δ} times a slowly varying function, where δ depends on the precise fractal dimension. Furthermore, we show that the broadcast speedup is due in part to an interesting self-similar phenomenon, that we denote as {\em information teleportation}. This phenomenon arises as a consequence of the topology of the vehicle traffic, and triggers an acceleration of the broadcast time. We show that our model fits real cities where open traffic data sets are available. We present simulations confirming the validity of the bounds in multiple realistic settings, including scenarios with variable speed, using both QualNet and a discrete-event simulator in Matlab. △ Less","8 August, 2019",https://arxiv.org/pdf/1712.04054
The Computational Complexity of Financial Networks with Credit Default Swaps,Steffen Schuldenzucker;Sven Seuken;Stefano Battiston,"The 2008 financial crisis has been attributed to ""excessive complexity"" of the financial system due to financial innovation. We employ computational complexity theory to make this notion precise. Specifically, we consider the problem of clearing a financial network after a shock. Prior work has shown that when banks can only enter into simple debt contracts with each other, then this problem can be solved in polynomial time. In contrast, if they can also enter into credit default swaps (CDSs), i.e., financial derivative contracts that depend on the default of another bank, a solution may not even exist. In this work, we show that deciding if a solution exists is NP-complete if CDSs are allowed. This remains true if we relax the problem to \varepsilon-approximate solutions, for a constant \varepsilon. We further show that, under sufficient conditions where a solution is guaranteed to exist, the approximate search problem is PPAD-complete for constant \varepsilon. We then try to isolate the ""origin"" of the complexity. It turns out that already determining which banks default is hard. Further, we show that the complexity is not driven by the dependence of counterparties on each other, but rather hinges on the presence of so-called naked CDSs. If naked CDSs are not present, we receive a simple polynomial-time algorithm. Our results are of practical importance for regulators' stress tests and regulatory policy. △ Less","20 May, 2019",https://arxiv.org/pdf/1710.01578
Systematic Innovation Mounted Software Development Process and Intuitive Project Management Framework for Lean Startups,Song-Kyoo Kim,"This paper provides a new process which integrates an inventive problem solving method into one modern software development program, making it part of the software development process. The research question is how to improve software development process which tech startups could adopt with minor project management skills. The Systematic Innovation Mounted Software Development Process, a combination of Agile and Systematic Innovation, provides an alternative development process which is targeted to adapt idea generation into software products. The intuitive project management framework helps technology driven companies to manage their software projects more effectively. The implication and aim of this research are providing the guideline to help the entrepreneurs for managing their project properly. The Systematic Innovation model helps to generate new ideas and innovative ways to solve problems with the collaboration with the existing Agile model. The new software development process and associated techniques could impact the current software development industry significantly, especially software startup companies, because these powerful tools can help reduce managerial workloads of the companies and give them more time to remain focused on their key technologies. △ Less","25 November, 2019",https://arxiv.org/pdf/1708.06900
A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity,Pablo Hernandez-Leal;Michael Kaisers;Tim Baarslag;Enrique Munoz de Cote,"The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached non-stationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we reflect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research. △ Less","11 March, 2019",https://arxiv.org/pdf/1707.09183
CatBoost: unbiased boosting with categorical features,Liudmila Prokhorenkova;Gleb Gusev;Aleksandr Vorobev;Anna Veronika Dorogush;Andrey Gulin,"This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to fight a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results. △ Less","20 January, 2019",https://arxiv.org/pdf/1706.09516
"Fast and Accurate Sparse Coding of Visual Stimuli with a Simple, Ultra-Low-Energy Spiking Architecture",Walt Woods;Christof Teuscher,"Memristive crossbars have become a popular means for realizing unsupervised and supervised learning techniques. In previous neuromorphic architectures with leaky integrate-and-fire neurons, the crossbar itself has been separated from the neuron capacitors to preserve mathematical rigor. In this work, we sought to simplify the design, creating a fast circuit that consumed significantly lower power at a minimal cost of accuracy. We also showed that connecting the neurons directly to the crossbar resulted in a more efficient sparse coding architecture, and alleviated the need to pre-normalize receptive fields. This work provides derivations for the design of such a network, named the Simple Spiking Locally Competitive Algorithm, or SSLCA, as well as CMOS designs and results on the CIFAR and MNIST datasets. Compared to a non-spiking model which scored 33% on CIFAR-10 with a single-layer classifier, this hardware scored 32% accuracy. When used with a state-of-the-art deep learning classifier, the non-spiking model achieved 82% and our simplified, spiking model achieved 80%, while compressing the input data by 92%. Compared to a previously proposed spiking model, our proposed hardware consumed 99% less energy to do the same work at 21x the throughput. Accuracy held out with online learning to a write variance of 3%, suitable for the often-reported 4-bit resolution required for neuromorphic algorithms; with offline learning to a write variance of 27%; and with read variance to 40%. The proposed architecture's excellent accuracy, throughput, and significantly lower energy usage demonstrate the utility of our innovations. △ Less","22 January, 2019",https://arxiv.org/pdf/1704.05877
"The Emergence of Canalization and Evolvability in an Open-Ended, Interactive Evolutionary System",Joost Huizinga;Kenneth O. Stanley;Jeff Clune,"Natural evolution has produced a tremendous diversity of functional organisms. Many believe an essential component of this process was the evolution of evolvability, whereby evolution speeds up its ability to innovate by generating a more adaptive pool of offspring. One hypothesized mechanism for evolvability is developmental canalization, wherein certain dimensions of variation become more likely to be traversed and others are prevented from being explored (e.g. offspring tend to have similarly sized legs, and mutations affect the length of both legs, not each leg individually). While ubiquitous in nature, canalization almost never evolves in computational simulations of evolution. Not only does that deprive us of in silico models in which to study the evolution of evolvability, but it also raises the question of which conditions give rise to this form of evolvability. Answering this question would shed light on why such evolvability emerged naturally and could accelerate engineering efforts to harness evolution to solve important engineering challenges. In this paper we reveal a unique system in which canalization did emerge in computational evolution. We document that genomes entrench certain dimensions of variation that were frequently explored during their evolutionary history. The genetic representation of these organisms also evolved to be highly modular and hierarchical, and we show that these organizational properties correlate with increased fitness. Interestingly, the type of computational evolutionary experiment that produced this evolvability was very different from traditional digital evolution in that there was no objective, suggesting that open-ended, divergent evolutionary processes may be necessary for the evolution of evolvability. △ Less","14 February, 2019",https://arxiv.org/pdf/1704.05143
TipTop: (Almost) Exact Solutions for Influence Maximization in Billion-scale Networks,Xiang Li;J. David Smith;Thang N. Dinh;My T. Thai,"In this paper, we study the Cost-aware Target Viral Marketing (CTVM) problem, a generalization of Influence Maximization (IM). CTVM asks for the most cost-effective users to influence the most relevant users. In contrast to the vast literature, we attempt to offer exact solutions. As the problem is NP-hard, thus, exact solutions are intractable, we propose TipTop, a (1-ε)-optimal solution for arbitrary ε>0 that scales to very large networks such as Twitter. At the heart of TipTop lies an innovative technique that reduces the number of samples as much as possible. This allows us to exactly solve CTVM on a much smaller space of generated samples using Integer Programming. Furthermore, TipTop lends a tool for researchers to benchmark their solutions against the optimal one in large-scale networks, which is currently not available. △ Less","7 February, 2019",https://arxiv.org/pdf/1701.08462
Feedback Capacity of Gaussian Channels Revisited,Ather Gattami,"In this paper, we revisit the problem of finding the average capacity of the Gaussian feedback channel. First, we consider the problem of finding the average capacity of the analog Gaussian noise channel where the noise has an arbitrary spectral density. We introduce a new approach to the problem where we solve the problem over a finite number of transmissions and then consider the limit of an infinite number of transmissions. Further, we consider the important special case of stationary Gaussian noise with finite memory. We show that the channel capacity at stationarity can be found by solving a semi-definite program, and hence computationally tractable. We also give new proofs and results of the non stationary solution which bridges the gap between results in the literature for the stationary and non stationary feedback channel capacities. It's shown that a linear communication feedback strategy is optimal. Similar to the solution of the stationary problem, it's shown that the optimal linear strategy is to transmit a linear combination of the information symbols to be communicated and the innovations for the estimation error of the state of the noise process. △ Less","23 January, 2019",https://arxiv.org/pdf/1511.06866
Logarithmic Space and Permutations,Clément Aubert;Thomas Seiller,"In a recent work, Girard proposed a new and innovative approach to computational complexity based on the proofs-as-programs correspondence. In a previous paper, the authors showed how Girard proposal succeeds in obtaining a new characterization of co-NL languages as a set of operators acting on a Hilbert Space. In this paper, we extend this work by showing that it is also possible to define a set of operators characterizing the class L of logarithmic space languages. △ Less","3 June, 2019",https://arxiv.org/pdf/1301.3189
Modeling Cultural Dynamics,Liane Gabora,"EVOC (for EVOlution of Culture) is a computer model of culture that enables us to investigate how various factors such as barriers to cultural diffusion, the presence and choice of leaders, or changes in the ratio of innovation to imitation affect the diversity and effectiveness of ideas. It consists of neural network based agents that invent ideas for actions, and imitate neighbors' actions. The model is based on a theory of culture according to which what evolves through culture is not memes or artifacts, but the internal models of the world that give rise to them, and they evolve not through a Darwinian process of competitive exclusion but a Lamarckian process involving exchange of innovation protocols. EVOC shows an increase in mean fitness of actions over time, and an increase and then decrease in the diversity of actions. Diversity of actions is positively correlated with population size and density, and with barriers between populations. Slowly eroding borders increase fitness without sacrificing diversity by fostering specialization followed by sharing of fit actions. Introducing a leader that broadcasts its actions throughout the population increases the fitness of actions but reduces diversity of actions. Increasing the number of leaders reduces this effect. Efforts are underway to simulate the conditions under which an agent immigrating from one culture to another contributes new ideas while still fitting in. △ Less","9 July, 2019",https://arxiv.org/pdf/0811.2551
