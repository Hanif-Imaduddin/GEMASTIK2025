title,authors,abstract,submitted_date,pdf_link
Beyond NGS data sharing and towards open science,Bruno Dantas;Calmenelias Fleitas;Alexandre P. Francisco;José Simão;Cátia Vaz,"Biosciences have been revolutionized by next generation sequencing (NGS) technologies in last years, leading to new perspectives in medical, industrial and environmental applications. And although our motivation comes from biosciences, the following is true for many areas of science: published results are usually hard to reproduce either because data is not available or tools are not readily available, which delays the adoption of new methodologies and hinders innovation. Our focus is on tool readiness and pipelines availability. Even though most tools are freely available, pipelines for data analysis are in general barely described and their configuration is far from trivial, with many parameters to be tuned. In this paper we discuss how to effectively build and use pipelines, relying on state of the art computing technologies to execute them without users need to configure, install and manage tools, servers and complex workflow management systems. We perform an in depth comparative analysis of state of the art frameworks and systems. The NGSPipes framework is proposed showing that we can have public pipelines ready to process and analyse experimental data, produced for instance by high-throughput technologies, but without relying on centralized servers or Web services. The NGSPipes framework and underlying architecture provides a major step towards open science and true collaboration in what concerns tools and pipelines among computational biology researchers and practitioners. We show that it is possible to execute data analysis pipelines in a decentralized and platform independent way. Approaches like the one proposed are crucial for archiving and reusing data analysis pipelines at medium/long-term. NGSPipes framework is freely available at http://ngspipes.github.io/. △ Less","11 November, 2016",https://arxiv.org/pdf/1701.03507
Efficient Multi-Dimensional Mapping Using QAM Constellations for BICM-ID,Hassan M. Navazi;Md. Jahangir Hossain,"Bit-interleaved coded modulation with iterative decoding (BICM-ID) offers very good error performance over additive white Gaussian noise (AWGN) and fading channels if it uses a wisely designed signal mapping. Further, error performance of BICM-ID can significantly be improved by employing multi-dimensional (MD) modulation. However, suitable MD mappings are obtained by computer search techniques except for MD modulations that use smaller constellation e.g., binary phase shift keying (BPSK), quadrature phase shift keying (QPSK) and $8$-ary phase shift keying ($8$-PSK) as basic modulation. The alphabet size of MD modulations increases exponentially as the order of the basic modulation increases and computer search becomes intractable. In this paper, we propose a systematic mapping method for MD modulations. The innovativeness of our proposed method is that it generates MD mappings using $16$- and $64$-quadrature amplitude modulation (QAM) very efficiently. The presented numerical results show that the proposed method improves bit error rate (BER) of BICM-ID. △ Less","29 December, 2016",https://arxiv.org/pdf/1701.01167
Unique Sense: Smart Computing Prototype for Industry 4.0 Revolution with IOT and Bigdata Implementation Model,S. Vijaykumar;S. G. Saravanakumar;M. Balamurugan,"Today, The Computing architectures are one of the most complex constrained developing area in the research field. Which delivers solution for different domains computation problem from its stack above. The architectural integration constrains makes difficulties to customize and modify the system for dynamic industrial and business needs. This model is the initiation towards the solution for findings of Industry 4.0 and Bigdata needs. This Unique sense smart computing implementation model for Industry 4.0 holds the innovative Smart computing prototype is a part of UNIQUE SENSE computing architecture which can delivers alternate solution for today's computing architecture to satisfy the future generation needs of diversified technologies and techniques, which brings extended support to the ubiquitous environment. Primitively the industrial 4.0 having a lots of chained interlinked process which also holds valuable information. So it is especially designed for fault tolerance data processing integrated system. This implementation model constructed in the way that smart control and selfaccessible system for next generation cyber physical machine and automation controlling system. Also that focusing towards dynamic customization, reusability, eco friendliness for next generation controlling and computation power. △ Less","27 November, 2016",https://arxiv.org/pdf/1612.09325
AZ Model for Software Development,Ahmed Mateen;Muhammad Azeem;Mohammad Shafiq,"Know a days Computer system become essential and it is most commonly used in every field of life. The computer saves time and use to solve complex and extensive problem quickly in an efficient way. For this purpose software programs are develop to facilitate the works for administrator, offices, banks etc. so Quality is the most important factor as it mostly defines CUSTOMER SATISFACTION which directly related to success of the project so there are many approaches (methodologies) have been developed for this purpose occasionally. The main study of this paper is to propose a new methodology for the development of the software which focuses on the quality improvement of all kind of product. This study will also discuss the features and limitation of the traditional methodologies like waterfall iterative spiral RUP and Agile and show how the new innovative methodology is better than previous one. △ Less","28 December, 2016",https://arxiv.org/pdf/1612.08811
Equibit: A Peer-to-Peer Electronic Equity System,Brent Kievit-Kylar;Chris Horlacher;Marc Godard;Christian Saucier,"The registration, transfer, clearing and settlement of equities represents a significant part of economic activity currently underserved by modern technological innovation. In addition, recent events have revealed problems of transparency, inviting public criticism and scrutiny from regulatory authorities. A peer-to-peer platform facilitating the creation and exchange of directly registered shares represents a shift in equity markets towards more efficient, transparent operations as well as greater accessibility to the investing public and issuing companies. Blockchain technology solves the problem of transaction processing and clearing but the fungibility of their units pose a challenge in identifying the issuers and holders of specific equities. Furthermore, as the issuers are in a constant state of flux the benefits of a decentralized network are lost if a central manager is required to cancel equities from companies that no longer exist. We propose a solution to these problems using digital signatures, based on blockchain technology. △ Less","20 December, 2016",https://arxiv.org/pdf/1612.06953
The Blockchain: A Gentle Four Page Introduction,Jan Hendrik Witte,"Blockchain is a distributed database that keeps a chronologically-growing list (chain) of records (blocks) secure from tampering and revision. While computerisation has changed the nature of a ledger from clay tables in the old days to digital records in modern days, blockchain technology is the first true innovation in record keeping that could potentially revolutionise the basic principles of information keeping. In this note, we provide a brief self-contained introduction to how the blockchain works. △ Less","6 December, 2016",https://arxiv.org/pdf/1612.06244
Accurate Reconstruction of Finite Rate of Innovation Signals on the Sphere,Yahya Sattar;Zubair Khalid;Rodney A. Kennedy,"We develop a method for the accurate reconstruction of non-bandlimited finite rate of innovation signals on the sphere. For signals consisting of a finite number of Dirac functions on the sphere, we develop an annihilating filter based method for the accurate recovery of parameters of the Dirac functions using a finite number of observations of the bandlimited signal. In comparison to existing techniques, the proposed method enables more accurate reconstruction primarily due to better conditioning of systems involved in the recovery of parameters. For the recovery of $K$ Diracs on the sphere, the proposed method requires samples of the signal bandlimited in the spherical harmonic~(SH) domain at SH degree equal or greater than $ K + \sqrt{K + \frac{1}{4}} - \frac{1}{2}$. In comparison to the existing state-of-the art technique, the required bandlimit, and consequently the number of samples, of the proposed method is the same or less. We also conduct numerical experiments to demonstrate that the proposed technique is more accurate than the existing methods by a factor of $10^{7}$ or more for $2 \le K\le 20$. △ Less","19 December, 2016",https://arxiv.org/pdf/1612.06159
Open Government Data in Russian Federation,Dmitrij Koznov;Olga Andreeva;Uolevi Nikula;Andrey Maglyas;Dmitry Muromtsev;Irina Radchenko,"Open data can increase transparency and accountability of a country government, leading to free information sharing and stimulation of new innovations. This paper analyses government open data policy as well as open data initiatives and trends in Russian Federation. The OCDE analytical framework for national open government data portals and supporting initiatives is used as the bases for the study. The key issues of Russian open government data movement are summarized and aggregated. The paper argues the necessity of systematic development of the open data ecosystem, the leading role of the government in data release, a deeper business involvement, and a reduction of bureaucratic barriers. △ Less","15 December, 2016",https://arxiv.org/pdf/1612.05164
Efficient and Fair Collaborative Mobile Internet Access,George Iosifidis;Lin Gao;Jianwei Huang;Leandros Tassiulas,"The surging global mobile data traffic challenges the economic viability of cellular networks and calls for innovative solutions to reduce the network congestion and improve user experience. In this context, user-provided networks (UPNs), where mobile users share their Internet access by exploiting their diverse network resources and needs, turn out to be very promising. Heterogeneous users with advanced handheld devices can form connections in a distributed fashion and unleash dormant network resources at the network edge. However, the success of such services heavily depends on users' willingness to contribute their resources, such as network access and device battery energy. In this paper, we introduce a general framework for UPN services and design a bargaining-based distributed incentive mechanism to ensure users participation. The proposed mechanism determines the resources that each user should contribute in order to maximise the aggregate data rate in UPN, and fairly allocate the benefit among the users. The numerical results verify that the service can always improve performance, and such improvement increases with the diversity of the users' resources. Quantitatively, it can reach an average 30% increase of the total served traffic for a typical scenario even with only 6 mobile users. △ Less","15 December, 2016",https://arxiv.org/pdf/1612.05129
Lightweight compression with encryption based on Asymmetric Numeral Systems,Jarek Duda;Marcin Niemiec,"Data compression combined with effective encryption is a common requirement of data storage and transmission. Low cost of these operations is often a high priority in order to increase transmission speed and reduce power usage. This requirement is crucial for battery-powered devices with limited resources, such as autonomous remote sensors or implants. Well-known and popular encryption techniques are frequently too expensive. This problem is on the increase as machine-to-machine communication and the Internet of Things are becoming a reality. Therefore, there is growing demand for finding trade-offs between security, cost and performance in lightweight cryptography. This article discusses Asymmetric Numeral Systems -- an innovative approach to entropy coding which can be used for compression with encryption. It provides compression ratio comparable with arithmetic coding at similar speed as Huffman coding, hence, this coding is starting to replace them in new compressors. Additionally, by perturbing its coding tables, the Asymmetric Numeral System makes it possible to simultaneously encrypt the encoded message at nearly no additional cost. The article introduces this approach and analyzes its security level. The basic application is reducing the number of rounds of some cipher used on ANS-compressed data, or completely removing additional encryption layer if reaching a satisfactory protection level. △ Less","14 December, 2016",https://arxiv.org/pdf/1612.04662
The Innovative Behaviour of Software Engineers: Findings from a Pilot Case Study,Cleviton Monteiro;Fabio Queda Bueno da Silva;Luiz Fernando Capretz,"Context: In the workplace, some individuals engage in the voluntary and intentional generation, promotion, and realization of new ideas for the benefit of individual performance, group effectiveness, or the organization. The literature classifies this phenomenon as innovative behaviour. Despite its importance to the development of innovation, innovative behaviour has not been fully investigated in software engineering. Objective: To understand the factors that support or inhibit innovative behaviour in software engineering practice. Method: We conducted a pilot case study in a Canadian software company using interviews and observations as data collection techniques. Using qualitative analysis, we identified relevant factors and relationships not addressed by studies from other areas. Results: Individual innovative behaviour is influenced by individual attitudes and also by situational factors such as relationships in the workplace, organizational characteristics, and project type. We built a model to express the interacting effects of these factors. Conclusions: Innovative behaviour is dependent on individual and contextual factors. Our results contribute to relevant impacts on research and practice, and to topics that deserve further study. △ Less","2 December, 2016",https://arxiv.org/pdf/1612.04648
The Ethereum Scratch Off Puzzle,Abrahim Ladha;Sharbani Pandit;Sanya Ralhan,"Ethereum represents new innovation in the fields of cryptocurrency which has become relatively stagnate, promising many things, including an entire programming language and development enviroment built into the network. However the current trend is to write implementations and proof of concepts before doing the rigor involved with proving security. Miller's recent thesis is an attempt to remedy this, and we apply his provable security techniques to the algorithm description of CASPER, the new ""proof-of-stake"" consensus protocol scheme to be implemented in ethereum. We conclude by stating it satisfies almost all the definitions, except one, leaving room for improvement. △ Less","14 December, 2016",https://arxiv.org/pdf/1612.04518
Faster and Simpler Algorithm for Optimal Strategies of Blotto Game,Soheil Behnezhad;Sina Dehghani;Mahsa Derakhshan;MohammadTaghi HajiAghayi;Saeed Seddighin,"In the Colonel Blotto game, which was initially introduced by Borel in 1921, two colonels simultaneously distribute their troops across different battlefields. The winner of each battlefield is determined independently by a winner-take-all rule. The ultimate payoff of each colonel is the number of battlefields he wins. This game is commonly used for analyzing a wide range of applications such as the U.S presidential election, innovative technology competitions, advertisements, etc. There have been persistent efforts for finding the optimal strategies for the Colonel Blotto game. After almost a century Ahmadinejad, Dehghani, Hajiaghayi, Lucier, Mahini, and Seddighin provided a poly-time algorithm for finding the optimal strategies. They first model the problem by a Linear Program (LP) and use Ellipsoid method to solve it. However, despite the theoretical importance of their algorithm, it is highly impractical. In general, even Simplex method (despite its exponential running-time) performs better than Ellipsoid method in practice. In this paper, we provide the first polynomial-size LP formulation of the optimal strategies for the Colonel Blotto game. We use linear extension techniques. Roughly speaking, we project the strategy space polytope to a higher dimensional space, which results in a lower number of facets for the polytope. We use this polynomial-size LP to provide a novel, simpler and significantly faster algorithm for finding the optimal strategies for the Colonel Blotto game. We further show this representation is asymptotically tight in terms of the number of constraints. We also extend our approach to multi-dimensional Colonel Blotto games, and implement our algorithm to observe interesting properties of Colonel Blotto; for example, we observe the behavior of players in the discrete model is very similar to the previously studied continuous model. △ Less","26 December, 2016",https://arxiv.org/pdf/1612.04029
Performance Improvements of Probabilistic Transcript-adapted ASR with Recurrent Neural Network and Language-specific Constraints,Xiang Kong;Preethi Jyothi;Mark Hasegawa-Johnson,"Mismatched transcriptions have been proposed as a mean to acquire probabilistic transcriptions from non-native speakers of a language.Prior work has demonstrated the value of these transcriptions by successfully adapting cross-lingual ASR systems for different tar-get languages. In this work, we describe two techniques to refine these probabilistic transcriptions: a noisy-channel model of non-native phone misperception is trained using a recurrent neural net-work, and decoded using minimally-resourced language-dependent pronunciation constraints. Both innovations improve quality of the transcript, and both innovations reduce phone error rate of a trainedASR, by 7% and 9% respectively △ Less","12 December, 2016",https://arxiv.org/pdf/1612.03991
FaaS: Federation-as-a-Service,Francesco Paolo Schiavo;Vladimiro Sassone;Luca Nicoletti;Andrea Margheri,"This document is the main high-level architecture specification of the SUNFISH cloud federation solution. Its main objective is to introduce the concept of Federation-as-a-Service (FaaS) and the SUNFISH platform. FaaS is the new and innovative cloud federation service proposed by the SUNFISH project. The document defines the functionalities of FaaS, its governance and precise objectives. With respect to these objectives, the document proposes the high-level architecture of the SUNFISH platform: the software architecture that permits realising a FaaS federation. More specifically, the document describes all the components forming the platform, the offered functionalities and their high-level interactions underlying the main FaaS functionalities. The document concludes by outlining the main implementation strategies towards the actual implementation of the proposed cloud federation solution. △ Less","12 December, 2016",https://arxiv.org/pdf/1612.03937
Effector Detection in Social Networks,Guangmo;Tong;Shasha Li;Weili Wu;Ding-Zhu Du,"In a social network, influence diffusion is the process of spreading innovations from user to user. An activation state identifies who are the active users who have adopted the target innovation. Given an activation state of a certain diffusion, effector detection aims to reveal the active users who are able to best explain the observed state. In this paper, we tackle the effector detection problem from two perspectives. The first approach is based on the influence distance that measures the chance that an active user can activate its neighbors. For a certain pair of users, the shorter the influence distance, the higher probability that one can activate the other. Given an activation state, the effectors are expected to have short influence distance to active users while long to inactive users. By this idea, we propose the influence-distance-based effector detection problem and provide a 3-approximation. Second, we address the effector detection problem by the maximum likelihood estimation (MLE) approach. We prove that the optimal MLE can be obtained in polynomial time for connected directed acyclic graphs. For general graphs, we first extract a directed acyclic subgraph that can well preserve the information in the original graph and then apply the MLE approach to the extracted subgraph to obtain the effectors. The effectiveness of our algorithms is experimentally verified via simulations on the real-world social network. △ Less","12 December, 2016",https://arxiv.org/pdf/1612.03864
SimTensor: A synthetic tensor data generator,Hadi Fanaee-T;Joao Gama,"SimTensor is a multi-platform, open-source software for generating artificial tensor data (either with CP/PARAFAC or Tucker structure) for reproducible research on tensor factorization algorithms. SimTensor is a stand-alone application based on MATALB. It provides a wide range of facilities for generating tensor data with various configurations. It comes with a user-friendly graphical user interface, which enables the user to generate tensors with complicated settings in an easy way. It also has this facility to export generated data to universal formats such as CSV and HDF5, which can be imported via a wide range of programming languages (C, C++, Java, R, Fortran, MATLAB, Perl, Python, and many more). The most innovative part of SimTensor is this that can generate temporal tensors with periodic waves, seasonal effects and streaming structure. it can apply constraints such as non-negativity and different kinds of sparsity to the data. SimTensor also provides this facility to simulate different kinds of change-points and inject various types of anomalies. The source code and binary versions of SimTensor is available for download in http://www.simtensor.org. △ Less","9 December, 2016",https://arxiv.org/pdf/1612.03772
Panoptic Studio: A Massively Multiview System for Social Interaction Capture,Hanbyul Joo;Tomas Simon;Xulong Li;Hao Liu;Lei Tan;Lin Gui;Sean Banerjee;Timothy Godisart;Bart Nabbe;Iain Matthews;Takeo Kanade;Shohei Nobuhara;Yaser Sheikh,"We present an approach to capture the 3D motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent; (2) subtle motion needs to be measured over a space large enough to host a social group; (3) human appearance and configuration variation is immense; and (4) attaching markers to the body may prime the nature of interactions. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the integration of perceptual analyses over a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. Our algorithm is designed to fuse the ""weak"" perceptual processes in the large number of views by progressively generating skeletal proposals from low-level appearance cues, and a framework for temporal refinement is also presented by associating body parts to reconstructed dense 3D trajectory stream. Our system and method are the first in reconstructing full body motion of more than five people engaged in social interactions without using markers. We also empirically demonstrate the impact of the number of views in achieving this goal. △ Less","9 December, 2016",https://arxiv.org/pdf/1612.03153
Authoring image decompositions with generative models,Jason Rock;Theerasit Issaranon;Aditya Deshpande;David Forsyth,"We show how to extend traditional intrinsic image decompositions to incorporate further layers above albedo and shading. It is hard to obtain data to learn a multi-layer decomposition. Instead, we can learn to decompose an image into layers that are ""like this"" by authoring generative models for each layer using proxy examples that capture the Platonic ideal (Mondrian images for albedo; rendered 3D primitives for shading; material swatches for shading detail). Our method then generates image layers, one from each model, that explain the image. Our approach rests on innovation in generative models for images. We introduce a Convolutional Variational Auto Encoder (conv-VAE), a novel VAE architecture that can reconstruct high fidelity images. The approach is general, and does not require that layers admit a physical interpretation. △ Less","5 December, 2016",https://arxiv.org/pdf/1612.01479
Inspiration or Preparation? Explaining Creativity in Scientific Enterprise,Xinyang Zhang;Dashun Wang;Ting Wang,"Human creativity is the ultimate driving force behind scientific progress. While the building blocks of innovations are often embodied in existing knowledge, it is creativity that blends seemingly disparate ideas. Existing studies have made striding advances in quantifying creativity of scientific publications by investigating their citation relationships. Yet, little is known hitherto about the underlying mechanisms governing scientific creative processes, largely due to that a paper's references, at best, only partially reflect its authors' actual information consumption. This work represents an initial step towards fine-grained understanding of creative processes in scientific enterprise. In specific, using two web-scale longitudinal datasets (120.1 million papers and 53.5 billion web requests spanning 4 years), we directly contrast authors' information consumption behaviors against their knowledge products. We find that, of 59.0\% papers across all scientific fields, 25.7\% of their creativity can be readily explained by information consumed by their authors. Further, by leveraging these findings, we develop a predictive framework that accurately identifies the most critical knowledge to fostering target scientific innovations. We believe that our framework is of fundamental importance to the study of scientific creativity. It promotes strategies to stimulate and potentially automate creative processes, and provides insights towards more effective designs of information recommendation platforms. △ Less","5 December, 2016",https://arxiv.org/pdf/1612.01450
Universality of the SIS prevalence in networks,Piet Van Mieghem,"Epidemic models are increasingly used in real-world networks to understand diffusion phenomena (such as the spread of diseases, emotions, innovations, failures) or the transport of information (such as news, memes in social on-line networks). A new analysis of the prevalence, the expected number of infected nodes in a network, is presented and physically interpreted. The analysis method is based on spectral decomposition and leads to a universal, analytic curve, that can bound the time-varying prevalence in any finite time interval. Moreover, that universal curve also applies to various types of Susceptible-Infected-Susceptible (SIS) (and Susceptible-Infected-Removed (SIR)) infection processes, with both homogenous and heterogeneous infection characteristics (curing and infection rates), in temporal and even disconnected graphs and in SIS processes with and without self-infections. The accuracy of the universal curve is comparable to that of well-established mean-field approximations. △ Less","5 December, 2016",https://arxiv.org/pdf/1612.01386
Disruptive innovations in RoboCup 2D Soccer Simulation League: from Cyberoos'98 to Gliders2016,Mikhail Prokopenko;Peter Wang,"We review disruptive innovations introduced in the RoboCup 2D Soccer Simulation League over the twenty years since its inception, and trace the progress of our champion team (Gliders). We conjecture that the League has been developing as an ecosystem shaped by diverse approaches taken by participating teams, increasing in its overall complexity. A common feature is that different champion teams succeeded in finding a way to decompose the enormous search-space of possible single- and multi-agent behaviours, by automating the exploration of the problem space with various techniques which accelerated the software development efforts. These methods included interactive debugging, machine learning, automated planning, and opponent modelling. The winning approach developed by Gliders is centred on human-based evolutionary computation which optimised several components such as an action-dependent evaluation function, dynamic tactics with Voronoi diagrams, information dynamics, and bio-inspired collective behaviour. △ Less","3 December, 2016",https://arxiv.org/pdf/1612.00947
FRIDA: FRI-Based DOA Estimation for Arbitrary Array Layouts,Hanjie Pan;Robin Scheibler;Eric Bezzam;Ivan Dokmanic;Martin Vetterli,"In this paper we present FRIDA---an algorithm for estimating directions of arrival of multiple wideband sound sources. FRIDA combines multi-band information coherently and achieves state-of-the-art resolution at extremely low signal-to-noise ratios. It works for arbitrary array layouts, but unlike the various steered response power and subspace methods, it does not require a grid search. FRIDA leverages recent advances in sampling signals with a finite rate of innovation. It is based on the insight that for any array layout, the entries of the spatial covariance matrix can be linearly transformed into a uniformly sampled sum of sinusoids. △ Less","2 December, 2016",https://arxiv.org/pdf/1612.00876
Multi-resolution Data Fusion for Super-Resolution Electron Microscopy,Suhas Sreehari;S. V. Venkatakrishnan;Katherine L. Bouman;Jeffrey P. Simmons;Lawrence F. Drummy;Charles A. Bouman,"Perhaps surprisingly, the total electron microscopy (EM) data collected to date is less than a cubic millimeter. Consequently, there is an enormous demand in the materials and biological sciences to image at greater speed and lower dosage, while maintaining resolution. Traditional EM imaging based on homogeneous raster-order scanning severely limits the volume of high-resolution data that can be collected, and presents a fundamental limitation to understanding physical processes such as material deformation, crack propagation, and pyrolysis. We introduce a novel multi-resolution data fusion (MDF) method for super-resolution computational EM. Our method combines innovative data acquisition with novel algorithmic techniques to dramatically improve the resolution/volume/speed trade-off. The key to our approach is to collect the entire sample at low resolution, while simultaneously collecting a small fraction of data at high resolution. The high-resolution measurements are then used to create a material-specific patch-library that is used within the ""plug-and-play"" framework to dramatically improve super-resolution of the low-resolution data. We present results using FEI electron microscope data that demonstrate super-resolution factors of 4x, 8x, and 16x, while substantially maintaining high image quality and reducing dosage. △ Less","28 November, 2016",https://arxiv.org/pdf/1612.00874
A Pilot Case Study on Innovative Behaviour: Lessons Learned and Directions for Future Work,Cleviton Monteiro;Fabio Queda Bueno da Silva;Luiz Fernando Capretz,"Context: A case study is a powerful research strategy for investigating complex social-technical and managerial phenomena in real life settings. However, when the phenomenon has not been fully discovered or understood, pilot case studies are important to refine the research problem, the research variables, and the case study design before launching a full-scale investigation. The role of pilot case studies has not been fully addressed in empirical software engineering research literature. Objective: To explore the use of pilot case studies in the design of full-scale case studies, and to report the main lessons learned from an industrial pilot study. Method: We designed and conducted an exploratory case study to identify new relevant research variables that influence the innovative behaviour of software engineers in the industrial setting and to refine the full-scale case study design for the next phase of our research. Results: The use of a pilot case study identified several important research variables that were missing in the initial framework. The pilot study also supported a more sophisticated case study design, which was used to guide a full-scale study. Conclusions: When a research topic has not been fully discovered or understood, it is difficult to create a case study design that covers the relevant research variables and their potential relationships. Conducting a full-scale case study using an untested case design can lead to waste of resources and time if the design has to be reworked during the study. In these situations, the use of pilot case studies can significantly improve the case study design. △ Less","2 December, 2016",https://arxiv.org/pdf/1612.00730
The Tyranny of Data? The Bright and Dark Sides of Data-Driven Decision-Making for Social Good,Bruno Lepri;Jacopo Staiano;David Sangokoya;Emmanuel Letouzé;Nuria Oliver,"The unprecedented availability of large-scale human behavioral data is profoundly changing the world we live in. Researchers, companies, governments, financial institutions, non-governmental organizations and also citizen groups are actively experimenting, innovating and adapting algorithmic decision-making tools to understand global patterns of human behavior and provide decision support to tackle problems of societal importance. In this chapter, we focus our attention on social good decision-making algorithms, that is algorithms strongly influencing decision-making and resource optimization of public goods, such as public health, safety, access to finance and fair employment. Through an analysis of specific use cases and approaches, we highlight both the positive opportunities that are created through data-driven algorithmic decision-making, and the potential negative consequences that practitioners should be aware of and address in order to truly realize the potential of this emergent field. We elaborate on the need for these algorithms to provide transparency and accountability, preserve privacy and be tested and evaluated in context, by means of living lab approaches involving citizens. Finally, we turn to the requirements which would make it possible to leverage the predictive power of data-driven human behavior analysis while ensuring transparency, accountability, and civic participation. △ Less","2 December, 2016",https://arxiv.org/pdf/1612.00323
Sub 100nW volatile nano-metal-oxide memristor as synaptic-like encoder of neuronal spikes,Isha Gupta;Alexantrou Serb;Ali Khiat;Ralf Zeitler;Stefano Vassanelli;Themistoklis Prodromakis,"Advanced neural interfaces mediate a bio-electronic link between the nervous system and microelectronic devices, bearing great potential as innovative therapy for various diseases. Spikes from a large number of neurons are recorded leading to creation of big data that require on-line processing under most stringent conditions, such as minimal power dissipation and on-chip space occupancy. Here, we present a new concept where the inherent volatile properties of a nano-scale memristive device are used to detect and compress information on neural spikes as recorded by a multi-electrode array. Simultaneously, and similarly to a biological synapse, information on spike amplitude and frequency is transduced in metastable resistive state transitions of the device, which is inherently capable of self-resetting and of continuous encoding of spiking activity. Furthermore, operating the memristor in a very high resistive state range reduces its average in-operando power dissipation to less than 100 nW, demonstrating the potential to build highly scalable, yet energy-efficient on-node processors for advanced neural interfaces. △ Less","29 November, 2016",https://arxiv.org/pdf/1611.09671
Social Behavior Prediction from First Person Videos,Shan Su;Jung Pyo Hong;Jianbo Shi;Hyun Soo Park,"This paper presents a method to predict the future movements (location and gaze direction) of basketball players as a whole from their first person videos. The predicted behaviors reflect an individual physical space that affords to take the next actions while conforming to social behaviors by engaging to joint attention. Our key innovation is to use the 3D reconstruction of multiple first person cameras to automatically annotate each other's the visual semantics of social configurations. We leverage two learning signals uniquely embedded in first person videos. Individually, a first person video records the visual semantics of a spatial and social layout around a person that allows associating with past similar situations. Collectively, first person videos follow joint attention that can link the individuals to a group. We learn the egocentric visual semantics of group movements using a Siamese neural network to retrieve future trajectories. We consolidate the retrieved trajectories from all players by maximizing a measure of social compatibility---the gaze alignment towards joint attention predicted by their social formation, where the dynamics of joint attention is learned by a long-term recurrent convolutional network. This allows us to characterize which social configuration is more plausible and predict future group trajectories. △ Less","28 November, 2016",https://arxiv.org/pdf/1611.09464
PVANet: Lightweight Deep Neural Networks for Real-time Object Detection,Sanghoon Hong;Byungseok Roh;Kye-Hyeon Kim;Yeongjae Cheon;Minje Park,"In object detection, reducing computational cost is as important as improving accuracy for most practical usages. This paper proposes a novel network structure, which is an order of magnitude lighter than other state-of-the-art networks while maintaining the accuracy. Based on the basic principle of more layers with less channels, this new deep neural network minimizes its redundancy by adopting recent innovations including C.ReLU and Inception structure. We also show that this network can be trained efficiently to achieve solid results on well-known object detection benchmarks: 84.9% and 84.2% mAP on VOC2007 and VOC2012 while the required compute is less than 10% of the recent ResNet-101. △ Less","9 December, 2016",https://arxiv.org/pdf/1611.08588
Component-based Synthesis of Table Consolidation and Transformation Tasks from Examples,Yu Feng;Ruben Martins;Jacob Van Geffen;Isil Dillig;Swarat Chaudhuri,"This paper presents an example-driven synthesis technique for automating a large class of data preparation tasks that arise in data science. Given a set of input tables and an out- put table, our approach synthesizes a table transformation program that performs the desired task. Our approach is not restricted to a fixed set of DSL constructs and can synthesize programs from an arbitrary set of components, including higher-order combinators. At a high-level, our approach performs type-directed enumerative search over partial pro- grams but incorporates two key innovations that allow it to scale: First, our technique can utilize any first-order specification of the components and uses SMT-based deduction to reject partial programs. Second, our algorithm uses partial evaluation to increase the power of deduction and drive enumerative search. We have evaluated our synthesis algorithm on dozens of data preparation tasks obtained from on-line forums, and we show that our approach can automatically solve a large class of problems encountered by R users. △ Less","22 November, 2016",https://arxiv.org/pdf/1611.07502
3D Image Reconstruction from X-Ray Measurements with Overlap,Maria Klodt;Raphael Hauser,"3D image reconstruction from a set of X-ray projections is an important image reconstruction problem, with applications in medical imaging, industrial inspection and airport security. The innovation of X-ray emitter arrays allows for a novel type of X-ray scanners with multiple simultaneously emitting sources. However, two or more sources emitting at the same time can yield measurements from overlapping rays, imposing a new type of image reconstruction problem based on nonlinear constraints. Using traditional linear reconstruction methods, respective scanner geometries have to be implemented such that no rays overlap, which severely restricts the scanner design. We derive a new type of 3D image reconstruction model with nonlinear constraints, based on measurements with overlapping X-rays. Further, we show that the arising optimization problem is partially convex, and present an algorithm to solve it. Experiments show highly improved image reconstruction results from both simulated and real-world measurements. △ Less","22 November, 2016",https://arxiv.org/pdf/1611.07390
End-to-End Subtitle Detection and Recognition for Videos in East Asian Languages via CNN Ensemble with Near-Human-Level Performance,Yan Xu;Siyuan Shan;Ziming Qiu;Zhipeng Jia;Zhengyang Shen;Yipei Wang;Mengfei Shi;Eric I-Chao Chang,"In this paper, we propose an innovative end-to-end subtitle detection and recognition system for videos in East Asian languages. Our end-to-end system consists of multiple stages. Subtitles are firstly detected by a novel image operator based on the sequence information of consecutive video frames. Then, an ensemble of Convolutional Neural Networks (CNNs) trained on synthetic data is adopted for detecting and recognizing East Asian characters. Finally, a dynamic programming approach leveraging language models is applied to constitute results of the entire body of text lines. The proposed system achieves average end-to-end accuracies of 98.2% and 98.3% on 40 videos in Simplified Chinese and 40 videos in Traditional Chinese respectively, which is a significant outperformance of other existing methods. The near-perfect accuracy of our system dramatically narrows the gap between human cognitive ability and state-of-the-art algorithms used for such a task. △ Less","18 November, 2016",https://arxiv.org/pdf/1611.06159
CoSTAR: Instructing Collaborative Robots with Behavior Trees and Vision,Chris Paxton;Andrew Hundt;Felix Jonathan;Kelleher Guerin;Gregory D. Hager,"For collaborative robots to become useful, end users who are not robotics experts must be able to instruct them to perform a variety of tasks. With this goal in mind, we developed a system for end-user creation of robust task plans with a broad range of capabilities. CoSTAR: the Collaborative System for Task Automation and Recognition is our winning entry in the 2016 KUKA Innovation Award competition at the Hannover Messe trade show, which this year focused on Flexible Manufacturing. CoSTAR is unique in how it creates natural abstractions that use perception to represent the world in a way users can both understand and utilize to author capable and robust task plans. Our Behavior Tree-based task editor integrates high-level information from known object segmentation and pose estimation with spatial reasoning and robot actions to create robust task plans. We describe the cross-platform design and implementation of this system on multiple industrial robots and evaluate its suitability for a wide variety of use cases. △ Less","18 November, 2016",https://arxiv.org/pdf/1611.06145
Ranking medical jargon in electronic health record notes by adapted distant supervision,Jinying Chen;Abhyuday N. Jagannatha;Samah J. Jarad;Hong Yu,"Objective: Allowing patients to access their own electronic health record (EHR) notes through online patient portals has the potential to improve patient-centered care. However, medical jargon, which abounds in EHR notes, has been shown to be a barrier for patient EHR comprehension. Existing knowledge bases that link medical jargon to lay terms or definitions play an important role in alleviating this problem but have low coverage of medical jargon in EHRs. We developed a data-driven approach that mines EHRs to identify and rank medical jargon based on its importance to patients, to support the building of EHR-centric lay language resources. Methods: We developed an innovative adapted distant supervision (ADS) model based on support vector machines to rank medical jargon from EHRs. For distant supervision, we utilized the open-access, collaborative consumer health vocabulary, a large, publicly available resource that links lay terms to medical jargon. We explored both knowledge-based features from the Unified Medical Language System and distributed word representations learned from unlabeled large corpora. We evaluated the ADS model using physician-identified important medical terms. Results: Our ADS model significantly surpassed two state-of-the-art automatic term recognition methods, TF*IDF and C-Value, yielding 0.810 ROC-AUC versus 0.710 and 0.667, respectively. Our model identified 10K important medical jargon terms after ranking over 100K candidate terms mined from over 7,500 EHR narratives. Conclusion: Our work is an important step towards enriching lexical resources that link medical jargon to lay terms/definitions to support patient EHR comprehension. The identified medical jargon terms and their rankings are available upon request. △ Less","14 November, 2016",https://arxiv.org/pdf/1611.04491
On Service-Chaining Strategies using Virtual Network Functions in Operator Networks,Abhishek Gupta;M. Farhan Habib;Uttam Mandal;Pulak Chowdhury;Massimo Tornatore;Biswanath Mukherjee,"Network functions (e.g., firewalls, load balancers, etc.) have been traditionally provided through proprietary hardware appliances. Often, hardware appliances need to be hardwired back to back to form a service chain providing chained network functions. Hardware appliances cannot be provisioned on demand since they are statically embedded in the network topology, making creation, insertion, modification, upgrade, and removal of service chains complex, and also slowing down service innovation. Hence, network operators are starting to deploy Virtual Network Functions (VNFs), which are virtualized over commodity hardware. VNFs can be deployed in Data Centers (DCs) or in Network Function Virtualization (NFV) capable network elements (nodes) such as routers and switches. NFV capable nodes and DCs together form a Network enabled Cloud (NeC) that helps to facilitate the dynamic service chaining required to support evolving network traffic and its service demands. In this study, we focus on the VNF service chain placement and traffic routing problem, and build a model for placing a VNF service chain while minimizing network resource consumption. Our results indicate that a NeC having a DC and NFV capable nodes can significantly reduce network-resource consumption. △ Less","10 November, 2016",https://arxiv.org/pdf/1611.03453
Learning an Astronomical Catalog of the Visible Universe through Scalable Bayesian Inference,Jeffrey Regier;Kiran Pamnany;Ryan Giordano;Rollin Thomas;David Schlegel;Jon McAuliffe;Prabhat,"Celeste is a procedure for inferring astronomical catalogs that attains state-of-the-art scientific results. To date, Celeste has been scaled to at most hundreds of megabytes of astronomical images: Bayesian posterior inference is notoriously demanding computationally. In this paper, we report on a scalable, parallel version of Celeste, suitable for learning catalogs from modern large-scale astronomical datasets. Our algorithmic innovations include a fast numerical optimization routine for Bayesian posterior inference and a statistically efficient scheme for decomposing astronomical optimization problems into subproblems. Our scalable implementation is written entirely in Julia, a new high-level dynamic programming language designed for scientific and numerical computing. We use Julia's high-level constructs for shared and distributed memory parallelism, and demonstrate effective load balancing and efficient scaling on up to 8192 Xeon cores on the NERSC Cori supercomputer. △ Less","10 November, 2016",https://arxiv.org/pdf/1611.03404
Stitching Inter-Domain Paths over IXPs,Vasileios Kotronis;Rowan Kloti;Matthias Rost;Panagiotis Georgopoulos;Bernhard Ager;Stefan Schmid;Xenofontas Dimitropoulos,"Modern Internet applications, from HD video-conferencing to health monitoring and remote control of power-plants, pose stringent demands on network latency, bandwidth and availability. An approach to support such applications and provide inter-domain guarantees, enabling new avenues for innovation, is using centralized inter-domain routing brokers. These entities centralize routing control for mission-critical traffic across domains, working in parallel to BGP. In this work, we propose using IXPs as natural points for stitching inter-domain paths under the control of inter-domain routing brokers. To evaluate the potential of this approach, we first map the global substrate of inter-IXP pathlets that IXP members could offer, based on measurements for 229 IXPs worldwide. We show that using IXPs as stitching points has two useful properties. Up to 91 % of the total IPv4 address space can be served by such inter-domain routing brokers when working in concert with just a handful of large IXPs and their associated ISP members. Second, path diversity on the inter-IXP graph increases by up to 29 times, as compared to current BGP valley-free routing. To exploit the rich path diversity, we introduce algorithms that inter-domain routing brokers can use to embed paths, subject to bandwidth and latency constraints. We show that our algorithms scale to the sizes of the measured graphs and can serve diverse simulated path request mixes. Our work highlights a novel direction for SDN innovation across domains, based on logically centralized control and programmable IXP fabrics. △ Less","8 November, 2016",https://arxiv.org/pdf/1611.02642
Citation algorithms for identifying research milestones driving biomedical innovation,Jordan A. Comins;Loet Leydesdorff,"Scientific activity plays a major role in innovation for biomedicine and healthcare. For instance, fundamental research on disease pathologies and mechanisms can generate potential targets for drug therapy. This co-evolution is punctuated by papers which provide new perspectives and open new domains. Despite the relationship between scientific discovery and biomedical advancement, identifying these research milestones that truly impact biomedical innovation can be difficult and is largely based solely on the opinions of subject matter experts. Here, we consider whether a new class of citation algorithms that identify seminal scientific works in a field, Reference Publication Year Spectroscopy (RPYS) and multi-RPYS, can identify the connections between innovation (e.g. therapeutic treatments) and the foundational research underlying them. Specifically, we assess whether the results of these analytic techniques converge with expert opinions on research milestones driving biomedical innovation in the treatment of Basal Cell Carcinoma. Our results show that these algorithms successfully identify the majority of milestone papers detailed by experts (Wong and Dlugosz 2014) thereby validating the power of these algorithms to converge on independent opinions of seminal scientific works derived by subject matter experts. These advances offer an opportunity to identify scientific activities enabling innovation in biomedicine. △ Less","5 November, 2016",https://arxiv.org/pdf/1611.01658
Learning to Rank Scientific Documents from the Crowd,Jesse M Lingeman;Hong Yu,"Finding related published articles is an important task in any science, but with the explosion of new work in the biomedical domain it has become especially challenging. Most existing methodologies use text similarity metrics to identify whether two articles are related or not. However biomedical knowledge discovery is hypothesis-driven. The most related articles may not be ones with the highest text similarities. In this study, we first develop an innovative crowd-sourcing approach to build an expert-annotated document-ranking corpus. Using this corpus as the gold standard, we then evaluate the approaches of using text similarity to rank the relatedness of articles. Finally, we develop and evaluate a new supervised model to automatically rank related scientific articles. Our results show that authors' ranking differ significantly from rankings by text-similarity-based models. By training a learning-to-rank model on a subset of the annotated corpus, we found the best supervised learning-to-rank model (SVM-Rank) significantly surpassed state-of-the-art baseline systems. △ Less","4 November, 2016",https://arxiv.org/pdf/1611.01400
Deep Convolutional Neural Network Design Patterns,Leslie N. Smith;Nicholay Topin,"Recent research in the deep learning field has produced a plethora of new architectures. At the same time, a growing number of groups are applying deep learning to new applications. Some of these groups are likely to be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore opt to use an older architecture (i.e., Alexnet). Here we attempt to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures. In addition, we describe several architectural innovations, including Fractal of FractalNet network, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope others are inspired to build on our preliminary work. △ Less","14 November, 2016",https://arxiv.org/pdf/1611.00847
Limitations and Alternatives for the Evaluation of Large-scale Link Prediction,Dario Garcia-Gasulla;Eduard Ayguadé;Jesús Labarta;Ulises Cortés,"Link prediction, the problem of identifying missing links among a set of inter-related data entities, is a popular field of research due to its application to graph-like domains. Producing consistent evaluations of the performance of the many link prediction algorithms being proposed can be challenging due to variable graph properties, such as size and density. In this paper we first discuss traditional data mining solutions which are applicable to link prediction evaluation, arguing about their capacity for producing faithful and useful evaluations. We also introduce an innovative modification to a traditional evaluation methodology with the goal of adapting it to the problem of evaluating link prediction algorithms when applied to large graphs, by tackling the problem of class imbalance. We empirically evaluate the proposed methodology and, building on these findings, make a case for its importance on the evaluation of large-scale graph processing. △ Less","25 November, 2016",https://arxiv.org/pdf/1611.00547
Low Cost Autonomous Navigation and Control of a Mechanically Balanced Bicycle with Dual Locomotion Mode,Ayush Pandey;Subhamoy Mahajan;Adarsh Kosta;Dhananjay Yadav;Vikas Pandey;Saurav Sahay;Siddharth Jha;Shubh Agarwal;Aashay Bhise;Raushan Kumar;Aniket Bhushan;Vraj Parikh;Ankit Lohani;Saurabh Dash;Himanshu Choudhary;Rahul Kumar;Anurag Sharma;Arnab Mondal;Chendika Karthik Sai;P N Vamshi,"On the lines of the huge and varied efforts in the field of automation with respect to technology development and innovation of vehicles to make them run autonomously, this paper presents an innovation to a bicycle. A normal daily use bicycle was modified at low cost such that it runs autonomously, while maintaining its original form i.e. the manual drive. Hence, a bicycle which could be normally driven by any human and with a press of switch could run autonomously according to the needs of the user has been developed. △ Less","1 November, 2016",https://arxiv.org/pdf/1611.00331
Mining Social Media for Open Innovation in Transportation Systems,Daniela Ulloa;Pedro Saleiro;Rosaldo J. F. Rossetti;Elis Regina Silva,"This work proposes a novel framework for the development of new products and services in transportation through an open innovation approach based on automatic content analysis of social media data. The framework is able to extract users comments from Online Social Networks (OSN), to process and analyze text through information extraction and sentiment analysis techniques to obtain relevant information about product reception on the market. A use case was developed using the mobile application Uber, which is today one of the fastest growing technology companies in the world. We measured how a controversial, highly diffused event influences the volume of tweets about Uber and the perception of its users. While there is no change in the image of Uber, a large increase in the number of tweets mentioning the company is observed, which meant a free and important diffusion of its product. △ Less","31 October, 2016",https://arxiv.org/pdf/1610.09894
Supporting novel biomedical research via multilayer collaboration networks,Konstantin Kuzmin;Xiaoyan Lu;Partha Sarathi Mukherjee;Juntao Zhuang;Chris Gaiteri;Boleslaw K Szymanski,"The value of research containing novel combinations of molecules can be seen in many innovative and award-winning research programs. Despite calls to use innovative approaches to address common diseases, an increasing majority of research funding goes toward ""safe"" incremental research. Counteracting this trend by nurturing novel and potentially transformative scientific research is challenging, it must be supported in competition with established research programs. Therefore, we propose a tool that helps to resolve the tension between safe but fundable research vs. high-risk but potentially transformational research. It does this by identifying hidden overlapping interest around novel molecular research topics. Specifically, it identifies paths of molecular interactions that connect research topics and hypotheses that would not typically be associated, as the basis for scientific collaboration. Because these collaborations are related to the scientists' present trajectory, they are low risk and can be initiated rapidly. Unlike most incremental steps, these collaborations have the potential for leaps in understanding, as they reposition research for novel disease applications. We demonstrate the use of this tool to identify scientists who could contribute to understanding the cellular role of genes with novel associations with Alzheimer's disease, which have not been thoroughly characterized, in part due to the funding emphasis on established research. △ Less","28 October, 2016",https://arxiv.org/pdf/1610.09253
The Composition and Formation of Effective Teams. Computer Science meets Psychology,Ewa Andrejczuk;Rita Berger;Juan A. Rodriguez-Aguilar;Carles Sierra;Víctor Marín-Puchades,"Nowadays the composition and formation of effective teams is highly important for both companies to assure their competitiveness and for a wide range of emerging applications exploiting multiagent collaboration (e.g. crowdsourcing, human-agent collaborations). The aim of this article is to provide an integrative perspective on team composition, team formation and their relationship with team performance. Thus, we review the contributions in both the computer science literature and the organisational psychology literature dealing with these topics. Our purpose is twofold. First, we aim at identifying the strengths and weaknesses of the contributions made by these two diverse bodies of research. Second, we pursue to identify cross-fertilisation opportunities that help both disciplines benefit from one another. Given the volume of existing literature, our review is not intended to be exhaustive. Instead, we have preferred to focus on the most significant contributions in both fields together with recent contributions that break new ground to spur innovative research. △ Less","27 October, 2016",https://arxiv.org/pdf/1610.08804
VNF Placement with Replication for Load Balancing in NFV Networks,Francisco Carpio;Samia Dhahri;Admela Jukan,"Network Function Virtualization (NFV) is a new paradigm, enabling service innovation through virtualization of traditional network functions located flexibly in the network in form of Virtual Network Functions (VNFs). Since VNFs can only be placed onto servers located in networked data centers, which is the NFV's salient feature, the traffic directed to these data center areas has significant impact on network load balancing. Network load balancing can be even more critical for an ordered sequence of VNFs, also known as Service Function Chains (SFCs), a common cloud and network service approach today. To balance the network load, VNF's can be placed in a smaller cluster of servers in the network thus minimizing the distance to the data center. The optimization of the placement of these clusters is a challenge as also other factors need to be considered, such as the resource utilization. To address this issue, we study the problem of VNF placement with replications, and especially the potential of VNFs replications to help load balance the network. We design and compare three optimization methods, including Linear Programing (LP) model, Genetic Algorithm (GA) and Random Fit Placement Algorithm (RFPA) for the allocation and replication of VNFs. Our results show that the optimum placement and replication can significantly improve load balancing, for which we also propose a GA heuristics applicable to larger networks. △ Less","26 October, 2016",https://arxiv.org/pdf/1610.08266
The Third Offset and a Fifth Domain? Balancing Game-Changing Innovation and Cyber Risk Mitigation,Craig Jackson;Robert Templeman,"Cyber has changed the scope of the national security mission and is placing new strains on our diplomatic, warfighting, legal, and economic/budgetary processes. Cybersecurity processes and techniques are increasingly critical to our warfighting missions, but they can also inhibit the pace and potential for high impact, game-changing innovation. Throughout its history, the Navy has shown the ability of innovation (in policy, process, and technology) to change the game when our security is on the line. We believe the Navy is capable of dramatically impacting not only the U.S. capabilities in cyber conflict and information operations, but also in cyber defense and information assurance, as well as cybersecurity for our society. While cyber risk management is challenging, the transition from DoD Information Assurance Certification and Accreditation Process (DIACAP) to the Risk Management Framework (RMF) has the potential to harmonize our cybersecurity efforts with our need (and demonstrated ability to provide) for game-changing strategies, tactics, and technologies. We offer a foundation for the foregoing assertions and recommendations on ways to encourage innovation in the context of effective cyber risk management. △ Less","25 October, 2016",https://arxiv.org/pdf/1610.07982
Challenges to be addressed for realising an Ephemeral Cloud Federation,Emanuele Carlini;Massimo Coppola;Patrizio Dazzi;Matteo Mordacchini,"This paper sketches the challenges to address to realise a support able to achieve an Ephemeral Cloud Federation, an innovative cloud computing paradigm that enables the exploitation of a dynamic, personalised and context-aware set of resources. The aim of the Ephemeral Federation is to answer to the need of combining private data-centres with both federation of cloud providers and the resource on the edge of the network. The goal of the Ephemeral Federation is to deliver a context-aware and personalised federations of computational, data and network resources, able to manage their heterogeneity in a highly distributed deployment, which can dynamically bring data and computation close to the final user. △ Less","24 October, 2016",https://arxiv.org/pdf/1610.07371
Developing and Assessing MATLAB Exercises for Active Concept Learning,S. H. Song;Marco Antonelli;Tony Fung;Brandon D. Armstrong;Amy Chong;Albert Lo;Bertram E. Shi,"New technologies, such as MOOCs, provide innovative methods to tackle new challenges in teaching and learning, such as globalization and changing contemporary culture and to remove the limits of conventional classrooms. However, they also bring challenges in course delivery and assessment, due to factors such as less direct student-instructor interaction. These challenges are especially severe in engineering education, which relies heavily on experiential learning, such as computer simulations and laboratory exercises, to assist students in understanding concepts. As a result, effective design of experiential learning components is extremely critical for engineering MOOCs. In this paper, we will share our experience gained through developing and offering a MOOC on communication systems, with special focus on the development and assessment of MATLAB exercises for active concept learning. Our approach introduced students to concepts using learning components commonly provided by many MOOC platforms (e.g., online lectures and quizzes), and augmented the student experience with MATLAB based computer simulations and exercises to enable more concrete and detailed understanding of the material. We describe here a systematic approach to MATLAB problem design and assessment, based on our experience with the MATLAB server provided by MathWorks and integrated with the edX MOOC platform. We discuss the effectiveness of the instructional methods as evaluated through students' learning performance. We analyze the impact of the course design tools from both the instructor and the student perspective. △ Less","23 October, 2016",https://arxiv.org/pdf/1610.07129
Automated Big Text Security Classification,Khudran Alzhrani;Ethan M. Rudd;Terrance E. Boult;C. Edward Chow,"In recent years, traditional cybersecurity safeguards have proven ineffective against insider threats. Famous cases of sensitive information leaks caused by insiders, including the WikiLeaks release of diplomatic cables and the Edward Snowden incident, have greatly harmed the U.S. government's relationship with other governments and with its own citizens. Data Leak Prevention (DLP) is a solution for detecting and preventing information leaks from within an organization's network. However, state-of-art DLP detection models are only able to detect very limited types of sensitive information, and research in the field has been hindered due to the lack of available sensitive texts. Many researchers have focused on document-based detection with artificially labeled ""confidential documents"" for which security labels are assigned to the entire document, when in reality only a portion of the document is sensitive. This type of whole-document based security labeling increases the chances of preventing authorized users from accessing non-sensitive information within sensitive documents. In this paper, we introduce Automated Classification Enabled by Security Similarity (ACESS), a new and innovative detection model that penetrates the complexity of big text security classification/detection. To analyze the ACESS system, we constructed a novel dataset, containing formerly classified paragraphs from diplomatic cables made public by the WikiLeaks organization. To our knowledge this paper is the first to analyze a dataset that contains actual formerly sensitive information annotated at paragraph granularity. △ Less","21 October, 2016",https://arxiv.org/pdf/1610.06856
Novelty Learning via Collaborative Proximity Filtering,Arun Kumar;Paul Schrater,"The vast majority of recommender systems model preferences as static or slowly changing due to observable user experience. However, spontaneous changes in user preferences are ubiquitous in many domains like media consumption and key factors that drive changes in preferences are not directly observable. These latent sources of preference change pose new challenges. When systems do not track and adapt to users' tastes, users lose confidence and trust, increasing the risk of user churn. We meet these challenges by developing a model of novelty preferences that learns and tracks latent user tastes. We combine three innovations: a new measure of item similarity based on patterns of consumption co-occurrence; model for {\em spontaneous} changes in preferences; and a learning agent that tracks each user's dynamic preferences and learns individualized policies for variety. The resulting framework adaptively provides users with novelty tailored to their preferences for change per se. △ Less","20 October, 2016",https://arxiv.org/pdf/1610.06633
Efficient Estimation of Compressible State-Space Models with Application to Calcium Signal Deconvolution,Abbas Kazemipour;Ji Liu;Patrick Kanold;Min Wu;Behtash Babadi,"In this paper, we consider linear state-space models with compressible innovations and convergent transition matrices in order to model spatiotemporally sparse transient events. We perform parameter and state estimation using a dynamic compressed sensing framework and develop an efficient solution consisting of two nested Expectation-Maximization (EM) algorithms. Under suitable sparsity assumptions on the innovations, we prove recovery guarantees and derive confidence bounds for the state estimates. We provide simulation studies as well as application to spike deconvolution from calcium imaging data which verify our theoretical results and show significant improvement over existing algorithms. △ Less","20 October, 2016",https://arxiv.org/pdf/1610.06461
Security Testbed for the Internet of Things,Shachar Siboni;Vinay Sachidananda;Asaf Shabtai;Yuval Elovici,"The Internet of Things (IoT) is a global ecosystem of information and communication technologies aimed at connecting any type of object (thing), at any time and in any place, to each other and to the Internet. One of the major problems associated with the IoT is maintaining security; the heterogeneous nature of such deployments poses a challenge to many aspects of security, including security testing and analysis. In addition, there is no existing mechanism that performs security testing for IoT devices in different contexts. In this paper, we propose an innovative security testbed framework targeted at IoT devices. The security testbed supports both standard and context-based security testing, with a set of security tests conducted under the different environmental conditions in which IoT devices operate. The requirements and architectural design of the proposed testbed are discussed, and the testbed operation is demonstrated in several testing scenarios. △ Less","19 October, 2016",https://arxiv.org/pdf/1610.05971
Rule Extraction Algorithm for Deep Neural Networks: A Review,Tameru Hailesilassie,"Despite the highest classification accuracy in wide varieties of application areas, artificial neural network has one disadvantage. The way this Network comes to a decision is not easily comprehensible. The lack of explanation ability reduces the acceptability of neural network in data mining and decision system. This drawback is the reason why researchers have proposed many rule extraction algorithms to solve the problem. Recently, Deep Neural Network (DNN) is achieving a profound result over the standard neural network for classification and recognition problems. It is a hot machine learning area proven both useful and innovative. This paper has thoroughly reviewed various rule extraction algorithms, considering the classification scheme: decompositional, pedagogical, and eclectics. It also presents the evaluation of these algorithms based on the neural network structure with which the algorithm is intended to work. The main contribution of this review is to show that there is a limited study of rule extraction algorithm from DNN. △ Less","16 September, 2016",https://arxiv.org/pdf/1610.05267
Making Mainstream Synthesizers with Csound,Gleb G. Rogozinsky;Eugene Cherny;Ivan Osipenko,"For more than the past twenty years, Csound has been one of the leaders in the world of the computer music research, implementing innovative synthesis methods and making them available beyond the academic environments from which they often arise, and into the hands of musicians and sound designers throughout the world. In its present state, Csound offers an efficient environment for sound experimentation, allowing the user to work with almost any known sound synthesis or signal processing method through its vast collection of ready-made opcodes. But despite all this potential, the shared resource of Csound instruments still lacks quality reproductions of well-known synthesizers; even with its ability to generate commercial standard user interfaces and with the possibility to compile Csound instruments in such as fashion so that they can be used with no knowledge of Csound code. To fill this gap, the authors have implemented two commercial-style synthesizers as VST plug-ins using the Csound front-end ""Cabbage"". This paper describes their architecture and some of the Csound specific challenges involved in the development of fully featured synthesizers. △ Less","16 October, 2016",https://arxiv.org/pdf/1610.04922
A Survey on Various Data Mining Techniques for ECG Meta Analysis,Kratika Tyagi;Prof. Sanjeev Thakur,"Data Mining is the process of examining the information from different point of view and compressing it for the relevant data. This data can also be utilized to build the incomes. Data Mining is also known as Data or Knowledge Discovery. The basic purpose of data mining is to search patterns which have minimal user inputs and efforts. Data Mining plays a very crucial role in the various fields. There are various data mining procedures which can be connected in different fields of innovation. By using data mining techniques, it is observed that less time is taken for the prediction of any disease with more accuracy. In this paper we would review various data mining techniques which are categorized under classification, regression and clustering and apply these algorithms over an ECG dataset. The purpose of this work is to determine the most suitable data mining technique and use it to improve the accuracy of analyzing ECG data for better decision making. △ Less","22 April, 2016",https://arxiv.org/pdf/1610.04577
Bibliometric Index for Academic Leadership,Yang Liu;Fengrong Ou;Yan Deng;Bo Wu;Ruxi Liu;Hui Hua;Yuyuan Guan;Rentong Chen;Lars Gjesteby;Jiansheng Yang;Michael Vannier;Ge Wang,"Academic leadership is essential for research innovation and impact. Until now, there has been no dedicated measure of leadership by bibliometrics. Popular bibliometric indices are mainly based on academic output, such as the journal impact factor and the number of citations. Here we develop an academic leadership index based on readily available bibliometric data that is sensitive to not only academic output but also research efficiency. Our leadership index was tested in two studies on peer-reviewed journal papers by extramurally-funded principal investigators in the field of life sciences from China and the USA, respectively. The leadership performance of these principal investigators was quantified and compared relative to university rank and other factors. As a validation measure, we show that the highest average leadership index was achieved by principal investigators at top national universities in both countries. More interestingly, our results also indicate that on an individual basis, strong leadership and high efficiency are not necessarily associated with those at top-tier universities nor with the most funding. This leadership index may become the basis of a comprehensive merit system, facilitating academic evaluation and resource management. △ Less","12 October, 2016",https://arxiv.org/pdf/1610.03706
Maximum entropy models capture melodic styles,Jason Sakellariou;Francesca Tria;Vittorio Loreto;François Pachet,"We introduce a Maximum Entropy model able to capture the statistics of melodies in music. The model can be used to generate new melodies that emulate the style of the musical corpus which was used to train it. Instead of using the n-body interactions of (n-1)-order Markov models, traditionally used in automatic music generation, we use a k-nearest neighbour model with pairwise interactions only. In that way, we keep the number of parameters low and avoid over-fitting problems typical of Markov models. We show that long-range musical phrases don't need to be explicitly enforced using high-order Markov interactions, but can instead emerge from multiple, competing, pairwise interactions. We validate our Maximum Entropy model by contrasting how much the generated sequences capture the style of the original corpus without plagiarizing it. To this end we use a data-compression approach to discriminate the levels of borrowing and innovation featured by the artificial sequences. The results show that our modelling scheme outperforms both fixed-order and variable-order Markov models. This shows that, despite being based only on pairwise interactions, this Maximum Entropy scheme opens the possibility to generate musically sensible alterations of the original phrases, providing a way to generate innovation. △ Less","11 October, 2016",https://arxiv.org/pdf/1610.03414
Towards Policy Enforcement Point as a Service (PEPS),Arash Shaghaghi;Mohamed Ali;Kaafar;Sandra Scott-Hayward;Salil S. Kanhere;Sanjay Jha,"In this paper, we coin the term Policy Enforcement as a Service (PEPS), which enables the provision of innovative inter-layer and inter-domain Access Control. We leverage the architecture of Software-Defined-Network (SDN) to introduce a common network-level enforcement point, which is made available to a range of access control systems. With our PEPS model, it is possible to have a `defense in depth' protection model and drop unsuccessful access requests before engaging the data provider (e.g. a database system). Moreover, the current implementation of access control within the `trusted' perimeter of an organization is no longer a restriction so that the potential for novel, distributed and cooperative security services can be realized. We conduct an analysis of the security requirements and technical challenges for implementing Policy Enforcement as a Service. To illustrate the benefits of our proposal in practice, we include a report on our prototype PEPS-enabled location-based access control. △ Less","8 October, 2016",https://arxiv.org/pdf/1610.02526
A Robust Framework for Classifying Evolving Document Streams in an Expert-Machine-Crowd Setting,Muhammad Imran;Sanjay Chawla;Carlos Castillo,"An emerging challenge in the online classification of social media data streams is to keep the categories used for classification up-to-date. In this paper, we propose an innovative framework based on an Expert-Machine-Crowd (EMC) triad to help categorize items by continuously identifying novel concepts in heterogeneous data streams often riddled with outliers. We unify constrained clustering and outlier detection by formulating a novel optimization problem: COD-Means. We design an algorithm to solve the COD-Means problem and show that COD-Means will not only help detect novel categories but also seamlessly discover human annotation errors and improve the overall quality of the categorization process. Experiments on diverse real data sets demonstrate that our approach is both effective and efficient. △ Less","6 October, 2016",https://arxiv.org/pdf/1610.01858
Multiple Regularizations Deep Learning for Paddy Growth Stages Classification from LANDSAT-8,Ines Heidieni Ikasari;Vina Ayumi;Mohamad Ivan Fanany;Sidik Mulyono,"This study uses remote sensing technology that can provide information about the condition of the earth's surface area, fast, and spatially. The study area was in Karawang District, lying in the Northern part of West Java-Indonesia. We address a paddy growth stages classification using LANDSAT 8 image data obtained from multi-sensor remote sensing image taken in October 2015 to August 2016. This study pursues a fast and accurate classification of paddy growth stages by employing multiple regularizations learning on some deep learning methods such as DNN (Deep Neural Networks) and 1-D CNN (1-D Convolutional Neural Networks). The used regularizations are Fast Dropout, Dropout, and Batch Normalization. To evaluate the effectiveness, we also compared our method with other machine learning methods such as (Logistic Regression, SVM, Random Forest, and XGBoost). The data used are seven bands of LANDSAT-8 spectral data samples that correspond to paddy growth stages data obtained from i-Sky (eye in the sky) Innovation system. The growth stages are determined based on paddy crop phenology profile from time series of LANDSAT-8 images. The classification results show that MLP using multiple regularization Dropout and Batch Normalization achieves the highest accuracy for this dataset. △ Less","6 October, 2016",https://arxiv.org/pdf/1610.01795
Learning How to Communicate in the Internet of Things: Finite Resources and Heterogeneity,Taehyeun Park;Nof Abuzainab;Walid Saad,"For a seamless deployment of the Internet of Things (IoT), there is a need for self-organizing solutions to overcome key IoT challenges that include data processing, resource management, coexistence with existing wireless networks, and improved IoT-wide event detection. One of the most promising solutions to address these challenges is via the use of innovative learning frameworks that will enable the IoT devices to operate autonomously in a dynamic environment. However, developing learning mechanisms for the IoT requires coping with unique IoT properties in terms of resource constraints, heterogeneity, and strict quality-of-service requirements. In this paper, a number of emerging learning frameworks suitable for IoT applications are presented. In particular, the advantages, limitations, IoT applications, and key results pertaining to machine learning, sequential learning, and reinforcement learning are studied. For each type of learning, the computational complexity, required information, and learning performance are discussed. Then, to handle the heterogeneity of the IoT, a new framework based on the powerful tools of cognitive hierarchy theory is introduced. This framework is shown to efficiently capture the different IoT device types and varying levels of available resources among the IoT devices. In particular, the different resource capabilities of IoT devices are mapped to different levels of rationality in cognitive hierarchy theory, thus enabling the IoT devices to use different learning frameworks depending on their available resources. Finally, key results on the use of cognitive hierarchy theory in the IoT are presented. △ Less","13 October, 2016",https://arxiv.org/pdf/1610.01586
5G-ICN : Delivering ICN Services over 5G using Network Slicing,Ravishankar Ravindran;Asit Chakraborti;Syed Obaid Amin;Aytac Azgin;Guoqiang Wang,"The challenging requirements of 5G--from both the applications and the architecture perspectives--motivate the need to explore the feasibility of delivering services over new network architectures. As 5G proposes application-centric network slicing, which enables the use of new data planes realizable over a programmable compute, storage, and transport infrastructure, we consider Information-centric Networking (ICN) as a candidate network architecture to realize 5G objectives. This can co-exist with end-to-end IP services that are offered today. To this effect, we first propose a 5G-ICN architecture and compare its benefits (i.e., innovative services offered by leveraging ICN features) to current 3GPP-based mobile architectures. We then introduce a general application-driven framework that emphasizes on the flexibility afforded by Network Function Virtualization (NFV) and Software Defined Networking (SDN) over which 5G-ICN can be realized. We specifically focus on the issue of how mobility-as-a-service (MaaS) can be realized as a 5G-ICN slice, and give an in-depth overview on resource provisioning and inter-dependencies and -coordinations among functional 5G-ICN slices to meet the MaaS objectives. △ Less","4 October, 2016",https://arxiv.org/pdf/1610.01182
Harnessing the Potential of the American Community Survey: Delving into Methods of Data Delivery,Eve Ahearn;Olga Ianiuk,"The American Community Survey (ACS) is the bedrock underpinning any analysis of the US population, urban areas included. The Census Bureau delivers the ACS data in multiple formats, yet in each the raw data is difficult to export in bulk and difficult to sift through. We argue that Enigma's approach to the data delivery, such as our raw data and metadata presentation, reflects the survey's logical structure. It can be explored, interlinked, and searched; making it easier to retrieve the appropriate data applicable to a question at hand. We make the use of data more liquid via curated tables and API access; even metadata and notes from technical documentation are programmatically accessible. Additionally, we are working towards opening our scalable and reproducible ingestion process of ACS estimations. This paper details all of the ways the Census Bureau currently makes the data available, the barriers each of these raise to applying this data in analysis and how our approach overcomes them. Finally, this paper will address other recent innovations in making Census datasets more usable, the use cases suited to each and how they fit into the wider application of data science. △ Less","7 October, 2016",https://arxiv.org/pdf/1609.09758
Knowledge management metrics for Public Organizations: A literature review-based proposal,Pérez López-Portillo;Héctor;Vázquez González;Edgar René;Romero Hidalgo;Jorge Alberto,"Knowledge Management (KM) is a relatively new phenomenon that appears in the field of Public Sector Organizations (PSO) bringing new paradigms of organizational management, challenges, risks and opportunities for its implementation, development and evaluation. KM can be seen as a systematic and deliberate effort to coordinate people, technology, organizational structures and its environment through knowledge reuse and innovation. This management approach has been established in parallel with the development and use of information and communications technologies (ICT). Nowadays more PSO are embodying KM practices in their core processes for support them, and as an advanced management strategy to create a new culture based on technology and resources efficiency. In this paper, we observed that KM can support organizational goals in PSO. The aim of this paper is to understand KM factors and its associated components, and propose KM metrics for measure KM programs in PSO. Through a critical literature review we analysed diverse studies related with KM performance indicators in PSO, then based on previous works we summarized the more convenient this purpose. We found that, in academic literature, studies about KM measurement in PSO are uncommon and emerging. As well, in the last section of this paper, we present a proposal of KM metrics for PSO, and some recommendations and practical implications for KM metrics development in PSO. This academic endeavour seeks to contribute to theoretical debate about KM measure development for KM initiatives in PSO. △ Less","29 September, 2016",https://arxiv.org/pdf/1609.09541
"A Simple, Fast and Highly-Accurate Algorithm to Recover 3D Shape from 2D Landmarks on a Single Image",Ruiqi Zhao;Yan Wang;Aleix Martinez,"Three-dimensional shape reconstruction of 2D landmark points on a single image is a hallmark of human vision, but is a task that has been proven difficult for computer vision algorithms. We define a feed-forward deep neural network algorithm that can reconstruct 3D shapes from 2D landmark points almost perfectly (i.e., with extremely small reconstruction errors), even when these 2D landmarks are from a single image. Our experimental results show an improvement of up to two-fold over state-of-the-art computer vision algorithms; 3D shape reconstruction of human faces is given at a reconstruction error < .004, cars at .0022, human bodies at .022, and highly-deformable flags at an error of .0004. Our algorithm was also a top performer at the 2016 3D Face Alignment in the Wild Challenge competition (done in conjunction with the European Conference on Computer Vision, ECCV) that required the reconstruction of 3D face shape from a single image. The derived algorithm can be trained in a couple hours and testing runs at more than 1, 000 frames/s on an i7 desktop. We also present an innovative data augmentation approach that allows us to train the system efficiently with small number of samples. And the system is robust to noise (e.g., imprecise landmark points) and missing data (e.g., occluded or undetected landmark points). △ Less","28 September, 2016",https://arxiv.org/pdf/1609.09058
A Case in Kenya: Unlocking bottlenecks in public health supply chains through data dashboards and enhanced governance structures,Yasmin Chandani;Elizabeth A. Bunde;Wambui Waithaka;Eric Wakaria;James Riungu;Judith Njumwah-Kariuki;Alexis Strader,"The link between data and governance are key to making public health supply chains more integrated and responsive in order to get life-saving commodities to those in need. In particular, considering its significant health challenges, poor maternal and child health indicators, and major recent devolution in political authority, Kenya represents a country in need of an innovative revamp of their data management and governance. John Snow, Inc. (JSI) adapted various elements of proven interventions to build a customized structure to support routine data collection in order to drive decision making around supply chain improvement. △ Less","27 September, 2016",https://arxiv.org/pdf/1609.08755
TMI! How Knowledge Platforms Tame the Information Overload and Advance Global Development Through Technology,Rob Goodier;Iana Aranda;Laura MacDonald,"Finding reliable data to inform decisions about technology for global development remains a challenge. Easily accessible ""Knowledge platforms"" are a way to curate and standardize information about technology for development. Three collaborators, Engineering for Change, the Global Alliance for Clean Cookstoves and the Center for Affordable Water and Sanitation Technology (CAWST) are working together to create platforms to serve the global development sector. Such platforms could be the first step in making decisions about how to solve a problem that needs a technology-based solution. They could motivate manufacturers to demonstrate compliance with quality standards, and they could encourage independent research into each product's impact. Years of development experience worldwide have yielded a set of best practices to increase the life of a project. These platforms clarify those practices to improve quality and reduce waste. As the platforms mature, mining them for data could identify trends that influence the entire technology ecosystem. Those could include decisions about performance targets, future innovation, pricing, compliance with regulations, and funding priorities. △ Less","27 September, 2016",https://arxiv.org/pdf/1609.08753
Survey of Inter-satellite Communication for Small Satellite Systems: Physical Layer to Network Layer View,Radhika Radhakrishnan;William Edmonson;Fatemeh Afghah;R. Rodriguez-Osorio;Frank Pinto;Scott Burleigh,"Small satellite systems enable whole new class of missions for navigation, communications, remote sensing and scientific research for both civilian and military purposes. As individual spacecraft are limited by the size, mass and power constraints, mass-produced small satellites in large constellations or clusters could be useful in many science missions such as gravity mapping, tracking of forest fires, finding water resources, etc. Constellation of satellites provide improved spatial and temporal resolution of the target. Small satellite constellations contribute innovative applications by replacing a single asset with several very capable spacecraft which opens the door to new applications. With increasing levels of autonomy, there will be a need for remote communication networks to enable communication between spacecraft. These space based networks will need to configure and maintain dynamic routes, manage intermediate nodes, and reconfigure themselves to achieve mission objectives. Hence, inter-satellite communication is a key aspect when satellites fly in formation. In this paper, we present the various researches being conducted in the small satellite community for implementing inter-satellite communications based on the Open System Interconnection (OSI) model. This paper also reviews the various design parameters applicable to the first three layers of the OSI model, i.e., physical, data link and network layer. Based on the survey, we also present a comprehensive list of design parameters useful for achieving inter-satellite communications for multiple small satellite missions. Specific topics include proposed solutions for some of the challenges faced by small satellite systems, enabling operations using a network of small satellites, and some examples of small satellite missions involving formation flying aspects. △ Less","27 September, 2016",https://arxiv.org/pdf/1609.08583
A Model for the Adoption Process of Information System Security Innovations in Organisations: A Theoretical Perspective,Mumtaz Abdul Hameed;Nalin Asanka Gamagedara Arachchilage,"In this paper, we develop a theoretical model for the adoption process of Information System Security innovations in organisations. The model stemmed from the Diffusion of Innovation theory (DOI), the Technology Acceptance Model (TAM), the Theory of Planned Behaviour (TPB) and the Technology-Organisation-Environment (TOE) framework. The model portrays Information System Security adoption process progressing in a sequence of stages. The study considers the adoption process from the initiation stage until the acquisition of innovation as an organisational level judgement while the process of innovation assimilation and integration is assessed in terms of the user behaviour within the organisation. The model also introduces several factors that influence the Information System Security innovation adoption. By merging the organisational adoption and user acceptance of innovation in a single depiction, this research contributes to IS security literature a more comprehensive model for IS security adoption in organisation, compare to any of the past representations. △ Less","26 September, 2016",https://arxiv.org/pdf/1609.07911
Using an innovative assessment approach on a real-world group based software project,Susan Bergin;Aidan Mooney,"Currently, there is a lack of practical, real-world projects on Computer Science (CS) courses at Maynooth University. Generally CS undergraduate modules are composed of 24 hours of lectures and 24 hours of labs where students learn theoretical concepts in the lectures and apply their understanding to practical lab-based exercises. The problem with this approach is that students do not gain any awareness of, or learn how to solve tasks that they are likely to encounter in a real-world industrial setting; nor do they gain experience of working as part of a team even though most software development positions involve team-based work. This paper reports on a web-based development module that incorporated a real-world group based project was re-designed and delivered. The module went well; however, assessing the work fairly was found to be difficult, especially where team members contributed at considerably varying levels was a challenge. Of particular concern was that some hard-working students were penalised by other students poor work and lazy students were rewarded because of more hard-working students work. This action research project will attempt to re-address how to assess this group-based work with a cohort of students. The goal of the research is to implement an innovative assessment structure, using peer-, self-, and co-assessment, for a group based real-world project, that is deemed fair and reasonable and provided a good learning environment. △ Less","26 September, 2016",https://arxiv.org/pdf/1609.07899
"The End of effective Law Enforcement in the Cloud? To encypt, or not to encrypt",Steven Ryder;Nhien-An Le-Khac,"With an exponentially increasing usage of cloud services, the need for forensic investigations of virtual space is equally in constantly increasing demand, which includes as a very first approach, the gaining of access to it as well as the data stored. This is an aspect that faces a number of challenges, stemming not only from the technical difficulties and peculiarities, but equally covers the interaction with an emerging line of businesses offering cloud storage and services. Beyond the forensic aspects, it also covers to an ever increasing amount the non-forensic considerations, such as the availability of logs and archives, legal and data protection considerations from a global perspective and the clashes in between, as well as the ever competing interests between law enforcement to seize evidence which is non-physical, and businesses who need to be able to continue to operate and provide their hosted services, even if law enforcement seek to collect evidence. The trend post-Snowden has been unequivocally towards default encryption, and driven by market leaders such as Apple, motivated to a large extent by the perceived demands for privacy of the consumer. The central question to be explored in this paper is to what extent this trend towards default encryption will have a negative impact on law enforcement investigations and possibilities, and will at the end attempt to provide a solution, which takes into account the needs of both law enforcement, but also of the service providers. It is hoped that the recommendations from this paper will be able to have an impact in the ability for law enforcement to continue with their investigations in an efficient manner, whilst also safeguarding the ability for business to thrive and continue to develop and offer new and innovative solutions, which do not put law enforcement at risk. △ Less","24 September, 2016",https://arxiv.org/pdf/1609.07602
Learning and Inference of Dexterous Grasps for Novel Objects with Underactuated Hands,Marek Kopicki;Carlos J. Rosales;Hamal Marino;Marco Gabiccini;Jeremy L. Wyatt,"Recent advances have been made in learning of grasps for fully actuated hands. A typical approach learns the target locations of finger links on the object. When a new object must be grasped, new finger locations are generated, and a collision free reach-to-grasp trajectory is planned. This assumes a collision free trajectory to the final grasp. This is not possible with underactuated hands, which cannot be guaranteed to avoid contact, and in fact exploit contacts with the object during grasping, so as to reach an equilibrium state in which the object is held securely. Unfortunately, these contact interactions are i) not directly controllable, and ii) hard to monitor during a real grasp. We overcome these problems so as to permit learning of transferrable grasps for underactuated hands. We make two main technical innovations. First, we model contact interactions during the grasp implicitly. We do this by modelling motor commands that lead reliably to the equilibrium state, rather than modelling contact changes themselves. This alters our reach-to-grasp model. Second, we extend our contact model learning algorithm to work with multiple training examples for each grasp type. This requires the ability to learn which parts of the hand reliably interact with the object during a particular grasp. Our approach learns from a rigid body simulation. This enables us to learn how to approach the object and close the underactuated hand from a variety of poses. From nine training grasps on three objects the method transferred grasps to previously unseen, novel objects, that differ significantly from the training objects, with an 80% success rate. △ Less","24 September, 2016",https://arxiv.org/pdf/1609.07592
21st Century Computer Architecture,Mark D. Hill;Sarita Adve;Luis Ceze;Mary Jane Irwin;David Kaeli;Margaret Martonosi;Josep Torrellas;Thomas F. Wenisch;David Wood;Katherine Yelick,"Because most technology and computer architecture innovations were (intentionally) invisible to higher layers, application and other software developers could reap the benefits of this progress without engaging in it. Higher performance has both made more computationally demanding applications feasible (e.g., virtual assistants, computer vision) and made less demanding applications easier to develop by enabling higher-level programming abstractions (e.g., scripting languages and reusable components). Improvements in computer system cost-effectiveness enabled value creation that could never have been imagined by the field's founders (e.g., distributed web search sufficiently inexpensive so as to be covered by advertising links). The wide benefits of computer performance growth are clear. Recently, Danowitz et al. apportioned computer performance growth roughly equally between technology and architecture, with architecture credited with ~80x improvement since 1985. As semiconductor technology approaches its ""end-of-the-road"" (see below), computer architecture will need to play an increasing role in enabling future ICT innovation. But instead of asking, ""How can I make my chip run faster?,"" architects must now ask, ""How can I enable the 21st century infrastructure, from sensors to clouds, adding value from performance to privacy, but without the benefit of near-perfect technology scaling?"". The challenges are many, but with appropriate investment, opportunities abound. Underlying these opportunities is a common theme that future architecture innovations will require the engagement of and investments from innovators in other ICT layers. △ Less","21 September, 2016",https://arxiv.org/pdf/1609.06756
Dense Wide-Baseline Scene Flow From Two Handheld Video Cameras,Christian Richardt;Hyeongwoo Kim;Levi Valgaerts;Christian Theobalt,"We propose a new technique for computing dense scene flow from two handheld videos with wide camera baselines and different photometric properties due to different sensors or camera settings like exposure and white balance. Our technique innovates in two ways over existing methods: (1) it supports independently moving cameras, and (2) it computes dense scene flow for wide-baseline scenarios.We achieve this by combining state-of-the-art wide-baseline correspondence finding with a variational scene flow formulation. First, we compute dense, wide-baseline correspondences using DAISY descriptors for matching between cameras and over time. We then detect and replace occluded pixels in the correspondence fields using a novel edge-preserving Laplacian correspondence completion technique. We finally refine the computed correspondence fields in a variational scene flow formulation. We show dense scene flow results computed from challenging datasets with independently moving, handheld cameras of varying camera settings. △ Less","16 September, 2016",https://arxiv.org/pdf/1609.05115
DiNoDB: an Interactive-speed Query Engine for Ad-hoc Queries on Temporary Data,Yongchao Tian;Ioannis Alagiannis;Erietta Liarou;Anastasia Ailamaki;Pietro Michiardi;Marko Vukolic,"As data sets grow in size, analytics applications struggle to get instant insight into large datasets. Modern applications involve heavy batch processing jobs over large volumes of data and at the same time require efficient ad-hoc interactive analytics on temporary data. Existing solutions, however, typically focus on one of these two aspects, largely ignoring the need for synergy between the two. Consequently, interactive queries need to re-iterate costly passes through the entire dataset (e.g., data loading) that may provide meaningful return on investment only when data is queried over a long period of time. In this paper, we propose DiNoDB, an interactive-speed query engine for ad-hoc queries on temporary data. DiNoDB avoids the expensive loading and transformation phase that characterizes both traditional RDBMSs and current interactive analytics solutions. It is tailored to modern workflows found in machine learning and data exploration use cases, which often involve iterations of cycles of batch and interactive analytics on data that is typically useful for a narrow processing window. The key innovation of DiNoDB is to piggyback on the batch processing phase the creation of metadata that DiNoDB exploits to expedite the interactive queries. Our experimental analysis demonstrates that DiNoDB achieves very good performance for a wide range of ad-hoc queries compared to alternatives %such as Hive, Stado, SparkSQL and Impala. △ Less","16 September, 2016",https://arxiv.org/pdf/1609.05096
A Perspective on Deep Imaging,Ge Wang,"The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning on medical imaging, major challenges must be addressed. △ Less","4 November, 2016",https://arxiv.org/pdf/1609.04375
Quantitative identification of technological discontinuities using simulation modeling,Hyunseok Park;Christopher L. Magee,"The aim of this paper is to develop and test metrics to quantitatively identify technological discontinuities in a knowledge network. We developed five metrics based on innovation theories and tested the metrics by a simulation model-based knowledge network and hypothetically designed discontinuity. The designed discontinuity is modeled as a node which combines two different knowledge streams and whose knowledge is dominantly persistent in the knowledge network. The performances of the proposed metrics were evaluated by how well the metrics can distinguish the designed discontinuity from other nodes on the knowledge network. The simulation results show that the persistence times # of converging main paths provides the best performance in identifying the designed discontinuity: the designed discontinuity was identified as one of the top 3 patents with 96~99% probability by Metric 5 and it is, according to the size of a domain, 12~34% better than the performance of the second best metric. Beyond the simulation analysis, we tested the metrics using a patent set representative of the Magnetic information storage domain. The three representative patents associated with a well-known breakthrough technology in the domain, the giant magneto-resistance (GMR) spin valve sensor, were selected based on the qualitative studies, and the metrics were tested by how well the metrics identify the selected patents as top-ranked patents. The empirical results fully support the simulation results and therefore the persistence times # of converging main paths is recommended for identifying technological discontinuities for any technology. △ Less","13 September, 2016",https://arxiv.org/pdf/1609.03806
Network Map Reduce,Haoyu Song;Jun Gong;Hongfei Chen,"Networking data analytics is increasingly used for enhanced network visibility and controllability. We draw the similarities between the Software Defined Networking (SDN) architecture and the MapReduce programming model. Inspired by the similarity, we suggest the necessary data plane innovations to make network data plane devices function as distributed mappers and optionally, reducers. A streaming network data MapReduce architecture can therefore conveniently solve a series of network monitoring and management problems. Unlike the traditional networking data analytical system, our proposed system embeds the data analytics engine directly in the network infrastructure. The affinity leads to a concise system architecture and better cost performance ratio. On top of this architecture, we propose a general MapReduce-like programming model for real-time and one-pass networking data analytics, which involves joint in-network and out-of-network computing. We show this model can address a wide range of interactive queries from various network applications. This position paper strives to make a point that the white-box trend does not necessarily lead to simple and dumb networking devices. Rather, the defining characteristics of the next generation white-box are open and programmable, so that the network devices can be made smart and versatile to support new services and applications. △ Less","9 September, 2016",https://arxiv.org/pdf/1609.02982
No Free Charge Theorem: a Covert Channel via USB Charging Cable on Mobile Devices,Riccardo Spolaor;Laila Abudahi;Veelasha Moonsamy;Mauro Conti;Radha Poovendran,"More and more people are regularly using mobile and battery-powered handsets, such as smartphones and tablets. At the same time, thanks to the technological innovation and to the high user demands, those devices are integrating extensive functionalities and developers are writing battery-draining apps, which results in a surge of energy consumption of these devices. This scenario leads many people to often look for opportunities to charge their devices at public charging stations: the presence of such stations is already prominent around public areas such as hotels, shopping malls, airports, gyms and museums, and is expected to significantly grow in the future. While most of the time the power comes for free, there is no guarantee that the charging station is not maliciously controlled by an adversary, with the intention to exfiltrate data from the devices that are connected to it. In this paper, we illustrate for the first time how an adversary could leverage a maliciously controlled charging station to exfiltrate data from the smartphone via a USB charging cable (i.e., without using the data transfer functionality), controlling a simple app running on the device, and without requiring any permission to be granted by the user to send data out of the device. We show the feasibility of the proposed attack through a prototype implementation in Android, which is able to send out potentially sensitive information, such as IMEI, contacts' phone number, and pictures. △ Less","9 September, 2016",https://arxiv.org/pdf/1609.02750
An improved uncertainty decoding scheme with weighted samples for DNN-HMM hybrid systems,Christian Huemmer;Ramón Fernández Astudillo;Walter Kellermann,"In this paper, we advance a recently-proposed uncertainty decoding scheme for DNN-HMM (deep neural network - hidden Markov model) hybrid systems. This numerical sampling concept averages DNN outputs produced by a finite set of feature samples (drawn from a probabilistic distortion model) to approximate the posterior likelihoods of the context-dependent HMM states. As main innovation, we propose a weighted DNN-output averaging based on a minimum classification error criterion and apply it to a probabilistic distortion model for spatial diffuseness features. The experimental evaluation is performed on the 8-channel REVERB Challenge task using a DNN-HMM hybrid system with multichannel front-end signal enhancement. We show that the recognition accuracy of the DNN-HMM hybrid system improves by incorporating uncertainty decoding based on random sampling and that the proposed weighted DNN-output averaging further reduces the word error rate scores. △ Less","4 August, 2016",https://arxiv.org/pdf/1609.02082
"Multi-Tenancy Issues with Service Delivery in Developing Economies: Privacy, Trust and Availability Concerns",Ezer Osei Yeboah-Boateng;Akosua Boakyewaa Appiah-Nketiah,"Cloud computing is a new paradigm and innovation in the technology service delivery. It is utilized for IT-enabled value creation. The capex-free nature of cloud service delivery renders it very attractive to many SMEs. But it is saddled with multi-tenancy issues; prominent under this study are concerns of privacy, trust and availability. How do end-users trust providers with their sensitive data? How secured and confidential are their corporate assets? Amidst the perennial power outages (a.k.a. Dumsor), what is the acceptable available uptime? We sampled and interviewed cloud service providers (CSPs) as well as end-users in Ghana, a developing economy. We also gleaned through some secondary data to ascertain some operational concerns. The results indicate that security and service level agreements (SLAs) are key concerns in respect of privacy and trust issues. Similarly, perennial power outages and security were key availability concerns. This was expected as end-users use cloud services for mission critical information assets, and so requires high availability. The implications are that the cyber-security concerns ought to be addressed if SMEs in developing economies are to adopt and accept cloud computing resources for IT-enabled competitive advantage. △ Less","5 September, 2016",https://arxiv.org/pdf/1609.01822
Fleet management for autonomous vehicles,Sahar Bsaybes;Alain Quilliot;Annegret K. Wagler,"The VIPAFLEET project consists in developing models and algorithms for man- aging a fleet of Individual Public Autonomous Vehicles (VIPA). Hereby, we consider a fleet of cars distributed at specified stations in an industrial area to supply internal transportation, where the cars can be used in different modes of circulation (tram mode, elevator mode, taxi mode). One goal is to develop and implement suitable algorithms for each mode in order to satisfy all the requests under an economic point of view by minimizing the total tour length or the makespan. The innovative idea and challenge of the project is to develop and install a dynamic fleet management system that allows the operator to switch between the differ- ent modes within the different periods of the day according to the dynamic transportation demands of the users. We model the underlying online transportation system and propose an according fleet management framework, to handle modes, demands and commands. We propose for each mode appropriate online algorithms and evaluate their performance. △ Less","6 September, 2016",https://arxiv.org/pdf/1609.01634
Untangling the role of diverse social dimensions in the diffusion of microfinance,Elisa Omodei;Alex Arenas,"Ties between individuals on a social network can represent different dimensions of interactions, and the spreading of information and innovations on these networks could potentially be driven by some dimensions more than by others. In this paper we investigate this issue by studying the diffusion of microfinance within rural India villages and accounting for the whole multilayer structure of the underlying social networks. We define a new measure of node centrality, diffusion versatility, and show that this is a better predictor of microfinance participation rate than previously introduced measures defined on aggregated single-layer social networks. Moreover, we untangle the role played by each social dimension and find that the most prominent role is played by the nodes that are central on layers concerned with trust, shedding new light on the key triggers of the diffusion of microfinance. △ Less","24 November, 2016",https://arxiv.org/pdf/1609.01455
Accelerating More Secure RC4 : Implementation of Seven FPGA Designs in Stages upto 8 byte per clock,Rourab Paul;Hemanta Dey;Amlan Chakrabarti;Ranjan Ghosh,"RC4 can be made more secured if an additional RC4-like Post-KSA Random Shuffing (PKRS) process is introduced between KSA and PRGA. It can also be made significantly faster if RC4 bytes are processed in a FPGA embedded system using multiple coprocessors functioning in parallel. The PKRS process is tuned to form as many S-boxes as required by particular design architectures involving multiple coprocessors, each one undertaking byte-by-byte processing. Following a ecent idea [1] [2] the speed of execution of each processor is also enhanced by another fold if the byte-by-byte processing is replaced by a scheme of processing two consecutive bytes together. Adopting some new innovative concepts, three hardware design architectures are proposed in a suitable FPGA embedded system involving 1, 2 and 4 coprocessors functioning in parallel and a study is made on accelerating RC4 by processing bytes in byte-by-byte mode achieving throughputs from 1-byte-in-1-clock to 4-bytes-in-1-clock. The hardware designs are appropriately upgraded to accelerate RC4 further by processing 2 onsecutive RC4 bytes together and it has been possible to achieve a maximum throughput of 8-bytes per clock in Xilinx Virtex-5 LX110t FPGA [3] architecture followed by secured data communication between two FPGA boards. △ Less","20 September, 2016",https://arxiv.org/pdf/1609.01389
An Analysis of the Cloud Computing Security Problem,Mohamed Almorsy;John Grundy;Ingo Müller,"Cloud computing is a new computational paradigm that offers an innovative business model for organizations to adopt IT without upfront investment. Despite the potential gains achieved from the cloud computing, the model security is still questionable which impacts the cloud model adoption. The security problem becomes more complicated under the cloud model as new dimensions have entered into the problem scope related to the model architecture, multi-tenancy, elasticity, and layers dependency stack. In this paper we introduce a detailed analysis of the cloud security problem. We investigated the problem from the cloud architecture perspective, the cloud offered characteristics perspective, the cloud stakeholders' perspective, and the cloud service delivery models perspective. Based on this analysis we derive a detailed specification of the cloud security problem and key features that should be covered by any proposed security solution. △ Less","5 September, 2016",https://arxiv.org/pdf/1609.01107
GTApprox: surrogate modeling for industrial design,Mikhail Belyaev;Evgeny Burnaev;Ermek Kapushev;Maxim Panov;Pavel Prikhodko;Dmitry Vetrov;Dmitry Yarotsky,"We describe GTApprox - a new tool for medium-scale surrogate modeling in industrial design. Compared to existing software, GTApprox brings several innovations: a few novel approximation algorithms, several advanced methods of automated model selection, novel options in the form of hints. We demonstrate the efficiency of GTApprox on a large collection of test problems. In addition, we describe several applications of GTApprox to real engineering problems. △ Less","5 September, 2016",https://arxiv.org/pdf/1609.01088
CNSMO: A Network Services Manager/Orchestrator Tool for Cloud Federated Environments,J. Aznar;E. Escalona;I. Canyameres;O. Moya;A. Viñes,"Application service providers (ASPs) now develop, deploy, and maintain complex computing platforms within multiple cloud infrastructures to improve resilience, responsiveness and elasticity of their applications. On the other hand, complex applications have little control and visibility over network resources, and need to use low-level hacks to extract network properties and prioritize traffic. This biased view, limits tenants flexibility while deploying their applications and prevents them from implementing part of the application logic in the network. In this paper, we propose the CNSMO (CYCLONE Network Services Manager/Orchestrator) tool to bring the innovation at federated cloud environments by bridging these network service capabilities to cloud based services as part of the overall CYCLONE solution. The integration of networking aspects with purely federated clouds, will allow users to request specific infrastructures and manage their dedicated set of coordinated network and IT resources in an easy and transparent way while operating dynamic deployments of distributed applications. △ Less","5 September, 2016",https://arxiv.org/pdf/1609.01043
868 MHz Wireless Sensor Network - A Study,Pushpam Aji John;Rudolf Agren;Yu-Jung Chen;Christian Rohner;Edith Ngai,"Today 2.4 GHz based wireless sensor networks are increasing at a tremendous pace, and are seen in widespread applications. Product innovation and support by many vendors in 2.4 GHz makes it a preferred choice, but the networks are prone to issues like interference, and range issues. On the other hand, the less popular 868 MHz in the ISM band has not seen significant usage. In this paper we explore the use of 868 MHz channel to implement a wireless sensor network, and study the efficacy of this channel △ Less","2 September, 2016",https://arxiv.org/pdf/1609.00475
A Compression-Complexity Measure of Integrated Information,Mohit Virmani;Nithin Nagaraj,"Quantifying integrated information is a leading approach towards building a fundamental theory of consciousness. Integrated Information Theory (IIT) has gained attention in this regard due to its theoretically strong framework. However, it faces some limitations such as current state dependence, computationally expensive and inability to be applied to real brain data. On the other hand, Perturbational Complexity Index (PCI) is a clinical measure for distinguishing different levels of consciousness. Though PCI claims to capture the functional differentiation and integration in brain networks (similar to IIT), its link to integrated information theories is rather weak. Inspired by these two approaches, we propose a new measure - Φ^C using a novel compression-complexity perspective that serves as a bridge between the two, for the first time. Φ^C is founded on the principles of lossless data compression based complexity measures which characterize the dynamical complexity of brain networks. Φ^{C} exhibits following salient innovations: (i) mathematically well bounded, (ii) negligible current state dependence unlike Φ, (iii) integrated information measured as compression-complexity rather than as an infotheoretic quantity, and (iv) faster to compute since number of atomic bipartitions scales linearly with the number of nodes of the network, thus avoiding combinatorial explosion. Our computer simulations show that Φ^C has similar hierarchy to <Φ> for several multiple-node networks and it demonstrates a rich interplay between differentiation, integration and entropy of the nodes of a network. Φ^C is a promising heuristic measure to characterize the quantity of integrated information (and hence a measure of quantity of consciousness) in larger networks like human brain and provides an opportunity to test the predictions of brain complexity on real neural data. △ Less","13 December, 2016",https://arxiv.org/pdf/1608.08450
PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection,Kye-Hyeon Kim;Sanghoon Hong;Byungseok Roh;Yeongjae Cheon;Minje Park,"This paper presents how we can achieve the state-of-the-art accuracy in multi-category object detection task while minimizing the computational cost by adapting and combining recent technical innovations. Following the common pipeline of ""CNN feature extraction + region proposal + RoI classification"", we mainly redesign the feature extraction part, since region proposal part is not computationally expensive and classification part can be efficiently compressed with common techniques like truncated SVD. Our design principle is ""less channels with more layers"" and adoption of some building blocks including concatenated ReLU, Inception, and HyperNet. The designed network is deep and thin and trained with the help of batch normalization, residual connections, and learning rate scheduling based on plateau detection. We obtained solid results on well-known object detection benchmarks: 83.8% mAP (mean average precision) on VOC2007 and 82.5% mAP on VOC2012 (2nd place), while taking only 750ms/image on Intel i7-6700K CPU with a single core and 46ms/image on NVIDIA Titan X GPU. Theoretically, our network requires only 12.3% of the computational cost compared to ResNet-101, the winner on VOC2012. △ Less","30 September, 2016",https://arxiv.org/pdf/1608.08021
Optimal QoS-Aware Channel Assignment in D2D Communications with Partial CSI,Rui Wang;Jun Zhang;S. H. Song;Khaled B. Letaief,"In this paper, we propose effective channel assignment algorithms for network utility maximization in a cellular network with underlaying device-to-device (D2D) communications. A major innovation is the consideration of partial channel state information (CSI), i.e., the base station (BS) is assumed to be able to acquire `partial' instantaneous CSI of the cellular and D2D links, as well as, the interference links. In contrast to existing works, multiple D2D links are allowed to share the same channel, and the quality of service (QoS) requirements for both the cellular and D2D links are enforced. We first develop an optimal channel assignment algorithm based on dynamic programming (DP), which enjoys a much lower complexity compared to exhaustive search and will serve as a performance benchmark. To further reduce complexity, we propose a cluster-based sub-optimal channel assignment algorithm. New closed-form expressions for the expected weighted sum-rate and the successful transmission probabilities are also derived. Simulation results verify the effectiveness of the proposed algorithms. Moreover, by comparing different partial CSI scenarios, we observe that the CSI of the D2D communication links and the interference links from the D2D transmitters to the BS significantly affects the network performance, while the CSI of the interference links from the BS to the D2D receivers only has a negligible impact. △ Less","29 August, 2016",https://arxiv.org/pdf/1608.08014
Private and Truthful Aggregative Game for Large-Scale Spectrum Sharing,Pan Zhou;Wenqi Wei;Kaigui Bian;Dapeng Oliver Wu;Yuchong Hu;Qian Wang,"Thanks to the rapid development of information technology, the size of the wireless network becomes larger and larger, which makes spectrum resources more precious than ever before. To improve the efficiency of spectrum utilization, game theory has been applied to study the spectrum sharing in wireless networks for a long time. However, the scale of wireless network in existing studies is relatively small. In this paper, we introduce a novel game and model the spectrum sharing problem as an aggregative game for large-scale, heterogeneous, and dynamic networks. The massive usage of spectrum also leads to easier privacy divulgence of spectrum users' actions, which calls for privacy and truthfulness guarantees in wireless network. In a large decentralized scenario, each user has no priori about other users' decisions, which forms an incomplete information game. A ""weak mediator"", e.g., the base station or licensed spectrum regulator, is introduced and turns this game into a complete one, which is essential to reach a Nash equilibrium (NE). By utilizing past experience on the channel access, we propose an online learning algorithm to improve the utility of each user, achieving NE over time. Our learning algorithm also provides no regret guarantee to each user. Our mechanism admits an approximate ex-post NE. We also prove that it satisfies the joint differential privacy and is incentive-compatible. Efficiency of the approximate NE is evaluated, and the innovative scaling law results are disclosed. Finally, we provide simulation results to verify our analysis. △ Less","4 November, 2016",https://arxiv.org/pdf/1608.05537
Low-Earth Orbit Determination from Gravity Gradient Measurements,Xiucong Sun;Pei Chen;Christophe Macabiau;Chao Han,"An innovative orbit determination method which makes use of gravity gradients for Low-Earth-Orbiting satellites is proposed. The measurement principle of gravity gradiometry is briefly reviewed and the sources of measurement error are analyzed. An adaptive hybrid least squares batch filter based on linearization of the orbital equation and unscented transformation of the measurement equation is developed to estimate the orbital states and the measurement biases. The algorithm is tested with the actual flight data from the European Space Agency Gravity field and steady-state Ocean Circulation Explorer. The orbit determination results are compared with the GPS-derived orbits. The radial and cross-track position errors are on the order of tens of meters, whereas the along-track position error is over one order of magnitude larger. The gravity gradient based orbit determination method is promising for potential use in GPS-denied spacecraft navigation. △ Less","11 August, 2016",https://arxiv.org/pdf/1608.03367
DeepCAMP: Deep Convolutional Action & Attribute Mid-Level Patterns,Ali Diba;Ali Mohammad Pazandeh;Hamed Pirsiavash;Luc Van Gool,"The recognition of human actions and the determination of human attributes are two tasks that call for fine-grained classification. Indeed, often rather small and inconspicuous objects and features have to be detected to tell their classes apart. In order to deal with this challenge, we propose a novel convolutional neural network that mines mid-level image patches that are sufficiently dedicated to resolve the corresponding subtleties. In particular, we train a newly de- signed CNN (DeepPattern) that learns discriminative patch groups. There are two innovative aspects to this. On the one hand we pay attention to contextual information in an origi- nal fashion. On the other hand, we let an iteration of feature learning and patch clustering purify the set of dedicated patches that we use. We validate our method for action clas- sification on two challenging datasets: PASCAL VOC 2012 Action and Stanford 40 Actions, and for attribute recogni- tion we use the Berkeley Attributes of People dataset. Our discriminative mid-level mining CNN obtains state-of-the- art results on these datasets, without a need for annotations about parts and poses. △ Less","10 August, 2016",https://arxiv.org/pdf/1608.03217
A Framework for Context-Driven End-to-End QoS Control in Converged Networks,S. Y. Yerima;G. P. Parr;C. Peoples;S. McCLean;P. J. Morrow,"This paper presents a framework for context-driven policy-based QoS control and end-to-end resource management in converged next generation networks. The Converged Networks QoS Framework (CNQF) is being developed within the IU-ATC project, and comprises distributed functional entities whose instances co-ordinate the converged network infrastructure to facilitate scalable and efficient end-to-end QoS management. The CNQF design leverages aspects of TISPAN, IETF and 3GPP policy-based management architectures whilst also introducing important innovative extensions to support context-aware QoS control in converged networks. The framework architecture is presented and its functionalities and operation in specific application scenarios are described. △ Less","2 August, 2016",https://arxiv.org/pdf/1608.00839
3D visualization of astronomy data cubes using immersive displays,Gilles Ferrand;Jayanne English;Pourang Irani,"We report on an exploratory project aimed at performing immersive 3D visualization of astronomical data, starting with spectral-line radio data cubes from galaxies. This work is done as a collaboration between the Department of Physics and Astronomy and the Department of Computer Science at the University of Manitoba. We are building our prototype using the 3D engine Unity, because of its ease of use for integration with advanced displays such as a CAVE environment, a zSpace tabletop, or virtual reality headsets. We address general issues regarding 3D visualization, such as: load and convert astronomy data, perform volume rendering on the GPU, and produce physically meaningful visualizations using principles of visual literacy. We discuss some challenges to be met when designing a user interface that allows us to take advantage of this new way of exploring data. We hope to lay the foundations for an innovative framework useful for all astronomers who use spectral line data cubes, and encourage interested parties to join our efforts. This pilot project addresses the challenges presented by frontier astronomy experiments, such as the Square Kilometre Array and its precursors. △ Less","29 July, 2016",https://arxiv.org/pdf/1607.08874
arXiv@25: Key findings of a user survey,Oya Y. Rieger;Gail Steinhart;Deborah Cooper,"As part of its 25th anniversary vision-setting process, the arXiv team at Cornell University Library conducted a user survey in April 2016 to seek input from the global user community about arXiv's current services and future directions. We were heartened to receive 36,000 responses from 127 countries, representing arXiv's diverse, global community. The prevailing message is that users are happy with the service as it currently stands, with 95 percent of survey respondents indicating they are very satisfied or satisfied with arXiv. Furthermore, 72 percent of respondents indicated that arXiv should continue to focus on its main purpose, which is to quickly make available scientific papers, and this will be enough to sustain the value of arXiv in the future. This theme was pervasively reflected in the open text comments; a significant number of respondents suggested remaining focused on the core mission and enabling arXiv's partners and related service providers to continue to build new services and innovations on top of arXiv. △ Less","27 July, 2016",https://arxiv.org/pdf/1607.08212
Understanding Communication Patterns in MOOCs: Combining Data Mining and qualitative methods,Rebecca Eynon;Isis Hjorth;Taha Yasseri;Nabeel Gillani,"Massive Open Online Courses (MOOCs) offer unprecedented opportunities to learn at scale. Within a few years, the phenomenon of crowd-based learning has gained enormous popularity with millions of learners across the globe participating in courses ranging from Popular Music to Astrophysics. They have captured the imaginations of many, attracting significant media attention - with The New York Times naming 2012 ""The Year of the MOOC."" For those engaged in learning analytics and educational data mining, MOOCs have provided an exciting opportunity to develop innovative methodologies that harness big data in education. △ Less","25 July, 2016",https://arxiv.org/pdf/1607.07495
"Novel Graph Processor Architecture, Prototype System, and Results",William S. Song;Vitaliy Gleyzer;Alexei Lomakin;Jeremy Kepner,"Graph algorithms are increasingly used in applications that exploit large databases. However, conventional processor architectures are inadequate for handling the throughput and memory requirements of graph computation. Lincoln Laboratory's graph-processor architecture represents a rethinking of parallel architectures for graph problems. Our processor utilizes innovations that include a sparse matrix-based graph instruction set, a cacheless memory system, accelerator-based architecture, a systolic sorter, high-bandwidth multi-dimensional toroidal communication network, and randomized communications. A field-programmable gate array (FPGA) prototype of the new graph processor has been developed with significant performance enhancement over conventional processors in graph computational throughput. △ Less","21 July, 2016",https://arxiv.org/pdf/1607.06541
On the estimation of stellar parameters with uncertainty prediction from Generative Artificial Neural Networks: application to Gaia RVS simulated spectra,C. Dafonte;D. Fustes;M. Manteiga;D. Garabato;M. A. Alvarez;A. Ulla;C. Allende Prieto,"Aims. We present an innovative artificial neural network (ANN) architecture, called Generative ANN (GANN), that computes the forward model, that is it learns the function that relates the unknown outputs (stellar atmospheric parameters, in this case) to the given inputs (spectra). Such a model can be integrated in a Bayesian framework to estimate the posterior distribution of the outputs. Methods. The architecture of the GANN follows the same scheme as a normal ANN, but with the inputs and outputs inverted. We train the network with the set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]), obtaining the stellar spectra for such inputs. The residuals between the spectra in the grid and the estimated spectra are minimized using a validation dataset to keep solutions as general as possible. Results. The performance of both conventional ANNs and GANNs to estimate the stellar parameters as a function of the star brightness is presented and compared for different Galactic populations. GANNs provide significantly improved parameterizations for early and intermediate spectral types with rich and intermediate metallicities. The behaviour of both algorithms is very similar for our sample of late-type stars, obtaining residuals in the derivation of [Fe/H] and [alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs<12, which accounts for a number in the order of four million stars to be observed by the Radial Velocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty estimation of computed astrophysical parameters is crucial for the validation of the parameterization itself and for the subsequent exploitation by the astronomical community. GANNs produce not only the parameters for a given spectrum, but a goodness-of-fit between the observed spectrum and the predicted one for a given set of parameters. Moreover, they allow us to obtain the full posterior distribution... △ Less","19 July, 2016",https://arxiv.org/pdf/1607.05954
Towards Personality-Aware Recommendation,Giorgio Roffo,"In the last decade new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. In this new context, personality is a key determinant in the decision making of the consumer when shopping. The two main reasons are: firstly, a person's buying choices are influenced by psychological factors like impulsiveness, and secondly, some consumers may be more susceptible to making impulse purchases than others. To the best of our knowledge, the impact of personality factors on advertisements has been largely neglected at the level of recommender systems. This work proposes a highly innovative research which uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. As a matter of fact, the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of state-of-the-art algorithms. We present the ADS Dataset, a publicly available benchmark for computational advertising enriched with Big-Five users' personality factors and 1,200 personal users' pictures. The proposed benchmark allows two main tasks: rating prediction over 300 real advertisements (i.e., Rich Media Ads, Image Ads, Text Ads) and click-through rate prediction. Moreover, this work carries out experiments, reviews various evaluation criteria used in the literature, and provides a library for each one of them within one integrated toolbox. △ Less","23 July, 2016",https://arxiv.org/pdf/1607.05088
Multi-modal dictionary learning for image separation with application in art investigation,Nikos Deligiannis;Joao F. C. Mota;Bruno Cornelis;Miguel R. D. Rodrigues;Ingrid Daubechies,"In support of art investigation, we propose a new source separation method that unmixes a single X-ray scan acquired from double-sided paintings. In this problem, the X-ray signals to be separated have similar morphological characteristics, which brings previous source separation methods to their limits. Our solution is to use photographs taken from the front and back-side of the panel to drive the separation process. The crux of our approach relies on the coupling of the two imaging modalities (photographs and X-rays) using a novel coupled dictionary learning framework able to capture both common and disparate features across the modalities using parsimonious representations; the common component models features shared by the multi-modal images, whereas the innovation component captures modality-specific information. As such, our model enables the formulation of appropriately regularized convex optimization procedures that lead to the accurate separation of the X-rays. Our dictionary learning framework can be tailored both to a single- and a multi-scale framework, with the latter leading to a significant performance improvement. Moreover, to improve further on the visual quality of the separated images, we propose to train coupled dictionaries that ignore certain parts of the painting corresponding to craquelure. Experimentation on synthetic and real data - taken from digital acquisition of the Ghent Altarpiece (1432) - confirms the superiority of our method against the state-of-the-art morphological component analysis technique that uses either fixed or trained dictionaries to perform image separation. △ Less","14 July, 2016",https://arxiv.org/pdf/1607.04147
From Collective Adaptive Systems to Human Centric Computation and Back: Spatial Model Checking for Medical Imaging,Gina Belmonte;Vincenzo Ciancia;Diego Latella;Mieke Massink,"Recent research on formal verification for Collective Adaptive Systems (CAS) pushed advancements in spatial and spatio-temporal model checking, and as a side result provided novel image analysis methodologies, rooted in logical methods for topological spaces. Medical Imaging (MI) is a field where such technologies show potential for ground-breaking innovation. In this position paper, we present a preliminary investigation centred on applications of spatial model checking to MI. The focus is shifted from pure logics to a mixture of logical, statistical and algorithmic approaches, driven by the logical nature intrinsic to the specification of the properties of interest in the field. As a result, novel operators are introduced, that could as well be brought back to the setting of CAS. △ Less","8 July, 2016",https://arxiv.org/pdf/1607.02235
Formal Definitions of Unbounded Evolution and Innovation Reveal Universal Mechanisms for Open-Ended Evolution in Dynamical Systems,Alyssa M Adams;Hector Zenil;Paul CW Davies;Sara I Walker,"Open-ended evolution (OEE) is relevant to a variety of biological, artificial and technological systems, but has been challenging to reproduce in silico. Most theoretical efforts focus on key aspects of open-ended evolution as it appears in biology. We recast the problem as a more general one in dynamical systems theory, providing simple criteria for open-ended evolution based on two hallmark features: unbounded evolution and innovation. We define unbounded evolution as patterns that are non-repeating within the expected Poincare recurrence time of an equivalent isolated system, and innovation as trajectories not observed in isolated systems. As a case study, we implement novel variants of cellular automata (CA) in which the update rules are allowed to vary with time in three alternative ways. Each is capable of generating conditions for open-ended evolution, but vary in their ability to do so. We find that state-dependent dynamics, widely regarded as a hallmark of life, statistically out-performs other candidate mechanisms, and is the only mechanism to produce open-ended evolution in a scalable manner, essential to the notion of ongoing evolution. This analysis suggests a new framework for unifying mechanisms for generating OEE with features distinctive to life and its artifacts, with broad applicability to biological and artificial systems. △ Less","18 December, 2016",https://arxiv.org/pdf/1607.01750
Innovation diffusion equations on correlated scale-free networks,M. L. Bertotti;J. Brunner;G. Modanese,"We introduce a heterogeneous network structure into the Bass diffusion model, in order to study the diffusion times of innovation or information in networks with a scale-free structure, typical of regions where diffusion is sensitive to geographic and logistic influences (like for instance Alpine regions). We consider both the diffusion peak times of the total population and of the link classes. In the familiar trickle-down processes the adoption curve of the hubs is found to anticipate the total adoption in a predictable way. In a major departure from the standard model, we model a trickle-up process by introducing heterogeneous publicity coefficients (which can also be negative for the hubs, thus turning them into stiflers) and a stochastic term which represents the erratic generation of innovation at the periphery of the network. The results confirm the robustness of the Bass model and expand considerably its range of applicability. △ Less","1 July, 2016",https://arxiv.org/pdf/1607.01265
Statistical physics of linear and bilinear inference problems,Christophe Schülke,"The recent development of compressed sensing has led to spectacular advances in the understanding of sparse linear estimation problems as well as in algorithms to solve them. It has also triggered a new wave of developments in the related fields of generalized linear and bilinear inference problems, that have very diverse applications in signal processing and are furthermore a building block of deep neural networks. These problems have in common that they combine a linear mixing step and a nonlinear, probabilistic sensing step, producing indirect measurements of a signal of interest. Such a setting arises in problems as different as medical or astronomical imaging, clustering, matrix completion or blind source separation. The aim of this thesis is to propose efficient algorithms for this class of problems and to perform their theoretical analysis. To this end, it uses belief propagation, thanks to which high-dimensional distributions can be sampled efficiently, thus making a Bayesian approach to inference tractable. The resulting algorithms undergo phase transitions just as physical systems do. These phase transitions can be analyzed using the replica method, initially developed in statistical physics of disordered systems. The analysis reveals phases in which inference is easy, hard or impossible. These phases correspond to different energy landscapes of the problem. The main contributions of this thesis can be divided into three categories. First, the application of known algorithms to concrete problems: community detection, superposition codes and an innovative imaging system. Second, a new, efficient message-passing algorithm for a class of problems called blind sensor calibration. Third, a theoretical analysis of matrix compressed sensing and of instabilities in Bayesian bilinear inference algorithms. △ Less","3 July, 2016",https://arxiv.org/pdf/1607.00675
Using Social Media to Promote STEM Education: Matching College Students with Role Models,Ling He;Lee Murphy;Jiebo Luo,"STEM (Science, Technology, Engineering, and Mathematics) fields have become increasingly central to U.S. economic competitiveness and growth. The shortage in the STEM workforce has brought promoting STEM education upfront. The rapid growth of social media usage provides a unique opportunity to predict users' real-life identities and interests from online texts and photos. In this paper, we propose an innovative approach by leveraging social media to promote STEM education: matching Twitter college student users with diverse LinkedIn STEM professionals using a ranking algorithm based on the similarities of their demographics and interests. We share the belief that increasing STEM presence in the form of introducing career role models who share similar interests and demographics will inspire students to develop interests in STEM related fields and emulate their models. Our evaluation on 2,000 real college students demonstrated the accuracy of our ranking algorithm. We also design a novel implementation that recommends matched role models to the students. △ Less","1 July, 2016",https://arxiv.org/pdf/1607.00405
Want Drugs? Use Python,Michał Nowotka;George Papadatos;Mark Davies;Nathan Dedman;Anne Hersey,"We describe how Python can be leveraged to streamline the curation, modelling and dissemination of drug discovery data as well as the development of innovative, freely available tools for the related scientific community. We look at various examples, such as chemistry toolkits, machine-learning applications and web frameworks and show how Python can glue it all together to create efficient data science pipelines. △ Less","1 July, 2016",https://arxiv.org/pdf/1607.00378
Ordering as privileged information,Thomas Vacek,"We propose to accelerate the rate of convergence of the pattern recognition task by directly minimizing the variance diameters of certain hypothesis spaces, which are critical quantities in fast-convergence results.We show that the variance diameters can be controlled by dividing hypothesis spaces into metric balls based on a new order metric. This order metric can be minimized as an ordinal regression problem, leading to a LUPI (Learning Using Privileged Information) application where we take the privileged information as some desired ordering, and construct a faster-converging hypothesis space by empirically restricting some larger hypothesis space according to that ordering. We give a risk analysis of the approach. We discuss the difficulties with model selection and give an innovative technique for selecting multiple model parameters. Finally, we provide some data experiments. △ Less","30 June, 2016",https://arxiv.org/pdf/1606.09577
The Future of Computing Research: Industry-Academic Collaborations,Nady Boules;Khari Douglas;Stuart Feldman;Limor Fix;Gregory Hager;Brent Hailpern;Martial Hebert;Dan Lopresti;Beth Mynatt;Chris Rossbach;Helen Wright,"IT-driven innovation is an enormous factor in the worldwide economic leadership of the United States. It is larger than finance, construction, or transportation, and it employs nearly 6% of the US workforce. The top three companies, as measured by market capitalization, are IT companies - Apple, Google (now Alphabet), and Microsoft. Facebook, a relatively recent entry in the top 10 list by market capitalization has surpassed Walmart, the nation's largest retailer, and the largest employer in the world. The net income of just the top three exceeds $80 billion - roughly 100 times the total budget of the NSF CISE directorate which funds 87% of computing research. In short, the direct return on federal research investments in IT research has been enormously profitable to the nation. The IT industry ecosystem is also evolving. The time from conception to market of successful products has been cut from years to months. Product life cycles are increasingly a year or less. This change has pressured companies to focus industrial R&D on a pipeline or portfolio of technologies that bring immediate, or almost immediate, value to the companies. To defeat the competition and stay ahead of the pack, a company must devote resources to realizing gains that are shorter term, and must remain agile to respond quickly to market changes driven by new technologies, new startups, evolving user experience expectations, and the continuous consumer demand for new and exciting products. Amidst this landscape, the Computing Community Consortium convened a round-table of industry and academic participants to better understand the landscape of industry-academic interaction, and to discuss possible actions that might be taken to enhance those interactions. We close with some recommendations for actions that could expand the lively conversation we experienced at the round-table to a national scale. △ Less","29 June, 2016",https://arxiv.org/pdf/1606.09236
A Benes Based NoC Switching Architecture for Mixed Criticality Embedded Systems,Steve Kerrison;David May;Kerstin Eder,"Multi-core, Mixed Criticality Embedded (MCE) real-time systems require high timing precision and predictability to guarantee there will be no interference between tasks. These guarantees are necessary in application areas such as avionics and automotive, where task interference or missed deadlines could be catastrophic, and safety requirements are strict. In modern multi-core systems, the interconnect becomes a potential point of uncertainty, introducing major challenges in proving behaviour is always within specified constraints, limiting the means of growing system performance to add more tasks, or provide more computational resources to existing tasks. We present MCENoC, a Network-on-Chip (NoC) switching architecture that provides innovations to overcome this with predictable, formally verifiable timing behaviour that is consistent across the whole NoC. We show how the fundamental properties of Benes networks benefit MCE applications and meet our architecture requirements. Using SystemVerilog Assertions (SVA), formal properties are defined that aid the refinement of the specification of the design as well as enabling the implementation to be exhaustively formally verified. We demonstrate the performance of the design in terms of size, throughput and predictability, and discuss the application level considerations needed to exploit this architecture. △ Less","28 June, 2016",https://arxiv.org/pdf/1606.08686
"Simultaneous Mode, Input and State Estimation for Switched Linear Stochastic Systems",Sze Zheng Yong;Minghui Zhu;Emilio Frazzoli,"In this paper, we propose a filtering algorithm for simultaneously estimating the mode, input and state of hidden mode switched linear stochastic systems with unknown inputs. Using a multiple-model approach with a bank of linear input and state filters for each mode, our algorithm relies on the ability to find the most probable model as a mode estimate, which we show is possible with input and state filters by identifying a key property, that a particular residual signal we call generalized innovation is a Gaussian white noise. We also provide an asymptotic analysis for the proposed algorithm and provide sufficient conditions for asymptotically achieving convergence to the true model (consistency), or to the 'closest' model according to an information-theoretic measure (convergence). A simulation example of intention-aware vehicles at an intersection is given to demonstrate the effectiveness of our approach. △ Less","27 June, 2016",https://arxiv.org/pdf/1606.08323
Going Digital: A Survey on Digitalization and Large Scale Data Analytics in Healthcare,Volker Tresp;J. Marc Overhage;Markus Bundschus;Shahrooz Rabizadeh;Peter A. Fasching;Shipeng Yu,"We provide an overview of the recent trends towards digitalization and large scale data analytics in healthcare. It is expected that these trends are instrumental in the dramatic changes in the way healthcare will be organized in the future. We discuss the recent political initiatives designed to shift care delivery processes from paper to electronic, with the goals of more effective treatments with better outcomes; cost pressure is a major driver of innovation. We describe newly developed networks of healthcare providers, research organizations and commercial vendors to jointly analyze data for the development of decision support systems. We address the trend towards continuous healthcare where health is monitored by wearable and stationary devices; a related development is that patients increasingly assume responsibility for their own health data. Finally we discuss recent initiatives towards a personalized medicine, based on advances in molecular medicine, data management, and data analytics. △ Less","19 October, 2016",https://arxiv.org/pdf/1606.08075
Analytics-as-a-Service in a Multi-Cloud Environment through Semantically enabled Hierarchical Data Processing,Prem Prakash Jayaraman;Charith Perera;Dimitrios Georgakopoulos;Schahram Dustdar;Dhavalkumar Thakker;Rajiv Ranjan,"A large number of cloud middleware platforms and tools are deployed to support a variety of Internet of Things (IoT) data analytics tasks. It is a common practice that such cloud platforms are only used by its owners to achieve their primary and predefined objectives, where raw and processed data are only consumed by them. However, allowing third parties to access processed data to achieve their own objectives significantly increases integration, cooperation, and can also lead to innovative use of the data. Multicloud, privacy-aware environments facilitate such data access, allowing different parties to share processed data to reduce computation resource consumption collectively. However, there are interoperability issues in such environments that involve heterogeneous data and analytics-as-a-service providers. There is a lack of both - architectural blueprints that can support such diverse, multi-cloud environments, and corresponding empirical studies that show feasibility of such architectures. In this paper, we have outlined an innovative hierarchical data processing architecture that utilises semantics at all the levels of IoT stack in multicloud environments. We demonstrate the feasibility of such architecture by building a system based on this architecture using OpenIoT as a middleware, and Google Cloud and Microsoft Azure as cloud environments. The evaluation shows that the system is scalable and has no significant limitations or overheads. △ Less","25 June, 2016",https://arxiv.org/pdf/1606.07935
Harnessing the Power of the Crowd to Increase Capacity for Data Science in the Social Sector,Peter Bull;Isaac Slavitt;Greg Lipstein,"We present three case studies of organizations using a data science competition to answer a pressing question. The first is in education where a nonprofit that creates smart school budgets wanted to automatically tag budget line items. The second is in public health, where a low-cost, nonprofit women's health care provider wanted to understand the effect of demographic and behavioral questions on predicting which services a woman would need. The third and final example is in government innovation: using online restaurant reviews from Yelp, competitors built models to forecast which restaurants were most likely to have hygiene violations when visited by health inspectors. Finally, we reflect on the unique benefits of the open, public competition model. △ Less","24 June, 2016",https://arxiv.org/pdf/1606.07781
A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization,Lu Wang;Hema Raghavan;Vittorio Castelli;Radu Florian;Claire Cardie,"We consider the problem of using sentence compression techniques to facilitate query-focused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task. △ Less","23 June, 2016",https://arxiv.org/pdf/1606.07548
Adaptive Task Assignment in Online Learning Environments,Per-Arne Andersen;Christian Kråkevik;Morten Goodwin;Anis Yazidi,"With the increasing popularity of online learning, intelligent tutoring systems are regaining increased attention. In this paper, we introduce adaptive algorithms for personalized assignment of learning tasks to student so that to improve his performance in online learning environments. As main contribution of this paper, we propose a a novel Skill-Based Task Selector (SBTS) algorithm which is able to approximate a student's skill level based on his performance and consequently suggest adequate assignments. The SBTS is inspired by the class of multi-armed bandit algorithms. However, in contrast to standard multi-armed bandit approaches, the SBTS aims at acquiring two criteria related to student learning, namely: which topics should the student work on, and what level of difficulty should the task be. The SBTS centers on innovative reward and punishment schemes in a task and skill matrix based on the student behaviour. To verify the algorithm, the complex student behaviour is modelled using a neighbour node selection approach based on empirical estimations of a students learning curve. The algorithm is evaluated with a practical scenario from a basic java programming course. The SBTS is able to quickly and accurately adapt to the composite student competency --- even with a multitude of student models. △ Less","23 June, 2016",https://arxiv.org/pdf/1606.07233
Understanding Innovation to Drive Sustainable Development,Prasanna Sattigeri;Aurélie Lozano;Aleksandra Mojsilović;Kush R. Varshney;Mahmoud Naghshineh,"Innovation is among the key factors driving a country's economic and social growth. But what are the factors that make a country innovative? How do they differ across different parts of the world and different stages of development? In this work done in collaboration with the World Economic Forum (WEF), we analyze the scores obtained through executive opinion surveys that constitute the WEF's Global Competitiveness Index in conjunction with other country-level metrics and indicators to identify actionable levers of innovation. The findings can help country leaders and organizations shape the policies to drive developmental activities and increase the capacity of innovation. △ Less","15 June, 2016",https://arxiv.org/pdf/1606.06177
A Hybrid ICT-Solution for Smart Meter Data Analytics,Xiufeng Liu;Per Sieverts Nielsen,"Smart meters are increasingly used worldwide. Smart meters are the advanced meters capable of measuring energy consumption at a fine-grained time interval, e.g., every 15 minutes. Smart meter data are typically bundled with social economic data in analytics, such as meter geographic locations, weather conditions and user information, which makes the data sets very sizable and the analytics complex. Data mining and emerging cloud computing technologies make collecting, processing, and analyzing the so-called big data possible. This paper proposes an innovative ICT-solution to streamline smart meter data analytics. The proposed solution offers an information integration pipeline for ingesting data from smart meters, a scalable platform for processing and mining big data sets, and a web portal for visualizing analytics results. The implemented system has a hybrid architecture of using Spark or Hive for big data processing, and using the machine learning toolkit, MADlib, for doing in-database data analytics in PostgreSQL database. This paper evaluates the key technologies of the proposed ICT-solution, and the results show the effectiveness and efficiency of using the system for both batch and online analytics. △ Less","18 June, 2016",https://arxiv.org/pdf/1606.05787
DeepFood: Deep Learning-Based Food Image Recognition for Computer-Aided Dietary Assessment,Chang Liu;Yu Cao;Yan Luo;Guanling Chen;Vinod Vokkarane;Yunsheng Ma,"Worldwide, in 2014, more than 1.9 billion adults, 18 years and older, were overweight. Of these, over 600 million were obese. Accurately documenting dietary caloric intake is crucial to manage weight loss, but also presents challenges because most of the current methods for dietary assessment must rely on memory to recall foods eaten. The ultimate goal of our research is to develop computer-aided technical solutions to enhance and improve the accuracy of current measurements of dietary intake. Our proposed system in this paper aims to improve the accuracy of dietary assessment by analyzing the food images captured by mobile devices (e.g., smartphone). The key technique innovation in this paper is the deep learning-based food image recognition algorithms. Substantial research has demonstrated that digital imaging accurately estimates dietary intake in many environments and it has many advantages over other methods. However, how to derive the food information (e.g., food type and portion size) from food image effectively and efficiently remains a challenging and open research problem. We propose a new Convolutional Neural Network (CNN)-based food image recognition algorithm to address this problem. We applied our proposed approach to two real-world food image data sets (UEC-256 and Food-101) and achieved impressive results. To the best of our knowledge, these results outperformed all other reported work using these two data sets. Our experiments have demonstrated that the proposed approach is a promising solution for addressing the food image recognition problem. Our future work includes further improving the performance of the algorithms and integrating our system into a real-world mobile and cloud computing-based system to enhance the accuracy of current measurements of dietary intake. △ Less","17 June, 2016",https://arxiv.org/pdf/1606.05675
Effective information spreading based on local information in correlated networks,Lei Gao;Wei Wang;Liming Pan;Ming Tang;Hai-Feng Zhang,"Using network-based information to facilitate information spreading is an essential task for spreading dynamics in complex networks, which will benefit the promotion of technical innovations, healthy behaviors, new products, etc. Focusing on degree correlated networks, we propose a preferential contact strategy based on the local network structure and local informed density to promote the information spreading. During the spreading process, an informed node will preferentially select a contact target among its neighbors, basing on their degrees or local informed densities. By extensively implementing numerical simulations in synthetic and empirical networks, we find that when only consider the local structure information, the convergence time of information spreading will be remarkably reduced if low-degree neighbors are favored as contact targets. Meanwhile, the minimum convergence time depends non-monotonically on degree-degree correlation, and moderate correlation coefficients result in most efficient information spreading. Incorporating the informed density information into contact strategy, the convergence time of information spreading can be further reduced. Finally, we show that by using local informed density is more effective as compared with the global case. △ Less","16 June, 2016",https://arxiv.org/pdf/1606.05408
TwinCloud: Secure Cloud Sharing Without Explicit Key Management,Kemal Bicakci;Davut Deniz Yavuz;Sezin Gurkan,"With the advent of cloud technologies, there is a growing number of easy-to-use services to store files and share them with other cloud users. By providing security features, cloud service providers try to encourage users to store personal files or corporate documents on their servers. However, their server-side encryption solutions are not satisfactory when the server itself is not trusted. Although, there are several client-side solutions to provide security for cloud sharing, they are not used extensively because of usability issues in key management. In this paper, we propose TwinCloud which is an innovative solution with the goal of providing a secure system to users without compromising the usability of cloud sharing. TwinCloud achieves this by bringing a novel solution to the complex key exchange problem and by providing a simple and practical approach to store and share files by hiding all the cryptographic and key-distribution operations from users. Serving as a gateway, TwinCloud uses two or more cloud providers to store the encryption keys and encrypted files in separate clouds which ease the secure sharing without a need for trust to either of the cloud service providers with the assumption that they do not collude with each other. We implemented TwinCloud as a lightweight application and make it available as open-source. The results of our usability study show the prospect of the secure sharing solution of TwinCloud. △ Less","13 July, 2016",https://arxiv.org/pdf/1606.04705
Secure Compute-and-Forward Transmission With Artificial Noise and Full-Duplex Devices,Stefano Tomasin,"We consider a wiretap channel with an eavesdropper (Eve) and an honest but curious relay (Ray). Ray and the destination (Bob) are full-duplex (FD) devices. Since we aim at not revealing information on the secret message to the relay, we consider the scaled compute-and-forward (SCF) where scaled lattice coding is used in the transmission by both the source (Alice) and Bob in order to allow Ray to decode only a linear combination of the two messages. At the same time Ray transmits artificial noise (AN) to confuse Eve. When Ray relays the decoded linear combination, Alice and Bob are transmitting AN against Eve. This can be a 5G cellular communication scenario where a mobile terminal (MT) aims at transmitting a secret message to a FD base station (BS), with the assistance of a network FD relay. With respect to existing literature the innovations of this paper are: a) Bob and Ray are FD devices; b) Alice, Ray and Bob transmit also AN; and c) the channel to Eve is not known to Alice, Bob and Ray. For this scenario we derive bounds on both the secrecy outage probability under Rayleigh fading conditions of the channels to Eve, and the achievable secrecy-outage rates. △ Less","14 June, 2016",https://arxiv.org/pdf/1606.04458
"Engaging Learning Analytics in MOOCS: the good, the bad, and the ugly",Mohammad Khalil;Behnam Taraghi;Martin Ebner,"Learning Analytics is an emerging field in the vast areas of Educational Technology and Technology Enhanced Learning (TEL). It provides tools and techniques that offer researchers the ability to analyze, study, and benchmark institutions, learners and teachers as well as online learning environments such as MOOCs. Massive Open Online Courses (MOOCs) are considered to be a very active and an innovative form of bringing educational content to a broad community. Due to the reasons of being free and accessible to the public, MOOCs attracted a large number of heterogeneous learners who differ in education level, gender, and age. However, there are pressing demands to adjust the quality of the hosted courses, as well as controlling the high dropout ratio and the lack of interaction. With the help of Learning Analytics, it is possible to contain such issues. In this publication, we discuss the principles of engaging Learning Analytics in MOOCs learning environments and review its potential and capabilities (the good), constraints (the bad), and fallacy analytics (the ugly) based on our experience in last years. △ Less","12 June, 2016",https://arxiv.org/pdf/1606.03776
An SME's Adoption of a Cloud Based Integrated Management System (IMS) When Certifying Against Management System Standards (MSS),Ming Hock Yew;Jenson Goh,"This case study introduces a four step approach used by a Singapore small and medium enterprise (SME) in implementing a cloud computing based integrated management system (IMS) to meet the ISO 9001, ISO 14001, and OHSAS 18001 certification requirements. The objectives of this case study are to study: (1) the challenges encountered by an SME during the IMS integration process at each of the four levels of integration (2) the extent which Frugal IT Innovation and Technology Acceptance Model (TAM3) concepts apply to our four step approach. The four step approach was assessed against a framework of four integration levels of the management systems (MS), and also the applicability of two IS theories (Frugal IT Innovation and TAM3). Data was collected via: (1) direct observations (2) participant observations; (3) interviews with key personnel involved in the project; and (4) analysis of documents pertaining to the project. This case study provides an exemplary model on using IS theories and public cloud technologies to develop an effective IMS that other SMEs can learn from. △ Less","10 June, 2016",https://arxiv.org/pdf/1606.03546
Firm Growth and Innovation in the ERP Industry: A Systems Thinking Approach,Srujana Pinjala;Rahul Roy;Priya Seetharaman,"Achievement and sustenance of growth are essential themes in organizational literature. In our paper, we develop models using systems thinking approach to understand how firms achieve and sustain growth in a technology-intensive product domain. We augment these to explain the possible impact of a disruptive technological innovation. We use enterprise software industry as the context where SAP has been acknowledged as the market leader. We find that product differentiation and learning effects helped SAP establish itself, and this growth was further sustained through networks and complementors. Introducing cloud computing as the disruptive innovation, we explain its impact on a firm. Analysis reveals that for the next wave of growth to occur, and to tap into newer markets, it would be imperative for SAP to create attractive cloud based offerings. We also discuss how the model can be enhanced by considering competitor dynamics. △ Less","10 June, 2016",https://arxiv.org/pdf/1606.03539
IT Managers' Perception and Response to Digital Disruption: An Exploratory Study,Alemayehu Molla;Vanessa Cooper;Vass Karpathiou,"Digital disruption introduces technology-enabled changes at a pace and scale that fundamentally shifts established ways in which we live and work. Given the impact of digital disruption can span the individual, organisational, industry and societal levels, it has received growing attention in Information Systems. A less researched area of digital disruption, however, is the perspective of IT managers in organisations. This paper draws on i) situational awareness theory to identify IT managers' perceptions of digital disruption, and ii) disruptive innovation theory to identify IT managers' responses to digital disruption. A case study of senior IT managers in an Australian university identifies that IT managers perceive digital disruption from the technological, learning and sense-making perspectives. IT managers' first-order responses to digital disruption were shown to address resource, process and value-based issues while second-order responses focused on the need to build dynamic capabilities in order to be prepared for future digital disruption. △ Less","10 June, 2016",https://arxiv.org/pdf/1606.03534
Green Information Technology as Administrative innovation - Organizational factors for successful implementation: Literature Review,Badrunnesa Zaman;Darshana Sedera,"There is a considerable amount of awareness of environmental issues and corporate responsibility for sustainability. As such, from a technological viewpoint, Green IT has become an important topic in contemporary organizations. Consequently, organisations are expected to be innovative in their business practices to become more sustainable. Yet, the popularity and adoption of such initiatives amongst employees remain low. Furthermore, the management practices for adhering to Green IT are largely dormant, lacking active incentives for employees to engage in Green IT initiatives. This study observes the phenomenon of Green IT through administrative innovation. In doing so this paper performs a comprehensive analysis of 137 papers published between 2007 and 2015. The paper reveals organizational factors for successful implementation of Green IT as administrative innovation that can be useful to both academia and practice. △ Less","10 June, 2016",https://arxiv.org/pdf/1606.03503
A Generalized Bass Model for Product Growth in Networks,Vahideh H. Manshadi;Sidhant Misra,"Many products and innovations become well-known and widely adopted through the social interactions of individuals in a population. The Bass diffusion model has been widely used to model the temporal evolution of adoption in such social systems. In the model, the likelihood of a new adoption is proportional to the number of previous adopters, implicitly assuming a global (or homogeneous) interaction among all individuals in the network. Such global interactions do not exist in many large social networks, however. Instead, individuals typically interact with a small part of the larger population. To quantify the growth rate (or equivalently the adoption timing) in networks with limited interactions, we study a stochastic adoption process where the likelihood that each individual adopts is proportional to the number of adopters among the small group of persons he/she interacts with (and not the entire population of adopters). When the underlying network of interactions is a random k-regular graph, we compute the sample path limit of the fraction of adopters. We show the limit coincides with the solution of a differential equation which can viewed as a generalization of the Bass diffusion model. When the degree k is bounded, we show the adoption curve differs significantly from the one corresponds to the Bass diffusion model. In particular, the adoption grows more slowly than what the Bass model projects. In addition, the adoption curve is asymmetric, unlike that of the Bass diffusion model. Such asymmetry has important consequences for the estimation of market potential. Finally, we calculate the timing of early adoptions at finer scales, e.g., logarithmic in the population size. △ Less","10 June, 2016",https://arxiv.org/pdf/1606.03386
IDNet: Smartphone-based Gait Recognition with Convolutional Neural Networks,Matteo Gadaleta;Michele Rossi,"Here, we present IDNet, a user authentication framework from smartphone-acquired motion signals. Its goal is to recognize a target user from their way of walking, using the accelerometer and gyroscope (inertial) signals provided by a commercial smartphone worn in the front pocket of the user's trousers. IDNet features several innovations including: i) a robust and smartphone-orientation-independent walking cycle extraction block, ii) a novel feature extractor based on convolutional neural networks, iii) a one-class support vector machine to classify walking cycles, and the coherent integration of these into iv) a multi-stage authentication technique. IDNet is the first system that exploits a deep learning approach as universal feature extractors for gait recognition, and that combines classification results from subsequent walking cycles into a multi-stage decision making framework. Experimental results show the superiority of our approach against state-of-the-art techniques, leading to misclassification rates (either false negatives or positives) smaller than 0.15% with fewer than five walking cycles. Design choices are discussed and motivated throughout, assessing their impact on the user authentication performance. △ Less","19 October, 2016",https://arxiv.org/pdf/1606.03238
Network as a Service: The New Vista of Opportunities,Junaid Qadir;Nadeem Ahmed;Faqir Zarrar Yousaf;Ali Taqweem,"The networking industry, compared to the compute industry, has been slow in evolving from a closed ecosystem with limited abstractions to a more open ecosystem with well-defined sophisticated high level abstractions. This has resulted in an ossified Internet architecture that inhibits innovation and is unnecessarily complex. Fortunately, there has been an exciting flux of rapid developments in networking in recent times with prominent trends emerging that have brought us to the cusp of a major paradigm shift. In particular, the emergence of technologies such as cloud computing, software defined networking (SDN), and network virtualization are driving a new vision of `networking as a service' (NaaS) in which networks are managed flexibly and efficiently cloud computing style. These technologies promise to both facilitate architectural and technological innovation while also simplifying commissioning, orchestration, and composition of network services. In this article, we introduce our readers to these technologies. In the coming few years, the trends of cloud computing, SDN, and network virtualization will further strengthen each other's value proposition symbiotically and NaaS will increasingly become the dominant mode of commissioning new networks. △ Less","10 June, 2016",https://arxiv.org/pdf/1606.03060
A Framework for Techniques for Information Technology Enabled Innovation,Yajur Chadha;Aditi Mehra;Shirley Gregor;Alex Richardson,"Australia is seen as lagging in the innovation that is needed for corporate success and national productivity gains. There is an apparent lack of consistent and integrated advice to managers on how to undertake innovation. Thus, this study aims to develop and investigate a framework that relates innovation practices to the type of innovation outcome, in the context of Information Technology (IT) enabled innovations. An Innovation Practice Framework was developed based on the Knowledge-Innovation Matrix (KIM) proposed by Gregor and Hevner (2015). Eleven commonly used innovation techniques (practices) were identified and placed in one or more of the quadrants: invention, advancement, exaptation and exploitation. Interviews were conducted with key informants in nine organisations in the Australian Capital Territory. Results showed that the least used techniques were skunk works and crowdsourcing. The most used techniques were traditional market research, brainstorming and design thinking. The Innovation Practice Framework was given some support, with genius grants being related to invention outcomes, design thinking with exaptation, traditional R&D with advancement and managerial scanning with exploitation. The study contributes theoretically with the new Innovation Practice Framework and has the potential to be useful to managers in showing how benefits can be gained from a range of innovation practices. Further work is in progress. △ Less","8 June, 2016",https://arxiv.org/pdf/1606.02480
The Potential Impact of Digital Currencies on the Australian Economy,Mustafa Ally;Michael Gardiner;Michael Lane,"Crypto-currencies like Bitcoins are relatively recent phenomena on the online Internet landscape and an emerging force in the financial sector. While not conforming to traditional institutional practices, they are gaining increasing acceptance as viable commercial currencies. In this conceptual paper we discuss the potential impact of digital currency technology on the Australian economy, including the (i) payments sector, (ii) retail sector, and (iii) banking sector; and explore potential ways in which Australia can take advantage of digital currency technology to establish itself as a market leader in this field. The emergence of this new and potentially disruptive technology provides both opportunities as well as risks. In order to support innovation and the needs of the growing Australian digital currency industry it is important to define digital currencies and examine the impact regulatory frameworks could have on the further adoption and diffusion of the technology. △ Less","8 June, 2016",https://arxiv.org/pdf/1606.02462
Open-source Hardware: Opportunities and Challenges,Gagan Gupta;Tony Nowatzki;Vinay Gangadhar;Karthikeyan Sankaralingam,"Innovation in hardware is slowing due to rising costs of chip design and diminishing benefits from Moore's law and Dennard scaling. Software innovation, on the other hand, is flourishing, helped in good measure by a thriving open-source ecosystem. We believe that open source can similarly help hardware innovation, but has not yet due to several reasons. We identify these reasons and how the industry, academia, and the hardware community at large can come together to address them. △ Less","11 June, 2016",https://arxiv.org/pdf/1606.01980
Content-based Cognitive Interference Control for City Monitoring Applications in the Urban IoT,Sabur Baidya;Marco Levorato,"In the Urban Internet of Things devices and systems are interconnected at the city scale to provide innovative services to the citizens.However, the traffic generated by the sensing and processing systems may overload local access networks. A coexistence problem arises where concurrent applications mutually interfere and compete for available resources. This effect is further aggravated by the multiple scales involved and heterogeneity of the networks supporting the urban IoT. One of the main contributions of this paper is the introduction of the notion of content-oriented cognitive interference control in heterogeneous local access networks supporting computing and data processing in the urban IoT. A network scenario where multiple communication technologies, such as Device-to-Device and Long Term Evolution (LTE), is considered. The focus of the present paper is on city monitoring applications, where a video data stream generated by a camera system is remotely processed to detect objects. The cognitive network paradigm is extended to dynamically shape the interference pattern generated by concurrent data streams and induce a packet loss trajectory compatible with video processing algorithms. Numerical results show that the proposed cognitive transmission strategy enables a significant throughput increase of interfering applications for a target accuracy of the monitoring application. △ Less","6 June, 2016",https://arxiv.org/pdf/1606.01965
Optimizing Point-to-Multipoint Transmissions in High Speed Packet Access Networks,G. Araniti;V. Scordamaglia;A. Molinaro;A. Iera;G. Interdonato;F. Spanò,"In this paper an innovative Radio Resource Management (RRM) algorithm is proposed with the purpose of increasing High Speed Packet Access (HSPA) performances, in terms of system capacity and service quality, when the Multimedia Broadcast Multicast Services (MBMS) is supplied. The proposed RRM algorithm exploits channel quality indications to set up point-to-multipoint connections to subgroups of multicast users and to select the proper modulation and coding schemes on the downlink. The number of subgroups is determined through an optimization technique that also takes into account the user satisfaction. An exhaustive simulation campaign is conducted to compare the proposed algorithm with the most promising approaches in the literature. Comparisons aim to assess the capability of the proposed RRM algorithm to efficiently manage group oriented services by providing an increment in terms of user satisfaction. △ Less","6 June, 2016",https://arxiv.org/pdf/1606.01943
Open Food Network: the Role of ICT to Support Regional Food Supply Chains in Australia,Sherah Kurnia;Serenity Hill;Md Mahbubur Rahim;Kirsten Larsen;Patrice Braun;Danny Samson,"Many organizations have introduced various ICT-enabled innovations to improve economic, environmental and social performance. The Open Food Network (OFN) is an example of ICT-enabled innovation that has the potential to enhance the sustainability of regional food supply chain by improving farmers access to local and regional markets and consumers access to fresh local produce, as well as optimizing the regional food distribution and improving local community welfare. OFN has just been recently launched in Australia and currently there is a limited understanding of the actual impacts and appropriate strategies to encourage wider adoption. This research in progress aims to evaluate the effectiveness of the OFN system in connecting and supporting the sustainability of regional food supply chain communities in Australia that will help devise strategies for expanding the use beyond Australia. Through a preliminary focus group with a number of early adopters of the OFN, this study identifies the actual use of OFN, benefits to main stakeholders and a number of challenges. The findings contribute to a longer term research program that investigates how ICT can support sustainability initiatives within organizations and supply chains. △ Less","4 June, 2016",https://arxiv.org/pdf/1606.01456
Understanding Knowledge Leakage & BYOD (Bring Your Own Device): A Mobile Worker Perspective,Carlos Andres Agudelo;Rachelle Bosua;Atif Ahmad;Sean B. Maynard,"Knowledge sharing drives innovation and the opportunity to develop a sustainable competitive advantage. However, in the extant knowledge management and information security literature, leakage from sharing activities is neglected. The risk of knowledge leakage is exacerbated with the pervasive use of mobile devices and the adoption of BYOD (Bring Your Own Device). Thus, this research-in-progress paper examines the role of the behavior of mobile workers that engage in accidental knowledge leakage through the use of BYOD. We use the Decomposed Theory of Planned Behavior (DTPB) to explain the causes behind this phenomenon and how it negatively impacts organization's competitive advantage. The contributions of this study are the following. First, it posits that the reasons of knowledge leakage by mobile workers through BYOD can be explained using DTPB. Second, the paper proposes a conceptual model for research based on DTPB constructs whilst adding other variables such as BYOD and mobile device usage context. Finally, the conceptual study outlines the potential contributions and implications of this research. △ Less","4 June, 2016",https://arxiv.org/pdf/1606.01450
Information Technology Platforms: Definition and Research Directions,Ruonan Sun;Shirley Gregor;Byron Keating,"The concept of an information technology (IT) related platform is broad and covers phenomena ranging from the operating system Linux to the Internet. Such platforms are of increasing importance to innovation and value creation across many facets of industry and daily life. There is, however, a lack of common understanding in both research and industry about what is mean by the term platform when related to IT. This lack of consensus is detrimental to research and knowledge development. Thus, the aims of this study are to: (i) provide a sound definition of the IT-platform concept by identifying its distinguishing dimensions; and (ii) identify important current research directions for the IT-platform concept. To achieve these aims a systematic literature review was undertaken with 133 relevant articles taken from major information systems journals, conferences, and business publications. The study contributes by providing a sound base for future research into IT-platforms. △ Less","4 June, 2016",https://arxiv.org/pdf/1606.01445
Visualizing the invisible the relentless pursuit of MTech Imaging,Jenson Goh;Jeffery Beng-Huat Tan;Janice Sio-Nee Tan,"This teaching case describes the challenges faced by MTech Imaging, a Singapore small and medium enterprise (SME) that specializes in providing thermal imaging solutions. In recent years, the company has relentlessly strived to become a digital innovative solution provider. This push has led to the development of a disruptive digital innovation called the AXION platform. Students are provided with vivid accounts of the journey undertaken by MTech to develop the AXION platform, the industry it competes in, and the challenges it faces in attempting to disrupt its industry through the introduction of the AXION platform. The case seeks to achieve three learning objectives: (1) allow students to learn from MTechs experiences in developing disruptive digital innovation; (2) immerse students as senior management of MTech to substantiate the best digital innovation strategy to adopt in order to disrupt its industry; and (3) expose students to the challenges of driving the adoption of such digital innovation in the market. It is hoped that the case can inspire students to become effective digital innovation entrepreneurs. △ Less","4 June, 2016",https://arxiv.org/pdf/1606.01430
Using Collaborative Visual Analytics for Innovative Industry-inspired Learning Activities,Olivera Marjanovic,"Inspired by the leading industry practices, this paper describes an innovative learning activity that combines data visualization and collaboration structured around sharing, co-creation and negotiation of departmental/disciplinary insights across data silos, using both internal and external data. This activity was designed within a much larger project aiming to translate practitioner stories and experiences in using visual analytics into educational scenarios and innovative industry-informed learning activities for students. In addition to giving students access to state-of-the-art tools for visualization (SAS-VA) and collaboration (Yammer), an even more important educational objective is to expose students to current industry practices with individual data-driven disciplinary insights no longer considered to sufficient when dealing with complex multi-disciplinary challenges. Starting from a data-informed business scenario, the paper describes the main steps of our innovative data visualisation and collaboration activity, discusses possible alternative software platforms and offers some ideas for the future work. △ Less","4 June, 2016",https://arxiv.org/pdf/1606.01427
A Multi Perspective Approach for Understanding the Determinants of Cloud Computing Adoption among Australian SMEs,Salim Alismaili;Mengxiang Li;Jun Shen;Qiang He,"Cloud computing is proved to be an effective computing technology for organisations through the advantages that it offers such as cost-effectiveness, IT technical agility and scalability, enhancing businesses processes, and increasing enterprises competitiveness. In Australia, there is an emerging trend that small and medium-sized enterprises (SMEs) begin to adopt this technology in the conventional working practices. However, there is a dearth of prior studies on examining the factors that influence the cloud computing adoption among Australian SMEs. To fill the empirical vacuum, this research-in-progress proposes an integrated framework for examining the determinants of cloud computing service adoption with the consideration of the unique characteristics of Australian SMEs, such as relatively low adoption of cloud computing services, less innovative, and limited knowledge about cloud computing and its benefits and hindrances. To this end, we are conducting consecutive studies to investigate this research issue. An exploratory interview study will be undertaken to identify and verify the unique characteristics of Australian SMEs toward the cloud computing adoption. This is followed by an organisational level survey that examines the effects of those determinants on cloud computing adoption. Finally, a decision model for cloud computing adoption among Australian SMEs will be developed by using a Multi Criteria Decision Approach (MCDA) through rating, prioritising, and ranking of various criteria and alternatives available to the decision makers. Adopting the mixed-method research fashion, this research-in-progress intends to make significant implications to scholars and practitioners alike in the cloud computing research and applications areas. △ Less","28 May, 2016",https://arxiv.org/pdf/1606.00745
Decoding Emotional Experience through Physiological Signal Processing,Maria S. Perez-Rosero;Behnaz Rezaei;Murat Akcakaya;Sarah Ostadabbas,"There is an increasing consensus among re- searchers that making a computer emotionally intelligent with the ability to decode human affective states would allow a more meaningful and natural way of human-computer interactions (HCIs). One unobtrusive and non-invasive way of recognizing human affective states entails the exploration of how physiological signals vary under different emotional experiences. In particular, this paper explores the correlation between autonomically-mediated changes in multimodal body signals and discrete emotional states. In order to fully exploit the information in each modality, we have provided an innovative classification approach for three specific physiological signals including Electromyogram (EMG), Blood Volume Pressure (BVP) and Galvanic Skin Response (GSR). These signals are analyzed as inputs to an emotion recognition paradigm based on fusion of a series of weak learners. Our proposed classification approach showed 88.1% recognition accuracy, which outperformed the conventional Support Vector Machine (SVM) classifier with 17% accuracy improvement. Furthermore, in order to avoid information redundancy and the resultant over-fitting, a feature reduction method is proposed based on a correlation analysis to optimize the number of features required for training and validating each weak learner. Results showed that despite the feature space dimensionality reduction from 27 to 18 features, our methodology preserved the recognition accuracy of about 85.0%. This reduction in complexity will get us one step closer towards embedding this human emotion encoder in the wireless and wearable HCI platforms. △ Less","1 June, 2016",https://arxiv.org/pdf/1606.00370
RCFD: A Frequency Based Channel Access Scheme for Full Duplex Wireless Networks,Michele Luvisotto;Alireza Sadeghi;Farshad Lahouti;Stefano Vitturi;Michele Zorzi,"Recently, several working implementations of in--band full--duplex wireless systems have been presented, where the same node can transmit and receive simultaneously in the same frequency band. The introduction of such a possibility at the physical layer could lead to improved performance but also poses several challenges at the MAC layer. In this paper, an innovative mechanism of channel contention in full--duplex OFDM wireless networks is proposed. This strategy is able to ensure efficient transmission scheduling with the result of avoiding collisions and effectively exploiting full--duplex opportunities. As a consequence, considerable performance improvements are observed with respect to standard and state--of--the--art MAC protocols for wireless networks, as highlighted by extensive simulations performed in ad hoc wireless networks with varying number of nodes. △ Less","31 May, 2016",https://arxiv.org/pdf/1605.09716
Predicting System-level Power for a Hybrid Supercomputer,Alina Sîrbu;Ozalp Babaoglu,"For current High Performance Computing systems to scale towards the holy grail of ExaFLOP performance, their power consumption has to be reduced by at least one order of magnitude. This goal can be achieved only through a combination of hardware and software advances. Being able to model and accurately predict the power consumption of large computational systems is necessary for software-level innovations such as proactive and power-aware scheduling, resource allocation and fault tolerance techniques. In this paper we present a 2-layer model of power consumption for a hybrid supercomputer (which held the top spot of the Green500 list on July 2013) that combines CPU, GPU and MIC technologies to achieve higher energy efficiency. Our model takes as input workload information - the number and location of resources that are used by each job at a certain time - and calculates the resulting system-level power consumption. When jobs are submitted to the system, the workload configuration can be foreseen based on the scheduler policies, and our model can then be applied to predict the ensuing system-level power consumption. Additionally, alternative workload configurations can be evaluated from a power perspective and more efficient ones can be selected. Applications of the model include not only power-aware scheduling but also prediction of anomalous behavior. △ Less","31 May, 2016",https://arxiv.org/pdf/1605.09530
Census Tract License Areas: Disincentive for Sharing the 3.5GHz band?,Elma Avdic;Irene Macaluso;Nicola Marchetti;Linda Doyle,"Flexible licensing model is a necessary enabler of the technical and procedural complexities of Spectrum Access System (SAS)-based sharing framework. The purpose of this study is to explore the effectiveness of 3.5GHz Licensing Framework - based on census tracts as area units, areas whose main characteristic is population. As such, the boundary of census tract does not follow the edge of wireless network coverage. We demonstrate why census tracts are not suitable for small cell networks licensing, by (1) gathering and analysing the official census data, (2) exploring the boundaries of census tracts which are in the shape of nonconvex polygons and (3) giving a measure of effectiveness of the licensing scheme through metrics of area loss and the number of people per census tract with access to spectrum. Results show that census tracts severely impact the effectiveness of the licensing framework since almost entire strategically important cities in the U.S. will not avail from spectrum use in 3.5GHz band. Our paper does not seek to challenge the core notion of geographic licensing concept, but seeks a corrective that addresses the way the license is issued for a certain area of operation. The effects that inappropriate size of the license has on spectrum assignments lead to spectrum being simply wasted in geography, time and frequency or not being assigned in a fair manner. The corrective is necessary since the main goal of promoting innovative sharing in 3.5 GHz band is to put spectrum to more efficient use. △ Less","30 May, 2016",https://arxiv.org/pdf/1605.09184
An Overview of End-to-End Verifiable Voting Systems,Syed Taha Ali;Judy Murray,"Advances in E2E verifiable voting have the potential to fundamentally restore trust in elections and democratic processes in society. In this chapter, we provide a comprehensive introduction to the field. We trace the evolution of privacy and verifiability properties in the research literature and describe the operations of current state-of-the-art E2E voting systems. We also discuss outstanding challenges to the deployment of E2E voting systems, including technical, legal, and usability constraints. Our intention, in writing this chapter, has been to make the innovations in this domain accessible to a wider audience. We have therefore eschewed description of complex cryptographic mechanisms and instead attempt to communicate the fundamental intuition behind the design of E2E voting systems. We hope our work serves as a useful resource and assists in the future development of E2E voting. △ Less","27 May, 2016",https://arxiv.org/pdf/1605.08554
Journeys & Notes: Designing Social Computing for Non-Places,Justin Cranshaw;Andrés Monroy-Hernández;S. A. Needham,"In this work we present a mobile application we designed and engineered to enable people to log their travels near and far, leave notes behind, and build a community around spaces in between destinations. Our design explores new ground for location-based social computing systems, identifying opportunities where these systems can foster the growth of on-line communities rooted at non-places. In our work we develop, explore, and evaluate several innovative features designed around four usage scenarios: daily commuting, long-distance traveling, quantified traveling, and journaling. We present the results of two small-scale user studies, and one large-scale, world-wide deployment, synthesizing the results as potential opportunities and lessons learned in designing social computing for non-places. △ Less","27 May, 2016",https://arxiv.org/pdf/1605.08548
Steganalysis via a Convolutional Neural Network using Large Convolution Filters for Embedding Process with Same Stego Key,Jean-François Couchot;Raphaël Couturier;Christophe Guyeux;Michel Salomon,"For the past few years, in the race between image steganography and steganalysis, deep learning has emerged as a very promising alternative to steganalyzer approaches based on rich image models combined with ensemble classifiers. A key knowledge of image steganalyzer, which combines relevant image features and innovative classification procedures, can be deduced by a deep learning approach called Convolutional Neural Networks (CNN). These kind of deep learning networks is so well-suited for classification tasks based on the detection of variations in 2D shapes that it is the state-of-the-art in many image recognition problems. In this article, we design a CNN-based steganalyzer for images obtained by applying steganography with a unique embedding key. This one is quite different from the previous study of {\em Qian et al.} and its successor, namely {\em Pibre et al.} The proposed architecture embeds less convolutions, with much larger filters in the final convolutional layer, and is more general: it is able to deal with larger images and lower payloads. For the ""same embedding key"" scenario, our proposal outperforms all other steganalyzers, in particular the existing CNN-based ones, and defeats many state-of-the-art image steganography schemes. △ Less","30 July, 2016",https://arxiv.org/pdf/1605.07946
DP-EM: Differentially Private Expectation Maximization,Mijung Park;Jimmy Foulds;Kamalika Chaudhuri;Max Welling,"The iterative nature of the expectation maximization (EM) algorithm presents a challenge for privacy-preserving estimation, as each iteration increases the amount of noise needed. We propose a practical private EM algorithm that overcomes this challenge using two innovations: (1) a novel moment perturbation formulation for differentially private EM (DP-EM), and (2) the use of two recently developed composition methods to bound the privacy ""cost"" of multiple EM iterations: the moments accountant (MA) and zero-mean concentrated differential privacy (zCDP). Both MA and zCDP bound the moment generating function of the privacy loss random variable and achieve a refined tail bound, which effectively decrease the amount of additive noise. We present empirical results showing the benefits of our approach, as well as similar performance between these two composition methods in the DP-EM setting for Gaussian mixture models. Our approach can be readily extended to many iterative learning algorithms, opening up various exciting future directions. △ Less","31 October, 2016",https://arxiv.org/pdf/1605.06995
Learning to Communicate with Deep Multi-Agent Reinforcement Learning,Jakob N. Foerster;Yannis M. Assael;Nando de Freitas;Shimon Whiteson,"We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate end-to-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains. △ Less","24 May, 2016",https://arxiv.org/pdf/1605.06676
The Bass diffusion model on networks with correlations and inhomogeneous advertising,M. L. Bertotti;J. Brunner;G. Modanese,"The Bass model, which is an effective forecasting tool for innovation diffusion based on large collections of empirical data, assumes an homogeneous diffusion process. We introduce a network structure into this model and we investigate numerically the dynamics in the case of networks with link density P(k)=c/k^γ, where k=1, \ldots , N. The resulting curve of the total adoptions in time is qualitatively similar to the homogeneous Bass curve corresponding to a case with the same average number of connections. The peak of the adoptions, however, tends to occur earlier, particularly when γ and N are large (i.e., when there are few hubs with a large maximum number of connections). Most interestingly, the adoption curve of the hubs anticipates the total adoption curve in a predictable way, with peak times which can be, for instance when N=100, between 10% and 60% of the total adoptions peak. This may allow to monitor the hubs for forecasting purposes. We also consider the case of networks with assortative and disassortative correlations and a case of inhomogeneous advertising where the publicity terms are ""targeted"" on the hubs while maintaining their total cost constant. △ Less","19 May, 2016",https://arxiv.org/pdf/1605.06308
Consensus+Innovations Distributed Kalman Filter with Optimized Gains,Subhro Das;José M. F. Moura,"In this paper, we address the distributed filtering and prediction of time-varying random fields represented by linear time-invariant (LTI) dynamical systems. The field is observed by a sparsely connected network of agents/sensors collaborating among themselves. We develop a Kalman filter type consensus+innovations distributed linear estimator of the dynamic field termed as Consensus+Innovations Kalman Filter. We analyze the convergence properties of this distributed estimator. We prove that the mean-squared error of the estimator asymptotically converges if the degree of instability of the field dynamics is within a pre-specified threshold defined as tracking capacity of the estimator. The tracking capacity is a function of the local observation models and the agent communication network. We design the optimal consensus and innovation gain matrices yielding distributed estimates with minimized mean-squared error. Through numerical evaluations, we show that, the distributed estimator with optimal gains converges faster and with approximately 3dB better mean-squared error performance than previous distributed estimators. △ Less","13 October, 2016",https://arxiv.org/pdf/1605.06096
SONATA: Service Programming and Orchestration for Virtualized Software Networks,Sevil Dräxler;Manuel Peuster;Holger Karl;Michael Bredel;Johannes Lessmann;Thomas Soenen;Wouter Tavernier;Sharon Mendel-Brin;George Xilouris,"In conventional large-scale networks, creation and management of network services are costly and complex tasks that often consume a lot of resources, including time and manpower. Network softwarization and network function virtualization have been introduced to tackle these problems. They replace the hardware-based network service components and network control mechanisms with software components running on general-purpose hardware, aiming at decreasing costs and complexity of implementing new services, maintaining the implemented services, and managing available resources in service provisioning platforms and underlying infrastructures. To experience the full potential of these approaches, innovative development support tools and service provisioning environments are needed. To answer these needs, we introduce the SONATA architecture, a service programming, orchestration, and management framework. We present a development toolchain for virtualized network services, fully integrated with a service platform and orchestration system. We motivate the modular and flexible architecture of our system and discuss its main components and features, such as function- and service-specific managers that allow fine- grained service management, slicing support to facilitate multi-tenancy, recursiveness for improved scalability, and full-featured DevOps support. △ Less","19 May, 2016",https://arxiv.org/pdf/1605.05850
CSA++: Fast Pattern Search for Large Alphabets,Simon Gog;Alistair Moffat;Matthias Petri,"Indexed pattern search in text has been studied for many decades. For small alphabets, the FM-Index provides unmatched performance, in terms of both space required and search speed. For large alphabets -- for example, when the tokens are words -- the situation is more complex, and FM-Index representations are compact, but potentially slow. In this paper we apply recent innovations from the field of inverted indexing and document retrieval to compressed pattern search, including for alphabets into the millions. Commencing with the practical compressed suffix array structure developed by Sadakane, we show that the Elias-Fano code-based approach to document indexing can be adapted to provide new tradeoff options in indexed pattern search, and offers significantly faster pattern processing compared to previous implementations, as well as reduced space requirements. We report a detailed experimental evaluation that demonstrates the relative advantages of the new approach, using the standard Pizza&Chili methodology and files, as well as applied use-cases derived from large-scale data compression, and from natural language processing. For large alphabets, the new structure gives rise to space requirements that are close to those of the most highly-compressed FM-Index variants, in conjunction with unparalleled search throughput rates. △ Less","17 May, 2016",https://arxiv.org/pdf/1605.05404
A Biased Review of Biases in Twitter Studies on Political Collective Action,Peter Cihon;Taha Yasseri,"In recent years researchers have gravitated to social media platforms, especially Twitter, as fertile ground for empirical analysis of social phenomena. Social media provides researchers access to trace data of interactions and discourse that once went unrecorded in the offline world. Researchers have sought to use these data to explain social phenomena both particular to social media and applicable to the broader social world. This paper offers a minireview of Twitter-based research on political crowd behavior. This literature offers insight into particular social phenomena on Twitter, but often fails to use standardized methods that permit interpretation beyond individual studies. Moreover, the literature fails to ground methodologies and results in social or political theory, divorcing empirical research from the theory needed to interpret it. Rather, papers focus primarily on methodological innovations for social media analyses, but these too often fail to sufficiently demonstrate the validity of such methodologies. This minireview considers a small number of selected papers; we analyze their (often lack of) theoretical approaches, review their methodological innovations, and offer suggestions as to the relevance of their results for political scientists and sociologists. △ Less","16 May, 2016",https://arxiv.org/pdf/1605.04774
Mapping knowledge translation and innovation processes in Cancer Drug Development: the case of liposomal doxorubicin,David Fajardo-Ortiz;Luis Duran;Laura Moreno;Hector Ochoa;Victor-M Castano,"We explored how the knowledge translation and innovation processes are structured when they result in innovations, as in the case of liposomal doxorubicin research. In order to map the processes, a literature network analysis was made through Cytoscape and semantic analysis was performed by GOPubmed which is based in the controlled vocabularies MeSH (Medical Subject Headings) and GO (Gene Ontology). We found clusters related to different stages of the technological development (invention, innovation and imitation) and the knowledge translation process (preclinical, translational and clinical research), and we were able to map the historic emergence of Doxil as a paradigmatic nanodrug. This research could be a powerful methodological tool for decision-making and innovation management in drug delivery research. △ Less","12 April, 2016",https://arxiv.org/pdf/1605.02041
Does disaggregated electricity feedback reduce domestic electricity consumption? A systematic review of the literature,Jack Kelly;William Knottenbelt,"We examine 12 studies on the efficacy of disaggregated energy feedback. The average electricity reduction across these studies is 4.5%. However, 4.5% may be a positively-biased estimate of the savings achievable across the entire population because all 12 studies are likely to be prone to opt-in bias hence none test the effect of disaggregated feedback on the general population. Disaggregation may not be required to achieve these savings: Aggregate feedback alone drives 3% reductions; and the 4 studies which directly compared aggregate feedback against disaggregated feedback found that aggregate feedback is at least as effective as disaggregated feedback, possibly because web apps are viewed less often than in-home-displays (in the short-term, at least) and because some users do not trust fine-grained disaggregation (although this may be an issue with the specific user interface studied). Disaggregated electricity feedback may help a motivated sub-group of the population to save more energy but fine-grained disaggregation may not be necessary to achieve these energy savings. Disaggregation has many uses beyond those discussed in this paper but, on the specific question of promoting energy reduction in the general population, there is no robust evidence that current forms of disaggregated energy feedback are more effective than aggregate energy feedback. The effectiveness of disaggregated feedback may increase if the general population become more energy-conscious (e.g. if energy prices rise or concern about climate change deepens); or if users' trust in fine-grained disaggregation improves; or if innovative new approaches or alternative disaggregation strategies (e.g. disaggregating by behaviour rather than by appliance) out-perform existing feedback. We also discuss opportunities for new research into the effectiveness of disaggregated feedback. △ Less","4 May, 2016",https://arxiv.org/pdf/1605.00962
Single Image 3D Interpreter Network,Jiajun Wu;Tianfan Xue;Joseph J. Lim;Yuandong Tian;Joshua B. Tenenbaum;Antonio Torralba;William T. Freeman,"Understanding 3D object structure from a single image is an important but difficult task in computer vision, mostly due to the lack of 3D object annotations in real images. Previous work tackles this problem by either solving an optimization task given 2D keypoint positions, or training on synthetic data with ground truth 3D information. In this work, we propose 3D INterpreter Network (3D-INN), an end-to-end framework which sequentially estimates 2D keypoint heatmaps and 3D object structure, trained on both real 2D-annotated images and synthetic 3D data. This is made possible mainly by two technical innovations. First, we propose a Projection Layer, which projects estimated 3D structure to 2D space, so that 3D-INN can be trained to predict 3D structural parameters supervised by 2D annotations on real images. Second, heatmaps of keypoints serve as an intermediate representation connecting real and synthetic data, enabling 3D-INN to benefit from the variation and abundance of synthetic 3D objects, without suffering from the difference between the statistics of real and synthesized images due to imperfect rendering. The network achieves state-of-the-art performance on both 2D keypoint estimation and 3D structure recovery. We also show that the recovered 3D information can be used in other vision applications, such as 3D rendering and image retrieval. △ Less","4 October, 2016",https://arxiv.org/pdf/1604.08685
Semantic Reasoning for Context-aware Internet of Things Applications,Altti Ilari Maarala;Xiang Su;Jukka Riekki,"Advances in ICT are bringing into reality the vision of a large number of uniquely identifiable, interconnected objects and things that gather information from diverse physical environments and deliver the information to a variety of innovative applications and services. These sensing objects and things form the Internet of Things (IoT) that can improve energy and cost efficiency and automation in many different industry fields such as transportation and logistics, health care and manufacturing, and facilitate our everyday lives as well. IoT applications rely on real-time context data and allow sending information for driving the behaviors of users in intelligent environments. △ Less","28 April, 2016",https://arxiv.org/pdf/1604.08340
"A ""Social Bitcoin"" could sustain a democratic digital world",Kaj-Kolja Kleineberg;Dirk Helbing,"A multidimensional financial system could provide benefits for individuals, companies, and states. Instead of top-down control, which is destined to eventually fail in a hyperconnected world, a bottom-up creation of value can unleash creative potential and drive innovations. Multiple currency dimensions can represent different externalities and thus enable the design of incentives and feedback mechanisms that foster the ability of complex dynamical systems to self-organize and lead to a more resilient society and sustainable economy. Modern information and communication technologies play a crucial role in this process, as Web 2.0 and online social networks promote cooperation and collaboration on unprecedented scales. Within this contribution, we discuss how one dimension of a multidimensional currency system could represent socio-digital capital (Social Bitcoins) that can be generated in a bottom-up way by individuals who perform search and navigation tasks in a future version of the digital world. The incentive to mine Social Bitcoins could sustain digital diversity, which mitigates the risk of totalitarian control by powerful monopolies of information and can create new business opportunities needed in times where a large fraction of current jobs is estimated to disappear due to computerisation. △ Less","21 September, 2016",https://arxiv.org/pdf/1604.08168
Sleeping Beauties in Meme Diffusion,Leihan Zhang;Ke Xu;Jichang Zhao,"A sleeping beauty in diffusion indicates that the information, can be ideas or innovations, will experience a hibernation before a sudden spike of popularity and it is widely found in citation history of scientific publications. However, in this study, we demonstrate that the sleeping beauty is an interesting and unexceptional phenomenon in information diffusion and even more inspiring, there exist two consecutive sleeping beauties in the entire lifetime of propagation, suggesting that the information, including scientific topics, search queries or Wikipedia entries, which we call memes, will go unnoticed for a period and suddenly attracts some attention, and then it falls asleep again and later wakes up with another unexpected popularity peak. Further explorations on this phenomenon show that intervals between two wake ups follow an exponential distribution and the second wake up generally reaches its peak at a higher velocity. In addition, higher volume of the first wake up will lead to even much higher popularity of the second wake up with great odds. Taking these findings into consideration, an upgraded Bass model is presented to well describe the diffusion dynamics of memes on different media. Our results can help understand the common mechanism behind propagation of different memes and are instructive to locate the tipping point in marketing or find innovative publications in science. △ Less","5 December, 2016",https://arxiv.org/pdf/1604.07532
Training Deep Nets with Sublinear Memory Cost,Tianqi Chen;Bing Xu;Chiyuan Zhang;Carlos Guestrin,"We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences. △ Less","22 April, 2016",https://arxiv.org/pdf/1604.06174
Local Binary Pattern for Word Spotting in Handwritten Historical Document,Sounak Dey;Anguelos Nicolaou;Josep Llados;Umapada Pal,"Digital libraries store images which can be highly degraded and to index this kind of images we resort to word spot- ting as our information retrieval system. Information retrieval for handwritten document images is more challenging due to the difficulties in complex layout analysis, large variations of writing styles, and degradation or low quality of historical manuscripts. This paper presents a simple innovative learning-free method for word spotting from large scale historical documents combining Local Binary Pattern (LBP) and spatial sampling. This method offers three advantages: firstly, it operates in completely learning free paradigm which is very different from unsupervised learning methods, secondly, the computational time is significantly low because of the LBP features which are very fast to compute, and thirdly, the method can be used in scenarios where annotations are not available. Finally we compare the results of our proposed retrieval method with the other methods in the literature. △ Less","21 April, 2016",https://arxiv.org/pdf/1604.05907
Sleeping Beauties Cited in Patents: Is there also a Dormitory of Inventions?,Anthony F. J. van Raan,"A Sleeping Beauty in Science is a publication that goes unnoticed (sleeps) for a long time and then, almost suddenly, attracts a lot of attention (is awakened by a prince). In our foregoing study we found that roughly half of the Sleeping Beauties are application-oriented and thus are potential Sleeping Innovations. In this paper we investigate a new topic: Sleeping Beauties that are cited in patents. In this way we explore the existence of a dormitory of inventions. We find that patent citation may occur before or after the awakening and that the depth of the sleep, i.e., citation rate during the sleeping period, is no predictor for later scientific or technological impact of the Sleeping Beauty. Inventor-author self-citations occur only in a small minority of the Sleeping Beauties that are cited in patents, but other types of inventor-author links occur more frequently. We analyze whether they deal with new topics by measuring the time-dependent evolution in the entire scientific literature of the number of papers related to both the precisely defined topics as well as the broader research theme of the Sleeping Beauty during and after the sleeping time. We focus on the awakening by analyzing the first group of papers that cites the Sleeping Beauty. Next, we create concept maps of the topic-related and the citing papers for a time period immediately following the awakening and for the most recent period. Finally, we make an extensive assessment of the cited and citing relations of the Sleeping Beauty. We find that tunable co-citation analysis is a powerful tool to discover the prince and other important application-oriented work directly related to the Sleeping Beauty, for instance papers written by authors who cite Sleeping Beauties in both the patents of which they are the inventors, as well as in their scientific papers. △ Less","19 April, 2016",https://arxiv.org/pdf/1604.05750
The effect of network structure on innovation initiation process: an evolutionary dynamics approach,Afshin Jafari;S. Peyman Shariatpanahi;Mohammad Mahdi Zolfagharzadeh;Mehdi Mohammadi,"In this paper we have proposed a basic agent-based model based on evolutionary dynamics for investigating innovation initiation process. In our model we suppose each agent will represent a firm which is interacting with other firms through a given network structure. We consider a two-hit process for presenting a potentially successful innovation in this model and therefore at each time step each firm can be in on of three different stages which are respectively, Ordinary, Innovative, and Successful. We design different experiments in order to investigate how different interaction networks may affect the process of presenting a successful innovation to the market. In this experiments, we use five different network structures, i.e. Erdős and Rényi, Ring Lattice, Small World, Scale-Free and Distance-Based networks. According to the results of the simulations, for less frequent innovations like radical innovation, local structures are showing a better performance comparing to Scale-Free and Erdős and Rényi networks. Although as we move toward more frequent innovations, like incremental innovations, difference between network structures becomes less and non-local structures show relatively better performance. △ Less","16 April, 2016",https://arxiv.org/pdf/1604.04758
Toward a Science of Autonomy for Physical Systems: Construction,Miroslaw Skibniewski;Mani Golparvar-Fard,"Today, ensuring and improving safety, productivity, quality, and sustainability in construction, operation, and maintenance of national civil infrastructure systems through advances in robotics and automation is a national imperative. By ""national civil infrastructure"" we refer to the 4.5M commercial buildings, 3.9M miles of public roads, 2M miles of oil and natural gas pipelines, 600K bridges, 190K cell phone towers, 120K miles of major railroads, 100K miles of levees, 84K dams, 50K miles of electrical power lines, 25K miles of commercially navigable waterways, and 5K public-use airports in the United States, all of which are critical to our national economy and society. By ""ensuring and improving safety, productivity, quality, and sustainability"" we mean to guarantee performance across the entire life cycle of civil infrastructure, from construction to operation and from maintenance to disposal. By ""advances in robotics and automation"" we mean innovative research and education that will provide tools for systematic and timely execution of construction, and also collection, analysis, visualization, and operational use of big data in built environments for project monitoring and control purposes. In the following, we discuss the current state of construction and operation of the national civil infrastructure systems in detail and present several opportunities for improvements through research and education on robotics and automation. △ Less","12 April, 2016",https://arxiv.org/pdf/1604.03563
Accelerating Science: A Computing Research Agenda,Vasant G. Honavar;Mark D. Hill;Katherine Yelick,"The emergence of ""big data"" offers unprecedented opportunities for not only accelerating scientific advances but also enabling new modes of discovery. Scientific progress in many disciplines is increasingly enabled by our ability to examine natural phenomena through the computational lens, i.e., using algorithmic or information processing abstractions of the underlying processes; and our ability to acquire, share, integrate and analyze disparate types of data. However, there is a huge gap between our ability to acquire, store, and process data and our ability to make effective use of the data to advance discovery. Despite successful automation of routine aspects of data management and analytics, most elements of the scientific process currently require considerable human expertise and effort. Accelerating science to keep pace with the rate of data acquisition and data processing calls for the development of algorithmic or information processing abstractions, coupled with formal methods and tools for modeling and simulation of natural processes as well as major innovations in cognitive tools for scientists, i.e., computational tools that leverage and extend the reach of human intellect, and partner with humans on a broad range of tasks in scientific discovery (e.g., identifying, prioritizing formulating questions, designing, prioritizing and executing experiments designed to answer a chosen question, drawing inferences and evaluating the results, and formulating new questions, in a closed-loop fashion). This calls for concerted research agenda aimed at: Development, analysis, integration, sharing, and simulation of algorithmic or information processing abstractions of natural processes, coupled with formal methods and tools for their analyses and simulation; Innovations in cognitive tools that augment and extend human intellect and partner with humans in all aspects of science. △ Less","6 April, 2016",https://arxiv.org/pdf/1604.02006
A Convolutional Neural Network Neutrino Event Classifier,A. Aurisano;A. Radovic;D. Rocco;A. Himmel;M. D. Messier;E. Niner;G. Pawloski;F. Psihas;A. Sousa;P. Vahle,"Convolutional neural networks (CNNs) have been widely applied in the computer vision community to solve complex problems in image recognition and analysis. We describe an application of the CNN technology to the problem of identifying particle interactions in sampling calorimeters used commonly in high energy physics and high energy neutrino physics in particular. Following a discussion of the core concepts of CNNs and recent innovations in CNN architectures related to the field of deep learning, we outline a specific application to the NOvA neutrino detector. This algorithm, CVN (Convolutional Visual Network) identifies neutrino interactions based on their topology without the need for detailed reconstruction and outperforms algorithms currently in use by the NOvA collaboration. △ Less","12 August, 2016",https://arxiv.org/pdf/1604.01444
A Software Methodology for Compiling Quantum Programs,Thomas Häner;Damian S. Steiger;Krysta Svore;Matthias Troyer,"Quantum computers promise to transform our notions of computation by offering a completely new paradigm. To achieve scalable quantum computation, optimizing compilers and a corresponding software design flow will be essential. We present a software architecture for compiling quantum programs from a high-level language program to hardware-specific instructions. We describe the necessary layers of abstraction and their differences and similarities to classical layers of a computer-aided design flow. For each layer of the stack, we discuss the underlying methods for compilation and optimization. Our software methodology facilitates more rapid innovation among quantum algorithm designers, quantum hardware engineers, and experimentalists. It enables scalable compilation of complex quantum algorithms and can be targeted to any specific quantum hardware implementation. △ Less","11 May, 2016",https://arxiv.org/pdf/1604.01401
Heavy hitters via cluster-preserving clustering,Kasper Green Larsen;Jelani Nelson;Huy L. Nguyen;Mikkel Thorup,"In turnstile \ell_p \varepsilon-heavy hitters, one maintains a high-dimensional x\in\mathbb{R}^n subject to \texttt{update}(i,Δ) causing x_i\leftarrow x_i + Δ, where i\in[n], Δ\in\mathbb{R}. Upon receiving a query, the goal is to report a small list L\subset[n], |L| = O(1/\varepsilon^p), containing every ""heavy hitter"" i\in[n] with |x_i| \ge \varepsilon \|x_{\overline{1/\varepsilon^p}}\|_p, where x_{\overline{k}} denotes the vector obtained by zeroing out the largest k entries of x in magnitude. For any p\in(0,2] the CountSketch solves \ell_p heavy hitters using O(\varepsilon^{-p}\log n) words of space with O(\log n) update time, O(n\log n) query time to output L, and whose output after any query is correct with high probability (whp) 1 - 1/poly(n). Unfortunately the query time is very slow. To remedy this, the work [CM05] proposed for p=1 in the strict turnstile model, a whp correct algorithm achieving suboptimal space O(\varepsilon^{-1}\log^2 n), worse update time O(\log^2 n), but much better query time O(\varepsilon^{-1}poly(\log n)). We show this tradeoff between space and update time versus query time is unnecessary. We provide a new algorithm, ExpanderSketch, which in the most general turnstile model achieves optimal O(\varepsilon^{-p}\log n) space, O(\log n) update time, and fast O(\varepsilon^{-p}poly(\log n)) query time, and whp correctness. Our main innovation is an efficient reduction from the heavy hitters to a clustering problem in which each heavy hitter is encoded as some form of noisy spectral cluster in a much bigger graph, and the goal is to identify every cluster. Since every heavy hitter must be found, correctness requires that every cluster be found. We then develop a ""cluster-preserving clustering"" algorithm, partitioning the graph into clusters without destroying any original cluster. △ Less","5 April, 2016",https://arxiv.org/pdf/1604.01357
A Modified CSMA/CA Protocol for OFDM Underwater Networks: Cross Layer Design,Yanling Yin;Sumit Roy;Payman Arabshahi,"The underwater acoustic channel continues to present significant challenges to efficient throughput performance of underwater acoustic sensor networks (UASNs) in varying scenarios. As a result, cross-layer approaches that explore joint PHY/MAC strategies are worthy of further exploration. We consider a recent high-speed OFDM modem and propose a new cross-layer solution based on modified CSMA/CA, for a canonical star network topology with few nodes (the most common scenario in UASNs). Some innovations to an adaptive OFDM PHY link are developed to jointly select the modulation, convolutional coding and frequency diversity order (different transmission modes) for matching varying channel conditions. Additionally, receiver logic that disambiguates the cause of packet loss between a) that caused by channel vs. b) that due to collisions is used to modify the ARQ/backoff logic for retransmissions with CSMA/CA random access. Simulation results reveal that the cross-layer design can effectively increase network throughput. △ Less","21 March, 2016",https://arxiv.org/pdf/1604.00875
Discriminative Phrase Embedding for Paraphrase Identification,Wenpeng Yin;Hinrich Schütze,"This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification. △ Less","2 April, 2016",https://arxiv.org/pdf/1604.00503
Building an Internet Radio System with Interdisciplinary factored system for automatic content recommendation,Krzysztof Wołk,"Automatic systems for music content recommendation have assumed a new role in recent years. These systems have transformed from being just a convenient, standalone tool into an inseparable element of modern living. In addition, not only do these systems strongly influence human moods and feelings with the selection of proper music content, but they also provide significant commercial and advertising opportunities. This research aims to examine and implement two such systems available for the automatic recognition and recommendation of music and advertisement content for Internet radio. Through analysis of the practical issues of application fields and spheres of influence, conclusions will be drawn about the possible perspectives on and future role of such systems. Other content adaptation that is based on music genres will be discussed, as wellAnother aim of this study is to provide an innovative Internet radio implementation as compared to traditional radio and other Internet broadcast solutions. This will include automatic content recommendation systems for listeners and marketing companies, as well as the usage of a voice synthesizer in in automatic program scheduling. △ Less","1 April, 2016",https://arxiv.org/pdf/1604.00233
It's Moving! A Probabilistic Model for Causal Motion Segmentation in Moving Camera Videos,Pia Bideau;Erik Learned-Miller,"The human ability to detect and segment moving objects works in the presence of multiple objects, complex background geometry, motion of the observer, and even camouflage. In addition to all of this, the ability to detect motion is nearly instantaneous. While there has been much recent progress in motion segmentation, it still appears we are far from human capabilities. In this work, we derive from first principles a new likelihood function for assessing the probability of an optical flow vector given the 3D motion direction of an object. This likelihood uses a novel combination of the angle and magnitude of the optical flow to maximize the information about the true motions of objects. Using this new likelihood and several innovations in initialization, we develop a motion segmentation algorithm that beats current state-of-the-art methods by a large margin. We compare to five state-of-the-art methods on two established benchmarks, and a third new data set of camouflaged animals, which we introduce to push motion segmentation to the next level. △ Less","1 April, 2016",https://arxiv.org/pdf/1604.00136
Predictive Modeling of Opinion and Connectivity Dynamics in Social Networks,Ajay Saini;Natasha Markuzon,"Recent years saw an increased interest in modeling and understanding the mechanisms of opinion and innovation spread through human networks. Using analysis of real-world social data, researchers are able to gain a better understanding of the dynamics of social networks and subsequently model the changes in such networks over time. We developed a social network model that both utilizes an agent-based approach with a dynamic update of opinions and connections between agents and reflects opinion propagation and structural changes over time as observed in real-world data. We validate the model using data from the Social Evolution dataset of the MIT Human Dynamics Lab describing changes in friendships and health self-perception in a targeted student population over a nine-month period. We demonstrate the effectiveness of the approach by predicting changes in both opinion spread and connectivity of the network. We also use the model to evaluate how the network parameters, such as the level of `openness' and willingness to incorporate opinions of neighboring agents, affect the outcome. The model not only provides insight into the dynamics of ever changing social networks, but also presents a tool with which one can investigate opinion propagation strategies for networks of various structures and opinion distributions. △ Less","27 March, 2016",https://arxiv.org/pdf/1603.08252
Predicting Cyber Attack Rates with Extreme Values,Zhenxin Zhan;Maochao Xu;Shouhuai Xu,"It is important to understand to what extent, and in what perspectives, cyber attacks can be predicted. Despite its evident importance, this problem was not investigated until very recently, when we proposed using the innovative methodology of {\em gray-box prediction}. This methodology advocates the use of gray-box models, which accommodate the statistical properties/phenomena exhibited by the data. Specifically, we showed that gray-box models that accommodate the Long-Range Dependence (LRD) phenomenon can predict the attack rate (i.e., the number of attacks per unit time) 1-hour ahead-of-time with an accuracy of 70.2-82.1\%. To the best of our knowledge, this is the first result showing the feasibility of prediction in this domain. We observe that the prediction errors are partly caused by the models' incapability in predicting the large attack rates, which are called {\em extreme values} in statistics. This motivates us to analyze the {\em extreme-value phenomenon}, by using two complementary approaches: the Extreme Value Theory (EVT) and the Time Series Theory (TST). In this paper, we show that EVT can offer long-term predictions (e.g., 24-hour ahead-of-time), while gray-box TST models can predict attack rates 1-hour ahead-of-time with an accuracy of 86.0-87.9\%. We explore connections between the two approaches, and point out future research directions. Although our prediction study is based on specific cyber attack data, our methodology can be equally applied to analyze any cyber attack data of its kind. △ Less","24 March, 2016",https://arxiv.org/pdf/1603.07432
Signed Link Analysis in Social Media Networks,Ghazaleh Beigi;Jiliang Tang;Huan Liu,"Numerous real-world relations can be represented by signed networks with positive links (e.g., trust) and negative links (e.g., distrust). Link analysis plays a crucial role in understanding the link formation and can advance various tasks in social network analysis such as link prediction. The majority of existing works on link analysis have focused on unsigned social networks. The existence of negative links determines that properties and principles of signed networks are substantially distinct from those of unsigned networks, thus we need dedicated efforts on link analysis in signed social networks. In this paper, following social theories in link analysis in unsigned networks, we adopt three social science theories, namely Emotional Information, Diffusion of Innovations and Individual Personality, to guide the task of link analysis in signed networks. △ Less","22 March, 2016",https://arxiv.org/pdf/1603.06878
Rapid building detection using machine learning,Joseph Paul Cohen;Wei Ding;Caitlin Kuhlman;Aijun Chen;Liping Di,"This work describes algorithms for performing discrete object detection, specifically in the case of buildings, where usually only low quality RGB-only geospatial reflective imagery is available. We utilize new candidate search and feature extraction techniques to reduce the problem to a machine learning (ML) classification task. Here we can harness the complex patterns of contrast features contained in training data to establish a model of buildings. We avoid costly sliding windows to generate candidates; instead we innovatively stitch together well known image processing techniques to produce candidates for building detection that cover 80-85% of buildings. Reducing the number of possible candidates is important due to the scale of the problem. Each candidate is subjected to classification which, although linear, costs time and prohibits large scale evaluation. We propose a candidate alignment algorithm to boost classification performance to 80-90% precision with a linear time algorithm and show it has negligible cost. Also, we propose a new concept called a Permutable Haar Mesh (PHM) which we use to form and traverse a search space to recover candidate buildings which were lost in the initial preprocessing phase. △ Less","14 March, 2016",https://arxiv.org/pdf/1603.04392
Server-side verification of client behavior in cryptographic protocols,Andrew Chi;Robert Cochran;Marie Nesfield;Michael K. Reiter;Cynthia Sturton,"Numerous exploits of client-server protocols and applications involve modifying clients to behave in ways that untampered clients would not, such as crafting malicious packets. In this paper, we demonstrate practical verification of a cryptographic protocol client's messaging behavior as being consistent with the client program it is believed to be running. Moreover, we accomplish this without modifying the client in any way, and without knowing all of the client-side inputs driving its behavior. Our toolchain for verifying a client's messages explores multiple candidate execution paths in the client concurrently, an innovation that we show is both specifically useful for cryptographic protocol clients and more generally useful for client applications of other types, as well. In addition, our toolchain includes a novel approach to symbolically executing the client software in multiple passes that defers expensive functions until their inputs can be inferred and concretized. We demonstrate client verification on OpenSSL to show that, e.g., Heartbleed exploits can be detected without Heartbleed-specific filtering and within seconds of the first malicious packet, and that verification of legitimate clients can keep pace with, e.g., Gmail workloads. △ Less","13 March, 2016",https://arxiv.org/pdf/1603.04085
Indoor 5G 3GPP-like Channel Models for Office and Shopping Mall Environments,Katsuyuki Haneda;Lei Tian;Henrik Asplund;Jian Li;Yi Wang;David Steer;Clara Li;Tommaso Balercia;Sunguk Lee;YoungSuk Kim;Amitava Ghosh;Timothy Thomas;Takehiro Nakamura;Yuichi Kakishima;Tetsuro Imai;Haralabos Papadopoulas;Theodore S. Rappaport;George R. MacCartney Jr.;Mathew K. Samimi;Shu Sun;Ozge Koymen;Sooyoung Hur;Jeongho Park;Charlie Zhang;Evangelos Mellios,"Future mobile communications systems are likely to be very different to those of today with new service innovations driven by increasing data traffic demand, increasing processing power of smart devices and new innovative applications. To meet these service demands the telecommunications industry is converging on a common set of 5G requirements which includes network speeds as high as 10 Gbps, cell edge rate greater than 100 Mbps, and latency of less than 1 msec. To reach these 5G requirements the industry is looking at new spectrum bands in the range up to 100 GHz where there is spectrum availability for wide bandwidth channels. For the development of new 5G systems to operate in bands up to 100 GHz there is a need for accurate radio propagation models which are not addressed by existing channel models developed for bands below 6 GHz. This paper presents a preliminary overview of the 5G channel models for bands up to 100 GHz in indoor offices and shopping malls, derived from extensive measurements across a multitude of bands. These studies have found some extensibility of the existing 3GPP models to the higher frequency bands up to 100 GHz. The measurements indicate that the smaller wavelengths introduce an increased sensitivity of the propagation models to the scale of the environment and show some frequency dependence of the path loss as well as increased occurrence of blockage. Further, the penetration loss is highly dependent on the material and tends to increase with frequency. The small-scale characteristics of the channel such as delay spread and angular spread and the multipath richness is somewhat similar over the frequency range, which is encouraging for extending the existing 3GPP models to the wider frequency range. Further work will be carried out to complete these models, but this paper presents the first steps for an initial basis for the model development. △ Less","13 March, 2016",https://arxiv.org/pdf/1603.04079
Automating the Horae: Boundary-work in the age of computers,Luis Reyes-Galindo,"This paper describes the intense software filtering that has allowed the arXiv eprint repository to sort and process large numbers of submissions with minimal human intervention, making it one of the most important and influential cases of open access repositories to date. The paper narrates arXiv's transformation, using sophisticated sorting-filtering algorithms to decrease human workload, from a small mailing list used by a few hundred researchers to a site that processes thousands of papers per month. However there are significant negative consequences for authors who have been filtered out of the main categories. There is thus a continued need to check and balance arXiv's boundaries, based in the essential tension between stability and innovation. △ Less","11 March, 2016",https://arxiv.org/pdf/1603.03824
Modeling Information Diffusion in Social Networks,Nima Heidari,"One major feature of social networks (e.g., massive online social networks) is the dissemination of information, such as news, rumors and opinions. Information can be propagated via natural connections in written, oral or electronic forms. The physics of information diffusion has been changed with the mainstream adoption of the Internet and Web. Until a few years ago, the major barrier for someone who wanted a piece of information to spread through a community was the cost of the technical infrastructure required to reach a large number of people. Today, with widespread access to the Internet, this bottleneck has largely been removed. Information diffusion has been one of the focuses in social network research area, due to its importance in social interactions and everyday life. More recently, during the last twenty to thirty years, there has been interest and attention not just in observing information and innovation flow, but also in influencing and creating them. Modeling information diffusion in networks enables us to reason about its spread. △ Less","29 March, 2016",https://arxiv.org/pdf/1603.02178
A Real-Time and Energy-Efficient Implementation of Difference-of-Gaussian with Flexible Thin-Film Transistors,Nan Wu;Zheyu Liu;Fei Qiao;Xiaojun Guo;Qi Wei;Yuan Xie;Huazhong Yang,"With many advantageous features, softness and better biocompatibility, flexible electronic devices have developed rapidly and increasingly attracted attention. Many currently applications with flexible devices are sensors and drivers, while there is nearly no utilization aiming at complex computation since flexible devices have lower electron mobility, simple structure and large process variation. In this paper, we proposed an innovative method that enabled flexible devices to implement real-time and energy-efficient Difference-of-Gaussian, which illustrated feasibility and potentials for them to achieve complicated real-time computation in future generation products. △ Less","7 March, 2016",https://arxiv.org/pdf/1603.01954
An Argument-based Creative Assistant for Harmonic Blending,Maximos Kaliakatsos-Papakostas;Roberto Confalonieri;Joseph Corneli;Asterios Zacharakis;Emilios Cambouropoulos,"Conceptual blending is a powerful tool for computational creativity where, for example, the properties of two harmonic spaces may be combined in a consistent manner to produce a novel harmonic space. However, deciding about the importance of property features in the input spaces and evaluating the results of conceptual blending is a nontrivial task. In the specific case of musical harmony, defining the salient features of chord transitions and evaluating invented harmonic spaces requires deep musicological background knowledge. In this paper, we propose a creative tool that helps musicologists to evaluate and to enhance harmonic innovation. This tool allows a music expert to specify arguments over given transition properties. These arguments are then considered by the system when defining combinations of features in an idiom-blending process. A music expert can assess whether the new harmonic idiom makes musicological sense and re-adjust the arguments (selection of features) to explore alternative blends that can potentially produce better harmonic spaces. We conclude with a discussion of future work that would further automate the harmonisation process. △ Less","5 March, 2016",https://arxiv.org/pdf/1603.01770
A Framework for End-to-End Evaluation of 5G mmWave Cellular Networks in ns-3,Russell Ford;Menglei Zhang;Sourjya Dutta;Marco Mezzavilla;Sundeep Rangan;Michele Zorzi,"The growing demand for ubiquitous mobile data services along with the scarcity of spectrum in the sub-6 GHz bands has given rise to the recent interest in developing wireless systems that can exploit the large amount of spectrum available in the millimeter wave (mmWave) frequency range. Due to its potential for multi-gigabit and ultra-low latency links, mmWave technology is expected to play a central role in 5th Generation (5G) cellular networks. Overcoming the poor radio propagation and sensitivity to blockages at higher frequencies presents major challenges, which is why much of the current research is focused at the physical layer. However, innovations will be required at all layers of the protocol stack to effectively utilize the large air link capacity and provide the end-to-end performance required by future networks. Discrete-event network simulation will be an invaluable tool for researchers to evaluate novel 5G protocols and systems from an end-to-end perspective. In this work, we present the first-of-its-kind, open-source framework for modeling mmWave cellular networks in the ns-3 simulator. Channel models are provided along with a configurable physical and MAC-layer implementation, which can be interfaced with the higher-layer protocols and core network model from the ns-3 LTE module to simulate end-to-end connectivity. The framework is demonstrated through several example simulations showing the performance of our custom mmWave stack. △ Less","8 May, 2016",https://arxiv.org/pdf/1602.06932
Interactive Storytelling over Document Collections,Dipayan Maiti;Mohammad Raihanul Islam;Scotland Leman;Naren Ramakrishnan,"Storytelling algorithms aim to 'connect the dots' between disparate documents by linking starting and ending documents through a series of intermediate documents. Existing storytelling algorithms are based on notions of coherence and connectivity, and thus the primary way by which users can steer the story construction is via design of suitable similarity functions. We present an alternative approach to storytelling wherein the user can interactively and iteratively provide 'must use' constraints to preferentially support the construction of some stories over others. The three innovations in our approach are distance measures based on (inferred) topic distributions, the use of constraints to define sets of linear inequalities over paths, and the introduction of slack and surplus variables to condition the topic distribution to preferentially emphasize desired terms over others. We describe experimental results to illustrate the effectiveness of our interactive storytelling approach over multiple text datasets. △ Less","21 February, 2016",https://arxiv.org/pdf/1602.06566
Key Economic and Environmental Perspectives on Sustainability in the ICT Sector,Hassan Hamdoun;Jafar A. Alzubi;Omar A. Alzubi;Solomon Mangeni,"Telecommunication networks have become as critical to the 21st century development as were railways, roads and canals, to the 19th Century developments and is now seen as enabler to a more sustained business, environment and society as a whole. Still fascinating has been and is the exponential rate of growth in this industry. This is one sector where the next revolution is always just around the corner whether known or unknown. The telecoms industry is categorized by high rates of innovation in a rapidly changing technological environment. This in turn is associated with an immense range of sustainability concerns and challenges for the Telecoms service providers, the service users and the whole industry and its far reaching influence on other industries. This paper discusses three key aspects of such challenges namely; the question of sustainable power/energy supply for the industry when the change is resulting in increasing energy and operational cost, the exploitation of technologies advancement for sustainability and their business and environmental benefits. △ Less","17 February, 2016",https://arxiv.org/pdf/1602.05559
Diffusion of innovation in large scale graphs,Fabio Fagnani;Lorenzo Zino,"Will a new smartphone application diffuse deeply in the population or will it sink into oblivion soon? To predict this, we argue that common models of spread of innovations based on cascade dynamics or epidemics may not be fully adequate. Therefore we propose a novel stochastic network dynamics modeling the spread of a new technological asset, whose adoption is based on the word-of-mouth and the persuasion strength increases the more the product is diffused. In this paper we carry on an analysis on large scale graphs to show off how the parameters of the model, the topology of the graph and, possibly, the initial diffusion of the asset, determine whether the spread of the asset is successful or not. In particular, by means of stochastic dominations and deterministic approximations, we provide some general results for a large class of expansive graphs. Finally we present numerical simulations trying to expand the analytical results we proved to even more general topologies. △ Less","17 February, 2016",https://arxiv.org/pdf/1602.05413
Designing and Implementing Future Aerial Communication Networks,Sathyanarayanan Chandrasekharan;Karina Gomez;Akram Al-Hourani;Sithamparanathan Kandeepan;Tinku Rasheed;Leonardo Goratti;Laurent Reynaud;David Grace;Isabelle Bucaille;Thomas Wirth;Sandy Allsopp,"Providing ""connectivity from the sky"" is the new innovative trend in wireless communications. High and low altitude platforms, drones, aircrafts and airships are being considered as the candidates for deploying wireless communications complementing the terrestrial communication infrastructure. In this article, we report the detailed account of the design and implementation challenges of an aerial network consisting of LTE Advanced (LTE-A) base stations. In particular, we review achievements and innovations harnessed by an aerial network composed of Helikite platforms. Helikites can be raised in the sky to bring Internet access during special events and in the aftermath of an emergency. The trial phase of the system mounting LTE-A technology onboard Helikites to serve users on the ground showed not only to be very encouraging but that such a system could offer even a longer lasting solution provided that inefficiency in powering the radio frequency equipment in the Helikite can be overcome. △ Less","17 February, 2016",https://arxiv.org/pdf/1602.05318
Encoding Distortion Modeling For DWT-Based Wireless EEG Monitoring System,Alaa Awad;Medhat H. M. Elsayed;Amr Mohamed,"Recent advances in wireless body area sensor net- works leverage wireless and mobile communication technologies to facilitate development of innovative medical applications that can significantly enhance healthcare services and improve quality of life. Specifically, Electroencephalography (EEG)-based applications lie at the heart of these promising technologies. However, the design and operation of such applications is challenging. Power consumption requirements of the sensor nodes may turn some of these applications impractical. Hence, implementing efficient encoding schemes are essential to reduce power consumption in such applications. In this paper, we propose an analytical distortion model for the EEG-based encoding systems. Using this model, the encoder can effectively reconfigure its complexity by adjusting its control parameters to satisfy application constraints while maintaining reconstruction accuracy at the receiver side. The simulation results illustrate that the main parameters that affect the distortion are compression ratio and filter length of the considered DWT-based encoder. Furthermore, it is found that the wireless channel variations have a significant influence on the estimated distortion at the receiver side. △ Less","16 February, 2016",https://arxiv.org/pdf/1602.04974
Information Diffusion of Topic Propagation in Social Media,Shahin Mahdizadehaghdam;Han Wang;Hamid Krim;Liyi Dai,"Real-world social and/or operational networks consist of agents with associated states, whose connectivity forms complex topologies. This complexity is further compounded by interconnected information layers, consisting, for instance, documents/resources of the agents which mutually share topical similarities. Our goal in this work is to predict the specific states of the agents, as their observed resources evolve in time and get updated. The information diffusion among the agents and the publications themselves effectively result in a dynamic process which we capture by an interconnected system of networks (i.e. layered). More specifically, we use a notion of a supra-Laplacian matrix to address such a generalized diffusion of an interconnected network starting with the classical ""graph Laplacian"". The auxiliary and external input update is modeled by a multidimensional Brownian process, yielding two contributions to the variations in the states of the agents: one that is due to the intrinsic interactions in the network system, and the other due to the external inputs or innovations. A variation on this theme, a priori knowledge of a fraction of the agents' states is shown to lead to a Kalman predictor problem. This helps us refine the predicted states exploiting the error in estimating the states of agents. Three real-world datasets are used to evaluate and validate the information diffusion process in this novel layered network approach. Our results demonstrate a lower prediction error when using the interconnected network rather than the single connectivity layer between the agents. The prediction error is further improved by using the estimated diffusion connection and by applying the Kalman approach with partial observations. △ Less","15 February, 2016",https://arxiv.org/pdf/1602.04854
Designing Intelligent Instruments,Kevin H. Knuth;Philip M. Erner;Scott Frasso,"Remote science operations require automated systems that can both act and react with minimal human intervention. One such vision is that of an intelligent instrument that collects data in an automated fashion, and based on what it learns, decides which new measurements to take. This innovation implements experimental design and unites it with data analysis in such a way that it completes the cycle of learning. This cycle is the basis of the Scientific Method. The three basic steps of this cycle are hypothesis generation, inquiry, and inference. Hypothesis generation is implemented by artificially supplying the instrument with a parameterized set of possible hypotheses that might be used to describe the physical system. The act of inquiry is handled by an inquiry engine that relies on Bayesian adaptive exploration where the optimal experiment is chosen as the one which maximizes the expected information gain. The inference engine is implemented using the nested sampling algorithm, which provides the inquiry engine with a set of posterior samples from which the expected information gain can be estimated. With these computational structures in place, the instrument will refine its hypotheses, and repeat the learning cycle by taking measurements until the system under study is described within a pre-specified tolerance. We will demonstrate our first attempts toward achieving this goal with an intelligent instrument constructed using the LEGO MINDSTORMS NXT robotics platform. △ Less","13 February, 2016",https://arxiv.org/pdf/1602.04290
"Deep Learning on FPGAs: Past, Present, and Future",Griffin Lacey;Graham W. Taylor;Shawki Areibi,"The rapid growth of data size and accessibility in recent years has instigated a shift of philosophy in algorithm design for artificial intelligence. Instead of engineering algorithms by hand, the ability to learn composable systems automatically from massive amounts of data has led to ground-breaking performance in important domains such as computer vision, speech recognition, and natural language processing. The most popular class of techniques used in these domains is called deep learning, and is seeing significant attention from industry. However, these models require incredible amounts of data and compute power to train, and are limited by the need for better hardware acceleration to accommodate scaling beyond current data and model sizes. While the current solution has been to use clusters of graphics processing units (GPU) as general purpose processors (GPGPU), the use of field programmable gate arrays (FPGA) provide an interesting alternative. Current trends in design tools for FPGAs have made them more compatible with the high-level software practices typically practiced in the deep learning community, making FPGAs more accessible to those who build and deploy models. Since FPGA architectures are flexible, this could also allow researchers the ability to explore model-level optimizations beyond what is possible on fixed architectures such as GPUs. As well, FPGAs tend to provide high performance per watt of power consumption, which is of particular importance for application scientists interested in large scale server-based deployment or resource-limited embedded applications. This review takes a look at deep learning and FPGAs from a hardware acceleration perspective, identifying trends and innovations that make these technologies a natural fit, and motivates a discussion on how FPGAs may best serve the needs of the deep learning community moving forward. △ Less","12 February, 2016",https://arxiv.org/pdf/1602.04283
Enhancing Patient Appointments Scheduling that Uses Mobile Technology,Godphrey G Kyambille;Khamisi Kalegele,"Appointment scheduling systems are utilized mainly by specialty care clinics to manage access to service providers as well as by hospitals to schedule patient appointments. When attending hospitals in Tanzania, patients experience challenges to see an appropriate specialist doctor because of service interval inconsistency. Timely availability of doctors is critical whenever a patient needs to see a specialist doctor for treatment and a serious bottleneck lies in the application of appropriate technology techniques to enhance appointment scheduling. In this paper, we present a mobile based application scheduling system for managing patient appointments. Furthermore, forthcoming opportunities for the innovative use of the mobile based application scheduling system are identified. △ Less","10 February, 2016",https://arxiv.org/pdf/1602.03337
Science on YouTube: What users find when they search for climate science and climate manipulation,Joachim Allgaier,"Online video-sharing sites such as YouTube are very popular and also used by a lot of people to obtain knowledge and information, also on science, health and technology. Technically they could be valuable tools for the public communication of science and technology, but the users of YouTube are also confronted with conspiracy theories and erroneous and misleading information that deviates from scientific consensus views. This contribution details the results of a study that investigates what kind of information users find when they are searching for climate science and climate manipulation topics on YouTube and whether this information corresponds with or challenges scientific consensus views. An innovative methodological approach using the anonymization network Tor is introduced for drawing randomized samples of YouTube videos. This approach was used to select and examine a sample of 140 YouTube videos on climate topics. △ Less","18 April, 2016",https://arxiv.org/pdf/1602.02692
Economic and Technological Complexity: A Model Study of Indicators of Knowledge-based Innovation Systems,Inga Ivanova;Oivind Strand;Duncan Kushnir;Loet Leydesdorff,"The Economic Complexity Index (ECI; Hidalgo & Hausmann, 2009) measures the complexity of national economies in terms of product groups. Analogously to ECI, a Patent Complexity Index (PatCI) can be developed on the basis of a matrix of nations versus patent classes. Using linear algebra, the three dimensions: countries, product groups, and patent classes can be combined into a measure of ""Triple Helix"" complexity (THCI) including the trilateral interaction terms between knowledge production, wealth generation, and (national) control. THCI can be expected to capture the extent of systems integration between the global dynamics of markets (ECI) and technologies (PatCI) in each national system of innovation. We measure ECI, PatCI, and THCI during the period 2000-2014 for the 34 OECD member states, the BRICS countries, and a group of emerging and affiliated economies (Argentina, Hong Kong, Indonesia, Malaysia, Romania, and Singapore). The three complexity indicators are correlated between themselves; but the correlations with GDP per capita are virtually absent. Of the world's major economies, Japan scores highest on all three indicators, while China has been increasingly successful in combining economic and technological complexity. We could not reproduce the correlation between ECI and average income that has been central to the argument about the fruitfulness of the economic complexity approach. △ Less","7 December, 2016",https://arxiv.org/pdf/1602.02348
Under a cloud of uncertainty: Legal questions affecting Internet storage and transmission of copyright-protected video content,Fraida Fund;S. Amir Hosseini;Shivendra S. Panwar,"The rapid growth of multimedia consumption has triggered technical, economic, and business innovations that improve the quality and accessibility of content. It has also opened new markets, promising large revenues for industry players. However, new technologies also pose new questions regarding the legal aspects of content delivery, which are often resolved through litigation between copyright owners and content distributors. The precedents set by these cases will act as a game changer in the content delivery industry and will shape the existing offerings in the market in terms of how new technologies can be deployed and what kind of pricing strategies can be associated with them. In this paper, we offer a tutorial on key copyright and communications laws and decisions related to storage and transmission of video content over the Internet. We summarize legal limitations on the deployment of new technologies and pricing mechanisms, and explain the implications of recent lawsuits. Understanding these concerns is essential for engineers engaged in designing the technical and economic aspects of video delivery systems. △ Less","3 February, 2016",https://arxiv.org/pdf/1602.01547
Reverse Nearest Neighbor Heat Maps: A Tool for Influence Exploration,Yu Sun;Rui Zhang;Andy Yuan Xue;Jianzhong Qi;Xiaoyong Du,"We study the problem of constructing a reverse nearest neighbor (RNN) heat map by finding the RNN set of every point in a two-dimensional space. Based on the RNN set of a point, we obtain a quantitative influence (i.e., heat) for the point. The heat map provides a global view on the influence distribution in the space, and hence supports exploratory analyses in many applications such as marketing and resource management. To construct such a heat map, we first reduce it to a problem called Region Coloring (RC), which divides the space into disjoint regions within which all the points have the same RNN set. We then propose a novel algorithm named CREST that efficiently solves the RC problem by labeling each region with the heat value of its containing points. In CREST, we propose innovative techniques to avoid processing expensive RNN queries and greatly reduce the number of region labeling operations. We perform detailed analyses on the complexity of CREST and lower bounds of the RC problem, and prove that CREST is asymptotically optimal in the worst case. Extensive experiments with both real and synthetic data sets demonstrate that CREST outperforms alternative algorithms by several orders of magnitude. △ Less","2 February, 2016",https://arxiv.org/pdf/1602.00389
Distributed Constrained Recursive Nonlinear Least-Squares Estimation: Algorithms and Asymptotics,Anit Kumar Sahu;Soummya Kar;Jose' M. F. Moura;H. Vincent Poor,"This paper focuses on the problem of recursive nonlinear least squares parameter estimation in multi-agent networks, in which the individual agents observe sequentially over time an independent and identically distributed (i.i.d.) time-series consisting of a nonlinear function of the true but unknown parameter corrupted by noise. A distributed recursive estimator of the \emph{consensus} + \emph{innovations} type, namely \mathcal{CIWNLS}, is proposed, in which the agents update their parameter estimates at each observation sampling epoch in a collaborative way by simultaneously processing the latest locally sensed information~(\emph{innovations}) and the parameter estimates from other agents~(\emph{consensus}) in the local neighborhood conforming to a pre-specified inter-agent communication topology. Under rather weak conditions on the connectivity of the inter-agent communication and a \emph{global observability} criterion, it is shown that at every network agent, the proposed algorithm leads to consistent parameter estimates. Furthermore, under standard smoothness assumptions on the local observation functions, the distributed estimator is shown to yield order-optimal convergence rates, i.e., as far as the order of pathwise convergence is concerned, the local parameter estimates at each agent are as good as the optimal centralized nonlinear least squares estimator which would require access to all the observations across all the agents at all times. In order to benchmark the performance of the proposed distributed \mathcal{CIWNLS} estimator with that of the centralized nonlinear least squares estimator, the asymptotic normality of the estimate sequence is established and the asymptotic covariance of the distributed estimator is evaluated. Finally, simulation results are presented which illustrate and verify the analytical findings. △ Less","19 October, 2016",https://arxiv.org/pdf/1602.00382
A Biologically Inspired Model of Distributed Online Communication Supporting Efficient Search and Diffusion of Innovation,Soumya Banerjee,"We inhabit a world that is not only small but supports efficient decentralized search - an individual using local information can establish a line of communication with another completely unknown individual. Here we augment a hierarchical social network model with communication between and within communities. We argue that organization into communities would decrease overall decentralized search times. We take inspiration from the biological immune system which organizes search for pathogens in a hybrid modular strategy. Our strategy has relevance in search for rare amounts of information in online social networks. Our work also has implications for design of efficient online networks that could have an impact on networks of human collaboration, scientific collaboration and networks used in targeted manhunts. Real world systems, like online social networks, have high associated delays for long-distance links, since they are built on top of physical networks. Such systems have been shown to densify. Hence such networks will have a communication cost due to space and the requirement of maintaining connections. We have incorporated such a non-spatial cost to communication. We introduce the notion of a community size that increases with the size of the system, which is shown to reduce the time to search for information in networks. Our final strategy balances search times and participation costs and is shown to decrease time to find information in decentralized search in online social networks. Our strategy also balances strong-ties and weak-ties over long distances and may ultimately lead to more productive and innovative networks of human communication and enterprise. We hope that this work will lay the foundation for strategies aimed at producing global scale human interaction networks that are sustainable and lead to a more networked, diverse and prosperous society. △ Less","29 January, 2016",https://arxiv.org/pdf/1601.08021
"Local cascades induced global contagion: How heterogeneous thresholds, exogenous effects, and unconcerned behaviour govern online adoption spreading",Márton Karsai;Gerardo Iñiguez;Riivo Kikas;Kimmo Kaski;János Kertész,"Adoption of innovations, products or online services is commonly interpreted as a spreading process driven to large extent by social influence and conditioned by the needs and capacities of individuals. To model this process one usually introduces behavioural threshold mechanisms, which can give rise to the evolution of global cascades if the system satisfies a set of conditions. However, these models do not address temporal aspects of the emerging cascades, which in real systems may evolve through various pathways ranging from slow to rapid patterns. Here we fill this gap through the analysis and modelling of product adoption in the world's largest voice over internet service, the social network of Skype. We provide empirical evidence about the heterogeneous distribution of fractional behavioural thresholds, which appears to be independent of the degree of adopting egos. We show that the structure of real-world adoption clusters is radically different from previous theoretical expectations, since vulnerable adoptions --induced by a single adopting neighbour-- appear to be important only locally, while spontaneous adopters arriving at a constant rate and the involvement of unconcerned individuals govern the global emergence of social spreading. △ Less","29 January, 2016",https://arxiv.org/pdf/1601.07995
Living Innovation Laboratory Model Design and Implementation,Yuting Zheng,"Living Innovation Laboratory (LIL) is an open and recyclable way for multidisciplinary researchers to remote control resources and co-develop user centered projects. In the past few years, there were several papers about LIL published and trying to discuss and define the model and architecture of LIL. People all acknowledge about the three characteristics of LIL: user centered, co-creation, and context aware, which make it distinguished from test platform and other innovation approaches. Its existing model consists of five phases: initialization, preparation, formation, development, and evaluation. Goal Net is a goal-oriented methodology to formularize a progress. In this thesis, Goal Net is adopted to subtract a detailed and systemic methodology for LIL. LIL Goal Net Model breaks the five phases of LIL into more detailed steps. Big data, crowd sourcing, crowd funding and crowd testing take place in suitable steps to realize UUI, MCC and PCA throughout the innovation process in LIL 2.0. It would become a guideline for any company or organization to develop a project in the form of an LIL 2.0 project. To prove the feasibility of LIL Goal Net Model, it was applied to two real cases. One project is a Kinect game and the other one is an Internet product. They were both transformed to LIL 2.0 successfully, based on LIL goal net based methodology. The two projects were evaluated by phenomenography, which was a qualitative research method to study human experiences and their relations in hope of finding the better way to improve human experiences. Through phenomenographic study, the positive evaluation results showed that the new generation of LIL had more advantages in terms of effectiveness and efficiency. △ Less","26 January, 2016",https://arxiv.org/pdf/1601.07250
Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA,Daniel Christen,"This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky issues of syntactic ambiguity reduction and word sense disambiguation as well as providing innovative and original solutions for constituent generation and constraints management. To provide an insight into how it operates, the system's general architecture and components, as well as its lexical, syntactic and semantic resources are described. After that, the paper addresses the mechanism that performs selective parsing through an interaction between syntactic and semantic information, leading the parser to a coherent and accurate interpretation of the input text. △ Less","21 January, 2016",https://arxiv.org/pdf/1601.05768
Discovering and Characterizing Mobility Patterns in Urban Spaces: A Study of Manhattan Taxi Data,Lisette Espín-Noboa;Florian Lemmerich;Philipp Singer;Markus Strohmaier,"Nowadays, human movement in urban spaces can be traced digitally in many cases. It can be observed that movement patterns are not constant, but vary across time and space. In this work,we characterize such spatio-temporal patterns with an innovative combination of two separate approaches that have been utilized for studying human mobility in the past. First, by using non-negative tensor factorization (NTF), we are able to cluster human behavior based on spatio-temporal dimensions. Second, for understanding these clusters, we propose to use HypTrails, a Bayesian approach for expressing and comparing hypotheses about human trails. To formalize hypotheses we utilize data that is publicly available on the Web, namely Foursquare data and census data provided by an open data platform. By applying this combination of approaches to taxi data in Manhattan, we can discover and explain different patterns in human mobility that cannot be identified in a collective analysis. As one example, we can find a group of taxi rides that end at locations with a high number of party venues (according to Foursquare) on weekend nights. Overall, our work demonstrates that human mobility is not one-dimensional but rather contains different facets both in time and space which we explain by utilizing online data. The findings of this paper argue for a more fine-grained analysis of human mobility in order to make more informed decisions for e.g., enhancing urban structures, tailored traffic control and location-based recommender systems. △ Less","9 February, 2016",https://arxiv.org/pdf/1601.05274
Harvest the potential of massive MIMO with multi-layer techniques,Mingjie Feng;Shiwen Mao,"Massive MIMO is envisioned as a promising technology for 5G wireless networks due to its high potential to improve both spectral and energy efficiency. Although the massive MIMO system is based on innovations in the physical layer, the upper layer techniques also play important roles in harvesting the performance gains of massive MIMO. In this article, we begin with an analysis of the benefits and challenges of massive MIMO systems. We then investigate the multi-layer techniques for incorporating massive MIMO in several important network deployment scenarios. We conclude this article with a discussion of open and potential problems for future research. △ Less","17 November, 2016",https://arxiv.org/pdf/1601.04217
Keyboard Surface Interaction: Making the keyboard into a pointing device,Julian Ramos;Zhen Li;Johana Rosas;Nikola Banovic;Jennifer Mankoff;Anind Dey,"Pointing devices that reside on the keyboard can reduce the overall time needed to perform mixed pointing and typing tasks, since the hand of the user does not have to reach for the pointing device. However, previous implementations of this kind of device have a higher movement time compared to the mouse and trackpad due to large error rate, low speed and spatial resolution. In this paper we introduce Keyboard Surface Interaction (KSI), an interaction approach that turns the surface of a keyboard into an interaction surface and allows users to rest their hands on the keyboard at all times to minimize fatigue. We developed a proof-of-concept implementation, Fingers, which we optimized over a series of studies. Finally, we evaluated Fingers against the mouse and trackpad in a user study with 25 participants on a Fitts law test style, mixed typing and pointing task. Results showed that for users with more exposure to KSI, our KSI device had better performance (reduced movement and homing time) and reduced discomfort compared to the trackpad. When compared to the mouse, KSI had reduced homing time and reduced discomfort, but increased movement time. This interaction approach is not only a new way to capitalize on the space on top of the keyboard, but also a call to innovate and think beyond the touchscreen, touchpad, and mouse as our main pointing devices. The results of our studies serve as a specification for future KSI devices. △ Less","15 January, 2016",https://arxiv.org/pdf/1601.04029
Live-action Virtual Reality Games,Luis Valente;Esteban Clua;Alexandre Ribeiro Silva;Bruno Feijó,"This paper proposes the concept of ""live-action virtual reality games"" as a new genre of digital games based on an innovative combination of live-action, mixed-reality, context-awareness, and interaction paradigms that comprise tangible objects, context-aware input devices, and embedded/embodied interactions. Live-action virtual reality games are ""live-action games"" because a player physically acts out (using his/her real body and senses) his/her ""avatar"" (his/her virtual representation) in the game stage, which is the mixed-reality environment where the game happens. The game stage is a kind of ""augmented virtuality""; a mixed-reality where the virtual world is augmented with real-world information. In live-action virtual reality games, players wear HMD devices and see a virtual world that is constructed using the physical world architecture as the basic geometry and context information. Physical objects that reside in the physical world are also mapped to virtual elements. Live-action virtual reality games keeps the virtual and real-worlds superimposed, requiring players to physically move in the environment and to use different interaction paradigms (such as tangible and embodied interaction) to complete game activities. This setup enables the players to touch physical architectural elements (such as walls) and other objects, ""feeling"" the game stage. Players have free movement and may interact with physical objects placed in the game stage, implicitly and explicitly. Live-action virtual reality games differ from similar game concepts because they sense and use contextual information to create unpredictable game experiences, giving rise to emergent gameplay. △ Less","7 January, 2016",https://arxiv.org/pdf/1601.01645
Wikiometrics: A Wikipedia Based Ranking System,Gilad Katz;Lior Rokach,"We present a new concept - Wikiometrics - the derivation of metrics and indicators from Wikipedia. Wikipedia provides an accurate representation of the real world due to its size, structure, editing policy and popularity. We demonstrate an innovative mining methodology, where different elements of Wikipedia - content, structure, editorial actions and reader reviews - are used to rank items in a manner which is by no means inferior to rankings produced by experts or other methods. We test our proposed method by applying it to two real-world ranking problems: top world universities and academic journals. Our proposed ranking methods were compared to leading and widely accepted benchmarks, and were found to be extremely correlative but with the advantage of the data being publically available. △ Less","8 January, 2016",https://arxiv.org/pdf/1601.01058
Learning Local Image Descriptors with Deep Siamese and Triplet Convolutional Networks by Minimising Global Loss Functions,Vijay Kumar B G;Gustavo Carneiro;Ian Reid,"Recent innovations in training deep convolutional neural network (ConvNet) models have motivated the design of new methods to automatically learn local image descriptors. The latest deep ConvNets proposed for this task consist of a siamese network that is trained by penalising misclassification of pairs of local image patches. Current results from machine learning show that replacing this siamese by a triplet network can improve the classification accuracy in several problems, but this has yet to be demonstrated for local image descriptor learning. Moreover, current siamese and triplet networks have been trained with stochastic gradient descent that computes the gradient from individual pairs or triplets of local image patches, which can make them prone to overfitting. In this paper, we first propose the use of triplet networks for the problem of local image descriptor learning. Furthermore, we also propose the use of a global loss that minimises the overall classification error in the training set, which can improve the generalisation capability of the model. Using the UBC benchmark dataset for comparing local image descriptors, we show that the triplet network produces a more accurate embedding than the siamese network in terms of the UBC dataset errors. Moreover, we also demonstrate that a combination of the triplet and global losses produces the best embedding in the field, using this triplet network. Finally, we also show that the use of the central-surround siamese network trained with the global loss produces the best result of the field on the UBC dataset. Pre-trained models are available online at https://github.com/vijaykbg/deep-patchmatch △ Less","1 August, 2016",https://arxiv.org/pdf/1512.09272
Knowledge and Influence of MOOC Courses on Initial Teacher Training,Jessica Perez-Parras;Jose Gomez-Galan,"The impact of MOOC courses in the processes of distance learning has been extremely important from the very beginning. They offer an innovative model of massive teaching, which exploits in a paradigmatic manner the potential and relevance that ICT's currently have in modern society. The present article has as its primary objective the analysis of the presence of these courses and the role that they represent in teacher training, and their knowledge and influence on the future teachers that are currently being formed at university level. A case study has been carried out with descriptive not experimental methodology, from a quantitative base. The sample study has been undertaken in Spain (n=200). Its main result being the determination of the minimal impact that the MOOC phenomenon has had on the students polled. Equally, a significant lack of knowledge has been revealed in all its dimensions (professional, pedagogical, structural, etc.), with only a minority of those in the sample group having indicated that they have studied any of the courses, or know to some extent the main platforms of the world in which they are offered. A large number of those surveyed therefore are unaware of the existence of these courses. As a result, it has been established that, regardless of the quality of the learning and the didactic and methodological characteristics that the MOOC courses offer, their study and analysis is considered necessary for future educational professionals. It is imperative that at the level of Higher Education, and especially in the faculties of teacher training, that the most recent advances in the field of ICT's are introduced in the study plan and in the academic programs, for they constitute the base of modern society. △ Less","21 May, 2016",https://arxiv.org/pdf/1512.08456
"A Triple Helix Model of Medical Innovation: Supply, Demand, and Technological Capabilities in terms of Medical Subject Headings",Alexander M. Petersen;Daniele Rotolo;Loet Leydesdorff,"We develop a model of innovation that enables us to trace the interplay among three key dimensions of the innovation process: (i) demand of and (ii) supply for innovation, and (iii) technological capabilities available to generate innovation in the forms of products, processes, and services. Building on triple helix research, we use entropy statistics to elaborate an indicator of mutual information among these dimensions that can provide indication of reduction of uncertainty. To do so, we focus on the medical context, where uncertainty poses significant challenges to the governance of innovation. We use the Medical Subject Headings (MeSH) of MEDLINE/PubMed to identify publications classified within the categories ""Diseases"" (C), ""Drugs and Chemicals"" (D), ""Analytic, Diagnostic, and Therapeutic Techniques and Equipment"" (E) and use these as knowledge representations of demand, supply, and technological capabilities, respectively. Three case-studies of medical research areas are used as representative 'entry perspectives' of the medical innovation process. These are: (i) human papilloma virus, (ii) RNA interference, and (iii) magnetic resonance imaging. We find statistically significant periods of synergy among demand, supply, and technological capabilities (C-D-E) that point to three-dimensional interactions as a fundamental perspective for the understanding and governance of the uncertainty associated with medical innovation. Among the pairwise configurations in these contexts, the demand-technological capabilities (C-E) provided the strongest link, followed by the supply-demand (D-C) and the supply-technological capabilities (D-E) channels. △ Less","4 January, 2016",https://arxiv.org/pdf/1512.07250
Discovering the Skyline of Web Databases,Abolfazl Asudeh;Saravanan Thirumuruganathan;Nan Zhang;Gautam Das,"Many web databases are ""hidden"" behind proprietary search interfaces that enforce the top-k output constraint, i.e., each query returns at most k of all matching tuples, preferentially selected and returned according to a proprietary ranking function. In this paper, we initiate research into the novel problem of skyline discovery over top-k hidden web databases. Since skyline tuples provide critical insights into the database and include the top-ranked tuple for every possible ranking function following the monotonic order of attribute values, skyline discovery from a hidden web database can enable a wide variety of innovative third-party applications over one or multiple web databases. Our research in the paper shows that the critical factor affecting the cost of skyline discovery is the type of search interface controls provided by the website. As such, we develop efficient algorithms for three most popular types, i.e., one-ended range, free range and point predicates, and then combine them to support web databases that feature a mixture of these types. Rigorous theoretical analysis and extensive real-world online and offline experiments demonstrate the effectiveness of our proposed techniques and their superiority over baseline solutions. △ Less","20 March, 2016",https://arxiv.org/pdf/1512.02138
SentiBench - a benchmark comparison of state-of-the-practice sentiment analysis methods,Filipe Nunes Ribeiro;Matheus Araújo;Pollyanna Gonçalves;Fabrício Benevenuto;Marcos André Gonçalves,"In the last few years thousands of scientific papers have investigated sentiment analysis, several startups that measure opinions on real data have emerged and a number of innovative products related to this theme have been developed. There are multiple methods for measuring sentiments, including lexical-based and supervised machine learning methods. Despite the vast interest on the theme and wide popularity of some methods, it is unclear which one is better for identifying the polarity (i.e., positive or negative) of a message. Accordingly, there is a strong need to conduct a thorough apple-to-apple comparison of sentiment analysis methods, \textit{as they are used in practice}, across multiple datasets originated from different data sources. Such a comparison is key for understanding the potential limitations, advantages, and disadvantages of popular methods. This article aims at filling this gap by presenting a benchmark comparison of twenty-four popular sentiment analysis methods (which we call the state-of-the-practice methods). Our evaluation is based on a benchmark of eighteen labeled datasets, covering messages posted on social networks, movie and product reviews, as well as opinions and comments in news articles. Our results highlight the extent to which the prediction performance of these methods varies considerably across datasets. Aiming at boosting the development of this research area, we open the methods' codes and datasets used in this article, deploying them in a benchmark system, which provides an open API for accessing and comparing sentence-level sentiment analysis methods. △ Less","14 July, 2016",https://arxiv.org/pdf/1512.01818
PatchBatch: a Batch Augmented Loss for Optical Flow,David Gadot;Lior Wolf,"We propose a new pipeline for optical flow computation, based on Deep Learning techniques. We suggest using a Siamese CNN to independently, and in parallel, compute the descriptors of both images. The learned descriptors are then compared efficiently using the L2 norm and do not require network processing of patch pairs. The success of the method is based on an innovative loss function that computes higher moments of the loss distributions for each training batch. Combined with an Approximate Nearest Neighbor patch matching method and a flow interpolation technique, state of the art performance is obtained on the most challenging and competitive optical flow benchmarks. △ Less","10 April, 2016",https://arxiv.org/pdf/1512.01815
Compressive Sampling using Annihilating Filter-based Low-Rank Interpolation,Jong Chul Ye;Jong Min Kim;Kyong Hwan Jin;Kiryung Lee,"While the recent theory of compressed sensing provides an opportunity to overcome the Nyquist limit in recovering sparse signals, a solution approach usually takes a form of inverse problem of the unknown signal, which is crucially dependent on specific signal representation. In this paper, we propose a drastically different two-step Fourier compressive sampling framework in continuous domain that can be implemented as a measurement domain interpolation, after which a signal reconstruction can be done using classical analytic reconstruction methods. The main idea is originated from the fundamental duality between the sparsity in the primary space and the low-rankness of a structured matrix in the spectral domain, which shows that a low-rank interpolator in the spectral domain can enjoy all the benefit of sparse recovery with performance guarantees. Most notably, the proposed low-rank interpolation approach can be regarded as a generalization of recent spectral compressed sensing to recover large class of finite rate of innovations (FRI) signals at near optimal sampling rate. Moreover, for the case of cardinal representation, we can show that the proposed low-rank interpolation will benefit from inherent regularization and the optimal incoherence parameter. Using the powerful dual certificates and golfing scheme, we show that the new framework still achieves the near-optimal sampling rate for general class of FRI signal recovery, and the sampling rate can be further reduced for the class of cardinal splines. Numerical results using various type of FRI signals confirmed that the proposed low-rank interpolation approach has significant better phase transition than the conventional CS approaches. △ Less","26 September, 2016",https://arxiv.org/pdf/1511.08975
Position Estimation of Robotic Mobile Nodes in Wireless Testbed using GENI,Ahmed Abdelhadi;Felipe Rechia;Arvind Narayanan;Thiago Teixeira;Ricardo Lent;Driss Benhaddou;Hyunwoo Lee;T. Charles Clancy,"We present a low complexity experimental RF-based indoor localization system based on the collection and processing of WiFi RSSI signals and processing using a RSS-based multi-lateration algorithm to determine a robotic mobile node's location. We use a real indoor wireless testbed called w-iLab.t that is deployed in Zwijnaarde, Ghent, Belgium. One of the unique attributes of this testbed is that it provides tools and interfaces using Global Environment for Network Innovations (GENI) project to easily create reproducible wireless network experiments in a controlled environment. We provide a low complexity algorithm to estimate the location of the mobile robots in the indoor environment. In addition, we provide a comparison between some of our collected measurements with their corresponding location estimation and the actual robot location. The comparison shows an accuracy between 0.65 and 5 meters. △ Less","8 February, 2016",https://arxiv.org/pdf/1511.08936
Mapping Technology Space by Normalizing Patent Networks,Jeff Alstott;Giorgio Triulzi;Bowen Yan;Jianxi Luo,"Technology is a complex system, with technologies relating to each other in a space that can be mapped as a network. The technology network's structure can reveal properties of technologies and of human behavior, if it can be mapped accurately. Technology networks have been made from patent data, using several measures of proximity. These measures, however, are influenced by factors of the patenting system that do not reflect technologies or their proximity. We introduce a method to precisely normalize out multiple impinging factors in patent data and extract the true signal of technological proximity, by comparing the empirical proximity measures with what they would be in random situations that remove the impinging factors. With this method, we created technology networks, using data from 3.9 million patents. After normalization, different measures of proximity became more correlated with each other, approaching a single dimension of technological proximity. The normalized technology networks were sparse, with few pairs of technology domains being significantly related. The normalized network corresponded with human behavior: we analyzed the patenting histories of 2.8 million inventors and found they were more likely to invent in two different technology domains if the pair was closely related in the technology network. We also analyzed 250 thousand firms' patents and found that, in contrast, firms' inventive activities were only modestly associated with the technology network; firms' portfolios combined pairs of technology domains about twice as often as inventors. These results suggest that controlling for impinging factors provides meaningful measures of technological proximity for patent-based mapping of the technology space, and that this map can be used to aid in technology innovation planning and management. △ Less","15 June, 2016",https://arxiv.org/pdf/1509.07285
Adoption as a Social Marker: Innovation Diffusion with Outgroup Aversion,Paul E. Smaldino;Marco A. Janssen;Vicken Hillis;Jenna Bednar,"Social identities are among the key factors driving behavior in complex societies. Signals of social identity are known to influence individual behaviors in the adoption of innovations. Yet the population-level consequences of identity signaling on the diffusion of innovations are largely unknown. Here we use both analytical and agent-based modeling to consider the spread of a beneficial innovation in a structured population in which there exist two groups who are averse to being mistaken for each other. We investigate the dynamics of adoption and consider the role of structural factors such as demographic skew and communication scale on population-level outcomes. We find that outgroup aversion can lead to adoption being delayed or suppressed in one group, and that population-wide underadoption is common. Comparing the two models, we find that differential adoption can arise due to structural constraints on information flow even in the absence of intrinsic between-group differences in adoption rates. Further, we find that patterns of polarization in adoption at both local and global scales depend on the details of demographic organization and the scale of communication. This research has particular relevance to widely beneficial but identity-relevant products and behaviors, such as green technologies, where overall levels of adoption determine the positive benefits that accrue to society at large. △ Less","21 July, 2016",https://arxiv.org/pdf/1507.04775
Visibility-Aware Optimal Contagion of Malware Epidemics,Soheil Eshghi;Saswati Sarkar;Santosh S. Venkatesh,"Recent innovations in the design of computer viruses have led to new trade-offs for the attacker. Multiple variants of a malware may spread at different rates and have different levels of visibility to the network. In this work we examine the optimal strategies for the attacker so as to trade off the extent of spread of the malware against the need for stealth. We show that in the mean-field deterministic regime, this spread-stealth trade-off is optimized by computationally simple single-threshold policies. Specifically, we show that only one variant of the malware is spread by the attacker at each time, as there exists a time up to which the attacker prioritizes maximizing the spread of the malware, and after which she prioritizes stealth. △ Less","30 October, 2016",https://arxiv.org/pdf/1507.03528
Filtering Patent Maps for Visualization of Diversification Paths of Inventors and Organizations,Bowen Yan;Jianxi Luo,"In the information science literature, recent studies have used patent databases and patent classification information to construct network maps of patent technology classes. In such a patent technology map, almost all pairs of technology classes are connected, whereas most of the connections between them are extremely weak. This observation suggests the possibility of filtering the patent network map by removing weak links. However, removing links may reduce the explanatory power of the network on inventor or organization diversification. The network links may explain the patent portfolio diversification paths of inventors and inventing organizations. We measure the diversification explanatory power of the patent network map, and present a method to objectively choose an optimal trade-off between explanatory power and removing weak links. We show that this method can remove a degree of arbitrariness compared with previous filtering methods based on arbitrary thresholds, and also identify previous filtering methods that created filters outside the optimal trade-off. The filtered map aims to aid in network visualization analyses of the technological diversification of inventors, organizations and other innovation agents, and potential foresight analysis. Such applications to a prolific inventor (Leonard Forbes) and company (Google) are demonstrated. △ Less","25 June, 2016",https://arxiv.org/pdf/1507.01338
Adaptive Normalized Risk-Averting Training For Deep Neural Networks,Zhiguang Wang;Tim Oates;James Lo,"This paper proposes a set of new error criteria and learning approaches, Adaptive Normalized Risk-Averting Training (ANRAT), to attack the non-convex optimization problem in training deep neural networks (DNNs). Theoretically, we demonstrate its effectiveness on global and local convexity lower-bounded by the standard L_p-norm error. By analyzing the gradient on the convexity index λ, we explain the reason why to learn λ adaptively using gradient descent works. In practice, we show how this method improves training of deep neural networks to solve visual recognition tasks on the MNIST and CIFAR-10 datasets. Without using pretraining or other tricks, we obtain results comparable or superior to those reported in recent literature on the same tasks using standard ConvNets + MSE/cross entropy. Performance on deep/shallow multilayer perceptrons and Denoised Auto-encoders is also explored. ANRAT can be combined with other quasi-Newton training methods, innovative network variants, regularization techniques and other specific tricks in DNNs. Other than unsupervised pretraining, it provides a new perspective to address the non-convex optimization problem in DNNs. △ Less","9 June, 2016",https://arxiv.org/pdf/1506.02690
M-Flash: Fast Billion-scale Graph Computation Using a Bimodal Block Processing Model,Hugo Gualdron;Robson Cordeiro;Jose Rodrigues-Jr;Duen Chau;Minsuk Kahng;U Kang,"Recent graph computation approaches have demonstrated that a single PC can perform efficiently on billion-scale graphs. While these approaches achieve scalability by optimizing I/O operations, they do not fully exploit the capabilities of modern hard drives and processors. To overcome their performance, in this work, we introduce the Bimodal Block Processing (BBP), an innovation that is able to boost the graph computation by minimizing the I/O cost even further. With this strategy, we achieved the following contributions: (1) M-Flash, the fastest graph computation framework to date; (2) a flexible and simple programming model to easily implement popular and essential graph algorithms, including the first single-machine billion-scale eigensolver; and (3) extensive experiments on real graphs with up to 6.6 billion edges, demonstrating M-Flash's consistent and significant speedup. △ Less","14 September, 2016",https://arxiv.org/pdf/1506.01406
ORFEL: efficient detection of defamation or illegitimate promotion in online recommendation,Gabriel Gimenes;Robson Cordeiro;Jose F. Rodrigues-Jr,"What if a successful company starts to receive a torrent of low-valued (one or two stars) recommendations in its mobile apps from multiple users within a short (say one month) period of time? Is it legitimate evidence that the apps have lost in quality, or an intentional plan (via lockstep behavior) to steal market share through defamation? In the case of a systematic attack to one's reputation, it might not be possible to manually discern between legitimate and fraudulent interaction within the huge universe of possibilities of user-product recommendation. Previous works have focused on this issue, but none of them took into account the context, modeling, and scale that we consider in this paper. Here, we propose the novel method Online-Recommendation Fraud ExcLuder (ORFEL) to detect defamation and/or illegitimate promotion of online products by using vertex-centric asynchronous parallel processing of bipartite (users-products) graphs. With an innovative algorithm, our results demonstrate both efficacy and efficiency -- over 95% of potential attacks were detected, and ORFEL was at least two orders of magnitude faster than the state-of-the-art. Over a novel methodology, our main contributions are: (1) a new algorithmic solution; (2) one scalable approach; and (3) a novel context and modeling of the problem, which now addresses both defamation and illegitimate promotion. Our work deals with relevant issues of the Web 2.0, potentially augmenting the credibility of online recommendation to prevent losses to both customers and vendors. △ Less","23 November, 2016",https://arxiv.org/pdf/1505.06747
Machine learning based data mining for Milky Way filamentary structures reconstruction,Giuseppe Riccio;Stefano Cavuoti;Eugenio Schisano;Massimo Brescia;Amata Mercurio;Davide Elia;Milena Benedettini;Stefano Pezzuto;Sergio Molinari;Anna Maria Di Giorgio,"We present an innovative method called FilExSeC (Filaments Extraction, Selection and Classification), a data mining tool developed to investigate the possibility to refine and optimize the shape reconstruction of filamentary structures detected with a consolidated method based on the flux derivative analysis, through the column-density maps computed from Herschel infrared Galactic Plane Survey (Hi-GAL) observations of the Galactic plane. The present methodology is based on a feature extraction module followed by a machine learning model (Random Forest) dedicated to select features and to classify the pixels of the input images. From tests on both simulations and real observations the method appears reliable and robust with respect to the variability of shape and distribution of filaments. In the cases of highly defined filament structures, the presented method is able to bridge the gaps among the detected fragments, thus improving their shape reconstruction. From a preliminary ""a posteriori"" analysis of derived filament physical parameters, the method appears potentially able to add a sufficient contribution to complete and refine the filament reconstruction. △ Less","11 October, 2016",https://arxiv.org/pdf/1505.06621
What Is an Emerging Technology?,Daniele Rotolo;Diana Hicks;Ben R. Martin,"There is considerable and growing interest in the emergence of novel technologies, especially from the policy-making perspective. Yet as an area of study, emerging technologies lacks key foundational elements, namely a consensus on what classifies a technology as 'emergent' and strong research designs that operationalize central theoretical concepts. The present paper aims to fill this gap by developing a definition of 'emerging technologies' and linking this conceptual effort with the development of a framework for the operationalisation of technological emergence. The definition is developed by combining a basic understanding of the term and in particular the concept of 'emergence' with a review of key innovation studies dealing with definitional issues of technological emergence. The resulting definition identifies five attributes that feature in the emergence of novel technologies. These are: (i) radical novelty, (ii) relatively fast growth, (iii) coherence, (iv) prominent impact, and (v) uncertainty and ambiguity. The framework for operationalising emerging technologies is then elaborated on the basis of the proposed attributes. To do so, we identify and review major empirical approaches (mainly in, although not limited to, the scientometric domain) for the detection and study of emerging technologies (these include indicators and trend analysis, citation analysis, co-word analysis, overlay mapping, and combinations thereof) and elaborate on how these can be used to operationalise the different attributes of emergence. △ Less","4 January, 2016",https://arxiv.org/pdf/1503.00673
Learning Precise Spike Train to Spike Train Transformations in Multilayer Feedforward Neuronal Networks,Arunava Banerjee,"We derive a synaptic weight update rule for learning temporally precise spike train to spike train transformations in multilayer feedforward networks of spiking neurons. The framework, aimed at seamlessly generalizing error backpropagation to the deterministic spiking neuron setting, is based strictly on spike timing and avoids invoking concepts pertaining to spike rates or probabilistic models of spiking. The derivation is founded on two innovations. First, an error functional is proposed that compares the spike train emitted by the output neuron of the network to the desired spike train by way of their putative impact on a virtual postsynaptic neuron. This formulation sidesteps the need for spike alignment and leads to closed form solutions for all quantities of interest. Second, virtual assignment of weights to spikes rather than synapses enables a perturbation analysis of individual spike times and synaptic weights of the output as well as all intermediate neurons in the network, which yields the gradients of the error functional with respect to the said entities. Learning proceeds via a gradient descent mechanism that leverages these quantities. Simulation experiments demonstrate the efficacy of the proposed learning framework. The experiments also highlight asymmetries between synapses on excitatory and inhibitory neurons. △ Less","8 January, 2016",https://arxiv.org/pdf/1412.4210
Maximizing Social Influence in Nearly Optimal Time,Christian Borgs;Michael Brautbar;Jennifer Chayes;Brendan Lucier,"Diffusion is a fundamental graph process, underpinning such phenomena as epidemic disease contagion and the spread of innovation by word-of-mouth. We address the algorithmic problem of finding a set of k initial seed nodes in a network so that the expected size of the resulting cascade is maximized, under the standard independent cascade model of network diffusion. Runtime is a primary consideration for this problem due to the massive size of the relevant input networks. We provide a fast algorithm for the influence maximization problem, obtaining the near-optimal approximation factor of (1 - 1/e - epsilon), for any epsilon > 0, in time O((m+n)k log(n) / epsilon^2). Our algorithm is runtime-optimal (up to a logarithmic factor) and substantially improves upon the previously best-known algorithms which run in time Omega(mnk POLY(1/epsilon)). Furthermore, our algorithm can be modified to allow early termination: if it is terminated after O(beta(m+n)k log(n)) steps for some beta < 1 (which can depend on n), then it returns a solution with approximation factor O(beta). Finally, we show that this runtime is optimal (up to logarithmic factors) for any beta and fixed seed size k. △ Less","21 June, 2016",https://arxiv.org/pdf/1212.0884
Revolvable Indoor Panoramas Using a Rectified Azimuthal Projection,Chamberlain Fong,"We present an algorithm for converting an indoor spherical panorama into a photograph with a simulated overhead view. The resulting image will have an extremely wide field of view covering up to 4π steradians of the spherical panorama. We argue that our method complements the stereographic projection commonly used in the ""little planet"" effect. The stereographic projection works well in creating little planets of outdoor scenes; whereas our method is a well-suited counterpart for indoor scenes. The main innovation of our method is the introduction of a novel azimuthal map projection that can smoothly blend between the stereographic projection and the Lambert azimuthal equal-area projection. Our projection has an adjustable parameter that allows one to control and compromise between distortions in shape and distortions in size within the projected panorama. This extra control parameter gives our projection the ability to produce superior results over the stereographic projection. △ Less","1 May, 2016",https://arxiv.org/pdf/1206.2068
