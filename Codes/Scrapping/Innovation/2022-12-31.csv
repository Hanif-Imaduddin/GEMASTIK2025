title,authors,abstract,submitted_date,pdf_link
Categorisation of future applications for Augmented Reality in human lunar exploration,Paul Topf Aguiar de Medeiros;Paul Njayou;Flavie Rometsch;Tommy Nilsson;Leonie Becker;Aidan Cowley,"The European Space Agency (ESA) has a clear mission to go forward to the Moon in preparation of human presence on Mars. One of the technologies looked at to increase safety and efficiency of astronauts in this context is Augmented Reality (AR). This technology allows digital visual information to be overlaid onto the user's environment through some type of display or projector. In recent years separate studies have been conducted to test the potential value of AR for astronauts by implementing a few functionalities on an AR display followed by testing in terrestrial analogue environments. One of the groups contributing to these investigations is Spaceship EAC (SSEAC). SSEAC is a group of interns and trainees at the European Astronaut Centre (EAC) focusing on emerging technologies for human space exploration. This paper presents an outcome of SSEAC's activities related to AR for lunar extravehicular activities (EVAs), in which an approach similar to design thinking was used to explore, identify, and structure the opportunities offered by this technology. The resulting categorization of AR use cases can be used to identify new functionalities to test through prototyping and usability tests and can also be used to relate individual studies to each other to gain insight into the overall potential value AR has to offer to human lunar exploration. The approach adopted in this paper is based on the Fuzzy Front End (FFE) model from the innovation management domain. Utilising a user-driven instead of technology-driven method resulted in findings that are relevant irrespective of the hardware and software implementation. Instead, the outcome is an overview of use cases in which some type of AR system could provide value by contributing to increased astronaut safety, efficiency and/or efficacy. △ Less","19 November, 2022",https://arxiv.org/pdf/2301.00838
A deep real options policy for sequential service region design and timing,Srushti Rath;Joseph Y. J. Chow,"As various city agencies and mobility operators navigate toward innovative mobility solutions, there is a need for strategic flexibility in well-timed investment decisions in the design and timing of mobility service regions, i.e. cast as ""real options"" (RO). This problem becomes increasingly challenging with multiple interacting RO in such investments. We propose a scalable machine learning based RO framework for multi-period sequential service region design & timing problem for mobility-on-demand services, framed as a Markov decision process with non-stationary stochastic variables. A value function approximation policy from literature uses multi-option least squares Monte Carlo simulation to get a policy value for a set of interdependent investment decisions as deferral options (CR policy). The goal is to determine the optimal selection and timing of a set of zones to include in a service region. However, prior work required explicit enumeration of all possible sequences of investments. To address the combinatorial complexity of such enumeration, we propose a new variant ""deep"" RO policy using an efficient recurrent neural network (RNN) based ML method (CR-RNN policy) to sample sequences to forego the need for enumeration, making network design & timing policy tractable for large scale implementation. Experiments on multiple service region scenarios in New York City (NYC) shows the proposed policy substantially reduces the overall computational cost (time reduction for RO evaluation of > 90% of total investment sequences is achieved), with zero to near-zero gap compared to the benchmark. A case study of sequential service region design for expansion of MoD services in Brooklyn, NYC show that using the CR-RNN policy to determine optimal RO investment strategy yields a similar performance (0.5% within CR policy value) with significantly reduced computation time (about 5.4 times faster). △ Less","30 December, 2022",https://arxiv.org/pdf/2212.14800
Countering Malicious Content Moderation Evasion in Online Social Networks: Simulation and Detection of Word Camouflage,Álvaro Huertas-García;Alejandro Martín;Javier Huertas Tato;David Camacho,"Content moderation is the process of screening and monitoring user-generated content online. It plays a crucial role in stopping content resulting from unacceptable behaviors such as hate speech, harassment, violence against specific groups, terrorism, racism, xenophobia, homophobia, or misogyny, to mention some few, in Online Social Platforms. These platforms make use of a plethora of tools to detect and manage malicious information; however, malicious actors also improve their skills, developing strategies to surpass these barriers and continuing to spread misleading information. Twisting and camouflaging keywords are among the most used techniques to evade platform content moderation systems. In response to this recent ongoing issue, this paper presents an innovative approach to address this linguistic trend in social networks through the simulation of different content evasion techniques and a multilingual Transformer model for content evasion detection. In this way, we share with the rest of the scientific community a multilingual public tool, named ""pyleetspeak"" to generate/simulate in a customizable way the phenomenon of content evasion through automatic word camouflage and a multilingual Named-Entity Recognition (NER) Transformer-based model tuned for its recognition and detection. The multilingual NER model is evaluated in different textual scenarios, detecting different types and mixtures of camouflage techniques, achieving an overall weighted F1 score of 0.8795. This article contributes significantly to countering malicious information by developing multilingual tools to simulate and detect new methods of evasion of content on social networks, making the fight against information disorders more effective. △ Less","27 December, 2022",https://arxiv.org/pdf/2212.14727
POIBERT: A Transformer-based Model for the Tour Recommendation Problem,Ngai Lam Ho;Kwan Hui Lim,"Tour itinerary planning and recommendation are challenging problems for tourists visiting unfamiliar cities. Many tour recommendation algorithms only consider factors such as the location and popularity of Points of Interest (POIs) but their solutions may not align well with the user's own preferences and other location constraints. Additionally, these solutions do not take into consideration of the users' preference based on their past POIs selection. In this paper, we propose POIBERT, an algorithm for recommending personalized itineraries using the BERT language model on POIs. POIBERT builds upon the highly successful BERT language model with the novel adaptation of a language model to our itinerary recommendation task, alongside an iterative approach to generate consecutive POIs. Our recommendation algorithm is able to generate a sequence of POIs that optimizes time and users' preference in POI categories based on past trajectories from similar tourists. Our tour recommendation algorithm is modeled by adapting the itinerary recommendation problem to the sentence completion problem in natural language processing (NLP). We also innovate an iterative algorithm to generate travel itineraries that satisfies the time constraints which is most likely from past trajectories. Using a Flickr dataset of seven cities, experimental results show that our algorithm out-performs many sequence prediction algorithms based on measures in recall, precision and F1-scores. △ Less","16 December, 2022",https://arxiv.org/pdf/2212.13900
Saliency-Augmented Memory Completion for Continual Learning,Guangji Bai;Chen Ling;Yuyang Gao;Liang Zhao,"Continual Learning is considered a key step toward next-generation Artificial Intelligence. Among various methods, replay-based approaches that maintain and replay a small episodic memory of previous samples are one of the most successful strategies against catastrophic forgetting. However, since forgetting is inevitable given bounded memory and unbounded tasks, how to forget is a problem continual learning must address. Therefore, beyond simply avoiding catastrophic forgetting, an under-explored issue is how to reasonably forget while ensuring the merits of human memory, including 1. storage efficiency, 2. generalizability, and 3. some interpretability. To achieve these simultaneously, our paper proposes a new saliency-augmented memory completion framework for continual learning, inspired by recent discoveries in memory completion separation in cognitive neuroscience. Specifically, we innovatively propose to store the part of the image most important to the tasks in episodic memory by saliency map extraction and memory encoding. When learning new tasks, previous data from memory are inpainted by an adaptive data generation module, which is inspired by how humans complete episodic memory. The module's parameters are shared across all tasks and it can be jointly trained with a continual learning classifier as bilevel optimization. Extensive experiments on several continual learning and image classification benchmarks demonstrate the proposed method's effectiveness and efficiency. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13242
"""Just a Normal Day in the Metaverse"" -- Distraction Conflicts of Knowledge Work in Virtual Environments",Julian Marx;Jonas Rieskamp;Milad Mirbabaie,"The changing nature of knowledge work creates demands for emerging technologies as enablers for workplace innovation. One emerging technology to potentially remedy drawbacks of remote work arrangements are meta-verses that merge physical reality with digital virtuality. In the literature, such innovations in the knowledge work sector have been primarily examined against the backdrop of collaboration as a dependent variable. In this paper, however, we investigate knowledge work in metaverses from a distraction-conflict perspective because independent, uninterrupted activities are as much characteristic of knowledge work as collaboration. Preliminary findings show that knowledge workers in meta-verses experience arousal from the 1) presence, appearance, and behaviour of other avatars, 2) realism, novelty, and affordances of the virtual environment, and 3) technological friction and navigation. This work has the theoretical implication that distraction-conflict theory must be extended to incorporate additional sources of arousal when applied to the context of knowledge work in metaverses. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13063
OMSN and FAROS: OCTA Microstructure Segmentation Network and Fully Annotated Retinal OCTA Segmentation Dataset,Peng Xiao;Xiaodong Hu;Ke Ma;Gengyuan Wang;Ziqing Feng;Yuancong Huang;Jin Yuan,"The lack of efficient segmentation methods and fully-labeled datasets limits the comprehensive assessment of optical coherence tomography angiography (OCTA) microstructures like retinal vessel network (RVN) and foveal avascular zone (FAZ), which are of great value in ophthalmic and systematic diseases evaluation. Here, we introduce an innovative OCTA microstructure segmentation network (OMSN) by combining an encoder-decoder-based architecture with multi-scale skip connections and the split-attention-based residual network ResNeSt, paying specific attention to OCTA microstructural features while facilitating better model convergence and feature representations. The proposed OMSN achieves excellent single/multi-task performances for RVN or/and FAZ segmentation. Especially, the evaluation metrics on multi-task models outperform single-task models on the same dataset. On this basis, a fully annotated retinal OCTA segmentation (FAROS) dataset is constructed semi-automatically, filling the vacancy of a pixel-level fully-labeled OCTA dataset. OMSN multi-task segmentation model retrained with FAROS further certifies its outstanding accuracy for simultaneous RVN and FAZ segmentation. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.13059
Influence of AI in human lives,Meenu Varghese;Satheesh Raj;Vigneshwaran Venkatesh,"Artificial Intelligence is one of the most significant and prominent technological innovations which has reshaped all aspects of human life on the lines of ease from magnitudes like shopping, data collection, driving, everyday life, medical approach and many more. On the contrary, although recent developments in both subjects that are backed by technology, progress on AI alongside CE must have mostly been undertaken in isolation, providing little understanding into how the two areas intersect. Artificial intelligence is now widely used in services, from back-office tasks to front-line interactions with customers. This trend has accelerated in recent years. Artificial intelligence (AI)-based virtual assistants are changing successful engagement away from being dominated by humans and toward being dominated by technologies. As a result, people are expected to solve their own problems before calling customer care representatives, eventually emerging as a crucial component of providing services as value co-creators. AI-powered chats may potentially go awry, which could enrage, perplex, and anger customers. Considering all these, the main objectives of this study will engage the following 1. To identify the alterations in the scope of human searches for information offered by the application of AI? 2. To analyse how AI helps in the way someone drives the car 3. To evaluate how AI has changed the way customer interact with the customers △ Less","15 December, 2022",https://arxiv.org/pdf/2212.12305
Time to Market Reduction for Hydrogen Fuel Cell Stacks using Generative Adversarial Networks,Nicolas Morizet;Perceval Desforges;Christophe Geissler;Elodie Pahon;Samir Jemeï;Daniel Hissel,"To face the dependency on fossil fuels and limit carbon emissions, fuel cells are a very promising technology and appear to be a key candidate to tackle the increase of the energy demand and promote the energy transition. To meet future needs for both transport and stationary applications, the time to market of fuel cell stacks must be drastically reduced. Here, a new concept to shorten their development time by introducing a disruptive and highefficiency data augmentation approach based on artificial intelligence is presented. Our results allow reducing the testing time before introducing a product on the market from a thousand to a few hours. The innovative concept proposed here can support engineering and research tasks during the fuel cell development process to achieve decreased development costs alongside a reduced time to market. △ Less","22 December, 2022",https://arxiv.org/pdf/2212.11733
DuAT: Dual-Aggregation Transformer Network for Medical Image Segmentation,Feilong Tang;Qiming Huang;Jinfeng Wang;Xianxu Hou;Jionglong Su;Jingxin Liu,"Transformer-based models have been widely demonstrated to be successful in computer vision tasks by modelling long-range dependencies and capturing global representations. However, they are often dominated by features of large patterns leading to the loss of local details (e.g., boundaries and small objects), which are critical in medical image segmentation. To alleviate this problem, we propose a Dual-Aggregation Transformer Network called DuAT, which is characterized by two innovative designs, namely, the Global-to-Local Spatial Aggregation (GLSA) and Selective Boundary Aggregation (SBA) modules. The GLSA has the ability to aggregate and represent both global and local spatial features, which are beneficial for locating large and small objects, respectively. The SBA module is used to aggregate the boundary characteristic from low-level features and semantic information from high-level features for better preserving boundary details and locating the re-calibration objects. Extensive experiments in six benchmark datasets demonstrate that our proposed model outperforms state-of-the-art methods in the segmentation of skin lesion images, and polyps in colonoscopy images. In addition, our approach is more robust than existing methods in various challenging situations such as small object segmentation and ambiguous object boundaries. △ Less","21 December, 2022",https://arxiv.org/pdf/2212.11677
BDSP: A Fair Blockchain-enabled Framework for Privacy-Enhanced Enterprise Data Sharing,Lam Duc Nguyen;James Hoang;Qin Wang;Qinghua Lu;Sherry Xu;Shiping Chen,"Across industries, there is an ever-increasing rate of data sharing for collaboration and innovation between organizations and their customers, partners, suppliers, and internal teams. However, many enterprises are restricted from freely sharing data due to regulatory restrictions across different regions, performance issues in moving large volume data, or requirements to maintain autonomy. In such situations, the enterprise can benefit from the concept of federated learning, in which machine learning models are constructed at various geographic sites. In this paper, we introduce a general framework, namely BDSP, to share data among enterprises based on Blockchain and federated learning techniques. Specifically, we propose a transparency contribution accounting mechanism to estimate the valuation of data and implement a proof-of-concept for further evaluation. The extensive experimental results show that the proposed BDSP has a competitive performance with higher training accuracy, an increase of over 5%, and lower communication overhead, reducing 3 times, compared to baseline approaches. △ Less","16 December, 2022",https://arxiv.org/pdf/2212.11128
THMA: Tencent HD Map AI System for Creating HD Map Annotations,Kun Tang;Xu Cao;Zhipeng Cao;Tong Zhou;Erlong Li;Ao Liu;Shengtao Zou;Chang Liu;Shuqi Mei;Elena Sizikova;Chao Zheng,"Nowadays, autonomous vehicle technology is becoming more and more mature. Critical to progress and safety, high-definition (HD) maps, a type of centimeter-level map collected using a laser sensor, provide accurate descriptions of the surrounding environment. The key challenge of HD map production is efficient, high-quality collection and annotation of large-volume datasets. Due to the demand for high quality, HD map production requires significant manual human effort to create annotations, a very time-consuming and costly process for the map industry. In order to reduce manual annotation burdens, many artificial intelligence (AI) algorithms have been developed to pre-label the HD maps. However, there still exists a large gap between AI algorithms and the traditional manual HD map production pipelines in accuracy and robustness. Furthermore, it is also very resource-costly to build large-scale annotated datasets and advanced machine learning algorithms for AI-based HD map automatic labeling systems. In this paper, we introduce the Tencent HD Map AI (THMA) system, an innovative end-to-end, AI-based, active learning HD map labeling system capable of producing and labeling HD maps with a scale of hundreds of thousands of kilometers. In THMA, we train AI models directly from massive HD map datasets via supervised, self-supervised, and weakly supervised learning to achieve high accuracy and efficiency required by downstream users. THMA has been deployed by the Tencent Map team to provide services to downstream companies and users, serving over 1,000 labeling workers and producing more than 30,000 kilometers of HD map data per day at most. More than 90 percent of the HD map data in Tencent Map is labeled automatically by THMA, accelerating the traditional HD map labeling process by more than ten times. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.11123
Space-Terrestrial Cooperation Over Spatially Correlated Channels Relying on Imperfect Channel Estimates: Uplink Performance Analysis and Optimization,Trinh Van Chien;Eva Lagunas;Tiep M. Hoang;Symeon Chatzinotas;Björn Ottersten;Lajos Hanzo,"A whole suite of innovative technologies and architectures have emerged in response to the rapid growth of wireless traffic. This paper studies an integrated network design that boosts system capacity through cooperation between wireless access points (APs) and a satellite for enhancing the network's spectral efficiency. We first mathematically derive an achievable throughput expression for the uplink (UL) data transmission over spatially correlated Rician channels. Our generic achievable throughput expression is applicable for arbitrary received signal detection techniques under realistic imperfect channel estimates. A closed-form expression is then obtained for the ergodic UL data throughput when maximum ratio combining is utilized for detecting the desired signals. As for our resource allocation contributions, we formulate the max-min fairness and total transmit power optimization problems relying on the channel statistics for performing power allocation. The solution of each optimization problem is derived in form of a low-complexity iterative design, in which each data power variable is updated relying on a closed-form expression. Our integrated hybrid network concept allows users to be served that may not otherwise be accommodated due to the excessive data demands. The algorithms proposed to allow us to address the congestion issues appearing when at least one user is served at a rate below the target. The mathematical analysis is also illustrated with the aid of our numerical results that show the added benefits of considering the space links in terms of improving the ergodic data throughput. Furthermore, the proposed algorithms smoothly circumvent any potential congestion, especially in face of high rate requirements and weak channel conditions. △ Less","21 December, 2022",https://arxiv.org/pdf/2212.10828
Spoken Language Understanding for Conversational AI: Recent Advances and Future Direction,Soyeon Caren Han;Siqu Long;Henry Weld;Josiah Poon,"When a human communicates with a machine using natural language on the web and online, how can it understand the human's intention and semantic context of their talk? This is an important AI task as it enables the machine to construct a sensible answer or perform a useful action for the human. Meaning is represented at the sentence level, identification of which is known as intent detection, and at the word level, a labelling task called slot filling. This dual-level joint task requires innovative thinking about natural language and deep learning network design, and as a result, many approaches and models have been proposed and applied. This tutorial will discuss how the joint task is set up and introduce Spoken Language Understanding/Natural Language Understanding (SLU/NLU) with Deep Learning techniques. We will cover the datasets, experiments and metrics used in the field. We will describe how the machine uses the latest NLP and Deep Learning techniques to address the joint task, including recurrent and attention-based Transformer networks and pre-trained models (e.g. BERT). We will then look in detail at a network that allows the two levels of the task, intent classification and slot filling, to interact to boost performance explicitly. We will do a code demonstration of a Python notebook for this model and attendees will have an opportunity to watch coding demo tasks on this joint NLU to further their understanding. △ Less","20 December, 2022",https://arxiv.org/pdf/2212.10728
Co-designing for a Hybrid Workplace Experience in Software Development,Zhendong Wang;Yi-Hung Chou;Kayla Fathi;Tobias Schimmer;Peter Colligan;David Redmiles;Rafael Prikladnicki,"With increasing demands for flexible work models, many IT organizations have adapted to hybrid work that promises enhanced team productivity as well as work satisfaction. To achieve productive engineering practice, collaborative product innovation, and effective mentorship in the ensuing hybrid work, we introduce a workshop approach on co-designing for a hybrid workplace experience and provide implications for continuously improving collaborative software development at scale. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09638
Towards Assessing Data Bias in Clinical Trials,Chiara Criscuolo;Tommaso Dolci;Mattia Salnitri,"Algorithms and technologies are essential tools that pervade all aspects of our daily lives. In the last decades, health care research benefited from new computer-based recruiting methods, the use of federated architectures for data storage, the introduction of innovative analyses of datasets, and so on. Nevertheless, health care datasets can still be affected by data bias. Due to data bias, they provide a distorted view of reality, leading to wrong analysis results and, consequently, decisions. For example, in a clinical trial that studied the risk of cardiovascular diseases, predictions were wrong due to the lack of data on ethnic minorities. It is, therefore, of paramount importance for researchers to acknowledge data bias that may be present in the datasets they use, eventually adopt techniques to mitigate them and control if and how analyses results are impacted. This paper proposes a method to address bias in datasets that: (i) defines the types of data bias that may be present in the dataset, (ii) characterizes and quantifies data bias with adequate metrics, (iii) provides guidelines to identify, measure, and mitigate data bias for different data sources. The method we propose is applicable both for prospective and retrospective clinical trials. We evaluate our proposal both through theoretical considerations and through interviews with researchers in the health care environment. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09633
Exploring a multi_stage feedback teaching mode for graduate students of software engineering discipline based on project_driven competition,Xiangdong Pei;Rui Zhang,"Aiming at the current problems of theory-oriented,practice-light,and lack of innovation ability in the teaching of postgraduate software engineering courses,a multi-stage feedback teaching mode for software engineering postgraduates based on competition project_driven is proposed. The model is driven by the competition project,and implementing suggestions are given in terms of stage allocation of software engineering course tasks and ability cultivation,competition case design and process evaluation improvement,etc. Through the implementation of this teaching mode,students enthusiasm and initiative are expected to be stimulated,and the overall development of students professional skills and comprehension ability would be improved to meet the demand of society for software engineering technical talents. △ Less","19 December, 2022",https://arxiv.org/pdf/2212.09394
A Study on the Intersection of GPU Utilization and CNN Inference,Jack Kosaian;Amar Phanishayee,"There has been significant progress in developing neural network architectures that both achieve high predictive performance and that also achieve high application-level inference throughput (e.g., frames per second). Another metric of increasing importance is GPU utilization during inference: the measurement of how well a deployed neural network uses the computational capabilities of the GPU on which it runs. Achieving high GPU utilization is critical to increasing application-level throughput and ensuring a good return on investment for deploying GPUs. This paper analyzes the GPU utilization of convolutional neural network (CNN) inference. We first survey the GPU utilization of CNNs to show that there is room to improve the GPU utilization of many of these CNNs. We then investigate the GPU utilization of networks within a neural architecture search (NAS) search space, and explore how using GPU utilization as a metric could potentially be used to accelerate NAS itself. Our study makes the case that there is room to improve the inference-time GPU utilization of CNNs and that knowledge of GPU utilization has the potential to benefit even applications that do not target utilization itself. We hope that the results of this study will spur future innovation in designing GPU-efficient neural networks. △ Less","15 December, 2022",https://arxiv.org/pdf/2212.07936
Assessing the Maturity of Digital Twinning Solutions for Ports,Robert Klar;Anna Fredriksson;Vangelis Angelakis,"Ports are striving for innovative technological solutions to cope with the increasing growth in demand of goods transport, while at the same time improving their environmental footprint. An emerging technology that has the potential to substantially increase the effectiveness of the multifaceted and interconnected port processes is that of digital twins. Innovation-leading ports recognizing the potential of twinning have already started working on it. However, since there is no clear consensus on what a digital twin of a complex system comprises and how it should be designed, deployed digital twin solutions for ports often differ significantly. This article addresses this issue by initially identifying three core aspect underpinning digital twins of complex systems, such as ports, and outlining five successive maturity levels based on these aspects' instantiation. These identified aspects and the derived maturity levels are then used to examine real-world cases by critically evaluating existing digital twinning solutions in the port of Singapore, the Mawan port of Shanghai, and that of Rotterdam. These being three of the world's innovation-leading ports, we naturally find in them most of the identified core aspects to be in line with their twinning implementation, which has reached, in all three, a higher level of maturity. Although, our work on maturity levels and core aspects can provide a guideline for designing and benchmarking future digital twinning solutions for ports, the capacity for innovation via twinning, even in the port domain, is highly contextual with key paragon being the availability of financial and technical resources. △ Less","15 December, 2022",https://arxiv.org/pdf/2212.07722
Triangulating Python Performance Issues with Scalene,Emery D. Berger;Sam Stern;Juan Altmayer Pizzorno,"This paper proposes Scalene, a profiler specialized for Python. Scalene combines a suite of innovations to precisely and simultaneously profile CPU, memory, and GPU usage, all with low overhead. Scalene's CPU and memory profilers help Python programmers direct their optimization efforts by distinguishing between inefficient Python and efficient native execution time and memory usage. Scalene's memory profiler employs a novel sampling algorithm that lets it operate with low overhead yet high precision. It also incorporates a novel algorithm that automatically pinpoints memory leaks, whether within Python or across the Python-native boundary. Scalene tracks a new metric called copy volume, which highlights costly copying operations that can occur when Python silently converts between C and Python data representations, or between CPU and GPU. Since its introduction, Scalene has been widely adopted, with over 500,000 downloads to date. We present experience reports from developers who used Scalene to achieve significant performance improvements and memory savings. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.07597
Solve the Puzzle of Instance Segmentation in Videos: A Weakly Supervised Framework with Spatio-Temporal Collaboration,Liqi Yan;Qifan Wang;Siqi Ma;Jingang Wang;Changbin Yu,"Instance segmentation in videos, which aims to segment and track multiple objects in video frames, has garnered a flurry of research attention in recent years. In this paper, we present a novel weakly supervised framework with \textbf{S}patio-\textbf{T}emporal \textbf{C}ollaboration for instance \textbf{Seg}mentation in videos, namely \textbf{STC-Seg}. Concretely, STC-Seg demonstrates four contributions. First, we leverage the complementary representations from unsupervised depth estimation and optical flow to produce effective pseudo-labels for training deep networks and predicting high-quality instance masks. Second, to enhance the mask generation, we devise a puzzle loss, which enables end-to-end training using box-level annotations. Third, our tracking module jointly utilizes bounding-box diagonal points with spatio-temporal discrepancy to model movements, which largely improves the robustness to different object appearances. Finally, our framework is flexible and enables image-level instance segmentation methods to operate the video-level task. We conduct an extensive set of experiments on the KITTI MOTS and YT-VIS datasets. Experimental results demonstrate that our method achieves strong performance and even outperforms fully supervised TrackR-CNN and MaskTrack R-CNN. We believe that STC-Seg can be a valuable addition to the community, as it reflects the tip of an iceberg about the innovative opportunities in the weakly supervised paradigm for instance segmentation in videos. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.07592
Child PalmID: Contactless Palmprint Recognition,Anil K. Jain;Akash Godbole;Anjoo Bhatnagar;Prem Sewak Sudhish,"Developing and least developed countries face the dire challenge of ensuring that each child in their country receives required doses of vaccination, adequate nutrition and proper medication. International agencies such as UNICEF, WHO and WFP, among other organizations, strive to find innovative solutions to determine which child has received the benefits and which have not. Biometric recognition systems have been sought out to help solve this problem. To that end, this report establishes a baseline accuracy of a commercial contactless palmprint recognition system that may be deployed for recognizing children in the age group of one to five years old. On a database of contactless palmprint images of one thousand unique palms from 500 children, we establish SOTA authentication accuracy of 90.85% @ FAR of 0.01%, rank-1 identification accuracy of 99.0% (closed set), and FPIR=0.01 @ FNIR=0.3 for open-set identification using PalmMobile SDK from Armatura. △ Less","14 December, 2022",https://arxiv.org/pdf/2212.07299
Improving Accuracy Without Losing Interpretability: A ML Approach for Time Series Forecasting,Yiqi Sun;Zhengxin Shi;Jianshen Zhang;Yongzhi Qi;Hao Hu;Zuojun Max Shen,"In time series forecasting, decomposition-based algorithms break aggregate data into meaningful components and are therefore appreciated for their particular advantages in interpretability. Recent algorithms often combine machine learning (hereafter ML) methodology with decomposition to improve prediction accuracy. However, incorporating ML is generally considered to sacrifice interpretability inevitably. In addition, existing hybrid algorithms usually rely on theoretical models with statistical assumptions and focus only on the accuracy of aggregate predictions, and thus suffer from accuracy problems, especially in component estimates. In response to the above issues, this research explores the possibility of improving accuracy without losing interpretability in time series forecasting. We first quantitatively define interpretability for data-driven forecasts and systematically review the existing forecasting algorithms from the perspective of interpretability. Accordingly, we propose the W-R algorithm, a hybrid algorithm that combines decomposition and ML from a novel perspective. Specifically, the W-R algorithm replaces the standard additive combination function with a weighted variant and uses ML to modify the estimates of all components simultaneously. We mathematically analyze the theoretical basis of the algorithm and validate its performance through extensive numerical experiments. In general, the W-R algorithm outperforms all decomposition-based and ML benchmarks. Based on P50_QL, the algorithm relatively improves by 8.76% in accuracy on the practical sales forecasts of JD.com and 77.99% on a public dataset of electricity loads. This research offers an innovative perspective to combine the statistical and ML algorithms, and JD.com has implemented the W-R algorithm to make accurate sales predictions and guide its marketing activities. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06620
Prescriptive Process Monitoring in Intelligent Process Automation with Chatbot Orchestration,Sergey Zeltyn;Segev Shlomov;Avi Yaeli;Alon Oved,"Business processes that involve AI-powered automation have been gaining importance and market share in recent years. These business processes combine the characteristics of classical business process management, goal-driven chatbots, conversational recommendation systems, and robotic process automation. In the new context, prescriptive process monitoring demands innovative approaches. Unfortunately, data logs from these new processes are still not available in the public domain. We describe the main challenges in this new domain and introduce a synthesized dataset that is based on an actual use case of intelligent process automation with chatbot orchestration. Using this dataset, we demonstrate crowd-wisdom and goal-driven approaches to prescriptive process monitoring. △ Less","13 December, 2022",https://arxiv.org/pdf/2212.06564
Single Cell Training on Architecture Search for Image Denoising,Bokyeung Lee;Kyungdeuk Ko;Jonghwan Hong;Hanseok Ko,"Neural Architecture Search (NAS) for automatically finding the optimal network architecture has shown some success with competitive performances in various computer vision tasks. However, NAS in general requires a tremendous amount of computations. Thus reducing computational cost has emerged as an important issue. Most of the attempts so far has been based on manual approaches, and often the architectures developed from such efforts dwell in the balance of the network optimality and the search cost. Additionally, recent NAS methods for image restoration generally do not consider dynamic operations that may transform dimensions of feature maps because of the dimensionality mismatch in tensor calculations. This can greatly limit NAS in its search for optimal network structure. To address these issues, we re-frame the optimal search problem by focusing at component block level. From previous work, it's been shown that an effective denoising block can be connected in series to further improve the network performance. By focusing at block level, the search space of reinforcement learning becomes significantly smaller and evaluation process can be conducted more rapidly. In addition, we integrate an innovative dimension matching modules for dealing with spatial and channel-wise mismatch that may occur in the optimal design search. This allows much flexibility in optimal network search within the cell block. With these modules, then we employ reinforcement learning in search of an optimal image denoising network at a module level. Computational efficiency of our proposed Denoising Prior Neural Architecture Search (DPNAS) was demonstrated by having it complete an optimal architecture search for an image restoration task by just one day with a single GPU. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.06368
Towards Seamless Management of AI Models in High-Performance Computing,Sixing Yu;Murali Emani;Chunhua Liao;Pei-Hung Lin;Tristan Vanderbruggen;Xipeng Shen;Ali Jannesari,"With the increasing prevalence of artificial intelligence (AI) in diverse science/engineering communities, AI models emerge on an unprecedented scale among various domains. However, given the complexity and diversity of the software and hardware environments, reusing AI artifacts (models and datasets) is extremely challenging, especially with AI-driven science applications. Building an ecosystem to run and reuse AI applications/datasets at scale efficiently becomes increasingly essential for diverse science and engineering and high-performance computing (HPC) communities. In this paper, we innovate over an HPC-AI ecosystem -- HPCFair, which enables the Findable, Accessible, Interoperable, and Reproducible (FAIR) principles. HPCFair enables the collection of AI models/datasets allowing users to download/upload AI artifacts with authentications. Most importantly, our proposed framework provides user-friendly APIs for users to easily run inference jobs and customize AI artifacts to their tasks as needed. Our results show that, with HPCFair API, users irrespective of technical expertise in AI, can easily leverage AI artifacts to their tasks with minimal effort. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.06352
Mixed Supervision of Histopathology Improves Prostate Cancer Classification from MRI,Abhejit Rajagopal;Antonio C. Westphalen;Nathan Velarde;Tim Ullrich;Jeffry P. Simko;Hao Nguyen;Thomas A. Hope;Peder E. Z. Larson;Kirti Magudia,"Non-invasive prostate cancer detection from MRI has the potential to revolutionize patient care by providing early detection of clinically-significant disease (ISUP grade group >= 2), but has thus far shown limited positive predictive value. To address this, we present an MRI-based deep learning method for predicting clinically significant prostate cancer applicable to a patient population with subsequent ground truth biopsy results ranging from benign pathology to ISUP grade group~5. Specifically, we demonstrate that mixed supervision via diverse histopathological ground truth improves classification performance despite the cost of reduced concordance with image-based segmentation. That is, where prior approaches have utilized pathology results as ground truth derived from targeted biopsies and whole-mount prostatectomy to strongly supervise the localization of clinically significant cancer, our approach also utilizes weak supervision signals extracted from nontargeted systematic biopsies with regional localization to improve overall performance. Our key innovation is performing regression by distribution rather than simply by value, enabling use of additional pathology findings traditionally ignored by deep learning strategies. We evaluated our model on a dataset of 973 (testing n=160) multi-parametric prostate MRI exams collected at UCSF from 2015-2018 followed by MRI/ultrasound fusion (targeted) biopsy and systematic (nontargeted) biopsy of the prostate gland, demonstrating that deep networks trained with mixed supervision of histopathology can significantly exceed the performance of the Prostate Imaging-Reporting and Data System (PI-RADS) clinical standard for prostate MRI interpretation. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.06336
Text Mining-Based Patent Analysis for Automated Rule Checking in AEC,Zhe Zheng;Bo-Rui Kang;Qi-Tian Yuan;Yu-Cheng Zhou;Xin-Zheng Lu;Jia-Rui Lin,"Automated rule checking (ARC), which is expected to promote the efficiency of the compliance checking process in the architecture, engineering, and construction (AEC) industry, is gaining increasing attention. Throwing light on the ARC application hotspots and forecasting its trends are useful to the related research and drive innovations. Therefore, this study takes the patents from the database of the Derwent Innovations Index database (DII) and China national knowledge infrastructure (CNKI) as data sources and then carried out a three-step analysis including (1) quantitative characteristics (i.e., annual distribution analysis) of patents, (2) identification of ARC topics using a latent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of ARC topics. The results show that the research hotspots and trends of Chinese and English patents are different. The contributions of this study have three aspects: (1) an approach to a comprehensive analysis of patents by integrating multiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the application hotspots and development trends of ARC are reviewed based on patent analysis; and (3) a signpost for technological development and innovation of ARC is provided. △ Less","12 December, 2022",https://arxiv.org/pdf/2212.05891
Future Space Networks: Toward the Next Giant Leap for Humankind,Mohammed Y. Abdelsadek;Aizaz U. Chaudhry;Tasneem Darwish;Eylem Erdogan;Gunes Karabulut-Kurt;Pablo G. Madoery;Olfa Ben Yahia;Halim Yanikomeroglu,"Due to the unprecedented advances in satellite fabrication and deployment, innovative communications and networking technologies, ambitious space projects and programs, and the resurgence of interest in satellite networks, there is a need to redefine space networks (SpaceNets) to incorporate all of these evolutions. This paper introduces a vision for future SpaceNets that considers advances in several related domains. First, we present a reference architecture that captures the various network entities and terminals in a holistic manner. Based on this, space, air, and ground use cases are studied. Then, the architectures and technologies that enable the envisaged SpaceNets are investigated. In so doing, we highlight the activities and projects of different standardization bodies, satellite operators, and national organizations towards the envisioned SpaceNets. Finally, the challenges, potential solutions, and open issues from communications and networking perspectives are discussed. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.05668
Designing Human-Centered Algorithms for the Public Sector: A Case Study of the U.S. Child-Welfare System,Devansh Saxena,"The U.S. Child Welfare System (CWS) is increasingly seeking to emulate business models of the private sector centered in efficiency, cost reduction, and innovation through the adoption of algorithms. These data-driven systems purportedly improve decision-making, however, the public sector poses its own set of challenges with respect to the technical, theoretical, cultural, and societal implications of algorithmic decision-making. To fill these gaps, my dissertation comprises four studies that examine: 1) how caseworkers interact with algorithms in their day-to-day discretionary work, 2) the impact of algorithmic decision-making on the nature of practice, organization, and street-level decision-making, 3) how casenotes can help unpack patterns of invisible labor and contextualize decision-making processes, and 4) how casenotes can help uncover deeper systemic constraints and risk factors that are hard to quantify but directly impact families and street-level decision-making. My goal for this research is to investigate systemic disparities and design and develop algorithmic systems that are centered in the theory of practice and improve the quality of human discretionary work. These studies have provided actionable steps for human-centered algorithm design in the public sector. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.05556
A systematic literature review on Robotic Process Automation security,Nishith Gajjar;Keyur Rathod;Khushali Jani,"The technocrat epoch is overflowing with new technologies and such cutting-edge facilities accompany the risks and pitfalls. Robotic process automation is another innovation that empowers the computerization of high-volume, manual, repeatable, everyday practice, rule-based, and unmotivating human errands. The principal objective of Robotic Process Automation is to supplant monotonous human errands with a virtual labor force or a computerized specialist playing out a similar work as the human laborer used to perform. This permits human laborers to zero in on troublesome undertakings and critical thinking. Robotic Process Automation instruments are viewed as straightforward and strong for explicit business process computerization. Robotic Process Automation comprises intelligence to decide if a process should occur. It has the capability to analyze the data presented and provide a decision based on the logic parameters set in place by the developer. Moreover, it does not demand for system integration, like other forms of automation. Be that as it may since the innovation is yet arising, the Robotic Process Automation faces a few difficulties during the execution. △ Less","11 December, 2022",https://arxiv.org/pdf/2212.05544
"Personalized local heating neutralizing individual, spatial and temporal thermo-physiological variances in extreme cold environments",Yi Ju;Xinyuan Ju;Hui Zhang;Bin Cao;Bin Liu;Yingxin Zhu,"In this paper, we investigate the feasibility, robustness and optimization of introducing personal comfort systems (PCS), apparatuses that promises in energy saving and comfort improvement, into a broader range of environments. We report a series of laboratory experiments systematically examining the effect of personalized heating in neutralizing individual, spatial and temporal variations of thermal demands. The experiments were conducted in an artificial climate chamber at -15 degC in order to simulate extreme cold environments. We developed a heating garment with 20 pieces of 20 * 20 cm2 heating cloth (grouped into 9 regions) comprehensively covering human body. Surface temperatures of the garment can be controlled independently, quickly (within 20 seconds), precisely (within 1 degC) and easily (through a tablet) up to 45 degC. Participants were instructed to adjust surface temperatures of each segment to their preferences, with their physiological, psychological and adjustment data collected. We found that active heating could significantly and stably improve thermal satisfaction. The overall TSV and TCV were improved 1.50 and 1.53 during the self-adjustment phase. Preferred heating surface temperatures for different segments varied widely. Further, even for the same segment, individual differences among participants were considerable. Such variances were observed through local heating powers, while unnoticeable among thermal perception votes. In other words, all these various differences could be neutralized given the flexibility in personalized adjustments. Our research reaffirms the paradigm of ""adaptive thermal comfort"" and will promote innovations on human-centric design for more efficient PCSs. △ Less","27 December, 2022",https://arxiv.org/pdf/2212.05439
A systematic literature review on insider threats,Angad Pal Singh;Ankit Sharma,"Insider threats is the most concerned cybersecurity problem which is poorly addressed by widely used security solutions. Despite the fact that there have been several scientific publications in this area, but from our innovative study classification and structural taxonomy proposals, we argue to provide the more information about insider threats and defense measures used to counter them. While adopting the current grounded theory method for a thorough literature evaluation, our categorization's goal is to organize knowledge in insider threat research. Along with an analysis of major recent studies on detecting insider threats, the major goal of the study is to develop a classification of current types of insiders, levels of access, motivations behind it, insider profiling, security properties, and methods they use to attack. This includes use of machine learning algorithm, behavior analysis, methods of detection and evaluation. Moreover, actual incidents related to insider attacks have also been analyzed. △ Less","10 December, 2022",https://arxiv.org/pdf/2212.05347
DUNE: Improving Accuracy for Sketch-INT Network Measurement Systems,Zhongxiang Wei;Ye Tian;Wei Chen;Liyuan Gu;Xinming Zhang,"In-band Network Telemetry (INT) and sketching algorithms are two promising directions for measuring network traffics in real time. To combine sketch with INT and preserve their advantages, a representative approach is to use INT to send a switch sketch in small pieces (called sketchlets) to end-host for reconstructing an identical sketch. However, in this paper, we reveal that when naively selecting buckets to sketchlets, the end-host reconstructed sketch is inaccurate. To overcome this problem, we present DUNE, an innovative sketch-INT network measurement system. DUNE incorporates two key innovations: First, we design a novel scatter sketchlet that is more efficient in transferring measurement data by allowing a switch to select individual buckets to add to sketchlets; Second, we propose lightweight data structures for tracing ""freshness"" of the sketch buckets, and present algorithms for smartly selecting buckets that contain valuable measurement data to send to end-host. We theoretically prove the effectiveness of our proposed methods, and implement a prototype on commodity programmable switch. The results of extensive experiments driven by real-world traffics on DUNE suggest that our proposed system can substantially improve the measurement accuracy at a trivial cost. △ Less","9 December, 2022",https://arxiv.org/pdf/2212.04816
Improving the Utilization of Digital Services - Evaluating Contest - Driven Open Data Development and the Adoption of Cloud Services,Workneh Yilma Ayele,"There is a growing interest in utilizing digital services, such as software apps and cloud-based software services. The utilization of digital services is increasing more rapidly than any other segment of world trade. The availability of open data unlocks the possibility of generating market possibilities in the public and private sectors. Digital service utilization can be improved by adopting cloud-based software services and open data innovation for service development. However, open data has no value unless utilized, and little is known about developing digital services using open data. Evaluation of digital service development processes to service deployment is indispensable. Despite this, existing evaluation models are not specifically designed to measure open data innovation contests. Additionally, existing cloud-based digital service implications are not used directly to adopt the technology, and empirical research needs to be included. The research question addressed in this thesis is: ""How can contest-driven innovation of open data digital services be evaluated and the adoption of digital services be supported to improve the utilization of digital services?"" The research approaches used are design science research, descriptive statistics, and case study. This thesis proposes Digital Innovation Contest Measurement Model (DICM-model) and Designing and Refining DICM (DRD-method) for designing and refining DICM-model to provide more agility. Additionally, a framework of barriers constraining developers of open data services from developing viable services is also presented. This framework enables requirement and cloud engineers to prioritize factors responsible for effective adoption. Future research possibilities are automation of idea generation, ex-post evaluation of the proposed artifacts, and expanding cloud-based digital service adoption from suppliers' perspectives. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.04491
Patterns of Sociotechnical Design Preferences of Chatbots for Intergenerational Collaborative Innovation : A Q Methodology Study,Irawan Nurhas;Pouyan Jahanbin;Jan Pawlowski;Stephen Wingreen;Stefan Geisler,"Chatbot technology is increasingly emerging as a virtual assistant. Chatbots could allow individuals and organizations to accomplish objectives that are currently not fully optimized for collaboration across an intergenerational context. This paper explores the preferences of chatbots as a companion in intergenerational innovation. The Q methodology was used to investigate different types of collaborators and determine how different choices occur between collaborators that merge the problem and solution domains of chatbots' design within intergenerational settings. The study's findings reveal that various chatbot design priorities are more diverse among younger adults than senior adults. Additionally, our research further outlines the principles of chatbot design and how chatbots will support both generations. This research is the first step towards cultivating a deeper understanding of different age groups' subjective design preferences for chatbots functioning as a companion in the workplace. Moreover, this study demonstrates how the Q methodology can guide technological development by shifting the approach from an age-focused design to a common goal-oriented design within a multigenerational context. △ Less","7 December, 2022",https://arxiv.org/pdf/2212.03485
Artificial Intelligence Security Competition (AISC),Yinpeng Dong;Peng Chen;Senyou Deng;Lianji L;Yi Sun;Hanyu Zhao;Jiaxing Li;Yunteng Tan;Xinyu Liu;Yangyi Dong;Enhui Xu;Jincai Xu;Shu Xu;Xuelin Fu;Changfeng Sun;Haoliang Han;Xuchong Zhang;Shen Chen;Zhimin Sun;Junyi Cao;Taiping Yao;Shouhong Ding;Yu Wu;Jian Lin;Tianpeng Wu,"The security of artificial intelligence (AI) is an important research area towards safe, reliable, and trustworthy AI systems. To accelerate the research on AI security, the Artificial Intelligence Security Competition (AISC) was organized by the Zhongguancun Laboratory, China Industrial Control Systems Cyber Emergency Response Team, Institute for Artificial Intelligence, Tsinghua University, and RealAI as part of the Zhongguancun International Frontier Technology Innovation Competition (https://www.zgc-aisc.com/en). The competition consists of three tracks, including Deepfake Security Competition, Autonomous Driving Security Competition, and Face Recognition Security Competition. This report will introduce the competition rules of these three tracks and the solutions of top-ranking teams in each track. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.03412
A Comprehensively Improved Hybrid Algorithm for Learning Bayesian Networks: Multiple Compound Memory Erasing,Baokui Mou,"Using a Bayesian network to analyze the causal relationship between nodes is a hot spot. The existing network learning algorithms are mainly constraint-based and score-based network generation methods. The constraint-based method is mainly the application of conditional independence (CI) tests, but the inaccuracy of CI tests in the case of high dimensionality and small samples has always been a problem for the constraint-based method. The score-based method uses the scoring function and search strategy to find the optimal candidate network structure, but the search space increases too much with the increase of the number of nodes, and the learning efficiency is very low. This paper presents a new hybrid algorithm, MCME (multiple compound memory erasing). This method retains the advantages of the first two methods, solves the shortcomings of the above CI tests, and makes innovations in the scoring function in the direction discrimination stage. A large number of experiments show that MCME has better or similar performance than some existing algorithms. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.03103
Data-driven Innovation: Understanding the Direction for Future Research,Sasari Samarasinghe;Sachithra Lokuge,"In the contemporary age of information, organisations have realised the importance of data to innovate and thereby attain a competitive advantage. As a result, firms are more focused on understanding the potential to achieve data-driven innovation (DDI). Researchers too have focused on examining this novel phenomenon in a broader scope. In this study, we conducted a systematic and comprehensive review of the literature to understand the DDI phenomenon. The findings of this study benefit scholars in determining the gaps in the current body of knowledge as well as for practitioners to improve their data strategy to enhance and develop innovation capabilities. △ Less","4 December, 2022",https://arxiv.org/pdf/2212.03061
Thermal Dissipation Resulting from Everyday Interactions as a Sensing Modality -- The MIDAS Touch,Farooq Dar;Hilary Emenike;Zhigang Yin;Mohan Liyanage;Rajesh Sharma;Agustin Zuniga;Mohammad A. Hoque;Marko Radeta;Petteri Nurmi;Huber Flores,"We contribute MIDAS as a novel sensing solution for characterizing everyday objects using thermal dissipation. MIDAS takes advantage of the fact that anytime a person touches an object it results in heat transfer. By capturing and modeling the dissipation of the transferred heat, e.g., through the decrease in the captured thermal radiation, MIDAS can characterize the object and determine its material. We validate MIDAS through extensive empirical benchmarks and demonstrate that MIDAS offers an innovative sensing modality that can recognize a wide range of materials with up to 83% accuracy and generalize to variations in the people interacting with objects. We also demonstrate that MIDAS can detect thermal dissipation through objects, up to 2 mm thickness, and support analysis of multiple objects that are interacted with △ Less","6 December, 2022",https://arxiv.org/pdf/2212.02918
A Time Series Approach to Explainability for Neural Nets with Applications to Risk-Management and Fraud Detection,Marc Wildi;Branka Hadji Misheva,"Artificial intelligence is creating one of the biggest revolution across technology driven application fields. For the finance sector, it offers many opportunities for significant market innovation and yet broad adoption of AI systems heavily relies on our trust in their outputs. Trust in technology is enabled by understanding the rationale behind the predictions made. To this end, the concept of eXplainable AI emerged introducing a suite of techniques attempting to explain to users how complex models arrived at a certain decision. For cross-sectional data classical XAI approaches can lead to valuable insights about the models' inner workings, but these techniques generally cannot cope well with longitudinal data (time series) in the presence of dependence structure and non-stationarity. We here propose a novel XAI technique for deep learning methods which preserves and exploits the natural time ordering of the data. △ Less","6 December, 2022",https://arxiv.org/pdf/2212.02906
Cooperative control of environmental extremes by artificial intelligent agents,Martí Sánchez-Fibla;Clément Moulin-Frier;Ricard Solé,"Humans have been able to tackle biosphere complexities by acting as ecosystem engineers, profoundly changing the flows of matter, energy and information. This includes major innovations that allowed to reduce and control the impact of extreme events. Modelling the evolution of such adaptive dynamics can be challenging given the potentially large number of individual and environmental variables involved. This paper shows how to address this problem by using fire as the source of external, bursting and wide fluctuations. Fire propagates on a spatial landscape where a group of agents harvest and exploit trees while avoiding the damaging effects of fire spreading. The agents need to solve a conflict to reach a group-level optimal state: while tree harvesting reduces the propagation of fires, it also reduces the availability of resources provided by trees. It is shown that the system displays two major evolutionary innovations that end up in an ecological engineering strategy that favours high biomass along with the suppression of large fires. The implications for potential A.I. management of complex ecosystems are discussed. △ Less","5 December, 2022",https://arxiv.org/pdf/2212.02395
Deep Learning for Multiscale Damage Analysis via Physics-Informed Recurrent Neural Network,Shiguang Deng,"Direct numerical simulation of hierarchical materials via homogenization-based concurrent multiscale models poses critical challenges for 3D large scale engineering applications, as the computation of highly nonlinear and path-dependent material constitutive responses at the lower scale causes prohibitively high computational costs. In this work, we propose a physics-informed data-driven deep learning model as an efficient surrogate to emulate the effective responses of heterogeneous microstructures under irreversible elasto-plastic hardening and softening deformation. Our contribution contains several major innovations. First, we propose a novel training scheme to generate arbitrary loading sequences in the sampling space confined by deformation constraints where the simulation cost of homogenizing microstructural responses per sequence is dramatically reduced via mechanistic reduced-order models. Second, we develop a new sequential learner that incorporates thermodynamics consistent physics constraints by customizing training loss function and data flow architecture. We additionally demonstrate the integration of trained surrogate within the framework of classic multiscale finite element solver. Our numerical experiments indicate that our model shows a significant accuracy improvement over pure data-driven emulator and a dramatic efficiency boost than reduced models. We believe our data-driven model provides a computationally efficient and mechanics consistent alternative for classic constitutive laws beneficial for potential high-throughput simulations that needs material homogenization of irreversible behaviors. △ Less","26 December, 2022",https://arxiv.org/pdf/2212.01880
Quantum NETwork: from theory to practice,Kun Fang;Jingtian Zhao;Xiufan Li;Yifei Li;Runyao Duan,"The quantum internet is envisioned as the ultimate stage of the quantum revolution, which surpasses its classical counterpart in various aspects, such as the efficiency of data transmission, the security of network services, and the capability of information processing. Given its disruptive impact on the national security and the digital economy, a global race to build scalable quantum networks has already begun. With the joint effort of national governments, industrial participants and research institutes, the development of quantum networks has advanced rapidly in recent years, bringing the first primitive quantum networks within reach. In this work, we aim to provide an up-to-date review of the field of quantum networks from both theoretical and experimental perspectives, contributing to a better understanding of the building blocks required for the establishment of a global quantum internet. We also introduce a newly developed quantum network toolkit to facilitate the exploration and evaluation of innovative ideas. Particularly, it provides dual quantum computing engines, supporting simulations in both the quantum circuit and measurement-based models. It also includes a compilation scheme for mapping quantum network protocols onto quantum circuits, enabling their emulations on real-world quantum hardware devices. We showcase the power of this toolkit with several featured demonstrations, including a simulation of the Micius quantum satellite experiment, a testing of a four-layer quantum network architecture with resource management, and a quantum emulation of the CHSH game. We hope this work can give a better understanding of the state-of-the-art development of quantum networks and provide the necessary tools to make further contributions along the way. △ Less","2 December, 2022",https://arxiv.org/pdf/2212.01226
Fair Generative Models via Transfer Learning,Christopher TH Teo;Milad Abdollahzadeh;Ngai-Man Cheung,"This work addresses fair generative models. Dataset biases have been a major cause of unfairness in deep generative models. Previous work had proposed to augment large, biased datasets with small, unbiased reference datasets. Under this setup, a weakly-supervised approach has been proposed, which achieves state-of-the-art quality and fairness in generated samples. In our work, based on this setup, we propose a simple yet effective approach. Specifically, first, we propose fairTL, a transfer learning approach to learn fair generative models. Under fairTL, we pre-train the generative model with the available large, biased datasets and subsequently adapt the model using the small, unbiased reference dataset. We find that our fairTL can learn expressive sample generation during pre-training, thanks to the large (biased) dataset. This knowledge is then transferred to the target model during adaptation, which also learns to capture the underlying fair distribution of the small reference dataset. Second, we propose fairTL++, where we introduce two additional innovations to improve upon fairTL: (i) multiple feedback and (ii) Linear-Probing followed by Fine-Tuning (LP-FT). Taking one step further, we consider an alternative, challenging setup when only a pre-trained (potentially biased) model is available but the dataset that was used to pre-train the model is inaccessible. We demonstrate that our proposed fairTL and fairTL++ remain very effective under this setup. We note that previous work requires access to the large, biased datasets and is incapable of handling this more challenging setup. Extensive experiments show that fairTL and fairTL++ achieve state-of-the-art in both quality and fairness of generated samples. The code and additional resources can be found at bearwithchris.github.io/fairTL/. △ Less","1 December, 2022",https://arxiv.org/pdf/2212.00926
Predicting Digital Asset Prices using Natural Language Processing: a survey,Trang Tran,"Blockchain technology has changed how people think about how they used to store and trade their assets, as it introduced us to a whole new way to transact: using digital currencies. One of the major innovations of blockchain technology is decentralization, meaning that traditional financial intermediaries, such as asset-backed security issuers and banks, are eliminated in the process. Even though blockchain technology has been utilized in a wide range of industries, its most prominent application is still cryptocurrencies, with Bitcoin being the first proposed. At its peak in 2021, the market cap for Bitcoin once surpassed 1 trillion US dollars. The open nature of the crypto market poses various challenges and concerns for both potential retail investors and institutional investors, as the price of the investment is highly volatile, and its fluctuations are unpredictable. The rise of Machine Learning, and Natural Language Processing, in particular, has shed some light on monitoring and predicting the price behaviors of cryptocurrencies. This paper aims to review and analyze the recent efforts in applying Machine Learning and Natural Language Processing methods to predict the prices and analyze the behaviors of digital assets such as Bitcoin and Ethereum. △ Less","28 November, 2022",https://arxiv.org/pdf/2212.00726
Advanced Audio Aid for Blind People,Savera Sarwar;Muhammad Turab;Danish Channa;Aisha Chandio;M. Uzair Sohu;Vikram Kumar,"One of the most important senses in human life is vision, without it life is totally filled with darkness. According to WHO globally millions of people are visually impaired estimated there are 285 million, of whom some millions are blind. Unfortunately, there are around 2.4 million people are blind in our beloved country Pakistan. Human are a crucial part of society and the blind community is a main part of society. The technologies are grown so far to make the life of humans easier more comfortable and more reliable for. However, this disability of the blind community would reduce their chance of using such innovative products. Therefore, the visually impaired community believe that they are burden to other societies and they do not capture in normal activities separates the blind people from society and because of this believe did not participate in the normally tasks of society . The visual impair people mainly face most of the problems in this real-time The aim of this work is to turn the real time world into an audio world by telling blind person about the objects in their way and can read printed text. This will enable blind persons to identify the things and read the text without any external help just by using the object detection and reading system in real time. Objective of this work: i) Object detection ii) Read printed text, using state-of-the-art (SOTA) technology. △ Less","17 November, 2022",https://arxiv.org/pdf/2212.00004
Carbon Emission Prediction on the World Bank Dataset for Canada,Aman Desai;Shyamal Gandhi;Sachin Gupta;Manan Shah;Samir Patel,"The continuous rise in CO2 emission into the environment is one of the most crucial issues facing the whole world. Many countries are making crucial decisions to control their carbon footprints to escape some of their catastrophic outcomes. There has been a lot of research going on to project the amount of carbon emissions in the future, which can help us to develop innovative techniques to deal with it in advance. Machine learning is one of the most advanced and efficient techniques for predicting the amount of carbon emissions from current data. This paper provides the methods for predicting carbon emissions (CO2 emissions) for the next few years. The predictions are based on data from the past 50 years. The dataset, which is used for making the prediction, is collected from World Bank datasets. This dataset contains CO2 emissions (metric tons per capita) of all the countries from 1960 to 2018. Our method consists of using machine learning techniques to take the idea of what carbon emission measures will look like in the next ten years and project them onto the dataset taken from the World Bank's data repository. The purpose of this research is to compare how different machine learning models (Decision Tree, Linear Regression, Random Forest, and Support Vector Machine) perform on a similar dataset and measure the difference between their predictions. △ Less","26 November, 2022",https://arxiv.org/pdf/2211.17010
Towards a Taxonomy of Industrial Challenges and Enabling Technologies in Industry 4.0,Roberto Figliè;Riccardo Amadio;Marios Tyrovolas;Chrysostomos Stylios;Łukasz Paśko;Dorota Stadnicka;Anna Carreras-Coch;Agustín Zaballos;Joan Navarro;Daniele Mazzei,"Today, one of the biggest challenges for digital transformation in the Industry 4.0 paradigm is the lack of mutual understanding between the academic and the industrial world. On the one hand, the industry fails to apply new technologies and innovations from scientific research. At the same time, academics struggle to find and focus on real-world applications for their developing technological solutions. Moreover, the increasing complexity of industrial challenges and technologies is widening this hiatus. To reduce this knowledge and communication gap, this article proposes a mixed approach of humanistic and engineering techniques applied to the technological and enterprise fields. The study's results are represented by a taxonomy in which industrial challenges and I4.0-focused technologies are categorized and connected through academic and grey literature analysis. This taxonomy also formed the basis for creating a public web platform where industrial practitioners can identify candidate solutions for an industrial challenge. At the same time, from the educational perspective, the learning procedure can be supported since, through this tool, academics can identify real-world scenarios to integrate digital technologies' teaching process. △ Less","29 November, 2022",https://arxiv.org/pdf/2211.16563
"Performance Evaluation, Optimization and Dynamic Decision in Blockchain Systems: A Recent Overview",Quan-Lin Li;Yan-Xia Chang;Qing Wang,"With rapid development of blockchain technology as well as integration of various application areas, performance evaluation, performance optimization, and dynamic decision in blockchain systems are playing an increasingly important role in developing new blockchain technology. This paper provides a recent systematic overview of this class of research, and especially, developing mathematical modeling and basic theory of blockchain systems. Important examples include (a) performance evaluation: Markov processes, queuing theory, Markov reward processes, random walks, fluid and diffusion approximations, and martingale theory; (b) performance optimization: Linear programming, nonlinear programming, integer programming, and multi-objective programming; (c) optimal control and dynamic decision: Markov decision processes, and stochastic optimal control; and (d) artificial intelligence: Machine learning, deep reinforcement learning, and federated learning. So far, a little research has focused on these research lines. We believe that the basic theory with mathematical methods, algorithms and simulations of blockchain systems discussed in this paper will strongly support future development and continuous innovation of blockchain technology. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15907
Deep Grading based on Collective Artificial Intelligence for AD Diagnosis and Prognosis,Huy-Dung Nguyen;Michaël Clément;Boris Mansencal;Pierrick Coupé,"Accurate diagnosis and prognosis of Alzheimer's disease are crucial to develop new therapies and reduce the associated costs. Recently, with the advances of convolutional neural networks, methods have been proposed to automate these two tasks using structural MRI. However, these methods often suffer from lack of interpretability, generalization, and can be limited in terms of performance. In this paper, we propose a novel deep framework designed to overcome these limitations. Our framework consists of two stages. In the first stage, we propose a deep grading model to extract meaningful features. To enhance the robustness of these features against domain shift, we introduce an innovative collective artificial intelligence strategy for training and evaluating steps. In the second stage, we use a graph convolutional neural network to better capture AD signatures. Our experiments based on 2074 subjects show the competitive performance of our deep framework compared to state-of-the-art methods on different datasets for both AD diagnosis and prognosis. △ Less","28 November, 2022",https://arxiv.org/pdf/2211.15192
Dense Text Retrieval based on Pretrained Language Models: A Survey,Wayne Xin Zhao;Jing Liu;Ruiyang Ren;Ji-Rong Wen,"Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to user's queries in natural language. From classic retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn the text representation and model the relevance matching. The recent success of pretrained language models (PLMs) sheds light on developing more capable text retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is referred to as dense retrieval, since it employs dense vectors (a.k.a., embeddings) to represent the texts. Considering the rapid progress on dense retrieval, in this survey, we systematically review the recent advances on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related work by four major aspects, including architecture, training, indexing and integration, and summarize the mainstream techniques for each aspect. We thoroughly survey the literature, and include 300+ related reference papers on dense retrieval. To support our survey, we create a website for providing useful resources, and release a code repertory and toolkit for implementing dense retrieval models. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval. △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14876
Devils in the Clouds: An Evolutionary Study of Telnet Bot Loaders,Yuhui Zhu;Zhenxiang Chen;Qiben Yan;Shanshan Wang;Alberto Giaretta;Enlong Li;Lizhi Peng;Chuan Zhao;Mauro Conti,"One of the innovations brought by Mirai and its derived malware is the adoption of self-contained loaders for infecting IoT devices and recruiting them in botnets. Functionally decoupled from other botnet components and not embedded in the payload, loaders cannot be analysed using conventional approaches that rely on honeypots for capturing samples. Different approaches are necessary for studying the loaders evolution and defining a genealogy. To address the insufficient knowledge about loaders' lineage in existing studies, in this paper, we propose a semantic-aware method to measure, categorize, and compare different loader servers, with the goal of highlighting their evolution, independent from the payload evolution. Leveraging behavior-based metrics, we cluster the discovered loaders and define eight families to determine the genealogy and draw a homology map. Our study shows that the source code of Mirai is evolving and spawning new botnets with new capabilities, both on the client side and the server side. In turn, shedding light on the infection loaders can help the cybersecurity community to improve detection and prevention tools. △ Less","27 November, 2022",https://arxiv.org/pdf/2211.14790
Mixture Manifold Networks: A Computationally Efficient Baseline for Inverse Modeling,Gregory P. Spell;Simiao Ren;Leslie M. Collins;Jordan M. Malof,"We propose and show the efficacy of a new method to address generic inverse problems. Inverse modeling is the task whereby one seeks to determine the control parameters of a natural system that produce a given set of observed measurements. Recent work has shown impressive results using deep learning, but we note that there is a trade-off between model performance and computational time. For some applications, the computational time at inference for the best performing inverse modeling method may be overly prohibitive to its use. We present a new method that leverages multiple manifolds as a mixture of backward (e.g., inverse) models in a forward-backward model architecture. These multiple backwards models all share a common forward model, and their training is mitigated by generating training examples from the forward model. The proposed method thus has two innovations: 1) the multiple Manifold Mixture Network (MMN) architecture, and 2) the training procedure involving augmenting backward model training data using the forward model. We demonstrate the advantages of our method by comparing to several baselines on four benchmark inverse problems, and we furthermore provide analysis to motivate its design. △ Less","25 November, 2022",https://arxiv.org/pdf/2211.14366
Confidence Interval Construction for Multivariate time series using Long Short Term Memory Network,Aryan Bhambu;Arabin Kumar Dey,In this paper we propose a novel procedure to construct a confidence interval for multivariate time series predictions using long short term memory network. The construction uses a few novel block bootstrap techniques. We also propose an innovative block length selection procedure for each of these schemes. Two novel benchmarks help us to compare the construction of this confidence intervals by different bootstrap techniques. We illustrate the whole construction through S\&P 500 and Dow Jones Index datasets. △ Less,"25 November, 2022",https://arxiv.org/pdf/2211.13915
The Westermo test results data set,Per Erik Strandberg,"There is a growing body of knowledge in the computer science, software engineering, software testing and software test automation disciplines. However, there is a challenge for researchers to evaluate their research findings, innovations and tools due to lack of realistic data. This paper presents the Westermo test results data set, more than one million verdicts from testing of embedded systems, from more than five hundred consecutive days of nightly testing. The data also contains information on code changes in both the software under test and the test framework used for testing. This data set can support the research community in particular with respect to the regression test selection problem, flaky tests, test results visualization, etc. △ Less","24 November, 2022",https://arxiv.org/pdf/2211.13622
Assessment of Human Behavior in Virtual Reality by Eye Tracking,Hong Gao,"Virtual reality (VR) is not a new technology but has been in development for decades, driven by advances in computer technology. Currently, VR technology is increasingly being used in applications to enable immersive, yet controlled research settings. Education and entertainment are two important application areas, where VR has been considered a key enabler of immersive experiences and their further advancement. At the same time, the study of human behavior in such innovative environments is expected to contribute to a better design of VR applications. Therefore, modern VR devices are consistently equipped with eye-tracking technology, enabling thus further studies of human behavior through the collection of process data. In particular, eye-tracking technology in combination with machine learning techniques and explainable models can provide new insights for a deeper understanding of human behavior during immersion in virtual environments. In this work, a systematic computational framework based on eye-tracking and behavioral user data and state-of-the-art machine learning approaches is proposed to understand human behavior and individual differences in VR contexts. This computational framework is then employed in three user studies across two different domains. In the educational domain, two different immersive VR classrooms were created where students can learn and teachers can train. In terms of VR entertainment, eye movements open a new avenue to evaluate VR locomotion techniques from the perspective of user cognitive load and user experience. This work paves the way for assessing human behavior in VR scenarios and provides profound insights into the way of designing, evaluating, and improving interactive VR systems. In particular, more effective and customizable virtual environments can be created to provide users with tailored experiences. △ Less","23 November, 2022",https://arxiv.org/pdf/2211.12846
Big Earth Data and Machine Learning for Sustainable and Resilient Agriculture,Vasileios Sitokonstantinou,"Big streams of Earth images from satellites or other platforms (e.g., drones and mobile phones) are becoming increasingly available at low or no cost and with enhanced spatial and temporal resolution. This thesis recognizes the unprecedented opportunities offered by the high quality and open access Earth observation data of our times and introduces novel machine learning and big data methods to properly exploit them towards developing applications for sustainable and resilient agriculture. The thesis addresses three distinct thematic areas, i.e., the monitoring of the Common Agricultural Policy (CAP), the monitoring of food security and applications for smart and resilient agriculture. The methodological innovations of the developments related to the three thematic areas address the following issues: i) the processing of big Earth Observation (EO) data, ii) the scarcity of annotated data for machine learning model training and iii) the gap between machine learning outputs and actionable advice. This thesis demonstrated how big data technologies such as data cubes, distributed learning, linked open data and semantic enrichment can be used to exploit the data deluge and extract knowledge to address real user needs. Furthermore, this thesis argues for the importance of semi-supervised and unsupervised machine learning models that circumvent the ever-present challenge of scarce annotations and thus allow for model generalization in space and time. Specifically, it is shown how merely few ground truth data are needed to generate high quality crop type maps and crop phenology estimations. Finally, this thesis argues there is considerable distance in value between model inferences and decision making in real-world scenarios and thereby showcases the power of causal and interpretable machine learning in bridging this gap. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12584
Smart Agriculture : A Novel Multilevel Approach for Agricultural Risk Assessment over Unstructured Data,Hasna Najmi;Mounia Mikram;Maryem Rhanoui;Siham Yousfi,"Detecting opportunities and threats from massive text data is a challenging task for most. Traditionally, companies would rely mainly on structured data to detect and predict risks, losing a huge amount of information that could be extracted from unstructured text data. Fortunately, artificial intelligence came to remedy this issue by innovating in data extraction and processing techniques, allowing us to understand and make use of Natural Language data and turning it into structures that a machine can process and extract insight from. Uncertainty refers to a state of not knowing what will happen in the future. This paper aims to leverage natural language processing and machine learning techniques to model uncertainties and evaluate the risk level in each uncertainty cluster using massive text data. △ Less","22 November, 2022",https://arxiv.org/pdf/2211.12515
Multi-task Learning for Camera Calibration,Talha Hanif Butt;Murtaza Taj,"For a number of tasks, such as 3D reconstruction, robotic interface, autonomous driving, etc., camera calibration is essential. In this study, we present a unique method for predicting intrinsic (principal point offset and focal length) and extrinsic (baseline, pitch, and translation) properties from a pair of images. We suggested a novel method where camera model equations are represented as a neural network in a multi-task learning framework, in contrast to existing methods, which build a comprehensive solution. By reconstructing the 3D points using a camera model neural network and then using the loss in reconstruction to obtain the camera specifications, this innovative camera projection loss (CPL) method allows us that the desired parameters should be estimated. As far as we are aware, our approach is the first one that uses an approach to multi-task learning that includes mathematical formulas in a framework for learning to estimate camera parameters to predict both the extrinsic and intrinsic parameters jointly. Additionally, we provided a new dataset named as CVGL Camera Calibration Dataset [1] which has been collected using the CARLA Simulator [2]. Actually, we show that our suggested strategy out performs both conventional methods and methods based on deep learning on 6 out of 10 parameters that were assessed using both real and synthetic data. Our code and generated dataset are available at https://github.com/thanif/Camera-Calibration-through-Camera-Projection-Loss. △ Less","23 December, 2022",https://arxiv.org/pdf/2211.12432
PointCA: Evaluating the Robustness of 3D Point Cloud Completion Models Against Adversarial Examples,Shengshan Hu;Junwei Zhang;Wei Liu;Junhui Hou;Minghui Li;Leo Yu Zhang;Hai Jin;Lichao Sun,"Point cloud completion, as the upstream procedure of 3D recognition and segmentation, has become an essential part of many tasks such as navigation and scene understanding. While various point cloud completion models have demonstrated their powerful capabilities, their robustness against adversarial attacks, which have been proven to be fatally malicious towards deep neural networks, remains unknown. In addition, existing attack approaches towards point cloud classifiers cannot be applied to the completion models due to different output forms and attack purposes. In order to evaluate the robustness of the completion models, we propose PointCA, the first adversarial attack against 3D point cloud completion models. PointCA can generate adversarial point clouds that maintain high similarity with the original ones, while being completed as another object with totally different semantic information. Specifically, we minimize the representation discrepancy between the adversarial example and the target point set to jointly explore the adversarial point clouds in the geometry space and the feature space. Furthermore, to launch a stealthier attack, we innovatively employ the neighbourhood density information to tailor the perturbation constraint, leading to geometry-aware and distribution-adaptive modifications for each point. Extensive experiments against different premier point cloud completion networks show that PointCA can cause a performance degradation from 77.9% to 16.7%, with the structure chamfer distance kept below 0.01. We conclude that existing completion models are severely vulnerable to adversarial examples, and state-of-the-art defenses for point cloud classification will be partially invalid when applied to incomplete and uneven point cloud data. △ Less","1 December, 2022",https://arxiv.org/pdf/2211.12294
COVID-Net Assistant: A Deep Learning-Driven Virtual Assistant for COVID-19 Symptom Prediction and Recommendation,Pengyuan Shi;Yuetong Wang;Saad Abbasi;Alexander Wong,"As the COVID-19 pandemic continues to put a significant burden on healthcare systems worldwide, there has been growing interest in finding inexpensive symptom pre-screening and recommendation methods to assist in efficiently using available medical resources such as PCR tests. In this study, we introduce the design of COVID-Net Assistant, an efficient virtual assistant designed to provide symptom prediction and recommendations for COVID-19 by analyzing users' cough recordings through deep convolutional neural networks. We explore a variety of highly customized, lightweight convolutional neural network architectures generated via machine-driven design exploration (which we refer to as COVID-Net Assistant neural networks) on the Covid19-Cough benchmark dataset. The Covid19-Cough dataset comprises 682 cough recordings from a COVID-19 positive cohort and 642 from a COVID-19 negative cohort. Among the 682 cough recordings labeled positive, 382 recordings were verified by PCR test. Our experimental results show promising, with the COVID-Net Assistant neural networks demonstrating robust predictive performance, achieving AUC scores of over 0.93, with the best score over 0.95 while being fast and efficient in inference. The COVID-Net Assistant models are made available in an open source manner through the COVID-Net open initiative and, while not a production-ready solution, we hope their availability acts as a good resource for clinical scientists, machine learning researchers, as well as citizen scientists to develop innovative solutions. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11944
Preprint: Open Source Compiling for V1Model RMT Switch: Making Data Center Networking Innovation Accessible,Debobroto Das Robin;Javed I. Khan,"Very few of the innovations in deep networking have seen data center scale implementation. Because the Data Center network's extreme scale performance requires hardware implementation, which is only accessible to a few. However, the emergence of reconfigurable match-action table (RMT) paradigm-based switches have finally opened up the development life cycle of data plane devices. The P4 language is the dominant language choice for programming these devices. Now, Network operators can implement the desired feature over white box RMT switches. The process involves an innovator writing new algorithms in the P4 language and getting them compiled for the target hardware. However, there is still a roadblock. After designing an algorithm, the P4 program's compilation technology is not fully open-source. Thus, it is very difficult for an average researcher to get deep insight into the performance of his/her innovation when executed at the silicon level. There is no open-source compiler backend available for this purpose. Proprietary compiler backends provided by different hardware vendors are available for this purpose. However, they are closed-source and do not provide access to the internal mapping mechanisms. Which inhibits experimenting with new mapping algorithms and innovative instruction sets for reconfigurable match-action table architecture. This paper describes our work toward an open-source compiler backend for compiling P416 targeted for the V1Model architecture-based programmable switches. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11916
"Intelligent Computing: The Latest Advances, Challenges and Future",Shiqiang Zhu;Ting Yu;Tao Xu;Hongyang Chen;Schahram Dustdar;Sylvain Gigan;Deniz Gunduz;Ekram Hossain;Yaochu Jin;Feng Lin;Bo Liu;Zhiguo Wan;Ji Zhang;Zhifeng Zhao;Wentao Zhu;Zuoning Chen;Tariq Durrani;Huaimin Wang;Jiangxing Wu;Tongyi Zhang;Yunhe Pan,"Computing is a critical driving force in the development of human civilization. In recent years, we have witnessed the emergence of intelligent computing, a new computing paradigm that is reshaping traditional computing and promoting digital revolution in the era of big data, artificial intelligence and internet-of-things with new computing theories, architectures, methods, systems, and applications. Intelligent computing has greatly broadened the scope of computing, extending it from traditional computing on data to increasingly diverse computing paradigms such as perceptual intelligence, cognitive intelligence, autonomous intelligence, and human-computer fusion intelligence. Intelligence and computing have undergone paths of different evolution and development for a long time but have become increasingly intertwined in recent years: intelligent computing is not only intelligence-oriented but also intelligence-driven. Such cross-fertilization has prompted the emergence and rapid advancement of intelligent computing. Intelligent computing is still in its infancy and an abundance of innovations in the theories, systems, and applications of intelligent computing are expected to occur soon. We present the first comprehensive survey of literature on intelligent computing, covering its theory fundamentals, the technological fusion of intelligence and computing, important applications, challenges, and future perspectives. We believe that this survey is highly timely and will provide a comprehensive reference and cast valuable insights into intelligent computing for academic and industrial researchers and practitioners. △ Less","21 November, 2022",https://arxiv.org/pdf/2211.11281
Non-reversible Parallel Tempering for Deep Posterior Approximation,Wei Deng;Qian Zhang;Qi Feng;Faming Liang;Guang Lin,"Parallel tempering (PT), also known as replica exchange, is the go-to workhorse for simulations of multi-modal distributions. The key to the success of PT is to adopt efficient swap schemes. The popular deterministic even-odd (DEO) scheme exploits the non-reversibility property and has successfully reduced the communication cost from O(P^2) to O(P) given sufficiently many P chains. However, such an innovation largely disappears in big data due to the limited chains and few bias-corrected swaps. To handle this issue, we generalize the DEO scheme to promote non-reversibility and propose a few solutions to tackle the underlying bias caused by the geometric stopping time. Notably, in big data scenarios, we obtain an appealing communication cost O(P\log P) based on the optimal window size. In addition, we also adopt stochastic gradient descent (SGD) with large and constant learning rates as exploration kernels. Such a user-friendly nature enables us to conduct approximation tasks for complex posteriors without much tuning costs. △ Less","19 November, 2022",https://arxiv.org/pdf/2211.10837
Potential Auto-driving Threat: Universal Rain-removal Attack,Jinchegn Hu;Jihao Li;Zhuoran Hou;Jingjing Jiang;Cunjia Liu;Yuanjian Zhang,"The problem of robustness in adverse weather conditions is considered a significant challenge for computer vision algorithms in the applicants of autonomous driving. Image rain removal algorithms are a general solution to this problem. They find a deep connection between raindrops/rain-streaks and images by mining the hidden features and restoring information about the rain-free environment based on the powerful representation capabilities of neural networks. However, previous research has focused on architecture innovations and has yet to consider the vulnerability issues that already exist in neural networks. This research gap hints at a potential security threat geared toward the intelligent perception of autonomous driving in the rain. In this paper, we propose a universal rain-removal attack (URA) on the vulnerability of image rain-removal algorithms by generating a non-additive spatial perturbation that significantly reduces the similarity and image quality of scene restoration. Notably, this perturbation is difficult to recognise by humans and is also the same for different target images. Thus, URA could be considered a critical tool for the vulnerability detection of image rain-removal algorithms. It also could be developed as a real-world artificial intelligence attack method. Experimental results show that URA can reduce the scene repair capability by 39.5% and the image generation quality by 26.4%, targeting the state-of-the-art (SOTA) single-image rain-removal algorithms currently available. △ Less","17 November, 2022",https://arxiv.org/pdf/2211.09959
"Explainable, Domain-Adaptive, and Federated Artificial Intelligence in Medicine",Ahmad Chaddad;Qizong lu;Jiali Li;Yousef Katib;Reem Kateb;Camel Tanougast;Ahmed Bouridane;Ahmed Abdulkadir,"Artificial intelligence (AI) continues to transform data analysis in many domains. Progress in each domain is driven by a growing body of annotated data, increased computational resources, and technological innovations. In medicine, the sensitivity of the data, the complexity of the tasks, the potentially high stakes, and a requirement of accountability give rise to a particular set of challenges. In this review, we focus on three key methodological approaches that address some of the particular challenges in AI-driven medical decision making. (1) Explainable AI aims to produce a human-interpretable justification for each output. Such models increase confidence if the results appear plausible and match the clinicians expectations. However, the absence of a plausible explanation does not imply an inaccurate model. Especially in highly non-linear, complex models that are tuned to maximize accuracy, such interpretable representations only reflect a small portion of the justification. (2) Domain adaptation and transfer learning enable AI models to be trained and applied across multiple domains. For example, a classification task based on images acquired on different acquisition hardware. (3) Federated learning enables learning large-scale models without exposing sensitive personal health information. Unlike centralized AI learning, where the centralized learning machine has access to the entire training data, the federated learning process iteratively updates models across multiple sites by exchanging only parameter updates, not personal health data. This narrative review covers the basic concepts, highlights relevant corner-stone and state-of-the-art research in the field, and discusses perspectives. △ Less","16 November, 2022",https://arxiv.org/pdf/2211.09317
Data-pooling Reinforcement Learning for Personalized Healthcare Intervention,Xinyun Chen;Pengyi Shi;Shanwen Pu,"Motivated by the emerging needs of personalized preventative intervention in many healthcare applications, we consider a multi-stage, dynamic decision-making problem in the online setting with unknown model parameters. To deal with the pervasive issue of small sample size in personalized planning, we develop a novel data-pooling reinforcement learning (RL) algorithm based on a general perturbed value iteration framework. Our algorithm adaptively pools historical data, with three main innovations: (i) the weight of pooling ties directly to the performance of decision (measured by regret) as opposed to estimation accuracy in conventional methods; (ii) no parametric assumptions are needed between historical and current data; and (iii) requiring data-sharing only via aggregate statistics, as opposed to patient-level data. Our data-pooling algorithm framework applies to a variety of popular RL algorithms, and we establish a theoretical performance guarantee showing that our pooling version achieves a regret bound strictly smaller than that of the no-pooling counterpart. We substantiate the theoretical development with empirically better performance of our algorithm via a case study in the context of post-discharge intervention to prevent unplanned readmissions, generating practical insights for healthcare management. In particular, our algorithm alleviates privacy concerns about sharing health data, which (i) opens the door for individual organizations to levering public datasets or published studies to better manage their own patients; and (ii) provides the basis for public policy makers to encourage organizations to share aggregate data to improve population health outcomes for the broader community. △ Less","16 November, 2022",https://arxiv.org/pdf/2211.08998
The Future of Hackathon Research and Practice,Jeanette Falk;Alexander Nolte;Daniela Huppenkothen;Marion Weinzierl;Kiev Gama;Daniel Spikol;Erik Tollerud;Neil Chue Hong;Ines Knäpper;Linda Bailey Hayden,"Hackathons are time-bounded collaborative events which have become a global phenomenon adopted by both researchers and practitioners in a plethora of contexts. Hackathon events are generally used to accelerate the development of, for example, scientific results and collaborations, communities, and innovative prototypes addressing urgent challenges. As hackathons have been adopted into many different contexts, the events have also been adapted in numerous ways corresponding to the unique needs and situations of organizers, participants and other stakeholders. While these interdisciplinary adaptions, in general affords many advantages - such as tailoring the format to specific needs - they also entail certain challenges, specifically: 1) limited exchange of best practices, 2) limited exchange of research findings, and 3) larger overarching questions that require interdisciplinary collaboration are not discovered and remain unaddressed. We call for interdisciplinary collaborations to address these challenges. As a first initiative towards this, we performed an interdisciplinary collaborative analysis in the context of a workshop at the Lorentz Center, Leiden in December 2021. In this paper, we present the results of this analysis in terms of six important areas which we envision to contribute to maturing hackathon research and practice: 1) hackathons for different purposes, 2) socio-technical event design, 3) scaling up, 4) making hackathons equitable, 5) studying hackathons, and 6) hackathon goals and how to reach them. We present these areas in terms of the state of the art and research proposals and conclude the paper by suggesting next steps needed for advancing hackathon research and practice. △ Less","16 November, 2022",https://arxiv.org/pdf/2211.08963
Fast and Accurate FSA System Using ELBERT: An Efficient and Lightweight BERT,Siyuan Lu;Chenchen Zhou;Keli Xie;Jun Lin;Zhongfeng Wang,"With the development of deep learning and Transformer-based pre-trained models like BERT, the accuracy of many NLP tasks has been dramatically improved. However, the large number of parameters and computations also pose challenges for their deployment. For instance, using BERT can improve the predictions in the financial sentiment analysis (FSA) task but slow it down, where speed and accuracy are equally important in terms of profits. To address these issues, we first propose an efficient and lightweight BERT (ELBERT) along with a novel confidence-window-based (CWB) early exit mechanism. Based on ELBERT, an innovative method to accelerate text processing on the GPU platform is developed, solving the difficult problem of making the early exit mechanism work more effectively with a large input batch size. Afterward, a fast and high-accuracy FSA system is built. Experimental results show that the proposed CWB early exit mechanism achieves significantly higher accuracy than existing early exit methods on BERT under the same computation cost. By using this acceleration method, our FSA system can boost the processing speed by nearly 40 times to over 1000 texts per second with sufficient accuracy, which is nearly twice as fast as FastBERT, thus providing a more powerful text processing capability for modern trading systems. △ Less","5 December, 2022",https://arxiv.org/pdf/2211.08842
MeSHwA: The case for a Memory-Safe Software and Hardware Architecture for Serverless Computing,Anjo Vahldiek-Oberwagner;Mona Vij,"Motivated by developer productivity, serverless computing, and microservices have become the de facto development model in the cloud. Microservices decompose monolithic applications into separate functional units deployed individually. This deployment model, however, costs CSPs a large infrastructure tax of more than 25%. To overcome these limitations, CSPs shift workloads to Infrastructure Processing Units (IPUs) like Amazon's Nitro or, complementary, innovate by building on memory-safe languages and novel software abstractions. Based on these trends, we hypothesize a \arch providing a general-purpose runtime environment to specialize functionality when needed and strongly isolate components. To achieve this goal, we investigate building a single address space OS or a multi-application library OS, possible hardware implications, and demonstrate their capabilities, drawbacks and requirements. The goal is to bring the advantages to all application workloads including legacy and memory-unsafe applications, and analyze how hardware may improve the efficiency and security. △ Less","15 November, 2022",https://arxiv.org/pdf/2211.08056
A Novel Design and Improvement of 15-Bar Assembly Tensegrity Robotics Structure,Yunyi Chu,"While the ultimate goal is to produce a tensegrity more than 6 struts, e.g. a 15-bar tensegrity, past experience has demonstrated that we must first develop an innovative system that will facilitate the assembly of a general n-bar tensegrity. To be successful, we believe the development of the new assembly methodology must encompass not only the design of the clamping system but also the design of the tensegrity itself, including the struts, the springs and the spring-to-strut connectors. We therefore propose to develop the 15-bar in two phases: Phase I will be the development of an innovative assembly method, and Phase II will focus on the design and manufacture of a 15-bar tensegrity, with a new strut design probably being part of this. Longer term goals will be aimed at repackaging the wireless electronics on the new struts and adding encoders to control the phase of the motors shafts. △ Less","29 September, 2022",https://arxiv.org/pdf/2211.07515
Unsupervised Face Recognition using Unlabeled Synthetic Data,Fadi Boutros;Marcel Klemt;Meiling Fang;Arjan Kuijper;Naser Damer,"Over the past years, the main research innovations in face recognition focused on training deep neural networks on large-scale identity-labeled datasets using variations of multi-class classification losses. However, many of these datasets are retreated by their creators due to increased privacy and ethical concerns. Very recently, privacy-friendly synthetic data has been proposed as an alternative to privacy-sensitive authentic data to comply with privacy regulations and to ensure the continuity of face recognition research. In this paper, we propose an unsupervised face recognition model based on unlabeled synthetic data (USynthFace). Our proposed USynthFace learns to maximize the similarity between two augmented images of the same synthetic instance. We enable this by a large set of geometric and color transformations in addition to GAN-based augmentation that contributes to the USynthFace model training. We also conduct numerous empirical studies on different components of our USynthFace. With the proposed set of augmentation operations, we proved the effectiveness of our USynthFace in achieving relatively high recognition accuracies using unlabeled synthetic data. △ Less","14 November, 2022",https://arxiv.org/pdf/2211.07371
Deep Learning-enabled Virtual Histological Staining of Biological Samples,Bijie Bai;Xilin Yang;Yuzhu Li;Yijie Zhang;Nir Pillar;Aydogan Ozcan,"Histological staining is the gold standard for tissue examination in clinical pathology and life-science research, which visualizes the tissue and cellular structures using chromatic dyes or fluorescence labels to aid the microscopic assessment of tissue. However, the current histological staining workflow requires tedious sample preparation steps, specialized laboratory infrastructure, and trained histotechnologists, making it expensive, time-consuming, and not accessible in resource-limited settings. Deep learning techniques created new opportunities to revolutionize staining methods by digitally generating histological stains using trained neural networks, providing rapid, cost-effective, and accurate alternatives to standard chemical staining methods. These techniques, broadly referred to as virtual staining, were extensively explored by multiple research groups and demonstrated to be successful in generating various types of histological stains from label-free microscopic images of unstained samples; similar approaches were also used for transforming images of an already stained tissue sample into another type of stain, performing virtual stain-to-stain transformations. In this Review, we provide a comprehensive overview of the recent research advances in deep learning-enabled virtual histological staining techniques. The basic concepts and the typical workflow of virtual staining are introduced, followed by a discussion of representative works and their technical innovations. We also share our perspectives on the future of this emerging field, aiming to inspire readers from diverse scientific fields to further expand the scope of deep learning-enabled virtual histological staining techniques and their applications. △ Less","13 November, 2022",https://arxiv.org/pdf/2211.06822
Innovative Drug-like Molecule Generation from Flow-based Generative Model,Haotian Zhang;Linxiaoyi Wan,"To design a drug given a biological molecule by using deep learning methods, there are many successful models published recently. People commonly used generative models to design new molecules given certain protein. LiGAN was regarded as the baseline of deep learning model which was developed on convolutional neural networks. Recently, GraphBP showed its ability to predict innovative ""real"" chemicals that the binding affinity outperformed with traditional molecular docking methods by using a flow-based generative model with a graph neural network and multilayer perception. However, all those methods regarded proteins as rigid bodies and only include a very small part of proteins related to binding. However, the dynamics of proteins are essential for drug binding. Based on GraphBP, we proposed to generate more solid work derived from protein data bank. The results will be evaluated by validity and binding affinity by using a computational chemistry algorithm. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06566
TAPAS: a Toolbox for Adversarial Privacy Auditing of Synthetic Data,Florimond Houssiau;James Jordon;Samuel N. Cohen;Owen Daniel;Andrew Elliott;James Geddes;Callum Mole;Camila Rangel-Smith;Lukasz Szpruch,"Personal data collected at scale promises to improve decision-making and accelerate innovation. However, sharing and using such data raises serious privacy concerns. A promising solution is to produce synthetic data, artificial records to share instead of real data. Since synthetic records are not linked to real persons, this intuitively prevents classical re-identification attacks. However, this is insufficient to protect privacy. We here present TAPAS, a toolbox of attacks to evaluate synthetic data privacy under a wide range of scenarios. These attacks include generalizations of prior works and novel attacks. We also introduce a general framework for reasoning about privacy threats to synthetic data and showcase TAPAS on several examples. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06550
Social Construction of XAI: Do We Need One Definition to Rule Them All?,Upol Ehsan;Mark O. Riedl,"There is a growing frustration amongst researchers and developers in Explainable AI (XAI) around the lack of consensus around what is meant by 'explainability'. Do we need one definition of explainability to rule them all? In this paper, we argue why a singular definition of XAI is neither feasible nor desirable at this stage of XAI's development. We view XAI through the lenses of Social Construction of Technology (SCOT) to explicate how diverse stakeholders (relevant social groups) have different interpretations (interpretative flexibility) that shape the meaning of XAI. Forcing a standardization (closure) on the pluralistic interpretations too early can stifle innovation and lead to premature conclusions. We share how we can leverage the pluralism to make progress in XAI without having to wait for a definitional consensus. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06499
Integrating machine learning concepts into undergraduate classes,Chinmay Sahu;Blaine Ayotte;Mahesh K. Banavar,"In this innovative practice work-in-progress paper, we compare two different methods to teach machine learning concepts to undergraduate students in Electrical Engineering. While machine learning is now being offered as a senior-level elective in several curricula, this does not mean all students are exposed to it. Exposure to the concepts and practical applications of machine learning will assist in the creation of a workforce ready to tackle problems related to machine learning, currently a hot topic in industry. Preliminary assessments indicate that this approach promotes student learning. While students prefer the proposed side-by-side teaching approach, numerical comparisons show that the workshop approach may be more effective for student learning, indicating that further work in this area is required. △ Less","9 November, 2022",https://arxiv.org/pdf/2211.06491
CryptoHalal: An Intelligent Decision-System for Identifying Halal and Haram Cryptocurrencies,Shahad Al-Khalifa,"In this research, we discussed a rising issue for Muslims in today world that involves a financial and technical innovation, namely: cryptocurrencies. We found out through a questionnaire that many Muslims are having a hard time finding the jurisprudence rulings on certain cryptocurrencies. Therefore, the objective of this research is to investigate and identify features that play a part in determining the jurisprudence rulings on cryptocurrencies. We have collected a dataset containing 106 cryptocurrencies classified into 56 Halal and 50 Haram cryptocurrencies, and used 20 handcrafted features. Moreover, based on these identified features, we designed an intelligent system that contains a Machine Learning model for classifying cryptocurrencies into Halal and Haram. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.06305
Chopin: Combining Distributed and Centralized Schedulers for Self-Adjusting Datacenter Networks,Neta Rozen Schiff;Klaus-Tycho Foerster;Stefan Schmid;David Hay,"The performance of distributed and data-centric applications often critically depends on the interconnecting network. Emerging reconfigurable datacenter networks (RDCNs) are a particularly innovative approach to improve datacenter throughput. Relying on a dynamic optical topology which can be adjusted towards the workload in a demand-aware manner, RDCNs allow to exploit temporal and spatial locality in the communication pattern, and to provide topological shortcuts for frequently communicating racks. The key challenge, however, concerns how to realize demand-awareness in RDCNs in a scalable fashion. This paper presents and evaluates Chopin, a hybrid scheduler for self-adjusting networks that provides demand-awareness at low overhead, by combining centralized and distributed approaches. Chopin allocates optical circuits to elephant flows, through its slower centralized scheduler, utilizing global information. Chopin's distributed scheduler is orders of magnitude faster and can swiftly react to changes in the traffic and adjust the optical circuits accordingly, by using only local information and running at each rack separately. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.06131
Turning disruptive power of Blockchain in the insurance market into innovative opportunities,Wadnerson Boileau,"Insurance has been around for more than centuries. This risk mitigation strategy has been utilized in maritime commerce as early thousand years ago, where Asian merchant seafarers were pooling together their wares in collective funds to pay for damages of individual capsized ship. In 2018, insurance industry made up 6 percent of global GDP while financial industry amounted to about 7 to 9 percent of the US GDP.2020, the industry net premiums totaled USD1.28 trillion, by 2030, blockchain insurance market value is estimated to reach USD39.5 Billion. Despite of growing reform, the insurance market is dominated by intermediaries assisting people to match their insurance needs. While many predictions focused on artificial intelligence, cloud computing, blockchain stands out as the most disruptive technology that can change the driving forces underlying the global economy. This paper presents a blockchain business use case and how insurance industry can turn blockchain disruptive power into innovative opportunities. △ Less","4 December, 2022",https://arxiv.org/pdf/2211.05830
Optimizing the Age of Information in Mixed-Critical Wireless Communication Networks,Robert-Jeron Reifert;Stefan Roth;Aydin Sezgin,"Beyond fifth generation wireless communication networks (B5G) are applied in many use-cases, such as industrial control systems, smart public transport, and power grids. Those applications require innovative techniques for timely transmission and increased wireless network capacities. Hence, this paper proposes optimizing the data freshness measured by the age of information (AoI) in dense internet of things (IoT) sensor-actuator networks. Given different priorities of data-streams, i.e., different sensitivities to outdated information, mixed-criticality is introduced by analyzing different functions of the age, i.e., we consider linear and exponential aging functions. An intricate non-convex optimization problem managing the physical transmission time and packet outage probability is derived. Such problem is tackled using stochastic reformulations, successive convex approximations, and fractional programming, resulting in an efficient iterative algorithm for AoI optimization. Simulation results validate the proposed scheme's performance in terms of AoI, mixed-criticality, and scalability. The proposed non-orthogonal transmission is shown to outperform an orthogonal access scheme in various deployment cases. Results emphasize the potential gains for dense B5G empowered IoT networks in minimizing the AoI. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.05797
Privacy-Preserving Machine Learning for Collaborative Data Sharing via Auto-encoder Latent Space Embeddings,Ana María Quintero-Ossa;Jesús Solano;Hernán Jarcía;David Zarruk;Alejandro Correa Bahnsen;Carlos Valencia,"Privacy-preserving machine learning in data-sharing processes is an ever-critical task that enables collaborative training of Machine Learning (ML) models without the need to share the original data sources. It is especially relevant when an organization must assure that sensitive data remains private throughout the whole ML pipeline, i.e., training and inference phases. This paper presents an innovative framework that uses Representation Learning via autoencoders to generate privacy-preserving embedded data. Thus, organizations can share the data representation to increase machine learning models' performance in scenarios with more than one data source for a shared predictive downstream task. △ Less","10 November, 2022",https://arxiv.org/pdf/2211.05717
MuMIC -- Multimodal Embedding for Multi-label Image Classification with Tempered Sigmoid,Fengjun Wang;Sarai Mizrachi;Moran Beladev;Guy Nadav;Gil Amsalem;Karen Lastmann Assaraf;Hadas Harush Boker,"Multi-label image classification is a foundational topic in various domains. Multimodal learning approaches have recently achieved outstanding results in image representation and single-label image classification. For instance, Contrastive Language-Image Pretraining (CLIP) demonstrates impressive image-text representation learning abilities and is robust to natural distribution shifts. This success inspires us to leverage multimodal learning for multi-label classification tasks, and benefit from contrastively learnt pretrained models. We propose the Multimodal Multi-label Image Classification (MuMIC) framework, which utilizes a hardness-aware tempered sigmoid based Binary Cross Entropy loss function, thus enables the optimization on multi-label objectives and transfer learning on CLIP. MuMIC is capable of providing high classification performance, handling real-world noisy data, supporting zero-shot predictions, and producing domain-specific image embeddings. In this study, a total of 120 image classes are defined, and more than 140K positive annotations are collected on approximately 60K Booking.com images. The final MuMIC model is deployed on Booking.com Content Intelligence Platform, and it outperforms other state-of-the-art models with 85.6% GAP@10 and 83.8% GAP on all 120 classes, as well as a 90.1% macro mAP score across 32 majority classes. We summarize the modeling choices which are extensively tested through ablation studies. To the best of our knowledge, we are the first to adapt contrastively learnt multimodal pretraining for real-world multi-label image classification problems, and the innovation can be transferred to other domains. △ Less","2 November, 2022",https://arxiv.org/pdf/2211.05232
Piano Learning and Improvisation through Adaptive Visualisation and Digital Augmentation,Jordan Aiko Deja,"The task of learning the piano has been a centuries-old challenge for novices, experts and technologists. Several innovations have been introduced to support proper posture, movement, and motivation, while sight-reading and improvisation remain the least-explored areas. In this PhD, we address this gap by redesigning the piano augmentation as an interactive and adaptive space. Specifically, we will explore how to support learners with adaptive visualisations through a two-pronged approach: (1) by designing adaptive visualisations based on the proficiency of the learner to support regular piano playing and (2) by assisting them with expert annotations projected on the piano to encourage improvisation. To this end, we will build a model to understand the complexities of learners' spatiotemporal data and use these to support learning. We will then evaluate our approach through user studies enabling practice and improvisation. Our work contributes to how adaptive visualisations can push music instrument learning and support multi-target selection tasks in immersive spaces. △ Less","11 November, 2022",https://arxiv.org/pdf/2211.04989
A Method to Judge the Style of Classical Poetry Based on Pre-trained Model,Ziyao Wang;Jiandong Zhang;Jun Ma,"One of the important topics in the research field of Chinese classical poetry is to analyze the poetic style. By examining the relevant works of previous dynasties, researchers judge a poetic style mostly by their subjective feelings, and refer to the previous evaluations that have become a certain conclusion. Although this judgment method is often effective, there may be some errors. This paper builds the most perfect data set of Chinese classical poetry at present, trains a BART-poem pre -trained model on this data set, and puts forward a generally applicable poetry style judgment method based on this BART-poem model, innovatively introduces in-depth learning into the field of computational stylistics, and provides a new research method for the study of classical poetry. This paper attempts to use this method to solve the problem of poetry style identification in the Tang and Song Dynasties, and takes the poetry schools that are considered to have a relatively clear and consistent poetic style, such as the Hongzheng Qizi and Jiajing Qizi, Jiangxi poetic school and Tongguang poetic school, as the research object, and takes the poems of their representative poets for testing. Experiments show that the judgment results of the tested poetry work made by the model are basically consistent with the conclusions given by critics of previous dynasties, verify some avant-garde judgments of Mr. Qian Zhongshu, and better solve the task of poetry style recognition in the Tang and Song dynasties. △ Less","8 November, 2022",https://arxiv.org/pdf/2211.04657
Privacy Meets Explainability: A Comprehensive Impact Benchmark,Saifullah Saifullah;Dominique Mercier;Adriano Lucieri;Andreas Dengel;Sheraz Ahmed,"Since the mid-10s, the era of Deep Learning (DL) has continued to this day, bringing forth new superlatives and innovations each year. Nevertheless, the speed with which these innovations translate into real applications lags behind this fast pace. Safety-critical applications, in particular, underlie strict regulatory and ethical requirements which need to be taken care of and are still active areas of debate. eXplainable AI (XAI) and privacy-preserving machine learning (PPML) are both crucial research fields, aiming at mitigating some of the drawbacks of prevailing data-hungry black-box models in DL. Despite brisk research activity in the respective fields, no attention has yet been paid to their interaction. This work is the first to investigate the impact of private learning techniques on generated explanations for DL-based models. In an extensive experimental analysis covering various image and time series datasets from multiple domains, as well as varying privacy techniques, XAI methods, and model architectures, the effects of private training on generated explanations are studied. The findings suggest non-negligible changes in explanations through the introduction of privacy. Apart from reporting individual effects of PPML on XAI, the paper gives clear recommendations for the choice of techniques in real applications. By unveiling the interdependencies of these pivotal technologies, this work is a first step towards overcoming the remaining hurdles for practically applicable AI in safety-critical domains. △ Less","8 November, 2022",https://arxiv.org/pdf/2211.04110
Progress and summary of reinforcement learning on energy management of MPS-EV,Jincheng Hu;Yang Lin;Liang Chu;Zhuoran Hou;Jihan Li;Jingjing Jiang;Yuanjian Zhang,"The high emission and low energy efficiency caused by internal combustion engines (ICE) have become unacceptable under environmental regulations and the energy crisis. As a promising alternative solution, multi-power source electric vehicles (MPS-EVs) introduce different clean energy systems to improve powertrain efficiency. The energy management strategy (EMS) is a critical technology for MPS-EVs to maximize efficiency, fuel economy, and range. Reinforcement learning (RL) has become an effective methodology for the development of EMS. RL has received continuous attention and research, but there is still a lack of systematic analysis of the design elements of RL-based EMS. To this end, this paper presents an in-depth analysis of the current research on RL-based EMS (RL-EMS) and summarizes the design elements of RL-based EMS. This paper first summarizes the previous applications of RL in EMS from five aspects: algorithm, perception scheme, decision scheme, reward function, and innovative training method. The contribution of advanced algorithms to the training effect is shown, the perception and control schemes in the literature are analyzed in detail, different reward function settings are classified, and innovative training methods with their roles are elaborated. Finally, by comparing the development routes of RL and RL-EMS, this paper identifies the gap between advanced RL solutions and existing RL-EMS. Finally, this paper suggests potential development directions for implementing advanced artificial intelligence (AI) solutions in EMS. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.04001
Final Report on MITRE Evaluations for the DARPA Big Mechanism Program,Matthew Peterson;Tonia Korves;Christopher Garay;Robyn Kozierok;Lynette Hirschman,"This report presents the evaluation approach developed for the DARPA Big Mechanism program, which aimed at developing computer systems that will read research papers, integrate the information into a computer model of cancer mechanisms, and frame new hypotheses. We employed an iterative, incremental approach to the evaluation of the three phases of the program. In Phase I, we evaluated the ability of system and human teams ability to read-with-a-model to capture mechanistic information from the biomedical literature, integrated with information from expert curated biological databases. In Phase II we evaluated the ability of systems to assemble fragments of information into a mechanistic model. The Phase III evaluation focused on the ability of systems to provide explanations of experimental observations based on models assembled (largely automatically) by the Big Mechanism process. The evaluation for each phase built on earlier evaluations and guided developers towards creating capabilities for the new phase. The report describes our approach, including innovations such as a reference set (a curated data set limited to major findings of each paper) to assess the accuracy of systems in extracting mechanistic findings in the absence of a gold standard, and a method to evaluate model-based explanations of experimental data. Results of the evaluation and supporting materials are included in the appendices. △ Less","7 November, 2022",https://arxiv.org/pdf/2211.03943
Changes from Classical Statistics to Modern Statistics and Data Science,Kai Zhang;Shan Liu;Momiao Xiong,"A coordinate system is a foundation for every quantitative science, engineering, and medicine. Classical physics and statistics are based on the Cartesian coordinate system. The classical probability and hypothesis testing theory can only be applied to Euclidean data. However, modern data in the real world are from natural language processing, mathematical formulas, social networks, transportation and sensor networks, computer visions, automations, and biomedical measurements. The Euclidean assumption is not appropriate for non Euclidean data. This perspective addresses the urgent need to overcome those fundamental limitations and encourages extensions of classical probability theory and hypothesis testing , diffusion models and stochastic differential equations from Euclidean space to non Euclidean space. Artificial intelligence such as natural language processing, computer vision, graphical neural networks, manifold regression and inference theory, manifold learning, graph neural networks, compositional diffusion models for automatically compositional generations of concepts and demystifying machine learning systems, has been rapidly developed. Differential manifold theory is the mathematic foundations of deep learning and data science as well. We urgently need to shift the paradigm for data analysis from the classical Euclidean data analysis to both Euclidean and non Euclidean data analysis and develop more and more innovative methods for describing, estimating and inferring non Euclidean geometries of modern real datasets. A general framework for integrated analysis of both Euclidean and non Euclidean data, composite AI, decision intelligence and edge AI provide powerful innovative ideas and strategies for fundamentally advancing AI. We are expected to marry statistics with AI, develop a unified theory of modern statistics and drive next generation of AI and data science. △ Less","30 October, 2022",https://arxiv.org/pdf/2211.03756
"An approach to standardize, automate omni-channel and AI transactional digital service creation",Antoine Aamarcha;Martin Caussanel;Hadrien Lanneau;Kevin Mege;Florian Peyron,"Our work is at the crossroads of two categories of technologies. On the one hand, omnichannel digit services, to address the needs of users in the most seamless way. On the other hand, low code approaches, to build simply even complex software applications. In this twofold context, we propose DSUL (Digital Service Universal Language). It allows to build omnichannel services with minimal work from their designers. We describe precisely how DSUL operates, and its innovation in regard to the state of the art. We also consider the various methods to evaluate this framework. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.03543
Recent Developments in Structure-Based Virtual Screening Approaches,Christoph Gorgulla,"Drug development is a wide scientific field that faces many challenges these days. Among them are extremely high development costs, long development times, as well as a low number of new drugs that are approved each year. To solve these problems, new and innovate technologies are needed that make the drug discovery process of small-molecules more time and cost-efficient, and which allow to target previously undruggable target classes such as protein-protein interactions. Structure-based virtual screenings have become a leading contender in this context. In this review, we give an introduction to the foundations of structure-based virtual screenings, and survey their progress in the past few years. We outline key principles, recent success stories, new methods, available software, and promising future research directions. Virtual screenings have an enormous potential for the development of new small-molecule drugs, and are already starting to transform early-stage drug discovery. △ Less","6 November, 2022",https://arxiv.org/pdf/2211.03208
"""Seeing Sound"": Audio Classification with the Wigner-Wille Distribution and Convolutional Neural Networks",Antonios Marios Christonasis;Stef van Eijndhoven;Peter Duin,"With big data becoming increasingly available, IoT hardware becoming widely adopted, and AI capabilities becoming more powerful, organizations are continuously investing in sensing. Data coming from sensor networks are currently combined with sensor fusion and AI algorithms to drive innovation in fields such as self-driving cars. Data from these sensors can be utilized in numerous use cases, including alerts in safety systems of urban settings, for events such as gun shots and explosions. Moreover, diverse types of sensors, such as sound sensors, can be utilized in low-light conditions or at locations where a camera is not available. This paper investigates the potential of the utilization of sound-sensor data in an urban context. Technically, we propose a novel approach of classifying sound data using the Wigner-Ville distribution and Convolutional Neural Networks. In this paper, we report on the performance of the approach on open-source datasets. The concept and work presented is based on my doctoral thesis, which was performed as part of the Engineering Doctorate program in Data Science at the University of Eindhoven, in collaboration with the Dutch National Police. Additional work on real-world datasets was performed during the thesis, which are not presented here due to confidentiality. △ Less","6 November, 2022",https://arxiv.org/pdf/2211.03202
"Rankers, Rankees, & Rankings: Peeking into the Pandora's Box from a Socio-Technical Perspective",Jun Yuan;Julia Stoyanovich;Aritra Dasgupta,"Algorithmic rankers have a profound impact on our increasingly data-driven society. From leisurely activities like the movies that we watch, the restaurants that we patronize; to highly consequential decisions, like making educational and occupational choices or getting hired by companies -- these are all driven by sophisticated yet mostly inaccessible rankers. A small change to how these algorithms process the rankees (i.e., the data items that are ranked) can have profound consequences. For example, a change in rankings can lead to deterioration of the prestige of a university or have drastic consequences on a job candidate who missed out being in the list of the preferred top-k for an organization. This paper is a call to action to the human-centered data science research community to develop principled methods, measures, and metrics for studying the interactions among the socio-technical context of use, technological innovations, and the resulting consequences of algorithmic rankings on multiple stakeholders. Given the spate of new legislations on algorithmic accountability, it is imperative that researchers from social science, human-computer interaction, and data science work in unison for demystifying how rankings are produced, who has agency to change them, and what metrics of socio-technical impact one must use for informing the context of use. △ Less","5 November, 2022",https://arxiv.org/pdf/2211.02932
Forecasting User Interests Through Topic Tag Predictions in Online Health Communities,Amogh Subbakrishna Adishesha;Lily Jakielaszek;Fariha Azhar;Peixuan Zhang;Vasant Honavar;Fenglong Ma;Chandra Belani;Prasenjit Mitra;Sharon Xiaolei Huang,"The increasing reliance on online communities for healthcare information by patients and caregivers has led to the increase in the spread of misinformation, or subjective, anecdotal and inaccurate or non-specific recommendations, which, if acted on, could cause serious harm to the patients. Hence, there is an urgent need to connect users with accurate and tailored health information in a timely manner to prevent such harm. This paper proposes an innovative approach to suggesting reliable information to participants in online communities as they move through different stages in their disease or treatment. We hypothesize that patients with similar histories of disease progression or course of treatment would have similar information needs at comparable stages. Specifically, we pose the problem of predicting topic tags or keywords that describe the future information needs of users based on their profiles, traces of their online interactions within the community (past posts, replies) and the profiles and traces of online interactions of other users with similar profiles and similar traces of past interaction with the target users. The result is a variant of the collaborative information filtering or recommendation system tailored to the needs of users of online health communities. We report results of our experiments on an expert curated data set which demonstrate the superiority of the proposed approach over the state of the art baselines with respect to accurate and timely prediction of topic tags (and hence information sources of interest). △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02789
The Tensor Data Platform: Towards an AI-centric Database System,Apurva Gandhi;Yuki Asada;Victor Fu;Advitya Gemawat;Lihao Zhang;Rathijit Sen;Carlo Curino;Jesús Camacho-Rodríguez;Matteo Interlandi,"Database engines have historically absorbed many of the innovations in data processing, adding features to process graph data, XML, object oriented, and text among many others. In this paper, we make the case that it is time to do the same for AI -- but with a twist! While existing approaches have tried to achieve this by integrating databases with external ML tools, in this paper we claim that achieving a truly AI-centric database requires moving the DBMS engine, at its core, from a relational to a tensor abstraction. This allows us to: (1) support multi-modal data processing such as images, videos, audio, text as well as relational; (2) leverage the wellspring of innovation in HW and runtimes for tensor computation; and (3) exploit automatic differentiation to enable a novel class of ""trainable"" queries that can learn to perform a task. To support the above scenarios, we introduce TDP: a system that builds upon our prior work mapping relational queries to tensors. Thanks to a tighter integration with the tensor runtime, TDP is able to provide a broader coverage of new emerging scenarios requiring access to multi-modal data and automatic differentiation. △ Less","4 November, 2022",https://arxiv.org/pdf/2211.02753
Driving innovation through project based learning: A pre-university STEAM for Social Good initiative,Gayathri Manikutty;Sreejith Sasidharan;Bhavani Rao,"The Covid pandemic is a clarion call for increased sensitivity to the interconnected nature of social problems facing our world today. A future-oriented education on critical issues, such as those outlined in the United Nations Sustainable Development Goals (UN SDGs) and designing potential solutions for such problems is an imperative skill that must be imparted to children to help them navigate their future in today's unpredictable world. Towards this goal, we have been conducting 3.5 month-long mentoring programs for pre-university students in India to participate in a STEAM for Social Good innovation challenge conducted annually by the Government of India. Using digital and physical computing skills, we helped children explore creative solutions for social problems through a constructionist approach to learning, wherein they ideated and reflected upon the problems in their communities. The children learnt the Engineering Design Thinking process and worked in online groups of two or three, from concept to completion. Despite the constraints posed by the pandemic, they explored creative ways to think about design and innovation. They completed a variety of tasks by making, tinkering, engineering, assembling, and programming to grasp the intricate relationship between software and hardware. Subsequently, the children showcased their creative abilities through video storytelling to a panel of domain experts. In this paper, we present the children's perspective of their experiences through this journey, the evaluation metrics based on IEEE design principles, and our learnings from conducting this initiative as a university-school partnership model for 84 middle and high school students. The aspirational intent of this initiative is to make the children better social problem solvers and help them perceive social problems as opportunities to enhance life for themselves and their communities. △ Less","3 November, 2022",https://arxiv.org/pdf/2211.01998
Enhancing Patent Retrieval using Text and Knowledge Graph Embeddings: A Technical Note,L Siddharth;Guangtong Li;Jianxi Luo,"Patent retrieval influences several applications within engineering design research, education, and practice as well as applications that concern innovation, intellectual property, and knowledge management etc. In this article, we propose a method to retrieve patents relevant to an initial set of patents, by synthesizing state-of-the-art techniques among natural language processing and knowledge graph embedding. Our method involves a patent embedding that captures text, citation, and inventor information, which individually represent different facets of knowledge communicated through a patent document. We obtain text embeddings using Sentence-BERT applied to titles and abstracts. We obtain citation and inventor embeddings through TransE that is trained using the corresponding knowledge graphs. We identify using a classification task that the concatenation of text, citation, and inventor embeddings offers a plausible representation of a patent. While the proposed patent embedding could be used to associate a pair of patents, we observe using a recall task that multiple initial patents could be associated with a target patent using mean cosine similarity, which could then be utilized to rank all target patents and retrieve the most relevant ones. We apply the proposed patent retrieval method to a set of patents corresponding to a product family and an inventor's portfolio. △ Less","3 November, 2022",https://arxiv.org/pdf/2211.01976
CircleSnake: Instance Segmentation with Circle Representation,Ethan H. Nguyen;Haichun Yang;Zuhayr Asad;Ruining Deng;Agnes B. Fogo;Yuankai Huo,"Circle representation has recently been introduced as a medical imaging optimized representation for more effective instance object detection on ball-shaped medical objects. With its superior performance on instance detection, it is appealing to extend the circle representation to instance medical object segmentation. In this work, we propose CircleSnake, a simple end-to-end circle contour deformation-based segmentation method for ball-shaped medical objects. Compared to the prevalent DeepSnake method, our contribution is three-fold: (1) We replace the complicated bounding box to octagon contour transformation with a computation-free and consistent bounding circle to circle contour adaption for segmenting ball-shaped medical objects; (2) Circle representation has fewer degrees of freedom (DoF=2) as compared with the octagon representation (DoF=8), thus yielding a more robust segmentation performance and better rotation consistency; (3) To the best of our knowledge, the proposed CircleSnake method is the first end-to-end circle representation deep segmentation pipeline method with consistent circle detection, circle contour proposal, and circular convolution. The key innovation is to integrate the circular graph convolution with circle detection into an end-to-end instance segmentation framework, enabled by the proposed simple and consistent circle contour representation. Glomeruli are used to evaluate the performance of the benchmarks. From the results, CircleSnake increases the average precision of glomerular detection from 0.559 to 0.614. The Dice score increased from 0.804 to 0.849. The code has been released: https://github.com/hrlblab/CircleSnake △ Less","2 November, 2022",https://arxiv.org/pdf/2211.01254
Interactive Imitation Learning in Robotics: A Survey,Carlos Celemin;Rodrigo Pérez-Dattari;Eugenio Chisari;Giovanni Franzese;Leandro de Souza Rosa;Ravi Prakash;Zlatan Ajanović;Marta Ferraz;Abhinav Valada;Jens Kober,"Interactive Imitation Learning (IIL) is a branch of Imitation Learning (IL) where human feedback is provided intermittently during robot execution allowing an online improvement of the robot's behavior. In recent years, IIL has increasingly started to carve out its own space as a promising data-driven alternative for solving complex robotic tasks. The advantages of IIL are its data-efficient, as the human feedback guides the robot directly towards an improved behavior, and its robustness, as the distribution mismatch between the teacher and learner trajectories is minimized by providing feedback directly over the learner's trajectories. Nevertheless, despite the opportunities that IIL presents, its terminology, structure, and applicability are not clear nor unified in the literature, slowing down its development and, therefore, the research of innovative formulations and discoveries. In this article, we attempt to facilitate research in IIL and lower entry barriers for new practitioners by providing a survey of the field that unifies and structures it. In addition, we aim to raise awareness of its potential, what has been accomplished and what are still open research questions. We organize the most relevant works in IIL in terms of human-robot interaction (i.e., types of feedback), interfaces (i.e., means of providing feedback), learning (i.e., models learned from feedback and function approximators), user experience (i.e., human perception about the learning process), applications, and benchmarks. Furthermore, we analyze similarities and differences between IIL and RL, providing a discussion on how the concepts offline, online, off-policy and on-policy learning should be transferred to IIL from the RL literature. We particularly focus on robotic applications in the real world and discuss their implications, limitations, and promising future areas of research. △ Less","31 October, 2022",https://arxiv.org/pdf/2211.00600
Classical ensemble of Quantum-classical ML algorithms for Phishing detection in Ethereum transaction networks,Anupama Ray;Sai Sakunthala Guddanti;Vishnu Ajith;Dhinakaran Vinayagamurthy,"Ethereum is one of the most valuable blockchain networks in terms of the total monetary value locked in it, and arguably been the most active network where new blockchain innovations in research and applications are demonstrated. But, this also leads to Ethereum network being susceptible to a wide variety of threats and attacks in an attempt to gain unreasonable advantage or to undermine the value of the users. Even with the state-of-art classical ML algorithms, detecting such attacks is still hard. This motivated us to build a hybrid system of quantum-classical algorithms that improves phishing detection in financial transaction networks. This paper presents a classical ensemble pipeline of classical and quantum algorithms and a detailed study benchmarking existing Quantum Machine Learning algorithms such as Quantum Support Vector Machine and Variational Quantum Classifier. With the current generation of quantum hardware available, smaller datasets are more suited to the QML models and most research restricts to hundreds of samples. However, we experimented on different data sizes and report results with a test data of 12K transaction nodes, which is to the best of the authors knowledge the largest QML experiment run so far on any real quantum hardware. The classical ensembles of quantum-classical models improved the macro F-score and phishing F-score. One key observation is QSVM constantly gives lower false positives, thereby higher precision compared with any other classical or quantum network, which is always preferred for any anomaly detection problem. This is true for QSVMs when used individually or via bagging of same models or in combination with other classical/quantum models making it the most advantageous quantum algorithm so far. The proposed ensemble framework is generic and can be applied for any classification task △ Less","30 October, 2022",https://arxiv.org/pdf/2211.00004
Leveraging Pre-trained Models for Failure Analysis Triplets Generation,Kenneth Ezukwoke;Anis Hoayek;Mireille Batton-Hubert;Xavier Boucher;Pascal Gounet;Jerome Adrian,"Pre-trained Language Models recently gained traction in the Natural Language Processing (NLP) domain for text summarization, generation and question-answering tasks. This stems from the innovation introduced in Transformer models and their overwhelming performance compared with Recurrent Neural Network Models (Long Short Term Memory (LSTM)). In this paper, we leverage the attention mechanism of pre-trained causal language models such as Transformer model for the downstream task of generating Failure Analysis Triplets (FATs) - a sequence of steps for analyzing defected components in the semiconductor industry. We compare different transformer models for this generative task and observe that Generative Pre-trained Transformer 2 (GPT2) outperformed other transformer model for the failure analysis triplet generation (FATG) task. In particular, we observe that GPT2 (trained on 1.5B parameters) outperforms pre-trained BERT, BART and GPT3 by a large margin on ROUGE. Furthermore, we introduce Levenshstein Sequential Evaluation metric (LESE) for better evaluation of the structured FAT data and show that it compares exactly with human judgment than existing metrics. △ Less","31 October, 2022",https://arxiv.org/pdf/2210.17497
SIX-Trust for 6G: Towards a Secure and Trustworthy 6G Network,Yiying Wang;Xin Kang;Tieyan Li;Haiguang Wang;Cheng-Kang Chu;Zhongding Lei,"Recent years have witnessed a digital explosion with the deployment of 5G and proliferation of 5G-enabled innovations. Compared with 5G, 6G is envisioned to achieve much higher performance in terms of latency, data rate, connectivity, energy efficiency, coverage and mobility. To fulfil these expectations, 6G will experience a number of paradigm shifts, such as exploiting new spectrum, applying ubiquitous ML/AI technologies and building a space-air-ground-sea integrated network. However, these paradigm shifts may lead to numerous new security and privacy issues, which traditional security measures may not be able to deal with. To tackle these issues and build a trustworthy 6G network, we introduce a novel trust framework named as SIX-Trust, which composes of 3 layers: sustainable trust (S-Trust), infrastructure trust (I-Trust) and xenogenesis trust (X-Trust). Each layer plays a different role, and the importance of each layer varies for different application scenarios of 6G. For each layer, we briefly introduce its related enabling technologies, and demonstrate how these technologies can be applied to enhance trust and security of the 6G network. In general, SIX-Trust provides a holistic framework for defining and modeling trust of 6G, which can facilitate establishing a trustworthy 6G network. △ Less","31 October, 2022",https://arxiv.org/pdf/2210.17291
A Pipeline for Analysing Grant Applications,Shuaiqun Pan;Sergio J. Rodríguez Méndez;Kerry Taylor,"Data mining techniques can transform massive amounts of unstructured data into quantitative data that quickly reveal insights, trends, and patterns behind the original data. In this paper, a data mining model is applied to analyse the 2019 grant applications submitted to an Australian Government research funding agency to investigate whether grant schemes successfully identifies innovative project proposals, as intended. The grant applications are peer-reviewed research proposals that include specific ``innovation and creativity'' (IC) scores assigned by reviewers. In addition to predicting the IC score for each research proposal, we are particularly interested in understanding the vocabulary of innovative proposals. In order to solve this problem, various data mining models and feature encoding algorithms are studied and explored. As a result, we propose a model with the best performance, a Random Forest (RF) classifier over documents encoded with features denoting the presence or absence of unigrams. In specific, the unigram terms are encoded by a modified Term Frequency - Inverse Document Frequency (TF-IDF) algorithm, which only implements the IDF part of TF-IDF. Besides the proposed model, this paper also presents a rigorous experimental pipeline for analysing grant applications, and the experimental results prove its feasibility. △ Less","30 October, 2022",https://arxiv.org/pdf/2210.16843
Not Another Day Zero: Design Hackathons for Community-Based Water Quality Monitoring,Srishti Gupta;Chun-Hua Tsai;John M. Carroll,"This study looks at water quality monitoring and management as a new form of community engagement. Through a series of a unique research method called `design hackathons', we engaged with a hyperlocal community of citizens who are actively involved in monitoring and management of their local watershed. These design hackathons sought to understand the motivation, practices, collaboration and experiences of these citizens. Qualitative analysis of data revealed the nature of the complex stakeholder network, workflow practices, initiatives to engage with a larger community, current state of technological infrastructure being used, and innovative design scenarios proposed by the hackathon participants. Based on this comprehensive analysis, we conceptualize water quality monitoring and management as community-based monitoring and management, and water data as community data. Such a conceptualization sheds light on how these practices can help in preempting water crisis by empowering citizens through increased awareness, active participation and informal learning of water data and resources. △ Less","28 October, 2022",https://arxiv.org/pdf/2210.16381
PolyGloT: A Personalized and Gamified eTutoring System,Antonio Bucchiarone;Tommaso Martorella;Diego Colombo,"The digital age is changing the role of educators and pushing for a paradigm shift in the education system as a whole. Growing demand for general and specialized education inside and outside classrooms is at the heart of this rising trend. In modern, heterogeneous learning environments, the one-size-fits-all approach is proven to be fundamentally flawed. Individualization through adaptivity is, therefore, crucial to nurture individual potential and address accessibility needs and neurodiversity. By formalizing a learning framework that takes into account all these different aspects, we aim to define and implement an open, content-agnostic, and extensible eTutoring platform to design and consume adaptive and gamified learning experiences. Adaptive technology supplementing teaching can extend the reach of every teacher, making it possible to scale 1-1 learning experiences. There are many successful existing technologies available but they come with fixed environments that are not always suitable for the targeted audiences of the course material. This paper presents PolyGloT, a system able to help teachers to design and implement a gamified and adaptive learning paths. Through it we address some important issues including the engagement, fairness, and effectiveness of learning environments. We do not only propose an innovative platform that could foster the learning process of different disciplines, but it could also help teachers and instructors in organizing learning material in an easy-access repository △ Less","27 October, 2022",https://arxiv.org/pdf/2210.15256
ViT-CAT: Parallel Vision Transformers with Cross Attention Fusion for Popularity Prediction in MEC Networks,Zohreh HajiAkhondi-Meybodi;Arash Mohammadi;Ming Hou;Jamshid Abouei;Konstantinos N. Plataniotis,"Mobile Edge Caching (MEC) is a revolutionary technology for the Sixth Generation (6G) of wireless networks with the promise to significantly reduce users' latency via offering storage capacities at the edge of the network. The efficiency of the MEC network, however, critically depends on its ability to dynamically predict/update the storage of caching nodes with the top-K popular contents. Conventional statistical caching schemes are not robust to the time-variant nature of the underlying pattern of content requests, resulting in a surge of interest in using Deep Neural Networks (DNNs) for time-series popularity prediction in MEC networks. However, existing DNN models within the context of MEC fail to simultaneously capture both temporal correlations of historical request patterns and the dependencies between multiple contents. This necessitates an urgent quest to develop and design a new and innovative popularity prediction architecture to tackle this critical challenge. The paper addresses this gap by proposing a novel hybrid caching framework based on the attention mechanism. Referred to as the parallel Vision Transformers with Cross Attention (ViT-CAT) Fusion, the proposed architecture consists of two parallel ViT networks, one for collecting temporal correlation, and the other for capturing dependencies between different contents. Followed by a Cross Attention (CA) module as the Fusion Center (FC), the proposed ViT-CAT is capable of learning the mutual information between temporal and spatial correlations, as well, resulting in improving the classification accuracy, and decreasing the model's complexity about 8 times. Based on the simulation results, the proposed ViT-CAT architecture outperforms its counterparts across the classification accuracy, complexity, and cache-hit ratio. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.15125
"M^3
ViT: Mixture-of-Experts Vision Transformer for Efficient Multi-task Learning with Model-Accelerator Co-design",Hanxue Liang;Zhiwen Fan;Rishov Sarkar;Ziyu Jiang;Tianlong Chen;Kai Zou;Yu Cheng;Cong Hao;Zhangyang Wang,"Multi-task learning (MTL) encapsulates multiple learned tasks in a single model and often lets those tasks learn better jointly. However, when deploying MTL onto those real-world systems that are often resource-constrained or latency-sensitive, two prominent challenges arise: (i) during training, simultaneously optimizing all tasks is often difficult due to gradient conflicts across tasks; (ii) at inference, current MTL regimes have to activate nearly the entire model even to just execute a single task. Yet most real systems demand only one or two tasks at each moment, and switch between tasks as needed: therefore such all tasks activated inference is also highly inefficient and non-scalable. In this paper, we present a model-accelerator co-design framework to enable efficient on-device MTL. Our framework, dubbed M^3ViT, customizes mixture-of-experts (MoE) layers into a vision transformer (ViT) backbone for MTL, and sparsely activates task-specific experts during training. Then at inference with any task of interest, the same design allows for activating only the task-corresponding sparse expert pathway, instead of the full model. Our new model design is further enhanced by hardware-level innovations, in particular, a novel computation reordering scheme tailored for memory-constrained MTL that achieves zero-overhead switching between tasks and can scale to any number of experts. When executing single-task inference, M^{3}ViT achieves higher accuracies than encoder-focused MTL methods, while significantly reducing 88% inference FLOPs. When implemented on a hardware platform of one Xilinx ZCU104 FPGA, our co-design framework reduces the memory requirement by 2.4 times, while achieving energy efficiency up to 9.23 times higher than a comparable FPGA baseline. Code is available at: https://github.com/VITA-Group/M3ViT. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.14793
Enhancing Product Safety in E-Commerce with NLP,Kishaloy Halder;Josip Krapac;Dmitry Goryunov;Anthony Brew;Matti Lyra;Alsida Dizdari;William Gillett;Adrien Renahy;Sinan Tang,"Ensuring safety of the products offered to the customers is of paramount importance to any e- commerce platform. Despite stringent quality and safety checking of products listed on these platforms, occasionally customers might receive a product that can pose a safety issue arising out of its use. In this paper, we present an innovative mechanism of how a large scale multinational e-commerce platform, Zalando, uses Natural Language Processing techniques to assist timely investigation of the potentially unsafe products mined directly from customer written claims in unstructured plain text. We systematically describe the types of safety issues that concern Zalando customers. We demonstrate how we map this core business problem into a supervised text classification problem with highly imbalanced, noisy, multilingual data in a AI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven evaluation. Finally, we present detailed ablation studies to show a comprehensive comparison between different classification techniques. We conclude the work with how this NLP model was deployed. △ Less","25 October, 2022",https://arxiv.org/pdf/2210.14363
Finding Early Adopters of Innovation in Social Network,Balázs R. Sziklai;Balázs Lengyel,"Social networks play a fundamental role in the diffusion of innovation through peers' influence on adoption. Thus, network position including a wide range of network centrality measures have been used to describe individuals' affinity to adopt an innovation and their ability to propagate diffusion. Yet, social networks are assortative in terms of susceptibility and influence and in terms of network centralities as well. This makes the identification of influencers difficult especially since susceptibility and centrality does not always go hand in hand. Here we propose the Top Candidate algorithm, an expert recommendation method, to rank individuals based on their perceived expertise, which resonates well with the assortative nature of innovators and early adopters. Leveraging adoption data from two online social networks that are assortative in terms of adoption but represent different levels of assortativity of network centralities, we demonstrate that the Top Candidate ranking is more efficient in capturing early adopters than other widely used indices. Top Candidate nodes adopt earlier and have higher reach among innovators, early adopters and early majority than nodes highlighted by other methods. These results suggest that the Top Candidate method can identify good seeds for influence maximization campaigns on social networks. △ Less","25 October, 2022",https://arxiv.org/pdf/2210.13907
Salient Object Detection via Dynamic Scale Routing,Zhenyu Wu;Shuai Li;Chenglizhao Chen;Hong Qin;Aimin Hao,"Recent research advances in salient object detection (SOD) could largely be attributed to ever-stronger multi-scale feature representation empowered by the deep learning technologies. The existing SOD deep models extract multi-scale features via the off-the-shelf encoders and combine them smartly via various delicate decoders. However, the kernel sizes in this commonly-used thread are usually ""fixed"". In our new experiments, we have observed that kernels of small size are preferable in scenarios containing tiny salient objects. In contrast, large kernel sizes could perform better for images with large salient objects. Inspired by this observation, we advocate the ""dynamic"" scale routing (as a brand-new idea) in this paper. It will result in a generic plug-in that could directly fit the existing feature backbone. This paper's key technical innovations are two-fold. First, instead of using the vanilla convolution with fixed kernel sizes for the encoder design, we propose the dynamic pyramid convolution (DPConv), which dynamically selects the best-suited kernel sizes w.r.t. the given input. Second, we provide a self-adaptive bidirectional decoder design to accommodate the DPConv-based encoder best. The most significant highlight is its capability of routing between feature scales and their dynamic collection, making the inference process scale-aware. As a result, this paper continues to enhance the current SOTA performance. Both the code and dataset are publicly available at https://github.com/wuzhenyubuaa/DPNet. △ Less","25 October, 2022",https://arxiv.org/pdf/2210.13821
Novelty Detection in Time Series via Weak Innovations Representation: A Deep Learning Approach,Xinyi Wang;Mei-jen Lee;Qing Zhao;Lang Tong,"We consider novelty detection in time series with unknown and nonparametric probability structures. A deep learning approach is proposed to causally extract an innovations sequence consisting of novelty samples statistically independent of all past samples of the time series. A novelty detection algorithm is developed for the online detection of novel changes in the probability structure in the innovations sequence. A minimax optimality under a Bayes risk measure is established for the proposed novelty detection method, and its robustness and efficacy are demonstrated in experiments using real and synthetic datasets. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13358
SurferMonkey: A Decentralized Anonymous Blockchain Intercommunication System via Zero Knowledge Proofs,Miguel Díaz Montiel;Rachid Guerraoui;Pierre-Louis Roman,"Blockchain intercommunication systems enable the exchanges of messages between blockchains. This interoperability promotes innovation, unlocks liquidity and access to assets. However, blockchains are isolated systems that originally were not designed for interoperability. This makes cross-chain communication, or bridges for short, insecure by nature. More precisely, cross-chain systems face security challenges in terms of selfish rational players such as maximal extractable value (MEV) and censorship. We propose to solve these challenges using zero knowledge proofs (ZKPs) for cross-chain communication. Securing cross-chain communication is remarkably more complex than securing single-chain events as such a system must preserve user security against both on- and off-chain analysis. To achieve this goal, we propose the following pair of contributions: the DACT protocol and the SurferMonkey infrastructure that supports the DACT protocol. The DACT protocol is a global solution for the anonymity and security challenges of agnostic blockchain intercommunication. DACT breaks on- and off-chain analysis thanks to the use of ZKPs. SurferMonkey is a decentralized infrastructure that implements DACT in practice. Since SurferMonkey works at the blockchain application layer, any decentralized application (dApp) can use SurferMonkey to send any type of message to a dApp on another blockchain. With SurferMonkey, users can neither be censored nor be exposed to MEV. By applying decentralized proactive security, we obtain resilience against selfish rational players, and raise the security bar against cyberattacks. We have implemented a proof of concept (PoC) of SurferMonkey by reverse engineering Tornado Cash and by applying IDEN3 ZKP circuits. SurferMonkey enables new usecases, ranging from anonymous voting and gaming, to a new phase of anonymous decentralized finance (aDeFi). △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13242
A Novel Frame Structure for Cloud-Based Audio-Visual Speech Enhancement in Multimodal Hearing-aids,Abhijeet Bishnu;Ankit Gupta;Mandar Gogate;Kia Dashtipour;Ahsan Adeel;Amir Hussain;Mathini Sellathurai;Tharmalingam Ratnarajah,"In this paper, we design a first of its kind transceiver (PHY layer) prototype for cloud-based audio-visual (AV) speech enhancement (SE) complying with high data rate and low latency requirements of future multimodal hearing assistive technology. The innovative design needs to meet multiple challenging constraints including up/down link communications, delay of transmission and signal processing, and real-time AV SE models processing. The transceiver includes device detection, frame detection, frequency offset estimation, and channel estimation capabilities. We develop both uplink (hearing aid to the cloud) and downlink (cloud to hearing aid) frame structures based on the data rate and latency requirements. Due to the varying nature of uplink information (audio and lip-reading), the uplink channel supports multiple data rate frame structure, while the downlink channel has a fixed data rate frame structure. In addition, we evaluate the latency of different PHY layer blocks of the transceiver for developed frame structures using LabVIEW NXG. This can be used with software defined radio (such as Universal Software Radio Peripheral) for real-time demonstration scenarios. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13127
EpipolarNVS: leveraging on Epipolar geometry for single-image Novel View Synthesis,Gaétan Landreau;Mohamed Tamaazousti,"Novel-view synthesis (NVS) can be tackled through different approaches, depending on the general setting: a single source image to a short video sequence, exact or noisy camera pose information, 3D-based information such as point clouds etc. The most challenging scenario, the one where we stand in this work, only considers a unique source image to generate a novel one from another viewpoint. However, in such a tricky situation, the latest learning-based solutions often struggle to integrate the camera viewpoint transformation. Indeed, the extrinsic information is often passed as-is, through a low-dimensional vector. It might even occur that such a camera pose, when parametrized as Euler angles, is quantized through a one-hot representation. This vanilla encoding choice prevents the learnt architecture from inferring novel views on a continuous basis (from a camera pose perspective). We claim it exists an elegant way to better encode relative camera pose, by leveraging 3D-related concepts such as the epipolar constraint. We, therefore, introduce an innovative method that encodes the viewpoint transformation as a 2D feature image. Such a camera encoding strategy gives meaningful insights to the network regarding how the camera has moved in space between the two views. By encoding the camera pose information as a finite number of coloured epipolar lines, we demonstrate through our experiments that our strategy outperforms vanilla encoding. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13077
Hardness in Markov Decision Processes: Theory and Practice,Michelangelo Conserva;Paulo Rauber,"Meticulously analysing the empirical strengths and weaknesses of reinforcement learning methods in hard (challenging) environments is essential to inspire innovations and assess progress in the field. In tabular reinforcement learning, there is no well-established standard selection of environments to conduct such analysis, which is partially due to the lack of a widespread understanding of the rich theory of hardness of environments. The goal of this paper is to unlock the practical usefulness of this theory through four main contributions. First, we present a systematic survey of the theory of hardness, which also identifies promising research directions. Second, we introduce Colosseum, a pioneering package that enables empirical hardness analysis and implements a principled benchmark composed of environments that are diverse with respect to different measures of hardness. Third, we present an empirical analysis that provides new insights into computable measures. Finally, we benchmark five tabular agents in our newly proposed benchmark. While advancing the theoretical understanding of hardness in non-tabular reinforcement learning remains essential, our contributions in the tabular setting are intended as solid steps towards a principled non-tabular benchmark. Accordingly, we benchmark four agents in non-tabular versions of Colosseum environments, obtaining results that demonstrate the generality of tabular hardness measures. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.13075
The Future of Work: Agile in a Hybrid World,Dennis Mancl;Steven D. Fraser,"An agile organization adapts what they are building to match their customer's evolving needs. Agile teams also adapt to changes in their organization's work environment. The latest change is the evolving environment of ""hybrid"" work - a mix of in-person and virtual staff. Team members might sometimes work together in the office, work from home, or work in other locations, and they may struggle to sustain a high level of collaboration and innovation. It isn't just pandemic social distancing - many of us want to work from home to eliminate our commute and spend more time with family. Are there learnings and best practices that organizations can use to become and stay effective in a hybrid world? An XP 2022 panel organized by Steven Fraser (Innoxec) discussed these questions in June 2022. The panel was facilitated by Hendrik Esser (Ericsson) and featured Alistair Cockburn (Heart of Agile), Sandy Mamoli (Nomad8), Nils Brede Moe (SINTEF), Jaana Nyfjord (Spotify), and Darja Smite (Blekinge Institute of Technology). △ Less","22 October, 2022",https://arxiv.org/pdf/2210.12591
Probing Transfer in Deep Reinforcement Learning without Task Engineering,Andrei A. Rusu;Sebastian Flennerhag;Dushyant Rao;Razvan Pascanu;Raia Hadsell,"We evaluate the use of original game curricula supported by the Atari 2600 console as a heterogeneous transfer benchmark for deep reinforcement learning agents. Game designers created curricula using combinations of several discrete modifications to the basic versions of games such as Space Invaders, Breakout and Freeway, making them progressively more challenging for human players. By formally organising these modifications into several factors of variation, we are able to show that Analyses of Variance (ANOVA) are a potent tool for studying the effects of human-relevant domain changes on the learning and transfer performance of a deep reinforcement learning agent. Since no manual task engineering is needed on our part, leveraging the original multi-factorial design avoids the pitfalls of unintentionally biasing the experimental setup. We find that game design factors have a large and statistically significant impact on an agent's ability to learn, and so do their combinatorial interactions. Furthermore, we show that zero-shot transfer from the basic games to their respective variations is possible, but the variance in performance is also largely explained by interactions between factors. As such, we argue that Atari game curricula offer a challenging benchmark for transfer learning in RL, that can help the community better understand the generalisation capabilities of RL agents along dimensions which meaningfully impact human generalisation performance. As a start, we report that value-function finetuning of regularly trained agents achieves positive transfer in a majority of cases, but significant headroom for algorithmic innovation remains. We conclude with the observation that selective transfer from multiple variants could further improve performance. △ Less","22 October, 2022",https://arxiv.org/pdf/2210.12448
Spectrum-BERT: Pre-training of Deep Bidirectional Transformers for Spectral Classification of Chinese Liquors,Yansong Wang;Yundong Sun;Yansheng Fu;Dongjie Zhu;Zhaoshuo Tian,"Spectral detection technology, as a non-invasive method for rapid detection of substances, combined with deep learning algorithms, has been widely used in food detection. However, in real scenarios, acquiring and labeling spectral data is an extremely labor-intensive task, which makes it impossible to provide enough high-quality data for training efficient supervised deep learning models. To better leverage limited samples, we apply pre-training & fine-tuning paradigm to the field of spectral detection for the first time and propose a pre-training method of deep bidirectional transformers for spectral classification of Chinese liquors, abbreviated as Spectrum-BERT. Specifically, first, to retain the model's sensitivity to the characteristic peak position and local information of the spectral curve, we innovatively partition the curve into multiple blocks and obtain the embeddings of different blocks, as the feature input for the next calculation. Second, in the pre-training stage, we elaborately design two pre-training tasks, Next Curve Prediction (NCP) and Masked Curve Model (MCM), so that the model can effectively utilize unlabeled samples to capture the potential knowledge of spectral data, breaking the restrictions of the insufficient labeled samples, and improving the applicability and performance of the model in practical scenarios. Finally, we conduct a large number of experiments on the real liquor spectral dataset. In the comparative experiments, the proposed Spectrum-BERT significantly outperforms the baselines in multiple metrics and this advantage is more significant on the imbalanced dataset. Moreover, in the parameter sensitivity experiment, we also analyze the model performance under different parameter settings, to provide a reference for subsequent research. △ Less","22 October, 2022",https://arxiv.org/pdf/2210.12440
Ethics for Digital Medicine: A Path for Ethical Emerging Medical IoT Design,Sudeep Pasricha,"The dawn of the digital medicine era, ushered in by increasingly powerful embedded systems and Internet of Things (IoT) computing devices, is creating new therapies and biomedical solutions that promise to positively transform our quality of life. However, the digital medicine revolution also creates unforeseen and complex ethical, regulatory, and societal issues. In this article, we reflect on the ethical challenges facing digital medicine. We discuss the perils of ethical oversights in medical devices, and the role of professional codes and regulatory oversight towards the ethical design, deployment, and operation of digital medicine devices that safely and effectively meet the needs of patients. We advocate for an ensemble approach of intensive education, programmable ethical behaviors, and ethical analysis frameworks, to prevent mishaps and sustain ethical innovation, design, and lifecycle management of emerging digital medicine devices. △ Less","21 October, 2022",https://arxiv.org/pdf/2210.12007
WristSketcher: Creating Dynamic Sketches in AR with a Sensing Wristband,Enting Ying;Tianyang Xiong;Shihui Guo;Ming Qiu;Yipeng Qin;Hongbo Fu,"Restricted by the limited interaction area of native AR glasses (e.g., touch bars), it is challenging to create sketches in AR glasses. Recent works have attempted to use mobile devices (e.g., tablets) or mid-air bare-hand gestures to expand the interactive spaces and can work as the 2D/3D sketching input interfaces for AR glasses. Between them, mobile devices allow for accurate sketching but are often heavy to carry, while sketching with bare hands is zero-burden but can be inaccurate due to arm instability. In addition, mid-air bare-hand sketching can easily lead to social misunderstandings and its prolonged use can cause arm fatigue. As a new attempt, in this work, we present WristSketcher, a new AR system based on a flexible sensing wristband for creating 2D dynamic sketches, featuring an almost zero-burden authoring model for accurate and comfortable sketch creation in real-world scenarios. Specifically, we have streamlined the interaction space from the mid-air to the surface of a lightweight sensing wristband, and implemented AR sketching and associated interaction commands by developing a gesture recognition method based on the sensing pressure points on the wristband. The set of interactive gestures used by our WristSketcher is determined by a heuristic study on user preferences. Moreover, we endow our WristSketcher with the ability of animation creation, allowing it to create dynamic and expressive sketches. Experimental results demonstrate that our WristSketcher i) faithfully recognizes users' gesture interactions with a high accuracy of 96.0%; ii) achieves higher sketching accuracy than Freehand sketching; iii) achieves high user satisfaction in ease of use, usability and functionality; and iv) shows innovation potentials in art creation, memory aids, and entertainment applications. △ Less","26 October, 2022",https://arxiv.org/pdf/2210.11674
Optical Networking in Future-land: From Optical-bypass-enabled to Optical-processing-enabled Paradigm,Dao Thanh Hai,"Conventional wisdom in designing the optical switching nodes is rooted in the intuition that when an optical channel crossing an intermediate node, it should be maximally isolated from other optical channels to avoid interference. Such long-established paradigm perceiving the interference of optical channels transiting at the same node as an adversarial factor and should therefore circumvent, albeit reasonable, may leave vast unexplored opportunities. Indeed, rapid advances in all-optical signal processing technologies has brought opportunities to re-define the optical node architecture by upgrading its naive functionalities from simply add/drop and cross-connecting to proactively mixing optical channels in photonic domain. Specifically, all-optical channel (de-) aggregation technologies have been remarkably advancing in recent years, permitting two or more optical channels at lower bit-rate and/or modulation formats could be all-optically aggregated to a single channel of higher-rate and/or higher-order modulation format and vice versa. Such evolutionary technique is poised to disrupt the existing ecosystem for optical network design and planning, and thus necessitates for a radical change to unlock new potentials. In addressing this disruptive idea, we present a new paradigm for future optical networks, namely, optical-processing-enabled networks powered by in-network all-optical mixing capability. We introduce the operational principle of optical channel (de-) aggregation and show how spectrally beneficial such innovative operations could yield by an illustrative example. Next, in order to maximize the aggregation opportunity, we present a mathematical model for optimal routing based on integer linear programming model. Numerical results on the realistic network topology COST239 are provided to quantify the spectral gain of aggregation-aware routing compared to the conventional one. △ Less","20 October, 2022",https://arxiv.org/pdf/2210.11496
ProSky: NEAT Meets NOMA-mmWave in the Sky of 6G,Ahmed Benfaid;Nadia Adem;Abdurrahman Elmaghbub,"Rendering to their abilities to provide ubiquitous connectivity, flexibly and cost effectively, unmanned aerial vehicles (UAVs) have been getting more and more research attention. To take the UAVs' performance to the next level, however, they need to be merged with some other technologies like non-orthogonal multiple access (NOMA) and millimeter wave (mmWave), which both promise high spectral efficiency (SE). As managing UAVs efficiently may not be possible using model-based techniques, another key innovative technology that UAVs will inevitably need to leverage is artificial intelligence (AI). Designing an AI-based technique that adaptively allocates radio resources and places UAVs in 3D space to meet certain communication objectives, however, is a tough row to hoe. In this paper, we propose a neuroevolution of augmenting topologies NEAT framework, referred to as ProSky, to manage NOMA-mmWave-UAV networks. ProSky exhibits a remarkable performance improvement over a model-based method. Moreover, ProSky learns 5.3 times faster than and outperforms, in both SE and energy efficiency EE while being reasonably fair, a deep reinforcement learning DRL based scheme. The ProSky source code is accessible to use here: https://github.com/Fouzibenfaid/ProSky △ Less","12 October, 2022",https://arxiv.org/pdf/2210.11406
Gaussian-Bernoulli RBMs Without Tears,Renjie Liao;Simon Kornblith;Mengye Ren;David J. Fleet;Geoffrey Hinton,"We revisit the challenging problem of training Gaussian-Bernoulli restricted Boltzmann machines (GRBMs), introducing two innovations. We propose a novel Gibbs-Langevin sampling algorithm that outperforms existing methods like Gibbs sampling. We propose a modified contrastive divergence (CD) algorithm so that one can generate images with GRBMs starting from noise. This enables direct comparison of GRBMs with deep generative models, improving evaluation protocols in the RBM literature. Moreover, we show that modified CD and gradient clipping are enough to robustly train GRBMs with large learning rates, thus removing the necessity of various tricks in the literature. Experiments on Gaussian Mixtures, MNIST, FashionMNIST, and CelebA show GRBMs can generate good samples, despite their single-hidden-layer architecture. Our code is released at: \url{https://github.com/lrjconan/GRBM}. △ Less","19 October, 2022",https://arxiv.org/pdf/2210.10318
LAVA: Label-efficient Visual Learning and Adaptation,Islam Nassar;Munawar Hayat;Ehsan Abbasnejad;Hamid Rezatofighi;Mehrtash Harandi;Gholamreza Haffari,"We present LAVA, a simple yet effective method for multi-domain visual transfer learning with limited data. LAVA builds on a few recent innovations to enable adapting to partially labelled datasets with class and domain shifts. First, LAVA learns self-supervised visual representations on the source dataset and ground them using class label semantics to overcome transfer collapse problems associated with supervised pretraining. Secondly, LAVA maximises the gains from unlabelled target data via a novel method which uses multi-crop augmentations to obtain highly robust pseudo-labels. By combining these ingredients, LAVA achieves a new state-of-the-art on ImageNet semi-supervised protocol, as well as on 7 out of 10 datasets in multi-domain few-shot learning on the Meta-dataset. Code and models are made available. △ Less","19 October, 2022",https://arxiv.org/pdf/2210.10317
Guiding Data-Driven Design Ideation by Knowledge Distance,Jianxi Luo;Serhad Sarica;Kristin Wood,"Data-driven conceptual design methods and tools aim to inspire human ideation for new design concepts by providing external inspirational stimuli. In prior studies, the stimuli have been limited in terms of coverage, granularity, and retrieval guidance. Here, we present a knowledge based expert system that provides design stimuli across the semantic, document and field levels simultaneously from all fields of engineering and technology and that follows creativity theories to guide the retrieval and use of stimuli according to the knowledge distance. The system is centered on the use of a network of all technology fields in the patent classification system, to store and organize the world's cumulative data on the technological knowledge, concepts, and solutions in the total patent database according to statistically estimated knowledge distance between technology fields. In turn, knowledge distance guides the network-based exploration and retrieval of inspirational stimuli for inferences across near and far fields to generate new design ideas by analogy and combination. With two case studies, we showcase the effectiveness of using the system to explore and retrieve multilevel inspirational stimuli and generate new design ideas for both problem solving and open ended innovation. These case studies also demonstrate the computer aided ideation process, which is data-driven, computationally augmented, theoretically grounded, visually inspiring, and rapid. △ Less","18 October, 2022",https://arxiv.org/pdf/2210.10104
RPM: Generalizable Behaviors for Multi-Agent Reinforcement Learning,Wei Qiu;Xiao Ma;Bo An;Svetlana Obraztsova;Shuicheng Yan;Zhongwen Xu,"Despite the recent advancement in multi-agent reinforcement learning (MARL), the MARL agents easily overfit the training environment and perform poorly in the evaluation scenarios where other agents behave differently. Obtaining generalizable policies for MARL agents is thus necessary but challenging mainly due to complex multi-agent interactions. In this work, we model the problem with Markov Games and propose a simple yet effective method, ranked policy memory (RPM), to collect diverse multi-agent trajectories for training MARL policies with good generalizability. The main idea of RPM is to maintain a look-up memory of policies. In particular, we try to acquire various levels of behaviors by saving policies via ranking the training episode return, i.e., the episode return of agents in the training environment; when an episode starts, the learning agent can then choose a policy from the RPM as the behavior policy. This innovative self-play training framework leverages agents' past policies and guarantees the diversity of multi-agent interaction in the training data. We implement RPM on top of MARL algorithms and conduct extensive experiments on Melting Pot. It has been demonstrated that RPM enables MARL agents to interact with unseen agents in multi-agent generalization evaluation scenarios and complete given tasks, and it significantly boosts the performance up to 402% on average. △ Less","18 October, 2022",https://arxiv.org/pdf/2210.09646
Split-KalmanNet: A Robust Model-Based Deep Learning Approach for SLAM,Geon Choi;Jeonghun Park;Nir Shlezinger;Yonina C. Eldar;Namyoon Lee,"Simultaneous localization and mapping (SLAM) is a method that constructs a map of an unknown environment and localizes the position of a moving agent on the map simultaneously. Extended Kalman filter (EKF) has been widely adopted as a low complexity solution for online SLAM, which relies on a motion and measurement model of the moving agent. In practice, however, acquiring precise information about these models is very challenging, and the model mismatch effect causes severe performance loss in SLAM. In this paper, inspired by the recently proposed KalmanNet, we present a robust EKF algorithm using the power of deep learning for online SLAM, referred to as Split-KalmanNet. The key idea of Split-KalmanNet is to compute the Kalman gain using the Jacobian matrix of a measurement function and two recurrent neural networks (RNNs). The two RNNs independently learn the covariance matrices for a prior state estimate and the innovation from data. The proposed split structure in the computation of the Kalman gain allows to compensate for state and measurement model mismatch effects independently. Numerical simulation results verify that Split-KalmanNet outperforms the traditional EKF and the state-of-the-art KalmanNet algorithm in various model mismatch scenarios. △ Less","18 October, 2022",https://arxiv.org/pdf/2210.09636
Relationships between patenting trends and research activity for green energy technologies,Regina Tuganova;Anna Permyakova;Anna Kuznetsova;Karina Rakhmanova;Natalia Monzul;Roman Uvarov;Elizaveta Kovtun;Semen Budennyy,"Green technology is viewed as a means of creating a sustainable society and a catalyst for sustainable development by the global community. It is responsible for both the potential reduction of production waste and the reduction of carbon footprint and CO2 emissions. However, alongside with the growing popularity of green technologies, there is an emerging skepticism about their contribution to solving environmental challenges. This article focuses on three areas of eco-innovation in green technology: renewable energy, hydrogen power, and decarbonization. Our main goal is to analyze the relationship between publication activity and the number of patented research results, thus shedding light on the real-world applicability of scientific outcomes. We used several bibliometric methods for analyzing global publication and patent activity, applied to the Scopus citation database and the European Patent Office's patent database. Our results show that the advancement of research in all three areas of eco-innovation does not automatically lead to the increase in the number of patents. We offer possible reasons for such dependency based on the observations of the worldwide tendencies in green innovation sphere. △ Less","18 October, 2022",https://arxiv.org/pdf/2210.09611
A novel statistical methodology for quantifying the spatial arrangements of axons in peripheral nerves,Abida Sanjana Shemonti;Emanuele Plebani;Natalia P. Biscola;Deborah M. Jaffey;Leif A. Havton;Janet R. Keast;Alex Pothen;M. Murat Dundar;Terry L. Powley;Bartek Rajwa,"A thorough understanding of the neuroanatomy of peripheral nerves is required for a better insight into their function and the development of neuromodulation tools and strategies. In biophysical modeling, it is commonly assumed that the complex spatial arrangement of myelinated and unmyelinated axons in peripheral nerves is random, however, in reality the axonal organization is inhomogeneous and anisotropic. Present quantitative neuroanatomy methods analyze peripheral nerves in terms of the number of axons and the morphometric characteristics of the axons, such as area and diameter. In this study, we employed spatial statistics and point process models to describe the spatial arrangement of axons and Sinkhorn distances to compute the similarities between these arrangements (in terms of first- and second-order statistics) in various vagus and pelvic nerve cross-sections. We utilized high-resolution TEM images that have been segmented using a custom-built high-throughput deep learning system based on a highly modified U-Net architecture. Our findings show a novel and innovative approach to quantifying similarities between spatial point patterns using metrics derived from the solution to the optimal transport problem. We also present a generalizable pipeline for quantitative analysis of peripheral nerve architecture. Our data demonstrate differences between male- and female-originating samples and similarities between the pelvic and abdominal vagus nerves. △ Less","17 October, 2022",https://arxiv.org/pdf/2210.09554
CrossRE: A Cross-Domain Dataset for Relation Extraction,Elisa Bassignana;Barbara Plank,"Relation Extraction (RE) has attracted increasing attention, but current RE evaluation is limited to in-domain evaluation setups. Little is known on how well a RE system fares in challenging, but realistic out-of-distribution evaluation setups. To address this gap, we propose CrossRE, a new, freely-available cross-domain benchmark for RE, which comprises six distinct text domains and includes multi-label annotations. An additional innovation is that we release meta-data collected during annotation, to include explanations and flags of difficult instances. We provide an empirical evaluation with a state-of-the-art model for relation classification. As the meta-data enables us to shed new light on the state-of-the-art model, we provide a comprehensive analysis on the impact of difficult cases and find correlations between model and human annotations. Overall, our empirical investigation highlights the difficulty of cross-domain RE. We release our dataset, to spur more research in this direction. △ Less","17 October, 2022",https://arxiv.org/pdf/2210.09345
"Decentralized nation, solving the web identity crisis",Frederic Jumelle;Timothy Pagett;Ryan Lemand,"The web of today whether you prefer to call it web 2.0, web 3.0, web 5.0 or even the metaverse is at a critical stage of evolution and challenge, largely centered around its crisis of identity. Like teenagers who cannot assess properly their reason for being and do not seem ready to take responsibility for their actions, we are constantly blaming the very system we are trying to get away from. To truly realize the benefits from innovation and technology, this crisis has to be resolved, not just through tactical solutions but through developments that enhance the sustainability of the web and its benefits. Significant strides are being made in the evolution of digital services enabled by technology, regulation, and the sheer pace of societal change. The journey to the decentralized web is mirroring the convergence of the physical and digital worlds across all economies and is increasingly embracing the digital native world. Technology has provided the foundational platform for individuals and entities to create and manage wealth, potentially without the need for big institutions. Ironically, despite all of the advancements, we are still facing an unprecedented and increasing wealth gap. Clearly, the system is broken, not just around the edges but at the very core of the democratic underpinning of our society. In this whitepaper, we propose how artificial intelligence on blockchain can be used to generate a new class of identity through direct human computer interaction. We demonstrate how this, combined with new perspectives for sustaining community and governance embedded within the use of blockchain technology, will underpin a sustainable solution to protect identity, authorship and privacy at the same time while contributing to restore trust amongst members of a future decentralized nation and hence contribute to solving the web most significant identity crisis. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.08978
Coordinated Science Laboratory 70th Anniversary Symposium: The Future of Computing,Klara Nahrstedt;Naresh Shanbhag;Vikram Adve;Nancy Amato;Romit Roy Choudhury;Carl Gunter;Nam Sung Kim;Olgica Milenkovic;Sayan Mitra;Lav Varshney;Yurii Vlasov;Sarita Adve;Rashid Bashir;Andreas Cangellaris;James DiCarlo;Katie Driggs-Campbell;Nick Feamster;Mattia Gazzola;Karrie Karahalios;Sanmi Koyejo;Paul Kwiat;Bo Li;Negar Mehr;Ravish Mehra;Andrew Miller,"In 2021, the Coordinated Science Laboratory CSL, an Interdisciplinary Research Unit at the University of Illinois Urbana-Champaign, hosted the Future of Computing Symposium to celebrate its 70th anniversary. CSL's research covers the full computing stack, computing's impact on society and the resulting need for social responsibility. In this white paper, we summarize the major technological points, insights, and directions that speakers brought forward during the Future of Computing Symposium. Participants discussed topics related to new computing paradigms, technologies, algorithms, behaviors, and research challenges to be expected in the future. The symposium focused on new computing paradigms that are going beyond traditional computing and the research needed to support their realization. These needs included stressing security and privacy, the end to end human cyber physical systems and with them the analysis of the end to end artificial intelligence needs. Furthermore, advances that enable immersive environments for users, the boundaries between humans and machines will blur and become seamless. Particular integration challenges were made clear in the final discussion on the integration of autonomous driving, robo taxis, pedestrians, and future cities. Innovative approaches were outlined to motivate the next generation of researchers to work on these challenges. The discussion brought out the importance of considering not just individual research areas, but innovations at the intersections between computing research efforts and relevant application domains, such as health care, transportation, energy systems, and manufacturing. △ Less","4 October, 2022",https://arxiv.org/pdf/2210.08974
Industry-Scale Orchestrated Federated Learning for Drug Discovery,Martijn Oldenhof;Gergely Ács;Balázs Pejó;Ansgar Schuffenhauer;Nicholas Holway;Noé Sturm;Arne Dieckmann;Oliver Fortmeier;Eric Boniface;Clément Mayer;Arnaud Gohier;Peter Schmidtke;Ritsuya Niwayama;Dieter Kopecky;Lewis Mervin;Prakash Chandra Rathi;Lukas Friedrich;András Formanek;Peter Antal;Jordon Rahaman;Adam Zalewski;Wouter Heyndrickx;Ezron Oluoch;Manuel Stößel;Michal Vančo,"To apply federated learning to drug discovery we developed a novel platform in the context of European Innovative Medicines Initiative (IMI) project MELLODDY (grant n°831472), which was comprised of 10 pharmaceutical companies, academic research labs, large industrial companies and startups. The MELLODDY platform was the first industry-scale platform to enable the creation of a global federated model for drug discovery without sharing the confidential data sets of the individual partners. The federated model was trained on the platform by aggregating the gradients of all contributing partners in a cryptographic, secure way following each training iteration. The platform was deployed on an Amazon Web Services (AWS) multi-account architecture running Kubernetes clusters in private subnets. Organisationally, the roles of the different partners were codified as different rights and permissions on the platform and administrated in a decentralized way. The MELLODDY platform generated new scientific discoveries which are described in a companion paper. △ Less","12 December, 2022",https://arxiv.org/pdf/2210.08871
Boosting Performance of a Baseline Visual Place Recognition Technique by Predicting the Maximally Complementary Technique,Connor Malone;Stephen Hausler;Tobias Fischer;Michael Milford,"One recent promising approach to the Visual Place Recognition (VPR) problem has been to fuse the place recognition estimates of multiple complementary VPR techniques using methods such as SRAL and multi-process fusion. These approaches come with a substantial practical limitation: they require all potential VPR methods to be brute-force run before they are selectively fused. The obvious solution to this limitation is to predict the viable subset of methods ahead of time, but this is challenging because it requires a predictive signal within the imagery itself that is indicative of high performance methods. Here we propose an alternative approach that instead starts with a known single base VPR technique, and learns to predict the most complementary additional VPR technique to fuse with it, that results in the largest improvement in performance. The key innovation here is to use a dimensionally reduced difference vector between the query image and the top-retrieved reference image using this baseline technique as the predictive signal of the most complementary additional technique, both during training and inference. We demonstrate that our approach can train a single network to select performant, complementary technique pairs across datasets which span multiple modes of transportation (train, car, walking) as well as to generalise to unseen datasets, outperforming multiple baseline strategies for manually selecting the best technique pairs based on the same training data. △ Less","14 October, 2022",https://arxiv.org/pdf/2210.07509
Neighborhood Structure Configuration Models,Felix I. Stamm;Michael Scholkemper;Markus Strohmaier;Michael T. Schaub,"We develop a new method to efficiently sample synthetic networks that preserve the d-hop neighborhood structure of a given network for any given d. The proposed algorithm trades off the diversity in network samples against the depth of the neighborhood structure that is preserved. Our key innovation is to employ a colored Configuration Model with colors derived from iterations of the so-called Color Refinement algorithm. We prove that with increasing iterations the preserved structural information increases: the generated synthetic networks and the original network become more and more similar, and are eventually indistinguishable in terms of centrality measures such as PageRank, HITS, Katz centrality and eigenvector centrality. Our work enables to efficiently generate samples with a precisely controlled similarity to the original network, especially for large networks. △ Less","13 October, 2022",https://arxiv.org/pdf/2210.06843
Towards visually prompted keyword localisation for zero-resource spoken languages,Leanne Nortje;Herman Kamper,"Imagine being able to show a system a visual depiction of a keyword and finding spoken utterances that contain this keyword from a zero-resource speech corpus. We formalise this task and call it visually prompted keyword localisation (VPKL): given an image of a keyword, detect and predict where in an utterance the keyword occurs. To do VPKL, we propose a speech-vision model with a novel localising attention mechanism which we train with a new keyword sampling scheme. We show that these innovations give improvements in VPKL over an existing speech-vision model. We also compare to a visual bag-of-words (BoW) model where images are automatically tagged with visual labels and paired with unlabelled speech. Although this visual BoW can be queried directly with a written keyword (while our's takes image queries), our new model still outperforms the visual BoW in both detection and localisation, giving a 16% relative improvement in localisation F1. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06229
Can Artificial Intelligence Reconstruct Ancient Mosaics?,Fernando Moral-Andrés;Elena Merino-Gómez;Pedro Reviriego;Fabrizio Lombardi,"A large number of ancient mosaics have not reached us because they have been destroyed by erosion, earthquakes, looting or even used as materials in newer construction. To make things worse, among the small fraction of mosaics that we have been able to recover, many are damaged or incomplete. Therefore, restoration and reconstruction of mosaics play a fundamental role to preserve cultural heritage and to understand the role of mosaics in ancient cultures. This reconstruction has traditionally been done manually and more recently using computer graphics programs but always by humans. In the last years, Artificial Intelligence (AI) has made impressive progress in the generation of images from text descriptions and reference images. State of the art AI tools such as DALL-E2 can generate high quality images from text prompts and can take a reference image to guide the process. In august 2022, DALL-E2 launched a new feature called outpainting that takes as input an incomplete image and a text prompt and then generates a complete image filling the missing parts. In this paper, we explore whether this innovative technology can be used to reconstruct mosaics with missing parts. Hence a set of ancient mosaics have been used and reconstructed using DALL-E2; results are promising showing that AI is able to interpret the key features of the mosaics and is able to produce reconstructions that capture the essence of the scene. However, in some cases AI fails to reproduce some details, geometric forms or introduces elements that are not consistent with the rest of the mosaic. This suggests that as AI image generation technology matures in the next few years, it could be a valuable tool for mosaic reconstruction going forward. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.06145
Regularized Graph Structure Learning with Semantic Knowledge for Multi-variates Time-Series Forecasting,Hongyuan Yu;Ting Li;Weichen Yu;Jianguo Li;Yan Huang;Liang Wang;Alex Liu,"Multivariate time-series forecasting is a critical task for many applications, and graph time-series network is widely studied due to its capability to capture the spatial-temporal correlation simultaneously. However, most existing works focus more on learning with the explicit prior graph structure, while ignoring potential information from the implicit graph structure, yielding incomplete structure modeling. Some recent works attempt to learn the intrinsic or implicit graph structure directly while lacking a way to combine explicit prior structure with implicit structure together. In this paper, we propose Regularized Graph Structure Learning (RGSL) model to incorporate both explicit prior structure and implicit structure together, and learn the forecasting deep networks along with the graph structure. RGSL consists of two innovative modules. First, we derive an implicit dense similarity matrix through node embedding, and learn the sparse graph structure using the Regularized Graph Generation (RGG) based on the Gumbel Softmax trick. Second, we propose a Laplacian Matrix Mixed-up Module (LM3) to fuse the explicit graph and implicit graph together. We conduct experiments on three real-word datasets. Results show that the proposed RGSL model outperforms existing graph forecasting algorithms with a notable margin, while learning meaningful graph structure simultaneously. Our code and models are made publicly available at https://github.com/alipay/RGSL.git. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06126
Estimating the Pose of a Euro Pallet with an RGB Camera based on Synthetic Training Data,Markus Knitt;Jakob Schyga;Asan Adamanov;Johannes Hinckeldeyn;Jochen Kreutzfeldt,"Estimating the pose of a pallet and other logistics objects is crucial for various use cases, such as automatized material handling or tracking. Innovations in computer vision, computing power, and machine learning open up new opportunities for device-free localization based on cameras and neural networks. Large image datasets with annotated poses are required for training the network. Manual annotation, especially of 6D poses, is an extremely labor-intensive process. Hence, newer approaches often leverage synthetic training data to automatize the process of generating annotated image datasets. In this work, the generation of synthetic training data for 6D pose estimation of pallets is presented. The data is then used to train the Deep Object Pose Estimation (DOPE) algorithm. The experimental validation of the algorithm proves that the 6D pose estimation of a standardized Euro pallet with a Red-Green-Blue (RGB) camera is feasible. The comparison of the results from three varying datasets under different lighting conditions shows the relevance of an appropriate dataset design to achieve an accurate and robust localization. The quantitative evaluation shows an average position error of less than 20 cm for the preferred dataset. The validated training dataset and a photorealistic model of a Euro pallet are publicly provided. △ Less","12 October, 2022",https://arxiv.org/pdf/2210.06001
Image Projective Transformation Rectification with Synthetic Data for Smartphone-captured Chest X-ray Photos Classification,Chak Fong Chong;Yapeng Wang;Benjamin Ng;Wuman Luo;Xu Yang,"Classification on smartphone-captured chest X-ray (CXR) photos to detect pathologies is challenging due to the projective transformation caused by the non-ideal camera position. Recently, various rectification methods have been proposed for different photo rectification tasks such as document photos, license plate photos, etc. Unfortunately, we found that none of them is suitable for CXR photos, due to their specific transformation type, image appearance, annotation type, etc. In this paper, we propose an innovative deep learning-based Projective Transformation Rectification Network (PTRN) to automatically rectify CXR photos by predicting the projective transformation matrix. To the best of our knowledge, it is the first work to predict the projective transformation matrix as the learning goal for photo rectification. Additionally, to avoid the expensive collection of natural data, synthetic CXR photos are generated under the consideration of natural perturbations, extra screens, etc. We evaluate the proposed approach in the CheXphoto smartphone-captured CXR photos classification competition hosted by the Stanford University Machine Learning Group, our approach won first place with a huge performance improvement (ours 0.850, second-best 0.762, in AUC). A deeper study demonstrates that the use of PTRN successfully achieves the classification performance on the spatially transformed CXR photos to the same level as on the high-quality digital CXR images, indicating PTRN can eliminate all negative impacts of projective transformation on the CXR photos. △ Less","30 November, 2022",https://arxiv.org/pdf/2210.05954
Quantifying hierarchy in scientific teams,Fengli Xu;Lingfei Wu;James A. Evans,"This paper provides a detailed description of the data collection and machine learning model used in our recent PNAS paper ""Flat Teams Drive Scientific Innovation"" Xu et al. [2022a]. Here, we discuss how the features of scientific publication can be used to estimate the implicit hierarchy in the corresponding author teams. Besides, we also describe the method of evaluating the impact of team hierarchy on scientific outputs. More details will be updated in this article continuously. Raw data and Readme document can be accessed in this GitHub repository Xu et al. [2022b]. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05852
Deep Active Ensemble Sampling For Image Classification,Salman Mohamadi;Gianfranco Doretto;Donald A. Adjeroh,"Conventional active learning (AL) frameworks aim to reduce the cost of data annotation by actively requesting the labeling for the most informative data points. However, introducing AL to data hungry deep learning algorithms has been a challenge. Some proposed approaches include uncertainty-based techniques, geometric methods, implicit combination of uncertainty-based and geometric approaches, and more recently, frameworks based on semi/self supervised techniques. In this paper, we address two specific problems in this area. The first is the need for efficient exploitation/exploration trade-off in sample selection in AL. For this, we present an innovative integration of recent progress in both uncertainty-based and geometric frameworks to enable an efficient exploration/exploitation trade-off in sample selection strategy. To this end, we build on a computationally efficient approximate of Thompson sampling with key changes as a posterior estimator for uncertainty representation. Our framework provides two advantages: (1) accurate posterior estimation, and (2) tune-able trade-off between computational overhead and higher accuracy. The second problem is the need for improved training protocols in deep AL. For this, we use ideas from semi/self supervised learning to propose a general approach that is independent of the specific AL technique being used. Taken these together, our framework shows a significant improvement over the state-of-the-art, with results that are comparable to the performance of supervised-learning under the same setting. We show empirical results of our framework, and comparative performance with the state-of-the-art on four datasets, namely, MNIST, CIFAR10, CIFAR100 and ImageNet to establish a new baseline in two different settings. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05770
Map-free Visual Relocalization: Metric Pose Relative to a Single Image,Eduardo Arnold;Jamie Wynn;Sara Vicente;Guillermo Garcia-Hernando;Áron Monszpart;Victor Adrian Prisacariu;Daniyar Turmukhambetov;Eric Brachmann,"Can we relocalize in a scene represented by a single reference image? Standard visual relocalization requires hundreds of images and scale calibration to build a scene-specific 3D map. In contrast, we propose Map-free Relocalization, i.e., using only one photo of a scene to enable instant, metric scaled relocalization. Existing datasets are not suitable to benchmark map-free relocalization, due to their focus on large scenes or their limited variability. Thus, we have constructed a new dataset of 655 small places of interest, such as sculptures, murals and fountains, collected worldwide. Each place comes with a reference image to serve as a relocalization anchor, and dozens of query images with known, metric camera poses. The dataset features changing conditions, stark viewpoint changes, high variability across places, and queries with low to no visual overlap with the reference image. We identify two viable families of existing methods to provide baseline results: relative pose regression, and feature matching combined with single-image depth prediction. While these methods show reasonable performance on some favorable scenes in our dataset, map-free relocalization proves to be a challenge that requires new, innovative solutions. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05494
Computer Vision based inspection on post-earthquake with UAV synthetic dataset,Mateusz Żarski;Bartosz Wójcik;Jarosław A. Miszczak;Bartłomiej Blachowski;Mariusz Ostrowski,"The area affected by the earthquake is vast and often difficult to entirely cover, and the earthquake itself is a sudden event that causes multiple defects simultaneously, that cannot be effectively traced using traditional, manual methods. This article presents an innovative approach to the problem of detecting damage after sudden events by using an interconnected set of deep machine learning models organized in a single pipeline and allowing for easy modification and swapping models seamlessly. Models in the pipeline were trained with a synthetic dataset and were adapted to be further evaluated and used with unmanned aerial vehicles (UAVs) in real-world conditions. Thanks to the methods presented in the article, it is possible to obtain high accuracy in detecting buildings defects, segmenting constructions into their components and estimating their technical condition based on a single drone flight. △ Less","11 October, 2022",https://arxiv.org/pdf/2210.05282
ConchShell: A Generative Adversarial Networks that Turns Pictures into Piano Music,Wanpeng Fan;Yuanzhi Su;Yuxin Huang,"We present ConchShell, a multi-modal generative adversarial framework that takes pictures as input to the network and generates piano music samples that match the picture context. Inspired by I3D, we introduce a novel image feature representation method: time-convolutional neural network (TCNN), which is used to forge features for images in the temporal dimension. Although our image data consists of only six categories, our proposed framework will be innovative and commercially meaningful. The project will provide technical ideas for work such as 3D game voice overs, short-video soundtracks, and real-time generation of metaverse background music.We have also released a new dataset, the Beach-Ocean-Piano Dataset (BOPD) 1, which contains more than 3,000 images and more than 1,500 piano pieces. This dataset will support multimodal image-to-music research. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.05076
Tango or Square Dance? How Tightly Should we Integrate Network Functionality in Browsers?,Alex Davidson;Matthias Frei;Marten Gartner;Hamed Haddadi;Jordi Subirà Nieto;Adrian Perrig;Philipp Winter;François Wirz,"The question at which layer network functionality is presented or abstracted remains a research challenge. Traditionally, network functionality was either placed into the core network, middleboxes, or into the operating system -- but recent developments have expanded the design space to directly introduce functionality into the application (and in particular into the browser) as a way to expose it to the user. Given the context of emerging path-aware networking technology, an interesting question arises: which layer should handle the new features? We argue that the browser is becoming a powerful platform for network innovation, where even user-driven properties can be implemented in an OS-agnostic fashion. We demonstrate the feasibility of geo-fenced browsing using a prototype browser extension, realized by the SCION path-aware networking architecture, without introducing any significant performance overheads. △ Less","10 October, 2022",https://arxiv.org/pdf/2210.04791
Augmentations in Hypergraph Contrastive Learning: Fabricated and Generative,Tianxin Wei;Yuning You;Tianlong Chen;Yang Shen;Jingrui He;Zhangyang Wang,"This paper targets at improving the generalizability of hypergraph neural networks in the low-label regime, through applying the contrastive learning approach from images/graphs (we refer to it as HyperGCL). We focus on the following question: How to construct contrastive views for hypergraphs via augmentations? We provide the solutions in two folds. First, guided by domain knowledge, we fabricate two schemes to augment hyperedges with higher-order relations encoded, and adopt three vertex augmentation strategies from graph-structured data. Second, in search of more effective views in a data-driven manner, we for the first time propose a hypergraph generative model to generate augmented views, and then an end-to-end differentiable pipeline to jointly learn hypergraph augmentations and model parameters. Our technical innovations are reflected in designing both fabricated and generative augmentations of hypergraphs. The experimental findings include: (i) Among fabricated augmentations in HyperGCL, augmenting hyperedges provides the most numerical gains, implying that higher-order information in structures is usually more downstream-relevant; (ii) Generative augmentations do better in preserving higher-order information to further benefit generalizability; (iii) HyperGCL also boosts robustness and fairness in hypergraph representation learning. Codes are released at https://github.com/weitianxin/HyperGCL. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03801
Single Image Super-Resolution Based on Capsule Neural Networks,George Corrêa de Araújo;Helio Pedrini,"Single image super-resolution (SISR) is the process of obtaining one high-resolution version of a low-resolution image by increasing the number of pixels per unit area. This method has been actively investigated by the research community, due to the wide variety of real-world problems where it can be applied, from aerial and satellite imaging to compressed image and video enhancement. Despite the improvements achieved by deep learning in the field, the vast majority of the used networks are based on traditional convolutions, with the solutions focusing on going deeper and/or wider, and innovations coming from jointly employing successful concepts from other fields. In this work, we decided to step up from the traditional convolutions and adopt the concept of capsules. Since their overwhelming results both in image classification and segmentation problems, we question how suitable they are for SISR. We also verify that different solutions share most of their configurations, and argue that this trend leads to fewer explorations of network varieties. During our experiments, we check various strategies to improve results, ranging from new and different loss functions to changes in the capsule layers. Our network achieved good results with fewer convolutional-based layers, showing that capsules might be a concept worth applying in the image super-resolution problem. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.03743
Artificial Intelligence and Natural Language Processing and Understanding in Space: A Methodological Framework and Four ESA Case Studies,José Manuel Gómez-Pérez;Andrés García-Silva;Rosemarie Leone;Mirko Albani;Moritz Fontaine;Charles Poncet;Leopold Summerer;Alessandro Donati;Ilaria Roma;Stefano Scaglioni,"The European Space Agency is well known as a powerful force for scientific discovery in numerous areas related to Space. The amount and depth of the knowledge produced throughout the different missions carried out by ESA and their contribution to scientific progress is enormous, involving large collections of documents like scientific publications, feasibility studies, technical reports, and quality management procedures, among many others. Through initiatives like the Open Space Innovation Platform, ESA also acts as a hub for new ideas coming from the wider community across different challenges, contributing to a virtuous circle of scientific discovery and innovation. Handling such wealth of information, of which large part is unstructured text, is a colossal task that goes beyond human capabilities, hence requiring automation. In this paper, we present a methodological framework based on artificial intelligence and natural language processing and understanding to automatically extract information from Space documents, generating value from it, and illustrate such framework through several case studies implemented across different functional areas of ESA, including Mission Design, Quality Assurance, Long-Term Data Preservation, and the Open Space Innovation Platform. In doing so, we demonstrate the value of these technologies in several tasks ranging from effortlessly searching and recommending Space information to automatically determining how innovative an idea can be, answering questions about Space, and generating quizzes regarding quality procedures. Each of these accomplishments represents a step forward in the application of increasingly intelligent AI systems in Space, from structuring and facilitating information access to intelligent systems capable to understand and reason with such information. △ Less","24 October, 2022",https://arxiv.org/pdf/2210.03640
Specialized Re-Ranking: A Novel Retrieval-Verification Framework for Cloth Changing Person Re-Identification,Renjie Zhang;Yu Fang;Huaxin Song;Fangbin Wan;Yanwei Fu;Hirokazu Kato;Yang Wu,"Cloth changing person re-identification(Re-ID) can work under more complicated scenarios with higher security than normal Re-ID and biometric techniques and is therefore extremely valuable in applications. Meanwhile, higher flexibility in appearance always leads to more similar-looking confusing images, which is the weakness of the widely used retrieval methods. In this work, we shed light on how to handle these similar images. Specifically, we propose a novel retrieval-verification framework. Given an image, the retrieval module can search for similar images quickly. Our proposed verification network will then compare the input image and the candidate images by contrasting those local details and give a similarity score. An innovative ranking strategy is also introduced to take a good balance between retrieval and verification results. Comprehensive experiments are conducted to show the effectiveness of our framework and its capability in improving the state-of-the-art methods remarkably on both synthetic and realistic datasets. △ Less","7 October, 2022",https://arxiv.org/pdf/2210.03592
A ResNet is All You Need? Modeling A Strong Baseline for Detecting Referable Diabetic Retinopathy in Fundus Images,Tomás Castilla;Marcela S. Martínez;Mercedes Leguía;Ignacio Larrabide;José Ignacio Orlando,"Deep learning is currently the state-of-the-art for automated detection of referable diabetic retinopathy (DR) from color fundus photographs (CFP). While the general interest is put on improving results through methodological innovations, it is not clear how good these approaches perform compared to standard deep classification models trained with the appropriate settings. In this paper we propose to model a strong baseline for this task based on a simple and standard ResNet-18 architecture. To this end, we built on top of prior art by training the model with a standard preprocessing strategy but using images from several public sources and an empirically calibrated data augmentation setting. To evaluate its performance, we covered multiple clinically relevant perspectives, including image and patient level DR screening, discriminating responses by input quality and DR grade, assessing model uncertainties and analyzing its results in a qualitative manner. With no other methodological innovation than a carefully designed training, our ResNet model achieved an AUC = 0.955 (0.953 - 0.956) on a combined test set of 61007 test images from different public datasets, which is in line or even better than what other more complex deep learning models reported in the literature. Similar AUC values were obtained in 480 images from two separate in-house databases specially prepared for this study, which emphasize its generalization ability. This confirms that standard networks can still be strong baselines for this task if properly trained. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.03180
TrustVault: A privacy-first data wallet for the European Blockchain Services Infrastructure,Sharif Jacobino;Johan Pouwelse,"The European Union is on course to introduce a European Digital Identity that will be available to all EU citizens and businesses. This will have a huge impact on how citizens and businesses interact online. Big Tech companies currently dictate how digital identities are used. As a result, they have amassed vast amounts of private user data. Movements like Self-Sovereign Identity aim to give users control over their online identity. TrustVault is the first data wallet that gives users back control of their identity and all their data. TrustVault allows users to store all their data on their smartphones and control with whom they share it. The user has fine-grained access control based on verifiable user attributes. EBSI connects TrustVault to the European Self-Sovereign Identity Framework allowing users to use Verifiable Credentials from public and private institutions in their access control policies. The system is serverless and has no Trusted Third Parties. TrustVault replaces the for-profit infrastructure of Big Tech with a public and transparent platform for innovation. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.02987
When not to use machine learning: a perspective on potential and limitations,M. R. Carbone,"The unparalleled success of artificial intelligence (AI) in the technology sector has catalyzed an enormous amount of research in the scientific community. It has proven to be a powerful tool, but as with any rapidly developing field, the deluge of information can be overwhelming, confusing and sometimes misleading. This can make it easy to become lost in the same hype cycles that have historically ended in the periods of scarce funding and depleted expectations known as AI Winters. Furthermore, while the importance of innovative, high-risk research cannot be overstated, it is also imperative to understand the fundamental limits of available techniques, especially in young fields where the rules appear to be constantly rewritten and as the likelihood of application to high-stakes scenarios increases. In this perspective, we highlight the guiding principles of data-driven modeling, how these principles imbue models with almost magical predictive power, and how they also impose limitations on the scope of problems they can address. Particularly, understanding when not to use data-driven techniques, such as machine learning, is not something commonly explored, but is just as important as knowing how to apply the techniques properly. We hope that the discussion to follow provides researchers throughout the sciences with a better understanding of when said techniques are appropriate, the pitfalls to watch for, and most importantly, the confidence to leverage the power they can provide. △ Less","6 October, 2022",https://arxiv.org/pdf/2210.02666
Towards Better Semantic Understanding of Mobile Interfaces,Srinivas Sunkara;Maria Wang;Lijuan Liu;Gilles Baechler;Yu-Chung Hsiao;Jindong;Chen;Abhanshu Sharma;James Stout,"Improving the accessibility and automation capabilities of mobile devices can have a significant positive impact on the daily lives of countless users. To stimulate research in this direction, we release a human-annotated dataset with approximately 500k unique annotations aimed at increasing the understanding of the functionality of UI elements. This dataset augments images and view hierarchies from RICO, a large dataset of mobile UIs, with annotations for icons based on their shapes and semantics, and associations between different elements and their corresponding text labels, resulting in a significant increase in the number of UI elements and the categories assigned to them. We also release models using image-only and multimodal inputs; we experiment with various architectures and study the benefits of using multimodal inputs on the new dataset. Our models demonstrate strong performance on an evaluation set of unseen apps, indicating their generalizability to newer screens. These models, combined with the new dataset, can enable innovative functionalities like referring to UI elements by their labels, improved coverage and better semantics for icons etc., which would go a long way in making UIs more usable for everyone. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02663
Jitter Does Matter: Adapting Gaze Estimation to New Domains,Ruicong Liu;Yiwei Bao;Mingjie Xu;Haofei Wang;Yunfei Liu;Feng Lu,"Deep neural networks have demonstrated superior performance on appearance-based gaze estimation tasks. However, due to variations in person, illuminations, and background, performance degrades dramatically when applying the model to a new domain. In this paper, we discover an interesting gaze jitter phenomenon in cross-domain gaze estimation, i.e., the gaze predictions of two similar images can be severely deviated in target domain. This is closely related to cross-domain gaze estimation tasks, but surprisingly, it has not been noticed yet previously. Therefore, we innovatively propose to utilize the gaze jitter to analyze and optimize the gaze domain adaptation task. We find that the high-frequency component (HFC) is an important factor that leads to jitter. Based on this discovery, we add high-frequency components to input images using the adversarial attack and employ contrastive learning to encourage the model to obtain similar representations between original and perturbed data, which reduces the impacts of HFC. We evaluate the proposed method on four cross-domain gaze estimation tasks, and experimental results demonstrate that it significantly reduces the gaze jitter and improves the gaze estimation performance in target domains. △ Less","5 October, 2022",https://arxiv.org/pdf/2210.02082
Federated Graph-based Networks with Shared Embedding,Tianyi Yu;Pei Lai;Fei Teng,"Nowadays, user privacy is becoming an issue that cannot be bypassed for system developers, especially for that of web applications where data can be easily transferred through internet. Thankfully, federated learning proposes an innovative method to train models with distributed devices while data are kept in local storage. However, unlike general neural networks, although graph-based networks have achieved great success in classification tasks and advanced recommendation system, its high performance relies on the rich context provided by a graph structure, which is vulnerable when data attributes are incomplete. Therefore, the latter becomes a realistic problem when implementing federated learning for graph-based networks. Knowing that data embedding is a representation in a different space, we propose our Federated Graph-based Networks with Shared Embedding (Feras), which uses shared embedding data to train the network and avoids the direct sharing of original data. A solid theoretical proof of the convergence of Feras is given in this work. Experiments on different datasets (PPI, Flickr, Reddit) are conducted to show the efficiency of Feras for centralized learning. Finally, Feras enables the training of current graph-based models in the federated learning framework for privacy concern. △ Less","3 October, 2022",https://arxiv.org/pdf/2210.01803
Designing a parallel suffix sort,Kunal Chowdhury,"Suffix sort plays a critical role in various computational algorithms including genomics as well as in frequently used day to day software applications. The sorting algorithm becomes tricky when we have lot of repeated characters in the string for a given radix. Various innovative implementations are available in this area e.g., Manber Myers. We present here an analysis that uses a concept around generalized polynomial factorization to sort these suffixes. The initial generation of these substring specific polynomial can be efficiently done using parallel threads and shared memory. The set of distinct factors and their order are known beforehand, and this helps us to sort the polynomials (equivalent of strings) accordingly. △ Less","4 October, 2022",https://arxiv.org/pdf/2210.01475
"FRIDA: A Collaborative Robot Painter with a Differentiable, Real2Sim2Real Planning Environment",Peter Schaldenbrand;James McCann;Jean Oh,"Painting is an artistic process of rendering visual content that achieves the high-level communication goals of an artist that may change dynamically throughout the creative process. In this paper, we present a Framework and Robotics Initiative for Developing Arts (FRIDA) that enables humans to produce paintings on canvases by collaborating with a painter robot using simple inputs such as language descriptions or images. FRIDA introduces several technical innovations for computationally modeling a creative painting process. First, we develop a fully differentiable simulation environment for painting, adopting the idea of real to simulation to real (real2sim2real). We show that our proposed simulated painting environment is higher fidelity to reality than existing simulation environments used for robot painting. Second, to model the evolving dynamics of a creative process, we develop a planning approach that can continuously optimize the painting plan based on the evolving canvas with respect to the high-level goals. In contrast to existing approaches where the content generation process and action planning are performed independently and sequentially, FRIDA adapts to the stochastic nature of using paint and a brush by continually re-planning and re-assessing its semantic goals based on its visual perception of the painting progress. We describe the details on the technical approach as well as the system integration. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00664
A Smart Recycling Bin Using Waste Image Classification At The Edge,Xueying Li;Ryan Grammenos,"Rapid economic growth gives rise to the urgent demand for a more efficient waste recycling system. This work thereby developed an innovative recycling bin that automatically separates urban waste to increase the recycling rate. We collected 1800 recycling waste images and combined them with an existing public dataset to train classification models for two embedded systems, Jetson Nano and K210, targeting different markets. The model reached an accuracy of 95.98% on Jetson Nano and 96.64% on K210. A bin program was designed to collect feedback from users. On Jetson Nano, the overall power consumption of the application was reduced by 30% from the previous work to 4.7 W, while the second system, K210, only needed 0.89 W of power to operate. In summary, our work demonstrated a fully functional prototype of an energy-saving, high-accuracy smart recycling bin, which can be commercialized in the future to improve urban waste recycling. △ Less","2 October, 2022",https://arxiv.org/pdf/2210.00448
"Leveraging Industry 4.0 -- Deep Learning, Surrogate Model and Transfer Learning with Uncertainty Quantification Incorporated into Digital Twin for Nuclear System",M. Rahman;Abid Khan;Sayeed Anowar;Md Al-Imran;Richa Verma;Dinesh Kumar;Kazuma Kobayashi;Syed Alam,"Industry 4.0 targets the conversion of the traditional industries into intelligent ones through technological revolution. This revolution is only possible through innovation, optimization, interconnection, and rapid decision-making capability. Numerical models are believed to be the key components of Industry 4.0, facilitating quick decision-making through simulations instead of costly experiments. However, numerical investigation of precise, high-fidelity models for optimization or decision-making is usually time-consuming and computationally expensive. In such instances, data-driven surrogate models are excellent substitutes for fast computational analysis and the probabilistic prediction of the output parameter for new input parameters. The emergence of Internet of Things (IoT) and Machine Learning (ML) has made the concept of surrogate modeling even more viable. However, these surrogate models contain intrinsic uncertainties, originate from modeling defects, or both. These uncertainties, if not quantified and minimized, can produce a skewed result. Therefore, proper implementation of uncertainty quantification techniques is crucial during optimization, cost reduction, or safety enhancement processes analysis. This chapter begins with a brief overview of the concept of surrogate modeling, transfer learning, IoT and digital twins. After that, a detailed overview of uncertainties, uncertainty quantification frameworks, and specifics of uncertainty quantification methodologies for a surrogate model linked to a digital twin is presented. Finally, the use of uncertainty quantification approaches in the nuclear industry has been addressed. △ Less","30 September, 2022",https://arxiv.org/pdf/2210.00074
Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs,Tae Soo Kim;Geonwoon Jang;Sanghyup Lee;Thijs Kooi,"As deep networks require large amounts of accurately labeled training data, a strategy to collect sufficiently large and accurate annotations is as important as innovations in recognition methods. This is especially true for building Computer Aided Detection (CAD) systems for chest X-rays where domain expertise of radiologists is required to annotate the presence and location of abnormalities on X-ray images. However, there lacks concrete evidence that provides guidance on how much resource to allocate for data annotation such that the resulting CAD system reaches desired performance. Without this knowledge, practitioners often fall back to the strategy of collecting as much detail as possible on as much data as possible which is cost inefficient. In this work, we investigate how the cost of data annotation ultimately impacts the CAD model performance on classification and segmentation of chest abnormalities in frontal-view X-ray images. We define the cost of annotation with respect to the following three dimensions: quantity, quality and granularity of labels. Throughout this study, we isolate the impact of each dimension on the resulting CAD model performance on detecting 10 chest abnormalities in X-rays. On a large scale training data with over 120K X-ray images with gold-standard annotations, we find that cost-efficient annotations provide great value when collected in large amounts and lead to competitive performance when compared to models trained with only gold-standard annotations. We also find that combining large amounts of cost efficient annotations with only small amounts of expensive labels leads to competitive CAD models at a much lower cost. △ Less","30 September, 2022",https://arxiv.org/pdf/2209.15314
Effective Early Stopping of Point Cloud Neural Networks,Thanasis Zoumpekas;Maria Salamó;Anna Puig,"Early stopping techniques can be utilized to decrease the time cost, however currently the ultimate goal of early stopping techniques is closely related to the accuracy upgrade or the ability of the neural network to generalize better on unseen data without being large or complex in structure and not directly with its efficiency. Time efficiency is a critical factor in neural networks, especially when dealing with the segmentation of 3D point cloud data, not only because a neural network itself is computationally expensive, but also because point clouds are large and noisy data, making learning processes even more costly. In this paper, we propose a new early stopping technique based on fundamental mathematics aiming to upgrade the trade-off between the learning efficiency and accuracy of neural networks dealing with 3D point clouds. Our results show that by employing our early stopping technique in four distinct and highly utilized neural networks in segmenting 3D point clouds, the training time efficiency of the models is greatly improved, with efficiency gain values reaching up to 94\%, while the models achieving in just a few epochs approximately similar segmentation accuracy metric values like the ones that are obtained in the training of the neural networks in 200 epochs. Also, our proposal outperforms four conventional early stopping approaches in segmentation accuracy, implying a promising innovative early stopping technique in point cloud segmentation. △ Less","30 September, 2022",https://arxiv.org/pdf/2209.15308
Reinforcement Learning Algorithms: An Overview and Classification,Fadi AlMahamid;Katarina Grolinger,"The desire to make applications and machines more intelligent and the aspiration to enable their operation without human interaction have been driving innovations in neural networks, deep learning, and other machine learning techniques. Although reinforcement learning has been primarily used in video games, recent advancements and the development of diverse and powerful reinforcement algorithms have enabled the reinforcement learning community to move from playing video games to solving complex real-life problems in autonomous systems such as self-driving cars, delivery drones, and automated robotics. Understanding the environment of an application and the algorithms' limitations plays a vital role in selecting the appropriate reinforcement learning algorithm that successfully solves the problem on hand in an efficient manner. Consequently, in this study, we identify three main environment types and classify reinforcement learning algorithms according to those environment types. Moreover, within each category, we identify relationships between algorithms. The overview of each algorithm provides insight into the algorithms' foundations and reviews similarities and differences among algorithms. This study provides a perspective on the field and helps practitioners and researchers to select the appropriate algorithm for their use case. △ Less","29 September, 2022",https://arxiv.org/pdf/2209.14940
A Multiagent Framework for the Asynchronous and Collaborative Extension of Multitask ML Systems,Andrea Gesmundo,"The traditional ML development methodology does not enable a large number of contributors, each with distinct objectives, to work collectively on the creation and extension of a shared intelligent system. Enabling such a collaborative methodology can accelerate the rate of innovation, increase ML technologies accessibility and enable the emergence of novel capabilities. We believe that this novel methodology for ML development can be demonstrated through a modularized representation of ML models and the definition of novel abstractions allowing to implement and execute diverse methods for the asynchronous use and extension of modular intelligent systems. We present a multiagent framework for the collaborative and asynchronous extension of dynamic large-scale multitask systems. △ Less","29 December, 2022",https://arxiv.org/pdf/2209.14745
D2D Multi-hop Energy Efficiency Toward EMS in B5G,Asma AL-Mansor;Nor Kamariah Noordin;Abdu Saif;Saeed Hamood Alsamhi,"Beyond fifth generation (B5G), communication has attracted much attention from academia, industry, and mobile network operators due to network densification, ultra-low latency communication, and enhanced energy and spectrum efficiency. However, a post-disaster emergency management system (EMS), which increasingly relies heavily on wireless communication infrastructure, is falling far behind in innovation and funding. Because the B5G concept represents a telecommunications industry revolution, EMS provisioning is intended to be dispersed, autonomous, and robust to network weaknesses caused by human and natural calamities. When the network is congested, partially functioning, or entirely isolated, we provide multi-device-to-device (D2D) communication to extend the communication coverage area with improved energy efficiency. Furthermore, we examine D2D multi-hop energy efficiency performance in the proposed network. The results demonstrate that the improved D2D multi-hop energy efficiency can improve the EMS effectively and efficiently in extending the coverage area and enhancing energy efficiency. Moreover, the proposed approach has been proven to increase energy efficiency, which acts as a suitable network design to recover from natural disasters and potentially save many lives △ Less","27 September, 2022",https://arxiv.org/pdf/2209.13743
Building a National Smart Campus to support sustainable business development: An ecosystem approach,Larry Abdullai;Jari Porras;Sanaul Haque,"Universities are racing towards making their campuses and cities smart in response to the global digitalization trend. However, the sustainability impact of Smart Campus research, development, and innovation services on other relevant stakeholders such as the small and medium-sized businesses, remain under-investigated. The Finnish National Smart Campus project seeks to bridge this gap by orchestrating a SC ecosystem where eight SC collaborate to bring trailblazing services to businesses and society. To maximize the sustainability impact of the SC ecosystem, this study used a participatory workshop to identify the challenges of SC, provide a step-by-step guide on how to identify other relevant stakeholders, and ascertain the perceived sustainability impact using one of the SC ecosystems RDIs as a case study. The preliminary results revealed that barriers to university-industry ecosystem development include (i), the lack of clarity in the shared goals (i.e., value proposition) between actors and (ii), weak stakeholder involvement in university RDI processes. Finally, this paper proposed a SC ecosystem model which offers a mindset shift for higher educational institutions in promoting the convergence of SC services and sustainability to support the sustainable development of Finnish-based SMEs. △ Less","22 September, 2022",https://arxiv.org/pdf/2209.13613
Semi-Blind Source Separation with Learned Constraints,Rémi Carloni Gertosio;Jérôme Bobin;Fabio Acero,"Blind source separation (BSS) algorithms are unsupervised methods, which are the cornerstone of hyperspectral data analysis by allowing for physically meaningful data decompositions. BSS problems being ill-posed, the resolution requires efficient regularization schemes to better distinguish between the sources and yield interpretable solutions. For that purpose, we investigate a semi-supervised source separation approach in which we combine a projected alternating least-square algorithm with a learning-based regularization scheme. In this article, we focus on constraining the mixing matrix to belong to a learned manifold by making use of generative models. Altogether, we show that this allows for an innovative BSS algorithm, with improved accuracy, which provides physically interpretable solutions. The proposed method, coined sGMCA, is tested on realistic hyperspectral astrophysical data in challenging scenarios involving strong noise, highly correlated spectra and unbalanced sources. The results highlight the significant benefit of the learned prior to reduce the leakages between the sources, which allows an overall better disentanglement. △ Less","27 September, 2022",https://arxiv.org/pdf/2209.13585
FORLORN: A Framework for Comparing Offline Methods and Reinforcement Learning for Optimization of RAN Parameters,Vegard Edvardsen;Gard Spreemann;Jeriek Van den Abeele,"The growing complexity and capacity demands for mobile networks necessitate innovative techniques for optimizing resource usage. Meanwhile, recent breakthroughs have brought Reinforcement Learning (RL) into the domain of continuous control of real-world systems. As a step towards RL-based network control, this paper introduces a new framework for benchmarking the performance of an RL agent in network environments simulated with ns-3. Within this framework, we demonstrate that an RL agent without domain-specific knowledge can learn how to efficiently adjust Radio Access Network (RAN) parameters to match offline optimization in static scenarios, while also adapting on the fly in dynamic scenarios, in order to improve the overall user experience. Our proposed framework may serve as a foundation for further work in developing workflows for designing RL-based RAN control algorithms. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.13540
"Waste Management Hackathon Providing New Ideas to Increase Citizen Awareness, Motivation and Engagement",Inna Sosunova;Jari Porras;Ekaterina Makarova;Andrei Rybin,"This paper describes the International Disruptive Information Solutions hackathon and one the winning solutions. The purpose of the hackathon was to promote the use of disruptive ICT technologies (e.g. IoT, Big data, AI, blockchain) in urban infrastructures to create innovative waste management solutions in a smart city context. 29 students enrolled into this hackathon and in the end 4 teams submitted their solutions to the challenges. The winning proposal EcoQ, an approach for plogging collecting trashes while jogging, answered more than well to the presented challenge on waste management and engagement. The original idea was extended and partly refocused during an internship. As the outcome of the internship a mobile application for organizing and holding waste collection events was developed. This mobile application was shortly tested in a real environment and it provides a working citizen-centric platform, which enables anyone to arrange waste management events, and motivates other residents to participate in these activities. △ Less","27 September, 2022",https://arxiv.org/pdf/2209.13391
On Three-Valued Modal Logics: from a Four-Valued Perspective,Xinyu Wang;Yang Song;Satoshi Tojo,"This paper aims at providing a comprehensive solution to the archaic open problem: how to define semantics of three-valued modal logic with vivid intuitive picture, convincing philosophical justification as well as versatile practical usage. Based on an existing line of work concerned with investigating three-valued logic out of innovative angles of view, we adopt a detour approach to interpret three-valued logic from a four-valued perspective, which results in the invention of an universal and systematic methodology for developing, explaining as well as utilizing three-valued modal logic. We illustrate our method through two concrete cases, one deontic and another epistemic, for both of which a sound and strongly complete natural deduction proof system is also presented in detail. We perceive our three-valued modal logic as a lightweight candidate to merge deontic or epistemic notion into temporal logic, without heavier burden of multiple modalities. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.13079
Effective Invertible Arbitrary Image Rescaling,Zhihong Pan;Baopu Li;Dongliang He;Wenhao Wu;Errui Ding,"Great successes have been achieved using deep learning techniques for image super-resolution (SR) with fixed scales. To increase its real world applicability, numerous models have also been proposed to restore SR images with arbitrary scale factors, including asymmetric ones where images are resized to different scales along horizontal and vertical directions. Though most models are only optimized for the unidirectional upscaling task while assuming a predefined downscaling kernel for low-resolution (LR) inputs, recent models based on Invertible Neural Networks (INN) are able to increase upscaling accuracy significantly by optimizing the downscaling and upscaling cycle jointly. However, limited by the INN architecture, it is constrained to fixed integer scale factors and requires one model for each scale. Without increasing model complexity, a simple and effective invertible arbitrary rescaling network (IARN) is proposed to achieve arbitrary image rescaling by training only one model in this work. Using innovative components like position-aware scale encoding and preemptive channel splitting, the network is optimized to convert the non-invertible rescaling cycle to an effectively invertible process. It is shown to achieve a state-of-the-art (SOTA) performance in bidirectional arbitrary rescaling without compromising perceptual quality in LR outputs. It is also demonstrated to perform well on tests with asymmetric scales using the same network architecture. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.13055
Shrinking unit: a Graph Convolution-Based Unit for CNN-like 3D Point Cloud Feature Extractors,Alberto Tamajo;Bastian Plaß;Thomas Klauer,"3D point clouds have attracted increasing attention in architecture, engineering, and construction due to their high-quality object representation and efficient acquisition methods. Consequently, many point cloud feature detection methods have been proposed in the literature to automate some workflows, such as their classification or part segmentation. Nevertheless, the performance of point cloud automated systems significantly lags behind their image counterparts. While part of this failure stems from the irregularity, unstructuredness, and disorder of point clouds, which makes the task of point cloud feature detection significantly more challenging than the image one, we argue that a lack of inspiration from the image domain might be the primary cause of such a gap. Indeed, given the overwhelming success of Convolutional Neural Networks (CNNs) in image feature detection, it seems reasonable to design their point cloud counterparts, but none of the proposed approaches closely resembles them. Specifically, even though many approaches generalise the convolution operation in point clouds, they fail to emulate the CNNs multiple-feature detection and pooling operations. For this reason, we propose a graph convolution-based unit, dubbed Shrinking unit, that can be stacked vertically and horizontally for the design of CNN-like 3D point cloud feature extractors. Given that self, local and global correlations between points in a point cloud convey crucial spatial geometric information, we also leverage them during the feature extraction process. We evaluate our proposal by designing a feature extractor model for the ModelNet-10 benchmark dataset and achieve 90.64% classification accuracy, demonstrating that our innovative idea is effective. Our code is available at github.com/albertotamajo/Shrinking-unit. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.12770
"Embedding digital participatory budgeting within local government: motivations, strategies and barriers faced",Jonathan Davies;Miguel Arana-Catania;Rob Procter,"The challenging task of embedding innovative participatory processes and technologies within local government often falls upon local council officers. Using qualitative data collection and analysis, we investigate the ongoing work of Scottish local councils seeking to run the process of participatory budgeting (PB) within their institution, the use of digital platforms to support this and the challenges faced. In doing so this paper draws on empirical material to support the growing discussion on the dynamics or forces behind embedding. Our analysis shows that formal agreement alone does not make the process a certainty. Local council officers must work as mediators in the transitional space between representative structures and new, innovative ways of working, unsettling the entrenched power dynamics. To do so they must be well trained and well resourced, including the ability to use digital platforms effectively as part of the process. This provides the necessary, accessible, transparent and deliberative space for participation. △ Less","26 September, 2022",https://arxiv.org/pdf/2209.12598
Neural State-Space Modeling with Latent Causal-Effect Disentanglement,Maryam Toloubidokhti;Ryan Missel;Xiajun Jiang;Niels Otani;Linwei Wang,"Despite substantial progress in deep learning approaches to time-series reconstruction, no existing methods are designed to uncover local activities with minute signal strength due to their negligible contribution to the optimization loss. Such local activities however can signify important abnormal events in physiological systems, such as an extra foci triggering an abnormal propagation of electrical waves in the heart. We discuss a novel technique for reconstructing such local activity that, while small in signal strength, is the cause of subsequent global activities that have larger signal strength. Our central innovation is to approach this by explicitly modeling and disentangling how the latent state of a system is influenced by potential hidden internal interventions. In a novel neural formulation of state-space models (SSMs), we first introduce causal-effect modeling of the latent dynamics via a system of interacting neural ODEs that separately describes 1) the continuous-time dynamics of the internal intervention, and 2) its effect on the trajectory of the system's native state. Because the intervention can not be directly observed but have to be disentangled from the observed subsequent effect, we integrate knowledge of the native intervention-free dynamics of a system, and infer the hidden intervention by assuming it to be responsible for differences observed between the actual and hypothetical intervention-free dynamics. We demonstrated a proof-of-concept of the presented framework on reconstructing ectopic foci disrupting the course of normal cardiac electrical propagation from remote observations. △ Less","25 September, 2022",https://arxiv.org/pdf/2209.12387
Social Assistive Robotics for Autistic Children,Stefania Brighenti;Federico Buratto;Fernando Vito Falcone;Cristina Gena;Claudio Mattutino;Matteo Nazzario,"This paper introduces the project Social Assistive Robotics for Autistic Children aimed at using robotic therapy for autism. The goal of the project is testing autistic children's interactions with the social robot NAO. In particular the robot will support the operators (psychologists, educators, speech therapists etc.) in their work. The innovative aspect of the project is that the children robot interaction will consider the children's emotions and specific features and the robot will adapt its behavior accordingly. △ Less","25 September, 2022",https://arxiv.org/pdf/2209.12289
Secure Decentralized IoT Service Platform using Consortium Blockchain,Ruipeng Zhang;Chen Xu;Mengjun Xie,"Blockchain technology has gained increasing popularity in the research of Internet of Things (IoT) systems in the past decade. As a distributed and immutable ledger secured by strong cryptography algorithms, the blockchain brings a new perspective to secure IoT systems. Many studies have been devoted to integrating blockchain into IoT device management, access control, data integrity, security, and privacy. In comparison, the blockchain-facilitated IoT communication is much less studied. Nonetheless, we see the potential of blockchain in decentralizing and securing IoT communications. This paper proposes an innovative IoT service platform powered by consortium blockchain technology. The presented solution abstracts machine-to-machine (M2M) and human-to-machine (H2M) communications into services provided by IoT devices. Then, it materializes data exchange of the IoT network through smart contracts and blockchain transactions. Additionally, we introduce the auxiliary storage layer to the proposed platform to address various data storage requirements. Our proof-of-concept implementation is tested against various workloads and connection sizes under different block configurations to evaluate the platform's transaction throughput, latency, and hardware utilization. The experiment results demonstrate that our solution can maintain high performance under most testing scenarios and provide valuable insights on optimizing the blockchain configuration to achieve the best performance. △ Less","25 September, 2022",https://arxiv.org/pdf/2209.12145
Asset Pricing and Deep Learning,Chen Zhang,"Traditional machine learning methods have been widely studied in financial innovation. My study focuses on the application of deep learning methods on asset pricing. I investigate various deep learning methods for asset pricing, especially for risk premia measurement. All models take the same set of predictive signals (firm characteristics, systematic risks and macroeconomics). I demonstrate high performance of all kinds of state-of-the-art (SOTA) deep learning methods, and figure out that RNNs with memory mechanism and attention have the best performance in terms of predictivity. Furthermore, I demonstrate large economic gains to investors using deep learning forecasts. The results of my comparative experiments highlight the importance of domain knowledge and financial theory when designing deep learning models. I also show return prediction tasks bring new challenges to deep learning. The time varying distribution causes distribution shift problem, which is essential for financial time series prediction. I demonstrate that deep learning methods can improve asset risk premium measurement. Due to the booming deep learning studies, they can constantly promote the study of underlying financial mechanisms behind asset pricing. I also propose a promising research method that learning from data and figuring out the underlying economic mechanisms through explainable artificial intelligence (AI) methods. My findings not only justify the value of deep learning in blooming fintech development, but also highlight their prospects and advantages over traditional machine learning methods. △ Less","24 September, 2022",https://arxiv.org/pdf/2209.12014
Reversible Data Hiding in Encrypted Text Using Paillier Cryptosystem,Asad Malik;Aeyan Ashraf;Hanzhou Wu;Minoru Kuribayashi,"Reversible Data Hiding in Encrypted Domain (RDHED) is an innovative method that can keep cover information secret and allows the data hider to insert additional information into it. This article presents a novel data hiding technique in an encrypted text called Reversible Data Hiding in Encrypted Text (RDHET). Initially, the original text is converted into their ASCII values. After that, the Paillier cryptosystem is adopted to encrypt all ASCII values of the original text and send it to the data hider for further processing. At the data hiding phase, the secret data are embedded into homomorphically encrypted text using a technique that does not lose any information, i.e., the homomorphic properties of the Paillier cryptosystem. Finally, the embedded secret data and the original text are recovered at the receiving end without any loss. Experimental results show that the proposed scheme is vital in the context of encrypted text processing at cloud-based services. Moreover, the scheme works well, especially for the embedding phase, text recovery, and performance on different security key sizes. △ Less","23 September, 2022",https://arxiv.org/pdf/2209.11802
The SpeakIn Speaker Verification System for Far-Field Speaker Verification Challenge 2022,Yu Zheng;Jinghan Peng;Yihao Chen;Yajun Zhang;Jialong Wang;Min Liu;Minqiang Xu,"This paper describes speaker verification (SV) systems submitted by the SpeakIn team to the Task 1 and Task 2 of the Far-Field Speaker Verification Challenge 2022 (FFSVC2022). SV tasks of the challenge focus on the problem of fully supervised far-field speaker verification (Task 1) and semi-supervised far-field speaker verification (Task 2). In Task 1, we used the VoxCeleb and FFSVC2020 datasets as train datasets. And for Task 2, we only used the VoxCeleb dataset as train set. The ResNet-based and RepVGG-based architectures were developed for this challenge. Global statistic pooling structure and MQMHA pooling structure were used to aggregate the frame-level features across time to obtain utterance-level representation. We adopted AM-Softmax and AAM-Softmax to classify the resulting embeddings. We innovatively propose a staged transfer learning method. In the pre-training stage we reserve the speaker weights, and there are no positive samples to train them in this stage. Then we fine-tune these weights with both positive and negative samples in the second stage. Compared with the traditional transfer learning strategy, this strategy can better improve the model performance. The Sub-Mean and AS-Norm backend methods were used to solve the problem of domain mismatch. In the fusion stage, three models were fused in Task1 and two models were fused in Task2. On the FFSVC2022 leaderboard, the EER of our submission is 3.0049% and the corresponding minDCF is 0.2938 in Task1. In Task2, EER and minDCF are 6.2060% and 0.5232 respectively. Our approach leads to excellent performance and ranks 1st in both challenge tasks. △ Less","23 September, 2022",https://arxiv.org/pdf/2209.11625
Analysis on Blockchain Consensus Mechanism Based on Proof of Work and Proof of Stake,Shi Yan,"In the white book of Bitcion, Satoshi Nakamoto described a bitcoin system that can realize point-to-point online payment without a third-party organization. After supporting this magical application scenario and subverting the traditional centralized system, the blockchain technology has attracted worldwide attention, triggered a research upsurge of blockchain consensus algorithm, and produced a large number of innovative applications. Although various consensus algorithms continue to evolve with the iteration of blockchain products and applications, Proof of Work (POW) and Proof of Skake (POS) algorithms are still the core of consensus algorithms. This paper discusses two algorithms of POW and POS in blockchain consensus mechanism, and analyzes the advantages and the existing problems of the two consensus mechanisms. Since consensus mechanism is the main focus of blockchain technology and has many influencing factors, this paper discusses the current problems and some improved ideas, and selects some typical algorithms for a more systematic introduction. In addition, some important issues related to safety and performance are also discussed. This paper provides the researchers a great reference on blockchain consensus mechanism. △ Less","23 September, 2022",https://arxiv.org/pdf/2209.11545
Blockchain-Oriented Services Computing in Action: Insights from a User Study,Giovanni Quattrocchi;Damian Andrew Tamburri;WIllem-Jan Van Den Heuvel,"Blockchain architectures promise disruptive innovation but factually they pose many architectural restrictions to classical service-based applications and show considerable design, implementation, and operations overhead. Furthermore, the relation between such overheads and user benefits is not clear yet. To shed light on the aforementioned relations, a service-based blockchain architecture was designed and deployed as part of a field study in real-life experimentation. An observational approach was then performed to elaborate on the technology-acceptance of the service-based blockchain architecture in question. Evidence shows that the resulting architecture is, in principle, not different than other less complex equivalents; furthermore, the architectural limitations posed by the blockchain-oriented design demand a significant additional effort to be put onto even the simplest of functionalities. We conclude that further research shall be invested in clarifying further the design principles we learned as part of this study as well as any trade-offs posed by blockchain-oriented service design and operation. △ Less","22 September, 2022",https://arxiv.org/pdf/2209.11320
Beyond Voxel Prediction Uncertainty: Identifying brain lesions you can trust,Benjamin Lambert;Florence Forbes;Senan Doyle;Alan Tucholka;Michel Dojat,"Deep neural networks have become the gold-standard approach for the automated segmentation of 3D medical images. Their full acceptance by clinicians remains however hampered by the lack of intelligible uncertainty assessment of the provided results. Most approaches to quantify their uncertainty, such as the popular Monte Carlo dropout, restrict to some measure of uncertainty in prediction at the voxel level. In addition not to be clearly related to genuine medical uncertainty, this is not clinically satisfying as most objects of interest (e.g. brain lesions) are made of groups of voxels whose overall relevance may not simply reduce to the sum or mean of their individual uncertainties. In this work, we propose to go beyond voxel-wise assessment using an innovative Graph Neural Network approach, trained from the outputs of a Monte Carlo dropout model. This network allows the fusion of three estimators of voxel uncertainty: entropy, variance, and model's confidence; and can be applied to any lesion, regardless of its shape or size. We demonstrate the superiority of our approach for uncertainty estimate on a task of Multiple Sclerosis lesions segmentation. △ Less","22 September, 2022",https://arxiv.org/pdf/2209.10877
Metaball-Imaging Discrete Element Lattice Boltzmann Method for fluid-particle system of complex morphologies with settling case study,Yifeng Zhao;Pei Zhang;Liang Lei;S. A. Galindo-Torres;Stan Z. Li,"Fluid-particle systems are highly sensitive to particle morphologies. While many attempts have been made on shape descriptors and coupling schemes, how to simulate the particle-particle and particle-fluid interactions with a balance between accuracy and efficiency is still a challenge, especially when complex-shaped particles are considered. This study presents a Metaball-Imaging (MI) based Discrete Element Lattice Boltzmann Method (DELBM) for fluid simulations with irregular shaped particles. The major innovation is the MI algorithm to capture the real grain shape for DELBM simulations,where the Metaball function is utilized as the mathematical representation due to its versatile and efficient expressiveness of complex shapes.The contact detection is tackled robustly by gradient calculation of the closest point with a Newton-Raphson based scheme. And the coupling with LBM is accomplished by a classic sharp-interface scheme. As for refiling, a local refiling algorithm based on the bounce back rule is implemented. Validations on three settling experiments of irregular-shaped natural cobblestones indicate the proposed model to be effective and powerful in probing micromechanics of irregular-shaped granular media immersed in fluid systems. The potential of this model on studies of shape-induced physical processes is further investigated with numerical examples on the ""drafting, kissing and tumbling"" phenomenon of pair particles in various shapes. △ Less","22 November, 2022",https://arxiv.org/pdf/2209.10411
"A Systematic Literature Review of Soft Computing Techniques for Software Maintainability Prediction: State-of-the-Art, Challenges and Future Directions",Gokul Yenduri;Thippa Reddy Gadekallu,"The software is changing rapidly with the invention of advanced technologies and methodologies. The ability to rapidly and successfully upgrade software in response to changing business requirements is more vital than ever. For the long-term management of software products, measuring software maintainability is crucial. The use of soft computing techniques for software maintainability prediction has shown immense promise in software maintenance process by providing accurate prediction of software maintainability. To better understand the role of soft computing techniques for software maintainability prediction, we aim to provide a systematic literature review of soft computing techniques for software maintainability prediction. Firstly, we provide a detailed overview of software maintainability. Following this, we explore the fundamentals of software maintainability and the reasons for adopting soft computing methodologies for predicting software maintainability. Later, we examine the soft computing approaches employed in the process of software maintainability prediction. Furthermore, we discuss the difficulties and potential solutions associated with the use of soft computing techniques to predict software maintainability. Finally, we conclude the review with some promising future directions to drive further research innovations and developments in this promising area. △ Less","21 September, 2022",https://arxiv.org/pdf/2209.10131
The language and social behavior of innovators,A. Fronzetti Colladon;L. Toschi;E. Ughetto;F. Greco,"Innovators are creative people who can conjure the ground-breaking ideas that represent the main engine of innovative organizations. Past research has extensively investigated who innovators are and how they behave in work-related activities. In this paper, we suggest that it is necessary to analyze how innovators behave in other contexts, such as in informal communication spaces, where knowledge is shared without formal structure, rules, and work obligations. Drawing on communication and network theory, we analyze about 38,000 posts available in the intranet forum of a large multinational company. From this, we explain how innovators differ from other employees in terms of social network behavior and language characteristics. Through text mining, we find that innovators write more, use a more complex language, introduce new concepts/ideas, and use positive but factual-based language. Understanding how innovators behave and communicate can support the decision-making processes of managers who want to foster innovation. △ Less","20 September, 2022",https://arxiv.org/pdf/2209.09511
Mapping Climate Change Research via Open Repositories & AI: advantages and limitations for an evidence-based R&D policy-making,Nicandro Bovenzi;Nicolau Duran-Silva;Francesco Alessandro Massucci;Francesco Multari;César Parra-Rojas;Josep Pujol-Llatse,"In the last few years, several initiatives have been starting to offer access to research outputs data and metadata in an open fashion. The platforms developed by those initiatives are opening up scientific production to the wider public and they can be an invaluable asset for evidence-based policy-making in Science, Technology and Innovation (STI). These resources can indeed facilitate knowledge discovery and help identify available R&D assets and relevant actors within specific research niches of interest. Ideally, to gain a comprehensive view of entire STI ecosystems, the information provided by each of these resources should be combined and analysed accordingly. To ensure so, at least a certain degree of interoperability should be guaranteed across data sources, so that data could be better aggregated and complemented and that evidence provided towards policy-making is more complete and reliable. Here, we study whether this is the case for the case of mapping Climate Action research in the whole Denmark STI ecosystem, by using 4 popular open access STI data sources, namely OpenAire, Open Alex, CORDIS and Kohesio. △ Less","19 September, 2022",https://arxiv.org/pdf/2209.09246
Mapping STI ecosystems via Open Data: overcoming the limitations of conflicting taxonomies. A case study for Climate Change Research in Denmark,Nicandro Bovenzi;Nicolau Duran-Silva;Francesco Alessandro Massucci;Francesco Multari;Cèsar Parra-Rojas;Josep Pujol-Llatse,"Science, Technology and Innovation (STI) decision-makers often need to have a clear vision of what is researched and by whom to design effective policies. Such a vision is provided by effective and comprehensive mappings of the research activities carried out within their institutional boundaries. A major challenge to be faced in this context is the difficulty in accessing the relevant data and in combining information coming from different sources: indeed, traditionally, STI data has been confined within closed data sources and, when available, it is categorised with different taxonomies. Here, we present a proof-of-concept study of the use of Open Resources to map the research landscape on the Sustainable Development Goal (SDG) 13-Climate Action, for an entire country, Denmark, and we map it on the 25 ERC panels. △ Less","19 September, 2022",https://arxiv.org/pdf/2209.08920
An Interactive Knowledge-based Multi-objective Evolutionary Algorithm Framework for Practical Optimization Problems,Abhiroop Ghosh;Kalyanmoy Deb;Erik Goodman;Ronald Averill,"Experienced users often have useful knowledge and intuition in solving real-world optimization problems. User knowledge can be formulated as inter-variable relationships to assist an optimization algorithm in finding good solutions faster. Such inter-variable interactions can also be automatically learned from high-performing solutions discovered at intermediate iterations in an optimization run - a process called innovization. These relations, if vetted by the users, can be enforced among newly generated solutions to steer the optimization algorithm towards practically promising regions in the search space. Challenges arise for large-scale problems where the number of such variable relationships may be high. This paper proposes an interactive knowledge-based evolutionary multi-objective optimization (IK-EMO) framework that extracts hidden variable-wise relationships as knowledge from evolving high-performing solutions, shares them with users to receive feedback, and applies them back to the optimization process to improve its effectiveness. The knowledge extraction process uses a systematic and elegant graph analysis method which scales well with number of variables. The working of the proposed IK-EMO is demonstrated on three large-scale real-world engineering design problems. The simplicity and elegance of the proposed knowledge extraction process and achievement of high-performing solutions quickly indicate the power of the proposed framework. The results presented should motivate further such interaction-based optimization studies for their routine use in practice. △ Less","18 September, 2022",https://arxiv.org/pdf/2209.08604
Survival of dominated strategies under imitation dynamics,Panayotis Mertikopoulos;Yannick Viossat,"The literature on evolutionary game theory suggests that pure strategies that are strictly dominated by other pure strategies always become extinct under imitative game dynamics, but they can survive under innovative dynamics. As we explain, this is because innovative dynamics favour rare strategies while standard imitative dynamics do not. However, as we also show, there are reasonable imitation protocols that favour rare or frequent strategies, thus allowing strictly dominated strategies to survive in large classes of imitation dynamics. Dominated strategies can persist at nontrivial frequencies even when the level of domination is not small. △ Less","17 September, 2022",https://arxiv.org/pdf/2209.08416
langcc: A Next-Generation Compiler Compiler,Joe Zimmerman,"Traditionally, parsing has been a laborious and error-prone component of compiler development, and most parsers for full industrial programming languages are still written by hand. The author [Zim22] shows that automatic parser generation can be practical, via a number of new innovations upon the standard LR paradigm of Knuth et al. With this methodology, we can automatically generate efficient parsers for virtually all languages that are intuitively ""easy to parse"". This includes Golang 1.17.8 and Python 3.9.12, for which our generated parsers are, respectively, 1.2x and 4.3x faster than the standard parsers. This document is a companion technical report which describes the software implementation of that work, which is available open-source at https://github.com/jzimmerman/langcc. △ Less","17 September, 2022",https://arxiv.org/pdf/2209.08385
Private Synthetic Data for Multitask Learning and Marginal Queries,Giuseppe Vietri;Cedric Archambeau;Sergul Aydore;William Brown;Michael Kearns;Aaron Roth;Ankit Siva;Shuai Tang;Zhiwei Steven Wu,"We provide a differentially private algorithm for producing synthetic data simultaneously useful for multiple tasks: marginal queries and multitask machine learning (ML). A key innovation in our algorithm is the ability to directly handle numerical features, in contrast to a number of related prior approaches which require numerical features to be first converted into {high cardinality} categorical features via {a binning strategy}. Higher binning granularity is required for better accuracy, but this negatively impacts scalability. Eliminating the need for binning allows us to produce synthetic data preserving large numbers of statistical queries such as marginals on numerical features, and class conditional linear threshold queries. Preserving the latter means that the fraction of points of each class label above a particular half-space is roughly the same in both the real and synthetic data. This is the property that is needed to train a linear classifier in a multitask setting. Our algorithm also allows us to produce high quality synthetic data for mixed marginal queries, that combine both categorical and numerical features. Our method consistently runs 2-5x faster than the best comparable techniques, and provides significant accuracy improvements in both marginal queries and linear prediction tasks for mixed-type datasets. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07400
Efficient first-order predictor-corrector multiple objective optimization for fair misinformation detection,Eric Enouen;Katja Mathesius;Sean Wang;Arielle Carr;Sihong Xie,"Multiple-objective optimization (MOO) aims to simultaneously optimize multiple conflicting objectives and has found important applications in machine learning, such as minimizing classification loss and discrepancy in treating different populations for fairness. At optimality, further optimizing one objective will necessarily harm at least another objective, and decision-makers need to comprehensively explore multiple optima (called Pareto front) to pinpoint one final solution. We address the efficiency of finding the Pareto front. First, finding the front from scratch using stochastic multi-gradient descent (SMGD) is expensive with large neural networks and datasets. We propose to explore the Pareto front as a manifold from a few initial optima, based on a predictor-corrector method. Second, for each exploration step, the predictor solves a large-scale linear system that scales quadratically in the number of model parameters and requires one backpropagation to evaluate a second-order Hessian-vector product per iteration of the solver. We propose a Gauss-Newton approximation that only scales linearly, and that requires only first-order inner-product per iteration. This also allows for a choice between the MINRES and conjugate gradient methods when approximately solving the linear system. The innovations make predictor-corrector possible for large networks. Experiments on multi-objective (fairness and accuracy) misinformation detection tasks show that 1) the predictor-corrector method can find Pareto fronts better than or similar to SMGD with less time; and 2) the proposed first-order method does not harm the quality of the Pareto front identified by the second-order method, while further reduce running time. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07245
Responsible AI Implementation: A Human-centered Framework for Accelerating the Innovation Process,Dian Tjondronegoro;Elizabeth Yuwono;Brent Richards;Damian Green;Siiri Hatakka,"There is still a significant gap between expectations and the successful adoption of AI to innovate and improve businesses. Due to the emergence of deep learning, AI adoption is more complex as it often incorporates big data and the internet of things, affecting data privacy. Existing frameworks have identified the need to focus on human-centered design, combining technical and business/organizational perspectives. However, trust remains a critical issue that needs to be designed from the beginning. The proposed framework expands from the human-centered design approach, emphasizing and maintaining the trust that underpins the process. This paper proposes a theoretical framework for responsible artificial intelligence (AI) implementation. The proposed framework emphasizes a synergistic business technology approach for the agile co-creation process. The aim is to streamline the adoption process of AI to innovate and improve business by involving all stakeholders throughout the project so that the AI technology is designed, developed, and deployed in conjunction with people and not in isolation. The framework presents a fresh viewpoint on responsible AI implementation based on analytical literature review, conceptual framework design, and practitioners' mediating expertise. The framework emphasizes establishing and maintaining trust throughout the human-centered design and agile development of AI. This human-centered approach is aligned with and enabled by the privacy by design principle. The creators of the technology and the end-users are working together to tailor the AI solution specifically for the business requirements and human characteristics. An illustrative case study on adopting AI for assisting planning in a hospital will demonstrate that the proposed framework applies to real-life applications. △ Less","15 September, 2022",https://arxiv.org/pdf/2209.07076
Data Science Approach to predict the winning Fantasy Cricket Team Dream 11 Fantasy Sports,Sachin Kumar S;Prithvi HV;C Nandini,"The evolution of digital technology and the increasing popularity of sports inspired the innovators to take the experience of users with a proclivity towards sports to a whole new different level, by introducing Fantasy Sports Platforms FSPs. The application of Data Science and Analytics is Ubiquitous in the Modern World. Data Science and Analytics open doors to gain a deeper understanding and help in the decision making process. We firmly believed that we could adopt Data Science to predict the winning fantasy cricket team on the FSP, Dream 11. We built a predictive model that predicts the performance of players in a prospective game. We used a combination of Greedy and Knapsack Algorithms to prescribe the combination of 11 players to create a fantasy cricket team that has the most significant statistical odds of finishing as the strongest team thereby giving us a higher chance of winning the pot of bets on the Dream 11 FSP. We used PyCaret Python Library to help us understand and adopt the best Regressor Algorithm for our problem statement to make precise predictions. Further, we used Plotly Python Library to give us visual insights into the team, and players performances by accounting for the statistical, and subjective factors of a prospective game. The interactive plots help us to bolster the recommendations of our predictive model. You either win big, win small, or lose your bet based on the performance of the players selected for your fantasy team in the prospective game, and our model increases the probability of you winning big. △ Less","14 September, 2022",https://arxiv.org/pdf/2209.06999
Exploiting Expert Knowledge for Assigning Firms to Industries: A Novel Deep Learning Method,Xiaohang Zhao;Xiao Fang;Jing He;Lihua Huang,"Industry assignment, which assigns firms to industries according to a predefined Industry Classification System (ICS), is fundamental to a large number of critical business practices, ranging from operations and strategic decision making by firms to economic analyses by government agencies. Three types of expert knowledge are essential to effective industry assignment: definition-based knowledge (i.e., expert definitions of each industry), structure-based knowledge (i.e., structural relationships among industries as specified in an ICS), and assignment-based knowledge (i.e., prior firm-industry assignments performed by domain experts). Existing industry assignment methods utilize only assignment-based knowledge to learn a model that classifies unassigned firms to industries, and overlook definition-based and structure-based knowledge. Moreover, these methods only consider which industry a firm has been assigned to, but ignore the time-specificity of assignment-based knowledge, i.e., when the assignment occurs. To address the limitations of existing methods, we propose a novel deep learning-based method that not only seamlessly integrates the three types of knowledge for industry assignment but also takes the time-specificity of assignment-based knowledge into account. Methodologically, our method features two innovations: dynamic industry representation and hierarchical assignment. The former represents an industry as a sequence of time-specific vectors by integrating the three types of knowledge through our proposed temporal and spatial aggregation mechanisms. The latter takes industry and firm representations as inputs, computes the probability of assigning a firm to different industries, and assigns the firm to the industry with the highest probability. △ Less","11 September, 2022",https://arxiv.org/pdf/2209.05943
A new Reinforcement Learning framework to discover natural flavor molecules,Luana P. Queiroz;Carine M. Rebello;Erbet A. Costa;Vinícius V. Santana;Bruno C. L. Rodrigues;Alírio E. Rodrigues;Ana M. Ribeiro;Idelfonso B. R. Nogueira,"The flavor is the focal point in the flavor industry, which follows social tendencies and behaviors. The research and development of new flavoring agents and molecules are essential in this field. On the other hand, the development of natural flavors plays a critical role in modern society. In light of this, the present work proposes a novel framework based on Scientific Machine Learning to undertake an emerging problem in flavor engineering and industry. Therefore, this work brings an innovative methodology to design new natural flavor molecules. The molecules are evaluated regarding the synthetic accessibility, the number of atoms, and the likeness to a natural or pseudo-natural product. △ Less","13 September, 2022",https://arxiv.org/pdf/2209.05859
"Data Innovation in Demography, Migration and Human Mobility",Claudio Bosco;Sara Grubanov-Boskovic;Stefano Iacus;Umberto Minora;Francesco Sermi;Spyridon Spyratos,"With the consolidation of the culture of evidence-based policymaking, the availability of data has become central to policymakers. Nowadays, innovative data sources offer an opportunity to describe demographic, mobility, and migratory phenomena more accurately by making available large volumes of real-time and spatially detailed data. At the same time, however, data innovation has led to new challenges (ethics, privacy, data governance models, data quality) for citizens, statistical offices, policymakers and the private sector. Focusing on the fields of demography, mobility, and migration studies, the aim of this report is to assess the current state of data innovation in the scientific literature as well as to identify areas in which data innovation has the most concrete potential for policymaking. Consequently, this study has reviewed more than 300 articles and scientific reports, as well as numerous tools, that employed non-traditional data sources to measure vital population events (mortality, fertility), migration and human mobility, and the population change and population distribution. The specific findings of our report form the basis of a discussion on a) how innovative data is used compared to traditional data sources; b) domains in which innovative data have the greatest potential to contribute to policymaking; c) the prospects of innovative data transition towards systematically contributing to official statistics and policymaking. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.05460
Communication-Efficient and Privacy-Preserving Feature-based Federated Transfer Learning,Feng Wang;M. Cenk Gursoy;Senem Velipasalar,"Federated learning has attracted growing interest as it preserves the clients' privacy. As a variant of federated learning, federated transfer learning utilizes the knowledge from similar tasks and thus has also been intensively studied. However, due to the limited radio spectrum, the communication efficiency of federated learning via wireless links is critical since some tasks may require thousands of Terabytes of uplink payload. In order to improve the communication efficiency, we in this paper propose the feature-based federated transfer learning as an innovative approach to reduce the uplink payload by more than five orders of magnitude compared to that of existing approaches. We first introduce the system design in which the extracted features and outputs are uploaded instead of parameter updates, and then determine the required payload with this approach and provide comparisons with the existing approaches. Subsequently, we analyze the random shuffling scheme that preserves the clients' privacy. Finally, we evaluate the performance of the proposed learning scheme via experiments on an image classification task to show its effectiveness. △ Less","12 September, 2022",https://arxiv.org/pdf/2209.05395
Innovative ideas for teaching supports: Application to Graph theory,Nicolas Catusse;Hadrien Cambazard;Nadia Brauner;Bernard Penz;Florian Fontan,"Teaching graph theory with the most adequate tools requires time and ideas. We present how an open community of teachers shares contents and ideas on an innovative platform. The objective is to get the students autonomous in their training with activities that give them immediate feedback on their understanding. Beyond learning, the very large collection of exercises of various levels can also be used to evaluate the student's level. The proposed activities can be algorithm's code in classical programming languages (e.g. Java, Python) that the student can test with predefined tests proposed by the teacher or collections of generated questions. △ Less","12 September, 2022",https://arxiv.org/pdf/2209.05078
Multi-modal Streaming 3D Object Detection,Mazen Abdelfattah;Kaiwen Yuan;Z. Jane Wang;Rabab Ward,"Modern autonomous vehicles rely heavily on mechanical LiDARs for perception. Current perception methods generally require 360° point clouds, collected sequentially as the LiDAR scans the azimuth and acquires consecutive wedge-shaped slices. The acquisition latency of a full scan (~ 100ms) may lead to outdated perception which is detrimental to safe operation. Recent streaming perception works proposed directly processing LiDAR slices and compensating for the narrow field of view (FOV) of a slice by reusing features from preceding slices. These works, however, are all based on a single modality and require past information which may be outdated. Meanwhile, images from high-frequency cameras can support streaming models as they provide a larger FoV compared to a LiDAR slice. However, this difference in FoV complicates sensor fusion. To address this research gap, we propose an innovative camera-LiDAR streaming 3D object detection framework that uses camera images instead of past LiDAR slices to provide an up-to-date, dense, and wide context for streaming perception. The proposed method outperforms prior streaming models on the challenging NuScenes benchmark. It also outperforms powerful full-scan detectors while being much faster. Our method is shown to be robust to missing camera images, narrow LiDAR slices, and small camera-LiDAR miscalibration. △ Less","11 September, 2022",https://arxiv.org/pdf/2209.04966
Towards Security Enhancement of Blockchain-based Supply Chain Management,Abdul Khalique Shaikh A. K. Al-Alawi;L. R.;Al-Busaidi;R.;Shaikh,"The cybersecurity of modern systems has dramatically increased attention from both industrial and academia perspectives. In the recent era, the popularity of the blockchain-based system has traditionally been emergent among various industrials sectors especially in supply chain management due to its streamlined nature. This reveals the importance of the quality aspects from a supply chain management perspective. Many industries realized the importance of having quality systems for supply chain management and logistics. The emergence of blockchain technology has created several potential innovations in handling and tracking business activities over the supply chain processes as specific. This paper shed the light on the blockchain and specifically on a smart contract technology which been used to handle the process of creation, verification and checking data over the supply chain management process. Then, touch upon the area of blockchain cybersecurity in the supply chain context. More and more, since the smart contract handles the transfer of data over different locations, then the security protection should be strong enough to secure the data and the assets from any attacks. Finally, the paper examines the main security attacks that affect the data on the blockchain and propose a solution △ Less","11 September, 2022",https://arxiv.org/pdf/2209.04917
Web 3.0 Adoption Behavior: PLS-SEM and Sentiment Analysis,Sheikh M. Hizam;Waqas Ahmed;Habiba Akter;Ilham Sentosa;Mohamad N. Masrek,"Web 3.0 is considered as future of Internet where decentralization, user personalization and privacy protection would be the main aspects of Internet. Aim of this research work is to elucidate the adoption behavior of Web 3.0through a multi-analytical approach based on Partial Least Squares Structural Equation Modelling (PLS-SEM) and Twitter sentiment analysis. A theoretical framework centered on Performance Expectancy (PE), Electronic Word-of-Mouth (eWOM) and Digital Dexterity (DD), was hypothesized towards Behavioral Intention (INT) of the Web 3.0 adoption. Surveyed data were collected through online questionnaires and 167 responses were analyzed through PLS-SEM. While 3,989 tweets of Web3 were analyzed by VADER sentiment analysis tool in RapidMiner. PLS-SEM results showed that DD and eWOM had significant impact while PE had no effect on INT. Moreover, these results were also validated by PLS-Predict method. While sentiment analysis explored that 56% tweets on Web 3.0 were positive in sense and 7% depicted negative sentiment while remaining were neutral. Such inferences are novel in nature and an innovative addition to web informatics and could support the stakeholders towards web technology integration △ Less","11 September, 2022",https://arxiv.org/pdf/2209.04900
Audio Analytics-based Human Trafficking Detection Framework for Autonomous Vehicles,Sagar Dasgupta;Kazi Shakib;Mizanur Rahman;Silvana V Croope;Steven Jones,"Human trafficking is a universal problem, persistent despite numerous efforts to combat it globally. Individuals of any age, race, ethnicity, sex, gender identity, sexual orientation, nationality, immigration status, cultural background, religion, socioeconomic class, and education can be a victim of human trafficking. With the advancements in technology and the introduction of autonomous vehicles (AVs), human traffickers will adopt new ways to transport victims, which could accelerate the growth of organized human trafficking networks, which can make the detection of trafficking in persons more challenging for law enforcement agencies. The objective of this study is to develop an innovative audio analytics-based human trafficking detection framework for autonomous vehicles. The primary contributions of this study are to: (i) define four non-trivial, feasible, and realistic human trafficking scenarios for AVs; (ii) create a new and comprehensive audio dataset related to human trafficking with five classes i.e., crying, screaming, car door banging, car noise, and conversation; and (iii) develop a deep 1-D Convolution Neural Network (CNN) architecture for audio data classification related to human trafficking. We have also conducted a case study using the new audio dataset and evaluated the audio classification performance of the deep 1-D CNN. Our analyses reveal that the deep 1-D CNN can distinguish sound coming from a human trafficking victim from a non-human trafficking sound with an accuracy of 95%, which proves the efficacy of our framework. △ Less","8 September, 2022",https://arxiv.org/pdf/2209.04071
The Science Gateway Community Institute's Consulting Services Program: Lessons for Research Software Engineering Organizations,Marlon Pierce;Michael Zentner;Maytal Dahan;Sandra Gesing;Claire Stirm;Linda Bailey Hayden,"The Science Gateways Community Institute (SGCI) is an NSF Software Infrastructure for Sustained Innovation (S2I2) funded project that leads and supports the science gateway community. Major activities for SGCI include a) sustainability training, including the Focus Week week-long course designed to help science gateway operators develop sustainability plans, and the Jumpstart virtual short-course; b) usability and user experience consulting; c) a community catalog of science gateways and science gateway software; d) workforce development activities, including a coding institute for students, internship opportunities, and hackathons; e) an annual conference; and f) in-depth technical support for client gateway projects. The goals of SGCI's Embedded Technical Support component are to help the institute's clients to create new science gateways or to significantly enhance existing science gateways. Examples of the latter include helping to implement major new capabilities and to implement significant usability improvements suggested by SGCI's usability consultants. The Embedded Technical Support component was managed by Indiana University and involved research software engineers at San Diego Supercomputer Center, Texas Advanced Computing Center, Indiana University, and Purdue University (through 2019). Since 2016, the component has involved 20 research software engineers as consultants and has conducted 59 client consultations. This short paper provides a summary of lessons learned from the Embedded Technical Support program that may be useful for the research software engineering community. △ Less","7 September, 2022",https://arxiv.org/pdf/2209.03958
Multi-Granularity Prediction for Scene Text Recognition,Peng Wang;Cheng Da;Cong Yao,"Scene text recognition (STR) has been an active research topic in computer vision for years. To tackle this challenging problem, numerous innovative methods have been successively proposed and incorporating linguistic knowledge into STR models has recently become a prominent trend. In this work, we first draw inspiration from the recent progress in Vision Transformer (ViT) to construct a conceptually simple yet powerful vision STR model, which is built upon ViT and outperforms previous state-of-the-art models for scene text recognition, including both pure vision models and language-augmented methods. To integrate linguistic knowledge, we further propose a Multi-Granularity Prediction strategy to inject information from the language modality into the model in an implicit way, i.e. , subword representations (BPE and WordPiece) widely-used in NLP are introduced into the output space, in addition to the conventional character level representation, while no independent language model (LM) is adopted. The resultant algorithm (termed MGP-STR) is able to push the performance envelop of STR to an even higher level. Specifically, it achieves an average recognition accuracy of 93.35% on standard benchmarks. Code is available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR. △ Less","17 October, 2022",https://arxiv.org/pdf/2209.03592
"A Survey on Open-Source-Defined Wireless Networks: Framework, Key Technology, and Implementation",Liqiang Zhao;Muhammad Muhammad Bala;Wu Gang;Pan Chengkang;Yuan Yannan;Tian Zhigang;Yu-Chee Tseng;Chen Xiang;Bin Shen;Chih-Lin I,"The realization of open-source-defined wireless networks in the telecommunication domain is accomplished through the fifth-generation network (5G). In contrast to its predecessors (3G and 4G), the 5G network can support a wide variety of heterogeneous use cases with challenging requirements from both the Internet and the Internet of Things (IoT). The future sixth-generation (6G) network will not only extend 5G capabilities but also innovate new functionalities to address emerging academic and engineering challenges. The research community has identified these challenges could be overcome by open-source-defined wireless networks, which is based on open-source software and hardware. In this survey, we present an overview of different aspects of open-source-defined wireless networks, comprising motivation, frameworks, key technologies, and implementation. We start by introducing the motivation and explore several frameworks with classification into three different categories: black-box, grey-box, and white-box. We review research efforts related to open-source-defined Core Network (CN), Radio Access Network (RAN), Multi-access Edge Computing (MEC), the capabilities of security threats, open-source hardware, and various implementations, including testbeds. The last but most important in this survey, lessons learned, future research direction, open research issues, pitfalls, and limitations of existing surveys on open-source wireless networks are included to motivate and encourage future research. △ Less","5 September, 2022",https://arxiv.org/pdf/2209.01891
"Migration patterns, friendship networks, and the diaspora: the potential of Facebook Social Connectedness Index to anticipate displacement patterns induced by Russia invasion of Ukraine in the European Union",Umberto Minora;Martina Belmonte;Claudio Bosco;Drew Johnston;Eugenia Giraudy;Stefano Iacus;Francesco Sermi,"The conflict in Ukraine is causing large-scale displacement in Europe and in the World. Based on the United Nations High Commissioner for Refugees (UNHCR) estimates, more than 7 million people fled the country as of 5 September 2022. In this context, it is extremely important to anticipate where these people are moving so that national to local authorities can better manage challenges related to their reception and integration. This work shows how innovative data from social media can provide useful insights on conflict-induced migration flows. In particular, we explore the potential of Facebook's Social Connectedness Index (SCI) for predicting migration flows in the context of the war in Ukraine, building on previous research findings that the presence of a diaspora network is one of the major migration drivers. To do so, we first evaluate the relationship between the Ukrainian diaspora and the number of refugees from Ukraine registered for Temporary Protection or similar national schemes as a proxy of migratory flows into the EU. We find a very strong correlation between the two (Pearson's r=0.94, p<0.0001), which indicates that the diaspora is attracting the people fleeing the war, who tend to reach their compatriots, in particular in the countries where the Ukrainian immigration was more a recent phenomenon. Second, we compare Facebook's SCI with available official data on diaspora at regional level in Europe. Our results suggest that the index, along with other readily available covariates, is a strong predictor of the Ukrainian diaspora at regional scale. Finally, we discuss the potential of Facebook's SCI to provide timely and spatially detailed information on human diaspora for those countries where this information might be missing or outdated, and to complement official statistics for fast policy response during conflicts. △ Less","15 December, 2022",https://arxiv.org/pdf/2209.01833
Data-Assisted Vision-Based Hybrid Control for Robust Stabilization with Obstacle Avoidance via Learning of Perception Maps,Alejandro Murillo-Gonzalez;Jorge I. Poveda,"We study the problem of target stabilization with robust obstacle avoidance in robots and vehicles that have access only to vision-based sensors for the purpose of realtime localization. This problem is particularly challenging due to the topological obstructions induced by the obstacle, which preclude the existence of smooth feedback controllers able to achieve simultaneous stabilization and robust obstacle avoidance. To overcome this issue, we develop a vision-based hybrid controller that switches between two different feedback laws depending on the current position of the vehicle using a hysteresis mechanism and a data-assisted supervisor. The main innovation of the paper is the incorporation of suitable perception maps into the hybrid controller. These maps can be learned from data obtained from cameras in the vehicles and trained via convolutional neural networks (CNN). Under suitable assumptions on this perception map, we establish theoretical guarantees for the trajectories of the vehicle in terms of convergence and obstacle avoidance. Moreover, the proposed vision-based hybrid controller is numerically tested under different scenarios, including noisy data, sensors with failures, and cameras with occlusions. △ Less","4 September, 2022",https://arxiv.org/pdf/2209.01597
Low-Power Hardware-Based Deep-Learning Diagnostics Support Case Study,Khushal Sethi;Vivek Parmar;Manan Suri,"Deep learning research has generated widespread interest leading to emergence of a large variety of technological innovations and applications. As significant proportion of deep learning research focuses on vision based applications, there exists a potential for using some of these techniques to enable low-power portable health-care diagnostic support solutions. In this paper, we propose an embedded-hardware-based implementation of microscopy diagnostic support system for PoC case study on: (a) Malaria in thick blood smears, (b) Tuberculosis in sputum samples, and (c) Intestinal parasite infection in stool samples. We use a Squeeze-Net based model to reduce the network size and computation time. We also utilize the Trained Quantization technique to further reduce memory footprint of the learned models. This enables microscopy-based detection of pathogens that classifies with laboratory expert level accuracy as a standalone embedded hardware platform. The proposed implementation is 6x more power-efficient compared to conventional CPU-based implementation and has an inference time of \sim 3 ms/sample. △ Less","3 September, 2022",https://arxiv.org/pdf/2209.01507
eDWaaS: A Scalable Educational Data Warehouse as a Service,Anupam Khan;Sourav Ghosh;Soumya K. Ghosh,"The university management is perpetually in the process of innovating policies to improve the quality of service. Intellectual growth of the students, the popularity of university are some of the major areas that management strives to improve upon. Relevant historical data is needed in support of taking any decision. Furthermore, providing data to various university ranking frameworks is a frequent activity in recent years. The format of such requirement changes frequently which requires efficient manual effort. Maintaining a data warehouse can be a solution to this problem. However, both in-house and outsourced implementation of a dedicated data warehouse may not be a cost-effective and smart solution. This work proposes an educational data warehouse as a service (eDWaaS) model to store historical data for multiple universities. The proposed multi-tenant schema facilitates the universities to maintain their data warehouse in a cost-effective solution. It also addresses the scalability issues in implementing such data warehouse as a service model. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.01045
"Authentication, Authorization, and Selective Disclosure for IoT data sharing using Verifiable Credentials and Zero-Knowledge Proofs",Nikos Fotiou;Iakovos Pittaras;Spiros Chadoulos;Vasilios A. Siris;George C. Polyzos;Nikolaos Ipiotis;Stratos Keranidis,"As IoT becomes omnipresent vast amounts of data are generated, which can be used for building innovative applications. However,interoperability issues and security concerns, prevent harvesting the full potentials of these data. In this paper we consider the use case of data generated by smart buildings. Buildings are becoming ever ""smarter"" by integrating IoT devices that improve comfort through sensing and automation. However, these devices and their data are usually siloed in specific applications or manufacturers, even though they can be valuable for various interested stakeholders who provide different types of ""over the top"" services, e.g., energy management. Most data sharing techniques follow an ""all or nothing"" approach, creating significant security and privacy threats, when even partially revealed, privacy-preserving, data subsets can fuel innovative applications. With these in mind we develop a platform that enables controlled, privacy-preserving sharing of data items. Our system innovates in two directions: Firstly, it provides a framework for allowing discovery and selective disclosure of IoT data without violating their integrity. Secondly, it provides a user-friendly, intuitive mechanisms allowing efficient, fine-grained access control over the shared data. Our solution leverages recent advances in the areas of Self-Sovereign Identities, Verifiable Credentials, and Zero-Knowledge Proofs, and it integrates them in a platform that combines the industry-standard authorization framework OAuth 2.0 and the Web of Things specifications. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00586
Secrecy Analysis for IRS-aided Wiretap MIMO Communications: Fundamental Limits and System Design,Xin Zhang;Shenghui Song,"In order to meet the demands of future innovative applications, many efforts have been made to exceed the limits predicted by Shannon's Theory. Besides the investigation of beyond-Shannon metrics such as security, latency, and semantics, another direction is to jointly design the transceiver and the environment by utilizing the intelligent reflecting surface (IRS). In this paper, we consider the analysis and design of IRS-aided multiple-input multiple-output (MIMO) secure communications, which has attracted much research attention but still in its infancy. For example, despite their importance, the fundamental limits of IRS-aided wiretap MIMO communications are not yet available in the literature. In this paper, we will investigate these fundamental limits by determining the ergodic secrecy rate (ESR) and secrecy outage probability (SOP). For that purpose, the central limit theorem (CLT) for the joint distributions of the mutual information (MI) statistics over the IRS-aided MIMO secure communication channel is derived by utilizing the random matrix theory (RMT). The derived CLT is then used to obtain the closed form expressions for the ESR and SOP, which are also extended to the scenario with multiple multi-antenna eavesdroppers. Based on the theoretical results, algorithms for maximizing the artificial noise (AN) aided ESR and minimizing the SOP are proposed. Numerical results validate the accuracy of the theoretical results and effectiveness of the proposed optimization algorithms. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00392
Monocular Camera-based Complex Obstacle Avoidance via Efficient Deep Reinforcement Learning,Jianchuan Ding;Lingping Gao;Wenxi Liu;Haiyin Piao;Jia Pan;Zhenjun Du;Xin Yang;Baocai Yin,"Deep reinforcement learning has achieved great success in laser-based collision avoidance works because the laser can sense accurate depth information without too much redundant data, which can maintain the robustness of the algorithm when it is migrated from the simulation environment to the real world. However, high-cost laser devices are not only difficult to deploy for a large scale of robots but also demonstrate unsatisfactory robustness towards the complex obstacles, including irregular obstacles, e.g., tables, chairs, and shelves, as well as complex ground and special materials. In this paper, we propose a novel monocular camera-based complex obstacle avoidance framework. Particularly, we innovatively transform the captured RGB images to pseudo-laser measurements for efficient deep reinforcement learning. Compared to the traditional laser measurement captured at a certain height that only contains one-dimensional distance information away from the neighboring obstacles, our proposed pseudo-laser measurement fuses the depth and semantic information of the captured RGB image, which makes our method effective for complex obstacles. We also design a feature extraction guidance module to weight the input pseudo-laser measurement, and the agent has more reasonable attention for the current state, which is conducive to improving the accuracy and efficiency of the obstacle avoidance policy. △ Less","1 September, 2022",https://arxiv.org/pdf/2209.00296
Scan-based immersed isogeometric flow analysis,Clemens V. Verhoosel;E. Harald van Brummelen;Sai C. Divi;Frits de Prenter,"This chapter reviews the work conducted by our team on scan-based immersed isogeometric analysis for flow problems. To leverage the advantageous properties of isogeometric analysis on complex scan-based domains, various innovations have been made: (i) A spline-based segmentation strategy has been developed to extract a geometry suitable for immersed analysis directly from scan data; (ii) A stabilized equal-order velocity-pressure formulation for the Stokes problem has been proposed to attain stable results on immersed domains; (iii) An adaptive integration quadrature procedure has been developed to improve computational efficiency; (iv) A mesh refinement strategy has been developed to capture small features at a priori unknown locations, without drastically increasing the computational cost of the scan-based analysis workflow. We review the key ideas behind each of these innovations, and illustrate these using a selection of simulation results from our work. A patient-specific scan-based analysis case is reproduced to illustrate how these innovations enable the simulation of flow problems on complex scan data. △ Less","31 August, 2022",https://arxiv.org/pdf/2208.14994
Innovation and informal knowledge exchanges between firms,Juste Raimbault,"Firm clusters are seen as having a positive effect on innovations, what can be interpreted as economies of scale or knowledge spillovers. The processes underlying the success of these clusters remain difficult to isolate. We propose in this paper a stylised agent-based model to test the role of geographical proximity and informal knowledge exchanges between firms on the emergence of innovations. The model is run on synthetic firm clusters. Sensitivity analysis and systematic model exploration unveil a strong impact of interaction distance on innovations, with a qualitative shift when spatial interactions are more intense. Model bi-objective optimisation shows a compromise between innovation and product diversity, suggesting trade-offs for clusters in practice. This model provides thus a first basis to systematically explore the interplay between firm cluster geography and innovation, from an evolutionary perspective. △ Less","31 August, 2022",https://arxiv.org/pdf/2208.14719
WikiLink: an encyclopedia-based semantic network for design innovation,Haoyu Zuo;Qianzhi Jing;Tianqi Song;Huiting Liu;Lingyun Sun;Peter Childs;Liuqing Chen,"Data-driven design and innovation is a process to reuse and provide valuable and useful information. However, existing semantic networks for design innovation is built on data source restricted to technological and scientific information. Besides, existing studies build the edges of a semantic network only on either statistical or semantic relationships, which is less likely to make full use of the benefits from both types of relationships and discover implicit knowledge for design innovation. Therefore, we constructed WikiLink, a semantic network based on Wikipedia. Combined weight which fuses both the statistic and semantic weights between concepts is introduced in WikiLink, and four algorithms are developed for inspiring new ideas. Evaluation experiments are undertaken and results show that the network is characterised by high coverage of terms, relationships and disciplines, which proves the network's effectiveness and usefulness. Then a demonstration and case study results indicate that WikiLink can serve as an idea generation tool for innovation in conceptual design. The source code of WikiLink and the backend data are provided open-source for more users to explore and build on. △ Less","30 August, 2022",https://arxiv.org/pdf/2208.14349
ANT: Exploiting Adaptive Numerical Data Type for Low-bit Deep Neural Network Quantization,Cong Guo;Chen Zhang;Jingwen Leng;Zihan Liu;Fan Yang;Yunxin Liu;Minyi Guo;Yuhao Zhu,"Quantization is a technique to reduce the computation and memory cost of DNN models, which are getting increasingly large. Existing quantization solutions use fixed-point integer or floating-point types, which have limited benefits, as both require more bits to maintain the accuracy of original models. On the other hand, variable-length quantization uses low-bit quantization for normal values and high-precision for a fraction of outlier values. Even though this line of work brings algorithmic benefits, it also introduces significant hardware overheads due to variable-length encoding and decoding. In this work, we propose a fixed-length adaptive numerical data type called ANT to achieve low-bit quantization with tiny hardware overheads. Our data type ANT leverages two key innovations to exploit the intra-tensor and inter-tensor adaptive opportunities in DNN models. First, we propose a particular data type, flint, that combines the advantages of float and int for adapting to the importance of different values within a tensor. Second, we propose an adaptive framework that selects the best type for each tensor according to its distribution characteristics. We design a unified processing element architecture for ANT and show its ease of integration with existing DNN accelerators. Our design results in 2.8\times speedup and 2.5\times energy efficiency improvement over the state-of-the-art quantization accelerators. △ Less","30 August, 2022",https://arxiv.org/pdf/2208.14286
Building the Learning Environment for Sustainable Development: a Co-creation approach,Ewa Duda,"Education for sustainable development supports the improvement of knowledge, skills, attitudes and behaviors related to global challenges such as climate change, global warming and environmental degradation, among others. It is increasingly taking place through projects based on information and communication technologies. The effectiveness of the actions taken depends not only on the quality of the project activities or the sophistication of the innovative tools used. Social commitment also depends on the beliefs and moral judgements manifested by potential recipients of educational activities on environmental issues. This study aimed to identify the beliefs and moral judgements that may facilitate or hinder the implementation of educational activities based on information and communication technology, shaping pro-environmental attitudes and behavior among city dwellers. Based on the co-creation workshops conducted, five general categories emerged: responsibility, sense of empowerment, local leadership, real eco-approach, and eco-knowledge. The research findings may contribute to the design of educational activities dedicated to shaping the pro-environmental behavior of city dwellers. △ Less","6 September, 2022",https://arxiv.org/pdf/2208.14151
An efficient and flexible inference system for serving heterogeneous ensembles of deep neural networks,Pierrick Pochelu;Serge G. Petiton;Bruno Conche,"Ensembles of Deep Neural Networks (DNNs) have achieved qualitative predictions but they are computing and memory intensive. Therefore, the demand is growing to make them answer a heavy workload of requests with available computational resources. Unlike recent initiatives on inference servers and inference frameworks, which focus on the prediction of single DNNs, we propose a new software layer to serve with flexibility and efficiency ensembles of DNNs. Our inference system is designed with several technical innovations. First, we propose a novel procedure to find a good allocation matrix between devices (CPUs or GPUs) and DNN instances. It runs successively a worst-fit to allocate DNNs into the memory devices and a greedy algorithm to optimize allocation settings and speed up the ensemble. Second, we design the inference system based on multiple processes to run asynchronously: batching, prediction, and the combination rule with an efficient internal communication scheme to avoid overhead. Experiments show the flexibility and efficiency under extreme scenarios: It successes to serve an ensemble of 12 heavy DNNs into 4 GPUs and at the opposite, one single DNN multi-threaded into 16 GPUs. It also outperforms the simple baseline consisting of optimizing the batch size of DNNs by a speedup up to 2.7X on the image classification task. △ Less","30 August, 2022",https://arxiv.org/pdf/2208.14049
Enjoy the Ride Consciously with CAWA: Context-Aware Advisory Warnings for Automated Driving,Erfan Pakdamanian;Erzhen Hu;Shili Sheng;Sarit Kraus;Seongkook Heo;Lu Feng,"In conditionally automated driving, drivers decoupled from driving while immersed in non-driving-related tasks (NDRTs) could potentially either miss the system-initiated takeover request (TOR) or a sudden TOR may startle them. To better prepare drivers for a safer takeover in an emergency, we propose novel context-aware advisory warnings (CAWA) for automated driving to gently inform drivers. This will help them stay vigilant while engaging in NDRTs. The key innovation is that CAWA adapts warning modalities according to the context of NDRTs. We conducted a user study to investigate the effectiveness of CAWA. The study results show that CAWA has statistically significant effects on safer takeover behavior, improved driver situational awareness, less attention demand, and more positive user feedback, compared with uniformly distributed speech-based warnings across all NDRTs. △ Less","29 August, 2022",https://arxiv.org/pdf/2208.13900
Local Verlet buffer approach for broad-phase interaction detection in Discrete Element Method,Abdoul Wahid Mainassara Checkaraou;Xavier Besseron;Alban Rousset;Fenglei Qi;Bernhard Peters,"The Extended Discrete Element Method (XDEM) is an innovative numerical simulation technique that extends the dynamics of granular materials known as Discrete Element Method (DEM) by additional properties such as the thermodynamic state, stress/strain for each particle. Such DEM simulations used by industries to set up their experimental processes are complexes and heavy in computation time. At each time step, those simulations generate a list of interacting particles and this phase is one of the most computationally expensive parts of a DEM simulation. The Verlet buffer method, initially introduced in Molecular Dynamic (MD) (and also used in DEM), allows keeping the interaction list for many time steps by extending each particle neighbourhood by a certain extension range, and thus broadening the interaction list. The method relies on the temporal coherency of DEM, which guarantees that no particles move erratically from one time step to the next. In the classical approach, all the particles have their neighbourhood extended by the same value which leads to suboptimal performances in simulations where different flow regimes coexist. Additionally, and unlike in MD, there is no comprehensive study analysing the different parameters that affect the performance of the Verlet buffer method in DEM. In this work, we propose a new method for the dynamic update of the neighbour list that depends on the particles individual displacement and define a particle-specific extension range based on the local flow regime. The interaction list is analysed throughout the simulation based on the particle's displacement allowing a flexible update according to the flow regime conditions. We evaluate the influence of the Verlet extension range on the execution time through different test cases and analyse empirically the extension range value giving the best performance. △ Less","25 August, 2022",https://arxiv.org/pdf/2208.13770
Towards Robust Face Recognition with Comprehensive Search,Manyuan Zhang;Guanglu Song;Yu Liu;Hongsheng Li,"Data cleaning, architecture, and loss function design are important factors contributing to high-performance face recognition. Previously, the research community tries to improve the performance of each single aspect but failed to present a unified solution on the joint search of the optimal designs for all three aspects. In this paper, we for the first time identify that these aspects are tightly coupled to each other. Optimizing the design of each aspect actually greatly limits the performance and biases the algorithmic design. Specifically, we find that the optimal model architecture or loss function is closely coupled with the data cleaning. To eliminate the bias of single-aspect research and provide an overall understanding of the face recognition model design, we first carefully design the search space for each aspect, then a comprehensive search method is introduced to jointly search optimal data cleaning, architecture, and loss function design. In our framework, we make the proposed comprehensive search as flexible as possible, by using an innovative reinforcement learning based approach. Extensive experiments on million-level face recognition benchmarks demonstrate the effectiveness of our newly-designed search space for each aspect and the comprehensive search. We outperform expert algorithms developed for each single research track by large margins. More importantly, we analyze the difference between our searched optimal design and the independent design of the single factors. We point out that strong models tend to optimize with more difficult training datasets and loss functions. Our empirical study can provide guidance in future research towards more robust face recognition systems. △ Less","12 September, 2022",https://arxiv.org/pdf/2208.13600
ProxiTrak: Intelligent Enablement of Social Distancing & Contact Tracing for a Safer Workplace in the New Normal,Vivek Chandel;Snehasis Banerjee;Avik Ghose,This paper describes an innovative solution that enables the enterprises to bring their associates (or employees) back to physical workspaces for critical operations in a safe manner in the wake of current COVID-19 pandemic.,"25 August, 2022",https://arxiv.org/pdf/2208.13550
An Open-Source P416 Compiler Backend for Reconfigurable Match-Action Table Switches,Debobroto Das Robin;Javed I. Khan,"The P4 language has become the dominant choice for programming the reconfigurable match-action table based programmable switches. V1Model architecture is the most widely available realization of this paradigm. The open-source compiler frontend developed by the P4 consortium can execute syntax analysis and derive a hardware-independent representation of a program written using the latest version of P4 (also known as P416 ). A compiler backend is required to map this intermediate representation to the hardware resources of a V1Model switch. However, there is no open-source compiler backend available to check the realizability of a P416 program over a V1Model switch. Proprietary tools provided by different hardware vendors are available for this purpose. However, they are closed source and do not provide access to the internal mapping mechanisms. Which inhibits experimenting with new mapping algorithms and innovative instruction sets for reconfigurable match-action table architecture. Moreover, the proprietary compiler backends are costly and come with various non-disclosure agreements. These factors pose serious challenges to programmable switch-related research. In this work, we present an open-source P416 compiler backend for the V1Model architecture-based programmable switches. It uses heuristic-based mapping algorithms to map a P416 program over the hardware resources of a V1Model switch. It allows developers to rapidly prototype different mapping algorithms. It also gives various resource usage statistics of a P416 program, enabling comparison among multiple P416 schemes. △ Less","26 August, 2022",https://arxiv.org/pdf/2208.12892
Arbitrary Shape Text Detection via Segmentation with Probability Maps,Shi-Xue Zhang;Xiaobin Zhu;Lei Chen;Jie-Bo Hou;Xu-Cheng Yin,"Arbitrary shape text detection is a challenging task due to the significantly varied sizes and aspect ratios, arbitrary orientations or shapes, inaccurate annotations, etc. Due to the scalability of pixel-level prediction, segmentation-based methods can adapt to various shape texts and hence attracted considerable attention recently. However, accurate pixel-level annotations of texts are formidable, and the existing datasets for scene text detection only provide coarse-grained boundary annotations. Consequently, numerous misclassified text pixels or background pixels inside annotations always exist, degrading the performance of segmentation-based text detection methods. Generally speaking, whether a pixel belongs to text or not is highly related to the distance with the adjacent annotation boundary. With this observation, in this paper, we propose an innovative and robust segmentation-based detection method via probability maps for accurately detecting text instances. To be concrete, we adopt a Sigmoid Alpha Function (SAF) to transfer the distances between boundaries and their inside pixels to a probability map. However, one probability map can not cover complex probability distributions well because of the uncertainty of coarse-grained text boundary annotations. Therefore, we adopt a group of probability maps computed by a series of Sigmoid Alpha Functions to describe the possible probability distributions. In addition, we propose an iterative model to learn to predict and assimilate probability maps for providing enough information to reconstruct text instances. Finally, simple region growth algorithms are adopted to aggregate probability maps to complete text instances. Experimental results demonstrate that our method achieves state-of-the-art performance in terms of detection accuracy on several benchmarks. △ Less","25 August, 2022",https://arxiv.org/pdf/2208.12419
Segmentation of Parotid Gland Tumors Using Multimodal MRI and Contrastive Learning,Zi'an Xu;Yin Dai;Fayu Liu;Boyuan Wu;Weibing Chen;Lifu Shi,"Parotid gland tumor is a common type of head and neck tumor. Segmentation of the parotid glands and tumors by MR images is important for the treatment of parotid gland tumors. However, segmentation of the parotid glands is particularly challenging due to their variable shape and low contrast with surrounding structures. Recently deep learning has developed rapidly, which can handle complex problems. However, most of the current deep learning methods for processing medical images are still based on supervised learning. Compared with natural images, medical images are difficult to acquire and costly to label. Contrastive learning, as an unsupervised learning method, can more effectively utilize unlabeled medical images. In this paper, we used a Transformer-based contrastive learning method and innovatively trained the contrastive learning network with transfer learning. Then, the output model was transferred to the downstream parotid segmentation task, which improved the performance of the parotid segmentation model on the test set. The improved DSC was 89.60%, MPA was 99.36%, MIoU was 85.11%, and HD was 2.98. All four metrics showed significant improvement compared to the results of using a supervised learning model as a pre-trained model for the parotid segmentation network. In addition, we found that the improvement of the segmentation network by the contrastive learning model was mainly in the encoder part, so this paper also tried to build a contrastive learning network for the decoder part and discussed the problems encountered in the process of building. △ Less","26 December, 2022",https://arxiv.org/pdf/2208.12413
"Overbook in Advance, Trade in Future: Computing Resource Provisioning in Hybrid Device-Edge-Cloud Networks",Minghui Liwang;Xianbin Wang,"The big data processing in distributed Internet of Things (IoT) systems calls for innovative computing architectures and resource provisioning techniques to support real-time and cost-effective computing services. This article introduces a novel overbooking-promoted forward trading mechanism named Overbook in Advance, Trade in Future (OATF), where computing resources can be traded across three parties, i.e. end-users, an edge server and a remote cloud server, under a hybrid device-edge-cloud network with uncertainties (e.g., ""no shows""). More importantly, OATF encourages a feasible overbooking rate that allows the edge server to overbook resources to multiple end-users (e.g., exceed the resource supply), while purchasing backup resources from the cloud server, by determining rights and obligations associated with forward contracts in advance via analyzing historical statistics (e.g., network, resource dynamics). Such a mechanism can greatly improve time efficiency and resource utilization thanks to overbooking and pre-signed forward contracts. Critical issues such as overbooking rate design and risk management are carefully investigated in this article, while an interesting case study is proposed with mathematical analysis. Comprehensive simulations demonstrate that OATF achieves mutually beneficial utilities for different parties (cloud, edge, and end-users), as well as substantial resource usage and commendable time efficiency, in comparison with conventional trading methods. △ Less","25 August, 2022",https://arxiv.org/pdf/2208.11972
Orthogonal Time Frequency Space (OTFS) Modulation for Wireless Communications,Shuangyang Li,"Orthogonal time frequency space (OTFS) modulation is a recently proposed multi-carrier transmission scheme, which innovatively multiplexes the information symbols in the delay-Doppler (DD) domain instead of the conventional time-frequency (TF) domain. The DD domain symbol multiplexing gives rise to a direct interaction between the DD domain information symbols and DD domain channel responses, which are usually quasi-static, compact, separable, and potentially sparse. Therefore, OTFS modulation enjoys appealing advantages over the conventional orthogonal frequency-division multiplexing (OFDM) modulation for wireless communications. In this thesis, we investigate the related subjects of OTFS modulation for wireless communications, specifically focusing on its signal detection, performance analysis, and applications. These important aspects are discussed based on the review of the state-of-the-art and a detailed derivation of OTFS modulation from the discrete Zak transform (DZT). Finally, a summary of future research directions on OTFS modulation are also provided. △ Less","24 August, 2022",https://arxiv.org/pdf/2208.11807
Financial Index Tracking via Quantum Computing with Cardinality Constraints,Samuel Palmer;Konstantinos Karagiannis;Adam Florence;Asier Rodriguez;Roman Orus;Harish Naik;Samuel Mugel,"In this work, we demonstrate how to apply non-linear cardinality constraints, important for real-world asset management, to quantum portfolio optimization. This enables us to tackle non-convex portfolio optimization problems using quantum annealing that would otherwise be challenging for classical algorithms. Being able to use cardinality constraints for portfolio optimization opens the doors to new applications for creating innovative portfolios and exchange-traded-funds (ETFs). We apply the methodology to the practical problem of enhanced index tracking and are able to construct smaller portfolios that significantly outperform the risk profile of the target index whilst retaining high degrees of tracking. △ Less","24 August, 2022",https://arxiv.org/pdf/2208.11380
Integrative conformal p-values for powerful out-of-distribution testing with labeled outliers,Ziyi Liang;Matteo Sesia;Wenguang Sun,"This paper develops novel conformal methods to test whether a new observation was sampled from the same distribution as a reference set. Blending inductive and transductive conformal inference in an innovative way, the described methods can re-weight standard conformal p-values based on dependent side information from known out-of-distribution data in a principled way, and can automatically take advantage of the most powerful model from any collection of one-class and binary classifiers. The solution can be implemented either through sample splitting or via a novel transductive cross-validation+ scheme which may also be useful in other applications of conformal inference, due to tighter guarantees compared to existing cross-validation approaches. After studying false discovery rate control and power within a multiple testing framework with several possible outliers, the proposed solution is shown to outperform standard conformal p-values through simulations as well as applications to image recognition and tabular data. △ Less","23 August, 2022",https://arxiv.org/pdf/2208.11111
Latent Variable Models in the Era of Industrial Big Data: Extension and Beyond,Xiangyin Kong;Xiaoyu Jiang;Bingxin Zhang;Jinsong Yuan;Zhiqiang Ge,"A rich supply of data and innovative algorithms have made data-driven modeling a popular technique in modern industry. Among various data-driven methods, latent variable models (LVMs) and their counterparts account for a major share and play a vital role in many industrial modeling areas. LVM can be generally divided into statistical learning-based classic LVM and neural networks-based deep LVM (DLVM). We first discuss the definitions, theories and applications of classic LVMs in detail, which serves as both a comprehensive tutorial and a brief application survey on classic LVMs. Then we present a thorough introduction to current mainstream DLVMs with emphasis on their theories and model architectures, soon afterwards provide a detailed survey on industrial applications of DLVMs. The aforementioned two types of LVM have obvious advantages and disadvantages. Specifically, classic LVMs have concise principles and good interpretability, but their model capacity cannot address complicated tasks. Neural networks-based DLVMs have sufficient model capacity to achieve satisfactory performance in complex scenarios, but it comes at sacrifices in model interpretability and efficiency. Aiming at combining the virtues and mitigating the drawbacks of these two types of LVMs, as well as exploring non-neural-network manners to build deep models, we propose a novel concept called lightweight deep LVM (LDLVM). After proposing this new idea, the article first elaborates the motivation and connotation of LDLVM, then provides two novel LDLVMs, along with thorough descriptions on their principles, architectures and merits. Finally, outlooks and opportunities are discussed, including important open questions and possible research directions. △ Less","5 October, 2022",https://arxiv.org/pdf/2208.10847
Evaluating Cardiovascular Surgical Planning in Mobile Augmented Reality,Haoyang Yang;Pratham Darrpan Mehta;Jonathan Leo;Zhiyan Zhou;Megan Dass;Anish Upadhayay;Timothy C. Slesnick;Fawwaz Shaw;Amanda Randles;Duen Horng Chau,"Advanced surgical procedures for congenital heart diseases (CHDs) require precise planning before the surgeries. The conventional approach utilizes 3D-printing and cutting physical heart models, which is a time and resource intensive process. While rapid advances in augmented reality (AR) technologies have the potential to streamline surgical planning, there is limited research that evaluates such AR approaches with medical experts. This paper presents an evaluation with 6 experts, 4 cardiothoracic surgeons, and 2 cardiologists, from Children's Healthcare of Atlanta (CHOA) Heart Center to validate the usability and technical innovations of CardiacAR, a prototype mobile AR surgical planning application. Potential future improvements based on user feedback are also proposed to further improve the design of CardiacAR and broaden its access. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10639
Scalable Hybrid Classification-Regression Solution for High-Frequency Nonintrusive Load Monitoring,Govind Saraswat;Blake Lundstrom;Murti V Salapaka,"Residential buildings with the ability to monitor and control their net-load (sum of load and generation) can provide valuable flexibility to power grid operators. We present a novel multiclass nonintrusive load monitoring (NILM) approach that enables effective net-load monitoring capabilities at high-frequency with minimal additional equipment and cost. The proposed machine learning based solution provides accurate multiclass state predictions while operating at a faster timescale (able to provide a prediction for each 60-Hz ac cycle used in US power grid) without relying on event-detection techniques. We also introduce an innovative hybrid classification-regression method that allows for the prediction of not only load on/off states via classification but also individual load operating power levels via regression. A test bed with eight residential appliances is used for validating the NILM approach. Results show that the overall method has high accuracy and, good scaling and generalization properties. Furthermore, the method is shown to have sufficient response time (within 160ms, corresponding to 10 ac cycles) to support building grid-interactive control at fast timescales relevant to the provision of grid frequency support services. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10638
Simulation-Informed Revenue Extrapolation with Confidence Estimate for Scaleup Companies Using Scarce Time-Series Data,Lele Cao;Sonja Horn;Vilhelm von Ehrenheim;Richard Anselmo Stahl;Henrik Landgren,"Investment professionals rely on extrapolating company revenue into the future (i.e. revenue forecast) to approximate the valuation of scaleups (private companies in a high-growth stage) and inform their investment decision. This task is manual and empirical, leaving the forecast quality heavily dependent on the investment professionals' experiences and insights. Furthermore, financial data on scaleups is typically proprietary, costly and scarce, ruling out the wide adoption of data-driven approaches. To this end, we propose a simulation-informed revenue extrapolation (SiRE) algorithm that generates fine-grained long-term revenue predictions on small datasets and short time-series. SiRE models the revenue dynamics as a linear dynamical system (LDS), which is solved using the EM algorithm. The main innovation lies in how the noisy revenue measurements are obtained during training and inferencing. SiRE works for scaleups that operate in various sectors and provides confidence estimates. The quantitative experiments on two practical tasks show that SiRE significantly surpasses the baseline methods by a large margin. We also observe high performance when SiRE extrapolates long-term predictions from short time-series. The performance-efficiency balance and result explainability of SiRE are also validated empirically. Evaluated from the perspective of investment professionals, SiRE can precisely locate the scaleups that have a great potential return in 2 to 5 years. Furthermore, our qualitative inspection illustrates some advantageous attributes of the SiRE revenue forecasts. △ Less","26 September, 2022",https://arxiv.org/pdf/2208.10375
A Survey of Distributed Ledger Technology for IoT Verticals,Rongxin Xu;Qiujun Lan;Shiva Raj Pokhrel;Gang Li,"The Internet of Things (IoT) and Distributed ledger technology (DLT) have significantly changed our daily lives. Due to their distributed operational environment and naturally decentralized applications, the convergence of these two technologies indicates a more lavish arrangement for the future. This article develops a comprehensive survey to investigate and illustrate state-of-the-art DLT for various IoT use cases, from smart homes to autonomous vehicles and smart cities. We develop a novel framework for conducting a systematic and comprehensive review of DLT over IoT by extending the knowledge graph approach. With relevant insights from this review, we extract innovative and pragmatic techniques to DLT design that enable high-performance, sustainable, and highly scalable IoT systems. Our findings support designing an end-to-end IoT-native DLT architecture for the future that fully coordinates network-assisted functionalities. △ Less","22 August, 2022",https://arxiv.org/pdf/2208.10120
A Survey of Augmented Piano Prototypes: Has Augmentation Improved Learning Experiences?,Jordan Aiko Deja;Sven Mayer;Klen Čopič Pucihar;Matjaž Kljun,"Humans have been developing and playing musical instruments for millennia. With technological advancements, instruments were becoming ever more sophisticated. In recent decades computer-supported innovations have also been introduced in hardware design, usability, and aesthetics. One of the most commonly digitally augmented instruments is the piano. Besides electronic keyboards, several prototypes augmenting pianos with different projections providing various levels of interactivity on and around the keyboard have been implemented in order to support piano players. However, it is still not understood if these solutions are indeed supporting the learning process. In this paper we present a systematic review of augmented piano prototypes focusing on instrument learning, which is based on the four themes derived from interviews of piano experts to better understand the problems of teaching the piano. These themes are: (i) synchronised movement and body posture, (ii) sight-reading, (iii) ensuring motivation, and (iv) encouraging improvisation. We found that prototypes are saturated on the synchronisation themes, and there are opportunities for sight-reading, motivation, and improvisation themes. We conclude by presenting recommendations on augmenting piano systems towards enriching the piano learning experience as well as on possible directions to expand knowledge in the area. △ Less","3 November, 2022",https://arxiv.org/pdf/2208.09929
Intelligent Omni-Surfaces: Simultaneous Refraction and Reflection for Full-dimensional Wireless Communications,Hongliang Zhang;Boya Di,"The development of metasurfaces has unlocked various use cases in wireless communication networks to improve performance by manipulating the propagation environment. Intelligent omni-surface (IOS), an innovative technique in this category, is proposed for coverage extension. In contrast to the widely studied reflective metasurfaces, i.e., intelligent reflecting surfaces (IRSs), which can only serve receivers located on the same side of the transmitter, the IOS can achieve full-dimensional wireless communications by enabling the simultaneous reflection and refraction of the surface, and thus users on both sides can be served. In this paper, we provide a comprehensive overview of the state-of-the-art in IOS from the perspective of wireless communications, with the emphasis on their design principles, channel modeling, beamforming design, experimental implementation and measurements, as well as possible applications in future cellular networks. We first describe the basic concepts of metasurfaces, and introduce the corresponding design principles for different types of metasurfaces. Moreover, we elaborate on the reflective-refractive model for each IOS element and the channel model for IOS-aided wireless communication systems. Furthermore, we show how to achieve full-dimensional wireless communications with the IOS for three different scenarios. In particular, we present the implementation of an IOS-aided wireless communication prototype and report its experimental measurement results. Finally, we outline some potential future directions and challenges in this area. △ Less","20 August, 2022",https://arxiv.org/pdf/2208.09676
Physical Computing for Materials Acceleration Platforms,Erik Peterson;Alexander Lavin,"A ''technology lottery'' describes a research idea or technology succeeding over others because it is suited to the available software and hardware, not necessarily because it is superior to alternative directions--examples abound, from the synergies of deep learning and GPUs to the disconnect of urban design and autonomous vehicles. The nascent field of Self-Driving Laboratories (SDL), particularly those implemented as Materials Acceleration Platforms (MAPs), is at risk of an analogous pitfall: the next logical step for building MAPs is to take existing lab equipment and workflows and mix in some AI and automation. In this whitepaper, we argue that the same simulation and AI tools that will accelerate the search for new materials, as part of the MAPs research program, also make possible the design of fundamentally new computing mediums. We need not be constrained by existing biases in science, mechatronics, and general-purpose computing, but rather we can pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. Here we outline a simulation-based MAP program to design computers that use physics itself to solve optimization problems. Such systems mitigate the hardware-software-substrate-user information losses present in every other class of MAPs and they perfect alignment between computing problems and computing mediums eliminating any technology lottery. We offer concrete steps toward early ''Physical Computing (PC) -MAP'' advances and the longer term cyber-physical R&D which we expect to introduce a new era of innovative collaboration between materials researchers and computer scientists. △ Less","17 August, 2022",https://arxiv.org/pdf/2208.08566
Dismantling Complex Networks by a Neural Model Trained from Tiny Networks,Jiazheng Zhang;Bang Wang,"Can we employ one neural model to efficiently dismantle many complex yet unique networks? This article provides an affirmative answer. Diverse real-world systems can be abstracted as complex networks each consisting of many functional nodes and edges. Percolation theory has indicated that removing only a few vital nodes can cause the collapse of whole network. However, finding the least number of such vital nodes is a rather challenging task for large networks due to its NP-hardness. Previous studies have proposed many centrality measures and heuristic algorithms to tackle this network dismantling (ND) problem. Different from theirs, this article tries to approach the ND task by designing a neural model which can be trained from tiny synthetic networks but will be applied for various real-world networks. It seems a discouraging mission at first sight, as network sizes and topologies are quite different across distinct real-world networks. Nonetheless, this article initiates insightful efforts of designing and training a neural influence ranking model (NIRM). Experiments on fifteen real-world networks validate its effectiveness for its mostly requiring fewer vital nodes to dismantle a network, compared with the state-of-the-art competitors. The key to its success lies in that our NIRM can efficiently encode both local structural and global topological signals for ranking nodes, in addition to our innovative labelling method in training dataset construction. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.07792
"Traditional methods in Edge, Corner and Boundary detection",Sai Pavan Tadem,"This is a review paper of traditional approaches for edge, corner, and boundary detection methods. There are many real-world applications of edge, corner, and boundary detection methods. For instance, in medical image analysis, edge detectors are used to extract the features from the given image. In modern innovations like autonomous vehicles, edge detection and segmentation are the most crucial things. If we want to detect motion or track video, corner detectors help. I tried to compare the results of detectors stage-wise wherever it is possible and also discussed the importance of image prepossessing to minimise the noise. Real-world images are used to validate detector performance and limitations. △ Less","12 August, 2022",https://arxiv.org/pdf/2208.07714
Educating Reflective Systems Developers at Scale: Towards productive feedback in a semi-capstone large-scale software engineering course,Torgeir Dingsøyr,"Feedback is critical in education. This Innovative Practice Full Paper reports lessons learned from improving the quality of feedback in a semi-capstone software engineering course, with particular focus on how to deliver productive feedback in large scale during project work. The bachelor-level introduction to software engineering course is taken by about 500 students from eight study programs, organised into 72 project teams. The course aims to educate reflective systems developers. The teaching staff includes 29 teaching assistants as supervisor and product owners for teams. Project teams get feedback on seven deliverables as part of formative portfolio assessment. Students expressed frustration on feedback not being aligned, that they got critique on topics not stated in assignments and that teaching assistants were reluctant to discuss the feedback. This article provides a description of the course design, an assessment of the quality of feedback and lessons learned from three main changes: Revising assignments and rubrics, reorganising the teaching staff and increasing training of teaching assistants. In discussing the changes, we draw on a survey to students with 142 respondents, a survey to teaching assistants with 18 respondents, meeting minutes from a student reference group and experience reports from teaching assistants as well as literature and own experience. The article concludes with three actionable lessons learned for large-scale semi-capstone courses. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.07640
Inhale: Enabling High-Performance and Energy-Efficient In-SRAM Cryptographic Hash for IoT,Jingyao Zhang;Elaheh Sadredini,"In the age of big data, information security has become a major issue of debate, especially with the rise of the Internet of Things (IoT), where attackers can effortlessly obtain physical access to edge devices. The hash algorithm is the current foundation for data integrity and authentication. However, it is challenging to provide a high-performance, high-throughput, and energy-efficient solution on resource-constrained edge devices. In this paper, we propose Inhale, an in-SRAM architecture to effectively compute hash algorithms with innovative data alignment and efficient read/write strategies to implicitly execute data shift operations through the in-situ controller. We present two variations of Inhale: Inhale-Opt, which is optimized for latency, throughput, and area-overhead; and Inhale-Flex, which offers flexibility in repurposing a part of last-level caches for hash computation. We thoroughly evaluate our proposed architectures on both SRAM and ReRAM memories and compare them with the state-of-the-art in-memory and ASIC accelerators. Our performance evaluation confirms that Inhale can achieve 1.4x - 14.5x higher throughput-per-area and about two-orders-of-magnitude higher throughput-per-area-per-energy compared to the state-of-the-art solutions. △ Less","16 August, 2022",https://arxiv.org/pdf/2208.07570
SYN-MAD 2022: Competition on Face Morphing Attack Detection Based on Privacy-aware Synthetic Training Data,Marco Huber;Fadi Boutros;Anh Thi Luu;Kiran Raja;Raghavendra Ramachandra;Naser Damer;Pedro C. Neto;Tiago Gonçalves;Ana F. Sequeira;Jaime S. Cardoso;João Tremoço;Miguel Lourenço;Sergio Serra;Eduardo Cermeño;Marija Ivanovska;Borut Batagelj;Andrej Kronovšek;Peter Peer;Vitomir Štruc,"This paper presents a summary of the Competition on Face Morphing Attack Detection Based on Privacy-aware Synthetic Training Data (SYN-MAD) held at the 2022 International Joint Conference on Biometrics (IJCB 2022). The competition attracted a total of 12 participating teams, both from academia and industry and present in 11 different countries. In the end, seven valid submissions were submitted by the participating teams and evaluated by the organizers. The competition was held to present and attract solutions that deal with detecting face morphing attacks while protecting people's privacy for ethical and legal reasons. To ensure this, the training data was limited to synthetic data provided by the organizers. The submitted solutions presented innovations that led to outperforming the considered baseline in many experimental settings. The evaluation benchmark is now available at: https://github.com/marcohuber/SYN-MAD-2022. △ Less","15 August, 2022",https://arxiv.org/pdf/2208.07337
Learn2Trust: A video and streamlit-based educational programme for AI-based medical image analysis targeted towards medical students,Hanna Siebert;Marian Himstedt;Mattias Heinrich,"In order to be able to use artificial intelligence (AI) in medicine without scepticism and to recognise and assess its growing potential, a basic understanding of this topic is necessary among current and future medical staff. Under the premise of ""trust through understanding"", we developed an innovative online course as a learning opportunity within the framework of the German KI Campus (AI campus) project, which is a self-guided course that teaches the basics of AI for the analysis of medical image data. The main goal is to provide a learning environment for a sufficient understanding of AI in medical image analysis so that further interest in this topic is stimulated and inhibitions towards its use can be overcome by means of positive application experience. The focus was on medical applications and the fundamentals of machine learning. The online course was divided into consecutive lessons, which include theory in the form of explanatory videos, practical exercises in the form of Streamlit and practical exercises and/or quizzes to check learning progress. A survey among the participating medical students in the first run of the course was used to analyse our research hypotheses quantitatively. △ Less","15 August, 2022",https://arxiv.org/pdf/2208.07314
Flow-Guided Transformer for Video Inpainting,Kaidong Zhang;Jingjing Fu;Dong Liu,"We propose a flow-guided transformer, which innovatively leverage the motion discrepancy exposed by optical flows to instruct the attention retrieval in transformer for high fidelity video inpainting. More specially, we design a novel flow completion network to complete the corrupted flows by exploiting the relevant flow features in a local temporal window. With the completed flows, we propagate the content across video frames, and adopt the flow-guided transformer to synthesize the rest corrupted regions. We decouple transformers along temporal and spatial dimension, so that we can easily integrate the locally relevant completed flows to instruct spatial attention only. Furthermore, we design a flow-reweight module to precisely control the impact of completed flows on each spatial transformer. For the sake of efficiency, we introduce window partition strategy to both spatial and temporal transformers. Especially in spatial transformer, we design a dual perspective spatial MHSA, which integrates the global tokens to the window-based attention. Extensive experiments demonstrate the effectiveness of the proposed method qualitatively and quantitatively. Codes are available at https://github.com/hitachinsk/FGT. △ Less","13 August, 2022",https://arxiv.org/pdf/2208.06768
MAIScope: A low-cost portable microscope with built-in vision AI to automate microscopic diagnosis of diseases in remote rural settings,Rohan Sangameswaran,"According to the World Health Organization(WHO), malaria is estimated to have killed 627,000 people and infected over 241 million people in 2020 alone, a 12% increase from 2019. Microscopic diagnosis of blood cells is the standard testing procedure to diagnose malaria. However, this style of diagnosis is expensive, time-consuming, and greatly subjective to human error, especially in developing nations that lack well-trained personnel to perform high-quality microscopy examinations. This paper proposes Mass-AI-Scope (MAIScope): a novel, low-cost, portable device that can take microscopic images and automatically detect malaria parasites with embedded AI. The device has two subsystems. The first subsystem is an on-device multi-layered deep learning network, that detects red blood cells (RBCs) from microscopic images, followed by a malaria parasite classifier that recognizes malaria parasites in the individual RBCs. The testing and validation demonstrated a high average accuracy of 89.9% for classification and average precision of 61.5% for detection models using TensorFlow Lite while addressing limited storage and computational capacity. This system also has cloud synchronization, which sends images to the cloud when connected to the Internet for analysis and model improvement purposes. The second subsystem is the hardware which consists of components like Raspberry Pi, a camera, a touch screen display, and an innovative low-cost bead microscope. Evaluation of the bead microscope demonstrated similar image quality with that of expensive light microscopes. The device is designed to be portable and work in remote environments without the Internet or power. The solution is extensible to other diseases requiring microscopy and can help standardize automation of disease diagnosis in rural parts of developing nations. △ Less","12 August, 2022",https://arxiv.org/pdf/2208.06114
Secure ambient intelligence prototype for airports,Nayra Rodríguez-Pérez;Josué Toledo-Castro;Pino Caballero-Gil;Iván Santos-González;Candelaria Hernández-Goya,"Nowadays, many technological advances applied to the Internet of Things (IoT) make the introduction of innovative sensors aimed to deploy efficient wireless sensor networks possible. In order to improve the environment and people's lives, real time analysis of certain environmental variables may favor the reduction of health risks related to the deterioration of air quality. To this respect, the proposed system implements a particular prototype of IoT device characterized by the assembly of ambient sensors capable of measuring pollutant gases, temperature and humidity. For this purpose, Raspberry Pi and Arduino platforms are used. Several security methods are introduced to ensure the integrity of air quality data by implementing Merkle Trees on each IoT node and on the Cloud server. Besides, the authenticity of IoT devices and the confidentiality of communications are guaranteed by implementing HTTPS requests. Finally, authentication tokens are used to identify system users, and different security rules are applied to manage database operations. △ Less","11 August, 2022",https://arxiv.org/pdf/2208.05734
Methodological monotheism across fields of science in contemporary quantitative research,Andres F. Castro Torres;Aliakbar Akbaritabar,"The importance of research teams' diversity for the progress of science is highlighted extensively. Despite the seemingly hegemonic role of hypothesis testing in modern quantitative research, little attention has been devoted to the diversity of quantitative methods, epitomized by the linear model framework of analysis. Using bibliometric data from the Web of Science, we conduct a large-scale and cross-disciplinary assessment of the prevalence of linear-model-based research from 1990 to 2022. In absolute terms, linear models are widely used across all fields of science. In relative terms, three patterns suggest linear models are hegemonic among Social Sciences. First, there is a high and growing prevalence of linear-model-based research. Second, global patterns of linear-model-based research prevalence align with global inequalities in knowledge production. Third, there was a citation premium to linear-model-based research until 2012 for articles' number of citations and for the entire period in terms of having at least one citation. Previous research suggests that the confluence of these patterns may be detrimental to the Social Sciences as it potentially marginalizes theories incompatible with the linear models' framework, lowers the diversity of narratives about social phenomena, and prevents innovative and path-breaking research, limiting the breadth of research. △ Less","10 August, 2022",https://arxiv.org/pdf/2208.05373
A Comparison of Spatiotemporal Visualizations for 3D Urban Analytics,Roberta Mota;Nivan Ferreira;Julio Daniel Silva;Marius Horga;Marcos Lage;Luis Ceferino;Usman Alim;Ehud Sharlin;Fabio Miranda,"Recent technological innovations have led to an increase in the availability of 3D urban data, such as shadow, noise, solar potential, and earthquake simulations. These spatiotemporal datasets create opportunities for new visualizations to engage experts from different domains to study the dynamic behavior of urban spaces in this under explored dimension. However, designing 3D spatiotemporal urban visualizations is challenging, as it requires visual strategies to support analysis of time-varying data referent to the city geometry. Although different visual strategies have been used in 3D urban visual analytics, the question of how effective these visual designs are at supporting spatiotemporal analysis on building surfaces remains open. To investigate this, in this paper we first contribute a series of analytical tasks elicited after interviews with practitioners from three urban domains. We also contribute a quantitative user study comparing the effectiveness of four representative visual designs used to visualize 3D spatiotemporal urban data: spatial juxtaposition, temporal juxtaposition, linked view, and embedded view. Participants performed a series of tasks that required them to identify extreme values on building surfaces over time. Tasks varied in granularity for both space and time dimensions. Our results demonstrate that participants were more accurate using plot-based visualizations (linked view, embedded view) but faster using color-coded visualizations (spatial juxtaposition, temporal juxtaposition). Our results also show that, with increasing task complexity, plot-based visualizations perform better in preserving efficiency (time, accuracy) compared to color-coded visualizations. Based on our findings, we present a set of takeaways with design recommendations for 3D spatiotemporal urban visualizations for researchers and practitioners. △ Less","10 August, 2022",https://arxiv.org/pdf/2208.05370
Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code,Qing Huang;Zhiqiang Yuan;Zhenchang Xing;Xiwei Xu;Liming Zhu;Qinghua Lu,"Partial code usually involves non-fully-qualified type names (non-FQNs) and undeclared receiving objects. Resolving the FQNs of these non-FQN types and undeclared receiving objects (referred to as type inference) is the prerequisite to effective search and reuse of partial code. Existing dictionary-lookup based methods build a symbolic knowledge base of API names and code contexts, which involve significant compilation overhead and are sensitive to unseen API names and code context variations. In this paper, we formulate type inference as a cloze-style fill-in-blank language task. Built on source code naturalness, our approach fine-tunes a code masked language model (MLM) as a neural knowledge base of code elements with a novel ""pre-train, prompt and predict"" paradigm from raw source code. Our approach is lightweight and has minimum requirements on code compilation. Unlike existing symbolic name and context matching for type inference, our prompt-tuned code MLM packs FQN syntax and usage in its parameters and supports fuzzy neural type inference. We systematically evaluate our approach on a large amount of source code from GitHub and Stack Overflow. Our results confirm the effectiveness of our approach design and the practicality for partial code type inference. As the first of its kind, our neural type inference method opens the door to many innovative ways of using partial code. △ Less","26 August, 2022",https://arxiv.org/pdf/2208.05361
"Listen to Users, but Only 85% of the Time: How Black Swans Can Save Innovation in a Data-Driven World",Maximilian Speicher,"Data-driven design is a proven success factor that more and more digital businesses embrace. At the same time, academics and practitioners alike warn that when virtually everything must be tested and proven with numbers, that can stifle creativity and innovation. This article argues that Taleb's Black Swan theory can solve this dilemma. It shows that online experimentation, and therefore digital design, are fat-tailed phenomena and, hence, prone to Black Swans. It introduces the notion of Black Swan designs -- ""crazy"" designs that make sense only in hindsight -- along with four specific criteria. To ensure incremental improvements and their potential for innovation, businesses should apply Taleb's barbell strategy: Invest 85-90% of resources into data-driven approaches and 10-15% into potential Black Swans. △ Less","10 August, 2022",https://arxiv.org/pdf/2208.05347
Efficient Joint-Dimensional Search with Solution Space Regularization for Real-Time Semantic Segmentation,Peng Ye;Baopu Li;Tao Chen;Jiayuan Fan;Zhen Mei;Chen Lin;Chongyan Zuo;Qinghua Chi;Wanli Ouyan,"Semantic segmentation is a popular research topic in computer vision, and many efforts have been made on it with impressive results. In this paper, we intend to search an optimal network structure that can run in real-time for this problem. Towards this goal, we jointly search the depth, channel, dilation rate and feature spatial resolution, which results in a search space consisting of about 2.78*10^324 possible choices. To handle such a large search space, we leverage differential architecture search methods. However, the architecture parameters searched using existing differential methods need to be discretized, which causes the discretization gap between the architecture parameters found by the differential methods and their discretized version as the final solution for the architecture search. Hence, we relieve the problem of discretization gap from the innovative perspective of solution space regularization. Specifically, a novel Solution Space Regularization (SSR) loss is first proposed to effectively encourage the supernet to converge to its discrete one. Then, a new Hierarchical and Progressive Solution Space Shrinking method is presented to further achieve high efficiency of searching. In addition, we theoretically show that the optimization of SSR loss is equivalent to the L_0-norm regularization, which accounts for the improved search-evaluation gap. Comprehensive experiments show that the proposed search scheme can efficiently find an optimal network structure that yields an extremely fast speed (175 FPS) of segmentation with a small model size (1 M) while maintaining comparable accuracy. △ Less","10 August, 2022",https://arxiv.org/pdf/2208.05271
Inaccuracy rates for distributed inference over random networks with applications to social learning,Dragana Bajovic,"This paper studies probabilistic rates of convergence for consensus+innovations type of algorithms in random, generic networks. For each node, we find a lower and also a family of upper bounds on the large deviations rate function, thus enabling the computation of the exponential convergence rates for the events of interest on the iterates. Relevant applications include error exponents in distributed hypothesis testing, rates of convergence of beliefs in social learning, and inaccuracy rates in distributed estimation. The bounds on the rate function have a very particular form at each node: they are constructed as the convex envelope between the rate function of the hypothetical fusion center and the rate function corresponding to a certain topological mode of the node's presence. We further show tightness of the discovered bounds for several cases, such as pendant nodes and regular networks, thus establishing the first proof of the large deviations principle for consensus+innovations and social learning in random networks. △ Less","10 August, 2022",https://arxiv.org/pdf/2208.05236
Adaptive Admittance Control for Safety-Critical Physical Human Robot Collaboration,Yuzhu Sun;Mien Van;Stephen McIlvanna;Sean McLoone;Dariusz Ceglarek,"Physical human-robot collaboration requires strict safety guarantees since robots and humans work in a shared workspace. This letter presents a novel control framework to handle safety-critical position-based constraints for human-robot physical interaction. The proposed methodology is based on admittance control, exponential control barrier functions (ECBFs) and quadratic program (QP) to achieve compliance during the force interaction between human and robot, while simultaneously guaranteeing safety constraints. In particular, the formulation of admittance control is rewritten as a second-order nonlinear control system, and the interaction forces between humans and robots are regarded as the control input. A virtual force feedback for admittance control is provided in real-time by using the ECBFs-QP framework as a compensator of the external human forces. A safe trajectory is therefore derived from the proposed adaptive admittance control scheme for a low-level controller to track. The innovation of the proposed approach is that the proposed controller will enable the robot to comply with human forces with natural fluidity without violation of any safety constraints even in cases where human external forces incidentally force the robot to violate constraints. The effectiveness of our approach is demonstrated in simulation studies on a two-link planar robot manipulator. △ Less","9 August, 2022",https://arxiv.org/pdf/2208.05061
Lisbon Hotspots: Wi-Fi access point dataset for time-bound location proofs,Rui Claro;Samih Eisa;Miguel L. Pardal,"Wi-Fi hotspots are a valuable resource for people on the go, especially tourists, as they provide a means to connect personal devices to the Internet. This extra connectivity can be helpful in many situations, e.g., to enable map and chat applications to operate outdoors when cellular connectivity is unavailable or is expensive. Retail stores and many public services have recognized that hotspots have potential to attract and retain customers, so many of them offer free and open Wi-Fi. In busy cities, with many locals and visitors, the number of hotspots is very significant. Some of these hotspots are available for long periods of time, while others are short-lived. When we have many users with devices collecting hotspot observations, they can be used to detect the location -- using the long-lived hotspots -- and to prove the time when the location was visited -- using the short-lived hotspots observed by others users at the location. In this article, we present a dataset of collected Wi-Fi data from the most important tourist locations in the city of Lisbon, Portugal, over a period of months, that was used to show the feasibility of using hotspot data for location detection and proof. The obtained data and algorithms were assessed for a specific use case: smart tourism. We also present the data model used to store the observations and the algorithms developed to detect and prove location of a user device at a specific time. The Lisbon Hotspots dataset, LXspots, is made publicly available to the scientific community so that other researchers can also make use of it to develop new and innovative mobile and Internet of Things applications. △ Less","5 August, 2022",https://arxiv.org/pdf/2208.04741
Reflections on the Evolution of Computer Science Education,Sreekrishnan Venkateswaran,"Computer Science education has been evolving over the years to reflect applied realities. Until about a decade ago, theory of computation, algorithm design and system software dominated the curricula. Most courses were considered core and were hence mandatory; the programme structure did not allow much of a choice or variety. This column analyses why this changed Circa 2010 when elective subjects across scores of topics become part of mainstream education to reflect the on-going lateral acceleration of Computer Science. Fundamental discoveries in artificial intelligence, machine learning, virtualization and cloud computing are several decades old. Many core theories in data science are centuries old. Yet their leverage exploded only after Circa 2010, when the stage got set for people-centric problem solving in massive scale. This was due in part to the rush of innovative real-world applications that reached the common man through the ubiquitous smart phone. AI/ML modules arrived in popular programming languages; they could be used to build and train models on powerful - yet affordable - compute on public clouds reachable through high-speed Internet connectivity. Academia responded by adapting Computer Science curricula to align it with the changing technology landscape. The goal of this experiential piece is to trigger a lively discussion on the past and future of Computer Science education. △ Less","9 July, 2022",https://arxiv.org/pdf/2208.04713
The Transform-o-meter: A method to forecast the transformative impact of innovation,Hector G. T. Torres,"With the advent of Transformative Artificial Intelligence, it is now more important than ever to be able to both measure and forecast the transformative impact/potential of innovation. However, current methods fall short when faced with this task. This paper introduces the Transform-o-meter; a methodology that can be used to achieve the aforementioned goal, and be applied to any innovation, both material and immaterial. While this method can effectively be used for the mentioned purpose, it should be taken as a first approach; to be iterated, researched, and expanded further upon. △ Less","15 July, 2022",https://arxiv.org/pdf/2208.04711
COROID: A Crowdsourcing-based Companion Drones to Tackle Current and Future Pandemics,Ashish Rauniyar;Desta Haileselassie Hagos;Debesh Jha;Jan Erik Håkegård,"Due to the current COVID-19 virus, which has already been declared a pandemic by the World Health Organization (WHO), we are witnessing the greatest pandemic of the decade. Millions of people are being infected, resulting in thousands of deaths every day across the globe. Even it was difficult for the best healthcare-providing countries could not handle the pandemic because of the strain of treating thousands of patients at a time. The count of infections and deaths is increasing at an alarming rate because of the spread of the virus. We believe that innovative technologies could help reduce pandemics to a certain extent until we find a definite solution from the medical field to handle and treat such pandemic situations. Technology innovation has the potential to introduce new technologies that could support people and society during these difficult times. Therefore, this paper proposes the idea of using drones as a companion to tackle current and future pandemics. Our COROID drone is based on the principle of crowdsourcing sensors data of the public's smart devices, which can correlate the reading of the infrared cameras equipped on the COROID drones. To the best of our knowledge, this concept has yet to be investigated either as a concept or as a product. Therefore, we believe that the COROID drone is innovative and has a huge potential to tackle COVID-19 and future pandemics. △ Less","19 July, 2022",https://arxiv.org/pdf/2208.04704
AI in Telemedicine: An Appraisal on Deep Learning-Based Approaches to Virtual Diagnostic Solutions (VDS),Ozioma Collins Oguine;Kanyifeechukwu Jane Oguine,"Advancements in Telemedicine as an approach to healthcare delivery have heralded a new dawn in modern Medicine. Its fast-paced development in our contemporary society is credence to the advances in Artificial Intelligence and Information Technology. This paper carries out a descriptive study to broadly explore AI's implementations in healthcare delivery with a more holistic view of the usability of various Telemedical Innovations in enhancing Virtual Diagnostic Solutions (VDS). This research further explores notable developments in Deep Learning model optimizations for Virtual Diagnostic Solutions. A further research review on the prospects of Virtual Diagnostic Solutions (VDS) and foreseeable challenges was also highlighted. Conclusively, this research gives a general overview of Artificial Intelligence in Telemedicine with a central focus on Deep Learning-based approaches to Virtual Diagnostic Solutions. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.04690
Connected Vehicle Platforms for Dynamic Insurance,Christian Colot;Francois Robinet;Geoffrey Nichils;Raphael Frank,"Following a regulatory change in Europe which mandates that car manufacturers include an eCall system in new vehicles, many car manufacturers are adding additional services on top, so that more and more cars become connected vehicles and act like IoT sensors. In the following study, we analyse the maturity level of this new technology to build insurance products that would take vehicle usage into account. For this, the connectivity of recent cars a-priori eligible has been first tested. Then, an ad-hoc platform has been designed to collect driving data. In particular, 4 cars have been connected to this platform for periods of over one month. Our results highlight that, while this technological innovation appears very promising in the future, the pricing, the lack of uniformity of data collected and the enrollment process are currently three pain points that should be addressed to offer large-scale opportunities. In the meantime, this technology might still be used for high value use cases such as the insurance of luxurious cars. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.04688
Image Quality Assessment with Gradient Siamese Network,Heng Cong;Lingzhi Fu;Rongyu Zhang;Yusheng Zhang;Hao Wang;Jiarong He;Jin Gao,"In this work, we introduce Gradient Siamese Network (GSN) for image quality assessment. The proposed method is skilled in capturing the gradient features between distorted images and reference images in full-reference image quality assessment(IQA) task. We utilize Central Differential Convolution to obtain both semantic features and detail difference hidden in image pair. Furthermore, spatial attention guides the network to concentrate on regions related to image detail. For the low-level, mid-level and high-level features extracted by the network, we innovatively design a multi-level fusion method to improve the efficiency of feature utilization. In addition to the common mean square error supervision, we further consider the relative distance among batch samples and successfully apply KL divergence loss to the image quality assessment task. We experimented the proposed algorithm GSN on several publicly available datasets and proved its superior performance. Our network won the second place in NTIRE 2022 Perceptual Image Quality Assessment Challenge track 1 Full-Reference. △ Less","8 August, 2022",https://arxiv.org/pdf/2208.04081
Data Leaves: Scenario-oriented Metadata for Data Federative Innovation,Yukio Ohsawa;Kaira Sekiguchi;Tomohide Maekawa;Hiroki Yamaguchi;Son Yeon Hyuk;Sae Kondo,"A method for representing the digest information of each dataset is proposed, oriented to the aid of innovative thoughts and the communication of data users who attempt to create valuable products, services, and business models using or combining datasets. Compared with methods for connecting datasets via shared attributes (i.e., variables), this method connects datasets via events, situations, or actions in a scenario that is supposed to be active in the real world. This method reflects the consideration of the fitness of each metadata to the feature concept, which is an abstract of the information or knowledge expected to be acquired from data; thus, the users of the data acquire practical knowledge that fits the requirements of real businesses and real life, as well as grounds for realistic application of AI technologies to data. △ Less","7 August, 2022",https://arxiv.org/pdf/2208.03722
Strategic differences between regional investments into graphene technology and how corporations and universities manage patent portfolios,Ai Linh Nguyen;Wenyuan Liu;Khiam Aik Khor;Andrea Nanetti;Siew Ann Cheong,"Nowadays, patenting activities are essential in converting applied science to technology in the prevailing innovation model. To gain strategic advantages in the technological competitions between regions, nations need to leverage the investments of public and private funds to diversify over all technologies or specialize in a small number of technologies. In this paper, we investigated who the leaders are at the regional and assignee levels, how they attained their leadership positions, and whether they adopted diversification or specialization strategies, using a dataset of 176,193 patent records on graphene between 1986 and 2017 downloaded from Derwent Innovation. By applying a co-clustering method to the IPC subclasses in the patents and using a z-score method to extract keywords from their titles and abstracts, we identified seven graphene technology areas emerging in the sequence synthesis - composites - sensors - devices - catalyst - batteries - water treatment. We then examined the top regions in their investment preferences and their changes in rankings over time and found that they invested in all seven technology areas. In contrast, at the assignee level, some were diversified while others were specialized. We found that large entities diversified their portfolios across multiple technology areas, while small entities specialized around their core competencies. In addition, we found that universities had higher entropy values than corporations on average, leading us to the hypothesis that corporations file, buy, or sell patents to enable product development. In contrast, universities focus only on licensing their patents. We validated this hypothesis through an aggregate analysis of reassignment and licensing and a more detailed analysis of three case studies - SAMSUNG, RICE UNIVERSITY, and DYSON. △ Less","7 August, 2022",https://arxiv.org/pdf/2208.03719
Reconfigurable Intelligent Surface Enabled Over-the-Air Uplink Non-orthogonal Multiple Access,Emre Arslan;Fatih Kilinc;Sultangali Arzykulov;Ali Tugberk Dogukan;Abdulkadir Celik;Ertugrul Basar;Ahmad M. Eltawil,"Innovative reconfigurable intelligent surface (RIS) technologies are rising and recognized as promising candidates to enhance 6G and beyond wireless communication systems. RISs acquire the ability to manipulate electromagnetic signals, thus, offering a degree of control over the wireless channel and the potential for many more benefits. Furthermore, active RIS designs have recently been introduced to combat the critical double fading problem and other impairments passive RIS designs may possess. In this paper, the potential and flexibility of active RIS technology are exploited for uplink systems to achieve virtual non-orthogonal multiple access (NOMA) through power disparity over-the-air rather than controlling transmit powers at the user side. Specifically, users with identical transmit power, path loss, and distance can communicate with a base station sharing time and frequency resources in a NOMA fashion with the aid of the proposed hybrid RIS system. Here, the RIS is partitioned into active and passive parts and the distinctive partitions serve different users aligning their phases accordingly while introducing a power difference to the users' signals to enable NOMA. First, the end-to-end system model is presented considering two users. Furthermore, outage probability calculations and theoretical error probability analysis are discussed and reinforced with computer simulation results. △ Less","6 August, 2022",https://arxiv.org/pdf/2208.03582
Latent Multi-Relation Reasoning for GAN-Prior based Image Super-Resolution,Jiahui Zhang;Fangneng Zhan;Yingchen Yu;Rongliang Wu;Xiaoqin Zhang;Shijian Lu,"Recently, single image super-resolution (SR) under large scaling factors has witnessed impressive progress by introducing pre-trained generative adversarial networks (GANs) as priors. However, most GAN-Priors based SR methods are constrained by an attribute disentanglement problem in inverted latent codes which directly leads to mismatches of visual attributes in the generator layers and further degraded reconstruction. In addition, stochastic noises fed to the generator are employed for unconditional detail generation, which tends to produce unfaithful details that compromise the fidelity of the generated SR image. We design LAREN, a LAtent multi-Relation rEasoNing technique that achieves superb large-factor SR through graph-based multi-relation reasoning in latent space. LAREN consists of two innovative designs. The first is graph-based disentanglement that constructs a superior disentangled latent space via hierarchical multi-relation reasoning. The second is graph-based code generation that produces image-specific codes progressively via recursive relation reasoning which enables prior GANs to generate desirable image details. Extensive experiments show that LAREN achieves superior large-factor image SR and outperforms the state-of-the-art consistently across multiple benchmarks. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02861
OCFR 2022: Competition on Occluded Face Recognition From Synthetically Generated Structure-Aware Occlusions,Pedro C. Neto;Fadi Boutros;Joao Ribeiro Pinto;Naser Damer;Ana F. Sequeira;Jaime S. Cardoso;Messaoud Bengherabi;Abderaouf Bousnat;Sana Boucheta;Nesrine Hebbadj;Mustafa Ekrem Erakın;Uğur Demir;Hazım Kemal Ekenel;Pedro Beber de Queiroz Vidal;David Menotti,"This work summarizes the IJCB Occluded Face Recognition Competition 2022 (IJCB-OCFR-2022) embraced by the 2022 International Joint Conference on Biometrics (IJCB 2022). OCFR-2022 attracted a total of 3 participating teams, from academia. Eventually, six valid submissions were submitted and then evaluated by the organizers. The competition was held to address the challenge of face recognition in the presence of severe face occlusions. The participants were free to use any training data and the testing data was built by the organisers by synthetically occluding parts of the face images using a well-known dataset. The submitted solutions presented innovations and performed very competitively with the considered baseline. A major output of this competition is a challenging, realistic, and diverse, and publicly available occluded face recognition benchmark with well defined evaluation protocols. △ Less","15 August, 2022",https://arxiv.org/pdf/2208.02760
Requirements Analysis and Management for Benefiting Openness,Johan Linåker;Krzysztof Wnuk,"Requirements Engineering has recently been greatly influenced by the way how firms use Open Source Software (OSS) and Software Ecosystems (SECOs) as a part of their product development and business models. This is further emphasized by the paradigm of Open Innovation, which highlights how firms should strive to use both internal and external resources to advance their internal innovation and technology capabilities. The evolution from market-driven requirements engineering and management processes, has reshaped the understanding of what a requirement is, and how it is documented and used. In this work, we suggest a model for analyzing and managing requirements that is designed in the context of OSS and SECOs, including the advances and challenges that it brings. The model clarifies how the main stages of requirements engineering and management processes can be adjusted to benefit from the openness that the new context offers. We believe that the model is a first step towards the inevitable adaptation of requirements engineering to an open and informal arena, where processes and collaboration are decentralized, transparency and governance are the key success factors. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.02629
How Firms Adapt and Interact in Open Source Ecosystems: Analyzing Stakeholder Influence and Collaboration Patterns,Johan Linåker;Patrick Rempel;Björn Regnell;Patrick Mäder,"[Context and motivation] Ecosystems developed as Open Source Software (OSS) are considered to be highly innovative and reactive to new market trends due to their openness and wide-ranging contributor base. Participation in OSS often implies opening up of the software development process and exposure towards new stakeholders. [Question/Problem] Firms considering to engage in such an environment should carefully consider potential opportunities and challenges upfront. The openness may lead to higher innovation potential but also to frictional losses for engaged firms. Further, as an ecosystem progresses, power structures and influence on feature selection may fluctuate accordingly. [Principal ideas/results] We analyze the Apache Hadoop ecosystem in a quantitative longitudinal case study to investigate changing stakeholder influence and collaboration patterns. Further, we investigate how its innovation and time-to-market evolve at the same time. [Contribution] Findings show collaborations between and influence shifting among rivaling and non-competing firms. Network analysis proves valuable on how an awareness of past, present and emerging stakeholders, in regards to power structure and collaborations may be created. Furthermore, the ecosystem's innovation and time-to-market show strong variations among the release history. Indications were also found that these characteristics are influenced by the way how stakeholders collaborate with each other. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.02628
"Report on the software ""SemanticModellingFramework""",Andreas Scalas,"The evolution of 3D visual content calls for innovative methods for modelling shapes based on their intended usage, function and role in a complex scenario. Even if different attempts have been done in this direction, shape modelling still mainly focuses on geometry. However, 3D models have a structure, given by the arrangement of salient parts, and shape and structure are deeply related to semantics and functionality. Changing geometry without semantic clues may invalidate such functionalities or the meaning of objects or their parts. Here, the problem is approached by considering semantics as the formalised knowledge related to a category of objects; the geometry can vary provided that the semantics is preserved. The semantics and the variable geometry of a class of shapes is represented through the parametric template: an annotated 3D model whose geometry can be deformed provided that some semantic constraints remain satisfied. In this work, the design and development of a framework for the semantics-aware modelling of shapes is presented, offering the user a single application environment where the whole workflow of defining the parametric template and applying semantics-aware deformations can take place. In particular, the system provides tools for the selection and annotation of geometry based on a formalised contextual knowledge; shape analysis methods to derive new knowledge implicitly encoded in the geometry, and possibly enrich the given semantics; a set of constraints that the user can apply to salient parts and a deformation operation that takes into account the semantic constraints and provides an optimal solution. The framework is modular so that new tools can be continuously added. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02577
Adaptive Latent Factor Analysis via Generalized Momentum-Incorporated Particle Swarm Optimization,Jiufang Chen;Ye Yuan,"Stochastic gradient descent (SGD) algorithm is an effective learning strategy to build a latent factor analysis (LFA) model on a high-dimensional and incomplete (HDI) matrix. A particle swarm optimization (PSO) algorithm is commonly adopted to make an SGD-based LFA model's hyper-parameters, i.e, learning rate and regularization coefficient, self-adaptation. However, a standard PSO algorithm may suffer from accuracy loss caused by premature convergence. To address this issue, this paper incorporates more historical information into each particle's evolutionary process for avoiding premature convergence following the principle of a generalized-momentum (GM) method, thereby innovatively achieving a novel GM-incorporated PSO (GM-PSO). With it, a GM-PSO-based LFA (GMPL) model is further achieved to implement efficient self-adaptation of hyper-parameters. The experimental results on three HDI matrices demonstrate that the GMPL model achieves a higher prediction accuracy for missing data estimation in industrial applications. △ Less","3 August, 2022",https://arxiv.org/pdf/2208.02423
Internet of Things (IoT) based ECG System for Rural Health Care,Md. Obaidur Rahman;Mohammod Abul Kashem;Al-Akhir Nayan;Most. Fahmida Akter;Fazly Rabbi;Marzia Ahmed;Mohammad Asaduzzaman,"Nearly 30% of the people in the rural areas of Bangladesh are below the poverty level. Moreover, due to the unavailability of modernized healthcare-related technology, nursing and diagnosis facilities are limited for rural people. Therefore, rural people are deprived of proper healthcare. In this perspective, modern technology can be facilitated to mitigate their health problems. ECG sensing tools are interfaced with the human chest, and requisite cardiovascular data is collected through an IoT device. These data are stored in the cloud incorporates with the MQTT and HTTP servers. An innovative IoT-based method for ECG monitoring systems on cardiovascular or heart patients has been suggested in this study. The ECG signal parameters P, Q, R, S, T are collected, pre-processed, and predicted to monitor the cardiovascular conditions for further health management. The machine learning algorithm is used to determine the significance of ECG signal parameters and error rate. The logistic regression model fitted the better agreements between the train and test data. The prediction has been performed to determine the variation of PQRST quality and its suitability in the ECG Monitoring System. Considering the values of quality parameters, satisfactory results are obtained. The proposed IoT-based ECG system reduces the health care cost and complexity of cardiovascular diseases in the future. △ Less","26 July, 2022",https://arxiv.org/pdf/2208.02226
GPPF: A General Perception Pre-training Framework via Sparsely Activated Multi-Task Learning,Benyuan Sun;Jin Dai;Zihao Liang;Congying Liu;Yi Yang;Bo Bai,"Pre-training over mixtured multi-task, multi-domain, and multi-modal data remains an open challenge in vision perception pre-training. In this paper, we propose GPPF, a General Perception Pre-training Framework, that pre-trains a task-level dynamic network, which is composed by knowledge ""legos"" in each layers, on labeled multi-task and multi-domain datasets. By inspecting humans' innate ability to learn in complex environment, we recognize and transfer three critical elements to deep networks: (1) simultaneous exposure to diverse cross-task and cross-domain information in each batch. (2) partitioned knowledge storage in separate lego units driven by knowledge sharing. (3) sparse activation of a subset of lego units for both pre-training and downstream tasks. Noteworthy, the joint training of disparate vision tasks is non-trivial due to their differences in input shapes, loss functions, output formats, data distributions, etc. Therefore, we innovatively develop a plug-and-play multi-task training algorithm, which supports Single Iteration Multiple Tasks (SIMT) concurrently training. SIMT lays the foundation of pre-training with large-scale multi-task multi-domain datasets and is proved essential for stable training in our GPPF experiments. Excitingly, the exhaustive experiments show that, our GPPF-R50 model achieves significant improvements of 2.5-5.8 over a strong baseline of the 8 pre-training tasks in GPPF-15M and harvests a range of SOTAs over the 22 downstream tasks with similar computation budgets. We also validate the generalization ability of GPPF to SOTA vision transformers with consistent improvements. These solid experimental results fully prove the effective knowledge learning, storing, sharing, and transfer provided by our novel GPPF framework. △ Less","4 August, 2022",https://arxiv.org/pdf/2208.02148
A Survey on the Perception of Innovation in a Large Product-Focused Software Organization,Johan Linåker;Husan Munir;Per Runeson;Björn Regnell;Claes Schrewelius,"Context. Innovation is promoted in companies to help them stay competitive. Four types of innovation are defined: product, process, business, and organizational. Objective. We want to understand the perception of the innovation concept in industry, and particularly how the innovation types relate to each other. Method. We launched a survey at a branch of a multi-national corporation. Results. From a qualitative analysis of the 229 responses, we see that the understanding of the innovation concept is somewhat narrow, and mostly related to product innovation. A majority of respondents indicate that product innovation triggers process, business, and organizational innovation, rather than vice versa. However, there is a complex interdependency between the types. We also identify challenges related to each of the types. Conclusion. Increasing awareness and knowledge of different types of innovation, may improve the innovation. Further, they cannot be handled one by one, but in their interdependent relations. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.02002
Evaluating and improving social awareness of energy communities through semantic network analysis of online news,C. Piselli;A. Fronzetti Colladon;L. Segneri;A. L. Pisello,"The implementation of energy communities represents a cross-disciplinary phenomenon that has the potential to support the energy transition while fostering citizens' participation throughout the energy system and their exploitation of renewables. An important role is played by online information sources in engaging people in this process and increasing their awareness of associated benefits. In this view, this work analyses online news data on energy communities to understand people's awareness and the media importance of this topic. We use the Semantic Brand Score (SBS) indicator as an innovative measure of semantic importance, combining social network analysis and text mining methods. Results show different importance trends for energy communities and other energy and society-related topics, also allowing the identification of their connections. Our approach gives evidence to information gaps and possible actions that could be taken to promote a low-carbon energy transition. △ Less","3 August, 2022",https://arxiv.org/pdf/2208.01892
Collaboration in Open Government Data Ecosystems: Open Cross-sector Sharing and Co-development of Data and Software,Johan Linåker;Per Runeson,"Background: Open innovation highlights the potential benefits of external collaboration and knowledge-sharing, often exemplified through Open Source Software (OSS). The public sector has thus far mainly focused on the sharing of Open Government Data (OGD), often with a supply-driven approach with limited feedback-loops. We hypothesize that public sector organizations can extend the open innovation benefits by also creating platforms, where OGD, related OSS, and open standards are collaboratively developed and shared. Objective: The objective of this study is to explore how public sector organizations in the role of platform providers facilitate such collaboration in the form of OGD ecosystems and how the ecosystem's governance may be structured to support the collaboration. Method: We conduct an exploratory multiple-case study of two such ecosystems, focused on OGD related to the Swedish labor market and public transport sector, respectively. Data is gathered through interviews, document studies, and prolonged engagement at one of the platform providers. Results: The study presents governance structure and collaboration practices of the two ecosystems and discusses how these contribute to the platform providers' goals. The case studies highlight the need for platform providers to take an active and multi-functional role in enabling the sharing of data and software from and between the members of the ecosystem. Conclusions: We conclude that OGD ecosystems offer public sector organizations a possibility to catalyze the potential innovation output of OGD, but that it requires investment and adoption of an open and collaborative mindset. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.01746
Requirements engineering in open innovation: a research agenda,Johan Linåker;Björn Regnell;Hussan Munir,"In recent years Open Innovation (OI) has gained much attention and made firms aware that they need to consider the open environment surrounding them. To facilitate this shift Requirements Engineering (RE) needs to be adapted in order to manage the increase and complexity of new requirements sources as well as networks of stakeholders. In response we build on and advance an earlier proposed software engineering framework for fostering OI, focusing on stakeholder management, when to open up, and prioritization and release planning. Literature in open source RE is contrasted against recent findings of OI in software engineering to establish a current view of the area. Based on the synthesized findings we propose a research agenda within the areas under focus, along with a framing-model to help researchers frame and break down their research questions to consider the different angles implied by the OI model. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.01741
Sustaining Open Data as a Digital Common -- Design principles for Common Pool Resources applied to Open Data Ecosystems,Johan Linåker;Per Runeson,"Motivation. Digital commons is an emerging phenomenon and of increasing importance, as we enter a digital society. Open data is one example that makes up a pivotal input and foundation for many of today's digital services and applications. Ensuring sustainable provisioning and maintenance of the data, therefore, becomes even more important. Aim. We aim to investigate how such provisioning and maintenance can be collaboratively performed in the community surrounding a common. Specifically, we look at Open Data Ecosystems (ODEs), a type of community of actors, openly sharing and evolving data on a technological platform. Method. We use Elinor Ostrom's design principles for Common Pool Resources as a lens to systematically analyze the governance of earlier reported cases of ODEs using a theory-oriented software engineering framework. Results. We find that, while natural commons must regulate consumption, digital commons such as open data maintained by an ODE must stimulate both use and data provisioning. Governance needs to enable such stimulus while also ensuring that the collective action can still be coordinated and managed within the frame of available maintenance resources of a community. Subtractability is, in this sense, a concern regarding the resources required to maintain the quality and value of the data, rather than the availability of data. Further, we derive empirically-based recommended practices for ODEs based on the design principles by Ostrom for how to design a governance structure in a way that enables a sustainable and collaborative provisioning and maintenance of the data. Conclusion. ODEs are expected to play a role in data provisioning which democratize the digital society and enables innovation from smaller commercial actors. Our empirically based guidelines intend to support this development. △ Less","9 August, 2022",https://arxiv.org/pdf/2208.01694
Comparative Analysis of State-of-the-Art Deep Learning Models for Detecting COVID-19 Lung Infection from Chest X-Ray Images,Zeba Ghaffar;Pir Masoom Shah;Hikmat Khan;Syed Farhan Alam Zaidi;Abdullah Gani;Izaz Ahmad Khan;Munam Ali Shah;Saif ul Islam,"The ongoing COVID-19 pandemic has already taken millions of lives and damaged economies across the globe. Most COVID-19 deaths and economic losses are reported from densely crowded cities. It is comprehensible that the effective control and prevention of epidemic/pandemic infectious diseases is vital. According to WHO, testing and diagnosis is the best strategy to control pandemics. Scientists worldwide are attempting to develop various innovative and cost-efficient methods to speed up the testing process. This paper comprehensively evaluates the applicability of the recent top ten state-of-the-art Deep Convolutional Neural Networks (CNNs) for automatically detecting COVID-19 infection using chest X-ray images. Moreover, it provides a comparative analysis of these models in terms of accuracy. This study identifies the effective methodologies to control and prevent infectious respiratory diseases. Our trained models have demonstrated outstanding results in classifying the COVID-19 infected chest x-rays. In particular, our trained models MobileNet, EfficentNet, and InceptionV3 achieved a classification average accuracy of 95\%, 95\%, and 94\% test set for COVID-19 class classification, respectively. Thus, it can be beneficial for clinical practitioners and radiologists to speed up the testing, detection, and follow-up of COVID-19 cases. △ Less","30 June, 2022",https://arxiv.org/pdf/2208.01637
Using Software Product Lines to Create Blockchain Products: Application to Supply Chain Traceability,Nicolas Six;Nicolas Herbaut;Roberto Erick Lopez-Herrejon;Camille Salinesi,"In recent years, blockchain has been growing rapidly from a niche technology to a promising solution for many sectors, due to its unique properties that empower the design of innovative applications. Nevertheless, the development of blockchain applications is still a challenge. Due to the technological novelty, only a few developers are familiar with blockchain technologies and smart contracts. Others might face a steep learning curve or difficulties to reuse existing code to build blockchain applications. This study proposes a novel approach to tackle these issues, through software product line engineering. To support the approach, a web platform to configure and generate a blockchain application for on-chain traceability is introduced. First, a feature model has been designed to model core features of the chosen domain, based on the existing literature. Then, a configurator has been implemented to support the feature selection phase. Finally, a generator is able to ingest such configurations to generate on-the-shelf blockchain products. The generalizability of the contribution is validated by reproducing on-chain traceability applications proposed in the literature by using the platform. This work provides the first evidence that the implementation of blockchain applications using software product lines enhances the quality of produced applications and reduces the time to market. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01497
Distributed Computations with Layered Resolution,Homa Esfahanizadeh;Alejandro Cohen;Muriel Médard;Shlomo Shamai,"Modern computationally-heavy applications are often time-sensitive, demanding distributed strategies to accelerate them. On the other hand, distributed computing suffers from the bottleneck of slow workers in practice. Distributed coded computing is an attractive solution that adds redundancy such that a subset of distributed computations suffices to obtain the final result. However, the final result is still either obtained within a desired time or not, and for the latter, the resources that are spent are wasted. In this paper, we introduce the novel concept of layered-resolution distributed coded computations such that lower resolutions of the final result are obtained from collective results of the workers -- at an earlier stage than the final result. This innovation makes it possible to have more effective deadline-based systems, since even if a computational job is terminated because of timing, an approximated version of the final result can be released. Based on our theoretical and empirical results, the average execution delay for the first resolution is notably smaller than the one for the final resolution. Moreover, the probability of meeting a deadline is one for the first resolution in a setting where the final resolution exceeds the deadline almost all the time, reducing the success rate of the systems with no layering. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01437
Open innovation using open source tools: a case study at Sony Mobile,Hussan Munir;Johan Linåker;Krzysztof Wnuk;Per Runeson;Björn Regnell,"Despite growing interest of Open Innovation (OI) in Software Engineering (SE), little is known about what triggers software organizations to adopt it and how this affects SE practices. OI can be realized in numerous of ways, including Open Source Software (OSS) involvement. Outcomes from OI are not restricted to product innovation but also include process innovation, e.g. improved SE practices and methods. This study explores the involvement of a software organization (Sony Mobile) in OSS communities from an OI perspective and what SE practices (requirements engineering and testing) have been adapted in relation to OI. It also highlights the innovative outcomes resulting from OI. An exploratory embedded case study investigates how Sony Mobile use and contribute to Jenkins and Gerrit; the two central OSS tools in their continuous integration tool chain. Quantitative analysis was performed on change log data from source code repositories in order to identify the top contributors and triangulated with the results from five semi-structured interviews to explore the nature of the commits. The findings of the case study include five major themes: i) The process of opening up towards the tool communities correlates in time with a general adoption of OSS in the organization. ii) Assets not seen as competitive advantage nor a source of revenue are made open to OSS communities, and gradually, the organization turns more open. iii) The requirements engineering process towards the community is informal and based on engagement. iv) The need for systematic and automated testing is still in its infancy, but the needs are identified. v) The innovation outcomes included free features and maintenance, and were believed to increase speed and quality in development. Adopting OI was a result of a paradigm shift of moving from Windows to Linux. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.01406
Parameterizing Kterm Hashing,Dominik Wurzer;Yumeng Qin,"Kterm Hashing provides an innovative approach to novelty detection on massive data streams. Previous research focused on maximizing the efficiency of Kterm Hashing and succeeded in scaling First Story Detection to Twitter-size data stream without sacrificing detection accuracy. In this paper, we focus on improving the effectiveness of Kterm Hashing. Traditionally, all kterms are considered as equally important when calculating a document's degree of novelty with respect to the past. We believe that certain kterms are more important than others and hypothesize that uniform kterm weights are sub-optimal for determining novelty in data streams. To validate our hypothesis, we parameterize Kterm Hashing by assigning weights to kterms based on their characteristics. Our experiments apply Kterm Hashing in a First Story Detection setting and reveal that parameterized Kterm Hashing can surpass state-of-the-art detection accuracy and significantly outperform the uniformly weighted approach. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01340
Automatic Classification of Bug Reports Based on Multiple Text Information and Reports' Intention,Fanqi Meng;Xuesong Wang;Jingdong Wang;Peifang Wang,"With the rapid growth of software scale and complexity, a large number of bug reports are submitted to the bug tracking system. In order to speed up defect repair, these reports need to be accurately classified so that they can be sent to the appropriate developers. However, the existing classification methods only use the text information of the bug report, which leads to their low performance. To solve the above problems, this paper proposes a new automatic classification method for bug reports. The innovation is that when categorizing bug reports, in addition to using the text information of the report, the intention of the report (i.e. suggestion or explanation) is also considered, thereby improving the performance of the classification. First, we collect bug reports from four ecosystems (Apache, Eclipse, Gentoo, Mozilla) and manually annotate them to construct an experimental data set. Then, we use Natural Language Processing technology to preprocess the data. On this basis, BERT and TF-IDF are used to extract the features of the intention and the multiple text information. Finally, the features are used to train the classifiers. The experimental result on five classifiers (including K-Nearest Neighbor, Naive Bayes, Logistic Regression, Support Vector Machine, and Random Forest) show that our proposed method achieves better performance and its F-Measure achieves from 87.3% to 95.5%. △ Less","2 August, 2022",https://arxiv.org/pdf/2208.01274
A Multifaceted Benchmarking of Synthetic Electronic Health Record Generation Models,Chao Yan;Yao Yan;Zhiyu Wan;Ziqi Zhang;Larsson Omberg;Justin Guinney;Sean D. Mooney;Bradley A. Malin,"Synthetic health data have the potential to mitigate privacy concerns when sharing data to support biomedical research and the development of innovative healthcare applications. Modern approaches for data generation based on machine learning, generative adversarial networks (GAN) methods in particular, continue to evolve and demonstrate remarkable potential. Yet there is a lack of a systematic assessment framework to benchmark methods as they emerge and determine which methods are most appropriate for which use cases. In this work, we introduce a generalizable benchmarking framework to appraise key characteristics of synthetic health data with respect to utility and privacy metrics. We apply the framework to evaluate synthetic data generation methods for electronic health records (EHRs) data from two large academic medical centers with respect to several use cases. The results illustrate that there is a utility-privacy tradeoff for sharing synthetic EHR data. The results further indicate that no method is unequivocally the best on all criteria in each use case, which makes it evident why synthetic data generation methods need to be assessed in context. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.01230
"Toward 6G TKμ
Extreme Connectivity: Architecture, Key Technologies and Experiments",Xiaohu You;Yongming Huang;Shengheng Liu;Dongming Wang;Junchao Ma;Chuan Zhang;Hang Zhan;Cheng Zhang;Jiao Zhang;Jin Li;Min Zhu;Jianjie You;Dongjie Liu;Shiwen He;Guanghui He;Fengyi Yang;Yang Liu;Jianjun Wu;Jianmin Lu;Ge Li;Xiaowu Chen;Wenguang Chen;Wen Gao,"Sixth-generation (6G) networks are evolving towards new features and order-of-magnitude enhancement of systematic performance metrics compared to the current 5G. In particular, the 6G networks are expected to achieve extreme connectivity performance with Tbps-scale data rate, Kbps/Hz-scale spectral efficiency, and μs-scale latency. To this end, an original three-layer 6G network architecture is designed to realise uniform full-spectrum cell-free radio access and provide task-centric agile proximate support for diverse applications. The designed architecture is featured by super edge node (SEN) which integrates connectivity, computing, AI, data, etc. On this basis, a technological framework of pervasive multi-level (PML) AI is established in the centralised unit to enable task-centric near-real-time resource allocation and network automation. We then introduce a radio access network (RAN) architecture of full spectrum uniform cell-free networks, which is among the most attractive RAN candidates for 6G TKμ extreme connectivity. A few most promising key technologies, i.e., cell-free massive MIMO, photonics-assisted Terahertz wireless access and spatiotemporal two-dimensional channel coding are further discussed. A testbed is implemented and extensive trials are conducted to evaluate innovative technologies and methodologies. The proposed 6G network architecture and technological framework demonstrate exciting potentials for full-service and full-scenario applications. △ Less","11 October, 2022",https://arxiv.org/pdf/2208.01190
Subgraph Neighboring Relations Infomax for Inductive Link Prediction on Knowledge Graphs,Xiaohan Xu;Peng Zhang;Yongquan He;Chengpeng Chao;Chaoyang Yan,"Inductive link prediction for knowledge graph aims at predicting missing links between unseen entities, those not shown in training stage. Most previous works learn entity-specific embeddings of entities, which cannot handle unseen entities. Recent several methods utilize enclosing subgraph to obtain inductive ability. However, all these works only consider the enclosing part of subgraph without complete neighboring relations, which leads to the issue that partial neighboring relations are neglected, and sparse subgraphs are hard to be handled. To address that, we propose Subgraph Neighboring Relations Infomax, SNRI, which sufficiently exploits complete neighboring relations from two aspects: neighboring relational feature for node feature and neighboring relational path for sparse subgraph. To further model neighboring relations in a global way, we innovatively apply mutual information (MI) maximization for knowledge graph. Experiments show that SNRI outperforms existing state-of-art methods by a large margin on inductive link prediction task, and verify the effectiveness of exploring complete neighboring relations in a global way to characterize node features and reason on sparse subgraphs. △ Less","26 August, 2022",https://arxiv.org/pdf/2208.00850
Design Guidelines for Apache Kafka Driven Data Management and Distribution in Smart Cities,Theofanis P. Raptis;Claudio Cicconetti;Manolis Falelakis;Tassos Kanellos;Tomás Pariente Lobo,"Smart city management is going through a remarkable transition, in terms of quality and diversity of services provided to the end-users. The stakeholders that deliver pervasive applications are now able to address fundamental challenges in the big data value chain, from data acquisition, data analysis and processing, data storage and curation, and data visualisation in real scenarios. Industry 4.0 is pushing this trend forward, demanding for servitization of products and data, also for the smart cities sector where humans, sensors and devices are operating in strict collaboration. The data produced by the ubiquitous devices must be processed quickly to allow the implementation of reactive services such as situational awareness, video surveillance and geo-localization, while always ensuring the safety and privacy of involved citizens. This paper proposes a modular architecture to (i) leverage innovative technologies for data acquisition, management and distribution (such as Apache Kafka and Apache NiFi), (ii) develop a multi-layer engineering solution for revealing valuable and hidden societal knowledge in smart cities environment, and (iii) tackle the main issues in tasks involving complex data flows and provide general guidelines to solve them. We derived some guidelines from an experimental setting performed together with leading industrial technical departments to accomplish an efficient system for monitoring and servitization of smart city assets, with a scalable platform that confirms its usefulness in numerous smart city use cases with different needs. △ Less","1 August, 2022",https://arxiv.org/pdf/2208.00786
Public Sector Platforms going Open: Creating and Growing an Ecosystem with Open Collaborative Development,Johan Linåker;Per Runeson,"Background: By creating ecosystems around platforms of Open Source Software (OSS) and Open Data (OD), and adopting open collaborative development practices, platform providers may exploit open innovation benefits. However, adopting such practices in a traditionally closed organization is a maturity process that we hypothesize cannot be undergone without friction. Objective: This study aims to investigate what challenges may occur for a newly-turned platform provider in the public sector, aiming to adopt open collaborative practices to create an ecosystem around the development of the underpinning platform. Method: An exploratory case-study is conducted at a Swedish public sector platform provider, which is creating an ecosystem around OSS and OD, related to the labor market. Data is collected through interviews, document studies, and prolonged engagement. Results: Findings highlight a fear among developers of being publicly questioned for their work, as they represent a government agency undergoing constant scrutiny. Issue trackers, roadmaps, and development processes are generally closed, while multiple channels are used for communication, causing internal and external confusion. Some developers are reluctant to communicate externally as they believe it interferes with their work. Lack of health metrics limits possibilities to follow ecosystem growth and for actors to make investment decisions. Further, an autonomous team structure is reported to complicate internal communication and enforcement of the common vision, as well as collaboration. A set of interventions for addressing the challenges are proposed, based on related work. Conclusions: We conclude that several cultural, organizational, and process-related challenges may reside, and by understanding these early on, platform providers can be preemptive in their work of building healthy ecosystems. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.00510
The impact of Twitter on political influence on the choice of a running mate: Social Network Analysis and Semantic Analysis -- A Review,Immaculate Wanza;Irad Kamuti;David Gichohi;Kinyua Gikunda,"In this new era of social media, social networks are becoming increasingly important sources of user-generated content on the internet. These kinds of information resources, which include a lot of people's feelings, opinions, feedback, and reviews, are very useful for big businesses, markets, politics, journalism, and many other fields. Politics is one of the most talked-about and popular topics on social media networks right now. Many politicians use micro-blogging services like Twitter because they have a large number of followers and supporters on those networks. Politicians, political parties, political organizations, and foundations use social media networks to communicate with citizens ahead of time. Today, social media is used by hundreds of thousands of political groups and politicians. On these social media networks, every politician and political party has millions of followers, and politicians find new and innovative ways to urge individuals to participate in politics. Furthermore, social media assists politicians in various decision-making processes by providing recommendations, such as developing policies and strategies based on previous experiences, recommending and selecting suitable candidates for a particular constituency, recommending a suitable person for a particular position in the party, and launching a political campaign based on citizen sentiments on various issues and controversies, among other things. This research is a review on the use of social network analysis (SNA) and semantic analysis (SA) on the Twitter platform to study the supporters networks of political leaders because it can help in decision-making when predicting their political futures. △ Less","31 July, 2022",https://arxiv.org/pdf/2208.00479
enpheeph: A Fault Injection Framework for Spiking and Compressed Deep Neural Networks,Alessio Colucci;Andreas Steininger;Muhammad Shafique,"Research on Deep Neural Networks (DNNs) has focused on improving performance and accuracy for real-world deployments, leading to new models, such as Spiking Neural Networks (SNNs), and optimization techniques, e.g., quantization and pruning for compressed networks. However, the deployment of these innovative models and optimization techniques introduces possible reliability issues, which is a pillar for DNNs to be widely used in safety-critical applications, e.g., autonomous driving. Moreover, scaling technology nodes have the associated risk of multiple faults happening at the same time, a possibility not addressed in state-of-the-art resiliency analyses. Towards better reliability analysis for DNNs, we present enpheeph, a Fault Injection Framework for Spiking and Compressed DNNs. The enpheeph framework enables optimized execution on specialized hardware devices, e.g., GPUs, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools. To evaluate our enpheeph framework, we analyze the resiliency of different DNN and SNN models, with different compression techniques. By injecting a random and increasing number of faults, we show that DNNs can show a reduction in accuracy with a fault rate as low as 7 x 10 ^ (-7) faults per parameter, with an accuracy drop higher than 40%. Run-time overhead when executing enpheeph is less than 20% of the baseline execution time when executing 100 000 faults concurrently, at least 10x lower than state-of-the-art frameworks, making enpheeph future-proof for complex fault injection scenarios. We release enpheeph at https://github.com/Alexei95/enpheeph. △ Less","30 July, 2022",https://arxiv.org/pdf/2208.00328
How to Enable Collaboration in Open Government Data Ecosystems: A Public Platform Provider's Perspective,Johan Linåker;Per Runeson,"Objective: Our objective is to explore how public entities in the role of platform providers can address this issue by enabling collaboration within their OGD ecosystems, both in terms of the OGD published on the underpinning platform, as well as any related Open Source Software (OSS) and standards. Method: We conducted an exploratory multiple-case study of four OGD ecosystems with diverse characteristics. Data was collected through semi-structured interviews, and in one of the cases through a prolonged engagement. The data was then coded using a set of \textit{apriori} codes. Results: The study descriptively presents each case based on the coding, along with synthesis in the form of a conceptual model that highlights different attributes of OGD ecosystems. For example, we observe how collaboration can be enabled through different types of ownership of the platform provider, how the ecosystem's scope can vary, what roles the platform provider may undertake, how to enable open collaboration, and how to collaborate in terms of data sharing, OSS development, and standards. For each aspect, we provide recommendations based on the explored cases that, together with the model, may help public entities in designing and orchestrating new or existing OGD ecosystems. Conclusions: We conclude that enabling and facilitating collaboration in an OGD ecosystem is a complex exercise, yet believe that it offers new ways for public entities in how they can leverage the power of open innovation to address their goals and directives. △ Less","30 July, 2022",https://arxiv.org/pdf/2208.00305
Efficient Compilation and Mapping of Fixed Function Combinational Logic onto Digital Signal Processors Targeting Neural Network Inference and Utilizing High-level Synthesis,Soheil Nazar Shahsavani;Arash Fayyazi;Mahdi Nazemi;Massoud Pedram,"Recent efforts for improving the performance of neural network (NN) accelerators that meet today's application requirements have given rise to a new trend of logic-based NN inference relying on fixed function combinational logic. Mapping such large Boolean functions with many input variables and product terms to digital signal processors (DSPs) on Field-programmable gate arrays (FPGAs) needs a novel framework considering the structure and the reconfigurability of DSP blocks during this process. The proposed methodology in this paper maps the fixed function combinational logic blocks to a set of Boolean functions where Boolean operations corresponding to each function are mapped to DSP devices rather than look-up tables (LUTs) on the FPGAs to take advantage of the high performance, low latency, and parallelism of DSP blocks. % This paper also presents an innovative design and optimization methodology for compilation and mapping of NNs, utilizing fixed function combinational logic to DSPs on FPGAs employing high-level synthesis flow. % Our experimental evaluations across several \REVone{datasets} and selected NNs demonstrate the comparable performance of our framework in terms of the inference latency and output accuracy compared to prior art FPGA-based NN accelerators employing DSPs. △ Less","30 July, 2022",https://arxiv.org/pdf/2208.00302
Unfolding Values through Systematic Guidance: Conducting a Value-Centered Participatory Workshop for a Patient-Oriented Data Donation,David Leimstädtner;Peter Sörries;Claudia Müller-Birn,"Routinely collected clinical patient data posits a valuable resource for data-driven medical innovation. Such secondary data use for medical research purposes is dependent on the patient's consent. To gain an understanding of the patients' values and needs regarding medical data donations, we developed a participatory workshop method, integrating approaches from value-sensitive and reflective design to explore patients' values and translate them into hypothetical, ideal design solutions. The data gathered in the workshop are used to derive practicable design requirements for patient-oriented data donation technologies. In this paper, we introduce the workshop process and evaluate its application. △ Less","29 July, 2022",https://arxiv.org/pdf/2207.14681
Enhanced Laser-Scan Matching with Online Error Estimation for Highway and Tunnel Driving,Matthew McDermott;Jason Rife,"Lidar data can be used to generate point clouds for the navigation of autonomous vehicles or mobile robotics platforms. Scan matching, the process of estimating the rigid transformation that best aligns two point clouds, is the basis for lidar odometry, a form of dead reckoning. Lidar odometry is particularly useful when absolute sensors, like GPS, are not available. Here we propose the Iterative Closest Ellipsoidal Transform (ICET), a scan matching algorithm which provides two novel improvements over the current state-of-the-art Normal Distributions Transform (NDT). Like NDT, ICET decomposes lidar data into voxels and fits a Gaussian distribution to the points within each voxel. The first innovation of ICET reduces geometric ambiguity along large flat surfaces by suppressing the solution along those directions. The second innovation of ICET is to infer the output error covariance associated with the position and orientation transformation between successive point clouds; the error covariance is particularly useful when ICET is incorporated into a state-estimation routine such as an extended Kalman filter. We constructed a simulation to compare the performance of ICET and NDT in 2D space both with and without geometric ambiguity and found that ICET produces superior estimates while accurately predicting solution accuracy. △ Less","29 July, 2022",https://arxiv.org/pdf/2207.14674
A Civil Protection Early Warning System to Improve the Resilience of Adriatic-Ionian Territories to Natural and Man-made Risk,Agorakis Bompotas;Christos Anagnostopoulos;Athanasios Kalogeras;Georgios Kalogeras;Georgios Mylonas;Kyriakos Stefanidis;Christos Alexakos;Miranda Dandoulaki,"We are currently witnessing an increased occurrence of extreme weather events, causing a great deal of disruption and distress across the globe. In this setting, the importance and utility of Early Warning Systems is becoming increasingly obvious. In this work, we present the design of an early warning system called TransCPEarlyWarning, aimed at seven countries in the Adriatic-Ionian area in Europe. The overall objective is to increase the level of cooperation among national civil protection institutions in these countries, addressing natural and man-made risks from the early warning stage and improving the intervention capabilities of civil protection mechanisms. The system utilizes an innovative approach with a lever effect, while also aiming to support the whole system of Civil Protection. △ Less","28 July, 2022",https://arxiv.org/pdf/2207.13941
PASTA-GAN++: A Versatile Framework for High-Resolution Unpaired Virtual Try-on,Zhenyu Xie;Zaiyu Huang;Fuwei Zhao;Haoye Dong;Michael Kampffmeyer;Xin Dong;Feida Zhu;Xiaodan Liang,"Image-based virtual try-on is one of the most promising applications of human-centric image generation due to its tremendous real-world potential. In this work, we take a step forwards to explore versatile virtual try-on solutions, which we argue should possess three main properties, namely, they should support unsupervised training, arbitrary garment categories, and controllable garment editing. To this end, we propose a characteristic-preserving end-to-end network, the PAtch-routed SpaTially-Adaptive GAN++ (PASTA-GAN++), to achieve a versatile system for high-resolution unpaired virtual try-on. Specifically, our PASTA-GAN++ consists of an innovative patch-routed disentanglement module to decouple the intact garment into normalized patches, which is capable of retaining garment style information while eliminating the garment spatial information, thus alleviating the overfitting issue during unsupervised training. Furthermore, PASTA-GAN++ introduces a patch-based garment representation and a patch-guided parsing synthesis block, allowing it to handle arbitrary garment categories and support local garment editing. Finally, to obtain try-on results with realistic texture details, PASTA-GAN++ incorporates a novel spatially-adaptive residual module to inject the coarse warped garment feature into the generator. Extensive experiments on our newly collected UnPaired virtual Try-on (UPT) dataset demonstrate the superiority of PASTA-GAN++ over existing SOTAs and its ability for controllable garment editing. △ Less","27 July, 2022",https://arxiv.org/pdf/2207.13475
Implementation Of Tiny Machine Learning Models On Arduino 33 BLE For Gesture And Speech Recognition,Viswanatha V;Ramachandra A. C;Raghavendra Prasanna;Prem Chowdary Kakarla;Viveka Simha PJ;Nishant Mohan,"In this article gesture recognition and speech recognition applications are implemented on embedded systems with Tiny Machine Learning (TinyML). It features 3-axis accelerometer, 3-axis gyroscope and 3-axis magnetometer. The gesture recognition,provides an innovative approach nonverbal communication. It has wide applications in human-computer interaction and sign language. Here in the implementation of hand gesture recognition, TinyML model is trained and deployed from EdgeImpulse framework for hand gesture recognition and based on the hand movements, Arduino Nano 33 BLE device having 6-axis IMU can find out the direction of movement of hand. The Speech is a mode of communication. Speech recognition is a way by which the statements or commands of human speech is understood by the computer which reacts accordingly. The main aim of speech recognition is to achieve communication between man and machine. Here in the implementation of speech recognition, TinyML model is trained and deployed from EdgeImpulse framework for speech recognition and based on the keywords pronounced by human, Arduino Nano 33 BLE device having built-in microphone can make an RGB LED glow like red, green or blue based on keyword pronounced. The results of each application are obtained and listed in the results section and given the analysis upon the results. △ Less","23 July, 2022",https://arxiv.org/pdf/2207.12866
Variational multiscale reinforcement learning for discovering reduced order closure models of nonlinear spatiotemporal transport systems,Omer San;Suraj Pawar;Adil Rasheed,"A central challenge in the computational modeling and simulation of a multitude of science applications is to achieve robust and accurate closures for their coarse-grained representations due to underlying highly nonlinear multiscale interactions. These closure models are common in many nonlinear spatiotemporal systems to account for losses due to reduced order representations, including many transport phenomena in fluids. Previous data-driven closure modeling efforts have mostly focused on supervised learning approaches using high fidelity simulation data. On the other hand, reinforcement learning (RL) is a powerful yet relatively uncharted method in spatiotemporally extended systems. In this study, we put forth a modular dynamic closure modeling and discovery framework to stabilize the Galerkin projection based reduced order models that may arise in many nonlinear spatiotemporal dynamical systems with quadratic nonlinearity. However, a key element in creating a robust RL agent is to introduce a feasible reward function, which can be constituted of any difference metrics between the RL model and high fidelity simulation data. First, we introduce a multi-modal RL (MMRL) to discover mode-dependant closure policies that utilize the high fidelity data in rewarding our RL agent. We then formulate a variational multiscale RL (VMRL) approach to discover closure models without requiring access to the high fidelity data in designing the reward function. Specifically, our chief innovation is to leverage variational multiscale formalism to quantify the difference between modal interactions in Galerkin systems. Our results in simulating the viscous Burgers equation indicate that the proposed VMRL method leads to robust and accurate closure parameterizations, and it may potentially be used to discover scale-aware closure models for complex dynamical systems. △ Less","7 July, 2022",https://arxiv.org/pdf/2207.12854
Peduncle Gripping and Cutting Force for Strawberry Harvesting Robotic End-effector Design,Vishnu Rajendran S;Soran Parsa;Simon Parsons;Amir Ghalamzan Esfahani,"Robotic harvesting of strawberries has gained much interest in the recent past. Although there are many innovations, they haven't yet reached a level that is comparable to an expert human picker. The end effector unit plays a major role in defining the efficiency of such a robotic harvesting system. Even though there are reports on various end effectors for strawberry harvesting, but there they lack a picture of certain parameters that the researchers can rely upon to develop new end effectors. These parameters include the limit of gripping force that can be applied on the peduncle for effective gripping, the force required to cut the strawberry peduncle, etc. These estimations would be helpful in the design cycle of the end effectors that target to grip and cut the strawberry peduncle during the harvesting action. This paper studies the estimation and analysis of these parameters experimentally. It has been estimated that the peduncle gripping force can be limited to 10 N. This enables an end effector to grip a strawberry of mass up to 50 grams with a manipulation acceleration of 50 m/s^2 without squeezing the peduncle. The study on peduncle cutting force reveals that a force of 15 N is sufficient to cut a strawberry peduncle using a blade with a wedge angle of 16.6 degrees at a 30-degree orientation. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.12552
"New technological trajectories and research directions in Cloud Computing Technology, 2004-2021",Mario Coccia;Saeed Roshani,"The goal of this study is to explore emerging trends in cloud computing technology that can support an economic and social change. We apply the methods of entity linking, which links word strings to entities from a knowledge base, to extract main keywords in cloud computing from accumulated publications from 2004 to 2021. Results suggest that in cloud computing research, Internet of things has an accelerated technological and scientific growth compared to the other topics. Other critical research fields that support the evolution of cloud computing are mathematical optimization and virtual machine. These findings reveal a technological competition between Cloud systems infrastructure, hardware development side, and computing and software development to play the main role in technological evolution of Cloud Computing. Moreover, this study shows that technological development of virtual machines and computing device can be of critical importance to foster an economic and technological change in many sectors. However, the implementation of cloud computing has to be supported by skill development, organizational change, and adopter engagement, to foster the management and the diffusion of cloud technologies and the exploitation of cloud-based infrastructures for competitive advantage of firms in fast-changing markets. This study can provide main information to extend the knowledge having theoretical implications to explain characteristics of the evolution of science and technology in this field of research and practical implications of innovation management for the appropriate allocation of resources towards new technological trajectories in cloud computing having a potential of growth and beneficial impact in society. △ Less","9 July, 2022",https://arxiv.org/pdf/2207.12093
Representational Ethical Model Calibration,Robert Carruthers;Isabel Straw;James K Ruffle;Daniel Herron;Amy Nelson;Danilo Bzdok;Delmiro Fernandez-Reyes;Geraint Rees;Parashkev Nachev,"Equity is widely held to be fundamental to the ethics of healthcare. In the context of clinical decision-making, it rests on the comparative fidelity of the intelligence -- evidence-based or intuitive -- guiding the management of each individual patient. Though brought to recent attention by the individuating power of contemporary machine learning, such epistemic equity arises in the context of any decision guidance, whether traditional or innovative. Yet no general framework for its quantification, let alone assurance, currently exists. Here we formulate epistemic equity in terms of model fidelity evaluated over learnt multi-dimensional representations of identity crafted to maximise the captured diversity of the population, introducing a comprehensive framework for Representational Ethical Model Calibration. We demonstrate use of the framework on large-scale multimodal data from UK Biobank to derive diverse representations of the population, quantify model performance, and institute responsive remediation. We offer our approach as a principled solution to quantifying and assuring epistemic equity in healthcare, with applications across the research, clinical, and regulatory domains. △ Less","18 October, 2022",https://arxiv.org/pdf/2207.12043
ConceptBeam: Concept Driven Target Speech Extraction,Yasunori Ohishi;Marc Delcroix;Tsubasa Ochiai;Shoko Araki;Daiki Takeuchi;Daisuke Niizumi;Akisato Kimura;Noboru Harada;Kunio Kashino,"We propose a novel framework for target speech extraction based on semantic information, called ConceptBeam. Target speech extraction means extracting the speech of a target speaker in a mixture. Typical approaches have been exploiting properties of audio signals, such as harmonic structure and direction of arrival. In contrast, ConceptBeam tackles the problem with semantic clues. Specifically, we extract the speech of speakers speaking about a concept, i.e., a topic of interest, using a concept specifier such as an image or speech. Solving this novel problem would open the door to innovative applications such as listening systems that focus on a particular topic discussed in a conversation. Unlike keywords, concepts are abstract notions, making it challenging to directly represent a target concept. In our scheme, a concept is encoded as a semantic embedding by mapping the concept specifier to a shared embedding space. This modality-independent space can be built by means of deep metric learning using paired data consisting of images and their spoken captions. We use it to bridge modality-dependent information, i.e., the speech segments in the mixture, and the specified, modality-independent concept. As a proof of our scheme, we performed experiments using a set of images associated with spoken captions. That is, we generated speech mixtures from these spoken captions and used the images or speech signals as the concept specifiers. We then extracted the target speech using the acoustic characteristics of the identified segments. We compare ConceptBeam with two methods: one based on keywords obtained from recognition systems and another based on sound source separation. We show that ConceptBeam clearly outperforms the baseline methods and effectively extracts speech based on the semantic representation. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.11964
FileInsurer: A Scalable and Reliable Protocol for Decentralized File Storage in Blockchain,Hongyin Chen;Yuxuan Lu;Yukun Cheng,"With the development of blockchain applications, the requirements for file storage in blockchain are increasing rapidly. Many protocols, including Filecoin, Arweave, and Sia, have been proposed to provide scalable decentralized file storage for blockchain applications. However, the reliability is not well promised by existing protocols. Inspired by the idea of insurance, we innovatively propose a decentralized file storage protocol in blockchain, named as FileInsurer, to achieve both scalability and reliability. While ensuring scalability by distributed storage, FileInsurer guarantees reliability by enhancing robustness and fully compensating for the file loss. Specifically, under mild conditions, we prove that no more than 0.1\% value of all files should be compensated even if half of the storage collapses. Therefore, only a relatively small deposit needs to be pledged by storage providers to cover the potential file loss. Because of lower burdens of deposit, storage providers have more incentives to participate in the storage network. FileInsurer can run in the top layer of the InterPlanetary File System (IPFS), and thus it can be directly applied in Web 3.0, Non-Fungible Tokens, and Metaverse. △ Less","24 July, 2022",https://arxiv.org/pdf/2207.11657
A Novel Rapid-flooding Approach with Real-time Delay Compensation for Wireless Sensor Network Time Synchronization,Fanrong Shi;Simon X. Yang;Xianguo Tuo;Lili Ran;Yuqing Huang,"One-way-broadcast based flooding time synchronization algorithms are commonly used in wireless sensor networks (WSNs). However, the packet delay and clock drift pose challenges to accuracy, as they entail serious by-hop error accumulation problems in the WSNs. To overcome it, a rapid flooding multi-broadcast time synchronization with real-time delay compensation (RDC-RMTS) is proposed in this paper. By using a rapid-flooding protocol, flooding latency of the referenced time information is significantly reduced in the RDC-RMTS. In addition, a new joint clock skew-offset maximum likelihood estimation is developed to obtain the accurate clock parameter estimations, and the real-time packet delay estimation. Moreover, an innovative implementation of the RDC-RMTS is designed with an adaptive clock offset estimation. The experimental results indicate that, the RDC-RMTS can easily reduce the variable delay and significantly slow the growth of by-hop error accumulation. Thus, the proposed RDC-RMTS can achieve accurate time synchronization in large-scale complex WSNs. △ Less","22 July, 2022",https://arxiv.org/pdf/2207.11402
Intelligent Amphibious Ground-Aerial Vehicles: State of the Art Technology for Future Transportation,Xinyu Zhang;Jiangeng Huang;Yuanhao Huang;Kangyao Huang;Lei Yang;Yan Han;Li Wang;Huaping Liu;Jianxi Luo;Jun Li,"Amphibious ground-aerial vehicles fuse flying and driving modes to enable more flexible air-land mobility and have received growing attention recently. By analyzing the existing amphibious vehicles, we highlight the autonomous fly-driving functionality for the effective uses of amphibious vehicles in complex three-dimensional urban transportation systems. We review and summarize the key enabling technologies for intelligent flying-driving in existing amphibious vehicle designs, identify major technological barriers and propose potential solutions for future research and innovation. This paper aims to serve as a guide for research and development of intelligent amphibious vehicles for urban transportation toward the future. △ Less","22 July, 2022",https://arxiv.org/pdf/2207.11384
Twitmo: A Twitter Data Topic Modeling and Visualization Package for R,Andreas Buchmüller;Gillian Kant;Christoph Weisser;Benjamin Säfken;Krisztina Kis-Katos;Thomas Kneib,"We present Twitmo, a package that provides a broad range of methods to collect, pre-process, analyze and visualize geo-tagged Twitter data. Twitmo enables the user to collect geo-tagged Tweets from Twitter and and provides a comprehensive and user-friendly toolbox to generate topic distributions from Latent Dirichlet Allocations (LDA), correlated topic models (CTM) and structural topic models (STM). Functions are included for pre-processing of text, model building and prediction. In addition, one of the innovations of the package is the automatic pooling of Tweets into longer pseudo-documents using hashtags and cosine similarities for better topic coherence. The package additionally comes with functionality to visualize collected data sets and fitted models in static as well as interactive ways and offers built-in support for model visualizations via LDAvis providing great convenience for researchers in this area. The Twitmo package is an innovative toolbox that can be used to analyze public discourse of various topics, political parties or persons of interest in space and time. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.11236
DJI drone IDs are not encrypted,Conner Bender,"Drones are widely used in the energy, construction, agriculture, transportation, warehousing, real estate and movie industries. Key applications include surveys, inspections, deliveries and cinematography. With approximately 70-80% of the global market share of commercial off-the-shelf drones, Da-Jiang Innovations (DJI), headquartered in Shenzhen, China, essentially monopolizes the drone market. As commercial-off-the-shelf drone sales steadily rise, the Federal Aviation Administration has instituted regulations to protect the federal airspace. DJI has become a pioneer in developing remote identification technology in the form of drone ID (also known as AeroScope signals). Despite claims from the company touting its implementation of drone ID technology as ""encrypted"" yet later being proved incorrect for the claim, it remains a mystery on how one can grab and decode drone IDs over the air with low-cost radio frequency hardware in real-time. This research paper discusses a methodology using radio software and hardware to detect both Enhanced Wi-Fi and OcuSync drone IDs, the three types of drone ID packet structures and a functioning prototype of a DJI OcuSync detection system equipped with two HackRF Ones. △ Less","16 July, 2022",https://arxiv.org/pdf/2207.10795
Federated Semi-Supervised Domain Adaptation via Knowledge Transfer,Madhureeta Das;Xianhao Chen;Xiaoyong Yuan;Lan Zhang,"Given the rapidly changing machine learning environments and expensive data labeling, semi-supervised domain adaptation (SSDA) is imperative when the labeled data from the source domain is statistically different from the partially labeled data from the target domain. Most prior SSDA research is centrally performed, requiring access to both source and target data. However, data in many fields nowadays is generated by distributed end devices. Due to privacy concerns, the data might be locally stored and cannot be shared, resulting in the ineffectiveness of existing SSDA research. This paper proposes an innovative approach to achieve SSDA over multiple distributed and confidential datasets, named by Federated Semi-Supervised Domain Adaptation (FSSDA). FSSDA integrates SSDA with federated learning based on strategically designed knowledge distillation techniques, whose efficiency is improved by performing source and target training in parallel. Moreover, FSSDA controls the amount of knowledge transferred across domains by properly selecting a key parameter, i.e., the imitation parameter. Further, the proposed FSSDA can be effectively generalized to multi-source domain adaptation scenarios. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of FSSDA design. △ Less","25 July, 2022",https://arxiv.org/pdf/2207.10727
Detecting Small Query Graphs in A Large Graph via Neural Subgraph Search,Yunsheng Bai;Derek Xu;Yizhou Sun;Wei Wang,"Recent advances have shown the success of using reinforcement learning and search to solve NP-hard graph-related tasks, such as Traveling Salesman Optimization, Graph Edit Distance computation, etc. However, it remains unclear how one can efficiently and accurately detect the occurrences of a small query graph in a large target graph, which is a core operation in graph database search, biomedical analysis, social group finding, etc. This task is called Subgraph Matching which essentially performs subgraph isomorphism check between a query graph and a large target graph. One promising approach to this classical problem is the ""learning-to-search"" paradigm, where a reinforcement learning (RL) agent is designed with a learned policy to guide a search algorithm to quickly find the solution without any solved instances for supervision. However, for the specific task of Subgraph Matching, though the query graph is usually small given by the user as input, the target graph is often orders-of-magnitude larger. It poses challenges to the neural network design and can lead to solution and reward sparsity. In this paper, we propose NSUBS with two innovations to tackle the challenges: (1) A novel encoder-decoder neural network architecture to dynamically compute the matching information between the query and the target graphs at each search state; (2) A novel look-ahead loss function for training the policy network. Experiments on six large real-world target graphs show that NSUBS can significantly improve the subgraph matching performance. △ Less","29 September, 2022",https://arxiv.org/pdf/2207.10305
The Anatomy of Video Editing: A Dataset and Benchmark Suite for AI-Assisted Video Editing,Dawit Mureja Argaw;Fabian Caba Heilbron;Joon-Young Lee;Markus Woodson;In So Kweon,"Machine learning is transforming the video editing industry. Recent advances in computer vision have leveled-up video editing tasks such as intelligent reframing, rotoscoping, color grading, or applying digital makeups. However, most of the solutions have focused on video manipulation and VFX. This work introduces the Anatomy of Video Editing, a dataset, and benchmark, to foster research in AI-assisted video editing. Our benchmark suite focuses on video editing tasks, beyond visual effects, such as automatic footage organization and assisted video assembling. To enable research on these fronts, we annotate more than 1.5M tags, with relevant concepts to cinematography, from 196176 shots sampled from movie scenes. We establish competitive baseline methods and detailed analyses for each of the tasks. We hope our work sparks innovative research towards underexplored areas of AI-assisted video editing. △ Less","21 July, 2022",https://arxiv.org/pdf/2207.09812
"Generalizable and Robust Deep Learning Algorithm for Atrial Fibrillation Diagnosis Across Ethnicities, Ages and Sexes",Shany Biton;Mohsin Aldhafeeri;Erez Marcusohn;Kenta Tsutsui;Tom Szwagier;Adi Elias;Julien Oster;Jean Marc Sellal;Mahmoud Suleiman;Joachim A. Behar,"To drive health innovation that meets the needs of all and democratize healthcare, there is a need to assess the generalization performance of deep learning (DL) algorithms across various distribution shifts to ensure that these algorithms are robust. This retrospective study is, to the best of our knowledge, the first to develop and assess the generalization performance of a deep learning (DL) model for AF events detection from long term beat-to-beat intervals across ethnicities, ages and sexes. The new recurrent DL model, denoted ArNet2, was developed on a large retrospective dataset of 2,147 patients totaling 51,386 hours of continuous electrocardiogram (ECG). The models generalization was evaluated on manually annotated test sets from four centers (USA, Israel, Japan and China) totaling 402 patients. The model was further validated on a retrospective dataset of 1,730 consecutives Holter recordings from the Rambam Hospital Holter clinic, Haifa, Israel. The model outperformed benchmark state-of-the-art models and generalized well across ethnicities, ages and sexes. Performance was higher for female than male and young adults (less than 60 years old) and showed some differences across ethnicities. The main finding explaining these variations was an impairment in performance in groups with a higher prevalence of atrial flutter (AFL). Our findings on the relative performance of ArNet2 across groups may have clinical implications on the choice of the preferred AF examination method to use relative to the group of interest. △ Less","20 July, 2022",https://arxiv.org/pdf/2207.09667
GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features,Van-Quang Nguyen;Masanori Suganuma;Takayuki Okatani,"Current state-of-the-art methods for image captioning employ region-based features, as they provide object-level information that is essential to describe the content of images; they are usually extracted by an object detector such as Faster R-CNN. However, they have several issues, such as lack of contextual information, the risk of inaccurate detection, and the high computational cost. The first two could be resolved by additionally using grid-based features. However, how to extract and fuse these two types of features is uncharted. This paper proposes a Transformer-only neural architecture, dubbed GRIT (Grid- and Region-based Image captioning Transformer), that effectively utilizes the two visual features to generate better captions. GRIT replaces the CNN-based detector employed in previous methods with a DETR-based one, making it computationally faster. Moreover, its monolithic design consisting only of Transformers enables end-to-end training of the model. This innovative design and the integration of the dual visual features bring about significant performance improvement. The experimental results on several image captioning benchmarks show that GRIT outperforms previous methods in inference accuracy and speed. △ Less","20 July, 2022",https://arxiv.org/pdf/2207.09666
LightSolver -- A New Quantum-inspired Solver Cracks the 3-Regular 3-XORSAT Challenge,Idan Meirzada;Assaf Kalinski;Dov Furman;Tsafrir Armon;Talya Vaknin;Harel Primack;Chene Tradonsky;Ruti Ben-Shlomi,"The increasing complexity of required computational tasks alongside the inherent limitations in conventional computing calls for disruptive innovation. LightSolver devised a new quantum-inspired computing paradigm, which utilizes an all-optical platform for solving hard optimization problems. In this work, LightSolver introduces its digital simulator and joins the 3-Regular 3-XORSAT (3R3X) challenge, which aims to map the best available state-of-the-art classical and quantum solvers. So far, the challenge has resulted in a clear exponential barrier in terms of time-to-solution (TTS), preventing the inspected platforms from solving problems larger than a few hundred variables. LightSolver's simulator is the first to break the exponential barrier, outperforming both classical and quantum platforms by several orders-of-magnitude and extending the maximal problem size to more than 16,000 variables. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09517
Contaminant source identification in groundwater by means of artificial neural network,Daniele Secci;Laura Molino;Andrea Zanini,"In a desired environmental protection system, groundwater may not be excluded. In addition to the problem of over-exploitation, in total disagreement with the concept of sustainable development, another not negligible issue concerns the groundwater contamination. Mainly, this aspect is due to intensive agricultural activities or industrialized areas. In literature, several papers have dealt with transport problem, especially for inverse problems in which the release history or the source location are identified. The innovative aim of the paper is to develop a data-driven model that is able to analyze multiple scenarios, even strongly non-linear, in order to solve forward and inverse transport problems, preserving the reliability of the results and reducing the uncertainty. Furthermore, this tool has the characteristic of providing extremely fast responses, essential to identify remediation strategies immediately. The advantages produced by the model were compared with literature studies. In this regard, a feedforward artificial neural network, which has been trained to handle different cases, represents the data-driven model. Firstly, to identify the concentration of the pollutant at specific observation points in the study area (forward problem); secondly, to deal with inverse problems identifying the release history at known source location; then, in case of one contaminant source, identifying the release history and, at the same time, the location of the source in a specific sub-domain of the investigated area. At last, the observation error is investigated and estimated. The results are satisfactorily achieved, highlighting the capability of the ANN to deal with multiple scenarios by approximating nonlinear functions without the physical point of view that describes the phenomenon, providing reliable results, with very low computational burden and uncertainty. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09459
Bayesian Generational Population-Based Training,Xingchen Wan;Cong Lu;Jack Parker-Holder;Philip J. Ball;Vu Nguyen;Binxin Ru;Michael A. Osborne,"Reinforcement learning (RL) offers the potential for training generally capable agents that can interact autonomously in the real world. However, one key limitation is the brittleness of RL algorithms to core hyperparameters and network architecture choice. Furthermore, non-stationarities such as evolving training data and increased agent complexity mean that different hyperparameters and architectures may be optimal at different points of training. This motivates AutoRL, a class of methods seeking to automate these design choices. One prominent class of AutoRL methods is Population-Based Training (PBT), which have led to impressive performance in several large scale settings. In this paper, we introduce two new innovations in PBT-style methods. First, we employ trust-region based Bayesian Optimization, enabling full coverage of the high-dimensional mixed hyperparameter search space. Second, we show that using a generational approach, we can also learn both architectures and hyperparameters jointly on-the-fly in a single training run. Leveraging the new highly parallelizable Brax physics engine, we show that these innovations lead to large performance gains, significantly outperforming the tuned baseline while learning entire configurations on the fly. Code is available at https://github.com/xingchenwan/bgpbt. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09405
Data-Centric Epidemic Forecasting: A Survey,Alexander Rodríguez;Harshavardhan Kamarthi;Pulak Agarwal;Javen Ho;Mira Patel;Suchet Sapre;B. Aditya Prakash,"The COVID-19 pandemic has brought forth the importance of epidemic forecasting for decision makers in multiple domains, ranging from public health to the economy as a whole. While forecasting epidemic progression is frequently conceptualized as being analogous to weather forecasting, however it has some key differences and remains a non-trivial task. The spread of diseases is subject to multiple confounding factors spanning human behavior, pathogen dynamics, weather and environmental conditions. Research interest has been fueled by the increased availability of rich data sources capturing previously unobservable facets and also due to initiatives from government public health and funding agencies. This has resulted, in particular, in a spate of work on 'data-centered' solutions which have shown potential in enhancing our forecasting capabilities by leveraging non-traditional data sources as well as recent innovations in AI and machine learning. This survey delves into various data-driven methodological and practical advancements and introduces a conceptual framework to navigate through them. First, we enumerate the large number of epidemiological datasets and novel data streams that are relevant to epidemic forecasting, capturing various factors like symptomatic online surveys, retail and commerce, mobility, genomics data and more. Next, we discuss methods and modeling paradigms focusing on the recent data-driven statistical and deep-learning based methods as well as on the novel class of hybrid models that combine domain knowledge of mechanistic models with the effectiveness and flexibility of statistical approaches. We also discuss experiences and challenges that arise in real-world deployment of these forecasting systems including decision-making informed by forecasts. Finally, we highlight some challenges and open problems found across the forecasting pipeline. △ Less","20 July, 2022",https://arxiv.org/pdf/2207.09370
A Massively-Parallel 3D Simulator for Soft and Hybrid Robots,Joel Clay;Sofia Wyetzner;Alex Gaudio;Boxi Xia;Andrew Moshova;Jacob Austin;Max Segan;Hod Lipson,"Simulation is an important step in robotics for creating control policies and testing various physical parameters. Soft robotics is a field that presents unique physical challenges for simulating its subjects due to the nonlinearity of deformable material components along with other innovative, and often complex, physical properties. Because of the computational cost of simulating soft and heterogeneous objects with traditional techniques, rigid robotics simulators are not well suited to simulating soft robots. Thus, many engineers must build their own one-off simulators tailored to their system, or use existing simulators with reduced performance. In order to facilitate the development of this exciting technology, this work presents an interactive-speed, accurate, and versatile simulator for a variety of types of soft robots. Cronos, our open-source 3D simulation engine, parallelizes a mass-spring model for ultra-fast performance on both deformable and rigid objects. Our approach is applicable to a wide array of nonlinear material configurations, including high deformability, volumetric actuation, or heterogenous stiffness. This versatility provides the ability to mix materials and geometric components freely within a single robot simulation. By exploiting the flexibility and scalability of nonlinear Hookean mass-spring systems, this framework simulates soft and rigid objects via a highly parallel model for near real-time speed. We describe an efficient GPU CUDA implementation, which we demonstrate to achieve computation of over 1 billion elements per second on consumer-grade GPU cards. Dynamic physical accuracy of the system is validated by comparing results to Euler-Bernoulli beam theory, natural frequency predictions, and empirical data of a soft structure under large deformation. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09334
A-SFS: Semi-supervised Feature Selection based on Multi-task Self-supervision,Zhifeng Qiu;Wanxin Zeng;Dahua Liao;Ning Gui,"Feature selection is an important process in machine learning. It builds an interpretable and robust model by selecting the features that contribute the most to the prediction target. However, most mature feature selection algorithms, including supervised and semi-supervised, fail to fully exploit the complex potential structure between features. We believe that these structures are very important for the feature selection process, especially when labels are lacking and data is noisy. To this end, we innovatively introduce a deep learning-based self-supervised mechanism into feature selection problems, namely batch-Attention-based Self-supervision Feature Selection(A-SFS). Firstly, a multi-task self-supervised autoencoder is designed to uncover the hidden structure among features with the support of two pretext tasks. Guided by the integrated information from the multi-self-supervised learning model, a batch-attention mechanism is designed to generate feature weights according to batch-based feature selection patterns to alleviate the impacts introduced by a handful of noisy data. This method is compared to 14 major strong benchmarks, including LightGBM and XGBoost. Experimental results show that A-SFS achieves the highest accuracy in most datasets. Furthermore, this design significantly reduces the reliance on labels, with only 1/10 labeled data needed to achieve the same performance as those state of art baselines. Results show that A-SFS is also most robust to the noisy and missing data. △ Less","19 July, 2022",https://arxiv.org/pdf/2207.09061
MobileCodec: Neural Inter-frame Video Compression on Mobile Devices,Hoang Le;Liang Zhang;Amir Said;Guillaume Sautiere;Yang Yang;Pranav Shrestha;Fei Yin;Reza Pourreza;Auke Wiggers,"Realizing the potential of neural video codecs on mobile devices is a big technological challenge due to the computational complexity of deep networks and the power-constrained mobile hardware. We demonstrate practical feasibility by leveraging Qualcomm's technology and innovation, bridging the gap from neural network-based codec simulations running on wall-powered workstations, to real-time operation on a mobile device powered by Snapdragon technology. We show the first-ever inter-frame neural video decoder running on a commercial mobile phone, decoding high-definition videos in real-time while maintaining a low bitrate and high visual quality. △ Less","17 July, 2022",https://arxiv.org/pdf/2207.08338
Accelerated RRT* By Local Directional Visibility,Chenxi Feng;Haochen Wu,"RRT* is an efficient sampling-based motion planning algorithm. However, without taking advantages of accessible environment information, sampling-based algorithms usually result in sampling failures, generate useless nodes, and/or fail in exploring narrow passages. For this paper, in order to better utilize environment information and further improve searching efficiency, we proposed a novel approach to improve RRT* by 1) quantifying local knowledge of the obstacle configurations during neighbour rewiring in terms of directional visibility, 2) collecting environment information during searching, and 3) changing the sampling strategy biasing toward near-obstacle nodes after the first solution found. The proposed algorithm RRT* by Local Directional Visibility (RRT*-LDV) better utilizes local known information and innovates a weighted sampling strategy. The accelerated RRT*-LDV outperforms RRT* in convergence rate and success rate of finding narrow passages. A high Degree-Of-Freedom scenario is also experimented. △ Less","17 July, 2022",https://arxiv.org/pdf/2207.08283
Learning with Recoverable Forgetting,Jingwen Ye;Yifang Fu;Jie Song;Xingyi Yang;Songhua Liu;Xin Jin;Mingli Song;Xinchao Wang,"Life-long learning aims at learning a sequence of tasks without forgetting the previously acquired knowledge. However, the involved training data may not be life-long legitimate due to privacy or copyright reasons. In practical scenarios, for instance, the model owner may wish to enable or disable the knowledge of specific tasks or specific samples from time to time. Such flexible control over knowledge transfer, unfortunately, has been largely overlooked in previous incremental or decremental learning methods, even at a problem-setup level. In this paper, we explore a novel learning scheme, termed as Learning wIth Recoverable Forgetting (LIRF), that explicitly handles the task- or sample-specific knowledge removal and recovery. Specifically, LIRF brings in two innovative schemes, namely knowledge deposit and withdrawal, which allow for isolating user-designated knowledge from a pre-trained network and injecting it back when necessary. During the knowledge deposit process, the specified knowledge is extracted from the target network and stored in a deposit module, while the insensitive or general knowledge of the target network is preserved and further augmented. During knowledge withdrawal, the taken-off knowledge is added back to the target network. The deposit and withdraw processes only demand for a few epochs of finetuning on the removal data, ensuring both data and time efficiency. We conduct experiments on several datasets, and demonstrate that the proposed LIRF strategy yields encouraging results with gratifying generalization capability. △ Less","17 July, 2022",https://arxiv.org/pdf/2207.08224
Action-conditioned On-demand Motion Generation,Qiujing Lu;Yipeng Zhang;Mingjian Lu;Vwani Roychowdhury,"We propose a novel framework, On-Demand MOtion Generation (ODMO), for generating realistic and diverse long-term 3D human motion sequences conditioned only on action types with an additional capability of customization. ODMO shows improvements over SOTA approaches on all traditional motion evaluation metrics when evaluated on three public datasets (HumanAct12, UESTC, and MoCap). Furthermore, we provide both qualitative evaluations and quantitative metrics demonstrating several first-known customization capabilities afforded by our framework, including mode discovery, interpolation, and trajectory customization. These capabilities significantly widen the spectrum of potential applications of such motion generation models. The novel on-demand generative capabilities are enabled by innovations in both the encoder and decoder architectures: (i) Encoder: Utilizing contrastive learning in low-dimensional latent space to create a hierarchical embedding of motion sequences, where not only the codes of different action types form different groups, but within an action type, codes of similar inherent patterns (motion styles) cluster together, making them readily discoverable; (ii) Decoder: Using a hierarchical decoding strategy where the motion trajectory is reconstructed first and then used to reconstruct the whole motion sequence. Such an architecture enables effective trajectory control. Our code is released on the Github page: https://github.com/roychowdhuryresearch/ODMO △ Less","17 July, 2022",https://arxiv.org/pdf/2207.08164
On Curating Responsible and Representative Healthcare Video Recommendations for Patient Education and Health Literacy: An Augmented Intelligence Approach,Krishna Pothugunta;Xiao Liu;Anjana Susarla;Rema Padman,"Studies suggest that one in three US adults use the Internet to diagnose or learn about a health concern. However, such access to health information online could exacerbate the disparities in health information availability and use. Health information seeking behavior (HISB) refers to the ways in which individuals seek information about their health, risks, illnesses, and health-protective behaviors. For patients engaging in searches for health information on digital media platforms, health literacy divides can be exacerbated both by their own lack of knowledge and by algorithmic recommendations, with results that disproportionately impact disadvantaged populations, minorities, and low health literacy users. This study reports on an exploratory investigation of the above challenges by examining whether responsible and representative recommendations can be generated using advanced analytic methods applied to a large corpus of videos and their metadata on a chronic condition (diabetes) from the YouTube social media platform. The paper focusses on biases associated with demographic characters of actors using videos on diabetes that were retrieved and curated for multiple criteria such as encoded medical content and their understandability to address patient education and population health literacy needs. This approach offers an immense opportunity for innovation in human-in-the-loop, augmented-intelligence, bias-aware and responsible algorithmic recommendations by combining the perspectives of health professionals and patients into a scalable and generalizable machine learning framework for patient empowerment and improved health outcomes. △ Less","12 July, 2022",https://arxiv.org/pdf/2207.07915
Transfer learning for time series classification using synthetic data generation,Yarden Rotem;Nathaniel Shimoni;Lior Rokach;Bracha Shapira,"In this paper, we propose an innovative Transfer learning for Time series classification method. Instead of using an existing dataset from the UCR archive as the source dataset, we generated a 15,000,000 synthetic univariate time series dataset that was created using our unique synthetic time series generator algorithm which can generate data with diverse patterns and angles and different sequence lengths. Furthermore, instead of using classification tasks provided by the UCR archive as the source task as previous studies did,we used our own 55 regression tasks as the source tasks, which produced better results than selecting classification tasks from the UCR archive △ Less","16 July, 2022",https://arxiv.org/pdf/2207.07897
Quantifying human performance in chess,Sandeep Chowdhary;Iacopo Iacopini;Federico Battiston,"From sports to science, the recent availability of large-scale data has allowed to gain insights on the drivers of human innovation and success in a variety of domains. Here we quantify human performance in the popular game of chess by leveraging a very large dataset comprising of over 120 million games between almost 1 million players. We find that individuals encounter hot streaks of repeated success, longer for beginners than for expert players, and even longer cold streaks of unsatisfying performance. Skilled players can be distinguished from the others based on their gaming behaviour. Differences appear from the very first moves of the game, with experts tending to specialize and repeat the same openings while beginners explore and diversify more. However, experts experience a broader response repertoire, and display a deeper understanding of different variations within the same line. Over time, the opening diversity of a player tends to decrease, hinting at the development of individual playing styles. Nevertheless, we find that players are often not able to recognize their most successful openings. Overall, our work contributes to quantifying human performance in competitive settings, providing a first large-scale quantitative analysis of individual careers in chess, helping unveil the determinants separating elite from beginner performance. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.07780
Printable Flexible Robots for Remote Learning,Savita V. Kendre;Gus. T. Teran;Lauryn Whiteside;Tyler Looney;Ryley Wheelock;Surya Ghai;Markus P. Nemitz,"The COVID-19 pandemic has revealed the importance of digital fabrication to enable online learning, which remains a challenge for robotics courses. We introduce a teaching methodology that allows students to participate remotely in a hands-on robotics course involving the design and fabrication of robots. Our methodology employs 3D printing techniques with flexible filaments to create innovative soft robots; robots are made from flexible, as opposed to rigid, materials. Students design flexible robotic components such as actuators, sensors, and controllers using CAD software, upload their designs to a remote 3D printing station, monitor the print with a web camera, and inspect the components with lab staff before being mailed for testing and assembly. At the end of the course, students will have iterated through several designs and created fluidically-driven soft robots. Our remote teaching methodology enables educators to utilize 3D printing resources to teach soft robotics and cultivate creativity among students to design novel and innovative robots. Our methodology seeks to democratize robotics engineering by decoupling hands-on learning experiences from expensive equipment in the learning environment. △ Less","15 July, 2022",https://arxiv.org/pdf/2207.07729
Value-based Engineering with IEEE 7000TM,Sarah Spiekermann;Till Winkler,"Digital ethics is being discussed worldwide as a necessity to create more reliable IT systems. This discussion, fueled by the fear of uncontrollable artificial intelligence (AI) has moved many institutions and scientists to demand a value-based system engineering. This article presents how organizations can build responsible and ethically founded systems with the 'Value-based Engineering' (VBE) approach that was standardized in the IEEE 7000TM standard. VBE is a transparent, clearly-structured, step-by-step methodology combining innovation management, risk management, system and software engineering in one process framework. It embeds a robust value ontology and terminology. It has been tested in various case studies. This article introduces readers to the most important steps and contributions of the approach. △ Less","21 June, 2022",https://arxiv.org/pdf/2207.07599
Tell Me the Evidence? Dual Visual-Linguistic Interaction for Answer Grounding,Junwen Pan;Guanlin Chen;Yi Liu;Jiexiang Wang;Cheng Bian;Pengfei Zhu;Zhicheng Zhang,"Answer grounding aims to reveal the visual evidence for visual question answering (VQA), which entails highlighting relevant positions in the image when answering questions about images. Previous attempts typically tackle this problem using pretrained object detectors, but without the flexibility for objects not in the predefined vocabulary. However, these black-box methods solely concentrate on the linguistic generation, ignoring the visual interpretability. In this paper, we propose Dual Visual-Linguistic Interaction (DaVI), a novel unified end-to-end framework with the capability for both linguistic answering and visual grounding. DaVI innovatively introduces two visual-linguistic interaction mechanisms: 1) visual-based linguistic encoder that understands questions incorporated with visual features and produces linguistic-oriented evidence for further answer decoding, and 2) linguistic-based visual decoder that focuses visual features on the evidence-related regions for answer grounding. This way, our approach ranked the 1st place in the answer grounding track of 2022 VizWiz Grand Challenge. △ Less","20 June, 2022",https://arxiv.org/pdf/2207.05703
SD-GAN: Semantic Decomposition for Face Image Synthesis with Discrete Attribute,Zhou Kangneng;Zhu Xiaobin;Gao Daiheng;Lee Kai;Li Xinjie;Yin Xu-Cheng,"Manipulating latent code in generative adversarial networks (GANs) for facial image synthesis mainly focuses on continuous attribute synthesis (e.g., age, pose and emotion), while discrete attribute synthesis (like face mask and eyeglasses) receives less attention. Directly applying existing works to facial discrete attributes may cause inaccurate results. In this work, we propose an innovative framework to tackle challenging facial discrete attribute synthesis via semantic decomposing, dubbed SD-GAN. To be concrete, we explicitly decompose the discrete attribute representation into two components, i.e. the semantic prior basis and offset latent representation. The semantic prior basis shows an initializing direction for manipulating face representation in the latent space. The offset latent presentation obtained by 3D-aware semantic fusion network is proposed to adjust prior basis. In addition, the fusion network integrates 3D embedding for better identity preservation and discrete attribute synthesis. The combination of prior basis and offset latent representation enable our method to synthesize photo-realistic face images with discrete attributes. Notably, we construct a large and valuable dataset MEGN (Face Mask and Eyeglasses images crawled from Google and Naver) for completing the lack of discrete attributes in the existing dataset. Extensive qualitative and quantitative experiments demonstrate the state-of-the-art performance of our method. Our code is available at: https://github.com/MontaEllis/SD-GAN. △ Less","12 July, 2022",https://arxiv.org/pdf/2207.05300
Knowledge Graph Induction enabling Recommending and Trend Analysis: A Corporate Research Community Use Case,Nandana Mihindukulasooriya;Mike Sava;Gaetano Rossiello;Md Faisal Mahbub Chowdhury;Irene Yachbes;Aditya Gidh;Jillian Duckwitz;Kovit Nisar;Michael Santos;Alfio Gliozzo,"A research division plays an important role of driving innovation in an organization. Drawing insights, following trends, keeping abreast of new research, and formulating strategies are increasingly becoming more challenging for both researchers and executives as the amount of information grows in both velocity and volume. In this paper we present a use case of how a corporate research community, IBM Research, utilizes Semantic Web technologies to induce a unified Knowledge Graph from both structured and textual data obtained by integrating various applications used by the community related to research projects, academic papers, datasets, achievements and recognition. In order to make the Knowledge Graph more accessible to application developers, we identified a set of common patterns for exploiting the induced knowledge and exposed them as APIs. Those patterns were born out of user research which identified the most valuable use cases or user pain points to be alleviated. We outline two distinct scenarios: recommendation and analytics for business use. We will discuss these scenarios in detail and provide an empirical evaluation on entity recommendation specifically. The methodology used and the lessons learned from this work can be applied to other organizations facing similar challenges. △ Less","15 September, 2022",https://arxiv.org/pdf/2207.05188
"QAnon Propaganda on Twitter as Information Warfare: Influencers, Networks, and Narratives",L. Dilley;W. Welna;F. Foster,"QAnon refers to a set of far-right, conspiratorial ideologies that have risen in popularity in the U.S. since their initial promotion in 2017 on the 4chan internet message board. A central narrative element of QAnon is that a powerful group of elite, liberal members of the Democratic Party engage in morally reprehensible practices, but that former U.S. President Donald J. Trump was prosecuting them. Five studies investigated the influence and network connectivity of accounts promoting QAnon on Twitter from August, 2020 through January, 2021. Selection of Twitter accounts emphasized on-line influencers and ""persons of interest"" known or suspected of participation in QAnon propaganda promotion activities. Evidence of large-scale coordination among accounts promoting QAnon was observed, demonstrating rigorous, quantitative evidence of ""astroturfing"" in QAnon propaganda promotion on Twitter, as opposed to strictly ""grassroots"" activities of citizens acting independently. Further, evidence was obtained supporting that networks of extreme far-right adherents engaged in organized QAnon propaganda promotion, as revealed by network overlap among accounts promoting far-right extremist (e.g., anti-Semitic) content and insurrectionist themes; New Age, occult, and ""esoteric"" themes; and internet puzzle games like Cicada 3301 and other ""alternate reality games."" Based on well-grounded theories and findings from the social sciences, it is argued that QAnon propaganda on Twitter in the months circa the 2020 U.S. Presidential election likely reflected joint participation of multiple actors, including nation-states like Russia, in innovative misuse of social media toward undermining democratic processes by promoting ""magical"" thinking, ostracism of Democrats and liberals, and salience of White extinction narratives common among otherwise ideologically diverse groups on the extreme far-right. △ Less","11 July, 2022",https://arxiv.org/pdf/2207.05118
HEGrid: A High Efficient Multi-Channel Radio Astronomical Data Gridding Framework in Heterogeneous Computing Environments,Hao Wang;Ce Yu;Jian Xiao;Shanjiang Tang;Min Long;Ming Zhu,"The challenge to fully exploit the potential of existing and upcoming scientific instruments like large single-dish radio telescopes is to process the collected massive data effectively and efficiently. As a ""quasi 2D stencil computation"" with the ""Moore neighborhood pattern,"" gridding is the most computationally intensive step in data reduction pipeline for radio astronomy studies, enabling astronomers to create correct sky images for further analysis. However, the existing gridding frameworks can either only run on multi-core CPU architecture or do not support high-concurrency, multi-channel data gridding. Their performance is then limited, and there are emerging needs for innovative gridding frameworks to process data from large single-dish radio telescopes like the Five-hundred-meter Aperture Spherical Telescope (FAST). To address those challenges, we developed a High Efficient Gridding framework, HEGrid, by overcoming the above limitations. Specifically, we propose and construct the gridding pipeline in heterogeneous computing environments and achieve multi-pipeline concurrency for high performance multi-channel processing. Furthermore, we propose pipeline-based co-optimization to alleviate the potential negative performance impact of possible intra- and inter-pipeline low computation and I/O utilization, including component share-based redundancy elimination, thread-level data reuse and overlapping I/O and computation. Our experiments are based on both simulated datasets and actual FAST observational datasets. The results show that HEGrid outperforms other state-of-the-art gridding frameworks by up to 5.5x and has robust hardware portability, including AMD Radeon Instinct GPU and NVIDIA GPU. △ Less","10 July, 2022",https://arxiv.org/pdf/2207.04584
Delayed Impact of Interdisciplinary Research,Yang Zhang;Yang Wang;Haifeng Du;Shlomo Havlin,"Interdisciplinary research increasingly fuels innovation, and is considered to be a key to tomorrow breakthrough. Yet little is known about whether interdisciplinary research manifests delayed impact. Here, we use the time to reach the citation peak to quantify the highest impact time and citation dynamics, and examine its relationship with interdisciplinarity. Using large scale publication datasets, our results suggest that interdisciplinary papers show significant delayed impact both microscopically per paper and macroscopically collectively, as it takes longer time for interdisciplinary papers to reach their citation peak. Furthermore, we study the underlying forces of such delayed impact, finding that the effect goes beyond the Matthew effect (i.e., the rich-get-richer effect). Finally, we find that team size and content conventionality only partly account for this effect. Overall, our results suggest that governments, research administrators, funding agencies should be aware of this general feature of interdisciplinary science, which may have broad policy implications. △ Less","9 July, 2022",https://arxiv.org/pdf/2207.04244
Variational Approach for Intensity Domain Multi-exposure Image Fusion,Harbinder Singh;Dinesh Arora;Vinay Kumar,"Recent innovations shows that blending of details captured by single Low Dynamic Range (LDR) sensor overcomes the limitations of standard digital cameras to capture details from high dynamic range scene. We present a method to produce well-exposed fused image that can be displayed directly on conventional display devices. The ambition is to preserve details in poorly illuminated and brightly illuminated regions. Proposed approach does not require true radiance reconstruction and tone manipulation steps. The aforesaid objective is achieved by taking into account local information measure that select well-exposed regions across input exposures. In addition, Contrast Limited Adaptive Histogram equalization (CLAHE) is introduced to improve uniformity of input multi-exposure image prior to fusion. △ Less","9 July, 2022",https://arxiv.org/pdf/2207.04204
An Outlook on the Future Marine Traffic Management System for Autonomous Ships,Michele Martelli;Antonio Virdis;Alberto Gotta;Pietro CassarÀ;Maria Di Summa,"In the shipping digitalisation process, the peak will be reached with the advent of a wholly autonomous and at the same time safe and reliable ship. Full autonomy could be obtained by two linked Artificial-Intelligence systems representing the ship navigator and the ship engineer that possess sensing and analysis skills, situational awareness, planning, and control capabilities. Many efforts have been made in developing onboard systems; however, the shore facilities are not ready yet to deal with these new technologies. The paper aims to present the innovative technologies and methodologies needed to develop a futuristic Vessel Traffic System. The proposed systems will aim at faultless data acquisition and processing, provide input to decision-making systems, and suggest evasive manoeuvre; to deal with hazards and systems failure without human intervention onboard. The system is composed of three different and interacting layers. The first is an artificially intelligent tool to detect and control autonomous ships, thanks to situation recognition and obstacle avoidance strategies. The second is an orchestration and management platform designed to coordinate the sensing-actuation infrastructure and the AI algorithms results made available by multiple ships, mustering edge, and distributed computing techniques to fulfil the specific harsh requirements of the sea environment. The final part is a holistic guidance-navigation-control framework to manage autonomous ships navigation in a crowded area. Eventually, a cyber-physical scenario, using both a ship digital-twin and a real model-scale ship, is suggested to test and validate the innovative system without the availability of a full-scale scenario. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.04140
"The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications",Mirac Suzgun;Luke Melas-Kyriazi;Suproteem K. Sarkar;Scott Duke Kominers;Stuart M. Shieber,"Innovation is a major driver of economic and social development, and information about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Although the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, ML offers a promising set of techniques for evaluating novelty, summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike previously proposed patent datasets in NLP, HUPD contains the inventor-submitted versions of patent applications--not the final versions of granted patents--thereby allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured metadata alongside the text of patent filings: By providing each application's metadata along with all of its text fields, the dataset enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community--namely, binary classification of patent decisions. We additionally show the structured metadata provided in the dataset enables us to conduct explicit studies of concept shifts for this task. Finally, we demonstrate how HUPD can be used for three additional tasks: multi-class classification of patent subject areas, language modeling, and summarization. △ Less","8 July, 2022",https://arxiv.org/pdf/2207.04043
Balanced Self-Paced Learning for AUC Maximization,Bin Gu;Chenkang Zhang;Huan Xiong;Heng Huang,"Learning to improve AUC performance is an important topic in machine learning. However, AUC maximization algorithms may decrease generalization performance due to the noisy data. Self-paced learning is an effective method for handling noisy data. However, existing self-paced learning methods are limited to pointwise learning, while AUC maximization is a pairwise learning problem. To solve this challenging problem, we innovatively propose a balanced self-paced AUC maximization algorithm (BSPAUC). Specifically, we first provide a statistical objective for self-paced AUC. Based on this, we propose our self-paced AUC maximization formulation, where a novel balanced self-paced regularization term is embedded to ensure that the selected positive and negative samples have proper proportions. Specially, the sub-problem with respect to all weight variables may be non-convex in our formulation, while the one is normally convex in existing self-paced problems. To address this, we propose a doubly cyclic block coordinate descent method. More importantly, we prove that the sub-problem with respect to all weight variables converges to a stationary point on the basis of closed-form solutions, and our BSPAUC converges to a stationary point of our fixed optimization objective under a mild assumption. Considering both the deep learning and kernel-based implementations, experimental results on several large-scale datasets demonstrate that our BSPAUC has a better generalization performance than existing state-of-the-art AUC maximization methods. △ Less","7 July, 2022",https://arxiv.org/pdf/2207.03650
AFFORCE: Actionable Framework for Designing Crowdsourcing Experiences for Older Adults,Kinga Skorupska;Radosław Nielek;Wiesław Kopeć,"In this article we propose a unique framework for designing attractive and engaging crowdsourcing systems for older adults, which is called AFFORCE (Actionable Framework For Crowdsourcing Experiences). We first categorize and map mitigating factors and barriers to crowdsourcing for older adults to finally discuss, present and combine system elements addressing them into an actionable reference framework. This innovative framework is based on our experience with the design of crowdsourcing systems for older adults in exploratory cases and studies, related work, as well as our and related research at the intersection of older adults' use of ICT, crowdsourcing and citizen science. △ Less","7 July, 2022",https://arxiv.org/pdf/2207.03170
Energy-Efficient Communication Networks via Multiple Aerial Reconfigurable Intelligent Surfaces: DRL and Optimization Approach,Pyae Sone Aung;Yu Min Park;Yan Kyaw Tun;Zhu Han;Choong Seon Hong,"In the realm of wireless communications in 5G, 6G and beyond, deploying unmanned aerial vehicle (UAV) has been an innovative approach to extend the coverage area due to its easy deployment. Moreover, reconfigurable intelligent surface (RIS) has also emerged as a new paradigm with the goals of enhancing the average sum-rate as well as energy efficiency. By combining these attractive features, an energy-efficient RIS-mounted multiple UAVs (aerial RISs: ARISs) assisted downlink communication system is studied. Due to the obstruction, user equipments (UEs) can have a poor line of sight to communicate with the base station (BS). To solve this, multiple ARISs are implemented to assist the communication between the BS and UEs. Then, the joint optimization problem of deployment of ARIS, ARIS reflective elements on/off states, phase shift, and power control of the multiple ARISs-assisted communication system is formulated. The problem is challenging to solve since it is mixed-integer, non-convex, and NP-hard. To overcome this, it is decomposed into three sub-problems. Afterwards, successive convex approximation (SCA), actor-critic proximal policy optimization (AC-PPO), and whale optimization algorithm (WOA) are employed to solve these sub-problems alternatively. Finally, extensive simulation results have been generated to illustrate the efficacy of our proposed algorithms. △ Less","8 December, 2022",https://arxiv.org/pdf/2207.03149
The Case for Distributed Shared-Memory Databases with RDMA-Enabled Memory Disaggregation,Ruihong Wang;Jianguo Wang;Stratos Idreos;M. Tamer Özsu;Walid G. Aref,"Memory disaggregation (MD) allows for scalable and elastic data center design by separating compute (CPU) from memory. With MD, compute and memory are no longer coupled into the same server box. Instead, they are connected to each other via ultra-fast networking such as RDMA. MD can bring many advantages, e.g., higher memory utilization, better independent scaling (of compute and memory), and lower cost of ownership. This paper makes the case that MD can fuel the next wave of innovation on database systems. We observe that MD revives the great debate of ""shared what"" in the database community. We envision that distributed shared-memory databases (DSM-DB, for short) - that have not received much attention before - can be promising in the future with MD. We present a list of challenges and opportunities that can inspire next steps in system design making the case for DSM-DB. △ Less","6 July, 2022",https://arxiv.org/pdf/2207.03027
Microservice Architecture Reconstruction and Visualization Techniques: A Review,Tomas Cerny;Amr S. Abdelfattah;Vincent Bushong;Abdullah Al Maruf;Davide Taibi,"Microservice system solutions are driving digital transformation; however, fundamental tools and system perspectives are missing to better observe, understand, and manage these systems, their properties, and their dependencies. Microservices architecture leads towards decentralization, which implies many advantages to system operation; it, however, brings challenges to their development. Microservice systems often lack a system-centric perspective that would help engineers better cope with system evolution and quality assessment. In this work, we explored microservice-specific architecture reconstruction based on static analysis. Such reconstruction typically results in system models to visualize selected system-centric perspectives. Conventional models involve 2D methods; however, these methods are limited in utility when services proliferate. We considered various architectural perspectives relevant to microservices and assessed the relevancy of the traditional method, comparing it to alternative data visualization using 3D space. As a representative of the 3D method, we considered a 3D graph model presented in augmented reality. To begin testing the feasibility of deriving such perspectives from microservice systems, we developed and implemented prototype tools for software architecture reconstruction and visualization of compared perspectives. Using these prototypes, we performed a small user study with software practitioners to highlight the potentials and limitations of these innovative visualizations used for common practitioner reasoning and tasks. △ Less","23 July, 2022",https://arxiv.org/pdf/2207.02988
The use of Synthetic Data to solve the scalability and data availability problems in Smart City Digital Twins,Esteve Almirall;Davide Callegaro;Peter Bruins;Mar Santamaría;Pablo Martínez;Ulises Cortés,"The A.I. disruption and the need to compete on innovation are impacting cities that have an increasing necessity to become innovation hotspots. However, without proven solutions, experimentation, often unsuccessful, is needed. But experimentation in cities has many undesirable effects not only for its citizens but also reputational if unsuccessful. Digital Twins, so popular in other areas, seem like a promising way to expand experimentation proposals but in simulated environments, translating only the half-baked ones, the ones with higher probability of success, to real environments and therefore minimizing risks. However, Digital Twins are data intensive and need highly localized data, making them difficult to scale, particularly to small cities, and with the high cost associated to data collection. We present an alternative based on synthetic data that given some conditions, quite common in Smart Cities, can solve these two problems together with a proof-of-concept based on NO2 pollution. △ Less","6 July, 2022",https://arxiv.org/pdf/2207.02953
Combining Topic Modeling with Grounded Theory: Case Studies of Project Collaboration,Eyyub Can Odacioglu;Lihong Zhang;Richard Allmendinger,"This paper proposes an Artificial Intelligence (AI) Grounded Theory for management studies. We argue that this novel and rigorous approach that embeds topic modelling will lead to the latent knowledge to be found. We illustrate this abductive method using 51 case studies of collaborative innovation published by Project Management Institute (PMI). Initial results are presented and discussed that include 40 topics, 6 categories, 4 of which are core categories, and two new theories of project collaboration. △ Less","28 June, 2022",https://arxiv.org/pdf/2207.02212
Social Sensing and Human in the Loop Profiling during Pandemics: the Vitoria application,J. Fernandes;J. Sá Silva;A. Rodrigues;F. Boavida;R. Gaspar;C. Godinho;R. Francisco,"As the number of smart devices that surround us increases, so do the opportunities to leverage them to create socially- and context-aware systems. Smart devices can be used for better understanding human behaviour and its societal implications. As an example of a scenario in which the role of socially aware systems is crucial, consider the SARS-CoV-2 pandemic. In this paper we present an innovative Humanin-The-Loop Cyber Physical system that can collect passive data from people, such as physical activity, sleep information, and discrete location, as well as collect self-reported data, and provide individualised user feedback. In this paper, we also present a three and a half months field trial implemented in Portugal. This trial was part of a larger scope project that was supported by the Portuguese National Health System, to evaluate the indicators and effects of the pandemic. Results concerning various applications usage statistics are presented, comparing the most used applications, their objective and their usage pattern in work/non-work periods. Additionally,the time-lagged cross correlation between some of the collected metrics, Covid events, and media news, are explored. This type of applications can be used not only in the context of Covid but also in future pandemics, to assist individuals in self-regulation of their contagion risk, based on personalized information, while also function as a means for raising self-awareness of risks related to psychological wellbeing. △ Less","5 July, 2022",https://arxiv.org/pdf/2207.01920
ICE-NODE: Integration of Clinical Embeddings with Neural Ordinary Differential Equations,Asem Alaa;Erik Mayer;Mauricio Barahona,"Early diagnosis of disease can lead to improved health outcomes, including higher survival rates and lower treatment costs. With the massive amount of information available in electronic health records (EHRs), there is great potential to use machine learning (ML) methods to model disease progression aimed at early prediction of disease onset and other outcomes. In this work, we employ recent innovations in neural ODEs combined with rich semantic embeddings of clinical codes to harness the full temporal information of EHRs. We propose ICE-NODE (Integration of Clinical Embeddings with Neural Ordinary Differential Equations), an architecture that temporally integrates embeddings of clinical codes and neural ODEs to learn and predict patient trajectories in EHRs. We apply our method to the publicly available MIMIC-III and MIMIC-IV datasets, and we find improved prediction results compared to state-of-the-art methods, specifically for clinical codes that are not frequently observed in EHRs. We also show that ICE-NODE is more competent at predicting certain medical conditions, like acute renal failure, pulmonary heart disease and birth-related problems, where the full temporal information could provide important information. Furthermore, ICE-NODE is also able to produce patient risk trajectories over time that can be exploited for further detailed predictions of disease evolution. △ Less","31 July, 2022",https://arxiv.org/pdf/2207.01873
CAM/CAD Point Cloud Part Segmentation via Few-Shot Learning,Jiahui Wang;Haiyue Zhu;Haoren Guo;Abdullah Al Mamun;Vadakkepat Prahlad;Tong Heng Lee,"3D part segmentation is an essential step in advanced CAM/CAD workflow. Precise 3D segmentation contributes to lower defective rate of work-pieces produced by the manufacturing equipment (such as computer controlled CNCs), thereby improving work efficiency and attaining the attendant economic benefits. A large class of existing works on 3D model segmentation are mostly based on fully-supervised learning, which trains the AI models with large, annotated datasets. However, the disadvantage is that the resulting models from the fully-supervised learning methodology are highly reliant on the completeness of the available dataset, and its generalization ability is relatively poor to new unknown segmentation types (i.e. further additional novel classes). In this work, we propose and develop a noteworthy few-shot learning-based approach for effective part segmentation in CAM/CAD; and this is designed to significantly enhance its generalization ability and flexibly adapt to new segmentation tasks by using only relatively rather few samples. As a result, it not only reduces the requirements for the usually unattainable and exhaustive completeness of supervision datasets, but also improves the flexibility for real-world applications. As further improvement and innovation, we additionally adopt the transform net and the center loss block in the network. These characteristics serve to improve the comprehension for 3D features of the various possible instances of the whole work-piece and ensure the close distribution of the same class in feature space. △ Less","16 July, 2022",https://arxiv.org/pdf/2207.01218
Saliency-Regularized Deep Multi-Task Learning,Guangji Bai;Liang Zhao,"Multitask learning is a framework that enforces multiple learning tasks to share knowledge to improve their generalization abilities. While shallow multitask learning can learn task relations, it can only handle predefined features. Modern deep multitask learning can jointly learn latent features and task sharing, but they are obscure in task relation. Also, they predefine which layers and neurons should share across tasks and cannot learn adaptively. To address these challenges, this paper proposes a new multitask learning framework that jointly learns latent features and explicit task relations by complementing the strength of existing shallow and deep multitask learning scenarios. Specifically, we propose to model the task relation as the similarity between task input gradients, with a theoretical analysis of their equivalency. In addition, we innovatively propose a multitask learning objective that explicitly learns task relations by a new regularizer. Theoretical analysis shows that the generalizability error has been reduced thanks to the proposed regularizer. Extensive experiments on several multitask learning and image classification benchmarks demonstrate the proposed method effectiveness, efficiency as well as reasonableness in the learned task relation patterns. △ Less","3 July, 2022",https://arxiv.org/pdf/2207.01117
Mental Illness Classification on Social Media Texts using Deep Learning and Transfer Learning,Iqra Ameer;Muhammad Arif;Grigori Sidorov;Helena Gòmez-Adorno;Alexander Gelbukh,"Given the current social distance restrictions across the world, most individuals now use social media as their major medium of communication. Millions of people suffering from mental diseases have been isolated due to this, and they are unable to get help in person. They have become more reliant on online venues to express themselves and seek advice on dealing with their mental disorders. According to the World health organization (WHO), approximately 450 million people are affected. Mental illnesses, such as depression, anxiety, etc., are immensely common and have affected an individuals' physical health. Recently Artificial Intelligence (AI) methods have been presented to help mental health providers, including psychiatrists and psychologists, in decision making based on patients' authentic information (e.g., medical records, behavioral data, social media utilization, etc.). AI innovations have demonstrated predominant execution in numerous real-world applications broadening from computer vision to healthcare. This study analyzes unstructured user data on the Reddit platform and classifies five common mental illnesses: depression, anxiety, bipolar disorder, ADHD, and PTSD. We trained traditional machine learning, deep learning, and transfer learning multi-class models to detect mental disorders of individuals. This effort will benefit the public health system by automating the detection process and informing appropriate authorities about people who require emergency assistance. △ Less","3 July, 2022",https://arxiv.org/pdf/2207.01012
Complementary artificial intelligence designed to augment human discovery,Jamshid Sourati;James Evans,"Neither artificial intelligence designed to play Turing's imitation game, nor augmented intelligence built to maximize the human manipulation of information are tuned to accelerate innovation and improve humanity's collective advance against its greatest challenges. We reconceptualize and pilot beneficial AI to radically augment human understanding by complementing rather than competing with human cognitive capacity. Our approach to complementary intelligence builds on insights underlying the wisdom of crowds, which hinges on the independence and diversity of crowd members' information and approach. By programmatically incorporating information on the evolving distribution of scientific expertise from research papers, our approach follows the distribution of content in the literature while avoiding the scientific crowd and the hypotheses cognitively available to it. We use this approach to generate valuable predictions for what materials possess valuable energy-related properties (e.g., thermoelectricity), and what compounds possess valuable medical properties (e.g., asthma) that complement the human scientific crowd. We demonstrate that our complementary predictions, if identified by human scientists and inventors at all, are only discovered years further into the future. When we evaluate the promise of our predictions with first-principles equations, we demonstrate that increased complementarity of our predictions does not decrease and in some cases increases the probability that the predictions possess the targeted properties. In summary, by tuning AI to avoid the crowd, we can generate hypotheses unlikely to be imagined or pursued until the distant future and promise to punctuate scientific advance. By identifying and correcting for collective human bias, these models also suggest opportunities to improve human prediction by reformulating science education for discovery. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00902
"A fast converging particle swarm optimization through targeted, position-mutated, elitism (PSO-TPME)",Tamir Shaqarin;Bernd R. Noack,"We dramatically improve convergence speed and global exploration capabilities of particle swarm optimization (PSO) through a targeted position-mutated elitism (PSO-TPME). The three key innovations address particle classification, elitism, and mutation in the cognitive and social model. PSO-TPME is benchmarked against five popular PSO variants for multi-dimensional functions, which are extensively adopted in the optimization field, In particular, the convergence accuracy, convergence speed, and the capability to find global minima is investigated. The statistical error is assessed by numerous repetitions. The simulations demonstrate that proposed PSO variant outperforms the other variants in terms of convergence rate and accuracy by orders of magnitude. △ Less","19 August, 2022",https://arxiv.org/pdf/2207.00900
Ransomware Classification and Detection With Machine Learning Algorithms,Mohammad Masum;Md Jobair Hossain Faruk;Hossain Shahriar;Kai Qian;Dan Lo;Muhaiminul Islam Adnan,"Malicious attacks, malware, and ransomware families pose critical security issues to cybersecurity, and it may cause catastrophic damages to computer systems, data centers, web, and mobile applications across various industries and businesses. Traditional anti-ransomware systems struggle to fight against newly created sophisticated attacks. Therefore, state-of-the-art techniques like traditional and neural network-based architectures can be immensely utilized in the development of innovative ransomware solutions. In this paper, we present a feature selection-based framework with adopting different machine learning algorithms including neural network-based architectures to classify the security level for ransomware detection and prevention. We applied multiple machine learning algorithms: Decision Tree (DT), Random Forest (RF), Naive Bayes (NB), Logistic Regression (LR) as well as Neural Network (NN)-based classifiers on a selected number of features for ransomware classification. We performed all the experiments on one ransomware dataset to evaluate our proposed framework. The experimental results demonstrate that RF classifiers outperform other methods in terms of accuracy, F-beta, and precision scores. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00894
The Professionalization of the Hacker Industry,Tyson Brooks,"Society is inextricably dependent on the Internet and other globally interconnected infrastructures used in the provisioning of information services. The growth of information technology (IT) and information systems (IS) over the past decades has created an unprecedented demand for access to information. The implication of wireless mobility are great, and the commercial possibilities of new and innovative wireless flexibility are just beginning to be realized through the emergence of the Internet of Things (IoT). This article takes a look the history of hacking and professionalization of the hacker industry. As the hacker industry becomes more fully professionalized, it is becoming much more adaptive and flexible, making it harder for intelligence and law enforcement to confront. Furthermore, the hacker industry is blurring the distinction between motivated crime and traditional computer security threats - including the disruption of critical infrastructures or the penetration of networks. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00890
Computer-assisted Pronunciation Training -- Speech synthesis is almost all you need,Daniel Korzekwa;Jaime Lorenzo-Trueba;Thomas Drugman;Bozena Kostek,"The research community has long studied computer-assisted pronunciation training (CAPT) methods in non-native speech. Researchers focused on studying various model architectures, such as Bayesian networks and deep learning methods, as well as on the analysis of different representations of the speech signal. Despite significant progress in recent years, existing CAPT methods are not able to detect pronunciation errors with high accuracy (only 60\% precision at 40\%-80\% recall). One of the key problems is the low availability of mispronounced speech that is needed for the reliable training of pronunciation error detection models. If we had a generative model that could mimic non-native speech and produce any amount of training data, then the task of detecting pronunciation errors would be much easier. We present three innovative techniques based on phoneme-to-phoneme (P2P), text-to-speech (T2S), and speech-to-speech (S2S) conversion to generate correctly pronounced and mispronounced synthetic speech. We show that these techniques not only improve the accuracy of three machine learning models for detecting pronunciation errors but also help establish a new state-of-the-art in the field. Earlier studies have used simple speech generation techniques such as P2P conversion, but only as an additional mechanism to improve the accuracy of pronunciation error detection. We, on the other hand, consider speech generation to be the first-class method of detecting pronunciation errors. The effectiveness of these techniques is assessed in the tasks of detecting pronunciation and lexical stress errors. Non-native English speech corpora of German, Italian, and Polish speakers are used in the evaluations. The best proposed S2S technique improves the accuracy of detecting pronunciation errors in AUC metric by 41\% from 0.528 to 0.749 compared to the state-of-the-art approach. △ Less","2 July, 2022",https://arxiv.org/pdf/2207.00774
Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift,Wenao Ma;Cheng Chen;Shuang Zheng;Jing Qin;Huimao Zhang;Qi Dou,"Class distribution plays an important role in learning deep classifiers. When the proportion of each class in the test set differs from the training set, the performance of classification nets usually degrades. Such a label distribution shift problem is common in medical diagnosis since the prevalence of disease vary over location and time. In this paper, we propose the first method to tackle label shift for medical image classification, which effectively adapt the model learned from a single training label distribution to arbitrary unknown test label distribution. Our approach innovates distribution calibration to learn multiple representative classifiers, which are capable of handling different one-dominating-class distributions. When given a test image, the diverse classifiers are dynamically aggregated via the consistency-driven test-time adaptation, to deal with the unknown test label distribution. We validate our method on two important medical image classification tasks including liver fibrosis staging and COVID-19 severity prediction. Our experiments clearly show the decreased model performance under label shift. With our method, model performance significantly improves on all the test datasets with different label shifts for both medical image diagnosis tasks. △ Less","9 July, 2022",https://arxiv.org/pdf/2207.00769
FAIR principles for AI models with a practical application for accelerated high energy diffraction microscopy,Nikil Ravi;Pranshu Chaturvedi;E. A. Huerta;Zhengchun Liu;Ryan Chard;Aristana Scourtas;K. J. Schmidt;Kyle Chard;Ben Blaiszik;Ian Foster,"A concise and measurable set of FAIR (Findable, Accessible, Interoperable and Reusable) principles for scientific data is transforming the state-of-practice for data management and stewardship, supporting and enabling discovery and innovation. Learning from this initiative, and acknowledging the impact of artificial intelligence (AI) in the practice of science and engineering, we introduce a set of practical, concise, and measurable FAIR principles for AI models. We showcase how to create and share FAIR data and AI models within a unified computational framework combining the following elements: the Advanced Photon Source at Argonne National Laboratory, the Materials Data Facility, the Data and Learning Hub for Science, and funcX, and the Argonne Leadership Computing Facility (ALCF), in particular the ThetaGPU supercomputer and the SambaNova DataScale system at the ALCF AI Testbed. We describe how this domain-agnostic computational framework may be harnessed to enable autonomous AI-driven discovery. △ Less","21 December, 2022",https://arxiv.org/pdf/2207.00611
Predicting Ulnar Collateral Ligament Injury in Rookie Major League Baseball Pitchers,Sean A. Rendar;Fenglong Ma,"In the growing world of machine learning and data analytics, scholars are finding new and innovative ways to solve real-world problems. One solution comes by way of an intersection between healthcare, sports statistics, and data sciences. Within the realm of Major League Baseball (MLB), pitchers are regarded as the most important roster position. They often are among the highest paid players and are crucial to a franchise's success, but they are more at risk to suffer an injury that sidelines them for over a complete season. The ulnar collateral ligament (UCL) is a small ligament in the elbow that controls the strength and stability of a pitcher's throwing arm. Due to repetitive strain, it is not uncommon for pitchers to tear it partially or completely during their careers. Repairing this injury requires UCL reconstruction surgery, as known informally as Tommy John surgery. In this podium abstract, we want to investigate whether we can use machine learning techniques to predict UCL injury by analyzing online pitcher data. △ Less","30 June, 2022",https://arxiv.org/pdf/2207.00585
A Functional Architecture for 6G Special Purpose Industrial IoT Networks,{Nurul Huda Mahmood;Gilberto Berardinelli;Emil J. Khatib;Ramin Hashemi;Carlos de Lima;Matti Latva-aho,"Future industrial applications will encompass compelling new use cases requiring stringent performance guarantees over multiple key performance indicators (KPI) such as reliability, dependability, latency, time synchronization, security, etc. Achieving such stringent and diverse service requirements necessitates the design of a special-purpose Industrial Internet of Things (IIoT) network comprising a multitude of specialized functionalities and technological enablers. This article proposes an innovative architecture for such a special-purpose 6G IIoT network incorporating seven functional building blocks categorized into: special-purpose functionalities and enabling technologies. The former consists of Wireless Environment Control, Traffic/Channel Prediction, Proactive Resource Management and End-to-End Optimization functions; whereas the latter includes Synchronization and Coordination, Machine Learning and Artificial Intelligence Algorithms, and Auxiliary Functions. The proposed architecture aims at providing a resource-efficient and holistic solution for the complex and dynamically challenging requirements imposed by future 6G industrial use cases. Selected test scenarios are provided and assessed to illustrate cross-functional collaboration and demonstrate the applicability of the proposed architecture in a wireless IIoT network. △ Less","1 July, 2022",https://arxiv.org/pdf/2207.00264
"Designs, Motion Mechanism, Motion Coordination, and Communication of Bionic Robot Fishes: A Survey",Zhiwei Yu;Kai Li;Yu Ji;Simon X. Yang,"In the last few years, there have been many new developments and significant accomplishments in the research of bionic robot fishes. However, in terms of swimming performance, existing bionic robot fishes lag far behind fish, prompting researchers to constantly develop innovative designs of various bionic robot fishes. In this paper, the latest designs of robot fishes are presented in detail, distinguished by the propulsion mode. New robot fishes mainly include soft robot fishes and rigid-soft coupled robot fishes. The latest progress in the study of the swimming mechanism is analyzed on the basis of summarizing the main swimming theories of fish. The current state-of-the-art research in the new field of motion coordination and communication of multiple robot fishes is summarized. The general research trend in robot fishes is to utilize more efficient and robust methods to best mimic real fish while exhibiting superior swimming performance. The current challenges and potential future research directions are discussed. Various methods are needed to narrow the gap in swimming performance between robot fishes and fish. This paper is a first step to bring together roboticists and marine biologists interested in learning state-of-the-art research on bionic robot fishes. △ Less","30 June, 2022",https://arxiv.org/pdf/2206.15304
Weighted ensemble: Recent mathematical developments,D. Aristoff;J. Copperman;G. Simpson;R. J. Webber;D. M. Zuckerman,"The weighted ensemble (WE) method, an enhanced sampling approach based on periodically replicating and pruning trajectories in a set of parallel simulations, has grown increasingly popular for computational biochemistry problems, due in part to improved hardware and the availability of modern software. Algorithmic and analytical improvements have also played an important role, and progress has accelerated in recent years. Here, we discuss and elaborate on the WE method from a mathematical perspective, highlighting recent results which have begun to yield greater computational efficiency. Notable among these innovations are variance reduction approaches that optimize trajectory management for systems of arbitrary dimensionality. △ Less","29 June, 2022",https://arxiv.org/pdf/2206.14943
ITLingo Research Initiative in 2022,Alberto Rodrigues da Silva,"Several surveys and studies have noticed that cost, and quality problems result from mistakes that occurred in the early phases of the projects, for instance: poor definition of the project vision and respective value for the organization; misalignment between IT and business resources; failures in project management practices; or poor and low-quality technical documentation. These facts have emphasized the need for improving socio-technical disciplines such as project management, enterprise architecture, requirements engineering, or system design. These studies also noted the need to reduce the efforts involved in traditional development processes, for example, by automating some human-intensive and error-prone tasks. This article presents the scientific project we have conducted in these last years named ""ITLingo"". ITLingo is a research initiative that has proposed new languages, tools, and techniques to support users to improve such practices, mainly related to those disciplines. ITLingo users are IT engineers and managers in multiple roles like project managers, enterprise architects, business analysts, system architects, requirements engineers, or even system developers. The article describes the innovative nature of the activities carried out under the ITLingo umbrella and identifies future and open challenges. △ Less","16 June, 2022",https://arxiv.org/pdf/2206.14553
Interrelate Training and Searching: A Unified Online Clustering Framework for Speaker Diarization,Yifan Chen;Yifan Guo;Qingxuan Li;Gaofeng Cheng;Pengyuan Zhang;Yonghong Yan,"For online speaker diarization, samples arrive incrementally, and the overall distribution of the samples is invisible. Moreover, in most existing clustering-based methods, the training objective of the embedding extractor is not designed specially for clustering. To improve online speaker diarization performance, we propose a unified online clustering framework, which provides an interactive manner between embedding extractors and clustering algorithms. Specifically, the framework consists of two highly coupled parts: clustering-guided recurrent training (CGRT) and truncated beam searching clustering (TBSC). The CGRT introduces the clustering algorithm into the training process of embedding extractors, which could provide not only cluster-aware information for the embedding extractor, but also crucial parameters for the clustering process afterward. And with these parameters, which contain preliminary information of the metric space, the TBSC penalizes the probability score of each cluster, in order to output more accurate clustering results in online fashion with low latency. With the above innovations, our proposed online clustering system achieves 14.48\% DER with collar 0.25 at 2.5s latency on the AISHELL-4, while the DER of the offline agglomerative hierarchical clustering is 14.57\%. △ Less","28 June, 2022",https://arxiv.org/pdf/2206.13760
DigiTac: A DIGIT-TacTip Hybrid Tactile Sensor for Comparing Low-Cost High-Resolution Robot Touch,Nathan F. Lepora;Yijiong Lin;Ben Money-Coomes;John Lloyd,"Deep learning combined with high-resolution tactile sensing could lead to highly capable dexterous robots. However, progress is slow because of the specialist equipment and expertise. The DIGIT tactile sensor offers low-cost entry to high-resolution touch using GelSight-type sensors. Here we customize the DIGIT to have a 3D-printed sensing surface based on the TacTip family of soft biomimetic optical tactile sensors. The DIGIT-TacTip (DigiTac) enables direct comparison between these distinct tactile sensor types. For this comparison, we introduce a tactile robot system comprising a desktop arm, mounts and 3D-printed test objects. We use tactile servo control with a PoseNet deep learning model to compare the DIGIT, DigiTac and TacTip for edge- and surface-following over 3D-shapes. All three sensors performed similarly at pose prediction, but their constructions led to differing performances at servo control, offering guidance for researchers selecting or innovating tactile sensors. All hardware and software for reproducing this study will be openly released. Project website: www.lepora.com/digitac. Project repository: www.github.com/nlepora/digitac-design. △ Less","18 July, 2022",https://arxiv.org/pdf/2206.13657
Reduced Optimal Power Flow Using Graph Neural Network,Thuan Pham;Xingpeng Li,"OPF problems are formulated and solved for power system operations, especially for determining generation dispatch points in real-time. For large and complex power system networks with large numbers of variables and constraints, finding the optimal solution for real-time OPF in a timely manner requires a massive amount of computing power. This paper presents a new method to reduce the number of constraints in the original OPF problem using a graph neural network (GNN). GNN is an innovative machine learning model that utilizes features from nodes, edges, and network topology to maximize its performance. In this paper, we proposed a GNN model to predict which lines would be heavily loaded or congested with given load profiles and generation capacities. Only these critical lines will be monitored in an OPF problem, creating a reduced OPF (ROPF) problem. Significant saving in computing time is expected from the proposed ROPF model. A comprehensive analysis of predictions from the GNN model was also made. It is concluded that the application of GNN for ROPF is able to reduce computing time while retaining solution quality. △ Less","27 June, 2022",https://arxiv.org/pdf/2206.13591
Robustness Implies Generalization via Data-Dependent Generalization Bounds,Kenji Kawaguchi;Zhun Deng;Kyle Luh;Jiaoyang Huang,"This paper proves that robustness implies generalization via data-dependent generalization bounds. As a result, robustness and generalization are shown to be connected closely in a data-dependent manner. Our bounds improve previous bounds in two directions, to solve an open problem that has seen little development since 2010. The first is to reduce the dependence on the covering number. The second is to remove the dependence on the hypothesis space. We present several examples, including ones for lasso and deep learning, in which our bounds are provably preferable. The experiments on real-world data and theoretical models demonstrate near-exponential improvements in various situations. To achieve these improvements, we do not require additional assumptions on the unknown distribution; instead, we only incorporate an observable and computable property of the training samples. A key technical innovation is an improved concentration bound for multinomial random variables that is of independent interest beyond robustness and generalization. △ Less","3 August, 2022",https://arxiv.org/pdf/2206.13497
EMVLight: a Multi-agent Reinforcement Learning Framework for an Emergency Vehicle Decentralized Routing and Traffic Signal Control System,Haoran Su;Yaofeng D. Zhong;Joseph Y. J. Chow;Biswadip Dey;Li Jin,"Emergency vehicles (EMVs) play a crucial role in responding to time-critical calls such as medical emergencies and fire outbreaks in urban areas. Existing methods for EMV dispatch typically optimize routes based on historical traffic-flow data and design traffic signal pre-emption accordingly; however, we still lack a systematic methodology to address the coupling between EMV routing and traffic signal control. In this paper, we propose EMVLight, a decentralized reinforcement learning (RL) framework for joint dynamic EMV routing and traffic signal pre-emption. We adopt the multi-agent advantage actor-critic method with policy sharing and spatial discounted factor. This framework addresses the coupling between EMV navigation and traffic signal control via an innovative design of multi-class RL agents and a novel pressure-based reward function. The proposed methodology enables EMVLight to learn network-level cooperative traffic signal phasing strategies that not only reduce EMV travel time but also shortens the travel time of non-EMVs. Simulation-based experiments indicate that EMVLight enables up to a 42.6\% reduction in EMV travel time as well as an 23.5\% shorter average travel time compared with existing approaches. △ Less","29 June, 2022",https://arxiv.org/pdf/2206.13441
Implementing a Chatbot Solution for Learning Management System,Dimitrios Chaskopoulos;Jonas Eilertsen Hægdahl;Petter Sagvold;Claire Trinquet;Maryam Edalati,"Innovation is a key component in trying new solutions for the students to learn efficiently and in ways that correspond to their own experience, where chatbots are one of these new solutions. One of the main problem that chatbots face today is to mimic human language, where they try to find the best answer to an input, which is not how a human conversation usually works, rather taking into account the previous messages and building onto them. Extreme programming methodology was chosen to use integrate ChatterBot, Pyside2, web scraping and Tampermonkey into Blackboard as a test case. Problems occurred with the bot and more training was needed for the bot to work perfectly, but the integration and web scraping worked, giving us a chatbot that was able to talk with. We showed the plausibility of integrating an AI bot in an educational setting. △ Less","30 June, 2022",https://arxiv.org/pdf/2206.13187
Unified BERT for Few-shot Natural Language Understanding,Junyu Lu;Ping Yang;Ruyi Gan;Jing Yang;Jiaxing Zhang,"Even as pre-trained language models share a semantic encoder, natural language understanding suffers from a diversity of output schemas. In this paper, we propose UBERT, a unified bidirectional language understanding model based on BERT framework, which can universally model the training objects of different NLU tasks through a biaffine network. Specifically, UBERT encodes prior knowledge from various aspects, uniformly constructing learning representations across multiple NLU tasks, which is conducive to enhancing the ability to capture common semantic understanding. By using the biaffine to model scores pair of the start and end position of the original text, various classification and extraction structures can be converted into a universal, span-decoding approach. Experiments show that UBERT wins the first price in the 2022 AIWIN - World Artificial Intelligence Innovation Competition, Chinese insurance few-shot multi-task track, and realizes the unification of extensive information extraction and linguistic reasoning tasks. △ Less","13 August, 2022",https://arxiv.org/pdf/2206.12094
"Cooperative Hybrid Networks with Active Relays and RISs for B5G: Applications, Challenges, and Research Directions",Zaid Abdullah;Steven Kisseleff;Wallace Alves Martins;Gaojie Chen;Luca Sanguinetti;Konstantinos Ntontin;Anastasios Papazafeiropoulos;Symeon Chatzinotas;Bjorn Ottersten,"Among the recent advances and innovations in wireless technologies, reconfigurable intelligent surfaces (RISs) have received much attention and are envisioned to be one of the enabling technologies for beyond 5G (B5G) networks. On the other hand, active (or classical) cooperative relays have played a key role in providing reliable and power-efficient communications in previous wireless generations. In this article, we focus on hybrid network architectures that amalgamate both active relays and RISs. The operation concept and protocols of each technology are first discussed. Subsequently, we present multiple use cases of cooperative hybrid networks where both active relays and RISs can coexist harmoniously for enhanced rate performance. Furthermore, a case study is provided which demonstrates the achievable rate performance of a communication network assisted by either an active relay, an RIS, or both, and with different relaying protocols. Finally, we provide the reader with the challenges and key research directions in this area. △ Less","17 October, 2022",https://arxiv.org/pdf/2206.11707
Rejection-proof Kidney Exchange Mechanisms,Danny Blom;Bart Smeulders;Frits C. R. Spieksma,"Kidney exchange programs (KEPs) form an innovative approach to increasing the donor pool through allowing the participation of renal patients together with a willing but incompatible donor. The aim of a KEP is to identify groups of incompatible donor-recipient pairs that could exchange donors leading to feasible transplants. As the size of a kidney exchange grows, a larger proportion of participants can be transplanted. Collaboration between multiple transplant centers, by merging their separate kidney exchange pools is thus desirable. As each transplant center has its own interest to provide the best care to its own patients, collaboration requires balancing individual and common objectives. We consider a class of algorithmic mechanisms for multi-center kidney exchange programs we call rejection-proof mechanisms. Such mechanisms propose solutions with the property that no player wishes to unilaterally deviate. We provide a mechanism optimizing social value under this restriction, though the underlying optimization problem is Sigma-2-p-Hard. We also describe a computationally easier but sub-optimal alternative. Experiments show that rejection-proofness can be achieved at limited cost compared to optimal solutions for regular kidney exchange. Computationally, we provide algorithms to compute optimal rejection-proof solutions for small and medium instance sizes. △ Less","23 June, 2022",https://arxiv.org/pdf/2206.11525
A novel adversarial learning strategy for medical image classification,Zong Fan;Xiaohui Zhang;Jacob A. Gasienica;Jennifer Potts;Su Ruan;Wade Thorstad;Hiram Gay;Pengfei Song;Xiaowei Wang;Hua Li,"Deep learning (DL) techniques have been extensively utilized for medical image classification. Most DL-based classification networks are generally structured hierarchically and optimized through the minimization of a single loss function measured at the end of the networks. However, such a single loss design could potentially lead to optimization of one specific value of interest but fail to leverage informative features from intermediate layers that might benefit classification performance and reduce the risk of overfitting. Recently, auxiliary convolutional neural networks (AuxCNNs) have been employed on top of traditional classification networks to facilitate the training of intermediate layers to improve classification performance and robustness. In this study, we proposed an adversarial learning-based AuxCNN to support the training of deep neural networks for medical image classification. Two main innovations were adopted in our AuxCNN classification framework. First, the proposed AuxCNN architecture includes an image generator and an image discriminator for extracting more informative image features for medical image classification, motivated by the concept of generative adversarial network (GAN) and its impressive ability in approximating target data distribution. Second, a hybrid loss function is designed to guide the model training by incorporating different objectives of the classification network and AuxCNN to reduce overfitting. Comprehensive experimental studies demonstrated the superior classification performance of the proposed model. The effect of the network-related factors on classification performance was investigated. △ Less","7 July, 2022",https://arxiv.org/pdf/2206.11501
Motion Gait: Gait Recognition via Motion Excitation,Yunpeng Zhang;Zhengyou Wang;Shanna Zhuang;Hui Wang,"Gait recognition, which can realize long-distance and contactless identification, is an important biometric technology. Recent gait recognition methods focus on learning the pattern of human movement or appearance during walking, and construct the corresponding spatio-temporal representations. However, different individuals have their own laws of movement patterns, simple spatial-temporal features are difficult to describe changes in motion of human parts, especially when confounding variables such as clothing and carrying are included, thus distinguishability of features is reduced. In this paper, we propose the Motion Excitation Module (MEM) to guide spatio-temporal features to focus on human parts with large dynamic changes, MEM learns the difference information between frames and intervals, so as to obtain the representation of temporal motion changes, it is worth mentioning that MEM can adapt to frame sequences with uncertain length, and it does not add any additional parameters. Furthermore, we present the Fine Feature Extractor (FFE), which independently learns the spatio-temporal representations of human body according to different horizontal parts of individuals. Benefiting from MEM and FFE, our method innovatively combines motion change information, significantly improving the performance of the model under cross appearance conditions. On the popular dataset CASIA-B, our proposed Motion Gait is better than the existing gait recognition methods. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11080
AI Challenges for Society and Ethics,Jess Whittlestone;Sam Clarke,"Artificial intelligence is already being applied in and impacting many important sectors in society, including healthcare, finance, and policing. These applications will increase as AI capabilities continue to progress, which has the potential to be highly beneficial for society, or to cause serious harm. The role of AI governance is ultimately to take practical steps to mitigate this risk of harm while enabling the benefits of innovation in AI. This requires answering challenging empirical questions about current and potential risks and benefits of AI: assessing impacts that are often widely distributed and indirect, and making predictions about a highly uncertain future. It also requires thinking through the normative question of what beneficial use of AI in society looks like, which is equally challenging. Though different groups may agree on high-level principles that uses of AI should respect (e.g., privacy, fairness, and autonomy), challenges arise when putting these principles into practice. For example, it is straightforward to say that AI systems must protect individual privacy, but there is presumably some amount or type of privacy that most people would be willing to give up to develop life-saving medical treatments. Despite these challenges, research can and has made progress on these questions. The aim of this chapter will be to give readers an understanding of this progress, and of the challenges that remain. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.11068
Estimation of Electric Vehicle Public Charging Demand using Cellphone Data and Points of Interest-based Segmentation,Victor Radermecker;Lieselot Vanhaverbeke,"The race for road electrification has started, and convincing drivers to switch from fuel-powered vehicles to electric vehicles requires robust Electric Vehicle (EV) charging infrastructure. This article proposes an innovative EV charging demand estimation and segmentation method. First, we estimate the charging demand at a neighborhood granularity using cellular signaling data. Second, we propose a segmentation model to partition the total charging needs among different charging technology: normal, semi-rapid, and fast charging. The segmentation model, an approach based on the city's points of interest, is a state-of-the-art method that derives useful trends applicable to city planning. A case study for the city of Brussels is proposed. △ Less","2 June, 2022",https://arxiv.org/pdf/2206.11065
Template-based Approach to Zero-shot Intent Recognition,Dmitry Lamanov;Pavel Burnyshev;Ekaterina Artemova;Valentin Malykh;Andrey Bout;Irina Piontkovskaya,"The recent advances in transfer learning techniques and pre-training of large contextualized encoders foster innovation in real-life applications, including dialog assistants. Practical needs of intent recognition require effective data usage and the ability to constantly update supported intents, adopting new ones, and abandoning outdated ones. In particular, the generalized zero-shot paradigm, in which the model is trained on the seen intents and tested on both seen and unseen intents, is taking on new importance. In this paper, we explore the generalized zero-shot setup for intent recognition. Following best practices for zero-shot text classification, we treat the task with a sentence pair modeling approach. We outperform previous state-of-the-art f1-measure by up to 16\% for unseen intents, using intent labels and user utterances and without accessing external sources (such as knowledge bases). Further enhancement includes lexicalization of intent labels, which improves performance by up to 7\%. By using task transferring from other sentence pair tasks, such as Natural Language Inference, we gain additional improvements. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.10914
A Feature Memory Rearrangement Network for Visual Inspection of Textured Surface Defects Toward Edge Intelligent Manufacturing,Haiming Yao;Wenyong Yu;Xue Wang,"Recent advances in the industrial inspection of textured surfaces-in the form of visual inspection-have made such inspections possible for efficient, flexible manufacturing systems. We propose an unsupervised feature memory rearrangement network (FMR-Net) to accurately detect various textural defects simultaneously. Consistent with mainstream methods, we adopt the idea of background reconstruction; however, we innovatively utilize artificial synthetic defects to enable the model to recognize anomalies, while traditional wisdom relies only on defect-free samples. First, we employ an encoding module to obtain multiscale features of the textured surface. Subsequently, a contrastive-learning-based memory feature module (CMFM) is proposed to obtain discriminative representations and construct a normal feature memory bank in the latent space, which can be employed as a substitute for defects and fast anomaly scores at the patch level. Next, a novel global feature rearrangement module (GFRM) is proposed to further suppress the reconstruction of residual defects. Finally, a decoding module utilizes the restored features to reconstruct the normal texture background. In addition, to improve inspection performance, a two-phase training strategy is utilized for accurate defect restoration refinement, and we exploit a multimodal inspection method to achieve noise-robust defect localization. We verify our method through extensive experiments and test its practical deployment in collaborative edge--cloud intelligent manufacturing scenarios by means of a multilevel detection method, demonstrating that FMR-Net exhibits state-of-the-art inspection accuracy and shows great potential for use in edge-computing-enabled smart industries. △ Less","22 June, 2022",https://arxiv.org/pdf/2206.10830
A method for ethical AI in Defence: A case study on developing trustworthy autonomous systems,Tara Roberson;Stephen Bornstein;Rain Liivoja;Simon Ng;Jason Scholz;S. Kate Devitt,"What does it mean to be responsible and responsive when developing and deploying trusted autonomous systems in Defence? In this short reflective article, we describe a case study of building a trusted autonomous system - Athena AI - within an industry-led, government-funded project with diverse collaborators and stakeholders. Using this case study, we draw out lessons on the value and impact of embedding responsible research and innovation-aligned, ethics-by-design approaches and principles throughout the development of technology at high translation readiness levels. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10769
CoCoPIE XGen: A Full-Stack AI-Oriented Optimizing Framework,Xiaofeng Li;Bin Ren;Xipeng Shen;Yanzhi Wang,"There is a growing demand for shifting the delivery of AI capability from data centers on the cloud to edge or end devices, exemplified by the fast emerging real-time AI-based apps running on smartphones, AR/VR devices, autonomous vehicles, and various IoT devices. The shift has however been seriously hampered by the large growing gap between DNN computing demands and the computing power on edge or end devices. This article presents the design of XGen, an optimizing framework for DNN designed to bridge the gap. XGen takes cross-cutting co-design as its first-order consideration. Its full-stack AI-oriented optimizations consist of a number of innovative optimizations at every layer of the DNN software stack, all designed in a cooperative manner. The unique technology makes XGen able to optimize various DNNs, including those with an extreme depth (e.g., BERT, GPT, other transformers), and generate code that runs several times faster than those from existing DNN frameworks, while delivering the same level of accuracy. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10620
Faster Diffusion Cardiac MRI with Deep Learning-based breath hold reduction,Michael Tanzer;Pedro Ferreira;Andrew Scott;Zohya Khalique;Maria Dwornik;Dudley Pennell;Guang Yang;Daniel Rueckert;Sonia Nielles-Vallespin,"Diffusion Tensor Cardiac Magnetic Resonance (DT-CMR) enables us to probe the microstructural arrangement of cardiomyocytes within the myocardium in vivo and non-invasively, which no other imaging modality allows. This innovative technology could revolutionise the ability to perform cardiac clinical diagnosis, risk stratification, prognosis and therapy follow-up. However, DT-CMR is currently inefficient with over six minutes needed to acquire a single 2D static image. Therefore, DT-CMR is currently confined to research but not used clinically. We propose to reduce the number of repetitions needed to produce DT-CMR datasets and subsequently de-noise them, decreasing the acquisition time by a linear factor while maintaining acceptable image quality. Our proposed approach, based on Generative Adversarial Networks, Vision Transformers, and Ensemble Learning, performs significantly and considerably better than previous proposed approaches, bringing single breath-hold DT-CMR closer to reality. △ Less","21 June, 2022",https://arxiv.org/pdf/2206.10543
German AI Start-Ups and AI Ethics: Using A Social Practice Lens for Assessing and Implementing Socio-Technical Innovation,Mona Sloane;Janina Zakrzewski,"Within the current AI ethics discourse, there is a gap in empirical research on understanding how AI practitioners understand ethics and socially organize to operationalize ethical concerns, particularly in the context of AI start-ups. This gap intensifies the risk of a disconnect between scholarly research, innovation, and application. This risk materializes acutely as mounting pressures to identify and mitigate the potential harms of AI systems have created an urgent need to assess and implement socio-technical innovation for fairness, accountability, and transparency. Building on social practice theory, we address this need via a framework that allows AI researchers, practitioners, and regulators to systematically analyze existing cultural understandings, histories, and social practices of ethical AI to define appropriate strategies for effectively implementing socio-technical innovations. Our contributions are threefold: 1) we introduce a practice-based approach for understanding ethical AI; 2) we present empirical findings from our study on the operationalization of ethics in German AI start-ups to underline that AI ethics and social practices must be understood in their specific cultural and historical contexts; and 3) based on our empirical findings, we suggest that ethical AI practices can be broken down into principles, needs, narratives, materializations, and cultural genealogies to form a useful backdrop for considering socio-technical innovations. △ Less","20 June, 2022",https://arxiv.org/pdf/2206.09978
The Cost of the GDPR for Apps? Nearly Impossible to Study without Platform Data,Konrad Kollnig;Reuben Binns,"A recently published pre-print titled 'GDPR and the Lost Generation of Innovative Apps' by Janßen et al. observes that a third of apps on the Google Play Store disappeared from this app store around the introduction of the GDPR in May 2018. The authors deduce 'that GDPR is the cause'. The effects of the GDPR on the app economy are an important field to study. Unfortunately, the paper currently lacks a control condition and a key variable. As a result, the effects on app exits reported in the paper are likely overestimated, as we will discuss. We believe there are other factors which may better explain these changes in the Play Store aside from the GDPR. △ Less","20 June, 2022",https://arxiv.org/pdf/2206.09734
A Simple Proof of PreciseQMA = PSPACE,Yulong Li,"We give an alternative proof of PreciseQMA = PSPACE, first proved by Fefferman and Lin (Innov. Theor. Comp. Sci. 2018), where PreciseQMA is the class Quantum Merlin-Arthur with inverse exponential completeness-soundness gap. We adapt the proof of Quantum Cook-Levin Theorem to prove the inclusion PSPACE in PreciseQMA.","18 June, 2022",https://arxiv.org/pdf/2206.09230
Free-form Lesion Synthesis Using a Partial Convolution Generative Adversarial Network for Enhanced Deep Learning Liver Tumor Segmentation,Yingao Liu;Fei Yang;Yidong Yang,"Automatic deep learning segmentation models has been shown to improve both the segmentation efficiency and the accuracy. However, training a robust segmentation model requires considerably large labeled training samples, which may be impractical. This study aimed to develop a deep learning framework for generating synthetic lesions that can be used to enhance network training. The lesion synthesis network is a modified generative adversarial network (GAN). Specifically, we innovated a partial convolution strategy to construct an Unet-like generator. The discriminator is designed using Wasserstein GAN with gradient penalty and spectral normalization. A mask generation method based on principal component analysis was developed to model various lesion shapes. The generated masks are then converted into liver lesions through a lesion synthesis network. The lesion synthesis framework was evaluated for lesion textures, and the synthetic lesions were used to train a lesion segmentation network to further validate the effectiveness of this framework. All the networks are trained and tested on the public dataset from LITS. The synthetic lesions generated by the proposed approach have very similar histogram distributions compared to the real lesions for the two employed texture parameters, GLCM-energy and GLCM-correlation. The Kullback-Leibler divergence of GLCM-energy and GLCM-correlation were 0.01 and 0.10, respectively. Including the synthetic lesions in the tumor segmentation network improved the segmentation dice performance of U-Net significantly from 67.3% to 71.4% (p<0.05). Meanwhile, the volume precision and sensitivity improve from 74.6% to 76.0% (p=0.23) and 66.1% to 70.9% (p<0.01), respectively. The synthetic data significantly improves the segmentation performance. △ Less","25 October, 2022",https://arxiv.org/pdf/2206.09065
Truly Unordered Probabilistic Rule Sets for Multi-class Classification,Lincen Yang;Matthijs van Leeuwen,"Rule set learning has long been studied and has recently been frequently revisited due to the need for interpretable models. Still, existing methods have several shortcomings: 1) most recent methods require a binary feature matrix as input, while learning rules directly from numeric variables is understudied; 2) existing methods impose orders among rules, either explicitly or implicitly, which harms interpretability; and 3) currently no method exists for learning probabilistic rule sets for multi-class target variables (there is only one for probabilistic rule lists). We propose TURS, for Truly Unordered Rule Sets, which addresses these shortcomings. We first formalize the problem of learning truly unordered rule sets. To resolve conflicts caused by overlapping rules, i.e., instances covered by multiple rules, we propose a novel approach that exploits the probabilistic properties of our rule sets. We next develop a two-phase heuristic algorithm that learns rule sets by carefully growing rules. An important innovation is that we use a surrogate score to take the global potential of the rule set into account when learning a local rule. Finally, we empirically demonstrate that, compared to non-probabilistic and (explicitly or implicitly) ordered state-of-the-art methods, our method learns rule sets that not only have better interpretability but also better predictive performance. △ Less","18 July, 2022",https://arxiv.org/pdf/2206.08804
A Numerical Reasoning Question Answering System with Fine-grained Retriever and the Ensemble of Multiple Generators for FinQA,Bin Wang;Jiangzhou Ju;Yunlin Mao;Xin-Yu Dai;Shujian Huang;Jiajun Chen,"The numerical reasoning in the financial domain -- performing quantitative analysis and summarizing the information from financial reports -- can greatly increase business efficiency and reduce costs of billions of dollars. Here, we propose a numerical reasoning question answering system to answer numerical reasoning questions among financial text and table data sources, consisting of a retriever module, a generator module, and an ensemble module. Specifically, in the retriever module, in addition to retrieving the whole row data, we innovatively design a cell retriever that retrieves the gold cells to avoid bringing unrelated and similar cells in the same row to the inputs of the generator module. In the generator module, we utilize multiple generators to produce programs, which are operation steps to answer the question. Finally, in the ensemble module, we integrate multiple programs to choose the best program as the output of our system. In the final private test set in FinQA Competition, our system obtains 69.79 execution accuracy. △ Less","16 June, 2022",https://arxiv.org/pdf/2206.08506
Global Convergence of Federated Learning for Mixed Regression,Lili Su;Jiaming Xu;Pengkun Yang,"This paper studies the problem of model training under Federated Learning when clients exhibit cluster structure. We contextualize this problem in mixed regression, where each client has limited local data generated from one of k unknown regression models. We design an algorithm that achieves global convergence from any initialization, and works even when local data volume is highly unbalanced -- there could exist clients that contain O(1) data points only. Our algorithm first runs moment descent on a few anchor clients (each with \tildeΩ(k) data points) to obtain coarse model estimates. Then each client alternately estimates its cluster labels and refines the model estimates based on FedAvg or FedProx. A key innovation in our analysis is a uniform estimate on the clustering errors, which we prove by bounding the VC dimension of general polynomial concept classes based on the theory of algebraic geometry. △ Less","14 June, 2022",https://arxiv.org/pdf/2206.07279
TriHorn-Net: A Model for Accurate Depth-Based 3D Hand Pose Estimation,Mohammad Rezaei;Razieh Rastgoo;Vassilis Athitsos,"3D hand pose estimation methods have made significant progress recently. However, the estimation accuracy is often far from sufficient for specific real-world applications, and thus there is significant room for improvement. This paper proposes TriHorn-Net, a novel model that uses specific innovations to improve hand pose estimation accuracy on depth images. The first innovation is the decomposition of the 3D hand pose estimation into the estimation of 2D joint locations in the depth image space (UV), and the estimation of their corresponding depths aided by two complementary attention maps. This decomposition prevents depth estimation, which is a more difficult task, from interfering with the UV estimations at both the prediction and feature levels. The second innovation is PixDropout, which is, to the best of our knowledge, the first appearance-based data augmentation method for hand depth images. Experimental results demonstrate that the proposed model outperforms the state-of-the-art methods on three public benchmark datasets. Our implementation is available at https://github.com/mrezaei92/TriHorn-Net. △ Less","26 June, 2022",https://arxiv.org/pdf/2206.07117
Evaluating histopathology transfer learning with ChampKit,Jakub R. Kaczmarzyk;Tahsin M. Kurc;Shahira Abousamra;Rajarsi Gupta;Joel H. Saltz;Peter K. Koo,"Histopathology remains the gold standard for diagnosis of various cancers. Recent advances in computer vision, specifically deep learning, have facilitated the analysis of histopathology images for various tasks, including immune cell detection and microsatellite instability classification. The state-of-the-art for each task often employs base architectures that have been pretrained for image classification on ImageNet. The standard approach to develop classifiers in histopathology tends to focus narrowly on optimizing models for a single task, not considering the aspects of modeling innovations that improve generalization across tasks. Here we present ChampKit (Comprehensive Histopathology Assessment of Model Predictions toolKit): an extensible, fully reproducible benchmarking toolkit that consists of a broad collection of patch-level image classification tasks across different cancers. ChampKit enables a way to systematically document the performance impact of proposed improvements in models and methodology. ChampKit source code and data are freely accessible at https://github.com/kaczmarj/champkit . △ Less","14 June, 2022",https://arxiv.org/pdf/2206.06862
"Don't ""research fast and break things"": On the ethics of Computational Social Science",David Leslie,"This article is concerned with setting up practical guardrails within the research activities and environments of CSS. It aims to provide CSS scholars, as well as policymakers and other stakeholders who apply CSS methods, with the critical and constructive means needed to ensure that their practices are ethical, trustworthy, and responsible. It begins by providing a taxonomy of the ethical challenges faced by researchers in the field of CSS. These are challenges related to (1) the treatment of research subjects, (2) the impacts of CSS research on affected individuals and communities, (3) the quality of CSS research and to its epistemological status, (4) research integrity, and (5) research equity. Taking these challenges as a motivation for cultural transformation, it then argues for the end-to-end incorporation of habits of responsible research and innovation (RRI) into CSS practices, focusing on the role that contextual considerations, anticipatory reflection, impact assessment, public engagement, and justifiable and well-documented action should play across the research lifecycle. In proposing the inclusion of habits of RRI in CSS practices, the chapter lays out several practical steps needed for ethical, trustworthy, and responsible CSS research activities. These include stakeholder engagement processes, research impact assessments, data lifecycle documentation, bias self-assessments, and transparent research reporting protocols. △ Less","12 June, 2022",https://arxiv.org/pdf/2206.06370
Learning Generalized Wireless MAC Communication Protocols via Abstraction,Luciano Miuccio;Salvatore Riolo;Sumudu Samarakoon;Daniela Panno;Mehdi Bennis,"To tackle the heterogeneous requirements of beyond 5G (B5G) and future 6G wireless networks, conventional medium access control (MAC) procedures need to evolve to enable base stations (BSs) and user equipments (UEs) to automatically learn innovative MAC protocols catering to extremely diverse services. This topic has received significant attention, and several reinforcement learning (RL) algorithms, in which BSs and UEs are cast as agents, are available with the aim of learning a communication policy based on agents' local observations. However, current approaches are typically overfitted to the environment they are trained in, and lack robustness against unseen conditions, failing to generalize in different environments. To overcome this problem, in this work, instead of learning a policy in the high dimensional and redundant observation space, we leverage the concept of observation abstraction (OA) rooted in extracting useful information from the environment. This in turn allows learning communication protocols that are more robust and with much better generalization capabilities than current baselines. To learn the abstracted information from observations, we propose an architecture based on autoencoder (AE) and imbue it into a multi-agent proximal policy optimization (MAPPO) framework. Simulation results corroborate the effectiveness of leveraging abstraction when learning protocols by generalizing across environments, in terms of number of UEs, number of data packets to transmit, and channel conditions. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.06331
"Vildehaye: A Family of Versatile, Widely-Applicable, and Field-Proven Lightweight Wildlife Tracking and Sensing Tags",Sivan Toledo;Shai Mendel;Anat Levi;Yoni Vortman;Wiebke Ullmann;Lena-Rosa Scherer;Jan Pufelski;Frank van Maarseveen;Bas Denissen;Allert Bijleveld;Yotam Orchan;Yoav Bartan;Sivan Margalit;Idan Talmon;Ran Nathan,"We describe the design and implementation of Vildehaye, a family of versatile, widely-applicable, and field-proven tags for wildlife sensing and radio tracking. The family includes 6 distinct hardware designs for tags, 3 add-on boards, a programming adapter, and base stations; modular firmware for tags and base stations (both standalone low-power embedded base stations and base stations tethered to a computer running Linux or Windows); and desktop software for programming and configuring tags, monitoring tags, and downloading and processing sensor data. The tags are versatile: they support multiple packet formats, data rates, and frequency bands; they can be configured for minimum mass (down to less than 1g), making them applicable to a wide range of flying and terrestrial animals, or for inclusion of important sensors and large memories; they can transmit packets compatible with time-of-arrival transmitter-localization systems, tag identification and state packets, and they can reliably upload sensor data through their radio link. The system has been designed, upgraded, and maintained as an academic research project, but it has been extensively used by 5 different groups of ecologists in 4 countries over a period of 5 years. More than 7100 tags have been produced and most of these have been deployed. Production used 41 manufacturing runs. The tags have been used in studies that so far resulted in 9 scientific publications in ecology (including in Science). The paper describes innovative design aspects of Vildehaye, field-use experiences, and lessons from the design, implementation, and maintenance of the system. Both the hardware and software of the system are open. △ Less","4 May, 2022",https://arxiv.org/pdf/2206.06171
MLLess: Achieving Cost Efficiency in Serverless Machine Learning Training,Pablo Gimeno Sarroca;Marc Sánchez-Artigas,"Function-as-a-Service (FaaS) has raised a growing interest in how to ""tame"" serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional ""serverful"" computing. To help in this endeavor, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two innovative optimizations tailored to the traits of serverless computing: on one hand, a significance filter, to make indirect communication more effective, and on the other hand, a scale-in auto-tuner, to reduce cost by benefiting from the FaaS sub-second billing model (often per 100ms). Our results certify that MLLess can be 15X faster than serverful ML systems at a lower cost for sparse ML models that exhibit fast convergence such as sparse logistic regression and matrix factorization. Furthermore, our results show that MLLess can easily scale out to increasingly large fleets of serverless workers. △ Less","12 June, 2022",https://arxiv.org/pdf/2206.05786
DRAformer: Differentially Reconstructed Attention Transformer for Time-Series Forecasting,Benhan Li;Shengdong Du;Tianrui Li;Jie Hu;Zhen Jia,"Time-series forecasting plays an important role in many real-world scenarios, such as equipment life cycle forecasting, weather forecasting, and traffic flow forecasting. It can be observed from recent research that a variety of transformer-based models have shown remarkable results in time-series forecasting. However, there are still some issues that limit the ability of transformer-based models on time-series forecasting tasks: (i) learning directly on raw data is susceptible to noise due to its complex and unstable feature representation; (ii) the self-attention mechanisms pay insufficient attention to changing features and temporal dependencies. In order to solve these two problems, we propose a transformer-based differentially reconstructed attention model DRAformer. Specifically, DRAformer has the following innovations: (i) learning against differenced sequences, which preserves clear and stable sequence features by differencing and highlights the changing properties of sequences; (ii) the reconstructed attention: integrated distance attention exhibits sequential distance through a learnable Gaussian kernel, distributed difference attention calculates distribution difference by mapping the difference sequence to the adaptive feature space, and the combination of the two effectively focuses on the sequences with prominent associations; (iii) the reconstructed decoder input, which extracts sequence features by integrating variation information and temporal correlations, thereby obtaining a more comprehensive sequence representation. Extensive experiments on four large-scale datasets demonstrate that DRAformer outperforms state-of-the-art baselines. △ Less","11 June, 2022",https://arxiv.org/pdf/2206.05495
Monitoring and Proactive Management of QoS Levels in Pervasive Applications,Georgios Boulougaris;Kostas Kolomvatsos,"The advent of Edge Computing (EC) as a promising paradigm that provides multiple computation and analytics capabilities close to data sources opens new pathways for novel applications. Nonetheless, the limited computational capabilities of EC nodes and the expectation of ensuring high levels of QoS during tasks execution impose strict requirements for innovative management approaches. Motivated by the need of maintaining a minimum level of QoS during EC nodes functioning, we elaborate a distributed and intelligent decision-making approach for tasks scheduling. Our aim is to enhance the behavior of EC nodes making them capable of securing high QoS levels. We propose that nodes continuously monitor QoS levels and systematically evaluate the probability of violating them to proactively decide some tasks to be offloaded to peer nodes or Cloud. We present, describe and evaluate the proposed scheme through multiple experimental scenarios revealing its performance and the benefits of the envisioned monitoring mechanism when serving processing requests in very dynamic environments like the EC. △ Less","11 June, 2022",https://arxiv.org/pdf/2206.05478
Fast building segmentation from satellite imagery and few local labels,Caleb Robinson;Anthony Ortiz;Hogeun Park;Nancy Lozano Gracia;Jon Kher Kaw;Tina Sederholm;Rahul Dodhia;Juan M. Lavista Ferres,"Innovations in computer vision algorithms for satellite image analysis can enable us to explore global challenges such as urbanization and land use change at the planetary level. However, domain shift problems are a common occurrence when trying to replicate models that drive these analyses to new areas, particularly in the developing world. If a model is trained with imagery and labels from one location, then it usually will not generalize well to new locations where the content of the imagery and data distributions are different. In this work, we consider the setting in which we have a single large satellite imagery scene over which we want to solve an applied problem -- building footprint segmentation. Here, we do not necessarily need to worry about creating a model that generalizes past the borders of our scene but can instead train a local model. We show that surprisingly few labels are needed to solve the building segmentation problem with very high-resolution (0.5m/px) satellite imagery with this setting in mind. Our best model trained with just 527 sparse polygon annotations (an equivalent of 1500 x 1500 densely labeled pixels) has a recall of 0.87 over held out footprints and a R2 of 0.93 on the task of counting the number of buildings in 200 x 200-meter windows. We apply our models over high-resolution imagery in Amman, Jordan in a case study on urban change detection. △ Less","10 June, 2022",https://arxiv.org/pdf/2206.05377
Social Network Structure Shapes Innovation: Experience-sharing in RL with SAPIENS,Eleni Nisioti;Mateo Mahaut;Pierre-Yves Oudeyer;Ida Momennejad;Clément Moulin-Frier,"Human culture relies on innovation: our ability to continuously explore how existing elements can be combined to create new ones. Innovation is not solitary, it relies on collective search and accumulation. Reinforcement learning (RL) approaches commonly assume that fully-connected groups are best suited for innovation. However, human laboratory and field studies have shown that hierarchical innovation is more robustly achieved by dynamic social network structures. In dynamic settings, humans oscillate between innovating individually or in small clusters, and then sharing outcomes with others. To our knowledge, the role of social network structure on innovation has not been systematically studied in RL. Here, we use a multi-level problem setting (WordCraft), with three different innovation tasks to test the hypothesis that the social network structure affects the performance of distributed RL algorithms. We systematically design networks of DQNs sharing experiences from their replay buffers in varying structures (fully-connected, small world, dynamic, ring) and introduce a set of behavioral and mnemonic metrics that extend the classical reward-focused evaluation framework of RL. Comparing the level of innovation achieved by different social network structures across different tasks shows that, first, consistent with human findings, experience sharing within a dynamic structure achieves the highest level of innovation in tasks with a deceptive nature and large search spaces. Second, experience sharing is not as helpful when there is a single clear path to innovation. Third, the metrics we propose, can help understand the success of different social network structures on different tasks, with the diversity of experiences on an individual and group level lending crucial insights. △ Less","18 November, 2022",https://arxiv.org/pdf/2206.05060
The 1st Data Science for Pavements Challenge,Ashkan Behzadian;Tanner Wambui Muturi;Tianjie Zhang;Hongmin Kim;Amanda Mullins;Yang Lu;Neema Jasika Owor;Yaw Adu-Gyamfi;William Buttlar;Majidifard Hamed;Armstrong Aboah;David Mensching;Spragg Robert;Matthew Corrigan;Jack Youtchef;Dave Eshan,"The Data Science for Pavement Challenge (DSPC) seeks to accelerate the research and development of automated vision systems for pavement condition monitoring and evaluation by providing a platform with benchmarked datasets and codes for teams to innovate and develop machine learning algorithms that are practice-ready for use by industry. The first edition of the competition attracted 22 teams from 8 countries. Participants were required to automatically detect and classify different types of pavement distresses present in images captured from multiple sources, and under different conditions. The competition was data-centric: teams were tasked to increase the accuracy of a predefined model architecture by utilizing various data modification methods such as cleaning, labeling and augmentation. A real-time, online evaluation system was developed to rank teams based on the F1 score. Leaderboard results showed the promise and challenges of machine for advancing automation in pavement monitoring and evaluation. This paper summarizes the solutions from the top 5 teams. These teams proposed innovations in the areas of data cleaning, annotation, augmentation, and detection parameter tuning. The F1 score for the top-ranked team was approximately 0.9. The paper concludes with a review of different experiments that worked well for the current challenge and those that did not yield any significant improvement in model accuracy. △ Less","10 June, 2022",https://arxiv.org/pdf/2206.04874
The Developers' Design Thinking Toolbox in Hackathons: A Study on the Recurring Design Methods in Software Development Marathons,Kiev Gama;George Valença;Pedro Alessio;Rafael Formiga;André Neves;Nycolas Lacerda,"Hackathons are time-bounded collaborative events of intense teamwork to build prototypes usually in the form of software, aiming to specific challenges proposed by the organizers. These events became a widespread practice in the IT industry, universities and many other scenarios, as a result of a growing open-innovation trend in the last decade. Since the main deliverable of these events is a demonstrable version of an idea, such as early hardware or software prototypes, the short time frame requires participants to quickly understand the proposed challenge or even identify issues related to a given domain. To create solutions, teams follow an ad-hoc but effective design approach, that many times seems informal since the background of the participants is rather centered on technical aspects (e.g., web and mobile programming) and does not involve any training in Design Thinking. To understand this creative process, we conducted 37 interviews (32 hackathons winners and 5 hackathon organizers) with people from 16 countries. We aimed to identify the design processes and recurring design methods applied by winners in these events. Also, we conducted a focus group with 8 people experienced in hackathons (participants and organizers) to discuss our findings. Our analysis revealed that although hackathon winners with IT background have no formal training on Design Thinking, they are aware of many design methods, typically following a sequence of phases that involve divergent and convergent thinking to explore the problem space and propose alternatives in a solution space, which is the rationale behind Design Thinking. We derived a set of recommendations based on design strategies that seem to lead to successful hackathon participation. These recommendations can also be useful to organizers who intend to enhance the experience of newcomers in hackathons. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.04744
PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies,Guocheng Qian;Yuchen Li;Houwen Peng;Jinjie Mai;Hasan Abed Al Kader Hammoud;Mohamed Elhoseiny;Bernard Ghanem,"PointNet++ is one of the most influential neural architectures for point cloud understanding. Although the accuracy of PointNet++ has been largely surpassed by recent networks such as PointMLP and Point Transformer, we find that a large portion of the performance gain is due to improved training strategies, i.e. data augmentation and optimization techniques, and increased model sizes rather than architectural innovations. Thus, the full potential of PointNet++ has yet to be explored. In this work, we revisit the classical PointNet++ through a systematic study of model training and scaling strategies, and offer two major contributions. First, we propose a set of improved training strategies that significantly improve PointNet++ performance. For example, we show that, without any change in architecture, the overall accuracy (OA) of PointNet++ on ScanObjectNN object classification can be raised from 77.9% to 86.1%, even outperforming state-of-the-art PointMLP. Second, we introduce an inverted residual bottleneck design and separable MLPs into PointNet++ to enable efficient and effective model scaling and propose PointNeXt, the next version of PointNets. PointNeXt can be flexibly scaled up and outperforms state-of-the-art methods on both 3D classification and segmentation tasks. For classification, PointNeXt reaches an overall accuracy of 87.7 on ScanObjectNN, surpassing PointMLP by 2.3%, while being 10x faster in inference. For semantic segmentation, PointNeXt establishes a new state-of-the-art performance with 74.9% mean IoU on S3DIS (6-fold cross-validation), being superior to the recent Point Transformer. The code and models are available at https://github.com/guochengqian/pointnext. △ Less","12 October, 2022",https://arxiv.org/pdf/2206.04670
Reconstruct Face from Features Using GAN Generator as a Distribution Constraint,Xingbo Dong;Zhihui Miao;Lan Ma;Jiajun Shen;Zhe Jin;Zhenhua Guo;Andrew Beng Jin Teoh,"Face recognition based on the deep convolutional neural networks (CNN) shows superior accuracy performance attributed to the high discriminative features extracted. Yet, the security and privacy of the extracted features from deep learning models (deep features) have been often overlooked. This paper proposes the reconstruction of face images from deep features without accessing the CNN network configurations as a constrained optimization problem. Such optimization minimizes the distance between the features extracted from the original face image and the reconstructed face image. Instead of directly solving the optimization problem in the image space, we innovatively reformulate the problem by looking for a latent vector of a GAN generator, then use it to generate the face image. The GAN generator serves as a dual role in this novel framework, i.e., face distribution constraint of the optimization goal and a face generator. On top of the novel optimization task, we also propose an attack pipeline to impersonate the target user based on the generated face image. Our results show that the generated face images can achieve a state-of-the-art successful attack rate of 98.0\% on LFW under type-I attack @ FAR of 0.1\%. Our work sheds light on the biometric deployment to meet the privacy-preserving and security policies. △ Less","9 June, 2022",https://arxiv.org/pdf/2206.04295
IoT based SMEs shop management system,Ummae Hamida Hannan;Md Reaz Uddin Chowdhury;Md. Goalm Rahaman;Sakib Mahmud Galib;Md. Taimur Ahad,"The Internet of Things (IoT) is an idea that intends to interface arranged data frameworks to actual items. The Internet of Things (IoT) has applications in pretty much every part of life in this day and age, and stock administration is no special case. IoT gives an answer for this issue by making it simpler to interface every one of the various organizations in a strategic framework utilizing Wireless Sensor Networks. An Interactive Shopping Model and an Automated Inventory Intelligent Management System that uses the Internet of Things to give constant item following, the board, and observing. A study and examination of the commonness of IoT among assembling SMEs is introduced, just as the current impediments and possibilities for permitting prescient investigation. The four examination capacities are depicted alongside an outline of the IoT empowering agents. Future patterns and difficulties in arising innovative work subjects are featured, for example, making IoT advances available to SMEs. The motivation behind this paper is to look at how the Internet of Things is changing our lives and work spaces, just as to feature probably the best strategic approaches, insights, and patterns. Considering the developing significance of big business IoT and the exploration hole in this field, an IoT design and the IoT administration industry will be examined. A model is needed to choose and send IoT administrations in different authoritative settings. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03580
Toward IoT enabled smart offices: Achieving Sustainable Development Goals,Syeda Nishat Tasnim;Md Taimur Ahad,"Despite research advocating the Internet of Things (IoT) as an effective in-office monitoring system, little research has presented societal and climate centric discussions. Whereas the United Nations (UN) and other development agencies concerned with climate impact, are advocating transformative actions towards smart cities, very little research in the IoT domain analyzes the advantages of IoT in achieving sustainable development goals (SDGs) to fill this gap. In this study, a smart office (SO) was developed in a Cisco packet tracer. We then presented the SO through the lens of SDGs. We suggest that SOs support targets mentioned in Goal 6, 7, 8, 9, 11 and 12 of the SDGs. This research is crucial - both for developing and developed economies, as we move toward industrialization, while ignoring the adverse impacts of industrialization. This work is expected to provide a pathway with technological innovation toward a more sustainable world for IT practitioners, governments and development agencies. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03556
Code-DKT: A Code-based Knowledge Tracing Model for Programming Tasks,Yang Shi;Min Chi;Tiffany Barnes;Thomas Price,"Knowledge tracing (KT) models are a popular approach for predicting students' future performance at practice problems using their prior attempts. Though many innovations have been made in KT, most models including the state-of-the-art Deep KT (DKT) mainly leverage each student's response either as correct or incorrect, ignoring its content. In this work, we propose Code-based Deep Knowledge Tracing (Code-DKT), a model that uses an attention mechanism to automatically extract and select domain-specific code features to extend DKT. We compared the effectiveness of Code-DKT against Bayesian and Deep Knowledge Tracing (BKT and DKT) on a dataset from a class of 50 students attempting to solve 5 introductory programming assignments. Our results show that Code-DKT consistently outperforms DKT by 3.07-4.00% AUC across the 5 assignments, a comparable improvement to other state-of-the-art domain-general KT models over DKT. Finally, we analyze problem-specific performance through a set of case studies for one assignment to demonstrate when and how code features improve Code-DKT's predictions. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03545
Early Abnormal Detection of Sewage Pipe Network: Bagging of Various Abnormal Detection Algorithms,Zhen-Yu Zhang;Guo-Xiang Shao;Chun-Ming Qiu;Yue-Jie Hou;En-Ming Zhao;Chi-Chun Zhou,"Abnormalities of the sewage pipe network will affect the normal operation of the whole city. Therefore, it is important to detect the abnormalities early. This paper propose an early abnormal-detection method. The abnormalities are detected by using the conventional algorithms, such as isolation forest algorithm, two innovations are given: (1) The current and historical data measured by the sensors placed in the sewage pipe network (such as ultrasonic Doppler flowmeter) are taken as the overall dataset, and then the general dataset is detected by using the conventional anomaly detection method to diagnose the anomaly of the data. The anomaly refers to the sample different from the others samples in the whole dataset. Because the definition of anomaly is not through the algorithm, but the whole dataset, the construction of the whole dataset is the key to propose the early abnormal-detection algorithms. (2) A bagging strategy for a variety of conventional anomaly detection algorithms is proposed to achieve the early detection of anomalies with the high precision and recall. The results show that this method can achieve the early anomaly detection with the highest precision of 98.21%, the recall rate 63.58% and F1-score of 0.774. △ Less","5 June, 2022",https://arxiv.org/pdf/2206.03321
Collaborative Intelligence Orchestration: Inconsistency-Based Fusion of Semi-Supervised Learning and Active Learning,Jiannan Guo;Yangyang Kang;Yu Duan;Xiaozhong Liu;Siliang Tang;Wenqiao Zhang;Kun Kuang;Changlong Sun;Fei Wu,"While annotating decent amounts of data to satisfy sophisticated learning models can be cost-prohibitive for many real-world applications. Active learning (AL) and semi-supervised learning (SSL) are two effective, but often isolated, means to alleviate the data-hungry problem. Some recent studies explored the potential of combining AL and SSL to better probe the unlabeled data. However, almost all these contemporary SSL-AL works use a simple combination strategy, ignoring SSL and AL's inherent relation. Further, other methods suffer from high computational costs when dealing with large-scale, high-dimensional datasets. Motivated by the industry practice of labeling data, we propose an innovative Inconsistency-based virtual aDvErsarial Active Learning (IDEAL) algorithm to further investigate SSL-AL's potential superiority and achieve mutual enhancement of AL and SSL, i.e., SSL propagates label information to unlabeled samples and provides smoothed embeddings for AL, while AL excludes samples with inconsistent predictions and considerable uncertainty for SSL. We estimate unlabeled samples' inconsistency by augmentation strategies of different granularities, including fine-grained continuous perturbation exploration and coarse-grained data transformations. Extensive experiments, in both text and image domains, validate the effectiveness of the proposed algorithm, comparing it against state-of-the-art baselines. Two real-world case studies visualize the practical industrial value of applying and deploying the proposed data sampling algorithm. △ Less","7 June, 2022",https://arxiv.org/pdf/2206.03288
Future Computer Systems and Networking Research in the Netherlands: A Manifesto,Alexandru Iosup;Fernando Kuipers;Ana Lucia Varbanescu;Paola Grosso;Animesh Trivedi;Jan Rellermeyer;Lin Wang;Alexandru Uta;Francesco Regazzoni,"Our modern society and competitive economy depend on a strong digital foundation and, in turn, on sustained research and innovation in computer systems and networks (CompSys). With this manifesto, we draw attention to CompSys as a vital part of ICT. Among ICT technologies, CompSys covers all the hardware and all the operational software layers that enable applications; only application-specific details, and often only application-specific algorithms, are not part of CompSys. Each of the Top Sectors of the Dutch Economy, each route in the National Research Agenda, and each of the UN Sustainable Development Goals pose challenges that cannot be addressed without groundbreaking CompSys advances. Looking at the 2030-2035 horizon, important new applications will emerge only when enabled by CompSys developments. Triggered by the COVID-19 pandemic, millions moved abruptly online, raising infrastructure scalability and data sovereignty issues; but governments processing social data and responsible social networks still require a paradigm shift in data sovereignty and sharing. AI already requires massive computer systems which can cost millions per training task, but the current technology leaves an unsustainable energy footprint including large carbon emissions. Computational sciences such as bioinformatics, and ""Humanities for all"" and ""citizen data science"", cannot become affordable and efficient until computer systems take a generational leap. Similarly, the emerging quantum internet depends on (traditional) CompSys to bootstrap operation for the foreseeable future. Large commercial sectors, including finance and manufacturing, require specialized computing and networking or risk becoming uncompetitive. And, at the core of Dutch innovation, promising technology hubs, deltas, ports, and smart cities, could see their promise stagger due to critical dependency on non-European technology. △ Less","26 May, 2022",https://arxiv.org/pdf/2206.03259
Deep Reinforcement Learning for Cybersecurity Threat Detection and Protection: A Review,Mohit Sewak;Sanjay K. Sahay;Hemant Rathore,"The cybersecurity threat landscape has lately become overly complex. Threat actors leverage weaknesses in the network and endpoint security in a very coordinated manner to perpetuate sophisticated attacks that could bring down the entire network and many critical hosts in the network. Increasingly advanced deep and machine learning-based solutions have been used in threat detection and protection. The application of these techniques has been reviewed well in the scientific literature. Deep Reinforcement Learning has shown great promise in developing AI-based solutions for areas that had earlier required advanced human cognizance. Different techniques and algorithms under deep reinforcement learning have shown great promise in applications ranging from games to industrial processes, where it is claimed to augment systems with general AI capabilities. These algorithms have recently also been used in cybersecurity, especially in threat detection and endpoint protection, where these are showing state-of-the-art results. Unlike supervised machines and deep learning, deep reinforcement learning is used in more diverse ways and is empowering many innovative applications in the threat defense landscape. However, there does not exist any comprehensive review of these unique applications and accomplishments. Therefore, in this paper, we intend to fill this gap and provide a comprehensive review of the different applications of deep reinforcement learning in cybersecurity threat detection and protection. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.02733
A Survey on Sentence Embedding Models Performance for Patent Analysis,Hamid Bekamiri;Daniel S. Hain;Roman Jurowetzki,"Patent data is an important source of knowledge for innovation research, while the technological similarity between pairs of patents is a key enabling indicator for patent analysis. Recently researchers have been using patent vector space models based on different NLP embeddings models to calculate the technological similarity between pairs of patents to help better understand innovations, patent landscaping, technology mapping, and patent quality evaluation. More often than not, Text Embedding is a vital precursor to patent analysis tasks. A pertinent question then arises: How should we measure and evaluate the accuracy of these embeddings? To the best of our knowledge, there is no comprehensive survey that builds a clear delineation of embedding models' performance for calculating patent similarity indicators. Therefore, in this study, we provide an overview of the accuracy of these algorithms based on patent classification performance and propose a standard library and dataset for assessing the accuracy of embeddings models based on PatentSBERTa approach. In a detailed discussion, we report the performance of the top 3 algorithms at section, class, and subclass levels. The results based on the first claim of patents show that PatentSBERTa, Bert-for-patents, and TF-IDF Weighted Word Embeddings have the best accuracy for computing sentence embeddings at the subclass level. According to the first results, the performance of the models in different classes varies, which shows researchers in patent analysis can utilize the results of this study to choose the best proper model based on the specific section of patent data they used. △ Less","5 August, 2022",https://arxiv.org/pdf/2206.02690
Towards Fair Federated Recommendation Learning: Characterizing the Inter-Dependence of System and Data Heterogeneity,Kiwan Maeng;Haiyu Lu;Luca Melis;John Nguyen;Mike Rabbat;Carole-Jean Wu,"Federated learning (FL) is an effective mechanism for data privacy in recommender systems by running machine learning model training on-device. While prior FL optimizations tackled the data and system heterogeneity challenges faced by FL, they assume the two are independent of each other. This fundamental assumption is not reflective of real-world, large-scale recommender systems -- data and system heterogeneity are tightly intertwined. This paper takes a data-driven approach to show the inter-dependence of data and system heterogeneity in real-world data and quantifies its impact on the overall model quality and fairness. We design a framework, RF^2, to model the inter-dependence and evaluate its impact on state-of-the-art model optimization techniques for federated recommendation tasks. We demonstrate that the impact on fairness can be severe under realistic heterogeneity scenarios, by up to 15.8--41x compared to a simple setup assumed in most (if not all) prior work. It means when realistic system-induced data heterogeneity is not properly modeled, the fairness impact of an optimization can be downplayed by up to 41x. The result shows that modeling realistic system-induced data heterogeneity is essential to achieving fair federated recommendation learning. We plan to open-source RF^2 to enable future design and evaluation of FL innovations. △ Less","30 May, 2022",https://arxiv.org/pdf/2206.02633
Automated Circuit Sizing with Multi-objective Optimization based on Differential Evolution and Bayesian Inference,Catalin Visan;Octavian Pascu;Marius Stanescu;Elena-Diana Sandru;Cristian Diaconu;Andi Buzo;Georg Pelz;Horia Cucu,"With the ever increasing complexity of specifications, manual sizing for analog circuits recently became very challenging. Especially for innovative, large-scale circuits designs, with tens of design variables, operating conditions and conflicting objectives to be optimized, design engineers spend many weeks, running time-consuming simulations, in their attempt at finding the right configuration. Recent years brought machine learning and optimization techniques to the field of analog circuits design, with evolutionary algorithms and Bayesian models showing good results for circuit sizing. In this context, we introduce a design optimization method based on Generalized Differential Evolution 3 (GDE3) and Gaussian Processes (GPs). The proposed method is able to perform sizing for complex circuits with a large number of design variables and many conflicting objectives to be optimized. While state-of-the-art methods reduce multi-objective problems to single-objective optimization and potentially induce a prior bias, we search directly over the multi-objective space using Pareto dominance and ensure that diverse solutions are provided to the designers to choose from. To the best of our knowledge, the proposed method is the first to specifically address the diversity of the solutions, while also focusing on minimizing the number of simulations required to reach feasible configurations. We evaluate the introduced method on two voltage regulators showing different levels of complexity and we highlight that the proposed innovative candidate selection method and survival policy leads to obtaining feasible solutions, with a high degree of diversity, much faster than with GDE3 or Bayesian Optimization-based algorithms. △ Less","6 June, 2022",https://arxiv.org/pdf/2206.02391
"A review of machine learning approaches, challenges and prospects for computational tumor pathology",Liangrui Pan;Zhichao Feng;Shaoliang Peng,"Computational pathology is part of precision oncology medicine. The integration of high-throughput data including genomics, transcriptomics, proteomics, metabolomics, pathomics, and radiomics into clinical practice improves cancer treatment plans, treatment cycles, and cure rates, and helps doctors open up innovative approaches to patient prognosis. In the past decade, rapid advances in artificial intelligence, chip design and manufacturing, and mobile computing have facilitated research in computational pathology and have the potential to provide better-integrated solutions for whole-slide images, multi-omics data, and clinical informatics. However, tumor computational pathology now brings some challenges to the application of tumour screening, diagnosis and prognosis in terms of data integration, hardware processing, network sharing bandwidth and machine learning technology. This review investigates image preprocessing methods in computational pathology from a pathological and technical perspective, machine learning-based methods, and applications of computational pathology in breast, colon, prostate, lung, and various tumour disease scenarios. Finally, the challenges and prospects of machine learning in computational pathology applications are discussed. △ Less","31 May, 2022",https://arxiv.org/pdf/2206.01728
Incorporating Explicit Uncertainty Estimates into Deep Offline Reinforcement Learning,David Brandfonbrener;Remi Tachet des Combes;Romain Laroche,"Most theoretically motivated work in the offline reinforcement learning setting requires precise uncertainty estimates. This requirement restricts the algorithms derived in that work to the tabular and linear settings where such estimates exist. In this work, we develop a novel method for incorporating scalable uncertainty estimates into an offline reinforcement learning algorithm called deep-SPIBB that extends the SPIBB family of algorithms to environments with larger state and action spaces. We use recent innovations in uncertainty estimation from the deep learning community to get more scalable uncertainty estimates to plug into deep-SPIBB. While these uncertainty estimates do not allow for the same theoretical guarantees as in the tabular case, we argue that the SPIBB mechanism for incorporating uncertainty is more robust and flexible than pessimistic approaches that incorporate the uncertainty as a value function penalty. We bear this out empirically, showing that deep-SPIBB outperforms pessimism based approaches with access to the same uncertainty estimates and performs at least on par with a variety of other strong baselines across several environments and datasets. △ Less","2 June, 2022",https://arxiv.org/pdf/2206.01085
Residual Multiplicative Filter Networks for Multiscale Reconstruction,Shayan Shekarforoush;David B. Lindell;David J. Fleet;Marcus A. Brubaker,"Coordinate networks like Multiplicative Filter Networks (MFNs) and BACON offer some control over the frequency spectrum used to represent continuous signals such as images or 3D volumes. Yet, they are not readily applicable to problems for which coarse-to-fine estimation is required, including various inverse problems in which coarse-to-fine optimization plays a key role in avoiding poor local minima. We introduce a new coordinate network architecture and training scheme that enables coarse-to-fine optimization with fine-grained control over the frequency support of learned reconstructions. This is achieved with two key innovations. First, we incorporate skip connections so that structure at one scale is preserved when fitting finer-scale structure. Second, we propose a novel initialization scheme to provide control over the model frequency spectrum at each stage of optimization. We demonstrate how these modifications enable multiscale optimization for coarse-to-fine fitting to natural images. We then evaluate our model on synthetically generated datasets for the the problem of single-particle cryo-EM reconstruction. We learn high resolution multiscale structures, on par with the state-of-the art. △ Less","26 October, 2022",https://arxiv.org/pdf/2206.00746
Testing Research Software: A Survey,Nasir U. Eisty;Jeffrey C. Carver,"Background: Research software plays an important role in solving real-life problems, empowering scientific innovations, and handling emergency situations. Therefore, the correctness and trustworthiness of research software are of absolute importance. Software testing is an important activity for identifying problematic code and helping to produce high-quality software. However, testing of research software is difficult due to the complexity of the underlying science, relatively unknown results from scientific algorithms, and the culture of the research software community. Aims: The goal of this paper is to better understand current testing practices, identify challenges, and provide recommendations on how to improve the testing process for research software development. Method: We surveyed members of the research software developer community to collect information regarding their knowledge about and use of software testing in their projects. Results: We analysed 120 responses and identified that even though research software developers report they have an average level of knowledge about software testing, they still find it difficult due to the numerous challenges involved. However, there are a number of ways, such as proper training, that can improve the testing process for research software. Conclusions: Testing can be challenging for any type of software. This difficulty is especially present in the development of research software, where software engineering activities are typically given less attention. To produce trustworthy results from research software, there is a need for a culture change so that testing is valued and teams devote appropriate effort to writing and executing tests. △ Less","31 May, 2022",https://arxiv.org/pdf/2205.15982
Collaborative Sensing in Perceptive Mobile Networks: Opportunities and Challenges,Lei Xie;S. H. Song;Yonina C. Eldar;Khaled B. Letaief,"With the development of innovative applications that demand accurate environment information, e.g., autonomous driving, sensing becomes an important requirement for future wireless networks. To this end, integrated sensing and communication (ISAC) provides a promising platform to exploit the synergy between sensing and communication, where perceptive mobile networks (PMNs) were proposed to add accurate sensing capability to existing wireless networks. The well-developed cellular networks offer exciting opportunities for sensing, including large coverage, strong computation and communication power, and most importantly networked sensing, where the perspectives from multiple sensing nodes can be collaboratively utilized for sensing the same target. However, PMNs also face big challenges such as the inherent interference between sensing and communication, the complex sensing environment, and the tracking of high-speed targets by cellular networks. This paper provides a comprehensive review on the design of PMNs, covering the popular network architectures, sensing protocols, standing research problems, and available solutions. Several future research directions that are critical for the development of PMNs are also discussed. △ Less","31 May, 2022",https://arxiv.org/pdf/2205.15805
Augmenting Scientific Creativity with an Analogical Search Engine,Hyeonsu B. Kang;Xin Qian;Tom Hope;Dafna Shahaf;Joel Chan;Aniket Kittur,"Analogies have been central to creative problem-solving throughout the history of science and technology. As the number of scientific papers continues to increase exponentially, there is a growing opportunity for finding diverse solutions to existing problems. However, realizing this potential requires the development of a means for searching through a large corpus that goes beyond surface matches and simple keywords. Here we contribute the first end-to-end system for analogical search on scientific papers and evaluate its effectiveness with scientists' own problems. Using a human-in-the-loop AI system as a probe we find that our system facilitates creative ideation, and that ideation success is mediated by an intermediate level of matching on the problem abstraction (i.e., high versus low). We also demonstrate a fully automated AI search engine that achieves a similar accuracy with the human-in-the-loop system. We conclude with design implications for enabling automated analogical inspiration engines to accelerate scientific innovation. △ Less","30 May, 2022",https://arxiv.org/pdf/2205.15476
Environmental Monitoring for Smart Cities,Bacco Manlio;Delmastro Franca;Ferro Erina;Gotta Alberto,"This work presents an innovative, multidisciplinary and cost-effective ecosystem of ICT solutions able to collect, process and distribute geo-referenced information about the influence of pollution and micro-climatic conditions on the quality of life in Smart Cities. The system has been developed and experimentally evaluated in the framework of the research project SHE, co-funded by the Tuscany Region (Italy). Specifically, an innovative monitoring network has been developed, constituted by fixed and mobile sensor nodes, which provided comparable measurements in stationary and mobile conditions. In addition, sensor data have been enriched with those generated by citizens through the use of a dedicated mobile application, exploiting participatory sensing and MSN paradigms. △ Less","30 May, 2022",https://arxiv.org/pdf/2205.15147
AttentionCode: Ultra-Reliable Feedback Codes for Short-Packet Communications,Yulin Shao;Emre Ozfatura;Alberto Perotti;Branislav Popovic;Deniz Gunduz,"Ultra-reliable short-packet communication is a major challenge in future wireless networks with critical applications. To achieve ultra-reliable communications beyond 99.999%, this paper envisions a new interaction-based communication paradigm that exploits feedback from the receiver. We present AttentionCode, a new class of feedback codes leveraging deep learning (DL) technologies. The underpinnings of AttentionCode are three architectural innovations: AttentionNet, input restructuring, and adaptation to fading channels, accompanied by several training methods, including large-batch training, distributed learning, look-ahead optimizer, training-test signal-to-noise ratio (SNR) mismatch, and curriculum learning. The training methods can potentially be generalized to other wireless communication applications with machine learning. Numerical experiments verify that AttentionCode establishes a new state of the art among all DL-based feedback codes in both additive white Gaussian noise (AWGN) channels and fading channels. In AWGN channels with noiseless feedback, for example, AttentionCode achieves a block error rate (BLER) of 10^{-7} when the forward channel SNR is 0 dB for a block size of 50 bits, demonstrating the potential of AttentionCode to provide ultra-reliable short-packet communications. △ Less","24 December, 2022",https://arxiv.org/pdf/2205.14955
A New High-Performance Approach to Approximate Pattern-Matching for Plagiarism Detection in Blockchain-Based Non-Fungible Tokens (NFTs),Ciprian Pungila;Darius Galis;Viorel Negru,"We are presenting a fast and innovative approach to performing approximate pattern-matching for plagiarism detection, using an NDFA-based approach that significantly enhances performance compared to other existing similarity measures. We outline the advantages of our approach in the context of blockchain-based non-fungible tokens (NFTs). We present, formalize, discuss and test our proposed approach in several real-world scenarios and with different similarity measures commonly used in plagiarism detection, and observe significant throughput enhancements throughout the entire spectrum of tests, with little to no compromises on the accuracy of the detection process overall. We conclude that our approach is suitable and adequate to perform approximate pattern-matching for plagiarism detection, and outline research directions for future improvements. △ Less","28 May, 2022",https://arxiv.org/pdf/2205.14492
Fast Object Placement Assessment,Li Niu;Qingyang Liu;Zhenchen Liu;Jiangtong Li,"Object placement assessment (OPA) aims to predict the rationality score of a composite image in terms of the placement (e.g., scale, location) of inserted foreground object. However, given a pair of scaled foreground and background, to enumerate all the reasonable locations, existing OPA model needs to place the foreground at each location on the background and pass the obtained composite image through the model one at a time, which is very time-consuming. In this work, we investigate a new task named as fast OPA. Specifically, provided with a scaled foreground and a background, we only pass them through the model once and predict the rationality scores for all locations. To accomplish this task, we propose a pioneering fast OPA model with several innovations (i.e., foreground dynamic filter, background prior transfer, and composite feature mimicking) to bridge the performance gap between slow OPA model and fast OPA model. Extensive experiments on OPA dataset show that our proposed fast OPA model performs on par with slow OPA model but runs significantly faster. △ Less","27 May, 2022",https://arxiv.org/pdf/2205.14280
Writes Hurt: Lessons in Cache Design for Optane NVRAM,Alexandra Fedorova;Keith Smith;Keith Bostic;Alexander Gorrod;Sue LoVerso;Michael Cahill,"Intel OptaneTM DC Persistent Memory resides on the memory bus and approaches DRAM in access latency. One avenue for its adoption is to employ it in place of persistent storage; another is to use it as a cheaper and denser extension of DRAM. In pursuit of the latter goal, we present the design of a volatile Optane NVRAM cache as a component in a storage engine underlying MongoDB. The primary innovation in our design is a new cache admission policy. We discover that on Optane NVRAM, known for its limited write throughput, the presence of writes disproportionately affects the throughput of reads, much more so than on DRAM. Therefore, an admission policy that indiscriminately admits new data (and thus generates writes), severely limits the rate of data retrieval and results in exceedingly poor performance for the cache overall. We design an admission policy that balances the rate of admission with the rate of lookups using dynamically observed characteristics of the workload. Our implementation outperforms OpenCAS (an off-the-shelf Optane-based block cache) in all cases, and Intel Memory Mode in cases where the database size exceeds the available NVRAM. Our cache is decoupled from the rest of the storage engine and uses generic metrics to guide its admission policy; therefore our design can be easily adopted in other systems. △ Less","24 May, 2022",https://arxiv.org/pdf/2205.14122
Forecasting countries' gross domestic product from patent data,Yucheng Ye;Shuqi Xu;Manuel Sebastian Mariani;Linyuan Lü,"Recent strides in economic complexity have shown that the future economic development of nations can be predicted with a single ""economic fitness"" variable, which captures countries' competitiveness in international trade. The predictions by this low-dimensional approach could match or even outperform predictions based on much more sophisticated methods, such as those by the International Monetary Fund (IMF). However, all prior works in economic complexity aimed to quantify countries' fitness from World Trade export data, without considering the possibility to infer countries' potential for growth from alternative sources of data. Here, motivated by the long-standing relationship between technological development and economic growth, we aim to forecast countries' growth from patent data. Specifically, we construct a citation network between countries from the European Patent Office (EPO) dataset. Initial results suggest that the H-index centrality in this network is a potential candidate to gauge national economic performance. To validate this conjecture, we construct a two-dimensional plane defined by the H-index and GDP per capita, and use a forecasting method based on dynamical systems to test the predicting accuracy of the H-index. We find that the predictions based on the H-index-GDP plane outperform the predictions by IMF by approximately 35%, and they marginally outperform those by the economic fitness extracted from trade data. Our results could inspire further attempts to identify predictors of national growth from different sources of data related to scientific and technological innovation. △ Less","27 May, 2022",https://arxiv.org/pdf/2205.13779
Representing Polymers as Periodic Graphs with Learned Descriptors for Accurate Polymer Property Predictions,Evan R. Antoniuk;Peggy Li;Bhavya Kailkhura;Anna M. Hiszpanski,"One of the grand challenges of utilizing machine learning for the discovery of innovative new polymers lies in the difficulty of accurately representing the complex structures of polymeric materials. Although a wide array of hand-designed polymer representations have been explored, there has yet to be an ideal solution for how to capture the periodicity of polymer structures, and how to develop polymer descriptors without the need for human feature design. In this work, we tackle these problems through the development of our periodic polymer graph representation. Our pipeline for polymer property predictions is comprised of our polymer graph representation that naturally accounts for the periodicity of polymers, followed by a message-passing neural network (MPNN) that leverages the power of graph deep learning to automatically learn chemically-relevant polymer descriptors. Across a diverse dataset of 10 polymer properties, we find that this polymer graph representation consistently outperforms hand-designed representations with a 20% average reduction in prediction error. Our results illustrate how the incorporation of chemical intuition through directly encoding periodicity into our polymer graph representation leads to a considerable improvement in the accuracy and reliability of polymer property predictions. We also demonstrate how combining polymer graph representations with message-passing neural network architectures can automatically extract meaningful polymer features that are consistent with human intuition, while outperforming human-derived features. This work highlights the advancement in predictive capability that is possible if using chemical descriptors that are specifically optimized for capturing the unique chemical structure of polymers. △ Less","27 May, 2022",https://arxiv.org/pdf/2205.13757
Looking at Creative ML Blindspots with a Sociological Lens,Katharina Burgdorf;Negar Rostamzadeh;Ramya Srinivasan;Jennifer Lena,"How can researchers from the creative ML/AI community and sociology of culture engage in fruitful collaboration? How do researchers from both fields think (differently) about creativity and the production of creative work? While the ML community considers creativity as a matter of technical expertise and acumen, social scientists have emphasized the role of embeddedness in cultural production. This perspective aims to bridge both disciplines and proposes a conceptual and methodological toolkit for collaboration. We provide a systematic review of recent research in both fields and offer three perspectives around which to structure interdisciplinary research on cultural production: people, processes, and products. We thereby provide necessary grounding work to support multidisciplinary researchers to navigate conceptual and methodological hurdles in their collaboration. Our research will be of interest to ML researchers and sociologists interested in creativity that aim to conduct innovative research by bridging both fields. △ Less","26 May, 2022",https://arxiv.org/pdf/2205.13683
VizInspect Pro -- Automated Optical Inspection (AOI) solution,Faraz Waseem;Sanjit Menon;Haotian Xu;Debashis Mondal,"Traditional vision based Automated Optical Inspection (referred to as AOI in paper) systems present multiple challenges in factory settings including inability to scale across multiple product lines, requirement of vendor programming expertise, little tolerance to variations and lack of cloud connectivity for aggregated insights. The lack of flexibility in these systems presents a unique opportunity for a deep learning based AOI system specifically for factory automation. The proposed solution, VizInspect pro is a generic computer vision based AOI solution built on top of Leo - An edge AI platform. Innovative features that overcome challenges of traditional vision systems include deep learning based image analysis which combines the power of self-learning with high speed and accuracy, an intuitive user interface to configure inspection profiles in minutes without ML or vision expertise and the ability to solve complex inspection challenges while being tolerant to deviations and unpredictable defects. This solution has been validated by multiple external enterprise customers with confirmed value propositions. In this paper we show you how this solution and platform solved problems around model development, deployment, scaling multiple inferences and visualizations. △ Less","25 May, 2022",https://arxiv.org/pdf/2205.13095
The Next-Generation OS Process Abstraction,Rodrigo Siqueira;Nelson Lago;Fabio Kon;Dejan Milojičić,"Operating Systems are built upon a set of abstractions to provide resource management and programming APIs for common functionality, such as synchronization, communication, protection, and I/O. The process abstraction is the bridge across these two aspects; unsurprisingly, research efforts pay particular attention to the process abstraction, aiming at enhancing security, improving performance, and supporting hardware innovations. However, given the intrinsic difficulties to implement modifications at the OS level, recent endeavors have not yet been widely adopted in production-oriented OSes. Still, we believe the current hardware evolution and new application requirements provide favorable conditions to change this trend. This paper evaluates recent research on OS process features identifying potential evolution paths. We derive a set of relevant process characteristics, and propose how to extend them as to benefit OSes and applications. △ Less","24 May, 2022",https://arxiv.org/pdf/2205.12270
HCFRec: Hash Collaborative Filtering via Normalized Flow with Structural Consensus for Efficient Recommendation,Fan Wang;Weiming Liu;Chaochao Chen;Mengying Zhu;Xiaolin Zheng,"The ever-increasing data scale of user-item interactions makes it challenging for an effective and efficient recommender system. Recently, hash-based collaborative filtering (Hash-CF) approaches employ efficient Hamming distance of learned binary representations of users and items to accelerate recommendations. However, Hash-CF often faces two challenging problems, i.e., optimization on discrete representations and preserving semantic information in learned representations. To address the above two challenges, we propose HCFRec, a novel Hash-CF approach for effective and efficient recommendations. Specifically, HCFRec not only innovatively introduces normalized flow to learn the optimal hash code by efficiently fit a proposed approximate mixture multivariate normal distribution, a continuous but approximately discrete distribution, but also deploys a cluster consistency preserving mechanism to preserve the semantic structure in representations for more accurate recommendations. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our HCFRec compared to the state-of-art methods in terms of effectiveness and efficiency. △ Less","24 May, 2022",https://arxiv.org/pdf/2205.12042
Evolution of biomedical innovation quantified via billions of distinct article-level MeSH keyword combinations,Alexander M. Petersen,"We develop a systematic approach to measuring combinatorial innovation in the biomedical sciences based upon the comprehensive ontology of Medical Subject Headings (MeSH). This approach leverages an expert-defined knowledge ontology that features both breadth (27,875 MeSH analyzed across 25 million articles indexed by PubMed from 1902 onwards) and depth (we differentiate between Major and Minor MeSH terms to identify differences in the knowledge network representation constructed from primary research topics only). With this level of uniform resolution we differentiate between three different modes of innovation contributing to the combinatorial knowledge network: (i) conceptual innovation associated with the emergence of new concepts and entities (measured as the entry of new MeSH); and (ii) recombinant innovation, associated with the emergence of new combinations, which itself consists of two types: peripheral (i.e., combinations involving new knowledge) and core (combinations comprised of pre-existing knowledge only). Another relevant question we seek to address is whether examining triplet and quartet combinations, in addition to the more traditional dyadic or pairwise combinations, provide evidence of any new phenomena associated with higher-order combinations. Analysis of the size, growth, and coverage of combinatorial innovation yield results that are largely independent of the combination order, thereby suggesting that the common dyadic approach is sufficient to capture essential phenomena. Our main results are twofold: (a) despite the persistent addition of new MeSH terms, the network is densifying over time meaning that scholars are increasingly exploring and realizing the vast space of all knowledge combinations; and (b) conceptual innovation is increasingly concentrated within single research articles, a harbinger of the recent paradigm shift towards convergence science. △ Less","23 May, 2022",https://arxiv.org/pdf/2205.11632
Fed-DART and FACT: A solution for Federated Learning in a production environment,Nico Weber;Patrick Holzer;Tania Jacob;Enislay Ramentol,"Federated Learning as a decentralized artificial intelligence (AI) solution solves a variety of problems in industrial applications. It enables a continuously self-improving AI, which can be deployed everywhere at the edge. However, bringing AI to production for generating a real business impact is a challenging task. Especially in the case of Federated Learning, expertise and resources from multiple domains are required to realize its full potential. Having this in mind we have developed an innovative Federated Learning framework FACT based on Fed-DART, enabling an easy and scalable deployment, helping the user to fully leverage the potential of their private and decentralized data. △ Less","23 May, 2022",https://arxiv.org/pdf/2205.11267
Improved Healthcare Access in Low-resource Regions: A Review of Technological Solutions,Bishal Lamichhane;Navaraj Neupane,"Technological advancements have led to significant improvements in healthcare for prevention, diagnosis, treatments, and care. While resourceful regions can capitalize on state-of-the-art healthcare technologies, there might be barriers and delays in technology-enabled healthcare availability for a low-resource region. Unique innovations guided by the constraints of low-resource regions are required to truly make healthcare technologies ubiquitous and achieve the goal of ""healthcare for all"". In this review, we identified several research and development works that have investigated technology-based healthcare innovations targeted at low-resource regions. We found three main pillars of work towards this end: low-cost hardware for the affordability of medical devices, use of information and communication technology (ICT) tools for scalability and operational efficiencies in healthcare services, and mobile health solutions. Several emerging technologies are also promising for healthcare in low-resource regions, such as artificial intelligence, the Internet of Things (IoT), and blockchain technology. We discuss these emerging technologies too in this review. △ Less","22 May, 2022",https://arxiv.org/pdf/2205.10913
Evaluation of User Perception on Biometric Fingerprint System,Jones Yeboah;Victor Adewopo;Sylvia Azumah;Izunna Okpala,"Biometric systems involve security assurance to make our system highly secured and robust. Nowadays, biometric technology has been fixed into new systems with the aim of enforcing strong privacy and security. Several innovative system have been introduced, and most of them have biometrics installed to protect military bases, banking machines, and other sophisticated systems, such as online tracking systems. Businesses can now focus on their core functions and feel confident about their data security. Despite the benefits and enhancements in security that biometrics offer, there are also some vulnerabilities. This study aimed to investigate the biometric vulnerabilities in a healthcare facility and propose possible countermeasures for biometric system vulnerabilities. △ Less","21 May, 2022",https://arxiv.org/pdf/2205.10695
Fast Change Identification in Multi-Play Bandits and its Applications in Wireless Networks,Gourab Ghatak,"Next-generation wireless services are characterized by a diverse set of requirements, to sustain which, the wireless access points need to probe the users in the network periodically. In this regard, we study a novel multi-armed bandit (MAB) setting that mandates probing all the arms periodically while keeping track of the best current arm in a non-stationary environment. In particular, we develop \texttt{TS-GE} that balances the regret guarantees of classical Thompson sampling (TS) with the broadcast probing (BP) of all the arms simultaneously in order to actively detect a change in the reward distributions. The main innovation in the algorithm is in identifying the changed arm by an optional subroutine called group exploration (GE) that scales as \log_2(K) for a K-armed bandit setting. We characterize the probability of missed detection and the probability of false-alarm in terms of the environment parameters. We highlight the conditions in which the regret guarantee of \texttt{TS-GE} outperforms that of the state-of-the-art algorithms, in particular, \texttt{ADSWITCH} and \texttt{M-UCB}. We demonstrate the efficacy of \texttt{TS-GE} by employing it in two wireless system application - task offloading in mobile-edge computing (MEC) and an industrial internet-of-things (IIoT) network designed for simultaneous wireless information and power transfer (SWIPT). △ Less","24 November, 2022",https://arxiv.org/pdf/2205.10366
Lossless Acceleration for Seq2seq Generation with Aggressive Decoding,Tao Ge;Heming Xia;Xin Sun;Si-Qing Chen;Furu Wei,"We study lossless acceleration for seq2seq generation with a novel decoding algorithm -- Aggressive Decoding. Unlike the previous efforts (e.g., non-autoregressive decoding) speeding up seq2seq generation at the cost of quality loss, our approach aims to yield the identical (or better) generation compared with autoregressive decoding but in a significant speedup, achieved by innovative cooperation of aggressive decoding and verification that are both efficient due to parallel computing. We propose two Aggressive Decoding paradigms for 2 kinds of seq2seq tasks: 1) For the seq2seq tasks whose inputs and outputs are highly similar (e.g., Grammatical Error Correction), we propose Input-guided Aggressive Decoding (IAD) that aggressively copies from the input sentence as drafted decoded tokens to verify in parallel; 2) For other general seq2seq tasks (e.g., Machine Translation), we propose Generalized Aggressive Decoding (GAD) that first employs an additional non-autoregressive decoding model for aggressive decoding and then verifies in parallel in the autoregressive manner. We test Aggressive Decoding on the most popular 6-layer Transformer model on GPU in multiple seq2seq tasks: 1) For IAD, we show that it can introduce a 7x-9x speedup for the Transformer in Grammatical Error Correction and Text Simplification tasks with the identical results as greedy decoding; 2) For GAD, we observe a 3x-5x speedup with the identical or even better quality in two important seq2seq tasks: Machine Translation and Abstractive Summarization. Moreover, Aggressive Decoding can benefit even more from stronger computing devices that are better at parallel computing. Given the lossless quality as well as significant and promising speedup, we believe Aggressive Decoding may potentially evolve into a de facto standard for efficient and lossless seq2seq generation in the near future. △ Less","20 May, 2022",https://arxiv.org/pdf/2205.10350
"Nothing makes sense in deep learning, except in the light of evolution",Artem Kaznatcheev;Konrad Paul Kording,"Deep Learning (DL) is a surprisingly successful branch of machine learning. The success of DL is usually explained by focusing analysis on a particular recent algorithm and its traits. Instead, we propose that an explanation of the success of DL must look at the population of all algorithms in the field and how they have evolved over time. We argue that cultural evolution is a useful framework to explain the success of DL. In analogy to biology, we use `development' to mean the process converting the pseudocode or text description of an algorithm into a fully trained model. This includes writing the programming code, compiling and running the program, and training the model. If all parts of the process don't align well then the resultant model will be useless (if the code runs at all!). This is a constraint. A core component of evolutionary developmental biology is the concept of deconstraints -- these are modification to the developmental process that avoid complete failure by automatically accommodating changes in other components. We suggest that many important innovations in DL, from neural networks themselves to hyperparameter optimization and AutoGrad, can be seen as developmental deconstraints. These deconstraints can be very helpful to both the particular algorithm in how it handles challenges in implementation and the overall field of DL in how easy it is for new ideas to be generated. We highlight how our perspective can both advance DL and lead to new insights for evolutionary biology. △ Less","20 May, 2022",https://arxiv.org/pdf/2205.10320
Topology-aware Graph Neural Networks for Learning Feasible and Adaptive ac-OPF Solutions,Shaohui Liu;Chengyang Wu;Hao Zhu,"Solving the optimal power flow (OPF) problem is a fundamental task to ensure the system efficiency and reliability in real-time electricity grid operations. We develop a new topology-informed graph neural network (GNN) approach for predicting the optimal solutions of real-time ac-OPF problem. To incorporate grid topology to the NN model, the proposed GNN-for-OPF framework innovatively exploits the locality property of locational marginal prices and voltage magnitude. Furthermore, we develop a physics-aware (ac-)flow feasibility regularization approach for general OPF learning. The advantages of our proposed designs include reduced model complexity, improved generalizability and feasibility guarantees. By providing the analytical understanding on the graph subspace stability under grid topology contingency, we show the proposed GNN can quickly adapt to varying grid topology by an efficient re-training strategy. Numerical tests on various test systems of different sizes have validated the prediction accuracy, improved flow feasibility, and topology adaptivity capability of our proposed GNN-based learning framework. △ Less","1 November, 2022",https://arxiv.org/pdf/2205.10129
Can Foundation Models Wrangle Your Data?,Avanika Narayan;Ines Chami;Laurel Orr;Simran Arora;Christopher Ré,"Foundation Models (FMs) are models trained on large corpora of data that, at very large scale, can generalize to new tasks without any task-specific finetuning. As these models continue to grow in size, innovations continue to push the boundaries of what these models can do on language and image tasks. This paper aims to understand an underexplored area of FMs: classical data tasks like cleaning and integration. As a proof-of-concept, we cast five data cleaning and integration tasks as prompting tasks and evaluate the performance of FMs on these tasks. We find that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks. We identify specific research challenges and opportunities that these models present, including challenges with private and domain specific data, and opportunities to make data management systems more accessible to non-experts. We make our code and experiments publicly available at: https://github.com/HazyResearch/fm_data_tasks. △ Less","24 December, 2022",https://arxiv.org/pdf/2205.09911
Single-cell Subcellular Protein Localisation Using Novel Ensembles of Diverse Deep Architectures,Syed Sameed Husain;Eng-Jon Ong;Dmitry Minskiy;Mikel Bober-Irizar;Amaia Irizar;Miroslaw Bober,"Unravelling protein distributions within individual cells is key to understanding their function and state and indispensable to developing new treatments. Here we present the Hybrid subCellular Protein Localiser (HCPL), which learns from weakly labelled data to robustly localise single-cell subcellular protein patterns. It comprises innovative DNN architectures exploiting wavelet filters and learnt parametric activations that successfully tackle drastic cell variability. HCPL features correlation-based ensembling of novel architectures that boosts performance and aids generalisation. Large-scale data annotation is made feasible by our ""AI-trains-AI"" approach, which determines the visual integrity of cells and emphasises reliable labels for efficient training. In the Human Protein Atlas context, we demonstrate that HCPL defines state-of-the-art in the single-cell classification of protein localisation patterns. To better understand the inner workings of HCPL and assess its biological relevance, we analyse the contributions of each system component and dissect the emergent features from which the localisation predictions are derived. △ Less","16 September, 2022",https://arxiv.org/pdf/2205.09841
"A toolbox for idea generation and evaluation: Machine learning, data-driven, and contest-driven approaches to support idea generation",Workneh Yilma Ayele,"The significance and abundance of data are increasing due to the growing digital data generated from social media, sensors, scholarly literature, patents, different forms of documents published online, databases, product manuals, etc. Various data sources can be used to generate ideas, yet, in addition to bias, the size of the available digital data is a major challenge when it comes to manual analysis. Hence, human-machine interaction is essential for generating valuable ideas where machine learning and data-driven techniques generate patterns from data and serve human sense-making. However, the use of machine learning and data-driven approaches to generate ideas is a relatively new area. Moreover, it is also possible to stimulate innovation using contest-driven idea generation and evaluation. The results and contributions of this thesis can be viewed as a toolbox of idea-generation techniques, including a list of data-driven and machine learning techniques with corresponding data sources and models to support idea generation. In addition, the results include two models, one method and one framework, to better support data-driven and contest- driven idea generation. The beneficiaries of these artefacts are practitioners in data and knowledge engineering, data mining project managers, and innovation agents. Innovation agents include incubators, contest organizers, consultants, innovation accelerators, and industries. Since the proposed artefacts consist of process models augmented with AI techniques, human-centred AI is a promising area of research that can contribute to the artefacts' further development and promote creativity. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.09840
Practical Skills Demand Forecasting via Representation Learning of Temporal Dynamics,Maysa M. Garcia de Macedo;Wyatt Clarke;Eli Lucherini;Tyler Baldwin;Dilermando Queiroz Neto;Rogerio de Paula;Subhro Das,"Rapid technological innovation threatens to leave much of the global workforce behind. Today's economy juxtaposes white-hot demand for skilled labor against stagnant employment prospects for workers unprepared to participate in a digital economy. It is a moment of peril and opportunity for every country, with outcomes measured in long-term capital allocation and the life satisfaction of billions of workers. To meet the moment, governments and markets must find ways to quicken the rate at which the supply of skills reacts to changes in demand. More fully and quickly understanding labor market intelligence is one route. In this work, we explore the utility of time series forecasts to enhance the value of skill demand data gathered from online job advertisements. This paper presents a pipeline which makes one-shot multi-step forecasts into the future using a decade of monthly skill demand observations based on a set of recurrent neural network methods. We compare the performance of a multivariate model versus a univariate one, analyze how correlation between skills can influence multivariate model results, and present predictions of demand for a selection of skills practiced by workers in the information technology industry. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.09508
Neural ODEs with Irregular and Noisy Data,Pawan Goyal;Peter Benner,"Measurement noise is an integral part while collecting data of a physical process. Thus, noise removal is necessary to draw conclusions from these data, and it often becomes essential to construct dynamical models using these data. We discuss a methodology to learn differential equation(s) using noisy and irregular sampled measurements. In our methodology, the main innovation can be seen in the integration of deep neural networks with the neural ordinary differential equations (ODEs) approach. Precisely, we aim at learning a neural network that provides (approximately) an implicit representation of the data and an additional neural network that models the vector fields of the dependent variables. We combine these two networks by constraining using neural ODEs. The proposed framework to learn a model describing the vector field is highly effective under noisy measurements. The approach can handle scenarios where dependent variables are not available at the same temporal grid. Moreover, a particular structure, e.g., second-order with respect to time, can easily be incorporated. We demonstrate the effectiveness of the proposed method for learning models using data obtained from various differential equations and present a comparison with the neural ODE method that does not make any special treatment to noise. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.09479
Truncated tensor Schatten p-norm based approach for spatiotemporal traffic data imputation with complicated missing patterns,Tong Nie;Guoyang Qin;Jian Sun,"Rapid advances in sensor, wireless communication, cloud computing and data science have brought unprecedented amount of data to assist transportation engineers and researchers in making better decisions. However, traffic data in reality often has corrupted or incomplete values due to detector and communication malfunctions. Data imputation is thus required to ensure the effectiveness of downstream data-driven applications. To this end, numerous tensor-based methods treating the imputation problem as the low-rank tensor completion (LRTC) have been attempted in previous works. To tackle rank minimization, which is at the core of the LRTC, most of aforementioned methods utilize the tensor nuclear norm (NN) as a convex surrogate for the minimization. However, the over-relaxation issue in NN refrains it from desirable performance in practice. In this paper, we define an innovative nonconvex truncated Schatten p-norm for tensors (TSpN) to approximate tensor rank and impute missing spatiotemporal traffic data under the LRTC framework. We model traffic data into a third-order tensor structure of (time intervals,locations (sensors),days) and introduce four complicated missing patterns, including random missing and three fiber-like missing cases according to the tensor mode-n fibers. Despite nonconvexity of the objective function in our model, we derive the global optimal solutions by integrating the alternating direction method of multipliers (ADMM) with generalized soft-thresholding (GST). In addition, we design a truncation rate decay strategy to deal with varying missing rate scenarios. Comprehensive experiments are finally conducted using real-world spatiotemporal datasets, which demonstrate that the proposed LRTC-TSpN method performs well under various missing cases, meanwhile outperforming other SOTA tensor-based imputation models in almost all scenarios. △ Less","19 May, 2022",https://arxiv.org/pdf/2205.09390
VLC Physical Layer Security through RIS-aided Jamming Receiver for 6G Wireless Networks,Simone Soderi;Alessandro Brighente;Federico Turrin;Mauro Conti,"Visible Light Communication (VLC) is one the most promising enabling technology for future 6G networks to overcome Radio-Frequency (RF)-based communication limitations thanks to a broader bandwidth, higher data rate, and greater efficiency. However, from the security perspective, VLCs suffer from all known wireless communication security threats (e.g., eavesdropping and integrity attacks). For this reason, security researchers are proposing innovative Physical Layer Security (PLS) solutions to protect such communication. Among the different solutions, the novel Reflective Intelligent Surface (RIS) technology coupled with VLCs has been successfully demonstrated in recent work to improve the VLC communication capacity. However, to date, the literature still lacks analysis and solutions to show the PLS capability of RIS-based VLC communication. In this paper, we combine watermarking and jamming primitives through the Watermark Blind Physical Layer Security (WBPLSec) algorithm to secure VLC communication at the physical layer. Our solution leverages RIS technology to improve the security properties of the communication. By using an optimization framework, we can calculate RIS phases to maximize the WBPLSec jamming interference schema over a predefined area in the room. In particular, compared to a scenario without RIS, our solution improves the performance in terms of secrecy capacity without any assumption about the adversary's location. We validate through numerical evaluations the positive impact of RIS-aided solution to increase the secrecy capacity of the legitimate jamming receiver in a VLC indoor scenario. Our results show that the introduction of RIS technology extends the area where secure communication occurs and that by increasing the number of RIS elements the outage probability decreases. △ Less","7 June, 2022",https://arxiv.org/pdf/2205.09026
Improving Pedestrian Priority via Grouping and Virtual Lanes,Yao Li;Vinu Kamalasanan;Mariana Batista;Monika Sester,"The shared space design is applied in urban streets to support barrier-free movement and integrate traffic participants (such as pedestrians, cyclists and vehicles) into a common road space. Regardless of the low-speed environment, sharing space with motor vehicles can make vulnerable road users feel uneasy. Yet, walking in groups increases their confidence as well as influence the yielding behavior of drivers. Therefore, we propose an innovative approach to support the crossing of pedestrians via grouping and project the virtual lanes in shared spaces. This paper presents the important components of the crowd steering system, discusses the enablers and gaps in the current approach, and illustrates the proposed idea with concept diagrams. △ Less","18 May, 2022",https://arxiv.org/pdf/2205.08783
Intuitive and Efficient Human-robot Collaboration via Real-time Approximate Bayesian Inference,Javier Felip Leon;David Gonzalez-Aguirre;Lama Nachman,"The combination of collaborative robots and end-to-end AI, promises flexible automation of human tasks in factories and warehouses. However, such promise seems a few breakthroughs away. In the meantime, humans and cobots will collaborate helping each other. For these collaborations to be effective and safe, robots need to model, predict and exploit human's intents for responsive decision making processes. Approximate Bayesian Computation (ABC) is an analysis-by-synthesis approach to perform probabilistic predictions upon uncertain quantities. ABC includes priors conveniently, leverages sampling algorithms for inference and is flexible to benefit from complex models, e.g. via simulators. However, ABC is known to be computationally too intensive to run at interactive frame rates required for effective human-robot collaboration tasks. In this paper, we formulate human reaching intent prediction as an ABC problem and describe two key performance innovations which allow computations at interactive rates. Our real-world experiments with a collaborative robot set-up, demonstrate the viability of our proposed approach. Experimental evaluations convey the advantages and value of human intent prediction for packing cooperative tasks. Qualitative results show how anticipating human's reaching intent improves human-robot collaboration without compromising safety. Quantitative task fluency metrics confirm the qualitative claims. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.08657
Internet of Spacecraft for Multi-planetary Defense and Prosperity,Yiming Huo,"Recent years have seen unprecedentedly fast-growing prosperity in the commercial space industry. Several privately funded aerospace manufacturers, such as Space Exploration Technologies Corporation (SpaceX) and Blue Origin have innovated what we used to know about this capital-intense industry and gradually reshaped the future of human civilization. As private spaceflight and multi-planetary immigration gradually become realities from science fiction (sci-fi) and theory, both opportunities and challenges are presented. In this article, a review of the progress in space exploration and the underlying space technologies is firstly provided. For the next, a revisit and a prediction are paid and made to the K-Pg extinction event, the Chelyabinsk event, extra-terrestrialization, terraforming, planetary defense, including the emerging near-Earth object (NEO) observation and NEO impact avoidance technologies and strategies. Furthermore, a framework of the Solar Communication and Defense Networks (SCADN) with advanced algorithms and high efficacy is proposed to enable an internet of distributed deep-space sensing, communications, and defense to cope with disastrous incidents such as asteroid/comet impacts. Furthermore, the perspectives on the legislation, management, and supervision of founding the proposed SCADN are also discussed in depth. △ Less","15 May, 2022",https://arxiv.org/pdf/2205.08567
Towards Resilient Access Equality for 6G Serverless p-LEO Satellite Networks,Lin Shih-Chun;Lin Chia-Hung;Chu Liang C.;Lien Shao-Yu,"Low earth orbit (LEO) mega-constellations, integrating government space systems and commercial practices, have emerged as enabling technologies for the sixth generation (6G) networks due to their good merits of global coverage and ubiquitous services for military and civilian use cases. However, convergent LEO-based satellite networking infrastructures still lack leveraging the synergy of space and terrestrial systems. This paper, therefore, extends conventional serverless cloud platforms with serverless edge learning architectures for 6G proliferated LEO (p-LEO) satellite ecosystems and provides a new distributed training design from a networking perspective. The proposed design dynamically orchestrates communications and computation functionalities and resources among heterogeneous physical units to efficiently fulfill multi-agent deep reinforcement learning for service-level agreements. Innovative ecosystem enhancements, including ultrabroadband access, anti-jammed transmissions, resilient networking, and related open challenges, are also investigated for end-to-end connectivity, communications, and learning performance. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.08430
Working with Affective Computing: Exploring UK Public Perceptions of AI enabled Workplace Surveillance,Lachlan Urquhart;Alex Laffer;Diana Miranda,"This paper explores public perceptions around the role of affective computing in the workplace. It uses a series of design fictions with 46 UK based participants, unpacking their perspectives on the advantages and disadvantages of tracking the emotional state of workers. The scenario focuses on mundane uses of biometric sensing in a sales environment, and how this could shape management approaches with workers. The paper structure is as follows: section 1 provides a brief introduction; section 2 provides an overview of the innovative design fiction methodology; section 3 explores wider shifts around IT in the workplace; section 4 provides some legal analysis exploring emergence of AI in the workplace; and section 5 presents themes from the study data. The latter section includes discussion on concerns around functionality and accuracy of affective computing systems, and their impacts on surveillance, human agency, and worker/management interactions. △ Less","17 May, 2022",https://arxiv.org/pdf/2205.08264
Near out-of-distribution detection for low-resolution radar micro-Doppler signatures,Martin Bauw;Santiago Velasco-Forero;Jesus Angulo;Claude Adnet;Olivier Airiau,"Near out-of-distribution detection (OODD) aims at discriminating semantically similar data points without the supervision required for classification. This paper puts forward an OODD use case for radar targets detection extensible to other kinds of sensors and detection scenarios. We emphasize the relevance of OODD and its specific supervision requirements for the detection of a multimodal, diverse targets class among other similar radar targets and clutter in real-life critical systems. We propose a comparison of deep and non-deep OODD methods on simulated low-resolution pulse radar micro-Doppler signatures, considering both a spectral and a covariance matrix input representation. The covariance representation aims at estimating whether dedicated second-order processing is appropriate to discriminate signatures. The potential contributions of labeled anomalies in training, self-supervised learning, contrastive learning insights and innovative training losses are discussed, and the impact of training set contamination caused by mislabelling is investigated. △ Less","5 July, 2022",https://arxiv.org/pdf/2205.07869
Social Aspects of Software Testing: Comparative Studies in Asia,Luiz Fernando Capretz;Jingdong Jia;Pradeep Waychal;Shuib Basri,"This study attempts to understand motivators and de-motivators that influence the decisions of software students to take up and sustain software testing careers across three different Asian countries, i.e., China, India, and Malaysia. The re-search question can be framed as How many software students across different Asian geographies are keen to take up testing careers, and what are the reasons for their choices? Towards an answer, we developed a cross-sectional but simple survey-based instrument. In this work, we investigated how software students perceived the software testing role. The results from China and India revealed that students are not very keen on taking up a software tester career, but the Malaysia students show a more positive attitude towards software testing. The study also pointed out the importance of considering software testing activities as a set of human-dependent tasks and emphasized the need for further re-search that examines critically individual assessments of software testers about software testing activities. This investigation can academics involved in software testing courses to understand the impacting factors on the motivation and de-motivators of their students, as well as to try convey positive view of testing as challenging and requires critical thinking and innovative ideas. △ Less","16 May, 2022",https://arxiv.org/pdf/2205.07781
Mask CycleGAN: Unpaired Multi-modal Domain Translation with Interpretable Latent Variable,Minfa Wang,"We propose Mask CycleGAN, a novel architecture for unpaired image domain translation built based on CycleGAN, with an aim to address two issues: 1) unimodality in image translation and 2) lack of interpretability of latent variables. Our innovation in the technical approach is comprised of three key components: masking scheme, generator and objective. Experimental results demonstrate that this architecture is capable of bringing variations to generated images in a controllable manner and is reasonably robust to different masks. △ Less","14 May, 2022",https://arxiv.org/pdf/2205.06969
Review on Panoramic Imaging and Its Applications in Scene Understanding,Shaohua Gao;Kailun Yang;Hao Shi;Kaiwei Wang;Jian Bai,"With the rapid development of high-speed communication and artificial intelligence technologies, human perception of real-world scenes is no longer limited to the use of small Field of View (FoV) and low-dimensional scene detection devices. Panoramic imaging emerges as the next generation of innovative intelligent instruments for environmental perception and measurement. However, while satisfying the need for large-FoV photographic imaging, panoramic imaging instruments are expected to have high resolution, no blind area, miniaturization, and multidimensional intelligent perception, and can be combined with artificial intelligence methods towards the next generation of intelligent instruments, enabling deeper understanding and more holistic perception of 360-degree real-world surrounding environments. Fortunately, recent advances in freeform surfaces, thin-plate optics, and metasurfaces provide innovative approaches to address human perception of the environment, offering promising ideas beyond conventional optical imaging. In this review, we begin with introducing the basic principles of panoramic imaging systems, and then describe the architectures, features, and functions of various panoramic imaging systems. Afterwards, we discuss in detail the broad application prospects and great design potential of freeform surfaces, thin-plate optics, and metasurfaces in panoramic imaging. We then provide a detailed analysis on how these techniques can help enhance the performance of panoramic imaging systems. We further offer a detailed analysis of applications of panoramic imaging in scene understanding for autonomous driving and robotics, spanning panoramic semantic image segmentation, panoramic depth estimation, panoramic visual localization, and so on. Finally, we cast a perspective on future potential and research directions for panoramic imaging instruments. △ Less","14 October, 2022",https://arxiv.org/pdf/2205.05570
Comparison of Brick and Project Haystack to Support Smart Building Applications,Caroline Quinn;J. J. McArthur,"Enabling buildings with Smart Building applications will help to achieve the ongoing efficient commissioning of buildings, ultimately attaining peak performance in energy use and improved occupant health and comfort, at minimum cost. For these technologies to be scalable data ontology must be adopted to semantically represent data generated by building mechanical systems, acting as conduit for connection to Smart Building applications. The viability of Brick and Project Haystack ontologies, as found by industry and academia, prompted a quantitative comparison of completeness and expressiveness using a case study with an industry ontology as the baseline. Additionally, a qualitative comparison was completed using key ontology qualities outlined in literature. A recommendation of Brick is made based on results. Brick achieved higher assessment values in completeness and expressiveness achieving 59% and 100% respectively, as compared to Haystacks 43% and 96%. Additionally, Brick exhibited five of six desirable qualities, where Haystack exhibited only three. The recommendation of the appropriate ontology forms the basis for longer-term Smart Building application development, which will support innovative approaches to sustainability in building operations across scale, as well as next-generation building controls and automation strategies. △ Less","25 August, 2022",https://arxiv.org/pdf/2205.05521
Keep Your Friends Close and Your Counterfactuals Closer: Improved Learning From Closest Rather Than Plausible Counterfactual Explanations in an Abstract Setting,Ulrike Kuhl;André Artelt;Barbara Hammer,"Counterfactual explanations (CFEs) highlight what changes to a model's input would have changed its prediction in a particular way. CFEs have gained considerable traction as a psychologically grounded solution for explainable artificial intelligence (XAI). Recent innovations introduce the notion of computational plausibility for automatically generated CFEs, enhancing their robustness by exclusively creating plausible explanations. However, practical benefits of such a constraint on user experience and behavior is yet unclear. In this study, we evaluate objective and subjective usability of computationally plausible CFEs in an iterative learning design targeting novice users. We rely on a novel, game-like experimental design, revolving around an abstract scenario. Our results show that novice users actually benefit less from receiving computationally plausible rather than closest CFEs that produce minimal changes leading to the desired outcome. Responses in a post-game survey reveal no differences in terms of subjective user experience between both groups. Following the view of psychological plausibility as comparative similarity, this may be explained by the fact that users in the closest condition experience their CFEs as more psychologically plausible than the computationally plausible counterpart. In sum, our work highlights a little-considered divergence of definitions of computational plausibility and psychological plausibility, critically confirming the need to incorporate human behavior, preferences and mental models already at the design stages of XAI approaches. In the interest of reproducible research, all source code, acquired user data, and evaluation scripts of the current study are available: https://github.com/ukuhl/PlausibleAlienZoo △ Less","11 May, 2022",https://arxiv.org/pdf/2205.05515
Bayesian Prior Learning via Neural Networks for Next-item Recommendation,Manoj Reddy Dareddy;Zijun Xue;Nicholas Lin;Junghoo Cho,"Next-item prediction is a a popular problem in the recommender systems domain. As the name suggests, the task is to recommend subsequent items that a user would be interested in given contextual information and historical interaction data. In our paper, we model a general notion of context via a sequence of item interactions. We model the next item prediction problem using the Bayesian framework and capture the probability of appearance of a sequence through the posterior mean of the Beta distribution. We train two neural networks to accurately predict the alpha & beta parameter values of the Beta distribution. Our novel approach of combining black-box style neural networks, known to be suitable for function approximation with Bayesian estimation methods have resulted in an innovative method that outperforms various state-of-the-art baselines. We demonstrate the effectiveness of our method in two real world datasets. Our framework is an important step towards the goal of building privacy preserving recommender systems. △ Less","10 May, 2022",https://arxiv.org/pdf/2205.05209
How Game Jams and Hackathons Accelerate Design Processes,Jeanette Falk,"This dissertation presents three years of research on how design processes in game jams and hackathons can be understood as accelerated. Hackathons and game jams can both be described as formats where participants engage in designing and developing prototypes during an intentionally short time frame, such as 48 hours, which is meant to facilitate creativity, and encourage fast decision making and rapid prototyping. Game jams and hackathons are organised in many different contexts and for many different purposes as well, such as: internally in companies to spark new ideas; for fostering citizen innovation for municipalities; in cultural and governmental agencies; integral parts of education; entry points for developers wanting to enter especially the game industry (Olesen, 2020; Kultima, 2015). During the recent decade, game jams and hackathons have been introduced to academia as well, as formats for teaching and learning, and as research platforms as well. Only few research contributions engage with understanding how accelerated design processes in game jams and hackathons unfold, or how the organisation of game jam and hackathon formats influence these accelerated design processes. The main contributions of my PhD project are: 1) Descriptive process-level knowledge, which contextualise and solidify how accelerated design processes unfold under the circumstances of a game jam and a hackathon. 2) Overviews of how game jams have been organised for supporting participants' creativity and of how hackathons have been used as means and as research focus within academia. 3) Exploring how game jam and hackathon formats may be organised in order to support knowledge generation such as within academia, and in order to support creativity. △ Less","11 May, 2022",https://arxiv.org/pdf/2205.04966
When does dough become a bagel? Analyzing the remaining mistakes on ImageNet,Vijay Vasudevan;Benjamin Caine;Raphael Gontijo-Lopes;Sara Fridovich-Keil;Rebecca Roelofs,"Image classification accuracy on the ImageNet dataset has been a barometer for progress in computer vision over the last decade. Several recent papers have questioned the degree to which the benchmark remains useful to the community, yet innovations continue to contribute gains to performance, with today's largest models achieving 90%+ top-1 accuracy. To help contextualize progress on ImageNet and provide a more meaningful evaluation for today's state-of-the-art models, we manually review and categorize every remaining mistake that a few top models make in order to provide insight into the long-tail of errors on one of the most benchmarked datasets in computer vision. We focus on the multi-label subset evaluation of ImageNet, where today's best models achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly half of the supposed mistakes are not mistakes at all, and we uncover new valid multi-labels, demonstrating that, without careful review, we are significantly underestimating the performance of these models. On the other hand, we also find that today's best models still make a significant number of mistakes (40%) that are obviously wrong to human reviewers. To calibrate future progress on ImageNet, we provide an updated multi-label evaluation set, and we curate ImageNet-Major: a 68-example ""major error"" slice of the obvious mistakes made by today's top models -- a slice where models should achieve near perfection, but today are far from doing so. △ Less","25 May, 2022",https://arxiv.org/pdf/2205.04596
A Music-Therapy Robotic Platform for Children with Autism: A Pilot Study,Huanghao Fengr;Mohammad H. Mahoor;Francesca Dino,"Children with Autism Spectrum Disorder (ASD) experience deficits in verbal and nonverbal communication skills including motor control, turn-taking, and emotion recognition. Innovative technology, such as socially assistive robots, has shown to be a viable method for Autism therapy. This paper presents a novel robot-based music-therapy platform for modeling and improving the social responses and behaviors of children with ASD. Our autonomous social interactive system consists of three modules. We adopted Short-time Fourier Transform and Levenshtein distance to fulfill the design requirements: a) ""music detection"" and b) ""smart scoring and feedback"", which allows NAO to understand music and provide additional practice and oral feedback to the users as applicable. We designed and implemented six Human-Robot-Interaction (HRI) sessions including four intervention sessions. Nine children with ASD and seven Typically Developing participated in a total of fifty HRI experimental sessions. Using our platform, we collected and analyzed data on social behavioral changes and emotion recognition using Electrodermal Activity (EDA) signals. The results of our experiments demonstrate most of the participants were able to complete motor control tasks with ~70% accuracy. Six out of the 9 ASD participants showed stable turn-taking behavior when playing music. The results of automated emotion classification using Support Vector Machines illustrate that emotional arousal in the ASD group can be detected and well recognized via EDA bio-signals. In summary, the results of our data analyses, including emotion classification using EDA signals, indicate that the proposed robot-music based therapy platform is an attractive and promising assistive tool to facilitate the improvement of fine motor control and turn-taking skills in children with ASD. △ Less","9 May, 2022",https://arxiv.org/pdf/2205.04251
AI Based Digital Twin Model for Cattle Caring,Xue Han;Zihuai Lin,"In this paper, we developed innovative digital twins of cattle status that are powered by artificial intelligence (AI). The work was built on a farm IoT system that remotely monitors and tracks the state of cattle. A digital twin model of cattle health based on Deep Learning (DL) was generated using the sensor data acquired from the farm IoT system. The health and physiological cycle of cattle can be monitored in real time, and the state of the next physiological cycle of cattle can be anticipated using this model. The basis of this work is the vast amount of data which is required to validate the legitimacy of the digital twins model. In terms of behavioural state, it was found that the cattle treated with a combination of topical anaesthetic and meloxicam exhibits the least pain reaction. The digital twins model developed in this work can be used to monitor the health of cattle △ Less","9 May, 2022",https://arxiv.org/pdf/2205.04034
A Survey on AI Sustainability: Emerging Trends on Learning Algorithms and Research Challenges,Zhenghua Chen;Min Wu;Alvin Chan;Xiaoli Li;Yew-Soon Ong,"Artificial Intelligence (AI) is a fast-growing research and development (R&D) discipline which is attracting increasing attention because of its promises to bring vast benefits for consumers and businesses, with considerable benefits promised in productivity growth and innovation. To date it has reported significant accomplishments in many areas that have been deemed as challenging for machines, ranging from computer vision, natural language processing, audio analysis to smart sensing and many others. The technical trend in realizing the successes has been towards increasing complex and large size AI models so as to solve more complex problems at superior performance and robustness. This rapid progress, however, has taken place at the expense of substantial environmental costs and resources. Besides, debates on the societal impacts of AI, such as fairness, safety and privacy, have continued to grow in intensity. These issues have presented major concerns pertaining to the sustainable development of AI. In this work, we review major trends in machine learning approaches that can address the sustainability problem of AI. Specifically, we examine emerging AI methodologies and algorithms for addressing the sustainability issue of AI in two major aspects, i.e., environmental sustainability and social sustainability of AI. We will also highlight the major limitations of existing studies and propose potential research challenges and directions for the development of next generation of sustainable AI techniques. We believe that this technical review can help to promote a sustainable development of AI R&D activities for the research community. △ Less","8 May, 2022",https://arxiv.org/pdf/2205.03824
A Review on Viewpoints and Path-planning for UAV-based 3D Reconstruction,Mehdi Maboudi;MohammadReza Homaei;Soohwan Song;Shirin Malihi;Mohammad Saadatseresht;Markus Gerke,"Unmanned aerial vehicles (UAVs) are widely used platforms to carry data capturing sensors for various applications. The reason for this success can be found in many aspects: the high maneuverability of the UAVs, the capability of performing autonomous data acquisition, flying at different heights, and the possibility to reach almost any vantage point. The selection of appropriate viewpoints and planning the optimum trajectories of UAVs is an emerging topic that aims at increasing the automation, efficiency and reliability of the data capturing process to achieve a dataset with desired quality. On the other hand, 3D reconstruction using the data captured by UAVs is also attracting attention in research and industry. This review paper investigates a wide range of model-free and model-based algorithms for viewpoint and path planning for 3D reconstruction of large-scale objects. The analyzed approaches are limited to those that employ a single-UAV as a data capturing platform for outdoor 3D reconstruction purposes. In addition to discussing the evaluation strategies, this paper also highlights the innovations and limitations of the investigated approaches. It concludes with a critical analysis of the existing challenges and future research perspectives. △ Less","7 May, 2022",https://arxiv.org/pdf/2205.03716
Anomaly Detection in Intra-Vehicle Networks,Ajeet Kumar Dwivedi,"The progression of innovation and technology and ease of inter-connectivity among networks has allowed us to evolve towards one of the promising areas, the Internet of Vehicles. Nowadays, modern vehicles are connected to a range of networks, including intra-vehicle networks and external networks. However, a primary challenge in the automotive industry is to make the vehicle safe and reliable; particularly with the loopholes in the existing traditional protocols, cyber-attacks on the vehicle network are rising drastically. Practically every vehicle uses the universal Controller Area Network (CAN) bus protocol for the communication between electronic control units to transmit key vehicle functionality and messages related to driver safety. The CAN bus system, although its critical significance, lacks the key feature of any protocol authentication and authorization. Resulting in compromises of CAN bus security leads to serious issues to both car and driver safety. This paper discusses the security issues of the CAN bus protocol and proposes an Intrusion Detection System (IDS) that detects known attacks on in-vehicle networks. Multiple Artificial Intelligence (AI) algorithms are employed to provide recognition of known potential cyber-attacks based on messages, timestamps, and data packets traveling through the CAN. The main objective of this paper is to accurately detect cyberattacks by considering time-series features and attack frequency. The majority of the evaluated AI algorithms, when considering attack frequency, correctly identify known attacks with remarkable accuracy of more than 99%. However, these models achieve approximately 92% to 97% accuracy when timestamps are not taken into account. Long Short Term Memory (LSTM), Xgboost, and SVC have proved to the well-performing classifiers. △ Less","6 May, 2022",https://arxiv.org/pdf/2205.03537
Inferring electrochemical performance and parameters of Li-ion batteries based on deep operator networks,Qiang Zheng;Xiaoguang Yin;Dongxiao Zhang,"The Li-ion battery is a complex physicochemical system that generally takes applied current as input and terminal voltage as output. The mappings from current to voltage can be described by several kinds of models, such as accurate but inefficient physics-based models, and efficient but sometimes inaccurate equivalent circuit and black-box models. To realize accuracy and efficiency simultaneously in battery modeling, we propose to build a data-driven surrogate for a battery system while incorporating the underlying physics as constraints. In this work, we innovatively treat the functional mapping from current curve to terminal voltage as a composite of operators, which is approximated by the powerful deep operator network (DeepONet). Its learning capability is firstly verified through a predictive test for Li-ion concentration at two electrodes. In this experiment, the physics-informed DeepONet is found to be more robust than the purely data-driven DeepONet, especially in temporal extrapolation scenarios. A composite surrogate is then constructed for mapping current curve and solid diffusivity to terminal voltage with three operator networks, in which two parallel physics-informed DeepONets are firstly used to predict Li-ion concentration at two electrodes, and then based on their surface values, a DeepONet is built to give terminal voltage predictions. Since the surrogate is differentiable anywhere, it is endowed with the ability to learn from data directly, which was validated by using terminal voltage measurements to estimate input parameters. The proposed surrogate built upon operator networks possesses great potential to be applied in on-board scenarios, such as battery management system, since it integrates efficiency and accuracy by incorporating underlying physics, and also leaves an interface for model refinement through a totally differentiable model structure. △ Less","6 May, 2022",https://arxiv.org/pdf/2205.03508
Living Innovation Lab: A Human Centric Computing toward Healthy Living,Swati Banerjee,"Living Lab is an umbrella term used for referring to a methodology of user-centric innovation in real-life environments within a wider network of relevant stake holders. Real-life environment refers to living houses and hospitals inter wined and connected together in a way which promotes direct usability of research by the end users. It primarily consists of three stages, Design thinking to actual Conceptualisation, Evaluation and Prototyping and Final product prototyping to commercialisation. The increasing demand of cutting age healthcare system is in itself a challenge and requires user involvement to mobilise knowledge to build a patient centered and knowledge-based economy. Innovations are constantly needed to reduce the problematic barriers to efficient knowledge exchange and improve collaborative problem solving. Living Innovation Lab, as open knowledge system, have immense potential to address these gaps that are underexplored in the healthcare system. △ Less","6 May, 2022",https://arxiv.org/pdf/2205.03324
Investigation of large-scale extended Granger causality (lsXGC) on synthetic functional MRI data,Axel Wismüller;Ali Vosoughi;Adora DSouza;Anas Abidin,"It is a challenging research endeavor to infer causal relationships in multivariate observational time-series. Such data may be represented by graphs, where nodes represent time-series, and edges directed causal influence scores between them. If the number of nodes exceeds the number of temporal observations, conventional methods, such as standard Granger causality, are of limited value, because estimating free parameters of time-series predictors lead to underdetermined problems. A typical example for this situation is functional Magnetic Resonance Imaging (fMRI), where the number of nodal observations is large, usually ranging from 10^2 to 10^5 time-series, while the number of temporal observations is low, usually less than 10^3. Hence, innovative approaches are required to address the challenges arising from such data sets. Recently, we have proposed the large-scale Extended Granger Causality (lsXGC) algorithm, which is based on augmenting a dimensionality-reduced representation of the system's state-space by supplementing data from the conditional source time-series taken from the original input space. Here, we apply lsXGC on synthetic fMRI data with known ground truth and compare its performance to state-of-the-art methods by leveraging the benefits of information-theoretic approaches. Our results suggest that the proposed lsXGC method significantly outperforms existing methods, both in diagnostic accuracy with Area Under the Receiver Operating Characteristic (AUROC = 0.849 vs.~[0.727, 0.762] for competing methods, p<\!10^{-8}), and computation time (3.4 sec vs.~[9.7, 4.8 \times 10^3] sec for competing methods) benchmarks, demonstrating the potential of lsXGC for analyzing large-scale networks in neuroimaging studies of the human brain. △ Less","6 May, 2022",https://arxiv.org/pdf/2205.03029
FastRE: Towards Fast Relation Extraction with Convolutional Encoder and Improved Cascade Binary Tagging Framework,Guozheng Li;Xu Chen;Peng Wang;Jiafeng Xie;Qiqing Luo,"Recent work for extracting relations from texts has achieved excellent performance. However, most existing methods pay less attention to the efficiency, making it still challenging to quickly extract relations from massive or streaming text data in realistic scenarios. The main efficiency bottleneck is that these methods use a Transformer-based pre-trained language model for encoding, which heavily affects the training speed and inference speed. To address this issue, we propose a fast relation extraction model (FastRE) based on convolutional encoder and improved cascade binary tagging framework. Compared to previous work, FastRE employs several innovations to improve efficiency while also keeping promising performance. Concretely, FastRE adopts a novel convolutional encoder architecture combined with dilated convolution, gated unit and residual connection, which significantly reduces the computation cost of training and inference, while maintaining the satisfactory performance. Moreover, to improve the cascade binary tagging framework, FastRE first introduces a type-relation mapping mechanism to accelerate tagging efficiency and alleviate relation redundancy, and then utilizes a position-dependent adaptive thresholding strategy to obtain higher tagging accuracy and better model generalization. Experimental results demonstrate that FastRE is well balanced between efficiency and performance, and achieves 3-10x training speed, 7-15x inference speed faster, and 1/100 parameters compared to the state-of-the-art models, while the performance is still competitive. △ Less","21 May, 2022",https://arxiv.org/pdf/2205.02490
Declaration-based Prompt Tuning for Visual Question Answering,Yuhang Liu;Wei Wei;Daowan Peng;Feida Zhu,"In recent years, the pre-training-then-fine-tuning paradigm has yielded immense success on a wide spectrum of cross-modal tasks, such as visual question answering (VQA), in which a visual-language (VL) model is first optimized via self-supervised task objectives, e.g., masked language modeling (MLM) and image-text matching (ITM), and then fine-tuned to adapt to downstream task (e.g., VQA) via a brand-new objective function, e.g., answer prediction. The inconsistency of the objective forms not only severely limits the generalization of pre-trained VL models to downstream tasks, but also requires a large amount of labeled data for fine-tuning. To alleviate the problem, we propose an innovative VL fine-tuning paradigm (named Declaration-based Prompt Tuning, abbreviated as DPT), which jointly optimizes the objectives of pre-training and fine-tuning of VQA model, boosting the effective adaptation of pre-trained VL models to the downstream task. Specifically, DPT reformulates the objective form of VQA task via (1) textual adaptation, which converts the given questions into declarative sentence-form for prompt-tuning, and (2) task adaptation, which optimizes the objective function of VQA problem in the manner of pre-training phase. Experimental results on GQA dataset show that DPT outperforms the fine-tuned counterpart by a large margin regarding accuracy in both fully-supervised (2.68%) and zero-shot/few-shot (over 31%) settings. All the data and codes will be available to facilitate future research. △ Less","5 May, 2022",https://arxiv.org/pdf/2205.02456
Surface Reconstruction from Point Clouds: A Survey and a Benchmark,Zhangjin Huang;Yuxin Wen;Zihao Wang;Jinjuan Ren;Kui Jia,"Reconstruction of a continuous surface of two-dimensional manifold from its raw, discrete point cloud observation is a long-standing problem. The problem is technically ill-posed, and becomes more difficult considering that various sensing imperfections would appear in the point clouds obtained by practical depth scanning. In literature, a rich set of methods has been proposed, and reviews of existing methods are also provided. However, existing reviews are short of thorough investigations on a common benchmark. The present paper aims to review and benchmark existing methods in the new era of deep learning surface reconstruction. To this end, we contribute a large-scale benchmarking dataset consisting of both synthetic and real-scanned data; the benchmark includes object- and scene-level surfaces and takes into account various sensing imperfections that are commonly encountered in practical depth scanning. We conduct thorough empirical studies by comparing existing methods on the constructed benchmark, and pay special attention on robustness of existing methods against various scanning imperfections; we also study how different methods generalize in terms of reconstructing complex surface shapes. Our studies help identify the best conditions under which different methods work, and suggest some empirical findings. For example, while deep learning methods are increasingly popular, our systematic studies suggest that, surprisingly, a few classical methods perform even better in terms of both robustness and generalization; our studies also suggest that the practical challenges of misalignment of point sets from multi-view scanning, missing of surface points, and point outliers remain unsolved by all the existing surface reconstruction methods. We expect that the benchmark and our studies would be valuable both for practitioners and as a guidance for new innovations in future research. △ Less","4 May, 2022",https://arxiv.org/pdf/2205.02413
"FedNest: Federated Bilevel, Minimax, and Compositional Optimization",Davoud Ataee Tarzanagh;Mingchen Li;Christos Thrampoulidis;Samet Oymak,"Standard federated optimization methods successfully apply to stochastic problems with single-level structure. However, many contemporary ML problems -- including adversarial robustness, hyperparameter tuning, and actor-critic -- fall under nested bilevel programming that subsumes minimax and compositional optimization. In this work, we propose \fedblo: A federated alternating stochastic gradient method to address general nested problems. We establish provable convergence rates for \fedblo in the presence of heterogeneous data and introduce variations for bilevel, minimax, and compositional optimization. \fedblo introduces multiple innovations including federated hypergradient computation and variance reduction to address inner-level heterogeneity. We complement our theory with experiments on hyperparameter \& hyper-representation learning and minimax optimization that demonstrate the benefits of our method in practice. Code is available at https://github.com/ucr-optml/FedNest. △ Less","13 September, 2022",https://arxiv.org/pdf/2205.02215
i-Code: An Integrative and Composable Multimodal Learning Framework,Ziyi Yang;Yuwei Fang;Chenguang Zhu;Reid Pryzant;Dongdong Chen;Yu Shi;Yichong Xu;Yao Qian;Mei Gao;Yi-Ling Chen;Liyang Lu;Yujia Xie;Robert Gmyr;Noel Codella;Naoyuki Kanda;Bin Xiao;Lu Yuan;Takuya Yoshioka;Michael Zeng;Xuedong Huang,"Human intelligence is multimodal; we integrate visual, linguistic, and acoustic signals to maintain a holistic worldview. Most current pretraining methods, however, are limited to one or two modalities. We present i-Code, a self-supervised pretraining framework where users may flexibly combine the modalities of vision, speech, and language into unified and general-purpose vector representations. In this framework, data from each modality are first given to pretrained single-modality encoders. The encoder outputs are then integrated with a multimodal fusion network, which uses novel attention mechanisms and other architectural innovations to effectively combine information from the different modalities. The entire system is pretrained end-to-end with new objectives including masked modality unit modeling and cross-modality contrastive learning. Unlike previous research using only video for pretraining, the i-Code framework can dynamically process single, dual, and triple-modality data during training and inference, flexibly projecting different combinations of modalities into a single representation space. Experimental results demonstrate how i-Code can outperform state-of-the-art techniques on five video understanding tasks and the GLUE NLP benchmark, improving by as much as 11% and demonstrating the power of integrative multimodal pretraining. △ Less","5 May, 2022",https://arxiv.org/pdf/2205.01818
Meta Learning for Natural Language Processing: A Survey,Hung-yi Lee;Shang-Wen Li;Ngoc Thang Vu,"Deep learning has been the mainstream technique in natural language processing (NLP) area. However, the techniques require many labeled data and are less generalizable across domains. Meta-learning is an arising field in machine learning studying approaches to learn better learning algorithms. Approaches aim at improving algorithms in various aspects, including data efficiency and generalizability. Efficacy of approaches has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings and application of meta-learning for various NLP problems and review the development of meta-learning in NLP community. △ Less","2 July, 2022",https://arxiv.org/pdf/2205.01500
Storyteller: The papers co-citing Sleeping Beauty and Prince before awakening,Takahiro Miura;Ichiro Sakata,"In the Cumulative Advantage(CA) model, which is one of the most fundamental approaches to understand the mechanism of citation dynamics, papers receive citations depending on how much they have been already cited. On the other hand, a substantial effect not included in CA is that some surprising discoveries suddenly acquire citations after a long time from publishing. This phenomenon is known as Sleeping Beauty(SB). Since disrupting discoveries need long-time discussion by the research community to accept, SBs can capture innovative findings and reveal the nature of disruptive scientific knowledge production. To research SBs citation burst mechanism, bibliometricians consider the existence of the Prince(PR) for each SBs, which can be the trigger of SBs awakeness. For example, the discovery of Green Fluorescent Protein(GFP), which got Nobel prize in chemistry, had been overlooked for 30 years until Chalfie and Tsien, who also received the prize, developed a method to use GFP as a marker protein in genetic engineering. However, how does Chalfies and Tsiens research relight the hidden knowledge in the research community? If we can clarify such a mechanism rediscovering from nearly nothing, it can be helpful in science support and policy decision-making. This study proposes a Storyteller that focuses on the connection between SB and PR before SB gets citation burst by co-citation. PR is found to be the paper awakening SB in retrospect, but it is not easy to detect it as the trigger of SBs awakeness at the time of PR submission. We named the papers which co-cites SB and PR before the citation burst of SB as Storyteller(ST) and analyze (1) how ST contributes to broadening the novelty of SB&PR connections and (2) how much ST leads the citation burst after awakening. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.01253
Applications of Deep Learning to the Design of Enhanced Wireless Communication Systems,Mathieu Goutay,"Innovation in the physical layer of communication systems has traditionally been achieved by breaking down the transceivers into sets of processing blocks, each optimized independently based on mathematical models. Conversely, deep learning (DL)-based systems are able to handle increasingly complex tasks for which no tractable models are available. This thesis aims at comparing different approaches to unlock the full potential of DL in the physical layer. First, we describe a neural network (NN)-based block strategy, where an NN is optimized to replace a block in a communication system. We apply this strategy to introduce a multi-user multiple-input multiple-output (MU-MIMO) detector that builds on top of an existing DL-based architecture. Second, we detail an end-to-end strategy, in which the transmitter and receiver are modeled as an autoencoder. This approach is illustrated with the design of waveforms that achieve high throughputs while satisfying peak-to-average power ratio (PAPR) and adjacent channel leakage ratio (ACLR) constraints. Lastly, we propose a hybrid strategy, where multiple DL components are inserted into a traditional architecture but are trained to optimize the end-to-end performance. To demonstrate its benefits, we propose a DL-enhanced MU-MIMO receiver that both enable lower bit error rates (BERs) compared to a conventional receiver and remains scalable to any number of users. Each approach has its own strengths and shortcomings. While the first one is the easiest to implement, its individual block optimization does not ensure the overall system optimality. On the other hand, systems designed with the second approach are computationally complex but allow for new opportunities such as pilotless transmissions. Finally, the combined flexibility and end-to-end performance gains of the third approach motivate its use for short-term practical implementations. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.01210
Machine Learning and Artificial Intelligence in Circular Economy: A Bibliometric Analysis and Systematic Literature Review,Abdulla All noman;Umma Habiba Akter;Tahmid Hasan Pranto;AKM Bahalul Haque,"With unorganized, unplanned and improper use of limited raw materials, an abundant amount of waste is being produced, which is harmful to our environment and ecosystem. While traditional linear production lines fail to address far-reaching issues like waste production and a shorter product life cycle, a prospective concept, namely circular economy (CE), has shown promising prospects to be adopted at industrial and governmental levels. CE aims to complete the product life cycle loop by bringing out the highest values from raw materials in the design phase and later on by reusing, recycling, and remanufacturing. Innovative technologies like artificial intelligence (AI) and machine learning(ML) provide vital assistance in effectively adopting and implementing CE in real-world practices. This study explores the adoption and integration of applied AI techniques in CE. First, we conducted bibliometric analysis on a collection of 104 SCOPUS indexed documents exploring the critical research criteria in AI and CE. Forty papers were picked to conduct a systematic literature review from these documents. The selected documents were further divided into six categories: sustainable development, reverse logistics, waste management, supply chain management, recycle & reuse, and manufacturing development. Comprehensive research insights and trends have been extracted and delineated. Finally, the research gap needing further attention has been identified and the future research directions have also been discussed. △ Less","1 April, 2022",https://arxiv.org/pdf/2205.01042
Data Justice in Practice: A Guide for Developers,David Leslie;Michael Katell;Mhairi Aitken;Jatinder Singh;Morgan Briggs;Rosamund Powell;Cami Rincón;Antonella Perini;Smera Jayadeva;Christopher Burr,"The Advancing Data Justice Research and Practice project aims to broaden understanding of the social, historical, cultural, political, and economic forces that contribute to discrimination and inequity in contemporary ecologies of data collection, governance, and use. This is the consultation draft of a guide for developers and organisations, which are producing, procuring, or using data-intensive technologies.In the first section, we introduce the field of data justice, from its early discussions to more recent proposals to relocate understandings of what data justice means. This section includes a description of the six pillars of data justice around which this guidance revolves. Next, to support developers in designing, developing, and deploying responsible and equitable data-intensive and AI/ML systems, we outline the AI/ML project lifecycle through a sociotechnical lens. To support the operationalisation data justice throughout the entirety of the AI/ML lifecycle and within data innovation ecosystems, we then present five overarching principles of responsible, equitable, and trustworthy data research and innovation practices, the SAFE-D principles-Safety, Accountability, Fairness, Explainability, and Data Quality, Integrity, Protection, and Privacy. The final section presents guiding questions that will help developers both address data justice issues throughout the AI/ML lifecycle and engage in reflective innovation practices that ensure the design, development, and deployment of responsible and equitable data-intensive and AI/ML systems. △ Less","12 April, 2022",https://arxiv.org/pdf/2205.01037
A Survey on Security Issues in Modern Implantable Devices: Solutions and Future Issues,Emmanuel Kwarteng;Mumin Cebe,"Implantable Medical Devices (IMD) is a fast pace growing medical field and continues to grow in the foreseeable future. Advancement in science and technology has led to the IMD devices offering advanced medical treatments. Modern IMDs can automatically monitor and manage different patients' health conditions without any manual intervention from medical professionals. While IMDs are also becoming more connected to enhance the delivery of care remotely and provide the means for both patients and physicians to adjust therapy at the comfort of their homes, it also increases security related concerns. Adversaries could take advantage and exploit device vulnerabilities to manipulate device settings remotely from anywhere around the world. This manuscript reviews the current threats, security goals, and proposed solutions by comparing them with their strengths and limitations. We also highlight the emerging IMD technologies and innovative ideas for new designs and implementations to improve the security of IMDs. Finally, we conclude the article with future research directions toward securing IMD systems to light the way for researchers. △ Less","2 May, 2022",https://arxiv.org/pdf/2205.00893
ONCE-3DLanes: Building Monocular 3D Lane Detection,Fan Yan;Ming Nie;Xinyue Cai;Jianhua Han;Hang Xu;Zhen Yang;Chaoqiang Ye;Yanwei Fu;Michael Bi Mi;Li Zhang,"We present ONCE-3DLanes, a real-world autonomous driving dataset with lane layout annotation in 3D space. Conventional 2D lane detection from a monocular image yields poor performance of following planning and control tasks in autonomous driving due to the case of uneven road. Predicting the 3D lane layout is thus necessary and enables effective and safe driving. However, existing 3D lane detection datasets are either unpublished or synthesized from a simulated environment, severely hampering the development of this field. In this paper, we take steps towards addressing these issues. By exploiting the explicit relationship between point clouds and image pixels, a dataset annotation pipeline is designed to automatically generate high-quality 3D lane locations from 2D lane annotations in 211K road scenes. In addition, we present an extrinsic-free, anchor-free method, called SALAD, regressing the 3D coordinates of lanes in image view without converting the feature map into the bird's-eye view (BEV). To facilitate future research on 3D lane detection, we benchmark the dataset and provide a novel evaluation metric, performing extensive experiments of both existing approaches and our proposed method. The aim of our work is to revive the interest of 3D lane detection in a real-world scenario. We believe our work can lead to the expected and unexpected innovations in both academia and industry. △ Less","14 May, 2022",https://arxiv.org/pdf/2205.00301
Flamingo: a Visual Language Model for Few-Shot Learning,Jean-Baptiste Alayrac;Jeff Donahue;Pauline Luc;Antoine Miech;Iain Barr;Yana Hasson;Karel Lenc;Arthur Mensch;Katie Millican;Malcolm Reynolds;Roman Ring;Eliza Rutherford;Serkan Cabi;Tengda Han;Zhitao Gong;Sina Samangooei;Marianne Monteiro;Jacob Menick;Sebastian Borgeaud;Andrew Brock;Aida Nematzadeh;Sahand Sharifzadeh;Mikolaj Binkowski;Ricardo Barreira;Oriol Vinyals,"Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data. △ Less","15 November, 2022",https://arxiv.org/pdf/2204.14198
Human's Role in-the-Loop,Avigdor Gal;Roee Shraga,"Data integration has been recently challenged by the need to handle large volumes of data, arriving at high velocity from a variety of sources, which demonstrate varying levels of veracity. This challenging setting, often referred to as big data, renders many of the existing techniques, especially those that are human-intensive, obsolete. Big data also produces technological advancements such as Internet of things, cloud computing, and deep learning, and accordingly, provides a new, exciting, and challenging research agenda. Given the availability of data and the improvement of machine learning techniques, this blog discusses the respective roles of humans and machines in achieving cognitive tasks in matching, aiming to determine whether traditional roles of humans and machines are subject to change. Such investigation, we believe, will pave a way to better utilize both human and machine resources in new and innovative manners. We shall discuss two possible modes of change, namely humans out and humans in. Humans out aim at exploring out-of-the-box latent matching reasoning using machine learning algorithms when attempting to overpower human matcher performance. Pursuing out-of-the-box thinking, machine and deep learning can be involved in matching. Humans in explores how to better involve humans in the matching loop by assigning human matchers with a symmetric role to algorithmic matcher in the matching process. △ Less","27 April, 2022",https://arxiv.org/pdf/2204.14192
RankMat : Matrix Factorization with Calibrated Distributed Embedding and Fairness Enhancement,Hao Wang,"Matrix Factorization is a widely adopted technique in the field of recommender system. Matrix Factorization techniques range from SVD, LDA, pLSA, SVD++, MatRec, Zipf Matrix Factorization and Item2Vec. In recent years, distributed word embeddings have inspired innovation in the area of recommender systems. Word2vec and GloVe have been especially emphasized in many industrial application scenario such as Xiaomi's recommender system. In this paper, we propose a new matrix factorization inspired by the theory of power law and GloVe. Instead of the exponential nature of GloVe model, we take advantage of Pareto Distribution to model our loss function. Our method is explainable in theory and easy-to-implement in practice. In the experiment section, we prove our approach is superior to vanilla matrix factorization technique and comparable with GloVe-based model in both accuracy and fairness metrics. △ Less","27 April, 2022",https://arxiv.org/pdf/2204.13016
A Survey on Formal Verification Approaches for Dependable Systems,Fayhaa Hameedi Khlaif;Shawkat Sabah Khairullah,"The complexity of digital embedded systems has been increasing in different safety-critical applications such as industrial automation, process control, transportation, and medical digital devices. The correct operation of these systems relies too heavily on the behavior of the embedded digital device. As a result, any mistake or error made during the design stage of the embedded device can change the overall functionality of the critical system and cause catastrophic consequences. To detect these errors and eliminate their effects on the system, new error detection approaches must be innovated and used in the design of the digital system. However, these methods require enormous costs and time. One of these methods being employed to solve this issue is called Verification and Validation (V&V) which confirms that the system behavior meets the requirements early in the development process, before moving on to the implementation phase. Because of their benefits and importance in the building of complex digital systems, the employment of formal V&V methods has recently attracted a lot of attention. This paper focuses on presenting various studies on formal verification approaches and how the V&V can be achieved for developing high dependable digital embedded systems △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12913
Framework for disruptive AI/ML Innovation,Wim Verleyen;William McGinnis,"This framework enables C suite executive leaders to define a business plan and manage technological dependencies for building AI/ML Solutions. The business plan of this framework provides components and background information to define strategy and analyze cost. Furthermore, the business plan represents the fundamentals of AI/ML Innovation and AI/ML Solutions. Therefore, the framework provides a menu for managing and investing in AI/ML. Finally, this framework is constructed with an interdisciplinary and holistic view of AI/ML Innovation and builds on advances in business strategy in harmony with technological progress for AI/ML. This framework incorporates value chain, supply chain, and ecosystem strategies. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12641
Novel Applications for VAE-based Anomaly Detection Systems,Luca Bergamin;Tommaso Carraro;Mirko Polato;Fabio Aiolli,"The recent rise in deep learning technologies fueled innovation and boosted scientific research. Their achievements enabled new research directions for deep generative modeling (DGM), an increasingly popular approach that can create novel and unseen data, starting from a given data set. As the technology shows promising applications, many ethical issues also arise. For example, their misuse can enable disinformation campaigns and powerful phishing attempts. Research also indicates different biases affect deep learning models, leading to social issues such as misrepresentation. In this work, we formulate a novel setting to deal with similar problems, showing that a repurposed anomaly detection system effectively generates novel data, avoiding generating specified unwanted data. We propose Variational Auto-encoding Binary Classifiers (V-ABC): a novel model that repurposes and extends the Auto-encoding Binary Classifier (ABC) anomaly detector, using the Variational Auto-encoder (VAE). We survey the limitations of existing approaches and explore many tools to show the model's inner workings in an interpretable way. This proposal has excellent potential for generative applications: models that rely on user-generated data could automatically filter out unwanted content, such as offensive language, obscene images, and misleading information. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12577
Digital Twins for Dynamic Management of Blockchain Systems,Georgios Diamantopoulos;Nikos Tziritas;Rami Bahsoon;Georgios Theodoropoulos,"Blockchain systems are challenged by the so-called Trilemma tradeoff: decentralization, scalability and security. Infrastructure and node configuration, choice of the Consensus Protocol and complexity of the application transactions are cited amongst the factors that affect the tradeoffs balance. Given that Blockchains are complex, dynamic dynamic systems, a dynamic approach to their management and reconfiguration at runtime is deemed necessary to reflect the changes in the state of the infrastructure and application. This paper introduces the utilisation of Digital Twins for this purpose. The novel contribution of the paper is design of a framework and conceptual architecture of a Digital Twin that can assist in maintaining the Trilemma tradeoffs of time critical systems. The proposed Digital Twin is illustrated via an innovative approach to dynamic selection of Consensus Protocols. Simulations results show that the proposed framework can effectively support the dynamic adaptation and management of the Blockchain △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12477
Unified GCNs: Towards Connecting GCNs with CNNs,Ziyan Zhang;Bo Jiang;Bin Luo,"Graph Convolutional Networks (GCNs) have been widely demonstrated their powerful ability in graph data representation and learning. Existing graph convolution layers are mainly designed based on graph signal processing and transform aspect which usually suffer from some limitations, such as over-smoothing, over-squashing and non-robustness, etc. As we all know that Convolution Neural Networks (CNNs) have received great success in many computer vision and machine learning. One main aspect is that CNNs leverage many learnable convolution filters (kernels) to obtain rich feature descriptors and thus can have high capacity to encode complex patterns in visual data analysis. Also, CNNs are flexible in designing their network architecture, such as MobileNet, ResNet, Xception, etc. Therefore, it is natural to arise a question: can we design graph convolutional layer as flexibly as that in CNNs? Innovatively, in this paper, we consider connecting GCNs with CNNs deeply from a general perspective of depthwise separable convolution operation. Specifically, we show that GCN and GAT indeed perform some specific depthwise separable convolution operations. This novel interpretation enables us to better understand the connections between GCNs (GCN, GAT) and CNNs and further inspires us to design more Unified GCNs (UGCNs). As two showcases, we implement two UGCNs, i.e., Separable UGCN (S-UGCN) and General UGCN (G-UGCN) for graph data representation and learning. Promising experiments on several graph representation benchmarks demonstrate the effectiveness and advantages of the proposed UGCNs. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12300
Open or not open: Are conventional radio access networks more secure and trustworthy than Open-RAN?,Felix Klement;Stefan Katzenbeisser;Vincent Ulitzsch;Juliane Krämer;Slawomir Stanczak;Zoran Utkovski;Igor Bjelakovic;Gerhard Wunder,"The Open RAN architecture is a promising and future-oriented architecture. It is intended to open up the radio access network (RAN) and enable more innovation and competition in the market. This will lead to RANs for current 5G networks, but especially for future 6G networks, evolving from the current highly integrated, vendor-specific RAN architecture towards disaggregated architectures with open interfaces that will enable to better tailor RAN solutions to the requirements of 5G and 6G applications. However, the introduction of such an open architecture substantially broadens the attack possibilities when compared to conventional RANs. In the past, this has often led to negative headlines that in summary have associated Open RAN with faulty or inadequate security. In this paper, we analyze what components are involved in an Open RAN deployment, how to assess the current state of security, and what measures need to be taken to ensure secure operation. △ Less","22 June, 2022",https://arxiv.org/pdf/2204.12227
Cross Pairwise Ranking for Unbiased Item Recommendation,Qi Wan;Xiangnan He;Xiang Wang;Jiancan Wu;Wei Guo;Ruiming Tang,"Most recommender systems optimize the model on observed interaction data, which is affected by the previous exposure mechanism and exhibits many biases like popularity bias. The loss functions, such as the mostly used pointwise Binary Cross-Entropy and pairwise Bayesian Personalized Ranking, are not designed to consider the biases in observed data. As a result, the model optimized on the loss would inherit the data biases, or even worse, amplify the biases. For example, a few popular items take up more and more exposure opportunities, severely hurting the recommendation quality on niche items -- known as the notorious Mathew effect. In this work, we develop a new learning paradigm named Cross Pairwise Ranking (CPR) that achieves unbiased recommendation without knowing the exposure mechanism. Distinct from inverse propensity scoring (IPS), we change the loss term of a sample -- we innovatively sample multiple observed interactions once and form the loss as the combination of their predictions. We prove in theory that this way offsets the influence of user/item propensity on the learning, removing the influence of data biases caused by the exposure mechanism. Advantageous to IPS, our proposed CPR ensures unbiased learning for each training instance without the need of setting the propensity scores. Experimental results demonstrate the superiority of CPR over state-of-the-art debiasing solutions in both model generalization and training efficiency. The codes are available at https://github.com/Qcactus/CPR. △ Less","26 April, 2022",https://arxiv.org/pdf/2204.12176
Surpassing the Human Accuracy: Detecting Gallbladder Cancer from USG Images with Curriculum Learning,Soumen Basu;Mayank Gupta;Pratyaksha Rana;Pankaj Gupta;Chetan Arora,"We explore the potential of CNN-based models for gallbladder cancer (GBC) detection from ultrasound (USG) images as no prior study is known. USG is the most common diagnostic modality for GB diseases due to its low cost and accessibility. However, USG images are challenging to analyze due to low image quality, noise, and varying viewpoints due to the handheld nature of the sensor. Our exhaustive study of state-of-the-art (SOTA) image classification techniques for the problem reveals that they often fail to learn the salient GB region due to the presence of shadows in the USG images. SOTA object detection techniques also achieve low accuracy because of spurious textures due to noise or adjacent organs. We propose GBCNet to tackle the challenges in our problem. GBCNet first extracts the regions of interest (ROIs) by detecting the GB (and not the cancer), and then uses a new multi-scale, second-order pooling architecture specializing in classifying GBC. To effectively handle spurious textures, we propose a curriculum inspired by human visual acuity, which reduces the texture biases in GBCNet. Experimental results demonstrate that GBCNet significantly outperforms SOTA CNN models, as well as the expert radiologists. Our technical innovations are generic to other USG image analysis tasks as well. Hence, as a validation, we also show the efficacy of GBCNet in detecting breast cancer from USG images. Project page with source code, trained models, and data is available at https://gbc-iitd.github.io/gbcnet △ Less","25 April, 2022",https://arxiv.org/pdf/2204.11433
"Applying Digital Twins in Metaverse: User Interface, Security and Privacy Challenges",Saeed Banaeian Far;Azadeh Imani Rad,"Digital Twins (DTs) are a conventional and well-known concept, proposed in 70s, that are popular in a broad spectrum of sciences, industry innovations, and consortium alliances. However, in the last few years, the growth of digital assets and online communications has attracted attention to DTs as highly accurate twins of physical objects. Metaverse, as a digital world, is a concept proposed in 1992 and has also become a popular paradigm and hot topic in public where DTs can play critical roles. This study first presents definitions, applications, and general challenges of DT and Metaverse. It then offers a three-layer architecture linking the physical world to the Metaverse through a user interface. Further, it investigates the security and privacy challenges of using DTs in Metaverse. Finally, a conclusion, including possible solutions for mentioned challenges and future works, will be provided. △ Less","24 April, 2022",https://arxiv.org/pdf/2204.11343
RealNet: Combining Optimized Object Detection with Information Fusion Depth Estimation Co-Design Method on IoT,Zhuohao Li;Fandi Gou;Qixin De;Leqi Ding;Yuanhang Zhang;Yunze Cai,"Depth Estimation and Object Detection Recognition play an important role in autonomous driving technology under the guidance of deep learning artificial intelligence. We propose a hybrid structure called RealNet: a co-design method combining the model-streamlined recognition algorithm, the depth estimation algorithm with information fusion, and deploying them on the Jetson-Nano for unmanned vehicles with monocular vision sensors. We use ROS for experiment. The method proposed in this paper is suitable for mobile platforms with high real-time request. Innovation of our method is using information fusion to compensate the problem of insufficient frame rate of output image, and improve the robustness of target detection and depth estimation under monocular vision.Object Detection is based on YOLO-v5. We have simplified the network structure of its DarkNet53 and realized a prediction speed up to 0.01s. Depth Estimation is based on the VNL Depth Estimation, which considers multiple geometric constraints in 3D global space. It calculates the loss function by calculating the deviation of the virtual normal vector VN and the label, which can obtain deeper depth information. We use PnP fusion algorithm to solve the problem of insufficient frame rate of depth map output. It solves the motion estimation depth from three-dimensional target to two-dimensional point based on corner feature matching, which is faster than VNL calculation. We interpolate VNL output and PnP output to achieve information fusion. Experiments show that this can effectively eliminate the jitter of depth information and improve robustness. At the control end, this method combines the results of target detection and depth estimation to calculate the target position, and uses a pure tracking control algorithm to track it. △ Less","24 April, 2022",https://arxiv.org/pdf/2204.11216
Musical Stylistic Analysis: A Study of Intervallic Transition Graphs via Persistent Homology,Martín Mijangos;Alessandro Bravetti;Pablo Padilla,"Topological data analysis has been recently applied to investigate stylistic signatures and trends in musical compositions. A useful tool in this area is Persistent Homology. In this paper, we develop a novel method to represent a weighted directed graph as a finite metric space and then use persistent homology to extract useful features. We apply this method to weighted directed graphs obtained from pitch transitions information of a given musical fragment and use these techniques to the study of stylistic trends. In particular, we are interested in using these tools to make quantitative stylistic comparisons. As a first illustration, we analyze a selection of string quartets by Haydn, Mozart and Beethoven and discuss possible implications of our results in terms of different approaches by these composers to stylistic exploration and variety. We observe that Haydn is stylistically the most conservative, followed by Mozart, while Beethoven is the most innovative, expanding and modifying the string quartet as a musical form. Finally we also compare the variability of different genres, namely minuets, allegros, prestos and adagios, by a given composer and conclude that the minuet is the most stable form of the string quartet movements. △ Less","23 April, 2022",https://arxiv.org/pdf/2204.11139
Industry-Academia Research Collaboration in Software Engineering: The Certus Model,Dusica Marijan;Arnaud Gotlieb,"Context: Research collaborations between software engineering industry and academia can provide significant benefits to both sides, including improved innovation capacity for industry, and real-world environment for motivating and validating research ideas. However, building scalable and effective research collaborations in software engineering is known to be challenging. While such challenges can be varied and many, in this paper we focus on the challenges of achieving participative knowledge creation supported by active dialog between industry and academia and continuous commitment to joint problem solving. Objective: This paper aims to understand what are the elements of a successful industry-academia collaboration that enable the culture of participative knowledge creation. Method: We conducted participant observation collecting qualitative data spanning 8 years of collaborative research between a software engineering research group on software V&V and the Norwegian IT sector. The collected data was analyzed and synthesized into a practical collaboration model, named the Certus Model. Results: The model is structured in seven phases, describing activities from setting up research projects to the exploitation of research results. As such, the Certus model advances other collaborations models from literature by delineating different phases covering the complete life cycle of participative research knowledge creation. Conclusion: The Certus model describes the elements of a research collaboration process between researchers and practitioners in software engineering, grounded on the principles of research knowledge co-creation and continuous commitment to joint problem solving. The model can be applied and tested in other contexts where it may be adapted to the local context through experimentation. △ Less","23 April, 2022",https://arxiv.org/pdf/2204.11039
A Fully-autonomous Framework of Unmanned Surface Vehicles in Maritime Environments using Gaussian Process Motion Planning,Jiawei Meng;Ankita Humne;Richard Bucknall;Brendan Englot;Yuanchang Liu,"Unmanned surface vehicles (USVs) are of increasing importance to a growing number of sectors in the maritime industry, including offshore exploration, marine transportation and defence operations. A major factor in the growth in use and deployment of USVs is the increased operational flexibility that is offered through use of autonomous navigation systems that generate optimised trajectories. Unlike path planning in terrestrial environments, planning in the maritime environment is more demanding as there is need to assure mitigating action is taken against the significant, random and often unpredictable environmental influences from winds and ocean currents. With the focus of these necessary requirements as the main basis of motivation, this paper proposes a novel motion planner, denoted as GPMP2*, extending the application scope of the fundamental GP-based motion planner, GPMP2, into complex maritime environments. An interpolation strategy based on Monte-Carlo stochasticity has been innovatively added to GPMP2* to produce a new algorithm named GPMP2* with Monte-Carlo stochasticity (MC-GPMP2*), which can increase the diversity of the paths generated. In parallel with algorithm design, a ROS based fully-autonomous framework for an advanced unmanned surface vehicle, the WAM-V 20 USV, has been proposed. The practicability of the proposed motion planner as well as the fully-autonomous framework have been functionally validated in a simulated inspection missions for an offshore wind farm in ROS. △ Less","21 May, 2022",https://arxiv.org/pdf/2204.10826
Boolean automata isolated cycles and tangential double-cycles dynamics,Jacques Demongeot;Tarek Melliti;Mathilde Noual;Damien Regnault;Sylvain Sené,"Our daily social and political life is more and more impacted by social networks. The functioning of our living bodies is deeply dependent on biological regulation networks such as neural, genetic, and protein networks. And the physical world in which we evolve, is also structured by systems of interacting particles. Interaction networks can be seen in all spheres of existence that concern us, and yet, our understanding of interaction networks remains severely limited by our present lack of both theoretical and applied insight into their clockworks. In the past, efforts at understanding interaction networks have mostly been directed towards applications. This has happened at the expense of developing understanding of the generic and fundamental aspects of interaction networks. Intrinsic properties of interaction networks (eg the ways in which they transmit information along entities, their ability to produce this or that kind of global dynamical behaviour depending on local interactions) are thus still not well understood. Lack of fundamental knowledge tends to limit the innovating power of applications. Without more theoretical fundamental knowledge, applications cannot evolve deeply and become more impacting. Hence, it is necessary to better apprehend and comprehend the intrinsic properties of interaction networks, notably the relations between their architecture and their dynamics and how they are affected by and set in time. In this chapter, we use the elementary mathematical model of Boolean automata networks as a formal archetype of interaction networks. We survey results concerning the role of feedback cycles and the role of intersections between feedback cycles, in shaping the asymptotic dynamical behaviours of interaction networks. △ Less","22 April, 2022",https://arxiv.org/pdf/2204.10686
Investigating User Radicalization: A Novel Dataset for Identifying Fine-Grained Temporal Shifts in Opinion,Flora Sakketou;Allison Lahnala;Liane Vogel;Lucie Flek,"There is an increasing need for the ability to model fine-grained opinion shifts of social media users, as concerns about the potential polarizing social effects increase. However, the lack of publicly available datasets that are suitable for the task presents a major challenge. In this paper, we introduce an innovative annotated dataset for modeling subtle opinion fluctuations and detecting fine-grained stances. The dataset includes a sufficient amount of stance polarity and intensity labels per user over time and within entire conversational threads, thus making subtle opinion fluctuations detectable both in long term and in short term. All posts are annotated by non-experts and a significant portion of the data is also annotated by experts. We provide a strategy for recruiting suitable non-experts. Our analysis of the inter-annotator agreements shows that the resulting annotations obtained from the majority vote of the non-experts are of comparable quality to the annotations of the experts. We provide analyses of the stance evolution in short term and long term levels, a comparison of language usage between users with vacillating and resolute attitudes, and fine-grained stance detection baselines. △ Less","29 April, 2022",https://arxiv.org/pdf/2204.10190
Study on emerging applications on data plane and optimization possibilities,Gereltsetseg Altangerel;Tejfel Mate,"By programming both the data plane and the control plane, network operators can adapt their networks to their needs. Thanks to research over the past decade, this concept has more formulized and more technologically feasible. However, since control plane programmability came first, it has already been successfully implemented in the real network and is beginning to pay off. Today, the data plane programmability is evolving very rapidly to reach this level, attracting the attention of researchers and developers: Designing data plane languages, application development on it, formulizing software switches and architecture that can run data plane codes and the applications, increasing performance of software switch, and so on. As the control plane and data plane become more open, many new innovations and technologies are emerging, but some experts warn that consumers may be confused as to which of the many technologies to choose. This is a testament to how much innovation is emerging in the network. This paper outlines some emerging applications on the data plane and offers opportunities for further improvement and optimization. Our observations show that most of the implementations are done in a test environment and have not been tested well enough in terms of performance, but there are many interesting works, for example, previous control plane solutions are being implemented in the data plane. △ Less","13 March, 2022",https://arxiv.org/pdf/2204.10186
A data filling methodology for time series based on CNN and (Bi)LSTM neural networks,Kostas Tzoumpas;Aaron Estrada;Pietro Miraglio;Pietro Zambelli,"In the process of collecting data from sensors, several circumstances can affect their continuity and validity, resulting in alterations of the data or loss of information. Although classical methods of statistics, such as interpolation-like techniques, can be used to approximate the missing data in a time series, the recent developments in Deep Learning (DL) have given impetus to innovative and much more accurate forecasting techniques. In the present paper, we develop two DL models aimed at filling data gaps, for the specific case of internal temperature time series obtained from monitored apartments located in Bolzano, Italy. The DL models developed in the present work are based on the combination of Convolutional Neural Networks (CNNs), Long Short-Term Memory Neural Networks (LSTMs), and Bidirectional LSTMs (BiLSTMs). Two key features of our models are the use of both pre- and post-gap data, and the exploitation of a correlated time series (the external temperature) in order to predict the target one (the internal temperature). Our approach manages to capture the fluctuating nature of the data and shows good accuracy in reconstructing the target time series. In addition, our models significantly improve the already good results from another DL architecture that is used as a baseline for the present work. △ Less","21 April, 2022",https://arxiv.org/pdf/2204.09994
Core Box Image Recognition and its Improvement with a New Augmentation Technique,E. E. Baraboshkin;A. E. Demidov;D. M. Orlov;D. A. Koroteev,"Most methods for automated full-bore rock core image analysis (description, colour, properties distribution, etc.) are based on separate core column analyses. The core is usually imaged in a box because of the significant amount of time taken to get an image for each core column. The work presents an innovative method and algorithm for core columns extraction from core boxes. The conditions for core boxes imaging may differ tremendously. Such differences are disastrous for machine learning algorithms which need a large dataset describing all possible data variations. Still, such images have some standard features - a box and core. Thus, we can emulate different environments with a unique augmentation described in this work. It is called template-like augmentation (TLA). The method is described and tested on various environments, and results are compared on an algorithm trained on both 'traditional' data and a mix of traditional and TLA data. The algorithm trained with TLA data provides better metrics and can detect core on most new images, unlike the algorithm trained on data without TLA. The algorithm for core column extraction implemented in an automated core description system speeds up the core box processing by a factor of 20. △ Less","20 April, 2022",https://arxiv.org/pdf/2204.08853
Learning Forward Dynamics Model and Informed Trajectory Sampler for Safe Quadruped Navigation,Yunho Kim;Chanyoung Kim;Jemin Hwangbo,"For autonomous quadruped robot navigation in various complex environments, a typical SOTA system is composed of four main modules -- mapper, global planner, local planner, and command-tracking controller -- in a hierarchical manner. In this paper, we build a robust and safe local planner which is designed to generate a velocity plan to track a coarsely planned path from the global planner. Previous works used waypoint-based methods (e.g. Proportional-Differential control and pure pursuit) which simplify the path tracking problem to local point-goal navigation. However, they suffer from frequent collisions in geometrically complex and narrow environments because of two reasons; the global planner uses a coarse and inaccurate model and the local planner is unable to track the global plan sufficiently well. Currently, deep learning methods are an appealing alternative because they can learn safety and path feasibility from experience more accurately. However, existing deep learning methods are not capable of planning for a long horizon. In this work, we propose a learning-based fully autonomous navigation framework composed of three innovative elements: a learned forward dynamics model (FDM), an online sampling-based model-predictive controller, and an informed trajectory sampler (ITS). Using our framework, a quadruped robot can autonomously navigate in various complex environments without a collision and generate a smoother command plan compared to the baseline method. Furthermore, our method can reactively handle unexpected obstacles on the planned path and avoid them. Project page https://awesomericky.github.io/projects/FDM_ITS_navigation/. △ Less","20 April, 2022",https://arxiv.org/pdf/2204.08647
An alternative approach for distributed parameter estimation under Gaussian settings,Subhro Das,"This paper takes a different approach for the distributed linear parameter estimation over a multi-agent network. The parameter vector is considered to be stochastic with a Gaussian distribution. The sensor measurements at each agent are linear and corrupted with additive white Gaussian noise. Under such settings, this paper presents a novel distributed estimation algorithm that fuses the the concepts of consensus and innovations by incorporating the consensus terms (of neighboring estimates) into the innovation terms. Under the assumption of distributed parameter observability, introduced in this paper, we design the optimal gain matrices such that the distributed estimates are consistent and achieves fast convergence. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.08317
ML_LTU at SemEval-2022 Task 4: T5 Towards Identifying Patronizing and Condescending Language,Tosin Adewumi;Lama Alkhaled;Hamam Mokayed;Foteini Liwicki;Marcus Liwicki,"This paper describes the system used by the Machine Learning Group of LTU in subtask 1 of the SemEval-2022 Task 4: Patronizing and Condescending Language (PCL) Detection. Our system consists of finetuning a pretrained Text-to-Text-Transfer Transformer (T5) and innovatively reducing its out-of-class predictions. The main contributions of this paper are 1) the description of the implementation details of the T5 model we used, 2) analysis of the successes & struggles of the model in this task, and 3) ablation studies beyond the official submission to ascertain the relative importance of data split. Our model achieves an F1 score of 0.5452 on the official test set. △ Less","5 May, 2022",https://arxiv.org/pdf/2204.07432
Expanding the Reach of Research Computing: A Landscape Study,Dhruva K. Chakravorty;Sarah K. Janes;James V. Howell;Lisa M. Perez;Amy Schultz;Marie Goldie;Austin L. Gamble;Rajiv Malkan;Honggao Liu;Daniel Mireles;Yuanqi Jing;Zhenhua He;Tim Cockerill,"Research-computing continues to play an ever increasing role in academia. Access to computing resources, however, varies greatly between institutions. Sustaining the growing need for computing skills and access to advanced cyberinfrastructure requires that computing resources be available to students at all levels of scholarship, including community colleges. The National Science Foundation-funded Building Research Innovation in Community Colleges (BRICCs) community set out to understand the challenges faced by administrators, researchers and faculty in building a sustainable research computing continuum that extends to smaller and two-year terminal degree granting institutions. BRICCs purpose is to address the technology gaps, and encourage the development of curriculum needed to grow a computationally proficient research workforce. Toward addressing these goals, we performed a landscape study that culminated with a community workshop. Here, we present our key findings from workshop discussions and identify next steps to be taken by BRICCs, funding agencies, and the broader cyberinfrastructure community. △ Less","18 April, 2022",https://arxiv.org/pdf/2204.07205
An Energy Aware Clustering Scheme for 5G-enabled Edge Computing based IoMT Framework,Jitendra Kumar Samriya;Mohit Kumar;Maria Ganzha;Marcin Paprzycki;Marek Bolanowski;Andrzej Paszkiewicz,"In recent years, 5G network systems start to offer communication infrastructure for Internet of Things (IoT) applications, especially for health care service pro-viders. In smart health care systems, edge computing enabled Internet of Medical Things (IoMT) is an innovative technology to provide online health care monitor-ing facility to patients. Here, energy consumption, along with extending the lifespan of biosensor network, is a key concern. In this contribution, a Chicken Swarm Optimization algorithm, based on Energy Efficient Multi-objective clus-tering scheme is applied in the context of IoMT system. An effective fitness func-tion is designed for cluster head selection., using multiple objectives, such as re-sidual energy, queuing delay, communication cost, link quality and node centrali-ty. Simulated outcomes of the proposed scheme are compared with the existing schemes in terms of parameters such as cluster formation time, energy consump-tion, network lifetime, throughput and propagation delay. △ Less","14 April, 2022",https://arxiv.org/pdf/2204.06850
Decentralized Collaborative Learning Framework for Next POI Recommendation,Jing Long;Tong Chen;Nguyen Quoc Viet Hung;Hongzhi Yin,"Next Point-of-Interest (POI) recommendation has become an indispensable functionality in Location-based Social Networks (LBSNs) due to its effectiveness in helping people decide the next POI to visit. However, accurate recommendation requires a vast amount of historical check-in data, thus threatening user privacy as the location-sensitive data needs to be handled by cloud servers. Although there have been several on-device frameworks for privacy-preserving POI recommendations, they are still resource-intensive when it comes to storage and computation, and show limited robustness to the high sparsity of user-POI interactions. On this basis, we propose a novel decentralized collaborative learning framework for POI recommendation (DCLR), which allows users to train their personalized models locally in a collaborative manner. DCLR significantly reduces the local models' dependence on the cloud for training, and can be used to expand arbitrary centralized recommendation models. To counteract the sparsity of on-device user data when learning each local model, we design two self-supervision signals to pretrain the POI representations on the server with geographical and categorical correlations of POIs. To facilitate collaborative learning, we innovatively propose to incorporate knowledge from either geographically or semantically similar users into each local model with attentive aggregation and mutual information maximization. The collaborative learning process makes use of communications between devices while requiring only minor engagement from the central server for identifying user groups, and is compatible with common privacy preservation mechanisms like differential privacy. We evaluate DCLR with two real-world datasets, where the results show that DCLR outperforms state-of-the-art on-device frameworks and yields competitive results compared with centralized counterparts. △ Less","31 July, 2022",https://arxiv.org/pdf/2204.06516
Enabling Synthetic Data adoption in regulated domains,Giorgio Visani;Giacomo Graffi;Mattia Alfero;Enrico Bagli;Davide Capuzzo;Federico Chesani,"The switch from a Model-Centric to a Data-Centric mindset is putting emphasis on data and its quality rather than algorithms, bringing forward new challenges. In particular, the sensitive nature of the information in highly regulated scenarios needs to be accounted for. Specific approaches to address the privacy issue have been developed, as Privacy Enhancing Technologies. However, they frequently cause loss of information, putting forward a crucial trade-off among data quality and privacy. A clever way to bypass such a conundrum relies on Synthetic Data: data obtained from a generative process, learning the real data properties. Both Academia and Industry realized the importance of evaluating synthetic data quality: without all-round reliable metrics, the innovative data generation task has no proper objective function to maximize. Despite that, the topic remains under-explored. For this reason, we systematically catalog the important traits of synthetic data quality and privacy, and devise a specific methodology to test them. The result is DAISYnt (aDoption of Artificial Intelligence SYnthesis): a comprehensive suite of advanced tests, which sets a de facto standard for synthetic data evaluation. As a practical use-case, a variety of generative algorithms have been trained on real-world Credit Bureau Data. The best model has been assessed, using DAISYnt on the different synthetic replicas. Further potential uses, among others, entail auditing and fine-tuning of generative models or ensuring high quality of a given synthetic dataset. From a prescriptive viewpoint, eventually, DAISYnt may pave the way to synthetic data adoption in highly regulated domains, ranging from Finance to Healthcare, through Insurance and Education. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.06297
5G Features and Standards for Vehicle Data Exploitation,Gorka Velez;Edoardo Bonetto;Daniele Brevi;Angel Martin;Gianluca Rizzi;Oscar Castañeda;Arslane Hamza Cherif;Marcos Nieto;Oihana Otaegui,"Cars capture and generate huge volumes of data in real-time about the driving dynamics, the environment, and the driver and passengers' activities. Due to the proliferation of cooperative, connected and automated mobility (CCAM), the value of data from vehicles is getting strategic, not just for the automotive industry, but also for many diverse stakeholders including small and medium-sized enterprises (SMEs) and start-ups. 5G can enable car-captured data to feed innovative applications and services deployed in the cloud ensuring lower latency and higher throughput than previous cellular technologies. This paper identifies and discusses the relevance of the main 5G features that can contribute to a scalable, flexible, reliable and secure data pipeline, pointing to the standards and technical reports that specify their implementation. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.06211
Retrieval of Scientific and Technological Resources for Experts and Scholars,Suyu Ouyang;Yingxia Shao;Ang Li,"Institutions of higher learning, research institutes and other scientific research units have abundant scientific and technological resources of experts and scholars, and these talents with great scientific and technological innovation ability are an important force to promote industrial upgrading. The scientific and technological resources of experts and scholars are mainly composed of basic attributes and scientific research achievements. The basic attributes include information such as research interests, institutions, and educational work experience. However, due to information asymmetry and other reasons, the scientific and technological resources of experts and scholars cannot be connected with the society in a timely manner, and social needs cannot be accurately matched with experts and scholars. Therefore, it is very necessary to build an expert and scholar information database and provide relevant expert and scholar retrieval services. This paper sorts out the related research work in this field from four aspects: text relation extraction, text knowledge representation learning, text vector retrieval and visualization system. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.06142
The MIT Supercloud Workload Classification Challenge,Benny J. Tang;Qiqi Chen;Matthew L. Weiss;Nathan Frey;Joseph McDonald;David Bestor;Charles Yee;William Arcand;Chansup Byun;Daniel Edelman;Matthew Hubbell;Michael Jones;Jeremy Kepner;Anna Klein;Adam Michaleas;Peter Michaleas;Lauren Milechin;Julia Mullen;Andrew Prout;Albert Reuther;Antonio Rosa;Andrew Bowne;Lindsey McEvoy;Baolin Li;Devesh Tiwari,"High-Performance Computing (HPC) centers and cloud providers support an increasingly diverse set of applications on heterogenous hardware. As Artificial Intelligence (AI) and Machine Learning (ML) workloads have become an increasingly larger share of the compute workloads, new approaches to optimized resource usage, allocation, and deployment of new AI frameworks are needed. By identifying compute workloads and their utilization characteristics, HPC systems may be able to better match available resources with the application demand. By leveraging datacenter instrumentation, it may be possible to develop AI-based approaches that can identify workloads and provide feedback to researchers and datacenter operators for improving operational efficiency. To enable this research, we released the MIT Supercloud Dataset, which provides detailed monitoring logs from the MIT Supercloud cluster. This dataset includes CPU and GPU usage by jobs, memory usage, and file system logs. In this paper, we present a workload classification challenge based on this dataset. We introduce a labelled dataset that can be used to develop new approaches to workload classification and present initial results based on existing approaches. The goal of this challenge is to foster algorithmic innovations in the analysis of compute workloads that can achieve higher accuracy than existing methods. Data and code will be made publicly available via the Datacenter Challenge website : https://dcc.mit.edu. △ Less","13 April, 2022",https://arxiv.org/pdf/2204.05839
DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection,Haibao Yu;Yizhen Luo;Mao Shu;Yiyi Huo;Zebang Yang;Yifeng Shi;Zhenglong Guo;Hanyu Li;Xing Hu;Jirui Yuan;Zaiqing Nie,"Autonomous driving faces great safety challenges for a lack of global perspective and the limitation of long-range perception capabilities. It has been widely agreed that vehicle-infrastructure cooperation is required to achieve Level 5 autonomy. However, there is still NO dataset from real scenarios available for computer vision researchers to work on vehicle-infrastructure cooperation-related problems. To accelerate computer vision research and innovation for Vehicle-Infrastructure Cooperative Autonomous Driving (VICAD), we release DAIR-V2X Dataset, which is the first large-scale, multi-modality, multi-view dataset from real scenarios for VICAD. DAIR-V2X comprises 71254 LiDAR frames and 71254 Camera frames, and all frames are captured from real scenes with 3D annotations. The Vehicle-Infrastructure Cooperative 3D Object Detection problem (VIC3D) is introduced, formulating the problem of collaboratively locating and identifying 3D objects using sensory inputs from both vehicle and infrastructure. In addition to solving traditional 3D object detection problems, the solution of VIC3D needs to consider the temporal asynchrony problem between vehicle and infrastructure sensors and the data transmission cost between them. Furthermore, we propose Time Compensation Late Fusion (TCLF), a late fusion framework for the VIC3D task as a benchmark based on DAIR-V2X. Find data, code, and more up-to-date information at https://thudair.baai.ac.cn/index and https://github.com/AIR-THU/DAIR-V2X. △ Less","12 April, 2022",https://arxiv.org/pdf/2204.05575
A Web-Scale Analysis of the Community Origins of Image Memes,Durim Morina;Michael S. Bernstein,"Where do the most popular online cultural artifacts such as image memes originate? Media narratives suggest that cultural innovations often originate in peripheral communities and then diffuse to the mainstream core; behavioral science suggests that intermediate network positions that bridge between the periphery and the core are especially likely to originate many influential cultural innovations. Research has yet to fully adjudicate between these predictions because prior work focuses on individual platforms such as Twitter; however, any single platform is only a small, incomplete part of the larger online cultural ecosystem. In this paper, we perform the first analysis of the origins and diffusion of image memes at web scale, via a one-month crawl of all indexible online communities that principally share meme images with English text overlays. Our results suggest that communities at the core of the network originate the most highly diffused image memes: the top 10% of communities by network centrality originate the memes that generate 62% of the image meme diffusion events on the web. A zero-inflated negative binomial regression confirms that memes from core communities are more likely to diffuse than those from peripheral communities even when controlling for community size and activity level. However, a replication analysis that follows the traditional approach of testing the same question only within a single large community, Reddit, finds the regression coefficients reversed -- underscoring the importance of engaging in web-scale, cross-community analyses. The ecosystem-level viewpoint of this work positions the web as a highly centralized generator of cultural artifacts such as image memes. △ Less","11 April, 2022",https://arxiv.org/pdf/2204.05439
Category-Aware Transformer Network for Better Human-Object Interaction Detection,Leizhen Dong;Zhimin Li;Kunlun Xu;Zhijun Zhang;Luxin Yan;Sheng Zhong;Xu Zou,"Human-Object Interactions (HOI) detection, which aims to localize a human and a relevant object while recognizing their interaction, is crucial for understanding a still image. Recently, transformer-based models have significantly advanced the progress of HOI detection. However, the capability of these models has not been fully explored since the Object Query of the model is always simply initialized as just zeros, which would affect the performance. In this paper, we try to study the issue of promoting transformer-based HOI detectors by initializing the Object Query with category-aware semantic information. To this end, we innovatively propose the Category-Aware Transformer Network (CATN). Specifically, the Object Query would be initialized via category priors represented by an external object detection model to yield better performance. Moreover, such category priors can be further used for enhancing the representation ability of features via the attention mechanism. We have firstly verified our idea via the Oracle experiment by initializing the Object Query with the groundtruth category information. And then extensive experiments have been conducted to show that a HOI detection model equipped with our idea outperforms the baseline by a large margin to achieve a new state-of-the-art result. △ Less","9 May, 2022",https://arxiv.org/pdf/2204.04911
Peekaboo: A Hub-Based Approach to Enable Transparency in Data Processing within Smart Homes (Extended Technical Report),Haojian Jin;Gram Liu;David Hwang;Swarun Kumar;Yuvraj Agarwal;Jason I. Hong,"We present Peekaboo, a new privacy-sensitive architecture for smart homes that leverages an in-home hub to pre-process and minimize outgoing data in a structured and enforceable manner before sending it to external cloud servers. Peekaboo's key innovations are (1) abstracting common data pre-processing functionality into a small and fixed set of chainable operators, and (2) requiring that developers explicitly declare desired data collection behaviors (e.g., data granularity, destinations, conditions) in an application manifest, which also specifies how the operators are chained together. Given a manifest, Peekaboo assembles and executes a pre-processing pipeline using operators pre-loaded on the hub. In doing so, developers can collect smart home data on a need-to-know basis; third-party auditors can verify data collection behaviors; and the hub itself can offer a number of centralized privacy features to users across apps and devices, without additional effort from app developers. We present the design and implementation of Peekaboo, along with an evaluation of its coverage of smart home scenarios, system performance, data minimization, and example built-in privacy features. △ Less","18 May, 2022",https://arxiv.org/pdf/2204.04540
The History of the Grid,Ian Foster;Carl Kesselman,"With the widespread availability of high-speed networks, it becomes feasible to outsource computing to remote providers and to federate resources from many locations. Such observations motivated the development, from the mid-1990s onwards, of a range of innovative Grid technologies, applications, and infrastructures. We review the history, current status, and future prospects for Grid computing. △ Less","8 April, 2022",https://arxiv.org/pdf/2204.04312
Predicting Berth Stay for Tanker Terminals: A Systematic and Dynamic Approach,Deqing Zhai;Xiuju Fu;Xiao Feng Yin;Haiyan Xu;Wanbing Zhang,"Given the trend of digitization and increasing number of maritime transport, prediction of vessel berth stay has been triggered for requirements of operation research and scheduling optimization problem in the era of maritime big data, which takes a significant part in port efficiency and maritime logistics enhancement. This study proposes a systematic and dynamic approach of predicting berth stay for tanker terminals. The approach covers three innovative aspects: 1) Data source employed is multi-faceted, including cargo operation data from tanker terminals, time-series data from automatic identification system (AIS), etc. 2) The process of berth stay is decomposed into multiple blocks according to data analysis and information extraction innovatively, and practical operation scenarios are also developed accordingly. 3) The predictive models of berth stay are developed on the basis of prior data analysis and information extraction under two methods, including regression and decomposed distribution. The models are evaluated under four dynamic scenarios with certain designated cargoes among two different terminals. The evaluation results show that the proposed approach can predict berth stay with the accuracy up to 98.81% validated by historical baselines, and also demonstrate the proposed approach has dynamic capability of predicting berth stay among the scenarios. The model may be potentially applied for short-term pilot-booking or scheduling optimizations within a reasonable time frame for advancement of port intelligence and logistics efficiency. △ Less","18 May, 2022",https://arxiv.org/pdf/2204.04085
Information-driven Path Planning for Hybrid Aerial Underwater Vehicles,Zheng Zeng;Chengke Xiong;Xinyi Yuan;Yulin Bai;Yufei Jin;Di Lu;Lian Lian,"This paper presents a novel Rapidly-exploring Adaptive Sampling Tree (RAST) algorithm for the adaptive sampling mission of a hybrid aerial underwater vehicle (HAUV) in an air-sea 3D environment. This algorithm innovatively combines the tournament-based point selection sampling strategy, the information heuristic search process and the framework of Rapidly-exploring Random Tree (RRT) algorithm. Hence can guide the vehicle to the region of interest to scientists for sampling and generate a collision-free path for maximizing information collection by the HAUV under the constraints of environmental effects of currents or wind and limited budget. The simulation results show that the fast search adaptive sampling tree algorithm has higher optimization performance, faster solution speed and better stability than the Rapidly-exploring Information Gathering Tree (RIGT) algorithm and the particle swarm optimization (PSO) algorithm. △ Less","8 April, 2022",https://arxiv.org/pdf/2204.03329
Accelerating Attention through Gradient-Based Learned Runtime Pruning,Zheng Li;Soroush Ghodrati;Amir Yazdanbakhsh;Hadi Esmaeilzadeh;Mingu Kang,"Self-attention is a key enabler of state-of-art accuracy for various transformer-based Natural Language Processing models. This attention mechanism calculates a correlation score for each word with respect to the other words in a sentence. Commonly, only a small subset of words highly correlates with the word under attention, which is only determined at runtime. As such, a significant amount of computation is inconsequential due to low attention scores and can potentially be pruned. The main challenge is finding the threshold for the scores below which subsequent computation will be inconsequential. Although such a threshold is discrete, this paper formulates its search through a soft differentiable regularizer integrated into the loss function of the training. This formulation piggy backs on the back-propagation training to analytically co-optimize the threshold and the weights simultaneously, striking a formally optimal balance between accuracy and computation pruning. To best utilize this mathematical innovation, we devise a bit-serial architecture, dubbed LeOPArd, for transformer language models with bit-level early termination microarchitectural mechanism. We evaluate our design across 43 back-end tasks for MemN2N, BERT, ALBERT, GPT-2, and Vision transformer models. Post-layout results show that, on average, LeOPArd yields 1.9x and 3.9x speedup and energy reduction, respectively, while keeping the average accuracy virtually intact (<0.2% degradation) △ Less","14 April, 2022",https://arxiv.org/pdf/2204.03227
Advancing Data Justice Research and Practice: An Integrated Literature Review,David Leslie;Michael Katell;Mhairi Aitken;Jatinder Singh;Morgan Briggs;Rosamund Powell;Cami Rincón;Thompson Chengeta;Abeba Birhane;Antonella Perini;Smera Jayadeva;Anjali Mazumder,"The Advancing Data Justice Research and Practice (ADJRP) project aims to widen the lens of current thinking around data justice and to provide actionable resources that will help policymakers, practitioners, and impacted communities gain a broader understanding of what equitable, freedom-promoting, and rights-sustaining data collection, governance, and use should look like in increasingly dynamic and global data innovation ecosystems. In this integrated literature review we hope to lay the conceptual groundwork needed to support this aspiration. The introduction motivates the broadening of data justice that is undertaken by the literature review which follows. First, we address how certain limitations of the current study of data justice drive the need for a re-location of data justice research and practice. We map out the strengths and shortcomings of the contemporary state of the art and then elaborate on the challenges faced by our own effort to broaden the data justice perspective in the decolonial context. The body of the literature review covers seven thematic areas. For each theme, the ADJRP team has systematically collected and analysed key texts in order to tell the critical empirical story of how existing social structures and power dynamics present challenges to data justice and related justice fields. In each case, this critical empirical story is also supplemented by the transformational story of how activists, policymakers, and academics are challenging longstanding structures of inequity to advance social justice in data innovation ecosystems and adjacent areas of technological practice. △ Less","6 April, 2022",https://arxiv.org/pdf/2204.03090
To Participate Or Not To Participate: An Investigation Of Strategic Participation In Standards,Paras Bhatt;Claire Vishik;Govind Hariharan;H. Raghav Rao,"Essential functionality in the ICT (Information and Communication Technology) space draws from standards such as HTTP (IETF RFC 2616, Bluetooth (IEEE 802.15) and various telecommunication standards (4G, 5G). They have fuelled rapid growth of ICT sector in the last decades by ensuring interoperability and consistency in computing environment. Research shows that firms that backed ICT standards and participated in standards development, have emerged as industry innovators. Standards development thus clearly has benefits for participating companies as well as technology development and innovation in general. However, significant costs are also associated with development of standards and need to be better understood to support investment in standardization necessary for todays ICT environment. We present a conceptual model that considers the potential for market innovation across a standards lifecycle and efficiency from standardization work, to build a forward-looking decision model that can guide an organizations standards development activities. We investigate and formalize motivations that drive firms to participate in standardization, specifically, changes in market innovation. Our model can serve as a strategic decision framework to drive assessments of a firms participation in standards development. We test our model with a use case on an established access control approach that was standardized more than two decades ago, Role Based Access Control (RBAC) using historical data. The investigation of the case study shows that change in market innovation is a significant indicator of success in standards development and are viable criteria to model a firms decision to participate (or not to participate) in a specific area of standardization. △ Less","6 April, 2022",https://arxiv.org/pdf/2204.03055
Hammer PDF: An Intelligent PDF Reader for Scientific Papers,Sheng-Fu Wang;Shu-Hang Liu;Tian-Yi Che;Yi-Fan Lu;Song-Xiao Yang;Heyan Huang;Xian-Ling Mao,"It is the most important way for researchers to acquire academic progress via reading scientific papers, most of which are in PDF format. However, existing PDF Readers like Adobe Acrobat Reader and Foxit PDF Reader are usually only for reading by rendering PDF files as a whole, and do not consider the multi-granularity content understanding of a paper itself. Specifically, taking a paper as a basic and separate unit, existing PDF Readers cannot access extended information about the paper, such as corresponding videos, blogs and codes. Meanwhile, they cannot understand the academic content of a paper, such as terms, authors, and citations. To solve these problems, we introduce Hammer PDF, an intelligent PDF Reader for scientific papers. Apart from basic reading functions, Hammer PDF has the following four innovative features: (1) information extraction ability, which can locate and mark spans like terms and other entities; (2) information extension ability, which can present relevant academic content of a paper, such as citations, references, codes, videos, blogs; (3) built-in Hammer Scholar, an academic search engine based on academic information collected from major academic databases; (4) built-in Q&A bot, which can find helpful conference information. The proposed Hammer PDF Reader can help researchers, especially those studying computer science, to improve the efficiency and experience of reading scientific papers. We have released Hammer PDF, available at https://pdf.hammerscholar.net/face. △ Less","18 June, 2022",https://arxiv.org/pdf/2204.02809
Faster-TAD: Towards Temporal Action Detection with Proposal Generation and Classification in a Unified Network,Shimin Chen;Chen Chen;Wei Li;Xunqiang Tao;Yandong Guo,"Temporal action detection (TAD) aims to detect the semantic labels and boundaries of action instances in untrimmed videos. Current mainstream approaches are multi-step solutions, which fall short in efficiency and flexibility. In this paper, we propose a unified network for TAD, termed Faster-TAD, by re-purposing a Faster-RCNN like architecture. To tackle the unique difficulty in TAD, we make important improvements over the original framework. We propose a new Context-Adaptive Proposal Module and an innovative Fake-Proposal Generation Block. What's more, we use atomic action features to improve the performance. Faster-TAD simplifies the pipeline of TAD and gets remarkable performance on lots of benchmarks, i.e., ActivityNet-1.3 (40.01% mAP), HACS Segments (38.39% mAP), SoccerNet-Action Spotting (54.09% mAP). It outperforms existing single-network detector by a large margin. △ Less","6 April, 2022",https://arxiv.org/pdf/2204.02674
Thinking inside The Box: Learning Hypercube Representations for Group Recommendation,Tong Chen;Hongzhi Yin;Jing Long;Quoc Viet Hung Nguyen;Yang Wang;Meng Wang,"As a step beyond traditional personalized recommendation, group recommendation is the task of suggesting items that can satisfy a group of users. In group recommendation, the core is to design preference aggregation functions to obtain a quality summary of all group members' preferences. Such user and group preferences are commonly represented as points in the vector space (i.e., embeddings), where multiple user embeddings are compressed into one to facilitate ranking for group-item pairs. However, the resulted group representations, as points, lack adequate flexibility and capacity to account for the multi-faceted user preferences. Also, the point embedding-based preference aggregation is a less faithful reflection of a group's decision-making process, where all users have to agree on a certain value in each embedding dimension instead of a negotiable interval. In this paper, we propose a novel representation of groups via the notion of hypercubes, which are subspaces containing innumerable points in the vector space. Specifically, we design the hypercube recommender (CubeRec) to adaptively learn group hypercubes from user embeddings with minimal information loss during preference aggregation, and to leverage a revamped distance metric to measure the affinity between group hypercubes and item points. Moreover, to counteract the long-standing issue of data sparsity in group recommendation, we make full use of the geometric expressiveness of hypercubes and innovatively incorporate self-supervision by intersecting two groups. Experiments on four real-world datasets have validated the superiority of CubeRec over state-of-the-art baselines. △ Less","4 December, 2022",https://arxiv.org/pdf/2204.02592
Mars: Near-Optimal Throughput with Shallow Buffers in Reconfigurable Datacenter Networks,Vamsi Addanki;Chen Avin;Stefan Schmid,"The performance of large-scale computing systems often critically depends on high-performance communication networks. Dynamically reconfigurable topologies, e.g., based on optical circuit switches, are emerging as an innovative new technology to deal with the explosive growth of datacenter traffic. Specifically, \emph{periodic} reconfigurable datacenter networks (RDCNs) such as RotorNet (SIGCOMM 2017), Opera (NSDI 2020) and Sirius (SIGCOMM 2020) have been shown to provide high throughput, by emulating a \emph{complete graph} through fast periodic circuit switch scheduling. However, to achieve such a high throughput, existing reconfigurable network designs pay a high price: in terms of potentially high delays, but also, as we show as a first contribution in this paper, in terms of the high buffer requirements. In particular, we show that under buffer constraints, emulating the high-throughput complete graph is infeasible at scale, and we uncover a spectrum of unvisited and attractive alternative RDCNs, which emulate regular graphs, but with lower node degree than the complete graph. We present Mars, a periodic reconfigurable topology which emulates a d-regular graph with near-optimal throughput. In particular, we systematically analyze how the degree~d can be optimized for throughput given the available buffer and delay tolerance of the datacenter. We further show empirically that Mars achieves higher throughput compared to existing systems when buffer sizes are bounded. △ Less","28 December, 2022",https://arxiv.org/pdf/2204.02525
Innovating at Speed and at Scale: A Next Generation Infrastructure for Accelerating Semiconductor Technologies,Richard A. Gottscho;Edlyn V. Levine;Tsu-Jae King Liu;Paul C. McIntyre;Subhasish Mitra;Boris Murmann;Jan M. Rabaey;Sayeef Salahuddin;Willy C. Shih;H. -S. Philip Wong,"Semiconductor innovation drives improvements to technologies that are critical to modern society. The country that successfully accelerates semiconductor innovation is positioned to lead future semiconductor-driven industries and benefit from the resulting economic growth. It is our view that a next generation infrastructure is necessary to accelerate and enhance semiconductor innovation in the U.S. In this paper, we propose such an advanced infrastructure composed of a national network of facilities with enhancements in technology and business models. These enhancements enable application-driven and challenge-based research and development, and ensure that facilities are accessible and sustainable. The main tenets are: a challenge-driven operational model, a next-generation infrastructure to serve that operational model, technology innovations needed for advanced facilities to speed up learning cycles, and innovative cost-effective business models for sustainability. Ultimately, the expected outcomes of such a participatory, scalable, and sustainable nation-level advanced infrastructure will have tremendous impact on government, industry, and academia alike. △ Less","7 March, 2022",https://arxiv.org/pdf/2204.02216
Microtransit adoption in the wake of the COVID-19 pandemic: evidence from a choice experiment with transit and car commuters,Jason Soria;Shelly Etzioni;Yoram Shiftan;Amanda Stathopoulos;Eran Ben-Elia,"On-demand mobility platforms play an increasingly important role in urban mobility systems. Impacts are still debated, as these platforms supply personalized and optimized services, while also contributing to existing sustainability challenges. Recently, microtransit services have emerged, promising to combine advantages of pooled on-demand rides with more sustainable fixed-route public transit services. Understanding traveler behavior becomes a primary focus to analyze adoption likelihood and perceptions of different microtransit attributes. The COVID-19 pandemic context adds an additional layer of complexity to analyzing mobility innovation acceptance. This study investigates the potential demand for microtransit options against the background of the pandemic. We use a stated choice experiment to study the decision-making of Israeli public transit and car commuters when offered to use novel microtransit options (sedan vs. passenger van). We investigate the tradeoffs related to traditional fare and travel time attributes, along with microtransit features; namely walking time to pickup location, vehicle sharing, waiting time, minimum advanced reservation time, and shelter at designated boarding locations. Additionally, we analyze two latent constructs: attitudes towards sharing, as well as experiences and risk-perceptions related to the COVID-19 pandemic. We develop Integrated Choice and Latent Variable models to compare the two commuter groups in terms of the likelihood to switch to microtransit, attribute trade-offs, sharing preferences and pandemic impacts. The results reveal high elasticities of several time and COVID effects for car commuters compared to relative insensitivity of transit commuters to the risk of COVID contraction. Moreover, for car commuters, those with strong sharing identities were more likely to be comfortable in COVID risk situations, and to accept microtransit. △ Less","5 April, 2022",https://arxiv.org/pdf/2204.01974
BcMON: Blockchain Middleware for Offline Networks,Yijing Lin;Zhipeng Gao;Qian Wang;Lanlan Rui;Yang Yang,"Blockchain is becoming a new generation of information infrastructures. However, the current blockchain solutions rely on a continuous connectivity network to query and modify the state of the blockchain. The emerging satellite technology seems to be a good catalyst to forward offline transactions to the blockchain. However, this approach suffers expensive costs, difficult interoperability, and limited computation problems. Therefore, we propose BcMON, the first blockchain middleware for offline networks. BcMON incorporates three innovative designs: 1) it reduces the costs of offline transactions accessing the blockchain through Short Message Service (SMS), 2) it validates the authenticity of offline cross-chain transactions by two-phase consensus, 3) it supports offline clients to perform complex queries and computations on the blockchains. The prototype of BcMON has been implemented to evaluate the performance of the proposed middleware, which can show its stability, efficiency, and scalability. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01964
Digital Twin Virtualization with Machine Learning for IoT and Beyond 5G Networks: Research Directions for Security and Optimal Control,Jithin Jagannath;Keyvan Ramezanpour;Anu Jagannath,"Digital twin (DT) technologies have emerged as a solution for real-time data-driven modeling of cyber physical systems (CPS) using the vast amount of data available by Internet of Things (IoT) networks. In this position paper, we elucidate unique characteristics and capabilities of a DT framework that enables realization of such promises as online learning of a physical environment, real-time monitoring of assets, Monte Carlo heuristic search for predictive prevention, on-policy, and off-policy reinforcement learning in real-time. We establish a conceptual layered architecture for a DT framework with decentralized implementation on cloud computing and enabled by artificial intelligence (AI) services for modeling, event detection, and decision-making processes. The DT framework separates the control functions, deployed as a system of logically centralized process, from the physical devices under control, much like software-defined networking (SDN) in fifth generation (5G) wireless networks. We discuss the moment of the DT framework in facilitating implementation of network-based control processes and its implications for critical infrastructure. To clarify the significance of DT in lowering the risk of development and deployment of innovative technologies on existing system, we discuss the application of implementing zero trust architecture (ZTA) as a necessary security framework in future data-driven communication networks. △ Less","10 April, 2022",https://arxiv.org/pdf/2204.01950
Estimating Fine-Grained Noise Model via Contrastive Learning,Yunhao Zou;Ying Fu,"Image denoising has achieved unprecedented progress as great efforts have been made to exploit effective deep denoisers. To improve the denoising performance in realworld, two typical solutions are used in recent trends: devising better noise models for the synthesis of more realistic training data, and estimating noise level function to guide non-blind denoisers. In this work, we combine both noise modeling and estimation, and propose an innovative noise model estimation and noise synthesis pipeline for realistic noisy image generation. Specifically, our model learns a noise estimation model with fine-grained statistical noise model in a contrastive manner. Then, we use the estimated noise parameters to model camera-specific noise distribution, and synthesize realistic noisy training data. The most striking thing for our work is that by calibrating noise models of several sensors, our model can be extended to predict other cameras. In other words, we can estimate cameraspecific noise models for unknown sensors with only testing images, without laborious calibration frames or paired noisy/clean data. The proposed pipeline endows deep denoisers with competitive performances with state-of-the-art real noise modeling methods. △ Less","2 April, 2022",https://arxiv.org/pdf/2204.01716
Exemplar Learning for Medical Image Segmentation,Qing En;Yuhong Guo,"Medical image annotation typically requires expert knowledge and hence incurs time-consuming and expensive data annotation costs. To alleviate this burden, we propose a novel learning scenario, Exemplar Learning (EL), to explore automated learning processes for medical image segmentation with a single annotated image example. This innovative learning task is particularly suitable for medical image segmentation, where all categories of organs can be presented in one single image and annotated all at once. To address this challenging EL task, we propose an Exemplar Learning-based Synthesis Net (ELSNet) framework for medical image segmentation that enables innovative exemplar-based data synthesis, pixel-prototype based contrastive embedding learning, and pseudo-label based exploitation of the unlabeled data. Specifically, ELSNet introduces two new modules for image segmentation: an exemplar-guided synthesis module, which enriches and diversifies the training set by synthesizing annotated samples from the given exemplar, and a pixel-prototype based contrastive embedding module, which enhances the discriminative capacity of the base segmentation model via contrastive representation learning. Moreover, we deploy a two-stage process for segmentation model training, which exploits the unlabeled data with predicted pseudo segmentation labels. To evaluate this new learning framework, we conduct extensive experiments on several organ segmentation datasets and present an in-depth analysis. The empirical results show that the proposed exemplar learning framework produces effective segmentation results. △ Less","8 October, 2022",https://arxiv.org/pdf/2204.01713
Residual-guided Personalized Speech Synthesis based on Face Image,Jianrong Wang;Zixuan Wang;Xiaosheng Hu;Xuewei Li;Qiang Fang;Li Liu,"Previous works derive personalized speech features by training the model on a large dataset composed of his/her audio sounds. It was reported that face information has a strong link with the speech sound. Thus in this work, we innovatively extract personalized speech features from human faces to synthesize personalized speech using neural vocoder. A Face-based Residual Personalized Speech Synthesis Model (FR-PSS) containing a speech encoder, a speech synthesizer and a face encoder is designed for PSS. In this model, by designing two speech priors, a residual-guided strategy is introduced to guide the face feature to approach the true speech feature in the training. Moreover, considering the error of feature's absolute values and their directional bias, we formulate a novel tri-item loss function for face encoder. Experimental results show that the speech synthesized by our model is comparable to the personalized speech synthesized by training a large amount of audio data in previous works. △ Less","1 April, 2022",https://arxiv.org/pdf/2204.01672
Deep Learning for Spectral Filling in Radio Frequency Applications,Matthew Setzler;Elizabeth Coda;Jeremiah Rounds;Michael Vann;Michael Girard,"Due to the Internet of Things (IoT) proliferation, Radio Frequency (RF) channels are increasingly congested with new kinds of devices, which carry unique and diverse communication needs. This poses complex challenges in modern digital communications, and calls for the development of technological innovations that (i) optimize capacity (bitrate) in limited bandwidth environments, (ii) integrate cooperatively with already-deployed RF protocols, and (iii) are adaptive to the ever-changing demands in modern digital communications. In this paper we present methods for applying deep neural networks for spectral filling. Given an RF channel transmitting digital messages with a pre-established modulation scheme, we automatically learn novel modulation schemes for sending extra information, in the form of additional messages, ""around"" the fixed-modulation signals (i.e., without interfering with them). In so doing, we effectively increase channel capacity without increasing bandwidth. We further demonstrate the ability to generate signals that closely resemble the original modulations, such that the presence of extra messages is undetectable to third-party listeners. We present three computational experiments demonstrating the efficacy of our methods, and conclude by discussing the implications of our results for modern RF applications. △ Less","31 March, 2022",https://arxiv.org/pdf/2204.01536
Explainable Online Lane Change Predictions on a Digital Twin with a Layer Normalized LSTM and Layer-wise Relevance Propagation,Christoph Wehner;Francis Powlesland;Bashar Altakrouri;Ute Schmid,"Artificial Intelligence and Digital Twins play an integral role in driving innovation in the domain of intelligent driving. Long short-term memory (LSTM) is a leading driver in the field of lane change prediction for manoeuvre anticipation. However, the decision-making process of such models is complex and non-transparent, hence reducing the trustworthiness of the smart solution. This work presents an innovative approach and a technical implementation for explaining lane change predictions of layer normalized LSTMs using Layer-wise Relevance Propagation (LRP). The core implementation includes consuming live data from a digital twin on a German highway, live predictions and explanations of lane changes by extending LRP to layer normalized LSTMs, and an interface for communicating and explaining the predictions to a human user. We aim to demonstrate faithful, understandable, and adaptable explanations of lane change prediction to increase the adoption and trustworthiness of AI systems that involve humans. Our research also emphases that explainability and state-of-the-art performance of ML models for manoeuvre anticipation go hand in hand without negatively affecting predictive effectiveness. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.01292
ScaleSFL: A Sharding Solution for Blockchain-Based Federated Learning,Evan Madill;Ben Nguyen;Carson K. Leung;Sara Rouhani,"Blockchain-based federated learning has gained significant interest over the last few years with the increasing concern for data privacy, advances in machine learning, and blockchain innovation. However, gaps in security and scalability hinder the development of real-world applications. In this study, we propose ScaleSFL, which is a scalable blockchain-based sharding solution for federated learning. ScaleSFL supports interoperability by separating the off-chain federated learning component in order to verify model updates instead of controlling the entire federated learning flow. We implemented ScaleSFL as a proof-of-concept prototype system using Hyperledger Fabric to demonstrate the feasibility of the solution. We present a performance evaluation of results collected through Hyperledger Caliper benchmarking tools conducted on model creation. Our evaluation results show that sharding can improve validation performance linearly while remaining efficient and secure. △ Less","3 April, 2022",https://arxiv.org/pdf/2204.01202
Risk-Aware Control and Optimization for High-Renewable Power Grids,Neil Barry;Minas Chatzos;Wenbo Chen;Dahye Han;Chaofan Huang;Roshan Joseph;Michael Klamkin;Seonho Park;Mathieu Tanneau;Pascal Van Hentenryck;Shangkun Wang;Hanyu Zhang;Haoruo Zhao,"The transition of the electrical power grid from fossil fuels to renewable sources of energy raises fundamental challenges to the market-clearing algorithms that drive its operations. Indeed, the increased stochasticity in load and the volatility of renewable energy sources have led to significant increases in prediction errors, affecting the reliability and efficiency of existing deterministic optimization models. The RAMC project was initiated to investigate how to move from this deterministic setting into a risk-aware framework where uncertainty is quantified explicitly and incorporated in the market-clearing optimizations. Risk-aware market-clearing raises challenges on its own, primarily from a computational standpoint. This paper reviews how RAMC approaches risk-aware market clearing and presents some of its innovations in uncertainty quantification, optimization, and machine learning. Experimental results on real networks are presented. △ Less","2 April, 2022",https://arxiv.org/pdf/2204.00950
Characterizing Spontaneous Ideation Contest on Social Media: Case Study on the Name Change of Facebook to Meta,Kunihiro Miyazaki;Takayuki Uchiba;Haewoon Kwak;Jisun An,"Collecting good ideas is vital for organizations, especially companies, to retain their competitiveness. Social media is gathering attention as a place to extract ideas efficiently; however, the characteristics of ideas and the posters of ideas on social media are underexamined. Thus, this study aims to characterize spontaneous ideation contests among social media users by taking an event of Facebook's name change to Meta as a case study. As a dataset, we comprehensively collect tweets containing new acronyms of Big Tech companies, which we treat as an ""idea"" in this work. In the analysis, we especially focus on the diversity of ideas, which would be the main reason for enlisting social media for idea generation. As the main results, we discovered that social media users offered a wider range of ideas than those in mainstream media. The follow-follower network of the users suggested that the users' position on the network is related to the preferred ideas. Additionally, we discovered a link between the amount of user interaction on social media and the diversity of ideas. This study would promote the use of social media as a part of open innovation and co-creation processes in the industry. △ Less","14 November, 2022",https://arxiv.org/pdf/2204.00910
UrbanFly: Uncertainty-Aware Planning for Navigation Amongst High-Rises with Monocular Visual-Inertial SLAM Maps,Sudarshan S Harithas;Ayyappa Swamy Thatavarthy;Gurkirat Singh;Arun K Singh;K Madhava Krishna,"We present UrbanFly: an uncertainty-aware real-time planning framework for quadrotor navigation in urban high-rise environments. A core aspect of UrbanFly is its ability to robustly plan directly on the sparse point clouds generated by a Monocular Visual Inertial SLAM (VINS) backend. It achieves this by using the sparse point clouds to build an uncertainty-integrated cuboid representation of the environment through a data-driven monocular plane segmentation network. Our chosen world model provides faster distance queries than the more common voxel-grid representation, and UrbanFly leverages this capability in two different ways leading to two trajectory optimizers. The first optimizer uses a gradient-free cross-entropy method to compute trajectories that minimize collision probability and smoothness cost. Our second optimizer is a simplified version of the first and uses a sequential convex programming optimizer initialized based on probabilistic safety estimates on a set of randomly drawn trajectories. Both our trajectory optimizers are made computationally tractable and independent of the nature of underlying uncertainty by embedding the distribution of collision violations in Reproducing Kernel Hilbert Space. Empowered by the algorithmic innovation, UrbanFly outperforms competing baselines in metrics such as collision rate, trajectory length, etc., on a high-fidelity AirSim simulator augmented with synthetic and real-world dataset scenes. △ Less","3 October, 2022",https://arxiv.org/pdf/2204.00865
PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation,Jing He;Yiyi Zhou;Qi Zhang;Jun Peng;Yunhang Shen;Xiaoshuai Sun;Chao Chen;Rongrong Ji,"Pixel synthesis is a promising research paradigm for image generation, which can well exploit pixel-wise prior knowledge for generation. However, existing methods still suffer from excessive memory footprint and computation overhead. In this paper, we propose a progressive pixel synthesis network towards efficient image generation, coined as PixelFolder. Specifically, PixelFolder formulates image generation as a progressive pixel regression problem and synthesizes images via a multi-stage structure, which can greatly reduce the overhead caused by large tensor transformations. In addition, we introduce novel pixel folding operations to further improve model efficiency while maintaining pixel-wise prior knowledge for end-to-end regression. With these innovative designs, we greatly reduce the expenditure of pixel synthesis, e.g., reducing 89% computation and 53% parameters compared with the latest pixel synthesis method CIPS. To validate our approach, we conduct extensive experiments on two benchmark datasets, namely FFHQ and LSUN Church. The experimental results show that with much less expenditure, PixelFolder obtains new state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77 FID and 2.45 FID on FFHQ and LSUN Church, respectively.Meanwhile, PixelFolder is also more efficient than the SOTA methods like StyleGAN2, reducing about 72% computation and 31% parameters, respectively. These results greatly validate the effectiveness of the proposed PixelFolder. △ Less","27 July, 2022",https://arxiv.org/pdf/2204.00833
RFID-Based Indoor Spatial Query Evaluation with Bayesian Filtering Techniques,Bo Hui;Wenlu Wang;Jiao Yu;Zhitao Gong;Wei-Shinn Ku;Min-Te Sun;Hua Lu,"People spend a significant amount of time in indoor spaces (e.g., office buildings, subway systems, etc.) in their daily lives. Therefore, it is important to develop efficient indoor spatial query algorithms for supporting various location-based applications. However, indoor spaces differ from outdoor spaces because users have to follow the indoor floor plan for their movements. In addition, positioning in indoor environments is mainly based on sensing devices (e.g., RFID readers) rather than GPS devices. Consequently, we cannot apply existing spatial query evaluation techniques devised for outdoor environments for this new challenge. Because Bayesian filtering techniques can be employed to estimate the state of a system that changes over time using a sequence of noisy measurements made on the system, in this research, we propose the Bayesian filtering-based location inference methods as the basis for evaluating indoor spatial queries with noisy RFID raw data. Furthermore, two novel models, indoor walking graph model and anchor point indexing model, are created for tracking object locations in indoor environments. Based on the inference method and tracking models, we develop innovative indoor range and k nearest neighbor (kNN) query algorithms. We validate our solution through use of both synthetic data and real-world data. Our experimental results show that the proposed algorithms can evaluate indoor spatial queries effectively and efficiently. We open-source the code, data, and floor plan at https://github.com/DataScienceLab18/IndoorToolKit. △ Less","25 May, 2022",https://arxiv.org/pdf/2204.00747
An Arduino based heartbeat detection device (ArdMob-ECG) for real-time ECG analysis,Tim Julian Möller;Martin Voss;Laura Kaltwasser,"This technical paper provides a tutorial to build a low-cost (10-100 USD) and easy to assemble ECG device (ArdMob-ECG) that can be easily used for a variety of different scientific studies. The advantage of this device is that it automatically stores the data and has a built-in detection algorithm for heartbeats. Compared to a clinical ECG, this device entails a serial interface that can send triggers via USB directly to a computer and software (e.g. Unity, Matlab) with minimal delay due to its architecture. Its software and hardware is open-source and publicly available. The performance of the device regarding sensitivity and specificity is comparable to a professional clinical ECG and is assessed in this paper. Due to the open-source software, a variety of different research questions and individual alterations can be adapted using this ECG. The code as well as the circuit is publicly available and accessible for everyone to promote a better health system in remote areas, Open Science, and to boost scientific progress and the development of new paradigms that ultimately foster innovation. △ Less","1 April, 2022",https://arxiv.org/pdf/2204.00513
Digital Mentor: towards a conversational bot to identify hypotheses for software startups,Jorge Melegati;Xiaofeng Wang,"Software startups develop innovative, software-intensive product and services. This context leads to uncertainty regarding the software they are building. Experimentation, a process of testing hypotheses about the product, helps these companies to reduce uncertainty through different evidence-based approaches. The first step in experimentation is to identify the hypotheses to be tested. HyMap is a technique where a facilitator helps a software startup founder to draw a cognitive map representing her understanding of the context and, based on that, create hypotheses about the software to be built. In this paper, we present the Digital Mentor, an working-in-progress conversational bot to help creating a HyMap without the need of a human facilitator. We report the proposed solution consisting of a web application with the backend of a natural language understanding system, the current state of development, the challenges we faced so far and the next steps we plan to move forward. △ Less","1 April, 2022",https://arxiv.org/pdf/2204.00455
Marginal Contrastive Correspondence for Guided Image Generation,Fangneng Zhan;Yingchen Yu;Rongliang Wu;Jiahui Zhang;Shijian Lu;Changgong Zhang,"Exemplar-based image translation establishes dense correspondences between a conditional input and an exemplar (from two different domains) for leveraging detailed exemplar styles to achieve realistic image translation. Existing work builds the cross-domain correspondences implicitly by minimizing feature-wise distances across the two domains. Without explicit exploitation of domain-invariant features, this approach may not reduce the domain gap effectively which often leads to sub-optimal correspondences and image translation. We design a Marginal Contrastive Learning Network (MCL-Net) that explores contrastive learning to learn domain-invariant features for realistic exemplar-based image translation. Specifically, we design an innovative marginal contrastive loss that guides to establish dense correspondences explicitly. Nevertheless, building correspondence with domain-invariant semantics alone may impair the texture patterns and lead to degraded texture generation. We thus design a Self-Correlation Map (SCM) that incorporates scene structures as auxiliary information which improves the built correspondences substantially. Quantitative and qualitative experiments on multifarious image translation tasks show that the proposed method outperforms the state-of-the-art consistently. △ Less","1 April, 2022",https://arxiv.org/pdf/2204.00442
Designing for emotion regulation interventions: an agenda for HCI theory and research,Petr Slovak;Alissa N. Antle;Nikki Theofanopoulou;Claudia Daudén Roquet;James J Gross;Katherine Isbister,"There is a growing interest in HCI to envision, design, and evaluate technology-enabled interventions that support users' emotion regulation. This interest stems in part from increased recognition that the ability to regulate emotions is critical to mental health, and that a lack of effective emotion regulation is a transdiagnostic factor for mental illness. However, the potential to combine innovative HCI designs with the theoretical grounding and state-of-art interventions from psychology has yet to be fully realised. In this paper, we synthesise HCI work on emotion regulation interventions and propose a three-part framework to guide technology designers in making: (i) theory-informed decisions about intervention targets; (ii) strategic decisions regarding the technology-enabled intervention mechanisms to be included in the system; and (iii) practical decisions around previous implementations of the selected intervention components. We show how this framework can both systematise HCI work to date and suggest a research agenda for future work. △ Less","4 April, 2022",https://arxiv.org/pdf/2204.00118
Ransomware Detection using Process Memory,Avinash Singh;Richard Adeyemi Ikuesan;Hein Venter,"Ransomware attacks have increased significantly in recent years, causing great destruction and damage to critical systems and business operations. Attackers are unfailingly finding innovative ways to bypass detection mechanisms, whichencouraged the adoption of artificial intelligence. However, most research summarizes the general features of AI and induces many false positives, as the behavior of ransomware constantly differs to bypass detection. Focusing on the key indicating features of ransomware becomes vital as this guides the investigator to the inner workings and main function of ransomware itself. By utilizing access privileges in process memory, the main function of the ransomware can be detected more easily and accurately. Furthermore, new signatures and fingerprints of ransomware families can be identified to classify novel ransomware attacks correctly. The current research used the process memory access privileges of the different memory regions of the behavior of an executable to quickly determine its intent before serious harm can occur. To achieve this aim, several well-known machine learning algorithms were explored with an accuracy range of 81.38 to 96.28 percents. The study thus confirms the feasibility of utilizing process memory as a detection mechanism for ransomware. △ Less","31 March, 2022",https://arxiv.org/pdf/2203.16871
Type-Directed Program Synthesis for RESTful APIs,Zheng Guo;David Cao;Davin Tjong;Jean Yang;Cole Schlesinger;Nadia Polikarpova,"With the rise of software-as-a-service and microservice architectures, RESTful APIs are now ubiquitous in mobile and web applications. A service can have tens or hundreds of API methods, making it a challenge for programmers to find the right combination of methods to solve their task. We present APIphany, a component-based synthesizer for programs that compose calls to RESTful APIs. The main innovation behind APIphany is the use of precise semantic types, both to specify user intent and to direct the search. APIphany contributes three novel mechanisms to overcome challenges in adapting component-based synthesis to the REST domain: (1) a type inference algorithm for augmenting REST specifications with semantic types; (2) an efficient synthesis technique for ""wrangling"" semi-structured data, which is commonly required in working with RESTful APIs; and (3) a new form of simulated execution to avoid executing APIs calls during synthesis. We evaluate APIphany on three real-world APIs and 32 tasks extracted from GitHub repositories and StackOverflow. In our experiments, APIphany found correct solutions to 29 tasks, with 23 of them reported among top ten synthesis results. △ Less","5 April, 2022",https://arxiv.org/pdf/2203.16697
Perfectly Perform Machine Learning Task with Imperfect Optical Hardware Accelerator,Jichao Fan;Yingheng Tang;Weilu Gao,"Optical architectures have been emerging as an energy-efficient and high-throughput hardware platform to accelerate computationally intensive general matrix-matrix multiplications (GEMMs) in modern machine learning (ML) algorithms. However, the inevitable imperfection and non-uniformity in large-scale optoelectronic devices prevent the scalable deployment of optical architectures, particularly those with innovative nano-devices. Here, we report an optical ML hardware to accelerate GEMM operations based on cascaded spatial light modulators and present a calibration procedure that enables accurate calculations despite the non-uniformity and imperfection in devices and system. We further characterize the hardware calculation accuracy under different configurations of electrical-optical interfaces. Finally, we deploy the developed optical hardware and calibration procedure to perform a ML task of predicting the intersubband plasmon frequency in single-wall carbon nanotubes. The obtained prediction accuracy from the optical hardware agrees well with that obtained using a general purpose electronic graphic process unit. △ Less","30 March, 2022",https://arxiv.org/pdf/2203.16603
"Near-Field Communications for 6G: Fundamentals, Challenges, Potentials, and Future Directions",Mingyao Cui;Zidong Wu;Yu Lu;Xiuhong Wei;Linglong Dai,"Extremely large antenna array (ELAA) is a common feature of several key candidate technologies for sixth-generation mobile networks (6G), such as ultra-massive multiple-input-multiple-output (UM-MIMO), cell-free massive MIMO, reconfigurable intelligent surface (RIS), and terahertz communications. Since the number of antennas is very large for ELAA, the electromagnetic radiation field needs to be modeled by near-field spherical waves, which is opposed to the conventional planar-wave-based radiation model of 5G massive MIMO. As a result, near-field communications will become essential in 6G wireless networks. In this article, we systematically investigate the emerging near-field communication techniques. Firstly, we present the fundamentals of near-field communications and the metric to determine the near-field ranges in typical communication scenarios. Then, we investigate recent studies specific to near-field communications by classifying them into two categories, i.e., techniques addressing the challenges and those exploiting the potentials in near-field regions. Their principles, recent progress, pros and cons are discussed. More importantly, several open problems and future research directions for near-field communications are pointed out. We believe that this article would inspire more innovations for this important research topic of near-field communications for 6G. △ Less","15 September, 2022",https://arxiv.org/pdf/2203.16318
LOCAT: Low-Overhead Online Configuration Auto-Tuning of Spark SQL Applications,Jinhan Xin;Kai Hwang;Zhibin Yu,"Spark SQL has been widely deployed in industry but it is challenging to tune its performance. Recent studies try to employ machine learning (ML) to solve this problem, but suffer from two drawbacks. First, it takes a long time (high overhead) to collect training samples. Second, the optimal configuration for one input data size of the same application might not be optimal for others. To address these issues, we propose a novel Bayesian Optimization (BO) based approach named LOCAT to automatically tune the configurations of Spark SQL applications online. LOCAT innovates three techniques. The first technique, named QCSA, eliminates the configuration-insensitive queries by Query Configuration Sensitivity Analysis (QCSA) when collecting training samples. The second technique, dubbed DAGP, is a Datasize-Aware Gaussian Process (DAGP) which models the performance of an application as a distribution of functions of configuration parameters as well as input data size. The third technique, called IICP, Identifies Important Configuration Parameters (IICP) with respect to performance and only tunes the important ones. As such, LOCAT can tune the configurations of a Spark SQL application with low overhead and adapt to different input data sizes. We employ Spark SQL applications from benchmark suites TPC-DS, TPC-H, and HiBench running on two significantly different clusters, a four-node ARM cluster and an eight-node x86 cluster, to evaluate LOCAT. The experimental results on the ARM cluster show that LOCAT accelerates the optimization procedures of the state-of-the-art approaches by at least 4.1x and up to 9.7x; moreover, LOCAT improves the application performance by at least 1.9x and up to 2.4x. On the x86 cluster, LOCAT shows similar results to those on the ARM cluster. △ Less","7 November, 2022",https://arxiv.org/pdf/2203.14889
Doodle It Yourself: Class Incremental Learning by Drawing a Few Sketches,Ayan Kumar Bhunia;Viswanatha Reddy Gajjala;Subhadeep Koley;Rohit Kundu;Aneeshan Sain;Tao Xiang;Yi-Zhe Song,"The human visual system is remarkable in learning new visual concepts from just a few examples. This is precisely the goal behind few-shot class incremental learning (FSCIL), where the emphasis is additionally placed on ensuring the model does not suffer from ""forgetting"". In this paper, we push the boundary further for FSCIL by addressing two key questions that bottleneck its ubiquitous application (i) can the model learn from diverse modalities other than just photo (as humans do), and (ii) what if photos are not readily accessible (due to ethical and privacy constraints). Our key innovation lies in advocating the use of sketches as a new modality for class support. The product is a ""Doodle It Yourself"" (DIY) FSCIL framework where the users can freely sketch a few examples of a novel class for the model to learn to recognize photos of that class. For that, we present a framework that infuses (i) gradient consensus for domain invariant learning, (ii) knowledge distillation for preserving old class information, and (iii) graph attention networks for message passing between old and novel classes. We experimentally show that sketches are better class support than text in the context of FSCIL, echoing findings elsewhere in the sketching literature. △ Less","28 March, 2022",https://arxiv.org/pdf/2203.14843
Multilayer patent citation networks: A comprehensive analytical framework for studying explicit technological relationships,Kyle Higham;Martina Contisciani;Caterina De Bacco,"The use of patent citation networks as research tools is becoming increasingly commonplace in the field of innovation studies. However, these networks rarely consider the contexts in which these citations are generated and are generally restricted to a single jurisdiction. Here, we propose and explore the use of a multilayer network framework that can naturally incorporate citation metadata and stretch across jurisdictions, allowing for a complete view of the global technological landscape that is accessible through patent data. Taking a conservative approach that links citation network layers through triadic patent families, we first observe that these layers contain complementary, rather than redundant, information about technological relationships. To probe the nature of this complementarity, we extract network communities from both the multilayer network and analogous single-layer networks, then directly compare their technological composition with established technological similarity networks. We find that while technologies are more splintered across communities in the multilayer case, the extracted communities match much more closely the established networks. We conclude that by capturing citation context, a multilayer representation of patent citation networks is, conceptually and empirically, better able to capture the significant nuance that exists in real technological relationships when compared to traditional, single-layer approaches. We suggest future avenues of research that take advantage of novel computational tools designed for use with multilayer networks. △ Less","27 March, 2022",https://arxiv.org/pdf/2203.14479
Benchmarking Deep AUROC Optimization: Loss Functions and Algorithmic Choices,Dixian Zhu;Xiaodong Wu;Tianbao Yang,"The area under the ROC curve (AUROC) has been vigorously applied for imbalanced classification and moreover combined with deep learning techniques. However, there is no existing work that provides sound information for peers to choose appropriate deep AUROC maximization techniques. In this work, we fill this gap from three aspects. (i) We benchmark a variety of loss functions with different algorithmic choices for deep AUROC optimization problem. We study the loss functions in two categories: pairwise loss and composite loss, which includes a total of 10 loss functions. Interestingly, we find composite loss, as an innovative loss function class, shows more competitive performance than pairwise loss from both training convergence and testing generalization perspectives. Nevertheless, data with more corrupted labels favors a pairwise symmetric loss. (ii) Moreover, we benchmark and highlight the essential algorithmic choices such as positive sampling rate, regularization, normalization/activation, and optimizers. Key findings include: higher positive sampling rate is likely to be beneficial for deep AUROC maximization; different datasets favors different weights of regularizations; appropriate normalization techniques, such as sigmoid and \ell_2 score normalization, could improve model performance. (iii) For optimization aspect, we benchmark SGD-type, Momentum-type, and Adam-type optimizers for both pairwise and composite loss. Our findings show that although Adam-type method is more competitive from training perspective, but it does not outperform others from testing perspective. △ Less","4 July, 2022",https://arxiv.org/pdf/2203.14177
FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset,Lizhen Wang;Zhiyuan Chen;Tao Yu;Chenguang Ma;Liang Li;Yebin Liu,"We present FaceVerse, a fine-grained 3D Neural Face Model, which is built from hybrid East Asian face datasets containing 60K fused RGB-D images and 2K high-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed to take better advantage of our hybrid dataset. In the coarse module, we generate a base parametric model from large-scale RGB-D images, which is able to predict accurate rough 3D face models in different genders, ages, etc. Then in the fine module, a conditional StyleGAN architecture trained with high-fidelity scan models is introduced to enrich elaborate facial geometric and texture details. Note that different from previous methods, our base and detailed modules are both changeable, which enables an innovative application of adjusting both the basic attributes and the facial details of 3D face models. Furthermore, we propose a single-image fitting framework based on differentiable rendering. Rich experiments show that our method outperforms the state-of-the-art methods. △ Less","27 May, 2022",https://arxiv.org/pdf/2203.14057
Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution,Guangyuan Li;Jun Lv;Yapeng Tian;Qi Dou;Chengyan Wang;Chenliang Xu;Jing Qin,"Magnetic resonance imaging (MRI) can present multi-contrast images of the same anatomical structures, enabling multi-contrast super-resolution (SR) techniques. Compared with SR reconstruction using a single-contrast, multi-contrast SR reconstruction is promising to yield SR images with higher quality by leveraging diverse yet complementary information embedded in different imaging modalities. However, existing methods still have two shortcomings: (1) they neglect that the multi-contrast features at different scales contain different anatomical details and hence lack effective mechanisms to match and fuse these features for better reconstruction; and (2) they are still deficient in capturing long-range dependencies, which are essential for the regions with complicated anatomical structures. We propose a novel network to comprehensively address these problems by developing a set of innovative Transformer-empowered multi-scale contextual matching and aggregation techniques; we call it McMRSR. Firstly, we tame transformers to model long-range dependencies in both reference and target images. Then, a new multi-scale contextual matching method is proposed to capture corresponding contexts from reference features at different scales. Furthermore, we introduce a multi-scale aggregation mechanism to gradually and interactively aggregate multi-scale matched features for reconstructing the target SR MR image. Extensive experiments demonstrate that our network outperforms state-of-the-art approaches and has great potential to be applied in clinical practice. Codes are available at https://github.com/XAIMI-Lab/McMRSR. △ Less","25 March, 2022",https://arxiv.org/pdf/2203.13963
Data-driven kinematics-consistent model order reduction of fluid-structure interaction problems: application to deformable microcapsules in a Stokes flow,Claire Dupont;Florian De Vuyst;Anne-Virginie Salsac,"In this paper, we present a generic approach of a dynamical data-driven model order reduction technique for three-dimensional fluid-structure interaction problems. A low-order continuous linear differential system is identified from snapshot solutions of a high-fidelity solver. The reduced order model (ROM) uses different ingredients like proper orthogonal decomposition (POD), dynamic mode decomposition (DMD) and Tikhonov-based robust identification techniques. An interpolation method is used to predict the capsule dynamics for any value of the governing non-dimensional parameters that are not in the training database. Then a dynamical system is built from the predicted solution. Numerical evidence shows the ability of the reduced model to predict the time-evolution of the capsule deformation from its initial state, whatever the parameter values. Accuracy and stability properties of the resulting low-order dynamical system are analyzed numerically. The numerical experiments show a very good agreement, measured in terms of modified Hausdorff distance between capsule solutions of the full-order and low-order models both in the case of confined and unconfined flows. This work is a first milestone to move towards real time simulation of fluid-structure problems, which can be extended to non-linear low-order systems to account for strong material and flow non-linearities. It is a valuable innovation tool for rapid design and for the development of innovative devices. △ Less","25 March, 2022",https://arxiv.org/pdf/2203.13725
MDsrv -- visual sharing and analysis of molecular dynamics simulations,Michelle Kampfrath;René Staritzbichler;Guillermo Pérez Hernández;Alexander S. Rose;Johanna K. S. Tiemann;Gerik Scheuermann;Daniel Wiegreffe;Peter W. Hildebrand,"Molecular dynamics simulation is a proven technique for computing and visualizing the time-resolved motion of macromolecules at atomic resolution. The MDsrv is a tool that streams MD trajectories and displays them interactively in web browsers without requiring advanced skills, facilitating interactive exploration and collaborative visual analysis. We have now enhanced the MDsrv to further simplify the upload and sharing of MD trajectories and improve their online viewing and analysis. With the new instance, the MDsrv simplifies the creation of sessions, which allows the exchange of MD trajectories with preset representations and perspectives. An important innovation is that the MDsrv can now access and visualize trajectories from remote datasets, which greatly expands its applicability and use, as the data no longer needs to be accessible on a local server. In addition, initial analyses such as sequence or structure alignments, distance measurements, or RMSD calculations have been implemented, which optionally support visual analysis. Finally, the MDsrv now offers a faster and more efficient visualization of even large trajectories. △ Less","4 May, 2022",https://arxiv.org/pdf/2203.13658
"Resilient Execution of Data-triggered Applications on Edge, Fog and Cloud Resources",Prateeksha Varshney;Shriram Ramesh;Shayal Chhabra;Aakash Khochare;Yogesh Simmhan,"Internet of Things (IoT) is leading to the pervasive availability of streaming data about the physical world, coupled with edge computing infrastructure deployed as part of smart cities and 5G rollout. These constrained, less reliable but cheap resources are complemented by fog resources that offer federated management and accelerated computing, and pay-as-you-go cloud resources. There is a lack of intuitive means to deploy application pipelines to consume such diverse streams, and to execute them reliably on edge and fog resources. We propose an innovative application model to declaratively specify queries to match streams of micro-batch data from stream sources and trigger the distributed execution of data pipelines. We also design a resilient scheduling strategy using advanced reservation on reliable fogs to guarantee dataflow completion within a deadline while minimizing the execution cost. Our detailed experiments on over 100 virtual IoT resources and for \approx 10k task executions, with comparison against baseline scheduling strategies, illustrates the cost-effectiveness, resilience and scalability of our framework. △ Less","24 March, 2022",https://arxiv.org/pdf/2203.13324
Sustainable Development Goal Target Interactions in the Philippines: A Two-Method Approach,Vena Pearl Bongolan;Spencer C. Soria;Roselle Leah K. Rivera,"In 2015, the United Nations adopted 17 Sustainable Development Goals (SDGs) with 169 targets for transformation toward a more sustainable future by 2030. This study seeks to evaluate and analyze SDG target interactions in the Philippines to resolve conflicting targets, and prioritize targets that reinforce others and have no conflicts. To evaluate all 14196 target interactions, two methods are employed. First, experts with over five years of SDG-related experience evaluated interactions using a 7-point scale. Second, a non-parametric Spearman rank correlation is used on official indicator data with resulting coefficients serving as interaction scores. Interaction scores are then coded as synergies (interact positively), trade-offs (negatively) or non-classified (neutrally). Targets are also modelled as nodes and interactions as edges in graphs presented in sdginteractions.herokuapp.com. Results from the two methods were synthesized to formulate recommendations for concerned parties. This includes resolving negative intra-goal target interactions involving targets 3.1 'Reduce maternal mortality', 3.6 'Reduce road injuries and deaths', and 3.7 'Universal access to sexual and reproductive care, family planning, and education'. Ugly targets (at least one negative interaction) including target 3.6, 3.7, and 8.2 'Diversify, innovate, and upgrade for economic productivity' need to be resolved. Targets that reinforce their corresponding SDGs should be prioritized, including 1.1 'Eradicate extreme poverty', 4.2 'Equal access to quality pre-primary education', 6.2 'End open defecation and provide access to sanitation and hygiene', 8.1 'Sustainable economic growth', and 9.4 'Upgrade all industries and infrastructures for sustainability'. Beautiful targets (no negative interactions) should also be prioritized, including target 8.5 and 17.5 'Invest in least developed countries'. △ Less","28 May, 2022",https://arxiv.org/pdf/2203.11768
Practical Stereo Matching via Cascaded Recurrent Network with Adaptive Correlation,Jiankun Li;Peisen Wang;Pengfei Xiong;Tao Cai;Ziwei Yan;Lei Yang;Jiangyu Liu;Haoqiang Fan;Shuaicheng Liu,"With the advent of convolutional neural networks, stereo matching algorithms have recently gained tremendous progress. However, it remains a great challenge to accurately extract disparities from real-world image pairs taken by consumer-level devices like smartphones, due to practical complicating factors such as thin structures, non-ideal rectification, camera module inconsistencies and various hard-case scenes. In this paper, we propose a set of innovative designs to tackle the problem of practical stereo matching: 1) to better recover fine depth details, we design a hierarchical network with recurrent refinement to update disparities in a coarse-to-fine manner, as well as a stacked cascaded architecture for inference; 2) we propose an adaptive group correlation layer to mitigate the impact of erroneous rectification; 3) we introduce a new synthetic dataset with special attention to difficult cases for better generalizing to real-world scenes. Our results not only rank 1st on both Middlebury and ETH3D benchmarks, outperforming existing state-of-the-art methods by a notable margin, but also exhibit high-quality details for real-life photos, which clearly demonstrates the efficacy of our contributions. △ Less","22 March, 2022",https://arxiv.org/pdf/2203.11483
Toward RIS-Enhanced Integrated Terrestrial/Non-Terrestrial Connectivity in 6G,Parisa Ramezani;Bin Lyu;Abbas Jamalipour,"The next generation of wireless systems will take the concept of communications and networking to another level through the seamless integration of terrestrial, aerial, satellite, maritime and underwater communication systems. Reconfigurable intelligent surface (RIS) is an innovative technology which, with its singular features and functionalities, can expedite the realization of this everywhere connectivity. Motivated by the unparalleled properties of this innovatory technology, this article provides a comprehensive discussion on how RIS can contribute to the actualization and proper functioning of future integrated terrestrial/non-terrestrial (INTENT) networks. As a case study, we explore the integration of RIS into non-orthogonal multiple access (NOMA)-based satellite communication networks and demonstrate the performance enhancement achieved by the inclusion of RIS via numerical simulations. Promising directions for future research in this area are set forth at the end of this article. △ Less","29 July, 2022",https://arxiv.org/pdf/2203.11312
Physics-driven Synthetic Data Learning for Biomedical Magnetic Resonance,Qinqin Yang;Zi Wang;Kunyuan Guo;Congbo Cai;Xiaobo Qu,"Deep learning has innovated the field of computational imaging. One of its bottlenecks is unavailable or insufficient training data. This article reviews an emerging paradigm, imaging physics-based data synthesis (IPADS), that can provide huge training data in biomedical magnetic resonance without or with few real data. Following the physical law of magnetic resonance, IPADS generates signals from differential equations or analytical solution models, making the learning more scalable, explainable, and better protecting privacy. Key components of IPADS learning, including signal generation models, basic deep learning network structures, enhanced data generation, and learning methods are discussed. Great potentials of IPADS have been demonstrated by representative applications in fast imaging, ultrafast signal reconstruction and accurate parameter quantification. Finally, open questions and future work have been discussed. △ Less","21 May, 2022",https://arxiv.org/pdf/2203.11178
A new perspective on probabilistic image modeling,Alexander Gepperth,"We present the Deep Convolutional Gaussian Mixture Model (DCGMM), a new probabilistic approach for image modeling capable of density estimation, sampling and tractable inference. DCGMM instances exhibit a CNN-like layered structure, in which the principal building blocks are convolutional Gaussian Mixture (cGMM) layers. A key innovation w.r.t. related models like sum-product networks (SPNs) and probabilistic circuits (PCs) is that each cGMM layer optimizes an independent loss function and therefore has an independent probabilistic interpretation. This modular approach permits intervening transformation layers to harness the full spectrum of (potentially non-invertible) mappings available to CNNs, e.g., max-pooling or half-convolutions. DCGMM sampling and inference are realized by a deep chain of hierarchical priors, where a sample generated by a given cGMM layer defines the parameters of sampling in the next-lower cGMM layer. For sampling through non-invertible transformation layers, we introduce a new gradient-based sharpening technique that exploits redundancy (overlap) in, e.g., half-convolutions. DCGMMs can be trained end-to-end by SGD from random initial conditions, much like CNNs. We show that DCGMMs compare favorably to several recent PC and SPN models in terms of inference, classification and sampling, the latter particularly for challenging datasets such as SVHN. We provide a public TF2 implementation. △ Less","21 March, 2022",https://arxiv.org/pdf/2203.11034
Technologies for AI-Driven Fashion Social Networking Service with E-Commerce,Jinseok Seol;Seongjae Kim;Sungchan Park;Holim Lim;Hyunsoo Na;Eunyoung Park;Dohee Jung;Soyoung Park;Kangwoo Lee;Sang-goo Lee,"The rapid growth of the online fashion market brought demands for innovative fashion services and commerce platforms. With the recent success of deep learning, many applications employ AI technologies such as visual search and recommender systems to provide novel and beneficial services. In this paper, we describe applied technologies for AI-driven fashion social networking service that incorporate fashion e-commerce. In the application, people can share and browse their outfit-of-the-day (OOTD) photos, while AI analyzes them and suggests similar style OOTDs and related products. To this end, we trained deep learning based AI models for fashion and integrated them to build a fashion visual search system and a recommender system for OOTD. With aforementioned technologies, the AI-driven fashion SNS platform, iTOO, has been successfully launched. △ Less","10 March, 2022",https://arxiv.org/pdf/2203.10996
SHREC 2021: Classification in cryo-electron tomograms,Ilja Gubins;Marten L. Chaillet;Gijs van der Schot;M. Cristina Trueba;Remco C. Veltkamp;Friedrich Förster;Xiao Wang;Daisuke Kihara;Emmanuel Moebel;Nguyen P. Nguyen;Tommi White;Filiz Bunyak;Giorgos Papoulias;Stavros Gerolymatos;Evangelia I. Zacharaki;Konstantinos Moustakas;Xiangrui Zeng;Sinuo Liu;Min Xu;Yaoyu Wang;Cheng Chen;Xuefeng Cui;Fa Zhang,"Cryo-electron tomography (cryo-ET) is an imaging technique that allows three-dimensional visualization of macro-molecular assemblies under near-native conditions. Cryo-ET comes with a number of challenges, mainly low signal-to-noise and inability to obtain images from all angles. Computational methods are key to analyze cryo-electron tomograms. To promote innovation in computational methods, we generate a novel simulated dataset to benchmark different methods of localization and classification of biological macromolecules in tomograms. Our publicly available dataset contains ten tomographic reconstructions of simulated cell-like volumes. Each volume contains twelve different types of complexes, varying in size, function and structure. In this paper, we have evaluated seven different methods of finding and classifying proteins. Seven research groups present results obtained with learning-based methods and trained on the simulated dataset, as well as a baseline template matching (TM), a traditional method widely used in cryo-ET research. We show that learning-based approaches can achieve notably better localization and classification performance than TM. We also experimentally confirm that there is a negative relationship between particle size and performance for all methods. △ Less","18 March, 2022",https://arxiv.org/pdf/2203.10035
Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation,Xingning Dong;Tian Gan;Xuemeng Song;Jianlong Wu;Yuan Cheng;Liqiang Nie,"Scene Graph Generation, which generally follows a regular encoder-decoder pipeline, aims to first encode the visual contents within the given image and then parse them into a compact summary graph. Existing SGG approaches generally not only neglect the insufficient modality fusion between vision and language, but also fail to provide informative predicates due to the biased relationship predictions, leading SGG far from practical. Towards this end, in this paper, we first present a novel Stacked Hybrid-Attention network, which facilitates the intra-modal refinement as well as the inter-modal interaction, to serve as the encoder. We then devise an innovative Group Collaborative Learning strategy to optimize the decoder. Particularly, based upon the observation that the recognition capability of one classifier is limited towards an extremely unbalanced dataset, we first deploy a group of classifiers that are expert in distinguishing different subsets of classes, and then cooperatively optimize them from two aspects to promote the unbiased SGG. Experiments conducted on VG and GQA datasets demonstrate that, we not only establish a new state-of-the-art in the unbiased metric, but also nearly double the performance compared with two baselines. △ Less","2 April, 2022",https://arxiv.org/pdf/2203.09811
Blockchain for the Metaverse: A Review,Thippa Reddy Gadekallu;Thien Huynh-The;Weizheng Wang;Gokul Yenduri;Pasika Ranaweera;Quoc-Viet Pham;Daniel Benevides da Costa;Madhusanka Liyanage,"Since Facebook officially changed its name to Metaverse in Oct. 2021, the metaverse has become a new norm of social networks and three-dimensional (3D) virtual worlds. The metaverse aims to bring 3D immersive and personalized experiences to users by leveraging many pertinent technologies. Despite great attention and benefits, a natural question in the metaverse is how to secure its users' digital content and data. In this regard, blockchain is a promising solution owing to its distinct features of decentralization, immutability, and transparency. To better understand the role of blockchain in the metaverse, we aim to provide an extensive survey on the applications of blockchain for the metaverse. We first present a preliminary to blockchain and the metaverse and highlight the motivations behind the use of blockchain for the metaverse. Next, we extensively discuss blockchain-based methods for the metaverse from technical perspectives, such as data acquisition, data storage, data sharing, data interoperability, and data privacy preservation. For each perspective, we first discuss the technical challenges of the metaverse and then highlight how blockchain can help. Moreover, we investigate the impact of blockchain on key-enabling technologies in the metaverse, including Internet-of-Things, digital twins, multi-sensory and immersive applications, artificial intelligence, and big data. We also present some major projects to showcase the role of blockchain in metaverse applications and services. Finally, we present some promising directions to drive further research innovations and developments towards the use of blockchain in the metaverse in the future. △ Less","21 March, 2022",https://arxiv.org/pdf/2203.09738
DCarbonX Decentralised Application: Carbon Market Case Study,Nida Khan;Tabrez Ahmad,"Decentralized applications developed using blockchain technology provide innovative business models to serve the human race and solve existing challenges. Climate change is one of the biggest problems humanity is facing and there is a dearth of solutions in tackling this grave impediment to the long-term sustainability of our planet. Accountability, greenwashing, traceability, impact assessment and trading of carbon credits are unresolved issues in the ESG sector. In this paper, we present a novel decentralized application software, DCarbonX, that solves the enumerated problems using NFTs on the blockchain platform, through smart contracts. The paper describes the functional architecture of DCarbonX, while elaborating on its salient features and utility in sustainable finance, in particular green sukuk. DCarbonX is a pioneering software providing an exchange for trading of carbon credits. The software facilitates logging of impact and traceable transactions in a carbon market, that would help to prevent duplication of records and greenwashing. The paper discusses the efforts being undertaken to achieve the climate goals as per the Paris Agreement and also highlights the pivotal obstacles to achieving carbon neutrality by 2050, as per COP26. The paper also encompasses a study on the applications of dapps in DeFi, Web 3.0 and ESG, among other areas and gives a comparative analysis of blockchain platforms for dapp development. The paper is also a pioneer in highlighting the challenges that plague dapp development, deployment and usage. △ Less","17 March, 2022",https://arxiv.org/pdf/2203.09508
A New Analytical Approximation of the Fluid Antenna System Channel,Malek Khammassi;Abla Kammoun;Mohamed-Slim Alouini,"Fluid antenna systems (FAS) are an emerging technology that promises a significant diversity gain even in the smallest spaces. Motivated by the groundbreaking potentials of liquid antennas, researchers in the wireless communication community are investigating a novel antenna system where a single antenna can freely switch positions along a small linear space to pick the strongest received signal. However, the FAS positions do not necessarily follow the ever-existing rule separating them by at least half the radiation wavelength. Previous work in the literature parameterized the channels of the FAS ports simply enough to provide a single-integral expression of the probability of outage and various insights on the achievable performance. Nevertheless, this channel model may not accurately capture the correlation between the ports, given by Jake's model. This work builds on the state-of-the-art and accurately approximates the FAS channel while maintaining analytical tractability. The approximation is performed in two stages. The first stage approximation considerably reduces the number of multi-fold integrals in the probability of outage expression, while the second stage approximation provides a single integral representation of the FAS probability of outage. Further, the performance of such innovative technology is investigated under a less-idealized correlation model. Numerical results validate our approximations of the FAS channel model and demonstrate a limited performance gain under realistic assumptions. Further, our work opens the door for future research to investigate scenarios in which the FAS provides a performance gain compared to the current multiple antennas solutions. △ Less","29 December, 2022",https://arxiv.org/pdf/2203.09318
A Survey of Multi-Tenant Deep Learning Inference on GPU,Fuxun Yu;Di Wang;Longfei Shangguan;Minjia Zhang;Chenchen Liu;Xiang Chen,"Deep Learning (DL) models have achieved superior performance. Meanwhile, computing hardware like NVIDIA GPUs also demonstrated strong computing scaling trends with 2x throughput and memory bandwidth for each generation. With such strong computing scaling of GPUs, multi-tenant deep learning inference by co-locating multiple DL models onto the same GPU becomes widely deployed to improve resource utilization, enhance serving throughput, reduce energy cost, etc. However, achieving efficient multi-tenant DL inference is challenging which requires thorough full-stack system optimization. This survey aims to summarize and categorize the emerging challenges and optimization opportunities for multi-tenant DL inference on GPU. By overviewing the entire optimization stack, summarizing the multi-tenant computing innovations, and elaborating the recent technological advances, we hope that this survey could shed light on new optimization perspectives and motivate novel works in future large-scale DL system optimization. △ Less","24 May, 2022",https://arxiv.org/pdf/2203.09040
Towards Practical Certifiable Patch Defense with Vision Transformer,Zhaoyu Chen;Bo Li;Jianghe Xu;Shuang Wu;Shouhong Ding;Wenqiang Zhang,"Patch attacks, one of the most threatening forms of physical attack in adversarial examples, can lead networks to induce misclassification by modifying pixels arbitrarily in a continuous region. Certifiable patch defense can guarantee robustness that the classifier is not affected by patch attacks. Existing certifiable patch defenses sacrifice the clean accuracy of classifiers and only obtain a low certified accuracy on toy datasets. Furthermore, the clean and certified accuracy of these methods is still significantly lower than the accuracy of normal classification networks, which limits their application in practice. To move towards a practical certifiable patch defense, we introduce Vision Transformer (ViT) into the framework of Derandomized Smoothing (DS). Specifically, we propose a progressive smoothed image modeling task to train Vision Transformer, which can capture the more discriminable local context of an image while preserving the global semantic information. For efficient inference and deployment in the real world, we innovatively reconstruct the global self-attention structure of the original ViT into isolated band unit self-attention. On ImageNet, under 2% area patch attacks our method achieves 41.70% certified accuracy, a nearly 1-fold increase over the previous best method (26.00%). Simultaneously, our method achieves 78.58% clean accuracy, which is quite close to the normal ResNet-101 accuracy. Extensive experiments show that our method obtains state-of-the-art clean and certified accuracy with inferring efficiently on CIFAR-10 and ImageNet. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08519
Building AI Innovation Labs together with Companies,Jens Heidrich;Andreas Jedlitschka;Adam Trendowicz;Anna Maria Vollmer,"In the future, most companies will be confronted with the topic of Artificial Intelligence (AI) and will have to decide on their strategy in this regards. Currently, a lot of companies are thinking about whether and how AI and the usage of data will impact their business model and what potential use cases could look like. One of the biggest challenges lies in coming up with innovative solution ideas with a clear business value. This requires business competencies on the one hand and technical competencies in AI and data analytics on the other hand. In this article, we present the concept of AI innovation labs and demonstrate a comprehensive framework, from coming up with the right ideas to incrementally implementing and evaluating them regarding their business value and their feasibility based on a company's capabilities. The concept is the result of nine years of working on data-driven innovations with companies from various domains. Furthermore, we share some lessons learned from its practical applications. Even though a lot of technical publications can be found in the literature regarding the development of AI models and many consultancy companies provide corresponding services for building AI innovations, we found very few publications sharing details about what an end-to-end framework could look like. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.08465
Differentiable Multi-Agent Actor-Critic for Multi-Step Radiology Report Summarization,Sanjeev Kumar Karn;Ning Liu;Hinrich Schuetze;Oladimeji Farri,"The IMPRESSIONS section of a radiology report about an imaging study is a summary of the radiologist's reasoning and conclusions, and it also aids the referring physician in confirming or excluding certain diagnoses. A cascade of tasks are required to automatically generate an abstractive summary of the typical information-rich radiology report. These tasks include acquisition of salient content from the report and generation of a concise, easily consumable IMPRESSIONS section. Prior research on radiology report summarization has focused on single-step end-to-end models -- which subsume the task of salient content acquisition. To fully explore the cascade structure and explainability of radiology report summarization, we introduce two innovations. First, we design a two-step approach: extractive summarization followed by abstractive summarization. Second, we additionally break down the extractive part into two independent tasks: extraction of salient (1) sentences and (2) keywords. Experiments on English radiology reports from two clinical sites show our novel approach leads to a more precise summary compared to single-step and to two-step-with-single-extractive-process baselines with an overall improvement in F1 score Of 3-4%. △ Less","29 April, 2022",https://arxiv.org/pdf/2203.08257
Competition-Level Code Generation with AlphaCode,Yujia Li;David Choi;Junyoung Chung;Nate Kushman;Julian Schrittwieser;Rémi Leblond;Tom Eccles;James Keeling;Felix Gimeno;Agustin Dal Lago;Thomas Hubert;Peter Choy;Cyprien de Masson d'Autume;Igor Babuschkin;Xinyun Chen;Po-Sen Huang;Johannes Welbl;Sven Gowal;Alexey Cherepanov;James Molloy;Daniel J. Mankowitz;Esme Sutherland Robson;Pushmeet Kohli;Nando de Freitas;Koray Kavukcuoglu,"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions. △ Less","8 February, 2022",https://arxiv.org/pdf/2203.07814
Social Choice Around the Block: On the Computational Social Choice of Blockchain,Davide Grossi,"One of the most innovative aspects of blockchain technology consists in the introduction of an incentive layer to regulate the behavior of distributed protocols. The designer of a blockchain system faces therefore issues that are akin to those relevant for the design of economic mechanisms, and faces them in a computational setting. From this perspective the present paper argues for the importance of computational social choice in blockchain research. It identifies a few challenges at the interface of the two fields that illustrate the strong potential for cross-fertilization between them. △ Less","15 March, 2022",https://arxiv.org/pdf/2203.07777
LDP: Learnable Dynamic Precision for Efficient Deep Neural Network Training and Inference,Zhongzhi Yu;Yonggan Fu;Shang Wu;Mengquan Li;Haoran You;Yingyan Lin,"Low precision deep neural network (DNN) training is one of the most effective techniques for boosting DNNs' training efficiency, as it trims down the training cost from the finest bit level. While existing works mostly fix the model precision during the whole training process, a few pioneering works have shown that dynamic precision schedules help DNNs converge to a better accuracy while leading to a lower training cost than their static precision training counterparts. However, existing dynamic low precision training methods rely on manually designed precision schedules to achieve advantageous efficiency and accuracy trade-offs, limiting their more comprehensive practical applications and achievable performance. To this end, we propose LDP, a Learnable Dynamic Precision DNN training framework that can automatically learn a temporally and spatially dynamic precision schedule during training towards optimal accuracy and efficiency trade-offs. It is worth noting that LDP-trained DNNs are by nature efficient during inference. Furthermore, we visualize the resulting temporal and spatial precision schedule and distribution of LDP trained DNNs on different tasks to better understand the corresponding DNNs' characteristics at different training stages and DNN layers both during and after training, drawing insights for promoting further innovations. Extensive experiments and ablation studies (seven networks, five datasets, and three tasks) show that the proposed LDP consistently outperforms state-of-the-art (SOTA) low precision DNN training techniques in terms of training efficiency and achieved accuracy trade-offs. For example, in addition to having the advantage of being automated, our LDP achieves a 0.31\% higher accuracy with a 39.1\% lower computational cost when training ResNet-20 on CIFAR-10 as compared with the best SOTA method. △ Less","15 March, 2022",https://arxiv.org/pdf/2203.07713
Evaluation of websites of state public health agencies during the COVID-19 pandemic demonstrating the degree of effort to design for accessibility,Arunkumar Pennathur;Amirmasoud Momenipour;Priyadarshini Pennathur;Brandon Murphy,"Since the beginning of the pandemic, every state public health agency in the United States has created and maintained a website dedicated to COVID 19. Our goal was to evaluate these websites for conformity to accessibility guidelines. Specifically, we assessed, on a scale of increasing levels of accessibility compliance requirements, the results of the efforts made by website developers to incorporate and meet accessibility compliance criteria. We focused on homepages and vaccine pages in 49 states. For this study, we used the automated AChecker tool to assess conformance to the WCAG 2.0 guidelines at A, AA and AAA levels of conformance, and conformance with the Section 508c standard. We also manually rated, on a scale 0 (none) to 3 (highest), the specific accessibility features, if any, that web developers had included on the pages. We found that accessibility violations were prevalent across states but to varying degrees for a specific accessibility criterion. Although violations were detected in all 4 POUR accessibility principles, the most number of known violations occurred in meeting the perceivability and operability principles. Most violations in 508c guidelines occurred in not providing functional text in scripting languages and in not providing text equivalents for nontext. The degree of effort and conformance significantly varied between states; a majority of states exhibited a lower degree of effort, while a few attempted innovative ways to enhance accessibility on their websites. The efforts seemed to focus on meeting the minimum threshold. It is not clear if websites were designed proactively for accessibility. △ Less","14 March, 2022",https://arxiv.org/pdf/2203.07201
MTLDesc: Looking Wider to Describe Better,Changwei Wang;Rongtao Xu;Yuyang Zhang;Shibiao Xu;Weiliang Meng;Bin Fan;Xiaopeng Zhang,"Limited by the locality of convolutional neural networks, most existing local features description methods only learn local descriptors with local information and lack awareness of global and surrounding spatial context. In this work, we focus on making local descriptors ""look wider to describe better"" by learning local Descriptors with More Than just Local information (MTLDesc). Specifically, we resort to context augmentation and spatial attention mechanisms to make our MTLDesc obtain non-local awareness. First, Adaptive Global Context Augmented Module and Diverse Local Context Augmented Module are proposed to construct robust local descriptors with context information from global to local. Second, Consistent Attention Weighted Triplet Loss is designed to integrate spatial attention awareness into both optimization and matching stages of local descriptors learning. Third, Local Features Detection with Feature Pyramid is given to obtain more stable and accurate keypoints localization. With the above innovations, the performance of our MTLDesc significantly surpasses the prior state-of-the-art local descriptors on HPatches, Aachen Day-Night localization and InLoc indoor localization benchmarks. △ Less","14 March, 2022",https://arxiv.org/pdf/2203.07003
First Experiences in Performance Benchmarking with the New SPEChpc 2021 Suites,Holger Brunst;Sunita Chandrasekaran;Florina Ciorba;Nick Hagerty;Robert Henschel;Guido Juckeland;Junjie Li;Veronica G. Melesse Vergara;Sandra Wienke;Miguel Zavala,"Modern HPC systems are built with innovative system architectures and novel programming models to further push the speed limit of computing. The increased complexity poses challenges for performance portability and performance evaluation. The Standard Performance Evaluation Corporation -SPEC has a long history of producing industry standard benchmarks for modern computer systems. SPEC is a newly released SPEChpc 2021 benchmark suites, developed by the High Performance Group, are a bold attempt to provide a fair and objective benchmarking tool designed for state of the art HPC systems. With the support of multiple host and accelerator programming models, the suites are portable across both homogeneous and heterogeneous architectures. Different workloads are developed to fit system sizes ranging from a few compute nodes to a few hundred compute nodes. In this manuscript, we take a first glance at these benchmark suites and evaluate their portability and basic performance characteristics on various popular and emerging HPC architectures, including x86 CPU, NVIDIA GPU, and AMD GPU. This study provides a first-hand experience of executing the SPEChpc 2021 suites at scale on production HPC systems, discusses real-world use cases, and serves as an initial guideline for using the benchmark suites. △ Less","28 March, 2022",https://arxiv.org/pdf/2203.06751
CEKD:Cross Ensemble Knowledge Distillation for Augmented Fine-grained Data,Ke Zhang;Jin Fan;Shaoli Huang;Yongliang Qiao;Xiaofeng Yu;Feiwei Qin,"Data augmentation has been proved effective in training deep models. Existing data augmentation methods tackle the fine-grained problem by blending image pairs and fusing corresponding labels according to the statistics of mixed pixels, which produces additional noise harmful to the performance of networks. Motivated by this, we present a simple yet effective cross ensemble knowledge distillation (CEKD) model for fine-grained feature learning. We innovatively propose a cross distillation module to provide additional supervision to alleviate the noise problem, and propose a collaborative ensemble module to overcome the target conflict problem. The proposed model can be trained in an end-to-end manner, and only requires image-level label supervision. Extensive experiments on widely used fine-grained benchmarks demonstrate the effectiveness of our proposed model. Specifically, with the backbone of ResNet-101, CEKD obtains the accuracy of 89.59%, 95.96% and 94.56% in three datasets respectively, outperforming state-of-the-art API-Net by 0.99%, 1.06% and 1.16%. △ Less","12 March, 2022",https://arxiv.org/pdf/2203.06551
Active Token Mixer,Guoqiang Wei;Zhizheng Zhang;Cuiling Lan;Yan Lu;Zhibo Chen,"The three existing dominant network families, i.e., CNNs, Transformers, and MLPs, differ from each other mainly in the ways of fusing spatial contextual information, leaving designing more effective token-mixing mechanisms at the core of backbone architecture development. In this work, we propose an innovative token-mixer, dubbed Active Token Mixer (ATM), to actively incorporate flexible contextual information distributed across different channels from other tokens into the given query token. This fundamental operator actively predicts where to capture useful contexts and learns how to fuse the captured contexts with the query token at channel level. In this way, the spatial range of token-mixing can be expanded to a global scope with limited computational complexity, where the way of token-mixing is reformed. We take ATM as the primary operator and assemble ATMs into a cascade architecture, dubbed ATMNet. Extensive experiments demonstrate that ATMNet is generally applicable and comprehensively surpasses different families of SOTA vision backbones by a clear margin on a broad range of vision tasks, including visual recognition and dense prediction tasks. Code is available at https://github.com/microsoft/ActiveMLP. △ Less","23 December, 2022",https://arxiv.org/pdf/2203.06108
Supporting Schema References in Keyword Queries over Relational Databases,Paulo Martins;Altigran da Silva;João Cavalcanti;Edleno de Moura,"Relational Keyword Search (R-KwS) systems enable naive/informal users to explore and retrieve information from relational databases without knowing schema details or query languages. These systems take the keywords from the input query, locate the elements of the target database that correspond to these keywords, and look for ways to ""connect"" these elements using information on referential integrity constraints, i.e., key/foreign key pairs. Although several such systems have been proposed in the literature, most of them only support queries whose keywords refer to the contents of the target database and just very few support queries in which keywords refer to elements of the database schema. This paper proposes LATHE, a novel R-KwS designed to support such queries. To this end, in our work, we first generalize the well-known concepts of Query Matches (QMs) and Candidate Joining Networks (CJNs) to handle keywords referring to schema elements and propose new algorithms to generate them. Then, we introduce an approach to automatically select the CJNs that are more likely to represent the user intent when issuing a keyword query. This approach includes two major innovations: a ranking algorithm for selecting better QMs, yielding the generation of fewer but better CJNs, and an eager evaluation strategy for pruning void useless CJNs. We present a comprehensive set of experiments performed with query sets and datasets previously used in experiments with state-of-the-art R-KwS systems and methods. Our results indicate that LATHE can handle a wider variety of keyword queries while remaining highly effective, even for large databases with intricate schemas. △ Less","11 March, 2022",https://arxiv.org/pdf/2203.05921
Visibility-Inspired Models of Touch Sensors for Navigation,Kshitij Tiwari;Basak Sakcak;Prasanna Routray;Manivannan M.;Steven M. LaValle,"This paper introduces mathematical models of \sensors\ for mobile robots based on visibility. Serving a purpose similar to the pinhole camera model for computer vision, the introduced models are expected to provide a useful, idealized characterization of task-relevant information that can be inferred from their outputs or observations. Possible tasks include navigation, localization and mapping when a mobile robot is deployed in an unknown environment. These models allow direct comparisons to be made between traditional depth sensors, highlighting cases in which touch sensing may be interchangeable with time of flight or vision sensors, and characterizing unique advantages provided by touch sensing. The models include contact detection, compression, load bearing, and deflection. The results could serve as a basic building block for innovative touch sensor designs for mobile robot sensor fusion systems. △ Less","28 July, 2022",https://arxiv.org/pdf/2203.04751
Anomaly Detection for Unmanned Aerial Vehicle Sensor Data Using a Stacked Recurrent Autoencoder Method with Dynamic Thresholding,Victoria Bell1;Divish Rengasamy;Benjamin Rothwell;Grazziela P Figueredo,"With substantial recent developments in aviation technologies, Unmanned Aerial Vehicles (UAVs) are becoming increasingly integrated in commercial and military operations internationally. Research into the applications of aircraft data is essential in improving safety, reducing operational costs, and developing the next frontier of aerial technology. Having an outlier detection system that can accurately identify anomalous behaviour in aircraft is crucial for these reasons. This paper proposes a system incorporating a Long Short-Term Memory (LSTM) Deep Learning Autoencoder based method with a novel dynamic thresholding algorithm and weighted loss function for anomaly detection of a UAV dataset, in order to contribute to the ongoing efforts that leverage innovations in machine learning and data analysis within the aviation industry. The dynamic thresholding and weighted loss functions showed promising improvements to the standard static thresholding method, both in accuracy-related performance metrics and in speed of true fault detection. △ Less","9 March, 2022",https://arxiv.org/pdf/2203.04734
Multi-modal Brain Tumor Segmentation via Missing Modality Synthesis and Modality-level Attention Fusion,Ziqi Huang;Li Lin;Pujin Cheng;Linkai Peng;Xiaoying Tang,"Multi-modal magnetic resonance (MR) imaging provides great potential for diagnosing and analyzing brain gliomas. In clinical scenarios, common MR sequences such as T1, T2 and FLAIR can be obtained simultaneously in a single scanning process. However, acquiring contrast enhanced modalities such as T1ce requires additional time, cost, and injection of contrast agent. As such, it is clinically meaningful to develop a method to synthesize unavailable modalities which can also be used as additional inputs to downstream tasks (e.g., brain tumor segmentation) for performance enhancing. In this work, we propose an end-to-end framework named Modality-Level Attention Fusion Network (MAF-Net), wherein we innovatively conduct patchwise contrastive learning for extracting multi-modal latent features and dynamically assigning attention weights to fuse different modalities. Through extensive experiments on BraTS2020, our proposed MAF-Net is found to yield superior T1ce synthesis performance (SSIM of 0.8879 and PSNR of 22.78) and accurate brain tumor segmentation (mean Dice scores of 67.9%, 41.8% and 88.0% on segmenting the tumor core, enhancing tumor and whole tumor). △ Less","9 March, 2022",https://arxiv.org/pdf/2203.04586
Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks,Yunshan Zhong;Mingbao Lin;Xunchao Li;Ke Li;Yunhang Shen;Fei Chao;Yongjian Wu;Rongrong Ji,"Light-weight super-resolution (SR) models have received considerable attention for their serviceability in mobile devices. Many efforts employ network quantization to compress SR models. However, these methods suffer from severe performance degradation when quantizing the SR models to ultra-low precision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In this paper, we identify that the performance drop comes from the contradiction between the layer-wise symmetric quantizer and the highly asymmetric activation distribution in SR models. This discrepancy leads to either a waste on the quantization levels or detail loss in reconstructed images. Therefore, we propose a novel activation quantizer, referred to as Dynamic Dual Trainable Bounds (DDTB), to accommodate the asymmetry of the activations. Specifically, DDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower bounds to tackle the highly asymmetric activations. 2) A dynamic gate controller to adaptively adjust the upper and lower bounds at runtime to overcome the drastically varying activation ranges over different samples.To reduce the extra overhead, the dynamic gate controller is quantized to 2-bit and applied to only part of the SR networks according to the introduced dynamic intensity. Extensive experiments demonstrate that our DDTB exhibits significant performance improvements in ultra-low precision. For example, our DDTB achieves a 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and scaling up output images to x4. Code is at \url{https://github.com/zysxmu/DDTB}. △ Less","3 July, 2022",https://arxiv.org/pdf/2203.03844
Conquering Data Variations in Resolution: A Slice-Aware Multi-Branch Decoder Network,Shuxin Wang;Shilei Cao;Zhizhong Chai;Dong Wei;Kai Ma;Liansheng Wang;Yefeng Zheng,"Fully convolutional neural networks have made promising progress in joint liver and liver tumor segmentation. Instead of following the debates over 2D versus 3D networks (for example, pursuing the balance between large-scale 2D pretraining and 3D context), in this paper, we novelly identify the wide variation in the ratio between intra- and inter-slice resolutions as a crucial obstacle to the performance. To tackle the mismatch between the intra- and inter-slice information, we propose a slice-aware 2.5D network that emphasizes extracting discriminative features utilizing not only in-plane semantics but also out-of-plane coherence for each separate slice. Specifically, we present a slice-wise multi-input multi-output architecture to instantiate such a design paradigm, which contains a Multi-Branch Decoder (MD) with a Slice-centric Attention Block (SAB) for learning slice-specific features and a Densely Connected Dice (DCD) loss to regularize the inter-slice predictions to be coherent and continuous. Based on the aforementioned innovations, we achieve state-of-the-art results on the MICCAI 2017 Liver Tumor Segmentation (LiTS) dataset. Besides, we also test our model on the ISBI 2019 Segmentation of THoracic Organs at Risk (SegTHOR) dataset, and the result proves the robustness and generalizability of the proposed method in other segmentation tasks. △ Less","7 March, 2022",https://arxiv.org/pdf/2203.03640
Remote blood pressure measurement via spatiotemporal mapping of a short-time facial video,Jialiang Zhuang;Bin Li;Yun Zhang;Yuheng Chen;Xiujuan Zheng,"Blood pressure (BP) monitoring is vital in daily healthcare, especially for cardiovascular diseases. However, BP values are mainly acquired through the contact sensing method, which is inconvenient and unfriendly to continuous BP measurement. Hence, we propose an efficient end-to-end network to estimate the BP values from a facial video to achieve remote BP measurement in daily life. In this study, we first derived a Spatial-temporal map of a short-time (~15s) facial video. According to the Spatial-temporal map, we then regressed the BP ranges by a designed blood pressure classifier and simultaneously calculated the specific value by a blood pressure calculator in each BP range. In addition, we also developed an innovative oversampling training strategy to handle the unbalanced data distribution problem. Finally, we trained the proposed network on a private dataset ASPD and tested it on the popular dataset MMSE-HR. As a result, the proposed network achieved a state-of-the-art MAE of 12.35 mmHg and 9.5 mmHg on systolic and diastolic BP measurements, which is better than the recent works. It concludes that the proposed method has excellent potential for camera-based BP monitoring in real-world scenarios. △ Less","23 June, 2022",https://arxiv.org/pdf/2203.03634
Automated Single-Label Patent Classification using Ensemble Classifiers,Eleni Kamateri;Vasileios Stamatis;Konstantinos Diamantaras;Michail Salampasis,"Many thousands of patent applications arrive at patent offices around the world every day. One important subtask when a patent application is submitted is to assign one or more classification codes from the complex and hierarchical patent classification schemes that will enable routing of the patent application to a patent examiner who is knowledgeable about the specific technical field. This task is typically undertaken by patent professionals, however due to the large number of applications and the potential complexity of an invention, they are usually overwhelmed. Therefore, there is a need for this code assignment manual task to be supported or even fully automated by classification systems that will classify patent applications, hopefully with an accuracy close to patent professionals. Like in many other text analysis problems, in the last years, this intellectually demanding task has been studied using word embeddings and deep learning techniques. In this paper we shortly review these research efforts and experiment with similar deep learning techniques using different feature representations on automatic patent classification in the level of sub-classes. On top of that, we present an innovative method of ensemble classifiers trained with different parts of the patent document. To the best of our knowledge, this is the first time that an ensemble method was proposed for the patent classification problem. Our first results are quite promising showing that an ensemble architecture of classifiers significantly outperforms current state-of-the-art techniques using the same classifiers as standalone solutions. △ Less","3 March, 2022",https://arxiv.org/pdf/2203.03552
On observability and optimal gain design for distributed linear filtering and prediction,Subhro Das,"This paper presents a new approach to distributed linear filtering and prediction. The problem under consideration consists of a random dynamical system observed by a multi-agent network of sensors where the network is sparse. Inspired by the consensus+innovations type of distributed estimation approaches, this paper proposes a novel algorithm that fuses the concepts of consensus and innovations. The paper introduces a definition of distributed observability, required by the proposed algorithm, which is a weaker assumption than that of global observability and connected network assumptions combined together. Following first principles, the optimal gain matrices are designed such that the mean-squared error of estimation is minimized at each agent and the distributed version of the algebraic Riccati equation is derived for computing the gains. △ Less","7 March, 2022",https://arxiv.org/pdf/2203.03521
"Deep Learning-Based Joint Control of Acoustic Echo Cancellation, Beamforming and Postfiltering",Thomas Haubner;Walter Kellermann,"We introduce a novel method for controlling the functionality of a hands-free speech communication device which comprises a model-based acoustic echo canceller (AEC), minimum variance distortionless response (MVDR) beamformer (BF) and spectral postfilter (PF). While the AEC removes the early echo component, the MVDR BF and PF suppress the residual echo and background noise. As key innovation, we suggest to use a single deep neural network (DNN) to jointly control the adaptation of the various algorithmic components. This allows for rapid convergence and high steady-state performance in the presence of high-level interfering double-talk. End-to-end training of the DNN using a time-domain speech extraction loss function avoids the design of individual control strategies. △ Less","10 August, 2022",https://arxiv.org/pdf/2203.01793
Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation,Wei Dai;Daniel Berleant,"Accuracies of deep learning (DL) classifiers are often unstable in that they may change significantly when retested on adversarial images, imperfect images, or perturbed images. This paper adds to the fundamental body of work on benchmarking the robustness of DL classifiers on defective images. To measure robust DL classifiers, previous research reported on single-factor corruption. We created comprehensive 69 benchmarking image sets, including a clean set, sets with single factor perturbations, and sets with two-factor perturbation conditions. The state-of-the-art two-factor perturbation includes (a) two digital perturbations (salt & pepper noise and Gaussian noise) applied in both sequences, and (b) one digital perturbation (salt & pepper noise) and a geometric perturbation (rotation) applied in both sequences. Previous research evaluating DL classifiers has often used top-1/top-5 accuracy. We innovate a new two-dimensional, statistical matrix to evaluating robustness of DL classifiers. Also, we introduce a new visualization tool, including minimum accuracy, maximum accuracy, mean accuracies, and coefficient of variation (CV), for benchmarking robustness of DL classifiers. Comparing with single factor corruption, we first report that using two-factor perturbed images improves both robustness and accuracy of DL classifiers. All source codes and related image sets are shared on the Website at http://cslinux.semo.edu/david/data to support future academic research and industry projects. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.01323
Information Gain Propagation: a new way to Graph Active Learning with Soft Labels,Wentao Zhang;Yexin Wang;Zhenbang You;Meng Cao;Ping Huang;Jiulong Shan;Zhi Yang;Bin Cui,"Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human effort. GNN-based Active Learning (AL) methods are proposed to improve the labeling efficiency by selecting the most valuable nodes to label. Existing methods assume an oracle can correctly categorize all the selected nodes and thus just focus on the node selection. However, such an exact labeling task is costly, especially when the categorization is out of the domain of individual expert (oracle). The paper goes further, presenting a soft-label approach to AL on GNNs. Our key innovations are: i) relaxed queries where a domain expert (oracle) only judges the correctness of the predicted labels (a binary question) rather than identifying the exact class (a multi-class question), and ii) new criteria of maximizing information gain propagation for active learner with relaxed queries and soft labels. Empirical studies on public datasets demonstrate that our method significantly outperforms the state-of-the-art GNN-based AL methods in terms of both accuracy and labeling cost. △ Less","2 March, 2022",https://arxiv.org/pdf/2203.01093
Synthesizing Fine-Grained Synchronization Protocols for Implicit Monitors (Extended Version),Kostas Ferles;Benjamin Sepanski;Rahul Krishnan;James Bornholt;Isil Dillig,"A monitor is a widely-used concurrent programming abstraction that encapsulates all shared state between threads. Monitors can be classified as being either implicit or explicit depending on the primitives they provide. Implicit monitors are much easier to program but typically not as efficient. To address this gap, there has been recent research on automatically synthesizing explicit-signal monitors from an implicit specification, but prior work does not exploit all paralellization opportunities due to the use of a single lock for the entire monitor. This paper presents a new technique for synthesizing fine-grained explicit-synchronization protocols from implicit monitors. Our method is based on two key innovations: First, we present a new static analysis for inferring safe interleavings that allow violating mutual exclusion of monitor operations without changing its semantics. Second, we use the results of this static analysis to generate a MaxSAT instance whose models correspond to correct-by-construction synchronization protocols. We have implemented our approach in a tool called Cortado and evaluate it on monitors that contain parallelization opportunities. Our evaluation shows that Cortado can synthesize synchronization policies that are competitive with, or even better than, expert-written ones on these benchmarks. △ Less","16 March, 2022",https://arxiv.org/pdf/2203.00783
Towards a Common Speech Analysis Engine,Hagai Aronowitz;Itai Gat;Edmilson Morais;Weizhong Zhu;Ron Hoory,"Recent innovations in self-supervised representation learning have led to remarkable advances in natural language processing. That said, in the speech processing domain, self-supervised representation learning-based systems are not yet considered state-of-the-art. We propose leveraging recent advances in self-supervised-based speech processing to create a common speech analysis engine. Such an engine should be able to handle multiple speech processing tasks, using a single architecture, to obtain state-of-the-art accuracy. The engine must also enable support for new tasks with small training datasets. Beyond that, a common engine should be capable of supporting distributed training with client in-house private data. We present the architecture for a common speech analysis engine based on the HuBERT self-supervised speech representation. Based on experiments, we report our results for language identification and emotion recognition on the standard evaluations NIST-LRE 07 and IEMOCAP. Our results surpass the state-of-the-art performance reported so far on these tasks. We also analyzed our engine on the emotion recognition task using reduced amounts of training data and show how to achieve improved results. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00613
"SwitchHit: A Probabilistic, Complementarity-Based Switching System for Improved Visual Place Recognition in Changing Environments",Maria Waheed;Michael Milford;Klaus McDonald-Maier;Shoaib Ehsan,"Visual place recognition (VPR), a fundamental task in computer vision and robotics, is the problem of identifying a place mainly based on visual information. Viewpoint and appearance changes, such as due to weather and seasonal variations, make this task challenging. Currently, there is no universal VPR technique that can work in all types of environments, on a variety of robotic platforms, and under a wide range of viewpoint and appearance changes. Recent work has shown the potential of combining different VPR methods intelligently by evaluating complementarity for some specific VPR datasets to achieve better performance. This, however, requires ground truth information (correct matches) which is not available when a robot is deployed in a real-world scenario. Moreover, running multiple VPR techniques in parallel may be prohibitive for resource-constrained embedded platforms. To overcome these limitations, this paper presents a probabilistic complementarity based switching VPR system, SwitchHit. Our proposed system consists of multiple VPR techniques, however, it does not simply run all techniques at once, rather predicts the probability of correct match for an incoming query image and dynamically switches to another complementary technique if the probability of correctly matching the query is below a certain threshold. This innovative use of multiple VPR techniques allow our system to be more efficient and robust than other combined VPR approaches employing brute force and running multiple VPR techniques at once. Thus making it more suitable for resource constrained embedded systems and achieving an overall superior performance from what any individual VPR method in the system could have by achieved running independently. △ Less","1 March, 2022",https://arxiv.org/pdf/2203.00591
Sensor technologies in cancer research for new directions in diagnosis and treatment: and exploratory analysis,Mario Coccia;Saeed Roshani;Melika Mosleh,"The goal of this study is an exploratory analysis concerning main sensor technologies applied in cancer research to detect new directions in diagnosis and treatments. The study focused on types of cancer having a high incidence and mortality worldwide: breast, lung, colorectal and prostate. Data of the Web of Science (WOS) core collection database are used to retrieve articles related to sensor technologies and cancer research over 1991-2021 period. We utilized Gephi software version 0.9.2 to visualize the co-word networks of the interaction between sensor technologies and cancers under study. Results show main clusters of interaction per typology of cancer. Biosensor is the only type of sensor that plays an essential role in all types of cancer: breast cancer, lung cancer, prostate cancer, and colorectal cancer. Electrochemical sensor is applied in all types of cancer under study except lung cancer. Electrochemical biosensor is used in breast cancer, lung cancer, and prostate cancer research but not colorectal cancer. Optical sensor can also be considered one of the sensor technologies that significantly is used in breast cancer, prostate cancer, and colorectal cancer. This study shows that this type of sensor is applied in more diversified approaches. Moreover, the oxygen sensor is mostly studied in lung cancer and breast cancer due to the usage in breath analysis for the treatment process. Finally, Cmos sensor is a technology used mainly in lung cancer and colorectal cancer. Results here suggest new directions for the evolution of science and technology of sensors in cancer research to support innovation and research policy directed to new technological trajectories having a potential of accelerated growth and positive social impact for diagnosis and treatments of cancer. △ Less","4 February, 2022",https://arxiv.org/pdf/2203.00502
ERF: Explicit Radiance Field Reconstruction From Scratch,Samir Aroudj;Steven Lovegrove;Eddy Ilg;Tanner Schmidt;Michael Goesele;Richard Newcombe,"We propose a novel explicit dense 3D reconstruction approach that processes a set of images of a scene with sensor poses and calibrations and estimates a photo-real digital model. One of the key innovations is that the underlying volumetric representation is completely explicit in contrast to neural network-based (implicit) alternatives. We encode scenes explicitly using clear and understandable mappings of optimization variables to scene geometry and their outgoing surface radiance. We represent them using hierarchical volumetric fields stored in a sparse voxel octree. Robustly reconstructing such a volumetric scene model with millions of unknown variables from registered scene images only is a highly non-convex and complex optimization problem. To this end, we employ stochastic gradient descent (Adam) which is steered by an inverse differentiable renderer. We demonstrate that our method can reconstruct models of high quality that are comparable to state-of-the-art implicit methods. Importantly, we do not use a sequential reconstruction pipeline where individual steps suffer from incomplete or unreliable information from previous stages, but start our optimizations from uniformed initial solutions with scene geometry and radiance that is far off from the ground truth. We show that our method is general and practical. It does not require a highly controlled lab setup for capturing, but allows for reconstructing scenes with a vast variety of objects, including challenging ones, such as outdoor plants or furry toys. Finally, our reconstructed scene models are versatile thanks to their explicit design. They can be edited interactively which is computationally too costly for implicit alternatives. △ Less","28 February, 2022",https://arxiv.org/pdf/2203.00051
Formalizing Oracle Trust Models for blockchain-based business applications. An example from the supply chain sector,Giulio Caldarelli,"Blockchain technology truly opened the gate to a wave of unparalleled innovations; however, despite the rapidly growing load of hype, the integration into the business, apart from a few applications, seems to be coming at a slower rate. One reason for that delay may be the need in the real-world applications for the so-called trust model. Trust models are rarely mentioned in blockchain application proposals despite their importance, which creates skepticism about their successful developments. To promote trust model implementation and help practitioners in its redaction, this article provides an outline of what a trust model is, why it is essential, and an example of how it is elaborated. The discussed example comes from a case study of a dairy company that implemented blockchain for the traceability of its products. Despite being tailored on a traceability project, the redaction and elements of the trust model, with few adjustments, could be easily readapted for other applications. △ Less","2 March, 2022",https://arxiv.org/pdf/2202.13930
AGMR-Net: Attention Guided Multiscale Recovery framework for stroke segmentation,Xiuquan Du;Kunpeng Ma;Yuhui Song,"Automatic and accurate lesion segmentation is critical for clinically estimating the lesion statuses of stroke diseases and developing appropriate diagnostic systems. Although existing methods have achieved remarkable results, further adoption of the models is hindered by: (1) inter-class indistinction, the normal brain tissue resembles the lesion in appearance. (2) intra-class inconsistency, large variability exists between different areas of the lesion. To solve these challenges in stroke segmentation, we propose a novel method, namely Attention Guided Multiscale Recovery framework (AGMR-Net) in this paper. Firstly, a coarse-grained patch attention module in the encoding is adopted to get a patch-based coarse-grained attention map in a multi-stage explicitly supervised way, enabling target spatial context saliency representation with a patch-based weighting technique that eliminates the effect of intra-class inconsistency. Secondly, to obtain a more detailed boundary partitioning to solve the challenge of the inter-class indistinction, a newly designed cross-dimensional feature fusion module is used to capture global contextual information to further guide the selective aggregation of 2D and 3D features, which can compensate for the lack of boundary learning capability of 2D convolution. Lastly, in the decoding stage, an innovative designed multi-scale deconvolution upsampling instead of linear interpolation enhances the recovery of target space and boundary information. The AGMR-Net is evaluated on the open dataset Anatomical Tracings of Lesions-After-Stroke (ATLAS), achieving the highest dice similarity coefficient (DSC) score of 0.594, Hausdorff distance of 27.005 mm, and average symmetry surface distance of 7.137 mm, which demonstrate that our proposed method outperforms other state-of-the-art methods and has great potential in the diagnosis of stroke. △ Less","16 April, 2022",https://arxiv.org/pdf/2202.13687
Point Set Self-Embedding,Ruihui Li;Xianzhi Li;Tien-Tsin Wong;Chi-Wing Fu,"This work presents an innovative method for point set self-embedding, that encodes the structural information of a dense point set into its sparser version in a visual but imperceptible form. The self-embedded point set can function as the ordinary downsampled one and be visualized efficiently on mobile devices. Particularly, we can leverage the self-embedded information to fully restore the original point set for detailed analysis on remote servers. This task is challenging since both the self-embedded point set and the restored point set should resemble the original one. To achieve a learnable self-embedding scheme, we design a novel framework with two jointly-trained networks: one to encode the input point set into its self-embedded sparse point set and the other to leverage the embedded information for inverting the original point set back. Further, we develop a pair of up-shuffle and down-shuffle units in the two networks, and formulate loss terms to encourage the shape similarity and point distribution in the results. Extensive qualitative and quantitative results demonstrate the effectiveness of our method on both synthetic and real-scanned datasets. △ Less","28 February, 2022",https://arxiv.org/pdf/2202.13577
A Bibliometric Horizon Scanning Methodology for Identifying Emerging Topics in the Scientific Literature,Artjay Javier;Beth Masimore;John Chase;F. G. Serpa;John T. Rigsby;Avory Bryant;Jeffrey Solka;Ryan J. Zelnio,"A bibliometric methodology for scanning for emerging science and technology areas is described, where topics in the science, technology and innovation enterprise are discovered using Latent Dirichlet Allocation, their growth rates are modeled using first-order rate kinetics, and research specialization of various entities in these topics is measured using the location quotient. Multiple interactive visualization interfaces that integrate these results together to assist human analysts are developed. This methodology is demonstrated by analyzing the last five years of publications, patents and grants (~ 14 million documents) showing, for example, that deep learning for machine vision is the fastest growing area, and that China has a stronger focus than the U.S. in this area. △ Less","27 February, 2022",https://arxiv.org/pdf/2202.13480
ICASSP 2022 Deep Noise Suppression Challenge,Harishchandra Dubey;Vishak Gopal;Ross Cutler;Ashkan Aazami;Sergiy Matusevych;Sebastian Braun;Sefik Emre Eskimez;Manthan Thakker;Takuya Yoshioka;Hannes Gamper;Robert Aichner,"The Deep Noise Suppression (DNS) challenge is designed to foster innovation in the area of noise suppression to achieve superior perceptual speech quality. This is the 4th DNS challenge, with the previous editions held at INTERSPEECH 2020, ICASSP 2021, and INTERSPEECH 2021. We open-source datasets and test sets for researchers to train their deep noise suppression models, as well as a subjective evaluation framework based on ITU-T P.835 to rate and rank-order the challenge entries. We provide access to DNSMOS P.835 and word accuracy (WAcc) APIs to challenge participants to help with iterative model improvements. In this challenge, we introduced the following changes: (i) Included mobile device scenarios in the blind test set; (ii) Included a personalized noise suppression track with baseline; (iii) Added WAcc as an objective metric; (iv) Included DNSMOS P.835; (v) Made the training datasets and test sets fullband (48 kHz). We use an average of WAcc and subjective scores P.835 SIG, BAK, and OVRL to get the final score for ranking the DNS models. We believe that as a research community, we still have a long way to go in achieving excellent speech quality in challenging noisy real-world scenarios. △ Less","26 February, 2022",https://arxiv.org/pdf/2202.13288
A Computer Vision-assisted Approach to Automated Real-Time Road Infrastructure Management,Philippe Heitzmann,"Accurate automated detection of road pavement distresses is critical for the timely identification and repair of potentially accident-inducing road hazards such as potholes and other surface-level asphalt cracks. Deployment of such a system would be further advantageous in low-resource environments where lack of government funding for infrastructure maintenance typically entails heightened risks of potentially fatal vehicular road accidents as a result of inadequate and infrequent manual inspection of road systems for road hazards. To remedy this, a recent research initiative organized by the Institute of Electrical and Electronics Engineers (""IEEE"") as part of their 2020 Global Road Damage Detection (""GRDC"") Challenge published in May 2020 a novel 21,041 annotated image dataset of various road distresses calling upon academic and other researchers to submit innovative deep learning-based solutions to these road hazard detection problems. Making use of this dataset, we propose a supervised object detection approach leveraging You Only Look Once (""YOLO"") and the Faster R-CNN frameworks to detect and classify road distresses in real-time via a vehicle dashboard-mounted smartphone camera, producing 0.68 F1-score experimental results ranking in the top 5 of 121 teams that entered this challenge as of December 2021. △ Less","26 February, 2022",https://arxiv.org/pdf/2202.13285
Impact of Interference Subtraction on Grant-Free Multiple Access with Massive MIMO,Lorenzo Valentini;Alberto Faedi;Marco Chiani;Enrico Paolini,"The design of highly scalable multiple access schemes is a main challenge in the evolution towards future massive machine-type communications, where reliability and latency constraints must be ensured to a large number of uncoordinated devices. In this scenario, coded random access (CRA) schemes, where successive interference cancellation algorithms allow large improvements with respect to classical random access protocols, have recently attracted an increasing interest. Impressive performance can be potentially obtained by combining CRA with massive multiple input multiple output (MIMO). In this paper we provide an analysis of such schemes focusing on the effects of imperfect channel estimation on successive interference cancellation. Based on the analysis we then propose an innovative signal processing algorithm for CRA in massive MIMO systems. △ Less","26 February, 2022",https://arxiv.org/pdf/2202.13156
Automated Extraction of Energy Systems Information from Remotely Sensed Data: A Review and Analysis,Simiao Ren;Wei Hu;Kyle Bradbury;Dylan Harrison-Atlas;Laura Malaguzzi Valeri;Brian Murray;Jordan M. Malof,"High quality energy systems information is a crucial input to energy systems research, modeling, and decision-making. Unfortunately, actionable information about energy systems is often of limited availability, incomplete, or only accessible for a substantial fee or through a non-disclosure agreement. Recently, remotely sensed data (e.g., satellite imagery, aerial photography) have emerged as a potentially rich source of energy systems information. However, the use of these data is frequently challenged by its sheer volume and complexity, precluding manual analysis. Recent breakthroughs in machine learning have enabled automated and rapid extraction of useful information from remotely sensed data, facilitating large-scale acquisition of critical energy system variables. Here we present a systematic review of the literature on this emerging topic, providing an in-depth survey and review of papers published within the past two decades. We first taxonomize the existing literature into ten major areas, spanning the energy value chain. Within each research area, we distill and critically discuss major features that are relevant to energy researchers, including, for example, key challenges regarding the accessibility and reliability of the methods. We then synthesize our findings to identify limitations and trends in the literature as a whole, and discuss opportunities for innovation. These include the opportunity to extend the methods beyond electricity to broader energy systems and wider geographic areas; and the ability to expand the use of these methods in research and decision making as satellite data become cheaper and easier to access. We also find that there are persistent challenges: limited standardization and rigor of performance assessments; limited sharing of code, which would improve replicability; and a limited consideration of the ethics and privacy of data. △ Less","2 October, 2022",https://arxiv.org/pdf/2202.12939
The evolution of scientific literature as metastable knowledge states,Sai Dileep Koneru;David Rench McCauley;Michael C. Smith;David Guarrera;Jenn Robinson;Sarah Rajtmajer,"The problem of identifying common concepts in the sciences and deciding when new ideas have emerged is an open one. Metascience researchers have sought to formalize principles underlying stages in the life-cycle of scientific research, determine how knowledge is transferred between scientists and stakeholders, and understand how new ideas are generated and take hold. Here, we model the state of scientific knowledge immediately preceding new directions of research as a metastable state and the creation of new concepts as combinatorial innovation. We find that, through the combined use of natural language clustering and citation graph analysis, we can predict the evolution of ideas over time and thus connect a single scientific article to past and future concepts in a way that goes beyond traditional citation and reference connections. △ Less","11 September, 2022",https://arxiv.org/pdf/2202.12913
A Systematic Literature Review about Idea Mining: The Use of Machine-driven Analytics to Generate Ideas,Workneh Y. Ayele;Gustaf Juell-Skielse,"Idea generation is the core activity of innovation. Digital data sources, which are sources of innovation, such as patents, publications, social media, websites, etc., are increasingly growing at unprecedented volume. Manual idea generation is time-consuming and is affected by the subjectivity of the individuals involved. Therefore, the use machine-driven data analytics techniques to analyze data to generate ideas and support idea generation by serving users is useful. The objective of this study is to study state-of the-art machine-driven analytics for idea generation and data sources, hence the result of this study will generally server as a guideline for choosing techniques and data sources. A systematic literature review is conducted to identify relevant scholarly literature from IEEE, Scopus, Web of Science and Google Scholar. We selected a total of 71 articles and analyzed them thematically. The results of this study indicate that idea generation through machine-driven analytics applies text mining, information retrieval (IR), artificial intelligence (AI), deep learning, machine learning, statistical techniques, natural language processing (NLP), NLP-based morphological analysis, network analysis, and bibliometric to support idea generation. The results include a list of techniques and procedures in idea generation through machine-driven idea analytics. Additionally, characterization and heuristics used in idea generation are summarized. For the future, tools designed to generate ideas could be explored. △ Less","30 January, 2022",https://arxiv.org/pdf/2202.12826
Improving Amharic Handwritten Word Recognition Using Auxiliary Task,Mesay Samuel Gondere;Lars Schmidt-Thieme;Durga Prasad Sharma;Abiot Sinamo Boltena,"Amharic is one of the official languages of the Federal Democratic Republic of Ethiopia. It is one of the languages that use an Ethiopic script which is derived from Gee'z, ancient and currently a liturgical language. Amharic is also one of the most widely used literature-rich languages of Ethiopia. There are very limited innovative and customized research works in Amharic optical character recognition (OCR) in general and Amharic handwritten text recognition in particular. In this study, Amharic handwritten word recognition will be investigated. State-of-the-art deep learning techniques including convolutional neural networks together with recurrent neural networks and connectionist temporal classification (CTC) loss were used to make the recognition in an end-to-end fashion. More importantly, an innovative way of complementing the loss function using the auxiliary task from the row-wise similarities of the Amharic alphabet was tested to show a significant recognition improvement over a baseline method. Such findings will promote innovative problem-specific solutions as well as will open insight to a generalized solution that emerges from problem-specific domains. △ Less","25 February, 2022",https://arxiv.org/pdf/2202.12687
Retriever: Learning Content-Style Representation as a Token-Level Bipartite Graph,Dacheng Yin;Xuanchi Ren;Chong Luo;Yuwang Wang;Zhiwei Xiong;Wenjun Zeng,"This paper addresses the unsupervised learning of content-style decomposed representation. We first give a definition of style and then model the content-style representation as a token-level bipartite graph. An unsupervised framework, named Retriever, is proposed to learn such representations. First, a cross-attention module is employed to retrieve permutation invariant (P.I.) information, defined as style, from the input data. Second, a vector quantization (VQ) module is used, together with man-induced constraints, to produce interpretable content tokens. Last, an innovative link attention module serves as the decoder to reconstruct data from the decomposed content and style, with the help of the linking keys. Being modal-agnostic, the proposed Retriever is evaluated in both speech and image domains. The state-of-the-art zero-shot voice conversion performance confirms the disentangling ability of our framework. Top performance is also achieved in the part discovery task for images, verifying the interpretability of our representation. In addition, the vivid part-based style transfer quality demonstrates the potential of Retriever to support various fascinating generative tasks. Project page at https://ydcustc.github.io/retriever-demo/. △ Less","24 February, 2022",https://arxiv.org/pdf/2202.12307
Matching Papers and Reviewers at Large Conferences,Kevin Leyton-Brown;Mausam;Yatin Nandwani;Hedayat Zarkoob;Chris Cameron;Neil Newman;Dinesh Raghu,"Peer-reviewed conferences, the main publication venues in CS, rely critically on matching highly qualified reviewers for each paper. Because of the growing scale of these conferences, the tight timelines on which they operate, and a recent surge in explicitly dishonest behavior, there is now no alternative to performing this matching in an automated way. This paper studies a novel reviewer-paper matching approach that was recently deployed in the 35th AAAI Conference on Artificial Intelligence (AAAI 2021), and has since been adopted (wholly or partially) by other conferences including ICML 2022, AAAI 2022, and IJCAI 2022. This approach has three main elements: (1) collecting and processing input data to identify problematic matches and generate reviewer-paper scores; (2) formulating and solving an optimization problem to find good reviewer-paper matchings; and (3) a two-phase reviewing process that shifts reviewing resources away from papers likely to be rejected and towards papers closer to the decision boundary. This paper also describes an evaluation of these innovations based on an extensive post-hoc analysis on real data -- including a comparison with the matching algorithm used in AAAI's previous (2020) iteration -- and supplements this with additional numerical experimentation. △ Less","5 August, 2022",https://arxiv.org/pdf/2202.12273
Temporal Convolution Domain Adaptation Learning for Crops Growth Prediction,Shengzhe Wang;Ling Wang;Zhihao Lin;Xi Zheng,"Existing Deep Neural Nets on crops growth prediction mostly rely on availability of a large amount of data. In practice, it is difficult to collect enough high-quality data to utilize the full potential of these deep learning models. In this paper, we construct an innovative network architecture based on domain adaptation learning to predict crops growth curves with limited available crop data. This network architecture overcomes the challenge of data availability by incorporating generated data from the developed crops simulation model. We are the first to use the temporal convolution filters as the backbone to construct a domain adaptation network architecture which is suitable for deep learning regression models with very limited training data of the target domain. We conduct experiments to test the performance of the network and compare our proposed architecture with other state-of-the-art methods, including a recent LSTM-based domain adaptation network architecture. The results show that the proposed temporal convolution-based network architecture outperforms all benchmarks not only in accuracy but also in model size and convergence rate. △ Less","24 February, 2022",https://arxiv.org/pdf/2202.12120
A Unified Framework for Campaign Performance Forecasting in Online Display Advertising,Jun Chen;Cheng Chen;Huayue Zhang;Qing Tan,"Advertisers usually enjoy the flexibility to choose criteria like target audience, geographic area and bid price when planning an campaign for online display advertising, while they lack forecast information on campaign performance to optimize delivery strategies in advance, resulting in a waste of labour and budget for feedback adjustments. In this paper, we aim to forecast key performance indicators for new campaigns given any certain criteria. Interpretable and accurate results could enable advertisers to manage and optimize their campaign criteria. There are several challenges for this very task. First, platforms usually offer advertisers various criteria when they plan an advertising campaign, it is difficult to estimate campaign performance unifiedly because of the great difference among bidding types. Furthermore, complex strategies applied in bidding system bring great fluctuation on campaign performance, making estimation accuracy an extremely tough problem. To address above challenges, we propose a novel Campaign Performance Forecasting framework, which firstly reproduces campaign performance on historical logs under various bidding types with a unified replay algorithm, in which essential auction processes like match and rank are replayed, ensuring the interpretability on forecast results. Then, we innovatively introduce a multi-task learning method to calibrate the deviation of estimation brought by hard-to-reproduce bidding strategies in replay. The method captures mixture calibration patterns among related forecast indicators to map the estimated results to the true ones, improving both accuracy and efficiency significantly. Experiment results on a dataset from Taobao.com demonstrate that the proposed framework significantly outperforms other baselines by a large margin, and an online A/B test verifies its effectiveness in the real world. △ Less","23 February, 2022",https://arxiv.org/pdf/2202.11877
Nuclei panoptic segmentation and composition regression with multi-task deep neural networks,Satoshi Kondo;Satoshi Kasai,"Nuclear segmentation, classification and quantification within Haematoxylin & Eosin stained histology images enables the extraction of interpretable cell-based features that can be used in downstream explainable models in computational pathology. The Colon Nuclei Identification and Counting (CoNIC) Challenge is held to help drive forward research and innovation for automatic nuclei recognition in computational pathology. This report describes our proposed method submitted to the CoNIC challenge. Our method employs a multi-task learning framework, which performs a panoptic segmentation task and a regression task. For the panoptic segmentation task, we use encoder-decoder type deep neural networks predicting a direction map in addition to a segmentation map in order to separate neighboring nuclei into different instances △ Less","23 February, 2022",https://arxiv.org/pdf/2202.11804
LUCE: A Blockchain-based data sharing platform for monitoring data license accountability and compliance,Visara Urovi;Vikas Jaiman;Arno Angerer;Michel Dumontier,"Easy access to data is one of the main avenues to accelerate scientific research. As a key element of scientific innovations, data sharing allows the reproduction of results, helps prevent data fabrication, falsification, and misuse. Although the research benefits from data reuse are widely acknowledged, the data collections existing today are still kept in silos. Indeed, monitoring what happens to data once they have been handed to a third party is currently not feasible within the current data-sharing practices. We propose a blockchain-based system to trace data collections, and potentially create a more trustworthy data sharing process. In this paper, we present the LUCE (License accoUntability and CompliancE) architecture as a decentralized blockchain-based platform supporting data sharing and reuse. LUCE is designed to provide full transparency on what happens to the data after they are shared with third parties. The contributions of this work are: the definition of a generic model and an implementation for decentralized data sharing accountability and compliance and to incorporates dynamic consent and legal compliance mechanisms. We test the scalability of the platform in a real-time environment where a growing number of users access and reuse different datasets. Compared to existing data-sharing solutions, LUCE provides transparency over data sharing practices, enables data reuse and supports regulatory requirements. The experimentation shows that the platform can be scaled for a large number of users. △ Less","23 February, 2022",https://arxiv.org/pdf/2202.11646
Absolute Zero-Shot Learning,Rui Gao;Fan Wan;Daniel Organisciak;Jiyao Pu;Junyan Wang;Haoran Duan;Peng Zhang;Xingsong Hou;Yang Long,"Considering the increasing concerns about data copyright and privacy issues, we present a novel Absolute Zero-Shot Learning (AZSL) paradigm, i.e., training a classifier with zero real data. The key innovation is to involve a teacher model as the data safeguard to guide the AZSL model training without data leaking. The AZSL model consists of a generator and student network, which can achieve date-free knowledge transfer while maintaining the performance of the teacher network. We investigate `black-box' and `white-box' scenarios in AZSL task as different levels of model security. Besides, we also provide discussion of teacher model in both inductive and transductive settings. Despite embarrassingly simple implementations and data-missing disadvantages, our AZSL framework can retain state-of-the-art ZSL and GZSL performance under the `white-box' scenario. Extensive qualitative and quantitative analysis also demonstrates promising results when deploying the model under `black-box' scenario. △ Less","23 February, 2022",https://arxiv.org/pdf/2202.11319
Neural Generalised AutoRegressive Conditional Heteroskedasticity,Zexuan Yin;Paolo Barucca,"We propose Neural GARCH, a class of methods to model conditional heteroskedasticity in financial time series. Neural GARCH is a neural network adaptation of the GARCH 1,1 model in the univariate case, and the diagonal BEKK 1,1 model in the multivariate case. We allow the coefficients of a GARCH model to be time varying in order to reflect the constantly changing dynamics of financial markets. The time varying coefficients are parameterised by a recurrent neural network that is trained with stochastic gradient variational Bayes. We propose two variants of our model, one with normal innovations and the other with Students t innovations. We test our models on a wide range of univariate and multivariate financial time series, and we find that the Neural Students t model consistently outperforms the others. △ Less","22 February, 2022",https://arxiv.org/pdf/2202.11285
Circuit and System Technologies for Energy-Efficient Edge Robotics,Zishen Wan;Ashwin Sanjay Lele;Arijit Raychowdhury,"As we march towards the age of ubiquitous intelligence, we note that AI and intelligence are progressively moving from the cloud to the edge. The success of Edge-AI is pivoted on innovative circuits and hardware that can enable inference and limited learning in resource-constrained edge autonomous systems. This paper introduces a series of ultra-low-power accelerator and system designs on enabling the intelligence in edge robotic platforms, including reinforcement learning neuromorphic control, swarm intelligence, and simultaneous mapping and localization. We put an emphasis on the impact of the mixed-signal circuit, neuro-inspired computing system, benchmarking and software infrastructure, as well as algorithm-hardware co-design to realize the most energy-efficient Edge-AI ASICs for the next-generation intelligent and autonomous systems. △ Less","22 February, 2022",https://arxiv.org/pdf/2202.11237
Outing Power Outages: Real-time and Predictive Socio-demographic Analytics for New York City,Samuel Eckstrom;Graham Murphy;Eileen Ye;Samrat Acharya;Robert Mieth;Yury Dvorkin,"Electrical outages continue to occur despite technological innovations and improvements to electric power distribution infrastructure. In this paper, we describe a tool that was designed to acquire and collect data on electric power outages in New York City since July 2020. The electrical outages are then displayed on a front-end application, which is publicly available. We use the collected outage data to analyze these outages and their socio-economic impacts on electricity vulnerable population groups. We determined that there was a slightly negative linear relationship between income and number of outages. Finally, a Markov Influence Graph was created to better understand the spatial and temporal relationships between outages. △ Less","22 February, 2022",https://arxiv.org/pdf/2202.11066
Targeting occupant feedback using digital twins: Adaptive spatial-temporal thermal preference sampling to optimize personal comfort models,Mahmoud Abdelrahman;Clayton Miller,"Collecting intensive longitudinal thermal preference data from building occupants is emerging as an innovative means of characterizing the performance of buildings and the people who use them. These techniques have occupants giving subjective feedback using smartphones or smartwatches frequently over the course of days or weeks. The intention is that the data will be collected with high spatial and temporal diversity to best characterize a building and the occupant's preferences. But in reality, leaving the occupant to respond in an ad-hoc or fixed interval way creates unneeded survey fatigue and redundant data. This paper outlines a scenario-based (virtual experiment) method for optimizing data sampling using a smartwatch to achieve comparable accuracy in a personal thermal preference model with fewer data. This method uses BIM-extracted spatial data and Graph Neural Network-based (GNN) modeling to find regions of similar comfort preference to identify the best scenarios for triggering the occupant to give feedback. This method is compared to two baseline scenarios that use conventional zoning and a generic 4x4 square meter grid method from two field-based data sets. The results show that the proposed Build2Vec method has an 18-23\% higher overall sampling quality than the spaces-based and square-grid-based sampling methods. The Build2Vec method also performs similar to the baselines when removing redundant occupant feedback points but with better scalability potential. △ Less","10 April, 2022",https://arxiv.org/pdf/2202.10707
Physics-Informed Graph Learning,Ciyuan Peng;Feng Xia;Vidya Saikrishna;Huan Liu,"An expeditious development of graph learning in recent years has found innumerable applications in several diversified fields. Of the main associated challenges are the volume and complexity of graph data. The graph learning models suffer from the inability to efficiently learn graph information. In order to indemnify this inefficacy, physics-informed graph learning (PIGL) is emerging. PIGL incorporates physics rules while performing graph learning, which has enormous benefits. This paper presents a systematic review of PIGL methods. We begin with introducing a unified framework of graph learning models followed by examining existing PIGL methods in relation to the unified framework. We also discuss several future challenges for PIGL. This survey paper is expected to stimulate innovative research and development activities pertaining to PIGL. △ Less","20 October, 2022",https://arxiv.org/pdf/2202.10679
Fast Semantic-Assisted Outlier Removal for Large-scale Point Cloud Registration,Giang Truong;Huu Le;Alvaro Parra;Syed Zulqarnain Gilani;Syed M. S. Islam;David Suter,"With current trends in sensors (cheaper, more volume of data) and applications (increasing affordability for new tasks, new ideas in what 3D data could be useful for); there is corresponding increasing interest in the ability to automatically, reliably, and cheaply, register together individual point clouds. The volume of data to handle, and still elusive need to have the registration occur fully reliably and fully automatically, mean there is a need to innovate further. One largely untapped area of innovation is that of exploiting the {\em semantic information} of the points in question. Points on a tree should match points on a tree, for example, and not points on car. Moreover, such a natural restriction is clearly human-like - a human would generally quickly eliminate candidate regions for matching based on semantics. Employing semantic information is not only efficient but natural. It is also timely - due to the recent advances in semantic classification capabilities. This paper advances this theme by demonstrating that state of the art registration techniques, in particular ones that rely on ""preservation of length under rigid motion"" as an underlying matching consistency constraint, can be augmented with semantic information. Semantic identity is of course also preserved under rigid-motion, but also under wider motions present in a scene. We demonstrate that not only the potential obstacle of cost of semantic segmentation, and the potential obstacle of the unreliability of semantic segmentation; are both no impediment to achieving both speed and accuracy in fully automatic registration of large scale point clouds. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10579
Artificial Intelligence for the Metaverse: A Survey,Thien Huynh-The;Quoc-Viet Pham;Xuan-Qui Pham;Thanh Thi Nguyen;Zhu Han;Dong-Seong Kim,"Along with the massive growth of the Internet from the 1990s until now, various innovative technologies have been created to bring users breathtaking experiences with more virtual interactions in cyberspace. Many virtual environments with thousands of services and applications, from social networks to virtual gaming worlds, have been developed with immersive experience and digital transformation, but most are incoherent instead of being integrated into a platform. In this context, metaverse, a term formed by combining meta and universe, has been introduced as a shared virtual world that is fueled by many emerging technologies, such as fifth-generation networks and beyond, virtual reality, and artificial intelligence (AI). Among such technologies, AI has shown the great importance of processing big data to enhance immersive experience and enable human-like intelligence of virtual agents. In this survey, we make a beneficial effort to explore the role of AI in the foundation and development of the metaverse. We first deliver a preliminary of AI, including machine learning algorithms and deep learning architectures, and its role in the metaverse. We then convey a comprehensive investigation of AI-based methods concerning six technical aspects that have potentials for the metaverse: natural language processing, machine vision, blockchain, networking, digital twin, and neural interface, and being potential for the metaverse. Subsequently, several AI-aided applications, such as healthcare, manufacturing, smart cities, and gaming, are studied to be deployed in the virtual worlds. Finally, we conclude the key contribution of this survey and open some future research directions in AI for the metaverse. △ Less","14 February, 2022",https://arxiv.org/pdf/2202.10336
Multi-Agent Reinforcement Learning for Network Selection and Resource Allocation in Heterogeneous multi-RAT Networks,Mhd Saria Allahham;Alaa Awad Abdellatif;Naram Mhaisen;Amr Mohamed;Aiman Erbad;Mohsen Guizani,"The rapid production of mobile devices along with the wireless applications boom is continuing to evolve daily. This motivates the exploitation of wireless spectrum using multiple Radio Access Technologies (multi-RAT) and developing innovative network selection techniques to cope with such intensive demand while improving Quality of Service (QoS). Thus, we propose a distributed framework for dynamic network selection at the edge level, and resource allocation at the Radio Access Network (RAN) level, while taking into consideration diverse applications' characteristics. In particular, our framework employs a deep Multi-Agent Reinforcement Learning (DMARL) algorithm, that aims to maximize the edge nodes' quality of experience while extending the battery lifetime of the nodes and leveraging adaptive compression schemes. Indeed, our framework enables data transfer from the network's edge nodes, with multi-RAT capabilities, to the cloud in a cost and energy-efficient manner, while maintaining QoS requirements of different supported applications. Our results depict that our solution outperforms state-of-the-art techniques of network selection in terms of energy consumption, latency, and cost. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10308
A Dynamic Model of a Skydiver With Validation in Wind Tunnel and Free Fall,Anna Clarke;Per-Olof Gutman,"An innovative approach of gaining insight into motor skills involved in human body flight is proposed. The key idea is the creation of a model autonomous system capable of virtually performing skydiving maneuvers. A dynamic skydiver model and simulator is developed, comprising biomechanical, aerodynamic, and kinematic models, dynamic equations of motion, and a virtual reality environment. Limb relative orientations, and resulting inertial body angular position and velocity are measured in skydiving experiments in a vertical wind tunnel and in free fall. These experimental data are compared with corresponding simulation data to tune and verify the model for basic skydiving maneuvers. The model is further extended to reconstruct advanced aerial maneuvers, such as transitions between stable equilibria. The experimental data are used to estimate skydiver's conscious inputs as a function of time, via an Unscented Kalman Filter modified for this purpose. △ Less","16 February, 2022",https://arxiv.org/pdf/2202.10233
Applications of blockchain and artificial intelligence technologies for enabling prosumers in smart grids: A review,Weiqi Hua;Ying Chen;Meysam Qadrdan;Jing Jiang;Hongjian Sun;Jianzhong Wu,"Governments' net zero emission target aims at increasing the share of renewable energy sources as well as influencing the behaviours of consumers to support the cost-effective balancing of energy supply and demand. These will be achieved by the advanced information and control infrastructures of smart grids which allow the interoperability among various stakeholders. Under this circumstance, increasing number of consumers produce, store, and consume energy, giving them a new role of prosumers. The integration of prosumers and accommodation of incurred bidirectional flows of energy and information rely on two key factors: flexible structures of energy markets and intelligent operations of power systems. The blockchain and artificial intelligence (AI) are innovative technologies to fulfil these two factors, by which the blockchain provides decentralised trading platforms for energy markets and the AI supports the optimal operational control of power systems. This paper attempts to address how to incorporate the blockchain and AI in the smart grids for facilitating prosumers to participate in energy markets. To achieve this objective, first, this paper reviews how policy designs price carbon emissions caused by the fossil-fuel based generation so as to facilitate the integration of prosumers with renewable energy sources. Second, the potential structures of energy markets with the support of the blockchain technologies are discussed. Last, how to apply the AI for enhancing the state monitoring and decision making during the operations of power systems is introduced. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10098
Analysis of Digital Sovereignty and Identity: From Digitization to Digitalization,Kheng-Leong Tan;Chi-Hung Chi;Kwok-Yan Lam,"Advances in emerging technologies have accelerated digital transformation with the pervasive digitalization of the economy and society, driving innovations such as smart cities, industry 4.0 and FinTech. Unlike digitization, digitalization is a transformation to improve processes by leveraging digital technologies and digitized data. The cyberspace has evolved from a hardware internetworking infrastructure to the notion of a virtual environment, transforming how people, business and government interact and operate. Through this transformation, lots of personal data are captured which individuals have no ownership or control over, threatening their privacy. It is therefore necessary for the data owners to have control over the ownership, custody and utilization of their data and to protect one's digital assets and identity through proper data governance, cybersecurity control and privacy protection. This results in the notions of data sovereignty and digital sovereignty - two conceptually related terms, but different focuses. This paper first explains these two concepts in terms of their guiding principles, laws and regulations requirements, and analyse and discuss the technical challenges of implementing these requirements. Next, to understand the emerging trend shift in digital sovereignty towards individuals to take complete control of the security and privacy of their own digital assets, this paper conducts a systematic study and analysis of Self-Sovereign Identity, and discuss existing solutions and point out that an efficient key management system, scalability and interoperability of the solutions and well established standards are some of its challenges and open problems to wide deployments. △ Less","21 February, 2022",https://arxiv.org/pdf/2202.10069
LiDAR-guided Stereo Matching with a Spatial Consistency Constraint,Yongjun Zhang;Siyuan Zou;Xinyi Liu;Xu Huang;Yi Wan;Yongxiang Yao,"The complementary fusion of light detection and ranging (LiDAR) data and image data is a promising but challenging task for generating high-precision and high-density point clouds. This study proposes an innovative LiDAR-guided stereo matching approach called LiDAR-guided stereo matching (LGSM), which considers the spatial consistency represented by continuous disparity or depth changes in the homogeneous region of an image. The LGSM first detects the homogeneous pixels of each LiDAR projection point based on their color or intensity similarity. Next, we propose a riverbed enhancement function to optimize the cost volume of the LiDAR projection points and their homogeneous pixels to improve the matching robustness. Our formulation expands the constraint scopes of sparse LiDAR projection points with the guidance of image information to optimize the cost volume of pixels as much as possible. We applied LGSM to semi-global matching and AD-Census on both simulated and real datasets. When the percentage of LiDAR points in the simulated datasets was 0.16%, the matching accuracy of our method achieved a subpixel level, while that of the original stereo matching algorithm was 3.4 pixels. The experimental results show that LGSM is suitable for indoor, street, aerial, and satellite image datasets and provides good transferability across semi-global matching and AD-Census. Furthermore, the qualitative and quantitative evaluations demonstrate that LGSM is superior to two state-of-the-art optimizing cost volume methods, especially in reducing mismatches in difficult matching areas and refining the boundaries of objects. △ Less","24 February, 2022",https://arxiv.org/pdf/2202.09953
Multi-objective optimization: basic approaches and moving beyond them through flexible skyline queries,Giovanni Lupi,"The area of scientific research that deals with the simultaneous optimization of several (possibly conflicting) criteria is named multi-objective optimization. The ability to efficiently filter and extract interesting data out of large datasets is one of the key tasks in modern database systems. This paper provides a general overview of the most common approaches employed to handle the problem in the field of databases, and describes a novel framework named flexible skylines. After analyzing the main differences between single and multi-optimization problems, I will discuss the three main basic approaches used to handle multi-optimization problems: lexicographic approach, top-k queries and skylines. Each methodology will be discussed, analyzing the pros, the range of applicability and the main issues, which motivate the need to introduce the flexible skylines innovative framework. A review of this approach will show its superiority with respect to the basic approaches, as well as the capability to overcome the majority of their drawbacks. △ Less","20 February, 2022",https://arxiv.org/pdf/2202.09857
Velocity Obstacle Based Risk-Bounded Motion Planning for Stochastic Multi-Agent Systems,Xiaoxue Zhang;Jun Ma;Zilong Cheng;Masayoshi Tomizuka;Tong Heng Lee,"In this paper, we present an innovative risk-bounded motion planning methodology for stochastic multi-agent systems. For this methodology, the disturbance, noise, and model uncertainty are considered; and a velocity obstacle method is utilized to formulate the collision-avoidance constraints in the velocity space. With the exploitation of geometric information of static obstacles and velocity obstacles, a distributed optimization problem with probabilistic chance constraints is formulated for the stochastic multi-agent system. Consequently, collision-free trajectories are generated under a prescribed collision risk bound. Due to the existence of probabilistic and disjunctive constraints, the distributed chance-constrained optimization problem is reformulated as a mixed-integer program by introducing the binary variable to improve computational efficiency. This approach thus renders it possible to execute the motion planning task in the velocity space instead of the position space, which leads to smoother collision-free trajectories for multi-agent systems and higher computational efficiency. Moreover, the risk of potential collisions is bounded with this robust motion planning methodology. To validate the effectiveness of the methodology, different scenarios for multiple agents are investigated, and the simulation results clearly show that the proposed approach can generate high-quality trajectories under a predefined collision risk bound and avoid potential collisions effectively in the velocity space. △ Less","20 February, 2022",https://arxiv.org/pdf/2202.09748
Dynamic Transaction Storage Strategies for a Sustainable Blockchain,Xiongfei Zhao;Yain-Whar Si,"As the core technology behind Bitcoin, Blockchain's decentralized, tamper-proof, and traceable features make it the preferred platform for organizational innovation. In current Bitcoin, block reward is halved every four years, and transaction fees are expected to become the majority of miner revenues around 2140. When transaction fee dominates mining rewards, strategic deviations such as Selfish Mining, Undercutting, and Mining Gap could threaten the integrity and security of the Blockchain. This paper proposes a set of Dynamic Transaction Storage (DTS) strategies for maintaining a sustainable Blockchain under the transaction-fee regime. We demonstrate that block incentive volatility can be reduced through systematic simulation by applying DTS strategies and avoiding strategic deviations. With DTS, public Blockchains such as Bitcoin become sustainable when the mining reward is solely based on the transaction fee. △ Less","18 February, 2022",https://arxiv.org/pdf/2202.09510
Prepare your video for streaming with Segue,Melissa Licciardello;Lukas Humbel;Fabian Rohr;Maximilian Grüner;Ankit Singla,"We identify new opportunities in video streaming, involving the joint consideration of offline video chunking and online rate adaptation. Due to a video's complexity varying over time, certain parts are more likely to cause performance impairments during playback with a particular rate adaptation algorithm. To address such an issue, we propose Segue, which carefully uses variable-length video segments, and augment specific segments with additional bitrate tracks. The key novelty of our approach is in making such decisions based on the video's time-varying complexity and the expected rate adaptation behavior over time. We propose and implement several methods for such adaptation-aware chunking. Our results show that Segue substantially reduces rebuffering and quality fluctuations, while maintaining video quality delivered; Segue improves QoE by 9% on average, and by 22% in low-bandwidth conditions. Finally, we view our problem framing as a first step in a new thread on algorithmic and design innovation in video streaming, and leave the reader with several interesting open questions. △ Less","12 April, 2022",https://arxiv.org/pdf/2202.09112
"Machine learning models and facial regions videos for estimating heart rate: a review on Patents, Datasets and Literature",Tiago Palma Pagano;Lucas Lemos Ortega;Victor Rocha Santos;Yasmin da Silva Bonfim;José Vinícius Dantas Paranhos;Paulo Henrique Miranda Sá;Lian Filipe Santana Nascimento;Ingrid Winkler;Erick Giovani Sperandio Nascimento,"Estimating heart rate is important for monitoring users in various situations. Estimates based on facial videos are increasingly being researched because it makes it possible to monitor cardiac information in a non-invasive way and because the devices are simpler, requiring only cameras that capture the user's face. From these videos of the user's face, machine learning is able to estimate heart rate. This study investigates the benefits and challenges of using machine learning models to estimate heart rate from facial videos, through patents, datasets, and articles review. We searched Derwent Innovation, IEEE Xplore, Scopus, and Web of Science knowledge bases and identified 7 patent filings, 11 datasets, and 20 articles on heart rate, photoplethysmography, or electrocardiogram data. In terms of patents, we note the advantages of inventions related to heart rate estimation, as described by the authors. In terms of datasets, we discovered that most of them are for academic purposes and with different signs and annotations that allow coverage for subjects other than heartbeat estimation. In terms of articles, we discovered techniques, such as extracting regions of interest for heart rate reading and using Video Magnification for small motion extraction, and models such as EVM-CNN and VGG-16, that extract the observed individual's heart rate, the best regions of interest for signal extraction and ways to process them. △ Less","17 February, 2022",https://arxiv.org/pdf/2202.08913
Distance learning as innovation technology of school geographical education,Myroslav Syvyi;Ordenbek Mazbayev;Olga Varakuta;Natalia Panteleeva;Olga Bondarenko,"The article substantiates the necessity of using innovative technologies in the process of studying and teaching geographical disciplines at secondary schools. Particular attention is paid to distance learning as a pedagogical innovation, its theoretical aspects and the ways of its introduction into the educational process. The relevance of using distance learning at the New Ukrainian School is proved. Its advantages and disadvantages are revealed. The examples of some forms of distance learning that will contribute to geographical competence development according to European requirements are provided. The article particularly focuses on the Massive Open Online Courses, modern websites, virtual portals of individual teachers, LearningApps.org portal, and Moodle. △ Less","17 February, 2022",https://arxiv.org/pdf/2202.08697
Where Is My Training Bottleneck? Hidden Trade-Offs in Deep Learning Preprocessing Pipelines,Alexander Isenko;Ruben Mayer;Jeffrey Jedele;Hans-Arno Jacobsen,"Preprocessing pipelines in deep learning aim to provide sufficient data throughput to keep the training processes busy. Maximizing resource utilization is becoming more challenging as the throughput of training processes increases with hardware innovations (e.g., faster GPUs, TPUs, and inter-connects) and advanced parallelization techniques that yield better scalability. At the same time, the amount of training data needed in order to train increasingly complex models is growing. As a consequence of this development, data preprocessing and provisioning are becoming a severe bottleneck in end-to-end deep learning pipelines. In this paper, we provide an in-depth analysis of data preprocessing pipelines from four different machine learning domains. We introduce a new perspective on efficiently preparing datasets for end-to-end deep learning pipelines and extract individual trade-offs to optimize throughput, preprocessing time, and storage consumption. Additionally, we provide an open-source profiling library that can automatically decide on a suitable preprocessing strategy to maximize throughput. By applying our generated insights to real-world use-cases, we obtain an increased throughput of 3x to 13x compared to an untuned system while keeping the pipeline functionally identical. These findings show the enormous potential of data pipeline tuning. △ Less","25 March, 2022",https://arxiv.org/pdf/2202.08679
An overview of deep learning in medical imaging,Imran Ul Haq,"Machine learning (ML) has seen enormous consideration during the most recent decade. This success started in 2012 when an ML model accomplished a remarkable triumph in the ImageNet Classification, the world's most famous competition for computer vision. This model was a kind of convolutional neural system (CNN) called deep learning (DL). Since then, researchers have started to participate efficiently in DL's fastest developing area of research. These days, DL systems are cutting-edge ML systems spanning a broad range of disciplines, from human language processing to video analysis, and commonly used in the scholarly world and enterprise sector. Recent advances can bring tremendous improvement to the medical field. Improved and innovative methods for data processing, image analysis and can significantly improve the diagnostic technologies and medicinal services gradually. A quick review of current developments with relevant problems in the field of DL used for medical imaging has been provided. The primary purposes of the review are four: (i) provide a brief prolog to DL by discussing different DL models, (ii) review of the DL usage for medical image analysis (classification, detection, segmentation, and registration), (iii) review seven main application fields of DL in medical imaging, (iv) give an initial stage to those keen on adding to the research area about DL in clinical imaging by providing links of some useful informative assets, such as freely available DL codes, public datasets Table 7, and medical imaging competition sources Table 8 and end our survey by outlining distinct continuous difficulties, lessons learned and future of DL in the field of medical science. △ Less","17 February, 2022",https://arxiv.org/pdf/2202.08546
A Creativity Survey of Parallel Sorting Algorithm,Tianyi Yu;Wei Li,"Sorting is one of the most fundamental problems in the field of computer science. With the rapid development of manycore processors, it shows great importance to design efficient parallel sort algorithm on manycore architecture. This paper studies the parallel memory sorting method on modern hardware, and summarizes its research status and progress. Classify the research problems, research methods and measurement methods of the target papers and references. In the end, we summarize all the researches and list the directions not researched and innovative places. Keywords: Sorting Algorithm, Parallel Algorithm, Parallel Optimization, CPU, GPU, Memory Hierarchy △ Less","17 February, 2022",https://arxiv.org/pdf/2202.08463
EdgeFormer: A Parameter-Efficient Transformer for On-Device Seq2seq Generation,Tao Ge;Si-Qing Chen;Furu Wei,"We introduce EdgeFormer -- a parameter-efficient Transformer for on-device seq2seq generation under the strict computation and memory constraints. Compared with the previous parameter-efficient Transformers, EdgeFormer applies two novel principles for cost-effective parameterization, allowing it to perform better given the same parameter budget; moreover, EdgeFormer is further enhanced by layer adaptation innovation that is proposed for improving the network with shared layers. Extensive experiments show EdgeFormer can effectively outperform previous parameter-efficient Transformer baselines and achieve competitive results under both the computation and memory constraints. Given the promising results, we release EdgeLM -- the pretrained version of EdgeFormer, which is the first publicly available pretrained on-device seq2seq model that can be easily fine-tuned for seq2seq tasks with strong results, facilitating on-device seq2seq generation in practice. △ Less","29 December, 2022",https://arxiv.org/pdf/2202.07959
Enhancing Healthcare System Using Blockchain Smart Contracts,Shashank Joshi;Arhan Choudhury;Ojas Saraswat,"The concept of blockchain has emerged as an effective solution for data-sensitive domains, such as healthcare, financial services, etc., due to its various attributes like immutability, non-repudiation, and availability. Thus, implementation of this technology in various domains rose exponentially; one of such fields is the healthcare supply chain. Managing healthcare supply chain processes effectively is very crucial for the healthcare system. Despite various innovations in the method of treatment methodologies, the healthcare supply chain management system is not up to the mark and lacks efficiency. The traditional healthcare supply chain system is time-consuming and lacks the work synergy among the various stakeholders of the supply chain. Thus, In this paper, we propose a framework based on blockchain smart contracts and decentralized storage to connect all the supply chain stakeholders. Smart contracts in the framework enforce and depict various interactions and transactions among the stakeholders, thus helping to automate these processes, promote transparency, improve efficiency, and minimize service time. The preliminary results show that the proposed framework is more efficient, secure, and economically feasible. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.07591
Texture Aware Autoencoder Pre-training And Pairwise Learning Refinement For Improved Iris Recognition,Manashi Chakraborty;Aritri Chakraborty;Prabir Kumar Biswas;Pabitra Mitra,"This paper presents a texture aware end-to-end trainable iris recognition system, specifically designed for datasets like iris having limited training data. We build upon our previous stagewise learning framework with certain key optimization and architectural innovations. First, we pretrain a Stage-1 encoder network with an unsupervised autoencoder learning optimized with an additional data relation loss on top of usual reconstruction loss. The data relation loss enables learning better texture representation which is pivotal for a texture rich dataset such as iris. Robustness of Stage-1 feature representation is further enhanced with an auxiliary denoising task. Such pre-training proves beneficial for effectively training deep networks on data constrained iris datasets. Next, in Stage-2 supervised refinement, we design a pairwise learning architecture for an end-to-end trainable iris recognition system. The pairwise learning includes the task of iris matching inside the training pipeline itself and results in significant improvement in recognition performance compared to usual offline matching. We validate our model across three publicly available iris datasets and the proposed model consistently outperforms both traditional and deep learning baselines for both Within-Dataset and Cross-Dataset configurations △ Less","15 February, 2022",https://arxiv.org/pdf/2202.07499
DualConv: Dual Convolutional Kernels for Lightweight Deep Neural Networks,Jiachen Zhong;Junying Chen;Ajmal Mian,"CNN architectures are generally heavy on memory and computational requirements which makes them infeasible for embedded systems with limited hardware resources. We propose dual convolutional kernels (DualConv) for constructing lightweight deep neural networks. DualConv combines 3\times3 and 1\times1 convolutional kernels to process the same input feature map channels simultaneously and exploits the group convolution technique to efficiently arrange convolutional filters. DualConv can be employed in any CNN model such as VGG-16 and ResNet-50 for image classification, YOLO and R-CNN for object detection, or FCN for semantic segmentation. In this paper, we extensively test DualConv for classification since these network architectures form the backbones for many other tasks. We also test DualConv for image detection on YOLO-V3. Experimental results show that, combined with our structural innovations, DualConv significantly reduces the computational cost and number of parameters of deep neural networks while surprisingly achieving slightly higher accuracy than the original models in some cases. We use DualConv to further reduce the number of parameters of the lightweight MobileNetV2 by 54% with only 0.68% drop in accuracy on CIFAR-100 dataset. When the number of parameters is not an issue, DualConv increases the accuracy of MobileNetV1 by 4.11% on the same dataset. Furthermore, DualConv significantly improves the YOLO-V3 object detection speed and improves its accuracy by 4.4% on PASCAL VOC dataset. △ Less","15 February, 2022",https://arxiv.org/pdf/2202.07481
An Edge-Cloud based Reference Architecture to support cognitive solutions in the Process Industry,Antonio Salis;Angelo Marguglio;Gabriele De Luca;Sergio Gusmeroli;Silvia Razzetti,"Process Industry (PI e.g. Steel, Metals, Chemicals, Cement, Asphalt, Ceramics) is one of the leading sectors of the world economy, characterized however by intense environmental impact, and very high energy consumption. In spite of a traditional low innovation pace in PI, in the recent years a strong push at worldwide level towards the dual objective of improving the efficiency of plants and the quality of products, significantly reducing the consumption of electricity and CO2 emissions has taken momentum. Digital Technologies (namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies) are enabling drivers for a Twin Digital-Green Transition, as well as foundations for human centric, safe, comfortable and inclusive work places. Currently, digital sensors in plants produce a large amount of data which in most cases constitutes just a potential and not a real value for Process Industry, often locked-in in close proprietary systems and seldomly exploited. Digital technologies, with process modelling-simulation via digital twins, can build a bridge between the physical and the virtual worlds, bringing innovation with great efficiency and drastic reduction of waste. In accordance with the guidelines of Industrie 4.0, the H2020 funded CAPRI project aims to innovate the process industry, with a modular and scalable Reference Architecture, based on open source software, which can be implemented both in brownfield and greenfield scenarios. The ability to distribute processing between the edge, where the data is created, and the cloud, where the greatest computational resources are available, facilitates the development of integrated digital solutions with cognitive capabilities. The reference architecture is being validated in the asphalt, steel & pharma pilot plants, allowing the development of integrated planning solutions, with scheduling and control of the plants. △ Less","6 July, 2022",https://arxiv.org/pdf/2202.06622
Exploring the Research Landscape of Pakistan: A Data-driven Analysis of Scopus Indexed Scientific Literature,Muhammad Bilal Ibrahim;Saeed-Ul Hassan,"Scientific contribution and research performance of a university, research group, or institute needs to be evaluated all the more with the increasing volume and fast-developing disciplines of research. The need of the time is to develop tools for strategic planning and management that will help research bodies to rank and benchmark themselves against international standards. This will enable them to invest appropriately in research areas of promising strength and gain maximally from them, thus fulfilling the ultimate purpose of positive impact of research on society. Our tool is capable of rating and benchmarking universities as well as research institutes in not only the major disciplines and sub-disciplines, but at the finest level of niche areas of science and technology too with the help of its innovative bibliometric indicators based on publications and citation analysis. The tool accepts inputs like discipline/subject area, university, and country and time window while using data retrieved from bibliography database, Scopus, to benchmark and rate the research body under consideration. We have evaluated that there are many niche subject areas in which small or medium size universities are performing good in comparison to the large universities. Most of these subject areas are of more significance in the present day and the future. Government and funds allocating bodies should take this factor in account that investing the right money at right place will give far better results than we they are having right now △ Less","13 February, 2022",https://arxiv.org/pdf/2202.06421
RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation,Zhen Li;Guenevere;Chen;Chen Chen;Yayi Zou;Shouhuai Xu,"Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style manipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called Robust coding style Patterns Generation (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manipulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8% and 41.0% of the success rate of targeted and untargeted attacks on average. △ Less","12 February, 2022",https://arxiv.org/pdf/2202.06043
End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking,Arpit Bansal;Avi Schwarzschild;Eitan Borgnia;Zeyad Emam;Furong Huang;Micah Goldblum;Tom Goldstein,"Machine learning systems perform well on pattern matching tasks, but their ability to perform algorithmic or logical reasoning is not well understood. One important reasoning capability is algorithmic extrapolation, in which models trained only on small/simple reasoning problems can synthesize complex strategies for large/complex problems at test time. Algorithmic extrapolation can be achieved through recurrent systems, which can be iterated many times to solve difficult reasoning problems. We observe that this approach fails to scale to highly complex problems because behavior degenerates when many iterations are applied -- an issue we refer to as ""overthinking."" We propose a recall architecture that keeps an explicit copy of the problem instance in memory so that it cannot be forgotten. We also employ a progressive training routine that prevents the model from learning behaviors that are specific to iteration number and instead pushes it to learn behaviors that can be repeated indefinitely. These innovations prevent the overthinking problem, and enable recurrent systems to solve extremely hard extrapolation tasks. △ Less","14 October, 2022",https://arxiv.org/pdf/2202.05826
Zero Shot Learning for Predicting Energy Usage of Buildings in Sustainable Design,Arun Zachariah;Praveen Rao;Brian Corn;Dominique Davison,"The 2030 Challenge is aimed at making all new buildings and major renovations carbon neutral by 2030. One of the potential solutions to meet this challenge is through innovative sustainable design strategies. For developing such strategies it is important to understand how the various building factors contribute to energy usage of a building, right at design time. The growth of artificial intelligence (AI) in recent years provides an unprecedented opportunity to advance sustainable design by learning complex relationships between building factors from available data. However, rich training datasets are needed for AI-based solutions to achieve good prediction accuracy. Unfortunately, obtaining training datasets are time consuming and expensive in many real-world applications. Motivated by these reasons, we address the problem of accurately predicting the energy usage of new or unknown building types, i.e., those building types that do not have any training data. We propose a novel approach based on zero-shot learning (ZSL) to solve this problem. Our approach uses side information from building energy modeling experts to predict the closest building types for a given new/unknown building type. We then obtain the predicted energy usage for the k-closest building types using the models learned during training and combine the predicted values using a weighted averaging function. We evaluated our approach on a dataset containing five building types generated using BuildSimHub, a popular platform for building energy modeling. Our approach achieved better average accuracy than a regression model (based on XGBoost) trained on the entire dataset of known building types. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.05206
DeepCENT: Prediction of Censored Event Time via Deep Learning,Jong-Hyeon Jeong;Yichen Jia,"With the rapid advances of deep learning, many computational methods have been developed to analyze nonlinear and complex right censored data via deep learning approaches. However, the majority of the methods focus on predicting survival function or hazard function rather than predicting a single valued time to an event. In this paper, we propose a novel method, DeepCENT, to directly predict the individual time to an event. It utilizes the deep learning framework with an innovative loss function that combines the mean square error and the concordance index. Most importantly, DeepCENT can handle competing risks, where one type of event precludes the other types of events from being observed. The validity and advantage of DeepCENT were evaluated using simulation studies and illustrated with three publicly available cancer data sets. △ Less","7 February, 2022",https://arxiv.org/pdf/2202.05155
"Work-from-home and its implication for project management, resilience and innovation -- a global survey on software companies",Anh Nguyen-Duc;Dron Khanna;Des Greer;Xiaofeng Wang;Luciana Martinez Zaina;Gerardo Matturro;Jorge Melegati;Eduardo Guerra;Giang Huong Le;Petri Kettunen;Sami Hyrynsalmi;Henry Edison;Afonso Sales;Didzis Rutitis;Kai-Kristian Kemell;Abdullah Aldaeej;Tommi Mikkonen;Juan Garbajosa;Pekka Abrahamsson,"[Context] The COVID-19 pandemic has had a disruptive impact on how people work and collaborate across all global economic sectors, including the software business. While remote working is not new for software engineers, forced Work-from-home situations to come with both constraints, limitations, and opportunities for individuals, software teams and software companies. As the ""new normal"" for working might be based on the current state of Work From Home (WFH), it is useful to understand what has happened and learn from that. [Objective] The goal of this study is to gain insights on how their WFH environment impacts software projects and software companies. We are also interested in understanding if the impact differs between software startups and established companies. [Method] We conducted a global-scale, cross-sectional survey during spring and summer 2021. Our results are based on quantitative and qualitative analysis of 297 valid responses. [Results] We observed a mixed perception of the impact of WFH on software project management, resilience, and innovation. Certain patterns on WFH, control and coordination mechanisms and collaborative tools are observed globally. We find that team, agility and leadership are the three most important factors for achieving resilience during the pandemic. Although startups do not perceive the impact of WFH differently, there is a difference between engineers who work in a small team context and those who work in a large team context. [Conclusion] The result suggests a contingency approach in studying and improving WFH practices and environment in the future software industry. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.04950
Networks and Identity Drive Geographic Properties of the Diffusion of Linguistic Innovation,Aparna Ananthasubramaniam;David Jurgens;Daniel M. Romero,"Adoption of cultural innovation (e.g., music, beliefs, language) is often geographically correlated, with adopters largely residing within the boundaries of relatively few well-studied, socially significant areas. These cultural regions are often hypothesized to be the result of either (i) identity performance driving the adoption of cultural innovation, or (ii) homophily in the networks underlying diffusion. In this study, we show that demographic identity and network topology are both required to model the diffusion of innovation, as they play complementary roles in producing its spatial properties. We develop an agent-based model of cultural adoption, and validate geographic patterns of transmission in our model against a novel dataset of innovative words that we identify from a 10% sample of Twitter. Using our model, we are able to directly compare a combined network + identity model of diffusion to simulated network-only and identity-only counterfactuals -- allowing us to test the separate and combined roles of network and identity. While social scientists often treat either network or identity as the core social structure in modeling culture change, we show that key geographic properties of diffusion actually depend on both factors as each one influences different mechanisms of diffusion. Specifically, the network principally drives spread among urban counties via weak-tie diffusion, while identity plays a disproportionate role in transmission among rural counties via strong-tie diffusion. Diffusion between urban and rural areas, a key component in innovation diffusing nationally, requires both network and identity. Our work suggests that models must integrate both factors in order to understand and reproduce the adoption of innovation. △ Less","10 February, 2022",https://arxiv.org/pdf/2202.04842
GenAD: General Representations of Multivariate Time Seriesfor Anomaly Detection,Xiaolei Hua;Lin Zhu;Shenglin Zhang;Zeyan Li;Su Wang;Dong Zhou;Shuo Wang;Chao Deng,"The reliability of wireless base stations in China Mobile is of vital importance, because the cell phone users are connected to the stations and the behaviors of the stations are directly related to user experience. Although the monitoring of the station behaviors can be realized by anomaly detection on multivariate time series, due to complex correlations and various temporal patterns of multivariate series in large-scale stations, building a general unsupervised anomaly detection model with a higher F1-score remains a challenging task. In this paper, we propose a General representation of multivariate time series for Anomaly Detection(GenAD). First, we pre-train a general model on large-scale wireless base stations with self-supervision, which can be easily transferred to a specific station anomaly detection with a small amount of training data. Second, we employ Multi-Correlation Attention and Time-Series Attention to represent the correlations and temporal patterns of the stations. With the above innovations, GenAD increases F1-score by total 9% on real-world datasets in China Mobile, while the performance does not significantly degrade on public datasets with only 10% of the training data. △ Less","8 February, 2022",https://arxiv.org/pdf/2202.04250
A Novel Image Descriptor with Aggregated Semantic Skeleton Representation for Long-term Visual Place Recognition,Nie Jiwei;Feng Joe-Mei;Xue Dingyu;Pan Feng;Liu Wei;Hu Jun;Cheng Shuai,"In a Simultaneous Localization and Mapping (SLAM) system, a loop-closure can eliminate accumulated errors, which is accomplished by Visual Place Recognition (VPR), a task that retrieves the current scene from a set of pre-stored sequential images through matching specific scene-descriptors. In urban scenes, the appearance variation caused by seasons and illumination has brought great challenges to the robustness of scene descriptors. Semantic segmentation images can not only deliver the shape information of objects but also their categories and spatial relations that will not be affected by the appearance variation of the scene. Innovated by the Vector of Locally Aggregated Descriptor (VLAD), in this paper, we propose a novel image descriptor with aggregated semantic skeleton representation (SSR), dubbed SSR-VLAD, for the VPR under drastic appearance-variation of environments. The SSR-VLAD of one image aggregates the semantic skeleton features of each category and encodes the spatial-temporal distribution information of the image semantic information. We conduct a series of experiments on three public datasets of challenging urban scenes. Compared with four state-of-the-art VPR methods- CoHOG, NetVLAD, LOST-X, and Region-VLAD, VPR by matching SSR-VLAD outperforms those methods and maintains competitive real-time performance at the same time. △ Less","8 February, 2022",https://arxiv.org/pdf/2202.03677
MINER: Multiscale Implicit Neural Representations,Vishwanath Saragadam;Jasper Tan;Guha Balakrishnan;Richard G. Baraniuk;Ashok Veeraraghavan,"We introduce a new neural signal model designed for efficient high-resolution representation of large-scale signals. The key innovation in our multiscale implicit neural representation (MINER) is an internal representation via a Laplacian pyramid, which provides a sparse multiscale decomposition of the signal that captures orthogonal parts of the signal across scales. We leverage the advantages of the Laplacian pyramid by representing small disjoint patches of the pyramid at each scale with a small MLP. This enables the capacity of the network to adaptively increase from coarse to fine scales, and only represent parts of the signal with strong signal energy. The parameters of each MLP are optimized from coarse-to-fine scale which results in faster approximations at coarser scales, thereby ultimately an extremely fast training process. We apply MINER to a range of large-scale signal representation tasks, including gigapixel images and very large point clouds, and demonstrate that it requires fewer than 25% of the parameters, 33% of the memory footprint, and 10% of the computation time of competing techniques such as ACORN to reach the same representation accuracy. △ Less","17 July, 2022",https://arxiv.org/pdf/2202.03532
Evaluation Methods and Measures for Causal Learning Algorithms,Lu Cheng;Ruocheng Guo;Raha Moraffah;Paras Sheth;K. Selcuk Candan;Huan Liu,"The convenient access to copious multi-faceted data has encouraged machine learning researchers to reconsider correlation-based learning and embrace the opportunity of causality-based learning, i.e., causal machine learning (causal learning). Recent years have therefore witnessed great effort in developing causal learning algorithms aiming to help AI achieve human-level intelligence. Due to the lack-of ground-truth data, one of the biggest challenges in current causal learning research is algorithm evaluations. This largely impedes the cross-pollination of AI and causal inference, and hinders the two fields to benefit from the advances of the other. To bridge from conventional causal inference (i.e., based on statistical methods) to causal learning with big data (i.e., the intersection of causal inference and machine learning), in this survey, we review commonly-used datasets, evaluation methods, and measures for causal learning using an evaluation pipeline similar to conventional machine learning. We focus on the two fundamental causal-inference tasks and causality-aware machine learning tasks. Limitations of current evaluation procedures are also discussed. We then examine popular causal inference tools/packages and conclude with primary challenges and opportunities for benchmarking causal learning algorithms in the era of big data. The survey seeks to bring to the forefront the urgency of developing publicly available benchmarks and consensus-building standards for causal learning evaluation with observational data. In doing so, we hope to broaden the discussions and facilitate collaboration to advance the innovation and application of causal learning. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02896
Aligning Eyes between Humans and Deep Neural Network through Interactive Attention Alignment,Yuyang Gao;Tong Sun;Liang Zhao;Sungsoo Hong,"While Deep Neural Networks (DNNs) are deriving the major innovations in nearly every field through their powerful automation, we are also witnessing the peril behind automation as a form of bias, such as automated racism, gender bias, and adversarial bias. As the societal impact of DNNs grows, finding an effective way to steer DNNs to align their behavior with the human mental model has become indispensable in realizing fair and accountable models. We propose a novel framework of Interactive Attention Alignment (IAA) that aims at realizing human-steerable Deep Neural Networks (DNNs). IAA leverages DNN model explanation method as an interactive medium that humans can use to unveil the cases of biased model attention and directly adjust the attention. In improving the DNN using human-generated adjusted attention, we introduce GRADIA, a novel computational pipeline that jointly maximizes attention quality and prediction accuracy. We evaluated IAA framework in Study 1 and GRADIA in Study 2 in a gender classification problem. Study 1 found applying IAA can significantly improve the perceived quality of model attention from human eyes. In Study 2, we found using GRADIA can (1) significantly improve the perceived quality of model attention and (2) significantly improve model performance in scenarios where the training samples are limited. We present implications for future interactive user interfaces design towards human-alignable AI. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02838
"Human rights, democracy, and the rule of law assurance framework for AI systems: A proposal",David Leslie;Christopher Burr;Mhairi Aitken;Michael Katell;Morgan Briggs;Cami Rincon,"Following on from the publication of its Feasibility Study in December 2020, the Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and its subgroups initiated efforts to formulate and draft its Possible Elements of a Legal Framework on Artificial Intelligence, based on the Council of Europe's standards on human rights, democracy, and the rule of law. This document was ultimately adopted by the CAHAI plenary in December 2021. To support this effort, The Alan Turing Institute undertook a programme of research that explored the governance processes and practical tools needed to operationalise the integration of human right due diligence with the assurance of trustworthy AI innovation practices. The resulting framework was completed and submitted to the Council of Europe in September 2021. It presents an end-to-end approach to the assurance of AI project lifecycles that integrates context-based risk analysis and appropriate stakeholder engagement with comprehensive impact assessment, and transparent risk management, impact mitigation, and innovation assurance practices. Taken together, these interlocking processes constitute a Human Rights, Democracy and the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the procedural requirements for principles-based human rights due diligence with the governance mechanisms needed to set up technical and socio-technical guardrails for responsible and trustworthy AI innovation practices. Its purpose is to provide an accessible and user-friendly set of mechanisms for facilitating compliance with a binding legal framework on artificial intelligence, based on the Council of Europe's standards on human rights, democracy, and the rule of law, and to ensure that AI innovation projects are carried out with appropriate levels of public accountability, transparency, and democratic governance. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02776
3D Map Reconstruction of an Orchard using an Angle-Aware Covering Control Strategy,Martina Mammarella;Cesare Donati;Takumi Shimizu;Masaya Suenaga;Lorenzo Comba;Alessandro Biglia;Kuniaki Uto;Takeshi Hatanaka;Paolo Gay;Fabrizio Dabbene,"In the last years, unmanned aerial vehicles are becoming a reality in the context of precision agriculture, mainly for monitoring, patrolling and remote sensing tasks, but also for 3D map reconstruction. In this paper, we present an innovative approach where a fleet of unmanned aerial vehicles is exploited to perform remote sensing tasks over an apple orchard for reconstructing a 3D map of the field, formulating the covering control problem to combine the position of a monitoring target and the viewing angle. Moreover, the objective function of the controller is defined by an importance index, which has been computed from a multi-spectral map of the field, obtained by a preliminary flight, using a semantic interpretation scheme based on a convolutional neural network. This objective function is then updated according to the history of the past coverage states, thus allowing the drones to take situation-adaptive actions. The effectiveness of the proposed covering control strategy has been validated through simulations on a Robot Operating System. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02758
Energy-Aware Edge Association for Cluster-based Personalized Federated Learning,Y. Li;X. Qin;H. Chen;K. Han;P. Zhang,"Federated Learning (FL) over wireless network enables data-conscious services by leveraging the ubiquitous intelligence at network edge for privacy-preserving model training. As the proliferation of context-aware services, the diversified personal preferences causes disagreeing conditional distributions among user data, which leads to poor inference performance. In this sense, clustered federated learning is proposed to group user devices with similar preference and provide each cluster with a personalized model. This calls for innovative design in edge association that involves user clustering and also resource management optimization. We formulate an accuracy-cost trade-off optimization problem by jointly considering model accuracy, communication resource allocation and energy consumption. To comply with parameter encryption techniques in FL, we propose an iterative solution procedure which employs deep reinforcement learning based approach at cloud server for edge association. The reward function consists of minimized energy consumption at each base station and the averaged model accuracy of all users. Under our proposed solution, multiple edge base station are fully exploited to realize cost efficient personalized federated learning without any prior knowledge on model parameters. Simulation results show that our proposed strategy outperforms existing strategies in achieving accurate learning at low energy cost. △ Less","6 February, 2022",https://arxiv.org/pdf/2202.02727
A Survey on Automated Sarcasm Detection on Twitter,Bleau Moores;Vijay Mago,"Automatic sarcasm detection is a growing field in computer science. Short text messages are increasingly used for communication, especially over social media platforms such as Twitter. Due to insufficient or missing context, unidentified sarcasm in these messages can invert the meaning of a statement, leading to confusion and communication failures. This paper covers a variety of current methods used for sarcasm detection, including detection by context, posting history and machine learning models. Additionally, a shift towards deep learning methods is observable, likely due to the benefit of using a model with induced instead of discrete features combined with the innovation of transformers. △ Less","5 February, 2022",https://arxiv.org/pdf/2202.02516
Application of Machine Learning-Based Pattern Recognition in IoT Devices: Review,Zachary Menter;Wei Tee;Rushit Dave,"The Internet of things (IoT) is a rapidly advancing area of technology that has quickly become more widespread in recent years. With greater numbers of everyday objects being connected to the Internet, many different innovations have been presented to make our everyday lives more straightforward. Pattern recognition is extremely prevalent in IoT devices because of the many applications and benefits that can come from it. A multitude of studies has been conducted with the intention of improving speed and accuracy, decreasing complexity, and reducing the overall required processing power of pattern recognition algorithms in IoT devices. After reviewing the applications of different machine learning algorithms, results vary from case to case, but a general conclusion can be drawn that the optimal machine learning-based pattern recognition algorithms to be used with IoT devices are support vector machine, k-nearest neighbor, and random forest. △ Less","9 January, 2022",https://arxiv.org/pdf/2202.02456
"Astronomical data organization, management and access in Scientific Data Lakes",Y. G. Grange;V. N. Pandey;X. Espinal;R. Di Maria;A. P. Millar,"The data volumes stored in telescope archives is constantly increasing due to the development and improvements in the instrumentation. Often the archives need to be stored over a distributed storage architecture, provided by independent compute centres. Such a distributed data archive requires overarching data management orchestration. Such orchestration comprises of tools which handle data storage and cataloguing, and steering transfers integrating different storage systems and protocols, while being aware of data policies and locality. In addition, it needs a common Authorisation and Authentication Infrastructure (AAI) layer which is perceived as a single entity by end users and provides transparent data access. The scientific domain of particle physics also uses complex and distributed data management systems. The experiments at the Large Hadron Collider\,(LHC) accelerator at CERN generate several hundred petabytes of data per year. This data is globally distributed to partner sites and users using national compute facilities. Several innovative tools were developed to successfully address the distributed computing challenges in the context of the Worldwide LHC Computing Grid (WLCG). The work being carried out in the ESCAPE project and in the Data Infrastructure for Open Science (DIOS) work package is to prototype a Scientific Data Lake using the tools developed in the context of the WLCG, harnessing different physics scientific disciplines addressing FAIR standards and Open Data. We present how the Scientific Data Lake prototype is applied to address astronomical data use cases. We introduce the software stack and also discuss some of the differences between the domains. △ Less","3 February, 2022",https://arxiv.org/pdf/2202.01828
Understanding Digital Government Transformation,Mamdouh Alenezi,"In today's era of innovation of technological progression, digitalisation has not only transformed individual lives but also has a prominent influence on business activities. The world is surviving in a global yet complex technological progression that not only changes the lives of civilians but is also transforming the public, private, and academic spheres of life. This research focuses on the digitalisation of governments, their challenges, and success factors. It is found that government faces difficulties in formulating strategies, proper planning, execution strategies, and a lack of organised information and expertise. However, success can be achieved by working on capabilities of the future workforce, creating leaders for tomorrow, generating digitalisation capabilities, and bringing a purpose-driven digitalisation before digital government transformation. Overall, the study's findings suggest that digital government transformation creates value, enhances relations, improves service delivery, grows economy, pushes economic activities, enhances citizen engagement, increases the policy implementation and their efficiency, and affects business growth positively. △ Less","10 January, 2022",https://arxiv.org/pdf/2202.01797
Fast Online Video Super-Resolution with Deformable Attention Pyramid,Dario Fuoli;Martin Danelljan;Radu Timofte;Luc Van Gool,"Video super-resolution (VSR) has many applications that pose strict causal, real-time, and latency constraints, including video streaming and TV. We address the VSR problem under these settings, which poses additional important challenges since information from future frames is unavailable. Importantly, designing efficient, yet effective frame alignment and fusion modules remain central problems. In this work, we propose a recurrent VSR architecture based on a deformable attention pyramid (DAP). Our DAP aligns and integrates information from the recurrent state into the current frame prediction. To circumvent the computational cost of traditional attention-based methods, we only attend to a limited number of spatial locations, which are dynamically predicted by the DAP. Comprehensive experiments and analysis of the proposed key innovations show the effectiveness of our approach. We significantly reduce processing time and computational complexity in comparison to state-of-the-art methods, while maintaining a high performance. We surpass state-of-the-art method EDVR-M on two standard benchmarks with a speed-up of over 3\times. △ Less","6 April, 2022",https://arxiv.org/pdf/2202.01731
Separating Rule Discovery and Global Solution Composition in a Learning Classifier System,Michael Heider;Helena Stegherr;Jonathan Wurth;Roman Sraj;Jörg Hähner,"While utilization of digital agents to support crucial decision making is increasing, trust in suggestions made by these agents is hard to achieve. However, it is essential to profit from their application, resulting in a need for explanations for both the decision making process and the model. For many systems, such as common black-box models, achieving at least some explainability requires complex post-processing, while other systems profit from being, to a reasonable extent, inherently interpretable. We propose a rule-based learning system specifically conceptualised and, thus, especially suited for these scenarios. Its models are inherently transparent and easily interpretable by design. One key innovation of our system is that the rules' conditions and which rules compose a problem's solution are evolved separately. We utilise independent rule fitnesses which allows users to specifically tailor their model structure to fit the given requirements for explainability. △ Less","18 April, 2022",https://arxiv.org/pdf/2202.01677
Generation Alpha: Understanding the Next Cohort of University Students,Rushan Ziatdinov;Juanee Cilliers,"Technology is changing at a blistering pace and is impacting on the way we consider knowledge as a free commodity, along with the ability to apply skills, concepts and understandings. Technology is aiding the way the world is evolving, and its contributions to education are not an exemption. While technology advances will play a crucial part in future teaching-learning approaches, educators will also be challenged by the next higher-education generation, the Alpha Generation. This entrepreneurial generation will embrace the innovation, progressiveness, and advancement with the expectation that one in two Generation Alphas will obtain a university degree. In anticipating the educational challenges and opportunities of the future higher education environment, this research reflected on Generation Alpha as the next cohort of university students, considering their preferred learning styles, perceptions and expectations relating to education. The research employed a theoretical analysis based on the characteristics and traits that distinguishes Generation Alpha, spearheaded by technology advances. The empirical investigation considered three independent studies that were previous conducted by authors from Slovakia, Hungary, Australia, and Turkey to understand the challenges and opportunities pertaining to Generation Alpha. The research identified the influence of social media, social connections, high levels of perceptions and the Generation Alpha's ability to interpret information as strengths to consider in future teaching-learning approaches in the higher education environment. This research concluded with recommendations on how universities could be transformed to ensure a better learning experience for Generation Alpha students, aligned with their characteristics, perceptions and expectations. △ Less","3 February, 2022",https://arxiv.org/pdf/2202.01422
"Understanding O-RAN: Architecture, Interfaces, Algorithms, Security, and Research Challenges",Michele Polese;Leonardo Bonati;Salvatore D'Oro;Stefano Basagni;Tommaso Melodia,"The Open Radio Access Network (RAN) and its embodiment through the O-RAN Alliance specifications are poised to revolutionize the telecom ecosystem. O-RAN promotes virtualized RANs where disaggregated components are connected via open interfaces and optimized by intelligent controllers. The result is a new paradigm for the RAN design, deployment, and operations: O-RAN networks can be built with multi-vendor, interoperable components, and can be programmatically optimized through a centralized abstraction layer and data-driven closed-loop control. Therefore, understanding O-RAN, its architecture, its interfaces, and workflows is key for researchers and practitioners in the wireless community. In this article, we present the first detailed tutorial on O-RAN. We also discuss the main research challenges and review early research results. We provide a deep dive of the O-RAN specifications, describing its architecture, design principles, and the O-RAN interfaces. We then describe how the O-RAN RAN Intelligent Controllers (RICs) can be used to effectively control and manage 3GPP-defined RANs. Based on this, we discuss innovations and challenges of O-RAN networks, including the Artificial Intelligence (AI) and Machine Learning (ML) workflows that the architecture and interfaces enable, security and standardization issues. Finally, we review experimental research platforms that can be used to design and test O-RAN networks, along with recent research results, and we outline future directions for O-RAN development. △ Less","1 August, 2022",https://arxiv.org/pdf/2202.01032
ColloSSL: Collaborative Self-Supervised Learning for Human Activity Recognition,Yash Jain;Chi Ian Tang;Chulhong Min;Fahim Kawsar;Akhil Mathur,"A major bottleneck in training robust Human-Activity Recognition models (HAR) is the need for large-scale labeled sensor datasets. Because labeling large amounts of sensor data is an expensive task, unsupervised and semi-supervised learning techniques have emerged that can learn good features from the data without requiring any labels. In this paper, we extend this line of research and present a novel technique called Collaborative Self-Supervised Learning (ColloSSL) which leverages unlabeled data collected from multiple devices worn by a user to learn high-quality features of the data. A key insight that underpins the design of ColloSSL is that unlabeled sensor datasets simultaneously captured by multiple devices can be viewed as natural transformations of each other, and leveraged to generate a supervisory signal for representation learning. We present three technical innovations to extend conventional self-supervised learning algorithms to a multi-device setting: a Device Selection approach which selects positive and negative devices to enable contrastive learning, a Contrastive Sampling algorithm which samples positive and negative examples in a multi-device setting, and a loss function called Multi-view Contrastive Loss which extends standard contrastive loss to a multi-device setting. Our experimental results on three multi-device datasets show that ColloSSL outperforms both fully-supervised and semi-supervised learning techniques in majority of the experiment settings, resulting in an absolute increase of upto 7.9% in F_1 score compared to the best performing baselines. We also show that ColloSSL outperforms the fully-supervised methods in a low-data regime, by just using one-tenth of the available labeled data in the best case. △ Less","1 February, 2022",https://arxiv.org/pdf/2202.00758
"Changes in co-publication patterns among China, the European Union (28) and the United States of America, 2016-2021",Caroline S. Wagner;Xiaojing Cai,"The COVID-19 global pandemic starting in January 2020 disrupted international collaborations in scholarly exchange, reducing mobility and connections across the globe. An examination of Web of Science-indexed publications from China, the European Union-28 and the United States of America shows a drop in publications numbers coming from the EU-28 and the United States in 2021. Importantly, cooperation between China and the United States drops without a corresponding drop between China and the EU-28. Moreover, the drop in China-USA cooperation can be seen beginning in 2019, before the pandemic, at a time when political tensions around science, technology, and innovation arose, with the United States claiming that China was violating intellectual property norms. The patterns suggest that political tensions, more than the pandemic, influenced the drop in China-USA cooperation. △ Less","11 February, 2022",https://arxiv.org/pdf/2202.00453
Deepfake pornography as a male gaze on fan culture,Inna Suvorova,"This essay shows the impact of deepfake technology on fan culture. The innovative technology provided the male audience with an instrument to express its ideas and plots. Which subsequently led to the rise of deepfake pornography. It is often seen as a part of celebrity studies; however, the essay shows that it could also be considered a type of fanfic and a product of participatory culture, sharing community origin, exploitation by commercial companies and deep sexualisation. These two branches of fanfic evolution can be connected via the genre of machinima pornography. Textual fanfics are mainly created by females for females, depicting males; otherwise, deepfake pornography and machinima are made by males and for males targeting females. △ Less","1 February, 2022",https://arxiv.org/pdf/2202.00374
Deep Learning Macroeconomics,Rafael R. S. Guimaraes,"Limited datasets and complex nonlinear relationships are among the challenges that may emerge when applying econometrics to macroeconomic problems. This research proposes deep learning as an approach to transfer learning in the former case and to map relationships between variables in the latter case. Although macroeconomists already apply transfer learning when assuming a given a priori distribution in a Bayesian context, estimating a structural VAR with signal restriction and calibrating parameters based on results observed in other models, to name a few examples, advance in a more systematic transfer learning strategy in applied macroeconomics is the innovation we are introducing. We explore the proposed strategy empirically, showing that data from different but related domains, a type of transfer learning, helps identify the business cycle phases when there is no business cycle dating committee and to quick estimate a economic-based output gap. Next, since deep learning methods are a way of learning representations, those that are formed by the composition of multiple non-linear transformations, to yield more abstract representations, we apply deep learning for mapping low-frequency from high-frequency variables. The results obtained show the suitability of deep learning models applied to macroeconomic problems. First, models learned to classify United States business cycles correctly. Then, applying transfer learning, they were able to identify the business cycles of out-of-sample Brazilian and European data. Along the same lines, the models learned to estimate the output gap based on the U.S. data and obtained good performance when faced with Brazilian data. Additionally, deep learning proved adequate for mapping low-frequency variables from high-frequency data to interpolate, distribute, and extrapolate time series by related series. △ Less","31 January, 2022",https://arxiv.org/pdf/2201.13380
"An end-to-end deep learning approach for extracting stochastic dynamical systems with α
-stable Lévy noise",Cheng Fang;Yubin Lu;Ting Gao;Jinqiao Duan,"Recently, extracting data-driven governing laws of dynamical systems through deep learning frameworks has gained a lot of attention in various fields. Moreover, a growing amount of research work tends to transfer deterministic dynamical systems to stochastic dynamical systems, especially those driven by non-Gaussian multiplicative noise. However, lots of log-likelihood based algorithms that work well for Gaussian cases cannot be directly extended to non-Gaussian scenarios which could have high error and low convergence issues. In this work, we overcome some of these challenges and identify stochastic dynamical systems driven by α-stable Lévy noise from only random pairwise data. Our innovations include: (1) designing a deep learning approach to learn both drift and diffusion coefficients for Lévy induced noise with α across all values, (2) learning complex multiplicative noise without restrictions on small noise intensity, (3) proposing an end-to-end complete framework for stochastic systems identification under a general input data assumption, that is, α-stable random variable. Finally, numerical experiments and comparisons with the non-local Kramers-Moyal formulas with moment generating function confirm the effectiveness of our method. △ Less","2 July, 2022",https://arxiv.org/pdf/2201.13114
A Flexible IAB Architecture for Beyond 5G Network,Shashi Ranjan;Pranav Jha;Abhay Karandikar;Prasanna Chaporkar,"IAB is an innovative wireless backhaul solution to provide cost-efficient deployment of small cells for successful 5G adoption. Besides, IAB can utilize the same spectrum for access and backhaul purposes. The 3GPP standardized IAB in Release 16 and would incorporate a few enhancements in the upcoming releases. The 3GPP IAB architecture, however, suffers from some limitations, such as it does not support mobile relays or dual-connectivity. This article presents a novel IAB architecture that addresses these limitations and is transparent to legacy operations of the 5G system. The architecture also supports multi-RAT coexistence where access and backhaul may belong to different RATs. These factors (and many others) enable operators to capitalize on the architecture for deploying IAB anywhere in a plug-and-play manner. We also show the merits of the architecture by evaluating its capacity and mobility robustness compared to the 3GPP architecture. Simulation results corroborate our design approach. Owing its robust design, the architecture can contend for standardization in B5G system. △ Less","31 January, 2022",https://arxiv.org/pdf/2201.13029
Potential destination discovery for low predictability individuals based on knowledge graph,Guilong Li;Yixian Chen;Qionghua Liao;Zhaocheng He,"Travelers may travel to locations they have never visited, which we call potential destinations of them. Especially under a very limited observation, travelers tend to show random movement patterns and usually have a large number of potential destinations, which make them difficult to handle for mobility prediction (e.g., destination prediction). In this paper, we develop a new knowledge graph-based framework (PDPFKG) for potential destination discovery of low predictability travelers by considering trip association relationships between them. We first construct a trip knowledge graph (TKG) to model the trip scenario by entities (e.g., travelers, destinations and time information) and their relationships, in which we introduce the concept of private relationship for complexity reduction. Then a modified knowledge graph embedding algorithm is implemented to optimize the overall graph representation. Based on the trip knowledge graph embedding model (TKGEM), the possible ranking of individuals' unobserved destinations to be chosen in the future can be obtained by calculating triples' distance. Empirically. PDPFKG is tested using an anonymous vehicular dataset from 138 intersections equipped with video-based vehicle detection systems in Xuancheng city, China. The results show that (i) the proposed method significantly outperforms baseline methods, and (ii) the results show strong consistency with traveler behavior in choosing potential destinations. Finally, we provide a comprehensive discussion of the innovative points of the methodology. △ Less","19 September, 2022",https://arxiv.org/pdf/2201.12845
Recognition of Implicit Geographic Movement in Text,Scott Pezanowski;Prasenjit Mitra,"Analyzing the geographic movement of humans, animals, and other phenomena is a growing field of research. This research has benefited urban planning, logistics, animal migration understanding, and much more. Typically, the movement is captured as precise geographic coordinates and time stamps with Global Positioning Systems (GPS). Although some research uses computational techniques to take advantage of implicit movement in descriptions of route directions, hiking paths, and historical exploration routes, innovation would accelerate with a large and diverse corpus. We created a corpus of sentences labeled as describing geographic movement or not and including the type of entity moving. Creating this corpus proved difficult without any comparable corpora to start with, high human labeling costs, and since movement can at times be interpreted differently. To overcome these challenges, we developed an iterative process employing hand labeling, crowd voting for confirmation, and machine learning to predict more labels. By merging advances in word embeddings with traditional machine learning models and model ensembling, prediction accuracy is at an acceptable level to produce a large silver-standard corpus despite the small gold-standard corpus training set. Our corpus will likely benefit computational processing of geography in text and spatial cognition, in addition to detection of movement. △ Less","30 January, 2022",https://arxiv.org/pdf/2201.12799
Flashlight: Enabling Innovation in Tools for Machine Learning,Jacob Kahn;Vineel Pratap;Tatiana Likhomanenko;Qiantong Xu;Awni Hannun;Jeff Cai;Paden Tomasello;Ann Lee;Edouard Grave;Gilad Avidov;Benoit Steiner;Vitaliy Liptchinsky;Gabriel Synnaeve;Ronan Collobert,"As the computational requirements for machine learning systems and the size and complexity of machine learning frameworks increases, essential framework innovation has become challenging. While computational needs have driven recent compiler, networking, and hardware advancements, utilization of those advancements by machine learning tools is occurring at a slower pace. This is in part due to the difficulties involved in prototyping new computational paradigms with existing frameworks. Large frameworks prioritize machine learning researchers and practitioners as end users and pay comparatively little attention to systems researchers who can push frameworks forward -- we argue that both are equally important stakeholders. We introduce Flashlight, an open-source library built to spur innovation in machine learning tools and systems by prioritizing open, modular, customizable internals and state-of-the-art, research-ready models and training setups across a variety of domains. Flashlight allows systems researchers to rapidly prototype and experiment with novel ideas in machine learning computation and has low overhead, competing with and often outperforming other popular machine learning frameworks. We see Flashlight as a tool enabling research that can benefit widely used libraries downstream and bring machine learning and systems researchers closer together. Flashlight is available at https://github.com/flashlight/flashlight . △ Less","22 June, 2022",https://arxiv.org/pdf/2201.12465
Competitive Algorithms and Reinforcement Learning for NOMA in IoT Networks,Zoubeir Mlika;Soumaya Cherkaoui,"This paper studies the problem of massive Internet of things (IoT) access in beyond fifth generation (B5G) networks using non-orthogonal multiple access (NOMA) technique. The problem involves massive IoT devices grouping and power allocation in order to respect the low latency as well as the limited operating energy of the IoT devices. The considered objective function, maximizing the number of successfully received IoT packets, is different from the classical sum-rate-related objective functions. The problem is first divided into multiple NOMA grouping subproblems. Then, using competitive analysis, an efficient online competitive algorithm (CA) is proposed to solve each subproblem. Next, to solve the power allocation problem, we propose a new reinforcement learning (RL) framework in which a RL agent learns to use the CA as a black box and combines the obtained solutions to each subproblem to determine the power allocation for each NOMA group. Our simulations results reveal that the proposed innovative RL framework outperforms deep-Q-learning methods and is close-to-optimal. △ Less","28 January, 2022",https://arxiv.org/pdf/2201.12395
Detecting Owner-member Relationship with Graph Convolution Network in Fisheye Camera System,Zizhang Wu;Jason Wang;Tianhao Xu;Fan Wang,"The owner-member relationship between wheels and vehicles contributes significantly to the 3D perception of vehicles, especially in embedded environments. However, to leverage this relationship we must face two major challenges: i) Traditional IoU-based heuristics have difficulty handling occluded traffic congestion scenarios. ii) The effectiveness and applicability of the solution in a vehicle-mounted system is difficult. To address these issues, we propose an innovative relationship prediction method, DeepWORD, by designing a graph convolutional network (GCN). Specifically, to improve the information richness, we use feature maps with local correlation as input to the nodes. Subsequently, we introduce a graph attention network (GAT) to dynamically correct the a priori estimation bias. Finally, we designed a dataset as a large-scale benchmark which has annotated owner-member relationship, called WORD. In the experiments we learned that the proposed method achieved state-of-the-art accuracy and real-time performance. The WORD dataset is made publicly available at https://github.com/NamespaceMain/ownermember-relationship-dataset. △ Less","28 January, 2022",https://arxiv.org/pdf/2201.12099
Human-centered mechanism design with Democratic AI,Raphael Koster;Jan Balaguer;Andrea Tacchetti;Ari Weinstein;Tina Zhu;Oliver Hauser;Duncan Williams;Lucy Campbell-Gillingham;Phoebe Thacker;Matthew Botvinick;Christopher Summerfield,"Building artificial intelligence (AI) that aligns with human values is an unsolved problem. Here, we developed a human-in-the-loop research pipeline called Democratic AI, in which reinforcement learning is used to design a social mechanism that humans prefer by majority. A large group of humans played an online investment game that involved deciding whether to keep a monetary endowment or to share it with others for collective benefit. Shared revenue was returned to players under two different redistribution mechanisms, one designed by the AI and the other by humans. The AI discovered a mechanism that redressed initial wealth imbalance, sanctioned free riders, and successfully won the majority vote. By optimizing for human preferences, Democratic AI may be a promising method for value-aligned policy innovation. △ Less","27 January, 2022",https://arxiv.org/pdf/2201.11441
"Bifrost: Secure, Scalable and Efficient File Sharing System Using Dual Deduplication",Hadi Sehat;Elena Pagnin;Daniel E. Lucani,"We consider the problem of sharing sensitive or valuable files across users while partially relying on a common, untrusted third-party, e.g., a Cloud Storage Provider (CSP). Although users can rely on a secure peer-to-peer (P2P) channel for file sharing, this introduces potential delay on the data transfer and requires the sender to remain active and connected while the transfer process occurs. Instead of using the P2P channel for the entire file, users can upload information about the file on a common CSP and share only the essential information that enables the receiver to download and recover the original file. This paper introduces Bifrost, an innovative file sharing system inspired by recent results on dual deduplication. Bifrost achieves the desired functionality and simultaneously guarantees that (1) the CSP can efficiently compress outsourced data; (2) the secure P2P channel is used only to transmit short, but crucial information; (3) users can check for data integrity, i.e., detect if the CSP alters the outsourced data; and (4) only the sender (data owner) and the intended receiver can access the file after sharing, i.e., the cloud or no malicious adversary can infer useful information about the shared file. We analyze compression and bandwidth performance using a proof-of-concept implementation. Our experiments show that secure file sharing can be achieved by sending only 650 bits on the P2P channel, irrespective of file size, while the CSP that aids the sharing can enjoy a compression rate of 86.9 %. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.10839
Making Internal Software Startups Work: How to Innovate Like a Venture Builder?,Anastasiia Tkalich;Nils Brede Moe;Rasmus Ulfsnes,"With the increasing availability of software usage and the influence of the Lean Startup mindset, more and more companies choose to innovate through internal software startups. Such startups aim at developing new business models while at the same time relying on the resources from the companies where they emerged. The evidence from both researchers and practitioners indicates that driving internal software startups is challenging. This paper seeks to address this problem by asking the research question: how to make internal software startups work? We examined a unique case of a venture builder, a company primarily focusing on building internal software startups and launching them as independent companies. Applying a Grounded Theory approach, we analyzed data on four internal software startups at the case company. The results suggest that four strategies drive the examined startups, cultural, financial, personnel, and venture arrangement. We interpret our results by drawing on earlier literature on intrapreneurship and internal ventures and suggest four recommendations to succeed with internal software startups 1 establish shared arenas for the employees, 2 provide necessary resources for experimentation in the initial phase and increase them incrementally, 3 build up in-house product management competence through coaching, and 4 harness employees own motivation to develop their own ideas. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.10811
An Automated Question-Answering Framework Based on Evolution Algorithm,Sinan Tan;Hui Xue;Qiyu Ren;Huaping Liu;Jing Bai,"Building a deep learning model for a Question-Answering (QA) task requires a lot of human effort, it may need several months to carefully tune various model architectures and find a best one. It's even harder to find different excellent models for multiple datasets. Recent works show that the best model structure is related to the dataset used, and one single model cannot adapt to all tasks. In this paper, we propose an automated Question-Answering framework, which could automatically adjust network architecture for multiple datasets. Our framework is based on an innovative evolution algorithm, which is stable and suitable for multiple dataset scenario. The evolution algorithm for search combine prior knowledge into initial population and use a performance estimator to avoid inefficient mutation by predicting the performance of candidate model architecture. The prior knowledge used in initial population could improve the final result of the evolution algorithm. The performance estimator could quickly filter out models with bad performance in population as the number of trials increases, to speed up the convergence. Our framework achieves 78.9 EM and 86.1 F1 on SQuAD 1.1, 69.9 EM and 72.5 F1 on SQuAD 2.0. On NewsQA dataset, the found model achieves 47.0 EM and 62.9 F1. △ Less","26 January, 2022",https://arxiv.org/pdf/2201.10797
Initial Investigations Towards Non-invasive Monitoring of Chronic Wound Healing Using Deep Learning and Ultrasound Imaging,Maja Schlereth;Daniel Stromer;Yash Mantri;Jason Tsujimoto;Katharina Breininger;Andreas Maier;Caesar Anderson;Pranav S. Garimella;Jesse V. Jokerst,"Chronic wounds including diabetic and arterial/venous insufficiency injuries have become a major burden for healthcare systems worldwide. Demographic changes suggest that wound care will play an even bigger role in the coming decades. Predicting and monitoring response to therapy in wound care is currently largely based on visual inspection with little information on the underlying tissue. Thus, there is an urgent unmet need for innovative approaches that facilitate personalized diagnostics and treatments at the point-of-care. It has been recently shown that ultrasound imaging can monitor response to therapy in wound care, but this work required onerous manual image annotations. In this study, we present initial results of a deep learning-based automatic segmentation of cross-sectional wound size in ultrasound images and identify requirements and challenges for future research on this application. Evaluation of the segmentation results underscores the potential of the proposed deep learning approach to complement non-invasive imaging with Dice scores of 0.34 (U-Net, FCN) and 0.27 (ResNet-U-Net) but also highlights the need for improving robustness further. We conclude that deep learning-supported analysis of non-invasive ultrasound images is a promising area of research to automatically extract cross-sectional wound size and depth information with potential value in monitoring response to therapy. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.10511
Interspecies Collaboration in the Design of Visual Identity: A Case Study,Bojan Jerbić;Marko Švaco;Filip Šuligoj;Bojan Šekoranja;Josip Vidaković;Marija Turković;Mihaela Lekić;Borjan Pavlek;Bruno Bolfan;Davor Bruketa;Dina Borošić;Barbara Bušić,"Design usually relies on human ingenuity, but the past decade has seen the field's toolbox expanding to Artificial Intelligence (AI) and its adjacent methods, making room for hybrid, algorithmic creations. This article aims to substantiate the concept of interspecies collaboration - that of natural and artificial intelligence - in the active co-creation of a visual identity, describing a case study of the Regional Center of Excellence for Robotic Technology (CRTA) which opened on 750 m2 in June 2021 within the University of Zagreb. The visual identity of the Center comprises three separately devised elements, each representative of the human-AI relationship and embedded in the institution's logo. Firstly, the letter ""C"" (from the CRTA acronym) was created using a Gaussian Mixture Model (GMM) applied to (x, y) coordinates that the neurosurgical robot RONNA, CRTA's flagship innovation, generated when hand-guided by a human operator. The second shape of the letter ""C"" was created by using the same (x, y) coordinates as inputs fed to a neural network whose goal was to output letters in a novel, AI-generated typography. A basic feedforward back-propagating neural network with two hidden layers was chosen for the task. The final and third design element was a trajectory the robot RONNA makes when performing a brain biopsy. As CRTA embodies a state-of-the-art venue for robotics research, the 'interspecies' approach was used to accentuate the importance of human-robot collaboration which is at the core of the newly opened Center, illustrating the potential of reciprocal and amicable relationship that humans could have with technology. △ Less","25 January, 2022",https://arxiv.org/pdf/2201.10393
The Vehicle Trajectory Prediction Based on ResNet and EfficientNet Model,Ruyi Qu;Shukai Huang;Jiexuan Zhou;ChenXi Fan;ZhiYuan Yan,"At present, a major challenge for the application of automatic driving technology is the accurate prediction of vehicle trajectory. With the vigorous development of computer technology and the emergence of convolution depth neural network, the accuracy of prediction results has been improved. But, the depth, width of the network and image resolution are still important reasons that restrict the accuracy of the model and the prediction results. The main innovation of this paper is the combination of RESNET network and efficient net network, which not only greatly increases the network depth, but also comprehensively changes the choice of network width and image resolution, so as to make the model performance better, but also save computing resources as much as possible. The experimental results also show that our proposed model obtains the optimal prediction results. Specifically, the loss value of our method is separately 4 less and 2.1 less than that of resnet and efficientnet method. △ Less","24 January, 2022",https://arxiv.org/pdf/2201.09973
"Low hardware consumption, resolution-configurable Gray code oscillator time-to-digital converters implemented in 16nm, 20nm and 28nm FPGAs",Yu Wang;Wujun Xie;Haochang Chen;David Day-Uei Li,"This paper presents a low hardware consumption, resolution-configurable, automatically calibrating Gray code oscillator time-to-digital converter (TDC) in Xilinx 16nm UltraScale+, 20nm UltraScale and 28nm Virtex-7 field-programmable gate arrays (FPGAs). The proposed TDC has several innovations: 1) a sampling matrix to improve the resolution. 2) a virtual bin calibration method (VBCM) to realize resolution configuration and automatic calibration. 3) a hardware implementation of the VBCM in standard FPGA devices. We implemented and evaluated a 16-channel TDC system in all three FPGAs. The UltraScale+ version achieved the best resolution (least significant bit, LSB) of 20.97 ps with 0.09 LSB averaged peak-peak differential linearity (DNLpk-pk). The UltraScale and Virtex-7 versions achieved the best resolutions of 36.01 ps with 0.10 LSB averaged DNLpk-pk and 34.84 ps with 0.08 LSB averaged DNLpk-pk, respectively. △ Less","12 May, 2022",https://arxiv.org/pdf/2201.09670
Towards Remote Robotic Competitions: An Internet-Connected Task Board and Dashboard,Peter So;Jonas Wittmann;Patrick Ruhkamp;Andriy Sarabakha;Sami Haddadin,"In this work we present a platform to assess robot platform skills using an internet-of-things (IoT) task board device to aggregate performances across remote sites. We demonstrate a concept for a modular, scale-able device and web dashboard enabling remote competitions as an alternative to in-person robot competitions. We share data from nine robot platforms located across four continents in three manipulation task categories of object localization, object insertion, and component disassembly through an organized international robot competition - the Robothon Grand Challenge. This paper discusses the design of an electronic task board, the strategies implemented by the top-performing teams and compares their results with a benchmark solution to the presented task board. Through this platform, we demonstrate fully remote, online competitions can generate innovative robotic solutions and tested a tool for measuring remote performances. Using the open-sourced task board code and design files, the reader can reproduce the benchmark solution or configure the platform for their own use case and share their results transparently without transporting their robot platform. △ Less","24 January, 2022",https://arxiv.org/pdf/2201.09565
Actor-Critic-Based Learning for Zero-touch Joint Resource and Energy Control in Network Slicing,Farhad Rezazadeh;Hatim Chergui;Loizos Christofi;Christos Verikoukis,"To harness the full potential of beyond 5G (B5G) communication systems, zero-touch network slicing (NS) is viewed as a promising fully-automated management and orchestration (MANO) system. This paper proposes a novel knowledge plane (KP)-based MANO framework that accommodates and exploits recent NS technologies and is termed KB5G. Specifically, we deliberate on algorithmic innovation and artificial intelligence (AI) in KB5G. We invoke a continuous model-free deep reinforcement learning (DRL) method to minimize energy consumption and virtual network function (VNF) instantiation cost. We present a novel Actor-Critic-based NS approach to stabilize learning called, twin-delayed double-Q soft Actor-Critic (TDSAC) method. The TDSAC enables central unit (CU) to learn continuously to accumulate the knowledge learned in the past to minimize future NS costs. Finally, we present numerical results to showcase the gain of the adopted approach and verify the performance in terms of energy consumption, CPU utilization, and time efficiency. △ Less","22 January, 2022",https://arxiv.org/pdf/2201.08985
BBA-net: A bi-branch attention network for crowd counting,Yi Hou;Chengyang Li;Fan Yang;Cong Ma;Liping Zhu;Yuan Li;Huizhu Jia;Xiaodong Xie,"In the field of crowd counting, the current mainstream CNN-based regression methods simply extract the density information of pedestrians without finding the position of each person. This makes the output of the network often found to contain incorrect responses, which may erroneously estimate the total number and not conducive to the interpretation of the algorithm. To this end, we propose a Bi-Branch Attention Network (BBA-NET) for crowd counting, which has three innovation points. i) A two-branch architecture is used to estimate the density information and location information separately. ii) Attention mechanism is used to facilitate feature extraction, which can reduce false responses. iii) A new density map generation method combining geometric adaptation and Voronoi split is introduced. Our method can integrate the pedestrian's head and body information to enhance the feature expression ability of the density map. Extensive experiments performed on two public datasets show that our method achieves a lower crowd counting error compared to other state-of-the-art methods. △ Less","22 January, 2022",https://arxiv.org/pdf/2201.08983
Exploring Fusion Strategies for Accurate RGBT Visual Object Tracking,Zhangyong Tang;Tianyang Xu;Hui Li;Xiao-Jun Wu;Xuefeng Zhu;Josef Kittler,"We address the problem of multi-modal object tracking in video and explore various options of fusing the complementary information conveyed by the visible (RGB) and thermal infrared (TIR) modalities including pixel-level, feature-level and decision-level fusion. Specifically, different from the existing methods, paradigm of image fusion task is heeded for fusion at pixel level. Feature-level fusion is fulfilled by attention mechanism with channels excited optionally. Besides, at decision level, a novel fusion strategy is put forward since an effortless averaging configuration has shown the superiority. The effectiveness of the proposed decision-level fusion strategy owes to a number of innovative contributions, including a dynamic weighting of the RGB and TIR contributions and a linear template update operation. A variant of which produced the winning tracker at the Visual Object Tracking Challenge 2020 (VOT-RGBT2020). The concurrent exploration of innovative pixel- and feature-level fusion strategies highlights the advantages of the proposed decision-level fusion method. Extensive experimental results on three challenging datasets, \textit{i.e.}, GTOT, VOT-RGBT2019, and VOT-RGBT2020, demonstrate the effectiveness and robustness of the proposed method, compared to the state-of-the-art approaches. Code will be shared at \textcolor{blue}{\emph{https://github.com/Zhangyong-Tang/DFAT}. △ Less","21 January, 2022",https://arxiv.org/pdf/2201.08673
Dynamic Deep Convolutional Candlestick Learner,Jun-Hao Chen;Yun-Cheng Tsai,"Candlestick pattern is one of the most fundamental and valuable graphical tools in financial trading that supports traders observing the current market conditions to make the proper decision. This task has a long history and, most of the time, human experts. Recently, efforts have been made to automatically classify these patterns with the deep learning models. The GAF-CNN model is a well-suited way to imitate how human traders capture the candlestick pattern by integrating spatial features visually. However, with the great potential of the GAF encoding, this classification task can be extended to a more complicated object detection level. This work presents an innovative integration of modern object detection techniques and GAF time-series encoding on candlestick pattern tasks. We make crucial modifications to the representative yet straightforward YOLO version 1 model based on our time-series encoding method and the property of such data type. Powered by the deep neural networks and the unique architectural design, the proposed model performs pretty well in candlestick classification and location recognition. The results show tremendous potential in applying modern object detection techniques on time-series tasks in a real-time manner. △ Less","21 January, 2022",https://arxiv.org/pdf/2201.08669
Hold On and Swipe: A Touch-Movement Based Continuous Authentication Schema based on Machine Learning,Rushit Dave;Naeem Seliya;Laura Pryor;Mounika Vanamala;Evelyn Sowells;Jacob mallet,"In recent years the amount of secure information being stored on mobile devices has grown exponentially. However, current security schemas for mobile devices such as physiological biometrics and passwords are not secure enough to protect this information. Behavioral biometrics have been heavily researched as a possible solution to this security deficiency for mobile devices. This study aims to contribute to this innovative research by evaluating the performance of a multimodal behavioral biometric based user authentication scheme using touch dynamics and phone movement. This study uses a fusion of two popular publicly available datasets the Hand Movement Orientation and Grasp dataset and the BioIdent dataset. This study evaluates our model performance using three common machine learning algorithms which are Random Forest Support Vector Machine and K-Nearest Neighbor reaching accuracy rates as high as 82% with each algorithm performing respectively for all success metrics reported. △ Less","21 January, 2022",https://arxiv.org/pdf/2201.08564
The Specialized High-Performance Network on Anton 3,Keun Sup Shim;Brian Greskamp;Brian Towles;Bruce Edwards;J. P. Grossman;David E. Shaw,"Molecular dynamics (MD) simulation, a computationally intensive method that provides invaluable insights into the behavior of biomolecules, typically requires large-scale parallelization. Implementation of fast parallel MD simulation demands both high bandwidth and low latency for inter-node communication, but in current semiconductor technology, neither of these properties is scaling as quickly as intra-node computational capacity. This disparity in scaling necessitates architectural innovations to maximize the utilization of computational units. For Anton 3, the latest in a family of highly successful special-purpose supercomputers designed for MD simulations, we thus designed and built a completely new specialized network as part of our ASIC. Tightly integrating this network with specialized computation pipelines enables Anton 3 to perform simulations orders of magnitude faster than any general-purpose supercomputer, and to outperform its predecessor, Anton 2 (the state of the art prior to Anton 3), by an order of magnitude. In this paper, we present the three key features of the network that contribute to the high performance of Anton 3. First, through architectural optimizations, the network achieves very low end-to-end inter-node communication latency for fine-grained messages, allowing for better overlap of computation and communication. Second, novel application-specific compression techniques reduce the size of most messages sent between nodes, thereby increasing effective inter-node bandwidth. Lastly, a new hardware synchronization primitive, called a network fence, supports fast fine-grained synchronization tailored to the data flow within a parallel MD application. These application-driven specializations to the network are critical for Anton 3's MD simulation performance advantage over all other machines. △ Less","20 January, 2022",https://arxiv.org/pdf/2201.08357
Data-Driven Innovation: What Is It,Jianxi Luo,"The future of innovation processes is anticipated to be more data-driven and empowered by the ubiquitous digitalization, increasing data accessibility and rapid advances in machine learning, artificial intelligence, and computing technologies. While the data-driven innovation (DDI) paradigm is emerging, it has yet been formally defined and theorized and often confused with several other data-related phenomena. This paper defines and crystalizes ""data-driven innovation"" as a formal innovation process paradigm, dissects its value creation, and distinguishes it from data-driven optimization (DDO), data-based innovation (DBI), and the traditional innovation processes that purely rely on human intelligence. With real-world examples and theoretical framing, I elucidate what DDI entails and how it addresses uncertainty and enhance creativity in the innovation process and present a process-based taxonomy of different data-driven innovation approaches. On this basis, I recommend the strategies and actions for innovators, companies, R&D organizations, and governments to enact data-driven innovation. △ Less","7 July, 2022",https://arxiv.org/pdf/2201.08184
Effective Anomaly Detection in Smart Home by Integrating Event Time Intervals,Chenxu Jiang;Chenglong Fu;Zhenyu Zhao;Xiaojiang Du;Yuede Ji,"Smart home IoT systems and devices are susceptible to attacks and malfunctions. As a result, users' concerns about their security and safety issues arise along with the prevalence of smart home deployments. In a smart home, various anomalies (such as fire or flooding) could happen, due to cyber attacks, device malfunctions, or human mistakes. These concerns motivate researchers to propose various anomaly detection approaches. Existing works on smart home anomaly detection focus on checking the sequence of IoT devices' events but leave out the temporal information of events. This limitation prevents them to detect anomalies that cause delay rather than missing/injecting events. To fill this gap, in this paper, we propose a novel anomaly detection method that takes the inter-event intervals into consideration. We propose an innovative metric to quantify the temporal similarity between two event sequences. We design a mechanism to learn the temporal patterns of event sequences of common daily activities. Delay-caused anomalies are detected by comparing the sequence with the learned patterns. We collect device events from a real-world testbed for training and testing. The experiment results show that our proposed method achieves accuracies of 93%, 88%, 89% for three daily activities. △ Less","19 January, 2022",https://arxiv.org/pdf/2201.07954
Towards deep observation: A systematic survey on artificial intelligence techniques to monitor fetus via Ultrasound Images,Mahmood Alzubaidi;Marco Agus;Khalid Alyafei;Khaled A Althelaya;Uzair Shah;Alaa Abd-Alrazaq;Mohammed Anbar;Michel Makhlouf;Mowafa Househ,"Developing innovative informatics approaches aimed to enhance fetal monitoring is a burgeoning field of study in reproductive medicine. Several reviews have been conducted regarding Artificial intelligence (AI) techniques to improve pregnancy outcomes. They are limited by focusing on specific data such as mother's care during pregnancy. This systematic survey aims to explore how artificial intelligence (AI) can assist with fetal growth monitoring via Ultrasound (US) image. We used eight medical and computer science bibliographic databases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM Library, Google Scholar, and the Web of Science. We retrieved studies published between 2010 to 2021. Data extracted from studies were synthesized using a narrative approach. Out of 1269 retrieved studies, we included 107 distinct studies from queries that were relevant to the topic in the survey. We found that 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound images (n=19). Classification is the most used method (n=42), followed by segmentation (n=31), classification integrated with segmentation (n=16) and other miscellaneous such as object-detection, regression and reinforcement learning (n=18). The most common areas within the pregnancy domain were the fetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen (n=10), and lastly the fetus face (n=10). In the most recent studies, deep learning techniques were primarily used (n=81), followed by machine learning (n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI techniques played a crucial role in predicting fetal diseases and identifying fetus anatomy structures during pregnancy. More research is required to validate this technology from a physician's perspective, such as pilot studies and randomized controlled trials on AI and its applications in a hospital setting. △ Less","25 August, 2022",https://arxiv.org/pdf/2201.07935
Problem examination for AI methods in product design,Philipp Rosenthal;Oliver Niggemann,"Artificial Intelligence (AI) has significant potential for product design: AI can check technical and non-technical constraints on products, it can support a quick design of new product variants and new AI methods may also support creativity. But currently product design and AI are separate communities fostering different terms and theories. This makes a mapping of AI approaches to product design needs difficult and prevents new solutions. As a solution, this paper first clarifies important terms and concepts for the interdisciplinary domain of AI methods in product design. A key contribution of this paper is a new classification of design problems using the four characteristics decomposability, inter-dependencies, innovation and creativity. Definitions of these concepts are given where they are lacking. Early mappings of these concepts to AI solutions are sketched and verified using design examples. The importance of creativity in product design and a corresponding gap in AI is pointed out for future research. △ Less","19 January, 2022",https://arxiv.org/pdf/2201.07642
RIS-Aware Indoor Network Planning: The Rennes Railway Station Case,Antonio Albanese;Guillermo Encinas-Lago;Vincenzo Sciancalepore;Xavier Costa-Pérez;Dinh-Thuy Phan-Huy;Stéphane Ros,"Future generations of wireless networks will offer newfangled performance via unprecedented solutions: the metasurface innovation will drive such a revolution by posing control onto the surrounding propagation environment, always portrayed as a tamper-proof black-box. The RIS technology, envisioned as the discrete version of a metasurface, can be dynamically configured to alter the propagation properties of the impinging signals by, e.g., steering the corresponding beams towards defined directions. This will unlock new application opportunities and deliver advanced end-user services. However, this fascinating solution comes at not negligible costs: RIS require ad-hoc design, deployment and management operations to be fully exploited. In this paper, we tackle the RIS placement problem from a theoretical viewpoint, showcasing a large-scale solution on synthetic topologies to improve communication performance while solving the ""dead-zone"" problem. Additionally, our mathematical framework is empirically validated within a realistic indoor scenario, the Rennes railway station, showing how a complex indoor propagation environment can be fully disciplined by an advanced RIS installation. △ Less","23 March, 2022",https://arxiv.org/pdf/2201.07591
WebUAV-3M: A Benchmark for Unveiling the Power of Million-Scale Deep UAV Tracking,Chunhui Zhang;Guanjie Huang;Li Liu;Shan Huang;Yinan Yang;Xiang Wan;Shiming Ge;Dacheng Tao,"Unmanned aerial vehicle (UAV) tracking is of great significance for a wide range of applications, such as delivery and agriculture. Previous benchmarks in this area mainly focused on small-scale tracking problems while ignoring the amounts of data, types of data modalities, diversities of target categories and scenarios, and evaluation protocols involved, greatly hiding the massive power of deep UAV tracking. In this work, we propose WebUAV-3M, the largest public UAV tracking benchmark to date, to facilitate both the development and evaluation of deep UAV trackers. WebUAV-3M contains over 3.3 million frames across 4,500 videos and offers 223 highly diverse target categories. Each video is densely annotated with bounding boxes by an efficient and scalable semiautomatic target annotation (SATA) pipeline. Importantly, to take advantage of the complementary superiority of language and audio, we enrich WebUAV-3M by innovatively providing both natural language specifications and audio descriptions. We believe that such additions will greatly boost future research in terms of exploring language features and audio cues for multimodal UAV tracking. In addition, a fine-grained UAV tracking-under-scenario constraint (UTUSC) evaluation protocol and seven challenging scenario subtest sets are constructed to enable the community to develop, adapt and evaluate various types of advanced trackers. We provide extensive evaluations and detailed analyses of 43 representative trackers and envision future research directions in the field of deep UAV tracking and beyond. The dataset, toolkits and baseline results are available at \url{https://github.com/983632847/WebUAV-3M}. △ Less","30 December, 2022",https://arxiv.org/pdf/2201.07425
"Defining Security Requirements with the Common Criteria: Applications, Adoptions, and Challenges",Nan Sun;Chang-Tsun Li;Hin Chan;Ba Dung Le;MD Zahidul Islam;Leo Yu Zhang;MD Rafiqul Islam;Warren Armstrong,"Advances of emerging Information and Communications Technology (ICT) technologies push the boundaries of what is possible and open up new markets for innovative ICT products and services. The adoption of ICT products and systems with security properties depends on consumers' confidence and markets' trust in the security functionalities and whether the assurance measures applied to these products meet the inherent security requirements. Such confidence and trust are primarily gained through the rigorous development of security requirements, validation criteria, evaluation, and certification. Common Criteria for Information Technology Security Evaluation (often referred to as Common Criteria or CC) is an international standard (ISO/IEC 15408) for cyber security certification. In this paper, we conduct a systematic review of the CC standards and its adoptions. Adoption barriers of the CC are also investigated based on the analysis of current trends in security evaluation. Specifically, we share the experiences and lessons gained through the recent Development of Australian Cyber Criteria Assessment (DACCA) project that promotes the CC among stakeholders in ICT security products related to specification, development, evaluation, certification and approval, procurement, and deployment. Best practices on developing Protection Profiles, recommendations, and future directions for trusted cybersecurity advancement are presented. △ Less","2 April, 2022",https://arxiv.org/pdf/2201.07417
What is the mission of innovation?,Julian D. Cortes,"Governments and organizations recognize the need to revisit a mission-driven innovation amidst national and organizational innovation policy formulations. Notwithstanding a fertile research agenda on mission statements (hereafter mission(s)), several lines of inquiry remain open, such as crossnational and multisectorial studies and an examination of research knowledge intensive institutions. In this article, we identify similarities and differences in the content of missions from government, private, higher education, and health research knowledge intensive institutions in a sample of over 1,900 institutions from 89 countries through the deployment of sentiment analysis, readability, and lexical diversity; semantic networks; and a similarity computation between document corpus. We found that missions of research knowledge intensive institutions are challenging to read texts with lower lexical diversity that favors positive rather than negative words. In stark contrast to this, the non-profit sector is consonant in multiple dimensions in its use of Corporate Social Responsibility jargon. The lexical appearance of research in the missions varies according to mission sectorial context, and each sector has a cluster specific focus. Utilizing the mission as a strategic planning tool in higher income regions might serve to explain corpora similarities shared by sectors and continents. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.07170
Scenario-based Requirements Engineering for Complex Smart City Projects,Carsten Wiecher;Philipp Tendyra;Carsten Wolff,"Various stakeholders with different backgrounds are involved in Smart City projects. These stakeholders define the project goals, e.g., based on participative approaches, market research or innovation management processes. To realize these goals often complex technical solutions must be designed and implemented. In practice, however, it is difficult to synchronize the technical design and implementation phase with the definition of moving Smart City goals. We hypothesize that this is due to a lack of a common language for the different stakeholder groups and the technical disciplines. We address this problem with scenario-based requirements engineering techniques. In particular, we use scenarios at different levels of abstraction and formalization that are connected end-to-end by appropriate methods and tools. This enables fast feedback loops to iteratively align technical requirements, stakeholder expectations, and Smart City goals. We demonstrate the applicability of our approach in a case study with different industry partners. △ Less","18 January, 2022",https://arxiv.org/pdf/2201.07115
A Review on Serious Games for Disaster Relief,Huansheng Ning;Zhangfeng Pi;Wenxi Wang;Fadi Farha;Shunkun Yang,"Human beings have been affected by disasters from the beginning of life, bringing them many sad memories. In the long struggle against disaster, people have devised a variety of methods to train relevant participants in disaster relief capabilities. However, many traditional training methods, such as disaster exercises may not provide effective training to meet the need of today. Serious games provide an innovative approach to train participants in disaster relief, and a large number of Serious Games for Disaster Relief (SGDRs) have been developed to train disaster planning and rescue capabilities. At the same time, there is no systematics phase description for disaster relief, which cannot effectively guide participants' work and training in disaster relief. Therefore, this paper proposes a comprehensive and professional disaster relief classification framework according to different relief work in each stage of the disaster. Based on this framework, we review the functions and technologies of serious games in each classification, which can offer reliable guidance for researchers to better understand and use SGDRs. In addition, we analyze the serious games in each category, point out the limitations, and provide some valuable advice for developers on game design. △ Less","10 January, 2022",https://arxiv.org/pdf/2201.06916
Flat Teams Drive Scientific Innovation,Fengli Xu;Lingfei Wu;James Evans,"With teams growing in all areas of scientific and scholarly research, we explore the relationship between team structure and the character of knowledge they produce. Drawing on 89,575 self-reports of team member research activity underlying scientific publications, we show how individual activities cohere into broad roles of (1) leadership through the direction and presentation of research and (2) support through data collection, analysis and discussion. The hidden hierarchy of a scientific team is characterized by its lead (or L)-ratio of members playing leadership roles to total team size. The L-ratio is validated through correlation with imputed contributions to the specific paper and to science as a whole, which we use to effectively extrapolate the L-ratio for 16,397,750 papers where roles are not explicit. We find that relative to flat, egalitarian teams, tall, hierarchical teams produce less novelty and more often develop existing ideas; increase productivity for those on top and decrease it for those beneath; increase short-term citations but decrease long-term influence. These effects hold within-person -- the same person on the same-sized team produces science much more likely to disruptively innovate if they work on a flat, high L-ratio team. These results suggest the critical role flat teams play for sustainable scientific advance and the training and advancement of scientists. △ Less","19 January, 2022",https://arxiv.org/pdf/2201.06726
DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale,Samyam Rajbhandari;Conglong Li;Zhewei Yao;Minjia Zhang;Reza Yazdani Aminabadi;Ammar Ahmad Awan;Jeff Rasley;Yuxiong He,"As the training of giant dense models hits the boundary on the availability and capability of the hardware resources today, Mixture-of-Experts (MoE) models become one of the most promising model architectures due to their significant training cost reduction compared to a quality-equivalent dense model. Its training cost saving is demonstrated from encoder-decoder models (prior works) to a 5x saving for auto-aggressive language models (this work along with parallel explorations). However, due to the much larger model size and unique architecture, how to provide fast MoE model inference remains challenging and unsolved, limiting its practical usage. To tackle this, we present DeepSpeed-MoE, an end-to-end MoE training and inference solution as part of the DeepSpeed library, including novel MoE architecture designs and model compression techniques that reduce MoE model size by up to 3.7x, and a highly optimized inference system that provides 7.3x better latency and cost compared to existing MoE inference solutions. DeepSpeed-MoE offers an unprecedented scale and efficiency to serve massive MoE models with up to 4.5x faster and 9x cheaper inference compared to quality-equivalent dense models. We hope our innovations and systems help open a promising path to new directions in the large model landscape, a shift from dense to sparse MoE models, where training and deploying higher-quality models with fewer resources becomes more widely possible. △ Less","21 July, 2022",https://arxiv.org/pdf/2201.05596
Model-Based Framework for exploiting sensors of IoT devices using a Botnet: A case study with Android,Zubair Khaliq;Dawood Ashraf Khan;Asif Iqbal Baba;Shahbaz Ali;Sheikh Umar Farooq,"Botnets have become a serious security threat not only to the Internet but also to the devices connected to it. Factors like the exponential growth of IoT, the COVID-19 pandemic that's sweeping the planet, and the ever-larger number of cyber-criminals who now have access to or have developed increasingly more sophisticated tools are incentivizing the growth of botnets in this domain. The recent outbreak of botnets like Dark Nexus (derived from Qbot and Mirai), Mukashi, LeetHozer, Hoaxcalls, etc. shows the alarming rate at which this threat is converging. The botnets have attributes that make them an excellent platform for malicious activities in IoT devices. These IoT devices are used by organizations that need to both innovate and safeguard the personal and confidential data of their customers, employees, and business partners. The IoT devices have built-in sensors or actuators which can be exploited to monitor or control the physical environment of the entities connected to them thereby violating the fundamental concept of privacy-by-design of these devices. In this paper, we design and describe a modular botnet framework for IoT. Our framework is communication channel independent because it utilizes various available communication channels for command and control of an IoT device. The framework uses an enhanced centralized architecture associated with a novel Domain Fluxing Technique. The proposed framework will provide insights into how privacy in IoT devices can be incorporated at design time to check the sensors and actuators in these devices against malicious exploitation consequently preserving privacy. This paper includes design considerations, command and control structures, characteristics, capabilities, intrusion, and other related work. Furthermore, proof of concept Botnet is implemented and explained using the developed framework. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05399
Towards a Fairer Digital Marketing Model,Leo Ardon;Dario Morelli;Francesco Villani;David Wheatley,"Surfing on the internet boom, the digital marketing industry has seen an exponential growth in the recent years and is often at the origin of the financial success of the biggest tech firms. In this paper we study the current landscape of this industry and comment on the monopoly that Google has managed to gain over the years through technical innovations and intelligent acquisitions. We then propose potential avenues to explore in an effort to help moving the digital marketing industry towards a fairer model. △ Less","14 January, 2022",https://arxiv.org/pdf/2201.05368
NG-Scope: Fine-Grained Telemetry for NextG Cellular Networks,Yaxiong Xie;Kyle Jamieson,"Accurate and highly-granular channel capacity telemetry of the cellular last hop is crucial for the effective operation of transport layer protocols and cutting-edge applications, such as video on demand and videotelephony. This paper presents the design, implementation, and experimental performance evaluation of NG-Scope, the first such telemetry tool able to fuse physical-layer channel occupancy readings from the cellular control channel with higher-layer packet arrival statistics and make accurate capacity estimates. NG-Scope handles the latest cellular innovations, such as when multiple base stations aggregate their signals together to serve mobile users. End-to-end experiments in a commercial cellular network demonstrate that wireless capacity varies significantly with channel quality, mobility, competing traffic within each cell, and the number of aggregated cells. Our experiments demonstrate significantly improved cell load estimation accuracy, missing the detection of less than 1% of data capacity overall, a reduction of 82% compared to OWL, the state-of-the-art in cellular monitoring. Further experiments show that MobileInsight-based CLAW has a root-mean-squared capacity error of 30.5 Mbit/s, which is 3.3x larger than NG-Scope (9.2 Mbit/s) △ Less","18 January, 2022",https://arxiv.org/pdf/2201.05281
Dynamic Local Searchable Symmetric Encryption,Brice Minaud;Michael Reichle,"In this article, we tackle for the first time the problem of dynamic memory-efficient Searchable Symmetric Encryption (SSE). In the term ""memory-efficient"" SSE, we encompass both the goals of local SSE, and page-efficient SSE. The centerpiece of our approach is a novel connection between those two goals. We introduce a map, called the Generic Local Transform, which takes as input a page-efficient SSE scheme with certain special features, and outputs an SSE scheme with strong locality properties. We obtain several results. (1) First, for page-efficient SSE, we build a dynamic scheme with page efficiency O(\log \log N) and storage efficiency O(1), called LayeredSSE. The main technical innovation behind LayeredSSE is a new weighted extension of the two-choice allocation process, of independent interest. (2) Second, we introduce the Generic Local Transform, and combine it with LayeredSSE to build a dynamic SSE scheme with storage efficiency O(1), locality O(1), and read efficiency O(\log\log N), under the condition that the longest list is of size O(N^{1-1/\log \log λ}). This matches, in every respect, the purely static construction of Asharov et al. presented at STOC 2016: dynamism comes at no extra cost. (3) Finally, by applying the Generic Local Transform to a variant of the Tethys scheme by Bossuat et al. from Crypto 2021, we build an unconditional static SSE with storage efficiency O(1), locality O(1), and read efficiency O(\log^\varepsilon N), for an arbitrarily small constant \varepsilon > 0. To our knowledge, this is the construction that comes closest to the lower bound presented by Cash and Tessaro at Eurocrypt 2014. △ Less","13 January, 2022",https://arxiv.org/pdf/2201.05006
Unified Mobility Estimation Mode,David Ziegler;Johannes Betz;Markus Lienkamp,"In literature, scientists describe human mobility in a range of granularities by several different models. Using frameworks like MATSIM, VehiLux, or Sumo, they often derive individual human movement indicators in their most detail. However, such agent-based models tend to be difficult and require much information and computational power to correctly predict the commutation behavior of large mobility systems. Mobility information can be costly and researchers often cannot acquire it dynamically over large areas, which leads to a lack of adequate calibration parameters, rendering the easy and spontaneous prediction of mobility in additional areas impossible. This paper targets this problem and represents a concept that combines multiple substantial mobility theorems formulated in recent years to reduce the amount of required information compared to existing simulations. Our concept also targets computational expenses and aims to reduce them to enable a global prediction of mobility. Inspired by methods from other domains, the core idea of the conceptional innovation can be compared to weather models, which predict weather on a large scale, on an adequate level of regional information (airspeed, air pressure, etc.), but without the detailed movement information of each air atom and its particular simulation. △ Less","13 January, 2022",https://arxiv.org/pdf/2201.04931
Weighted Sum Rate Maximization of the mmWave Cell-Free MIMO Downlink Relying on Hybrid Precoding,Chenghao Feng;Wenqian Shen;Jianping An;Lajos Hanzo,"The cell-free MIMO concept relying on hybrid precoding constitutes an innovative technique capable of dramatically increasing the network capacity of millimeter-wave (mmWave) communication systems. It dispenses with the cell boundary of conventional multi-cell MIMO systems, while drastically reducing the power consumption by limiting the number of radio frequency (RF) chains at the access points (APs). In this paper, we aim for maximizing the weighted sum rate (WSR) of mmWave cell-free MIMO systems by conceiving a low-complexity hybrid precoding algorithm. We formulate the WSR optimization problem subject to the transmit power constraint for each AP and the constant-modulus constraint for the phase shifters of the analog precoders. A block coordinate descent (BCD) algorithm is proposed for iteratively solving the problem. In each iteration, the classic Lagrangian multiplier method and the penalty dual decomposition (PDD) method are combined for obtaining near-optimal hybrid analog/digital precoding matrices. Furthermore, we extend our proposed algorithm for deriving closed-form expressions for the precoders of fully digital cell-free MIMO systems. Moreover, we present the convergency analysis and complexity analysis of our proposed method. Finally, our simulation results demonstrate the superiority of the algorithms proposed for both fully digital and hybrid precoding matrices. △ Less","12 January, 2022",https://arxiv.org/pdf/2201.04332
Matching-based Service Offloading for Compute-less Driven IoT Networks,Boubakr Nour;Soumaya Cherkaoui,"With the advent of the Internet of Things (IoT) and 5G networks, edge computing is offering new opportunities for business model and use cases innovations. Service providers can now virtualize the cloud beyond the data center to meet the latency, data sovereignty, reliability, and interoperability requirements. Yet, many new applications (e.g., augmented reality, virtual reality, artificial intelligence) are computation-intensive and delay-sensitivity. These applications are invoked heavily with similar inputs that could lead to the same output. Compute-less networks aim to implement a network with a minimum amount of computation and communication. This can be realized by offloading prevalent services to the edge and thus minimizing communication in the core network and eliminating redundant computations using the computation reuse concept. In this paper, we present matching-based services offloading schemes for compute-less IoT networks. We adopt the matching theory to match service offloading to the appropriate edge server(s). Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme that aims to offload the most invoked and highly reusable services to the appropriate edge servers. We further extend WHISTLE to provide horizontal one-to-many computation reuse sharing among edge servers which leads to bouncing less computation back to the cloud. We evaluate the efficiency and effectiveness of WHISTLE with a real-world dataset. The obtained findings show that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the computation up to 77%, and decrease the communication up to 71%. Theoretical analyses also prove the stability of the designed schemes. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04195
Multi-physics integration platform MuPIF: Application for composite material design,B. Patzák;V. Šmilauer;M. Horák;S. Šulc;E. Dvořáková,"This paper presents the design of the MuPIF distributed, multi-physics simulation platform and its performance in the context of the H2020 Composelector project. The description of MuPIF's model and data interfaces provides implementation and operational details that illustrate how MuPIF is a highly versatile and robust tool for various engineering applications. Its distributed design and the integration of models in a workflow allows MuPIF to simulate real industrial tasks. The design of a composite airplane frame for the Composelector project, focusing on innovative design of composite materials and structures, demonstrates MuPIF's capabilities and ability to be integrated into Business Decision Support Systems. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04130
Smart Director: An Event-Driven Directing System for Live Broadcasting,Yingwei Pan;Yue Chen;Qian Bao;Ning Zhang;Ting Yao;Jingen Liu;Tao Mei,"Live video broadcasting normally requires a multitude of skills and expertise with domain knowledge to enable multi-camera productions. As the number of cameras keep increasing, directing a live sports broadcast has now become more complicated and challenging than ever before. The broadcast directors need to be much more concentrated, responsive, and knowledgeable, during the production. To relieve the directors from their intensive efforts, we develop an innovative automated sports broadcast directing system, called Smart Director, which aims at mimicking the typical human-in-the-loop broadcasting process to automatically create near-professional broadcasting programs in real-time by using a set of advanced multi-view video analysis algorithms. Inspired by the so-called ""three-event"" construction of sports broadcast, we build our system with an event-driven pipeline consisting of three consecutive novel components: 1) the Multi-view Event Localization to detect events by modeling multi-view correlations, 2) the Multi-view Highlight Detection to rank camera views by the visual importance for view selection, 3) the Auto-Broadcasting Scheduler to control the production of broadcasting videos. To our best knowledge, our system is the first end-to-end automated directing system for multi-camera sports broadcasting, completely driven by the semantic understanding of sports events. It is also the first system to solve the novel problem of multi-view joint event detection by cross-view relation modeling. We conduct both objective and subjective evaluations on a real-world multi-camera soccer dataset, which demonstrate the quality of our auto-generated videos is comparable to that of the human-directed. Thanks to its faster response, our system is able to capture more fast-passing and short-duration events which are usually missed by human directors. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.04024
pymdp: A Python library for active inference in discrete state spaces,Conor Heins;Beren Millidge;Daphne Demekas;Brennan Klein;Karl Friston;Iain Couzin;Alexander Tschantz,"Active inference is an account of cognition and behavior in complex systems which brings together action, perception, and learning under the theoretical mantle of Bayesian inference. Active inference has seen growing applications in academic research, especially in fields that seek to model human or animal behavior. While in recent years, some of the code arising from the active inference literature has been written in open source languages like Python and Julia, to-date, the most popular software for simulating active inference agents is the DEM toolbox of SPM, a MATLAB library originally developed for the statistical analysis and modelling of neuroimaging data. Increasing interest in active inference, manifested both in terms of sheer number as well as diversifying applications across scientific disciplines, has thus created a need for generic, widely-available, and user-friendly code for simulating active inference in open-source scientific computing languages like Python. The Python package we present here, pymdp (see https://github.com/infer-actively/pymdp), represents a significant step in this direction: namely, we provide the first open-source package for simulating active inference with partially-observable Markov Decision Processes or POMDPs. We review the package's structure and explain its advantages like modular design and customizability, while providing in-text code blocks along the way to demonstrate how it can be used to build and run active inference processes with ease. We developed pymdp to increase the accessibility and exposure of the active inference framework to researchers, engineers, and developers with diverse disciplinary backgrounds. In the spirit of open-source software, we also hope that it spurs new innovation, development, and collaboration in the growing active inference community. △ Less","4 May, 2022",https://arxiv.org/pdf/2201.03904
A Saliency based Feature Fusion Model for EEG Emotion Estimation,Victor Delvigne;Antoine Facchini;Hazem Wannous;Thierry Dutoit;Laurence Ris;Jean-Philippe Vandeborre,"Among the different modalities to assess emotion, electroencephalogram (EEG), representing the electrical brain activity, achieved motivating results over the last decade. Emotion estimation from EEG could help in the diagnosis or rehabilitation of certain diseases. In this paper, we propose a dual model considering two different representations of EEG feature maps: 1) a sequential based representation of EEG band power, 2) an image-based representation of the feature vectors. We also propose an innovative method to combine the information based on a saliency analysis of the image-based model to promote joint learning of both model parts. The model has been evaluated on four publicly available datasets: SEED-IV, SEED, DEAP and MPED. The achieved results outperform results from state-of-the-art approaches for three of the proposed datasets with a lower standard deviation that reflects higher stability. For sake of reproducibility, the codes and models proposed in this paper are available at https://github.com/VDelv/Emotion-EEG. △ Less","4 April, 2022",https://arxiv.org/pdf/2201.03891
Machine learning enabling high-throughput and remote operations at large-scale user facilities,Tatiana Konstantinova;Phillip M. Maffettone;Bruce Ravel;Stuart I. Campbell;Andi M. Barbour;Daniel Olds,"Imaging, scattering, and spectroscopy are fundamental in understanding and discovering new functional materials. Contemporary innovations in automation and experimental techniques have led to these measurements being performed much faster and with higher resolution, thus producing vast amounts of data for analysis. These innovations are particularly pronounced at user facilities and synchrotron light sources. Machine learning (ML) methods are regularly developed to process and interpret large datasets in real-time with measurements. However, there remain conceptual barriers to entry for the facility general user community, whom often lack expertise in ML, and technical barriers for deploying ML models. Herein, we demonstrate a variety of archetypal ML models for on-the-fly analysis at multiple beamlines at the National Synchrotron Light Source II (NSLS-II). We describe these examples instructively, with a focus on integrating the models into existing experimental workflows, such that the reader can easily include their own ML techniques into experiments at NSLS-II or facilities with a common infrastructure. The framework presented here shows how with little effort, diverse ML models operate in conjunction with feedback loops via integration into the existing Bluesky Suite for experimental orchestration and data management. △ Less","9 January, 2022",https://arxiv.org/pdf/2201.03550
Cross-view Self-Supervised Learning on Heterogeneous Graph Neural Network via Bootstrapping,Minjae Park,"Heterogeneous graph neural networks can represent information of heterogeneous graphs with excellent ability. Recently, self-supervised learning manner is researched which learns the unique expression of a graph through a contrastive learning method. In the absence of labels, this learning methods show great potential. However, contrastive learning relies heavily on positive and negative pairs, and generating high-quality pairs from heterogeneous graphs is difficult. In this paper, in line with recent innovations in self-supervised learning called BYOL or bootstrapping, we introduce a that can generate good representations without generating large number of pairs. In addition, paying attention to the fact that heterogeneous graphs can be viewed from two perspectives, network schema and meta-path views, high-level expressions in the graphs are captured and expressed. The proposed model showed state-of-the-art performance than other methods in various real world datasets. △ Less","11 January, 2022",https://arxiv.org/pdf/2201.03340
An open tool based on lifex for myofibers generation in cardiac computational models,Pasquale C. Africa;Roberto Piersanti;Marco Fedele;Luca Dede';Alfio Quarteroni,"Modeling the whole cardiac function involves the solution of several complex multi-physics and multi-scale models that are highly computationally demanding, which call for simpler yet accurate, high-performance computational tools. Despite the efforts made by several research groups, no software for whole-heart fully-coupled cardiac simulations in the scientific community has reached full maturity yet. In this work we present the first publicly released package of lifex, a high-performance Finite Element solver for multi-physics and multi-scale problems developed in the framework of the iHEART project. The goal of lifex is twofold. On the one side, it aims at making in silico experiments easily reproducible and accessible to a wide community of users, including those with a background in medicine or bio-engineering. On the other hand, as an academic research library lifex can be exploited by scientific computing experts to explore new mathematical models and numerical methods within a robust development framework. lifex has been developed with a modular structure and will be released bundled in different modules. The tool presented here proposes an innovative generator for myocardial fibers based on Laplace-Dirichlet Rule-Based Methods, which are the essential building blocks for modeling the electrophysiological, mechanical and electromechanical cardiac function, from single-chamber to whole-heart simulations. This report comes with an extensive technical and mathematical documentation to welcome new users to the core structure of a prototypical lifex application and to provide them with a possible approach to include the generated cardiac fibers into more sophisticated computational pipelines. △ Less","7 October, 2022",https://arxiv.org/pdf/2201.03303
Nukhada USV: a Robot for Autonomous Surveying and Support to Underwater Operations,Èric Pairet;Simone Spanò;Nikita Mankovskii;Paolo Pellegrino;Igor Zhilin;Jeremy Nicola;Francesco La Gala;Giulia De Masi,"The Technology Innovation Institute in Abu Dhabi, United Arab Emirates, has recently finished the production and testing of a new unmanned surface vehicle, called Nukhada, specifically designed for autonomous survey, inspection, and support to underwater operations. This manuscript describes the main characteristics of the Nukhada USV, as well as some of the trials conducted during the development. △ Less","10 January, 2022",https://arxiv.org/pdf/2201.03168
DeHIN: A Decentralized Framework for Embedding Large-scale Heterogeneous Information Networks,Mubashir Imran;Hongzhi Yin;Tong Chen;Zi Huang;Kai Zheng,"Modeling heterogeneity by extraction and exploitation of high-order information from heterogeneous information networks (HINs) has been attracting immense research attention in recent times. Such heterogeneous network embedding (HNE) methods effectively harness the heterogeneity of small-scale HINs. However, in the real world, the size of HINs grow exponentially with the continuous introduction of new nodes and different types of links, making it a billion-scale network. Learning node embeddings on such HINs creates a performance bottleneck for existing HNE methods that are commonly centralized, i.e., complete data and the model are both on a single machine. To address large-scale HNE tasks with strong efficiency and effectiveness guarantee, we present \textit{Decentralized Embedding Framework for Heterogeneous Information Network} (DeHIN) in this paper. In DeHIN, we generate a distributed parallel pipeline that utilizes hypergraphs in order to infuse parallelization into the HNE task. DeHIN presents a context preserving partition mechanism that innovatively formulates a large HIN as a hypergraph, whose hyperedges connect semantically similar nodes. Our framework then adopts a decentralized strategy to efficiently partition HINs by adopting a tree-like pipeline. Then, each resulting subnetwork is assigned to a distributed worker, which employs the deep information maximization theorem to locally learn node embeddings from the partition it receives. We further devise a novel embedding alignment scheme to precisely project independently learned node embeddings from all subnetworks onto a common vector space, thus allowing for downstream tasks like link prediction and node classification. △ Less","7 January, 2022",https://arxiv.org/pdf/2201.02757
Provable Clustering of a Union of Linear Manifolds Using Optimal Directions,Mostafa Rahmani,"This paper focuses on the Matrix Factorization based Clustering (MFC) method which is one of the few closed form algorithms for the subspace clustering problem. Despite being simple, closed-form, and computation-efficient, MFC can outperform the other sophisticated subspace clustering methods in many challenging scenarios. We reveal the connection between MFC and the Innovation Pursuit (iPursuit) algorithm which was shown to be able to outperform the other spectral clustering based methods with a notable margin especially when the span of clusters are close. A novel theoretical study is presented which sheds light on the key performance factors of both algorithms (MFC/iPursuit) and it is shown that both algorithms can be robust to notable intersections between the span of clusters. Importantly, in contrast to the theoretical guarantees of other algorithms which emphasized on the distance between the subspaces as the key performance factor and without making the innovation assumption, it is shown that the performance of MFC/iPursuit mainly depends on the distance between the innovative components of the clusters. △ Less","7 January, 2022",https://arxiv.org/pdf/2201.02745
United adversarial learning for liver tumor segmentation and detection of multi-modality non-contrast MRI,Jianfeng Zhao;Dengwang Li;Shuo Li,"Simultaneous segmentation and detection of liver tumors (hemangioma and hepatocellular carcinoma (HCC)) by using multi-modality non-contrast magnetic resonance imaging (NCMRI) are crucial for the clinical diagnosis. However, it is still a challenging task due to: (1) the HCC information on NCMRI is invisible or insufficient makes extraction of liver tumors feature difficult; (2) diverse imaging characteristics in multi-modality NCMRI causes feature fusion and selection difficult; (3) no specific information between hemangioma and HCC on NCMRI cause liver tumors detection difficult. In this study, we propose a united adversarial learning framework (UAL) for simultaneous liver tumors segmentation and detection using multi-modality NCMRI. The UAL first utilizes a multi-view aware encoder to extract multi-modality NCMRI information for liver tumor segmentation and detection. In this encoder, a novel edge dissimilarity feature pyramid module is designed to facilitate the complementary multi-modality feature extraction. Second, the newly designed fusion and selection channel is used to fuse the multi-modality feature and make the decision of the feature selection. Then, the proposed mechanism of coordinate sharing with padding integrates the multi-task of segmentation and detection so that it enables multi-task to perform united adversarial learning in one discriminator. Lastly, an innovative multi-phase radiomics guided discriminator exploits the clear and specific tumor information to improve the multi-task performance via the adversarial learning strategy. The UAL is validated in corresponding multi-modality NCMRI (i.e. T1FS pre-contrast MRI, T2FS MRI, and DWI) and three phases contrast-enhanced MRI of 255 clinical subjects. The experiments show that UAL has great potential in the clinical diagnosis of liver tumors. △ Less","7 January, 2022",https://arxiv.org/pdf/2201.02629
A deep learning-based model reduction (DeePMR) method for simplifying chemical kinetics,Zhiwei Wang;Yaoyu Zhang;Enhan Zhao;Yiguang Ju;Weinan E;Zhi-Qin John Xu;Tianhan Zhang,"A deep learning-based model reduction (DeePMR) method for simplifying chemical kinetics is proposed and validated using high-temperature auto-ignitions, perfectly stirred reactors (PSR), and one-dimensional freely propagating flames of n-heptane/air mixtures. The mechanism reduction is modeled as an optimization problem on Boolean space, where a Boolean vector, each entry corresponding to a species, represents a reduced mechanism. The optimization goal is to minimize the reduced mechanism size given the error tolerance of a group of pre-selected benchmark quantities. The key idea of the DeePMR is to employ a deep neural network (DNN) to formulate the objective function in the optimization problem. In order to explore high dimensional Boolean space efficiently, an iterative DNN-assisted data sampling and DNN training procedure are implemented. The results show that DNN-assistance improves sampling efficiency significantly, selecting only 10^5 samples out of 10^{34} possible samples for DNN to achieve sufficient accuracy. The results demonstrate the capability of the DNN to recognize key species and reasonably predict reduced mechanism performance. The well-trained DNN guarantees the optimal reduced mechanism by solving an inverse optimization problem. By comparing ignition delay times, laminar flame speeds, temperatures in PSRs, the resulting skeletal mechanism has fewer species (45 species) but the same level of accuracy as the skeletal mechanism (56 species) obtained by the Path Flux Analysis (PFA) method. In addition, the skeletal mechanism can be further reduced to 28 species if only considering atmospheric, near-stoichiometric conditions (equivalence ratio between 0.6 and 1.2). The DeePMR provides an innovative way to perform model reduction and demonstrates the great potential of data-driven methods in the combustion area. △ Less","8 September, 2022",https://arxiv.org/pdf/2201.02025
"Machine Learning: Algorithms, Models, and Applications",Jaydip Sen;Sidra Mehtab;Rajdeep Sen;Abhishek Dutta;Pooja Kherwa;Saheel Ahmed;Pranay Berry;Sahil Khurana;Sonali Singh;David W. W Cadotte;David W. Anderson;Kalum J. Ost;Racheal S. Akinbo;Oladunni A. Daramola;Bongs Lainjo,"Recent times are witnessing rapid development in machine learning algorithm systems, especially in reinforcement learning, natural language processing, computer and robot vision, image processing, speech, and emotional processing and understanding. In tune with the increasing importance and relevance of machine learning models, algorithms, and their applications, and with the emergence of more innovative uses cases of deep learning and artificial intelligence, the current volume presents a few innovative research works and their applications in real world, such as stock trading, medical and healthcare systems, and software automation. The chapters in the book illustrate how machine learning and deep learning algorithms and models are designed, optimized, and deployed. The volume will be useful for advanced graduate and doctoral students, researchers, faculty members of universities, practicing data scientists and data engineers, professionals, and consultants working on the broad areas of machine learning, deep learning, and artificial intelligence. △ Less","6 January, 2022",https://arxiv.org/pdf/2201.01943
Automated Scoring of Graphical Open-Ended Responses Using Artificial Neural Networks,Matthias von Davier;Lillian Tyack;Lale Khorramdel,"Automated scoring of free drawings or images as responses has yet to be utilized in large-scale assessments of student achievement. In this study, we propose artificial neural networks to classify these types of graphical responses from a computer based international mathematics and science assessment. We are comparing classification accuracy of convolutional and feedforward approaches. Our results show that convolutional neural networks (CNNs) outperform feedforward neural networks in both loss and accuracy. The CNN models classified up to 97.71% of the image responses into the appropriate scoring category, which is comparable to, if not more accurate, than typical human raters. These findings were further strengthened by the observation that the most accurate CNN models correctly classified some image responses that had been incorrectly scored by the human raters. As an additional innovation, we outline a method to select human rated responses for the training sample based on an application of the expected response function derived from item response theory. This paper argues that CNN-based automated scoring of image responses is a highly accurate procedure that could potentially replace the workload and cost of second human raters for large scale assessments, while improving the validity and comparability of scoring complex constructed-response items. △ Less","5 January, 2022",https://arxiv.org/pdf/2201.01783
From the Ground Truth Up: Doing AI Ethics from Practice to Principles,James Brusseau,"Recent AI ethics has focused on applying abstract principles downward to practice. This paper moves in the other direction. Ethical insights are generated from the lived experiences of AI-designers working on tangible human problems, and then cycled upward to influence theoretical debates surrounding these questions: 1) Should AI as trustworthy be sought through explainability, or accurate performance? 2) Should AI be considered trustworthy at all, or is reliability a preferable aim? 3) Should AI ethics be oriented toward establishing protections for users, or toward catalyzing innovation? Specific answers are less significant than the larger demonstration that AI ethics is currently unbalanced toward theoretical principles, and will benefit from increased exposure to grounded practices and dilemmas. △ Less","5 January, 2022",https://arxiv.org/pdf/2201.01659
Extending the limit of molecular dynamics with ab initio accuracy to 10 billion atoms,Zhuoqiang Guo;Denghui Lu;Yujin Yan;Siyu Hu;Rongrong Liu;Guangming Tan;Ninghui Sun;Wanrun Jiang;Lijun Liu;Yixiao Chen;Linfeng Zhang;Mohan Chen;Han Wang;Weile Jia,"High-performance computing, together with a neural network model trained from data generated with first-principles methods, has greatly boosted applications of \textit{ab initio} molecular dynamics in terms of spatial and temporal scales on modern supercomputers. Previous state-of-the-art can achieve 1-2 nanoseconds molecular dynamics simulation per day for 100-million atoms on the entire Summit supercomputer. In this paper, we have significantly reduced the memory footprint and computational time by a comprehensive approach with both algorithmic and system innovations. The neural network model is compressed by model tabulation, kernel fusion, and redundancy removal. Then optimizations such as acceleration of customized kernel, tabulation of activation function, MPI+OpenMP parallelization are implemented on GPU and ARM architectures. Testing results of the copper system show that the optimized code can scale up to the entire machine of both Fugaku and Summit, and the corresponding system size can be extended by a factor of 134 to an unprecedented 17 billion atoms. The strong scaling of a 13.5-million atom copper system shows that the time-to-solution can be 7 times faster, reaching 11.2 nanoseconds per day. This work opens the door for unprecedentedly large-scale molecular dynamics simulations based on {\it ab initio} accuracy and can be potentially utilized in studying more realistic applications such as mechanical properties of metals, semiconductor devices, batteries, etc. The optimization techniques detailed in this paper also provide insight for relevant high-performance computing applications. △ Less","4 January, 2022",https://arxiv.org/pdf/2201.01446
Advancing 3D Medical Image Analysis with Variable Dimension Transform based Supervised 3D Pre-training,Shu Zhang;Zihao Li;Hong-Yu Zhou;Jiechao Ma;Yizhou Yu,"The difficulties in both data acquisition and annotation substantially restrict the sample sizes of training datasets for 3D medical imaging applications. As a result, constructing high-performance 3D convolutional neural networks from scratch remains a difficult task in the absence of a sufficient pre-training parameter. Previous efforts on 3D pre-training have frequently relied on self-supervised approaches, which use either predictive or contrastive learning on unlabeled data to build invariant 3D representations. However, because of the unavailability of large-scale supervision information, obtaining semantically invariant and discriminative representations from these learning frameworks remains problematic. In this paper, we revisit an innovative yet simple fully-supervised 3D network pre-training framework to take advantage of semantic supervisions from large-scale 2D natural image datasets. With a redesigned 3D network architecture, reformulated natural images are used to address the problem of data scarcity and develop powerful 3D representations. Comprehensive experiments on four benchmark datasets demonstrate that the proposed pre-trained models can effectively accelerate convergence while also improving accuracy for a variety of 3D medical imaging tasks such as classification, segmentation and detection. In addition, as compared to training from scratch, it can save up to 60% of annotation efforts. On the NIH DeepLesion dataset, it likewise achieves state-of-the-art detection performance, outperforming earlier self-supervised and fully-supervised pre-training approaches, as well as methods that do training from scratch. To facilitate further development of 3D medical models, our code and pre-trained model weights are publicly available at https://github.com/urmagicsmine/CSPR. △ Less","4 January, 2022",https://arxiv.org/pdf/2201.01426
ReWiS: Reliable Wi-Fi Sensing Through Few-Shot Multi-Antenna Multi-Receiver CSI Learning,Niloofar Bahadori;Jonathan Ashdown;Francesco Restuccia,"Thanks to the ubiquitousness of Wi-Fi access points and devices, Wi-Fi sensing enables transformative applications in remote health care, security, and surveillance. Existing work has explored the usage of machine learning on channel state information (CSI) computed from Wi-Fi packets to classify events of interest. However, most of these algorithms require a significant amount of data collection, as well as extensive computational power for additional CSI feature extraction. Moreover, the majority of these models suffer from poor accuracy when tested in a new/untrained environment. In this paper, we propose ReWiS, a novel framework for robust and environment-independent Wi-Fi sensing. The key innovation of ReWiS is to leverage few-shot learning (FSL) as the inference engine, which (i) reduces the need for extensive data collection and application-specific feature extraction; (ii) can rapidly generalize to new tasks by leveraging only a few new samples. We prototype ReWiS using off-the-shelf Wi-Fi equipment and showcase its performance by considering a compelling use case of human activity recognition. Thus, we perform an extensive data collection campaign in three different propagation environments with two human subjects. We evaluate the impact of each diversity component on the performance and compare ReWiS with a traditional convolutional neural network (CNN) approach. Experimental results show that ReWiS improves the performance by about 40% with respect to existing single-antenna low-resolution approaches. Moreover, when compared to a CNN-based approach, ReWiS shows a 35% more accuracy and less than 10% drop in accuracy when tested in different environments, while the CNN drops by more than 45%. △ Less","19 April, 2022",https://arxiv.org/pdf/2201.00869
A Cluster-Based Trip Prediction Graph Neural Network Model for Bike Sharing Systems,Bárbara Tavares;Cláudia Soares;Manuel Marques,"Bike Sharing Systems (BSSs) are emerging as an innovative transportation service. Ensuring the proper functioning of a BSS is crucial given that these systems are committed to eradicating many of the current global concerns, by promoting environmental and economic sustainability and contributing to improving the life quality of the population. Good knowledge of users' transition patterns is a decisive contribution to the quality and operability of the service. The analogous and unbalanced users' transition patterns cause these systems to suffer from bicycle imbalance, leading to a drastic customer loss in the long term. Strategies for bicycle rebalancing become important to tackle this problem and for this, bicycle traffic prediction is essential, as it allows to operate more efficiently and to react in advance. In this work, we propose a bicycle trips predictor based on Graph Neural Network embeddings, taking into consideration station groupings, meteorology conditions, geographical distances, and trip patterns. We evaluated our approach in the New York City BSS (CitiBike) data and compared it with four baselines, including the non-clustered approach. To address our problem's specificities, we developed the Adaptive Transition Constraint Clustering Plus (AdaTC+) algorithm, eliminating shortcomings of previous work. Our experiments evidence the clustering pertinence (88% accuracy compared with 83% without clustering) and which clustering technique best suits this problem. Accuracy on the Link Prediction task is always higher for AdaTC+ than benchmark clustering methods when the stations are the same, while not degrading performance when the network is upgraded, in a mismatch with the trained model. △ Less","3 January, 2022",https://arxiv.org/pdf/2201.00720
VISAS -- Detecting GPS spoofing attacks against drones by analyzing camera's video stream,Barak Davidovich;Ben Nassi;Yuval Elovici,"In this study, we propose an innovative method for the real-time detection of GPS spoofing attacks targeting drones, based on the video stream captured by a drone's camera. The proposed method collects frames from the video stream and their location (GPS); by calculating the correlation between each frame, our method can identify an attack on a drone. We first analyze the performance of the suggested method in a controlled environment by conducting experiments on a flight simulator that we developed. Then, we analyze its performance in the real world using a DJI drone. Our method can provide different levels of security against GPS spoofing attacks, depending on the detection interval required; for example, it can provide a high level of security to a drone flying at an altitude of 50-100 meters over an urban area at an average speed of 4 km/h in conditions of low ambient light; in this scenario, the method can provide a level of security that detects any GPS spoofing attack in which the spoofed location is a distance of 1-4 meters (an average of 2.5 meters) from the real location. △ Less","2 January, 2022",https://arxiv.org/pdf/2201.00419
DF-SSmVEP: Dual Frequency Aggregated Steady-State Motion Visual Evoked Potential Design with Bifold Canonical Correlation Analysis,Raika Karimi;Arash Mohammadi;Amir Asif;Habib Benali,"Recent advancements in Electroencephalography (EEG) sensor technologies and signal processing algorithms have paved the way for further evolution of Brain Computer Interfaces (BCI). When it comes to Signal Processing (SP) for BCI, there has been a surge of interest on Steady-State motion-Visual Evoked Potentials (SSmVEP), where motion stimulation is utilized to address key issues associated with conventional light-flashing/flickering. Such benefits, however, come with the price of having less accuracy and less Information Transfer Rate (ITR). In this regard, the paper focuses on the design of a novel SSmVEP paradigm without using resources such as trial time, phase, and/or number of targets to enhance the ITR. The proposed design is based on the intuitively pleasing idea of integrating more than one motion within a single SSmVEP target stimuli, simultaneously. To elicit SSmVEP, we designed a novel and innovative dual frequency aggregated modulation paradigm, referred to as the Dual Frequency Aggregated steady-state motion Visual Evoked Potential (DF-SSmVEP), by concurrently integrating ""Radial Zoom"" and ""Rotation"" motions in a single target without increasing the trial length. Compared to conventional SSmVEPs, the proposed DF-SSmVEP framework consists of two motion modes integrated and shown simultaneously each modulated by a specific target frequency. The paper also develops a specific unsupervised classification model, referred to as the Bifold Canonical Correlation Analysis (BCCA), based on two motion frequencies per target. The proposed DF-SSmVEP is evaluated based on a real EEG dataset and the results corroborate its superiority. The proposed DF-SSmVEP outperforms its counterparts and achieved an average ITR of 30.7 +/- 1.97 and an average accuracy of 92.5 +/- 2.04. △ Less","1 January, 2022",https://arxiv.org/pdf/2201.00283
HPRN: Holistic Prior-embedded Relation Network for Spectral Super-Resolution,Chaoxiong Wu;Jiaojiao Li;Rui Song;Yunsong Li;Qian Du,"Spectral super-resolution (SSR) refers to the hyperspectral image (HSI) recovery from an RGB counterpart. Due to the one-to-many nature of the SSR problem, a single RGB image can be reprojected to many HSIs. The key to tackle this ill-posed problem is to plug into multi-source prior information such as the natural spatial context-prior of RGB images, deep feature-prior or inherent statistical-prior of HSIs, etc., so as to effectively alleviate the degree of ill-posedness. However, most current approaches only consider the general and limited priors in their customized convolutional neural networks (CNNs), which leads to the inability to guarantee the confidence and fidelity of reconstructed spectra. In this paper, we propose a novel holistic prior-embedded relation network (HPRN) to integrate comprehensive priors to regularize and optimize the solution space of SSR. Basically, the core framework is delicately assembled by several multi-residual relation blocks (MRBs) that fully facilitate the transmission and utilization of the low-frequency content prior of RGBs. Innovatively, the semantic prior of RGB inputs is introduced to mark category attributes, and a semantic-driven spatial relation module (SSRM) is invented to perform the feature aggregation of clustered similar range for refining recovered characteristics. Additionally, we develop a transformer-based channel relation module (TCRM), which breaks the habit of employing scalars as the descriptors of channel-wise relations in the previous deep feature-prior, and replaces them with certain vectors to make the mapping function more robust and smoother. In order to maintain the mathematical correlation and spectral consistency between hyperspectral bands, the second-order prior constraints (SOPC) are incorporated into the loss function to guide the HSI reconstruction. △ Less","8 February, 2022",https://arxiv.org/pdf/2112.14608
Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data,Ruining Deng;Quan Liu;Can Cui;Zuhayr Asad;Haichun Yang;Yuankai Huo,"Computer-assisted quantitative analysis on Giga-pixel pathology images has provided a new avenue in histology examination. The innovations have been largely focused on cancer pathology (i.e., tumor segmentation and characterization). In non-cancer pathology, the learning algorithms can be asked to examine more comprehensive tissue types simultaneously, as a multi-label setting. The prior arts typically needed to train multiple segmentation networks in order to match the domain-specific knowledge for heterogeneous tissue types (e.g., glomerular tuft, glomerular unit, proximal tubular, distal tubular, peritubular capillaries, and arteries). In this paper, we propose a dynamic single segmentation network (Omni-Seg) that learns to segment multiple tissue types using partially labeled images (i.e., only one tissue type is labeled for each training image) for renal pathology. By learning from ~150,000 patch-wise pathological images from six tissue types, the proposed Omni-Seg network achieved superior segmentation accuracy and less resource consumption when compared to the previous the multiple-network and multi-head design. In the testing stage, the proposed method obtains ""completely labeled"" tissue segmentation results using only ""partially labeled"" training images. The source code is available at https://github.com/ddrrnn123/Omni-Seg △ Less","23 March, 2022",https://arxiv.org/pdf/2112.12665
Comparison and Analysis of Image-to-Image Generative Adversarial Networks: A Survey,Sagar Saxena;Mohammad Nayeem Teli,"Generative Adversarial Networks (GANs) have recently introduced effective methods of performing Image-to-Image translations. These models can be applied and generalized to a variety of domains in Image-to-Image translation without changing any parameters. In this paper, we survey and analyze eight Image-to-Image Generative Adversarial Networks: Pix2Pix, CycleGAN, CoGAN, StarGAN, MUNIT, StarGAN2, DA-GAN, and Self Attention GAN. Each of these models presented state-of-the-art results and introduced new techniques to build Image-to-Image GANs. In addition to a survey of the models, we also survey the 18 datasets they were trained on and the 9 metrics they were evaluated on. Finally, we present results of a controlled experiment for 6 of these models on a common set of metrics and datasets. The results were mixed and showed that, on certain datasets, tasks, and metrics, some models outperformed others. The last section of this paper discusses those results and establishes areas of future research. As researchers continue to innovate new Image-to-Image GANs, it is important to gain a good understanding of the existing methods, datasets, and metrics. This paper provides a comprehensive overview and discussion to help build this foundation. △ Less","26 August, 2022",https://arxiv.org/pdf/2112.12625
BEVDet: High-performance Multi-camera 3D Object Detection in Bird-Eye-View,Junjie Huang;Guan Huang;Zheng Zhu;Yun Ye;Dalong Du,"Autonomous driving perceives its surroundings for decision making, which is one of the most complex scenarios in visual perception. The success of paradigm innovation in solving the 2D object detection task inspires us to seek an elegant, feasible, and scalable paradigm for fundamentally pushing the performance boundary in this area. To this end, we contribute the BEVDet paradigm in this paper. BEVDet performs 3D object detection in Bird-Eye-View (BEV), where most target values are defined and route planning can be handily performed. We merely reuse existing modules to build its framework but substantially develop its performance by constructing an exclusive data augmentation strategy and upgrading the Non-Maximum Suppression strategy. In the experiment, BEVDet offers an excellent trade-off between accuracy and time-efficiency. As a fast version, BEVDet-Tiny scores 31.2% mAP and 39.2% NDS on the nuScenes val set. It is comparable with FCOS3D, but requires just 11% computational budget of 215.3 GFLOPs and runs 9.2 times faster at 15.6 FPS. Another high-precision version dubbed BEVDet-Base scores 39.3% mAP and 47.2% NDS, significantly exceeding all published results. With a comparable inference speed, it surpasses FCOS3D by a large margin of +9.8% mAP and +10.0% NDS. The source code is publicly available for further research at https://github.com/HuangJunJie2017/BEVDet . △ Less","16 June, 2022",https://arxiv.org/pdf/2112.11790
"Satellite-Based Communications Security: A Survey of Threats, Solutions, and Research Challenges",Pietro Tedeschi;Savio Sciancalepore;Roberto Di Pietro,"Satellite-based Communication systems are gaining renewed momentum in Industry and Academia, thanks to innovative services introduced by leading tech companies and the promising impact they can deliver towards the global connectivity objective tackled by early 6G initiatives. On the one hand, the emergence of new manufacturing processes and radio technologies promises to reduce service costs while guaranteeing outstanding communication latency, available bandwidth, flexibility, and coverage range. On the other hand, cybersecurity techniques and solutions applied in SATCOM links should be updated to reflect the substantial advancements in attacker capabilities characterizing the last two decades. However, business urgency and opportunities are leading operators towards challenging system trade-offs, resulting in an increased attack surface and a general relaxation of the available security services. In this paper, we tackle the cited problems and present a comprehensive survey on the link-layer security threats, solutions, and challenges faced when deploying and operating SATCOM systems.Specifically, we classify the literature on security for SATCOM systems into two main branches, i.e., physical-layer security and cryptography schemes.Then, we further identify specific research domains for each of the identified branches, focusing on dedicated security issues, including, e.g., physical-layer confidentiality, anti-jamming schemes, anti-spoofing strategies, and quantum-based key distribution schemes. For each of the above domains, we highlight the most essential techniques, peculiarities, advantages, disadvantages, lessons learned, and future directions.Finally, we also identify emerging research topics whose additional investigation by Academia and Industry could further attract researchers and investors, ultimately unleashing the full potential behind ubiquitous satellite communications. △ Less","29 July, 2022",https://arxiv.org/pdf/2112.11324
RetroComposer: Composing Templates for Template-Based Retrosynthesis Prediction,Chaochao Yan;Peilin Zhao;Chan Lu;Yang Yu;Junzhou Huang,"The main target of retrosynthesis is to recursively decompose desired molecules into available building blocks. Existing template-based retrosynthesis methods follow a template selection stereotype and suffer from limited training templates, which prevents them from discovering novel reactions. To overcome this limitation, we propose an innovative retrosynthesis prediction framework that can compose novel templates beyond training templates. As far as we know, this is the first method that uses machine learning to compose reaction templates for retrosynthesis prediction. Besides, we propose an effective reactant candidate scoring model that can capture atom-level transformations, which helps our method outperform previous methods on the USPTO-50K dataset. Experimental results show that our method can produce novel templates for 15 USPTO-50K test reactions that are not covered by training templates. We have released our source implementation. △ Less","22 December, 2022",https://arxiv.org/pdf/2112.11225
Protograph Bit-Interleaved Coded Modulation: A Bandwidth-Efficient Design Paradigm for 6G Wireless Communications,Yi Fang;Pingping Chen;Yong Liang Guan;Francis C. M. Lau;Yonghui Li;Guanrong Chen,"Bit-interleaved coded modulation (BICM) has attracted considerable attention from the research community in the past three decades, because it can achieve desirable error performance with relatively low implementation complexity for a large number of communication and storage systems. By exploiting the iterative demapping and decoding (ID), the BICM is able to approach capacity limits of coded modulation over various channels. In recent years, protograph low-density parity-check (PLDPC) codes and their spatially-coupled (SC) variants have emerged to be a pragmatic forward-error-correction (FEC) solution for BICM systems due to their tremendous error-correction capability and simple structures, and found widespread applications such as deep-space communication, satellite communication, wireless communication, optical communication, and data storage. This article offers a comprehensive survey on the state-of-the-art development of PLDPC-BICM and its innovative SC variants over a variety of channel models, e.g., additive white Gaussian noise (AWGN) channels, fading channels, Poisson pulse position modulation (PPM) channels, and flash-memory channels. Of particular interest is code construction, constellation shaping, as well as bit-mapper design, where the receiver is formulated as a serially-concatenated decoding framework consisting of a soft-decision demapper and a belief-propagation decoder. Finally, several promising research directions are discussed, which have not been adequately addressed in the current literature. △ Less","27 October, 2022",https://arxiv.org/pdf/2112.08557
Smoothness and effective regularizations in learned embeddings for shape matching,Riccardo Marin;Souhaib Attaiki;Simone Melzi;Emanuele Rodolà;Maks Ovsjanikov,"Many innovative applications require establishing correspondences among 3D geometric objects. However, the countless possible deformations of smooth surfaces make shape matching a challenging task. Finding an embedding to represent the different shapes in high-dimensional space where the matching is easier to solve is a well-trodden path that has given many outstanding solutions. Recently, a new trend has shown advantages in learning such representations. This novel idea motivated us to investigate which properties differentiate these data-driven embeddings and which ones promote state-of-the-art results. In this study, we analyze, for the first time, properties that arise in data-driven learned embedding and their relation to the shape-matching task. Our discoveries highlight the close link between matching and smoothness, which naturally emerge from training. Also, we demonstrate the relation between the orthogonality of the embedding and the bijectivity of the correspondence. Our experiments show exciting results, overcoming well-established alternatives and shedding a different light on relevant contexts and properties for learned embeddings. △ Less","8 June, 2022",https://arxiv.org/pdf/2112.07289
Marvin: an Innovative Omni-Directional Robotic Assistant for Domestic Environments,Andrea Eirale;Mauro Martini;Luigi Tagliavini;Dario Gandini;Marcello Chiaberge;Giuseppe Quaglia,"Population ageing and pandemics recently demonstrate to cause isolation of elderly people in their houses, generating the need for a reliable assistive figure. Robotic assistants are the new frontier of innovation for domestic welfare, and elderly monitoring is one of the services a robot can handle for collective well-being. Despite these emerging needs, in the actual landscape of robotic assistants there are no platform which successfully combines a reliable mobility in cluttered domestic spaces, with lightweight and offline Artificial Intelligence (AI) solutions for perception and interaction. In this work, we present Marvin, a novel assistive robotic platform we developed with a modular layer-based architecture, merging a flexible mechanical design with cutting-edge AI for perception and vocal control. We focus the design of Marvin on three target service functions: monitoring of elderly and reduced-mobility subjects, remote presence and connectivity, and night assistance. Compared to previous works, we propose a tiny omnidirectional platform, which enables agile mobility and effective obstacle avoidance. Moreover, we design a controllable positioning device, which easily allows the user to access the interface for connectivity and extends the visual range of the camera sensor. Nonetheless, we delicately consider the privacy issues arising from private data collection on cloud services, a critical aspect of commercial AI-based assistants. To this end, we demonstrate how lightweight deep learning solutions for visual perception and vocal command can be adopted, completely running offline on the embedded hardware of the robot. △ Less","14 July, 2022",https://arxiv.org/pdf/2112.05597
CALVIN: A Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks,Oier Mees;Lukas Hermann;Erick Rosete-Beas;Wolfram Burgard,"General-purpose robots coexisting with humans in their environment must learn to relate human language to their perceptions and actions to be useful in a range of daily tasks. Moreover, they need to acquire a diverse repertoire of general-purpose skills that allow composing long-horizon tasks by following unconstrained language instructions. In this paper, we present CALVIN (Composing Actions from Language and Vision), an open-source simulated benchmark to learn long-horizon language-conditioned tasks. Our aim is to make it possible to develop agents that can solve many robotic manipulation tasks over a long horizon, from onboard sensors, and specified only via human language. CALVIN tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets and supports flexible specification of sensor suites. We evaluate the agents in zero-shot to novel language instructions and to novel environments and objects. We show that a baseline model based on multi-context imitation learning performs poorly on CALVIN, suggesting that there is significant room for developing innovative agents that learn to relate human language to their world models with this benchmark. △ Less","13 July, 2022",https://arxiv.org/pdf/2112.03227
Make It Move: Controllable Image-to-Video Generation with Text Descriptions,Yaosi Hu;Chong Luo;Zhenzhong Chen,"Generating controllable videos conforming to user intentions is an appealing yet challenging topic in computer vision. To enable maneuverable control in line with user intentions, a novel video generation task, named Text-Image-to-Video generation (TI2V), is proposed. With both controllable appearance and motion, TI2V aims at generating videos from a static image and a text description. The key challenges of TI2V task lie both in aligning appearance and motion from different modalities, and in handling uncertainty in text descriptions. To address these challenges, we propose a Motion Anchor-based video GEnerator (MAGE) with an innovative motion anchor (MA) structure to store appearance-motion aligned representation. To model the uncertainty and increase the diversity, it further allows the injection of explicit condition and implicit randomness. Through three-dimensional axial transformers, MA is interacted with given image to generate next frames recursively with satisfying controllability and diversity. Accompanying the new task, we build two new video-text paired datasets based on MNIST and CATER for evaluation. Experiments conducted on these datasets verify the effectiveness of MAGE and show appealing potentials of TI2V task. Source code for model and datasets will be available soon. △ Less","31 March, 2022",https://arxiv.org/pdf/2112.02815
PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car,Tina Chen;Taotao Jing;Renran Tian;Yaobin Chen;Joshua Domeyer;Heishiro Toyoda;Rini Sherony;Zhengming Ding,"Prediction of pedestrian behavior is critical for fully autonomous vehicles to drive in busy city streets safely and efficiently. The future autonomous cars need to fit into mixed conditions with not only technical but also social capabilities. As more algorithms and datasets have been developed to predict pedestrian behaviors, these efforts lack the benchmark labels and the capability to estimate the temporal-dynamic intent changes of the pedestrians, provide explanations of the interaction scenes, and support algorithms with social intelligence. This paper proposes and shares another benchmark dataset called the IUPUI-CSRC Pedestrian Situated Intent (PSI) data with two innovative labels besides comprehensive computer vision labels. The first novel label is the dynamic intent changes for the pedestrians to cross in front of the ego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one is the text-based explanations of the driver reasoning process when estimating pedestrian intents and predicting their behaviors during the interaction period. These innovative labels can enable several computer vision tasks, including pedestrian intent/behavior prediction, vehicle-pedestrian interaction segmentation, and video-to-language mapping for explainable algorithms. The released dataset can fundamentally improve the development of pedestrian behavior prediction models and develop socially intelligent autonomous cars to interact with pedestrians efficiently. The dataset has been evaluated with different tasks and is released to the public to access. △ Less","11 June, 2022",https://arxiv.org/pdf/2112.02604
SGM3D: Stereo Guided Monocular 3D Object Detection,Zheyuan Zhou;Liang Du;Xiaoqing Ye;Zhikang Zou;Xiao Tan;Li Zhang;Xiangyang Xue;Jianfeng Feng,"Monocular 3D object detection aims to predict the object location, dimension and orientation in 3D space alongside the object category given only a monocular image. It poses a great challenge due to its ill-posed property which is critically lack of depth information in the 2D image plane. While there exist approaches leveraging off-the-shelve depth estimation or relying on LiDAR sensors to mitigate this problem, the dependence on the additional depth model or expensive equipment severely limits their scalability to generic 3D perception. In this paper, we propose a stereo-guided monocular 3D object detection framework, dubbed SGM3D, adapting the robust 3D features learned from stereo inputs to enhance the feature for monocular detection. We innovatively present a multi-granularity domain adaptation (MG-DA) mechanism to exploit the network's ability to generate stereo-mimicking features given only on monocular cues. Coarse BEV feature-level, as well as the fine anchor-level domain adaptation, are both leveraged for guidance in the monocular domain.In addition, we introduce an IoU matching-based alignment (IoU-MA) method for object-level domain adaptation between the stereo and monocular predictions to alleviate the mismatches while adopting the MG-DA. Extensive experiments demonstrate state-of-the-art results on KITTI and Lyft datasets. △ Less","24 February, 2022",https://arxiv.org/pdf/2112.01914
Deep Depth from Focus with Differential Focus Volume,Fengting Yang;Xiaolei Huang;Zihan Zhou,"Depth-from-focus (DFF) is a technique that infers depth using the focus change of a camera. In this work, we propose a convolutional neural network (CNN) to find the best-focused pixels in a focal stack and infer depth from the focus estimation. The key innovation of the network is the novel deep differential focus volume (DFV). By computing the first-order derivative with the stacked features over different focal distances, DFV is able to capture both the focus and context information for focus analysis. Besides, we also introduce a probability regression mechanism for focus estimation to handle sparsely sampled focal stacks and provide uncertainty estimation to the final prediction. Comprehensive experiments demonstrate that the proposed model achieves state-of-the-art performance on multiple datasets with good generalizability and fast speed. △ Less","17 March, 2022",https://arxiv.org/pdf/2112.01712
HRNET: AI on Edge for mask detection and social distancing,Kinshuk Sengupta;Praveen Ranjan Srivastava,"The purpose of the paper is to provide innovative emerging technology framework for community to combat epidemic situations. The paper proposes a unique outbreak response system framework based on artificial intelligence and edge computing for citizen centric services to help track and trace people eluding safety policies like mask detection and social distancing measure in public or workplace setup. The framework further provides implementation guideline in industrial setup as well for governance and contact tracing tasks. The adoption will thus lead in smart city planning and development focusing on citizen health systems contributing to improved quality of life. The conceptual framework presented is validated through quantitative data analysis via secondary data collection from researcher's public websites, GitHub repositories and renowned journals and further benchmarking were conducted for experimental results in Microsoft Azure cloud environment. The study includes selective AI-models for benchmark analysis and were assessed on performance and accuracy in edge computing environment for large scale societal setup. Overall YOLO model Outperforms in object detection task and is faster enough for mask detection and HRNetV2 outperform semantic segmentation problem applied to solve social distancing task in AI-Edge inferencing environmental setup. The paper proposes new Edge-AI algorithm for building technology-oriented solutions for detecting mask in human movement and social distance. The paper enriches the technological advancement in artificial intelligence and edge-computing applied to problems in society and healthcare systems. The framework further equips government agency, system providers to design and constructs technology-oriented models in community setup to Increase the quality of life using emerging technologies into smart urban environments. △ Less","3 February, 2022",https://arxiv.org/pdf/2111.15208
Natural Language Processing in-and-for Design Research,L Siddharth;Lucienne T. M. Blessing;Jianxi Luo,"We review the scholarly contributions that utilise Natural Language Processing (NLP) techniques to support the design process. Using a heuristic approach, we gathered 223 articles that are published in 32 journals within the period 1991-present. We present state-of-the-art NLP in-and-for design research by reviewing these articles according to the type of natural language text sources: internal reports, design concepts, discourse transcripts, technical publications, consumer opinions, and others. Upon summarizing and identifying the gaps in these contributions, we utilise an existing design innovation framework to identify the applications that are currently being supported by NLP. We then propose a few methodological and theoretical directions for future NLP in-and-for design research. △ Less","2 July, 2022",https://arxiv.org/pdf/2111.13827
MAE-DET: Revisiting Maximum Entropy Principle in Zero-Shot NAS for Efficient Object Detection,Zhenhong Sun;Ming Lin;Xiuyu Sun;Zhiyu Tan;Hao Li;Rong Jin,"In object detection, the detection backbone consumes more than half of the overall inference cost. Recent researches attempt to reduce this cost by optimizing the backbone architecture with the help of Neural Architecture Search (NAS). However, existing NAS methods for object detection require hundreds to thousands of GPU hours of searching, making them impractical in fast-paced research and development. In this work, we propose a novel zero-shot NAS method to address this issue. The proposed method, named MAE-DET, automatically designs efficient detection backbones via the Maximum Entropy Principle without training network parameters, reducing the architecture design cost to nearly zero yet delivering the state-of-the-art (SOTA) performance. Under the hood, MAE-DET maximizes the differential entropy of detection backbones, leading to a better feature extractor for object detection under the same computational budgets. After merely one GPU day of fully automatic design, MAE-DET innovates SOTA detection backbones on multiple detection benchmark datasets with little human intervention. Comparing to ResNet-50 backbone, MAE-DET is +2.0\% better in mAP when using the same amount of FLOPs/parameters, and is 1.54 times faster on NVIDIA V100 at the same mAP. Code and pre-trained models are available at https://github.com/alibaba/lightweight-neuralarchitecture-search. △ Less","15 June, 2022",https://arxiv.org/pdf/2111.13336
Scalable Learning for Optimal Load Shedding Under Power Grid Emergency Operations,Yuqi Zhou;Jeehyun Park;Hao Zhu,"Effective and timely responses to unexpected contingencies are crucial for enhancing the resilience of power grids. Given the fast, complex process of cascading propagation, corrective actions such as optimal load shedding (OLS) are difficult to attain in large-scale networks due to the computation complexity and communication latency issues. This work puts forth an innovative learning-for-OLS approach by constructing the optimal decision rules of load shedding under a variety of potential contingency scenarios through offline neural network (NN) training. Notably, the proposed NN-based OLS decisions are fully decentralized, enabling individual load centers to quickly react to the specific contingency using readily available local measurements. Numerical studies on the IEEE 14-bus system have demonstrated the effectiveness of our scalable OLS design for real-time responses to severe grid emergency events. △ Less","26 January, 2022",https://arxiv.org/pdf/2111.11980
Parallel Logic Programming: A Sequel,Agostino Dovier;Andrea Formisano;Gopal Gupta;Manuel V. Hermenegildo;Enrico Pontelli;Ricardo Rocha,"Multi-core and highly-connected architectures have become ubiquitous, and this has brought renewed interest in language-based approaches to the exploitation of parallelism. Since its inception, logic programming has been recognized as a programming paradigm with great potential for automated exploitation of parallelism. The comprehensive survey of the first twenty years of research in parallel logic programming, published in 2001, has served since as a fundamental reference to researchers and developers. The contents are quite valid today, but at the same time the field has continued evolving at a fast pace in the years that have followed. Many of these achievements and ongoing research have been driven by the rapid pace of technological innovation, that has led to advances such as very large clusters, the wide diffusion of multi-core processors, the game-changing role of general-purpose graphic processing units, and the ubiquitous adoption of cloud computing. This has been paralleled by significant advances within logic programming, such as tabling, more powerful static analysis and verification, the rapid growth of Answer Set Programming, and in general, more mature implementations and systems. This survey provides a review of the research in parallel logic programming covering the period since 2001, thus providing a natural continuation of the previous survey. The goal of the survey is to serve not only as a reference for researchers and developers of logic programming systems, but also as engaging reading for anyone interested in logic and as a useful source for researchers in parallel systems outside logic programming. Under consideration in Theory and Practice of Logic Programming (TPLP). △ Less","24 January, 2022",https://arxiv.org/pdf/2111.11218
On the Existence of Universal Lottery Tickets,Rebekka Burkholz;Nilanjana Laha;Rajarshi Mukherjee;Alkis Gotovos,"The lottery ticket hypothesis conjectures the existence of sparse subnetworks of large randomly initialized deep neural networks that can be successfully trained in isolation. Recent work has experimentally observed that some of these tickets can be practically reused across a variety of tasks, hinting at some form of universality. We formalize this concept and theoretically prove that not only do such universal tickets exist but they also do not require further training. Our proofs introduce a couple of technical innovations related to pruning for strong lottery tickets, including extensions of subset sum results and a strategy to leverage higher amounts of depth. Our explicit sparse constructions of universal function families might be of independent interest, as they highlight representational benefits induced by univariate convolutional architectures. △ Less","16 March, 2022",https://arxiv.org/pdf/2111.11146
Physics-informed neural networks for solving thermo-mechanics problems of functionally graded material,Mayank Raj;Pramod Kumbhar;Ratna Kumar Annabattula,"Differential equations are indispensable to engineering and hence to innovation. In recent years, physics-informed neural networks (PINN) have emerged as a novel method for solving differential equations. PINN method has the advantage of being meshless, scalable, and can potentially be intelligent in terms of transferring the knowledge learned from solving one differential equation to the other. The exploration in this field has majorly been limited to solving linear-elasticity problems, crack propagation problems. This study uses PINNs to solve coupled thermo-mechanics problems of materials with functionally graded properties. An in-depth analysis of the PINN framework has been carried out by understanding the training datasets, model architecture, and loss functions. The efficacy of the PINN models in solving thermo-mechanics differential equations has been measured by comparing the obtained solutions either with analytical solutions or finite element method-based solutions. While R2 score of more than 99% has been achieved in predicting primary variables such as displacement and temperature fields, achieving the same for secondary variables such as stress turns out to be more challenging. This study is the first to implement the PINN framework for solving coupled thermo-mechanics problems on composite materials. This study is expected to enhance the understanding of the novel PINN framework and will be seminal for further research on PINNs. △ Less","6 January, 2022",https://arxiv.org/pdf/2111.10751
Triples-to-Text Generation with Reinforcement Learning Based Graph-augmented Neural Networks,Hanning Gao;Lingfei Wu;Hongyun Zhang;Zhihua Wei;Po Hu;Fangli Xu;Bo Long,"Considering a collection of RDF triples, the RDF-to-text generation task aims to generate a text description. Most previous methods solve this task using a sequence-to-sequence model or using a graph-based model to encode RDF triples and to generate a text sequence. Nevertheless, these approaches fail to clearly model the local and global structural information between and within RDF triples. Moreover, the previous methods also face the non-negligible problem of low faithfulness of the generated text, which seriously affects the overall performance of these models. To solve these problems, we propose a model combining two new graph-augmented structural neural encoders to jointly learn both local and global structural information in the input RDF triples. To further improve text faithfulness, we innovatively introduce a reinforcement learning (RL) reward based on information extraction (IE). We first extract triples from the generated text using a pretrained IE model and regard the correct number of the extracted triples as the additional RL reward. Experimental results on two benchmark datasets demonstrate that our proposed model outperforms the state-of-the-art baselines, and the additional reinforcement learning reward does help to improve the faithfulness of the generated text. △ Less","23 March, 2022",https://arxiv.org/pdf/2111.10545
ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data,Gilad Baruch;Zhuoyuan Chen;Afshin Dehghan;Tal Dimry;Yuri Feigin;Peter Fu;Thomas Gebauer;Brandon Joffe;Daniel Kurz;Arik Schwartz;Elad Shulman,"Scene understanding is an active research area. Commercial depth sensors, such as Kinect, have enabled the release of several RGB-D datasets over the past few years which spawned novel methods in 3D scene understanding. More recently with the launch of the LiDAR sensor in Apple's iPads and iPhones, high quality RGB-D data is accessible to millions of people on a device they commonly use. This opens a whole new era in scene understanding for the Computer Vision community as well as app developers. The fundamental research in scene understanding together with the advances in machine learning can now impact people's everyday experiences. However, transforming these scene understanding methods to real-world experiences requires additional innovation and development. In this paper we introduce ARKitScenes. It is not only the first RGB-D dataset that is captured with a now widely available depth sensor, but to our best knowledge, it also is the largest indoor scene understanding data released. In addition to the raw and processed data from the mobile device, ARKitScenes includes high resolution depth maps captured using a stationary laser scanner, as well as manually labeled 3D oriented bounding boxes for a large taxonomy of furniture. We further analyze the usefulness of the data for two downstream tasks: 3D object detection and color-guided depth upsampling. We demonstrate that our dataset can help push the boundaries of existing state-of-the-art methods and it introduces new challenges that better represent real-world scenarios. △ Less","12 January, 2022",https://arxiv.org/pdf/2111.08897
INTERN: A New Learning Paradigm Towards General Vision,Jing Shao;Siyu Chen;Yangguang Li;Kun Wang;Zhenfei Yin;Yinan He;Jianing Teng;Qinghong Sun;Mengya Gao;Jihao Liu;Gengshi Huang;Guanglu Song;Yichao Wu;Yuming Huang;Fenggang Liu;Huan Peng;Shuo Qin;Chengyu Wang;Yujie Wang;Conghui He;Ding Liang;Yu Liu;Fengwei Yu;Junjie Yan;Dahua Lin,"Enormous waves of technological innovations over the past several years, marked by the advances in AI technologies, are profoundly reshaping the industry and the society. However, down the road, a key challenge awaits us, that is, our capability of meeting rapidly-growing scenario-specific demands is severely limited by the cost of acquiring a commensurate amount of training data. This difficult situation is in essence due to limitations of the mainstream learning paradigm: we need to train a new model for each new scenario, based on a large quantity of well-annotated data and commonly from scratch. In tackling this fundamental problem, we move beyond and develop a new learning paradigm named INTERN. By learning with supervisory signals from multiple sources in multiple stages, the model being trained will develop strong generalizability. We evaluate our model on 26 well-known datasets that cover four categories of tasks in computer vision. In most cases, our models, adapted with only 10% of the training data in the target domain, outperform the counterparts trained with the full set of data, often by a significant margin. This is an important step towards a promising prospect where such a model with general vision capability can dramatically reduce our reliance on data, thus expediting the adoption of AI technologies. Furthermore, revolving around our new paradigm, we also introduce a new data system, a new architecture, and a new benchmark, which, together, form a general vision ecosystem to support its future development in an open and inclusive manner. See project website at https://opengvlab.shlab.org.cn . △ Less","24 February, 2022",https://arxiv.org/pdf/2111.08687
Sequential Aggregation and Rematerialization: Distributed Full-batch Training of Graph Neural Networks on Large Graphs,Hesham Mostafa,"We present the Sequential Aggregation and Rematerialization (SAR) scheme for distributed full-batch training of Graph Neural Networks (GNNs) on large graphs. Large-scale training of GNNs has recently been dominated by sampling-based methods and methods based on non-learnable message passing. SAR on the other hand is a distributed technique that can train any GNN type directly on an entire large graph. The key innovation in SAR is the distributed sequential rematerialization scheme which sequentially re-constructs then frees pieces of the prohibitively large GNN computational graph during the backward pass. This results in excellent memory scaling behavior where the memory consumption per worker goes down linearly with the number of workers, even for densely connected graphs. Using SAR, we report the largest applications of full-batch GNN training to-date, and demonstrate large memory savings as the number of workers increases. We also present a general technique based on kernel fusion and attention-matrix rematerialization to optimize both the runtime and memory efficiency of attention-based models. We show that, coupled with SAR, our optimized attention kernels lead to significant speedups and memory savings in attention-based GNNs.We made the SAR GNN training library publicy available: \url{https://github.com/IntelLabs/SAR}. △ Less","15 April, 2022",https://arxiv.org/pdf/2111.06483
Edge-Cloud Polarization and Collaboration: A Comprehensive Survey for AI,Jiangchao Yao;Shengyu Zhang;Yang Yao;Feng Wang;Jianxin Ma;Jianwei Zhang;Yunfei Chu;Luo Ji;Kunyang Jia;Tao Shen;Anpeng Wu;Fengda Zhang;Ziqi Tan;Kun Kuang;Chao Wu;Fei Wu;Jingren Zhou;Hongxia Yang,"Influenced by the great success of deep learning via cloud computing and the rapid development of edge chips, research in artificial intelligence (AI) has shifted to both of the computing paradigms, i.e., cloud computing and edge computing. In recent years, we have witnessed significant progress in developing more advanced AI models on cloud servers that surpass traditional deep learning models owing to model innovations (e.g., Transformers, Pretrained families), explosion of training data and soaring computing capabilities. However, edge computing, especially edge and cloud collaborative computing, are still in its infancy to announce their success due to the resource-constrained IoT scenarios with very limited algorithms deployed. In this survey, we conduct a systematic review for both cloud and edge AI. Specifically, we are the first to set up the collaborative learning mechanism for cloud and edge modeling with a thorough review of the architectures that enable such mechanism. We also discuss potentials and practical experiences of some on-going advanced edge AI topics including pretraining models, graph neural networks and reinforcement learning. Finally, we discuss the promising directions and challenges in this field. △ Less","23 May, 2022",https://arxiv.org/pdf/2111.06061
An Instance-Dependent Analysis for the Cooperative Multi-Player Multi-Armed Bandit,Aldo Pacchiano;Peter Bartlett;Michael I. Jordan,"We study the problem of information sharing and cooperation in Multi-Player Multi-Armed bandits. We propose the first algorithm that achieves logarithmic regret for this problem when the collision reward is unknown. Our results are based on two innovations. First, we show that a simple modification to a successive elimination strategy can be used to allow the players to estimate their suboptimality gaps, up to constant factors, in the absence of collisions. Second, we leverage the first result to design a communication protocol that successfully uses the small reward of collisions to coordinate among players, while preserving meaningful instance-dependent logarithmic regret guarantees. △ Less","30 September, 2022",https://arxiv.org/pdf/2111.04873
"Twitter Big Data as a Resource for Exoskeleton Research: A Large-Scale Dataset of about 140,000 Tweets and 100 Research Questions",Nirmalya Thakur,"The exoskeleton technology has been rapidly advancing in the recent past due to its multitude of applications and diverse use-cases in assisted living, military, healthcare, firefighting, and industry 4.0. The exoskeleton market is projected to increase by multiple times of its current value within the next two years. Therefore, it is crucial to study the degree and trends of user interest, views, opinions, perspectives, attitudes, acceptance, feedback, engagement, buying behavior, and satisfaction, towards exoskeletons, for which the availability of Big Data of conversations about exoskeletons is necessary. The Internet of Everything style of today's living, characterized by people spending more time on the internet than ever before, with a specific focus on social media platforms, holds the potential for the development of such a dataset by the mining of relevant social media conversations. Twitter, one such social media platform, is highly popular amongst all age groups, where the topics found in the conversation paradigms include emerging technologies such as exoskeletons. To address this research challenge, this work makes two scientific contributions to this field. First, it presents an open-access dataset of about 140,000 tweets about exoskeletons that were posted in a 5-year period from May 21, 2017, to May 21, 2022. Second, based on a comprehensive review of the recent works in the fields of Big Data, Natural Language Processing, Information Retrieval, Data Mining, Pattern Recognition, and Artificial Intelligence that may be applied to relevant Twitter data for advancing research, innovation, and discovery in the field of exoskeleton research, a total of 100 Research Questions are presented for researchers to study, analyze, evaluate, ideate, and investigate based on this dataset. △ Less","20 July, 2022",https://arxiv.org/pdf/2111.04476
Computing Simple Mechanisms: Lift-and-Round over Marginal Reduced Forms,Yang Cai;Argyris Oikonomou;Mingfei Zhao,"We study revenue maximization in multi-item multi-bidder auctions under the natural item-independence assumption - a classical problem in Multi-Dimensional Bayesian Mechanism Design. One of the biggest challenges in this area is developing algorithms to compute (approximately) optimal mechanisms that are not brute-force in the size of the bidder type space, which is usually exponential in the number of items in multi-item auctions. Unfortunately, such algorithms were only known for basic settings of our problem when bidders have unit-demand [CHMS10,CMS15] or additive valuations [Yao15]. In this paper, we significantly improve the previous results and design the first algorithm that runs in time polynomial in the number of items and the number of bidders to compute mechanisms that are O(1)-approximations to the optimal revenue when bidders have XOS valuations, resolving an open problem raised in [CM16,CZ17]. Moreover, the computed mechanism has a simple structure: It is either a posted price mechanism or a two-part tariff mechanism. As a corollary of our result, we show how to compute an approximately optimal and simple mechanism efficiently using only sample access to the bidders' value distributions. Our algorithm builds on two innovations that allow us to search over the space of mechanisms efficiently: (i) a new type of succinct representation of mechanisms - the marginal reduced forms, and (ii) a novel Lift-and-Round procedure that concavifies the problem. △ Less","10 April, 2022",https://arxiv.org/pdf/2111.03962
Collage: Seamless Integration of Deep Learning Backends with Automatic Placement,Byungsoo Jeon;Sunghyun Park;Peiyuan Liao;Sheng Xu;Tianqi Chen;Zhihao Jia,"The strong demand for efficient and performant deployment of Deep Learning (DL) applications prompts the rapid development of a rich DL ecosystem. To keep up with this fast advancement, it is crucial for modern DL frameworks to efficiently integrate a variety of optimized tensor algebra libraries and runtimes as their backends and generate the fastest possible executable using these backends. However, current DL frameworks require significant manual effort and expertise to integrate every new backend while failing to unleash its full potential. Given the fast-evolving nature of the DL ecosystem, this manual approach often slows down continuous innovations across different layers; it prevents hardware vendors from the fast deployment of their cutting-edge libraries, DL framework developers must repeatedly adjust their hand-coded rules to accommodate new versions of libraries, and machine learning practitioners need to wait for the integration of new technologies and often encounter unsatisfactory performance. In this paper, we propose Collage, a DL framework that offers seamless integration of DL backends. Collage provides an expressive backend registration interface that allows users to precisely specify the capability of various backends. By leveraging the specifications of available backends, Collage automatically searches for an optimized backend placement strategy for a given workload and execution environment. Our evaluation shows that Collage outperforms the best existing framework for each hardware by 1.26\times, 1.43\times, 1.40\times on average on NVIDIA's RTX 2070 GPU, V100 GPU, and Intel's Xeon 8259CL CPU, respectively. Collage has been open-sourced and deployed in Apache TVM. △ Less","27 October, 2022",https://arxiv.org/pdf/2111.00655
Technology Fitness Landscape for Design Innovation: A Deep Neural Embedding Approach Based on Patent Data,Shuo Jiang;Jianxi Luo,"Technology is essential to innovation and economic prosperity. Understanding technological changes can guide innovators to find new directions of design innovation and thus make breakthroughs. In this work, we construct a technology fitness landscape via deep neural embeddings of patent data. The landscape consists of 1,757 technology domains and their respective improvement rates. In the landscape, we found a high hill related to information and communication technologies (ICT) and a vast low plain of the remaining domains. The landscape presents a bird's eye view of the structure of the total technology space, providing a new way for innovators to interpret technology evolution with a biological analogy, and a biologically-inspired inference to the next innovation. △ Less","21 October, 2022",https://arxiv.org/pdf/2110.13624
"Integration of Blockchain and Auction Models: A Survey, Some Applications, and Challenges",Zeshun Shi;Cees de Laat;Paola Grosso;Zhiming Zhao,"In recent years, blockchain has gained widespread attention as an emerging technology for decentralization, transparency, and immutability in advancing online activities over public networks. As an essential market process, auctions have been well studied and applied in many business fields due to their efficiency and contributions to fair trade. Complementary features between blockchain and auction models trigger a great potential for research and innovation. On the one hand, the decentralized nature of blockchain can provide a trustworthy, secure, and cost-effective mechanism to manage the auction process; on the other hand, auction models can be utilized to design incentive and consensus protocols in blockchain architectures. These opportunities have attracted enormous research and innovation activities in both academia and industry; however, there is a lack of an in-depth review of existing solutions and achievements. In this paper, we conduct a comprehensive state-of-the-art survey of these two research topics. We review the existing solutions for integrating blockchain and auction models, with some application-oriented taxonomies generated. Additionally, we highlight some open research challenges and future directions towards integrated blockchain-auction models. △ Less","28 November, 2022",https://arxiv.org/pdf/2110.12534
SOSP: Efficiently Capturing Global Correlations by Second-Order Structured Pruning,Manuel Nonnenmacher;Thomas Pfeil;Ingo Steinwart;David Reeb,"Pruning neural networks reduces inference time and memory costs. On standard hardware, these benefits will be especially prominent if coarse-grained structures, like feature maps, are pruned. We devise two novel saliency-based methods for second-order structured pruning (SOSP) which include correlations among all structures and layers. Our main method SOSP-H employs an innovative second-order approximation, which enables saliency evaluations by fast Hessian-vector products. SOSP-H thereby scales like a first-order method despite taking into account the full Hessian. We validate SOSP-H by comparing it to our second method SOSP-I that uses a well-established Hessian approximation, and to numerous state-of-the-art methods. While SOSP-H performs on par or better in terms of accuracy, it has clear advantages in terms of scalability and efficiency. This allowed us to scale SOSP-H to large-scale vision tasks, even though it captures correlations across all layers of the network. To underscore the global nature of our pruning methods, we evaluate their performance not only by removing structures from a pretrained network, but also by detecting architectural bottlenecks. We show that our algorithms allow to systematically reveal architectural bottlenecks, which we then remove to further increase the accuracy of the networks. △ Less","30 June, 2022",https://arxiv.org/pdf/2110.11395
Temporal Motifs in Patent Opposition and Collaboration Networks,Penghang Liu;Naoki Masuda;Tomomi Kito;A. Erdem Sarıyüce,"Patents are intellectual properties that reflect innovative activities of companies and organizations. The literature is rich with the studies that analyze the citations among the patents and the collaboration relations among companies that own the patents. However, the adversarial relations between the patent owners are not as well investigated. One proxy to model such relations is the patent opposition, which is a legal activity in which a company challenges the validity of a patent. Characterizing the patent oppositions, collaborations, and the interplay between them can help better understand the companies' business strategies. Temporality matters in this context as the order and frequency of oppositions and collaborations characterize their interplay. In this study, we construct a two-layer temporal network to model the patent oppositions and collaborations among the companies. We utilize temporal motifs to analyze the oppositions and collaborations from structural and temporal perspectives. We first characterize the frequent motifs in patent oppositions and investigate how often the companies of different sizes attack other companies. We show that large companies tend to engage in opposition with multiple companies. Then we analyze the temporal interplay between collaborations and oppositions. We find that two adversarial companies are more likely to collaborate in the future than two collaborating companies oppose each other in the future. △ Less","4 February, 2022",https://arxiv.org/pdf/2110.11198
Evaluating and Improving Interactions with Hazy Oracles,Stephan J. Lemmer;Jason J. Corso,"Many AI systems integrate sensor inputs, world knowledge, and human-provided information to perform inference. While such systems often treat the human input as flawless, humans are better thought of as hazy oracles whose input may be ambiguous or outside of the AI system's understanding. In such situations it makes sense for the AI system to defer its inference while it disambiguates the human-provided information by, for example, asking the human to rephrase the query. Though this approach has been considered in the past, current work is typically limited to application-specific methods and non-standardized human experiments. We instead introduce and formalize a general notion of deferred inference. Using this formulation, we then propose a novel evaluation centered around the Deferred Error Volume (DEV) metric, which explicitly considers the tradeoff between error reduction and the additional human effort required to achieve it. We demonstrate this new formalization and an innovative deferred inference method on the disparate tasks of Single-Target Video Object Tracking and Referring Expression Comprehension, ultimately reducing error by up to 48% without any change to the underlying model or its parameters. △ Less","30 November, 2022",https://arxiv.org/pdf/2110.10206
Theoretical Advances in Current Estimation and Navigation from a Glider-Based Acoustic Doppler Current Profiler (ADCP),Jacob Stevens-Haas;Sarah E. Webster;Aleksandr Aravkin,"We examine acoustic Doppler current profiler (ADCP) measurements from underwater gliders to determine glider position, glider velocity, and subsurface current. ADCPs, however, do not directly observe the quantities of interest; instead, they measure the relative motion of the vehicle and the water column. We examine the lineage of mathematical innovations that have previously been applied to this problem, discovering an unstated but incorrect assumption of independence. We reframe a recent method to form a joint probability model of current and vehicle navigation, which allows us to correct this assumption and extend the classic Kalman smoothing method. Detailed simulations affirm the efficacy of our approach for computing estimates and their uncertainty. The joint model developed here sets the stage for future work to incorporate constraints, range measurements, and robust statistical modeling. △ Less","20 July, 2022",https://arxiv.org/pdf/2110.10199
Scalable Learning Environments for Teaching Cybersecurity Hands-on,Jan Vykopal;Pavel Čeleda;Pavel Seda;Valdemar Švábenský;Daniel Tovarňák,"This Innovative Practice full paper describes a technical innovation for scalable teaching of cybersecurity hands-on classes using interactive learning environments. Hands-on experience significantly improves the practical skills of learners. However, the preparation and delivery of hands-on classes usually do not scale. Teaching even small groups of students requires a substantial effort to prepare the class environment and practical assignments. Further issues are associated with teaching large classes, providing feedback, and analyzing learning gains. We present our research effort and practical experience in designing and using learning environments that scale up hands-on cybersecurity classes. The environments support virtual networks with full-fledged operating systems and devices that emulate real-world systems. (...) Using the presented environments KYPO Cyber Range Platform and Cyber Sandbox Creator, we delivered the classes on-site or remotely for various target groups of learners (K-12, university students, and professional learners). The learners value the realistic nature of the environments that enable exercising theoretical concepts and tools. The instructors value time-efficiency when preparing and deploying the hands-on activities. Engineering and computing educators can freely use our software, which we have released under an open-source license. We also provide detailed documentation and exemplary hands-on training to help other educators adopt our teaching innovations and enable sharing of reusable components within the community. △ Less","5 January, 2022",https://arxiv.org/pdf/2110.10004
Visualizing Collective Idea Generation and Innovation Processes in Social Networks,Yiding Cao;Yingjun Dong;Minjun Kim;Neil G. MacLaren;Sriniwas Pandey;Shelley D. Dionne;Francis J. Yammarino;Hiroki Sayama,"Collective idea generation and innovation processes are complex and dynamic, involving a large amount of qualitative narrative information that is difficult to monitor, analyze, and visualize using traditional methods. In this study, we developed three new visualization methods for collective idea generation and innovation processes and applied them to data from online social network experiments. The first visualization is the Idea Cloud, which helps monitor collective idea posting activity and intuitively tracks idea clustering and transition. The second visualization is the Idea Geography, which helps understand how the idea space and its utility landscape are structured and how collaboration was performed in that space. The third visualization is the Idea Network, which connects idea dynamics with the social structure of the people who generated them, displaying how social influence among neighbors may have affected collaborative activities and where innovative ideas arose and spread in the social network. △ Less","16 July, 2022",https://arxiv.org/pdf/2110.09893
Improving Emotional Speech Synthesis by Using SUS-Constrained VAE and Text Encoder Aggregation,Fengyu Yang;Jian Luan;Yujun Wang,"Learning emotion embedding from reference audio is a straightforward approach for multi-emotion speech synthesis in encoder-decoder systems. But how to get better emotion embedding and how to inject it into TTS acoustic model more effectively are still under investigation. In this paper, we propose an innovative constraint to help VAE extract emotion embedding with better cluster cohesion. Besides, the obtained emotion embedding is used as query to aggregate latent representations of all encoder layers via attention. Moreover, the queries from encoder layers themselves are also helpful. Experiments prove the proposed methods can enhance the encoding of comprehensive syntactic and semantic information and produce more expressive emotional speech. △ Less","28 January, 2022",https://arxiv.org/pdf/2110.09780
EILEEN: A recommendation system for scientific publications and grants,Daniel E. Acuna;Kartik Nagre;Priya Matnani,"Finding relevant scientific articles is crucial for advancing knowledge. Recommendation systems are helpful for such purpose, although they have only been applied to science recently. This article describes EILEEN (Exploratory Innovator of LitEraturE Networks), a recommendation system for scientific publications and grants with open source code and datasets. We describe EILEEN's architecture for ingesting and processing documents and modeling the recommendation system and keyphrase estimator. Using a unique dataset of log-in user behavior, we validate our recommendation system against Latent Semantic Analysis (LSA) and the standard ranking from Elasticsearch (Lucene scoring). We find that a learning-to-rank with Random Forest achieves an AUC of 0.9, significantly outperforming both baselines. Our results suggest that we can substantially improve science recommendations and learn about scientists' behavior through their search behavior. We make our system available through eileen.io △ Less","23 March, 2022",https://arxiv.org/pdf/2110.09663
Hyperseed: Unsupervised Learning with Vector Symbolic Architectures,Evgeny Osipov;Sachin Kahawala;Dilantha Haputhanthri;Thimal Kempitiya;Daswin De Silva;Damminda Alahakoon;Denis Kleyko,"Motivated by recent innovations in biologically-inspired neuromorphic hardware, this article presents a novel unsupervised machine learning algorithm named Hyperseed that draws on the principles of Vector Symbolic Architectures (VSA) for fast learning of a topology preserving feature map of unlabelled data. It relies on two major operations of VSA, binding and bundling. The algorithmic part of Hyperseed is expressed within Fourier Holographic Reduced Representations model, which is specifically suited for implementation on spiking neuromorphic hardware. The two primary contributions of the Hyperseed algorithm are, few-shot learning and a learning rule based on single vector operation. These properties are empirically evaluated on synthetic datasets as well as on illustrative benchmark use-cases, IRIS classification, and a language identification task using n-gram statistics. The results of these experiments confirm the capabilities of Hyperseed and its applications in neuromorphic hardware. △ Less","29 September, 2022",https://arxiv.org/pdf/2110.08343
Trigger Hunting with a Topological Prior for Trojan Detection,Xiaoling Hu;Xiao Lin;Michael Cogswell;Yi Yao;Susmit Jha;Chao Chen,"Despite their success and popularity, deep neural networks (DNNs) are vulnerable when facing backdoor attacks. This impedes their wider adoption, especially in mission critical applications. This paper tackles the problem of Trojan detection, namely, identifying Trojaned models -- models trained with poisoned data. One popular approach is reverse engineering, i.e., recovering the triggers on a clean image by manipulating the model's prediction. One major challenge of reverse engineering approach is the enormous search space of triggers. To this end, we propose innovative priors such as diversity and topological simplicity to not only increase the chances of finding the appropriate triggers but also improve the quality of the found triggers. Moreover, by encouraging a diverse set of trigger candidates, our method can perform effectively in cases with unknown target labels. We demonstrate that these priors can significantly improve the quality of the recovered triggers, resulting in substantially improved Trojan detection accuracy as validated on both synthetic and publicly available TrojAI benchmarks. △ Less","2 April, 2022",https://arxiv.org/pdf/2110.08335
Why don't people use character-level machine translation?,Jindřich Libovický;Helmut Schmid;Alexander Fraser,"We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time. △ Less","27 April, 2022",https://arxiv.org/pdf/2110.08191
Domain Adaptive Semantic Segmentation via Regional Contrastive Consistency Regularization,Qianyu Zhou;Chuyun Zhuang;Ran Yi;Xuequan Lu;Lizhuang Ma,"Unsupervised domain adaptation (UDA) for semantic segmentation has been well-studied in recent years. However, most existing works largely neglect the local regional consistency across different domains and are less robust to changes in outdoor environments. In this paper, we propose a novel and fully end-to-end trainable approach, called regional contrastive consistency regularization (RCCR) for domain adaptive semantic segmentation. Our core idea is to pull the similar regional features extracted from the same location of different images, i.e., the original image and augmented image, to be closer, and meanwhile push the features from the different locations of the two images to be separated. We innovatively propose a region-wise contrastive loss with two sampling strategies to realize effective regional consistency. Besides, we present momentum projection heads, where the teacher projection head is the exponential moving average of the student. Finally, a memory bank mechanism is designed to learn more robust and stable region-wise features under varying environments. Extensive experiments on two common UDA benchmarks, i.e., GTAV to Cityscapes and SYNTHIA to Cityscapes, demonstrate that our approach outperforms the state-of-the-art methods. △ Less","11 September, 2022",https://arxiv.org/pdf/2110.05170
A New Innovation Concept on End user Contextual and Behavioural Perspectives,Reem Aman;Shah J. Miah;Janet Dzator,"The phenomenon of innovation has been shifting away from focusing on tangible to intangible modernization with its vitalizing context. This shift appears vitally in innovation developed by individual end-users in organizations and societies, including the exploration of the intangible end-user innovation existence and impact in the household sector on a national scale. Some examples of intangible end-user innovation include technique, service, and user behavior. Although, there is a variety of intangible end-user innovation discussed in the literature, limited understanding is existed for constructing an efficient and comprehensive typology, which encompasses the nature of this innovation phenomenon. This research study explores this original phenomenon for proposing a new concept that will act as an overarching descriptor of innovation types: idea, object, and behavior. This proposed concept, relating to intangible innovation, will explain the sequence within one or many connected intangible activities that provide novelty to its end-user relative to previous activities and practices. Using a design science research approach, the study comprises two goals: a) identifying opportunities and issues to measure intangible inputs to the innovation and b) proposing a framework for extending the existing innovation theories that to better capture intangible end-user innovation and its diffusion insights in their online environment across nations △ Less","25 February, 2022",https://arxiv.org/pdf/2110.04539
Towards Lightweight Applications: Asymmetric Enroll-Verify Structure for Speaker Verification,Qingjian Lin;Lin Yang;Xuyang Wang;Xiaoyi Qin;Junjie Wang;Ming Li,"With the development of deep learning, automatic speaker verification has made considerable progress over the past few years. However, to design a lightweight and robust system with limited computational resources is still a challenging problem. Traditionally, a speaker verification system is symmetrical, indicating that the same embedding extraction model is applied for both enrollment and verification in inference. In this paper, we come up with an innovative asymmetric structure, which takes the large-scale ECAPA-TDNN model for enrollment and the small-scale ECAPA-TDNNLite model for verification. As a symmetrical system, our proposed ECAPA-TDNNLite model achieves an EER of 3.07% on the Voxceleb1 original test set with only 11.6M FLOPS. Moreover, the asymmetric structure further reduces the EER to 2.31%, without increasing any computational costs during verification. △ Less","25 January, 2022",https://arxiv.org/pdf/2110.04438
Pose Refinement with Joint Optimization of Visual Points and Lines,Shuang Gao;Jixiang Wan;Yishan Ping;Xudong Zhang;Shuzhou Dong;Yuchen Yang;Haikuan Ning;Jijunnan Li;Yandong Guo,"High-precision camera re-localization technology in a pre-established 3D environment map is the basis for many tasks, such as Augmented Reality, Robotics and Autonomous Driving. The point-based visual re-localization approaches are well-developed in recent decades, but are insufficient in some feature-less cases. In this paper, we design a complete pipeline for camera pose refinement with points and lines, which contains the innovatively designed line extracting CNN named VLSE, the line matching and the pose optimization approaches. We adopt a novel line representation and customize a hybrid convolution block based on the Stacked Hourglass network, to detect accurate and stable line features on images. Then we apply a geometric-based strategy to obtain precise 2D-3D line correspondences using epipolar constraint and reprojection filtering. A following point-line joint cost function is constructed to optimize the camera pose with the initial coarse pose from the pure point-based localization. Sufficient experiments are conducted on open datasets, i.e, line extractor on Wireframe and YorkUrban, localization performance on InLoc duc1 and duc2, to confirm the effectiveness of our point-line joint pose optimization method. △ Less","26 July, 2022",https://arxiv.org/pdf/2110.03940
Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data,Jiaxing Huang;Dayan Guan;Aoran Xiao;Shijian Lu,"Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efficiency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adaptation without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA. HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target samples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Specifically, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups. △ Less","4 June, 2022",https://arxiv.org/pdf/2110.03374
SecurePtrs: Proving Secure Compilation with Data-Flow Back-Translation and Turn-Taking Simulation,Akram El-Korashy;Roberto Blanco;Jérémy Thibault;Adrien Durier;Deepak Garg;Catalin Hritcu,"Proving secure compilation of partial programs typically requires back-translating an attack against the compiled program to an attack against the source program. To prove back-translation, one can syntactically translate the target attacker to a source one -- i.e., syntax-directed back-translation -- or show that the interaction traces of the target attacker can also be emitted by source attackers -- i.e., trace-directed back-translation. Syntax-directed back-translation is not suitable when the target attacker may use unstructured control flow that the source language cannot directly represent. Trace-directed back-translation works with such syntactic dissimilarity because only the external interactions of the target attacker have to be mimicked in the source, not its internal control flow. Revealing only external interactions is, however, inconvenient when sharing memory via unforgeable pointers, since information about shared pointers stashed in private memory is not present on the trace. This made prior proofs unnecessarily complex, since the generated attacker had to instead stash all reachable pointers. In this work, we introduce more informative *data-flow traces*, combining the best of syntax- and trace-directed back-translation in a simpler technique that handles both syntactic dissimilarity and memory sharing well, and that is proved correct in Coq. Additionally, we develop a novel *turn-taking simulation* relation and use it to prove a recomposition lemma, which is key to reusing compiler correctness in such secure compilation proofs. We are the first to mechanize such a recomposition lemma in the presence of memory sharing. We use these two innovations in a secure compilation proof for a code generation compiler pass between a source language with structured control flow and a target language with unstructured control flow, both with safe pointers and components. △ Less","3 June, 2022",https://arxiv.org/pdf/2110.01439
Machine Learning with Knowledge Constraints for Process Optimization of Open-Air Perovskite Solar Cell Manufacturing,Zhe Liu;Nicholas Rolston;Austin C. Flick;Thomas W. Colburn;Zekun Ren;Reinhold H. Dauskardt;Tonio Buonassisi,"Perovskite photovoltaics (PV) have achieved rapid development in the past decade in terms of power conversion efficiency of small-area lab-scale devices; however, successful commercialization still requires further development of low-cost, scalable, and high-throughput manufacturing techniques. One of the critical challenges of developing a new fabrication technique is the high-dimensional parameter space for optimization, but machine learning (ML) can readily be used to accelerate perovskite PV scaling. Herein, we present an ML-guided framework of sequential learning for manufacturing process optimization. We apply our methodology to the Rapid Spray Plasma Processing (RSPP) technique for perovskite thin films in ambient conditions. With a limited experimental budget of screening 100 process conditions, we demonstrated an efficiency improvement to 18.5% as the best-in-our-lab device fabricated by RSPP, and we also experimentally found 10 unique process conditions to produce the top-performing devices of more than 17% efficiency, which is 5 times higher rate of success than the control experiments with pseudo-random Latin hypercube sampling. Our model is enabled by three innovations: (a) flexible knowledge transfer between experimental processes by incorporating data from prior experimental data as a probabilistic constraint; (b) incorporation of both subjective human observations and ML insights when selecting next experiments; (c) adaptive strategy of locating the region of interest using Bayesian optimization first, and then conducting local exploration for high-efficiency devices. Furthermore, in virtual benchmarking, our framework achieves faster improvements with limited experimental budgets than traditional design-of-experiments methods (e.g., one-variable-at-a-time sampling). △ Less","7 February, 2022",https://arxiv.org/pdf/2110.01387
Direct LiDAR Odometry: Fast Localization with Dense Point Clouds,Kenny Chen;Brett T. Lopez;Ali-akbar Agha-mohammadi;Ankur Mehta,"Field robotics in perceptually-challenging environments require fast and accurate state estimation, but modern LiDAR sensors quickly overwhelm current odometry algorithms. To this end, this paper presents a lightweight frontend LiDAR odometry solution with consistent and accurate localization for computationally-limited robotic platforms. Our Direct LiDAR Odometry (DLO) method includes several key algorithmic innovations which prioritize computational efficiency and enables the use of dense, minimally-preprocessed point clouds to provide accurate pose estimates in real-time. This is achieved through a novel keyframing system which efficiently manages historical map information, in addition to a custom iterative closest point solver for fast point cloud registration with data structure recycling. Our method is more accurate with lower computational overhead than the current state-of-the-art and has been extensively evaluated in multiple perceptually-challenging environments on aerial and legged robots as part of NASA JPL Team CoSTAR's research and development efforts for the DARPA Subterranean Challenge. △ Less","7 January, 2022",https://arxiv.org/pdf/2110.00605
Intelligent Reflecting Surface Aided Wireless Networks: From Single-Reflection to Multi-Reflection Design and Optimization,Weidong Mei;Beixiong Zheng;Changsheng You;Rui Zhang,"Intelligent reflecting surface (IRS) has emerged as a promising technique for wireless communication networks. By dynamically tuning the reflection amplitudes/phase shifts of a large number of passive elements, IRS enables flexible wireless channel control and configuration, and thereby enhances the wireless signal transmission rate and reliability significantly. Despite the vast literature on designing and optimizing assorted IRS-aided wireless systems, prior works have mainly focused on enhancing wireless links with single signal reflection only by one or multiple IRSs, which may be insufficient to boost the wireless link capacity under some harsh propagation conditions (e.g., indoor environment with dense blockages/obstructions). This issue can be tackled by employing two or more IRSs to assist each wireless link and jointly exploiting their single as well as multiple signal reflections over them. However, the resultant double-/multi-IRS aided wireless systems face more complex design issues as well as new practical challenges for implementation as compared to the conventional single-IRS counterpart, in terms of IRS reflection optimization, channel acquisition, as well as IRS deployment and association/selection. As such, a new paradigm for designing multi-IRS cooperative passive beamforming and joint active/passive beam routing arises which calls for innovative design approaches and optimization methods. In this paper, we give a tutorial overview of multi-IRS aided wireless networks, with an emphasis on addressing the new challenges due to multi-IRS signal reflection and routing. Moreover, we point out important directions worthy of research and investigation in the future. △ Less","25 April, 2022",https://arxiv.org/pdf/2109.13641
Ground material classification for UAV-based photogrammetric 3D data A 2D-3D Hybrid Approach,Meida Chen;Andrew Feng;Yu Hou;Kyle McCullough;Pratusha Bhuvana Prasad;Lucio Soibelman,"In recent years, photogrammetry has been widely used in many areas to create photorealistic 3D virtual data representing the physical environment. The innovation of small unmanned aerial vehicles (sUAVs) has provided additional high-resolution imaging capabilities with low cost for mapping a relatively large area of interest. These cutting-edge technologies have caught the US Army and Navy's attention for the purpose of rapid 3D battlefield reconstruction, virtual training, and simulations. Our previous works have demonstrated the importance of information extraction from the derived photogrammetric data to create semantic-rich virtual environments (Chen et al., 2019). For example, an increase of simulation realism and fidelity was achieved by segmenting and replacing photogrammetric trees with game-ready tree models. In this work, we further investigated the semantic information extraction problem and focused on the ground material segmentation and object detection tasks. The main innovation of this work was that we leveraged both the original 2D images and the derived 3D photogrammetric data to overcome the challenges faced when using each individual data source. For ground material segmentation, we utilized an existing convolutional neural network architecture (i.e., 3DMV) which was originally designed for segmenting RGB-D sensed indoor data. We improved its performance for outdoor photogrammetric data by introducing a depth pooling layer in the architecture to take into consideration the distance between the source images and the reconstructed terrain model. To test the performance of our improved 3DMV, a ground truth ground material database was created using data from the One World Terrain (OWT) data repository. Finally, a workflow for importing the segmented ground materials into a virtual simulation scene was introduced, and visual results are reported in this paper. △ Less","15 October, 2022",https://arxiv.org/pdf/2109.12221
Autonomy and Perception for Space Mining,Ragav Sachdeva;Ravi Hammond;James Bockman;Alec Arthur;Brandon Smart;Dustin Craggs;Anh-Dzung Doan;Thomas Rowntree;Elijah Schutz;Adrian Orenstein;Andy Yu;Tat-Jun Chin;Ian Reid,"Future Moon bases will likely be constructed using resources mined from the surface of the Moon. The difficulty of maintaining a human workforce on the Moon and communications lag with Earth means that mining will need to be conducted using collaborative robots with a high degree of autonomy. In this paper, we describe our solution for Phase 2 of the NASA Space Robotics Challenge, which provided a simulated lunar environment in which teams were tasked to develop software systems to achieve autonomous collaborative robots for mining on the Moon. Our 3rd place and innovation award winning solution shows how machine learning-enabled vision could alleviate major challenges posed by the lunar environment towards autonomous space mining, chiefly the lack of satellite positioning systems, hazardous terrain, and delicate robot interactions. A robust multi-robot coordinator was also developed to achieve long-term operation and effective collaboration between robots. △ Less","13 April, 2022",https://arxiv.org/pdf/2109.12109
Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement,Beomyoung Kim;Youngjoon Yoo;Chaeeun Rhee;Junmo Kim,"Weakly-supervised instance segmentation (WSIS) has been considered as a more challenging task than weakly-supervised semantic segmentation (WSSS). Compared to WSSS, WSIS requires instance-wise localization, which is difficult to extract from image-level labels. To tackle the problem, most WSIS approaches use off-the-shelf proposal techniques that require pre-training with instance or object level labels, deviating the fundamental definition of the fully-image-level supervised setting. In this paper, we propose a novel approach including two innovative components. First, we propose a semantic knowledge transfer to obtain pseudo instance labels by transferring the knowledge of WSSS to WSIS while eliminating the need for the off-the-shelf proposals. Second, we propose a self-refinement method to refine the pseudo instance labels in a self-supervised scheme and to use the refined labels for training in an online manner. Here, we discover an erroneous phenomenon, semantic drift, that occurred by the missing instances in pseudo instance labels categorized as background class. This semantic drift occurs confusion between background and instance in training and consequently degrades the segmentation performance. We term this problem as semantic drift problem and show that our proposed self-refinement method eliminates the semantic drift problem. The extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate the effectiveness of our approach, and we achieve a considerable performance without off-the-shelf proposal techniques. The code is available at https://github.com/clovaai/BESTIE. △ Less","29 March, 2022",https://arxiv.org/pdf/2109.09477
Self-supervised learning methods and applications in medical imaging analysis: A survey,Saeed Shurrab;Rehab Duwairi,"The scarcity of high-quality annotated medical imaging datasets is a major problem that collides with machine learning applications in the field of medical imaging analysis and impedes its advancement. Self-supervised learning is a recent training paradigm that enables learning robust representations without the need for human annotation which can be considered an effective solution for the scarcity of annotated medical data. This article reviews the state-of-the-art research directions in self-supervised learning approaches for image data with a concentration on their applications in the field of medical imaging analysis. The article covers a set of the most recent self-supervised learning methods from the computer vision field as they are applicable to the medical imaging analysis and categorize them as predictive, generative, and contrastive approaches. Moreover, the article covers 40 of the most recent research papers in the field of self-supervised learning in medical imaging analysis aiming at shedding the light on the recent innovation in the field. Finally, the article concludes with possible future research directions in the field. △ Less","20 July, 2022",https://arxiv.org/pdf/2109.08685
Wi-Fi Meets ML: A Survey on Improving IEEE 802.11 Performance with Machine Learning,Szymon Szott;Katarzyna Kosek-Szott;Piotr Gawłowicz;Jorge Torres Gómez;Boris Bellalta;Anatolij Zubow;Falko Dressler,"Wireless local area networks (WLANs) empowered by IEEE 802.11 (Wi-Fi) hold a dominant position in providing Internet access thanks to their freedom of deployment and configuration as well as the existence of affordable and highly interoperable devices. The Wi-Fi community is currently deploying Wi-Fi 6 and developing Wi-Fi 7, which will bring higher data rates, better multi-user and multi-AP support, and, most importantly, improved configuration flexibility. These technical innovations, including the plethora of configuration parameters, are making next-generation WLANs exceedingly complex as the dependencies between parameters and their joint optimization usually have a non-linear impact on network performance. The complexity is further increased in the case of dense deployments and coexistence in shared bands. While classical optimization approaches fail in such conditions, machine learning (ML) is able to handle complexity. Much research has been published on using ML to improve Wi-Fi performance and solutions are slowly being adopted in existing deployments. In this survey, we adopt a structured approach to describe the various Wi-Fi areas where ML is applied. To this end, we analyze over 250 papers in the field, providing readers with an overview of the main trends. Based on this review, we identify specific open challenges and provide general future research directions. △ Less","6 October, 2022",https://arxiv.org/pdf/2109.04786
Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers,Zhiqi Li;Wenhai Wang;Enze Xie;Zhiding Yu;Anima Anandkumar;Jose M. Alvarez;Ping Luo;Tong Lu,"Panoptic segmentation involves a combination of joint semantic segmentation and instance segmentation, where image contents are divided into two types: things and stuff. We present Panoptic SegFormer, a general framework for panoptic segmentation with transformers. It contains three innovative components: an efficient deeply-supervised mask decoder, a query decoupling strategy, and an improved post-processing method. We also use Deformable DETR to efficiently process multi-scale features, which is a fast and efficient version of DETR. Specifically, we supervise the attention modules in the mask decoder in a layer-wise manner. This deep supervision strategy lets the attention modules quickly focus on meaningful semantic regions. It improves performance and reduces the number of required training epochs by half compared to Deformable DETR. Our query decoupling strategy decouples the responsibilities of the query set and avoids mutual interference between things and stuff. In addition, our post-processing strategy improves performance without additional costs by jointly considering classification and segmentation qualities to resolve conflicting mask overlaps. Our approach increases the accuracy 6.2\% PQ over the baseline DETR model. Panoptic SegFormer achieves state-of-the-art results on COCO test-dev with 56.2\% PQ. It also shows stronger zero-shot robustness over existing methods. The code is released at \url{https://github.com/zhiqi-li/Panoptic-SegFormer}. △ Less","18 March, 2022",https://arxiv.org/pdf/2109.03814
FedZKT: Zero-Shot Knowledge Transfer towards Resource-Constrained Federated Learning with Heterogeneous On-Device Models,Lan Zhang;Dapeng Wu;Xiaoyong Yuan,"Federated learning enables multiple distributed devices to collaboratively learn a shared prediction model without centralizing their on-device data. Most of the current algorithms require comparable individual efforts for local training with the same structure and size of on-device models, which, however, impedes participation from resource-constrained devices. Given the widespread yet heterogeneous devices nowadays, in this paper, we propose an innovative federated learning framework with heterogeneous on-device models through Zero-shot Knowledge Transfer, named by FedZKT. Specifically, FedZKT allows devices to independently determine the on-device models upon their local resources. To achieve knowledge transfer across these heterogeneous on-device models, a zero-shot distillation approach is designed without any prerequisites for private on-device data, which is contrary to certain prior research based on a public dataset or a pre-trained data generator. Moreover, this compute-intensive distillation task is assigned to the server to allow the participation of resource-constrained devices, where a generator is adversarially learned with the ensemble of collected on-device models. The distilled central knowledge is then sent back in the form of the corresponding on-device model parameters, which can be easily absorbed on the device side. Extensive experimental studies demonstrate the effectiveness and robustness of FedZKT towards on-device knowledge agnostic, on-device model heterogeneity, and other challenging federated learning scenarios, such as heterogeneous on-device data and straggler effects. △ Less","5 April, 2022",https://arxiv.org/pdf/2109.03775
Safety-Critical Learning of Robot Control with Temporal Logic Specifications,Mingyu Cai;Cristian-Ioan Vasile,"Reinforcement learning (RL) is a promising approach. However, success is limited to real-world applications, because ensuring safe exploration and facilitating adequate exploitation is a challenge for controlling robotic systems with unknown models and measurement uncertainties. The learning problem becomes even more difficult for complex tasks over continuous state-action. In this paper, we propose a learning-based robotic control framework consisting of several aspects: (1) we leverage Linear Temporal Logic (LTL) to express complex tasks over infinite horizons that are translated to a novel automaton structure; (2) we detail an innovative reward scheme for LTL satisfaction with a probabilistic guarantee. Then, by applying a reward shaping technique, we develop a modular policy-gradient architecture exploiting the benefits of the automaton structure to decompose overall tasks and enhance the performance of learned controllers; (3) by incorporating Gaussian Processes (GPs) to estimate the uncertain dynamic systems, we synthesize a model-based safe exploration during the learning process using Exponential Control Barrier Functions (ECBFs) that generalize systems with high-order relative degrees; (4) to further improve the efficiency of exploration, we utilize the properties of LTL automata and ECBFs to propose a safe guiding process. Finally, we demonstrate the effectiveness of the framework via several robotic environments. We show an ECBF-based modular deep RL algorithm that achieves near-perfect success rates and safety guarding with high probability confidence during training. △ Less","26 August, 2022",https://arxiv.org/pdf/2109.02791
Ultra-high Resolution Image Segmentation via Locality-aware Context Fusion and Alternating Local Enhancement,Wenxi Liu;Qi Li;Xindai Lin;Weixiang Yang;Shengfeng He;Yuanlong Yu,"Ultra-high resolution image segmentation has raised increasing interests in recent years due to its realistic applications. In this paper, we innovate the widely used high-resolution image segmentation pipeline, in which an ultra-high resolution image is partitioned into regular patches for local segmentation and then the local results are merged into a high-resolution semantic mask. In particular, we introduce a novel locality-aware context fusion based segmentation model to process local patches, where the relevance between local patch and its various contexts are jointly and complementarily utilized to handle the semantic regions with large variations. Additionally, we present the alternating local enhancement module that restricts the negative impact of redundant information introduced from the contexts, and thus is endowed with the ability of fixing the locality-aware features to produce refined results. Furthermore, in comprehensive experiments, we demonstrate that our model outperforms other state-of-the-art methods in public benchmarks. Our released codes are available at: https://github.com/liqiokkk/FCtL. △ Less","6 December, 2022",https://arxiv.org/pdf/2109.02580
Estimating permeability of 3D micro-CT images by physics-informed CNNs based on DNS,Stephan Gärttner;Faruk O. Alpak;Andreas Meier;Nadja Ray;Florian Frank,"In recent years, convolutional neural networks (CNNs) have experienced an increasing interest in their ability to perform a fast approximation of effective hydrodynamic parameters in porous media research and applications. This paper presents a novel methodology for permeability prediction from micro-CT scans of geological rock samples. The training data set for CNNs dedicated to permeability prediction consists of permeability labels that are typically generated by classical lattice Boltzmann methods (LBM) that simulate the flow through the pore space of the segmented image data. We instead perform direct numerical simulation (DNS) by solving the stationary Stokes equation in an efficient and distributed-parallel manner. As such, we circumvent the convergence issues of LBM that frequently are observed on complex pore geometries, and therefore, improve the generality and accuracy of our training data set. Using the DNS-computed permeabilities, a physics-informed CNN PhyCNN) is trained by additionally providing a tailored characteristic quantity of the pore space. More precisely, by exploiting the connection to flow problems on a graph representation of the pore space, additional information about confined structures is provided to the network in terms of the maximum flow value, which is the key innovative component of our workflow. The robustness of this approach is reflected by very high prediction accuracy, which is observed for a variety of sandstone samples from archetypal rock formations. △ Less","13 April, 2022",https://arxiv.org/pdf/2109.01818
"Assessing the Performance of Online Students -- New Data, New Approaches, Improved Accuracy",Robin Schmucker;Jingbo Wang;Shijia Hu;Tom M. Mitchell,"We consider the problem of assessing the changing performance levels of individual students as they go through online courses. This student performance (SP) modeling problem is a critical step for building adaptive online teaching systems. Specifically, we conduct a study of how to utilize various types and large amounts of student log data to train accurate machine learning (ML) models that predict the performance of future students. This study is the first to use four very large sets of student data made available recently from four distinct intelligent tutoring systems. Our results include a new ML approach that defines a new state of the art for logistic regression based SP modeling, improving over earlier methods in several ways: First, we achieve improved accuracy by introducing new features that can be easily computed from conventional question-response logs (e.g., the pattern in the student 's most recent answers). Second, we take advantage of features of the student history that go beyond question-response pairs (e.g., features such as which video segments the student watched, or skipped) as well as information about prerequisite structure in the curriculum. Third, we train multiple specialized SP models for different aspects of the curriculum (e.g., specializing in early versus later segments of the student history), then combine these specialized models to create a group prediction of the SP. Taken together, these innovations yield an average AUC score across these four datasets of 0.808 compared to the previous best logistic regression approach score of 0.767, and also outperforming state-of-the-art deep neural net approaches. Importantly, we observe consistent improvements from each of our three methodological innovations, in each dataset, suggesting that our methods are of general utility and likely to produce improvements for other online tutoring systems as well. △ Less","8 February, 2022",https://arxiv.org/pdf/2109.01753
A Multi-Sensor Interface to Improve the Learning Experience in Arc Welding Training Tasks,Hoi-Yin Lee;Peng Zhou;Anqing Duan;Jiangliu Wang;Victor Wu;David Navarro-Alarcon,"This paper presents the development of a multi-sensor user interface to facilitate the instruction of arc welding tasks. Traditional methods to acquire hand-eye coordination skills are typically conducted through one-to-one instruction where trainees must wear protective helmets and conduct several tests. This approach is inefficient as the harmful light emitted from the electric arc impedes the close monitoring of the process; Practitioners can only observe a small bright spot. To tackle these problems, recent training approaches have leveraged virtual reality to safely simulate the process and visualize the geometry of the workpieces. However, the synthetic nature of these types of simulation platforms reduces their effectiveness as they fail to comprise actual welding interactions with the environment, which hinders the trainees' learning process. To provide users with a real welding experience, we have developed a new multi-sensor extended reality platform for arc welding training. Our system is composed of: (1) An HDR camera, monitoring the real welding spot in real-time; (2) A depth sensor, capturing the 3D geometry of the scene; and (3) A head-mounted VR display, visualizing the process safely. Our innovative platform provides users with a ""bot trainer"", virtual cues of the seam geometry, automatic spot tracking, and performance scores. To validate the platform's feasibility, we conduct extensive experiments with several welding training tasks. We show that compared with the traditional training practice and recent virtual reality approaches, our automated multi-sensor method achieves better performances in terms of accuracy, learning curve, and effectiveness. △ Less","28 June, 2022",https://arxiv.org/pdf/2109.01383
Understanding Data Storage and Ingestion for Large-Scale Deep Recommendation Model Training,Mark Zhao;Niket Agarwal;Aarti Basant;Bugra Gedik;Satadru Pan;Mustafa Ozdal;Rakesh Komuravelli;Jerry Pan;Tianshu Bao;Haowei Lu;Sundaram Narayanan;Jack Langman;Kevin Wilfong;Harsha Rastogi;Carole-Jean Wu;Christos Kozyrakis;Parik Pol,"Datacenter-scale AI training clusters consisting of thousands of domain-specific accelerators (DSA) are used to train increasingly-complex deep learning models. These clusters rely on a data storage and ingestion (DSI) pipeline, responsible for storing exabytes of training data and serving it at tens of terabytes per second. As DSAs continue to push training efficiency and throughput, the DSI pipeline is becoming the dominating factor that constrains the overall training performance and capacity. Innovations that improve the efficiency and performance of DSI systems and hardware are urgent, demanding a deep understanding of DSI characteristics and infrastructure at scale. This paper presents Meta's end-to-end DSI pipeline, composed of a central data warehouse built on distributed storage and a Data PreProcessing Service that scales to eliminate data stalls. We characterize how hundreds of models are collaboratively trained across geo-distributed datacenters via diverse and continuous training jobs. These training jobs read and heavily filter massive and evolving datasets, resulting in popular features and samples used across training jobs. We measure the intense network, memory, and compute resources required by each training job to preprocess samples during training. Finally, we synthesize key takeaways based on our production infrastructure characterization. These include identifying hardware bottlenecks, discussing opportunities for heterogeneous DSI hardware, motivating research in datacenter scheduling and benchmark datasets, and assimilating lessons learned in optimizing DSI infrastructure. △ Less","22 April, 2022",https://arxiv.org/pdf/2108.09373
F-PKI: Enabling Innovation and Trust Flexibility in the HTTPS Public-Key Infrastructure,Laurent Chuat;Cyrill Krähenbühl;Prateek Mittal;Adrian Perrig,"We present F-PKI, an enhancement to the HTTPS public-key infrastructure (or web PKI) that gives trust flexibility to both clients and domain owners, and enables certification authorities (CAs) to enforce stronger security measures. In today's web PKI, all CAs are equally trusted, and security is defined by the weakest link. We address this problem by introducing trust flexibility in two dimensions: with F-PKI, each domain owner can define a domain policy (specifying, for example, which CAs are authorized to issue certificates for their domain name) and each client can set or choose a validation policy based on trust levels. F-PKI thus supports a property that is sorely needed in today's Internet: trust heterogeneity. Different parties can express different trust preferences while still being able to verify all certificates. In contrast, today's web PKI only allows clients to fully distrust suspicious/misbehaving CAs, which is likely to cause collateral damage in the form of legitimate certificates being rejected. Our contribution is to present a system that is backward compatible, provides sensible security properties to both clients and domain owners, ensures the verifiability of all certificates, and prevents downgrade attacks. Furthermore, F-PKI provides a ground for innovation, as it gives CAs an incentive to deploy new security measures to attract more customers, without having these measures undercut by vulnerable CAs. △ Less","29 March, 2022",https://arxiv.org/pdf/2108.08581
Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery,Jason Portenoy;Marissa Radensky;Jevin West;Eric Horvitz;Daniel Weld;Tom Hope,"Isolated silos of scientific research and the growing challenge of information overload limit awareness across the literature and hinder innovation. Algorithmic curation and recommendation, which often prioritize relevance, can further reinforce these informational ""filter bubbles."" In response, we describe Bridger, a system for facilitating discovery of scholars and their work. We construct a faceted representation of authors with information gleaned from their papers and inferred author personas, and use it to develop an approach that locates commonalities and contrasts between scientists to balance relevance and novelty. In studies with computer science researchers, this approach helps users discover authors considered useful for generating novel research directions. We also demonstrate an approach for displaying information about authors, boosting the ability to understand the work of new, unfamiliar scholars. Our analysis reveals that Bridger connects authors who have different citation profiles and publish in different venues, raising the prospect of bridging diverse scientific communities. △ Less","31 January, 2022",https://arxiv.org/pdf/2108.05669
Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) for Short-Term Forecasting of Transit Passenger Flow,Yuxin He;Lishuai Li;Xinting Zhu;Kwok Leung Tsui,"Short-term forecasting of passenger flow is critical for transit management and crowd regulation. Spatial dependencies, temporal dependencies, inter-station correlations driven by other latent factors, and exogenous factors bring challenges to the short-term forecasts of passenger flow of urban rail transit networks. An innovative deep learning approach, Multi-Graph Convolutional-Recurrent Neural Network (MGC-RNN) is proposed to forecast passenger flow in urban rail transit systems to incorporate these complex factors. We propose to use multiple graphs to encode the spatial and other heterogenous inter-station correlations. The temporal dynamics of the inter-station correlations are also modeled via the proposed multi-graph convolutional-recurrent neural network structure. Inflow and outflow of all stations can be collectively predicted with multiple time steps ahead via a sequence to sequence(seq2seq) architecture. The proposed method is applied to the short-term forecasts of passenger flow in Shenzhen Metro, China. The experimental results show that MGC-RNN outperforms the benchmark algorithms in terms of forecasting accuracy. Besides, it is found that the inter-station driven by network distance, network structure, and recent flow patterns are significant factors for passenger flow forecasting. Moreover, the architecture of LSTM-encoder-decoder can capture the temporal dependencies well. In general, the proposed framework could provide multiple views of passenger flow dynamics for fine prediction and exhibit a possibility for multi-source heterogeneous data fusion in the spatiotemporal forecast tasks. △ Less","20 May, 2022",https://arxiv.org/pdf/2107.13226
Learned Sorted Table Search and Static Indexes in Small Model Space,Domenico Amato;Giosuè Lo Bosco;Raffaele Giancarlo,"Machine Learning Techniques, properly combined with Data Structures, have resulted in Learned Static Indexes, innovative and powerful tools that speed-up Binary Search, with the use of additional space with respect to the table being searched into. Such space is devoted to the Machine Learning Model. Although in their infancy, they are methodologically and practically important, due to the pervasiveness of Sorted Table Search procedures. In modern applications, model space is a key factor and, in fact, a major open question concerning this area is to assess to what extent one can enjoy the speed-up of Binary Search achieved by Learned Indexes while using constant or nearly constant space models. In this paper, we investigate the mentioned question by (a) introducing two new models, i.e., the Learned k-ary Search Model and the Synoptic Recursive Model Index, respectively; (b) systematically exploring the time-space trade-offs of a hierarchy of existing models, i.e., the ones in the reference software platform Searching on Sorted Data, together with the new ones proposed here. By adhering and extending the current benchmarking methodology, we experimentally show that the Learned k-ary Search Model can speed up Binary Search in constant additional space. Our second model, together with the bi-criteria Piece-wise Geometric Model index, can achieve a speed-up of Binary Search with a model space of 0:05% more than the one taken by the table, being competitive in terms of time-space trade-off with existing proposals. The Synoptic Recursive Model Index and the bi-criteria Piece-wise Geometric Model complement each other quite well across the various levels of the internal memory hierarchy. Finally, our findings stimulate research in this area, since they highlight the need for further studies regarding the time-space relation in Learned Indexes. △ Less","17 September, 2022",https://arxiv.org/pdf/2107.09480
GREN: Graph-Regularized Embedding Network for Weakly-Supervised Disease Localization in X-ray Images,Baolian Qi;Gangming Zhao;Xin Wei;Changde Du;Chengwei Pan;Yizhou Yu;Jinpeng Li,"Locating diseases in chest X-ray images with few careful annotations saves large human effort. Recent works approached this task with innovative weakly-supervised algorithms such as multi-instance learning (MIL) and class activation maps (CAM), however, these methods often yield inaccurate or incomplete regions. One of the reasons is the neglection of the pathological implications hidden in the relationship across anatomical regions within each image and the relationship across images. In this paper, we argue that the cross-region and cross-image relationship, as contextual and compensating information, is vital to obtain more consistent and integral regions. To model the relationship, we propose the Graph Regularized Embedding Network (GREN), which leverages the intra-image and inter-image information to locate diseases on chest X-ray images. GREN uses a pre-trained U-Net to segment the lung lobes, and then models the intra-image relationship between the lung lobes using an intra-image graph to compare different regions. Meanwhile, the relationship between in-batch images is modeled by an inter-image graph to compare multiple images. This process mimics the training and decision-making process of a radiologist: comparing multiple regions and images for diagnosis. In order for the deep embedding layers of the neural network to retain structural information (important in the localization task), we use the Hash coding and Hamming distance to compute the graphs, which are used as regularizers to facilitate training. By means of this, our approach achieves the state-of-the-art result on NIH chest X-ray dataset for weakly-supervised disease localization. Our codes are accessible online (https://github.com/qibaolian/GREN). △ Less","4 August, 2022",https://arxiv.org/pdf/2107.06442
Multi-modality Deep Restoration of Extremely Compressed Face Videos,Xi Zhang;Xiaolin Wu,"Arguably the most common and salient object in daily video communications is the talking head, as encountered in social media, virtual classrooms, teleconferences, news broadcasting, talk shows, etc. When communication bandwidth is limited by network congestions or cost effectiveness, compression artifacts in talking head videos are inevitable. The resulting video quality degradation is highly visible and objectionable due to high acuity of human visual system to faces. To solve this problem, we develop a multi-modality deep convolutional neural network method for restoring face videos that are aggressively compressed. The main innovation is a new DCNN architecture that incorporates known priors of multiple modalities: the video-synchronized speech signal and semantic elements of the compression code stream, including motion vectors, code partition map and quantization parameters. These priors strongly correlate with the latent video and hence they are able to enhance the capability of deep learning to remove compression artifacts. Ample empirical evidences are presented to validate the superior performance of the proposed DCNN method on face videos over the existing state-of-the-art methods. △ Less","4 March, 2022",https://arxiv.org/pdf/2107.05548
A precise bare simulation approach to the minimization of some distances. Foundations,Michel Broniatowski;Wolfgang Stummer,"In information theory -- as well as in the adjacent fields of statistics, machine learning, artificial intelligence, signal processing and pattern recognition -- many flexibilizations of the omnipresent Kullback-Leibler information distance (relative entropy) and of the closely related Shannon entropy have become frequently used tools. To tackle corresponding constrained minimization (respectively maximization) problems by a newly developed dimension-free bare (pure) simulation method, is the main goal of this paper. Almost no assumptions (like convexity) on the set of constraints are needed, within our discrete setup of arbitrary dimension, and our method is precise (i.e., converges in the limit). As a side effect, we also derive an innovative way of constructing new useful distances/divergences. To illustrate the core of our approach, we present numerous solved cases. The potential for widespread applicability is indicated, too; in particular, we deliver many recent references for uses of the involved distances/divergences and entropies in various different research fields (which may also serve as an interdisciplinary interface). △ Less","15 November, 2022",https://arxiv.org/pdf/2107.01693
Momentum Accelerates the Convergence of Stochastic AUPRC Maximization,Guanghui Wang;Ming Yang;Lijun Zhang;Tianbao Yang,"In this paper, we study stochastic optimization of areas under precision-recall curves (AUPRC), which is widely used for combating imbalanced classification tasks. Although a few methods have been proposed for maximizing AUPRC, stochastic optimization of AUPRC with convergence guarantee remains an undeveloped territory. A state-of-the-art complexity is O(1/ε^5) for finding an ε-stationary solution. In this paper, we further improve the stochastic optimization of AURPC by (i) developing novel stochastic momentum methods with a better iteration complexity of O(1/ε^4) for finding an ε-stationary solution; and (ii) designing a novel family of stochastic adaptive methods with the same iteration complexity, which enjoy faster convergence in practice. To this end, we propose two innovative techniques that are critical for improving the convergence: (i) the biased estimators for tracking individual ranking scores are updated in a randomized coordinate-wise manner; and (ii) a momentum update is used on top of the stochastic gradient estimator for tracking the gradient of the objective. The novel analysis of Adam-style updates is also one main contribution. Extensive experiments on various data sets demonstrate the effectiveness of the proposed algorithms. Of independent interest, the proposed stochastic momentum and adaptive algorithms are also applicable to a class of two-level stochastic dependent compositional optimization problems. △ Less","3 March, 2022",https://arxiv.org/pdf/2107.01173
An Investigation of the (In)effectiveness of Counterfactually Augmented Data,Nitish Joshi;He He,"While pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution (OOD) data. Recent work has explored using counterfactually-augmented data (CAD) -- data generated by minimally perturbing examples to flip the ground-truth label -- to identify robust features that are invariant under distribution shift. However, empirical results using CAD for OOD generalization have been mixed. To explain this discrepancy, we draw insights from a linear Gaussian model and demonstrate the pitfalls of CAD. Specifically, we show that (a) while CAD is effective at identifying robust features, it may prevent the model from learning unperturbed robust features; and (b) CAD may exacerbate existing spurious correlations in the data. On two crowdsourced CAD datasets, our results show that the lack of perturbation diversity limits their effectiveness on OOD generalization, calling for innovative crowdsourcing procedures to elicit diverse perturbation of examples. △ Less","16 March, 2022",https://arxiv.org/pdf/2107.00753
Limitations of machine learning for building energy prediction: ASHRAE Great Energy Predictor III Kaggle competition error analysis,Clayton Miller;Bianca Picchetti;Chun Fu;Jovan Pantelic,"Research is needed to explore the limitations and potential for improvement of machine learning for building energy prediction. With this aim, the ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition was launched in 2019. This effort was the largest building energy meter machine learning competition of its kind, with 4,370 participants who submitted 39,403 predictions. The test data set included two years of hourly whole building readings from 2,380 meters in 1,448 buildings at 16 locations. This paper analyzes the various sources and types of residual model error from an aggregation of the competition's top 50 solutions. This analysis reveals the limitations for machine learning using the standard model inputs of historical meter, weather, and basic building metadata. The errors are classified according to timeframe, behavior, magnitude, and incidence in single buildings or across a campus. The results show machine learning models have errors within a range of acceptability (RMSLE_scaled =< 0.1) on 79.1% of the test data. Lower magnitude (in-range) model errors (0.1 < RMSLE_scaled =< 0.3) occur in 16.1% of the test data. These errors could be remedied using innovative training data from onsite and web-based sources. Higher magnitude (out-of-range) errors (RMSLE_scaled > 0.3) occur in 4.8% of the test data and are unlikely to be accurately predicted. △ Less","20 February, 2022",https://arxiv.org/pdf/2106.13475
Spatio-Temporal SAR-Optical Data Fusion for Cloud Removal via a Deep Hierarchical Model,Alessandro Sebastianelli;Artur Nowakowski;Erika Puglisi;Maria Pia Del Rosso;Jamila Mifdal;Fiora Pirri;Pierre Philippe Mathieu;Silvia Liberata Ullo,"Cloud removal is a relevant topic in Remote Sensing as it fosters the usability of high-resolution optical images for Earth monitoring and study. Related techniques have been analyzed for years with a progressively clearer view of the appropriate methods to adopt, from multi-spectral to inpainting methods. Recent applications of deep generative models and sequence-to-sequence-based models have proved their capability to advance the field significantly. Nevertheless, there are still some gaps, mostly related to the amount of cloud coverage, the density and thickness of clouds, and the occurred temporal landscape changes. In this work, we fill some of these gaps by introducing a novel multi-modal method that uses different sources of information, both spatial and temporal, to restore the whole optical scene of interest. The proposed method introduces an innovative deep model, using the outcomes of both temporal-sequence blending and direct translation from Synthetic Aperture Radar (SAR) to optical images to obtain a pixel-wise restoration of the whole scene. The advantage of our approach is demonstrated across a variety of atmospheric conditions tested on a dataset we have generated and made available. Quantitative and qualitative results prove that the proposed method obtains cloud-free images, preserving scene details without resorting to a huge portion of a clean image and coping with landscape changes. △ Less","28 March, 2022",https://arxiv.org/pdf/2106.12226
Proof Blocks: Autogradable Scaffolding Activities for Learning to Write Proofs,Seth Poulsen;Mahesh Viswanathan;Geoffrey L. Herman;Matthew West,"Proof Blocks is a software tool which enables students to write proofs by dragging and dropping prewritten proof lines into the correct order. These proofs can be graded completely automatically, enabling students to receive rapid feedback on how they are doing with their proofs. When constructing a problem, the instructor specifies the dependency graph of the lines of the proof, so that any correct arrangement of the lines can receive full credit. This innovation can improve assessment tools by increasing the types of questions we can ask students about proofs, and can give greater access to proof knowledge by increasing the amount that students can learn on their own with the help of a computer. △ Less","4 May, 2022",https://arxiv.org/pdf/2106.11032
Multi-Layered Blockchain Governance Game,Song-Kyoo Kim,"The research designs a new integrated system for the security enhancement of a decentralized network by preventing damages from attackers, particularly for the 51 percent attack. The concept of multiple layered design based on Blockchain Governance Games frameworks could handle multiple number of networks analytically. The Multi-Layered Blockchain Governance Game is an innovative analytical model to find the best strategies for executing a safety operation to protect whole multiple layered network systems from attackers. This research fully analyzes a complex network with the compact mathematical forms and theoretically tractable results for predicting the moment of a safety operation execution are fully obtained. Additionally, simulation results are demonstrated to obtain the optimal values of configuring parameters of a blockchain-based security network. The Matlab codes for the simulations are publicly available to help those whom are constructing an enhanced decentralized security network architecture through this proposed integrated theoretical framework. △ Less","14 January, 2022",https://arxiv.org/pdf/2106.09518
Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery,Yulun Wu;Mikaela Cashman;Nicholas Choma;Érica T. Prates;Verónica G. Melesse Vergara;Manesh Shah;Andrew Chen;Austin Clyde;Thomas S. Brettin;Wibe A. de Jong;Neeraj Kumar;Martha S. Head;Rick L. Stevens;Peter Nugent;Daniel A. Jacobson;James B. Brown,"We developed Distilled Graph Attention Policy Network (DGAPN), a reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by efficiently navigating a physically constrained domain. The framework is examined on the task of generating molecules that are designed to bind, noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial Graph Attention (sGAT) mechanism that leverages self-attention over both node and edge attributes as well as encoding the spatial structure -- this capability is of considerable interest in synthetic biology and drug discovery. An attentional policy network is introduced to learn the decision rules for a dynamic, fragment-based chemical environment, and state-of-the-art policy gradient techniques are employed to train the network with stability. Exploration is driven by the stochasticity of the action space design and the innovation reward bonuses learned and proposed by random network distillation. In experiments, our framework achieved outstanding results compared to state-of-the-art algorithms, while reducing the complexity of paths to chemical synthesis. △ Less","11 May, 2022",https://arxiv.org/pdf/2106.02190
Sample-based Approximation of Nash in Large Many-Player Games via Gradient Descent,Ian Gemp;Rahul Savani;Marc Lanctot;Yoram Bachrach;Thomas Anthony;Richard Everett;Andrea Tacchetti;Tom Eccles;János Kramár,"Nash equilibrium is a central concept in game theory. Several Nash solvers exist, yet none scale to normal-form games with many actions and many players, especially those with payoff tensors too big to be stored in memory. In this work, we propose an approach that iteratively improves an approximation to a Nash equilibrium through joint play. It accomplishes this by tracing a previously established homotopy that defines a continuum of equilibria for the game regularized with decaying levels of entropy. This continuum asymptotically approaches the limiting logit equilibrium, proven by McKelvey and Palfrey (1995) to be unique in almost all games, thereby partially circumventing the well-known equilibrium selection problem of many-player games. To encourage iterates to remain near this path, we efficiently minimize average deviation incentive via stochastic gradient descent, intelligently sampling entries in the payoff tensor as needed. Monte Carlo estimates of the stochastic gradient from joint play are biased due to the appearance of a nonlinear max operator in the objective, so we introduce additional innovations to the algorithm to alleviate gradient bias. The descent process can also be viewed as repeatedly constructing and reacting to a polymatrix approximation to the game. In these ways, our proposed approach, average deviation incentive descent with adaptive sampling (ADIDAS), is most similar to three classical approaches, namely homotopy-type, Lyapunov, and iterative polymatrix solvers. The lack of local convergence guarantees for biased gradient descent prevents guaranteed convergence to Nash, however, we demonstrate through extensive experiments the ability of this approach to approximate a unique Nash in normal-form games with as many as seven players and twenty one actions (several billion outcomes) that are orders of magnitude larger than those possible with prior algorithms. △ Less","4 February, 2022",https://arxiv.org/pdf/2106.01285
Towards Deeper Deep Reinforcement Learning with Spectral Normalization,Johan Bjorck;Carla P. Gomes;Kilian Q. Weinberger,"In computer vision and natural language processing, innovations in model architecture that increase model capacity have reliably translated into gains in performance. In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algorithms often use small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overfitting; however, this hypothesis is untested. In this paper we investigate how RL agents are affected by exchanging the small MLPs with larger modern networks with skip connections and normalization, focusing specifically on actor-critic algorithms. We empirically verify that naively adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that instability from taking gradients through the critic is the culprit. We demonstrate that spectral normalization (SN) can mitigate this issue and enable stable training with large modern architectures. After smoothing with SN, larger models yield significant performance improvements -- suggesting that more ""easy"" gains may be had by focusing on model architectures in addition to algorithmic innovations. △ Less","3 January, 2022",https://arxiv.org/pdf/2106.01151
Hybrid Beamforming for Intelligent Reflecting Surface Aided Millimeter Wave MIMO Systems,Sung Hyuck Hong;Jaeyong Park;Sung-Jin Kim;Junil Choi,"As communication systems that employ millimeter wave (mmWave) frequency bands must use large antenna arrays to overcome the severe propagation loss of mmWave signals, hybrid beamforming has been considered as an integral component of mmWave communications. Recently, intelligent reflecting surface (IRS) has been proposed as an innovative technology that can significantly improve the performance of mmWave communication systems through the use of low-cost passive reflecting elements. In this paper, we study IRS-aided mmWave multiple-input multiple-output (MIMO) systems with hybrid beamforming architectures. We first exploit the sparse-scattering structure and large dimension of mmWave channels to develop the joint design of IRS reflection matrix and hybrid beamformer for narrowband MIMO systems. Then, we generalize the proposed joint design to broadband MIMO systems with orthogonal frequency division multiplexing (OFDM) modulation by leveraging the angular sparsity of frequency-selective mmWave channels. Simulation results demonstrate that the proposed joint designs can significantly enhance the spectral efficiency of the systems of interest and achieve superior performance over the existing designs. △ Less","27 June, 2022",https://arxiv.org/pdf/2105.13647
Embracing New Techniques in Deep Learning for Estimating Image Memorability,Coen D. Needell;Wilma A. Bainbridge,"Various work has suggested that the memorability of an image is consistent across people, and thus can be treated as an intrinsic property of an image. Using computer vision models, we can make specific predictions about what people will remember or forget. While older work has used now-outdated deep learning architectures to predict image memorability, innovations in the field have given us new techniques to apply to this problem. Here, we propose and evaluate five alternative deep learning models which exploit developments in the field from the last five years, largely the introduction of residual neural networks, which are intended to allow the model to use semantic information in the memorability estimation process. These new models were tested against the prior state of the art with a combined dataset built to optimize both within-category and across-category predictions. Our findings suggest that the key prior memorability network had overstated its generalizability and was overfit on its training set. Our new models outperform this prior model, leading us to conclude that Residual Networks outperform simpler convolutional neural networks in memorability regression. We make our new state-of-the-art model readily available to the research community, allowing memory researchers to make predictions about memorability on a wider range of images. △ Less","8 January, 2022",https://arxiv.org/pdf/2105.10598
Covariance-Free Sparse Bayesian Learning,Alexander Lin;Andrew H. Song;Berkin Bilgic;Demba Ba,"Sparse Bayesian learning (SBL) is a powerful framework for tackling the sparse coding problem while also providing uncertainty quantification. The most popular inference algorithms for SBL exhibit prohibitively large computational costs for high-dimensional problems due to the need to maintain a large covariance matrix. To resolve this issue, we introduce a new method for accelerating SBL inference -- named covariance-free expectation maximization (CoFEM) -- that avoids explicit computation of the covariance matrix. CoFEM solves multiple linear systems to obtain unbiased estimates of the posterior statistics needed by SBL. This is accomplished by exploiting innovations from numerical linear algebra such as preconditioned conjugate gradient and a little-known diagonal estimation rule. For a large class of compressed sensing matrices, we provide theoretical justifications for why our method scales well in high-dimensional settings. Through simulations, we show that CoFEM can be up to thousands of times faster than existing baselines without sacrificing coding accuracy. Through applications to calcium imaging deconvolution and multi-contrast MRI reconstruction, we show that CoFEM enables SBL to tractably tackle high-dimensional sparse coding problems of practical interest. △ Less","8 April, 2022",https://arxiv.org/pdf/2105.10439
Multi-modal Visual Place Recognition in Dynamics-Invariant Perception Space,Lin Wu;Teng Wang;Changyin Sun,"Visual place recognition is one of the essential and challenging problems in the fields of robotics. In this letter, we for the first time explore the use of multi-modal fusion of semantic and visual modalities in dynamics-invariant space to improve place recognition in dynamic environments. We achieve this by first designing a novel deep learning architecture to generate the static semantic segmentation and recover the static image directly from the corresponding dynamic image. We then innovatively leverage the spatial-pyramid-matching model to encode the static semantic segmentation into feature vectors. In parallel, the static image is encoded using the popular Bag-of-words model. On the basis of the above multi-modal features, we finally measure the similarity between the query image and target landmark by the joint similarity of their semantic and visual codes. Extensive experiments demonstrate the effectiveness and robustness of the proposed approach for place recognition in dynamic environments. △ Less","2 January, 2022",https://arxiv.org/pdf/2105.07800
Translating Extensive Form Games to Open Games with Agency,Matteo Capucci;Neil Ghani;Jérémy Ledent;Fredrik Nordvall Forsberg,"We show open games cover extensive form games with both perfect and imperfect information. Doing so forces us to address two current weaknesses in open games: the lack of a notion of player and their agency within open games, and the lack of choice operators. Using the former we construct the latter, and these choice operators subsume previous proposed operators for open games, thereby making progress towards a core, canonical and ergonomic calculus of game operators. Collectively these innovations increase the level of compositionality of open games, and demonstrate their expressiveness. △ Less","3 November, 2022",https://arxiv.org/pdf/2105.06763
Stochastic Neural Networks for Automatic Cell Tracking in Microscopy Image Sequences of Bacterial Colonies,Sorena Sarmadi;James J. Winkle;Razan N. Alnahhas;Matthew R. Bennett;Krešimir Josić;Andreas Mang;Robert Azencott,"Our work targets automated analysis to quantify the growth dynamics of a population of bacilliform bacteria. We propose an innovative approach to frame-sequence tracking of deformable-cell motion by the automated minimization of a new, specific cost functional. This minimization is implemented by dedicated Boltzmann machines (stochastic recurrent neural networks). Automated detection of cell divisions is handled similarly by successive minimizations of two cost functions, alternating the identification of children pairs and parent identification. We validate the proposed automatic cell tracking algorithm using (i) recordings of simulated cell colonies that closely mimic the growth dynamics of E. coli in microfluidic traps and (ii) real data. On a batch of 1100 simulated image frames, cell registration accuracies per frame ranged from 94.5% to 100%, with a high average. Our initial tests using experimental image sequences (i.e., real data) of E. coli colonies also yield convincing results, with a registration accuracy ranging from 90% to 100%. △ Less","8 July, 2022",https://arxiv.org/pdf/2104.13482
The latent structure of global scientific development,Lili Miao;Dakota Murray;Woo-Sung Jung;Vincent Larivière;Cassidy R. Sugimoto;Yong-Yeol Ahn,"Science is essential to innovation and economic prosperity. Although studies have shown that national scientific development is affected by geographic, historic, and economic factors, it remains unclear whether there are universal structures and trajectories of national scientific development that can inform forecasting and policymaking. Here, by examining countries' scientific 'exports'-publications that are indexed in international databases-we reveal a three-cluster structure in the relatedness network of disciplines that underpin national scientific development and the organization of global science. Tracing the evolution of national research portfolios reveals that while nations are proceeding to more diverse research profiles individually, scientific production is increasingly specialized in global science over the past decades. By uncovering the underlying structure of scientific development and connecting it with economic development, our results may offer a new perspective on the evolution of global science. △ Less","30 March, 2022",https://arxiv.org/pdf/2104.10812
Towards Electronic Structure-Based Ab-Initio Molecular Dynamics Simulations with Hundreds of Millions of Atoms,Robert Schade;Tobias Kenter;Hossam Elgabarty;Michael Lass;Ole Schütt;Alfio Lazzaro;Hans Pabst;Stephan Mohr;Jürg Hutter;Thomas D. Kühne;Christian Plessl,"We push the boundaries of electronic structure-based \textit{ab-initio} molecular dynamics (AIMD) beyond 100 million atoms. This scale is otherwise barely reachable with classical force-field methods or novel neural network and machine learning potentials. We achieve this breakthrough by combining innovations in linear-scaling AIMD, efficient and approximate sparse linear algebra, low and mixed-precision floating-point computation on GPUs, and a compensation scheme for the errors introduced by numerical approximations. The core of our work is the non-orthogonalized local submatrix method (NOLSM), which scales very favorably to massively parallel computing systems and translates large sparse matrix operations into highly parallel, dense matrix operations that are ideally suited to hardware accelerators. We demonstrate that the NOLSM method, which is at the center point of each AIMD step, is able to achieve a sustained performance of 324 PFLOP/s in mixed FP16/FP32 precision corresponding to an efficiency of 67.7% when running on 1536 NVIDIA A100 GPUs. △ Less","31 January, 2022",https://arxiv.org/pdf/2104.08245
QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,Michihiro Yasunaga;Hongyu Ren;Antoine Bosselut;Percy Liang;Jure Leskovec,"The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. In this work, we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on QA benchmarks in the commonsense (CommonsenseQA, OpenBookQA) and biomedical (MedQA-USMLE) domains. QA-GNN outperforms existing LM and LM+KG models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions. △ Less","12 December, 2022",https://arxiv.org/pdf/2104.06378
Non-autoregressive Transformer-based End-to-end ASR using BERT,Fu-Hao Yu;Kuan-Yu Chen,"Transformer-based models have led to significant innovation in classical and practical subjects as varied as speech processing, natural language processing, and computer vision. On top of the Transformer, attention-based end-to-end automatic speech recognition (ASR) models have recently become popular. Specifically, non-autoregressive modeling, which boasts fast inference and performance comparable to conventional autoregressive methods, is an emerging research topic. In the context of natural language processing, the bidirectional encoder representations from Transformers (BERT) model has received widespread attention, partially due to its ability to infer contextualized word representations and to enable superior performance for downstream tasks while needing only simple fine-tuning. Motivated by the success, we intend to view speech recognition as a downstream task of BERT, thus an ASR system is expected to be deduced by performing fine-tuning. Consequently, to not only inherit the advantages of non-autoregressive ASR models but also enjoy the benefits of a pre-trained language model (e.g., BERT), we propose a non-autoregressive Transformer-based end-to-end ASR model based on BERT. We conduct a series of experiments on the AISHELL-1 dataset that demonstrate competitive or superior results for the model when compared to state-of-the-art ASR systems. △ Less","17 May, 2022",https://arxiv.org/pdf/2104.04805
A Strategy for Advancing Research and Impact in New Computing Paradigms,Rajkumar Buyya;Sukhpal Singh Gill;Satish Narayana Srirama;Rami Bahsoon;San Murugesan,"In the world of Information Technology, new computing paradigms, driven by requirements of different classes of problems and applications, emerge rapidly. These new computing paradigms pose many new research challenges. Researchers from different disciplines are working together to develop innovative solutions addressing them. In newer research areas with many unknowns, creating roadmaps, enabling tools, inspiring technological and application demonstrators offer confidence and prove feasibility and effectiveness of new paradigm. Drawing on our experience, we share strategy for advancing the field and community building in new and emerging computing research areas. We discuss how the development simulators can be cost-effective in accelerating design of real systems. We highlight strategic role played by different types of publications, conferences, and educational programs. We illustrate effectiveness of elements of our strategy with a case study on progression of cloud computing paradigm. △ Less","27 January, 2022",https://arxiv.org/pdf/2104.04070
Classically-Verifiable Quantum Advantage from a Computational Bell Test,Gregory D. Kahanamoku-Meyer;Soonwon Choi;Umesh V. Vazirani;Norman Y. Yao,"We propose and analyze a novel interactive protocol for demonstrating quantum computational advantage, which is efficiently classically verifiable. Our protocol relies upon the cryptographic hardness of trapdoor claw-free functions (TCFs). Through a surprising connection to Bell's inequality, our protocol avoids the need for an adaptive hardcore bit, with essentially no increase in the quantum circuit complexity and no extra cryptographic assumptions. Crucially, this expands the set of compatible TCFs, and we propose two new constructions: one based upon the decisional Diffie-Hellman problem and the other based upon Rabin's function, x^2 \bmod N. We also describe two independent innovations which improve the efficiency of our protocol's implementation: (i) a scheme to discard so-called ""garbage bits"", thereby removing the need for reversibility in the quantum circuits, and (ii) a natural way of performing post-selection which significantly reduces the fidelity needed to demonstrate quantum advantage. These two constructions may also be of independent interest, as they may be applicable to other TCF-based quantum cryptography such as certifiable random number generation. Finally, we design several efficient circuits for x^2 \bmod N and describe a blueprint for their implementation on a Rydberg-atom-based quantum computer. △ Less","16 August, 2022",https://arxiv.org/pdf/2104.00687
Elsa: Energy-based learning for semi-supervised anomaly detection,Sungwon Han;Hyeonho Song;Seungeon Lee;Sungwon Park;Meeyoung Cha,"Anomaly detection aims at identifying deviant instances from the normal data distribution. Many advances have been made in the field, including the innovative use of unsupervised contrastive learning. However, existing methods generally assume clean training data and are limited when the data contain unknown anomalies. This paper presents Elsa, a novel semi-supervised anomaly detection approach that unifies the concept of energy-based models with unsupervised contrastive learning. Elsa instills robustness against any data contamination by a carefully designed fine-tuning step based on the new energy function that forces the normal data to be divided into classes of prototypes. Experiments on multiple contamination scenarios show the proposed model achieves SOTA performance. Extensive analyses also verify the contribution of each component in the proposed model. Beyond the experiments, we also offer a theoretical interpretation of why contrastive learning alone cannot detect anomalies under data contamination. △ Less","3 January, 2022",https://arxiv.org/pdf/2103.15296
Fully Automated 2D and 3D Convolutional Neural Networks Pipeline for Video Segmentation and Myocardial Infarction Detection in Echocardiography,Oumaima Hamila;Sheela Ramanna;Christopher J. Henry;Serkan Kiranyaz;Ridha Hamila;Rashid Mazhar;Tahir Hamid,"Cardiac imaging known as echocardiography is a non-invasive tool utilized to produce data including images and videos, which cardiologists use to diagnose cardiac abnormalities in general and myocardial infarction (MI) in particular. Echocardiography machines can deliver abundant amounts of data that need to be quickly analyzed by cardiologists to help them make a diagnosis and treat cardiac conditions. However, the acquired data quality varies depending on the acquisition conditions and the patient's responsiveness to the setup instructions. These constraints are challenging to doctors especially when patients are facing MI and their lives are at stake. In this paper, we propose an innovative real-time end-to-end fully automated model based on convolutional neural networks (CNN) to detect MI depending on regional wall motion abnormalities (RWMA) of the left ventricle (LV) from videos produced by echocardiography. Our model is implemented as a pipeline consisting of a 2D CNN that performs data preprocessing by segmenting the LV chamber from the apical four-chamber (A4C) view, followed by a 3D CNN that performs a binary classification to detect if the segmented echocardiography shows signs of MI. We trained both CNNs on a dataset composed of 165 echocardiography videos each acquired from a distinct patient. The 2D CNN achieved an accuracy of 97.18% on data segmentation while the 3D CNN achieved 90.9% of accuracy, 100% of precision and 95% of recall on MI detection. Our results demonstrate that creating a fully automated system for MI detection is feasible and propitious. △ Less","3 August, 2022",https://arxiv.org/pdf/2103.14734
Deepened Graph Auto-Encoders Help Stabilize and Enhance Link Prediction,Xinxing Wu;Qiang Cheng,"Graph neural networks have been used for a variety of learning tasks, such as link prediction, node classification, and node clustering. Among them, link prediction is a relatively under-studied graph learning task, with current state-of-the-art models based on one- or two-layer of shallow graph auto-encoder (GAE) architectures. In this paper, we focus on addressing a limitation of current methods for link prediction, which can only use shallow GAEs and variational GAEs, and creating effective methods to deepen (variational) GAE architectures to achieve stable and competitive performance. Our proposed methods innovatively incorporate standard auto-encoders (AEs) into the architectures of GAEs, where standard AEs are leveraged to learn essential, low-dimensional representations via seamlessly integrating the adjacency information and node features, while GAEs further build multi-scaled low-dimensional representations via residual connections to learn a compact overall embedding for link prediction. Empirically, extensive experiments on various benchmarking datasets verify the effectiveness of our methods and demonstrate the competitive performance of our deepened graph models for link prediction. Theoretically, we prove that our deep extensions inclusively express multiple polynomial filters with different orders. △ Less","26 August, 2022",https://arxiv.org/pdf/2103.11414
Automated liver tissues delineation techniques: A systematic survey on machine learning current trends and future orientations,Ayman Al-Kababji;Faycal Bensaali;Sarada Prasad Dakua;Yassine Himeur,"Machine learning and computer vision techniques have grown rapidly in recent years due to their automation, suitability, and ability to generate astounding results. Hence, in this paper, we survey the key studies that are published between 2014 and 2022, showcasing the different machine learning algorithms researchers have used to segment the liver, hepatic tumors, and hepatic-vasculature structures. We divide the surveyed studies based on the tissue of interest (hepatic-parenchyma, hepatic-tumors, or hepatic-vessels), highlighting the studies that tackle more than one task simultaneously. Additionally, the machine learning algorithms are classified as either supervised or unsupervised, and they are further partitioned if the amount of work that falls under a certain scheme is significant. Moreover, different datasets and challenges found in literature and websites containing masks of the aforementioned tissues are thoroughly discussed, highlighting the organizers' original contributions and those of other researchers. Also, the metrics used excessively in literature are mentioned in our review, stressing their relevance to the task at hand. Finally, critical challenges and future directions are emphasized for innovative researchers to tackle, exposing gaps that need addressing, such as the scarcity of many studies on the vessels' segmentation challenge and why their absence needs to be dealt with sooner than later. △ Less","28 July, 2022",https://arxiv.org/pdf/2103.06384
Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas,Tom Hope;Ronen Tamari;Hyeonsu Kang;Daniel Hershcovich;Joel Chan;Aniket Kittur;Dafna Shahaf,"Large repositories of products, patents and scientific papers offer an opportunity for building systems that scour millions of ideas and help users discover inspirations. However, idea descriptions are typically in the form of unstructured text, lacking key structure that is required for supporting creative innovation interactions. Prior work has explored idea representations that were either limited in expressivity, required significant manual effort from users, or dependent on curated knowledge bases with poor coverage. We explore a novel representation that automatically breaks up products into fine-grained functional aspects capturing the purposes and mechanisms of ideas, and use it to support important creative innovation interactions: functional search for ideas, and exploration of the design space around a focal problem by viewing related problem perspectives pooled from across many products. In user studies, our approach boosts the quality of creative search and inspirations, substantially outperforming strong baselines by 50-60%. △ Less","17 February, 2022",https://arxiv.org/pdf/2102.09761
Peering Beyond the Gradient Veil with Distributed Auto Differentiation,Bradley T. Baker;Aashis Khanal;Vince D. Calhoun;Barak Pearlmutter;Sergey M. Plis,"Although distributed machine learning has opened up many new and exciting research frontiers, fragmentation of models and data across different machines, nodes, and sites still results in considerable communication overhead, impeding reliable training in real-world contexts. The focus on gradients as the primary shared statistic during training has spawned a number of intuitive algorithms for distributed deep learning; however, gradient-centric training of large deep neural networks (DNNs) tends to be communication-heavy, often requiring additional adaptations such as sparsity constraints, compression, quantization, and more, to curtail bandwidth. We introduce an innovative, communication-friendly approach for training distributed DNNs, which capitalizes on the outer-product structure of the gradient as revealed by the mechanics of auto-differentiation. The exposed structure of the gradient evokes a new class of distributed learning algorithm, which is naturally more communication-efficient than full gradient sharing. Our approach, called distributed auto-differentiation (dAD), builds off a marriage of rank-based compression and the innate structure of the gradient as an outer-product. We demonstrate that dAD trains more efficiently than other state of the art distributed methods on modern architectures, such as transformers, when applied to large-scale text and imaging datasets. The future of distributed learning, we determine, need not be dominated by gradient-centric algorithms. △ Less","3 February, 2022",https://arxiv.org/pdf/2102.09631
HyMap: eliciting hypotheses in early-stage software startups using cognitive mapping,Jorge Melegati;Eduardo Guerra;Xiaofeng Wang,"Context: Software startups develop innovative, software-intensive products. Given the uncertainty associated with such an innovative context, experimentation is a valuable approach for these companies, especially in the early stages of the development, when implementing unnecessary features represents a higher risk for companies' survival. Nevertheless, researchers have argued that the lack of clearly defined practices led to limited adoption of experimentation. In this regard, the first step is to define the hypotheses based on which teams will create experiments. Objective: We aim to develop a systematic technique to identify hypotheses for early-stage software startups. Methods: We followed a Design Science approach consisted of three cycles in the construction phase, that involved seven startups in total, and an evaluation of the final artifact within three startups. Results: We developed the HyMap, a hypotheses elicitation technique based on cognitive mapping. It consists of a visual language to depict a cognitive map representing the founder's understanding of the product, and a process to elicit this map consisted of a series of questions the founder must answer. Our evaluation showed that the artifacts are clear, easy to use, and useful leading to hypotheses and facilitating founders to visualize their idea. Conclusion: Our study contributes to both descriptive and prescriptive bodies of knowledge. Regarding the first, it provides a better understanding of the guidance founders use to develop their startups and, for the latter, a technique to identify hypotheses in early-stage software startups. △ Less","9 January, 2022",https://arxiv.org/pdf/2102.09387
Block-Chain Technologies in Healthcare Analytics,Fathima Begum M;Subhashini Narayan,"Research has advanced to broaden its applications to cases of non-financial usage after the block-chain was presented by Bitcoin. Healthcare is one of the sectors in which block-chain has tremendous impacts. Exploration here is generally new yet developing quickly; along these lines, health informatics researchers and specialists are continually battling to stay up with research progress around there. The block-chain's information accessibility, protection and security characteristics have an auspicious future in healthcare, providing solutions to the healthcare framework's multifaceted nature, classification, integrity, interoperability and protection issues. This paper focuses on a scoping analysis of the current research into the use of healthcare system block-chain innovation. △ Less","28 June, 2022",https://arxiv.org/pdf/2102.08185
Wasserstein Graph Neural Networks for Graphs with Missing Attributes,Zhixian Chen;Tengfei Ma;Yangqiu Song;Yang Wang,"Missing node attributes is a common problem in real-world graphs. Graph neural networks have been demonstrated power in graph representation learning while their performance is affected by the completeness of graph information. Most of them are not specified for missing-attribute graphs and fail to leverage incomplete attribute information effectively. In this paper, we propose an innovative node representation learning framework, Wasserstein Graph Neural Network (WGNN), to mitigate the problem. To make the most of limited observed attribute information and capture the uncertainty caused by missing values, we express nodes as low-dimensional distributions derived from the decomposition of the attribute matrix. Furthermore, we strengthen the expressiveness of representations by developing a novel message passing schema that aggregates distributional information from neighbors in the Wasserstein space. We test WGNN in node classification tasks under two missing-attribute cases on both synthetic and real-world datasets. In addition, we find WGNN suitable to recover missing values and adapt them to tackle matrix completion problems with graphs of users and items. Experimental results on both tasks demonstrate the superiority of our method. △ Less","16 February, 2022",https://arxiv.org/pdf/2102.03450
Deep learning via LSTM models for COVID-19 infection forecasting in India,Rohitash Chandra;Ayush Jain;Divyanshu Singh Chauhan,"The COVID-19 pandemic continues to have major impact to health and medical infrastructure, economy, and agriculture. Prominent computational and mathematical models have been unreliable due to the complexity of the spread of infections. Moreover, lack of data collection and reporting makes modelling attempts difficult and unreliable. Hence, we need to re-look at the situation with reliable data sources and innovative forecasting models. Deep learning models such as recurrent neural networks are well suited for modelling spatiotemporal sequences. In this paper, we apply recurrent neural networks such as long short term memory (LSTM), bidirectional LSTM, and encoder-decoder LSTM models for multi-step (short-term) COVID-19 infection forecasting. We select Indian states with COVID-19 hotpots and capture the first (2020) and second (2021) wave of infections and provide two months ahead forecast. Our model predicts that the likelihood of another wave of infections in October and November 2021 is low; however, the authorities need to be vigilant given emerging variants of the virus. The accuracy of the predictions motivate the application of the method in other countries and regions. Nevertheless, the challenges in modelling remain due to the reliability of data and difficulties in capturing factors such as population density, logistics, and social aspects such as culture and lifestyle. △ Less","17 January, 2022",https://arxiv.org/pdf/2101.11881
Untargeted Poisoning Attack Detection in Federated Learning via Behavior Attestation,Ranwa Al Mallah;David Lopez;Godwin Badu Marfo;Bilal Farooq,"Federated Learning (FL) is a paradigm in Machine Learning (ML) that addresses data privacy, security, access rights and access to heterogeneous information issues by training a global model using distributed nodes. Despite its advantages, there is an increased potential for cyberattacks on FL-based ML techniques that can undermine the benefits. Model-poisoning attacks on FL target the availability of the model. The adversarial objective is to disrupt the training. We propose attestedFL, a defense mechanism that monitors the training of individual nodes through state persistence in order to detect a malicious worker. A fine-grained assessment of the history of the worker permits the evaluation of its behavior in time and results in innovative detection strategies. We present three lines of defense that aim at assessing if the worker is reliable by observing if the node is really training, advancing towards a goal. Our defense exposes an attacker's malicious behavior and removes unreliable nodes from the aggregation process so that the FL process converge faster. Through extensive evaluations and against various adversarial settings, attestedFL increased the accuracy of the model between 12% to 58% under different scenarios such as attacks performed at different stages of convergence, attackers colluding and continuous attacks. △ Less","16 January, 2022",https://arxiv.org/pdf/2101.10904
Towards Practical Robustness Analysis for DNNs based on PAC-Model Learning,Renjue Li;Pengfei Yang;Cheng-Chao Huang;Youcheng Sun;Bai Xue;Lijun Zhang,"To analyse local robustness properties of deep neural networks (DNNs), we present a practical framework from a model learning perspective. Based on black-box model learning with scenario optimisation, we abstract the local behaviour of a DNN via an affine model with the probably approximately correct (PAC) guarantee. From the learned model, we can infer the corresponding PAC-model robustness property. The innovation of our work is the integration of model learning into PAC robustness analysis: that is, we construct a PAC guarantee on the model level instead of sample distribution, which induces a more faithful and accurate robustness evaluation. This is in contrast to existing statistical methods without model learning. We implement our method in a prototypical tool named DeepPAC. As a black-box method, DeepPAC is scalable and efficient, especially when DNNs have complex structures or high-dimensional inputs. We extensively evaluate DeepPAC, with 4 baselines (using formal verification, statistical methods, testing and adversarial attack) and 20 DNN models across 3 datasets, including MNIST, CIFAR-10, and ImageNet. It is shown that DeepPAC outperforms the state-of-the-art statistical method PROVERO, and it achieves more practical robustness analysis than the formal verification tool ERAN. Also, its results are consistent with existing DNN testing work like DeepGini. △ Less","13 April, 2022",https://arxiv.org/pdf/2101.10102
A Robust Blockchain Readiness Index Model,Elias Iosif;Klitos Christodoulou;Andreas Vlachos,"As the blockchain ecosystem gets more mature many businesses, investors, and entrepreneurs are seeking opportunities on working with blockchain systems and cryptocurrencies. A critical challenge for these actors is to identify the most suitable environment to start or evolve their businesses. In general, the question is to identify which countries are offering the most suitable conditions to host their blockchain-based activities and implement their innovative projects. The Blockchain Readiness Index (BRI) provides a numerical metric (referred to as the blockchain readiness score) in measuring the maturity/readiness levels of a country in adopting blockchain and cryptocurrencies. In doing so, BRI leverages on techniques from information retrieval to algorithmically derive an index ranking for a set of countries. The index considers a range of indicators organized under five pillars: Government Regulation, Research, Technology, Industry, and User Engagement. In this paper, we further extent BRI with the capability of deriving the index - at the country level - even in the presence of missing information for the indicators. In doing so, we are proposing two weighting schemes namely, linear and sigmoid weighting for refining the initial estimates for the indicator values. A classification framework was employed to evaluate the effectiveness of the developed techniques which yielded to a significant classification accuracy. △ Less","4 February, 2022",https://arxiv.org/pdf/2101.09162
"ExpFinder: An Ensemble Expert Finding Model Integrating N
-gram Vector Space Model and μ
CO-HITS",Yong-Bin Kang;Hung Du;Abdur Rahim Mohammad Forkan;Prem Prakash Jayaraman;Amir Aryani;Timos Sellis,"Finding an expert plays a crucial role in driving successful collaborations and speeding up high-quality research development and innovations. However, the rapid growth of scientific publications and digital expertise data makes identifying the right experts a challenging problem. Existing approaches for finding experts given a topic can be categorised into information retrieval techniques based on vector space models, document language models, and graph-based models. In this paper, we propose \textit{ExpFinder}, a new ensemble model for expert finding, that integrates a novel N-gram vector space model, denoted as nVSM, and a graph-based model, denoted as \textit{$μ$CO-HITS}, that is a proposed variation of the CO-HITS algorithm. The key of nVSM is to exploit recent inverse document frequency weighting method for N-gram words and \textit{ExpFinder} incorporates nVSM into \textit{$μ$CO-HITS} to achieve expert finding. We comprehensively evaluate \textit{ExpFinder} on four different datasets from the academic domains in comparison with six different expert finding models. The evaluation results show that \textit{ExpFinder} is a highly effective model for expert finding, substantially outperforming all the compared models in 19% to 160.2%. △ Less","8 September, 2022",https://arxiv.org/pdf/2101.06821
Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation,Junliang Yu;Hongzhi Yin;Jundong Li;Qinyong Wang;Nguyen Quoc Viet Hung;Xiangliang Zhang,"Social relations are often used to improve recommendation quality when user-item interaction data is sparse in recommender systems. Most existing social recommendation models exploit pairwise relations to mine potential user preferences. However, real-life interactions among users are very complicated and user relations can be high-order. Hypergraph provides a natural way to model complex high-order relations, while its potentials for improving social recommendation are under-explored. In this paper, we fill this gap and propose a multi-channel hypergraph convolutional network to enhance social recommendation by leveraging high-order user relations. Technically, each channel in the network encodes a hypergraph that depicts a common high-order user relation pattern via hypergraph convolution. By aggregating the embeddings learned through multiple channels, we obtain comprehensive user representations to generate recommendation results. However, the aggregation operation might also obscure the inherent characteristics of different types of high-order connectivity information. To compensate for the aggregating loss, we innovatively integrate self-supervised learning into the training of the hypergraph convolutional network to regain the connectivity information with hierarchical mutual information maximization. The experimental results on multiple real-world datasets show that the proposed model outperforms the SOTA methods, and the ablation study verifies the effectiveness of the multi-channel setting and the self-supervised task. The implementation of our model is available via https://github.com/Coder-Yu/RecQ. △ Less","26 February, 2022",https://arxiv.org/pdf/2101.06448
SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things and Cyber-Physical Systems based on Machine Learning,Tanujay Saha;Najwa Aaraj;Neel Ajjarapu;Niraj K. Jha,"Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are increasingly being deployed across multiple functionalities, ranging from healthcare devices and wearables to critical infrastructures, e.g., nuclear power plants, autonomous vehicles, smart cities, and smart homes. These devices are inherently not secure across their comprehensive software, hardware, and network stacks, thus presenting a large attack surface that can be exploited by hackers. In this article, we present an innovative technique for detecting unknown system vulnerabilities, managing these vulnerabilities, and improving incident response when such vulnerabilities are exploited. The novelty of this approach lies in extracting intelligence from known real-world CPS/IoT attacks, representing them in the form of regular expressions, and employing machine learning (ML) techniques on this ensemble of regular expressions to generate new attack vectors and security vulnerabilities. Our results show that 10 new attack vectors and 122 new vulnerability exploits can be successfully generated that have the potential to exploit a CPS or an IoT ecosystem. The ML methodology achieves an accuracy of 97.4% and enables us to predict these attacks efficiently with an 87.2% reduction in the search space. We demonstrate the application of our method to the hacking of the in-vehicle network of a connected car. To defend against the known attacks and possible novel exploits, we discuss a defense-in-depth mechanism for various classes of attacks and the classification of data targeted by such attacks. This defense mechanism optimizes the cost of security measures based on the sensitivity of the protected resource, thus incentivizing its adoption in real-world CPS/IoT by cybersecurity practitioners. △ Less","19 October, 2022",https://arxiv.org/pdf/2101.02780
A Synergistic Kalman- and Deep Postfiltering Approach to Acoustic Echo Cancellation,Thomas Haubner;Mhd. Modar Halimeh;Andreas Brendel;Walter Kellermann,"We introduce a synergistic approach to double-talk robust acoustic echo cancellation combining adaptive Kalman filtering with a deep neural network-based postfilter. The proposed algorithm overcomes the well-known limitations of Kalman filter-based adaptation control in scenarios characterized by abrupt echo path changes. As the key innovation, we suggest to exploit the different statistical properties of the interfering signal components for robustly estimating the adaptation step size. This is achieved by leveraging the postfilter near-end estimate and the estimation error of the Kalman filter. The proposed synergistic scheme allows for rapid reconvergence of the adaptive filter after abrupt echo path changes without compromising the steady state performance achieved by state-of-the-art approaches in static scenarios. △ Less","4 March, 2022",https://arxiv.org/pdf/2012.08867
MLPerf Mobile Inference Benchmark,Vijay Janapa Reddi;David Kanter;Peter Mattson;Jared Duke;Thai Nguyen;Ramesh Chukka;Ken Shiring;Koan-Sin Tan;Mark Charlebois;William Chou;Mostafa El-Khamy;Jungwook Hong;Tom St. John;Cindy Trinh;Michael Buch;Mark Mazumder;Relia Markovic;Thomas Atta;Fatih Cakir;Masoud Charkhabi;Xiaodong Chen;Cheng-Ming Chiang;Dave Dexter;Terry Heo;Gunther Schmuelling,"This paper presents the first industry-standard open-source machine learning (ML) benchmark to allow perfor mance and accuracy evaluation of mobile devices with different AI chips and software stacks. The benchmark draws from the expertise of leading mobile-SoC vendors, ML-framework providers, and model producers. It comprises a suite of models that operate with standard data sets, quality metrics and run rules. We describe the design and implementation of this domain-specific ML benchmark. The current benchmark version comes as a mobile app for different computer vision and natural language processing tasks. The benchmark also supports non-smartphone devices, such as laptops and mobile PCs. Benchmark results from the first two rounds reveal the overwhelming complexity of the underlying mobile ML system stack, emphasizing the need for transparency in mobile ML performance analysis. The results also show that the strides being made all through the ML stack improve performance. Within six months, offline throughput improved by 3x, while latency reduced by as much as 12x. ML is an evolving field with changing use cases, models, data sets and quality targets. MLPerf Mobile will evolve and serve as an open-source community framework to guide research and innovation for mobile AI. △ Less","6 April, 2022",https://arxiv.org/pdf/2012.02328
Rethinking Weight Decay For Efficient Neural Network Pruning,Hugo Tessier;Vincent Gripon;Mathieu Léonardon;Matthieu Arzel;Thomas Hannagan;David Bertrand,"Introduced in the late 1980s for generalization purposes, pruning has now become a staple for compressing deep neural networks. Despite many innovations in recent decades, pruning approaches still face core issues that hinder their performance or scalability. Drawing inspiration from early work in the field, and especially the use of weight decay to achieve sparsity, we introduce Selective Weight Decay (SWD), which carries out efficient, continuous pruning throughout training. Our approach, theoretically grounded on Lagrangian smoothing, is versatile and can be applied to multiple tasks, networks, and pruning structures. We show that SWD compares favorably to state-of-the-art approaches, in terms of performance-to-parameters ratio, on the CIFAR-10, Cora, and ImageNet ILSVRC2012 datasets. △ Less","9 March, 2022",https://arxiv.org/pdf/2011.10520
Tight Bounds for Adversarially Robust Streams and Sliding Windows via Difference Estimators,David P. Woodruff;Samson Zhou,"In the adversarially robust streaming model, a stream of elements is presented to an algorithm and is allowed to depend on the output of the algorithm at earlier times during the stream. In the classic insertion-only model of data streams, Ben-Eliezer et. al. (PODS 2020, best paper award) show how to convert a non-robust algorithm into a robust one with a roughly 1/\varepsilon factor overhead. This was subsequently improved to a 1/\sqrt{\varepsilon} factor overhead by Hassidim et. al. (NeurIPS 2020, oral presentation), suppressing logarithmic factors. For general functions the latter is known to be best-possible, by a result of Kaplan et. al. (CRYPTO 2021). We show how to bypass this impossibility result by developing data stream algorithms for a large class of streaming problems, with no overhead in the approximation factor. Our class of streaming problems includes the most well-studied problems such as the L_2-heavy hitters problem, F_p-moment estimation, as well as empirical entropy estimation. We substantially improve upon all prior work on these problems, giving the first optimal dependence on the approximation factor. As in previous work, we obtain a general transformation that applies to any non-robust streaming algorithm and depends on the so-called twist number. However, the key technical innovation is that we apply the transformation to what we call a difference estimator for the streaming problem, rather than an estimator for the streaming problem itself. We then develop the first difference estimators for a wide range of problems. Our difference estimator methodology is not only applicable to the adversarially robust model, but to other streaming models where temporal properties of the data play a central role. (Abstract shortened to meet arXiv limit.) △ Less","21 October, 2022",https://arxiv.org/pdf/2011.07471
Fractal Autoencoders for Feature Selection,Xinxing Wu;Qiang Cheng,"Feature selection reduces the dimensionality of data by identifying a subset of the most informative features. In this paper, we propose an innovative framework for unsupervised feature selection, called fractal autoencoders (FAE). It trains a neural network to pinpoint informative features for global exploring of representability and for local excavating of diversity. Architecturally, FAE extends autoencoders by adding a one-to-one scoring layer and a small sub-neural network for feature selection in an unsupervised fashion. With such a concise architecture, FAE achieves state-of-the-art performances; extensive experimental results on fourteen datasets, including very high-dimensional data, have demonstrated the superiority of FAE over existing contemporary methods for unsupervised feature selection. In particular, FAE exhibits substantial advantages on gene expression data exploration, reducing measurement cost by about 15\% over the widely used L1000 landmark genes. Further, we show that the FAE framework is easily extensible with an application. △ Less","4 January, 2022",https://arxiv.org/pdf/2010.09430
Algorithmic Stability and Generalization of an Unsupervised Feature Selection Algorithm,Xinxing Wu;Qiang Cheng,"Feature selection, as a vital dimension reduction technique, reduces data dimension by identifying an essential subset of input features, which can facilitate interpretable insights into learning and inference processes. Algorithmic stability is a key characteristic of an algorithm regarding its sensitivity to perturbations of input samples. In this paper, we propose an innovative unsupervised feature selection algorithm attaining this stability with provable guarantees. The architecture of our algorithm consists of a feature scorer and a feature selector. The scorer trains a neural network (NN) to globally score all the features, and the selector adopts a dependent sub-NN to locally evaluate the representation abilities for selecting features. Further, we present algorithmic stability analysis and show that our algorithm has a performance guarantee via a generalization error bound. Extensive experimental results on real-world datasets demonstrate superior generalization performance of our proposed algorithm to strong baseline methods. Also, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically confirmed. △ Less","5 January, 2022",https://arxiv.org/pdf/2010.09416
Learning Accurate Entropy Model with Global Reference for Image Compression,Yichen Qian;Zhiyu Tan;Xiuyu Sun;Ming Lin;Dongyang Li;Zhenhong Sun;Hao Li;Rong Jin,"In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry. △ Less","4 January, 2022",https://arxiv.org/pdf/2010.08321
Guided Data Discovery in Interactive Visualizations via Active Search,Shayan Monadjemi;Sunwoo Ha;Quan Nguyen;Henry Chai;Roman Garnett;Alvitta Ottley,"Recent advances in visual analytics have enabled us to learn from user interactions and uncover analytic goals. These innovations set the foundation for actively guiding users during data exploration. Providing such guidance will become more critical as datasets grow in size and complexity, precluding exhaustive investigation. Meanwhile, the machine learning community also struggles with datasets growing in size and complexity, precluding exhaustive labeling. Active learning is a broad family of algorithms developed for actively guiding models during training. We will consider the intersection of these analogous research thrusts. First, we discuss the nuances of matching the choice of an active learning algorithm to the task at hand. This is critical for performance, a fact we demonstrate in a simulation study. We then present results of a user study for the particular task of data discovery guided by an active learning algorithm specifically designed for this task. △ Less","15 July, 2022",https://arxiv.org/pdf/2010.08155
Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation,Changhong Fu;Bowen Li;Fangqiang Ding;Fuling Lin;Geng Lu,"Aerial tracking, which has exhibited its omnipresent dedication and splendid performance, is one of the most active applications in the remote sensing field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system, equipped with a visual tracking approach, has been widely used in aviation, navigation, agriculture,transportation, and public security, etc. As is mentioned above, the UAV-based aerial tracking platform has been gradually developed from research to practical application stage, reaching one of the main aerial remote sensing technologies in the future. However, due to the real-world onerous situations, e.g., harsh external challenges, the vibration of the UAV mechanical structure (especially under strong wind conditions), the maneuvering flight in complex environment, and the limited computation resources onboard, accuracy, robustness, and high efficiency are all crucial for the onboard tracking methods. Recently, the discriminative correlation filter (DCF)-based trackers have stood out for their high computational efficiency and appealing robustness on a single CPU, and have flourished in the UAV visual tracking community. In this work, the basic framework of the DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art DCF-based trackers are orderly summarized according to their innovations for solving various issues. Besides, exhaustive and quantitative experiments have been extended on various prevailing UAV tracking benchmarks, i.e., UAV123, UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903 frames in total. The experiments show the performance, verify the feasibility, and demonstrate the current challenges of DCF-based trackers onboard UAV tracking. △ Less","24 May, 2022",https://arxiv.org/pdf/2010.06255
CoKe: Localized Contrastive Learning for Robust Keypoint Detection,Yutong Bai;Angtian Wang;Adam Kortylewski;Alan Yuille,"In this paper, we introduce a contrastive learning framework for keypoint detection (CoKe). Keypoint detection differs from other visual tasks where contrastive learning has been applied because the input is a set of images in which multiple keypoints are annotated. This requires the contrastive learning to be extended such that the keypoints are represented and detected independently, which enables the contrastive loss to make the keypoint features different from each other and from the background. Our approach has two benefits: It enables us to exploit contrastive learning for keypoint detection, and by detecting each keypoint independently the detection becomes more robust to occlusion compared to holistic methods, such as stacked hourglass networks, which attempt to detect all keypoints jointly. Our CoKe framework introduces several technical innovations. In particular, we introduce: (i) A clutter bank to represent non-keypoint features; (ii) a keypoint bank that stores prototypical representations of keypoints to approximate the contrastive loss between keypoints; and (iii) a cumulative moving average update to learn the keypoint prototypes while training the feature extractor. Our experiments on a range of diverse datasets (PASCAL3D+, MPII, ObjectNet3D) show that our approach works as well, or better than, alternative methods for keypoint detection, even for human keypoints, for which the literature is vast. Moreover, we observe that CoKe is exceptionally robust to partial occlusion and previously unseen object poses. △ Less","5 December, 2022",https://arxiv.org/pdf/2009.14115
"Sensing Technologies for Crowd Management, Adaptation, and Information Dissemination in Public Transportation Systems: A Review",Donatella Darsena;Giacinto Gelli;Ivan Iudice;Francesco Verde,"Management of crowd information in public transportation (PT) systems is crucial, both to foster sustainable mobility, by increasing the user's comfort and satisfaction during normal operation, as well as to cope with emergency situations, such as pandemic crises, as recently experienced with COVID-19 limitations. This paper presents a taxonomy and review of sensing technologies based on Internet of Things (IoT) for real-time crowd analysis, which can be adopted in the different segments of the PT system (buses/trams/trains, railway/metro stations, and bus/tram stops). To discuss such technologies in a clear systematic perspective, we introduce a reference architecture for crowd management, which employs modern information and communication technologies (ICT) in order to: (i) monitor and predict crowding events; (ii) implement crowd-aware policies for real-time and adaptive operation control in intelligent transportation systems (ITSs); (iii) inform in real-time the users of the crowding status of the PT system, by means of electronic displays installed inside vehicles or at bus/tram stops/stations, and/or by mobile transport applications. It is envisioned that the innovative crowd management functionalities enabled by ICT/IoT sensing technologies can be incrementally implemented as an add-on to state-of-the-art ITS platforms, which are already in use by major PT companies operating in urban areas. Moreover, it is argued that, in this new framework, additional services can be delivered to the passengers, such as, e.g., on-line ticketing, vehicle access control and reservation in severely crowded situations, and evolved crowd-aware route planning. △ Less","17 November, 2022",https://arxiv.org/pdf/2009.12619
Hierarchical Message-Passing Graph Neural Networks,Zhiqiang Zhong;Cheng-Te Li;Jun Pang,"Graph Neural Networks (GNNs) have become a prominent approach to machine learning with graphs and have been increasingly applied in a multitude of domains. Nevertheless, since most existing GNN models are based on flat message-passing mechanisms, two limitations need to be tackled: (i) they are costly in encoding long-range information spanning the graph structure; (ii) they are failing to encode features in the high-order neighbourhood in the graphs as they only perform information aggregation across the observed edges in the original graph. To deal with these two issues, we propose a novel Hierarchical Message-passing Graph Neural Networks framework. The key idea is generating a hierarchical structure that re-organises all nodes in a flat graph into multi-level super graphs, along with innovative intra- and inter-level propagation manners. The derived hierarchy creates shortcuts connecting far-away nodes so that informative long-range interactions can be efficiently accessed via message passing and incorporates meso- and macro-level semantics into the learned node representations. We present the first model to implement this framework, termed Hierarchical Community-aware Graph Neural Network (HC-GNN), with the assistance of a hierarchical community detection algorithm. The theoretical analysis illustrates HC-GNN's remarkable capacity in capturing long-range information without introducing heavy additional computation complexity. Empirical experiments conducted on 9 datasets under transductive, inductive, and few-shot settings exhibit that HC-GNN can outperform state-of-the-art GNN models in network analysis tasks, including node classification, link prediction, and community detection. Moreover, the model analysis further demonstrates HC-GNN's robustness facing graph sparsity and the flexibility in incorporating different GNN encoders. △ Less","26 October, 2022",https://arxiv.org/pdf/2009.03717
A Review on Drivers Red Light Running Behavior Predictions and Technology Based Countermeasures,Md Mostafizur Rahman Komol;Jack Pinnow;Mohammed Elhenawy;Shamsunnahar Yasmin;Mahmoud Masoud;Sebastien Glaser;Andry Rakotonirainy,"Red light running at signalised intersections is a growing road safety issue worldwide, leading to the rapid development of advanced intelligent transportation technologies and countermeasures. However, existing studies have yet to summarise and present the effect of these technology based innovations in improving safety. This paper represents a comprehensive review of red light running behaviour prediction methodologies and technology-based countermeasures. Specifically, the major focus of this study is to provide a comprehensive review on two streams of literature targeting red light running and stop and go behaviour at signalised intersection (1) studies focusing on modelling and predicting the red light running and stop and go related driver behaviour and (2) studies focusing on the effectiveness of different technology based countermeasures which combat such unsafe behaviour. The study provides a systematic guide to assist researchers and stakeholders in understanding how to best identify red light running and stop and go associated driving behaviour and subsequently implement countermeasures to combat such risky behaviour and improve the associated safety. △ Less","13 March, 2022",https://arxiv.org/pdf/2008.06727
RARTS: An Efficient First-Order Relaxed Architecture Search Method,Fanghui Xue;Yingyong Qi;Jack Xin,"Differentiable architecture search (DARTS) is an effective method for data-driven neural network design based on solving a bilevel optimization problem. Despite its success in many architecture search tasks, there are still some concerns about the accuracy of first-order DARTS and the efficiency of the second-order DARTS. In this paper, we formulate a single level alternative and a relaxed architecture search (RARTS) method that utilizes the whole dataset in architecture learning via both data and network splitting, without involving mixed second derivatives of the corresponding loss functions like DARTS. In our formulation of network splitting, two networks with different but related weights cooperate in search of a shared architecture. The advantage of RARTS over DARTS is justified by a convergence theorem and an analytically solvable model. Moreover, RARTS outperforms DARTS and its variants in accuracy and search efficiency, as shown in adequate experimental results. For the task of searching topological architecture, i.e., the edges and the operations, RARTS obtains a higher accuracy and 60\% reduction of computational cost than second-order DARTS on CIFAR-10. RARTS continues to out-perform DARTS upon transfer to ImageNet and is on par with recent variants of DARTS even though our innovation is purely on the training algorithm without modifying search space. For the task of searching width, i.e., the number of channels in convolutional layers, RARTS also outperforms the traditional network pruning benchmarks. Further experiments on the public architecture search benchmark like NATS-Bench also support the preeminence of RARTS. △ Less","24 June, 2022",https://arxiv.org/pdf/2008.03901
Energy-Aware Graph Task Scheduling in Software-Defined Air-Ground Integrated Vehicular Networks,Minghui LiWang;Zhibin Gao;Xianbin Wang,"The Software-Defined Air-Ground integrated Vehicular (SD-AGV) networks have emerged as a promising paradigm, which realize the flexible on-ground resource sharing to support innovative applications for UAVs with heavy computational overhead. In this paper, we investigate a vehicular cloud-assisted task scheduling problem in SD-AGV networks, where the computation-intensive tasks carried by UAVs, and the vehicular cloud are modeled via graph-based representation. To map each component of the graph tasks to a feasible vehicle, while achieving the trade-off among minimizing UAVs' task completion time, energy consumption, and the data exchange cost among moving vehicles, we formulate the problem as a mixed-integer non-linear programming problem, which is Np-hard. Moreover, the constraint associated with preserving task structures poses addressing the subgraph isomorphism problem over dynamic vehicular topology, that further complicates the algorithm design. Motivated by which, we propose an efficient decoupled approach by separating the template (feasible mappings between components and vehicles) searching from the transmission power allocation. For the former, we present an efficient algorithm of searching for all the isomorphic subgraphs with low computation complexity. For the latter, we introduce a power allocation algorithm by applying p-norm and convex optimization techniques. Extensive simulations demonstrate that the proposed approach outperforms the benchmark methods considering various problem sizes. △ Less","9 May, 2022",https://arxiv.org/pdf/2008.01144
PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming,Alexander K. Lew;Monica Agrawal;David Sontag;Vikash K. Mansinghka,"Data cleaning is naturally framed as probabilistic inference in a generative model of ground-truth data and likely errors, but the diversity of real-world error patterns and the hardness of inference make Bayesian approaches difficult to automate. We present PClean, a probabilistic programming language (PPL) for leveraging dataset-specific knowledge to automate Bayesian cleaning. Compared to general-purpose PPLs, PClean tackles a restricted problem domain, enabling three modeling and inference innovations: (1) a non-parametric model of relational database instances, which users' programs customize; (2) a novel sequential Monte Carlo inference algorithm that exploits the structure of PClean's model class; and (3) a compiler that generates near-optimal SMC proposals and blocked-Gibbs rejuvenation kernels based on the user's model and data. We show empirically that short (< 50-line) PClean programs can: be faster and more accurate than generic PPL inference on data-cleaning benchmarks; match state-of-the-art data-cleaning systems in terms of accuracy and runtime (unlike generic PPL inference in the same runtime); and scale to real-world datasets with millions of records. △ Less","18 November, 2022",https://arxiv.org/pdf/2007.11838
Adaptive Regret for Control of Time-Varying Dynamics,Paula Gradu;Elad Hazan;Edgar Minasyan,"We consider the problem of online control of systems with time-varying linear dynamics. This is a general formulation that is motivated by the use of local linearization in control of nonlinear dynamical systems. To state meaningful guarantees over changing environments, we introduce the metric of {\it adaptive regret} to the field of control. This metric, originally studied in online learning, measures performance in terms of regret against the best policy in hindsight on {\it any interval in time}, and thus captures the adaptation of the controller to changing dynamics. Our main contribution is a novel efficient meta-algorithm: it converts a controller with sublinear regret bounds into one with sublinear {\it adaptive regret} bounds in the setting of time-varying linear dynamical systems. The main technical innovation is the first adaptive regret bound for the more general framework of online convex optimization with memory. Furthermore, we give a lower bound showing that our attained adaptive regret bound is nearly tight for this general framework. △ Less","11 February, 2022",https://arxiv.org/pdf/2007.04393
OptiLIME: Optimized LIME Explanations for Diagnostic Computer Algorithms,Giorgio Visani;Enrico Bagli;Federico Chesani,"Local Interpretable Model-Agnostic Explanations (LIME) is a popular method to perform interpretability of any kind of Machine Learning (ML) model. It explains one ML prediction at a time, by learning a simple linear model around the prediction. The model is trained on randomly generated data points, sampled from the training dataset distribution and weighted according to the distance from the reference point - the one being explained by LIME. Feature selection is applied to keep only the most important variables. LIME is widespread across different domains, although its instability - a single prediction may obtain different explanations - is one of the major shortcomings. This is due to the randomness in the sampling step, as well as to the flexibility in tuning the weights and determines a lack of reliability in the retrieved explanations, making LIME adoption problematic. In Medicine especially, clinical professionals trust is mandatory to determine the acceptance of an explainable algorithm, considering the importance of the decisions at stake and the related legal issues. In this paper, we highlight a trade-off between explanation's stability and adherence, namely how much it resembles the ML model. Exploiting our innovative discovery, we propose a framework to maximise stability, while retaining a predefined level of adherence. OptiLIME provides freedom to choose the best adherence-stability trade-off level and more importantly, it clearly highlights the mathematical properties of the retrieved explanation. As a result, the practitioner is provided with tools to decide whether the explanation is reliable, according to the problem at hand. We extensively test OptiLIME on a toy dataset - to present visually the geometrical findings - and a medical dataset. In the latter, we show how the method comes up with meaningful explanations both from a medical and mathematical standpoint. △ Less","7 February, 2022",https://arxiv.org/pdf/2006.05714
Providing reliability in Recommender Systems through Bernoulli Matrix Factorization,Fernando Ortega;Raúl Lara-Cabrera;Ángel González-Prieto;Jesús Bobadilla,"Beyond accuracy, quality measures are gaining importance in modern recommender systems, with reliability being one of the most important indicators in the context of collaborative filtering. This paper proposes Bernoulli Matrix Factorization (BeMF), which is a matrix factorization model, to provide both prediction values and reliability values. BeMF is a very innovative approach from several perspectives: a) it acts on model-based collaborative filtering rather than on memory-based filtering, b) it does not use external methods or extended architectures, such as existing solutions, to provide reliability, c) it is based on a classification-based model instead of traditional regression-based models, and d) matrix factorization formalism is supported by the Bernoulli distribution to exploit the binary nature of the designed classification model. The experimental results show that the more reliable a prediction is, the less liable it is to be wrong: recommendation quality improves after the most reliable predictions are selected. State-of-the-art quality measures for reliability have been tested, which shows that BeMF outperforms previous baseline methods and models. △ Less","4 March, 2022",https://arxiv.org/pdf/2006.03481
Go viral or go broadcast? Characterizing the virality and growth of cascades,Yafei Zhang;Lin Wang;Jonathan J. H. Zhu;Xiaofan Wang,"Quantifying the virality of cascades is an important question across disciplines such as the transmission of disease, the spread of information and the diffusion of innovations. An appropriate virality metric should be able to disambiguate between a shallow, broadcast-like diffusion process and a deep, multi-generational branching process. Although several valuable works have been dedicated to this field, most of them fail to take the position of the diffusion source into consideration, which makes them fall into the trap of graph isomorphism and would result in imprecise estimation of cascade virality inevitably under certain circumstances. In this paper, we propose a root-aware approach to quantifying the virality of cascades with proper consideration of the root node in a diffusion tree. With applications on synthetic and empirical cascades, we show the properties and potential utility of the proposed virality measure. Based on preferential attachment mechanisms, we further introduce a model to mimic the growth of cascades. The proposed model enables the interpolation between broadcast and viral spreading during the growth of cascades. Through numerical simulations, we demonstrate the effectiveness of the proposed model in characterizing the virality of growing cascades. Our work contributes to the understanding of cascade virality and growth, and could offer practical implications in a range of policy domains including viral marketing, infectious disease and information diffusion. △ Less","28 June, 2022",https://arxiv.org/pdf/2006.01027
Interacting with Explanations through Critiquing,Diego Antognini;Claudiu Musat;Boi Faltings,"Using personalized explanations to support recommendations has been shown to increase trust and perceived quality. However, to actually obtain better recommendations, there needs to be a means for users to modify the recommendation criteria by interacting with the explanation. We present a novel technique using aspect markers that learns to generate personalized explanations of recommendations from review texts, and we show that human users significantly prefer these explanations over those produced by state-of-the-art techniques. Our work's most important innovation is that it allows users to react to a recommendation by critiquing the textual explanation: removing (symmetrically adding) certain aspects they dislike or that are no longer relevant (symmetrically that are of interest). The system updates its user model and the resulting recommendations according to the critique. This is based on a novel unsupervised critiquing method for single- and multi-step critiquing with textual explanations. Experiments on two real-world datasets show that our system is the first to achieve good performance in adapting to the preferences expressed in multi-step critiquing. △ Less","12 January, 2022",https://arxiv.org/pdf/2005.11067
Conformal Prediction: a Unified Review of Theory and New Challenges,Matteo Fontana;Gianluca Zeni;Simone Vantini,"In this work we provide a review of basic ideas and novel developments about Conformal Prediction -- an innovative distribution-free, non-parametric forecasting method, based on minimal assumptions -- that is able to yield in a very straightforward way predictions sets that are valid in a statistical sense also in in the finite sample case. The in-depth discussion provided in the paper covers the theoretical underpinnings of Conformal Prediction, and then proceeds to list the more advanced developments and adaptations of the original idea. △ Less","29 July, 2022",https://arxiv.org/pdf/2005.07972
"SCRDet++: Detecting Small, Cluttered and Rotated Objects via Instance-Level Feature Denoising and Rotation Loss Smoothing",Xue Yang;Junchi Yan;Wenlong Liao;Xiaokang Yang;Jin Tang;Tao He,"Small and cluttered objects are common in real-world which are challenging for detection. The difficulty is further pronounced when the objects are rotated, as traditional detectors often routinely locate the objects in horizontal bounding box such that the region of interest is contaminated with background or nearby interleaved objects. In this paper, we first innovatively introduce the idea of denoising to object detection. Instance-level denoising on the feature map is performed to enhance the detection to small and cluttered objects. To handle the rotation variation, we also add a novel IoU constant factor to the smooth L1 loss to address the long standing boundary problem, which to our analysis, is mainly caused by the periodicity of angular (PoA) and exchangeability of edges (EoE). By combing these two features, our proposed detector is termed as SCRDet++. Extensive experiments are performed on large aerial images public datasets DOTA, DIOR, UCAS-AOD as well as natural image dataset COCO, scene text dataset ICDAR2015, small traffic light dataset BSTLD and our released S^2TLD by this paper. The results show the effectiveness of our approach. The released dataset S2TLD is made public available, which contains 5,786 images with 14,130 traffic light instances across five categories. △ Less","28 April, 2022",https://arxiv.org/pdf/2004.13316
Cheating in online gaming spreads through observation and victimization,Ji Eun Kim;Milena Tsvetkova,"Antisocial behavior can be contagious, spreading from individual to individual and rippling through social networks. Moreover, it can spread not only through third-party influence from observation, just like innovations or individual behavior do, but also through direct experience, via ""pay-it-forward"" retaliation. Here, we distinguish between the effects of observation and victimization for the contagion of antisocial behavior by analyzing large-scale digital-trace data. We study the spread of cheating in more than a million matches of an online multiplayer first-person shooter game, in which up to 100 players compete individually or in teams against strangers. We identify event sequences in which a player who observes or is killed by a certain number of cheaters starts cheating, and evaluate the extent to which these sequences would appear if we preserve the team and interaction structure but assume alternative gameplay scenarios. The results reveal that social contagion is only likely to exist for those who both observe and experience cheating, suggesting that third-party influence and ""pay-it-forward"" reciprocity interact positively. In addition, the effect is present only for those who both observe and experience more than once, suggesting that cheating is more likely to spread after repeated or multi-source exposure. Approaching online games as models of social systems, we use the findings to discuss strategies for targeted interventions to stem the spread of cheating and antisocial behavior more generally in online communities, schools, organizations, and sports. △ Less","27 January, 2022",https://arxiv.org/pdf/2003.11139
Directional Message Passing for Molecular Graphs,Johannes Gasteiger;Janek Groß;Stephan Günnemann,"Graph neural networks have recently achieved great successes in predicting quantum mechanical properties of molecules. These models represent a molecule as a graph using only the distance between atoms (nodes). They do not, however, consider the spatial direction from one atom to another, despite directional information playing a central role in empirical potentials for molecules, e.g. in angular potentials. To alleviate this limitation we propose directional message passing, in which we embed the messages passed between atoms instead of the atoms themselves. Each message is associated with a direction in coordinate space. These directional message embeddings are rotationally equivariant since the associated directions rotate with the molecule. We propose a message passing scheme analogous to belief propagation, which uses the directional information by transforming messages based on the angle between them. Additionally, we use spherical Bessel functions and spherical harmonics to construct theoretically well-founded, orthogonal representations that achieve better performance than the currently prevalent Gaussian radial basis representations while using fewer than 1/4 of the parameters. We leverage these innovations to construct the directional message passing neural network (DimeNet). DimeNet outperforms previous GNNs on average by 76% on MD17 and by 31% on QM9. Our implementation is available online. △ Less","5 April, 2022",https://arxiv.org/pdf/2003.03123
Distributed Adaptive Newton Methods with Global Superlinear Convergence,Jiaqi Zhang;Keyou You;Tamer Başar,"This paper considers the distributed optimization problem where each node of a peer-to-peer network minimizes a finite sum of objective functions by communicating with its neighboring nodes. In sharp contrast to the existing literature where the fastest distributed algorithms converge either with a global linear or a local superlinear rate, we propose a distributed adaptive Newton (DAN) algorithm with a global quadratic convergence rate. Our key idea lies in the design of a finite-time set-consensus method with Polyak's adaptive stepsize. Moreover, we introduce a low-rank matrix approximation (LA) technique to compress the innovation of Hessian matrix so that each node only needs to transmit message of dimension \mathcal{O}(p) (where p is the dimension of decision vectors) per iteration, which is essentially the same as that of first-order methods. Nevertheless, the resulting DAN-LA converges to an optimal solution with a global superlinear rate. Numerical experiments on logistic regression problems are conducted to validate their advantages over existing methods. △ Less","14 January, 2022",https://arxiv.org/pdf/2002.07378
Stratified Rule-Aware Network for Abstract Visual Reasoning,Sheng Hu;Yuqing Ma;Xianglong Liu;Yanlu Wei;Shihao Bai,"Abstract reasoning refers to the ability to analyze information, discover rules at an intangible level, and solve problems in innovative ways. Raven's Progressive Matrices (RPM) test is typically used to examine the capability of abstract reasoning. The subject is asked to identify the correct choice from the answer set to fill the missing panel at the bottom right of RPM (e.g., a 3\times3 matrix), following the underlying rules inside the matrix. Recent studies, taking advantage of Convolutional Neural Networks (CNNs), have achieved encouraging progress to accomplish the RPM test. However, they partly ignore necessary inductive biases of RPM solver, such as order sensitivity within each row/column and incremental rule induction. To address this problem, in this paper we propose a Stratified Rule-Aware Network (SRAN) to generate the rule embeddings for two input sequences. Our SRAN learns multiple granularity rule embeddings at different levels, and incrementally integrates the stratified embedding flows through a gated fusion module. With the help of embeddings, a rule similarity metric is applied to guarantee that SRAN can not only be trained using a tuplet loss but also infer the best answer efficiently. We further point out the severe defects existing in the popular RAVEN dataset for RPM test, which prevent from the fair evaluation of the abstract reasoning ability. To fix the defects, we propose an answer set generation algorithm called Attribute Bisection Tree (ABT), forming an improved dataset named Impartial-RAVEN (I-RAVEN for short). Extensive experiments are conducted on both PGM and I-RAVEN datasets, showing that our SRAN outperforms the state-of-the-art models by a considerable margin. △ Less","7 June, 2022",https://arxiv.org/pdf/2002.06838
A simple certifying algorithm for 3-edge-connectivity,Yung H. Tsin,"A linear-time certifying algorithm for 3-edge-connectivity is presented. Given an undirected graph G, if G is 3-edge-connected, the algorithm generates a construction sequence as a positive certificate for G. Otherwise, the algorithm decomposes G into its 3-edge-connected components and at the same time generates a construction sequence for each connected component as well as the bridges and a cactus representation of the cut-pairs in G. All of these are done by making only one pass over G using an innovative graph contraction technique. Moreover, the graph need not be 2-edge-connected. △ Less","29 January, 2022",https://arxiv.org/pdf/2002.04727
Exploration and Coordination of Complementary Multi-Robot Teams in a Hunter and Gatherer Scenario,Mehdi Dadvar;Saeed Moazami;Harley R. Myler;Hassan Zargarzadeh,"The hunter and gatherer approach copes with the problem of dynamic multi-robot task allocation, where tasks are unknowingly distributed over an environment. This approach employs two complementary teams of agents: one agile in exploring (hunters) and another dexterous in completing (gatherers) the tasks. Although this approach has been studied from the task planning point of view in our previous works, the multi-robot exploration and coordination aspects of the problem remain uninvestigated. This paper proposes a multi-robot exploration algorithm for hunters based on innovative notions of ""expected information gain"" to minimize the collective cost of task accomplishments in a distributed manner. Besides, we present a coordination solution between hunters and gatherers by integrating the novel notion of profit margins into the concept of expected information gain. Statistical analysis of extensive simulation results confirms the efficacy of the proposed algorithms compared in different environments with varying levels of obstacles complexities. We also demonstrate that the lack of effective coordination between hunters and gatherers significantly hurts the total effectiveness of the planning, especially in environments containing dense obstacles and confined corridors. Finally, it is statistically proven that the overall workload is distributed equally for each type of agent which ensures that the proposed solution is not biased to a particular agent and all agents behave analogously under similar characteristics. △ Less","31 March, 2022",https://arxiv.org/pdf/1912.07521
Spatial-Aware GAN for Unsupervised Person Re-identification,Changgong Zhang;Fangneng Zhan,"The recent person re-identification research has achieved great success by learning from a large number of labeled person images. On the other hand, the learned models often experience significant performance drops when applied to images collected in a different environment. Unsupervised domain adaptation (UDA) has been investigated to mitigate this constraint, but most existing systems adapt images at pixel level only and ignore obvious discrepancies at spatial level. This paper presents an innovative UDA-based person re-identification network that is capable of adapting images at both spatial and pixel levels simultaneously. A novel disentangled cycle-consistency loss is designed which guides the learning of spatial-level and pixel-level adaptation in a collaborative manner. In addition, a novel multi-modal mechanism is incorporated which is capable of generating images of different geometry views and augmenting training images effectively. Extensive experiments over a number of public datasets show that the proposed UDA network achieves superior person re-identification performance as compared with the state-of-the-art. △ Less","3 October, 2022",https://arxiv.org/pdf/1911.11312
Innovation and Strategic Network Formation,Krishna Dasaratha,"We study a model of innovation with a large number of firms that create new technologies by combining several discrete ideas. These ideas are created via private investment and spread between firms. Firms face a choice between secrecy, which protects existing intellectual property, and openness, which facilitates learning from others. Their decisions determine interaction rates between firms, and these interaction rates enter our model as link probabilities in a learning network. Higher interaction rates impose both positive and negative externalities, as there is more learning but also more competition. We show that the equilibrium learning network is at a critical threshold between sparse and dense networks. At equilibrium, the positive externality from interaction dominates: the innovation rate and welfare would be dramatically higher if the network were denser. So there are large returns to increasing interaction rates above the critical threshold. Nevertheless, several natural types of interventions fail to move the equilibrium away from criticality. One effective policy solution is to introduce informational intermediaries, such as public innovators who do not have incentives to be secretive. These intermediaries can facilitate a high-innovation equilibrium by transmitting ideas from one private firm to another. △ Less","31 March, 2022",https://arxiv.org/pdf/1911.06872
Cumulo: A Dataset for Learning Cloud Classes,Valentina Zantedeschi;Fabrizio Falasca;Alyson Douglas;Richard Strange;Matt J. Kusner;Duncan Watson-Parris,"One of the greatest sources of uncertainty in future climate projections comes from limitations in modelling clouds and in understanding how different cloud types interact with the climate system. A key first step in reducing this uncertainty is to accurately classify cloud types at high spatial and temporal resolution. In this paper, we introduce Cumulo, a benchmark dataset for training and evaluating global cloud classification models. It consists of one year of 1km resolution MODIS hyperspectral imagery merged with pixel-width 'tracks' of CloudSat cloud labels. Bringing these complementary datasets together is a crucial first step, enabling the Machine-Learning community to develop innovative new techniques which could greatly benefit the Climate community. To showcase Cumulo, we provide baseline performance analysis using an invertible flow generative model (IResNet), which further allows us to discover new sub-classes for a given cloud class by exploring the latent space. To compare methods, we introduce a set of evaluation criteria, to identify models that are not only accurate, but also physically-realistic. CUMULO can be download from https://www.dropbox.com/sh/i3s9q2v2jjyk2it/AACxXnXfMF5wuIqLXqH4NJOra?dl=0 . △ Less","13 October, 2022",https://arxiv.org/pdf/1911.04227
"Efficient sampling and counting algorithms for the Potts model on \mathbb Z^d
at all temperatures",Christian Borgs;Jennifer Chayes;Tyler Helmuth;Will Perkins;Prasad Tetali,"For d \ge 2 and all q\geq q_{0}(d) we give an efficient algorithm to approximately sample from the q-state ferromagnetic Potts and random cluster models on finite tori (\mathbb Z / n \mathbb Z )^d for any inverse temperature β\geq 0. This shows that the physical phase transition of the Potts model presents no algorithmic barrier to efficient sampling, and stands in contrast to Markov chain mixing time results: the Glauber dynamics mix slowly at and below the critical temperature, and the Swendsen--Wang dynamics mix slowly at the critical temperature. We also provide an efficient algorithm (an FPRAS) for approximating the partition functions of these models at all temperatures. Our algorithms are based on representing the random cluster model as a contour model using Pirogov--Sinai theory, and then computing an accurate approximation of the logarithm of the partition function by inductively truncating the resulting cluster expansion. The main innovation of our approach is an algorithmic treatment of unstable ground states, which is essential for our algorithms to apply to all inverse temperatures β. By treating unstable ground states our work gives a general template for converting probabilistic applications of Pirogov-Sinai theory to efficient algorithms. △ Less","8 August, 2022",https://arxiv.org/pdf/1909.09298
Mapping Firms' Locations in Technological Space: A Topological Analysis of Patent Statistics,Emerson G. Escolar;Yasuaki Hiraoka;Mitsuru Igami;Yasin Ozcan,"Where do firms innovate? Mapping their locations and directions in technological space is challenging due to its high dimensionality. We propose a new method to characterize firms' inventive activities via topological data analysis (TDA) that represents high-dimensional data in a shape graph. Applying this method to 333 major firms' patents in 1976--2005 reveals substantial heterogeneity: some firms remain undifferentiated; others develop unique portfolios. Firms with unique trajectories, which we define and measure graph-theoretically as ""flares"" in the Mapper graph, perform better. This association is statistically and economically significant, and continues to hold after we control for portfolio size, firm survivorship, industry classification, and firm fixed effects. By contrast, existing techniques -- such as principal component analysis (PCA) and Jaffe's (1989) clustering method -- struggle to track these firm-level dynamics. △ Less","31 March, 2022",https://arxiv.org/pdf/1909.00257
Product Aesthetic Design: A Machine Learning Augmentation,Alex Burnap;John R. Hauser;Artem Timoshenko,"Aesthetics are critically important to market acceptance. In the automotive industry, an improved aesthetic design can boost sales by 30% or more. Firms invest heavily in designing and testing aesthetics. A single automotive ""theme clinic"" can cost over $100,000, and hundreds are conducted annually. We propose a model to augment the commonly-used aesthetic design process by predicting aesthetic scores and automatically generating innovative and appealing product designs. The model combines a probabilistic variational autoencoder (VAE) with adversarial components from generative adversarial networks (GAN) and a supervised learning component. We train and evaluate the model with data from an automotive partner-images of 203 SUVs evaluated by targeted consumers and 180,000 high-quality unrated images. Our model predicts well the appeal of new aesthetic designs-43.5% improvement relative to a uniform baseline and substantial improvement over conventional machine learning models and pretrained deep neural networks. New automotive designs are generated in a controllable manner for use by design teams. We empirically verify that automatically generated designs are (1) appealing to consumers and (2) resemble designs which were introduced to the market five years after our data were collected. We provide an additional proof-of-concept application using opensource images of dining room chairs. △ Less","15 November, 2022",https://arxiv.org/pdf/1907.07786
Cost-sensitive Boosting Pruning Trees for depression detection on Twitter,Lei Tong;Zhihua Liu;Zheheng Jiang;Feixiang Zhou;Long Chen;Jialin Lyu;Xiangrong Zhang;Qianni Zhang;Abdul Sadka Senior;Yinhai Wang;Ling Li;Huiyu Zhou,"Depression is one of the most common mental health disorders, and a large number of depressed people commit suicide each year. Potential depression sufferers usually do not consult psychological doctors because they feel ashamed or are unaware of any depression, which may result in severe delay of diagnosis and treatment. In the meantime, evidence shows that social media data provides valuable clues about physical and mental health conditions. In this paper, we argue that it is feasible to identify depression at an early stage by mining online social behaviours. Our approach, which is innovative to the practice of depression detection, does not rely on the extraction of numerous or complicated features to achieve accurate depression detection. Instead, we propose a novel classifier, namely, Cost-sensitive Boosting Pruning Trees (CBPT), which demonstrates a strong classification ability on two publicly accessible Twitter depression detection datasets. To comprehensively evaluate the classification capability of the CBPT, we use additional three datasets from the UCI machine learning repository and the CBPT obtains appealing classification results against several state of the arts boosting algorithms. Finally, we comprehensively explore the influence factors of model prediction, and the results manifest that our proposed framework is promising for identifying Twitter users with depression. △ Less","21 January, 2022",https://arxiv.org/pdf/1906.00398
MedGCN: Medication recommendation and lab test imputation via graph convolutional networks,Chengsheng Mao;Liang Yao;Yuan Luo,"Laboratory testing and medication prescription are two of the most important routines in daily clinical practice. Developing an artificial intelligence system that can automatically make lab test imputations and medication recommendations can save costs on potentially redundant lab tests and inform physicians of a more effective prescription. We present an intelligent medical system (named MedGCN) that can automatically recommend the patients' medications based on their incomplete lab tests, and can even accurately estimate the lab values that have not been taken. In our system, we integrate the complex relations between multiple types of medical entities with their inherent features in a heterogeneous graph. Then we model the graph to learn a distributed representation for each entity in the graph based on graph convolutional networks (GCN). By the propagation of graph convolutional networks, the entity representations can incorporate multiple types of medical information that can benefit multiple medical tasks. Moreover, we introduce a cross regularization strategy to reduce overfitting for multi-task training by the interaction between the multiple tasks. In this study, we construct a graph to associate 4 types of medical entities, i.e., patients, encounters, lab tests, and medications, and applied a graph neural network to learn node embeddings for medication recommendation and lab test imputation. we validate our MedGCN model on two real-world datasets: NMEDW and MIMIC-III. The experimental results on both datasets demonstrate that our model can outperform the state-of-the-art in both tasks. We believe that our innovative system can provide a promising and reliable way to assist physicians to make medication prescriptions and to save costs on potentially redundant lab tests. △ Less","3 February, 2022",https://arxiv.org/pdf/1904.00326
Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos,Kha Gia Quach;Ngan Le;Chi Nhan Duong;Ibsa Jalata;Kaushik Roy;Khoa Luu,"Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes are becoming an interest in both the security arena as well as social media. This work extends the earlier ER investigations, which focused on either group-level ER on single images or within a video, by fully investigating group-level expression recognition on crowd videos. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. In our approach, the fusing process is performed on the deep feature domain by a generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that models spatial information relationships. Furthermore, we extend our proposed spatial NVPF approach to the spatial-temporal NVPF approach to learn the temporal information between frames. To demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on AffectNet database to benchmark the proposed EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from publicly available sources. GECV dataset is a collection of videos containing crowds of people. Each video is labeled with emotion categories at three levels: individual faces, group of people, and the entire video frame. △ Less","23 March, 2022",https://arxiv.org/pdf/1811.11849
Renaissance: A Self-Stabilizing Distributed SDN Control Plane using In-band Communications,Marco Canini;Iosif Salem;Liron Schiff;Elad Michael Schiller;Stefan Schmid,"By introducing programmability, automated verification, and innovative debugging tools, Software-Defined Networks (SDNs) are poised to meet the increasingly stringent dependability requirements of today's communication networks. However, the design of fault-tolerant SDNs remains an open challenge. This paper considers the design of dependable SDNs through the lenses of self-stabilization - a very strong notion of fault-tolerance. In particular, we develop algorithms for an in-band and distributed control plane for SDNs, called Renaissance, which tolerate a wide range of failures. Our self-stabilizing algorithms ensure that after the occurrence of arbitrary failures, (i) every non-faulty SDN controller can reach any switch (or another controller) within a bounded communication delay (in the presence of a bounded number of failures) and (ii) every switch is managed by a controller. We evaluate Renaissance through a rigorous worst-case analysis as well as a prototype implementation (based on OVS and Floodlight, and Mininet). △ Less","29 March, 2022",https://arxiv.org/pdf/1712.07697
Blockchain: A Graph Primer,Cuneyt Gurcan Akcora;Yulia R. Gel;Murat Kantarcioglu,"Bitcoin and its underlying technology, blockchain, have gained significant popularity in recent years. Satoshi Nakamoto designed Bitcoin to enable a secure, distributed platform without the need for central authorities, and blockchain has been hailed as a paradigm that will be as impactful as Big Data, Cloud Computing, and Machine Learning. Blockchain incorporates innovative ideas from various fields, such as public-key encryption and distributed systems. As a result, readers often encounter resources that explain Blockchain technology from a single perspective, leaving them with more questions than answers. In this primer, we aim to provide a comprehensive view of blockchain. We will begin with a brief history and introduce the building blocks of the blockchain. As graph mining is a major area of blockchain analysis, we will delve into the graph-theoretical aspects of Blockchain technology. We will also discuss the future of blockchain and explain how extensions such as smart contracts and decentralized autonomous organizations will function. Our goal is to provide a concise but complete description of blockchain technology that is accessible to readers with no prior expertise in the field. △ Less","11 December, 2022",https://arxiv.org/pdf/1708.08749
An Actor-Critic Contextual Bandit Algorithm for Personalized Mobile Health Interventions,Huitian Lei;Yangyi Lu;Ambuj Tewari;Susan A. Murphy,"Increasing technological sophistication and widespread use of smartphones and wearable devices provide opportunities for innovative and highly personalized health interventions. A Just-In-Time Adaptive Intervention (JITAI) uses real-time data collection and communication capabilities of modern mobile devices to deliver interventions in real-time that are adapted to the in-the-moment needs of the user. The lack of methodological guidance in constructing data-based JITAIs remains a hurdle in advancing JITAI research despite the increasing popularity of JITAIs among clinical scientists. In this article, we make a first attempt to bridge this methodological gap by formulating the task of tailoring interventions in real-time as a contextual bandit problem. Interpretability requirements in the domain of mobile health lead us to formulate the problem differently from existing formulations intended for web applications such as ad or news article placement. Under the assumption of linear reward function, we choose the reward function (the ""critic"") parameterization separately from a lower dimensional parameterization of stochastic policies (the ""actor""). We provide an online actor-critic algorithm that guides the construction and refinement of a JITAI. Asymptotic properties of the actor-critic algorithm are developed and backed up by numerical experiments. Additional numerical experiments are conducted to test the robustness of the algorithm when idealized assumptions used in the analysis of contextual bandit algorithm are breached. △ Less","22 April, 2022",https://arxiv.org/pdf/1706.09090
