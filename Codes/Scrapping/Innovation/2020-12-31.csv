title,authors,abstract,submitted_date,pdf_link
Machine Learning Coupled Trajectory and Communication Design for UAV-Facilitated Wireless Networks,Aksh Garg,"Augmenting wireless networks with Unmanned Aerial Vehicles (UAVs), commonly referred to as drones, offers a promising avenue for providing reliable, cost-effective, and on-demand wireless services to desired areas. However, existing UAV communication and trajectory schemes are inefficient as they assume limited drone mobility and static transmission power. Furthermore, they tend to rely upon convex approximations to highly non-linear functions and fail to adopt a combination of heuristic and convex methods. This paper considers a Multi-UAV system where UAV-mounted mobile base stations serve users on the ground. An iterative approach using block gradient descent is used to jointly optimize user scheduling, UAV trajectories, and transmission power for maximizing throughput over all users. Subsequently, an innovative technique for initial trajectory predictions was developed using a K-means clustering algorithm for partitioning users into subgroups and a genetic algorithm for initializing shortest flight paths within clusters. Finally, convex optimization solvers such as MATLAB's Fmincon are used for fine-tuning parameters. Extensive simulation and optimization results demonstrate a 33.57%, 87.4%, and 53.2% increase in system throughput for the 1, 2, and 3 UAV scenarios respectively when compared to existing trajectory and communication design schemes. Furthermore, the K-means and genetic algorithm reveal additional improvements in throughput by around 15%. Our results note diminished increases in throughput for increases in UAV trajectory period as the period approaches higher values. Further research into joint adoption of convex and non-convex schemes as well as consideration of environment-dependent channel models would allow for a faster and more optimal deployment of UAVs. △ Less","22 December, 2020",https://arxiv.org/pdf/2101.10454
A Review of Machine Learning Techniques for Applied Eye Fundus and Tongue Digital Image Processing with Diabetes Management System,Wei Xiang Lim;Zhiyuan Chen;Amr Ahmed;Tissa Chandesa;Iman Liao,"Diabetes is a global epidemic and it is increasing at an alarming rate. The International Diabetes Federation (IDF) projected that the total number of people with diabetes globally may increase by 48%, from 425 million (year 2017) to 629 million (year 2045). Moreover, diabetes had caused millions of deaths and the number is increasing drastically. Therefore, this paper addresses the background of diabetes and its complications. In addition, this paper investigates innovative applications and past researches in the areas of diabetes management system with applied eye fundus and tongue digital images. Different types of existing applied eye fundus and tongue digital image processing with diabetes management systems in the market and state-of-the-art machine learning techniques from previous literature have been reviewed. The implication of this paper is to have an overview in diabetic research and what new machine learning techniques can be proposed in solving this global epidemic. △ Less","29 December, 2020",https://arxiv.org/pdf/2012.15025
Growing Deep Forests Efficiently with Soft Routing and Learned Connectivity,Jianghao Shen;Sicheng Wang;Zhangyang Wang,"Despite the latest prevailing success of deep neural networks (DNNs), several concerns have been raised against their usage, including the lack of intepretability the gap between DNNs and other well-established machine learning models, and the growingly expensive computational costs. A number of recent works [1], [2], [3] explored the alternative to sequentially stacking decision tree/random forest building blocks in a purely feed-forward way, with no need of back propagation. Since decision trees enjoy inherent reasoning transparency, such deep forest models can also facilitate the understanding of the internaldecision making process. This paper further extends the deep forest idea in several important aspects. Firstly, we employ a probabilistic tree whose nodes make probabilistic routing decisions, a.k.a., soft routing, rather than hard binary decisions.Besides enhancing the flexibility, it also enables non-greedy optimization for each tree. Second, we propose an innovative topology learning strategy: every node in the ree now maintains a new learnable hyperparameter indicating the probability that it will be a leaf node. In that way, the tree will jointly optimize both its parameters and the tree topology during training. Experiments on the MNIST dataset demonstrate that our empowered deep forests can achieve better or comparable performance than [1],[3] , with dramatically reduced model complexity. For example,our model with only 1 layer of 15 trees can perform comparably with the model in [3] with 2 layers of 2000 trees each. △ Less","29 December, 2020",https://arxiv.org/pdf/2012.14878
A Survey on Segment Routing with Emphasis on Use Cases in Large Provider Networks,Aniruddha Kushwaha;Sidharth Sharma;Ashwin Gumaste,"Segment routing is heralded as important technology innovation in large provider networks. In the domain of large telecom service providers, segment routing has the potential to be in the same league as MPLS and IPv6 as well as pave a way towards the successful implementation of SDNs. In this regard, this paper is a survey on the existing segment routing work in the community. We begin by describing how segment routing works, inclusive of the various building blocks. The paper describes the segment routing architecture in terms of its positioning vis-a-vis existing networking technologies, as well as the data-plane and the control plane. Label encoding techniques that are central towards implementing segment routing are then discussed. In order to make segment routing relevant to the current domain of providers, we postulate various use cases, their working and issues of interoperability. The paper also reviews the current set of segment routing implementation cases in the industry. Thereafter we position segment routing among the various provider manifestations. △ Less","29 December, 2020",https://arxiv.org/pdf/2012.14687
Deep Learning with Heterogeneous Graph Embeddings for Mortality Prediction from Electronic Health Records,Tingyi Wanyan;Hossein Honarvar;Ariful Azad;Ying Ding;Benjamin S. Glicksberg,"Computational prediction of in-hospital mortality in the setting of an intensive care unit can help clinical practitioners to guide care and make early decisions for interventions. As clinical data are complex and varied in their structure and components, continued innovation of modeling strategies is required to identify architectures that can best model outcomes. In this work, we train a Heterogeneous Graph Model (HGM) on Electronic Health Record data and use the resulting embedding vector as additional information added to a Convolutional Neural Network (CNN) model for predicting in-hospital mortality. We show that the additional information provided by including time as a vector in the embedding captures the relationships between medical concepts, lab tests, and diagnoses, which enhances predictive performance. We find that adding HGM to a CNN model increases the mortality prediction accuracy up to 4\%. This framework serves as a foundation for future experiments involving different EHR data types on important healthcare prediction tasks. △ Less","27 December, 2020",https://arxiv.org/pdf/2012.14065
Multi-Contrast Computed Tomography Healthy Kidney Atlas,Ho Hin Lee;Yucheng Tang;Kaiwen Xu;Shunxing Bao;Agnes B. Fogo;Raymond Harris;Mark P. de Caestecker;Mattias Heinrich;Jeffrey M. Spraggins;Yuankai Huo;Bennett A. Landman,"The construction of three-dimensional multi-modal tissue maps provides an opportunity to spur interdisciplinary innovations across temporal and spatial scales through information integration. While the preponderance of effort is allocated to the cellular level and explore the changes in cell interactions and organizations, contextualizing findings within organs and systems is essential to visualize and interpret higher resolution linkage across scales. There is a substantial normal variation of kidney morphometry and appearance across body size, sex, and imaging protocols in abdominal computed tomography (CT). A volumetric atlas framework is needed to integrate and visualize the variability across scales. However, there is no abdominal and retroperitoneal organs atlas framework for multi-contrast CT. Hence, we proposed a high-resolution CT retroperitoneal atlas specifically optimized for the kidney across non-contrast CT and early arterial, late arterial, venous and delayed contrast enhanced CT. Briefly, we introduce a deep learning-based volume of interest extraction method and an automated two-stage hierarchal registration pipeline to register abdominal volumes to a high-resolution CT atlas template. To generate and evaluate the atlas, multi-contrast modality CT scans of 500 subjects (without reported history of renal disease, age: 15-50 years, 250 males & 250 females) were processed. We demonstrate a stable generalizability of the atlas template for integrating the normal kidney variation from small to large, across contrast modalities and populations with great variability of demographics. The linkage of atlas and demographics provided a better understanding of the variation of kidney anatomy across populations. △ Less","23 December, 2020",https://arxiv.org/pdf/2012.12432
Influence Maximization Under Generic Threshold-based Non-submodular Model,Liang Ma,"As a widely observable social effect, influence diffusion refers to a process where innovations, trends, awareness, etc. spread across the network via the social impact among individuals. Motivated by such social effect, the concept of influence maximization is coined, where the goal is to select a bounded number of the most influential nodes (seed nodes) from a social network so that they can jointly trigger the maximal influence diffusion. A rich body of research in this area is performed under statistical diffusion models with provable submodularity, which essentially simplifies the problem as the optimal result can be approximated by the simple greedy search. When the diffusion models are non-submodular, however, the research community mostly focuses on how to bound/approximate them by tractable submodular functions so as to estimate the optimal result. In other words, there is still a lack of efficient methods that can directly resolve non-submodular influence maximization problems. In this regard, we fill the gap by proposing seed selection strategies using network graphical properties in a generalized threshold-based model, called influence barricade model, which is non-submodular. Specifically, under this model, we first establish theories to reveal graphical conditions that ensure the network generated by node removals has the same optimal seed set as that in the original network. We then exploit these theoretical conditions to develop efficient algorithms by strategically removing less-important nodes and selecting seeds only in the remaining network. To the best of our knowledge, this is the first graph-based approach that directly tackles non-submodular influence maximization. △ Less","18 December, 2020",https://arxiv.org/pdf/2012.12309
Teaching young learners a foreign language via tangible and graphical user interfaces,Heracles Michailidis;Eleni Michailidi;Stavroula Tavoultzidou;George F. Fragulis,"The use of tangible interfaces in teaching has been proved more effective, user -friendly and helpful in collaborative learning departments, when compared to traditional teaching approaches. In particular, the tangible interface ""Makey Makey""is a modern tool that enhances collaboration between pupils, with positive results in education, despite the limited research done on this interface so far. ""Makey Makey"" succeeds in motivating and engaging young learners in the learning process, showing better performance and scoring results. In addition, its use in teaching has been shown to benefit the learning process in every age learning group.The development and use of such an innovative teaching/learning approach helps young learners perceive the educational process in a different way and assimilate new cognitive fields more effectively. Moreover, educators profit as well, as they can eliminate difficulties and teach more efficiently using examples based on their teaching approach, while enhancing young learners parallel skills as well. This study will confirm previous research results stating that assimilation of new concepts is easier with tangible interfaces than with graphical ones, as well as that young learners participating in the survey have shown significant progress in knowledge acquisition when compared to their prior knowledge. △ Less","22 December, 2020",https://arxiv.org/pdf/2012.12000
"GDPR-inspired IoT Ontology enabling Semantic Interoperability, Federation of Deployments and Privacy-Preserving Applications",Rachit Agarwal;Tarek Elsaleh;Elias Tragos,"Testing and experimentation are crucial for promoting innovation and building systems that can evolve to meet high levels of service quality. IoT data that belong to users and from which their personal information can be inferred are frequently shared in the background of IoT systems with third parties for experimentation and building quality services. This data sharing raises privacy concerns especially since in most cases the data are gathered and shared without the user's knowledge or explicit consent or for different purposes than the one for which the data were initially gathered. With the introduction of GDPR, IoT systems and experimentation platforms that federate data from different deployments, testbeds and data providers must be privacy-preserving. The wide adoption of IoT applications in scenarios ranging from smart cities to Industry 4.0 has raised concerns with respect to the privacy of users' data collected using IoT devices. Many experimental smart city applications are also using crowdsourcing data. Inspired by the GDPR requirements, we propose an IoT ontology built using available standards that enhances privacy, enables semantic interoperability between IoT deployments and supports the development of privacy-preserving experimental IoT applications. On top, we propose recommendations on how to efficiently use the ontology within IoT testbed and federating platforms. Our ontology is validated for different quality assessment criteria using standard validation tools. We focus on ""experimentation"" without loss of generality, because it covers scenarios from both research and industry, that are directly linked with innovation. △ Less","18 December, 2020",https://arxiv.org/pdf/2012.10314
Robotics Enabling the Workforce,Henrik Christensen;Maria Gini;Odest Chadwicke Jenkins;Holly Yanco,"Robotics has the potential to magnify the skilled workforce of the nation by complementing our workforce with automation: teams of people and robots will be able to do more than either could alone. The economic engine of the U.S. runs on the productivity of our people. The rise of automation offers new opportunities to enhance the work of our citizens and drive the innovation and prosperity of our industries. Most critically, we need research to understand how future robot technologies can best complement our workforce to get the best of both human and automated labor in a collaborative team. Investments made in robotics research and workforce development will lead to increased GDP, an increased export-import ratio, a growing middle class of skilled workers, and a U.S.-based supply chain that can withstand global pandemics and other disruptions. In order to make the United States a leader in robotics, we need to invest in basic research, technology development, K-16 education, and lifelong learning. △ Less","16 December, 2020",https://arxiv.org/pdf/2012.09309
"Infrastructure for Artificial Intelligence, Quantum and High Performance Computing",William Gropp;Sujata Banerjee;Ian Foster,"High Performance Computing (HPC), Artificial Intelligence (AI)/Machine Learning (ML), and Quantum Computing (QC) and communications offer immense opportunities for innovation and impact on society. Researchers in these areas depend on access to computing infrastructure, but these resources are in short supply and are typically siloed in support of their research communities, making it more difficult to pursue convergent and interdisciplinary research. Such research increasingly depends on complex workflows that require different resources for each stage. This paper argues that a more-holistic approach to computing infrastructure, one that recognizes both the convergence of some capabilities and the complementary capabilities from new computing approaches, be it commercial cloud to Quantum Computing, is needed to support computer science research. △ Less","16 December, 2020",https://arxiv.org/pdf/2012.09303
Open Problems in Cooperative AI,Allan Dafoe;Edward Hughes;Yoram Bachrach;Tantum Collins;Kevin R. McKee;Joel Z. Leibo;Kate Larson;Thore Graepel,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences. △ Less","15 December, 2020",https://arxiv.org/pdf/2012.08630
Detecting Invisible People,Tarasha Khurana;Achal Dave;Deva Ramanan,"Monocular object detection and tracking have improved drastically in recent years, but rely on a key assumption: that objects are visible to the camera. Many offline tracking approaches reason about occluded objects post-hoc, by linking together tracklets after the object re-appears, making use of reidentification (ReID). However, online tracking in embodied robotic agents (such as a self-driving vehicle) fundamentally requires object permanence, which is the ability to reason about occluded objects before they re-appear. In this work, we re-purpose tracking benchmarks and propose new metrics for the task of detecting invisible objects, focusing on the illustrative case of people. We demonstrate that current detection and tracking systems perform dramatically worse on this task. We introduce two key innovations to recover much of this performance drop. We treat occluded object detection in temporal sequences as a short-term forecasting challenge, bringing to bear tools from dynamic sequence prediction. Second, we build dynamic models that explicitly reason in 3D, making use of observations produced by state-of-the-art monocular depth estimation networks. To our knowledge, ours is the first work to demonstrate the effectiveness of monocular depth estimation for the task of tracking and detecting occluded objects. Our approach strongly improves by 11.4% over the baseline in ablations and by 5.0% over the state-of-the-art in F1 score. △ Less","15 December, 2020",https://arxiv.org/pdf/2012.08419
Use of Technology and Innovations in the COVID-19 Pandemic Response in Africa,Adyasha Maharana;Morine Amutorine;Moinina David Sengeh;Elaine O. Nsoesie,"The use of technology has been ubiquitous in efforts to combat the ongoing public health crisis due to emergence and spread of the SARS-CoV-2 virus. African countries have made tremendous use of technology to disseminate information, counter the spread of COVID-19, and develop cutting-edge techniques to help with diagnosis, treatment and management of patients. The nature and outcomes of these efforts sometimes differ in Africa compared to other areas of the world due to its unique challenges and opportunities. Several countries have developed innovative technology-driven solutions to cater to a diverse population with varying access to technology. Much of the efforts are also earmarked by a flexible approach to problem solving, local tech entrepreneurship, and swift adoption of cutting-edge technology. △ Less","11 December, 2020",https://arxiv.org/pdf/2012.07741
Detecting Insincere Questions from Text: A Transfer Learning Approach,Ashwin Rachha;Gaurav Vanmane,"The internet today has become an unrivalled source of information where people converse on content based websites such as Quora, Reddit, StackOverflow and Twitter asking doubts and sharing knowledge with the world. A major arising problem with such websites is the proliferation of toxic comments or instances of insincerity wherein the users instead of maintaining a sincere motive indulge in spreading toxic and divisive content. The straightforward course of action in confronting this situation is detecting such content beforehand and preventing it from subsisting online. In recent times Transfer Learning in Natural Language Processing has seen an unprecedented growth. Today with the existence of transformers and various state of the art innovations, a tremendous growth has been made in various NLP domains. The introduction of BERT has caused quite a stir in the NLP community. As mentioned, when published, BERT dominated performance benchmarks and thereby inspired many other authors to experiment with it and publish similar models. This led to the development of a whole BERT-family, each member being specialized on a different task. In this paper we solve the Insincere Questions Classification problem by fine tuning four cutting age models viz BERT, RoBERTa, DistilBERT and ALBERT. △ Less","7 December, 2020",https://arxiv.org/pdf/2012.07587
Machine Learning and Data Analytics for Design and Manufacturing of High-Entropy Materials Exhibiting Mechanical or Fatigue Properties of Interest,Baldur Steingrimsson;Xuesong Fan;Anand Kulkarni;Michael C. Gao;Peter K. Liaw,"This chapter presents an innovative framework for the application of machine learning and data analytics for the identification of alloys or composites exhibiting certain desired properties of interest. The main focus is on alloys and composites with large composition spaces for structural materials. Such alloys or composites are referred to as high-entropy materials (HEMs) and are here presented primarily in context of structural applications. For each output property of interest, the corresponding driving (input) factors are identified. These input factors may include the material composition, heat treatment, manufacturing process, microstructure, temperature, strain rate, environment, or testing mode. The framework assumes the selection of an optimization technique suitable for the application at hand and the data available. Physics-based models are presented, such as for predicting the ultimate tensile strength (UTS) or fatigue resistance. We devise models capable of accounting for physics-based dependencies. We factor such dependencies into the models as a priori information. In case that an artificial neural network (ANN) is deemed suitable for the applications at hand, it is suggested to employ custom kernel functions consistent with the underlying physics, for the purpose of attaining tighter coupling, better prediction, and for extracting the most out of the - usually limited - input data available. △ Less","5 December, 2020",https://arxiv.org/pdf/2012.07583
Knowledge Graph Management on the Edge,Weiqin Xu;Olivier Curé;Philippe Calvez,"Edge computing emerges as an innovative platform for services requiring low latency decision making. Its success partly depends on the existence of efficient data management systems. We consider that knowledge graph management systems have a key role to play in this context due to their data integration and reasoning features. In this paper, we present SuccinctEdge, a compact, decompression-free, self-index, in-memory RDF store that can answer SPARQL queries, including those requiring reasoning services associated to some ontology. We provide details on its design and implementation before demonstrating its efficiency on real-world and synthetic datasets. △ Less","13 December, 2020",https://arxiv.org/pdf/2012.07108
Interacting discovery processes on complex networks,Iacopo Iacopini;Gabriele Di Bona;Enrico Ubaldi;Vittorio Loreto;Vito Latora,"Innovation is the driving force of human progress. Recent urn models reproduce well the dynamics through which the discovery of a novelty may trigger further ones, in an expanding space of opportunities, but neglect the effects of social interactions. Here we focus on the mechanisms of collective exploration and we propose a model in which many urns, representing different explorers, are coupled through the links of a social network and exploit opportunities coming from their contacts. We study different network structures showing, both analytically and numerically, that the pace of discovery of an explorer depends on its centrality in the social network. Our model sheds light on the role that social structures play in discovery processes. △ Less","11 December, 2020",https://arxiv.org/pdf/2012.06369
Privacy-preserving medical image analysis,Alexander Ziller;Jonathan Passerat-Palmbach;Théo Ryffel;Dmitrii Usynin;Andrew Trask;Ionésio Da Lima Costa Junior;Jason Mancuso;Marcus Makowski;Daniel Rueckert;Rickmer Braren;Georgios Kaissis,"The utilisation of artificial intelligence in medicine and healthcare has led to successful clinical applications in several domains. The conflict between data usage and privacy protection requirements in such systems must be resolved for optimal results as well as ethical and legal compliance. This calls for innovative solutions such as privacy-preserving machine learning (PPML). We present PriMIA (Privacy-preserving Medical Image Analysis), a software framework designed for PPML in medical imaging. In a real-life case study we demonstrate significantly better classification performance of a securely aggregated federated learning model compared to human experts on unseen datasets. Furthermore, we show an inference-as-a-service scenario for end-to-end encrypted diagnosis, where neither the data nor the model are revealed. Lastly, we empirically evaluate the framework's security against a gradient-based model inversion attack and demonstrate that no usable information can be recovered from the model. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.06354
Artificial Intelligence for COVID-19 Detection -- A state-of-the-art review,Parsa Sarosh;Shabir A. Parah;Romany F Mansur;G. M. Bhat,"The emergence of COVID-19 has necessitated many efforts by the scientific community for its proper management. An urgent clinical reaction is required in the face of the unending devastation being caused by the pandemic. These efforts include technological innovations for improvement in screening, treatment, vaccine development, contact tracing and, survival prediction. The use of Deep Learning (DL) and Artificial Intelligence (AI) can be sought in all of the above-mentioned spheres. This paper aims to review the role of Deep Learning and Artificial intelligence in various aspects of the overall COVID-19 management and particularly for COVID-19 detection and classification. The DL models are developed to analyze clinical modalities like CT scans and X-Ray images of patients and predict their pathological condition. A DL model aims to detect the COVID-19 pneumonia, classify and distinguish between COVID-19, Community-Acquired Pneumonia (CAP), Viral and Bacterial pneumonia, and normal conditions. Furthermore, sophisticated models can be built to segment the affected area in the lungs and quantify the infection volume for a better understanding of the extent of damage. Many models have been developed either independently or with the help of pre-trained models like VGG19, ResNet50, and AlexNet leveraging the concept of transfer learning. Apart from model development, data preprocessing and augmentation are also performed to cope with the challenge of insufficient data samples often encountered in medical applications. It can be evaluated that DL and AI can be effectively implemented to withstand the challenges posed by the global emergency △ Less","25 November, 2020",https://arxiv.org/pdf/2012.06310
Impact of Business technologies on the success of Ecommerce Strategies: SMEs Perspective,Z. H. A Almtiri;S. J. Miah,"The primary task of the study is to inspect the affiliation between the implementation of technology and e-commerce success. It is imperative to study such an important relationship that directly impacts the rapid growth of Internet technology, new dimensions of e-services, and innovative measures that are necessary factors for electronic commerce operations. Despite most Saudi Arabia retailers being aware of technological advancements, existing research reveals several challenges that hinder the adoption of e-commerce strategies, including the cost of installation and training. The advantages of e-commerce are frequently shown in recent studies. Internet technologies development has narrowed the difference between traditional trade and online business grounds, with additional traditional markets moving to online platforms. The Saudi Arabia community has been recognized as a potential hub for advancing technology-based programs, particularly e-commerce. △ Less","11 December, 2020",https://arxiv.org/pdf/2012.06214
EQG-RACE: Examination-Type Question Generation,Xin Jia;Wenjie Zhou;Xu Sun;Yunfang Wu,"Question Generation (QG) is an essential component of the automatic intelligent tutoring systems, which aims to generate high-quality questions for facilitating the reading practice and assessments. However, existing QG technologies encounter several key issues concerning the biased and unnatural language sources of datasets which are mainly obtained from the Web (e.g. SQuAD). In this paper, we propose an innovative Examination-type Question Generation approach (EQG-RACE) to generate exam-like questions based on a dataset extracted from RACE. Two main strategies are employed in EQG-RACE for dealing with discrete answer information and reasoning among long contexts. A Rough Answer and Key Sentence Tagging scheme is utilized to enhance the representations of input. An Answer-guided Graph Convolutional Network (AG-GCN) is designed to capture structure information in revealing the inter-sentences and intra-sentence relations. Experimental results show a state-of-the-art performance of EQG-RACE, which is apparently superior to the baselines. In addition, our work has established a new QG prototype with a reshaped dataset and QG method, which provides an important benchmark for related research in future work. We will make our data and code publicly available for further research. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.06106
Interdisciplinary Approaches to Understanding Artificial Intelligence's Impact on Society,Suresh Venkatasubramanian;Nadya Bliss;Helen Nissenbaum;Melanie Moses,"Innovations in AI have focused primarily on the questions of ""what"" and ""how""-algorithms for finding patterns in web searches, for instance-without adequate attention to the possible harms (such as privacy, bias, or manipulation) and without adequate consideration of the societal context in which these systems operate. In part, this is driven by incentives and forces in the tech industry, where a more product-driven focus tends to drown out broader reflective concerns about potential harms and misframings. But this focus on what and how is largely a reflection of the engineering and mathematics-focused training in computer science, which emphasizes the building of tools and development of computational concepts. As a result of this tight technical focus, and the rapid, worldwide explosion in its use, AI has come with a storm of unanticipated socio-technical problems, ranging from algorithms that act in racially or gender-biased ways, get caught in feedback loops that perpetuate inequalities, or enable unprecedented behavioral monitoring surveillance that challenges the fundamental values of free, democratic societies. Given that AI is no longer solely the domain of technologists but rather of society as a whole, we need tighter coupling of computer science and those disciplines that study society and societal values. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.06057
An Integrated Search Framework for Leveraging the Knowledge-Based Web Ecosystem,Dengya Zhu;Shastri Lakshman Nimmagadda;Torsten Reiners;Amit Rudra,"The explosion of information constrains the judgement of search terms associated with Knowledge-Based Web Ecosystem (KBWE), making the retrieval of relevant information and its knowledge management challenging. The existing information retrieval (IR) tools and their fusion in a framework need attention, in which search results can effectively be managed. In this article, we demonstrate the effective use of information retrieval services by a variety of users and agents in various KBWE scenarios. An innovative Integrated Search Framework (ISF) is proposed, which utilises crawling strategies, web search technologies and traditional database search methods. Besides, ISF offers comprehensive, dynamic, personalized, and organization-oriented information retrieval services, ranging from the Internet, extranet, intranet, to personal desktop. In this empirical research, experiments are carried out demonstrating the improvements in the search process, as discerned in the conceptual ISF. The experimental results show improved precision compared with other popular search engines. △ Less","9 December, 2020",https://arxiv.org/pdf/2012.05397
Rigid and Articulated Point Registration with Expectation Conditional Maximization,Radu Horaud;Florence Forbes;Manuel Yguel;Guillaume Dewaele;Jian Zhang,"This paper addresses the issue of matching rigid and articulated shapes through probabilistic point registration. The problem is recast into a missing data framework where unknown correspondences are handled via mixture models. Adopting a maximum likelihood principle, we introduce an innovative EM-like algorithm, namely the Expectation Conditional Maximization for Point Registration (ECMPR) algorithm. The algorithm allows the use of general covariance matrices for the mixture model components and improves over the isotropic covariance case. We analyse in detail the associated consequences in terms of estimation of the registration parameters, and we propose an optimal method for estimating the rotational and translational parameters based on semi-definite positive relaxation. We extend rigid registration to articulated registration. Robustness is ensured by detecting and rejecting outliers through the addition of a uniform component to the Gaussian mixture model at hand. We provide an in-depth analysis of our method and we compare it both theoretically and experimentally with other robust methods for point registration. △ Less","9 December, 2020",https://arxiv.org/pdf/2012.05191
Internet of Things-based innovations in Saudi healthcare sector: A methodological approach for investigating adoption issues,F H Masmali;S J Miah;NY Mathkoor,"Using today's Internet network capacities, this technology has extended various benefits in healthcare sectors. For instance, existing studies already indicated that information technology applications with IoT-based innovations may revolutionize the healthcare industry and subsequently help to improve the real-time reporting of patients' health data. It should be noted that the adoption of IoT and its relevant interventions in the health sector has not been as fast as the uptake been observed in other industries. To tackle this issue, we develop a qualitative phenomenological approach for investigating factors that affect IoT adoption and its integration into healthcare service delivery in Saudi Arabia. △ Less","9 December, 2020",https://arxiv.org/pdf/2012.04970
CX DB8: A queryable extractive summarizer and semantic search engine,Allen Roush,"Competitive Debate's increasingly technical nature has left competitors looking for tools to accelerate evidence production. We find that the unique type of extractive summarization performed by competitive debaters - summarization with a bias towards a particular target meaning - can be performed using the latest innovations in unsupervised pre-trained text vectorization models. We introduce CX_DB8, a queryable word-level extractive summarizer and evidence creation framework, which allows for rapid, biasable summarization of arbitarily sized texts. CX_DB8s usage of the embedding framework Flair means that as the underlying models improve, CX_DB8 will also improve. We observe that CX_DB8 also functions as a semantic search engine, and has application as a supplement to traditional ""find"" functionality in programs and webpages. CX_DB8 is currently used by competitive debaters and is made available to the public at https://github.com/Hellisotherpeople/CX_DB8 △ Less","7 December, 2020",https://arxiv.org/pdf/2012.03942
Accurate and Fast Federated Learning via Combinatorial Multi-Armed Bandits,Taehyeon Kim;Sangmin Bae;Jin-woo Lee;Seyoung Yun,"Federated learning has emerged as an innovative paradigm of collaborative machine learning. Unlike conventional machine learning, a global model is collaboratively learned while data remains distributed over a tremendous number of client devices, thus not compromising user privacy. However, several challenges still remain despite its glowing popularity; above all, the global aggregation in federated learning involves the challenge of biased model averaging and lack of prior knowledge in client sampling, which, in turn, leads to high generalization error and slow convergence rate, respectively. In this work, we propose a novel algorithm called FedCM that addresses the two challenges by utilizing prior knowledge with multi-armed bandit based client sampling and filtering biased models with combinatorial model averaging. Based on extensive evaluations using various algorithms and representative heterogeneous datasets, we showed that FedCM significantly outperformed the state-of-the-art algorithms by up to 37.25% and 4.17 times, respectively, in terms of generalization accuracy and convergence rate. △ Less","6 December, 2020",https://arxiv.org/pdf/2012.03270
Any-Width Networks,Thanh Vu;Marc Eder;True Price;Jan-Michael Frahm,"Despite remarkable improvements in speed and accuracy, convolutional neural networks (CNNs) still typically operate as monolithic entities at inference time. This poses a challenge for resource-constrained practical applications, where both computational budgets and performance needs can vary with the situation. To address these constraints, we propose the Any-Width Network (AWN), an adjustable-width CNN architecture and associated training routine that allow for fine-grained control over speed and accuracy during inference. Our key innovation is the use of lower-triangular weight matrices which explicitly address width-varying batch statistics while being naturally suited for multi-width operations. We also show that this design facilitates an efficient training routine based on random width sampling. We empirically demonstrate that our proposed AWNs compare favorably to existing methods while providing maximally granular control during inference. △ Less","5 December, 2020",https://arxiv.org/pdf/2012.03153
Majority Opinion Diffusion in Social Networks: An Adversarial Approach,Ahad N. Zehmakan,"We introduce and study a novel majority-based opinion diffusion model. Consider a graph G, which represents a social network. Assume that initially a subset of nodes, called seed nodes or early adopters, are colored either black or white, which correspond to positive or negative opinion regarding a consumer product or a technological innovation. Then, in each round an uncolored node, which is adjacent to at least one colored node, chooses the most frequent color among its neighbors. Consider a marketing campaign which advertises a product of poor quality and its ultimate goal is that more than half of the population believe in the quality of the product at the end of the opinion diffusion process. We focus on three types of attackers which can select the seed nodes in a deterministic or random fashion and manipulate almost half of them to adopt a positive opinion toward the product (that is, to choose black color). We say that an attacker succeeds if a majority of nodes are black at the end of the process. Our main purpose is to characterize classes of graphs where an attacker cannot succeed. In particular, we prove that if the maximum degree of the underlying graph is not too large or if it has strong expansion properties, then it is fairly resilient to such attacks. Furthermore, we prove tight bounds on the stabilization time of the process (that is, the number of rounds it needs to end) in both settings of choosing the seed nodes deterministically and randomly. We also provide several hardness results for some optimization problems regarding stabilization time and choice of seed nodes. △ Less","5 December, 2020",https://arxiv.org/pdf/2012.03143
Finite element modelling of in-stent restenosis,Kiran Manjunatha;Marek Behr;Felix Vogt;Stefanie Reese,"From the perspective of coronary heart disease, the development of stents has come significantly far in reducing the associated mortality rate, drug-eluting stents being the epitome of innovative and effective solutions. Within this work, the intricate process of in-stent restenosis is modelled considering one of the significant growth factors and its effect on constituents of the arterial wall. A multiphysical modelling approach is adopted in this regard. Experimental investigations from the literature have been used to hypothesize the governing equations and the corresponding parameters. A staggered solution strategy is utilised to capture the transport phenomena as well as the growth and remodeling that follows stent implantation. The model herein developed serves as a tool to predict in-stent restenosis depending on the endothelial injury sustained and the protuberance of stents into the lumen of the arteries. Keywords: in-stent restenosis, smooth mucsle cells, platelet-derived growth factor, extracellular matrix, growth △ Less","14 December, 2020",https://arxiv.org/pdf/2012.02959
DeepCrawl: Deep Reinforcement Learning for Turn-based Strategy Games,Alessandro Sestini;Alexander Kuhnle;Andrew D. Bagdanov,"In this paper we introduce DeepCrawl, a fully-playable Roguelike prototype for iOS and Android in which all agents are controlled by policy networks trained using Deep Reinforcement Learning (DRL). Our aim is to understand whether recent advances in DRL can be used to develop convincing behavioral models for non-player characters in videogames. We begin with an analysis of requirements that such an AI system should satisfy in order to be practically applicable in video game development, and identify the elements of the DRL model used in the DeepCrawl prototype. The successes and limitations of DeepCrawl are documented through a series of playability tests performed on the final game. We believe that the techniques we propose offer insight into innovative new avenues for the development of behaviors for non-player characters in video games, as they offer the potential to overcome critical issues with △ Less","3 December, 2020",https://arxiv.org/pdf/2012.01914
Exploration in Algorithm Engineering: Modeling Algorithms,Sabah Al-Fedaghi,"According to some algorithmicists, algorithmics traditionally uses algorithm theory, which stems from mathematics. The growing need for innovative algorithms has caused increasing gaps between theory and practice. Originally, this motivated the development of algorithm engineering, which is viewed as experimental techniques related to software engineering. Currently, algorithm engineering is a methodology for algorithmic research that combines theory with implementation and experimentation in order to produce better algorithms with high practical impact. Still, researchers have questioned whether the notion of algorithms can be defined in a fully generable way and discussed what kinds of entities algorithms actually are. They have also struggled to maintain a view that formulates algorithms mathematically (e.g., Turing machines and finite-state machines [FSMs]) while adapting a more applied view. Answering the question of what algorithms have practical applications in software specifications in particular, this paper proposes a diagrammatical definition of an algorithm based on a new modeling machine called a thinging machine (TM). The machine has five actions (e.g., create, process, release, transfer, and receive) that can form a network of machines. The paper explores the application of the definition in Turing machines and FSMs. The results point to the fact that the proposed definition can serve as a middle-ground representation of algorithms, a definition which is between formal specification and the commonly used informal definition (e.g., set of instructions). △ Less","3 December, 2020",https://arxiv.org/pdf/2012.01908
Use-cases of Blockchain Technology for Humanitarian Engineering,Arvind W. Kiwelekar;Sanil S. Gandhi;Laxaman D. Netak;Shankar B. Deosarkar,"Humanitarian Engineers need innovative methods to make technological interventions for solving societal problems. The emerging blockchain technology has the enormous potential to provide effective interventions in various developmental sectors, including Agriculture, Education, Health, and Transportation. In these sectors, mediators have been considered as one of the impediments for developmental work. Blockchain technology facilitates peer-to-peer business transactions, thus eliminating the role of mediators. Hence, the blockchain technology is emerging as an alternative to conventional mediator-centred solutions adopting client-server based Internet technologies. A combination of blockchain technology with other technologies can be used to address domain-specific challenges. For example, the combination of blockchain technology and Internet-of-Thing (IoT) has the potential to monitor the usage of scarce resources such as the level of ground-water and amount of energy consumption. The aims of this chapter are twofold. Firstly, it describes the primary building blocks of blockchain technology. Secondly, it illustrates various use-case scenarios of blockchain technology in the fields of Agriculture, Energy Health and others. △ Less","30 November, 2020",https://arxiv.org/pdf/2012.01168
Using game simulator Software Inc in the Software Engineering education,Tetiana A. Vakaliuk;Valerii V. Kontsedailo;Dmytro S. Antoniuk;Olha V. Korotun;Iryna S. Mintii;Andrey V. Pikilnyak,"The article presents the possibilities of using game simulator Sotware Inc in the training of future software engineer in higher education. Attention is drawn to some specific settings that need to be taken into account when training in the course of training future software engineers. More and more educational institutions are introducing new teaching methods, which result in the use of engineering students, in particular, future software engineers, to deal with real professional situations in the learning process. The use of modern ICT, including game simulators, in the educational process, allows to improve the quality of educational material and to enhance the educational effects from the use of innovative pedagogical programs and methods, as it gives teachers additional opportunities for constructing individual educational trajectories of students. The use of ICT allows for a differentiated approach to students with different levels of readiness to study. A feature of any software engineer is the need to understand the related subject area for which the software is being developed. An important condition for the preparation of a highly qualified specialist is the independent fulfillment by the student of scientific research, the generation, and implementation of his idea into a finished commercial product. In the process of research, students gain knowledge, skills of the future IT specialist and competences of the legal protection of the results of intellectual activity, technological audit, marketing, product realization in the market of innovations. Note that when the real-world practice is impossible for students, game simulators that simulate real software development processes are an alternative. △ Less","26 November, 2020",https://arxiv.org/pdf/2012.01127
Nuanced and Interrelated Mediations and Exigencies (NIME): Addressing the Prevailing Political and Epistemological Crises,Lauren Hayes;Adnan Marquez-Borbon,"Nearly two decades after its inception as a workshop at the ACM Conference on Human Factors in Computing Systems, NIME exists as an established international conference significantly distinct from its precursor. While this origin story is often noted, the implications of NIME's history as emerging from a field predominantly dealing with human-computer interaction have rarely been discussed. In this paper we highlight many of the recent -- and some not so recent -- challenges that have been brought upon the NIME community as it attempts to maintain and expand its identity as a platform for multidisciplinary research into HCI, interface design, and electronic and computer music. We discuss the relationship between the market demands of the neoliberal university -- which have underpinned academia's drive for innovation -- and the quantification and economisation of research performance which have facilitated certain disciplinary and social frictions to emerge within NIME-related research and practice. Drawing on work that engages with feminist theory and cultural studies, we suggest that critical reflection and moreover mediation is necessary in order to address burgeoning concerns which have been raised within the NIME discourse in relation to methodological approaches, `diversity and inclusion', `accessibility', and the fostering of rigorous interdisciplinary research. △ Less","1 December, 2020",https://arxiv.org/pdf/2012.00923
UniCon: Universal Neural Controller For Physics-based Character Motion,Tingwu Wang;Yunrong Guo;Maria Shugrina;Sanja Fidler,"The field of physics-based animation is gaining importance due to the increasing demand for realism in video games and films, and has recently seen wide adoption of data-driven techniques, such as deep reinforcement learning (RL), which learn control from (human) demonstrations. While RL has shown impressive results at reproducing individual motions and interactive locomotion, existing methods are limited in their ability to generalize to new motions and their ability to compose a complex motion sequence interactively. In this paper, we propose a physics-based universal neural controller (UniCon) that learns to master thousands of motions with different styles by learning on large-scale motion datasets. UniCon is a two-level framework that consists of a high-level motion scheduler and an RL-powered low-level motion executor, which is our key innovation. By systematically analyzing existing multi-motion RL frameworks, we introduce a novel objective function and training techniques which make a significant leap in performance. Once trained, our motion executor can be combined with different high-level schedulers without the need for retraining, enabling a variety of real-time interactive applications. We show that UniCon can support keyboard-driven control, compose motion sequences drawn from a large pool of locomotion and acrobatics skills and teleport a person captured on video to a physics-based virtual avatar. Numerical and qualitative results demonstrate a significant improvement in efficiency, robustness and generalizability of UniCon over prior state-of-the-art, showcasing transferability to unseen motions, unseen humanoid models and unseen perturbation. △ Less","30 November, 2020",https://arxiv.org/pdf/2011.15119
Advancements of federated learning towards privacy preservation: from federated learning to split learning,Chandra Thapa;M. A. P. Chamikara;Seyit A. Camtepe,"In the distributed collaborative machine learning (DCML) paradigm, federated learning (FL) recently attracted much attention due to its applications in health, finance, and the latest innovations such as industry 4.0 and smart vehicles. FL provides privacy-by-design. It trains a machine learning model collaboratively over several distributed clients (ranging from two to millions) such as mobile phones, without sharing their raw data with any other participant. In practical scenarios, all clients do not have sufficient computing resources (e.g., Internet of Things), the machine learning model has millions of parameters, and its privacy between the server and the clients while training/testing is a prime concern (e.g., rival parties). In this regard, FL is not sufficient, so split learning (SL) is introduced. SL is reliable in these scenarios as it splits a model into multiple portions, distributes them among clients and server, and trains/tests their respective model portions to accomplish the full model training/testing. In SL, the participants do not share both data and their model portions to any other parties, and usually, a smaller network portion is assigned to the clients where data resides. Recently, a hybrid of FL and SL, called splitfed learning, is introduced to elevate the benefits of both FL (faster training/testing time) and SL (model split and training). Following the developments from FL to SL, and considering the importance of SL, this chapter is designed to provide extensive coverage in SL and its variants. The coverage includes fundamentals, existing findings, integration with privacy measures such as differential privacy, open problems, and code implementation. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.14818
Recent Trends in Wearable Computing Research: A Systematic Review,Vicente J. P. Amorim;Ricardo A. O. Oliveira;Mauricio Jose da Silva,"Wearable devices are a trending topic in both commercial and academic areas. Increasing demand for innovation has led to increased research and new products, addressing new challenges and creating profitable opportunities. However, despite a number of reviews and surveys on wearable computing, a study outlining how this area has recently evolved, which provides a broad and objective view of the main topics addressed by scientists, is lacking. The systematic review of literature presented in this paper investigates recent trends in wearable computing studies, taking into account a set of constraints applied to relevant studies over a window of ten years. The extracted articles were considered as a means to extract valuable information, creating a useful data set to represent the current status. Results of this study faithfully portray evolving interests in wearable devices. The analysis conducted here involving studies made over the past ten years allows evaluation of the areas, research focus, and technologies that are currently at the forefront of wearable device development. Conclusions presented in this review aim to assist scientists to better perceive recent demand trends and how wearable technology can further evolve. Finally, this study should assist in outlining the next steps in current and future development. △ Less","27 November, 2020",https://arxiv.org/pdf/2011.13801
Nose to Glass: Looking In to Get Beyond,Josephine Seah,"Brought into the public discourse through investigative work by journalists and scholars, awareness of algorithmic harms is at an all-time high. An increasing amount of research has been conducted under the banner of enhancing responsible artificial intelligence (AI), with the goal of addressing, alleviating, and eventually mitigating the harms brought on by the roll out of algorithmic systems. Nonetheless, implementation of such tools remains low. Given this gap, this paper offers a modest proposal: that the field, particularly researchers concerned with responsible research and innovation, may stand to gain from supporting and prioritising more ethnographic work. This embedded work can flesh out implementation frictions and reveal organisational and institutional norms that existing work on responsible artificial intelligence AI has not yet been able to offer. In turn, this can contribute to more insights about the anticipation of risks and mitigation of harm. This paper reviews similar empirical work typically found elsewhere, commonly in science and technology studies and safety science research, and lays out challenges of this form of inquiry. △ Less","1 December, 2020",https://arxiv.org/pdf/2011.13153
Leveraging Architectural Support of Three Page Sizes with Trident,Venkat Sri Sai Ram;Ashish Panwar;Arkaprava Basu,"Large pages are commonly deployed to reduce address translation overheads for big-memory workloads. Modern x86-64 processors from Intel and AMD support two large page sizes -- 1GB and 2MB. However, previous works on large pages have primarily focused on 2MB pages, partly due to lack of substantial evidence on the profitability of 1GB pages to real-world applications. We argue that in fact, inadequate system software support is responsible for a decade of underutilized hardware support for 1GB pages. Through extensive experimentation on a real system, we demonstrate that 1GB pages can improve performance over 2MB pages, and when used in tandem with 2MB pages for an important set of applications; the support for the latter is crucial but missing in current systems. Our design and implementation of \trident{} in Linux fully exploit hardware supported large pages by dynamically and transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable. \trident{} speeds up eight memory-intensive applications by {18\%}, on average, over Linux's use of 2MB pages. We also propose \tridentpv{}, an extension to \trident{} that effectively virtualizes 1GB pages via copy-less promotion and compaction in the guest OS. Overall, this paper shows that even GB-sized pages have considerable practical significance with adequate software enablement, in turn motivating architects to continue investing/innovating in large pages. △ Less","24 November, 2020",https://arxiv.org/pdf/2011.12092
Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings,Brett D. Roads;Bradley C. Love,"Advances in object recognition flourished in part because of the availability of high-quality datasets and associated benchmarks. However, these benchmarks---such as ILSVRC---are relatively task-specific, focusing predominately on predicting class labels. We introduce a publicly-available dataset that embodies the task-general capabilities of human perception and reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ) is composed of human similarity judgments that supplement the ILSVRC validation set. The new dataset supports a range of task and performance metrics, including the evaluation of unsupervised learning algorithms. We demonstrate two methods of assessment: using the similarity judgments directly and using a psychological embedding trained on the similarity judgments. This embedding space contains an order of magnitude more points (i.e., images) than previous efforts based on human judgments. Scaling to the full 50,000 image set was made possible through a selective sampling process that used variational Bayesian inference and model ensembles to sample aspects of the embedding space that were most uncertain. This methodological innovation not only enables scaling, but should also improve the quality of solutions by focusing sampling where it is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity ratings and the embedding space to evaluate how well several popular models conform to human similarity judgments. One finding is that more complex models that perform better on task-specific benchmarks do not better conform to human semantic judgments. In addition to the human similarity judgments, pre-trained psychological embeddings and code for inferring variational embeddings are made publicly available. Collectively, ImageNet-HSJ assets support the appraisal of internal representations and the development of more human-like models. △ Less","22 November, 2020",https://arxiv.org/pdf/2011.11015
Distributed Deep Reinforcement Learning: An Overview,Mohammad Reza Samsami;Hossein Alimadad,"Deep reinforcement learning (DRL) is a very active research area. However, several technical and scientific issues require to be addressed, amongst which we can mention data inefficiency, exploration-exploitation trade-off, and multi-task learning. Therefore, distributed modifications of DRL were introduced; agents that could be run on many machines simultaneously. In this article, we provide a survey of the role of the distributed approaches in DRL. We overview the state of the field, by studying the key research works that have a significant impact on how we can use distributed methods in DRL. We choose to overview these papers, from the perspective of distributed learning, and not the aspect of innovations in reinforcement learning algorithms. Also, we evaluate these methods on different tasks and compare their performance with each other and with single actor and learner agents. △ Less","22 November, 2020",https://arxiv.org/pdf/2011.11012
Enhanced Innovized Repair Operator for Evolutionary Multi- and Many-objective Optimization,Sukrit Mittal;Dhish Kumar Saxena;Kalyanmoy Deb;Erik Goodman,"""Innovization"" is a task of learning common relationships among some or all of the Pareto-optimal (PO) solutions in multi- and many-objective optimization problems. Recent studies have shown that a chronological sequence of non-dominated solutions obtained in consecutive iterations during an optimization run also possess salient patterns that can be used to learn problem features to help create new and improved solutions. In this paper, we propose a machine-learning- (ML-) assisted modelling approach that learns the modifications in design variables needed to advance population members towards the Pareto-optimal set. We then propose to use the resulting ML model as an additional innovized repair (IR2) operator to be applied on offspring solutions created by the usual genetic operators, as a novel mean of improving their convergence properties. In this paper, the well-known random forest (RF) method is used as the ML model and is integrated with various evolutionary multi- and many-objective optimization algorithms, including NSGA-II, NSGA-III, and MOEA/D. On several test problems ranging from two to five objectives, we demonstrate improvement in convergence behaviour using the proposed IR2-RF operator. Since the operator does not demand any additional solution evaluations, instead using the history of gradual and progressive improvements in solutions over generations, the proposed ML-based optimization opens up a new direction of optimization algorithm development with advances in AI and ML approaches. △ Less","21 November, 2020",https://arxiv.org/pdf/2011.10760
Analytic Bipedal Walking with Fused Angles and Corrective Actions in the Tilt Phase Space,Philipp Allgeuer,"This work presents algorithms for the feedback-stabilised walking of bipedal humanoid robotic platforms, along with the underlying theoretical and sensorimotor frameworks required to achieve it. Bipedal walking is inherently complex and difficult to control due to the high level of nonlinearity and significant number of degrees of freedom of the concerned robots, the limited observability and controllability of the corresponding states, and the combination of imperfect actuation with less-than-ideal sensing. The presented methods deal with these issues in a multitude of ways, ranging from the development of an actuator control and feed-forward compensation scheme, to the inclusion of filtering in almost all of the gait stabilisation feedback pipelines. Two gaits are developed and investigated, the direct fused angle feedback gait, and the tilt phase controller. Both gaits follow the design philosophy of leveraging a semi-stable open-loop gait generator, and extending it through stabilising feedback via the means of so-called corrective actions. The idea of using corrective actions is to modify the generation of the open-loop joint waveforms in such a way that the balance of the robot is influenced and thereby ameliorated. Examples of such corrective actions include modifications of the arm swing and leg swing trajectories, the application of dynamic positional and rotational offsets to the hips and feet, and adjustments of the commanded step size and timing. Underpinning both feedback gaits and their corresponding gait generators are significant advances in the field of 3D rotation theory. These advances include the development of three novel rotation representations, the tilt angles, fused angles, and tilt phase space representations. All three of these representations are founded on a new innovative way of splitting 3D rotations into their respective yaw and tilt components. △ Less","23 December, 2020",https://arxiv.org/pdf/2011.10339
Neural network algorithm and its application in reactive distillation,Huihui Wang;Ruyang Mo,"Reactive distillation is a special distillation technology based on the coupling of chemical reaction and distillation. It has the characteristics of low energy consumption and high separation efficiency. However, because the combination of reaction and separation produces highly nonlinear robust behavior, the control and optimization of the reactive distillation process cannot use conventional methods, but must rely on neural network algorithms. This paper briefly describes the characteristics and research progress of reactive distillation technology and neural network algorithms, and summarizes the application of neural network algorithms in reactive distillation, aiming to provide reference for the development and innovation of industry technology. △ Less","15 November, 2020",https://arxiv.org/pdf/2011.09969
The human quest for discovering mathematical beauty in the arts,Stefano Balietti,"In the words of the twentieth-century British mathematician G. H. Hardy, ""the human function is to 'discover or observe' mathematics"" (1). For centuries, starting from the ancient Greeks, mankind has hunted for beauty and order in arts and in nature. This quest for mathematical beauty has led to the discovery of recurrent mathematical structures, such as the golden ratio, Fibonacci, and Lucas numbers, whose ubiquitous presences have been tantalizing the minds of artists and scientists alike. The captivation for this quest comes with high stakes. In fact, art is the definitive expression of human creativity, and its mathematical understanding would deliver us the keys for decoding human culture and its evolution (2). However, it was not until fairly recently that the scope and the scale of the human quest for mathematical beauty was radically expanded by the simultaneous confluence of three separate innovations. The mass digitization of large art archives, the surge in computational power, and the development of robust statistical methods to capture hidden patterns in vast amounts of data have made it possible to reveal the---otherwise unnoticeable to the human eye---mathematics concealed in large artistic corpora. Starting from its inception, marked by the foundational work by Birkhoff (3), progress in the broad field of computational aesthetics has reached a scale that would have been unimaginable just a decade ago. The recent expansion is not limited to the visual arts (2) but includes music (4), stories (5), language phonology (6), humor in jokes (7), and even equations (8); for a comprehensive review, see ref. 9. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.09861
Knowledge Management Competence and ISD Vendor Innovativeness in Turbulent Markets,Sachithra Lokuge;Maduka Subasinghage,"Continuous changes in the technology and the business landscape place high strain on managing knowledge in organisations. Prior researchers highlight a positive connotation with knowledge management competence and organisational innovativeness in a turbulent environment. However, the rapid changes in the market and technology landscape may exert an additional pressure on the employees and such pressures may ultimately hinder organisational innovativeness. Drawing on knowledge management and innovation literature, this research conceptualises a model that investigates this tenacious relationship between knowledge management competence and innovativeness specifically in turbulent dynamic markets, considering information systems development (ISD)-outsourcing as the context. Following a mixed method approach, this research expects to provide guidance for ISD-outsourcing vendors to manage innovation expectations, knowledge management process and performance of the employees in dynamic market conditions. △ Less","15 November, 2020",https://arxiv.org/pdf/2011.09840
Tracking and Visualizing Signs of Degradation for an Early Failure Prediction of a Rolling Bearing,Sana Talmoudi;Tetsuya Kanada;Yasuhisa Hirata,"Predictive maintenance, i.e. predicting failure to be few steps ahead of the fault, is one of the pillars of Industry 4.0. An effective method for that is to track early signs of degradation before a failure happens. This paper presents an innovative failure predictive scheme for machines. The proposed scheme combines the use of full spectrum of the vibration data caused by the machines and data visualization technologies. This scheme is featured by no training data required and by quick start after installation. First, we propose to use full spectrum (as high-dimensional data vector) with no cropping and no complex feature extraction and to visualize data behavior by mapping the high dimensional vectors into a 2D map. We then can ensure the simplicity of process and less possibility of overlooking of important information as well as providing a human-friendly and human-understandable output. Second, we propose Real-Time Data Tracker (RTDT) which predicts the failure at an appropriate time with sufficient time for maintenance by plotting real-time frequency spectrum data of the target machine on the 2D map composed from normal data. Third, we show the test results of our proposal using vibration data of bearings from real-world test-to-failure measurements provided by the public dataset, the IMS dataset. △ Less","17 November, 2020",https://arxiv.org/pdf/2011.09086
Traffic Characteristics of Virtual Reality over Edge-enabled Wi-Fi Networks,Seyedmohammad Salehi;Abdullah Alnajim;Xiaoqing Zhu;Malcolm Smith;Chien-Chung Shen;Leonard Cimini,"Virtual reality (VR) is becoming prevalent with a plethora of applications in education, healthcare, entertainment, etc. To increase the user mobility, and to reduce the energy consumption and production cost of VR head mounted displays (HMDs), wireless VR with edge-computing has been the focus of both industry and academia. However, transferring large video frames of VR applications with their stringent Quality of Service (QoS) requirements over wireless network requires innovations and optimizations across different network layers. In order to develop efficient architectures, protocols and scheduling mechanisms, the traffic characteristics of various types of VR applications are required. In this paper, we first compute the theoretical throughput requirements of an ideal VR experience as well as a popular VR HMD. We then examine the traffic characteristics of a set of VR applications using an edge-enabled Wi-Fi network. Our results reveal interesting findings that can be considered in developing new optimizations, protocols, access mechanisms and scheduling algorithms. △ Less","17 November, 2020",https://arxiv.org/pdf/2011.09035
A Tale of Two Referees,Hannes Leeb,"Success in academia hinges on publishing in top tier journals. This requires innovative results. And this requires clear and convincing presentation of said results. Presentation can make the difference of one tier in journal level. A lot of useful advice on this topic is available online from well-respected outlets; see, for example, El-Omar (2014); Gould (2014); Neiles et al. (2015); Notz and Kafadar (2011); or Sachdeva (2020). This text provides a different angle. △ Less","17 November, 2020",https://arxiv.org/pdf/2011.08637
Personalized Cardiovascular Disease Risk Mitigation via Longitudinal Inverse Classification,Michael T. Lash;W. Nick Street,"Cardiovascular disease (CVD) is a serious illness affecting millions world-wide and is the leading cause of death in the US. Recent years, however, have seen tremendous growth in the area of personalized medicine, a field of medicine that places the patient at the center of the medical decision-making and treatment process. Many CVD-focused personalized medicine innovations focus on genetic biomarkers, which provide person-specific CVD insights at the genetic level, but do not focus on the practical steps a patient could take to mitigate their risk of CVD development. In this work we propose longitudinal inverse classification, a recommendation framework that provides personalized lifestyle recommendations that minimize the predicted probability of CVD risk. Our framework takes into account historical CVD risk, as well as other patient characteristics, to provide recommendations. Our experiments show that earlier adoption of the recommendations elicited from our framework produce significant CVD risk reduction. △ Less","16 November, 2020",https://arxiv.org/pdf/2011.08254
Spatiotemporal Characteristics of Ride-sourcing Operation in Urban Area,Simon Oh;Daniel Kondor;Ravi Seshadri;Meng Zhou;Diem-Trinh Le;Moshe Ben-Akiva,"The emergence of ride-sourcing platforms has brought an innovative alternative in transportation, radically changed travel behaviors, and suggested new directions for transportation planners and operators. This paper provides an exploratory analysis on the operations of a ride-sourcing service using large-scale data on service performance. Observations over multiple days in Singapore suggest reproducible demand patterns and provide empirical estimates of fleet operations over time and space. During peak periods, we observe significant increases in the service rate along with surge price multipliers. We perform an in-depth analysis of fleet utilization rates and are able to explain daily patterns based on drivers' behavior by involving the number of shifts, shift duration, and shift start and end time choices. We also evaluate metrics of user experience, namely waiting and travel time distribution, and explain our empirical findings with distance metrics from driver trajectory analysis and congestion patterns. Our results of empirical observations on actual service in Singapore can help to understand the spatiotemporal characteristics of ride-sourcing services and provide important insights for transportation planning and operations. △ Less","15 November, 2020",https://arxiv.org/pdf/2011.07673
Rethinking the Role of Technology in Virtual Teams in Light of COVID-19,Mark Frost;Sophia Xiaoxia Duan,"The use of virtual teams by organisations has grown tremendously as a strategic response to COVID-19. However, the concept of virtual teams is not something new, with many businesses over the past three decades gradually incorporating virtual and/or dispersed teams into their processes. Research on virtual teams has followed that of co-located face-to-face teams through lenses such as trust, communication, teamwork, leadership and collaboration. This paper introduces a new paradigm for examining the development of virtual teams, arguably one that would facilitate the consideration of technology as part of a virtual team rather than simply as an alternate to face-to-face teams. That is, viewing the development of virtual teams with embedded technology within an organisation through an innovation framework. △ Less","14 November, 2020",https://arxiv.org/pdf/2011.07303
A Mixed-Method Landscape Analysis of SME-focused B2B Platforms in Germany,Tina Krell;Fabian Braesemann;Fabian Stephany;Nicolas Friederici;Philip Meier,"Digital platforms offer vast potential for increased value creation and innovation, especially through cross-organizational data sharing. It appears that SMEs in Germany are currently hesitant or unable to create their own platforms. To get a holistic overview of the structure of the German SME-focused platform landscape (that is platforms that are led by or targeting SMEs), we applied a mixed method approach of traditional desk research and a quantitative analysis. The study identified large geographical disparity along the borders of the new and old German federal states, and overall fewer platform ventures by SMEs, rather than large companies and startups. Platform ventures for SMEs are more likely set up as partnerships. We indicate that high capital intensity might be a reason for that. △ Less","13 November, 2020",https://arxiv.org/pdf/2011.06859
FS-HGR: Few-shot Learning for Hand Gesture Recognition via ElectroMyography,Elahe Rahimian;Soheil Zabihi;Amir Asif;Dario Farina;Seyed Farokh Atashzar;Arash Mohammadi,"This work is motivated by the recent advances in Deep Neural Networks (DNNs) and their widespread applications in human-machine interfaces. DNNs have been recently used for detecting the intended hand gesture through processing of surface electromyogram (sEMG) signals. The ultimate goal of these approaches is to realize high-performance controllers for prosthetic. However, although DNNs have shown superior accuracy than conventional methods when large amounts of data are available for training, their performance substantially decreases when data are limited. Collecting large datasets for training may be feasible in research laboratories, but it is not a practical approach for real-life applications. Therefore, there is an unmet need for the design of a modern gesture detection technique that relies on minimal training data while providing high accuracy. Here we propose an innovative and novel ""Few-Shot Learning"" framework based on the formulation of meta-learning, referred to as the FS-HGR, to address this need. Few-shot learning is a variant of domain adaptation with the goal of inferring the required output based on just one or a few training examples. More specifically, the proposed FS-HGR quickly generalizes after seeing very few examples from each class. The proposed approach led to 85.94% classification accuracy on new repetitions with few-shot observation (5-way 5-shot), 81.29% accuracy on new subjects with few-shot observation (5-way 5-shot), and 73.36% accuracy on new gestures with few-shot observation (5-way 5-shot). △ Less","11 November, 2020",https://arxiv.org/pdf/2011.06104
Ecole: A Gym-like Library for Machine Learning in Combinatorial Optimization Solvers,Antoine Prouvost;Justin Dumouchelle;Lara Scavuzzo;Maxime Gasse;Didier Chételat;Andrea Lodi,"We present Ecole, a new library to simplify machine learning research for combinatorial optimization. Ecole exposes several key decision tasks arising in general-purpose combinatorial optimization solvers as control problems over Markov decision processes. Its interface mimics the popular OpenAI Gym library and is both extensible and intuitive to use. We aim at making this library a standardized platform that will lower the bar of entry and accelerate innovation in the field. Documentation and code can be found at https://www.ecole.ai. △ Less","24 November, 2020",https://arxiv.org/pdf/2011.06069
I-BOT: Interference-Based Orchestration of Tasks for Dynamic Unmanaged Edge Computing,Shikhar Suryavansh;Chandan Bothra;Kwang Taik Kim;Mung Chiang;Chunyi Peng;Saurabh Bagchi,"In recent years, edge computing has become a popular choice for latency-sensitive applications like facial recognition and augmented reality because it is closer to the end users compared to the cloud. Although infrastructure providers are working toward creating managed edge networks, personal devices such as laptops and tablets, which are widely available and are underutilized, can also be used as potential edge devices. We call such devices Unmanaged Edge Devices (UEDs). Scheduling application tasks on such an unmanaged edge system is not straightforward because of three fundamental reasons-heterogeneity in the computational capacity of the UEDs, uncertainty in the availability of the UEDs (due to devices leaving the system), and interference among multiple tasks sharing a UED. In this paper, we present I-BOT, an interference-based orchestration scheme for latency-sensitive tasks on an Unmanaged Edge Platform (UEP). It minimizes the completion time of applications and is bandwidth efficient. I-BOT brings forth three innovations. First, it profiles and predicts the interference patterns of the tasks to make scheduling decisions. Second, it uses a feedback mechanism to adjust for changes in the computational capacity of the UEDs and a prediction mechanism to handle their sporadic exits. Third, it accounts for input dependence of tasks in its scheduling decision (such as, two tasks requiring the same input data). To evaluate I-BOT, we run end-to-end simulations with applications representing autonomous driving, composed of multiple tasks. We compare to two basic baselines (random and round-robin) and two state-of-the-arts, Lavea [SEC-2017] and Petrel [MSN-2018]. Compared to these baselines, I-BOT significantly reduces the average service time of application tasks. This reduction is more pronounced in dynamic heterogeneous environments, which would be the case in a UEP. △ Less","11 November, 2020",https://arxiv.org/pdf/2011.05925
Wearable Sensors for Individual Grip Force Profiling,Birgitta Dresp-Langley,"Biosensors and wearable sensor systems with transmitting capabilities are currently developed and used for the monitoring of health data, exercise activities, and other performance data. Unlike conventional approaches, these devices enable convenient, continuous, and/or unobtrusive monitoring of user behavioral signals in real time. Examples include signals relative to body motion, body temperature, blood flow parameters and a variety of biological or biochemical markers and, as will be shown in this chapter here, individual grip force data that directly translate into spatiotemporal grip force profiles for different locations on the fingers and palm of the hand. Wearable sensor systems combine innovation in sensor design, electronics, data transmission, power management, and signal processing for statistical analysis, as will be further shown herein. The first section of this chapter will provide an overview of the current state of the art in grip force profiling to highlight important functional aspects to be considered. In the next section, the contribution of wearable sensor technology in the form of sensor glove systems for the real-time monitoring of surgical task skill evolution in novices training in a simulator task will be described on the basis of recent examples. In the discussion, advantages and limitations will be weighed against each other. △ Less","11 November, 2020",https://arxiv.org/pdf/2011.05863
A review of neural network algorithms and their applications in supercritical extraction,Yu Qi;Zhaolan Zheng,"Neural network realizes multi-parameter optimization and control by simulating certain mechanisms of the human brain. It can be used in many fields such as signal processing, intelligent driving, optimal combination, vehicle abnormality detection, and chemical process optimization control. Supercritical extraction is a new type of high-efficiency chemical separation process, which is mainly used in the separation and purification of natural substances. There are many influencing factors. The neural network model can quickly optimize the process parameters and predict the experimental results under different process conditions. It is helpful to understand the inner law of the experiment and determine the optimal experimental conditions. This paper briefly describes the basic concepts and research progress of neural networks and supercritical extraction, and summarizes the application of neural network algorithms in supercritical extraction, aiming to provide reference for the development and innovation of industry technology. △ Less","30 October, 2020",https://arxiv.org/pdf/2011.05279
OpenKinoAI: An Open Source Framework for Intelligent Cinematography and Editing of Live Performances,Rémi Ronfard;Rémi Colin de Verdière,"OpenKinoAI is an open source framework for post-production of ultra high definition video which makes it possible to emulate professional multiclip editing techniques for the case of single camera recordings. OpenKinoAI includes tools for uploading raw video footage of live performances on a remote web server, detecting, tracking and recognizing the performers in the original material, reframing the raw video into a large choice of cinematographic rushes, editing the rushes into movies, and annotating rushes and movies for documentation purposes. OpenKinoAI is made available to promote research in multiclip video editing of ultra high definition video, and to allow performing artists and companies to use this research for archiving, documenting and sharing their work online in an innovative fashion. △ Less","30 October, 2020",https://arxiv.org/pdf/2011.05203
Kinematics-Guided Reinforcement Learning for Object-Aware 3D Ego-Pose Estimation,Zhengyi Luo;Ryo Hachiuma;Ye Yuan;Shun Iwase;Kris M. Kitani,"We propose a method for incorporating object interaction and human body dynamics into the task of 3D ego-pose estimation using a head-mounted camera. We use a kinematics model of the human body to represent the entire range of human motion, and a dynamics model of the body to interact with objects inside a physics simulator. By bringing together object modeling, kinematics modeling, and dynamics modeling in a reinforcement learning (RL) framework, we enable object-aware 3D ego-pose estimation. We devise several representational innovations through the design of the state and action space to incorporate 3D scene context and improve pose estimation quality. We also construct a fine-tuning step to correct the drift and refine the estimated human-object interaction. This is the first work to estimate a physically valid 3D full-body interaction sequence with objects (e.g., chairs, boxes, obstacles) from egocentric videos. Experiments with both controlled and in-the-wild settings show that our method can successfully extract an object-conditioned 3D ego-pose sequence that is consistent with the laws of physics. △ Less","8 December, 2020",https://arxiv.org/pdf/2011.04837
Autonomous Intruder Detection Using a ROS-Based Multi-Robot System Equipped with 2D-LiDAR Sensors,Mashnoon Islam;Touhid Ahmed;Abu Tammam Bin Nuruddin;Mashuda Islam;Shahnewaz Siddique,"The application of autonomous mobile robots in robotic security platforms is becoming a promising field of innovation due to their adaptive capability of responding to potential disturbances perceived through a wide range of sensors. Researchers have proposed systems that either focus on utilizing a single mobile robot or a system of cooperative multiple robots. However, very few of the proposed works, particularly in the field of multi-robot systems, are completely dependent on LiDAR sensors for achieving various tasks. This is essential when other sensors on a robot fail to provide peak performance in particular conditions, such as a camera operating in the absence of light. This paper proposes a multi-robot system that is developed using ROS (Robot Operating System) for intruder detection in a single-range-sensor-per-robot scenario with centralized processing of detections from all robots by our central bot MIDNet (Multiple Intruder Detection Network). This work is aimed at providing an autonomous multi-robot security solution for a warehouse in the absence of human personnel. △ Less","7 November, 2020",https://arxiv.org/pdf/2011.03838
Deeply-Supervised Density Regression for Automatic Cell Counting in Microscopy Images,Shenghua He;Kyaw Thu Minn;Lilianna Solnica-Krezel;Mark A. Anastasio;Hua Li,"Accurately counting the number of cells in microscopy images is required in many medical diagnosis and biological studies. This task is tedious, time-consuming, and prone to subjective errors. However, designing automatic counting methods remains challenging due to low image contrast, complex background, large variance in cell shapes and counts, and significant cell occlusions in two-dimensional microscopy images. In this study, we proposed a new density regression-based method for automatically counting cells in microscopy images. The proposed method processes two innovations compared to other state-of-the-art density regression-based methods. First, the density regression model (DRM) is designed as a concatenated fully convolutional regression network (C-FCRN) to employ multi-scale image features for the estimation of cell density maps from given images. Second, auxiliary convolutional neural networks (AuxCNNs) are employed to assist in the training of intermediate layers of the designed C-FCRN to improve the DRM performance on unseen datasets. Experimental studies evaluated on four datasets demonstrate the superior performance of the proposed method. △ Less","9 November, 2020",https://arxiv.org/pdf/2011.03683
Flexible Virtual Reality System for Neurorehabilitation and Quality of Life Improvement,Iulia-Cristina Stanica;Florica Moldoveanu;Giovanni-Paul Portelli;Maria-Iuliana Dascalu;Alin Moldoveanu;Mariana Georgiana Ristea,"As life expectancy is mostly increasing, the incidence of many neurological disorders is also constantly growing. For improving the physical functions affected by a neurological disorder, rehabilitation procedures are mandatory, and they must be performed regularly. Unfortunately, neurorehabilitation procedures have disadvantages in terms of costs, accessibility and a lack of therapists. This paper presents Immersive Neurorehabilitation Exercises Using Virtual Reality (INREX-VR), our innovative immersive neurorehabilitation system using virtual reality. The system is based on a thorough research methodology and is able to capture real-time user movements and evaluate joint mobility for both upper and lower limbs, record training sessions and save electromyography data. The use of the first-person perspective increases immersion, and the joint range of motion is calculated with the help of both the HTC Vive system and inverse kinematics principles applied on skeleton rigs. Tutorial exercises are demonstrated by a virtual therapist, as they were recorded with real-life physicians, and sessions can be monitored and configured through tele-medicine. Complex movements are practiced in gamified settings, encouraging self-improvement and competition. Finally, we proposed a training plan and preliminary tests which show promising results in terms of accuracy and user feedback. As future developments, we plan to improve the system's accuracy and investigate a wireless alternative based on neural networks. △ Less","6 November, 2020",https://arxiv.org/pdf/2011.03596
Alquist 3.0: Alexa Prize Bot Using Conversational Knowledge Graph,Jan Pichl;Petr Marek;Jakub Konrád;Petr Lorenc;Van Duy Ta;Jan Šedivý,"The third version of the open-domain dialogue system Alquist developed within the Alexa Prize 2020 competition is designed to conduct coherent and engaging conversations on popular topics. The main novel contribution is the introduction of a system leveraging an innovative approach based on a conversational knowledge graph and adjacency pairs. The conversational knowledge graph allows the system to utilize knowledge expressed during the dialogue in consequent turns and across conversations. Dialogue adjacency pairs divide the conversation into small conversational structures, which can be combined and allow the system to react to a wide range of user inputs flexibly. We discuss and describe Alquist's pipeline, data acquisition and processing, dialogue manager, NLG, knowledge aggregation, and a hierarchy of adjacency pairs. We present the experimental results of the individual parts of the system. △ Less","6 November, 2020",https://arxiv.org/pdf/2011.03261
Chasing Carbon: The Elusive Environmental Footprint of Computing,Udit Gupta;Young Geun Kim;Sylvia Lee;Jordan Tse;Hsien-Hsin S. Lee;Gu-Yeon Wei;David Brooks;Carole-Jean Wu,"Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems. △ Less","28 October, 2020",https://arxiv.org/pdf/2011.02839
"Digital Twins: State of the Art Theory and Practice, Challenges, and Open Research Questions",Angira Sharma;Edward Kosasih;Jie Zhang;Alexandra Brintrup;Anisoara Calinescu,"Digital Twin was introduced over a decade ago, as an innovative all-encompassing tool, with perceived benefits including real-time monitoring, simulation and forecasting. However, the theoretical framework and practical implementations of digital twins (DT) are still far from this vision. Although successful implementations exist, sufficient implementation details are not publicly available, therefore it is difficult to assess their effectiveness, draw comparisons and jointly advance the DT methodology. This work explores the various DT features and current approaches, the shortcomings and reasons behind the delay in the implementation and adoption of digital twin. Advancements in machine learning, internet of things and big data have contributed hugely to the improvements in DT with regards to its real-time monitoring and forecasting properties. Despite this progress and individual company-based efforts, certain research gaps exist in the field, which have caused delay in the widespread adoption of this concept. We reviewed relevant works and identified that the major reasons for this delay are the lack of a universal reference framework, domain dependence, security concerns of shared data, reliance of digital twin on other technologies, and lack of quantitative metrics. We define the necessary components of a digital twin required for a universal reference framework, which also validate its uniqueness as a concept compared to similar concepts like simulation, autonomous systems, etc. This work further assesses the digital twin applications in different domains and the current state of machine learning and big data in it. It thus answers and identifies novel research questions, both of which will help to better understand and advance the theory and practice of digital twins. △ Less","4 December, 2020",https://arxiv.org/pdf/2011.02833
End-to-end-Architekturen zur Datenmonetarisierung im IIoT. Konzepte und Implementierungen,Christoph F. Strnadl,"The value creation potential of the Internet of Things (IoT), that is the connection of arbitrary objects to the Internet, lies in the creation of business benefits through accessing and processing the circa 80 Zettabytes (1 ZB = 10^21 Bytes) of data produced by an estimated 40 billions of IoT endpoints (prognosis for 2025). This contribution derives and presents the information technology-related fundament and basis required to be able to reap this potential. Quantity and heterogeneity of the devices and machines especially encountered in the industry at large in the so-called industrial IoT (IIoT) require the use of a typically cloud-based IoT platform for logical concentration and more efficient management of the -- unavoidable in industry -- complexity. Stringent non-functional requirements especially regarding (low) latency, (high) bandwidth, access to large processing capacities, and security and privacy-related aspects necessitate the deployment of intermediary IoT gateways endowed with different capability sets in the edge continuum between IoT endpoints and the IoT platform in the cloud. This will be illustrated in the form of two use cases from corporate projects using a component architecture view point. Finally we argue that this classical concept of IoT projects needs to be strategically widened towards application integration (key word: IT/OT integration) and API management resulting in the coupling of a suitable integration and API management platforms to the IoT platform in order to use this end-to-end understanding of IoT/IIoT to fully leverage the innovation-stimulating and transformational character of IIoT and Industry 4.0. △ Less","5 November, 2020",https://arxiv.org/pdf/2011.02801
Fault Detection for Covered Conductors With High-Frequency Voltage Signals: From Local Patterns to Global Features,Kunjin Chen;Tomáš Vantuch;Yu Zhang;Jun Hu;Jinliang He,"The detection and characterization of partial discharge (PD) are crucial for the insulation diagnosis of overhead lines with covered conductors. With the release of a large dataset containing thousands of naturally obtained high-frequency voltage signals, data-driven analysis of fault-related PD patterns on an unprecedented scale becomes viable. The high diversity of PD patterns and background noise interferences motivates us to design an innovative pulse shape characterization method based on clustering techniques, which can dynamically identify a set of representative PD-related pulses. Capitalizing on those pulses as referential patterns, we construct insightful features and develop a novel machine learning model with a superior detection performance for early-stage covered conductor faults. The presented model outperforms the winning model in a Kaggle competition and provides the state-of-the-art solution to detect real-time disturbances in the field. △ Less","31 October, 2020",https://arxiv.org/pdf/2011.02336
NSF Convergence Approach to Transition Basic Research into Practice,Shelby Smith;Chaitanya Baru,"The National Science Foundation Convergence Accelerator addresses national-scale societal challenges through use-inspired convergence research. Leveraging a convergence approach the Convergence Accelerator builds upon basic research and discovery to make timely investments to strengthen the Nations innovation ecosystem associated with several key R&D priority areas and practices to include the coronavirus disease 2019, harnessing the data revolution, the future of work, and quantum technology. Artificial Intelligence is a key underlying theme across all of these areas. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.01251
U-Net and its variants for medical image segmentation: theory and applications,Nahian Siddique;Paheding Sidike;Colin Elkin;Vijay Devabhaktuni,"U-net is an image segmentation technique developed primarily for medical image analysis that can precisely segment images using a scarce amount of training data. These traits provide U-net with a very high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in all major image modalities from CT scans and MRI to X-rays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. As the potential of U-net is still increasing, in this review we look at the various developments that have been made in the U-net architecture and provide observations on recent trends. We examine the various innovations that have been made in deep learning and discuss how these tools facilitate U-net. Furthermore, we look at image modalities and application areas where U-net has been applied. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.01118
Generating Knowledge Graphs by Employing Natural Language Processing and Machine Learning Techniques within the Scholarly Domain,Danilo Dessì;Francesco Osborne;Diego Reforgiato Recupero;Davide Buscaldi;Enrico Motta,"The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) show the advantage of such an hybrid system over alternative approaches, and vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge. △ Less","28 October, 2020",https://arxiv.org/pdf/2011.01103
The 2020s Political Economy of Machine Translation,Steven Weber,"This paper explores the hypothesis that the diversity of human languages, right now a barrier to interoperability in communication and trade, will become significantly less of a barrier as machine translation technologies are deployed over the next several years.But this new boundary-breaking technology does not reduce all boundaries equally, and it creates new challenges for the distribution of ideas and thus for innovation and economic growth. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.01007
Multi-Modal Active Learning for Automatic Liver Fibrosis Diagnosis based on Ultrasound Shear Wave Elastography,Lufei Gao;Ruisong Zhou;Changfeng Dong;Cheng Feng;Zhen Li;Xiang Wan;Li Liu,"With the development of radiomics, noninvasive diagnosis like ultrasound (US) imaging plays a very important role in automatic liver fibrosis diagnosis (ALFD). Due to the noisy data, expensive annotations of US images, the application of Artificial Intelligence (AI) assisting approaches encounters a bottleneck. Besides, the use of mono-modal US data limits the further improve of the classification results. In this work, we innovatively propose a multi-modal fusion network with active learning (MMFN-AL) for ALFD to exploit the information of multiple modalities, eliminate the noisy data and reduce the annotation cost. Four image modalities including US and three types of shear wave elastography (SWEs) are exploited. A new dataset containing these modalities from 214 candidates is well-collected and pre-processed, with the labels obtained from the liver biopsy results. Experimental results show that our proposed method outperforms the state-of-the-art performance using less than 30% data, and by using only around 80% data, the proposed fusion network achieves high AUC 89.27% and accuracy 70.59%. △ Less","1 November, 2020",https://arxiv.org/pdf/2011.00694
Efficient Arabic emotion recognition using deep neural networks,Ahmed Ali;Yasser Hifny,"Emotion recognition from speech signal based on deep learning is an active research area. Convolutional neural networks (CNNs) may be the dominant method in this area. In this paper, we implement two neural architectures to address this problem. The first architecture is an attention-based CNN-LSTM-DNN model. In this novel architecture, the convolutional layers extract salient features and the bi-directional long short-term memory (BLSTM) layers handle the sequential phenomena of the speech signal. This is followed by an attention layer, which extracts a summary vector that is fed to the fully connected dense layer (DNN), which finally connects to a softmax output layer. The second architecture is based on a deep CNN model. The results on an Arabic speech emotion recognition task show that our innovative approach can lead to significant improvements (2.2% absolute improvements) over a strong deep CNN baseline system. On the other hand, the deep CNN models are significantly faster than the attention based CNN-LSTM-DNN models in training and classification. △ Less","31 October, 2020",https://arxiv.org/pdf/2011.00346
AGAIN-VC: A One-shot Voice Conversion using Activation Guidance and Adaptive Instance Normalization,Yen-Hao Chen;Da-Yi Wu;Tsung-Han Wu;Hung-yi Lee,"Recently, voice conversion (VC) has been widely studied. Many VC systems use disentangle-based learning techniques to separate the speaker and the linguistic content information from a speech signal. Subsequently, they convert the voice by changing the speaker information to that of the target speaker. To prevent the speaker information from leaking into the content embeddings, previous works either reduce the dimension or quantize the content embedding as a strong information bottleneck. These mechanisms somehow hurt the synthesis quality. In this work, we propose AGAIN-VC, an innovative VC system using Activation Guidance and Adaptive Instance Normalization. AGAIN-VC is an auto-encoder-based model, comprising of a single encoder and a decoder. With a proper activation as an information bottleneck on content embeddings, the trade-off between the synthesis quality and the speaker similarity of the converted speech is improved drastically. This one-shot VC system obtains the best performance regardless of the subjective or objective evaluations. △ Less","31 October, 2020",https://arxiv.org/pdf/2011.00316
World of Code: Enabling a Research Workflow for Mining and Analyzing the Universe of Open Source VCS data,Yuxing Ma;Tapajit Dey;Chris Bogart;Sadika Amreen;Marat Valiev;Adam Tutko;David Kennard;Russell Zaretzki;Audris Mockus,"Open source software (OSS) is essential for modern society and, while substantial research has been done on individual (typically central) projects, only a limited understanding of the periphery of the entire OSS ecosystem exists. For example, how are the tens of millions of projects in the periphery interconnected through. technical dependencies, code sharing, or knowledge flow? To answer such questions we: a) create a very large and frequently updated collection of version control data in the entire FLOSS ecosystems named World of Code (WoC), that can completely cross-reference authors, projects, commits, blobs, dependencies, and history of the FLOSS ecosystems and b) provide capabilities to efficiently correct, augment, query, and analyze that data. Our current WoC implementation is capable of being updated on a monthly basis and contains over 18B Git objects. To evaluate its research potential and to create vignettes for its usage, we employ WoC in conducting several research tasks. In particular, we find that it is capable of supporting trend evaluation, ecosystem measurement, and the determination of package usage. We expect WoC to spur investigation into global properties of OSS development leading to increased resiliency of the entire OSS ecosystem. Our infrastructure facilitates the discovery of key technical dependencies, code flow, and social networks that provide the basis to determine the structure and evolution of the relationships that drive FLOSS activities and innovation. △ Less","30 October, 2020",https://arxiv.org/pdf/2010.16196
Designing learning experiences for online teaching and learning,Nachamma Sockalingam;Junhua Liu,"Teaching is about constantly innovating strategies, ways and means to engage diverse students in active and meaningful learning. In line with this, SUTD adopts various student-centric teaching and learning teaching methods and approaches. This means that our graduate/undergraduate instructors have to be ready to teach using these student student-centric teaching and learning pedagogies. In this article, I share my experiences of redesigning this teaching course that is typically conducted face-to-face to a synchronous online course and also invite one of the participant in this course to reflect on his experience as a student. △ Less","26 October, 2020",https://arxiv.org/pdf/2010.15602
Displacement-Invariant Matching Cost Learning for Accurate Optical Flow Estimation,Jianyuan Wang;Yiran Zhong;Yuchao Dai;Kaihao Zhang;Pan Ji;Hongdong Li,"Learning matching costs has been shown to be critical to the success of the state-of-the-art deep stereo matching methods, in which 3D convolutions are applied on a 4D feature volume to learn a 3D cost volume. However, this mechanism has never been employed for the optical flow task. This is mainly due to the significantly increased search dimension in the case of optical flow computation, ie, a straightforward extension would require dense 4D convolutions in order to process a 5D feature volume, which is computationally prohibitive. This paper proposes a novel solution that is able to bypass the requirement of building a 5D feature volume while still allowing the network to learn suitable matching costs from data. Our key innovation is to decouple the connection between 2D displacements and learn the matching costs at each 2D displacement hypothesis independently, ie, displacement-invariant cost learning. Specifically, we apply the same 2D convolution-based matching net independently on each 2D displacement hypothesis to learn a 4D cost volume. Moreover, we propose a displacement-aware projection layer to scale the learned cost volume, which reconsiders the correlation between different displacement candidates and mitigates the multi-modal problem in the learned cost volume. The cost volume is then projected to optical flow estimation through a 2D soft-argmin layer. Extensive experiments show that our approach achieves state-of-the-art accuracy on various datasets, and outperforms all published optical flow methods on the Sintel benchmark. △ Less","28 October, 2020",https://arxiv.org/pdf/2010.14851
Artificial intelligence based writer identification generates new evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the Great Isaiah Scroll (1QIsaa),Mladen Popović;Maruf A. Dhali;Lambert Schomaker,"The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal culture. Palaeography - the study of ancient handwriting - can provide access to this scribal culture. However, one of the problems of traditional palaeography is to determine writer identity when the writing style is near uniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end, we used pattern recognition and artificial intelligence techniques to innovate the palaeography of the scrolls regarding writer identification and to pioneer the microlevel of individual scribes to open access to the Bible's ancient scribal culture. Although many scholars believe that 1QIsaa was written by one scribe, we report new evidence for a breaking point in the series of columns in this scroll. Without prior assumption of writer identity, based on point clouds of the reduced-dimensionality feature-space, we found that columns from the first and second halves of the manuscript ended up in two distinct zones of such scatter plots, notably for a range of digital palaeography tools, each addressing very different featural aspects of the script samples. In a secondary, independent, analysis, now assuming writer difference and using yet another independent feature method and several different types of statistical testing, a switching point was found in the column series. A clear phase transition is apparent around column 27. Given the statistically significant differences between the two halves, a tertiary, post-hoc analysis was performed. Demonstrating that two main scribes were responsible for the Great Isaiah Scroll, this study sheds new light on the Bible's ancient scribal culture by providing new, tangible evidence that ancient biblical texts were not copied by a single scribe only but that multiple scribes could closely collaborate on one particular manuscript. △ Less","27 October, 2020",https://arxiv.org/pdf/2010.14476
"P^2
Net: Augmented Parallel-Pyramid Net for Attention Guided Pose Estimation",Luanxuan Hou;Jie Cao;Yuan Zhao;Haifeng Shen;Jian Tang;Ran He,"We propose an augmented Parallel-Pyramid Net (P^2~Net) with feature refinement by dilated bottleneck and attention module. During data preprocessing, we proposed a differentiable auto data augmentation (DA^2) method. We formulate the problem of searching data augmentaion policy in a differentiable form, so that the optimal policy setting can be easily updated by back propagation during training. DA^2 improves the training efficiency. A parallel-pyramid structure is followed to compensate the information loss introduced by the network. We innovate two fusion structures, i.e. Parallel Fusion and Progressive Fusion, to process pyramid features from backbone network. Both fusion structures leverage the advantages of spatial information affluence at high resolution and semantic comprehension at low resolution effectively. We propose a refinement stage for the pyramid features to further boost the accuracy of our network. By introducing dilated bottleneck and attention module, we increase the receptive field for the features with limited complexity and tune the importance to different feature channels. To further refine the feature maps after completion of feature extraction stage, an Attention Module (AM) is defined to extract weighted features from different scale feature maps generated by the parallel-pyramid structure. Compared with the traditional up-sampling refining, AM can better capture the relationship between channels. Experiments corroborate the effectiveness of our proposed method. Notably, our method achieves the best performance on the challenging MSCOCO and MPII datasets. △ Less","25 October, 2020",https://arxiv.org/pdf/2010.14076
Dynamic Adversarial Patch for Evading Object Detection Models,Shahar Hoory;Tzvika Shapira;Asaf Shabtai;Yuval Elovici,"Recent research shows that neural networks models used for computer vision (e.g., YOLO and Fast R-CNN) are vulnerable to adversarial evasion attacks. Most of the existing real-world adversarial attacks against object detectors use an adversarial patch which is attached to the target object (e.g., a carefully crafted sticker placed on a stop sign). This method may not be robust to changes in the camera's location relative to the target object; in addition, it may not work well when applied to nonplanar objects such as cars. In this study, we present an innovative attack method against object detectors applied in a real-world setup that addresses some of the limitations of existing attacks. Our method uses dynamic adversarial patches which are placed at multiple predetermined locations on a target object. An adversarial learning algorithm is applied in order to generate the patches used. The dynamic attack is implemented by switching between optimized patches dynamically, according to the camera's position (i.e., the object detection system's position). In order to demonstrate our attack in a real-world setup, we implemented the patches by attaching flat screens to the target object; the screens are used to present the patches and switch between them, depending on the current camera location. Thus, the attack is dynamic and adjusts itself to the situation to achieve optimal results. We evaluated our dynamic patch approach by attacking the YOLOv2 object detector with a car as the target object and succeeded in misleading it in up to 90% of the video frames when filming the car from a wide viewing angle range. We improved the attack by generating patches that consider the semantic distance between the target object and its classification. We also examined the attack's transferability among different car models and were able to mislead the detector 71% of the time. △ Less","25 October, 2020",https://arxiv.org/pdf/2010.13070
A Characterization of 3D Printability,Ioannis Fudos;Margarita Ntousia;Vasiliki Stamati;Paschalis Charalampous;Theodora Kontodina;Ioannis Kostavelis;Dimitrios Tzovaras;Leonardo Bilalis,"Additive manufacturing technologies are positioned to provide an unprecedented innovative transformation in how products are designed and manufactured. Due to differences in the technical specifications of AM technologies, the final fabricated parts can vary significantly from the original CAD models, therefore raising issues regarding accuracy, surface finish, robustness, mechanical properties, functional and geometrical constraints. Various researchers have studied the correlation between AM technologies and design rules. In this work we propose a novel approach to assessing the capability of a 3D model to be printed successfully (a.k.a printability) on a specific AM machine. This is utilized by taking into consideration the model mesh complexity and certain part characteristics. A printability score is derived for a model in reference to a specific 3D printing technology, expressing the probability of obtaining a robust and accurate end result for 3D printing on a specific AM machine. The printability score can be used either to determine which 3D technology is more suitable for manufacturing a specific model or as a guide to redesign the model to ensure printability. We verify this framework by conducting 3D printing experiments for benchmark models which are printed on three AM machines employing different technologies: Fused Deposition Modeling (FDM), Binder Jetting (3DP), and Material Jetting (Polyjet). △ Less","24 October, 2020",https://arxiv.org/pdf/2010.12930
High-Throughput Image-Based Plant Stand Count Estimation Using Convolutional Neural Networks,Saeed Khaki;Hieu Pham;Ye Han;Wade Kent;Lizhi Wang,"The future landscape of modern farming and plant breeding is rapidly changing due to the complex needs of our society. The explosion of collectable data has started a revolution in agriculture to the point where innovation must occur. To a commercial organization, the accurate and efficient collection of information is necessary to ensure that optimal decisions are made at key points of the breeding cycle. However, due to the shear size of a breeding program and current resource limitations, the ability to collect precise data on individual plants is not possible. In particular, efficient phenotyping of crops to record its color, shape, chemical properties, disease susceptibility, etc. is severely limited due to labor requirements and, oftentimes, expert domain knowledge. In this paper, we propose a deep learning based approach, named DeepStand, for image-based corn stand counting at early phenological stages. The proposed method adopts a truncated VGG-16 network as a backbone feature extractor and merges multiple feature maps with different scales to make the network robust against scale variation. Our extensive computational experiments suggest that our proposed method can successfully count corn stands and out-perform other state-of-the-art methods. It is the goal of our work to be used by the larger agricultural community as a way to enable high-throughput phenotyping without the use of extensive time and labor requirements. △ Less","23 October, 2020",https://arxiv.org/pdf/2010.12552
EventKG+Click: A Dataset of Language-specific Event-centric User Interaction Traces,Sara Abdollahi;Simon Gottschalk;Elena Demidova,"An increasing need to analyse event-centric cross-lingual information calls for innovative user interaction models that assist users in crossing the language barrier. However, datasets that reflect user interaction traces in cross-lingual settings required to train and evaluate the user interaction models are mostly missing. In this paper, we present the EventKG+Click dataset that aims to facilitate the creation and evaluation of such interaction models. EventKG+Click builds upon the event-centric EventKG knowledge graph and language-specific information on user interactions with events, entities, and their relations derived from the Wikipedia clickstream. △ Less","23 October, 2020",https://arxiv.org/pdf/2010.12370
Digital Transformation: Environmental Friend or Foe? Panel Discussion at the Australasian Conference on Information Systems 2019,Sachithra Lokuge;Darshana Sedera;Vanessa Cooper;Frada Burstein,"The advent of digital technologies such as social media, mobile, analytics, cloud computing and internet-of-things has provided unique opportunities for organizations to engage in innovations that are affordable, easy-to-use, easy-to-learn and easy-to-implement. Transformations through such technologies often have positive impacts on business processes, products and services. As such, organizations have managed to increase productivity and efficiency, reduce cycle time and make substantial gains through digital transformation. Such transformations have also been positively associated with reducing harmful environmental impacts by providing organizations alternative ways of undertaking their business activities. However, in recent times, especially with an abundance of technologies being available at near-zero costs, questions regarding the potential negative impacts of digital transformation on the environment have arisen. The morass of the ubiquitous technologies around us necessitates the continuing creation of large data centers, that are increasing their capacity yielding a negative impact on the environment. Considering this dialectical contradiction, a panel was conducted at the Australasian Conference on Information Systems (ACIS) in Perth, Australia, in 2019. Its aim was to invigorate the dialogue regarding the impact of digital transformation on environmental sustainability and suggested some directions for future research in this area. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.12034
Theoretical opportunities for rural innovation and entrepreneurship research,Sachithra Lokuge,"Even though rural entrepreneurship and innovation has been studied for decades, the advent of social media, mobile, analytics, cloud computing and internet of things - also referred as digital technologies - (Nambisan 2013, Yoo et al. 2012) has provided new opportunities and challenges for this vast discipline. As a result, we see new business models, new processes, products and services offered using new digital technologies. Such changes challenge the orthodox view of IT entrepreneurship and innovation, opening new avenues for researches and challenges the existing theoretical understanding. This book chapter is an attempt to understand the existing literature on rural innovation and entrepreneurship in information systems discipline and identify opportunities for rural entrepreneurship and innovation in the digital era. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.12031
Once-for-All Adversarial Training: In-Situ Tradeoff between Robustness and Accuracy for Free,Haotao Wang;Tianlong Chen;Shupeng Gui;Ting-Kuei Hu;Ji Liu;Zhangyang Wang,"Adversarial training and its many variants substantially improve deep network robustness, yet at the cost of compromising standard accuracy. Moreover, the training process is heavy and hence it becomes impractical to thoroughly explore the trade-off between accuracy and robustness. This paper asks this new question: how to quickly calibrate a trained model in-situ, to examine the achievable trade-offs between its standard and robust accuracies, without (re-)training it many times? Our proposed framework, Once-for-all Adversarial Training (OAT), is built on an innovative model-conditional training framework, with a controlling hyper-parameter as the input. The trained model could be adjusted among different standard and robust accuracies ""for free"" at testing time. As an important knob, we exploit dual batch normalization to separate standard and adversarial feature statistics, so that they can be learned in one model without degrading performance. We further extend OAT to a Once-for-all Adversarial Training and Slimming (OATS) framework, that allows for the joint trade-off among accuracy, robustness and runtime efficiency. Experiments show that, without any re-training nor ensembling, OAT/OATS achieve similar or even superior performance compared to dedicatedly trained models at various configurations. Our codes and pretrained models are available at: https://github.com/VITA-Group/Once-for-All-Adversarial-Training. △ Less","10 November, 2020",https://arxiv.org/pdf/2010.11828
A Simple Methodology for Model-Driven Business Innovation and Low Code Implementation,Michele Missikoff,"Low Code platforms, according to Gartner Group, represent one of the more disruptive technologies in the development and maintenance of enterprise applications. The key factor is represented by the central involvement of business people and domain expert, with a substantial disintermediation with respect to technical people. In this paper we propose a methodology conceived to support non-technical people in addressing business process innovation and developing enterprise software application. The proposed methodology, called EasInnova, is solidly rooted in Model-Driven Engineering and adopts a three staged model of an innovation undertaking. The three stages are: AsIs that models the existing business scenario; Transformation that consists in the elaboration of the actual innovation; ToBe that concerns the modeling of new business scenario. The core of EasInnova is represented by a matrix where columns are the three innovation stages and the rows are the three Model-Driven Architecture layers: CIM, PIM, PSM. The cells indicate the steps to be followed in achieving the sought innovation. Finally, the produced models will be transferred onto a BonitaSoft, the Low Code platform selected in our work. The methodology is described by means of a simple example in the domain of home food delivery. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.11611
AI-lead Court Debate Case Investigation,Changzhen Ji;Xin Zhou;Conghui Zhu;Tiejun Zhao,"The multi-role judicial debate composed of the plaintiff, defendant, and judge is an important part of the judicial trial. Different from other types of dialogue, questions are raised by the judge, The plaintiff, plaintiff's agent defendant, and defendant's agent would be to debating so that the trial can proceed in an orderly manner. Question generation is an important task in Natural Language Generation. In the judicial trial, it can help the judge raise efficient questions so that the judge has a clearer understanding of the case. In this work, we propose an innovative end-to-end question generation model-Trial Brain Model (TBM) to build a Trial Brain, it can generate the questions the judge wants to ask through the historical dialogue between the plaintiff and the defendant. Unlike prior efforts in natural language generation, our model can learn the judge's questioning intention through predefined knowledge. We do experiments on real-world datasets, the experimental results show that our model can provide a more accurate question in the multi-role court debate scene. △ Less","15 November, 2020",https://arxiv.org/pdf/2010.11604
When Machine Learning Meets Congestion Control: A Survey and Comparison,Huiling Jiang;Qing Li;Yong Jiang;Gengbiao Shen;Richard Sinnott;Chen Tian;Mingwei Xu,"Machine learning (ML) has seen a significant surge and uptake across many diverse applications. The high flexibility, adaptability and computing capabilities it provides extends traditional approaches used in multiple fields including network operation and management. Numerous surveys have explored ML in the context of networking, such as traffic engineering, performance optimization and network security. Many ML approaches focus on clustering, classification, regression and reinforcement learning (RL). The innovation of this research and contribution of this paper lies in the detailed summary and comparison of learning-based congestion control (CC) approaches. Compared with traditional CC algorithms which are typically rule-based, capabilities to learn from historical experience are highly desirable. From the literature, it is observed that RL is a crucial trend among learning-based CC algorithms. In this paper, we explore the performance of RL-based CC algorithms and present current problems with RL-based CC algorithms. We outline challenges and trends related to learning-based CC algorithms. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.11397
Serverless Containers -- rising viable approach to Scientific Workflows,Krzysztof Burkat;Maciej Pawlik;Bartosz Balis;Maciej Malawski;Karan Vahi;Mats Rynge;Rafael Ferreira da Silva;Ewa Deelman,"Increasing popularity of the serverless computing approach has led to the emergence of new cloud infrastructures working in Container-as-a-Service (CaaS) model like AWS Fargate, Google Cloud Run, or Azure Container Instances. They introduce an innovative approach to running cloud containers where developers are freed from managing underlying resources. In this paper, we focus on evaluating capabilities of elastic containers and their usefulness for scientific computing in the scientific workflow paradigm using AWS Fargate and Google Cloud Run infrastructures. For experimental evaluation of our approach, we extended HyperFlow engine to support these CaaS platform, together with adapting four real-world scientific workflows composed of several dozen to over a hundred of tasks organized into a dependency graph. We used these workflows to create cost-performance benchmarks and flow execution plots, measuring delays, elasticity, and scalability. The experiments proved that serverless containers can be successfully applied for scientific workflows. Also, the results allow us to gain insights on specific advantages and limits of such platforms. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.11320
Joint Blind Room Acoustic Characterization From Speech And Music Signals Using Convolutional Recurrent Neural Networks,Paul Callens;Milos Cernak,"Acoustic environment characterization opens doors for sound reproduction innovations, smart EQing, speech enhancement, hearing aids, and forensics. Reverberation time, clarity, and direct-to-reverberant ratio are acoustic parameters that have been defined to describe reverberant environments. They are closely related to speech intelligibility and sound quality. As explained in the ISO3382 standard, they can be derived from a room measurement called the Room Impulse Response (RIR). However, measuring RIRs requires specific equipment and intrusive sound to be played. The recent audio combined with machine learning suggests that one could estimate those parameters blindly using speech or music signals. We follow these advances and propose a robust end-to-end method to achieve blind joint acoustic parameter estimation using speech and/or music signals. Our results indicate that convolutional recurrent neural networks perform best for this task, and including music in training also helps improve inference from speech. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.11167
Scalable HPC and AI Infrastructure for COVID-19 Therapeutics,Hyungro Lee;Andre Merzky;Li Tan;Mikhail Titov;Matteo Turilli;Dario Alfe;Agastya Bhati;Alex Brace;Austin Clyde;Peter Coveney;Heng Ma;Arvind Ramanathan;Rick Stevens;Anda Trifan;Hubertus Van Dam;Shunzhou Wan;Sean Wilkinson;Shantenu Jha,"COVID-19 has claimed more 1 million lives and resulted in over 40 million infections. There is an urgent need to identify drugs that can inhibit SARS-CoV-2. In response, the DOE recently established the Medical Therapeutics project as part of the National Virtual Biotechnology Laboratory, and tasked it with creating the computational infrastructure and methods necessary to advance therapeutics development. We discuss innovations in computational infrastructure and methods that are accelerating and advancing drug design. Specifically, we describe several methods that integrate artificial intelligence and simulation-based approaches, and the design of computational infrastructure to support these methods at scale. We discuss their implementation and characterize their performance, and highlight science advances that these capabilities have enabled. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.10517
KaFHCa: Key-establishment via Frequency Hopping Collisions,Muhammad Usman;Simone Raponi;Marwa Qaraqe;Gabriele Oligeri,"The massive deployment of IoT devices being utilized by home automation, industrial and military scenarios demands for high security and privacy standards to be achieved through innovative solutions. This paper proposes KaFHCa, a crypto-less protocol that generates shared secret keys by combining random frequency hopping collisions and source indistinguishability independently of the radio channel status. While other solutions tie the secret bit rate generation to the current radio channel conditions, thus becoming unpractical in static environments, KaFHCa guarantees almost the same secret bitrate independently of the channel conditions. KaFHCa generates shared secrets through random collisions of the transmitter and the receiver in the radio spectrum, and leverages on the fading phenomena to achieve source indistinguishability, thus preventing unauthorized eavesdroppers from inferring the key. The proposed solution is (almost) independent of the adversary position, works under the conservative assumption of channel fading (σ = 8dB), and is capable of generating a secret key of 128 bits with less than 564 transmissions. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.09642
Against Scale: Provocations and Resistances to Scale Thinking,Alex Hanna;Tina M. Park,"At the heart of what drives the bulk of innovation and activity in Silicon Valley and elsewhere is scalability. This unwavering commitment to scalability -- to identify strategies for efficient growth -- is at the heart of what we refer to as ""scale thinking."" Whether people are aware of it or not, scale thinking is all-encompassing. It is not just an attribute of one's product, service, or company, but frames how one thinks about the world (what constitutes it and how it can be observed and measured), its problems (what is a problem worth solving versus not), and the possible technological fixes for those problems. This paper examines different facets of scale thinking and its implication on how we view technology and collaborative work. We argue that technological solutions grounded in scale thinking are unlikely to be as liberatory or effective at deep, systemic change as their purveyors imagine. Rather, solutions which resist scale thinking are necessary to undo the social structures which lie at the heart of social inequality. We draw on recent work on mutual aid networks and propose questions to ask of collaborative work systems as a means to evaluate technological solutions and guide designers in identifying sites of resistance to scale thinking. △ Less","20 November, 2020",https://arxiv.org/pdf/2010.08850
Semantics of the Black-Box: Can knowledge graphs help make deep learning systems more interpretable and explainable?,Manas Gaur;Keyur Faldu;Amit Sheth,"The recent series of innovations in deep learning (DL) have shown enormous potential to impact individuals and society, both positively and negatively. The DL models utilizing massive computing power and enormous datasets have significantly outperformed prior historical benchmarks on increasingly difficult, well-defined research tasks across technology domains such as computer vision, natural language processing, signal processing, and human-computer interactions. However, the Black-Box nature of DL models and their over-reliance on massive amounts of data condensed into labels and dense representations poses challenges for interpretability and explainability of the system. Furthermore, DLs have not yet been proven in their ability to effectively utilize relevant domain knowledge and experience critical to human understanding. This aspect is missing in early data-focused approaches and necessitated knowledge-infused learning and other strategies to incorporate computational knowledge. This article demonstrates how knowledge, provided as a knowledge graph, is incorporated into DL methods using knowledge-infused learning, which is one of the strategies. We then discuss how this makes a fundamental difference in the interpretability and explainability of current approaches, and illustrate it with examples from natural language processing for healthcare and education applications. △ Less","11 December, 2020",https://arxiv.org/pdf/2010.08660
QReLU and m-QReLU: Two novel quantum activation functions to aid medical diagnostics,L. Parisi;D. Neagu;R. Ma;F. Campean,"The ReLU activation function (AF) has been extensively applied in deep neural networks, in particular Convolutional Neural Networks (CNN), for image classification despite its unresolved dying ReLU problem, which poses challenges to reliable applications. This issue has obvious important implications for critical applications, such as those in healthcare. Recent approaches are just proposing variations of the activation function within the same unresolved dying ReLU challenge. This contribution reports a different research direction by investigating the development of an innovative quantum approach to the ReLU AF that avoids the dying ReLU problem by disruptive design. The Leaky ReLU was leveraged as a baseline on which the two quantum principles of entanglement and superposition were applied to derive the proposed Quantum ReLU (QReLU) and the modified-QReLU (m-QReLU) activation functions. Both QReLU and m-QReLU are implemented and made freely available in TensorFlow and Keras. This original approach is effective and validated extensively in case studies that facilitate the detection of COVID-19 and Parkinson Disease (PD) from medical images. The two novel AFs were evaluated in a two-layered CNN against nine ReLU-based AFs on seven benchmark datasets, including images of spiral drawings taken via graphic tablets from patients with Parkinson Disease and healthy subjects, and point-of-care ultrasound images on the lungs of patients with COVID-19, those with pneumonia and healthy controls. Despite a higher computational cost, results indicated an overall higher classification accuracy, precision, recall and F1-score brought about by either quantum AFs on five of the seven bench-mark datasets, thus demonstrating its potential to be the new benchmark or gold standard AF in CNNs and aid image classification tasks involved in critical applications, such as medical diagnoses of COVID-19 and PD. △ Less","15 October, 2020",https://arxiv.org/pdf/2010.08031
A Methodology for Ethics-by-Design AI Systems: Dealing with Human Value Conflicts,Fabrice Muhlenbach,"The introduction of artificial intelligence into activities traditionally carried out by human beings produces brutal changes. This is not without consequences for human values. This paper is about designing and implementing models of ethical behaviors in AI-based systems, and more specifically it presents a methodology for designing systems that take ethical aspects into account at an early stage while finding an innovative solution to prevent human values from being affected. Two case studies where AI-based innovations complement economic and social proposals with this methodology are presented: one in the field of culture and operated by a private company, the other in the field of scientific research and supported by a state organization. △ Less","15 October, 2020",https://arxiv.org/pdf/2010.07610
Overview of C-ITS Deployment Projects in Europe and USA,Areti Kotsi;Evangelos Mitsakis;Dimitris Tzanis,"Cooperative Intelligent Transportation Systems (C-ITS) are technologies that enable vehicles to communicate with each other and with the road infrastructure. These innovative technologies enable road users and traffic managers to share useful information, assisting the coordination of their actions. During the last years various initiatives providing policy rules for C-ITS deployment and a large number of projects demonstrating C-ITS implementation have taken place in Europe and USA. However, the identification of the status of C-ITS deployment remains ambiguous at binational level. The purpose of this paper is to provide an overview of the European and US milestones, that have been reached so far in the field of C-ITS, by identifying and reporting the policy framework, as well as the projects concerning C-ITS deployment in Europe and USA. △ Less","14 October, 2020",https://arxiv.org/pdf/2010.07299
Handwriting Quality Analysis using Online-Offline Models,Yahia Hamdi;Hanen Akouaydi;Houcine Boubaker;Adel M. Alimi,"This work is part of an innovative e-learning project allowing the development of an advanced digital educational tool that provides feedback during the process of learning handwriting for young school children (three to eight years old). In this paper, we describe a new method for children handwriting quality analysis. It automatically detects mistakes, gives real-time on-line feedback for children's writing, and helps teachers comprehend and evaluate children's writing skills. The proposed method adjudges five main criteria shape, direction, stroke order, position respect to the reference lines, and kinematics of the trace. It analyzes the handwriting quality and automatically gives feedback based on the combination of three extracted models: Beta-Elliptic Model (BEM) using similarity detection (SD) and dissimilarity distance (DD) measure, Fourier Descriptor Model (FDM), and perceptive Convolutional Neural Network (CNN) with Support Vector Machine (SVM) comparison engine. The originality of our work lies partly in the system architecture which apprehends complementary dynamic, geometric, and visual representation of the examined handwritten scripts and in the efficient selected features adapted to various handwriting styles and multiple script languages such as Arabic, Latin, digits, and symbol drawing. The application offers two interactive interfaces respectively dedicated to learners, educators, experts or teachers and allows them to adapt it easily to the specificity of their disciples. The evaluation of our framework is enhanced by a database collected in Tunisia primary school with 400 children. Experimental results show the efficiency and robustness of our suggested framework that helps teachers and children by offering positive feedback throughout the handwriting learning process using tactile digital devices. △ Less","9 October, 2020",https://arxiv.org/pdf/2010.06693
Deep Delay Loop Reservoir Computing for Specific Emitter Identification,Silvija Kokalj-Filipovic;Paul Toliver;William Johnson;Raymond R. Hoare II;Joseph J. Jezak,"Current AI systems at the tactical edge lack the computational resources to support in-situ training and inference for situational awareness, and it is not always practical to leverage backhaul resources due to security, bandwidth, and mission latency requirements. We propose a solution through Deep delay Loop Reservoir Computing (DLR), a processing architecture supporting general machine learning algorithms on compact mobile devices by leveraging delay-loop (DL) reservoir computing in combination with innovative photonic hardware exploiting the inherent speed, and spatial, temporal and wavelength-based processing diversity of signals in the optical domain. DLR delivers reductions in form factor, hardware complexity, power consumption and latency, compared to State-of-the-Art . DLR can be implemented with a single photonic DL and a few electro-optical components. In certain cases multiple DL layers increase learning capacity of the DLR with no added latency. We demonstrate the advantages of DLR on the application of RF Specific Emitter Identification. △ Less","13 October, 2020",https://arxiv.org/pdf/2010.06649
On Deep Learning Techniques to Boost Monocular Depth Estimation for Autonomous Navigation,Raul de Queiroz Mendes;Eduardo Godinho Ribeiro;Nicolas dos Santos Rosa;Valdir Grassi Jr,"Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets. △ Less","28 December, 2020",https://arxiv.org/pdf/2010.06626
IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads,Aymen Al Saadi;Dario Alfe;Yadu Babuji;Agastya Bhati;Ben Blaiszik;Thomas Brettin;Kyle Chard;Ryan Chard;Peter Coveney;Anda Trifan;Alex Brace;Austin Clyde;Ian Foster;Tom Gibbs;Shantenu Jha;Kristopher Keipert;Thorsten Kurth;Dieter Kranzlmüller;Hyungro Lee;Zhuozhao Li;Heng Ma;Andre Merzky;Gerald Mathias;Alexander Partin;Junqi Yin,"The drug discovery process currently employed in the pharmaceutical industry typically requires about 10 years and $2-3 billion to deliver one new drug. This is both too expensive and too slow, especially in emergencies like the COVID-19 pandemic. In silicomethodologies need to be improved to better select lead compounds that can proceed to later stages of the drug discovery protocol accelerating the entire process. No single methodological approach can achieve the necessary accuracy with required efficiency. Here we describe multiple algorithmic innovations to overcome this fundamental limitation, development and deployment of computational infrastructure at scale integrates multiple artificial intelligence and simulation-based approaches. Three measures of performance are:(i) throughput, the number of ligands per unit time; (ii) scientific performance, the number of effective ligands sampled per unit time and (iii) peak performance, in flop/s. The capabilities outlined here have been used in production for several months as the workhorse of the computational infrastructure to support the capabilities of the US-DOE National Virtual Biotechnology Laboratory in combination with resources from the EU Centre of Excellence in Computational Biomedicine. △ Less","13 October, 2020",https://arxiv.org/pdf/2010.06574
DORi: Discovering Object Relationship for Moment Localization of a Natural-Language Query in Video,Cristian Rodriguez-Opazo;Edison Marrese-Taylor;Basura Fernando;Hongdong Li;Stephen Gould,"This paper studies the task of temporal moment localization in a long untrimmed video using natural language query. Given a query sentence, the goal is to determine the start and end of the relevant segment within the video. Our key innovation is to learn a video feature embedding through a language-conditioned message-passing algorithm suitable for temporal moment localization which captures the relationships between humans, objects and activities in the video. These relationships are obtained by a spatial sub-graph that contextualizes the scene representation using detected objects and human features conditioned in the language query. Moreover, a temporal sub-graph captures the activities within the video through time. Our method is evaluated on three standard benchmark datasets, and we also introduce YouCookII as a new benchmark for this task. Experiments show our method outperforms state-of-the-art methods on these datasets, confirming the effectiveness of our approach. △ Less","13 October, 2020",https://arxiv.org/pdf/2010.06260
New Formulas of Feedback Capacity for AGN Channels with Memory: A Time-Domain Sufficient Statistic Approach,Charalambos D. Charalambous;Christos Kourtellaris;Stelios Louka,"In the recent paper [1] it is shown, via an application example, that the Cover and Pombra [2] ""characterization of the n-block or transmission"" feedback capacity formula, of additive Gaussian noise (AGN) channels, is the subject of much confusion in the literature, with redundant incorrect results. The main objective of this paper is to clarify the main points of confusion and remove any further ambiguity. The first part of the paper applies time-domain methods, to derive for a first time, equivalent sequential characterizations of the Cover and Pombra characterization of feedback capacity of AGN channels driven by nonstationary and nonergodic Gaussian noise. The optimal channel input processes of the new equivalent sequential characterizations are expressed as functionals of a sufficient statistic and a Gaussian orthogonal innovations process. From the new representations follows that the Cover and Pombra n-block capacity formula is expressed as a functional of two generalized matrix difference Riccati equations (DRE) of filtering theory of Gaussian systems, contrary to results that appeared in the literature. In the second part of the paper the existence of the asymptotic limit of the n-block feedback capacity formula is shown to be equivalent to the convergence properties of solutions of the two generalized DREs. Further, necessary and or sufficient conditions are identified for existence of the asymptotic limits, for stable and unstable Gaussian noise, when the optimal input distributions are time-invariant, but not necessarily stationary. The paper contains an in depth analysis, with examples, of the specific technical issues, which are overlooked in past literature [3-7], that studied the AGN channel of [2], for stationary noises. △ Less","13 October, 2020",https://arxiv.org/pdf/2010.06226
The Wireless Train Communication Network: Roll2Rail vision,Juan Moreno García-Loygorri;Javier Goikoetxea;Eneko Echeverría;Aitor Arriola;Iñaki Val;Stephan Sand;Paul Unterhuber;Francisco del Río,"This paper explains the main results obtained from the research carried out in the work package 2 (WP2) of the Roll2Rail (R2R) project. This project aims to develop key technologies and to remove already identified blocking points for radical innovation in the field of railway vehicles, to increase their operational reliability and to reduce life-cycle costs. This project started in May 2015 and has been funded by the Horizon 2020 program of the European Commission. The goal for WP2 is to research on both technologies and architectures to develop a new wireless Train Communication Network (TCN) within IEC61375 standard series. This TCN is today entirely wired and is used for Train Control and Monitoring System (TCMS) functions (some of them safetyrelated), operator-oriented services and customer-oriented services. This paradigm shift from wired to wireless means a removal of wirings implies, among other benefits, a significant reduction of life cycle costs due to the removal of cables, and the simplification of the train coupling procedure, among others. △ Less","5 October, 2020",https://arxiv.org/pdf/2010.05630
Automatic Quantification of Settlement Damage using Deep Learning of Satellite Images,Lili Lu;Weisi Guo,"Humanitarian disasters and political violence cause significant damage to our living space. The reparation cost to homes, infrastructure, and the ecosystem is often difficult to quantify in real-time. Real-time quantification is critical to both informing relief operations, but also planning ahead for rebuilding. Here, we use satellite images before and after major crisis around the world to train a robust baseline Residual Network (ResNet) and a disaster quantification Pyramid Scene Parsing Network (PSPNet). ResNet offers robustness to poor image quality and can identify areas of destruction with high accuracy (92\%), whereas PSPNet offers contextualised quantification of built environment damage with good accuracy (84\%). As there are multiple damage dimensions to consider (e.g. economic loss and fatalities), we fit a multi-linear regression model to quantify the overall damage. To validate our combined system of deep learning and regression modeling, we successfully match our prediction to the ongoing recovery in the 2020 Beirut port explosion. These innovations provide a better quantification of overall disaster magnitude and inform intelligent humanitarian systems of unfolding disasters. △ Less","12 October, 2020",https://arxiv.org/pdf/2010.05512
Software Sustainability & High Energy Physics,Daniel S. Katz;Sudhir Malik;Mark S. Neubauer;Graeme A. Stewart;Kétévi A. Assamagan;Erin A. Becker;Neil P. Chue Hong;Ian A. Cosden;Samuel Meehan;Edward J. W. Moyse;Adrian M. Price-Whelan;Elizabeth Sexton-Kennedy;Meirin Oan Evans;Matthew Feickert;Clemens Lange;Kilian Lieret;Rob Quick;Arturo Sánchez Pineda;Christopher Tunnell,"New facilities of the 2020s, such as the High Luminosity Large Hadron Collider (HL-LHC), will be relevant through at least the 2030s. This means that their software efforts and those that are used to analyze their data need to consider sustainability to enable their adaptability to new challenges, longevity, and efficiency, over at least this period. This will help ensure that this software will be easier to develop and maintain, that it remains available in the future on new platforms, that it meets new needs, and that it is as reusable as possible. This report discusses a virtual half-day workshop on ""Software Sustainability and High Energy Physics"" that aimed 1) to bring together experts from HEP as well as those from outside to share their experiences and practices, and 2) to articulate a vision that helps the Institute for Research and Innovation in Software for High Energy Physics (IRIS-HEP) to create a work plan to implement elements of software sustainability. Software sustainability practices could lead to new collaborations, including elements of HEP software being directly used outside the field, and, as has happened more frequently in recent years, to HEP developers contributing to software developed outside the field rather than reinventing it. A focus on and skills related to sustainable software will give HEP software developers an important skill that is essential to careers in the realm of software, inside or outside HEP. The report closes with recommendations to improve software sustainability in HEP, aimed at the HEP community via IRIS-HEP and the HEP Software Foundation (HSF). △ Less","16 October, 2020",https://arxiv.org/pdf/2010.05102
Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data,William Huang;Haokun Liu;Samuel R. Bowman,"A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks---datasets collected from crowdworkers to create an evaluation task---while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data---data built by minimally editing a set of seed examples to yield counterfactual labels---to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable. △ Less","9 October, 2020",https://arxiv.org/pdf/2010.04762
Neural Networks as Functional Classifiers,Barinder Thind;Kevin Multani;Jiguo Cao,"In recent years, there has been considerable innovation in the world of predictive methodologies. This is evident by the relative domination of machine learning approaches in various classification competitions. While these algorithms have excelled at multivariate problems, they have remained dormant in the realm of functional data analysis. We extend notable deep learning methodologies to the domain of functional data for the purpose of classification problems. We highlight the effectiveness of our method in a number of classification applications such as classification of spectrographic data. Moreover, we demonstrate the performance of our classifier through simulation studies in which we compare our approach to the functional linear model and other conventional classification methods. △ Less","8 October, 2020",https://arxiv.org/pdf/2010.04305
The Short Anthropological Guide to the Study of Ethical AI,Alexandrine Royer,"Over the next few years, society as a whole will need to address what core values it wishes to protect when dealing with technology. Anthropology, a field dedicated to the very notion of what it means to be human, can provide some interesting insights into how to cope and tackle these changes in our Western society and other areas of the world. It can be challenging for social science practitioners to grasp and keep up with the pace of technological innovation, with many being unfamiliar with the jargon of AI. This short guide serves as both an introduction to AI ethics and social science and anthropological perspectives on the development of AI. It intends to provide those unfamiliar with the field with an insight into the societal impact of AI systems and how, in turn, these systems can lead us to rethink how our world operates. △ Less","7 October, 2020",https://arxiv.org/pdf/2010.03362
"Knowledge-enriched, Type-constrained and Grammar-guided Question Generation over Knowledge Bases",Sheng Bi;Xiya Cheng;Yuan-Fang Li;Yongzhen Wang;Guilin Qi,"Question generation over knowledge bases (KBQG) aims at generating natural-language questions about a subgraph, i.e. a set of (connected) triples. Two main challenges still face the current crop of encoder-decoder-based methods, especially on small subgraphs: (1) low diversity and poor fluency due to the limited information contained in the subgraphs, and (2) semantic drift due to the decoder's oblivion of the semantics of the answer entity. We propose an innovative knowledge-enriched, type-constrained and grammar-guided KBQG model, named KTG, to addresses the above challenges. In our model, the encoder is equipped with auxiliary information from the KB, and the decoder is constrained with word types during QG. Specifically, entity domain and description, as well as relation hierarchy information are considered to construct question contexts, while a conditional copy mechanism is incorporated to modulate question semantics according to current word types. Besides, a novel reward function featuring grammatical similarity is designed to improve both generative richness and syntactic correctness via reinforcement learning. Extensive experiments show that our proposed model outperforms existing methods by a significant margin on two widely-used benchmark datasets SimpleQuestion and PathQuestion. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.03157
A Course on Controllers,Bill Verplank;Craig Sapp;Max Mathews,"Over the last four years, we have developed a series of lectures, labs and project assignments aimed at introducing enough technology so that students from a mix of disciplines can design and build innovative interface devices. △ Less","4 October, 2020",https://arxiv.org/pdf/2010.01569
Embedded Systems and Computer Vision Techniques utilized in Spray Painting Robots: A Review,Soham Shah;Siddhi Vinayak Pandey;Archit Sorathiya;Raj Sheth;Alok Kumar Singh;Jignesh Thaker,"The advent of the era of machines has limited human interaction and this has increased their presence in the last decade. The requirement to increase the effectiveness, durability and reliability in the robots has also risen quite drastically too. Present paper covers the various embedded system and computer vision methodologies, techniques and innovations used in the field of spray painting robots. There have been many advancements in the sphere of painting robots utilized for high rise buildings, wall painting, road marking paintings, etc. Review focuses on image processing, computational and computer vision techniques that can be applied in the product to increase efficiency of the performance drastically. Image analysis, filtering, enhancement, object detection, edge detection methods, path and localization methods and fine tuning of parameters are being discussed in depth to use while developing such products. Dynamic system design is being deliberated by using which results in reduction of human interaction, environment sustainability and better quality of work in detail. Embedded systems involving the micro-controllers, processors, communicating devices, sensors and actuators, soft-ware to use them; is being explained for end-to-end development and enhancement of accuracy and precision in Spray Painting Robots. △ Less","2 October, 2020",https://arxiv.org/pdf/2010.01131
"Artificial Creations: Ascription, Ownership, Time-Specific Monopolies",Raj Shekhar,"Creativity has always been synonymous with humans. No other living species could boast of creativity as humans could. Even the smartest computers thrived only on the ingenious imaginations of its coders. However, that is steadily changing with highly advanced artificially intelligent systems that demonstrate incredible capabilities to autonomously (i.e., with minimal or no human input) produce creative products that would ordinarily deserve intellectual property status if created by a human. These systems could be called artificial creators and their creative products artificial creations. The use of artificial creators is likely to become a part of mainstream production practices in the creative and innovation industries sooner than we realize. When they do, intellectual property regimes (that are inherently designed to reward human creativity) must be sufficiently prepared to aptly respond to the phenomenon of what could be called artificial creativity. Needless to say, any such response must be guided by considerations of public welfare. This study analyzes what that response ought to look like by revisiting the determinants of intellectual property and critiquing its nature and modes. This understanding of intellectual property is then applied to investigate the determinants of intellectual property in artificial creations so as to determine the intrinsic justifications for intellectual property rewards for artificial creativity, and accordingly, develop general modalities for granting intellectual property status to artificial creations. Finally, the treatment of artificial works (i.e., copyrightable artificial creations) and artificial inventions (i.e., patentable artificial creations) by current intellectual property regimes is critiqued, and specific modalities for granting intellectual property status to artificial works and artificial inventions are developed. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00543
A survey on natural language processing (nlp) and applications in insurance,Antoine Ly;Benno Uthayasooriyar;Tingting Wang,"Text is the most widely used means of communication today. This data is abundant but nevertheless complex to exploit within algorithms. For years, scientists have been trying to implement different techniques that enable computers to replicate some mechanisms of human reading. During the past five years, research disrupted the capacity of the algorithms to unleash the value of text data. It brings today, many opportunities for the insurance industry.Understanding those methods and, above all, knowing how to apply them is a major challenge and key to unleash the value of text data that have been stored for many years. Processing language with computer brings many new opportunities especially in the insurance sector where reports are central in the information used by insurers. SCOR's Data Analytics team has been working on the implementation of innovative tools or products that enable the use of the latest research on text analysis. Understanding text mining techniques in insurance enhances the monitoring of the underwritten risks and many processes that finally benefit policyholders.This article proposes to explain opportunities that Natural Language Processing (NLP) are providing to insurance. It details different methods used today in practice traces back the story of them. We also illustrate the implementation of certain methods using open source libraries and python codes that we have developed to facilitate the use of these techniques.After giving a general overview on the evolution of text mining during the past few years,we share about how to conduct a full study with text mining and share some examples to serve those models into insurance products or services. Finally, we explained in more details every step that composes a Natural Language Processing study to ensure the reader can have a deep understanding on the implementation. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00462
Mediating Artificial Intelligence Developments through Negative and Positive Incentives,The Anh Han;Luis Moniz Pereira;Tom Lenaerts;Francisco C. Santos,"The field of Artificial Intelligence (AI) is going through a period of great expectations, introducing a certain level of anxiety in research, business and also policy. This anxiety is further energised by an AI race narrative that makes people believe they might be missing out. Whether real or not, a belief in this narrative may be detrimental as some stake-holders will feel obliged to cut corners on safety precautions, or ignore societal consequences just to ""win"". Starting from a baseline model that describes a broad class of technology races where winners draw a significant benefit compared to others (such as AI advances, patent race, pharmaceutical technologies), we investigate here how positive (rewards) and negative (punishments) incentives may beneficially influence the outcomes. We uncover conditions in which punishment is either capable of reducing the development speed of unsafe participants or has the capacity to reduce innovation through over-regulation. Alternatively, we show that, in several scenarios, rewarding those that follow safety measures may increase the development speed while ensuring safe choices. Moreover, in {the latter} regimes, rewards do not suffer from the issue of over-regulation as is the case for punishment. Overall, our findings provide valuable insights into the nature and kinds of regulatory actions most suitable to improve safety compliance in the contexts of both smooth and sudden technological shifts. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00403
Bringing Network Coding into SDN: A Case-study for Highly Meshed Heterogeneous Communications,Alejandro Cohen;Homa Esfahanizadeh;Bruno Sousa;João P. Vilela;Miguel Luís;Duarte Raposo;Francois Michel;Susana Sargento;Muriel Médard,"Modern communications have moved away from point-to-point models to increasingly heterogeneous network models. In this article, we propose a novel controller-based protocol to deploy adaptive causal network coding in heterogeneous and highly-meshed communication networks. Specifically, we consider using Software-Defined-Network (SDN) as the main controller. We first present an architecture for the highly-meshed heterogeneous multi-source multi-destination networks that represents the practical communication networks encountered in the fifth generation of wireless networks (5G) and beyond. Next, we present a promising solution to deploy network coding over the new architecture. In fact, we investigate how to generalize adaptive and causal random linear network coding (AC-RLNC), proposed for multipath multi-hop (MP-MH) communication channels, to a protocol for the new multi-source multi-destination network architecture using controller. To this end, we present a modularized implementation of AC-RLNC solution where the modules work together in a distributed fashion and perform the AC-RLNC technology. We also present a new controller-based setting through which the network coding modules can communicate and can attain their required information. Finally, we briefly discuss how the proposed architecture and network coding solution provide a good opportunity for future technologies, e.g., distributed coded computation and storage, mmWave communication environments, and innovative and efficient security features. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00343
Legal Judgment Prediction (LJP) Amid the Advent of Autonomous AI Legal Reasoning,Lance Eliot,"Legal Judgment Prediction (LJP) is a longstanding and open topic in the theory and practice-of-law. Predicting the nature and outcomes of judicial matters is abundantly warranted, keenly sought, and vigorously pursued by those within the legal industry and also by society as a whole. The tenuous act of generating judicially laden predictions has been limited in utility and exactitude, requiring further advancement. Various methods and techniques to predict legal cases and judicial actions have emerged over time, especially arising via the advent of computer-based modeling. There has been a wide range of approaches attempted, including simple calculative methods to highly sophisticated and complex statistical models. Artificial Intelligence (AI) based approaches have also been increasingly utilized. In this paper, a review of the literature encompassing Legal Judgment Prediction is undertaken, along with innovatively proposing that the advent of AI Legal Reasoning (AILR) will have a pronounced impact on how LJP is performed and its predictive accuracy. Legal Judgment Prediction is particularly examined using the Levels of Autonomy (LoA) of AI Legal Reasoning, plus, other considerations are explored including LJP probabilistic tendencies, biases handling, actor predictors, transparency, judicial reliance, legal case outcomes, and other crucial elements entailing the overarching legal judicial milieu. △ Less","28 September, 2020",https://arxiv.org/pdf/2009.14620
Towards Adaptive Semantic Segmentation by Progressive Feature Refinement,Bin Zhang;Shengjie Zhao;Rongqing Zhang,"As one of the fundamental tasks in computer vision, semantic segmentation plays an important role in real world applications. Although numerous deep learning models have made notable progress on several mainstream datasets with the rapid development of convolutional networks, they still encounter various challenges in practical scenarios. Unsupervised adaptive semantic segmentation aims to obtain a robust classifier trained with source domain data, which is able to maintain stable performance when deployed to a target domain with different data distribution. In this paper, we propose an innovative progressive feature refinement framework, along with domain adversarial learning to boost the transferability of segmentation networks. Specifically, we firstly align the multi-stage intermediate feature maps of source and target domain images, and then a domain classifier is adopted to discriminate the segmentation output. As a result, the segmentation models trained with source domain images can be transferred to a target domain without significant performance degradation. Experimental results verify the efficiency of our proposed method compared with state-of-the-art methods. △ Less","30 September, 2020",https://arxiv.org/pdf/2009.14420
The Development of Visualization Psychology Analysis Tools to Account for Trust,Rita Borgo;Darren J Edwards,"Defining trust is an important endeavor given its applicability to assessing public mood to much of the innovation in the newly formed autonomous industry, such as artificial intelligence (AI),medical bots, drones, autonomous vehicles, and smart factories [19].Through developing a reliable index or means to measure trust,this may have wide impact from fostering acceptance and adoption of smart systems to informing policy makers about the public atmosphere and willingness to adopt innovate change, and has been identified as an important indicator in a recent UK policy brief [8].In this paper, we reflect on the importance and potential impact of developing Visualization Psychology in the context of solving definitions and policy decision making problems for complex constructs such as ""trust"". △ Less","28 September, 2020",https://arxiv.org/pdf/2009.13200
The value chain of Industrial IoT and its reference framework for digitalization,Hang Song;Yuncheng Jiang,"Nowadays, we are rapidly moving beyond bespoke detailed solutions tailored for very specific problems, and we already build upon reusable and more general purpose infrastructures and tools, referring to them as IoT, Industrial IoT/Industry 4.0[1-3], etc. These are what will be discussed in this paper. When Industrial IoT (IIoT) is concerned about, the enormous innovation potential of IoT technologies are not only in the production of physical devices, but also in all activities performed by manufacturing industries, both in the pre-production (ideation, design, prototyping) and in the post-production (sales, training, maintenance, recycling) phases . It is also known that IIoT acquire and analyze data from connected devices, Cyber-Physical Systems (CPS), locations and people (e.g. operator); along with its contemporary new terms, such as 5G, Edge computing, and other ICT technologies with their applications[4] . More or less it is drawn upon on its combination with relative monitoring devices and actuators from operational technology (OT). IIoT helps regulate and monitor industrial systems [2], and it integrates/re-organize production resources flexibly, enhanced OT capability in the smart value chains enabling distributed decision-making of production. △ Less","27 September, 2020",https://arxiv.org/pdf/2009.13039
Sensor Fault Detection and Isolation via Networked Estimation: Full-Rank Dynamical Systems,Mohammadreza Doostmohammadian;Nader Meskin,"This paper considers the problem of simultaneous sensor fault detection, isolation, and networked estimation of linear full-rank dynamical systems. The proposed networked estimation is a variant of single time-scale protocol and is based on (i) consensus on \textit{a-priori} estimates and (ii) measurement innovation. The necessary connectivity condition on the sensor network and stabilizing block-diagonal gain matrix is derived based on our previous works. Considering additive faults in the presence of system and measurement noise, the estimation error at sensors is derived and proper residuals are defined for fault detection. Unlike many works in the literature, no simplifying upper-bound condition on the noise is considered and we assume Gaussian system/measurement noise. A probabilistic threshold is then defined for fault detection based on the estimation error covariance norm. Finally, a graph-theoretic sensor replacement scenario is proposed to recover possible loss of networked observability due to removing the faulty sensor. We examine the proposed fault detection and isolation scheme on an illustrative academic example to verify the results and make a comparison study with related literature. △ Less","25 September, 2020",https://arxiv.org/pdf/2009.12084
The Next Era of American Law Amid the Advent of Autonomous AI Legal Reasoning,Lance Eliot,"Legal scholars have postulated that there have been three eras of American law to-date, consisting in chronological order of the initial Age of Discovery, the Age of Faith, and then the Age of Anxiety. An open question that has received erudite attention in legal studies is what the next era, the fourth era, might consist of, and for which various proposals exist including examples such as the Age of Consent, the Age of Information, etc. There is no consensus in the literature as yet on what the fourth era is, and nor whether the fourth era has already begun or will instead emerge in the future. This paper examines the potential era-elucidating impacts amid the advent of autonomous Artificial Intelligence Legal Reasoning (AILR), entailing whether such AILR will be an element of a fourth era or a driver of a fourth, fifth, or perhaps the sixth era of American law. Also, a set of meta-characteristics about the means of identifying a legal era changeover are introduced, along with an innovative discussion of the role entailing legal formalism versus legal realism in the emergence of the American law eras. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.11647
Qlib: An AI-oriented Quantitative Investment Platform,Xiao Yang;Weiqing Liu;Dong Zhou;Jiang Bian;Tie-Yan Liu,"Quantitative investment aims to maximize the return and minimize the risk in a sequential trading period over a set of financial instruments. Recently, inspired by rapid development and great potential of AI technologies in generating remarkable innovation in quantitative investment, there has been increasing adoption of AI-driven workflow for quantitative research and practical investment. In the meantime of enriching the quantitative investment methodology, AI technologies have raised new challenges to the quantitative investment system. Particularly, the new learning paradigms for quantitative investment call for an infrastructure upgrade to accommodate the renovated workflow; moreover, the data-driven nature of AI technologies indeed indicates a requirement of the infrastructure with more powerful performance; additionally, there exist some unique challenges for applying AI technologies to solve different tasks in the financial scenarios. To address these challenges and bridge the gap between AI technologies and quantitative investment, we design and develop Qlib that aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment. △ Less","22 September, 2020",https://arxiv.org/pdf/2009.11189
AI and Legal Argumentation: Aligning the Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Legal argumentation is a vital cornerstone of justice, underpinning an adversarial form of law, and extensive research has attempted to augment or undertake legal argumentation via the use of computer-based automation including Artificial Intelligence (AI). AI advances in Natural Language Processing (NLP) and Machine Learning (ML) have especially furthered the capabilities of leveraging AI for aiding legal professionals, doing so in ways that are modeled here as CARE, namely Crafting, Assessing, Refining, and Engaging in legal argumentation. In addition to AI-enabled legal argumentation serving to augment human-based lawyering, an aspirational goal of this multi-disciplinary field consists of ultimately achieving autonomously effected human-equivalent legal argumentation. As such, an innovative meta-approach is proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to the maturation of AI and Legal Argumentation (AILA), proffering a new means of gauging progress in this ever-evolving and rigorously sought domain. △ Less","11 September, 2020",https://arxiv.org/pdf/2009.11180
Online AUC Optimization for Sparse High-Dimensional Datasets,Baojian Zhou;Yiming Ying;Steven Skiena,"The Area Under the ROC Curve (AUC) is a widely used performance measure for imbalanced classification arising from many application domains where high-dimensional sparse data is abundant. In such cases, each d dimensional sample has only k non-zero features with k \ll d, and data arrives sequentially in a streaming form. Current online AUC optimization algorithms have high per-iteration cost \mathcal{O}(d) and usually produce non-sparse solutions in general, and hence are not suitable for handling the data challenge mentioned above. In this paper, we aim to directly optimize the AUC score for high-dimensional sparse datasets under online learning setting and propose a new algorithm, \textsc{FTRL-AUC}. Our proposed algorithm can process data in an online fashion with a much cheaper per-iteration cost \mathcal{O}(k), making it amenable for high-dimensional sparse streaming data analysis. Our new algorithmic design critically depends on a novel reformulation of the U-statistics AUC objective function as the empirical saddle point reformulation, and the innovative introduction of the ""lazy update"" rule so that the per-iteration complexity is dramatically reduced from \mathcal{O}(d) to \mathcal{O}(k). Furthermore, \textsc{FTRL-AUC} can inherently capture sparsity more effectively by applying a generalized Follow-The-Regularized-Leader (FTRL) framework. Experiments on real-world datasets demonstrate that \textsc{FTRL-AUC} significantly improves both run time and model sparsity while achieving competitive AUC scores compared with the state-of-the-art methods. Comparison with the online learning method for logistic loss demonstrates that \textsc{FTRL-AUC} achieves higher AUC scores especially when datasets are imbalanced. △ Less","22 September, 2020",https://arxiv.org/pdf/2009.10867
Using Machine Learning to Develop a Novel COVID-19 Vulnerability Index (C19VI),Anuj Tiwari;Arya V. Dadhania;Vijay Avin Balaji Ragunathrao;Edson R. A. Oliveira,"COVID19 is now one of the most leading causes of death in the United States. Systemic health, social and economic disparities have put the minorities and economically poor communities at a higher risk than others. There is an immediate requirement to develop a reliable measure of county-level vulnerabilities that can capture the heterogeneity of both vulnerable communities and the COVID19 pandemic. This study reports a COVID19 Vulnerability Index (C19VI) for identification and mapping of vulnerable counties in the United States. We proposed a Random Forest machine learning based COVID19 vulnerability model using CDC sociodemographic and COVID19-specific themes. An innovative COVID19 Impact Assessment algorithm was also developed using homogeneity and trend assessment technique for evaluating severity of the pandemic in all counties and train RF model. Developed C19VI was statistically validated and compared with the CDC COVID19 Community Vulnerability Index (CCVI). Finally, using C19VI along with census data, we explored racial inequalities and economic disparities in COVID19 health outcomes amongst different regions in the United States. Our C19VI index indicates that 18.30% of the counties falls into very high vulnerability class, 24.34% in high, 23.32% in moderate, 22.34% in low, and 11.68% in very low. Furthermore, C19VI reveals that 75.57% of racial minorities and 82.84% of economically poor communities are very high or high COVID19 vulnerable regions. The proposed approach of vulnerability modeling takes advantage of both the well-established field of statistical analysis and the fast-evolving domain of machine learning. C19VI provides an accurate and more reliable way to measure county level vulnerability in the United States. This index aims at helping emergency planners to develop more effective mitigation strategies especially for the disproportionately impacted communities. △ Less","22 September, 2020",https://arxiv.org/pdf/2009.10808
A word recurrence based algorithm to extract genomic dictionaries,Vincenzo Bonnici;Giuditta Franco;Vincenzo Manca,"Genomes may be analyzed from an information viewpoint as very long strings, containing functional elements of variable length, which have been assembled by evolution. In this work an innovative information theory based algorithm is proposed, to extract significant (relatively small) dictionaries of genomic words. Namely, conceptual analyses are here combined with empirical studies, to open up a methodology for the extraction of variable length dictionaries from genomic sequences, based on the information content of some factors. Its application to human chromosomes highlights an original inter-chromosomal similarity in terms of factor distributions. △ Less","22 September, 2020",https://arxiv.org/pdf/2009.10449
Preserving Integrity in Online Social Networks,Alon Halevy;Cristian Canton Ferrer;Hao Ma;Umut Ozertem;Patrick Pantel;Marzieh Saeidi;Fabrizio Silvestri;Ves Stoyanov,"Online social networks provide a platform for sharing information and free expression. However, these networks are also used for malicious purposes, such as distributing misinformation and hate speech, selling illegal drugs, and coordinating sex trafficking or child exploitation. This paper surveys the state of the art in keeping online platforms and their users safe from such harm, also known as the problem of preserving integrity. This survey comes from the perspective of having to combat a broad spectrum of integrity violations at Facebook. We highlight the techniques that have been proven useful in practice and that deserve additional attention from the academic community. Instead of discussing the many individual violation types, we identify key aspects of the social-media eco-system, each of which is common to a wide variety violation types. Furthermore, each of these components represents an area for research and development, and the innovations that are found can be applied widely. △ Less","25 September, 2020",https://arxiv.org/pdf/2009.10311
Constraint Programming Algorithms for Route Planning Exploiting Geometrical Information,Alessandro Bertagnon,"Problems affecting the transport of people or goods are plentiful in industry and commerce and they also appear to be at the origin of much more complex problems. In recent years, the logistics and transport sector keeps growing supported by technological progress, i.e. companies to be competitive are resorting to innovative technologies aimed at efficiency and effectiveness. This is why companies are increasingly using technologies such as Artificial Intelligence (AI), Blockchain and Internet of Things (IoT). Artificial intelligence, in particular, is often used to solve optimization problems in order to provide users with the most efficient ways to exploit available resources. In this work we present an overview of our current research activities concerning the development of new algorithms, based on CLP techniques, for route planning problems exploiting the geometric information intrinsically present in many of them or in some of their variants. The research so far has focused in particular on the Euclidean Traveling Salesperson Problem (Euclidean TSP) with the aim to exploit the results obtained also to other problems of the same category, such as the Euclidean Vehicle Routing Problem (Euclidean VRP), in the future. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.10253
Super-teams or fair leagues? Parity policies by powerful regulators don't prevent capture,Adam Sawyer;Seth Frey,"Much of modern society is founded on orchestrating institutions that produce social goods by fostering motivated teams, pitting them against each other, and distributing the fruits of the arms races that ensue. However, even when the ""market maker"" is willing and able to maintain parity between teams, it may fail to maintain a level playing field, as some teams acquire enough advantage within the system to gain influence over it and institutionalize their advantage. Using outcomes of over 60,000 games from four professional basketball leagues and more than 100 years' worth of seasons, we compute the evolving rate of transitivity violations (A>B, B>C, but C>A) to measure the ability of leagues to maintain parity between teams, and support the efficient generation and distribution of innovation. Comparing against a baseline of randomly permuted outcomes, we find that basketball has become less competitive over time, suggesting that teams diverge in performance, and reflecting a possible failure of market makers to tame their overpowered teams. Our results suggest that rich-get-richer dynamics are so pernicious that they can even emerge under the watch of a powerful administrator that is motivated to prevent them. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.09990
Detailed Review of Cloud based Mobile application for the stroke patient,Balagopal Ramdurai,"In the current years, due to the significant developments in technologies in almost every domain, the standard of living has been improved. Emergence of latest innovations, advanced machinery and equipment especially in the healthcare domain, have simplified the diagonalizing process to a wide extent.","17 September, 2020",https://arxiv.org/pdf/2009.09837
Industrial Topics in Urban Labor System,Jaehyuk Park;Morgan R. Frank;Lijun Sun;Hyejin Youn,"Categorization is an essential component for us to understand the world for ourselves and to communicate it collectively. It is therefore important to recognize that classification system are not necessarily static, especially for economic systems, and even more so in urban areas where most innovation takes place and is implemented. Out-of-date classification systems would potentially limit further understanding of the current economy because things constantly change. Here, we develop an occupation-based classification system for the US labor economy, called industrial topics, that satisfy adaptability and representability. By leveraging the distributions of occupations across the US urban areas, we identify industrial topics - clusters of occupations based on their co-existence pattern. Industrial topics indicate the mechanisms under the systematic allocation of different occupations. Considering the densely connected occupations as an industrial topic, our approach characterizes regional economies by their topical composition. Unlike the existing survey-based top-down approach, our method provides timely information about the underlying structure of the regional economy, which is critical for policymakers and business leaders, especially in our fast-changing economy. △ Less","17 September, 2020",https://arxiv.org/pdf/2009.09799
MARS: Mixed Virtual and Real Wearable Sensors for Human Activity Recognition with Multi-Domain Deep Learning Model,Ling Pei;Songpengcheng Xia;Lei Chu;Fanyi Xiao;Qi Wu;Wenxian Yu;Robert Qiu,"Together with the rapid development of the Internet of Things (IoT), human activity recognition (HAR) using wearable Inertial Measurement Units (IMUs) becomes a promising technology for many research areas. Recently, deep learning-based methods pave a new way of understanding and performing analysis of the complex data in the HAR system. However, the performance of these methods is mostly based on the quality and quantity of the collected data. In this paper, we innovatively propose to build a large database based on virtual IMUs and then address technical issues by introducing a multiple-domain deep learning framework consisting of three technical parts. In the first part, we propose to learn the single-frame human activity from the noisy IMU data with hybrid convolutional neural networks (CNNs) in the semi-supervised form. For the second part, the extracted data features are fused according to the principle of uncertainty-aware consistency, which reduces the uncertainty by weighting the importance of the features. The transfer learning is performed in the last part based on the newly released Archive of Motion Capture as Surface Shapes (AMASS) dataset, containing abundant synthetic human poses, which enhances the variety and diversity of the training dataset and is beneficial for the process of training and feature transfer in the proposed method. The efficiency and effectiveness of the proposed method have been demonstrated in the real deep inertial poser (DIP) dataset. The experimental results show that the proposed methods can surprisingly converge within a few iterations and outperform all competing methods. △ Less","9 October, 2020",https://arxiv.org/pdf/2009.09404
Predicting Geographic Information with Neural Cellular Automata,Mingxiang Chen;Qichang Chen;Lei Gao;Yilin Chen;Zhecheng Wang,"This paper presents a novel framework using neural cellular automata (NCA) to regenerate and predict geographic information. The model extends the idea of using NCA to generate/regenerate a specific image by training the model with various geographic data, and thus, taking the traffic condition map as an example, the model is able to predict traffic conditions by giving certain induction information. Our research verified the analogy between NCA and gene in biology, while the innovation of the model significantly widens the boundary of possible applications based on NCAs. From our experimental results, the model shows great potentials in its usability and versatility which are not available in previous studies. The code for model implementation is available at https://redacted. △ Less","19 September, 2020",https://arxiv.org/pdf/2009.09347
High-Resolution Augmentation for Automatic Template-Based Matching of Human Models,Riccardo Marin;Simone Melzi;Emanuele Rodolà;Umberto Castellani,"We propose a new approach for 3D shape matching of deformable human shapes. Our approach is based on the joint adoption of three different tools: an intrinsic spectral matching pipeline, a morphable model, and an extrinsic details refinement. By operating in conjunction, these tools allow us to greatly improve the quality of the matching while at the same time resolving the key issues exhibited by each tool individually. In this paper we present an innovative High-Resolution Augmentation (HRA) strategy that enables highly accurate correspondence even in the presence of significant mesh resolution mismatch between the input shapes. This augmentation provides an effective workaround for the resolution limitations imposed by the adopted morphable model. The HRA in its global and localized versions represents a novel refinement strategy for surface subdivision methods. We demonstrate the accuracy of the proposed pipeline on multiple challenging benchmarks, and showcase its effectiveness in surface registration and texture transfer. △ Less","19 September, 2020",https://arxiv.org/pdf/2009.09312
Co-Evolution of Multi-Robot Controllers and Task Cues for Off-World Open Pit Mining,Jekan Thangavelautham;Yinan Xu,"Robots are ideal for open-pit mining on the Moon as its a dull, dirty, and dangerous task. The challenge is to scale up productivity with an ever-increasing number of robots. This paper presents a novel method for developing scalable controllers for use in multi-robot excavation and site-preparation scenarios. The controller starts with a blank slate and does not require human-authored operations scripts nor detailed modeling of the kinematics and dynamics of the excavator. The 'Artificial Neural Tissue' (ANT) architecture is used as a control system for autonomous robot teams to perform resource gathering. This control architecture combines a variable-topology neural-network structure with a coarse-coding strategy that permits specialized areas to develop in the tissue. Our work in this field shows that fleets of autonomous decentralized robots have an optimal operating density. Too few robots result in insufficient labor, while too many robots cause antagonism, where the robots undo each other's work and are stuck in gridlock. In this paper, we explore the use of templates and task cues to improve group performance further and minimize antagonism. Our results show light beacons and task cues are effective in sparking new and innovative solutions at improving robot performance when placed under stressful situations such as severe time-constraint. △ Less","18 September, 2020",https://arxiv.org/pdf/2009.09149
"Building power consumption datasets: Survey, taxonomy and future directions",Yassine Himeur;Abdullah Alsalemi;Faycal Bensaali;Abbes Amira,"In the last decade, extended efforts have been poured into energy efficiency. Several energy consumption datasets were henceforth published, with each dataset varying in properties, uses and limitations. For instance, building energy consumption patterns are sourced from several sources, including ambient conditions, user occupancy, weather conditions and consumer preferences. Thus, a proper understanding of the available datasets will result in a strong basis for improving energy efficiency. Starting from the necessity of a comprehensive review of existing databases, this work is proposed to survey, study and visualize the numerical and methodological nature of building energy consumption datasets. A total of thirty-one databases are examined and compared in terms of several features, such as the geographical location, period of collection, number of monitored households, sampling rate of collected data, number of sub-metered appliances, extracted features and release date. Furthermore, data collection platforms and related modules for data transmission, data storage and privacy concerns used in different datasets are also analyzed and compared. Based on the analytical study, a novel dataset has been presented, namely Qatar university dataset, which is an annotated power consumption anomaly detection dataset. The latter will be very useful for testing and training anomaly detection algorithms, and hence reducing wasted energy. Moving forward, a set of recommendations is derived to improve datasets collection, such as the adoption of multi-modal data collection, smart Internet of things data collection, low-cost hardware platforms and privacy and security mechanisms. In addition, future directions to improve datasets exploitation and utilization are identified, including the use of novel machine learning solutions, innovative visualization tools and explainable recommender systems. △ Less","17 September, 2020",https://arxiv.org/pdf/2009.08192
SLGAN: Style- and Latent-guided Generative Adversarial Network for Desirable Makeup Transfer and Removal,Daichi Horita;Kiyoharu Aizawa,"There are five features to consider when using generative adversarial networks to apply makeup to photos of the human face. These features include (1) facial components, (2) interactive color adjustments, (3) makeup variations, (4) robustness to poses and expressions, and the (5) use of multiple reference images. Several related works have been proposed, mainly using generative adversarial networks (GAN). Unfortunately, none of them have addressed all five features simultaneously. This paper closes the gap with an innovative style- and latent-guided GAN (SLGAN). We provide a novel, perceptual makeup loss and a style-invariant decoder that can transfer makeup styles based on histogram matching to avoid the identity-shift problem. In our experiments, we show that our SLGAN is better than or comparable to state-of-the-art methods. Furthermore, we show that our proposal can interpolate facial makeup images to determine the unique features, compare existing methods, and help users find desirable makeup configurations. △ Less","24 September, 2020",https://arxiv.org/pdf/2009.07557
Geometric Uncertainty in Patient-Specific Cardiovascular Modeling with Convolutional Dropout Networks,Gabriel Maher;Casey Fleeter;Daniele Schiavazzi;Alison Marsden,"We propose a novel approach to generate samples from the conditional distribution of patient-specific cardiovascular models given a clinically aquired image volume. A convolutional neural network architecture with dropout layers is first trained for vessel lumen segmentation using a regression approach, to enable Bayesian estimation of vessel lumen surfaces. This network is then integrated into a path-planning patient-specific modeling pipeline to generate families of cardiovascular models. We demonstrate our approach by quantifying the effect of geometric uncertainty on the hemodynamics for three patient-specific anatomies, an aorto-iliac bifurcation, an abdominal aortic aneurysm and a sub-model of the left coronary arteries. A key innovation introduced in the proposed approach is the ability to learn geometric uncertainty directly from training data. The results show how geometric uncertainty produces coefficients of variation comparable to or larger than other sources of uncertainty for wall shear stress and velocity magnitude, but has limited impact on pressure. Specifically, this is true for anatomies characterized by small vessel sizes, and for local vessel lesions seen infrequently during network training. △ Less","15 September, 2020",https://arxiv.org/pdf/2009.07395
Report prepared by the Montreal AI Ethics Institute (MAIEI) on Publication Norms for Responsible AI,Abhishek Gupta;Camylle Lanteigne;Victoria Heath,"The history of science and technology shows that seemingly innocuous developments in scientific theories and research have enabled real-world applications with significant negative consequences for humanity. In order to ensure that the science and technology of AI is developed in a humane manner, we must develop research publication norms that are informed by our growing understanding of AI's potential threats and use cases. Unfortunately, it's difficult to create a set of publication norms for responsible AI because the field of AI is currently fragmented in terms of how this technology is researched, developed, funded, etc. To examine this challenge and find solutions, the Montreal AI Ethics Institute (MAIEI) co-hosted two public consultations with the Partnership on AI in May 2020. These meetups examined potential publication norms for responsible AI, with the goal of creating a clear set of recommendations and ways forward for publishers. In its submission, MAIEI provides six initial recommendations, these include: 1) create tools to navigate publication decisions, 2) offer a page number extension, 3) develop a network of peers, 4) require broad impact statements, 5) require the publication of expected results, and 6) revamp the peer-review process. After considering potential concerns regarding these recommendations, including constraining innovation and creating a ""black market"" for AI research, MAIEI outlines three ways forward for publishers, these include: 1) state clearly and consistently the need for established norms, 2) coordinate and build trust as a community, and 3) change the approach. △ Less","4 October, 2020",https://arxiv.org/pdf/2009.07262
Detecting and adapting to crisis pattern with context based Deep Reinforcement Learning,Eric Benhamou;David Saltiel;Jean-Jacques Ohana;Jamal Atif,"Deep reinforcement learning (DRL) has reached super human levels in complex tasks like game solving (Go and autonomous driving). However, it remains an open question whether DRL can reach human level in applications to financial problems and in particular in detecting pattern crisis and consequently dis-investing. In this paper, we present an innovative DRL framework consisting in two sub-networks fed respectively with portfolio strategies past performances and standard deviations as well as additional contextual features. The second sub network plays an important role as it captures dependencies with common financial indicators features like risk aversion, economic surprise index and correlations between assets that allows taking into account context based information. We compare different network architectures either using layers of convolutions to reduce network's complexity or LSTM block to capture time dependency and whether previous allocations is important in the modeling. We also use adversarial training to make the final model more robust. Results on test set show this approach substantially over-performs traditional portfolio optimization methods like Markowitz and is able to detect and anticipate crisis like the current Covid one. △ Less","9 November, 2020",https://arxiv.org/pdf/2009.07200
Machine learning predicts early onset of fever from continuous physiological data of critically ill patients,Aditya Singh;Akram Mohammed;Lokesh Chinthala;Rishikesan Kamaleswaran,"Fever can provide valuable information for diagnosis and prognosis of various diseases such as pneumonia, dengue, sepsis, etc., therefore, predicting fever early can help in the effectiveness of treatment options and expediting the treatment process. This study aims to develop novel algorithms that can accurately predict fever onset in critically ill patients by applying machine learning technique on continuous physiological data. We analyzed continuous physiological data collected every 5-minute from a cohort of over 200,000 critically ill patients admitted to an Intensive Care Unit (ICU) over a 2-year period. Each episode of fever from the same patient were considered as an independent event, with separations of at least 24 hours. We extracted descriptive statistical features from six physiological data streams, including heart rate, respiration, systolic and diastolic blood pressure, mean arterial pressure, and oxygen saturation, and use these features to independently predict the onset of fever. Using a bootstrap aggregation method, we created a balanced dataset of 7,801 afebrile and febrile patients and analyzed features up to 4 hours before the fever onset. We found that supervised machine learning methods can predict fever up to 4 hours before onset in critically ill patients with high recall, precision, and F1-score. This study demonstrates the viability of using machine learning to predict fever among hospitalized adults. The discovery of salient physiomarkers through machine learning and deep learning techniques has the potential to further accelerate the development and implementation of innovative care delivery protocols and strategies for medically vulnerable patients. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.07103
Same data may bring conflict results: a caution to use the disruptive index,Guoqiang Liang;Yi Jiang;Haiyan Hou,"In the last two decades, scholars have designed various types of bibliographic related indicators to identify breakthrough-class academic achievements. In this study, we take a further step to look at properties of the promising disruptive index, thus deepening our understanding of this index and further facilitating its wise use in bibliometrics. Using publication records for Nobel laureates between 1900 and 2016, we calculate the DI of Nobel Prize-winning articles and its benchmark articles in each year and use the median DI to denote the central tendency in each year, and compare results between Medicine, Chemistry, and Physics. We find that conclusions based on DI depend on the length of their citation time window, and different citation time windows may cause different, even controversial, results. Also, discipline and time play a role on the length of citation window when using DI to measure the innovativeness of a scientific work. Finally, not all articles with DI equals to 1 were the breakthrough-class achievements. In other words, the DI stands up theoretically, but we should not neglect that the DI was only shaped by the number of citing articles and times the references have been cited, these data may vary from database to database. △ Less","15 September, 2020",https://arxiv.org/pdf/2009.06888
A Design Space of Vision Science Methods for Visualization Research,Madison Elliott;Christine Nothelfer;Cindy Xiong;Danielle Szafir,"A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.06855
Synbols: Probing Learning Algorithms with Synthetic Datasets,Alexandre Lacoste;Pau Rodríguez;Frédéric Branchaud-Charron;Parmida Atighehchian;Massimo Caccia;Issam Laradji;Alexandre Drouin;Matt Craddock;Laurent Charlin;David Vázquez,"Progress in the field of machine learning has been fueled by the introduction of benchmark datasets pushing the limits of existing algorithms. Enabling the design of datasets to test specific properties and failure modes of learning algorithms is thus a problem of high interest, as it has a direct impact on innovation in the field. In this sense, we introduce Synbols -- Synthetic Symbols -- a tool for rapidly generating new datasets with a rich composition of latent features rendered in low resolution images. Synbols leverages the large amount of symbols available in the Unicode standard and the wide range of artistic font provided by the open font community. Our tool's high-level interface provides a language for rapidly generating new distributions on the latent features, including various types of textures and occlusions. To showcase the versatility of Synbols, we use it to dissect the limitations and flaws in standard learning algorithms in various learning setups including supervised learning, active learning, out of distribution generalization, unsupervised representation learning, and object counting. △ Less","4 November, 2020",https://arxiv.org/pdf/2009.06415
Supervised learning for the prediction of firm dynamics,Falco J. Bargagli-Stoffi;Jan Niederreiter;Massimo Riccaboni,"Thanks to the increasing availability of granular, yet high-dimensional, firm level data, machine learning (ML) algorithms have been successfully applied to address multiple research questions related to firm dynamics. Especially supervised learning (SL), the branch of ML dealing with the prediction of labelled outcomes, has been used to better predict firms' performance. In this contribution, we will illustrate a series of SL approaches to be used for prediction tasks, relevant at different stages of the company life cycle. The stages we will focus on are (i) startup and innovation, (ii) growth and performance of companies, and (iii) firms exit from the market. First, we review SL implementations to predict successful startups and R&D projects. Next, we describe how SL tools can be used to analyze company growth and performance. Finally, we review SL applications to better forecast financial distress and company failure. In the concluding Section, we extend the discussion of SL methods in the light of targeted policies, result interpretability, and causality. △ Less","11 September, 2020",https://arxiv.org/pdf/2009.06413
A Detail Study of Security and Privacy issues of Internet of Things,Mohan Krishna Kagita;Navod Thilakarathne;Dharmendra Singh Rajput;Dr Surekha Lanka,"The Internet of Things, or IoT, refers to the billions of physical objects around the planet that are now connected to the Internet, many of which store and exchange the data without human interaction. In recent years the Internet of Things (IoT) has incredibly become a groundbreaking technical innovation that has contributed to massive impact in the ways where all the information is handled incorporate companies, computer devices, and even kitchen equipment and appliances, are designed and made. The main focus of this chapter is to systematically review the security and privacy of the Internet of Things in the present world. Most internet users are genuine, yet others are cybercriminals with individual expectations of misusing information. With such possibilities, users should know the potential security and privacy issues of IoT devices. IoT innovations are applied on numerous levels in a system that we use daily in our day-to-day life. Data confidentiality is a significant issue. The interconnection of various networks makes it impossible for users to assert extensive control of their data. Finally, this chapter discusses the IoT Security concerns in the literature and providing a critical review of the current approach and proposed solutions on present issues on the Privacy protection of IoT devices. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.06341
Spoiled for Choice? Personalized Recommendation for Healthcare Decisions: A Multi-Armed Bandit Approach,Tongxin Zhou;Yingfei Wang;Lu;Yan;Yong Tan,"Online healthcare communities provide users with various healthcare interventions to promote healthy behavior and improve adherence. When faced with too many intervention choices, however, individuals may find it difficult to decide which option to take, especially when they lack the experience or knowledge to evaluate different options. The choice overload issue may negatively affect users' engagement in health management. In this study, we take a design-science perspective to propose a recommendation framework that helps users to select healthcare interventions. Taking into account that users' health behaviors can be highly dynamic and diverse, we propose a multi-armed bandit (MAB)-driven recommendation framework, which enables us to adaptively learn users' preference variations while promoting recommendation diversity in the meantime. To better adapt an MAB to the healthcare context, we synthesize two innovative model components based on prominent health theories. The first component is a deep-learning-based feature engineering procedure, which is designed to learn crucial recommendation contexts in regard to users' sequential health histories, health-management experiences, preferences, and intrinsic attributes of healthcare interventions. The second component is a diversity constraint, which structurally diversifies recommendations in different dimensions to provide users with well-rounded support. We apply our approach to an online weight management context and evaluate it rigorously through a series of experiments. Our results demonstrate that each of the design components is effective and that our recommendation design outperforms a wide range of state-of-the-art recommendation systems. Our study contributes to the research on the application of business intelligence and has implications for multiple stakeholders, including online healthcare platforms, policymakers, and users. △ Less","13 September, 2020",https://arxiv.org/pdf/2009.06108
Tax Knowledge Graph for a Smarter and More Personalized TurboTax,Jay Yu;Kevin McCluskey;Saikat Mukherjee,"Most knowledge graph use cases are data-centric, focusing on representing data entities and their semantic relationships. There are no published success stories to represent large-scale complicated business logic with knowledge graph technologies. In this paper, we will share our innovative and practical approach to representing complicated U.S. and Canadian income tax compliance logic (calculations and rules) via a large-scale knowledge graph. We will cover how the Tax Knowledge Graph is constructed and automated, how it is used to calculate tax refunds, reasoned to find missing info, and navigated to explain the calculated results. The Tax Knowledge Graph has helped transform Intuit's flagship TurboTax product into a smart and personalized experience, accelerating and automating the tax preparation process while instilling confidence for millions of customers. △ Less","13 September, 2020",https://arxiv.org/pdf/2009.06103
To Root Artificial Intelligence Deeply in Basic Science for a New Generation of AI,Jingan Yang;Yang Peng,"One of the ambitions of artificial intelligence is to root artificial intelligence deeply in basic science while developing brain-inspired artificial intelligence platforms that will promote new scientific discoveries. The challenges are essential to push artificial intelligence theory and applied technologies research forward. This paper presents the grand challenges of artificial intelligence research for the next 20 years which include:~(i) to explore the working mechanism of the human brain on the basis of understanding brain science, neuroscience, cognitive science, psychology and data science; (ii) how is the electrical signal transmitted by the human brain? What is the coordination mechanism between brain neural electrical signals and human activities? (iii)~to root brain-computer interface~(BCI) and brain-muscle interface~(BMI) technologies deeply in science on human behaviour; (iv)~making research on knowledge-driven visual commonsense reasoning~(VCR), develop a new inference engine for cognitive network recognition~(CNR); (v)~to develop high-precision, multi-modal intelligent perceptrons; (vi)~investigating intelligent reasoning and fast decision-making systems based on knowledge graph~(KG). We believe that the frontier theory innovation of AI, knowledge-driven modeling methodologies for commonsense reasoning, revolutionary innovation and breakthroughs of the novel algorithms and new technologies in AI, and developing responsible AI should be the main research strategies of AI scientists in the future. △ Less","11 September, 2020",https://arxiv.org/pdf/2009.05678
Bias Variance Tradeoff in Analysis of Online Controlled Experiments,Ali Mahmoudzadeh;Sophia Liu;Sol Sadeghi;Paul Luo Li;Somit Gupta,"Many organizations utilize large-scale online controlled experiments (OCEs) to accelerate innovation. Having high statistical power to detect small differences between control and treatment accurately is critical, as even small changes in key metrics can be worth millions of dollars or indicate user dissatisfaction for a very large number of users. For large-scale OCE, the duration is typically short (e.g. two weeks) to expedite changes and improvements to the product. In this paper, we examine two common approaches for analyzing usage data collected from users within the time window of an experiment, which can differ in accuracy and power. The open approach includes all relevant usage data from all active users for the entire duration of the experiment. The bounded approach includes data from a fixed period of observation for each user (e.g. seven days after exposure) after the first time a user became active in the experiment window. △ Less","10 September, 2020",https://arxiv.org/pdf/2009.05015
A Probabilistic Approach for Data Management in Pervasive Computing Applications,Kostas Kolomvatsos,"Current advances in Pervasive Computing (PC) involve the adoption of the huge infrastructures of the Internet of Things (IoT) and the Edge Computing (EC). Both, IoT and EC, can support innovative applications around end users to facilitate their activities. Such applications are built upon the collected data and the appropriate processing demanded in the form of requests. To limit the latency, instead of relying on Cloud for data storage and processing, the research community provides a number of models for data management at the EC. Requests, usually defined in the form of tasks or queries, demand the processing of specific data. A model for pre-processing the data preparing them and detecting their statistics before requests arrive is necessary. In this paper, we propose a promising and easy to implement scheme for selecting the appropriate host of the incoming data based on a probabilistic approach. Our aim is to store similar data in the same distributed datasets to have, beforehand, knowledge on their statistics while keeping their solidity at high levels. As solidity, we consider the limited statistical deviation of data, thus, we can support the storage of highly correlated data in the same dataset. Additionally, we propose an aggregation mechanism for outliers detection applied just after the arrival of data. Outliers are transferred to Cloud for further processing. When data are accepted to be locally stored, we propose a model for selecting the appropriate datasets where they will be replicated for building a fault tolerant system. We analytically describe our model and evaluate it through extensive simulations presenting its pros and cons. △ Less","10 September, 2020",https://arxiv.org/pdf/2009.04739
Improving Investment Suggestions for Peer-to-Peer (P2P) Lending via Integrating Credit Scoring into Profit Scoring,Yan Wang;Xuelei Sherry Ni,"In the peer-to-peer (P2P) lending market, lenders lend the money to the borrowers through a virtual platform and earn the possible profit generated by the interest rate. From the perspective of lenders, they want to maximize the profit while minimizing the risk. Therefore, many studies have used machine learning algorithms to help the lenders identify the ""best"" loans for making investments. The studies have mainly focused on two categories to guide the lenders' investments: one aims at minimizing the risk of investment (i.e., the credit scoring perspective) while the other aims at maximizing the profit (i.e., the profit scoring perspective). However, they have all focused on one category only and there is seldom research trying to integrate the two categories together. Motivated by this, we propose a two-stage framework that incorporates the credit information into a profit scoring modeling. We conducted the empirical experiment on a real-world P2P lending data from the US P2P market and used the Light Gradient Boosting Machine (lightGBM) algorithm in the two-stage framework. Results show that the proposed two-stage method could identify more profitable loans and thereby provide better investment guidance to the investors compared to the existing one-stage profit scoring alone approach. Therefore, the proposed framework serves as an innovative perspective for making investment decisions in P2P lending. △ Less","9 September, 2020",https://arxiv.org/pdf/2009.04536
Plant Diseases recognition on images using Convolutional Neural Networks: A Systematic Review,Andre S. Abade;Paulo Afonso Ferreira;Flavio de Barros Vidal,"Plant diseases are considered one of the main factors influencing food production and minimize losses in production, and it is essential that crop diseases have fast detection and recognition. The recent expansion of deep learning methods has found its application in plant disease detection, offering a robust tool with highly accurate results. In this context, this work presents a systematic review of the literature that aims to identify the state of the art of the use of convolutional neural networks(CNN) in the process of identification and classification of plant diseases, delimiting trends, and indicating gaps. In this sense, we present 121 papers selected in the last ten years with different approaches to treat aspects related to disease detection, characteristics of the data set, the crops and pathogens investigated. From the results of the systematic review, it is possible to understand the innovative trends regarding the use of CNNs in the identification of plant diseases and to identify the gaps that need the attention of the research community. △ Less","9 September, 2020",https://arxiv.org/pdf/2009.04365
A Review of Geospatial Content in IEEE Visualization Publications,Alexander Yoshizumi;Megan M. Coffer;Elyssa L. Collins;Mollie D. Gaines;Xiaojie Gao;Kate Jones;Ian R. McGregor;Katie A. McQuillan;Vinicius Perin;Laura M. Tomkins;Thom Worm;Laura Tateosian,"Geospatial analysis is crucial for addressing many of the world's most pressing challenges. Given this, there is immense value in improving and expanding the visualization techniques used to communicate geospatial data. In this work, we explore this important intersection -- between geospatial analytics and visualization -- by examining a set of recent IEEE VIS Conference papers (a selection from 2017-2019) to assess the inclusion of geospatial data and geospatial analyses within these papers. After removing the papers with no geospatial data, we organize the remaining literature into geospatial data domain categories and provide insight into how these categories relate to VIS Conference paper types. We also contextualize our results by investigating the use of geospatial terms in IEEE Visualization publications over the last 30 years. Our work provides an understanding of the quantity and role of geospatial subject matter in recent IEEE VIS publications and supplies a foundation for future meta-analytical work around geospatial analytics and geovisualization that may shed light on opportunities for innovation. △ Less","7 September, 2020",https://arxiv.org/pdf/2009.03390
Cyber-Human System for Remote Collaborators,Srikanth Jonnada;Ram Dantu;Ishan Ranasinghe;Logan Widick;Mark Thompson;Janice A. Hauge,"With the increasing ubiquity of technology in our daily lives, the complexity of our environment and the mechanisms required to function have also increased exponentially. Failure of any of the mechanical and digital devices that we rely on can be extremely disruptive. At times, the presence of an expert is needed to analyze, troubleshoot, and fix the problem. The increased demand and rapidly evolving mechanisms have led to an insufficient amount of skilled workers, thus resulting in long waiting times for consumers, and correspondingly high prices for expert services. We assert that performing a repair task with the guidance of experts from any geographical location provides an appropriate solution to the growing demand for handyman skills. This paper proposes an innovative mechanism for two geographically separated people to collaborate on a physical task. It also offers novel methods to analyze the efficiency of a collaboration system and a collaboration protocol through complexity indices. Using the innovative Collaborative Appliance for Remote-help (CARE) and with the support of a remote expert, fifty-nine subjects with minimal or no prior mechanical knowledge were able to elevate a car for replacing a tire; in a second experiment, thirty subjects with minimal or no prior plumbing knowledge were able to change the cartridge of a faucet. In both cases, average times were close to standard average repair times, and more importantly, both tasks were completed with total accuracy. Our experiments and results show that one can use the developed mechanism and methods for expanding the protocols for a variety of home, vehicle, and appliance repairs and installations. △ Less","7 September, 2020",https://arxiv.org/pdf/2009.03143
A nested genetic algorithm strategy for the optimal plastic design of frames,A. Greco;F. Cannizzaro;R. Bruno;A. Pluchino,"An innovative strategy for the optimal design of planar frames able to resist to seismic excitations is here proposed. The procedure is based on genetic algorithms (GA) which are performed according to a nested structure suitable to be implemented in parallel computing on several devices. In particular, this solution foresees two nested genetic algorithms. The first one, named ""External GA"", seeks, among a predefined list of profiles, the size of the structural elements of the frame which correspond to the most performing solution associated to the highest value of an appropriate fitness function. The latter function takes into account, among other considerations, of the seismic safety factor and the failure mode which are calculated by means of the second algorithm, named ""Internal GA"". The details of the proposed procedure are provided and applications to the seismic design of two frames of different size are described. △ Less","4 September, 2020",https://arxiv.org/pdf/2009.02111
"The Pace of Artificial Intelligence Innovations: Speed, Talent, and Trial-and-Error",Xuli Tang;Xin Li;Ying Ding;Min Song;Yi Bu,"Innovations in artificial intelligence (AI) are occurring at speeds faster than ever witnessed before. However, few studies have managed to measure or depict this increasing velocity of innovations in the field of AI. In this paper, we combine data on AI from arXiv and Semantic Scholar to explore the pace of AI innovations from three perspectives: AI publications, AI players, and AI updates (trial and error). A research framework and three novel indicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed (US), are proposed to measure the pace of innovations in the field of AI. The results show that: (1) in 2019, more than 3 AI preprints were submitted to arXiv per hour, over 148 times faster than in 1994. Furthermore, there was one deep learning-related preprint submitted to arXiv every 0.87 hours in 2019, over 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers entered into the field of AI each hour in 2019, more than 175 times faster than in the 1990s. (3) As for AI updates (trial and error), one updated AI preprint was submitted to arXiv every 41 days, with around 33% of AI preprints having been updated at least twice in 2019. In addition, as reported in 2019, it took, on average, only around 0.2 year for AI preprints to receive their first citations, which is 5 times faster than 2000-2007. This swift pace in AI illustrates the increase in popularity of AI innovation. The systematic and fine-grained analysis of the AI field enabled to portrait the pace of AI innovation and demonstrated that the proposed approach can be adopted to understand other fast-growing fields such as cancer research and nano science. △ Less","3 September, 2020",https://arxiv.org/pdf/2009.01812
Simulation of an Elevator Group Control Using Generative Adversarial Networks and Related AI Tools,Tom Peetz;Sebastian Vogt;Martin Zaefferer;Thomas Bartz-Beielstein,"Testing new, innovative technologies is a crucial task for safety and acceptance. But how can new systems be tested if no historical real-world data exist? Simulation provides an answer to this important question. Classical simulation tools such as event-based simulation are well accepted. But most of these established simulation models require the specification of many parameters. Furthermore, simulation runs, e.g., CFD simulations, are very time consuming. Generative Adversarial Networks (GANs) are powerful tools for generating new data for a variety of tasks. Currently, their most frequent application domain is image generation. This article investigates the applicability of GANs for imitating simulations. We are comparing the simulation output of a technical system with the output of a GAN. To exemplify this approach, a well-known multi-car elevator system simulator was chosen. Our study demonstrates the feasibility of this approach. It also discusses pitfalls and technical problems that occurred during the implementation. Although we were able to show that in principle, GANs can be used as substitutes for expensive simulation runs, we also show that they cannot be used ""out of the box"". Fine tuning is needed. We present a proof-of-concept, which can serve as a starting point for further research. △ Less","3 September, 2020",https://arxiv.org/pdf/2009.01696
A free web service for fast COVID-19 classification of chest X-Ray images,Jose David Bermudez Castro;Ricardo Rei;Jose E. Ruiz;Pedro Achanccaray Diaz;Smith Arauco Canchumuni;Cristian Muñoz Villalobos;Felipe Borges Coelho;Leonardo Forero Mendoza;Marco Aurelio C. Pacheco,"The coronavirus outbreak became a major concern for society worldwide. Technological innovation and ingenuity are essential to fight COVID-19 pandemic and bring us one step closer to overcome it. Researchers over the world are working actively to find available alternatives in different fields, such as the Healthcare System, pharmaceutic, health prevention, among others. With the rise of artificial intelligence (AI) in the last 10 years, IA-based applications have become the prevalent solution in different areas because of its higher capability, being now adopted to help combat against COVID-19. This work provides a fast detection system of COVID-19 characteristics in X-Ray images based on deep learning (DL) techniques. This system is available as a free web deployed service for fast patient classification, alleviating the high demand for standards method for COVID-19 diagnosis. It is constituted of two deep learning models, one to differentiate between X-Ray and non-X-Ray images based on Mobile-Net architecture, and another one to identify chest X-Ray images with characteristics of COVID-19 based on the DenseNet architecture. For real-time inference, it is provided a pair of dedicated GPUs, which reduce the computational time. The whole system can filter out non-chest X-Ray images, and detect whether the X-Ray presents characteristics of COVID-19, highlighting the most sensitive regions. △ Less","27 August, 2020",https://arxiv.org/pdf/2009.01657
Derived metrics for the game of Go -- intrinsic network strength assessment and cheat-detection,Attila Egri-Nagy;Antti Törmänen,"The widespread availability of superhuman AI engines is changing how we play the ancient game of Go. The open-source software packages developed after the AlphaGo series shifted focus from producing strong playing entities to providing tools for analyzing games. Here we describe two ways of how the innovations of the second generation engines (e.g.~score estimates, variable komi) can be used for defining new metrics that help deepen our understanding of the game. First, we study how much information the search component contributes in addition to the raw neural network policy output. This gives an intrinsic strength measurement for the neural network. Second, we define the effect of a move by the difference in score estimates. This gives a fine-grained, move-by-move performance evaluation of a player. We use this in combating the new challenge of detecting online cheating. △ Less","13 November, 2020",https://arxiv.org/pdf/2009.01606
Best practices for software maturity improvement: a GÉANT case study,Bartosz Walter;Branko Marović;Ivan Garnizov;Marcin Wolski;Andrijana Todosijević,"Maturity models for software indicate the key areas that contribute to quality improvements. They usually combine technical, organisational and human aspects relevant for effective software development, to focus the efforts and draw the direction for optimisations. In this paper, we present the process of defining best practices that support the GÉANT Software Maturity Model (GSMM), aligned to the needs of a distributed, innovation-driven, pan-European organisation. Based on the identification of specific goals relevant for GÉANT and a preliminary maturity assessment, we created a catalogue of best practices that help the software teams to attain the goals defined in the GSMM. △ Less","3 September, 2020",https://arxiv.org/pdf/2009.01501
Benchmarking 50-Photon Gaussian Boson Sampling on the Sunway TaihuLight,Yuxuan Li;Mingcheng Chen;Yaojian Chen;Haitian Lu;Lin Gan;Chaoyang Lu;Jianwei Pan;Haohuan Fu;Guangwen Yang,"Boson sampling is expected to be one of an important milestones that will demonstrate quantum supremacy. The present work establishes the benchmarking of Gaussian boson sampling (GBS) with threshold detection based on the Sunway TaihuLight supercomputer. To achieve the best performance and provide a competitive scenario for future quantum computing studies, the selected simulation algorithm is fully optimized based on a set of innovative approaches, including a parallel scheme and instruction-level optimizing method. Furthermore, data precision and instruction scheduling are handled in a sophisticated manner by an adaptive precision optimization scheme and a DAG-based heuristic search algorithm, respectively. Based on these methods, a highly efficient and parallel quantum sampling algorithm is designed. The largest run enables us to obtain one Torontonian function of a 100 x 100 submatrix from 50-photon GBS within 20 hours in 128-bit precision and 2 days in 256-bit precision. △ Less","2 September, 2020",https://arxiv.org/pdf/2009.01177
"Structural Iterative Rounding for Generalized k
-Median Problems",Anupam Gupta;Benjamin Moseley;Rudy Zhou,"This paper considers approximation algorithms for generalized k-median problems. This class of problems can be informally described as k-median with a constant number of extra constraints, and includes k-median with outliers, and knapsack median. Our first contribution is a pseudo-approximation algorithm for generalized k-median that outputs a 6.387-approximate solution, with a constant number of fractional variables. The algorithm builds on the iterative rounding framework introduced by Krishnaswamy, Li, and Sandeep for k-median with outliers. The main technical innovation is allowing richer constraint sets in the iterative rounding and taking advantage of the structure of the resulting extreme points. Using our pseudo-approximation algorithm, we give improved approximation algorithms for k-median with outliers and knapsack median. This involves combining our pseudo-approximation with pre- and post-processing steps to round a constant number of fractional variables at a small increase in cost. Our algorithms achieve approximation ratios 6.994 + ε and 6.387 + ε for k-median with outliers and knapsack median, respectively. These improve on the best-known approximation ratio 7.081 + ε for both problems \cite{DBLP:conf/stoc/KrishnaswamyLS18}. △ Less","2 September, 2020",https://arxiv.org/pdf/2009.00808
Using Graphlet Spectrograms for Temporal Pattern Analysis of Virus-Research Collaboration Networks,Dimitris Floros;Tiancheng Liu;Nikos Pitsianis;Xiaobai Sun,"We introduce a new method for temporal pattern analysis of scientific collaboration networks. We investigate in particular virus research activities through five epidemic or pandemic outbreaks in the recent two decades and in the ongoing pandemic with COVID-19. Our method embodies two innovative components. The first is a simple model of temporal collaboration networks with time segmented in publication time and convolved in citation history, to effectively capture and accommodate collaboration activities at mixed time scales. The second component is the novel use of graphlets to encode topological structures and to detect change and persistence in collaboration activities over time. We discover in particular two unique and universal roles of bi-fork graphlet in (1) identifying bridges among triangle clusters and (2) quantifying grassroots as the backbone of every collaboration network. We present a number of intriguing patterns and findings about the virus-research activities. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00477
Advancing from Predictive Maintenance to Intelligent Maintenance with AI and IIoT,Haining Zheng;Antonio R. Paiva;Chris S. Gurciullo,"As Artificial Intelligent (AI) technology advances and increasingly large amounts of data become readily available via various Industrial Internet of Things (IIoT) projects, we evaluate the state of the art of predictive maintenance approaches and propose our innovative framework to improve the current practice. The paper first reviews the evolution of reliability modelling technology in the past 90 years and discusses major technologies developed in industry and academia. We then introduce the next generation maintenance framework - Intelligent Maintenance, and discuss its key components. This AI and IIoT based Intelligent Maintenance framework is composed of (1) latest machine learning algorithms including probabilistic reliability modelling with deep learning, (2) real-time data collection, transfer, and storage through wireless smart sensors, (3) Big Data technologies, (4) continuously integration and deployment of machine learning models, (5) mobile device and AR/VR applications for fast and better decision-making in the field. Particularly, we proposed a novel probabilistic deep learning reliability modelling approach and demonstrate it in the Turbofan Engine Degradation Dataset. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00351
A Novel Software-based Multi-path RDMA Solutionfor Data Center Networks,Feng Tian;Wendi Feng;Yang Zhang;Zhi-Li Zhang,"In this paper we propose Virtuoso, a purely software-based multi-path RDMA solution for data center networks (DCNs) to effectively utilize the rich multi-path topology for load balancing and reliability. As a ""middleware"" library operating at the user space, Virtuoso employs three innovative mechanisms to achieve its goal. In contrast to existing hardware-based MP-RDMA solution, Virtuoso can be readily deployed in DCNs with existing RDMA NICs. It also decouples path selection and load balancing mechanisms from hardware features, allowing DCN operators and applications to make flexible decisions by employing the best mechanisms (as ""plug-in"" software library modules) as needed. Our experiments show that Virtuoso is capable of fully utilizing multiple paths with negligible CPU overheads △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00243
"Urban Sensing based on Mobile Phone Data: Approaches, Applications and Challenges",Mohammadhossein Ghahramani;MengChu Zhou;Gang Wang,"Data volume grows explosively with the proliferation of powerful smartphones and innovative mobile applications. The ability to accurately and extensively monitor and analyze these data is necessary. Much concern in mobile data analysis is related to human beings and their behaviours. Due to the potential value that lies behind these massive data, there have been different proposed approaches for understanding corresponding patterns. To that end, monitoring people's interactions, whether counting them at fixed locations or tracking them by generating origin-destination matrices is crucial. The former can be used to determine the utilization of assets like roads and city attractions. The latter is valuable when planning transport infrastructure. Such insights allow a government to predict the adoption of new roads, new public transport routes, modification of existing infrastructure, and detection of congestion zones, resulting in more efficient designs and improvement. Smartphone data exploration can help research in various fields, e.g., urban planning, transportation, health care, and business marketing. It can also help organizations in decision making, policy implementation, monitoring and evaluation at all levels. This work aims to review the methods and techniques that have been implemented to discover knowledge from mobile phone data. We classify these existing methods and present a taxonomy of the related work by discussing their pros and cons. △ Less","29 August, 2020",https://arxiv.org/pdf/2008.12992
On segmentation of pectoralis muscle in digital mammograms by means of deep learning,Hossein Soleimani;Oleg V. Michailovich,"Computer-aided diagnosis (CAD) has long become an integral part of radiological management of breast disease, facilitating a number of important clinical applications, including quantitative assessment of breast density and early detection of malignancies based on X-ray mammography. Common to such applications is the need to automatically discriminate between breast tissue and adjacent anatomy, with the latter being predominantly represented by pectoralis major (or pectoral muscle). Especially in the case of mammograms acquired in the mediolateral oblique (MLO) view, the muscle is easily confusable with some elements of breast anatomy due to their morphological and photometric similarity. As a result, the problem of automatic detection and segmentation of pectoral muscle in MLO mammograms remains a challenging task, innovative approaches to which are still required and constantly searched for. To address this problem, the present paper introduces a two-step segmentation strategy based on a combined use of data-driven prediction (deep learning) and graph-based image processing. In particular, the proposed method employs a convolutional neural network (CNN) which is designed to predict the location of breast-pectoral boundary at different levels of spatial resolution. Subsequently, the predictions are used by the second stage of the algorithm, in which the desired boundary is recovered as a solution to the shortest path problem on a specially designed graph. The proposed algorithm has been tested on three different datasets (i.e., MIAS, CBIS-DDSm and InBreast) using a range of quantitative metrics. The results of comparative analysis show considerable improvement over state-of-the-art, while offering the possibility of model-free and fully automatic processing. △ Less","28 August, 2020",https://arxiv.org/pdf/2008.12904
Dynamic Graph Neural Network for Traffic Forecasting in Wide Area Networks,Tanwi Mallick;Mariam Kiran;Bashir Mohammed;Prasanna Balaprakash,"Wide area networking infrastructures (WANs), particularly science and research WANs, are the backbone for moving large volumes of scientific data between experimental facilities and data centers. With demands growing at exponential rates, these networks are struggling to cope with large data volumes, real-time responses, and overall network performance. Network operators are increasingly looking for innovative ways to manage the limited underlying network resources. Forecasting network traffic is a critical capability for proactive resource management, congestion mitigation, and dedicated transfer provisioning. To this end, we propose a nonautoregressive graph-based neural network for multistep network traffic forecasting. Specifically, we develop a dynamic variant of diffusion convolutional recurrent neural networks to forecast traffic in research WANs. We evaluate the efficacy of our approach on real traffic from ESnet, the U.S. Department of Energy's dedicated science network. Our results show that compared to classical forecasting methods, our approach explicitly learns the dynamic nature of spatiotemporal traffic patterns, showing significant improvements in forecasting accuracy. Our technique can surpass existing statistical and deep learning approaches by achieving approximately 20% mean absolute percentage error for multiple hours of forecasts despite dynamic network traffic settings. △ Less","28 August, 2020",https://arxiv.org/pdf/2008.12767
Development of a biomechanical motion sensorimotor platform for enhanced locomotion under microgravity conditions,Peter A. Johnson;John C. Johnson;Lucas Tombrowski;Svetozar Zirnov;Riley Witiw;Austin A. Mardon,"For humans accustomed to 1-G environments on Earth, microgravity conditions in orbit and on celestial bodies with lower gravitational field, such as the Moon, can be physiologically compromising. Of these, motor and fine-dexterity tasks involving the extremities, particularly in locomotion, grasp and release, are influenced becoming delayed and placing greater force demands. The authors hereby propose incorporating this same technological innovation into loading suits designed for use in orbit or celestial environment. Current prosthetic systems use electromyography (EMG)-based techniques for creating functional sensorimotor platforms. This model sustains a sensorimotor platform for prosthesis users. However, several limitations in practical use and signal detection have been identified in these systems. Accelerometer-based sensorimotor systems have been suggested to overcome these limitations but only proof-of-concept has been demonstrated. Our group previously suggested the combined application of prosthetic accelerometers in loading suits and EMGs for input signal detection, quantification, and predictive output modelling necessary for improved motion adjustments under microgravity conditions. It is anticipated this technology can enhance tasks such as repairs or construction or perhaps in recreational design when considering commercial and private human access to space. The incorporation of a prosthetic biomechanical sensorimotor system in loading suits to enhance locomotion under microgravity conditions are conceivable; however, demonstration of a proof-of-concept is required before implementation. △ Less","11 July, 2020",https://arxiv.org/pdf/2008.12225
How semantic and geometric information mutually reinforce each other in ToF object localization,Antoine Vanderschueren;Victor Joos;Christophe De Vleeschouwer,"We propose a novel approach to localize a 3D object from the intensity and depth information images provided by a Time-of-Flight (ToF) sensor. Our method uses two CNNs. The first one uses raw depth and intensity images as input, to segment the floor pixels, from which the extrinsic parameters of the camera are estimated. The second CNN is in charge of segmenting the object-of-interest. As a main innovation, it exploits the calibration estimated from the prediction of the first CNN to represent the geometric depth information in a coordinate system that is attached to the ground, and is thus independent of the camera elevation. In practice, both the height of pixels with respect to the ground, and the orientation of normals to the point cloud are provided as input to the second CNN. Given the segmentation predicted by the second CNN, the object is localized based on point cloud alignment with a reference model. Our experiments demonstrate that our proposed two-step approach improves segmentation and localization accuracy by a significant margin compared to a conventional CNN architecture, ignoring calibration and height maps, but also compared to PointNet++. △ Less","27 August, 2020",https://arxiv.org/pdf/2008.12002
Ethical behavior in humans and machines -- Evaluating training data quality for beneficial machine learning,Thilo Hagendorff,"Machine behavior that is based on learning algorithms can be significantly influenced by the exposure to data of different qualities. Up to now, those qualities are solely measured in technical terms, but not in ethical ones, despite the significant role of training and annotation data in supervised machine learning. This is the first study to fill this gap by describing new dimensions of data quality for supervised machine learning applications. Based on the rationale that different social and psychological backgrounds of individuals correlate in practice with different modes of human-computer-interaction, the paper describes from an ethical perspective how varying qualities of behavioral data that individuals leave behind while using digital technologies have socially relevant ramification for the development of machine learning applications. The specific objective of this study is to describe how training data can be selected according to ethical assessments of the behavior it originates from, establishing an innovative filter regime to transition from the big data rationale n = all to a more selective way of processing data for training sets in machine learning. The overarching aim of this research is to promote methods for achieving beneficial machine learning applications that could be widely useful for industry as well as academia. △ Less","26 August, 2020",https://arxiv.org/pdf/2008.11463
Data Mining Approach to Analyze Covid19 Dataset of Brazilian Patients,Josimar E. Chire Saire,"The pandemic originated by coronavirus(covid-19), name coined by World Health Organization during the first month in 2020. Actually, almost all the countries presented covid19 positive cases and governments are choosing different health policies to stop the infection and many research groups are working on patients data to understand the virus, at the same time scientists are looking for a vacuum to enhance imnulogy system to tack covid19 virus. One of top countries with more infections is Brazil, until August 11 had a total of 3,112,393 cases. Research Foundation of Sao Paulo State(Fapesp) released a dataset, it was an innovative in collaboration with hospitals(Einstein, Sirio-Libanes), laboratory(Fleury) and Sao Paulo University to foster reseach on this trend topic. The present paper presents an exploratory analysis of the datasets, using a Data Mining Approach, and some inconsistencies are found, i.e. NaN values, null references values for analytes, outliers on results of analytes, encoding issues. The results were cleaned datasets for future studies, but at least a 20\% of data were discarded because of non numerical, null values and numbers out of reference range. △ Less","25 August, 2020",https://arxiv.org/pdf/2008.11344
Historical Context and Key Features of Digital Money Tokens,Shreepad Shukla,"Digital money tokens have attracted the attention of financial institutions, central banks, regulators, international associations and fintechs. Their research and experimentation with digital money tokens has included creating innovative technical and operational frameworks. In this paper, we present a 'money tree' which places this recent concept of digital money tokens into a historical context by illustrating their evolution from more traditional forms of money. We then identify key features of digital money tokens with options and examples. We hope this paper will be of interest to the financial services industry and we look forward to feedback. △ Less","25 August, 2020",https://arxiv.org/pdf/2008.11084
"Precision Health Data: Requirements, Challenges and Existing Techniques for Data Security and Privacy",Chandra Thapa;Seyit Camtepe,"Precision health leverages information from various sources, including omics, lifestyle, environment, social media, medical records, and medical insurance claims to enable personalized care, prevent and predict illness, and precise treatments. It extensively uses sensing technologies (e.g., electronic health monitoring devices), computations (e.g., machine learning), and communication (e.g., interaction between the health data centers). As health data contain sensitive private information, including the identity of patient and carer and medical conditions of the patient, proper care is required at all times. Leakage of these private information affects the personal life, including bullying, high insurance premium, and loss of job due to the medical history. Thus, the security, privacy of and trust on the information are of utmost importance. Moreover, government legislation and ethics committees demand the security and privacy of healthcare data. Herein, in the light of precision health data security, privacy, ethical and regulatory requirements, finding the best methods and techniques for the utilization of the health data, and thus precision health is essential. In this regard, firstly, this paper explores the regulations, ethical guidelines around the world, and domain-specific needs. Then it presents the requirements and investigates the associated challenges. Secondly, this paper investigates secure and privacy-preserving machine learning methods suitable for the computation of precision health data along with their usage in relevant health projects. Finally, it illustrates the best available techniques for precision health data security and privacy with a conceptual system model that enables compliance, ethics clearance, consent management, medical innovations, and developments in the health domain. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.10733
Patching as Translation: the Data and the Metaphor,Yangruibo Ding;Baishakhi Ray;Premkumar Devanbu;Vincent J. Hellendoorn,"Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that ""software patching is like language translation"". We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as ""proof-of-concept"" tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation. △ Less","31 August, 2020",https://arxiv.org/pdf/2008.10707
Multidimensionality of Legal Singularity: Parametric Analysis and the Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Legal scholars have in the last several years embarked upon an ongoing discussion and debate over a potential Legal Singularity that might someday occur, involving a variant or law-domain offshoot leveraged from the Artificial Intelligence (AI) realm amid its many decades of deliberations about an overarching and generalized technological singularity (referred to classically as The Singularity). This paper examines the postulated Legal Singularity and proffers that such AI and Law cogitations can be enriched by these three facets addressed herein: (1) dovetail additionally salient considerations of The Singularity into the Legal Singularity, (2) make use of an in-depth and innovative multidimensional parametric analysis of the Legal Singularity as posited in this paper, and (3) align and unify the Legal Singularity with the Levels of Autonomy (LoA) associated with AI Legal Reasoning (AILR) as propounded in this paper. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.10575
Understanding the online behavior and risks of children: results of a large-scale national survey on 10-18 year olds,Evangelia Daskalaki;Katerina Psaroudaki;Marieva Karkanaki;Paraskevi Fragopoulou,"The Internet has opened up new horizons of knowledge, communication and entertainment in our lives. Through this, young people are presented with a wealth of opportunities and activities that can enhance their skills and empower their knowledge and creativity. However, the online engagement of young people often comes with significant risks, encountered by children accidentally or deliberately. The emergence of new online services at an unprecedented speed and innovation brings the need, internationally, for a constant monitoring and investigation of the rapidly changing landscape and the associated emerging risk factors that could potentially jeopardize children's development, opportunities and lives. The Greek Safer Internet Center conducted two large-scale surveys to understand children's internet engagement, aiming to contribute towards improved child protection policies that could guide the efforts of key stakeholders towards a safer cyberspace. The first survey took place at the end of 2018, with the approval of the Greek Ministry of Education and Religious Affairs, and was conducted online among 14,000 pupils aged 10-18 years from 400 schools spread in five different urban areas of Greece. A follow up survey was realized the following year, among 13,000 students of the same age group from 500 school units in six different prefectures of Greece. To our knowledge, it is the first tie national surveys of such scale are conducted in Greece. The paper presents the analysis of the collected data, and describe the underlined methodology based on which the survey was formulated and conducted according to international standards, around specific thematic areas, namely internet use and online behavior, parental engagement, confidence level of children, digital literacy, social media, and online risks. The results were mainly analysed based on educational level and gender. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.10274
Orderly Disorder in Point Cloud Domain,Morteza Ghahremani;Bernard Tiddeman;Yonghuai Liu;Ardhendu Behera,"In the real world, out-of-distribution samples, noise and distortions exist in test data. Existing deep networks developed for point cloud data analysis are prone to overfitting and a partial change in test data leads to unpredictable behaviour of the networks. In this paper, we propose a smart yet simple deep network for analysis of 3D models using `orderly disorder' theory. Orderly disorder is a way of describing the complex structure of disorders within complex systems. Our method extracts the deep patterns inside a 3D object via creating a dynamic link to seek the most stable patterns and at once, throws away the unstable ones. Patterns are more robust to changes in data distribution, especially those that appear in the top layers. Features are extracted via an innovative cloning decomposition technique and then linked to each other to form stable complex patterns. Our model alleviates the vanishing-gradient problem, strengthens dynamic link propagation and substantially reduces the number of parameters. Extensive experiments on challenging benchmark datasets verify the superiority of our light network on the segmentation and classification tasks, especially in the presence of noise wherein our network's performance drops less than 10% while the state-of-the-art networks fail to work. △ Less","21 August, 2020",https://arxiv.org/pdf/2008.09634
Graph Neural Networks for 3D Multi-Object Tracking,Xinshuo Weng;Yongxin Wang;Yunze Man;Kris Kitani,"3D Multi-object tracking (MOT) is crucial to autonomous systems. Recent work often uses a tracking-by-detection pipeline, where the feature of each object is extracted independently to compute an affinity matrix. Then, the affinity matrix is passed to the Hungarian algorithm for data association. A key process of this pipeline is to learn discriminative features for different objects in order to reduce confusion during data association. To that end, we propose two innovative techniques: (1) instead of obtaining the features for each object independently, we propose a novel feature interaction mechanism by introducing Graph Neural Networks; (2) instead of obtaining the features from either 2D or 3D space as in prior work, we propose a novel joint feature extractor to learn appearance and motion features from 2D and 3D space. Through experiments on the KITTI dataset, our proposed method achieves state-of-the-art 3D MOT performance. Our project website is at http://www.xinshuoweng.com/projects/GNN3DMOT. △ Less","20 August, 2020",https://arxiv.org/pdf/2008.09506
COOKIE: A Dataset for Conversational Recommendation over Knowledge Graphs in E-commerce,Zuohui Fu;Yikun Xian;Yaxin Zhu;Yongfeng Zhang;Gerard de Melo,"In this work, we present a new dataset for conversational recommendation over knowledge graphs in e-commerce platforms called COOKIE. The dataset is constructed from an Amazon review corpus by integrating both user-agent dialogue and custom knowledge graphs for recommendation. Specifically, we first construct a unified knowledge graph and extract key entities between user--product pairs, which serve as the skeleton of a conversation. Then we simulate conversations mirroring the human coarse-to-fine process of choosing preferred items. The proposed baselines and experiments demonstrate that our dataset is able to provide innovative opportunities for conversational recommendation. △ Less","20 August, 2020",https://arxiv.org/pdf/2008.09237
Toward an Abstract Model of Programmable Data Plane Devices,Debobroto Das Robin;Javed I. Khan,"SDN divides the networking landscape into 2 parts: control and data plane. SDN expanded it's foot mark starting with OpenFlow based highly flexible control plane and rigid data plane. Innovation and improvement in hardware design and development is bringing various new architectures for data plane. Data plane is becoming more programmable then ever before. A common abstract model of data plane is required to develop complex application over these heterogeneous data plane devices. It can also provide insight about performance optimization and bench-marking of programmable data plane devices. Moreover, to understand and utilize data plane's programmability, a detailed structural analysis and an identifiable matrix to compare different devices are required. In this work, an improved and structured abstract model of the programmable data plane devices is presented and features of its components are discussed in detail. Several commercially available programmable data plane devices are also compared based on those features. △ Less","19 August, 2020",https://arxiv.org/pdf/2008.08697
Black Re-ID: A Head-shoulder Descriptor for the Challenging Problem of Person Re-Identification,Boqiang Xu;Lingxiao He;Xingyu Liao;Wu Liu;Zhenan Sun;Tao Mei,"Person re-identification (Re-ID) aims at retrieving an input person image from a set of images captured by multiple cameras. Although recent Re-ID methods have made great success, most of them extract features in terms of the attributes of clothing (e.g., color, texture). However, it is common for people to wear black clothes or be captured by surveillance systems in low light illumination, in which cases the attributes of the clothing are severely missing. We call this problem the Black Re-ID problem. To solve this problem, rather than relying on the clothing information, we propose to exploit head-shoulder features to assist person Re-ID. The head-shoulder adaptive attention network (HAA) is proposed to learn the head-shoulder feature and an innovative ensemble method is designed to enhance the generalization of our model. Given the input person image, the ensemble method would focus on the head-shoulder feature by assigning a larger weight if the individual insides the image is in black clothing. Due to the lack of a suitable benchmark dataset for studying the Black Re-ID problem, we also contribute the first Black-reID dataset, which contains 1274 identities in training set. Extensive evaluations on the Black-reID, Market1501 and DukeMTMC-reID datasets show that our model achieves the best result compared with the state-of-the-art Re-ID methods on both Black and conventional Re-ID problems. Furthermore, our method is also proved to be effective in dealing with person Re-ID in similar clothing. Our code and dataset are avaliable on https://github.com/xbq1994/. △ Less","19 August, 2020",https://arxiv.org/pdf/2008.08528
Domain-specific Communication Optimization for Distributed DNN Training,Hao Wang;Jingrong Chen;Xinchen Wan;Han Tian;Jiacheng Xia;Gaoxiong Zeng;Weiyan Wang;Kai Chen;Wei Bai;Junchen Jiang,"Communication overhead poses an important obstacle to distributed DNN training and draws increasing attention in recent years. Despite continuous efforts, prior solutions such as gradient compression/reduction, compute/communication overlapping and layer-wise flow scheduling, etc., are still coarse-grained and insufficient for an efficient distributed training especially when the network is under pressure. We present DLCP, a novel solution exploiting the domain-specific properties of deep learning to optimize communication overhead of DNN training in a fine-grained manner. At its heart, DLCP comprises of several key innovations beyond prior work: e.g., it exploits {\em bounded loss tolerance} of SGD-based training to improve tail communication latency which cannot be avoided purely through gradient compression. It then performs fine-grained packet-level prioritization and dropping, as opposed to flow-level scheduling, based on layers and magnitudes of gradients to further speedup model convergence without affecting accuracy. In addition, it leverages inter-packet order-independency to perform per-packet load balancing without causing classical re-ordering issues. DLCP works with both Parameter Server and collective communication routines. We have implemented DLCP with commodity switches, integrated it with various training frameworks including TensorFlow, MXNet and PyTorch, and deployed it in our small-scale testbed with 10 Nvidia V100 GPUs. Our testbed experiments and large-scale simulations show that DLCP delivers up to 84.3\% additional training acceleration over the best existing solutions. △ Less","16 August, 2020",https://arxiv.org/pdf/2008.08445
"Modeling, Visualization, and Analysis of African Innovation Performance",Muhammad Omer;Moayad El-Amin;Ammar Nasr;Rami Ahmed,"In this paper we discuss the concepts and emergence of Innovation Performance, and how to quantify it, primarily working with data from the Global Innovation Index, with emphasis on the African Innovation Performance. We briefly overview existing literature on using machine learning for modeling innovation performance, and use simple machine learning techniques, to analyze and predict the ""Mobile App Creation Indicator"" from the Global Innovation Index, by using insights from the stack-overflow developers survey. Also, we build and compare models to predict the Innovation Output Sub-index, also from the Global Innovation Index. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.07882
The Relational Data Borg is Learning,Dan Olteanu,"This paper overviews an approach that addresses machine learning over relational data as a database problem. This is justified by two observations. First, the input to the learning task is commonly the result of a feature extraction query over the relational data. Second, the learning task requires the computation of group-by aggregates. This approach has been already investigated for a number of supervised and unsupervised learning tasks, including: ridge linear regression, factorisation machines, support vector machines, decision trees, principal component analysis, and k-means; and also for linear algebra over data matrices. The main message of this work is that the runtime performance of machine learning can be dramatically boosted by a toolbox of techniques that exploit the knowledge of the underlying data. This includes theoretical development on the algebraic, combinatorial, and statistical structure of relational data processing and systems development on code specialisation, low-level computation sharing, and parallelisation. These techniques aim at lowering both the complexity and the constant factors of the learning time. This work is the outcome of extensive collaboration of the author with colleagues from RelationalAI, in particular Mahmoud Abo Khamis, Molham Aref, Hung Ngo, and XuanLong Nguyen, and from the FDB research project, in particular Ahmet Kara, Milos Nikolic, Maximilian Schleich, Amir Shaikhha, Jakub Zavodny, and Haozhe Zhang. The author would also like to thank the members of the FDB project for the figures and examples used in this paper. The author is grateful for support from industry: Amazon Web Services, Google, Infor, LogicBlox, Microsoft Azure, RelationalAI; and from the funding agencies EPSRC and ERC. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 682588. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.07864
FEARLESS STEPS Challenge (FS-2): Supervised Learning with Massive Naturalistic Apollo Data,Aditya Joglekar;John H. L. Hansen;Meena Chandra Shekar;Abhijeet Sangwan,"The Fearless Steps Initiative by UTDallas-CRSS led to the digitization, recovery, and diarization of 19,000 hours of original analog audio data, as well as the development of algorithms to extract meaningful information from this multi-channel naturalistic data resource. The 2020 FEARLESS STEPS (FS-2) Challenge is the second annual challenge held for the Speech and Language Technology community to motivate supervised learning algorithm development for multi-party and multi-stream naturalistic audio. In this paper, we present an overview of the challenge sub-tasks, data, performance metrics, and lessons learned from Phase-2 of the Fearless Steps Challenge (FS-2). We present advancements made in FS-2 through extensive community outreach and feedback. We describe innovations in the challenge corpus development, and present revised baseline results. We finally discuss the challenge outcome and general trends in system development across both phases (Phase FS-1 Unsupervised, and Phase FS-2 Supervised) of the challenge, and its continuation into multi-channel challenge tasks for the upcoming Fearless Steps Challenge Phase-3. △ Less","15 August, 2020",https://arxiv.org/pdf/2008.06764
Tackling COVID-19 through Responsible AI Innovation: Five Steps in the Right Direction,David Leslie,"Innovations in data science and AI/ML have a central role to play in supporting global efforts to combat COVID-19. The versatility of AI/ML technologies enables scientists and technologists to address an impressively broad range of biomedical, epidemiological, and socioeconomic challenges. This wide-reaching scientific capacity, however, also raises a diverse array of ethical challenges. The need for researchers to act quickly and globally in tackling SARS-CoV-2 demands unprecedented practices of open research and responsible data sharing at a time when innovation ecosystems are hobbled by proprietary protectionism, inequality, and a lack of public trust. Moreover, societally impactful interventions like digital contact tracing are raising fears of surveillance creep and are challenging widely held commitments to privacy, autonomy, and civil liberties. Prepandemic concerns that data-driven innovations may function to reinforce entrenched dynamics of societal inequity have likewise intensified given the disparate impact of the virus on vulnerable social groups and the life-and-death consequences of biased and discriminatory public health outcomes. To address these concerns, I offer five steps that need to be taken to encourage responsible research and innovation. These provide a practice-based path to responsible AI/ML design and discovery centered on open, accountable, equitable, and democratically governed processes and products. When taken from the start, these steps will not only enhance the capacity of innovators to tackle COVID-19 responsibly, they will, more broadly, help to better equip the data science and AI/ML community to cope with future pandemics and to support a more humane, rational, and just society. △ Less","15 August, 2020",https://arxiv.org/pdf/2008.06755
Blockchain applications in Healthcare: A model for research,Amir Hussain Zolfaghari;Herbert Daly;Mahdi Nasiri;Roxana Sharifian,"Blockchain technology has rapidly evolved from an enabling technology for cryptocurrencies to a potential solution to a wider range of problems found in data-centric and distributed systems. Interest in this area has encouraged many recent innovations to address challenges that traditional approaches of design have been unable to meet. Healthcare Information Systems with issues around privacy, interoperability, data integrity, and access control is potentially an area where blockchain technology may have a significant impact. Blockchain, however, is a meta-technology, combining multiple techniques, as it is often important to determine how best to separate concerns in the design and implementation of such systems. This paper proposes a layered approach for the organization of blockchain in healthcare applications. Key issues driving the adoption of this technology are explored. A model presenting the points in each layer is explored. Finally, we present an example of how the perspective we describe can improve the development of Health Information Systems. △ Less","13 August, 2020",https://arxiv.org/pdf/2008.05683
Renal Cell Carcinoma Detection and Subtyping with Minimal Point-Based Annotation in Whole-Slide Images,Zeyu Gao;Pargorn Puttapirat;Jiangbo Shi;Chen Li,"Obtaining a large amount of labeled data in medical imaging is laborious and time-consuming, especially for histopathology. However, it is much easier and cheaper to get unlabeled data from whole-slide images (WSIs). Semi-supervised learning (SSL) is an effective way to utilize unlabeled data and alleviate the need for labeled data. For this reason, we proposed a framework that employs an SSL method to accurately detect cancerous regions with a novel annotation method called Minimal Point-Based annotation, and then utilize the predicted results with an innovative hybrid loss to train a classification model for subtyping. The annotator only needs to mark a few points and label them are cancer or not in each WSI. Experiments on three significant subtypes of renal cell carcinoma (RCC) proved that the performance of the classifier trained with the Min-Point annotated dataset is comparable to a classifier trained with the segmentation annotated dataset for cancer region detection. And the subtyping model outperforms a model trained with only diagnostic labels by 12% in terms of f1-score for testing WSIs. △ Less","12 August, 2020",https://arxiv.org/pdf/2008.05332
Pixel-level Corrosion Detection on Metal Constructions by Fusion of Deep Learning Semantic and Contour Segmentation,Iason Katsamenis;Eftychios Protopapadakis;Anastasios Doulamis;Nikolaos Doulamis;Athanasios Voulodimos,"Corrosion detection on metal constructions is a major challenge in civil engineering for quick, safe and effective inspection. Existing image analysis approaches tend to place bounding boxes around the defected region which is not adequate both for structural analysis and pre-fabrication, an innovative construction concept which reduces maintenance cost, time and improves safety. In this paper, we apply three semantic segmentation-oriented deep learning models (FCN, U-Net and Mask R-CNN) for corrosion detection, which perform better in terms of accuracy and time and require a smaller number of annotated samples compared to other deep models, e.g. CNN. However, the final images derived are still not sufficiently accurate for structural analysis and pre-fabrication. Thus, we adopt a novel data projection scheme that fuses the results of color segmentation, yielding accurate but over-segmented contours of a region, with a processed area of the deep masks, resulting in high-confidence corroded pixels. △ Less","12 August, 2020",https://arxiv.org/pdf/2008.05204
Research on the construction method of vehicle driving cycle based on Mean Shift clustering,Yongjiang He,"In this study, a novel method for the construction of a driving cycle based on Mean Shift clustering is proposed to solve the problems existing in the traditional micro-trips method. Firstly, 1701 kinematic segments are obtained by processing and dividing the driving data in real road conditions. Secondly, 12 kinematic parameters are calculated for each segment, and the dimensionality of parameters is reduced through principal component analysis (PCA). Three principal components are chosen to classify all cycles into three types by the Mean Shift algorithm. Finally, according to the principle of minimum deviation, representative micro-trips are selected from each type of cycle to complete the construction of the final driving cycle. Further, the construction method in this paper is compared with the micro-trips construction method by the K-Means clustering. The results show that the construction method by Mean Shift clustering can more effectively reflect the real driving data. This study realizes the innovation in the construction method of micro-trips and provides a preliminary theoretical basis for the formulation of automobile working condition standards, energy management of new-energy vehicles, and optimal control of vehicle dynamics in driverless vehicles. △ Less","11 August, 2020",https://arxiv.org/pdf/2008.05070
Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of AI/AGI Using Multiple Intelligences and Learning Styles,Andrzej Cichocki;Alexander P. Kuleshov,"This article discusses some trends and concepts in developing new generation of future Artificial General Intelligence (AGI) systems which relate to complex facets and different types of human intelligence, especially social, emotional, attentional and ethical intelligence. We describe various aspects of multiple human intelligences and learning styles, which may impact on a variety of AI problem domains. Using the concept of 'multiple intelligences' rather than a single type of intelligence, we categorize and provide working definitions of various AGI depending on their cognitive skills or capacities. Future AI systems will be able not only to communicate with human users and each other, but also to efficiently exchange knowledge and wisdom with abilities of cooperation, collaboration and even co-creating something new and valuable and have meta-learning capacities. Multi-agent systems such as these can be used to solve problems that would be difficult to solve by any individual intelligent agent. Key words: Artificial General Intelligence (AGI), multiple intelligences, learning styles, physical intelligence, emotional intelligence, social intelligence, attentional intelligence, moral-ethical intelligence, responsible decision making, creative-innovative intelligence, cognitive functions, meta-learning of AI systems. △ Less","11 December, 2020",https://arxiv.org/pdf/2008.04793
Socially-Aware Conference Participant Recommendation with Personality Traits,Feng Xia;Nana Yaw Asabere;Haifeng Liu;Zhen Chen;Wei Wang,"As a result of the importance of academic collaboration at smart conferences, various researchers have utilized recommender systems to generate effective recommendations for participants. Recent research has shown that the personality traits of users can be used as innovative entities for effective recommendations. Nevertheless, subjective perceptions involving the personality of participants at smart conferences are quite rare and haven't gained much attention. Inspired by the personality and social characteristics of users, we present an algorithm called Socially and Personality Aware Recommendation of Participants (SPARP). Our recommendation methodology hybridizes the computations of similar interpersonal relationships and personality traits among participants. SPARP models the personality and social characteristic profiles of participants at a smart conference. By combining the above recommendation entities, SPARP then recommends participants to each other for effective collaborations. We evaluate SPARP using a relevant dataset. Experimental results confirm that SPARP is reliable and outperforms other state-of-the-art methods. △ Less","8 August, 2020",https://arxiv.org/pdf/2008.04653
An Intelligent Control Strategy for buck DC-DC Converter via Deep Reinforcement Learning,Chenggang Cui;Nan Yan;Chuanlin Zhang,"As a typical switching power supply, the DC-DC converter has been widely applied in DC microgrid. Due to the variation of renewable energy generation, research and design of DC-DC converter control algorithm with outstanding dynamic characteristics has significant theoretical and practical application value. To mitigate the bus voltage stability issue in DC microgrid, an innovative intelligent control strategy for buck DC-DC converter with constant power loads (CPLs) via deep reinforcement learning algorithm is constructed for the first time. In this article, a Markov Decision Process (MDP) model and the deep Q network (DQN) algorithm are defined for DC-DC converter. A model-free based deep reinforcement learning (DRL) control strategy is appropriately designed to adjust the agent-environment interaction through the rewards/penalties mechanism towards achieving converge to nominal voltage. The agent makes approximate decisions by extracting the high-dimensional feature of complex power systems without any prior knowledge. Eventually, the simulation comparison results demonstrate that the proposed controller has stronger self-learning and self-optimization capabilities under the different scenarios. △ Less","11 August, 2020",https://arxiv.org/pdf/2008.04542
Comprehensiveness of Archives: A Modern AI-enabled Approach to Build Comprehensive Shared Cultural Heritage,Abhishek Gupta;Nikitasha Kapoor,"Archives play a crucial role in the construction and advancement of society. Humans place a great deal of trust in archives and depend on them to craft public policies and to preserve languages, cultures, self-identity, views and values. Yet, there are certain voices and viewpoints that remain elusive in the current processes deployed in the classification and discoverability of records and archives. In this paper, we explore the ramifications and effects of centralized, due process archival systems on marginalized communities. There is strong evidence to prove the need for progressive design and technological innovation while in the pursuit of comprehensiveness, equity and justice. Intentionality and comprehensiveness is our greatest opportunity when it comes to improving archival practices and for the advancement and thrive-ability of societies at large today. Intentionality and comprehensiveness is achievable with the support of technology and the Information Age we live in today. Reopening, questioning and/or purposefully including others voices in archival processes is the intention we present in our paper. We provide examples of marginalized communities who continue to lead ""community archive"" movements in efforts to reclaim and protect their cultural identity, knowledge, views and futures. In conclusion, we offer design and AI-dominant technological considerations worth further investigation in efforts to bridge systemic gaps and build robust archival processes. △ Less","11 August, 2020",https://arxiv.org/pdf/2008.04541
"PoCoNet: Better Speech Enhancement with Frequency-Positional Embeddings, Semi-Supervised Conversational Data, and Biased Loss",Umut Isik;Ritwik Giri;Neerad Phansalkar;Jean-Marc Valin;Karim Helwani;Arvindh Krishnaswamy,"Neural network applications generally benefit from larger-sized models, but for current speech enhancement models, larger scale networks often suffer from decreased robustness to the variety of real-world use cases beyond what is encountered in training data. We introduce several innovations that lead to better large neural networks for speech enhancement. The novel PoCoNet architecture is a convolutional neural network that, with the use of frequency-positional embeddings, is able to more efficiently build frequency-dependent features in the early layers. A semi-supervised method helps increase the amount of conversational training data by pre-enhancing noisy datasets, improving performance on real recordings. A new loss function biased towards preserving speech quality helps the optimization better match human perceptual opinions on speech quality. Ablation experiments and objective and human opinion metrics show the benefits of the proposed improvements. △ Less","10 August, 2020",https://arxiv.org/pdf/2008.04470
"Crowd, Lending, Machine, and Bias",Runshan Fu;Yan Huang;Param Vir Singh,"Big data and machine learning (ML) algorithms are key drivers of many fintech innovations. While it may be obvious that replacing humans with machine would increase efficiency, it is not clear whether and where machines can make better decisions than humans. We answer this question in the context of crowd lending, where decisions are traditionally made by a crowd of investors. Using data from Prosper.com, we show that a reasonably sophisticated ML algorithm predicts listing default probability more accurately than crowd investors. The dominance of the machine over the crowd is more pronounced for highly risky listings. We then use the machine to make investment decisions, and find that the machine benefits not only the lenders but also the borrowers. When machine prediction is used to select loans, it leads to a higher rate of return for investors and more funding opportunities for borrowers with few alternative funding options. We also find suggestive evidence that the machine is biased in gender and race even when it does not use gender and race information as input. We propose a general and effective ""debasing"" method that can be applied to any prediction focused ML applications, and demonstrate its use in our context. We show that the debiased ML algorithm, which suffers from lower prediction accuracy, still leads to better investment decisions compared with the crowd. These results indicate that ML can help crowd lending platforms better fulfill the promise of providing access to financial resources to otherwise underserved individuals and ensure fairness in the allocation of these resources. △ Less","19 July, 2020",https://arxiv.org/pdf/2008.04068
Learning to Learn in Collective Adaptive Systems: Mining Design Patterns for Data-driven Reasoning,Mirko D'Angelo;Sona Ghahremani;Simos Gerasimou;Johannes Grohmann;Ingrid Nunes;Sven Tomforde;Evangelos Pournaras,"Engineering collective adaptive systems (CAS) with learning capabilities is a challenging task due to their multi-dimensional and complex design space. Data-driven approaches for CAS design could introduce new insights enabling system engineers to manage the CAS complexity more cost-effectively at the design-phase. This paper introduces a systematic approach to reason about design choices and patterns of learning-based CAS. Using data from a systematic literature review, reasoning is performed with a novel application of data-driven methodologies such as clustering, multiple correspondence analysis and decision trees. The reasoning based on past experience as well as supporting novel and innovative design choices are demonstrated. △ Less","10 August, 2020",https://arxiv.org/pdf/2008.03995
Tactics for Internal Compliance: A Literature Review,Ralph Foorthuis,"Compliance of organizations with internal and external norms is a highly relevant topic for both practitioners and academics nowadays. However, the substantive, elementary compliance tactics that organizations can use for achieving internal compliance have been described in a fragmented manner and in the literatures of distinct academic disciplines. Using a multidisciplinary structured literature review of 134 publications, this study offers three contributions. First, we present a typology of 45 compliance tactics, which constitutes a comprehensive and rich overview of elementary ways for bringing the organization into compliance. Secondly, we provide an overview of fundamental concepts in the theory of compliance, which forms the basis for the framework we developed for positioning compliance tactics and for analyzing or developing compliance strategies. Thirdly, we present insights for moving from compliance tactics to compliance strategies. In the process, and using the multidisciplinary literature review to take a bird's-eye view, we demonstrate that compliance strategies need to be regarded as a richer concept than perceived hitherto. We also show that opportunities for innovation exist. △ Less","9 August, 2020",https://arxiv.org/pdf/2008.03775
Security Design Patterns in Distributed Microservice Architecture,Chaitanya K. Rudrabhatla,"Micro service architecture has revolutionized the landscape for the development of web and mobile applications alike. Due to the stateless nature and loose coupling involved in the design of micro services, native mobile applications can be developed by utilizing the same backend services which feed the inputs to the web application front ends. Extending the same concept, a plethora of automated devices, thanks to the advancements in the field of IOT, have come into existence which can feed on the same set of micro services. This concept of build once and utilize for many use cases has become a new norm in the enterprise design patterns. To handle the horizontal scalability needs of so many calling clients, significant advancements have been made on the containerization and their orchestration strategies on the public cloud platforms. However, scalable design techniques have led to the increased exposure of backend services to unwanted entities. This broadened the attack surface and also the risk. On top of it the mix of heterogeneous technologies in MSA, their distinct logging strategies, makes the central logging difficult, which in turn loosens the security. Additionally, the complexity around building the resilience for fault tolerance across the decentralized networks, adds to the security loop holes. The simple security designs which were once used with traditional web applications cannot be used for Microservice based applications. This paper articulates the innovative approaches of handling the security needs involved in protection of distributed services in Microservice architecture. △ Less","7 August, 2020",https://arxiv.org/pdf/2008.03395
Reliable Liver Fibrosis Assessment from Ultrasound using Global Hetero-Image Fusion and View-Specific Parameterization,Bowen Li;Ke Yan;Dar-In Tai;Yuankai Huo;Le Lu;Jing Xiao;Adam P. Harrison,"Ultrasound (US) is a critical modality for diagnosing liver fibrosis. Unfortunately, assessment is very subjective, motivating automated approaches. We introduce a principled deep convolutional neural network (CNN) workflow that incorporates several innovations. First, to avoid overfitting on non-relevant image features, we force the network to focus on a clinical region of interest (ROI), encompassing the liver parenchyma and upper border. Second, we introduce global heteroimage fusion (GHIF), which allows the CNN to fuse features from any arbitrary number of images in a study, increasing its versatility and flexibility. Finally, we use 'style'-based view-specific parameterization (VSP) to tailor the CNN processing for different viewpoints of the liver, while keeping the majority of parameters the same across views. Experiments on a dataset of 610 patient studies (6979 images) demonstrate that our pipeline can contribute roughly 7% and 22% improvements in partial area under the curve and recall at 90% precision, respectively, over conventional classifiers, validating our approach to this crucial problem. △ Less","7 August, 2020",https://arxiv.org/pdf/2008.03352
A Survey on Security and Privacy Issues in Edge Computing-Assisted Internet of Things,Abdulmalik Alwarafy;Khaled A. Al-Thelaya;Mohamed Abdallah;Jens Schneider;Mounir Hamdi,"Internet of Things (IoT) is an innovative paradigm envisioned to provide massive applications that are now part of our daily lives. Millions of smart devices are deployed within complex networks to provide vibrant functionalities including communications, monitoring, and controlling of critical infrastructures. However, this massive growth of IoT devices and the corresponding huge data traffic generated at the edge of the network created additional burdens on the state-of-the-art centralized cloud computing paradigm due to the bandwidth and resources scarcity. Hence, edge computing (EC) is emerging as an innovative strategy that brings data processing and storage near to the end users, leading to what is called EC-assisted IoT. Although this paradigm provides unique features and enhanced quality of service (QoS), it also introduces huge risks in data security and privacy aspects. This paper conducts a comprehensive survey on security and privacy issues in the context of EC-assisted IoT. In particular, we first present an overview of EC-assisted IoT including definitions, applications, architecture, advantages, and challenges. Second, we define security and privacy in the context of EC-assisted IoT. Then, we extensively discuss the major classifications of attacks in EC-assisted IoT and provide possible solutions and countermeasures along with the related research efforts. After that, we further classify some security and privacy issues as discussed in the literature based on security services and based on security objectives and functions. Finally, several open challenges and future research directions for secure EC-assisted IoT paradigm are also extensively provided. △ Less","5 August, 2020",https://arxiv.org/pdf/2008.03252
6VecLM: Language Modeling in Vector Space for IPv6 Target Generation,Tianyu Cui;Gang Xiong;Gaopeng Gou;Junzheng Shi;Wei Xia,"Fast IPv6 scanning is challenging in the field of network measurement as it requires exploring the whole IPv6 address space but limited by current computational power. Researchers propose to obtain possible active target candidate sets to probe by algorithmically analyzing the active seed sets. However, IPv6 addresses lack semantic information and contain numerous addressing schemes, leading to the difficulty of designing effective algorithms. In this paper, we introduce our approach 6VecLM to explore achieving such target generation algorithms. The architecture can map addresses into a vector space to interpret semantic relationships and uses a Transformer network to build IPv6 language models for predicting address sequence. Experiments indicate that our approach can perform semantic classification on address space. By adding a new generation approach, our model possesses a controllable word innovation capability compared to conventional language models. The work outperformed the state-of-the-art target generation algorithms on two active address datasets by reaching more quality candidate sets. △ Less","5 August, 2020",https://arxiv.org/pdf/2008.02213
Prime-Aware Adaptive Distillation,Youcai Zhang;Zhonghao Lan;Yuchen Dai;Fangao Zeng;Yan Bai;Jie Chang;Yichen Wei,"Knowledge distillation(KD) aims to improve the performance of a student network by mimicing the knowledge from a powerful teacher network. Existing methods focus on studying what knowledge should be transferred and treat all samples equally during training. This paper introduces the adaptive sample weighting to KD. We discover that previous effective hard mining methods are not appropriate for distillation. Furthermore, we propose Prime-Aware Adaptive Distillation (PAD) by the incorporation of uncertainty learning. PAD perceives the prime samples in distillation and then emphasizes their effect adaptively. PAD is fundamentally different from and would refine existing methods with the innovative view of unequal training. For this reason, PAD is versatile and has been applied in various tasks including classification, metric learning, and object detection. With ten teacher-student combinations on six datasets, PAD promotes the performance of existing distillation methods and outperforms recent state-of-the-art methods. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.01458
Distributed Non-Negative Tensor Train Decomposition,Manish Bhattarai;Gopinath Chennupati;Erik Skau;Raviteja Vangara;Hirsto Djidjev;Boian Alexandrov,"The era of exascale computing opens new venues for innovations and discoveries in many scientific, engineering, and commercial fields. However, with the exaflops also come the extra-large high-dimensional data generated by high-performance computing. High-dimensional data is presented as multidimensional arrays, aka tensors. The presence of latent (not directly observable) structures in the tensor allows a unique representation and compression of the data by classical tensor factorization techniques. However, the classical tensor methods are not always stable or they can be exponential in their memory requirements, which makes them not suitable for high-dimensional tensors. Tensor train (TT) is a state-of-the-art tensor network introduced for factorization of high-dimensional tensors. TT transforms the initial high-dimensional tensor in a network of three-dimensional tensors that requires only a linear storage. Many real-world data, such as, density, temperature, population, probability, etc., are non-negative and for an easy interpretation, the algorithms preserving non-negativity are preferred. Here, we introduce a distributed non-negative tensor-train and demonstrate its scalability and the compression on synthetic and real-world big datasets. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.01340
Generating Visually Aligned Sound from Videos,Peihao Chen;Yang Zhang;Mingkui Tan;Hongdong Xiao;Deng Huang;Chuang Gan,"We focus on the task of generating sound from natural videos, and the sound should be both temporally and content-wise aligned with visual signals. This task is extremely challenging because some sounds generated \emph{outside} a camera can not be inferred from video content. The model may be forced to learn an incorrect mapping between visual content and these irrelevant sounds. To address this challenge, we propose a framework named REGNET. In this framework, we first extract appearance and motion features from video frames to better distinguish the object that emits sound from complex background information. We then introduce an innovative audio forwarding regularizer that directly considers the real sound as input and outputs bottlenecked sound features. Using both visual and bottlenecked sound features for sound prediction during training provides stronger supervision for the sound prediction. The audio forwarding regularizer can control the irrelevant sound component and thus prevent the model from learning an incorrect mapping between video frames and sound emitted by the object that is out of the screen. During testing, the audio forwarding regularizer is removed to ensure that REGNET can produce purely aligned sound only from visual features. Extensive evaluations based on Amazon Mechanical Turk demonstrate that our method significantly improves both temporal and content-wise alignment. Remarkably, our generated sound can fool the human with a 68.12% success rate. Code and pre-trained models are publicly available at https://github.com/PeihaoChen/regnet △ Less","14 July, 2020",https://arxiv.org/pdf/2008.00820
Blackbox Trojanising of Deep Learning Models : Using non-intrusive network structure and binary alterations,Jonathan Pan,"Recent advancements in Artificial Intelligence namely in Deep Learning has heightened its adoption in many applications. Some are playing important roles to the extent that we are heavily dependent on them for our livelihood. However, as with all technologies, there are vulnerabilities that malicious actors could exploit. A form of exploitation is to turn these technologies, intended for good, to become dual-purposed instruments to support deviant acts like malicious software trojans. As part of proactive defense, researchers are proactively identifying such vulnerabilities so that protective measures could be developed subsequently. This research explores a novel blackbox trojanising approach using a simple network structure modification to any deep learning image classification model that would transform a benign model into a deviant one with a simple manipulation of the weights to induce specific types of errors. Propositions to protect the occurrence of such simple exploits are discussed in this research. This research highlights the importance of providing sufficient safeguards to these models so that the intended good of AI innovation and adoption may be protected. △ Less","2 August, 2020",https://arxiv.org/pdf/2008.00408
Opportunities and Challenges for Next Generation Computing,Gregory D. Hager;Mark D. Hill;Katherine Yelick,"Computing has dramatically changed nearly every aspect of our lives, from business and agriculture to communication and entertainment. As a nation, we rely on computing in the design of systems for energy, transportation and defense; and computing fuels scientific discoveries that will improve our fundamental understanding of the world and help develop solutions to major challenges in health and the environment. Computing has changed our world, in part, because our innovations can run on computers whose performance and cost-performance has improved a million-fold over the last few decades. A driving force behind this has been a repeated doubling of the transistors per chip, dubbed Moore's Law. A concomitant enabler has been Dennard Scaling that has permitted these performance doublings at roughly constant power, but, as we will see, both trends face challenges. Consider for a moment the impact of these two trends over the past 30 years. A 1980's supercomputer (e.g. a Cray 2) was rated at nearly 2 Gflops and consumed nearly 200 KW of power. At the time, it was used for high performance and national-scale applications ranging from weather forecasting to nuclear weapons research. A computer of similar performance now fits in our pocket and consumes less than 10 watts. What would be the implications of a similar computing/power reduction over the next 30 years - that is, taking a petaflop-scale machine (e.g. the Cray XK7 which requires about 500 KW for 1 Pflop (=1015 operations/sec) performance) and repeating that process? What is possible with such a computer in your pocket? How would it change the landscape of high capacity computing? In the remainder of this paper, we articulate some opportunities and challenges for dramatic performance improvements of both personal to national scale computing, and discuss some ""out of the box"" possibilities for achieving computing at this scale. △ Less","31 July, 2020",https://arxiv.org/pdf/2008.00023
A Review on the State of the Art in Non Contact Sensing for COVID-19,William Taylor;Qammer H. Abbasi;Kia Dashtipour;Shuja Ansari;Aziz Shah;Arslan Khan;Muhammad Ali Imran,"COVID-19 disease, caused by SARS-CoV-2, has resulted in a global pandemic recently. With no approved vaccination or treatment, governments around the world have issued guidance to their citizens to remain at home in efforts to control the spread of the disease. The goal of controlling the spread of the virus is to prevent strain on hospital. In this paper, we have focus on how non-invasive methods are being used to detect the COVID-19 and assist healthcare workers in caring for COVID-19 patients. Early detection of the COVID-19 virus can allow for early isolation to prevent further spread. This study outlines the advantages and disadvantages and a breakdown of the methods applied in the current state-of-the-art approaches. In addition, the paper highlights some future research directions, which are required to be explored further to come up with innovative technologies to control this pandemic. △ Less","28 July, 2020",https://arxiv.org/pdf/2007.16063
Deep-Learning based Inverse Modeling Approaches: A Subsurface Flow Example,Nanzhe Wang;Haibin Chang;Dongxiao Zhang,"Deep-learning has achieved good performance and shown great potential for solving forward and inverse problems. In this work, two categories of innovative deep-learning based inverse modeling methods are proposed and compared. The first category is deep-learning surrogate-based inversion methods, in which the Theory-guided Neural Network (TgNN) is constructed as a deep-learning surrogate for problems with uncertain model parameters. By incorporating physical laws and other constraints, the TgNN surrogate can be constructed with limited simulation runs and accelerate the inversion process significantly. Three TgNN surrogate-based inversion methods are proposed, including the gradient method, the iterative ensemble smoother (IES), and the training method. The second category is direct-deep-learning-inversion methods, in which TgNN constrained with geostatistical information, named TgNN-geo, is proposed for direct inverse modeling. In TgNN-geo, two neural networks are introduced to approximate the respective random model parameters and the solution. Since the prior geostatistical information can be incorporated, the direct-inversion method based on TgNN-geo works well, even in cases with sparse spatial measurements or imprecise prior statistics. Although the proposed deep-learning based inverse modeling methods are general in nature, and thus applicable to a wide variety of problems, they are tested with several subsurface flow problems. It is found that satisfactory results are obtained with a high efficiency. Moreover, both the advantages and disadvantages are further analyzed for the proposed two categories of deep-learning based inversion methods. △ Less","28 July, 2020",https://arxiv.org/pdf/2007.15580
The Making of 5G: Building an End-to-End 5G-Enabled System,Idelkys Quintana-Ramirez;Anthony Tsiopoulos;Maria A Lema;Fragkiskos Sardis;Luis Sequeira;James Arias;Aravindh Raman;Ali Azam;Mischa Dohler,"This article documents one of the world's first standards-compliant pre-commercial end-to-end 5th generation (5G) systems. Focus is on a standardized 5G architecture which includes the underlying 3GPP components but also the ETSI Network Function Virtualization (NFV) management and orchestration capabilities. The truly innovative character of 5G enabling fundamental changes to architecture and implementation is discussed, and details of monitoring and orchestration approaches that are deemed instrumental in unlocking the full potential of 5G. Finally, it is important to us to share the lessons learned which we hope are of use to industry and academia alike when building, deploying and testing emerging 5G systems. △ Less","30 July, 2020",https://arxiv.org/pdf/2007.15406
A Hybrid Adaptive Educational eLearning Project based on Ontologies Matching and Recommendation System,Vasiliki Demertzi;Konstantinos Demertzis,"The implementation of teaching interventions in learning needs has received considerable attention, as the provision of the same educational conditions to all students, is pedagogically ineffective. In contrast, more effectively considered the pedagogical strategies that adapt to the real individual skills of the students. An important innovation in this direction is the Adaptive Educational Systems (AES) that support automatic modeling study and adjust the teaching content on educational needs and students' skills. Effective utilization of these educational approaches can be enhanced with Artificial Intelligence (AI) technologies in order to the substantive content of the web acquires structure and the published information is perceived by the search engines. This study proposes a novel Adaptive Educational eLearning System (AEeLS) that has the capacity to gather and analyze data from learning repositories and to adapt these to the educational curriculum according to the student skills and experience. It is a novel hybrid machine learning system that combines a Semi-Supervised Classification method for ontology matching and a Recommendation Mechanism that uses a hybrid method from neighborhood-based collaborative and content-based filtering techniques, in order to provide a personalized educational environment for each student. △ Less","9 October, 2020",https://arxiv.org/pdf/2007.14771
Simulator as a Tool for the Future Maritime Education and Research: A Discussion,Yushan Pan;Arnfinn Oksavik;Hans Petter Hildre,"A few studies in the maritime domain utilize co-design in ship design workshops, however, none of them addresses a full picture of how co-design can make changes in simulation-based maritime education. In this paper, we reflect how co-design can help to foresight future skills in the maritime domain, especially on how to use simulators to support increasing competence of seafarers and in turn to redesign simulators to support maritime education. Thus, we address collaborative and innovative research activities, to enable all participants (seafarers, trainers, technicians, authorities etc.) to share their experiences so a joint recognition of needed future skills can be reached. Along with the ex-change of experiences, we assert that the supported simulations and simulator techniques could be designed to achieve sustainable growth for all participants. △ Less","29 July, 2020",https://arxiv.org/pdf/2007.14732
SparseTrain: Exploiting Dataflow Sparsity for Efficient Convolutional Neural Networks Training,Pengcheng Dai;Jianlei Yang;Xucheng Ye;Xingzhou Cheng;Junyu Luo;Linghao Song;Yiran Chen;Weisheng Zhao,"Training Convolutional Neural Networks (CNNs) usually requires a large number of computational resources. In this paper, \textit{SparseTrain} is proposed to accelerate CNN training by fully exploiting the sparsity. It mainly involves three levels of innovations: activation gradients pruning algorithm, sparse training dataflow, and accelerator architecture. By applying a stochastic pruning algorithm on each layer, the sparsity of back-propagation gradients can be increased dramatically without degrading training accuracy and convergence rate. Moreover, to utilize both \textit{natural sparsity} (resulted from ReLU or Pooling layers) and \textit{artificial sparsity} (brought by pruning algorithm), a sparse-aware architecture is proposed for training acceleration. This architecture supports forward and back-propagation of CNN by adopting 1-Dimensional convolution dataflow. We have built %a simple compiler to map CNNs topology onto \textit{SparseTrain}, and a cycle-accurate architecture simulator to evaluate the performance and efficiency based on the synthesized design with 14nm FinFET technologies. Evaluation results on AlexNet/ResNet show that \textit{SparseTrain} could achieve about 2.7 \times speedup and 2.2 \times energy efficiency improvement on average compared with the original training process. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.13595
"Identification, Tracking and Impact: Understanding the trade secret of catchphrases",Jagriti Jalal;Mayank Singh;Arindam Pal;Lipika Dey;Animesh Mukherjee,"Understanding the topical evolution in industrial innovation is a challenging problem. With the advancement in the digital repositories in the form of patent documents, it is becoming increasingly more feasible to understand the innovation secrets -- ""catchphrases"" of organizations. However, searching and understanding this enormous textual information is a natural bottleneck. In this paper, we propose an unsupervised method for the extraction of catchphrases from the abstracts of patents granted by the U.S. Patent and Trademark Office over the years. Our proposed system achieves substantial improvement, both in terms of precision and recall, against state-of-the-art techniques. As a second objective, we conduct an extensive empirical study to understand the temporal evolution of the catchphrases across various organizations. We also show how the overall innovation evolution in the form of introduction of newer catchphrases in an organization's patents correlates with the future citations received by the patents filed by that organization. Our code and data sets will be placed in the public domain soon. △ Less","20 July, 2020",https://arxiv.org/pdf/2007.13520
Attention-based Graph ResNet for Motor Intent Detection from Raw EEG signals,Shuyue Jia;Yimin Hou;Yan Shi;Yang Li,"In previous studies, decoding electroencephalography (EEG) signals has not considered the topological relationship of EEG electrodes. However, the latest neuroscience has suggested brain network connectivity. Thus, the exhibited interaction between EEG channels might not be appropriately measured via Euclidean distance. To fill the gap, an attention-based graph residual network, a novel structure of Graph Convolutional Neural Network (GCN), was presented to detect human motor intents from raw EEG signals, where the topological structure of EEG electrodes was built as a graph. Meanwhile, deep residual learning with a full-attention architecture was introduced to address the degradation problem concerning deeper networks in raw EEG motor imagery (MI) data. Individual variability, the critical and longstanding challenge underlying EEG signals, has been successfully handled with the state-of-the-art performance, 98.08% accuracy at the subject level, 94.28% for 20 subjects. Numerical results were promising that the implementation of the graph-structured topology was superior to decode raw EEG data. The innovative deep learning approach was expected to entail a universal method towards both neuroscience research and real-world EEG-based practical applications, e.g., seizure prediction. △ Less","25 June, 2020",https://arxiv.org/pdf/2007.13484
Trick the Body Trick the Mind: Avatar representation affects the perception of available action possibilities in Virtual Reality,Tugce Akkoc;Emre Ugur;Inci Ayhan,"In immersive Virtual Reality (VR), your brain can trick you into believing that your virtual hands are your real hands. Manipulating the representation of the body, namely the avatar, is a potentially powerful tool for the design of innovative interactive systems in VR. In this study, we investigated interactive behavior in VR by using the methods of experimental psychology. Objects with handles are known to potentiate the afforded action. Participants tend to respond faster when the handle is on the same side as the responding hand in bi-manual speed response tasks. In the first experiment, we successfully replicated this affordance effect in a Virtual Reality (VR) setting. In the second experiment, we showed that the affordance effect was influenced by the avatar, which was manipulated by two different hand types: 1) hand models with full finger tracking that are able to grasp objects, and 2) capsule-shaped -- fingerless -- hand models that are not able to grasp objects. We found that less than 5 minutes of adaptation to an avatar, significantly altered the affordance perception. Counter intuitively, action planning was significantly shorter with the hand model that is not able to grasp. Possibly, fewer action possibilities provided an advantage in processing time. The presence of a handle speeded up the initiation of the hand movement but slowed down the action completion because of ongoing action planning. The results were examined from a multidisciplinary perspective and the design implications for VR applications were discussed. △ Less","25 July, 2020",https://arxiv.org/pdf/2007.13048
Hide-and-Seek Privacy Challenge,James Jordon;Daniel Jarrett;Jinsung Yoon;Tavian Barnes;Paul Elbers;Patrick Thoral;Ari Ercole;Cheng Zhang;Danielle Belgrave;Mihaela van der Schaar,"The clinical time-series setting poses a unique combination of challenges to data modeling and sharing. Due to the high dimensionality of clinical time series, adequate de-identification to preserve privacy while retaining data utility is difficult to achieve using common de-identification techniques. An innovative approach to this problem is synthetic data generation. From a technical perspective, a good generative model for time-series data should preserve temporal dynamics, in the sense that new sequences respect the original relationships between high-dimensional variables across time. From the privacy perspective, the model should prevent patient re-identification by limiting vulnerability to membership inference attacks. The NeurIPS 2020 Hide-and-Seek Privacy Challenge is a novel two-tracked competition to simultaneously accelerate progress in tackling both problems. In our head-to-head format, participants in the synthetic data generation track (i.e. ""hiders"") and the patient re-identification track (i.e. ""seekers"") are directly pitted against each other by way of a new, high-quality intensive care time-series dataset: the AmsterdamUMCdb dataset. Ultimately, we seek to advance generative techniques for dense and high-dimensional temporal data streams that are (1) clinically meaningful in terms of fidelity and predictivity, as well as (2) capable of minimizing membership privacy risks in terms of the concrete notion of patient re-identification. △ Less","24 July, 2020",https://arxiv.org/pdf/2007.12087
Accurate RGB-D Salient Object Detection via Collaborative Learning,Wei Ji;Jingjing Li;Miao Zhang;Yongri Piao;Huchuan Lu,"Benefiting from the spatial cues embedded in depth images, recent progress on RGB-D saliency detection shows impressive ability on some challenge scenarios. However, there are still two limitations. One hand is that the pooling and upsampling operations in FCNs might cause blur object boundaries. On the other hand, using an additional depth-network to extract depth features might lead to high computation and storage cost. The reliance on depth inputs during testing also limits the practical applications of current RGB-D models. In this paper, we propose a novel collaborative learning framework where edge, depth and saliency are leveraged in a more efficient way, which solves those problems tactfully. The explicitly extracted edge information goes together with saliency to give more emphasis to the salient regions and object boundaries. Depth and saliency learning is innovatively integrated into the high-level feature learning process in a mutual-benefit manner. This strategy enables the network to be free of using extra depth networks and depth inputs to make inference. To this end, it makes our model more lightweight, faster and more versatile. Experiment results on seven benchmark datasets show its superior performance. △ Less","23 July, 2020",https://arxiv.org/pdf/2007.11782
Yggdrasil: Privacy-aware Dual Deduplication in Multi Client Settings,Hadi Sehat;Elena Pagnin;Daniel E. Lucani,"This paper proposes Yggdrasil, a protocol for privacy-aware dual data deduplication in multi client settings. Yggdrasil is designed to reduce the cloud storage space while safeguarding the privacy of the client's outsourced data. Yggdrasil combines three innovative tools to achieve this goal. First, generalized deduplication, an emerging technique to reduce data footprint. Second, non-deterministic transformations that are described compactly and improve the degree of data compression in the Cloud (across users). Third, data preprocessing in the clients in the form of lightweight, privacy-driven transformations prior to upload. This guarantees that an honest-but-curious Cloud service trying to retrieve the client's actual data will face a high degree of uncertainty as to what the original data is. We provide a mathematical analysis of the measure of uncertainty as well as the compression potential of our protocol. Our experiments with a HDFS log data set shows that 49% overall compression can be achieved, with clients storing only 12% for privacy and the Cloud storing the rest. This is achieved while ensuring that each fragment uploaded to the Cloud would have 10^296 possible original strings from the client. Higher uncertainty is possible, with some reduction of compression potential. △ Less","22 July, 2020",https://arxiv.org/pdf/2007.11403
Real-Time Instrument Segmentation in Robotic Surgery using Auxiliary Supervised Deep Adversarial Learning,Mobarakol Islam;Daniel A. Atputharuban;Ravikiran Ramesh;Hongliang Ren,"Robot-assisted surgery is an emerging technology which has undergone rapid growth with the development of robotics and imaging systems. Innovations in vision, haptics and accurate movements of robot arms have enabled surgeons to perform precise minimally invasive surgeries. Real-time semantic segmentation of the robotic instruments and tissues is a crucial step in robot-assisted surgery. Accurate and efficient segmentation of the surgical scene not only aids in the identification and tracking of instruments but also provided contextual information about the different tissues and instruments being operated with. For this purpose, we have developed a light-weight cascaded convolutional neural network (CNN) to segment the surgical instruments from high-resolution videos obtained from a commercial robotic system. We propose a multi-resolution feature fusion module (MFF) to fuse the feature maps of different dimensions and channels from the auxiliary and main branch. We also introduce a novel way of combining auxiliary loss and adversarial loss to regularize the segmentation model. Auxiliary loss helps the model to learn low-resolution features, and adversarial loss improves the segmentation prediction by learning higher order structural information. The model also consists of a light-weight spatial pyramid pooling (SPP) unit to aggregate rich contextual information in the intermediate stage. We show that our model surpasses existing algorithms for pixel-wise segmentation of surgical instruments in both prediction accuracy and segmentation time of high-resolution videos. △ Less","30 September, 2020",https://arxiv.org/pdf/2007.11319
Time-aware Graph Embedding: A temporal smoothness and task-oriented approach,Yonghui Xu;Shengjie Sun;Yuan Miao;Dong Yang;Xiaonan Meng;Yi Hu;Ke Wang;Hengjie Song;Chuanyan Miao,"Knowledge graph embedding, which aims to learn the low-dimensional representations of entities and relationships, has attracted considerable research efforts recently. However, most knowledge graph embedding methods focus on the structural relationships in fixed triples while ignoring the temporal information. Currently, existing time-aware graph embedding methods only focus on the factual plausibility, while ignoring the temporal smoothness which models the interactions between a fact and its contexts, and thus can capture fine-granularity temporal relationships. This leads to the limited performance of embedding related applications. To solve this problem, this paper presents a Robustly Time-aware Graph Embedding (RTGE) method by incorporating temporal smoothness. Two major innovations of our paper are presented here. At first, RTGE integrates a measure of temporal smoothness in the learning process of the time-aware graph embedding. Via the proposed additional smoothing factor, RTGE can preserve both structural information and evolutionary patterns of a given graph. Secondly, RTGE provides a general task-oriented negative sampling strategy associated with temporally-aware information, which further improves the adaptive ability of the proposed algorithm and plays an essential role in obtaining superior performance in various tasks. Extensive experiments conducted on multiple benchmark tasks show that RTGE can increase performance in entity/relationship/temporal scoping prediction tasks. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.11164
Garment Design with Generative Adversarial Networks,Chenxi Yuan;Mohsen Moghaddam,"The designers' tendency to adhere to a specific mental set and heavy emotional investment in their initial ideas often hinder their ability to innovate during the design thinking and ideation process. In the fashion industry, in particular, the growing diversity of customers' needs, the intense global competition, and the shrinking time-to-market (a.k.a., ""fast fashion"") further exacerbate this challenge for designers. Recent advances in deep generative models have created new possibilities to overcome the cognitive obstacles of designers through automated generation and/or editing of design concepts. This paper explores the capabilities of generative adversarial networks (GAN) for automated attribute-level editing of design concepts. Specifically, attribute GAN (AttGAN)---a generative model proven successful for attribute editing of human faces---is utilized for automated editing of the visual attributes of garments and tested on a large fashion dataset. The experiments support the hypothesized potentials of GAN for attribute-level editing of design concepts, and underscore several key limitations and research questions to be addressed in future work. △ Less","22 July, 2020",https://arxiv.org/pdf/2007.10947
TCIM: Triangle Counting Acceleration With Processing-In-MRAM Architecture,Xueyan Wang;Jianlei Yang;Yinglin Zhao;Yingjie Qi;Meichen Liu;Xingzhou Cheng;Xiaotao Jia;Xiaoming Chen;Gang Qu;Weisheng Zhao,"Triangle counting (TC) is a fundamental problem in graph analysis and has found numerous applications, which motivates many TC acceleration solutions in the traditional computing platforms like GPU and FPGA. However, these approaches suffer from the bandwidth bottleneck because TC calculation involves a large amount of data transfers. In this paper, we propose to overcome this challenge by designing a TC accelerator utilizing the emerging processing-in-MRAM (PIM) architecture. The true innovation behind our approach is a novel method to perform TC with bitwise logic operations (such as \texttt{AND}), instead of the traditional approaches such as matrix computations. This enables the efficient in-memory implementations of TC computation, which we demonstrate in this paper with computational Spin-Transfer Torque Magnetic RAM (STT-MRAM) arrays. Furthermore, we develop customized graph slicing and mapping techniques to speed up the computation and reduce the energy consumption. We use a device-to-architecture co-simulation framework to validate our proposed TC accelerator. The results show that our data mapping strategy could reduce 99.99\% of the computation and 72\% of the memory \texttt{WRITE} operations. Compared with the existing GPU or FPGA accelerators, our in-memory accelerator achieves speedups of 9\times and 23.4\times, respectively, and a 20.6\times energy efficiency improvement over the FPGA accelerator. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.10702
UVMBench: A Comprehensive Benchmark Suite for Researching Unified Virtual Memory in GPUs,Yongbin Gu;Wenxuan Wu;Yunfan Li;Lizhong Chen,"The recent introduction of Unified Virtual Memory (UVM) in GPUs offers a new programming model that allows GPUs and CPUs to share the same virtual memory space, which shifts the complex memory management from programmers to GPU driver/ hardware and enables kernel execution even when memory is oversubscribed. Meanwhile, UVM may also incur considerable performance overhead due to tracking and data migration along with special handling of page faults and page table walk. As UVM is attracting significant attention from the research community to develop innovative solutions to these problems, in this paper, we propose a comprehensive UVM benchmark suite named UVMBench to facilitate future research on this important topic. The proposed UVMBench consists of 32 representative benchmarks from a wide range of application domains. The suite also features unified programming implementation and diverse memory access patterns across benchmarks, thus allowing thorough evaluation and comparison with current state-of-the-art. A set of experiments have been conducted on real GPUs to verify and analyze the benchmark suite behaviors under various scenarios. △ Less","20 October, 2020",https://arxiv.org/pdf/2007.09822
Mapping computational thinking mindsets between educational levels with cognitive network science,Massimo Stella;Anastasiya Kapuza;Catherine Cramer;Stephen Uzzo,"Computational thinking is a way of reasoning about the world in terms of data. This mindset channels number crunching toward an ambition to discover knowledge through logic, models and simulations. Here we show how computational cognitive science can be used to reconstruct and analyse the structure of computational thinking mindsets (forma mentis in Latin) through complex networks. As a case study, we investigate cognitive networks tied to key concepts of computational thinking provided by: (i) 159 high school students enrolled in a science curriculum and (ii) 59 researchers in complex systems and simulations. Researchers' reconstructed forma mentis highlighted a positive mindset about scientific modelling, semantically framing data and simulations as ways of discovering nature. Students correctly identified different aspects of logic reasoning but perceived ""computation"" as a distressing, anxiety-eliciting task, framed with math jargon and lacking links to real-world discovery. Students' mindsets around ""data"", ""model"" and ""simulations"" critically revealed no awareness of numerical modelling as a way for understanding the world. Our findings provide evidence of a crippled computational thinking mindset in students, who acquire mathematical skills that are not channelled toward real-world discovery through coding. This unlinked knowledge ends up being perceived as distressing number-crunching expertise with no relevant outcome. The virtuous mindset of researchers reported here indicates that computational thinking can be restored by training students specifically in coding, modelling and simulations in relation to discovering nature. Our approach opens innovative ways for quantifying computational thinking and enhancing its development through mindset reconstruction. △ Less","18 July, 2020",https://arxiv.org/pdf/2007.09402
RISMA: Reconfigurable Intelligent Surfaces Enabling Beamforming for IoT Massive Access,Mursia;Placido;Sciancalepore;Vincenzo;Garcia-Saavedra;Andres;Cottatellucci;Laura;Costa-Perez;Xavier;Gesbert;David,"Massive access for Internet-of-Things (IoT) in beyond 5G networks represents a daunting challenge for conventional bandwidth-limited technologies. Millimeter-wave technologies (mmWave)---which provide large chunks of bandwidth at the cost of more complex wireless processors in harsher radio environments---is a promising alternative to accommodate massive IoT but its cost and power requirements are an obstacle for wide adoption in practice. In this context, meta-materials arise as a key innovation enabler to address this challenge by Re-configurable Intelligent Surfaces (RISs). In this paper we take on the challenge and study a beyond 5G scenario consisting of a multi-antenna base station (BS) serving a large set of single-antenna user equipments (UEs) with the aid of RISs to cope with non-line-of-sight paths. Specifically, we build a mathematical framework to jointly optimize the precoding strategy of the BS and the RIS parameters in order to minimize the system sum mean squared error (SMSE). This novel approach reveals convenient properties used to design two algorithms, RISMA and Lo-RISMA, which are able to either find simple and efficient solutions to our problem (the former) or accommodate practical constraints with low-resolution RISs (the latter). Numerical results show that our algorithms outperform conventional benchmarks that do not employ RIS (even with low-resolution meta-surfaces) with gains that span from 20% to 120% in sum rate performance. △ Less","18 July, 2020",https://arxiv.org/pdf/2007.09386
Semi-Supervised Learning Approach to Discover Enterprise User Insights from Feedback and Support,Xin Deng;Ross Smith;Genevieve Quintin,"With the evolution of the cloud and customer centric culture, we inherently accumulate huge repositories of textual reviews, feedback, and support data.This has driven enterprises to seek and research engagement patterns, user network analysis, topic detections, etc.However, huge manual work is still necessary to mine data to be able to mine actionable outcomes. In this paper, we proposed and developed an innovative Semi-Supervised Learning approach by utilizing Deep Learning and Topic Modeling to have a better understanding of the user voice.This approach combines a BERT-based multiclassification algorithm through supervised learning combined with a novel Probabilistic and Semantic Hybrid Topic Inference (PSHTI) Model through unsupervised learning, aiming at automating the process of better identifying the main topics or areas as well as the sub-topics from the textual feedback and support.There are three major break-through: 1. As the advancement of deep learning technology, there have been tremendous innovations in the NLP field, yet the traditional topic modeling as one of the NLP applications lag behind the tide of deep learning. In the methodology and technical perspective, we adopt transfer learning to fine-tune a BERT-based multiclassification system to categorize the main topics and then utilize the novel PSHTI model to infer the sub-topics under the predicted main topics. 2. The traditional unsupervised learning-based topic models or clustering methods suffer from the difficulty of automatically generating a meaningful topic label, but our system enables mapping the top words to the self-help issues by utilizing domain knowledge about the product through web-crawling. 3. This work provides a prominent showcase by leveraging the state-of-the-art methodology in the real production to help shed light to discover user insights and drive business investment priorities. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.09303
Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance,Minghua Liu;Xiaoshuai Zhang;Hao Su,"We are interested in reconstructing the mesh representation of object surfaces from point clouds. Surface reconstruction is a prerequisite for downstream applications such as rendering, collision avoidance for planning, animation, etc. However, the task is challenging if the input point cloud has a low resolution, which is common in real-world scenarios (e.g., from LiDAR or Kinect sensors). Existing learning-based mesh generative methods mostly predict the surface by first building a shape embedding that is at the whole object level, a design that causes issues in generating fine-grained details and generalizing to unseen categories. Instead, we propose to leverage the input point cloud as much as possible, by only adding connectivity information to existing points. Particularly, we predict which triplets of points should form faces. Our key innovation is a surrogate of local connectivity, calculated by comparing the intrinsic/extrinsic metrics. We learn to predict this surrogate using a deep point cloud network and then feed it to an efficient post-processing module for high-quality mesh generation. We demonstrate that our method can not only preserve details, handle ambiguous structures, but also possess strong generalizability to unseen categories by experiments on synthetic and real data. The code is available at https://github.com/Colin97/Point2Mesh. △ Less","30 September, 2020",https://arxiv.org/pdf/2007.09267
LEED: Label-Free Expression Editing via Disentanglement,Rongliang Wu;Shijian Lu,"Recent studies on facial expression editing have obtained very promising progress. On the other hand, existing methods face the constraint of requiring a large amount of expression labels which are often expensive and time-consuming to collect. This paper presents an innovative label-free expression editing via disentanglement (LEED) framework that is capable of editing the expression of both frontal and profile facial images without requiring any expression label. The idea is to disentangle the identity and expression of a facial image in the expression manifold, where the neutral face captures the identity attribute and the displacement between the neutral image and the expressive image captures the expression attribute. Two novel losses are designed for optimal expression disentanglement and consistent synthesis, including a mutual expression information loss that aims to extract pure expression-related features and a siamese loss that aims to enhance the expression similarity between the synthesized image and the reference image. Extensive experiments over two public facial expression datasets show that LEED achieves superior facial expression editing qualitatively and quantitatively. △ Less","17 July, 2020",https://arxiv.org/pdf/2007.08971
Deep ahead-of-threat virtual patching,Fady Copty;Andre Kassis;Sharon Keidar-Barner;Dov Murik,"Many applications have security vulnerabilities that can be exploited. It is practically impossible to find all of them due to the NP-complete nature of the testing problem. Security solutions provide defenses against these attacks through continuous application testing, fast-patching of vulnerabilities, automatic deployment of patches, and virtual patching detection techniques deployed in network and endpoint security tools. These techniques are limited by the need to find vulnerabilities before the black-hats. We propose an innovative technique to virtually patch vulnerabilities before they are found. We leverage testing techniques for supervised-learning data generation, and show how artificial intelligence techniques can use this data to create predictive deep neural-network models that read an application's input and predict in real time whether it is a potential malicious input. We set up an ahead-of-threat experiment in which we generated data on old versions of an application, and then evaluated the predictive model accuracy on vulnerabilities found years later. Our experiments show ahead-of-threat detection on LibXML2 and LibTIFF vulnerabilities with 91.3% and 93.7% accuracy, respectively. We expect to continue work on this field of research and provide ahead-of-threat virtual patching for more libraries. Success in this research can change the current state of endless racing after application vulnerabilities and put the defenders one step ahead of the attackers △ Less","16 July, 2020",https://arxiv.org/pdf/2007.08296
Collaborative Unsupervised Domain Adaptation for Medical Image Diagnosis,Yifan Zhang;Ying Wei;Qingyao Wu;Peilin Zhao;Shuaicheng Niu;Junzhou Huang;Mingkui Tan,"Deep learning based medical image diagnosis has shown great potential in clinical medicine. However, it often suffers two major difficulties in real-world applications: 1) only limited labels are available for model training, due to expensive annotation costs over medical images; 2) labeled images may contain considerable label noise (e.g., mislabeling labels) due to diagnostic difficulties of diseases. To address these, we seek to exploit rich labeled data from relevant domains to help the learning in the target task via {Unsupervised Domain Adaptation} (UDA). Unlike most UDA methods that rely on clean labeled data or assume samples are equally transferable, we innovatively propose a Collaborative Unsupervised Domain Adaptation algorithm, which conducts transferability-aware adaptation and conquers label noise in a collaborative way. We theoretically analyze the generalization performance of the proposed method, and also empirically evaluate it on both medical and general images. Promising experimental results demonstrate the superiority and generalization of the proposed method. △ Less","5 July, 2020",https://arxiv.org/pdf/2007.07222
Causal Inference using Gaussian Processes with Structured Latent Confounders,Sam Witty;Kenta Takatsu;David Jensen;Vikash Mansinghka,"Latent confounders---unobserved variables that influence both treatment and outcome---can bias estimates of causal effects. In some cases, these confounders are shared across observations, e.g. all students taking a course are influenced by the course's difficulty in addition to any educational interventions they receive individually. This paper shows how to semiparametrically model latent confounders that have this structure and thereby improve estimates of causal effects. The key innovations are a hierarchical Bayesian model, Gaussian processes with structured latent confounders (GP-SLC), and a Monte Carlo inference algorithm for this model based on elliptical slice sampling. GP-SLC provides principled Bayesian uncertainty estimates of individual treatment effect with minimal assumptions about the functional forms relating confounders, covariates, treatment, and outcome. Finally, this paper shows GP-SLC is competitive with or more accurate than widely used causal inference techniques on three benchmark datasets, including the Infant Health and Development Program and a dataset showing the effect of changing temperatures on state-wide energy consumption across New England. △ Less","14 July, 2020",https://arxiv.org/pdf/2007.07127
Questionnaire analysis to define the most suitable survey for port-noise investigation,Andrea Cerniglia;Davide Chiarella;Paola Cutugno;Lucia Marconi;Anna Magrini;Gelsomina Di Feo;Melissa Ferretti,"The high level of noise pollution affecting the areas between ports and logistic platforms represents a problem that can be faced from different points of view. Acoustic monitoring, mapping, short-term measurements, port and road traffic flows analyses can give useful indications on the strategies to be proposed for a better management of the problem. A survey campaign through the preparation of questionnaires to be submitted to the population exposed to noise in the back-port areas will help to better understand the subjective point of view. The paper analyses a sample of questions suitable for the specific research, chosen as part of the wide database of questionnaires internationally proposed for subjective investigations. The preliminary results of a first data collection campaign are considered to verify the adequacy of the number, the type of questions, and the type of sample noise used for the survey. The questionnaire will be optimized to be distributed in the TRIPLO project (TRansports and Innovative sustainable connections between Ports and LOgistic platforms). The results of this survey will be the starting point for the linguistic investigation carried out in combination with the acoustic monitoring, to improve understanding the connections between personal feeling and technical aspects. △ Less","14 July, 2020",https://arxiv.org/pdf/2007.06915
"Perfectly Sampling k\geq (8/3 +o(1))Δ
-Colorings in Graphs",Vishesh Jain;Ashwin Sah;Mehtaab Sawhney,"We present a randomized algorithm which takes as input an undirected graph G on n vertices with maximum degree Δ, and a number of colors k \geq (8/3 + o_Δ(1))Δ, and returns -- in expected time \tilde{O}(nΔ^{2}\log{k}) -- a proper k-coloring of G distributed perfectly uniformly on the set of all proper k-colorings of G. Notably, our sampler breaks the barrier at k = 3Δ encountered in recent work of Bhandari and Chakraborty [STOC 2020]. We also sketch how to modify our methods to relax the restriction on k to k \geq (8/3 - ε_0)Δ for an absolute constant ε_0 > 0. As in the work of Bhandari and Chakraborty, and the pioneering work of Huber [STOC 1998], our sampler is based on Coupling from the Past [Propp&Wilson, Random Struct. Algorithms, 1995] and the bounding chain method [Huber, STOC 1998; Häggström&Nelander, Scand. J. Statist., 1999]. Our innovations include a novel bounding chain routine inspired by Jerrum's analysis of the Glauber dynamics [Random Struct. Algorithms, 1995], as well as a preconditioning routine for bounding chains which uses the algorithmic Lovász Local Lemma [Moser&Tardos, J.ACM, 2010]. △ Less","13 July, 2020",https://arxiv.org/pdf/2007.06360
Radium: Improving Dynamic PoW Targeting,George Bissias,"Most PoW blockchain protocols operate with a simple mechanism whereby a threshold is set for each block and miners generate block hashes until one of those values falls below the threshold. Although largely effective, this mechanism produces blocks at a highly variable rate and also leaves a blockchain susceptible to chain death, i.e. abandonment in the event that the threshold is set too high to attract any miners. A recent innovation called real-time block rate targeting, or RTT, fixes these problems by reducing the target throughout the mining interval. RTT exhibits much less variable block times and even features the ability to fully adjust the target after each block. However, as we show in this paper, RTT also suffers from a critical vulnerability whereby miners deviate form the protocol to increase their profits. We introduce the Radium protocol, which mitigates this vulnerability in RTT while retaining lower variance block times, responsive target adjustment, and lowering the risk of chain death. We also show that Radium's susceptibility to the doublespend attack and orphaned blocks remains similar to Bitcoin. △ Less","12 July, 2020",https://arxiv.org/pdf/2007.05991
Exploit the potential of Multi-column architecture for Crowd Counting,Junhao Cheng;Zhuojun Chen;XinYu Zhang;Yizhou Li;Xiaoyuan Jing,"Crowd counting is an important yet challenging task in computer vision due to serious occlusions, complex background and large scale variations, etc. Multi-column architecture is widely adopted to overcome these challenges, yielding state-of-the-art performance in many public benchmarks. However, there still are two issues in such design: scale limitation and feature similarity. Further performance improvements are thus restricted. In this paper, we propose a novel crowd counting framework called Pyramid Scale Network (PSNet) to explicitly address these issues. Specifically, for scale limitation, we adopt three Pyramid Scale Modules (PSM) to efficiently capture multi-scale features, which integrate a message passing mechanism and an attention mechanism into multi-column architecture. Moreover, for feature similarity, a novel loss function named Multi-column variance loss is introduced to make the features learned by each column in PSM appropriately different from each other. To the best of our knowledge, PSNet is the first work to explicitly address scale limitation and feature similarity in multi-column design. Extensive experiments on five benchmark datasets demonstrate the effectiveness of the proposed innovations as well as the superior performance over the state-of-the-art. Our code is publicly available at: https://github.com/oahunc/Pyramid_Scale_Network △ Less","28 July, 2020",https://arxiv.org/pdf/2007.05779
Graph Neural Networks for Massive MIMO Detection,Andrea Scotti;Nima N. Moghadam;Dong Liu;Karl Gafvert;Jinliang Huang,"In this paper, we innovately use graph neural networks (GNNs) to learn a message-passing solution for the inference task of massive multiple multiple-input multiple-output (MIMO) detection in wireless communication. We adopt a graphical model based on the Markov random field (MRF) where belief propagation (BP) yields poor results when it assumes a uniform prior over the transmitted symbols. Numerical simulations show that, under the uniform prior assumption, our GNN-based MIMO detection solution outperforms the minimum mean-squared error (MMSE) baseline detector, in contrast to BP. Furthermore, experiments demonstrate that the performance of the algorithm slightly improves by incorporating MMSE information into the prior. △ Less","11 July, 2020",https://arxiv.org/pdf/2007.05703
Deep Contextual Clinical Prediction with Reverse Distillation,Rohan S. Kodialam;Rebecca Boiarsky;Justin Lim;Neil Dixit;Aditya Sai;David Sontag,"Healthcare providers are increasingly using machine learning to predict patient outcomes to make meaningful interventions. However, despite innovations in this area, deep learning models often struggle to match performance of shallow linear models in predicting these outcomes, making it difficult to leverage such techniques in practice. In this work, motivated by the task of clinical prediction from insurance claims, we present a new technique called Reverse Distillation which pretrains deep models by using high-performing linear models for initialization. We make use of the longitudinal structure of insurance claims datasets to develop Self Attention with Reverse Distillation, or SARD, an architecture that utilizes a combination of contextual embedding, temporal embedding and self-attention mechanisms and most critically is trained via reverse distillation. SARD outperforms state-of-the-art methods on multiple clinical prediction outcomes, with ablation studies revealing that reverse distillation is a primary driver of these improvements. Code is available at https://github.com/clinicalml/omop-learn. △ Less","16 December, 2020",https://arxiv.org/pdf/2007.05611
Neuromorphic Processing and Sensing: Evolutionary Progression of AI to Spiking,Philippe Reiter;Geet Rose Jose;Spyridon Bizmpikis;Ionela-Ancuţa Cîrjilă,"The increasing rise in machine learning and deep learning applications is requiring ever more computational resources to successfully meet the growing demands of an always-connected, automated world. Neuromorphic technologies based on Spiking Neural Network algorithms hold the promise to implement advanced artificial intelligence using a fraction of the computations and power requirements by modeling the functioning, and spiking, of the human brain. With the proliferation of tools and platforms aiding data scientists and machine learning engineers to develop the latest innovations in artificial and deep neural networks, a transition to a new paradigm will require building from the current well-established foundations. This paper explains the theoretical workings of neuromorphic technologies based on spikes, and overviews the state-of-art in hardware processors, software platforms and neuromorphic sensing devices. A progression path is paved for current machine learning specialists to update their skillset, as well as classification or predictive models from the current generation of deep neural networks to SNNs. This can be achieved by leveraging existing, specialized hardware in the form of SpiNNaker and the Nengo migration toolkit. First-hand, experimental results of converting a VGG-16 neural network to an SNN are shared. A forward gaze into industrial, medical and commercial applications that can readily benefit from SNNs wraps up this investigation into the neuromorphic computing future. △ Less","10 July, 2020",https://arxiv.org/pdf/2007.05606
Prediction of Traffic Flow via Connected Vehicles,Ranwa Al Mallah;Bilal Farooq;Alejandro Quintero,"We propose a Short-term Traffic flow Prediction (STP) framework so that transportation authorities take early actions to control flow and prevent congestion. We anticipate flow at future time frames on a target road segment based on historical flow data and innovative features such as real time feeds and trajectory data provided by Connected Vehicles (CV) technology. To cope with the fact that existing approaches do not adapt to variation in traffic, we show how this novel approach allows advanced modelling by integrating into the forecasting of flow, the impact of the various events that CV realistically encountered on segments along their trajectory. We solve the STP problem with a Deep Neural Networks (DNN) in a multitask learning setting augmented by input from CV. Results show that our approach, namely MTL-CV, with an average Root-Mean-Square Error (RMSE) of 0.052, outperforms state-of-the-art ARIMA time series (RMSE of 0.255) and baseline classifiers (RMSE of 0.122). Compared to single task learning with Artificial Neural Network (ANN), ANN had a lower performance, 0.113 for RMSE, than MTL-CV. MTL-CV learned historical similarities between segments, in contrast to using direct historical trends in the measure, because trends may not exist in the measure but do in the similarities. △ Less","4 December, 2020",https://arxiv.org/pdf/2007.05460
Deep Surrogate Models for Multi-dimensional Regression of Reactor Power,Akshay J. Dave;Jarod Wilson;Kaichao Sun,"There is renewed interest in developing small modular reactors and micro-reactors. Innovation is necessary in both construction and operation methods of these reactors to be financially attractive. For operation, an area of interest is the development of fully autonomous reactor control. Significant efforts are necessary to demonstrate an autonomous control framework for a nuclear system, while adhering to established safety criteria. Our group has proposed and received support for demonstration of an autonomous framework on a subcritical system: the MIT Graphite Exponential Pile. In order to have a fast response (on the order of miliseconds), we must extract specific capabilities of general-purpose system codes to a surrogate model. Thus, we have adopted current state-of-the-art neural network libraries to build surrogate models. This work focuses on establishing the capability of neural networks to provide an accurate and precise multi-dimensional regression of a nuclear reactor's power distribution. We assess using a neural network surrogate against a previously validated model: an MCNP5 model of the MIT reactor. The results indicate that neural networks are an appropriate choice for surrogate models to implement in an autonomous reactor control framework. The MAPE across all test datasets was < 1.16 % with a corresponding standard deviation of < 0.77 %. The error is low, considering that the node-wise fission power can vary from 7 kW to 30 kW across the core. △ Less","13 July, 2020",https://arxiv.org/pdf/2007.05435
Senior Living Communities: Made Safer by AI,Ashutosh Saxena;David R Cheriton,"There is a historically unprecedented shift in demographics towards seniors, which will result in significant housing development over the coming decade. This is an enormous opportunity for real-estate operators to innovate and address the demand in this growing market. However, investments in this area are fraught with risk. Seniors often have more health issues, and Covid-19 has exposed just how vulnerable they are -- especially those living in close proximity. Conventionally, most services for seniors are ""high-touch"", requiring close physical contact with trained caregivers. Not only are trained caregivers short in supply, but the pandemic has made it evident that conventional high-touch approaches to senior care are high-cost and greater risk. There are not enough caregivers to meet the needs of this emerging demographic, and even fewer who want to undertake the additional training and risk of working in a senior facility, especially given the current pandemic. In this article, we rethink the design of senior living facilities to mitigate the risks and costs using automation. With AI-enabled pervasive automation, we claim there is an opportunity, if not an urgency, to go from high-touch to almost ""no touch"" while dramatically reducing risk and cost. Although our vision goes beyond the current reality, we cite measurements from Caspar AI-enabled senior properties that show the potential benefit of this approach. △ Less","16 July, 2020",https://arxiv.org/pdf/2007.05129
Automated Chest CT Image Segmentation of COVID-19 Lung Infection based on 3D U-Net,Dominik Müller;Iñaki Soto Rey;Frank Kramer,"The coronavirus disease 2019 (COVID-19) affects billions of lives around the world and has a significant impact on public healthcare. Due to rising skepticism towards the sensitivity of RT-PCR as screening method, medical imaging like computed tomography offers great potential as alternative. For this reason, automated image segmentation is highly desired as clinical decision support for quantitative assessment and disease monitoring. However, publicly available COVID-19 imaging data is limited which leads to overfitting of traditional approaches. To address this problem, we propose an innovative automated segmentation pipeline for COVID-19 infected regions, which is able to handle small datasets by utilization as variant databases. Our method focuses on on-the-fly generation of unique and random image patches for training by performing several preprocessing methods and exploiting extensive data augmentation. For further reduction of the overfitting risk, we implemented a standard 3D U-Net architecture instead of new or computational complex neural network architectures. Through a 5-fold cross-validation on 20 CT scans of COVID-19 patients, we were able to develop a highly accurate as well as robust segmentation model for lungs and COVID-19 infected regions without overfitting on the limited data. Our method achieved Dice similarity coefficients of 0.956 for lungs and 0.761 for infection. We demonstrated that the proposed method outperforms related approaches, advances the state-of-the-art for COVID-19 segmentation and improves medical image analysis with limited data. The code and model are available under the following link: https://github.com/frankkramer-lab/covid19.MIScnn △ Less","24 June, 2020",https://arxiv.org/pdf/2007.04774
Brain Tumor Anomaly Detection via Latent Regularized Adversarial Network,Nan Wang;Chengwei Chen;Yuan Xie;Lizhuang Ma,"With the development of medical imaging technology, medical images have become an important basis for doctors to diagnose patients. The brain structure in the collected data is complicated, thence, doctors are required to spend plentiful energy when diagnosing brain abnormalities. Aiming at the imbalance of brain tumor data and the rare amount of labeled data, we propose an innovative brain tumor abnormality detection algorithm. The semi-supervised anomaly detection model is proposed in which only healthy (normal) brain images are trained. Model capture the common pattern of the normal images in the training process and detect anomalies based on the reconstruction error of latent space. Furthermore, the method first uses singular value to constrain the latent space and jointly optimizes the image space through multiple loss functions, which make normal samples and abnormal samples more separable in the feature-level. This paper utilizes BraTS, HCP, MNIST, and CIFAR-10 datasets to comprehensively evaluate the effectiveness and practicability. Extensive experiments on intra- and cross-dataset tests prove that our semi-supervised method achieves outperforms or comparable results to state-of-the-art supervised techniques. △ Less","9 July, 2020",https://arxiv.org/pdf/2007.04734
How URLLC can Benefit from NOMA-based Retransmissions,Radosław Kotaba;Carles Navarro Manchón;Tommaso Balercia;Petar Popovski,"Among the new types of connectivity unleashed by the emerging 5G wireless systems, Ultra-Reliable Low Latency Communication (URLLC) is perhaps the most innovative, yet challenging one. Ultra-reliability requires high levels of diversity, however, the reactive approach based on packet retransmission in HARQ protocols should be applied carefully to conform to the stringent latency constraints. The main premise of this paper is that the NOMA principle can be used to achieve highly efficient retransmissions by allowing concurrent use of wireless resources in the uplink. We introduce a comprehensive solution that accommodates multiple intermittently active users, each with its own HARQ process. The performance is investigated under two different assumptions about the Channel State Information (CSI) availability: statistical and instantaneous. The results show that NOMA can indeed lead to highly efficient system operation compared to the case in which all HARQ processes are run orthogonally. △ Less","9 July, 2020",https://arxiv.org/pdf/2007.04677
Attention-based Residual Speech Portrait Model for Speech to Face Generation,Jianrong Wang;Xiaosheng Hu;Li Liu;Wei Liu;Mei Yu;Tianyi Xu,"Given a speaker's speech, it is interesting to see if it is possible to generate this speaker's face. One main challenge in this task is to alleviate the natural mismatch between face and speech. To this end, in this paper, we propose a novel Attention-based Residual Speech Portrait Model (AR-SPM) by introducing the ideal of the residual into a hybrid encoder-decoder architecture, where face prior features are merged with the output of speech encoder to form the final face feature. In particular, we innovatively establish a tri-item loss function, which is a weighted linear combination of the L2-norm, L1-norm and negative cosine loss, to train our model by comparing the final face feature and true face feature. Evaluation on AVSpeech dataset shows that our proposed model accelerates the convergence of training, outperforms the state-of-the-art in terms of quality of the generated face, and achieves superior recognition accuracy of gender and age compared with the ground truth. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.04536
Epidemic Exposure Notification with Smartwatch: A Proximity-Based Privacy-Preserving Approach,Pai Chet Ng;Petros Spachos;Stefano Gregori;Konstantinos Plataniotis,"Businesses planning for the post-pandemic world are looking for innovative ways to protect the health and welfare of their employees and customers. Wireless technologies can play a key role in assisting contact tracing to quickly halt a local infection outbreak and prevent further spread. In this work, we present a wearable proximity and exposure notification solution based on a smartwatch that also promotes safe physical distancing in business, hospitality, or recreational facilities. Our proximity-based privacy-preserving contact tracing (P^3CT) leverages the Bluetooth Low Energy (BLE) technology for reliable proximity sensing, and an ambient signature protocol for preserving identity. Proximity sensing exploits the received signal strength (RSS) to detect the user's interaction and thus classifying them into low- or high-risk with respect to a patient diagnosed with an infectious disease. More precisely, a user is notified of their exposure based on their interactions, in terms of distance and time, with a patient. Our privacy-preserving protocol uses the ambient signatures to ensure that users' identities be anonymized. We demonstrate the feasibility of our proposed solution through extensive experimentation. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.04399
KIT MOMA: A Mobile Machines Dataset,Yusheng Xiang;Hongzhe Wang;Tianqing Su;Ruoyu Li;Christine Brach;Samuel S. Mao;Marcus Geimer,"Mobile machines typically working in a closed site, have a high potential to utilize autonomous driving technology. However, vigorously thriving development and innovation are happening mostly in the area of passenger cars. In contrast, although there are also many research pieces about autonomous driving or working in mobile machines, a consensus about the SOTA solution is still not achieved. We believe that the most urgent problem that should be solved is the absence of a public and challenging visual dataset, which makes the results from different researches comparable. To address the problem, we publish the KIT MOMA dataset, including eight classes of commonly used mobile machines, which can be used as a benchmark to evaluate the SOTA algorithms to detect mobile construction machines. The view of the gathered images is outside of the mobile machines since we believe fixed cameras on the ground are more suitable if all the interesting machines are working in a closed site. Most of the images in KIT MOMA are in a real scene, whereas some of the images are from the official website of top construction machine companies. Also, we have evaluated the performance of YOLO v3 on our dataset, indicating that the SOTA computer vision algorithms already show an excellent performance for detecting the mobile machines in a specific working site. Together with the dataset, we also upload the trained weights, which can be directly used by engineers from the construction machine industry. The dataset, trained weights, and updates can be found on our Github. Moreover, the demo can be found on our Youtube. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.04198
Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence,Shakir Mohamed;Marie-Therese Png;William Isaac,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.04068
Distributed Training of Deep Learning Models: A Taxonomic Perspective,Matthias Langer;Zhen He;Wenny Rahayu;Yanbo Xue,"Distributed deep learning systems (DDLS) train deep neural network models by utilizing the distributed resources of a cluster. Developers of DDLS are required to make many decisions to process their particular workloads in their chosen environment efficiently. The advent of GPU-based deep learning, the ever-increasing size of datasets and deep neural network models, in combination with the bandwidth constraints that exist in cluster environments require developers of DDLS to be innovative in order to train high quality models quickly. Comparing DDLS side-by-side is difficult due to their extensive feature lists and architectural deviations. We aim to shine some light on the fundamental principles that are at work when training deep neural networks in a cluster of independent machines by analyzing the general properties associated with training deep learning models and how such workloads can be distributed in a cluster to achieve collaborative model training. Thereby we provide an overview of the different techniques that are used by contemporary DDLS and discuss their influence and implications on the training process. To conceptualize and compare DDLS, we group different techniques into categories, thus establishing a taxonomy of distributed deep learning systems. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.03970
Data Science: A Comprehensive Overview,Longbing Cao,"The twenty-first century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This paper provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons and thinking about data science and analytics. △ Less","30 June, 2020",https://arxiv.org/pdf/2007.03606
A Decade of Social Bot Detection,Stefano Cresci,"On the morning of November 9th 2016, the world woke up to the shocking outcome of the US Presidential elections: Donald Trump was the 45th President of the United States of America. An unexpected event that still has tremendous consequences all over the world. Today, we know that a minority of social bots, automated social media accounts mimicking humans, played a central role in spreading divisive messages and disinformation, possibly contributing to Trump's victory. In the aftermath of the 2016 US elections, the world started to realize the gravity of widespread deception in social media. Following Trump's exploit, we witnessed to the emergence of a strident dissonance between the multitude of efforts for detecting and removing bots, and the increasing effects that these malicious actors seem to have on our societies. This paradox opens a burning question: What strategies should we enforce in order to stop this social bot pandemic? In these times, during the run-up to the 2020 US elections, the question appears as more crucial than ever. What stroke social, political and economic analysts after 2016, deception and automation, has been however a matter of study for computer scientists since at least 2010. In this work, we briefly survey the first decade of research in social bot detection. Via a longitudinal analysis, we discuss the main trends of research in the fight against bots, the major results that were achieved, and the factors that make this never-ending battle so challenging. Capitalizing on lessons learned from our extensive analysis, we suggest possible innovations that could give us the upper hand against deception and manipulation. Studying a decade of endeavours at social bot detection can also inform strategies for detecting and mitigating the effects of other, more recent, forms of online deception, such as strategic information operations and political trolls. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.03604
Extracting the fundamental diagram from aerial footage,Rafael Makrigiorgis;Panayiotis Kolios;Stelios Timotheou;Theocharis Theocharides;Christos G. Panayiotou,"Efficient traffic monitoring is playing a fundamental role in successfully tackling congestion in transportation networks. Congestion is strongly correlated with two measurable characteristics, the demand and the network density that impact the overall system behavior. At large, this system behavior is characterized through the fundamental diagram of a road segment, a region or the network. In this paper we devise an innovative way to obtain the fundamental diagram through aerial footage obtained from drone platforms. The derived methodology consists of 3 phases: vehicle detection, vehicle tracking and traffic state estimation. We elaborate on the algorithms developed for each of the 3 phases and demonstrate the applicability of the results in a real-world setting. △ Less","13 July, 2020",https://arxiv.org/pdf/2007.03227
Semi-Supervised Crowd Counting via Self-Training on Surrogate Tasks,Yan Liu;Lingqiao Liu;Peng Wang;Pingping Zhang;Yinjie Lei,"Most existing crowd counting systems rely on the availability of the object location annotation which can be expensive to obtain. To reduce the annotation cost, one attractive solution is to leverage a large number of unlabeled images to build a crowd counting model in semi-supervised fashion. This paper tackles the semi-supervised crowd counting problem from the perspective of feature learning. Our key idea is to leverage the unlabeled images to train a generic feature extractor rather than the entire network of a crowd counter. The rationale of this design is that learning the feature extractor can be more reliable and robust towards the inevitable noisy supervision generated from the unlabeled data. Also, on top of a good feature extractor, it is possible to build a density map regressor with much fewer density map annotations. Specifically, we proposed a novel semi-supervised crowd counting method which is built upon two innovative components: (1) a set of inter-related binary segmentation tasks are derived from the original density map regression task as the surrogate prediction target; (2) the surrogate target predictors are learned from both labeled and unlabeled data by utilizing a proposed self-training scheme which fully exploits the underlying constraints of these binary segmentation tasks. Through experiments, we show that the proposed method is superior over the existing semisupervised crowd counting method and other representative baselines. △ Less","18 July, 2020",https://arxiv.org/pdf/2007.03207
Cost-sensitive Multi-class AdaBoost for Understanding Driving Behavior with Telematics,Banghee So;Jean-Philippe Boucher;Emiliano A. Valdez,"Powered with telematics technology, insurers can now capture a wide range of data, such as distance traveled, how drivers brake, accelerate or make turns, and travel frequency each day of the week, to better decode driver's behavior. Such additional information helps insurers improve risk assessments for usage-based insurance (UBI), an increasingly popular industry innovation. In this article, we explore how to integrate telematics information to better predict claims frequency. For motor insurance during a policy year, we typically observe a large proportion of drivers with zero claims, a less proportion with exactly one claim, and far lesser with two or more claims. We introduce the use of a cost-sensitive multi-class adaptive boosting (AdaBoost) algorithm, which we call SAMME.C2, to handle such imbalances. To calibrate SAMME.C2 algorithm, we use empirical data collected from a telematics program in Canada and we find improved assessment of driving behavior with telematics relative to traditional risk variables. We demonstrate our algorithm can outperform other models that can handle class imbalances: SAMME, SAMME with SMOTE, RUSBoost, and SMOTEBoost. The sampled data on telematics were observations during 2013-2016 for which 50,301 are used for training and another 21,574 for testing. Broadly speaking, the additional information derived from vehicle telematics helps refine risk classification of drivers of UBI. △ Less","6 July, 2020",https://arxiv.org/pdf/2007.03100
Streaming Verification for Graph Problems: Optimal Tradeoffs and Nonlinear Sketches,Amit Chakrabarti;Prantar Ghosh;Justin Thaler,"We study graph computations in an enhanced data streaming setting, where a space-bounded client reading the edge stream of a massive graph may delegate some of its work to a cloud service. We seek algorithms that allow the client to verify a purported proof sent by the cloud service that the work done in the cloud is correct. A line of work starting with Chakrabarti et al. (ICALP 2009) has provided such algorithms, which we call schemes, for several statistical and graph-theoretic problems, many of which exhibit a tradeoff between the length of the proof and the space used by the streaming verifier. This work designs new schemes for a number of basic graph problems---including triangle counting, maximum matching, topological sorting, and single-source shortest paths---where past work had either failed to obtain smooth tradeoffs between these two key complexity measures or only obtained suboptimal tradeoffs. Our key innovation is having the verifier compute certain nonlinear sketches of the input stream, leading to either new or improved tradeoffs. In many cases, our schemes in fact provide optimal tradeoffs up to logarithmic factors. Specifically, for most graph problems that we study, it is known that the product of the verifier's space cost v and the proof length h must be at least Ω(n^2) for n-vertex graphs. However, matching upper bounds are only known for a handful of settings of h and v on the curve h \cdot v=\tildeΘ(n^2). For example, for counting triangles and maximum matching, schemes with costs lying on this curve are only known for (h=\tilde{O}(n^2), v=\tilde{O}(1)), (h=\tilde{O}(n), v=\tilde{O}(n)), and the trivial (h=\tilde{O}(1), v=\tilde{O}(n^2)). A major message of this work is that by exploiting nonlinear sketches, a significant ``portion'' of costs on the tradeoff curve h \cdot v = n^2 can be achieved. △ Less","6 July, 2020",https://arxiv.org/pdf/2007.03039
A study of multistage interconnection networks operating with wormhole routing and equipped with multi-lane storage,Eleftherios Stergiou,"Multistage interconnection networks (MINs) provide critical communication resources between network components with an attractive cost/performance relation. In this paper, a novel architecture for a MIN is proposed. In contrast to other existing networks, this new architecture operates via wormhole routing using a multi-lane equivalent-weighted fair queuing system. Associating the lane storage with each physical channel for movement of the flits allows for high levels of packet flow and makes the system more robust. The resulting flit scheduling schema has been studied thoroughly, and the results are presented in this manuscript. In addition to performance metrics, additional factors such as complexity, cost and reliability are investigated. Since a basic aim of the design of MINs is to achieve a good data flow control mechanism, this proposal is an extremely effective and robust solution. The rationale behind this innovative scheme is to introduce a more efficient network technology, thus providing a better quality of service (i.e. streaming media vs. file transfer). △ Less","6 July, 2020",https://arxiv.org/pdf/2007.02550
Contextual-Relation Consistent Domain Adaptation for Semantic Segmentation,Jiaxing Huang;Shijian Lu;Dayan Guan;Xiaobing Zhang,"Recent advances in unsupervised domain adaptation for semantic segmentation have shown great potentials to relieve the demand of expensive per-pixel annotations. However, most existing works address the domain discrepancy by aligning the data distributions of two domains at a global image level whereas the local consistencies are largely neglected. This paper presents an innovative local contextual-relation consistent domain adaptation (CrCDA) technique that aims to achieve local-level consistencies during the global-level alignment. The idea is to take a closer look at region-wise feature representations and align them for local-level consistencies. Specifically, CrCDA learns and enforces the prototypical local contextual-relations explicitly in the feature space of a labelled source domain while transferring them to an unlabelled target domain via backpropagation-based adversarial learning. An adaptive entropy max-min adversarial learning scheme is designed to optimally align these hundreds of local contextual-relations across domain without requiring discriminator or extra computation overhead. The proposed CrCDA has been evaluated extensively over two challenging domain adaptive segmentation tasks (e.g., GTA5 to Cityscapes and SYNTHIA to Cityscapes), and experiments demonstrate its superior segmentation performance as compared with state-of-the-art methods. △ Less","15 July, 2020",https://arxiv.org/pdf/2007.02424
Learning Color Compatibility in Fashion Outfits,Heming Zhang;Xuewen Yang;Jianchao Tan;Chi-Hao Wu;Jue Wang;C. -C. Jay Kuo,"Color compatibility is important for evaluating the compatibility of a fashion outfit, yet it was neglected in previous studies. We bring this important problem to researchers' attention and present a compatibility learning framework as solution to various fashion tasks. The framework consists of a novel way to model outfit compatibility and an innovative learning scheme. Specifically, we model the outfits as graphs and propose a novel graph construction to better utilize the power of graph neural networks. Then we utilize both ground-truth labels and pseudo labels to train the compatibility model in a weakly-supervised manner.Extensive experimental results verify the importance of color compatibility alone with the effectiveness of our framework. With color information alone, our model's performance is already comparable to previous methods that use deep image features. Our full model combining the aforementioned contributions set the new state-of-the-art in fashion compatibility prediction. △ Less","5 July, 2020",https://arxiv.org/pdf/2007.02388
Block Model Guided Unsupervised Feature Selection,Zilong Bai;Hoa Nguyen;Ian Davidson,"Feature selection is a core area of data mining with a recent innovation of graph-driven unsupervised feature selection for linked data. In this setting we have a dataset \mathbf{Y} consisting of n instances each with m features and a corresponding n node graph (whose adjacency matrix is \mathbf{A}) with an edge indicating that the two instances are similar. Existing efforts for unsupervised feature selection on attributed networks have explored either directly regenerating the links by solving for f such that f(\mathbf{y}_i,\mathbf{y}_j) \approx \mathbf{A}_{i,j} or finding community structure in \mathbf{A} and using the features in \mathbf{Y} to predict these communities. However, graph-driven unsupervised feature selection remains an understudied area with respect to exploring more complex guidance. Here we take the novel approach of first building a block model on the graph and then using the block model for feature selection. That is, we discover \mathbf{F}\mathbf{M}\mathbf{F}^T \approx \mathbf{A} and then find a subset of features \mathcal{S} that induces another graph to preserve both \mathbf{F} and \mathbf{M}. We call our approach Block Model Guided Unsupervised Feature Selection (BMGUFS). Experimental results show that our method outperforms the state of the art on several real-world public datasets in finding high-quality features for clustering. △ Less","5 July, 2020",https://arxiv.org/pdf/2007.02376
A Modern Non-SQL Approach to Radiology-Centric Search Engine Design with Clinical Validation,Ningcheng Li;Guy Maresh;Maxwell Cretcher;Khashayar Farsad;Ramsey Al-Hakim;John Kaufman;Judy Gichoya,"Healthcare data is increasing in size at an unprecedented speed with much attention on big data analysis and Artificial Intelligence application for quality assurance, clinical training, severity triaging, and decision support. Radiology is well-suited for innovation given its intrinsically paired linguistic and visual data. Previous attempts to unlock this information goldmine were encumbered by heterogeneity of human language, proprietary search algorithms, and lack of medicine-specific search performance matrices. We present a de novo process of developing a document-based, secure, efficient, and accurate search engine in the context of Radiology. We assess our implementation of the search engine with comparison to pre-existing manually collected clinical databases used previously for clinical research projects in addition to computational performance benchmarks and survey feedback. By leveraging efficient database architecture, search capability, and clinical thinking, radiologists are at the forefront of harnessing the power of healthcare data. △ Less","4 July, 2020",https://arxiv.org/pdf/2007.02124
Crowdfunding for Design Innovation: Prediction Model with Critical Factors,Chaoyang Song;Jianxi Luo;Katja Hölttä-Otto;Warren Seering;Kevin Otto,"Online reward-based crowdfunding campaigns have emerged as an innovative approach for validating demands, discovering early adopters, and seeking learning and feedback in the design processes of innovative products. However, crowdfunding campaigns for innovative products are faced with a high degree of uncertainty and suffer meager rates of success to fulfill their values for design. To guide designers and innovators for crowdfunding campaigns, this paper presents a data-driven methodology to build a prediction model with critical factors for crowdfunding success, based on public online crowdfunding campaign data. Specifically, the methodology filters 26 candidate factors in the Real-Win-Worth framework and identifies the critical ones via step-wise regression to predict the amount of crowdfunding. We demonstrate the methodology via deriving prediction models and identifying essential factors from 3D printer and smartwatch campaign data on Kickstarter and Indiegogo. The critical factors can guide campaign developments, and the prediction model may evaluate crowdfunding potential of innovations in contexts, to increase the chance of crowdfunding success of innovative products. △ Less","2 July, 2020",https://arxiv.org/pdf/2007.01404
A Semi-Supervised Generative Adversarial Network for Prediction of Genetic Disease Outcomes,Caio Davi;Ulisses Braga-Neto,"For most diseases, building large databases of labeled genetic data is an expensive and time-demanding task. To address this, we introduce genetic Generative Adversarial Networks (gGAN), a semi-supervised approach based on an innovative GAN architecture to create large synthetic genetic data sets starting with a small amount of labeled data and a large amount of unlabeled data. Our goal is to determine the propensity of a new individual to develop the severe form of the illness from their genetic profile alone. The proposed model achieved satisfactory results using real genetic data from different datasets and populations, in which the test populations may not have the same genetic profiles. The proposed model is self-aware and capable of determining whether a new genetic profile has enough compatibility with the data on which the network was trained and is thus suitable for prediction. The code and datasets used can be found at https://github.com/caio-davi/gGAN. △ Less","2 July, 2020",https://arxiv.org/pdf/2007.01200
Mining and Tailings Dam Detection In Satellite Imagery Using Deep Learning,Remis Balaniuk;Olga Isupova;Steven Reece,"This work explores the combination of free cloud computing, free open-source software, and deep learning methods to analyse a real, large-scale problem: the automatic country-wide identification and classification of surface mines and mining tailings dams in Brazil. Locations of officially registered mines and dams were obtained from the Brazilian government open data resource. Multispectral Sentinel-2 satellite imagery, obtained and processed at the Google Earth Engine platform, was used to train and test deep neural networks using the TensorFlow 2 API and Google Colab platform. Fully Convolutional Neural Networks were used in an innovative way, to search for unregistered ore mines and tailing dams in large areas of the Brazilian territory. The efficacy of the approach is demonstrated by the discovery of 263 mines that do not have an official mining concession. This exploratory work highlights the potential of a set of new technologies, freely available, for the construction of low cost data science tools that have high social impact. At the same time, it discusses and seeks to suggest practical solutions for the complex and serious problem of illegal mining and the proliferation of tailings dams, which pose high risks to the population and the environment, especially in developing countries. Code is made publicly available at: https://github.com/remis/mining-discovery-with-deep-learning. △ Less","2 July, 2020",https://arxiv.org/pdf/2007.01076
Drone swarms in fire suppression activities,Elena Ausonio;Patrizia Bagnerini;Marco Ghio,"Recent huge technological development of Unmanned Aerial Vehicles (UAVs) can provide breakthrough means of fighting wildland fires. We propose an innovative forest firefighting system based on the use of a swarm of hundreds of UAVs able to generate a continuous flow of extinguishing liquid on the fire front, simulating rain effect. Automatic battery replacement and refilling of the extinguishing liquid ensure the continuity of the action, and fire-resistant materials protect drones exposed to possible high temperatures. We demonstrate the validity of the approach in Mediterranean scrub first computing the critical water flow rate according to the main factors involved in the evolution of a fire, then estimating the number of linear meters of active fire front that can be extinguished depending on the number of drones available and the amount of extinguishing fluid carried. A fire propagation cellular automata model is also employed to study the evolution of the fire. Simulation results suggest that the proposed system can successfully integrate, or in case of low-intensity and limited extent fires completely replace, current forest firefighting techniques. △ Less","18 December, 2020",https://arxiv.org/pdf/2007.00883
A Statistical Overview on Data Privacy,Fang Liu,"The eruption of big data with the increasing collection and processing of vast volumes and variety of data have led to breakthrough discoveries and innovation in science, engineering, medicine, commerce, criminal justice, and national security that would not have been possible in the past. While there are many benefits to the collection and usage of big data, there are also growing concerns among the general public on what personal information is collected and how it is used. In addition to legal policies and regulations, technological tools and statistical strategies also exist to promote and safeguard individual privacy, while releasing and sharing useful population-level information. In this overview, I introduce some of these approaches, as well as the existing challenges and opportunities in statistical data privacy research and applications to better meet the practical needs of privacy protection and information sharing. △ Less","1 July, 2020",https://arxiv.org/pdf/2007.00765
Drug discovery with explainable artificial intelligence,José Jiménez-Luna;Francesca Grisoni;Gisbert Schneider,"Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for 'explainable' deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and dares a forecast of the future opportunities, potential applications, and remaining challenges. △ Less","2 July, 2020",https://arxiv.org/pdf/2007.00523
Pricing cyber insurance for a large-scale network,Lei Hua;Maochao Xu,"Facing the lack of cyber insurance loss data, we propose an innovative approach for pricing cyber insurance for a large-scale network based on synthetic data. The synthetic data is generated by the proposed risk spreading and recovering algorithm that allows infection and recovery events to occur sequentially, and allows dependence of random waiting time to infection for different nodes. The scale-free network framework is adopted to account for the topology uncertainty of the random large-scale network. Extensive simulation studies are conducted to understand the risk spreading and recovering mechanism, and to uncover the most important underwriting risk factors. A case study is also presented to demonstrate that the proposed approach and algorithm can be adapted accordingly to provide reference for cyber insurance pricing. △ Less","29 June, 2020",https://arxiv.org/pdf/2007.00454
"HPC AI500: The Methodology, Tools, Roofline Performance Models, and Metrics for Benchmarking HPC AI Systems",Zihan Jiang;Lei Wang;Xingwang Xiong;Wanling Gao;Chunjie Luo;Fei Tang;Chuanxin Lan;Hongxiao Li;Jianfeng Zhan,"The recent years witness a trend of applying large-scale distributed deep learning in both business and scientific computing areas, whose goal is to speed up the training time to achieve a state-of-the-art quality. The HPC community feels a great interest in building the HPC AI systems that are dedicated to running those workloads. The HPC AI benchmarks accelerate the process. Unfortunately, benchmarking HPC AI systems at scale raises serious challenges. None of previous HPC AI benchmarks achieve the goal of being equivalent, relevant, representative, affordable, and repeatable. This paper presents a comprehensive methodology, tools, Roofline performance models, and innovative metrics for benchmarking, optimizing, and ranking HPC AI systems, which we call HPC AI500 V2.0. We abstract the HPC AI system into nine independent layers, and present explicit benchmarking rules and procedures to assure equivalence of each layer, repeatability, and replicability. On the basis of AIBench -- by far the most comprehensive AI benchmarks suite, we present and build two HPC AI benchmarks from both business and scientific computing: Image Classification, and Extreme Weather Analytics, achieving both representativeness and affordability. To rank the performance and energy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPS per watt, which impose a penalty on failing to achieve the target quality. We propose using convolution and GEMM -- the two most intensively-used kernel functions to measure the upper bound performance of the HPC AI systems, and present HPC AI roofline models for guiding performance optimizations. The evaluations show our methodology, benchmarks, performance models, and metrics can measure, optimize, and rank the HPC AI systems in a scalable, simple, and affordable way. HPC AI500 V2.0 are publicly available from http://www.benchcouncil.org/benchhub/hpc-ai500-benchmark. △ Less","1 July, 2020",https://arxiv.org/pdf/2007.00279
Construction of confidence interval for a univariate stock price signal predicted through Long Short Term Memory Network,Shankhyajyoti De;Arabin Kumar Dey;Deepak Gauda,"In this paper, we show an innovative way to construct bootstrap confidence interval of a signal estimated based on a univariate LSTM model. We take three different types of bootstrap methods for dependent set up. We prescribe some useful suggestions to select the optimal block length while performing the bootstrapping of the sample. We also propose a benchmark to compare the confidence interval measured through different bootstrap strategies. We illustrate the experimental results through some stock price data set. △ Less","1 July, 2020",https://arxiv.org/pdf/2007.00254
Track Seeding and Labelling with Embedded-space Graph Neural Networks,Nicholas Choma;Daniel Murnane;Xiangyang Ju;Paolo Calafiura;Sean Conlon;Steven Farrell;Prabhat;Giuseppe Cerati;Lindsey Gray;Thomas Klijnsma;Jim Kowalkowski;Panagiotis Spentzouris;Jean-Roch Vlimant;Maria Spiropulu;Adam Aurisano;V Hewes;Aristeidis Tsaris;Kazuhiro Terao;Tracy Usher,"To address the unprecedented scale of HL-LHC data, the Exa.TrkX project is investigating a variety of machine learning approaches to particle track reconstruction. The most promising of these solutions, graph neural networks (GNN), process the event as a graph that connects track measurements (detector hits corresponding to nodes) with candidate line segments between the hits (corresponding to edges). Detector information can be associated with nodes and edges, enabling a GNN to propagate the embedded parameters around the graph and predict node-, edge- and graph-level observables. Previously, message-passing GNNs have shown success in predicting doublet likelihood, and we here report updates on the state-of-the-art architectures for this task. In addition, the Exa.TrkX project has investigated innovations in both graph construction, and embedded representations, in an effort to achieve fully learned end-to-end track finding. Hence, we present a suite of extensions to the original model, with encouraging results for hitgraph classification. In addition, we explore increased performance by constructing graphs from learned representations which contain non-linear metric structure, allowing for efficient clustering and neighborhood queries of data points. We demonstrate how this framework fits in with both traditional clustering pipelines, and GNN approaches. The embedded graphs feed into high-accuracy doublet and triplet classifiers, or can be used as an end-to-end track classifier by clustering in an embedded space. A set of post-processing methods improve performance with knowledge of the detector physics. Finally, we present numerical results on the TrackML particle tracking challenge dataset, where our framework shows favorable results in both seeding and track finding. △ Less","30 June, 2020",https://arxiv.org/pdf/2007.00149
Data Science: Challenges and Directions,Longbing Cao,"While data science has emerged as a contentious new scientific field, enormous debates and discussions have been made on it why we need data science and what makes it as a science. In reviewing hundreds of pieces of literature which include data science in their titles, we find that the majority of the discussions essentially concern statistics, data mining, machine learning, big data, or broadly data analytics, and only a limited number of new data-driven challenges and directions have been explored. In this paper, we explore the intrinsic challenges and directions inspired by comprehensively exploring the complexities and intelligence embedded in data science problems. We focus on the research and innovation challenges inspired by the nature of data science problems as complex systems, and the methodologies for handling such systems. △ Less","27 June, 2020",https://arxiv.org/pdf/2006.16966
A GRU-based Mixture Density Network for Data-Driven Dynamic Stochastic Programming,Xiaoming Li;Chun Wang;Xiao Huang;Yimin Nie,"The conventional deep learning approaches for solving time-series problem such as long-short term memory (LSTM) and gated recurrent unit (GRU) both consider the time-series data sequence as the input with one single unit as the output (predicted time-series result). Those deep learning approaches have made tremendous success in many time-series related problems, however, this cannot be applied in data-driven stochastic programming problems since the output of either LSTM or GRU is a scalar rather than probability distribution which is required by stochastic programming model. To fill the gap, in this work, we propose an innovative data-driven dynamic stochastic programming (DD-DSP) framework for time-series decision-making problem, which involves three components: GRU, Gaussian Mixture Model (GMM) and SP. Specifically, we devise the deep neural network that integrates GRU and GMM which is called GRU-based Mixture Density Network (MDN), where GRU is used to predict the time-series outcomes based on the recent historical data, and GMM is used to extract the corresponding probability distribution of predicted outcomes, then the results will be input as the parameters for SP. To validate our approach, we apply the framework on the car-sharing relocation problem. The experiment validations show that our framework is superior to data-driven optimization based on LSTM with the vehicle average moving lower than LSTM. △ Less","26 June, 2020",https://arxiv.org/pdf/2006.16845
Delayed Q-update: A novel credit assignment technique for deriving an optimal operation policy for the Grid-Connected Microgrid,Hyungjun Park;Daiki Min;Jong-hyun Ryu;Dong Gu Choi,"A microgrid is an innovative system that integrates distributed energy resources to supply electricity demand within electrical boundaries. This study proposes an approach for deriving a desirable microgrid operation policy that enables sophisticated controls in the microgrid system using the proposed novel credit assignment technique, delayed-Q update. The technique employs novel features such as the ability to tackle and resolve the delayed effective property of the microgrid, which prevents learning agents from deriving a well-fitted policy under sophisticated controls. The proposed technique tracks the history of the charging period and retroactively assigns an adjusted value to the ESS charging control. The operation policy derived using the proposed approach is well-fitted for the real effects of ESS operation because of the process of the technique. Therefore, it supports the search for a near-optimal operation policy under a sophisticatedly controlled microgrid environment. To validate our technique, we simulate the operation policy under a real-world grid-connected microgrid system and demonstrate the convergence to a near-optimal policy by comparing performance measures of our policy with benchmark policy and optimal policy. △ Less","20 October, 2020",https://arxiv.org/pdf/2006.16659
Efficient algorithm based on non-backtracking matrix for community detection in signed networks,Zhaoyue Zhong;Xiangrong Wang;Cunquan Qu;Guanghui Wang,"Community detection or clustering is a crucial task for understanding the structure of complex systems. In some networks, nodes are permitted to be linked by either ""positive"" or ""negative"" edges; such networks are called signed networks. Discovering communities in signed networks is more challenging than that in unsigned networks. In this study, we innovatively develop a non-backtracking matrix of signed networks, theoretically derive a detectability threshold for this matrix, and demonstrate the feasibility of using the matrix for community detection. We further improve the developed matrix by considering the balanced paths in the network (referred to as a balanced non-backtracking matrix). Simulation results demonstrate that the algorithm based on the balanced nonbacktracking matrix significantly outperforms those based on the adjacency matrix and the signed non-backtracking matrix. The proposed (improved) matrix shows great potential for detecting communities with or without overlap. △ Less","10 October, 2020",https://arxiv.org/pdf/2006.15471
Making DensePose fast and light,Ruslan Rakhimov;Emil Bogomolov;Alexandr Notchenko;Fung Mao;Alexey Artemov;Denis Zorin;Evgeny Burnaev,"DensePose estimation task is a significant step forward for enhancing user experience computer vision applications ranging from augmented reality to cloth fitting. Existing neural network models capable of solving this task are heavily parameterized and a long way from being transferred to an embedded or mobile device. To enable Dense Pose inference on the end device with current models, one needs to support an expensive server-side infrastructure and have a stable internet connection. To make things worse, mobile and embedded devices do not always have a powerful GPU inside. In this work, we target the problem of redesigning the DensePose R-CNN model's architecture so that the final network retains most of its accuracy but becomes more light-weight and fast. To achieve that, we tested and incorporated many deep learning innovations from recent years, specifically performing an ablation study on 23 efficient backbone architectures, multiple two-stage detection pipeline modifications, and custom model quantization methods. As a result, we achieved 17\times model size reduction and 2\times latency improvement compared to the baseline model. △ Less","9 July, 2020",https://arxiv.org/pdf/2006.15190
Fair navigation planning: a humanitarian robot use case,Martim Brandao,"In this paper we investigate potential issues of fairness related to the motion of mobile robots. We focus on the particular use case of humanitarian mapping and disaster response. We start by showing that there is a fairness dimension to robot navigation, and use a walkthrough example to bring out design choices and issues that arise during the development of a fair system. We discuss indirect discrimination, fairness-efficiency trade-offs, the existence of counter-productive fairness definitions, privacy and other issues. Finally, we conclude with a discussion of the potential of our methodology as a concrete responsible innovation tool for eliciting ethical issues in the design of autonomous systems. △ Less","25 June, 2020",https://arxiv.org/pdf/2006.14479
HARMer: Cyber-attacks Automation and Evaluation,Simon Yusuf Enoch;Zhibin Huang;Chun Yong Moon;Donghwan Lee;Myung Kil Ahn;Dong Seong Kim,"With the increasing growth of cyber-attack incidences, it is important to develop innovative and effective techniques to assess and defend networked systems against cyber attacks. One of the well-known techniques for this is performing penetration testing which is carried by a group of security professionals (i.e, red team). Penetration testing is also known to be effective to find existing and new vulnerabilities, however, the quality of security assessment can be depending on the quality of the red team members and their time and devotion to the penetration testing. In this paper, we propose a novel automation framework for cyber-attacks generation named `HARMer' to address the challenges with respect to manual attack execution by the red team. Our novel proposed framework, design, and implementation is based on a scalable graphical security model called Hierarchical Attack Representation Model (HARM). (1) We propose the requirements and the key phases for the automation framework. (2) We propose security metrics-based attack planning strategies along with their algorithms. (3) We conduct experiments in a real enterprise network and Amazon Web Services. The results show how the different phases of the framework interact to model the attackers' operations. This framework will allow security administrators to automatically assess the impact of various threats and attacks in an automated manner. △ Less","17 July, 2020",https://arxiv.org/pdf/2006.14352
Design And Develop Network Storage Virtualization By Using GNS3,Abdul Ahad Abro;Ufaque Shaikh,"Virtualization is an emerging and optimistic prospect in the IT industry. Its impact has a footprint widely in digital infrastructure. Many innovativeness sectors utilized the concept of virtualization to reduce the cost of frameworks. In this paper, we have designed and developed storage virtualization for physical functional solutions. It is an auspicious type of virtualization that is accessible, secure, scalable, and manageable. In the paper, we have proposed the pool storage method used the RAID-Z file system with the ZFS model which provides the duplication of site approach, compression blueprint, adequate backup methods, expansion in error-correcting techniques, and tested procedure on the real-time network location. Therefore, this study provides useful guidelines to design and develop optimized storage virtualization. △ Less","24 June, 2020",https://arxiv.org/pdf/2006.14074
Raising Expectations: Automating Expected Cost Analysis with Types,Di Wang;David M Kahn;Jan Hoffmann,"This article presents a type-based analysis for deriving upper bounds on the expected execution cost of probabilistic programs. The analysis is naturally compositional, parametric in the cost model, and supports higher order functions and inductive data types. The derived bounds are multivariate polynomials that are functions of data structures. Bound inference is enabled by local type rules that reduce type inference to linear constraint solving. The type system is based on the potential method of amortized analysis and extends automatic amortized resource analysis (AARA) for deterministic programs. A main innovation is that bounds can contain symbolic probabilities, which may appear in data structures and function arguments. Another contribution is a novel soundness proof that establishes the correctness of the derived bounds with respect to a distribution-based operational cost semantics that also includes nontrivial diverging behavior. For cost models like time, derived bounds imply termination with probability one. To highlight the novel ideas, the presentation focuses on linear potential and a core language. However, the analysis is implemented as an extension of Resource Aware ML and supports polynomial bounds and user defined data structures. The effectiveness of the technique is evaluated by analyzing the sample complexity of discrete distributions and with a novel average-case estimation for deterministic programs that combines expected cost analysis with statistical methods. △ Less","21 September, 2020",https://arxiv.org/pdf/2006.14010
Community-Based Data Integration of Course and Job Data in Support of Personalized Career-Education Recommendations,Guoqing Zhu;Naga Anjaneyulu Kopalle;Yongzhen Wang;Xiaozhong Liu;Kemi Jona;Katy Börner,"How does your education impact your professional career? Ideally, the courses you take help you identify, get hired for, and perform the job you always wanted. However, not all courses provide skills that transfer to existing and future jobs; skill terms used in course descriptions might be different from those listed in job advertisements; and there might exist a considerable skill gap between what is taught in courses and what is needed for a job. In this study, we propose a novel method to integrate extensive course description and job advertisement data by leveraging heterogeneous data integration and community detection. The innovative heterogeneous graph approach along with identified skill communities enables cross-domain information recommendation, e.g., given an educational profile, job recommendations can be provided together with suggestions on education opportunities for re- and upskilling in support of lifelong learning. △ Less","24 June, 2020",https://arxiv.org/pdf/2006.13864
Adoption of ICT innovations in the agriculture sector in Africa: A Systematic Literature Review,Claudia Ayim;Ayalew Kassahun;Bedir Tekinerdogan;Chris Addison,"According to the latest World Economic Forum report, about 70% of the African population depends on agriculture for their livelihood. This makes agriculture a critical sector within the African continent. Nonetheless, agricultural productivity is low and food insecurity is still a challenge. This has in recent years led to several initiatives in using ICT (Information Communication Technology) to improve agriculture productivity. This study aims to explore ICT innovations in the agriculture sector of Africa. To achieve this, we conducted a SLR (Systematic Literature Review) of the literature published since 2010. Our search yielded 779 papers, of which 23 papers were selected for a detailed analysis following a detailed exclusion and quality assessment criteria. The analysis of the selected papers shows that the main ICT technologies adopted are text and voice-based services targeting mobile phones. The analysis also shows that radios are still widely used in disseminating agriculture information to rural farmers, while computers are mainly used by researchers. Though the mobile-based services aimed at improving access to accurate and timely agriculture information, the literature reviews indicate that the adoption of the services is constrained by poor technological infrastructure, inappropriate ICT policies and low capacity levels of users, especially farmers, to using the technologies. The findings further indicate that literature on an appropriate theoretical framework for guiding ICT innovations is lacking. △ Less","24 June, 2020",https://arxiv.org/pdf/2006.13831
How do Agile Software Startups deal with uncertainties by Covid-19 pandemic?,Rafael da Camara;Marcelo Marinho;Suzana Sampaio;Saulo Cadete,"The dissipation of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has already taken on pandemic proportions, affecting over 100 countries in a couple of weeks. The evolution of the disease and its economic impact is highly uncertain, which brings challenges for newly created software companies. Software startups are companies that create innovative software products and services in a dynamic and fast-growing market. Agile Software Methods aims to enable startups in responding to uncertainty caused by Covid-19. This paper investigates the impact of Covid-19 in a real software startup context to understand how they have reacted against uncertainties caused by Covid-19. As a research methodology, action research within Di2Win, a Brazilian software startup, has been applied. The study was carried out throughout six sprints, during the quarantine. Practices employed to mitigate threats while simultaneously allowing teams to remain open to opportunities and challenges are detailed. This paper shares lessons learned that could help agile software startups improve their way of work in an uncertain environment caused by the Covid-19 pandemic. △ Less","10 August, 2020",https://arxiv.org/pdf/2006.13715
Coconut: a scalable bottom-up approach for building data series indexes,Haridimos Kondylakis;Niv Dayan;Kostas Zoumpatianos;Themis Palpanas,"Many modern applications produce massive amounts of data series that need to be analyzed, requiring efficient similarity search operations. However, the state-of-the-art data series indexes that are used for this purpose do not scale well for massive datasets in terms of performance, or storage costs. We pinpoint the problem to the fact that existing summarizations of data series used for indexing cannot be sorted while keeping similar data series close to each other in the sorted order. This leads to two design problems. First, traditional bulk-loading algorithms based on sorting cannot be used. Instead, index construction takes place through slow top-down insertions, which create a non-contiguous index that results in many random I/Os. Second, data series cannot be sorted and split across nodes evenly based on their median value; thus, most leaf nodes are in practice nearly empty. This further slows down query speed and amplifies storage costs. To address these problems, we present Coconut. The first innovation in Coconut is an inverted, sortable data series summarization that organizes data series based on a z-order curve, keeping similar series close to each other in the sorted order. As a result, Coconut is able to use bulk-loading techniques that rely on sorting to quickly build a contiguous index using large sequential disk I/Os. We then explore prefix-based and median-based splitting policies for bottom-up bulk-loading, showing that median-based splitting outperforms the state of the art, ensuring that all nodes are densely populated. Overall, we show analytically and empirically that Coconut dominates the state-of-the-art data series indexes in terms of construction speed, query speed, and storage costs. △ Less","19 June, 2020",https://arxiv.org/pdf/2006.13713
"Computational Support for Substance Use Disorder Prevention, Detection, Treatment, and Recovery",Lana Yarosh;Suzanne Bakken;Alan Borning;Munmun De Choudhury;Cliff Lampe;Elizabeth Mynatt;Stephen Schueller;Tiffany Veinot,"Substance Use Disorders (SUDs) involve the misuse of any or several of a wide array of substances, such as alcohol, opioids, marijuana, and methamphetamine. SUDs are characterized by an inability to decrease use despite severe social, economic, and health-related consequences to the individual. A 2017 national survey identified that 1 in 12 US adults have or have had a substance use disorder. The National Institute on Drug Abuse estimates that SUDs relating to alcohol, prescription opioids, and illicit drug use cost the United States over $520 billion annually due to crime, lost work productivity, and health care expenses. Most recently, the US Department of Health and Human Services has declared the national opioid crisis a public health emergency to address the growing number of opioid overdose deaths in the United States. In this interdisciplinary workshop, we explored how computational support - digital systems, algorithms, and sociotechnical approaches (which consider how technology and people interact as complex systems) - may enhance and enable innovative interventions for prevention, detection, treatment, and long-term recovery from SUDs. The Computing Community Consortium (CCC) sponsored a two-day workshop titled ""Computational Support for Substance Use Disorder Prevention, Detection, Treatment, and Recovery"" on November 14-15, 2019 in Washington, DC. As outcomes from this visioning process, we identified three broad opportunity areas for computational support in the SUD context: 1. Detecting and mitigating risk of SUD relapse, 2. Establishing and empowering social support networks, and 3. Collecting and sharing data meaningfully across ecologies of formal and informal care. △ Less","23 June, 2020",https://arxiv.org/pdf/2006.13259
PoseGAN: A Pose-to-Image Translation Framework for Camera Localization,Kanglin Liu;Qing Li;Guoping Qiu,"Camera localization is a fundamental requirement in robotics and computer vision. This paper introduces a pose-to-image translation framework to tackle the camera localization problem. We present PoseGANs, a conditional generative adversarial networks (cGANs) based framework for the implementation of pose-to-image translation. PoseGANs feature a number of innovations including a distance metric based conditional discriminator to conduct camera localization and a pose estimation technique for generated camera images as a stronger constraint to improve camera localization performance. Compared with learning-based regression methods such as PoseNet, PoseGANs can achieve better performance with model sizes that are 70% smaller. In addition, PoseGANs introduce the view synthesis technique to establish the correspondence between the 2D images and the scene, \textit{i.e.}, given a pose, PoseGANs are able to synthesize its corresponding camera images. Furthermore, we demonstrate that PoseGANs differ in principle from structure-based localization and learning-based regressions for camera localization, and show that PoseGANs exploit the geometric structures to accomplish the camera localization task, and is therefore more stable than and superior to learning-based regressions which rely on local texture features instead. In addition to camera localization and view synthesis, we also demonstrate that PoseGANs can be successfully used for other interesting applications such as moving object elimination and frame interpolation in video sequences. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12712
Risk-Sensitive Reinforcement Learning: a Martingale Approach to Reward Uncertainty,Nelson Vadori;Sumitra Ganesh;Prashant Reddy;Manuela Veloso,"We introduce a novel framework to account for sensitivity to rewards uncertainty in sequential decision-making problems. While risk-sensitive formulations for Markov decision processes studied so far focus on the distribution of the cumulative reward as a whole, we aim at learning policies sensitive to the uncertain/stochastic nature of the rewards, which has the advantage of being conceptually more meaningful in some cases. To this end, we present a new decomposition of the randomness contained in the cumulative reward based on the Doob decomposition of a stochastic process, and introduce a new conceptual tool - the \textit{chaotic variation} - which can rigorously be interpreted as the risk measure of the martingale component associated to the cumulative reward process. We innovate on the reinforcement learning side by incorporating this new risk-sensitive approach into model-free algorithms, both policy gradient and value function based, and illustrate its relevance on grid world and portfolio optimization problems. △ Less","15 September, 2020",https://arxiv.org/pdf/2006.12686
Prediction error-driven memory consolidation for continual learning. On the case of adaptive greenhouse models,Guido Schillaci;Luis Miranda;Uwe Schmidt,"This work presents an adaptive architecture that performs online learning and faces catastrophic forgetting issues by means of episodic memories and prediction-error driven memory consolidation. In line with evidences from the cognitive science and neuroscience, memories are retained depending on their congruency with the prior knowledge stored in the system. This is estimated in terms of prediction error resulting from a generative model. Moreover, this AI system is transferred onto an innovative application in the horticulture industry: the learning and transfer of greenhouse models. This work presents a model trained on data recorded from research facilities and transferred to a production greenhouse. △ Less","27 July, 2020",https://arxiv.org/pdf/2006.12616
AI-Augmented Multi Function Radar Engineering with Digital Twin: Towards Proactivity,Mathieu Klein;Thomas Carpentier;Eric Jeanclaude;Rami Kassab;Konstantinos Varelas;Nico de Bruijn;Frédéric Barbaresco;Yann Briheche;Yann Semet;Florence Aligne,"Thales new generation digital multi-missions radars, fully-digital and software-defined, like the Sea Fire and Ground Fire radars, benefit from a considerable increase of accessible degrees of freedoms to optimally design their operational modes. To effectively leverage these design choices and turn them into operational capabilities, it is necessary to develop new engineering tools, using artificial intelligence. Innovative optimization algorithms in the discrete and continuous domains, coupled with a radar Digital Twins, allowed construction of a generic tool for ""search"" mode design (beam synthesis, waveform and volume grid) compliant with the available radar time budget. The high computation speeds of these algorithms suggest tool application in a ""Proactive Radar"" configuration, which would dynamically propose to the operator, operational modes better adapted to environment, threats and the equipment failure conditions. △ Less","18 June, 2020",https://arxiv.org/pdf/2006.12384
Deep Learning feature selection to unhide demographic recommender systems factors,Jesús Bobadilla;Ángel González-Prieto;Fernando Ortega;Raúl Lara-Cabrera,"Extracting demographic features from hidden factors is an innovative concept that provides multiple and relevant applications. The matrix factorization model generates factors which do not incorporate semantic knowledge. This paper provides a deep learning-based method: DeepUnHide, able to extract demographic information from the users and items factors in collaborative filtering recommender systems. The core of the proposed method is the gradient-based localization used in the image processing literature to highlight the representative areas of each classification class. Validation experiments make use of two public datasets and current baselines. Results show the superiority of DeepUnHide to make feature selection and demographic classification, compared to the state of art of feature selection methods. Relevant and direct applications include recommendations explanation, fairness in collaborative filtering and recommendation to groups of users. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.12379
Artificial intelligence in space,George Anthony Gal;Cristiana Santos;Lucien Rapp;Réeka Markovich;Leendert van der Torre,"In the next coming years, space activities are expected to undergo a radical transformation with the emergence of new satellite systems or new services which will incorporate the contributions of artificial intelligence and machine learning defined as covering a wide range of innovations from autonomous objects with their own decision-making power to increasingly sophisticated services exploiting very large volumes of information from space. This chapter identifies some of the legal and ethical challenges linked to its use. These legal and ethical challenges call for solutions which the international treaties in force are not sufficient to determine and implement. For this reason, a legal methodology must be developed that makes it possible to link intelligent systems and services to a system of rules applicable thereto. It discusses existing legal AI-based tools amenable for making space law actionable, interoperable and machine readable for future compliance tools. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12362
Differentiable PAC-Bayes Objectives with Partially Aggregated Neural Networks,Felix Biggs;Benjamin Guedj,"We make three related contributions motivated by the challenge of training stochastic neural networks, particularly in a PAC-Bayesian setting: (1) we show how averaging over an ensemble of stochastic neural networks enables a new class of \emph{partially-aggregated} estimators; (2) we show that these lead to provably lower-variance gradient estimates for non-differentiable signed-output networks; (3) we reformulate a PAC-Bayesian bound for these networks to derive a directly optimisable, differentiable objective and a generalisation guarantee, without using a surrogate loss or loosening the bound. This bound is twice as tight as that of Letarte et al. (2019) on a similar network type. We show empirically that these innovations make training easier and lead to competitive guarantees. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12228
A deep convolutional neural network model for rapid prediction of fluvial flood inundation,Syed Kabir;Sandhya Patidar;Xilin Xia;Qiuhua Liang;Jeffrey Neal;Gareth Pender;.,"Most of the two-dimensional (2D) hydraulic/hydrodynamic models are still computationally too demanding for real-time applications. In this paper, an innovative modelling approach based on a deep convolutional neural network (CNN) method is presented for rapid prediction of fluvial flood inundation. The CNN model is trained using outputs from a 2D hydraulic model (i.e. LISFLOOD-FP) to predict water depths. The pre-trained model is then applied to simulate the January 2005 and December 2015 floods in Carlisle, UK. The CNN predictions are compared favourably with the outputs produced by LISFLOOD-FP. The performance of the CNN model is further confirmed by benchmarking against a support vector regression (SVR) method. The results show that the CNN model outperforms SVR by a large margin. The CNN model is highly accurate in capturing flooded cells as indicated by several quantitative assessment matrices. The estimated error for reproducing maximum flood depth is 0 ~ 0.2 meters for the 2005 event and 0 ~ 0.5 meters for the 2015 event at over 99% of the cells covering the computational domain. The proposed CNN method offers great potential for real-time flood modelling/forecasting considering its simplicity, superior performance and computational efficiency. △ Less","16 September, 2020",https://arxiv.org/pdf/2006.11555
Empirica: a virtual lab for high-throughput macro-level experiments,Abdullah Almaatouq;Joshua Becker;James P. Houghton;Nicolas Paton;Duncan J. Watts;Mark E. Whiting,"Virtual labs allow researchers to design high-throughput and macro-level experiments that are not feasible in traditional in-person physical lab settings. Despite the increasing popularity of online research, researchers still face many technical and logistical barriers when designing and deploying virtual lab experiments. While several platforms exist to facilitate the development of virtual lab experiments, they typically present researchers with a stark trade-off between usability and functionality. We introduce Empirica: a modular virtual lab that offers a solution to the usability-functionality trade-off by employing a ""flexible defaults"" design strategy. This strategy enables us to maintain complete ""build anything"" flexibility while offering a development platform that is accessible to novice programmers. Empirica's architecture is designed to allow for parameterizable experimental designs, reusable protocols, and rapid development. These features will increase the accessibility of virtual lab experiments, remove barriers to innovation in experiment design, and enable rapid progress in the understanding of distributed human computation. △ Less","30 December, 2020",https://arxiv.org/pdf/2006.11398
Measuring transnational social fields through binational link-tracing sampling,Marian-Gabriel Hâncean;Miranda J. Lubbers;José Luis Molina,"We advance binational link-tracing sampling design, an innovative data collection methodology for sampling from transnational social fields, i.e., transnational networks embedding migrants and non-migrants. This paper shows the practical challenges of such a design, the representativeness of the samples and the qualities of the resulted networks. We performed 303 face-to-face structured interviews on sociodemographic variables, migration trajectories and personal networks of people living in a Romanian migration sending community (Dâmboviţa) and in a migration receiving Spanish town (Castellón), simultaneously in both sites. Inter-connecting the personal networks, we built a multi-layered complex network structure embedding 4,855 nominated people, 5,477 directed ties (nominations) and 2,540 edges. Results indicate that the participants' unique identification is a particularly difficult challenge, the representativeness of the data is not optimal (homophily on observed attributes was detected in the nomination patterns), and the relational and attribute data allow to explore the social organization of the Romanian migrant enclave in Castellón, as well as its connectivity to other places. Furthermore, we provide methodological suggestions for improving link-tracing sampling from transnational networks of migration. Our research contributes to the emerging efforts of applying social network analysis to the study of international migration. △ Less","19 June, 2020",https://arxiv.org/pdf/2006.11380
A survey of face recognition techniques under occlusion,Dan Zeng;Raymond Veldhuis;Luuk Spreeuwers,"The limited capacity to recognize faces under occlusions is a long-standing problem that presents a unique challenge for face recognition systems and even for humans. The problem regarding occlusion is less covered by research when compared to other challenges such as pose variation, different expressions, etc. Nevertheless, occluded face recognition is imperative to exploit the full potential of face recognition for real-world applications. In this paper, we restrict the scope to occluded face recognition. First, we explore what the occlusion problem is and what inherent difficulties can arise. As a part of this review, we introduce face detection under occlusion, a preliminary step in face recognition. Second, we present how existing face recognition methods cope with the occlusion problem and classify them into three categories, which are 1) occlusion robust feature extraction approaches, 2) occlusion aware face recognition approaches, and 3) occlusion recovery based face recognition approaches. Furthermore, we analyze the motivations, innovations, pros and cons, and the performance of representative approaches for comparison. Finally, future challenges and method trends of occluded face recognition are thoroughly discussed. △ Less","19 June, 2020",https://arxiv.org/pdf/2006.11366
"SSHealth: Toward Secure, Blockchain-Enabled Healthcare Systems",Alaa Awad Abdellatif;Abeer Z. Al-Marridi;Amr Mohamed;Aiman Erbad;Carla Fabiana Chiasserini;Ahmed Refaey,"The future of healthcare systems is being shaped by incorporating emerged technological innovations to drive new models for patient care. By acquiring, integrating, analyzing, and exchanging medical data at different system levels, new practices can be introduced, offering a radical improvement to healthcare services. This paper presents a novel smart and secure Healthcare system (ssHealth), which, leveraging advances in edge computing and blockchain technologies, permits epidemics discovering, remote monitoring, and fast emergency response. The proposed system also allows for secure medical data exchange among local healthcare entities, thus realizing the integration of multiple national and international entities and enabling the correlation of critical medical events for, e.g., emerging epidemics management and control. In particular, we develop a blockchain-based architecture and enable a flexible configuration thereof, which optimize medical data sharing between different health entities and fulfil the diverse levels of Quality of Service (QoS) that ssHealth may require. Finally, we highlight the benefits of the proposed ssHealth system and possible directions for future research. △ Less","18 June, 2020",https://arxiv.org/pdf/2006.10843
List-Decodable Mean Estimation via Iterative Multi-Filtering,Ilias Diakonikolas;Daniel M. Kane;Daniel Kongsgaard,"We study the problem of {\em list-decodable mean estimation} for bounded covariance distributions. Specifically, we are given a set T of points in \mathbb{R}^d with the promise that an unknown α-fraction of points in T, where 0< α< 1/2, are drawn from an unknown mean and bounded covariance distribution D, and no assumptions are made on the remaining points. The goal is to output a small list of hypothesis vectors such that at least one of them is close to the mean of D. We give the first practically viable estimator for this problem. In more detail, our algorithm is sample and computationally efficient, and achieves information-theoretically near-optimal error. While the only prior algorithm for this setting inherently relied on the ellipsoid method, our algorithm is iterative and only uses spectral techniques. Our main technical innovation is the design of a soft outlier removal procedure for high-dimensional heavy-tailed datasets with a majority of outliers. △ Less","20 June, 2020",https://arxiv.org/pdf/2006.10715
Reinforcement Learning as Iterative and Amortised Inference,Beren Millidge;Alexander Tschantz;Anil K Seth;Christopher L Buckley,"There are several ways to categorise reinforcement learning (RL) algorithms, such as either model-based or model-free, policy-based or planning-based, on-policy or off-policy, and online or offline. Broad classification schemes such as these help provide a unified perspective on disparate techniques and can contextualise and guide the development of new algorithms. In this paper, we utilise the control as inference framework to outline a novel classification scheme based on amortised and iterative inference. We demonstrate that a wide range of algorithms can be classified in this manner providing a fresh perspective and highlighting a range of existing similarities. Moreover, we show that taking this perspective allows us to identify parts of the algorithmic design space which have been relatively unexplored, suggesting new routes to innovative RL algorithms. △ Less","5 July, 2020",https://arxiv.org/pdf/2006.10524
DREAM: Deep Regret minimization with Advantage baselines and Model-free learning,Eric Steinberger;Adam Lerer;Noam Brown,"We introduce DREAM, a deep reinforcement learning algorithm that finds optimal strategies in imperfect-information games with multiple agents. Formally, DREAM converges to a Nash Equilibrium in two-player zero-sum games and to an extensive-form coarse correlated equilibrium in all other games. Our primary innovation is an effective algorithm that, in contrast to other regret-based deep learning algorithms, does not require access to a perfect simulator of the game to achieve good performance. We show that DREAM empirically achieves state-of-the-art performance among model-free algorithms in popular benchmark games, and is even competitive with algorithms that do use a perfect simulator. △ Less","29 November, 2020",https://arxiv.org/pdf/2006.10410
CoinWatch: A Clone-Based Approach For Detecting Vulnerabilities in Cryptocurrencies,Qingze Hum;Wei Jin Tan;Shi Ying Tey;Latasha Lenus;Ivan Homoliak;Yun Lin;Jun Sun,"Cryptocurrencies have become very popular in recent years. Thousands of new cryptocurrencies have emerged, proposing new and novel techniques that improve on Bitcoin's core innovation of the blockchain data structure and consensus mechanism. However, cryptocurrencies are a major target for cyber-attacks, as they can be sold on exchanges anonymously and most cryptocurrencies have their codebases publicly available. One particular issue is the prevalence of code clones in cryptocurrencies, which may amplify security threats. If a vulnerability is found in one cryptocurrency, it might be propagated into other cloned cryptocurrencies. In this work, we propose a systematic remedy to this problem, and we propose CoinWatch (CW). Given a reported vulnerability at the input, CW uses the code evolution analysis and a clone detection technique for indication of cryptocurrencies that might be vulnerable. We applied CW on 1094 cryptocurrencies using 4 CVEs and obtained 786 true vulnerabilities present in 384 projects, which were confirmed with developers and successfully reported as CVE extensions. △ Less","28 October, 2020",https://arxiv.org/pdf/2006.10280
Enterprise System Lifecycle-wide Innovation,Sachithra Lokuge;Darshana Sedera,"Enterprise Systems purport to bring innovation to organizations. Yet, no past studies, neither from innovation nor from ES disciplines have merged their knowledge to understand how ES could facilitate lifecycle-wide innovation. Therefore, this study forms conceptual bridge between the two disciplines. In this research, we seek to understand how ES could facilitate innovation across its lifecycle phases. We associate classifications of innovation such as radical vs. incremental, administrative vs. technical innovation with the three phases of ES lifecycle. We introduce Continuous Restrained Innovation (CRI) as a new type of innovation specific to ES, considering restraints of technology, business processes and organization. Our empirical data collection at the implementation phase, using data from both the client and implementation partner, shows preliminary evidence of CRI. In addition, we state that both parties consider the implementation of ES as a radical innovation yet, are less interest in seeking further innovations through the system. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.10237
Fairness-Oriented Semi-Chaotic Genetic Algorithm-Based Channel Assignment Technique for Nodes Starvation Problem in Wireless Mesh Network,Fuad A. Ghaleb;Bander Ali Saleh Al-rimy;Maznah Kamat;Mohd. Foad Rohani;Shukor Abd Razak,"Multi-Radio Multi-Channel Wireless Mesh Networks (WMNs) have emerged as a scalable, reliable, and agile wireless network that supports many types of innovative technologies such as the Internet of Things (IoT) and vehicular networks. Due to the limited number of orthogonal channels, interference between channels adversely affects the fair distribution of bandwidth among mesh clients, causing node starvation in terms of insufficient bandwidth, which impedes the adoption of WMN as an efficient access technology. Therefore, a fair channel assignment is crucial for the mesh clients to utilize the available resources. However, the node starvation problem due to unfair channel distribution has been vastly overlooked during channel assignment by the extant research. Instead, existing channel assignment algorithms either reduce the total network interference or maximize the total network throughput, which neither guarantees a fair distribution of the channels nor eliminates node starvation. To this end, the Fairness-Oriented Semi-Chaotic Genetic Algorithm-Based Channel Assignment Technique (FA-SCGA-CAA) was proposed in this paper for Nodes Starvation Problem in Wireless Mesh Networks. FA-SCGA-CAA optimizes fairness based on multiple-criterion using a modified version of the Genetic Algorithm (GA). The modification includes proposing a semi-chaotic technique for creating the primary chromosome with powerful genes. Such a chromosome was used to create a strong population that directs the search towards the global minima in an effective and efficient way. The outcome is a nonlinear fairness oriented fitness function that aims at maximizing the link fairness while minimizing the link interference. Comparison with related work shows that the proposed FA_SCGA_CAA reduced the potential nodes starvation by 22% and improved network capacity utilization by 23%. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.09655
Response by the Montreal AI Ethics Institute to the European Commission's Whitepaper on AI,Abhishek Gupta;Camylle Lanteigne,"In February 2020, the European Commission (EC) published a white paper entitled, On Artificial Intelligence - A European approach to excellence and trust. This paper outlines the EC's policy options for the promotion and adoption of artificial intelligence (AI) in the European Union. The Montreal AI Ethics Institute (MAIEI) reviewed this paper and published a response addressing the EC's plans to build an ""ecosystem of excellence"" and an ""ecosystem of trust,"" as well as the safety and liability implications of AI, the internet of things (IoT), and robotics. MAIEI provides 15 recommendations in relation to the sections outlined above, including: 1) focus efforts on the research and innovation community, member states, and the private sector; 2) create alignment between trading partners' policies and EU policies; 3) analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy AI; 4) focus on coordination and policy alignment; 5) focus on mechanisms that promote private and secure sharing of data; 6) create a network of AI research excellence centres to strengthen the research and innovation community; 7) promote knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8) add nuance to the discussion regarding the opacity of AI systems; 9) create a process for individuals to appeal an AI system's decision or output; 10) implement new rules and strengthen existing regulations; 11) ban the use of facial recognition technology; 12) hold all AI systems to similar standards and compulsory requirements; 13) ensure biometric identification systems fulfill the purpose for which they are implemented; 14) implement a voluntary labelling system for systems that are not considered high-risk; 15) appoint individuals to the oversight process who understand AI systems well and are able to communicate potential risks. △ Less","16 June, 2020",https://arxiv.org/pdf/2006.09428
Group Decision Support for agriculture planning by a combination of Mathematical Model and Collaborative Tool,Pascale Zaraté;Alemany Mme;Ana Esteso Alvarez;Amir Sakka;Guy Camilleri,"Decision making in the Agriculture domain can be a complex task. The land area allocated to each crop should be fixed every season according to several parameters: prices, demand, harvesting periods, seeds, ground, season etc... The decision to make becomes more difficult when a group of farmers must fix the price and all parameters all together. Generally, optimization models are useful for farmers to find no dominated solutions, but it remains difficult if the farmers have to agree on one solution. We combine two approaches in order to support a group of farmers engaged in this kind of decision making process. We firstly generate a set of no dominated solutions thanks to a centralized optimization model. Based on this set of solution we then used a Group Decision Support System called GRUS for choosing the best solution for the group of farmers. The combined approach allows us to determine the best solution for the group in a consensual way. This combination of approaches is very innovative for the Agriculture. This approach has been tested in laboratory in a previous work. In the current work the same experiment has been conducted with real business (farmers) in order to benefit from their expertise. The two experiments are compared. △ Less","15 June, 2020",https://arxiv.org/pdf/2006.08151
Multiclass Disease Predictions Based on Integrated Clinical and Genomics Datasets,Moeez M. Subhani;Ashiq Anjum,"Clinical predictions using clinical data by computational methods are common in bioinformatics. However, clinical predictions using information from genomics datasets as well is not a frequently observed phenomenon in research. Precision medicine research requires information from all available datasets to provide intelligent clinical solutions. In this paper, we have attempted to create a prediction model which uses information from both clinical and genomics datasets. We have demonstrated multiclass disease predictions based on combined clinical and genomics datasets using machine learning methods. We have created an integrated dataset, using a clinical (ClinVar) and a genomics (gene expression) dataset, and trained it using instance-based learner to predict clinical diseases. We have used an innovative but simple way for multiclass classification, where the number of output classes is as high as 75. We have used Principal Component Analysis for feature selection. The classifier predicted diseases with 73\% accuracy on the integrated dataset. The results were consistent and competent when compared with other classification models. The results show that genomics information can be reliably included in datasets for clinical predictions and it can prove to be valuable in clinical diagnostics and precision medicine. △ Less","14 June, 2020",https://arxiv.org/pdf/2006.07879
Optimistic Distributionally Robust Policy Optimization,Jun Song;Chaoyue Zhao,"Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), as the widely employed policy based reinforcement learning (RL) methods, are prone to converge to a sub-optimal solution as they limit the policy representation to a particular parametric distribution class. To address this issue, we develop an innovative Optimistic Distributionally Robust Policy Optimization (ODRPO) algorithm, which effectively utilizes Optimistic Distributionally Robust Optimization (DRO) approach to solve the trust region constrained optimization problem without parameterizing the policies. Our algorithm improves TRPO and PPO with a higher sample efficiency and a better performance of the final policy while attaining the learning stability. Moreover, it achieves a globally optimal policy update that is not promised in the prevailing policy based RL algorithms. Experiments across tabular domains and robotic locomotion tasks demonstrate the effectiveness of our approach. △ Less","14 June, 2020",https://arxiv.org/pdf/2006.07815
Dynamic Attention Based Generative Adversarial Network with Phase Post-Processing for Speech Enhancement,Andong Li;Chengshi Zheng;Renhua Peng;Cunhang Fan;Xiaodong Li,"The generative adversarial networks (GANs) have facilitated the development of speech enhancement recently. Nevertheless, the performance advantage is still limited when compared with state-of-the-art models. In this paper, we propose a powerful Dynamic Attention Recursive GAN called DARGAN for noise reduction in the time-frequency domain. Different from previous works, we have several innovations. First, recursive learning, an iterative training protocol, is used in the generator, which consists of multiple steps. By reusing the network in each step, the noise components are progressively reduced in a step-wise manner. Second, the dynamic attention mechanism is deployed, which helps to re-adjust the feature distribution in the noise reduction module. Third, we exploit the deep Griffin-Lim algorithm as the module for phase postprocessing, which facilitates further improvement in speech quality. Experimental results on Voice Bank corpus show that the proposed GAN achieves state-of-the-art performance than previous GAN- and non-GAN-based models △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07530
Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity,Adrien Ecoffet;Jeff Clune;Joel Lehman,"Artificial life originated and has long studied the topic of open-ended evolution, which seeks the principles underlying artificial systems that innovate continually, inspired by biological evolution. Recently, interest has grown within the broader field of AI in a generalization of open-ended evolution, here called open-ended search, wherein such questions of open-endedness are explored for advancing AI, whatever the nature of the underlying search algorithm (e.g. evolutionary or gradient-based). For example, open-ended search might design new architectures for neural networks, new reinforcement learning algorithms, or most ambitiously, aim at designing artificial general intelligence. This paper proposes that open-ended evolution and artificial life have much to contribute towards the understanding of open-ended AI, focusing here in particular on the safety of open-ended search. The idea is that AI systems are increasingly applied in the real world, often producing unintended harms in the process, which motivates the growing field of AI safety. This paper argues that open-ended AI has its own safety challenges, in particular, whether the creativity of open-ended systems can be productively and predictably controlled. This paper explains how unique safety problems manifest in open-ended search, and suggests concrete contributions and research questions to explore them. The hope is to inspire progress towards creative, useful, and safe open-ended search algorithms. △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07495
The 4th Industrial Revolution Effect on the Enterprise Cyber Strategy,Christopher Gorham,"The Fourth (4th) Industrial Revolution represents the profound advancement of technology that will likely transform the boundaries between the digital and physical worlds in modern society. The impact of advance technology will disrupt almost every aspect of business and government communities alike. In the past few years, the advancement of information technologies has opened the door to artificial intelligence (AI), block chain technologies, robotics, virtual reality and the possibility of quantum computing being released in the commercial sector. The use of these innovative technologies will likely impact society by leveraging modern technological platforms such as cloud computing and AI. This also includes the release of 5G network technologies by Internet Service Providers (ISP) beginning in 2019. Networks that rely upon 5G technologies in combination with cloud computing platforms will open the door allow greater innovations and change the nature of how work is performed in the 4th Industrial Revolution. △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07488
High-Level ETL for Semantic Data Warehouses -- Full Version,Rudra Pratap Deb Nath;Oscar Romero;Torben Bach Pedersen;Katja Hose,"The popularity of the Semantic Web (SW) encourages organizations to organize and publish semantic data using the RDF model. This growth poses new requirements to Business Intelligence (BI) technologies to enable On-Line Analytical Processing (OLAP)-like analysis over semantic data. The incorporation of semantic data into a Data Warehouse (DW) is not supported by the traditional Extract-Transform-Load (ETL) tools because they do not consider semantic issues in the integration process. In this paper, we propose a layer-based integration process and a set of high-level RDF-based ETL constructs required to define, map, extract, process, transform, integrate, update, and load (multidimensional) semantic data. Different to other ETL tools, we automate the ETL data flows by creating metadata at the schema level. Therefore, it relieves ETL developers from the burden of manual mapping at the ETL operation level. We create a prototype, named Semantic ETL Construct (SETLCONSTRUCT), based on the innovative ETL constructs proposed here. To evaluate SETLCONSTRUCT, we create a multidimensional semantic DW by integrating a Danish Business dataset and an EU Subsidy dataset using it and compare it with the previous programmable framework SETLPROG in terms of productivity, development time and performance. The evaluation shows that 1) SETLCONSTRUCT uses 92% fewer Number of Typed Characters (NOTC) than SETLPROG, and SETLAUTO (the extension of SETLCONSTRUCT for generating ETL execution flow automatically) further reduces the Number of Used Concepts (NOUC) by another 25%; 2) using SETLCONSTRUCT, the development time is almost cut in half compared to SETLPROG, and is cut by another 27% using SETLAUTO; 3) SETLCONSTRUCT is scalable and has similar performance compared to SETLPROG. △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07180
Reserve Price Optimization for First Price Auctions,Zhe Feng;Sébastien Lahaie;Jon Schneider;Jinchao Ye,"The display advertising industry has recently transitioned from second- to first-price auctions as its primary mechanism for ad allocation and pricing. In light of this, publishers need to re-evaluate and optimize their auction parameters, notably reserve prices. In this paper, we propose a gradient-based algorithm to adaptively update and optimize reserve prices based on estimates of bidders' responsiveness to experimental shocks in reserves. Our key innovation is to draw on the inherent structure of the revenue objective in order to reduce the variance of gradient estimates and improve convergence rates in both theory and practice. We show that revenue in a first-price auction can be usefully decomposed into a \emph{demand} component and a \emph{bidding} component, and introduce techniques to reduce the variance of each component. We characterize the bias-variance trade-offs of these techniques and validate the performance of our proposed algorithm through experiments on synthetic data and real display ad auctions data from Google ad exchange. △ Less","28 June, 2020",https://arxiv.org/pdf/2006.06519
COVID-19 Contact-Tracing Mobile Apps: Evaluation and Assessment for Decision Makers,Ramesh Raskar;Greg Nadeau;John Werner;Rachel Barbar;Ashley Mehra;Gabriel Harp;Markus Leopoldseder;Bryan Wilson;Derrick Flakoll;Praneeth Vepakomma;Deepti Pahwa;Robson Beaudry;Emelin Flores;Maciej Popielarz;Akanksha Bhatia;Andrea Nuzzo;Matt Gee;Jay Summet;Rajeev Surati;Bikram Khastgir;Francesco Maria Benedetti;Kristen Vilcans;Sienna Leis;Khahlil Louisy,"A number of groups, from governments to non-profits, have quickly acted to innovate the contact-tracing process: they are designing, building, and launching contact-tracing apps in response to the COVID-19 crisis. A diverse range of approaches exist, creating challenging choices for officials looking to implement contact-tracing technology in their community and raising concerns about these choices among citizens asked to participate in contact tracing. We are frequently asked how to evaluate and differentiate between the options for contact-tracing applications. Here, we share the questions we ask about app features and plans when reviewing the many contact-tracing apps appearing on the global stage. △ Less","3 June, 2020",https://arxiv.org/pdf/2006.05812
"Blockchain in the management of science: conceptual models, promises and challenges",Artyom Kosmarski,"Blockchain has received much attention recently, due to its promises of verifiable, permanent, decentralized, and efficient data handling. In 2017-2019 blockchain and associated technologies such as smart contracts has progressed beyond cryptocurrencies, and has been adopted in banking, retail, healthcare, and other fields. This study critically examines recent applications of blockchain in science, touching upon different stages of research cycle, from data management to publishing, peer review, research evaluation and funding. The paper is based upon a review of blockchain projects, relevant literature, a set of interviews and focus groups with startup founders, scholars, librarians, IT experts from the EU, USA, Russia, and Belarus. Proponents of blockchain for science present this technology as a tool to make science free from bias, red tape, data fraud, as well as provide innovative means to secure financial backing for new ideas. However, these projects face a set of challenges. One issue concerns introducing crypto economy, with its financial incentives, into science, a field that emphasizes disinterested and non-pecuniary pursuit of truth. Another source of concern relates to the ongoing conflict between the principle of decentralization inherent to blockchain and the practice of forcing it from above, by the state and other centralized entities. △ Less","9 June, 2020",https://arxiv.org/pdf/2006.05483
Sustainability of ICT hardware procurement in Switzerland -- A status-quo analysis of the public procurement sector,Tobias Welz;Matthias Stuermer,"Sustainable procurement requires organizations to align their purchasing behavior with regard to broader goals linked to resource efficiency, climate change, social responsibility and other sustainability criteria. The level of sustainability in Information and Communication Technology (ICT) hardware procurement is analyzed for two reasons: First, ICT hardware belongs to the six key product groups in sustainable procurement. Second, ICT in general is expected to be an important enabler for low-carbon economies, providing solutions to reduce Green-House Gas (GHG) emissions. While the advantages of sustainable procurement are obvious, certain barriers hinder the adoption in day-to-day procurement. This case study on ICT hardware discusses the three important barriers ""lack of clear definitions per product group"", ""missing market intelligence about sustainable products"" and ""inflexible procedures and attitudes as barriers for innovative approaches"" based on an in-depth analysis of sustainable procurement of ICT hardware by the public sector in Switzerland. To this end, tender data published on the national procurement platform simap.ch is screened for sustainability criteria using the Common Procurement Vocabulary (CPV) nomenclature to identify relevant ICT projects. The results reveal to which extent such criteria as well as their determinants are currently included in public tenders. Using two different approaches, only a small number of tenders were found containing sustainability criteria of a wide range from basic to comprehensive. The overall performance of Swiss public procurement is benchmarked by comparing the identified sustainability criteria with available criteria from international key actors in sustainable procurement. Thus, this analysis provides novel insights on how public agencies today take sustainability into account when procuring ICT hardware. △ Less","9 June, 2020",https://arxiv.org/pdf/2006.05372
Enterprise Systems Lifecycle-wide Innovation Readiness,Sachithra Lokuge;Darshana Sedera,"Enterprise Systems have been touted as a key driver of delivering benefits through innovation in corporate Information Systems. The advent of such systems expects to deliver best practices that improve organizational performance. Yet, most Enterprise System installations struggle to see lifecycle-wide value of it. Considering that Enterprise Systems deliver lifecycle-wide innovation; we observe organizational readiness for lifecycle-wide Enterprise Systems innovation. The A VICTORY apriori model compares contributions of eight constructs for organizational readiness for continuous Enterprise Systems innovation. The model is tested responses of both client and implementation partner. Results indicate that six of the eight constructs of readiness make significant contributions to organizational readiness for Enterprise Systems innovation. △ Less","9 June, 2020",https://arxiv.org/pdf/2006.05089
"Softwarization, Virtualization, & Machine Learning For Intelligent & Effective V2X Communications",Abdallah Moubayed;Abdallah Shami,"The concept of the fifth generation (5G) mobile network system has emerged in recent years as telecommunication operators and service providers look to upgrade their infrastructure and delivery modes to meet the growing demand. Concepts such as softwarization, virtualization, and machine learning will be key components as innovative and flexible enablers of such networks. In particular, paradigms such as software-defined networks, software-defined perimeter, cloud & edge computing, and network function virtualization will play a major role in addressing several 5G networks' challenges, especially in terms of flexibility, programmability, scalability, and security. In this work, the role and potential of these paradigms in the context of V2X communication is discussed. To do so, the paper starts off by providing an overview and background of V2X communications. Then, the paper discusses in more details the various challenges facing V2X communications and some of the previous literature work done to tackle them. Furthermore, the paper describes how softwarization, virtualization, and machine learning can be adapted to tackle the challenges of such networks. △ Less","8 June, 2020",https://arxiv.org/pdf/2006.04595
Ethical Considerations and Statistical Analysis of Industry Involvement in Machine Learning Research,Thilo Hagendorff;Kristof Meding,"Industry involvement in the machine learning (ML) community seems to be increasing. However, the quantitative scale and ethical implications of this influence are rather unknown. For this purpose, we have not only carried out an informed ethical analysis of the field, but have inspected all papers of the main ML conferences NeurIPS, CVPR, and ICML of the last 5 years - almost 11,000 papers in total. Our statistical approach focuses on conflicts of interest, innovation and gender equality. We have obtained four main findings: (1) Academic-corporate collaborations are growing in numbers. At the same time, we found that conflicts of interest are rarely disclosed. (2) Industry publishes papers about trending ML topics on average two years earlier than academia does. (3) Industry papers are not lagging behind academic papers in regard to social impact considerations. (4) Finally, we demonstrate that industrial papers fall short of their academic counterparts with respect to the ratio of gender diversity. We believe that this work is a starting point for an informed debate within and outside of the ML community. △ Less","19 October, 2020",https://arxiv.org/pdf/2006.04541
Robust Controller Design for Stochastic Nonlinear Systems via Convex Optimization,Hiroyasu Tsukamoto;Soon-Jo Chung,"This paper presents ConVex optimization-based Stochastic steady-state Tracking Error Minimization (CV-STEM), a new state feedback control framework for a class of Ito stochastic nonlinear systems and Lagrangian systems. Its innovation lies in computing the control input by an optimal contraction metric, which greedily minimizes an upper bound of the steady-state mean squared tracking error of the system trajectories. Although the problem of minimizing the bound is non-convex, its equivalent convex formulation is proposed utilizing state-dependent coefficient parameterizations of the nonlinear system equation. It is shown using stochastic incremental contraction analysis that the CV-STEM provides a sufficient guarantee for exponential boundedness of the error for all time with L2-robustness properties. For the sake of its sampling-based implementation, we present discrete-time stochastic contraction analysis with respect to a state- and time-dependent metric along with its explicit connection to continuous-time cases. We validate the superiority of the CV-STEM to PID, H-infinity, and baseline nonlinear controllers for spacecraft attitude control and synchronization problems. △ Less","19 November, 2020",https://arxiv.org/pdf/2006.04359
Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud Object Detection,Liang Du;Xiaoqing Ye;Xiao Tan;Jianfeng Feng;Zhenbo Xu;Errui Ding;Shilei Wen,"Object detection from 3D point clouds remains a challenging task, though recent studies pushed the envelope with the deep learning techniques. Owing to the severe spatial occlusion and inherent variance of point density with the distance to sensors, appearance of a same object varies a lot in point cloud data. Designing robust feature representation against such appearance changes is hence the key issue in a 3D object detection method. In this paper, we innovatively propose a domain adaptation like approach to enhance the robustness of the feature representation. More specifically, we bridge the gap between the perceptual domain where the feature comes from a real scene and the conceptual domain where the feature is extracted from an augmented scene consisting of non-occlusion point cloud rich of detailed information. This domain adaptation approach mimics the functionality of the human brain when proceeding object perception. Extensive experiments demonstrate that our simple yet effective approach fundamentally boosts the performance of 3D point cloud object detection and achieves the state-of-the-art results. △ Less","8 June, 2020",https://arxiv.org/pdf/2006.04356
Generative Adversarial Phonology: Modeling unsupervised phonetic and phonological learning with neural networks,Gašper Beguš,"Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network's internal representations that correspond to phonetic and phonological properties. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English. The network successfully learns the allophonic alternation: the network's generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network's internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network's architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors, and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations. △ Less","6 June, 2020",https://arxiv.org/pdf/2006.03965
Blended Learning Content Generation: A Guide for Busy Academics,Richard Hill,A practical guide for university academics who need to create learning materials that support flexible delivery methods. Examples from the Computer Science domain are used to illustrate innovative approaches to engaging students with online and blended teaching resources. △ Less,"5 June, 2020",https://arxiv.org/pdf/2006.03730
How (Un)Happiness Impacts on Software Engineers in Agile Teams?,Luís Felipe Amorim;Marcelo Marinho;Suzana Sampaio,"Information technology (IT) organizations are increasing the use of agile practices, which are based on a people-centred culture alongside the software development process. Thus, it is vital to understand the social and human factors of the individuals working in agile environments, such as happiness and unhappiness and how these factors impact this kind of environment. Therefore, five case-studies were developed inside agile projects, in a company that values innovation, aiming to identify how (un)happiness impacts software engineers in agile environments. According to the answers gathered from 67 participants through a survey, interviews and using a cross-analysis, happiness factors identified by agile teams were effective communication, motivated members, collaboration among members, proactive members, and present leaders. △ Less","5 June, 2020",https://arxiv.org/pdf/2006.03546
Digital Currency and Economic Crises: Helping States Respond,Geoffrey Goodell;Hazem Danny Al-Nakib;Paolo Tasca,"The current crisis, at the time of writing, has had a profound impact on the financial world, introducing the need for creative approaches to revitalising the economy at the micro level as well as the macro level. In this informal analysis and design proposal, we describe how infrastructure for digital assets can serve as a useful monetary and fiscal policy tool and an enabler of existing tools in the future, particularly during crises, while aligning the trajectory of financial technology innovation toward a brighter future. We propose an approach to digital currency that would allow people without banking relationships to transact electronically and privately, including both internet purchases and point-of-sale purchases that are required to be cashless. We also propose an approach to digital currency that would allow for more efficient and transparent clearing and settlement, implementation of monetary and fiscal policy, and management of systemic risk. The digital currency could be implemented as central bank digital currency (CBDC), or it could be issued by the government and collateralised by public funds or Treasury assets. Our proposed architecture allows both manifestations and would be operated by banks and other money services businesses, operating within a framework overseen by government regulators. We argue that now is the time for action to undertake development of such a system, not only because of the current crisis but also in anticipation of future crises resulting from geopolitical risks, the continued globalisation of the digital economy, and the changing value and risks that technology brings. △ Less","2 August, 2020",https://arxiv.org/pdf/2006.03023
CircleNet: Anchor-free Detection with Circle Representation,Haichun Yang;Ruining Deng;Yuzhe Lu;Zheyu Zhu;Ye Chen;Joseph T. Roland;Le Lu;Bennett A. Landman;Agnes B. Fogo;Yuankai Huo,"Object detection networks are powerful in computer vision, but not necessarily optimized for biomedical object detection. In this work, we propose CircleNet, a simple anchor-free detection method with circle representation for detection of the ball-shaped glomerulus. Different from the traditional bounding box based detection method, the bounding circle (1) reduces the degrees of freedom of detection representation, (2) is naturally rotation invariant, (3) and optimized for ball-shaped objects. The key innovation to enable this representation is the anchor-free framework with the circle detection head. We evaluate CircleNet in the context of detection of glomerulus. CircleNet increases average precision of the glomerulus detection from 0.598 to 0.647. Another key advantage is that CircleNet achieves better rotation consistency compared with bounding box representations. △ Less","3 June, 2020",https://arxiv.org/pdf/2006.02474
Technological impact of biomedical research: the role of basicness and novelty,Qing Ke,"An ongoing interest in innovation studies is to understand how knowledge generated from scientific research can be used in the development of technologies. While previous inquiries have devoted to studying the scientific capacity of technologies and institutional factors facilitating technology transfer, little is known about the intrinsic characteristics of scientific publications that gain direct technological impact. Here we focus on two features, namely basicness and novelty. Using a corpus of 3.8 million papers published between 1980 and 1999, we find that basic science papers and novel papers are substantially more likely to achieve direct technological impact. Further analysis that limits to papers with technological impact reveals that basic science and novel science have more patent citations, experience shorter time lag, and have impact in broader technological fields. △ Less","3 June, 2020",https://arxiv.org/pdf/2006.02472
"AI-Powered Learning: Making Education Accessible, Affordable, and Achievable",Ashok Goel,"We have developed an AI-powered socio-technical system for making online learning in higher education more accessible, affordable and achievable. In particular, we have developed four novel and intertwined AI technologies: (1) VERA, a virtual experimentation research assistant for supporting inquiry-based learning of scientific knowledge, (2) Jill Watson Q&A, a virtual teaching assistant for answering questions based on educational documents including the VERA user reference guide, (3) Jill Watson SA, a virtual social agent that promotes online interactions, and (4) Agent Smith, that helps generate a Jill Watson Q&A agent for new documents such as class syllabi. The results are positive: (i) VERA enhances ecological knowledge and is freely available online; (ii) Jill Watson Q&A has been used by >4,000 students in >12 online classes and saved teachers >500 hours of work; (iii) Jill Q&A and Jill Watson SA promote learner engagement, interaction, and community; and (iv). Agent Smith helps generate Jill Watson Q&A for a new syllabus within ~25 hours. Put together, these innovative technologies help make online learning simultaneously more accessible (by making materials available online), affordable (by saving teacher time), and achievable (by providing learning assistance and fostering student engagement). △ Less","2 June, 2020",https://arxiv.org/pdf/2006.01908
Private 5G: The Future of Industrial Wireless,Adnan Aijaz,"High-performance wireless communication is crucial in digital transformation of industrial systems which is driven by Industry 4.0 and the Industrial Internet initiatives. Among the candidate industrial wireless technologies, 5G (cellular/mobile) holds significant potential. Operation of private (non-public) 5G networks in industrial environments is promising to fully unleash this potential. This article provides a technical overview of private 5G networks. It introduces the concept and functional architecture of private 5G while highlighting the key benefits and industrial use-cases. It explores spectrum opportunities for private 5G networks. It also discusses design aspects of private 5G along with the key challenges. Finally, it explores the emerging standardization and open innovation ecosystem for private 5G. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.01820
Reducing the X-ray radiation exposure frequency in cardio-angiography via deep-learning based video interpolation,Xiao-Lei Yin;Dong-Xue Liang;Lu Wang;Jing Qiu;Zhi-Yun Yang;Jun-Hui Xing;Jian-Zeng Dong;Zhao-Yuan Ma,"Cardiac coronary angiography is a major technology to assist doctors during cardiac interventional surgeries. Under the exposure of X-ray radiation, doctors inject contrast agents through catheters to determine the position and status of coronary vessels in real time. To get a coronary angiography video with a high frame rate, the doctor needs to increase the exposure frequency and intensity of the X-ray. This will inevitably increase the X-ray harm to both patients and surgeons. In this work, we innovatively utilize a deep-learning based video interpolation algorithm to interpolate coronary angiography videos. Moreover, we establish a new coronary angiography image dataset ,which contains 95,039 triplets images to retrain the video interpolation network model. Using the retrained network we synthesize high frame rate coronary angiography video from the low frame rate coronary angiography video. The average peak signal to noise ratio(PSNR) of those synthesized video frames reaches 34dB. Extensive experiment results demonstrate the feasibility of using the video frame interpolation algorithm to synthesize continuous and clear high frame rate coronary angiography video. With the help of this technology, doctors can significantly reduce exposure frequency and intensity of the X-ray during coronary angiography. △ Less","1 June, 2020",https://arxiv.org/pdf/2006.00781
MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning,Parvin Malekzadeh;Mohammad Salimibeni;Arash Mohammadi;Akbar Assa;Konstantinos N. Plataniotis,"There has been an increasing surge of interest on development of advanced Reinforcement Learning (RL) systems as intelligent approaches to learn optimal control policies directly from smart agents' interactions with the environment. Objectives: In a model-free RL method with continuous state-space, typically, the value function of the states needs to be approximated. In this regard, Deep Neural Networks (DNNs) provide an attractive modeling mechanism to approximate the value function using sample transitions. DNN-based solutions, however, suffer from high sensitivity to parameter selection, are prone to overfitting, and are not very sample efficient. A Kalman-based methodology, on the other hand, could be used as an efficient alternative. Such an approach, however, commonly requires a-priori information about the system (such as noise statistics) to perform efficiently. The main objective of this paper is to address this issue. Methods: As a remedy to the aforementioned problems, this paper proposes an innovative Multiple Model Kalman Temporal Difference (MM-KTD) framework, which adapts the parameters of the filter using the observed states and rewards. Moreover, an active learning method is proposed to enhance the sampling efficiency of the system. More specifically, the estimated uncertainty of the value functions are exploited to form the behaviour policy leading to more visits to less certain values, therefore, improving the overall learning sample efficiency. As a result, the proposed MM-KTD framework can learn the optimal policy with significantly reduced number of samples as compared to its DNN-based counterparts. Results: To evaluate performance of the proposed MM-KTD framework, we have performed a comprehensive set of experiments based on three RL benchmarks. Experimental results show superiority of the MM-KTD framework in comparison to its state-of-the-art counterparts. △ Less","30 May, 2020",https://arxiv.org/pdf/2006.00195
Exploring Spatial Significance via Hybrid Pyramidal Graph Network for Vehicle Re-identification,Fei Shen;Jianqing Zhu;Xiaobin Zhu;Yi Xie;Jingchang Huang,"Existing vehicle re-identification methods commonly use spatial pooling operations to aggregate feature maps extracted via off-the-shelf backbone networks. They ignore exploring the spatial significance of feature maps, eventually degrading the vehicle re-identification performance. In this paper, firstly, an innovative spatial graph network (SGN) is proposed to elaborately explore the spatial significance of feature maps. The SGN stacks multiple spatial graphs (SGs). Each SG assigns feature map's elements as nodes and utilizes spatial neighborhood relationships to determine edges among nodes. During the SGN's propagation, each node and its spatial neighbors on an SG are aggregated to the next SG. On the next SG, each aggregated node is re-weighted with a learnable parameter to find the significance at the corresponding location. Secondly, a novel pyramidal graph network (PGN) is designed to comprehensively explore the spatial significance of feature maps at multiple scales. The PGN organizes multiple SGNs in a pyramidal manner and makes each SGN handles feature maps of a specific scale. Finally, a hybrid pyramidal graph network (HPGN) is developed by embedding the PGN behind a ResNet-50 based backbone network. Extensive experiments on three large scale vehicle databases (i.e., VeRi776, VehicleID, and VeRi-Wild) demonstrate that the proposed HPGN is superior to state-of-the-art vehicle re-identification approaches. △ Less","4 June, 2020",https://arxiv.org/pdf/2005.14684
Use and Adaptation of Open Source Software for Capacity Building to Strengthen Health Research in Low- and Middle-Income Countries,Stefan Hochwarter;Salla Atkins;Vinod K. Diwan;Nabil Zary,"Health research capacity strengthening is of importance to reach health goals. The ARCADE projects' aim was to strengthen health research across Africa and Asia using innovative educational technologies. In the four years of the EU funded projects, challenges also of technical nature were identified. This article reports on a study conducted within the ARCADE projects. The study focused on addressing challenges of video conferencing in resource constrained settings and was conducted using action research. As a result, a plugin for the open source video conferencing system minisip was implemented and evaluated. The study showed that both the audio and video streams could be improved by the introduced plugin, which addressed one technical challenge. △ Less","28 May, 2020",https://arxiv.org/pdf/2005.14233
Aquareum: A Centralized Ledger Enhanced with Blockchain and Trusted Computing,Ivan Homoliak;Pawel Szalachowski,"Distributed ledger systems (i.e., blockchains) have received a lot of attention recently. They promise to enable mutually untrusted participants to execute transactions, while providing the immutability of the transaction history and censorship resistance. Although decentralized ledgers may become a disruptive innovation, as of today, they suffer from scalability, privacy, or governance issues. Therefore, they are inapplicable for many important use cases, where interestingly, centralized ledger systems quietly gain adoption and find new use cases. Unfortunately, centralized ledgers have also several drawbacks, like a lack of efficient verifiability or a higher risk of censorship and equivocation. In this paper, we present Aquareum, a novel framework for centralized ledgers removing their main limitations. By combining a trusted execution environment with a public blockchain platform, Aquareum provides publicly verifiable, non-equivocating, censorship-evident, private, and high-performance ledgers. Aquareum ledgers are integrated with a Turing-complete virtual machine, allowing arbitrary transaction processing logics, including tokens or client-specified smart contracts. Aquareum is fully implemented and deployment-ready, even with currently existing technologies. △ Less","27 May, 2020",https://arxiv.org/pdf/2005.13339
Oligopoly Dynamics,Bernardo Melo Pimentel,"The present notes summarise the oligopoly dynamics lectures professor Luís Cabral gave at the Bank of Portugal in September and October 2017. The lectures discuss a set industrial organisation problems in a dynamic environment, namely learning by doing, switching costs, price wars, networks and platforms, and ladder models of innovation. Methodologically, the materials cover analytical solutions of known points (e.g., δ= 0), the discussion of firms' strategies based on intuitions derived directly from their value functions with no model solving, and the combination of analytical and numerical procedures to reach model solutions. State space analysis is done for both continuous and discrete cases. All errors are my own. △ Less","27 May, 2020",https://arxiv.org/pdf/2005.13228
TSML (Time Series Machine Learnng),Paulito Palmes;Joern Ploennigs;Niall Brady,"Over the past years, the industrial sector has seen many innovations brought about by automation. Inherent in this automation is the installation of sensor networks for status monitoring and data collection. One of the major challenges in these data-rich environments is how to extract and exploit information from these large volume of data to detect anomalies, discover patterns to reduce downtimes and manufacturing errors, reduce energy usage, predict faults/failures, effective maintenance schedules, etc. To address these issues, we developed TSML. Its technology is based on using the pipeline of lightweight filters as building blocks to process huge amount of industrial time series data in parallel. △ Less","27 May, 2020",https://arxiv.org/pdf/2005.13191
On the benefit of 3-tier SOA architecture promoting information sharing among TMS systems and Brazilian e- Government Web Services: A CT-e case study,Thiago Suzuki;Larissa Suzuki,"Technological advances regarding software integration processes have revolutionized the communication between government and society, which are increasingly based on information and communication technologies (ICTs). Service-Oriented Architecture (SOA) has emerged originating new prospects for system integration within organizations and external partners, providing essential information for decision-making process. Brazilian e-Government initiatives has introduced as a national electronic document model known as Electronic Transportation Knowledge (CT-e), looking for simplifying additional obligations of taxpayers and, at the same time, allowing real-time monitoring of cargo transportation services provided by the Revenue. Nonetheless, there is a major challenge that prevents Transportation Management Systems (TMS) and Government to be benefited by this innovation, due to their distinct platforms and databases that prevent information exchange between them. This paper proposes an architectural solution to integrate TMS systems with CT-e Web Service applying concepts of SOA and N-tier architecture. Through a real case study of a large Cargo carrier, we report an increase in transportation knowledge management, speeding up in the communication and data validation process and several costs reduction including paper, printing, document storage and those involved in the necessary logistics that is needed to recover such documents. △ Less","26 May, 2020",https://arxiv.org/pdf/2005.13047
DimensionRank: Personal Neural Representations for Personalized General Search,Gregory Coppola,"Web Search and Social Media have always been two of the most important applications on the internet. We begin by giving a unified framework, called general search, of which which all search and social media products can be seen as instances. DimensionRank is our main contribution. This is an algorithm for personalized general search, based on neural networks. DimensionRank's bold innovation is to model and represent each user using their own unique personal neural representation vector, a learned representation in a real-valued multidimensional vector space. This is the first internet service we are aware of that to model each user with their own independent representation vector. This is also the first service we are aware of to attempt personalization for general web search. Also, neural representations allows us to present the first Reddit-style algorithm, that is immune to the problem of ""brigading"". We believe personalized general search will yield a search product orders of magnitude better than Google's one-size-fits-all web search algorithm. Finally, we announce Deep Revelations, a new search and social network internet application based on DimensionRank. △ Less","26 May, 2020",https://arxiv.org/pdf/2005.13007
Local Motion Planner for Autonomous Navigation in Vineyards with a RGB-D Camera-Based Algorithm and Deep Learning Synergy,Diego Aghi;Vittorio Mazzia;Marcello Chiaberge,"With the advent of agriculture 3.0 and 4.0, researchers are increasingly focusing on the development of innovative smart farming and precision agriculture technologies by introducing automation and robotics into the agricultural processes. Autonomous agricultural field machines have been gaining significant attention from farmers and industries to reduce costs, human workload, and required resources. Nevertheless, achieving sufficient autonomous navigation capabilities requires the simultaneous cooperation of different processes; localization, mapping, and path planning are just some of the steps that aim at providing to the machine the right set of skills to operate in semi-structured and unstructured environments. In this context, this study presents a low-cost local motion planner for autonomous navigation in vineyards based only on an RGB-D camera, low range hardware, and a dual layer control algorithm. The first algorithm exploits the disparity map and its depth representation to generate a proportional control for the robotic platform. Concurrently, a second back-up algorithm, based on representations learning and resilient to illumination variations, can take control of the machine in case of a momentaneous failure of the first block. Moreover, due to the double nature of the system, after initial training of the deep learning model with an initial dataset, the strict synergy between the two algorithms opens the possibility of exploiting new automatically labeled data, coming from the field, to extend the existing model knowledge. The machine learning algorithm has been trained and tested, using transfer learning, with acquired images during different field surveys in the North region of Italy and then optimized for on-device inference with model pruning and quantization. Finally, the overall system has been validated with a customized robot platform in the relevant environment. △ Less","26 May, 2020",https://arxiv.org/pdf/2005.12815
A Big Data Based Framework for Executing Complex Query Over COVID-19 Datasets (COVID-QF),Eman A. Khashan;Ali I. Eldesouky;M. Fadel;Sally M. Elghamrawy,"COVID-19's rapid global spread has driven innovative tools for Big Data Analytics. These have guided organizations in all fields of the health industry to track and minimized the effects of virus. Researchers are required to detect coronaviruses through artificial intelligence, machine learning, and natural language processing, and to gain a complete understanding of the disease. COVID-19 takes place in different countries in the world, with which only big data application and the work of NOSQL databases are suitable. There is a great number of platforms used for processing NOSQL Databases model like: Spark, H2O and Hadoop HDFS/MapReduce, which are proper to control and manage the enormous amount of data. Many challenges faced by large applications programmers, especially those that work on the COVID-19 databases through hybrid data models through different APIs and query. In this context, this paper proposes a storage framework to handle both SQL and NOSQL databases named (COVID-QF) for COVID-19 datasets in order to treat and handle the problems caused by virus spreading worldwide by reducing treatment times. In case of NoSQL database, COVID-QF uses Hadoop HDFS/Map Reduce and Apache Spark. The COVID-QF consists of three Layers: data collection layer, storage layer, and query Processing layer. The data is collected in the data collection layer. The storage layer divides data into collection of data-saving and processing blocks, and it connects the Connector of the spark with different databases engine to reduce time of saving and retrieving. While the Processing layer executes the request query and sends results. The proposed framework used three datasets increased for time for COVID-19 data (COVID-19-Merging, COVID-19-inside-Hubei and COVID-19-ex-Hubei) to test experiments of this study. The results obtained insure the superiority of the COVID-QF framework. △ Less","24 May, 2020",https://arxiv.org/pdf/2005.12271
Automating the Surveillance of Mosquito Vectors from Trapped Specimens Using Computer Vision Techniques,Mona Minakshi;Pratool Bharti;Willie B. McClinton III;Jamshidbek Mirzakhalov;Ryan M. Carney;Sriram Chellappan,"Among all animals, mosquitoes are responsible for the most deaths worldwide. Interestingly, not all types of mosquitoes spread diseases, but rather, a select few alone are competent enough to do so. In the case of any disease outbreak, an important first step is surveillance of vectors (i.e., those mosquitoes capable of spreading diseases). To do this today, public health workers lay several mosquito traps in the area of interest. Hundreds of mosquitoes will get trapped. Naturally, among these hundreds, taxonomists have to identify only the vectors to gauge their density. This process today is manual, requires complex expertise/ training, and is based on visual inspection of each trapped specimen under a microscope. It is long, stressful and self-limiting. This paper presents an innovative solution to this problem. Our technique assumes the presence of an embedded camera (similar to those in smart-phones) that can take pictures of trapped mosquitoes. Our techniques proposed here will then process these images to automatically classify the genus and species type. Our CNN model based on Inception-ResNet V2 and Transfer Learning yielded an overall accuracy of 80% in classifying mosquitoes when trained on 25,867 images of 250 trapped mosquito vector specimens captured via many smart-phone cameras. In particular, the accuracy of our model in classifying Aedes aegypti and Anopheles stephensi mosquitoes (both of which are deadly vectors) is amongst the highest. We present important lessons learned and practical impact of our techniques towards the end of the paper. △ Less","21 July, 2020",https://arxiv.org/pdf/2005.12188
How Does That Sound? Multi-Language SpokenName2Vec Algorithm Using Speech Generation and Deep Learning,Aviad Elyashar;Rami Puzis;Michael Fire,"Searching for information about a specific person is an online activity frequently performed by many users. In most cases, users are aided by queries containing a name and sending back to the web search engines for finding their will. Typically, Web search engines provide just a few accurate results associated with a name-containing query. Currently, most solutions for suggesting synonyms in online search are based on pattern matching and phonetic encoding, however very often, the performance of such solutions is less than optimal. In this paper, we propose SpokenName2Vec, a novel and generic approach which addresses the similar name suggestion problem by utilizing automated speech generation, and deep learning to produce spoken name embeddings. This sophisticated and innovative embeddings captures the way people pronounce names in any language and accent. Utilizing the name pronunciation can be helpful for both differentiating and detecting names that sound alike, but are written differently. The proposed approach was demonstrated on a large-scale dataset consisting of 250,000 forenames and evaluated using a machine learning classifier and 7,399 names with their verified synonyms. The performance of the proposed approach was found to be superior to 10 other algorithms evaluated in this study, including well used phonetic and string similarity algorithms, and two recently proposed algorithms. The results obtained suggest that the proposed approach could serve as a useful and valuable tool for solving the similar name suggestion problem. △ Less","21 July, 2020",https://arxiv.org/pdf/2005.11838
Delving into the Imbalance of Positive Proposals in Two-stage Object Detection,Zheng Ge;Zequn Jie;Xin Huang;Chengzheng Li;Osamu Yoshie,"Imbalance issue is a major yet unsolved bottleneck for the current object detection models. In this work, we observe two crucial yet never discussed imbalance issues. The first imbalance lies in the large number of low-quality RPN proposals, which makes the R-CNN module (i.e., post-classification layers) become highly biased towards the negative proposals in the early training stage. The second imbalance stems from the unbalanced ground-truth numbers across different testing images, resulting in the imbalance of the number of potentially existing positive proposals in testing phase. To tackle these two imbalance issues, we incorporates two innovations into Faster R-CNN: 1) an R-CNN Gradient Annealing (RGA) strategy to enhance the impact of positive proposals in the early training stage. 2) a set of Parallel R-CNN Modules (PRM) with different positive/negative sampling ratios during training on one same backbone. Our RGA and PRM can totally bring 2.0% improvements on AP on COCO minival. Experiments on CrowdHuman further validates the effectiveness of our innovations across various kinds of object detection tasks. △ Less","23 May, 2020",https://arxiv.org/pdf/2005.11472
A Dynamic Tree Algorithm for On-demand Peer-to-peer Ride-sharing Matching,Rui Yao;Shlomo Bekhor,"Innovative shared mobility services provide on-demand flexible mobility options and have the potential to alleviate traffic congestion. These attractive services are challenging from different perspectives. One major challenge in such systems is to find suitable ride-sharing matchings between drivers and passengers with respect to the system objective and constraints, and to provide optimal pickup and drop-off sequence to the drivers. In this paper, we develop an efficient dynamic tree algorithm to find the optimal pickup and drop-off sequence. The algorithm finds an initial solution to the problem, keeps track of previously explored feasible solutions, and reduces the solution search space when considering new requests. In addition, an efficient pre-processing procedure to select candidate passenger requests is proposed, which further improves the algorithm performance. Numerical experiments are conducted on a real size network to illustrate the efficiency of our algorithm. Sensitivity analysis suggests that small vehicle capacities and loose excess travel time constraints do not guarantee overall savings in vehicle kilometer traveled. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.11195
Living Machines: A study of atypical animacy,Mariona Coll Ardanuy;Federico Nanni;Kaspar Beelen;Kasra Hosseini;Ruth Ahnert;Jon Lawrence;Katherine McDonough;Giorgia Tolfo;Daniel CS Wilson;Barbara McGillivray,"This paper proposes a new approach to animacy detection, the task of determining whether an entity is represented as animate in a text. In particular, this work is focused on atypical animacy and examines the scenario in which typically inanimate objects, specifically machines, are given animate attributes. To address it, we have created the first dataset for atypical animacy detection, based on nineteenth-century sentences in English, with machines represented as either animate or inanimate. Our method builds on recent innovations in language modeling, specifically BERT contextualized word embeddings, to better capture fine-grained contextual properties of words. We present a fully unsupervised pipeline, which can be easily adapted to different contexts, and report its performance on an established animacy dataset and our newly introduced resource. We show that our method provides a substantially more accurate characterization of atypical animacy, especially when applied to highly complex forms of language use. △ Less","19 November, 2020",https://arxiv.org/pdf/2005.11140
Regulating Artificial Intelligence: Proposal for a Global Solution,Olivia J. Erdélyi;Judy Goldsmith,"With increasing ubiquity of artificial intelligence (AI) in modern societies, individual countries and the international community are working hard to create an innovation-friendly, yet safe, regulatory environment. Adequate regulation is key to maximize the benefits and minimize the risks stemming from AI technologies. Developing regulatory frameworks is, however, challenging due to AI's global reach and the existence of widespread misconceptions about the notion of regulation. We argue that AI-related challenges cannot be tackled effectively without sincere international coordination supported by robust, consistent domestic and international governance arrangements. Against this backdrop, we propose the establishment of an international AI governance framework organized around a new AI regulatory agency that -- drawing on interdisciplinary expertise -- could help creating uniform standards for the regulation of AI technologies and inform the development of AI policies around the world. We also believe that a fundamental change of mindset on what constitutes regulation is necessary to remove existing barriers that hamper contemporary efforts to develop AI regulatory regimes, and put forward some recommendations on how to achieve this, and what opportunities doing so would present. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.11072
NR V2X Communications at Millimeter Waves: An End-to-End Performance Evaluation,Tommaso Zugno;Matteo Drago;Marco Giordani;Michele Polese;Michele Zorzi,"3GPP NR V2X represents the new 3GPP standard for next-generation vehicular systems which, among other innovations, supports vehicle-to-vehicle (V2V) operations in the millimeter wave (mmWave) spectrum to address the communication requirements of future intelligent automotive networks. While mmWaves will enable massive data rates and low latency, the propagation characteristics at very high frequencies become very challenging, thereby calling for accurate performance evaluations as a means to properly assess the performance of such systems. Along these lines, in this paper MilliCar, the new ns-3 module based on the latest NR V2X specifications, is used to provide an end-to-end performance evaluation of mmWave V2V networks. We investigate the impact of different propagation scenarios and system parameters, including the inter-vehicle distance, the adopted frame numerology, and the modulation and coding scheme, and provide guidelines towards the most promising V2V deployment configurations. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.10148
Enabling RAN Slicing Through Carrier Aggregation in mmWave Cellular Networks,Matteo Pagin;Francesco Agostini;Tommaso Zugno;Michele Polese;Michele Zorzi,"The ever increasing number of connected devices and of new and heterogeneous mobile use cases implies that 5G cellular systems will face demanding technical challenges. For example, Ultra-Reliable Low-Latency Communication (URLLC) and enhanced Mobile Broadband (eMBB) scenarios present orthogonal Quality of Service (QoS) requirements that 5G aims to satisfy with a unified Radio Access Network (RAN) design. Network slicing and mmWave communications have been identified as possible enablers for 5G. They provide, respectively, the necessary scalability and flexibility to adapt the network to each specific use case environment, and low latency and multi-gigabit-per-second wireless links, which tap into a vast, currently unused portion of the spectrum. The optimization and integration of these technologies is still an open research challenge, which requires innovations at different layers of the protocol stack. This paper proposes to combine them in a RAN slicing framework for mmWaves, based on carrier aggregation. Notably, we introduce MilliSlice, a cross-carrier scheduling policy that exploits the diversity of the carriers and maximizes their utilization, thus simultaneously guaranteeing high throughput for the eMBB slices and low latency and high reliability for the URLLC flows. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.10142
Empowering Urban Governance through Urban Science: Multi-scale Dynamics of Urban Systems Worldwide,Juste Raimbault;Eric Denis;Denise Pumain,"The current science of cities can provide a useful foundation for future urban policies, provided that these proposals have been validated by correct observations of the diversity of situations in the world. However, international comparisons of the evolution of cities often produce uncertain results because national territorial frameworks are not always in strict correspondence with the dynamics of urban systems. We propose to provide various compositions of systems of cities to better take into account the dynamic networking of cities that go beyond regional and national territorial boundaries. Different models conceived for explaining city size and urban growth distributions enable to establish a correspondence between urban trajectories when observed at the level of cities and systems of cities. We test the validity and representativeness of several dynamic models of complex urban systems and their variations across regions of the world, at the macroscopic scale of systems of cities. The originality of the approach is in considering spatial interaction and evolutionary path dependence as major features in the general behavior of urban entities. The models studied include diverse and complementary processes, such as economic exchanges, diffusion of innovations and physical network flows. Complex systems' dynamics is in principle unpredictable, but contextualizing it regarding demographic, income and resource components may help in minimizing the forecasting errors. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.10007
An Innovative Approach to Determine Rebar Depth and Size by Comparing GPR Data with a Theoretical Database,Zhongming Xiang;Ge Ou;Abbas Rashidi,"Ground penetrating radar (GPR) is an efficient technique used for rapidly recognizing embedded rebar in concrete structures. However, due to the difficulty in extracting signals from GPR data and the intrinsic coupling between the rebar depth and size showing in the data, simultaneously determining rebar depth and size is challenging. This paper proposes an innovative algorithm to address this issue. First, the hyperbola signal from the GPR data is identified by direct wave removal, signal reconstruction and separation. Subsequently, a database is developed from a series of theoretical hyperbolas and then compared with the extracted hyperbola outlines. Finally, the rebar depth and size are determined by searching for the closest counterpart in the database. The obtained results are very promising and indicate that: (1) implementing the method presented in this paper can completely remove the direct wave noise from the GPR data, and can successfully extract the outlines from the interlaced hyperbolas; and (2) the proposed method can simultaneously determine the rebar depth and size with the accuracy of 100% and 95.11%, respectively. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.09643
A Semantically Enriched Dataset based on Biomedical NER for the COVID19 Open Research Dataset Challenge,Hermann Kroll;Jan Pirklbauer;Johannes Ruthmann;Wolf-Tilo Balke,"Research into COVID-19 is a big challenge and highly relevant at the moment. New tools are required to assist medical experts in their research with relevant and valuable information. The COVID-19 Open Research Dataset Challenge (CORD-19) is a ""call to action"" for computer scientists to develop these innovative tools. Many of these applications are empowered by entity information, i. e. knowing which entities are used within a sentence. For this paper, we have developed a pipeline upon the latest Named Entity Recognition tools for Chemicals, Diseases, Genes and Species. We apply our pipeline to the COVID-19 research challenge and share the resulting entity mentions with the community. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.08823
"An Overview on Audio, Signal, Speech, & Language Processing for COVID-19",Gauri Deshpande;Björn Schuller,"Recently, there has been an increased attention towards innovating, enhancing, building, and deploying applications of speech signal processing for providing assistance and relief to human mankind from the Coronavirus (COVID-19) pandemic. Many AI with speech initiatives are taken to combat with the present situation and also to create a safe and secure environment for the future. This paper summarises all these efforts taken by the re-search community towards helping the individuals and the society in the fight against COVID-19 over the past 3-4 months using speech signal processing. We also summarise the deep techniques used in this direction to come up with capable solutions in a short span of time. This paper further gives an overview of the contributions from non-speech modalities that may complement or serve as inspiration for audio and speech analysis. In addition, we discuss our observations with respect to solution usability, challenges, and the significant technology achievements. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.08579
Remote health monitoring and diagnosis in the time of COVID-19,Joachim A. Behar;Chengyu Liu;Kevin Kotzen;Kenta Tsutsui;Valentina D. A. Corino;Janmajay Singh;Marco A. F. Pimentel;Philip Warrick;Sebastian Zaunseder;Fernando Andreotti;David Sebag;Georgy Popanitsa;Patrick E. McSharry;Walter Karlen;Chandan Karmakar;Gari D. Clifford,"Coronavirus disease (COVID-19) is caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) that is rapidly spreading across the globe. The clinical spectrum of SARS-CoV-2 pneumonia ranges from mild to critically ill cases and requires early detection and monitoring, within a clinical environment for critical cases and remotely for mild cases. The fear of contamination in clinical environments has led to a dramatic reduction in on-site referrals for routine care. There has also been a perceived need to continuously monitor non-severe COVID- 19 patients, either from their quarantine site at home, or dedicated quarantine locations (e.g., hotels). Thus, the pandemic has driven incentives to innovate and enhance or create new routes for providing healthcare services at distance. In particular, this has created a dramatic impetus to find innovative ways to remotely and effectively monitor patient health status. In this paper we present a short review of remote health monitoring initiatives taken in 19 states during the time of the pandemic. We emphasize in the discussion particular aspects that are common ground for the reviewed states, in particular the future impact of the pandemic on remote health monitoring and consideration on data privacy. △ Less","15 October, 2020",https://arxiv.org/pdf/2005.08537
Towards in-store multi-person tracking using head detection and track heatmaps,Aibek Musaev;Jiangping Wang;Liang Zhu;Cheng Li;Yi Chen;Jialin Liu;Wanqi Zhang;Juan Mei;De Wang,"Computer vision algorithms are being implemented across a breadth of industries to enable technological innovations. In this paper, we study the problem of computer vision based customer tracking in retail industry. To this end, we introduce a dataset collected from a camera in an office environment where participants mimic various behaviors of customers in a supermarket. In addition, we describe an illustrative example of the use of this dataset for tracking participants based on a head tracking model in an effort to minimize errors due to occlusion. Furthermore, we propose a model for recognizing customers and staff based on their movement patterns. The model is evaluated using a real-world dataset collected in a supermarket over a 24-hour period that achieves 98% accuracy during training and 93% accuracy during evaluation. △ Less","1 July, 2020",https://arxiv.org/pdf/2005.08009
Broadcasting on trees near criticality,Yuzhou Gu;Hajir Roozbehani;Yury Polyanskiy,"We revisit the problem of broadcasting on d-ary trees: starting from a Bernoulli(1/2) random variable X_0 at a root vertex, each vertex forwards its value across binary symmetric channels \mathrm{BSC}_δ to d descendants. The goal is to reconstruct X_0 given the vector X_{L_h} of values of all variables at depth h. It is well known that reconstruction (better than a random guess) is possible as h\to \infty if and only if δ< δ_c(d). In this paper, we study the behavior of the mutual information and the probability of error when δ is slightly subcritical. The innovation of our work is application of the recently introduced ""less-noisy"" channel comparison techniques. For example, we are able to derive the positive part of the phase transition (reconstructability when δ<δ_c) using purely information-theoretic ideas. This is in contrast with previous derivations, which explicitly analyze distribution of the Hamming weight of X_{L_h} (a so-called Kesten-Stigum bound). △ Less","15 May, 2020",https://arxiv.org/pdf/2005.07801
Collective Risk Minimization via a Bayesian Model for Statistical Software Testing,Joachim Haensel;Christian M. Adriano;Johannes Dyck;Holger Giese,"In the last four years, the number of distinct autonomous vehicles platforms deployed in the streets of California increased 6-fold, while the reported accidents increased 12-fold. This can become a trend with no signs of subsiding as it is fueled by a constant stream of innovations in hardware sensors and machine learning software. Meanwhile, if we expect the public and regulators to trust the autonomous vehicle platforms, we need to find better ways to solve the problem of adding technological complexity without increasing the risk of accidents. We studied this problem from the perspective of reliability engineering in which a given risk of an accident has severity and probability of occurring. Timely information on accidents is important for engineers to anticipate and reuse previous failures to approximate the risk of accidents in a new city. However, this is challenging in the context of autonomous vehicles because of the sparse nature of data on the operational scenarios (driving trajectories in a new city). Our approach was to mitigate data sparsity by reducing the state space through monitoring of multiple-vehicles operations. We then minimized the risk of accidents by determining proper allocation of tests for each equivalence class. Our contributions comprise (1) a set of strategies to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian model that estimates changes in the risk of accidents, and (3) a feedback control-loop that minimizes these risks by reallocating test effort. Our results are promising in the sense that we were able to measure and control risk for a diversity of changes in the operational scenarios. We evaluated our models with data from two real cities with distinct traffic patterns and made the data available for the community. △ Less","15 May, 2020",https://arxiv.org/pdf/2005.07460
Robust On-Manifold Optimization for Uncooperative Space Relative Navigation with a Single Camera,Duarte Rondao;Nabil Aouf;Mark A. Richardson;Vincent Dubanchet,"Optical cameras are gaining popularity as the suitable sensor for relative navigation in space due to their attractive sizing, power and cost properties when compared to conventional flight hardware or costly laser-based systems. However, a camera cannot infer depth information on its own, which is often solved by introducing complementary sensors or a second camera. In this paper, an innovative model-based approach is instead demonstrated to estimate the six-dimensional pose of a target object relative to the chaser spacecraft using solely a monocular setup. The observed facet of the target is tackled as a classification problem, where the three-dimensional shape is learned offline using Gaussian mixture modeling. The estimate is refined by minimizing two different robust loss functions based on local feature correspondences. The resulting pseudo-measurements are then processed and fused with an extended Kalman filter. The entire optimization framework is designed to operate directly on the SE\text{(3)} manifold, uncoupling the process and measurement models from the global attitude state representation. It is validated on realistic synthetic and laboratory datasets of a rendezvous trajectory with the complex spacecraft Envisat. It is demonstrated how it achieves an estimate of the relative pose with high accuracy over its full tumbling motion. △ Less","14 May, 2020",https://arxiv.org/pdf/2005.07110
Providing a way to create balance between reliability and delays in SDN networks by using the appropriate placement of controllers,Amir Javadpour,"Computer networks covered the entire world and a serious and new development has not formed for many years. But companies and consumer organizations complain about the failure to add new features to their networks and according to their need, like much of the works to be done automatically and they also like to develop and expand their networks on the software side so they do not need new expensive hardware for many of the activities and needs of their network. Analysis of the control layers and writing data in Software-Defined Network facilitate network management and accelerate innovation in network. In order to develop broad networks of SDN, often a large number controller is needed and located position of controllers in the SDN networks and can be raised as an important and basic issue in the field of SDN network and have impact on reliability of SDN networks. This paper focused on latency and reliability in SDN networks. The latency here means the delay in response to the request of data path that has a significant impact on network latency. In this paper it is shown that the number of controllers and their position can be effective on two measures; reliability and latency in SDN networks. △ Less","24 April, 2020",https://arxiv.org/pdf/2005.06953
IEEE 7010: A New Standard for Assessing the Well-being Implications of Artificial Intelligence,Daniel S. Schiff;Aladdin Ayesh;Laura Musikanski;John C. Havens,"Artificial intelligence (AI) enabled products and services are becoming a staple of everyday life. While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue. This article introduces one of the first international standards focused on the social and ethical implications of AI: The Institute of Electrical and Electronics Engineering (IEEE) Standard (Std) 7010-2020 Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-being. Incorporating well-being factors throughout the lifecycle of AI is both challenging and urgent and IEEE 7010 provides key guidance for those who design, deploy, and procure these technologies. We begin by articulating the benefits of an approach for AI centered around well-being and the measurement of well-being data. Next, we provide an overview of IEEE 7010, including its key principles and how the standard relates to approaches and perspectives in place in the AI community. Finally, we indicate where future efforts are needed. △ Less","17 December, 2020",https://arxiv.org/pdf/2005.06620
"Boom, Bust, and Bitcoin: Bitcoin-Bubbles As Innovation Accelerators",Tobias A. Huber;Didier Sornette,"Bitcoin represents one of the most interesting technological breakthroughs and socio-economic experiments of the last decades. In this paper, we examine the role of speculative bubbles in the process of Bitcoin's technological adoption by analyzing its social dynamics. We trace Bitcoin's genesis and dissect the nature of its techno-economic innovation. In particular, we present an analysis of the techno-economic feedback loops that drive Bitcoin's price and network effects. Based on our analysis of Bitcoin, we test and further refine the Social Bubble Hypothesis, which holds that bubbles constitute an essential component in the process of technological innovation. We argue that a hierarchy of repeating and exponentially increasing series of bubbles and hype cycles, which has occurred over the past decade since its inception, has bootstrapped Bitcoin into existence. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06464
Visualising COVID-19 Research,Pierre Le Bras;Azimeh Gharavi;David A. Robb;Ana F. Vidal;Stefano Padilla;Mike J. Chantler,"The world has seen in 2020 an unprecedented global outbreak of SARS-CoV-2, a new strain of coronavirus, causing the COVID-19 pandemic, and radically changing our lives and work conditions. Many scientists are working tirelessly to find a treatment and a possible vaccine. Furthermore, governments, scientific institutions and companies are acting quickly to make resources available, including funds and the opening of large-volume data repositories, to accelerate innovation and discovery aimed at solving this pandemic. In this paper, we develop a novel automated theme-based visualisation method, combining advanced data modelling of large corpora, information mapping and trend analysis, to provide a top-down and bottom-up browsing and search interface for quick discovery of topics and research resources. We apply this method on two recently released publications datasets (Dimensions' COVID-19 dataset and the Allen Institute for AI's CORD-19). The results reveal intriguing information including increased efforts in topics such as social distancing; cross-domain initiatives (e.g. mental health and education); evolving research in medical topics; and the unfolding trajectory of the virus in different territories through publications. The results also demonstrate the need to quickly and automatically enable search and browsing of large corpora. We believe our methodology will improve future large volume visualisation and discovery systems but also hope our visualisation interfaces will currently aid scientists, researchers, and the general public to tackle the numerous issues in the fight against the COVID-19 pandemic. △ Less","15 May, 2020",https://arxiv.org/pdf/2005.06380
The Scalable Systems Laboratory: a Platform for Software Innovation for HEP,Robert Gardner;Lincoln Bryant;Mark Neubauer;Frank Wuerthwein;Judith Stephen;Andrew Chien,"The Scalable Systems Laboratory (SSL), part of the IRIS-HEP Software Institute, provides Institute participants and HEP software developers generally with a means to transition their R&D from conceptual toys to testbeds to production-scale prototypes. The SSL enables tooling, infrastructure, and services supporting the innovation of novel analysis and data architectures, development of software elements and tool-chains, reproducible functional and scalability testing of service components, and foundational systems R&D for accelerated services developed by the Institute. The SSL is constructed with a core team having expertise in scale testing and deployment of services across a wide range of cyberinfrastructure. The core team embeds and partners with other areas in the Institute, and with LHC and other HEP development and operations teams as appropriate, to define investigations and required service deployment patterns. We describe the approach and experiences with early application deployments, including analysis platforms and intelligent data delivery systems. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06151
Recurrent and Spiking Modeling of Sparse Surgical Kinematics,Neil Getty;Zixuan Zhao;Stephan Gruessner;Liaohai Chen;Fangfang Xia,"Robot-assisted minimally invasive surgery is improving surgeon performance and patient outcomes. This innovation is also turning what has been a subjective practice into motion sequences that can be precisely measured. A growing number of studies have used machine learning to analyze video and kinematic data captured from surgical robots. In these studies, models are typically trained on benchmark datasets for representative surgical tasks to assess surgeon skill levels. While they have shown that novices and experts can be accurately classified, it is not clear whether machine learning can separate highly proficient surgeons from one another, especially without video data. In this study, we explore the possibility of using only kinematic data to predict surgeons of similar skill levels. We focus on a new dataset created from surgical exercises on a simulation device for skill training. A simple, efficient encoding scheme was devised to encode kinematic sequences so that they were amenable to edge learning. We report that it is possible to identify surgical fellows receiving near perfect scores in the simulation exercises based on their motion characteristics alone. Further, our model could be converted to a spiking neural network to train and infer on the Nengo simulation framework with no loss in accuracy. Overall, this study suggests that building neuromorphic models from sparse motion features may be a potentially useful strategy for identifying surgeons and gestures with chips deployed on robotic systems to offer adaptive assistance during surgery and training with additional latency and privacy benefits. △ Less","11 June, 2020",https://arxiv.org/pdf/2005.05868
Neighborhood Matching Network for Entity Alignment,Yuting Wu;Xiao Liu;Yansong Feng;Zheng Wang;Dongyan Zhao,"Structural heterogeneity between knowledge graphs is an outstanding challenge for entity alignment. This paper presents Neighborhood Matching Network (NMN), a novel entity alignment framework for tackling the structural heterogeneity challenge. NMN estimates the similarities between entities to capture both the topological structure and the neighborhood difference. It provides two innovative components for better learning representations for entity alignment. It first uses a novel graph sampling method to distill a discriminative neighborhood for each entity. It then adopts a cross-graph neighborhood matching module to jointly encode the neighborhood difference for a given entity pair. Such strategies allow NMN to effectively construct matching-oriented entity representations while ignoring noisy neighbors that have a negative impact on the alignment task. Extensive experiments performed on three entity alignment datasets show that NMN can well estimate the neighborhood similarity in more tough cases and significantly outperforms 12 previous state-of-the-art methods. △ Less","12 May, 2020",https://arxiv.org/pdf/2005.05607
Making Robots Draw A Vivid Portrait In Two Minutes,Fei Gao;Jingjie Zhu;Zeyuan Yu;Peng Li;Tao Wang,"Significant progress has been made with artistic robots. However, existing robots fail to produce high-quality portraits in a short time. In this work, we present a drawing robot, which can automatically transfer a facial picture to a vivid portrait, and then draw it on paper within two minutes averagely. At the heart of our system is a novel portrait synthesis algorithm based on deep learning. Innovatively, we employ a self-consistency loss, which makes the algorithm capable of generating continuous and smooth brush-strokes. Besides, we propose a componential sparsity constraint to reduce the number of brush-strokes over insignificant areas. We also implement a local sketch synthesis algorithm, and several pre- and post-processing techniques to deal with the background and details. The portrait produced by our algorithm successfully captures individual characteristics by using a sparse set of continuous brush-strokes. Finally, the portrait is converted to a sequence of trajectories and reproduced by a 3-degree-of-freedom robotic arm. The whole portrait drawing robotic system is named AiSketcher. Extensive experiments show that AiSketcher can produce considerably high-quality sketches for a wide range of pictures, including faces in-the-wild and universal images of arbitrary content. To our best knowledge, AiSketcher is the first portrait drawing robot that uses neural style transfer techniques. AiSketcher has attended a quite number of exhibitions and shown remarkable performance under diverse circumstances. △ Less","21 July, 2020",https://arxiv.org/pdf/2005.05526
Luganda Text-to-Speech Machine,Irene Nandutu;Ernest Mwebaze,"In Uganda, Luganda is the most spoken native language. It is used for communication in informal as well as formal business transactions. The development of technology startups globally related to TTS has mainly been with languages like English, French, etc. These are added in TTS engines by Google, Microsoft among others, allowing developers in these regions to innovate TTS products. Luganda is not supported because the language is not built and trained on these engines. In this study, we analyzed the Luganda language structure and constructions and then proposed and developed a Luganda TTS. The system was built and trained using locally sourced Luganda language text and audio. The engine is now able to capture text and reads it aloud. We tested the accuracy using MRT and MOS. MRT and MOS tests results are quite good with MRT having better results. The results general score was 71%. This study will enhance previous solutions to NLP gaps in Uganda, as well as provide raw data such that other research in this area can take place. △ Less","11 May, 2020",https://arxiv.org/pdf/2005.05447
#lockdown: network-enhanced emotional profiling at the times of COVID-19,Massimo Stella;Valerio Restocchi;Simon De Deyne,"The COVID-19 pandemic forced countries all over the world to take unprecedented measures like nationwide lockdowns. To adequately understand the emotional and social repercussions, a large-scale reconstruction of how people perceived these unexpected events is necessary but currently missing. We address this gap through social media by introducing MERCURIAL (Multi-layer Co-occurrence Networks for Emotional Profiling), a framework which exploits linguistic networks of words and hashtags to reconstruct social discourse describing real-world events. We use MERCURIAL to analyse 101,767 tweets from Italy, the first country to react to the COVID-19 threat with a nationwide lockdown. The data were collected between 11th and 17th March, immediately after the announcement of the Italian lockdown and the WHO declaring COVID-19 a pandemic. Our analysis provides unique insights into the psychological burden of this crisis, focussing on: (i) the Italian official campaign for self-quarantine (#iorestoacasa}), (ii) national lockdown (#italylockdown), and (iii) social denounce (#sciacalli). Our exploration unveils evidence for the emergence of complex emotional profiles, where anger and fear (towards political debates and socio-economic repercussions) coexisted with trust, solidarity, and hope (related to the institutions and local communities). We discuss our findings in relation to mental well-being issues and coping mechanisms, like instigation to violence, grieving, and solidarity. We argue that our framework represents an innovative thermometer of emotional status, a powerful tool for policy makers to quickly gauge feelings in massive audiences and devise appropriate responses based on cognitive data. △ Less","9 May, 2020",https://arxiv.org/pdf/2005.04404
Building a PubMed knowledge graph,Jian Xu;Sunkyu Kim;Min Song;Minbyul Jeong;Donghyeon Kim;Jaewoo Kang;Justin F. Rousseau;Xin Li;Weijia Xu;Vetle I. Torvik;Yi Bu;Chongyan Chen;Islam Akef Ebeid;Daifeng Li;Ying Ding,"PubMed is an essential resource for the medical domain, but useful concepts are either difficult to extract or are ambiguated, which has significantly hindered knowledge discovery. To address this issue, we constructed a PubMed knowledge graph (PKG) by extracting bio-entities from 29 million PubMed abstracts, disambiguating author names, integrating funding data through the National Institutes of Health (NIH) ExPORTER, collecting affiliation history and educational background of authors from ORCID, and identifying fine-grained affiliation data from MapAffil. Through the integration of the credible multi-source data, we could create connections among the bio-entities, authors, articles, affiliations, and funding. Data validation revealed that the BioBERT deep learning method of bio-entity extraction significantly outperformed the state-of-the-art models based on the F1 score (by 0.51%), with the author name disambiguation (AND) achieving a F1 score of 98.09%. PKG can trigger broader innovations, not only enabling us to measure scholarly impact, knowledge usage, and knowledge transfer, but also assisting us in profiling authors and organizations based on their connections with bio-entities. The PKG is freely available on Figshare (https://figshare.com/s/6327a55355fc2c99f3a2, simplified version that exclude PubMed raw data) and TACC website (http://er.tacc.utexas.edu/datasets/ped, full version). △ Less","15 May, 2020",https://arxiv.org/pdf/2005.04308
Measuring the Algorithmic Efficiency of Neural Networks,Danny Hernandez;Tom B. Brown,"Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both. △ Less","8 May, 2020",https://arxiv.org/pdf/2005.04305
Text Recognition in the Wild: A Survey,Xiaoxue Chen;Lianwen Jin;Yuanzhi Zhu;Canjie Luo;Tianwei Wang,"The history of text can be traced back over thousands of years. Rich and precise semantic information carried by text is important in a wide range of vision-based application scenarios. Therefore, text recognition in natural scenes has been an active research field in computer vision and pattern recognition. In recent years, with the rise and development of deep learning, numerous methods have shown promising in terms of innovation, practicality, and efficiency. This paper aims to (1) summarize the fundamental problems and the state-of-the-art associated with scene text recognition; (2) introduce new insights and ideas; (3) provide a comprehensive review of publicly available resources; (4) point out directions for future work. In summary, this literature review attempts to present the entire picture of the field of scene text recognition. It provides a comprehensive reference for people entering this field, and could be helpful to inspire future research. Related resources are available at our Github repository: https://github.com/HCIILAB/Scene-Text-Recognition. △ Less","3 December, 2020",https://arxiv.org/pdf/2005.03492
A LiDAR-based real-time capable 3D Perception System for Automated Driving in Urban Domains,Jens Rieken;Markus Maurer,"We present a LiDAR-based and real-time capable 3D perception system for automated driving in urban domains. The hierarchical system design is able to model stationary and movable parts of the environment simultaneously and under real-time conditions. Our approach extends the state of the art by innovative in-detail enhancements for perceiving road users and drivable corridors even in case of non-flat ground surfaces and overhanging or protruding elements. We describe a runtime-efficient pointcloud processing pipeline, consisting of adaptive ground surface estimation, 3D clustering and motion classification stages. Based on the pipeline's output, the stationary environment is represented in a multi-feature mapping and fusion approach. Movable elements are represented in an object tracking system capable of using multiple reference points to account for viewpoint changes. We further enhance the tracking system by explicit consideration of occlusion and ambiguity cases. Our system is evaluated using a subset of the TUBS Road User Dataset. We enhance common performance metrics by considering application-driven aspects of real-world traffic scenarios. The perception system shows impressive results and is able to cope with the addressed scenarios while still preserving real-time capability. △ Less","7 May, 2020",https://arxiv.org/pdf/2005.03404
Role of Apache Software Foundation in Big Data Projects,Aleem Akhtar,"With the increase in amount of Big Data being generated each year, tools and technologies developed and used for the purpose of storing, processing and analyzing Big Data has also improved. Open-Source software has been an important factor in the success and innovation in the field of Big Data while Apache Software Foundation (ASF) has played a crucial role in this success and innovation by providing a number of state-of-the-art projects, free and open to the public. ASF has classified its project in different categories. In this report, projects listed under Big Data category are deeply analyzed and discussed with reference to one-of-the seven sub-categories defined. Our investigation has shown that many of the Apache Big Data projects are autonomous but some are built based on other Apache projects and some work in conjunction with other projects to improve and ease development in Big Data space. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.02829
A Standard-based Open Source IoT Platform: FIWARE,Flavio Cirillo;Gürkan Solmaz;Everton Luís Berz;Martin Bauer;Bin Cheng;Ernoe Kovacs,"The ever-increasing acceleration of technology evolution in all fields is rapidly changing the architectures of data-driven systems towards the Internet-of-Things concept. Many general and specific-purpose IoT platforms are already available. This article introduces the capabilities of the FIWARE framework that is transitioning from a research to a commercial level. We base our exposition on the analysis of three real-world use cases (global IoT market, analytics in smart cities, and IoT augmented autonomous driving) and their requirements that are addressed with the usage of FIWARE. We highlight the lessons learnt during the design, implementation and deployment phases for each of the use cases and their critical issues. Finally we give two examples showing that FIWARE still maintains openness to innovation: semantics and privacy. △ Less","6 May, 2020",https://arxiv.org/pdf/2005.02788
PolymoRF: Polymorphic Wireless Receivers Through Physical-Layer Deep Learning,Francesco Restuccia;Tommaso Melodia,"Today's wireless technologies are largely based on inflexible designs, which makes them inefficient and prone to a variety of wireless attacks. To address this key issue, wireless receivers will need to (i) infer on-the-fly the physical-layer parameters currently used by transmitters; and if needed, (ii) change their hardware and software structures to demodulate the incoming waveform. In this paper, we introduce PolymoRF, a deep learning-based polymorphic receiver able to reconfigure itself in real time based on the inferred waveform parameters. Our key technical innovations are (i) a novel embedded deep learning architecture, called RFNet, which enables the solution of key waveform inference problems; (ii) a generalized hardware/software architecture that integrates RFNet with radio components and signal processing. We prototype PolymoRF on a custom software-defined radio platform, and show through extensive over-the-air experiments that (i) RFNet achieves similar accuracy to that of state-of-the-art yet with 52x and 8x latency and hardware reduction; (ii) PolymoRF achieves throughput within 87% of a perfect-knowledge Oracle system, thus demonstrating for the first time that polymorphic receivers are feasible and effective. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.02262
NTIRE 2020 Challenge on Real-World Image Super-Resolution: Methods and Results,Andreas Lugmayr;Martin Danelljan;Radu Timofte;Namhyuk Ahn;Dongwoon Bai;Jie Cai;Yun Cao;Junyang Chen;Kaihua Cheng;SeYoung Chun;Wei Deng;Mostafa El-Khamy;Chiu Man Ho;Xiaozhong Ji;Amin Kheradmand;Gwantae Kim;Hanseok Ko;Kanghyu Lee;Jungwon Lee;Hao Li;Ziluan Liu;Zhi-Song Liu;Shuai Liu;Yunhua Lu;Zibo Meng,"This paper reviews the NTIRE 2020 challenge on real world super-resolution. It focuses on the participating methods and final results. The challenge addresses the real world setting, where paired true high and low-resolution images are unavailable. For training, only one set of source input images is therefore provided along with a set of unpaired high-quality target images. In Track 1: Image Processing artifacts, the aim is to super-resolve images with synthetically generated image processing artifacts. This allows for quantitative benchmarking of the approaches \wrt a ground-truth image. In Track 2: Smartphone Images, real low-quality smart phone images have to be super-resolved. In both tracks, the ultimate goal is to achieve the best perceptual quality, evaluated using a human study. This is the second challenge on the subject, following AIM 2019, targeting to advance the state-of-the-art in super-resolution. To measure the performance we use the benchmark protocol from AIM 2019. In total 22 teams competed in the final testing phase, demonstrating new and innovative solutions to the problem. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.01996
TIMELY: Pushing Data Movements and Interfaces in PIM Accelerators Towards Local and in Time Domain,Weitao Li;Pengfei Xu;Yang Zhao;Haitong Li;Yuan Xie;Yingyan Lin,"Resistive-random-access-memory (ReRAM) based processing-in-memory (R^2PIM) accelerators show promise in bridging the gap between Internet of Thing devices' constrained resources and Convolutional/Deep Neural Networks' (CNNs/DNNs') prohibitive energy cost. Specifically, R^2PIM accelerators enhance energy efficiency by eliminating the cost of weight movements and improving the computational density through ReRAM's high density. However, the energy efficiency is still limited by the dominant energy cost of input and partial sum (Psum) movements and the cost of digital-to-analog (D/A) and analog-to-digital (A/D) interfaces. In this work, we identify three energy-saving opportunities in R^2PIM accelerators: analog data locality, time-domain interfacing, and input access reduction, and propose an innovative R^2PIM accelerator called TIMELY, with three key contributions: (1) TIMELY adopts analog local buffers (ALBs) within ReRAM crossbars to greatly enhance the data locality, minimizing the energy overheads of both input and Psum movements; (2) TIMELY largely reduces the energy of each single D/A (and A/D) conversion and the total number of conversions by using time-domain interfaces (TDIs) and the employed ALBs, respectively; (3) we develop an only-once input read (O^2IR) mapping method to further decrease the energy of input accesses and the number of D/A conversions. The evaluation with more than 10 CNN/DNN models and various chip configurations shows that, TIMELY outperforms the baseline R^2PIM accelerator, PRIME, by one order of magnitude in energy efficiency while maintaining better computational density (up to 31.2\times) and throughput (up to 736.6\times). Furthermore, comprehensive studies are performed to evaluate the effectiveness of the proposed ALB, TDI, and O^2IR innovations in terms of energy savings and area reduction. △ Less","3 May, 2020",https://arxiv.org/pdf/2005.01206
Lupulus: A Flexible Hardware Accelerator for Neural Networks,Andreas Toftegaard Kristensen;Robert Giterman;Alexios Balatsoukas-Stimming;Andreas Burg,"Neural networks have become indispensable for a wide range of applications, but they suffer from high computational- and memory-requirements, requiring optimizations from the algorithmic description of the network to the hardware implementation. Moreover, the high rate of innovation in machine learning makes it important that hardware implementations provide a high level of programmability to support current and future requirements of neural networks. In this work, we present a flexible hardware accelerator for neural networks, called Lupulus, supporting various methods for scheduling and mapping of operations onto the accelerator. Lupulus was implemented in a 28nm FD-SOI technology and demonstrates a peak performance of 380 GOPS/GHz with latencies of 21.4ms and 183.6ms for the convolutional layers of AlexNet and VGG-16, respectively. △ Less","3 May, 2020",https://arxiv.org/pdf/2005.01016
Sl-EDGE: Network Slicing at the Edge,Salvatore D'Oro;Leonardo Bonati;Francesco Restuccia;Michele Polese;Michele Zorzi;Tommaso Melodia,"Network slicing of multi-access edge computing (MEC) resources is expected to be a pivotal technology to the success of 5G networks and beyond. The key challenge that sets MEC slicing apart from traditional resource allocation problems is that edge nodes depend on tightly-intertwined and strictly-constrained networking, computation and storage resources. Therefore, instantiating MEC slices without incurring in resource over-provisioning is hardly addressable with existing slicing algorithms. The main innovation of this paper is Sl-EDGE, a unified MEC slicing framework that allows network operators to instantiate heterogeneous slice services (e.g., video streaming, caching, 5G network access) on edge devices. We first describe the architecture and operations of Sl-EDGE, and then show that the problem of optimally instantiating joint network-MEC slices is NP-hard. Thus, we propose near-optimal algorithms that leverage key similarities among edge nodes and resource virtualization to instantiate heterogeneous slices 7.5x faster and within 0.25 of the optimum. We first assess the performance of our algorithms through extensive numerical analysis, and show that Sl-EDGE instantiates slices 6x more efficiently then state-of-the-art MEC slicing algorithms. Furthermore, experimental results on a 24-radio testbed with 9 smartphones demonstrate that Sl-EDGE provides at once highly-efficient slicing of joint LTE connectivity, video streaming over WiFi, and ffmpeg video transcoding. △ Less","2 May, 2020",https://arxiv.org/pdf/2005.00886
"A Survey on Cellular-connected UAVs: Design Challenges, Enabling 5G/B5G Innovations, and Experimental Advancements",Debashisha Mishra;Enrico Natalizio,"As an emerging field of aerial robotics, Unmanned Aerial Vehicles (UAVs) have gained significant research interest within the wireless networking research community. As soon as national legislations allow UAVs to fly autonomously, we will see swarms of UAV populating the sky of our smart cities to accomplish different missions: parcel delivery, infrastructure monitoring, event filming, surveillance, tracking, etc. The UAV ecosystem can benefit from existing 5G/B5G cellular networks, which can be exploited in different ways to enhance UAV communications. Because of the inherent characteristics of UAV pertaining to flexible mobility in 3D space, autonomous operation and intelligent placement, these smart devices cater to wide range of wireless applications and use cases. This work aims at presenting an in-depth exploration of integration synergies between 5G/B5G cellular systems and UAV technology, where the UAV is integrated as a new aerial User Equipment (UE) to existing cellular networks. In this integration, the UAVs perform the role of flying users within cellular coverage, thus they are termed as cellular-connected UAVs (a.k.a. UAV-UE, drone-UE, 5G-connected drone, or aerial user). The main focus of this work is to present an extensive study of integration challenges along with key 5G/B5G technological innovations and ongoing efforts in design prototyping and field trials corroborating cellular-connected UAVs. This study highlights recent progress updates with respect to 3GPP standardization and emphasizes socio-economic concerns that must be accounted before successful adoption of this promising technology. Various open problems paving the path to future research opportunities are also discussed. △ Less","2 May, 2020",https://arxiv.org/pdf/2005.00781
AVA: an Automatic eValuation Approach to Question Answering Systems,Thuy Vu;Alessandro Moschitti,"We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers, can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference text. This allows for effectively measuring the similarity between the reference and an automatic answer, biased towards the question semantics. To design, train and test AVA, we built multiple large training, development, and test sets on both public and industrial benchmarks. Our innovative solutions achieve up to 74.7% in F1 score in predicting human judgement for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an RMSE, ranging from 0.02 to 0.09, depending on the availability of multiple references. △ Less","2 May, 2020",https://arxiv.org/pdf/2005.00705
PAMTRI: Pose-Aware Multi-Task Learning for Vehicle Re-Identification Using Highly Randomized Synthetic Data,Zheng Tang;Milind Naphade;Stan Birchfield;Jonathan Tremblay;William Hodge;Ratnesh Kumar;Shuo Wang;Xiaodong Yang,"In comparison with person re-identification (ReID), which has been widely studied in the research community, vehicle ReID has received less attention. Vehicle ReID is challenging due to 1) high intra-class variability (caused by the dependency of shape and appearance on viewpoint), and 2) small inter-class variability (caused by the similarity in shape and appearance between vehicles produced by different manufacturers). To address these challenges, we propose a Pose-Aware Multi-Task Re-Identification (PAMTRI) framework. This approach includes two innovations compared with previous methods. First, it overcomes viewpoint-dependency by explicitly reasoning about vehicle pose and shape via keypoints, heatmaps and segments from pose estimation. Second, it jointly classifies semantic vehicle attributes (colors and types) while performing ReID, through multi-task learning with the embedded pose representations. Since manually labeling images with detailed pose and attribute information is prohibitive, we create a large-scale highly randomized synthetic dataset with automatically annotated vehicle attributes for training. Extensive experiments validate the effectiveness of each proposed component, showing that PAMTRI achieves significant improvement over state-of-the-art on two mainstream vehicle ReID benchmarks: VeRi and CityFlow-ReID. Code and models are available at https://github.com/NVlabs/PAMTRI. △ Less","1 May, 2020",https://arxiv.org/pdf/2005.00673
A model of urban evolution based on innovation diffusion,Juste Raimbault,"The dynamics of urban systems can be understood from an evolutionary perspective, in some sense extending biological and cultural evolution. Models for systems of cities implementing elementary evolutionary processes remain however to be investigated. We propose here such a model for urban dynamics at the macroscopic scale, in which the diffusion of innovations between cities captures transformation processes (mutations) and transmission processes (diffusion), using two coupled spatial interaction models. Explorations of the model on synthetic systems of cities show the role of spatial interaction and innovation diffusion ranges on measures of diversity and utility, and the existence of intermediate ranges yielding an optimal utility. Multi-objective optimization shows how the model produces a compromise between utility and diversity. This model paves the way towards more elaborated formalizations of urban evolution. △ Less","30 April, 2020",https://arxiv.org/pdf/2004.15023
"Memristors -- from In-memory computing, Deep Learning Acceleration, Spiking Neural Networks, to the Future of Neuromorphic and Bio-inspired Computing",Adnan Mehonic;Abu Sebastian;Bipin Rajendran;Osvaldo Simeone;Eleni Vasilaki;Anthony J. Kenyon,"Machine learning, particularly in the form of deep learning, has driven most of the recent fundamental developments in artificial intelligence. Deep learning is based on computational models that are, to a certain extent, bio-inspired, as they rely on networks of connected simple computing units operating in parallel. Deep learning has been successfully applied in areas such as object/pattern recognition, speech and natural language processing, self-driving vehicles, intelligent self-diagnostics tools, autonomous robots, knowledgeable personal assistants, and monitoring. These successes have been mostly supported by three factors: availability of vast amounts of data, continuous growth in computing power, and algorithmic innovations. The approaching demise of Moore's law, and the consequent expected modest improvements in computing power that can be achieved by scaling, raise the question of whether the described progress will be slowed or halted due to hardware limitations. This paper reviews the case for a novel beyond CMOS hardware technology, memristors, as a potential solution for the implementation of power-efficient in-memory computing, deep learning accelerators, and spiking neural networks. Central themes are the reliance on non-von-Neumann computing architectures and the need for developing tailored learning and inference algorithms. To argue that lessons from biology can be useful in providing directions for further progress in artificial intelligence, we briefly discuss an example based reservoir computing. We conclude the review by speculating on the big picture view of future neuromorphic and brain-inspired computing systems. △ Less","30 April, 2020",https://arxiv.org/pdf/2004.14942
The role of context in neural pitch accent detection in English,Elizabeth Nielsen;Mark Steedman;Sharon Goldwater,"Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast. In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech. We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task. Our model makes greater use of context by using full utterances as input and adding an LSTM layer. We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result. We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task. Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus. △ Less","12 October, 2020",https://arxiv.org/pdf/2004.14846
Design and Implementation of Air Selection based Augmented Reality Serious Game for Learning Capability Analysis,Harini. M;Harini. T;Roxanna Samuel,"Rising advancements and ICT have changed the way of life of society, every single logical zone are exploiting innovation to get a genuine improvement. Specialists understand the advantages of utilizing genuine games as a dependable device in psychoanalyst. Hence, the exploration looks at important issues in regards to Dyspraxia issue in youngsters and presents a similar report in the treatments strategies by utilizing a non autonomous riddle and by utilizing the game, a Serious Game created in the intension of helping kids suffering from Dyspraxia to enhance their engine aptitudes and deftness through innovation. The investigation of information results indicated that exist a critical distinction among the two strategies, demonstrating that youngsters spending time with Serious Game got little schedule in the movement running and furthermore enhanced execution. △ Less","30 April, 2020",https://arxiv.org/pdf/2004.14685
A Wearable Social Interaction Aid for Children with Autism,Nick Haber;Catalin Voss;Jena Daniels;Peter Washington;Azar Fazel;Aaron Kline;Titas De;Terry Winograd;Carl Feinstein;Dennis P. Wall,"With most recent estimates giving an incidence rate of 1 in 68 children in the United States, the autism spectrum disorder (ASD) is a growing public health crisis. Many of these children struggle to make eye contact, recognize facial expressions, and engage in social interactions. Today the standard for treatment of the core autism-related deficits focuses on a form of behavior training known as Applied Behavioral Analysis. To address perceived deficits in expression recognition, ABA approaches routinely involve the use of prompts such as flash cards for repetitive emotion recognition training via memorization. These techniques must be administered by trained practitioners and often at clinical centers that are far outnumbered by and out of reach from the many children and families in need of attention. Waitlists for access are up to 18 months long, and this wait may lead to children regressing down a path of isolation that worsens their long-term prognosis. There is an urgent need to innovate new methods of care delivery that can appropriately empower caregivers of children at risk or with a diagnosis of autism, and that capitalize on mobile tools and wearable devices for use outside of clinical settings. △ Less","19 April, 2020",https://arxiv.org/pdf/2004.14281
A Workflow Manager for Complex NLP and Content Curation Pipelines,Julián Moreno-Schneider;Peter Bourgonje;Florian Kintzel;Georg Rehm,"We present a workflow manager for the flexible creation and customisation of NLP processing pipelines. The workflow manager addresses challenges in interoperability across various different NLP tasks and hardware-based resource usage. Based on the four key principles of generality, flexibility, scalability and efficiency, we present the first version of the workflow manager by providing details on its custom definition language, explaining the communication components and the general system architecture and setup. We currently implement the system, which is grounded and motivated by real-world industry use cases in several innovation and transfer projects. △ Less","16 April, 2020",https://arxiv.org/pdf/2004.14130
6G White Paper on Machine Learning in Wireless Communication Networks,Samad Ali;Walid Saad;Nandana Rajatheva;Kapseok Chang;Daniel Steinbach;Benjamin Sliwa;Christian Wietfeld;Kai Mei;Hamid Shiri;Hans-Jürgen Zepernick;Thi My Chinh Chu;Ijaz Ahmad;Jyrki Huusko;Jaakko Suutala;Shubhangi Bhadauria;Vimal Bhatia;Rangeet Mitra;Saidhiraj Amuru;Robert Abbas;Baohua Shao;Michele Capobianco;Guanghui Yu;Maelick Claes;Teemu Karvonen;Mingzhe Chen,"The focus of this white paper is on machine learning (ML) in wireless communications. 6G wireless communication networks will be the backbone of the digital transformation of societies by providing ubiquitous, reliable, and near-instant wireless connectivity for humans and machines. Recent advances in ML research has led enable a wide range of novel technologies such as self-driving vehicles and voice assistants. Such innovation is possible as a result of the availability of advanced ML models, large datasets, and high computational power. On the other hand, the ever-increasing demand for connectivity will require a lot of innovation in 6G wireless networks, and ML tools will play a major role in solving problems in the wireless domain. In this paper, we provide an overview of the vision of how ML will impact the wireless communication systems. We first give an overview of the ML methods that have the highest potential to be used in wireless networks. Then, we discuss the problems that can be solved by using ML in various layers of the network such as the physical layer, medium access layer, and application layer. Zero-touch optimization of wireless networks using ML is another interesting aspect that is discussed in this paper. Finally, at the end of each section, important research questions that the section aims to answer are presented. △ Less","28 April, 2020",https://arxiv.org/pdf/2004.13875
PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning,Arthur Douillard;Matthieu Cord;Charles Ollion;Thomas Robert;Eduardo Valle,"Lifelong learning has attracted much attention, but existing works still struggle to fight catastrophic forgetting and accumulate knowledge over long stretches of incremental learning. In this work, we propose PODNet, a model inspired by representation learning. By carefully balancing the compromise between remembering the old classes and learning new ones, PODNet fights catastrophic forgetting, even over very long runs of small incremental tasks --a setting so far unexplored by current works. PODNet innovates on existing art with an efficient spatial-based distillation-loss applied throughout the model and a representation comprising multiple proxy vectors for each class. We validate those innovations thoroughly, comparing PODNet with three state-of-the-art models on three datasets: CIFAR100, ImageNet100, and ImageNet1000. Our results showcase a significant advantage of PODNet over existing art, with accuracy gains of 12.10, 6.51, and 2.85 percentage points, respectively. Code is available at https://github.com/arthurdouillard/incremental_learning.pytorch △ Less","6 October, 2020",https://arxiv.org/pdf/2004.13513
Modularized Transfomer-based Ranking Framework,Luyu Gao;Zhuyun Dai;Jamie Callan,"Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval. However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process. In this work, we modularize the Transformer ranker into separate modules for text representation and interaction. We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions. The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers. △ Less","6 October, 2020",https://arxiv.org/pdf/2004.13313
Towards an Integrated Platform for Big Data Analysis,Mahdi Bohlouli;Frank Schulz;Lefteris Angelis;David Pahor;Ivona Brandic;David Atlan;Rosemary Tate,"The amount of data in the world is expanding rapidly. Every day, huge amounts of data are created by scientific experiments, companies, and end users' activities. These large data sets have been labeled as ""Big Data"", and their storage, processing and analysis presents a plethora of new challenges to computer science researchers and IT professionals. In addition to efficient data management, additional complexity arises from dealing with semi-structured or unstructured data, and from time critical processing requirements. In order to understand these massive amounts of data, advanced visualization and data exploration techniques are required. Innovative approaches to these challenges have been developed during recent years, and continue to be a hot topic for re-search and industry in the future. An investigation of current approaches reveals that usually only one or two aspects are ad-dressed, either in the data management, processing, analysis or visualization. This paper presents the vision of an integrated plat-form for big data analysis that combines all these aspects. Main benefits of this approach are an enhanced scalability of the whole platform, a better parameterization of algorithms, a more efficient usage of system resources, and an improved usability during the end-to-end data analysis process. △ Less","26 April, 2020",https://arxiv.org/pdf/2004.13021
Neuromorphic Nearest-Neighbor Search Using Intel's Pohoiki Springs,E. Paxon Frady;Garrick Orchard;David Florey;Nabil Imam;Ruokun Liu;Joyesh Mishra;Jonathan Tse;Andreas Wild;Friedrich T. Sommer;Mike Davies,"Neuromorphic computing applies insights from neuroscience to uncover innovations in computing technology. In the brain, billions of interconnected neurons perform rapid computations at extremely low energy levels by leveraging properties that are foreign to conventional computing systems, such as temporal spiking codes and finely parallelized processing units integrating both memory and computation. Here, we showcase the Pohoiki Springs neuromorphic system, a mesh of 768 interconnected Loihi chips that collectively implement 100 million spiking neurons in silicon. We demonstrate a scalable approximate k-nearest neighbor (k-NN) algorithm for searching large databases that exploits neuromorphic principles. Compared to state-of-the-art conventional CPU-based implementations, we achieve superior latency, index build time, and energy efficiency when evaluated on several standard datasets containing over 1 million high-dimensional patterns. Further, the system supports adding new data points to the indexed database online in O(1) time unlike all but brute force conventional k-NN implementations. △ Less","27 April, 2020",https://arxiv.org/pdf/2004.12691
GymFG: A Framework with a Gym Interface for FlightGear,Andrew Wood;Ali Sydney;Peter Chin;Bishal Thapa;Ryan Ross,"Over the past decades, progress in deployable autonomous flight systems has slowly stagnated. This is reflected in today's production air-crafts, where pilots only enable simple physics-based systems such as autopilot for takeoff, landing, navigation, and terrain/traffic avoidance. Evidently, autonomy has not gained the trust of the community where higher problem complexity and cognitive workload are required. To address trust, we must revisit the process for developing autonomous capabilities: modeling and simulation. Given the prohibitive costs for live tests, we need to prototype and evaluate autonomous aerial agents in a high fidelity flight simulator with autonomous learning capabilities applicable to flight systems: such a open-source development platform is not available. As a result, we have developed GymFG: GymFG couples and extends a high fidelity, open-source flight simulator and a robust agent learning framework to facilitate learning of more complex tasks. Furthermore, we have demonstrated the use of GymFG to train an autonomous aerial agent using Imitation Learning. With GymFG, we can now deploy innovative ideas to address complex problems and build the trust necessary to move prototypes to the real-world. △ Less","26 April, 2020",https://arxiv.org/pdf/2004.12481
All you need is a second look: Towards Tighter Arbitrary shape text detection,Meng Cao;Yuexian Zou,"Deep learning-based scene text detection methods have progressed substantially over the past years. However, there remain several problems to be solved. Generally, long curve text instances tend to be fragmented because of the limited receptive field size of CNN. Besides, simple representations using rectangle or quadrangle bounding boxes fall short when dealing with more challenging arbitrary-shaped texts. In addition, the scale of text instances varies greatly which leads to the difficulty of accurate prediction through a single segmentation network. To address these problems, we innovatively propose a two-stage segmentation based arbitrary text detector named \textit{NASK} (\textbf{N}eed \textbf{A} \textbf{S}econd loo\textbf{K}). Specifically, \textit{NASK} consists of a Text Instance Segmentation network namely \textit{TIS} (1^{st} stage), a Text RoI Pooling module and a Fiducial pOint eXpression module termed as \textit{FOX} (2^{nd} stage). Firstly, \textit{TIS} conducts instance segmentation to obtain rectangle text proposals with a proposed Group Spatial and Channel Attention module (\textit{GSCA}) to augment the feature expression. Then, Text RoI Pooling transforms these rectangles to the fixed size. Finally, \textit{FOX} is introduced to reconstruct text instances with a more tighter representation using the predicted geometrical attributes including text center line, text line orientation, character scale and character orientation. Experimental results on two public benchmarks including \textit{Total-Text} and \textit{SCUT-CTW1500} have demonstrated that the proposed \textit{NASK} achieves state-of-the-art results. △ Less","26 April, 2020",https://arxiv.org/pdf/2004.12436
Beyond 512 Tokens: Siamese Multi-depth Transformer-based Hierarchical Encoder for Long-Form Document Matching,Liu Yang;Mingyang Zhang;Cheng Li;Michael Bendersky;Marc Najork,"Many natural language processing and information retrieval problems can be formalized as the task of semantic matching. Existing work in this area has been largely focused on matching between short texts (e.g., question answering), or between a short and a long text (e.g., ad-hoc retrieval). Semantic matching between long-form documents, which has many important applications like news recommendation, related article recommendation and document clustering, is relatively less explored and needs more research effort. In recent years, self-attention based models like Transformers and BERT have achieved state-of-the-art performance in the task of text matching. These models, however, are still limited to short text like a few sentences or one paragraph due to the quadratic computational complexity of self-attention with respect to input text length. In this paper, we address the issue by proposing the Siamese Multi-depth Transformer-based Hierarchical (SMITH) Encoder for long-form document matching. Our model contains several innovations to adapt self-attention models for longer text input. In order to better capture sentence level semantic relations within a document, we pre-train the model with a novel masked sentence block language modeling task in addition to the masked word language modeling task used by BERT. Our experimental results on several benchmark datasets for long-form document matching show that our proposed SMITH model outperforms the previous state-of-the-art models including hierarchical attention, multi-depth attention-based hierarchical recurrent neural network, and BERT. Comparing to BERT based baselines, our model is able to increase maximum input text length from 512 to 2048. We will open source a Wikipedia based benchmark dataset, code and a pre-trained checkpoint to accelerate future research on long-form document matching. △ Less","12 October, 2020",https://arxiv.org/pdf/2004.12297
QURATOR: Innovative Technologies for Content and Data Curation,Georg Rehm;Peter Bourgonje;Stefanie Hegele;Florian Kintzel;Julián Moreno Schneider;Malte Ostendorff;Karolina Zaczynska;Armin Berger;Stefan Grill;Sören Räuchle;Jens Rauenbusch;Lisa Rutenburg;André Schmidt;Mikka Wild;Henry Hoffmann;Julian Fink;Sarah Schulz;Jurica Seva;Joachim Quantz;Joachim Böttger;Josefine Matthey;Rolf Fricke;Jan Thomsen;Adrian Paschke;Jamal Al Qundus,"In all domains and sectors, the demand for intelligent systems to support the processing and generation of digital content is rapidly increasing. The availability of vast amounts of content and the pressure to publish new content quickly and in rapid succession requires faster, more efficient and smarter processing and generation methods. With a consortium of ten partners from research and industry and a broad range of expertise in AI, Machine Learning and Language Technologies, the QURATOR project, funded by the German Federal Ministry of Education and Research, develops a sustainable and innovative technology platform that provides services to support knowledge workers in various industries to address the challenges they face when curating digital content. The project's vision and ambition is to establish an ecosystem for content curation technologies that significantly pushes the current state of the art and transforms its region, the metropolitan area Berlin-Brandenburg, into a global centre of excellence for curation technologies. △ Less","25 April, 2020",https://arxiv.org/pdf/2004.12195
Efficient High Capacity Steganography Technique,Alan Anwer Abdulla;Sabah A. Jassim;Harin Sellahewa,"Performance indicators characterizing modern steganographic techniques include capacity (i.e. the quantity of data that can be hidden in the cover medium), stego quality (i.e. artifacts visibility), security (i.e. undetectability), and strength or robustness (intended as the resistance against active attacks aimed to destroy the secret message). Fibonacci based embedding techniques have been researched and proposed in the literature to achieve efficient steganography in terms of capacity with respect to stego quality. In this paper, we investigated an innovative idea that extends Fibonacci-like steganography by bit-plane(s) mapping instead of bit-plane(s) replacement. Our proposed algorithm increases embedding capacity using bit-plane mapping to embed two bits of the secret message in three bits of a pixel of the cover, at the expense of a marginal loss in stego quality. While existing Fibonacci embedding algorithms do not use certain intensities of the cover for embedding due to the limitation imposed by the Zeckendorf theorem, our proposal solve this problem and make all intensity values candidates for embedding. Experimental results demonstrate that the proposed technique double the embedding capacity when compared to existing Fibonacci methods, and it is secure against statistical attacks such as RS, POV, and difference image histogram (DIH). △ Less","24 April, 2020",https://arxiv.org/pdf/2004.11984
Improving embedding efficiency for digital steganography by exploiting similarities between secret and cover images,Alan A. Abdulla;Harin Sellahewa;Sabah A. Jassim,"Digital steganography is becoming a common tool for protecting sensitive communications in various applications such as crime(terrorism) prevention whereby law enforcing personals need to remotely compare facial images captured at the scene of crime with faces databases of known criminals(suspects); exchanging military maps or surveillance video in hostile environment(situations); privacy preserving in the healthcare systems when storing or exchanging patient medical images(records); and prevent bank customers accounts(records) from being accessed illegally by unauthorized users. Existing digital steganography schemes for embedding secret images in cover image files tend not to exploit various redundancies in the secret image bit-stream to deal with the various conflicting requirements on embedding capacity, stego-image quality, and un-detectibility. This paper is concerned with the development of innovative image procedures and data hiding schemes that exploit, as well as increase, similarities between secret image bit-stream and the cover image LSB plane. This will be achieved in two novel steps involving manipulating both the secret and the cover images,prior to embedding, to achieve higher 0:1 ratio in both the secret image bit-stream and the cover image LSB plane. The above two steps strategy has been exploited to use a bit-plane(s) mapping technique, instead of bit-plane(s) replacement to make each cover pixel usable for secret embedding. This paper will demonstrate that this strategy produces stego-images that have minimal distortion, high embedding efficiency, reasonably good stego-image quality and robustness against 3 well-known targeted steganalysis tools. △ Less","24 April, 2020",https://arxiv.org/pdf/2004.11974
Recent Advancements in Defected Ground Structure Based Near-Field Wireless Power Transfer Systems,Kassen Dautov;Mohammad Hashmi;Galymzhan Nauryzbayev;M. Nasimuddin,"The defected ground structure (DGS) technique enables miniaturization of the resonator which leads to the development of the compact near-field wireless power transfer (WPT) systems. In general, numerous challenges are inherent in the design of the DGS-based WPT systems and, hence, appropriate trade-offs for achieving optimal performance are required. Furthermore, the design advancements have led to the development of the DGS-based multi-band WPT systems to fulfill the needs of simultaneous data and power transfer. The innovations in the DGS-based WPT systems have also resulted in the definition of more commonly used figures-of-merit for the benchmarking of various performance metrics. The literature is replete with the design schemes to address one or more associated design challenges and successful WPT system realizations with enhanced performance. With this in mind, this paper touches upon the DGS-based WPTs developments and presents a concise report on the current state-of-the-art and future directions. △ Less","24 April, 2020",https://arxiv.org/pdf/2004.11673
Optimal Team Recruitment Strategies for Collaborative Mobile Crowdsourcing Systems,Aymen Hamrouni;Hakim Ghazzai;Turki Alelyani;Yehia Massoud,"The wide spread of mobile devices has enabled a new paradigm of innovation called Mobile Crowdsourcing (MCS) where the concept is to allow entities, e.g., individuals or local authorities, to hire workers to help from the crowd of connected people, to execute a task or service. Some complex tasks require the collaboration of multiple workers to ensure its successful completion. In this context, the task requester needs to hire a group of socially connected and collaborative workers that, at the same time, have sufficient skills to accomplish the task. In this paper, we develop two recruitment strategies for collaborative MCS frameworks in which, virtual teams are formed according to four different criteria: level of expertise, social relationship strength, recruitment cost, and recruiter's confidence level. The first proposed strategy is a platform-based approach which exploits the platform knowledge to form the team. The second one is a leader-based approach that uses team members' knowledge about their social network (SN) neighbors to designate a group leader that recruits its suitable team. Both approaches are modeled as integer linear programs resulting in optimal team formation. Experimental results show a performance trade-off between the two virtual team grouping strategies when varying the members SN edge degree. Compared to the leader-based strategy, the platform-based strategy recruits a more skilled team but with lower SN relationships and higher cost. △ Less","27 April, 2020",https://arxiv.org/pdf/2004.11512
Reusing empirical knowledge during cloud computing adoption,Mahdi Fahmideh;Ghassan Beydoun,"Moving legacy software systems to cloud platforms is an ever popular option. But, such an endeavour may not be hazard-free and demands a proper understanding of requirements and risks involved prior to taking any actions. The time is indeed ripe to undertake a realistic view of what migrating systems to the cloud may offer, an understanding of exceptional situations causing system quality goal failure, and insights on countermeasures. The cloud migration body of knowledge, although is useful, is dispersed over the current literature. It is hard for busy practitioners to digest, synthesize, and harness this body of knowledge into practice in a scenario of integrating legacy systems with cloud services. We address this issue by creating an innovative synergy between the approaches evidence-based software engineering and goal-oriented modelling. We develop an evidential repository of commonly occurred obstacles and platform agnostic resolution tactics related to making systems cloud-enabled. The repository is further utilized during the systematic goal-obstacle elaboration of given cloud migration scenarios. The applicability of the proposed framework is also demonstrated. △ Less","16 April, 2020",https://arxiv.org/pdf/2004.11268
Using DSP Slices as Content-Addressable Update Queues,Thomas B. Preußer;Monica Chiosa;Alexander Weiss;Gustavo Alonso,"Content-Addressable Memory (CAM) is a powerful abstraction for building memory caches, routing tables and hazard detection logic. Without a native CAM structure available on FPGA devices, their functionality must be emulated using the structural primitives at hand. Such an emulation causes significant overhead in the consumption of the underlying resources, typically general-purpose fabric and on-chip block RAM (BRAM). This often motivates mitigating trade-offs, such as the reduction of the associativity of memory caches. This paper describes a technique to implement the hazard resolution in a memory update queue that hides the off-chip memory readout latency of read-modify-write cycles while guaranteeing the delivery of the full memory bandwidth. The innovative use of DSP slices allows them to assume and combine the functions of (a) the tag and data storage, (b) the tag matching, and (c) the data update in this key-value storage scenario. The proposed approach provides designers with extra flexibility by adding this resource type as another option to implement CAM. △ Less","23 April, 2020",https://arxiv.org/pdf/2004.11080
Accurate runtime selection of optimal MPI collective algorithms using analytical performance modelling,Emin Nuriyev;Alexey Lastovetsky,"The performance of collective operations has been a critical issue since the advent of MPI. Many algorithms have been proposed for each MPI collective operation but none of them proved optimal in all situations. Different algorithms demonstrate superior performance depending on the platform, the message size, the number of processes, etc. MPI implementations perform the selection of the collective algorithm empirically, executing a simple runtime decision function. While efficient, this approach does not guarantee the optimal selection. As a more accurate but equally efficient alternative, the use of analytical performance models of collective algorithms for the selection process was proposed and studied. Unfortunately, the previous attempts in this direction have not been successful. We revisit the analytical model-based approach and propose two innovations that significantly improve the selective accuracy of analytical models: (1) We derive analytical models from the code implementing the algorithms rather than from their high-level mathematical definitions. This results in more detailed models. (2) We estimate model parameters separately for each collective algorithm and include the execution of this algorithm in the corresponding communication experiment. We experimentally demonstrate the accuracy and efficiency of our approach using Open MPI broadcast and gather algorithms and a Grid5000 cluster. △ Less","23 April, 2020",https://arxiv.org/pdf/2004.11062
Modeling Network Architecture: A Cloud Case Study,Sabah Al-Fedaghi;Dana Al-Qemlas,"The Internet s ability to support a wide range of services depends on the network architecture and theoretical and practical innovations necessary for future networks. Network architecture in this context refers to the structure of a computer network system as well as interactions among its physical components, their configuration, and communication protocols. Various descriptions of architecture have been developed over the years with an unusually large number of superficial icons and symbols. This situation has created a need for more coherent systematic representations of network architecture. This paper is intended to refine the design, analysis, and documentation of network architecture by adopting a conceptual model called a thinging (abstract) machine (TM), which views all components of a network in terms of a single notion: the flow of things in a TM. Since cloud computing has become increasingly popular in the last few years as a model for a shared pool of networks, servers, storage, and applications, we apply the TM to model a real case study of cloud networks. The resultant model introduces an integrated representation of computer networks. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.10350
On the Relevance of Wait-free Coordination Algorithms in Shared-Memory HPC:The Global Virtual Time Case,Alessandro Pellegrini;Francesco Quaglia,"High-performance computing on shared-memory/multi-core architectures could suffer from non-negligible performance bottlenecks due to coordination algorithms, which are nevertheless necessary to ensure the overall correctness and/or to support the execution of housekeeping operations, e.g. to recover computing resources (e.g., memory). Although more complex in design/development, a paradigm switch from classical coordination algorithms to wait-free ones could significantly boost the performance of HPC applications. In this paper we explore the relevance of this paradigm shift in shared-memory architectures, by focusing on the context of Parallel Discrete Event Simulation, where the Global Virtual Time (GVT) represents a fundamental coordination algorithm. It allows to compute the lower bound on the value of the logical time passed through by all the entities participating in a parallel/distributed computation. Hence it can be used to discriminate what events belong to the past history of the computation---thus being considered as committed---and allowing for memory recovery (e.g. of obsolete logs that were taken in order to support state recoverability) and non-revokable operations (e.g. I/O). We compare the reference (blocking) algorithm for shared memory, the one proposed by by Fujimoto and Hybinette \cite{Fuj97}, with an innovative wait-free implementation, emphasizing on what design choices must be made to enforce this paradigm shift, and what are the performance implications of removing critical sections in coordination algorithms. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.10033
The rise of science in low-carbon energy technologies,Kerstin Hötte;Anton Pichler;François Lafond,"Successfully combating climate change will require substantial technological improvements in Low-Carbon Energy Technologies (LCETs), but designing efficient allocation of R\&D budgets requires a better understanding of how LCETs rely on scientific knowledge. Using data covering almost all US patents and scientific articles that are cited by them over the past two centuries, we describe the evolution of knowledge bases of ten key LCETs and show how technological interdependencies have changed over time. The composition of low-carbon energy innovations shifted over time, from Hydro and Wind energy in the 19th and early 20th century, to Nuclear fission after World War II, and more recently to Solar PV and back to Wind. In recent years, Solar PV, Nuclear fusion and Biofuels (including energy from waste) have 35-65\% of their citations directed toward scientific papers, while this ratio is less than 10\% for Wind, Solar thermal, Hydro, Geothermal, and Nuclear fission. Over time, the share of patents citing science and the share of citations that are to scientific papers has been increasing for all technology types. The analysis of the scientific knowledge base of each LCET reveals three fairly separate clusters, with nuclear energy technologies, Biofuels and Waste, and all the other LCETs. Our detailed description of knowledge requirements for each LCET helps to design of targeted innovation policies. △ Less","4 September, 2020",https://arxiv.org/pdf/2004.09959
Innovation and Revenue: Deep Diving into the Temporal Rank-shifts of Fortune 500 Companies,Mayank Singh;Arindam Pal;Lipika Dey;Animesh Mukherjee,"Research and innovation is important agenda for any company to remain competitive in the market. The relationship between innovation and revenue is a key metric for companies to decide on the amount to be invested for future research. Two important parameters to evaluate innovation are the quantity and quality of scientific papers and patents. Our work studies the relationship between innovation and patenting activities for several Fortune 500 companies over a period of time. We perform a comprehensive study of the patent citation dataset available in the Reed Technology Index collected from the US Patent Office. We observe several interesting relations between parameters like the number of (i) patent applications, (ii) patent grants, (iii) patent citations and Fortune 500 ranks of companies. We also study the trends of these parameters varying over the years and derive causal explanations for these with qualitative and intuitive reasoning. To facilitate reproducible research, we make all the processed patent dataset publicly available at https://github.com/mayank4490/Innovation-and-revenue. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.09715
The Panacea Threat Intelligence and Active Defense Platform,Adam Dalton;Ehsan Aghaei;Ehab Al-Shaer;Archna Bhatia;Esteban Castillo;Zhuo Cheng;Sreekar Dhaduvai;Qi Duan;Md Mazharul Islam;Younes Karimi;Amir Masoumzadeh;Brodie Mather;Sashank Santhanam;Samira Shaikh;Tomek Strzalkowski;Bonnie J. Dorr,"We describe Panacea, a system that supports natural language processing (NLP) components for active defenses against social engineering attacks. We deploy a pipeline of human language technology, including Ask and Framing Detection, Named Entity Recognition, Dialogue Engineering, and Stylometry. Panacea processes modern message formats through a plug-in architecture to accommodate innovative approaches for message analysis, knowledge representation and dialogue generation. The novelty of the Panacea system is that uses NLP for cyber defense and engages the attacker using bots to elicit evidence to attribute to the attacker and to waste the attacker's time and resources. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.09662
A Study of Knowledge Sharing related to Covid-19 Pandemic in Stack Overflow,Konstantinos Georgiou;Nikolaos Mittas;Lefteris Angelis;Alexander Chatzigeorgiou,"The Covid-19 outbreak, beyond its tragic effects, has changed to an unprecedented extent almost every aspect of human activity throughout the world. At the same time, the pandemic has stimulated enormous amount of research by scientists across various disciplines, seeking to study the phenomenon itself, its epidemiological characteristics and ways to confront its consequences. Information Technology, and particularly Data Science, drive innovation in all related to Covid-19 biomedical fields. Acknowledging that software developers routinely resort to open question and answer communities like Stack Overflow to seek advice on solving technical issues, we have performed an empirical study to investigate the extent, evolution and characteristics of Covid-19 related posts. In particular, through the study of 464 Stack Overflow questions posted mainly in February and March 2020 and leveraging the power of text mining, we attempt to shed light into the interest of developers in Covid-19 related topics and the most popular technological problems for which the users seek information. The findings reveal that indeed this global crisis sparked off an intense and increasing activity in Stack Overflow with most post topics reflecting a strong interest on the analysis of Covid-19 data, primarily using Python technologies. △ Less","18 April, 2020",https://arxiv.org/pdf/2004.09495
A Novel Multi-Agent System for Complex Scheduling Problems,Peter Hillmann;Tobias Uhlig;Gabi Dreo Rodosek;Oliver Rose,"Complex scheduling problems require a large amount computation power and innovative solution methods. The objective of this paper is the conception and implementation of a multi-agent system that is applicable in various problem domains. Independent specialized agents handle small tasks, to reach a superordinate target. Effective coordination is therefore required to achieve productive cooperation. Role models and distributed artificial intelligence are employed to tackle the resulting challenges. We simulate a NP-hard scheduling problem to demonstrate the validity of our approach. In addition to the general agent based framework we propose new simulation-based optimization heuristics to given scheduling problems. Two of the described optimization algorithms are implemented using agents. This paper highlights the advantages of the agent-based approach, like the reduction in layout complexity, improved control of complicated systems, and extendability. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.09312
How context impacts on media choice,Stefan Stieglitz;Tobias Brockmann;Milad Mirbabaie,"The relevance of mobile working is steadily increasing. Based on new mobile devices (e.g. smartphones) and their innovative functionalities, an increasing amount of data is being made available ubiquitously. As a result, the growing diffusion of smartphones offers new potential for enterprises. Current mobile devices and related mobile networks have reached a high level of maturity. Thus, the organizational aspects of mobile work have become a focal point of interest for enterprises as well as for academics. This research article addresses the question: How does context influence the choice of communication channels of mobile knowledge workers? An explorative research approach is used to collect and analyse 418 communication incidents, which were initiated by mobile knowledge workers. The results indicate that (1) the context (e.g. travelling on trains) influences the usage of communication channels and (2) smartphones enable the usage of communication channels (e.g. email) in certain contexts. △ Less","18 April, 2020",https://arxiv.org/pdf/2004.08571
CAggNet: Crossing Aggregation Network for Medical Image Segmentation,Xu Cao;Yanghao Lin,"In this paper, we present Crossing Aggregation Network (CAggNet), a novel densely connected semantic segmentation approach for medical image analysis. The crossing aggregation network improves the idea from deep layer aggregation and makes significant innovations in semantic and spatial information fusion. In CAggNet, the simple skip connection structure of general U-Net is replaced by aggregations of multi-level down-sampling and up-sampling layers, which is a new form of nested skip connection. This aggregation architecture enables the network to fuse both coarse and fine features interactively in semantic segmentation. It also introduces weighted aggregation module to up-sample multi-scale output at the end of the network. We have evaluated and compared our CAggNet with several advanced U-Net based methods in two public medical image datasets, including the 2018 Data Science Bowl nuclei detection dataset and the 2015 MICCAI gland segmentation competition dataset. Experimental results indicate that CAggNet improves medical object recognition and achieves a more accurate and efficient segmentation compared to existing improved U-Net and UNet++ structure. △ Less","7 November, 2020",https://arxiv.org/pdf/2004.08237
Big data analytics architecture design,Mahdi Fahmideh;Ghassan Beydoun,"Objective. We propose an approach to reason about goals, obstacles, and to select suitable big data solution architecture that satisfy quality goal preferences and constraints of stakeholders at the presence of the decision outcome uncertainty. The approach will highlight situations that may impede the goals. They will be assessed and resolved to generate complete requirements of an architectural solution. Method. The approach employs goal-oriented modelling to identify obstacles causing quality goal failure and their corresponding resolution tactics. It combines fuzzy logic to explore uncertainties in solution architectures and to find an optimal set of architectural decisions for the big data enablement process of manufacturing systems. Result. The approach brings two innovations to the state of the art of big data analytics platform adoption in manufacturing systems. Firstly, A systematic goal-oriented modelling for exploring goals and obstacles in integrating manufacturing systems with data analytics platforms at the requirement level and, secondly, A systematic analysis of the architectural decisions under uncertainty incorporating the preferences of stakeholders. The efficacy of the approach is illustrated with a scenario of reengineering a hyper-connected manufacturing collaboration system to a new big data architecture. Keywords. big data, big data analytics platforms, manufacturing systems, goal-oriented modeling, fuzzy logic △ Less","16 April, 2020",https://arxiv.org/pdf/2004.08021
Coresets for Clustering in Excluded-minor Graphs and Beyond,Vladimir Braverman;Shaofeng H. -C. Jiang;Robert Krauthgamer;Xuan Wu,"Coresets are modern data-reduction tools that are widely used in data analysis to improve efficiency in terms of running time, space and communication complexity. Our main result is a fast algorithm to construct a small coreset for k-Median in (the shortest-path metric of) an excluded-minor graph. Specifically, we give the first coreset of size that depends only on k, ε and the excluded-minor size, and our running time is quasi-linear (in the size of the input graph). The main innovation in our new algorithm is that is iterative; it first reduces the n input points to roughly O(\log n) reweighted points, then to O(\log\log n), and so forth until the size is independent of n. Each step in this iterative size reduction is based on the importance sampling framework of Feldman and Langberg (STOC 2011), with a crucial adaptation that reduces the number of \emph{distinct points}, by employing a terminal embedding (where low distortion is guaranteed only for the distance from every terminal to all other points). Our terminal embedding is technically involved and relies on shortest-path separators, a standard tool in planar and excluded-minor graphs. Furthermore, our new algorithm is applicable also in Euclidean metrics, by simply using a recent terminal embedding result of Narayanan and Nelson, (STOC 2019), which extends the Johnson-Lindenstrauss Lemma. We thus obtain an efficient coreset construction in high-dimensional Euclidean spaces, thereby matching and simplifying state-of-the-art results (Sohler and Woodruff, FOCS 2018; Huang and Vishnoi, STOC 2020). In addition, we also employ terminal embedding with additive distortion to obtain small coresets in graphs with bounded highway dimension, and use applications of our coresets to obtain improved approximation schemes, e.g., an improved PTAS for planar k-Median via a new centroid set. △ Less","15 July, 2020",https://arxiv.org/pdf/2004.07718
An Application of Deep Reinforcement Learning to Algorithmic Trading,Thibaut Théate;Damien Ernst,"This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in stock markets. It proposes a novel DRL trading strategy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new trading strategy is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN strategy. △ Less","9 October, 2020",https://arxiv.org/pdf/2004.06627
Scholarly migration within Mexico: Analyzing internal migration among researchers using Scopus longitudinal bibliometric data,Andrea Miranda-González;Samin Aref;Tom Theile;Emilio Zagheni,"The migration of scholars is a major driver of innovation and of diffusion of knowledge. Although large-scale bibliometric data have been used to measure international migration of scholars, our understanding of internal migration among researchers is very limited. This is partly due to a lack of data aggregated at a suitable sub-national level. In this study, we analyze internal migration in Mexico based on over 1.1 million authorship records from the Scopus database. We trace the movements of scholars between Mexican states, and provide key demographic measures of internal migration for the 1996-2018 period. From a methodological perspective, we develop a new framework for enhancing data quality, inferring states from affiliations, and detecting moves from modal states for the purposes of studying internal migration among researchers. Substantively, we combine demographic and network science techniques to improve our understanding of internal migration patterns within country boundaries. The migration patterns between states in Mexico appear to be heterogeneous in size and direction across regions. However, while many scholars remain in their regions, there seems to be a preference for Mexico City and the surrounding states as migration destinations. We observed that over the past two decades, there has been a general decreasing trend in the crude migration intensity. However, the migration network has become more dense and more diverse, and has included greater exchanges between states along the Gulf and the Pacific Coast. Our analysis, which is mostly empirical in nature, lays the foundations for testing and developing theories that can rely on the analytical framework developed by migration scholars, and the richness of appropriately processed bibliometric data. △ Less","30 October, 2020",https://arxiv.org/pdf/2004.06539
COVID-19 Blockchain Framework: Innovative Approach,Mohamed Torky;Aboul Ella Hassanien,"The world is currently witnessing dangerous shifts in the epidemic of emerging SARS-CoV-2, the causative agent of (COVID-19) coronavirus. The infection, and death numbers reported by World Health Organization (WHO) about this epidemic forecasts an increasing threats to the lives of people and the economics of countries. The greatest challenge that most governments are currently suffering from is the lack of a precise mechanism to detect unknown infected cases and predict the infection risk of COVID-19 virus. In response to mitigate this challenge, this study proposes a novel innovative approach for mitigating big challenges of (COVID-19) coronavirus propagation and contagion. This study propose a blockchain-based framework which investigate the possibility of utilizing peer-to peer, time stamping, and decentralized storage advantages of blockchain to build a new system for verifying and detecting the unknown infected cases of COVID-19 virus. Moreover, the proposed framework will enable the citizens to predict the infection risk of COVID-19 virus within conglomerates of people or within public places through a novel design of P2P-Mobile Application. The proposed approach is forecasted to produce an effective system able to support governments, health authorities, and citizens to take critical decision regarding the infection detection, infection prediction, and infection avoidance. The framework is currently being developed and implemented as a new system consists of four components, Infection Verifier Subsystem, Blockchain platform, P2P-Mobile Application, and Mass-Surveillance System. This four components work together for detecting the unknown infected cases and predicting and estimating the infection Risk of Corona Virus (COVID-19). △ Less","5 April, 2020",https://arxiv.org/pdf/2004.06081
Software-Defined Network for End-to-end Networked Science at the Exascale,Inder Monga;Chin Guok;John MacAuley;Alex Sim;Harvey Newman;Justas Balcas;Phil DeMar;Linda Winkler;Tom Lehman;Xi Yang,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence. △ Less","13 April, 2020",https://arxiv.org/pdf/2004.05953
A Machine Learning Approach for Flagging Incomplete Bid-rigging Cartels,Hannes Wallimann;David Imhof;Martin Huber,"We propose a new method for flagging bid rigging, which is particularly useful for detecting incomplete bid-rigging cartels. Our approach combines screens, i.e. statistics derived from the distribution of bids in a tender, with machine learning to predict the probability of collusion. As a methodological innovation, we calculate such screens for all possible subgroups of three or four bids within a tender and use summary statistics like the mean, median, maximum, and minimum of each screen as predictors in the machine learning algorithm. This approach tackles the issue that competitive bids in incomplete cartels distort the statistical signals produced by bid rigging. We demonstrate that our algorithm outperforms previously suggested methods in applications to incomplete cartels based on empirical data from Switzerland. △ Less","12 April, 2020",https://arxiv.org/pdf/2004.05629
Gradients as Features for Deep Representation Learning,Fangzhou Mu;Yingyu Liang;Yin Li,"We address the challenging problem of deep representation learning--the efficient adaption of a pre-trained deep network to different tasks. Specifically, we propose to explore gradient-based features. These features are gradients of the model parameters with respect to a task-specific loss given an input sample. Our key innovation is the design of a linear model that incorporates both gradient and activation of the pre-trained network. We show that our model provides a local linear approximation to an underlying deep model, and discuss important theoretical insights. Moreover, we present an efficient algorithm for the training and inference of our model without computing the actual gradient. Our method is evaluated across a number of representation-learning tasks on several datasets and using different network architectures. Strong results are obtained in all settings, and are well-aligned with our theoretical insights. △ Less","11 April, 2020",https://arxiv.org/pdf/2004.05529
Towards Anomaly Detection in Dashcam Videos,Sanjay Haresh;Sateesh Kumar;M. Zeeshan Zia;Quoc-Huy Tran,"Inexpensive sensing and computation, as well as insurance innovations, have made smart dashboard cameras ubiquitous. Increasingly, simple model-driven computer vision algorithms focused on lane departures or safe following distances are finding their way into these devices. Unfortunately, the long-tailed distribution of road hazards means that these hand-crafted pipelines are inadequate for driver safety systems. We propose to apply data-driven anomaly detection ideas from deep learning to dashcam videos, which hold the promise of bridging this gap. Unfortunately, there exists almost no literature applying anomaly understanding to moving cameras, and correspondingly there is also a lack of relevant datasets. To counter this issue, we present a large and diverse dataset of truck dashcam videos, namely RetroTrucks, that includes normal and anomalous driving scenes. We apply: (i) one-class classification loss and (ii) reconstruction-based loss, for anomaly detection on RetroTrucks as well as on existing static-camera datasets. We introduce formulations for modeling object interactions in this context as priors. Our experiments indicate that our dataset is indeed more challenging than standard anomaly detection datasets, and previous anomaly detection methods do not perform well here out-of-the-box. In addition, we share insights into the behavior of these two important families of anomaly detection approaches on dashcam data. △ Less","11 May, 2020",https://arxiv.org/pdf/2004.05261
Global Public Health Surveillance using Media Reports: Redesigning GPHIN,Dave Carter;Marta Stojanovic;Philip Hachey;Kevin Fournier;Simon Rodier;Yunli Wang;Berry de Bruijn,"Global public health surveillance relies on reporting structures and transmission of trustworthy health reports. But in practice, these processes may not always be fast enough, or are hindered by procedural, technical, or political barriers. GPHIN, the Global Public Health Intelligence Network, was designed in the late 1990s to scour mainstream news for health events, as that travels faster and more freely. This paper outlines the next generation of GPHIN, which went live in 2017, and reports on design decisions underpinning its new functions and innovations. △ Less","9 April, 2020",https://arxiv.org/pdf/2004.04596
Time accelerated image super-resolution using shallow residual feature representative network,Meenu Ajith;Aswathy Rajendra Kurup;Manel Martínez-Ramón,"The recent advances in deep learning indicate significant progress in the field of single image super-resolution. With the advent of these techniques, high-resolution image with high peak signal to noise ratio (PSNR) and excellent perceptual quality can be reconstructed. The major challenges associated with existing deep convolutional neural networks are their computational complexity and time; the increasing depth of the networks, often result in high space complexity. To alleviate these issues, we developed an innovative shallow residual feature representative network (SRFRN) that uses a bicubic interpolated low-resolution image as input and residual representative units (RFR) which include serially stacked residual non-linear convolutions. Furthermore, the reconstruction of the high-resolution image is done by combining the output of the RFR units and the residual output from the bicubic interpolated LR image. Finally, multiple experiments have been performed on the benchmark datasets and the proposed model illustrates superior performance for higher scales. Besides, this model also exhibits faster execution time compared to all the existing approaches. △ Less","8 April, 2020",https://arxiv.org/pdf/2004.04093
LLHD: A Multi-level Intermediate Representation for Hardware Description Languages,Fabian Schuiki;Andreas Kurth;Tobias Grosser;Luca Benini,"Modern Hardware Description Languages (HDLs) such as SystemVerilog or VHDL are, due to their sheer complexity, insufficient to transport designs through modern circuit design flows. Instead, each design automation tool lowers HDLs to its own Intermediate Representation (IR). These tools are monolithic and mostly proprietary, disagree in their implementation of HDLs, and while many redundant IRs exists, no IR today can be used through the entire circuit design flow. To solve this problem, we propose the LLHD multi-level IR. LLHD is designed as simple, unambiguous reference description of a digital circuit, yet fully captures existing HDLs. We show this with our reference compiler on designs as complex as full CPU cores. LLHD comes with lowering passes to a hardware-near structural IR, which readily integrates with existing tools. LLHD establishes the basis for innovation in HDLs and tools without redundant compilers or disjoint IRs. For instance, we implement an LLHD simulator that runs up to 2.4x faster than commercial simulators but produces equivalent, cycle-accurate results. An initial vertically-integrated research prototype is capable of representing all levels of the IR, implements lowering from the behavioural to the structural IR, and covers a sufficient subset of SystemVerilog to support a full CPU design. △ Less","7 April, 2020",https://arxiv.org/pdf/2004.03494
Geometrically Principled Connections in Graph Neural Networks,Shunwang Gong;Mehdi Bahri;Michael M. Bronstein;Stefanos Zafeiriou,"Graph convolution operators bring the advantages of deep learning to a variety of graph and mesh processing tasks previously deemed out of reach. With their continued success comes the desire to design more powerful architectures, often by adapting existing deep learning techniques to non-Euclidean data. In this paper, we argue geometry should remain the primary driving force behind innovation in the emerging field of geometric deep learning. We relate graph neural networks to widely successful computer graphics and data approximation models: radial basis functions (RBFs). We conjecture that, like RBFs, graph convolution layers would benefit from the addition of simple functions to the powerful convolution kernels. We introduce affine skip connections, a novel building block formed by combining a fully connected layer with any graph convolution operator. We experimentally demonstrate the effectiveness of our technique and show the improved performance is the consequence of more than the increased number of parameters. Operators equipped with the affine skip connection markedly outperform their base performance on every task we evaluated, i.e., shape reconstruction, dense shape correspondence, and graph classification. We hope our simple and effective approach will serve as a solid baseline and help ease future research in graph neural networks. △ Less","6 April, 2020",https://arxiv.org/pdf/2004.02658
A Two-Stage Reconstruction of Microstructures with Arbitrarily Shaped Inclusions,R. Piasecki;W. Olchawa;D. Frączek;A. Bartecka,"The main goal of our research is to develop an effective method with a wide range of applications for the statistical reconstruction of heterogeneous microstructures with compact inclusions of any shape, such as highly irregular grains. The devised approach uses multi-scale extended entropic descriptors (ED) that quantify the degree of spatial non-uniformity of configurations of finite-sized objects. This technique is an innovative development of previously elaborated entropy methods for statistical reconstruction. Here, we discuss the two-dimensional case, but this method can be generalized into three dimensions. At the first stage, the developed procedure creates a set of black synthetic clusters that serve as surrogate inclusions. The clusters have the same individual areas and interfaces as their target counterparts, but random shapes. Then, from a given number of easy-to-generate synthetic cluster configurations, we choose the one with the lowest value of the cost function defined by us using extended ED. At the second stage, we make a significant change in the standard technique of simulated annealing (SA). Instead of swapping pixels of different phases, we randomly move each of the selected synthetic clusters. To demonstrate the accuracy of the method, we reconstruct and analyze two-phase microstructures with irregular inclusions of silica in rubber matrix as well as stones in cement paste. The results show that the two-stage reconstruction (TSR) method provides convincing realizations for these complex microstructures. The advantages of TSR include the ease of obtaining synthetic microstructures, very low computational costs, and satisfactory mapping in the statistical context of inclusion shapes. Finally, its simplicity should greatly facilitate independent applications. △ Less","9 July, 2020",https://arxiv.org/pdf/2004.02587
Speaker Recognition using SincNet and X-Vector Fusion,Mayank Tripathi;Divyanshu Singh;Seba Susan,"In this paper, we propose an innovative approach to perform speaker recognition by fusing two recently introduced deep neural networks (DNNs) namely - SincNet and X-Vector. The idea behind using SincNet filters on the raw speech waveform is to extract more distinguishing frequency-related features in the initial convolution layers of the CNN architecture. X-Vectors are used to take advantage of the fact that this embedding is an efficient method to churn out fixed dimension features from variable length speech utterances, something which is challenging in plain CNN techniques, making it efficient both in terms of speed and accuracy. Our approach uses the best of both worlds by combining X-vector in the later layers while using SincNet filters in the initial layers of our deep model. This approach allows the network to learn better embedding and converge quicker. Previous works use either X-Vector or SincNet Filters or some modifications, however we introduce a novel fusion architecture wherein we have combined both the techniques to gather more information about the speech signal hence, giving us better results. Our method focuses on the VoxCeleb1 dataset for speaker recognition, and we have used it for both training and testing purposes. △ Less","5 April, 2020",https://arxiv.org/pdf/2004.02219
Software Radios for Unmanned Aerial Systems,Keith Powell;Aly Sabri;Daniel Brennan;Vuk Marojevic;R. Michael Barts;Ashwin Panicker;Ozgur Ozdemir;Ismail Guvenc,"As new use cases are emerging for unmanned aerial systems (UAS), advanced wireless communications technologies and systems need to be implemented and widely tested. This requires a flexible platform for development, deployment, testing and demonstration of wireless systems with ground and aerial nodes, enabling effective 3D mobile communications and networking. In this paper, we provide a comparative overview of software-defined radios (SDRs), with a specific focus on SDR hardware and software that can be used for aerial wireless experimentation and research. We discuss SDR hardware requirements, features of available SDR hardware that can be suitable for small UAS, and power measurements carried out with a subset of these SDR hardware. We also present SDR software requirements, available open-source SDR software, and calibration/benchmarking of SDR software. As a case study, we present AERPAW: Aerial Experimentation and Research Platform for Advanced Wireless, and discuss various different experiments that can be supported in that platform using SDRs, for verification/testing of future wireless innovations, protocols, and technologies. △ Less","20 May, 2020",https://arxiv.org/pdf/2004.01987
GraphChallenge.org Sparse Deep Neural Network Performance,Jeremy Kepner;Simon Alford;Vijay Gadepally;Michael Jones;Lauren Milechin;Albert Reuther;Ryan Robinett;Sid Samsi,"The MIT/IEEE/Amazon GraphChallenge.org encourages community approaches to developing new solutions for analyzing graphs and sparse data. Sparse AI analytics present unique scalability difficulties. The Sparse Deep Neural Network (DNN) Challenge draws upon prior challenges from machine learning, high performance computing, and visual analytics to create a challenge that is reflective of emerging sparse AI systems. The sparse DNN challenge is based on a mathematically well-defined DNN inference computation and can be implemented in any programming environment. In 2019 several sparse DNN challenge submissions were received from a wide range of authors and organizations. This paper presents a performance analysis of the best performers of these submissions. These submissions show that their state-of-the-art sparse DNN execution time, T_{\rm DNN}, is a strong function of the number of DNN operations performed, N_{\rm op}. The sparse DNN challenge provides a clear picture of current sparse DNN systems and underscores the need for new innovations to achieve high performance on very large sparse DNNs. △ Less","5 April, 2020",https://arxiv.org/pdf/2004.01181
Multi-Modal Video Forensic Platform for Investigating Post-Terrorist Attack Scenarios,Alexander Schindler;Andrew Lindley;Anahid Jalali;Martin Boyer;Sergiu Gordea;Ross King,"The forensic investigation of a terrorist attack poses a significant challenge to the investigative authorities, as often several thousand hours of video footage must be viewed. Large scale Video Analytic Platforms (VAP) assist law enforcement agencies (LEA) in identifying suspects and securing evidence. Current platforms focus primarily on the integration of different computer vision methods and thus are restricted to a single modality. We present a video analytic platform that integrates visual and audio analytic modules and fuses information from surveillance cameras and video uploads from eyewitnesses. Videos are analyzed according their acoustic and visual content. Specifically, Audio Event Detection is applied to index the content according to attack-specific acoustic concepts. Audio similarity search is utilized to identify similar video sequences recorded from different perspectives. Visual object detection and tracking are used to index the content according to relevant concepts. Innovative user-interface concepts are introduced to harness the full potential of the heterogeneous results of the analytical modules, allowing investigators to more quickly follow-up on leads and eyewitness reports. △ Less","2 April, 2020",https://arxiv.org/pdf/2004.01023
SSHFD: Single Shot Human Fall Detection with Occluded Joints Resilience,Umar Asif;Stefan Von Cavallar;Jianbin Tang;Stefan Harrer,"Falling can have fatal consequences for elderly people especially if the fallen person is unable to call for help due to loss of consciousness or any injury. Automatic fall detection systems can assist through prompt fall alarms and by minimizing the fear of falling when living independently at home. Existing vision-based fall detection systems lack generalization to unseen environments due to challenges such as variations in physical appearances, different camera viewpoints, occlusions, and background clutter. In this paper, we explore ways to overcome the above challenges and present Single Shot Human Fall Detector (SSHFD), a deep learning based framework for automatic fall detection from a single image. This is achieved through two key innovations. First, we present a human pose based fall representation which is invariant to appearance characteristics. Second, we present neural network models for 3d pose estimation and fall recognition which are resilient to missing joints due to occluded body parts. Experiments on public fall datasets show that our framework successfully transfers knowledge of 3d pose estimation and fall recognition learnt purely from synthetic data to unseen real-world data, showcasing its generalization capability for accurate fall detection in real-world scenarios. △ Less","2 April, 2020",https://arxiv.org/pdf/2004.00797
A Blockchain-based Decentralized Federated Learning Framework with Committee Consensus,Yuzheng Li;Chuan Chen;Nan Liu;Huawei Huang;Zibin Zheng;Qiang Yan,"Federated learning has been widely studied and applied to various scenarios. In mobile computing scenarios, federated learning protects users from exposing their private data, while cooperatively training the global model for a variety of real-world applications. However, the security of federated learning is increasingly being questioned, due to the malicious clients or central servers' constant attack to the global model or user privacy data. To address these security issues, we proposed a decentralized federated learning framework based on blockchain, i.e., a Blockchain-based Federated Learning framework with Committee consensus (BFLC). The framework uses blockchain for the global model storage and the local model update exchange. To enable the proposed BFLC, we also devised an innovative committee consensus mechanism, which can effectively reduce the amount of consensus computing and reduce malicious attacks. We then discussed the scalability of BFLC, including theoretical security, storage optimization, and incentives. Finally, we performed experiments using real-world datasets to verify the effectiveness of the BFLC framework. △ Less","1 April, 2020",https://arxiv.org/pdf/2004.00773
Multi-Connectivity in 5G terrestrial-Satellite Networks: the 5G-ALLSTAR Solution,F. Lisi;G. Losquadro;A. Tortorelli;A. Ornatelli;M. Donsante,"The 5G-ALLSTAR project is aimed at integrating Terrestrial and Satellite Networks for satisfying the highly challenging and demanding requirements of the 5G use cases. The integration of the two networks is a key feature to assure the service continuity in challenging communication situations (e.g., emergency cases, marine, railway, etc.) by avoiding service interruptions. The 5G-ALLSTAR project proposes to develop Multi-Connectivity (MC) solutions in order to guarantee network reliability and improve the throughput and latency for each connection between User Equipment (UE) and network. In the 5G-ALLSTAR vision, we divide the gNB in two entities: 1) gNB-CU (Centralized Unit) and 2) gNB-DU (Distributed Unit) The gNB-CU integrates an innovative Traffic Flow Control algorithm able to optimize the network resources by coordinating the controlled gNB-DUs resources, while implementing MC solutions. The MC permits to connect each UE with simultaneous multiple access points (even different radio access technologies). This solution leads to have independent gNB-DU/s that contain the RLC, MAC and PHY layers. The 5G-ALLSTAR MC algorithms offer advanced functionalities to RRC layer (in the gNB-CU) that is, in turn, able to set up the SDAP, the PDCP and the lower layers in gNB-DU. In this regard, the AI-based MC algorithms, implemented in gNB-CU, by considering the network performances in the UE surrounding environment as well as the UE QoS requirements, will dynamically select the most promising access points able to guarantee the fulfilment of the requirements also enabling the optimal traffic splitting to cope with the connection reliability. In this paper, we present also an innovative AI-based framework, included within the Traffic Flow Control, able to address the MC objectives, by implementing a Reinforcement Learning algorithm in charge of solving the network control problem. △ Less","11 March, 2020",https://arxiv.org/pdf/2004.00368
When the Guard failed the Droid: A case study of Android malware,Harel Berger;Chen Hajaj;Amit Dvir,"Android malware is a persistent threat to billions of users around the world. As a countermeasure, Android malware detection systems are occasionally implemented. However, these systems are often vulnerable to \emph{evasion attacks}, in which an adversary manipulates malicious instances so that they are misidentified as benign. In this paper, we launch various innovative evasion attacks against several Android malware detection systems. The vulnerability inherent to all of these systems is that they are part of Androguard~\cite{desnos2011androguard}, a popular open source library used in Android malware detection systems. Some of the detection systems decrease to a 0\% detection rate after the attack. Therefore, the use of open source libraries in malware detection systems calls for caution. In addition, we present a novel evaluation scheme for evasion attack generation that exploits the weak spots of known Android malware detection systems. In so doing, we evaluate the functionality and maliciousness of the manipulated instances created by our evasion attacks. We found variations in both the maliciousness and functionality tests of our manipulated apps. We show that non-functional apps, while considered malicious, do not threaten users and are thus useless from an attacker's point of view. We conclude that evasion attacks must be assessed for both functionality and maliciousness to evaluate their impact, a step which is far from commonplace today. △ Less","31 March, 2020",https://arxiv.org/pdf/2003.14123
Secure Non-Orthogonal Multiple Access: An Interference Engineering Perspective,Lu Lv;Hai Jiang;Zhiguo Ding;Qiang Ye;Naofal Al-Dhahir;Jian Chen,"Non-orthogonal multiple access (NOMA) is an efficient approach that can improve spectrum utilization and support massive connectivity for next-generation wireless networks. However, over a wireless channel, the superimposed NOMA signals are highly susceptible to eavesdropping, potentially leading to severe leakage of confidential information. In this article, we unleash the potential of network interference and exploit it constructively to enhance physical-layer security in NOMA networks. Particularly, three different types of network interference, including artificial noise, specifically-designed jamming signals, and inter-user interference, are well engineered to intentionally reduce information leakage while mitigating the effect on signal reception quality of legitimate users, thereby significantly enhancing the transmission security of NOMA. Furthermore, we propose interference engineering strategies for more advanced full-duplex NOMA, intelligent reflecting surface NOMA, cognitive radio NOMA, and multi-cell NOMA networks, and discuss several open research problems and challenges, which could inspire innovative interference engineering designs for secure NOMA communications. △ Less","23 October, 2020",https://arxiv.org/pdf/2003.13488
Machine Learning Enabled Discovery of Application Dependent Design Principles for Two-dimensional Materials,Victor Venturi;Holden Parks;Zeeshan Ahmad;Venkatasubramanian Viswanathan,"The large-scale search for high-performing candidate 2D materials is limited to calculating a few simple descriptors, usually with first-principles density functional theory calculations. In this work, we alleviate this issue by extending and generalizing crystal graph convolutional neural networks to systems with planar periodicity, and train an ensemble of models to predict thermodynamic, mechanical, and electronic properties. To demonstrate the utility of this approach, we carry out a screening of nearly 45,000 structures for two largely disjoint applications: namely, mechanically robust composites and photovoltaics. An analysis of the uncertainty associated with our methods indicates the ensemble of neural networks is well-calibrated and has errors comparable with those from accurate first-principles density functional theory calculations. The ensemble of models allows us to gauge the confidence of our predictions, and to find the candidates most likely to exhibit effective performance in their applications. Since the datasets used in our screening were combinatorically generated, we are also able to investigate, using an innovative method, structural and compositional design principles that impact the properties of the structures surveyed and which can act as a generative model basis for future material discovery through reverse engineering. Our approach allowed us to recover some well-accepted design principles: for instance, we find that hybrid organic-inorganic perovskites with lead and tin tend to be good candidates for solar cell applications. △ Less","19 March, 2020",https://arxiv.org/pdf/2003.13418
"Zero-Rating and Net Neutrality: Who Wins, Who Loses?",Niloofar Bayat;Richard Ma;Vishal Misra;Dan Rubenstein,"An objective of network neutrality is that the design of regulations for the Internet will ensure that it remains a public, open platform where innovations can thrive. While there is broad agreement that preserving the content quality of service falls under the purview of net neutrality, the role of differential pricing, especially the practice of \emph {zero-rating} remains controversial. Even though some countries (India, Canada) have banned zero-rating, others have either taken no stance or explicitly allowed it (South Africa, Kenya, U.S.). In this paper, we model zero-rating options available between Internet service providers (ISPs) and content providers (CPs) and use these models to better understand the conditions under which offering zero-rated services are preferred, and who specifically gains in utility. We develop a formulation in which providers' incomes vary, from low-income startups to high-income incumbents, and where their decisions to zero-rate are a variation of the traditional prisoner's dilemma game. We find that if zero-rating is permitted, low-income CPs often lose utility, whereas high-income CPs often gain utility. We also study the competitiveness of the CP markets via the \emph{Herfindahl Index}. Our findings suggest that in most cases the introduction of zero-rating \emph{reduces} competitiveness. △ Less","13 February, 2020",https://arxiv.org/pdf/2003.13371
IoT Blockchain Solution for Air Quality Monitoring in SmartCities,Shajulin Benedict;Rumaize P.;Jaspreet Kaur,"IoT cloud enabled societal applications have dramatically increased in the recent past due to the thrust for innovations, notably through startup initiatives, in various sectors such as agriculture, healthcare, industry, and so forth. The existing IoT cloud solutions have led practitioners or researchers to a haphazard clutter of serious security hazards and performance inefficiencies. This paper proposes a blockchain enabled IoT cloud implementation to tackle the existing issues in smart cities. It particularly highlights the implementation of chaincodes for air quality monitoring systems in SmartCities; the proposed architecture named as IoT enabled Blockchain for Air Quality Monitoring System (IB-AQMS) is illustrated using experiments. Experimental results were carried out and the findings were disclosed in the paper. △ Less","28 March, 2020",https://arxiv.org/pdf/2003.12920
Orchestrating NLP Services for the Legal Domain,Julián Moreno-Schneider;Georg Rehm;Elena Montiel-Ponsoda;Víctor Rodriguez-Doncel;Artem Revenko;Sotirios Karampatakis;Maria Khvalchik;Christian Sageder;Jorge Gracia;Filippo Maganza,"Legal technology is currently receiving a lot of attention from various angles. In this contribution we describe the main technical components of a system that is currently under development in the European innovation project Lynx, which includes partners from industry and research. The key contribution of this paper is a workflow manager that enables the flexible orchestration of workflows based on a portfolio of Natural Language Processing and Content Curation services as well as a Multilingual Legal Knowledge Graph that contains semantic information and meaningful references to legal documents. We also describe different use cases with which we experiment and develop prototypical solutions. △ Less","28 March, 2020",https://arxiv.org/pdf/2003.12900
On the Emerging Area of Biocybersecurity and Relevant Considerations,Xavier-Lewis Palmer;Lucas Potter;Saltuk Karahan,"Biocybersecurity is a novel space for the 21st century that meets our innovations in biotechnology and computing head on. Within this space, many considerations are open for and demand consideration as groups endeavor to develop products and policies that adequately ensure asset management and protection. Herein, simplified and brief exploration is given followed by some surface discussion of impacts. These impacts concern the end user, ethical and legal considerations, international proceedings, business, and limitations. It is hoped that this will be helpful in future considerations towards biocybersecurity policy developments and implementations. Notice: This article has been queued for publication in the Proceedings of the 2020 Future of Information and Communication Conference (FICC) △ Less","24 March, 2020",https://arxiv.org/pdf/2003.12132
A lower bound for the ELBO of the Bernoulli Variational Autoencoder,Robert Sicks;Ralf Korn;Stefanie Schwaar,"We consider a variational autoencoder (VAE) for binary data. Our main innovations are an interpretable lower bound for its training objective, a modified initialization and architecture of such a VAE that leads to faster training, and a decision support for finding the appropriate dimension of the latent space via using a PCA. Numerical examples illustrate our theoretical result and the performance of the new architecture. △ Less","26 March, 2020",https://arxiv.org/pdf/2003.11830
Next-Generation Information Technology Systems for Fast Detectors in Electron Microscop,Dieter Weber;Alexander Clausen;Rafal E. Dunin-Borkowski,"The Gatan K2 IS direct electron detector (Gatan Inc., 2018), which was introduced in 2014, marked a watershed moment in the development of cameras for transmission electron microscopy (TEM) (Pan & Czarnik, 2016). Its pixel frequency, i.e. the number of data points (pixels) recorded per second, was two orders of magnitude higher than the fastest cameras available only five years before. Starting from 2009, the data rate of TEM cameras has outpaced the development of network, mass storage and memory bandwidth by almost two orders of magnitude. Consequently, solutions based on personal computers (PCs) that were adequate until then are no longer able to handle the resulting data rates. Instead, tailored high-performance setups are necessary. Similar developments have occurred for advanced X-ray sources such as the European XFEL, requiring special information technology (IT) systems for data handling (Sauter, Hattne, Grosse-Kunstleve, & Echols, 2013) (Fangohr, et al., 2018). Information and detector technology are currently under rapid development and involve disruptive technological innovations. This chapter briefly reviews the technological developments of the past 20 years, presents a snapshot of the current situation at the beginning of 2019 with many practical considerations, and looks forward to future developments. △ Less","25 March, 2020",https://arxiv.org/pdf/2003.11332
"A Review of Methods for Estimating Algorithmic Complexity: Options, Challenges, and New Directions",Hector Zenil,"Some established and also novel techniques in the field of applications of algorithmic (Kolmogorov) complexity currently co-exist for the first time and are here reviewed, ranging from dominant ones such as statistical lossless compression to newer approaches that advance, complement and also pose new challenges and may exhibit their own limitations. Evidence suggesting that these different methods complement each other for different regimes is presented and despite their many challenges, some of these methods can be better motivated by and better grounded in the principles of algorithmic information theory. It will be explained how different approaches to algorithmic complexity can explore the relaxation of different necessary and sufficient conditions in their pursuit of numerical applicability, with some of these approaches entailing greater risks than others in exchange for greater relevance. We conclude with a discussion of possible directions that may or should be taken into consideration to advance the field and encourage methodological innovation, but more importantly, to contribute to scientific discovery. This paper also serves as a rebuttal of claims made in a previously published minireview by another author, and offers an alternative account. △ Less","27 May, 2020",https://arxiv.org/pdf/2003.11044
Dynamic Reconstruction of Deformable Soft-tissue with Stereo Scope in Minimal Invasive Surgery,Jingwei Song;Jun Wang;Liang Zhao;Shoudong Huang;Gamini Dissanayake,"In minimal invasive surgery, it is important to rebuild and visualize the latest deformed shape of soft-tissue surfaces to mitigate tissue damages. This paper proposes an innovative Simultaneous Localization and Mapping (SLAM) algorithm for deformable dense reconstruction of surfaces using a sequence of images from a stereoscope. We introduce a warping field based on the Embedded Deformation (ED) nodes with 3D shapes recovered from consecutive pairs of stereo images. The warping field is estimated by deforming the last updated model to the current live model. Our SLAM system can: (1) Incrementally build a live model by progressively fusing new observations with vivid accurate texture. (2) Estimate the deformed shape of unobserved region with the principle As-Rigid-As-Possible. (3) Show the consecutive shape of models. (4) Estimate the current relative pose between the soft-tissue and the scope. In-vivo experiments with publicly available datasets demonstrate that the 3D models can be incrementally built for different soft-tissues with different deformations from sequences of stereo images obtained by laparoscopes. Results show the potential clinical application of our SLAM system for providing surgeon useful shape and texture information in minimal invasive surgery. △ Less","22 March, 2020",https://arxiv.org/pdf/2003.10867
Prob2Vec: Mathematical Semantic Embedding for Problem Retrieval in Adaptive Tutoring,Du Su;Ali Yekkehkhany;Yi Lu;Wenmiao Lu,"We propose a new application of embedding techniques for problem retrieval in adaptive tutoring. The objective is to retrieve problems whose mathematical concepts are similar. There are two challenges: First, like sentences, problems helpful to tutoring are never exactly the same in terms of the underlying concepts. Instead, good problems mix concepts in innovative ways, while still displaying continuity in their relationships. Second, it is difficult for humans to determine a similarity score that is consistent across a large enough training set. We propose a hierarchical problem embedding algorithm, called Prob2Vec, that consists of abstraction and embedding steps. Prob2Vec achieves 96.88\% accuracy on a problem similarity test, in contrast to 75\% from directly applying state-of-the-art sentence embedding methods. It is interesting that Prob2Vec is able to distinguish very fine-grained differences among problems, an ability humans need time and effort to acquire. In addition, the sub-problem of concept labeling with imbalanced training data set is interesting in its own right. It is a multi-label problem suffering from dimensionality explosion, which we propose ways to ameliorate. We propose the novel negative pre-training algorithm that dramatically reduces false negative and positive ratios for classification, using an imbalanced training data set. △ Less","20 March, 2020",https://arxiv.org/pdf/2003.10838
From Bit To Bedside: A Practical Framework For Artificial Intelligence Product Development In Healthcare,David Higgins;Vince I. Madai,"Artificial Intelligence (AI) in healthcare holds great potential to expand access to high-quality medical care, whilst reducing overall systemic costs. Despite hitting the headlines regularly and many publications of proofs-of-concept, certified products are failing to breakthrough to the clinic. AI in healthcare is a multi-party process with deep knowledge required in multiple individual domains. The lack of understanding of the specific challenges in the domain is, therefore, the major contributor to the failure to deliver on the big promises. Thus, we present a decision perspective framework, for the development of AI-driven biomedical products, from conception to market launch. Our framework highlights the risks, objectives and key results which are typically required to proceed through a three-phase process to the market launch of a validated medical AI product. We focus on issues related to Clinical validation, Regulatory affairs, Data strategy and Algorithmic development. The development process we propose for AI in healthcare software strongly diverges from modern consumer software development processes. We highlight the key time points to guide founders, investors and key stakeholders throughout their relevant part of the process. Our framework should be seen as a template for innovation frameworks, which can be used to coordinate team communications and responsibilities towards a reasonable product development roadmap, thus unlocking the potential of AI in medicine. △ Less","23 March, 2020",https://arxiv.org/pdf/2003.10303
Embedding technique and network analysis of scientific innovations emergence in an arXiv-based concept network,Serhii Brodiuk;Vasyl Palchykov;Yurij Holovatch,"Novelty is an inherent part of innovations and discoveries. Such processes may be considered as an appearance of new ideas or as an emergence of atypical connections between the existing ones. The importance of such connections hints for investigation of innovations through network or graph representation in the space of ideas. In such representation, a graph node corresponds to the relevant concept (idea), whereas an edge between two nodes means that the corresponding concepts have been used in a common context. In this study we address the question about a possibility to identify the edges between existing concepts where the innovations may emerge. To this end, we use a well-documented scientific knowledge landscape of 1.2M arXiv.org manuscripts dated starting from April 2007 and until September 2019. We extract relevant concepts for them using the ScienceWISE.info platform. Combining approaches developed in complex networks science and graph embedding, we discuss the predictability of edges (links) on the scientific knowledge landscape where the innovations may appear. △ Less","23 March, 2020",https://arxiv.org/pdf/2003.10289
A Comprehensive Survey on Moving Networks,Shan Jaffry;Rasheed Hussain;Xiang Gui;Syed Faraz Hasan,"The unprecedented increase in the demand for mobile data, fuelled by new emerging applications such as HD video streaming and heightened online activities has caused massive strain on the existing cellular networks. As a solution, the 5G technology has been introduced to improve network performance through various innovative features such as mmWave spectrum and HetNets. In essence, HetNets include several small cells underlaid within macro-cell to serve densely populated regions. Recently, a mobile layer of HetNet has been under consideration by the researchers and is often referred to as moving networks. Moving networks comprise of mobile cells that are primarily introduced to improve QoS for commuting users inside public transport because the QoS is deteriorated due to vehicular penetration losses. Furthermore, the users inside fast moving public transport also exert excessive load on the core network due to large group handovers. To this end, mobile cells will play a crucial role in reducing overall handover count and will help in alleviating these problems by decoupling in-vehicle users from the core network. To date, remarkable research results have been achieved by the research community in addressing challenges linked to moving networks. However, to the best of our knowledge, a discussion on moving networks in a holistic way is missing in the current literature. To fill the gap, in this paper, we comprehensively survey moving networks. We cover the technological aspects and their applications in the futuristic applications. We also discuss the use-cases and value additions that moving networks may bring to future cellular architecture and identify the challenges associated with them. Based on the identified challenges we discuss the future research directions. △ Less","22 March, 2020",https://arxiv.org/pdf/2003.09979
BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of Channels,Zan Shen;Jiang Qian;Bojin Zhuang;Shaojun Wang;Jing Xiao,"One-Shot methods have evolved into one of the most popular methods in Neural Architecture Search (NAS) due to weight sharing and single training of a supernet. However, existing methods generally suffer from two issues: predetermined number of channels in each layer which is suboptimal; and model averaging effects and poor ranking correlation caused by weight coupling and continuously expanding search space. To explicitly address these issues, in this paper, a Broadening-and-Shrinking One-Shot NAS (BS-NAS) framework is proposed, in which `broadening' refers to broadening the search space with a spring block enabling search for numbers of channels during training of the supernet; while `shrinking' refers to a novel shrinking strategy gradually turning off those underperforming operations. The above innovations broaden the search space for wider representation and then shrink it by gradually removing underperforming operations, followed by an evolutionary algorithm to efficiently search for the optimal architecture. Extensive experiments on ImageNet illustrate the effectiveness of the proposed BS-NAS as well as the state-of-the-art performance. △ Less","22 March, 2020",https://arxiv.org/pdf/2003.09821
Exploring Vibration-Defined Networking,John Pasquesi;Flavio Esposito;Gianluca Davoli;Jenna Gorlewicz,"The network management community has explored and exploited light, copper, and several wireless spectra (including acoustics) as a medium to transfer control or data traffic. Meanwhile, haptic technologies are being explored in end-user (wearable) devices, and Tactile Internet is being used merely as a metaphor. However, with rare exceptions and for smaller scoped projects, to our knowledge, vibration has been largely untouched as networking communication media. In this paper, we share the lessons learned while creating and optimizing a pilot testbed that serves as an inexpensive starting point for the exploration of vibration-defined networking. We demonstrated the feasibility (but not yet the scalability) of vibrations as a tool for a few network management mechanisms, such as resiliency, physical layer security, and as an innovative method for teaching networking concepts to individuals with visual impairments (VI). By demonstrating how vibrations could be programmable, we propose to the community a few open problems that could generate several applications. △ Less","21 March, 2020",https://arxiv.org/pdf/2003.09794
Deep Markov Spatio-Temporal Factorization,Amirreza Farnoosh;Behnaz Rezaei;Eli Zachary Sennesh;Zulqarnain Khan;Jennifer Dy;Ajay Satpute;J Benjamin Hutchinson;Jan-Willem van de Meent;Sarah Ostadabbas,"We introduce deep Markov spatio-temporal factorization (DMSTF), a generative model for dynamical analysis of spatio-temporal data. Like other factor analysis methods, DMSTF approximates high dimensional data by a product between time dependent weights and spatially dependent factors. These weights and factors are in turn represented in terms of lower dimensional latents inferred using stochastic variational inference. The innovation in DMSTF is that we parameterize weights in terms of a deep Markovian prior extendable with a discrete latent, which is able to characterize nonlinear multimodal temporal dynamics, and perform multidimensional time series forecasting. DMSTF learns a low dimensional spatial latent to generatively parameterize spatial factors or their functional forms in order to accommodate high spatial dimensionality. We parameterize the corresponding variational distribution using a bidirectional recurrent network in the low-level latent representations. This results in a flexible family of hierarchical deep generative factor analysis models that can be extended to perform time series clustering or perform factor analysis in the presence of a control signal. Our experiments, which include simulated and real-world data, demonstrate that DMSTF outperforms related methodologies in terms of predictive performance for unseen data, reveals meaningful clusters in the data, and performs forecasting in a variety of domains with potentially nonlinear temporal transitions. △ Less","18 August, 2020",https://arxiv.org/pdf/2003.09779
GraphChallenge.org Triangle Counting Performance,Siddharth Samsi;Jeremy Kepner;Vijay Gadepally;Michael Hurley;Michael Jones;Edward Kao;Sanjeev Mohindra;Albert Reuther;Steven Smith;William Song;Diane Staheli;Paul Monticciolo,"The rise of graph analytic systems has created a need for new ways to measure and compare the capabilities of graph processing systems. The MIT/Amazon/IEEE Graph Challenge has been developed to provide a well-defined community venue for stimulating research and highlighting innovations in graph analysis software, hardware, algorithms, and systems. GraphChallenge.org provides a wide range of pre-parsed graph data sets, graph generators, mathematically defined graph algorithms, example serial implementations in a variety of languages, and specific metrics for measuring performance. The triangle counting component of GraphChallenge.org tests the performance of graph processing systems to count all the triangles in a graph and exercises key graph operations found in many graph algorithms. In 2017, 2018, and 2019 many triangle counting submissions were received from a wide range of authors and organizations. This paper presents a performance analysis of the best performers of these submissions. These submissions show that their state-of-the-art triangle counting execution time, T_{\rm tri}, is a strong function of the number of edges in the graph, N_e, which improved significantly from 2017 (T_{\rm tri} \approx (N_e/10^8)^{4/3}) to 2018 (T_{\rm tri} \approx N_e/10^9) and remained comparable from 2018 to 2019. Graph Challenge provides a clear picture of current graph analysis systems and underscores the need for new innovations to achieve high performance on very large graphs. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.09269
MCC: a Tool for Unfolding Colored Petri Nets in PNML Format,Silvano Dal Zilio,"MCC is a tool designed for a very specific task: to transform the models of High-Level Petri nets, given in the PNML syntax, into equivalent Place/Transition nets. The name of the tool derives from the annual Model-Checking Contest, a competition of model-checking tools that provides a large and diverse collection of PNML models. This choice in naming serves to underline the main focus of the tool, which is to provide an open and efficient solution that lowers the access cost for developers wanting to engage in this competition. We describe the architecture and functionalities of our tool and show how it compares with other existing solutions. Despite the fact that the problem we target is abundantly covered in the literature, we show that it is still possible to innovate. To substantiate this assertion, we put a particular emphasis on two distinctive features of MCC that have proved useful when dealing with some of the most challenging colored models in the contest, namely the use of a restricted notion of higher-order invariant, and the support of a Petri net scripting language. △ Less","20 March, 2020",https://arxiv.org/pdf/2003.09134
Functional Data Analysis and Visualisation of Three-dimensional Surface Shape,Stanislav Katina;Liberty Vittert;Adrian W. Bowman,"The advent of high resolution imaging has made data on surface shape widespread. Methods for the analysis of shape based on landmarks are well established but high resolution data require a functional approach. The starting point is a systematic and consistent description of each surface shape. Three innovative forms of analysis are then introduced. The first uses surface integration to address issues of registration, principal component analysis and the measurement of asymmetry, all in functional form. Computational issues are handled through discrete approximations to integrals, based in this case on appropriate surface area weighted sums. The second innovation is to focus on sub-spaces where interesting behaviour such as group differences are exhibited, rather than on individual principal components. The third innovation concerns the comparison of individual shapes with a relevant control set, where the concept of a normal range is extended to the highly multivariate setting of surface shape. This has particularly strong applications to medical contexts where the assessment of individual patients is very important. All of these ideas are developed and illustrated in the important context of human facial shape, with a strong emphasis on the effective visual communication of effects of interest. △ Less","14 March, 2020",https://arxiv.org/pdf/2003.08817
Multi-task Collaborative Network for Joint Referring Expression Comprehension and Segmentation,Gen Luo;Yiyi Zhou;Xiaoshuai Sun;Liujuan Cao;Chenglin Wu;Cheng Deng;Rongrong Ji,"Referring expression comprehension (REC) and segmentation (RES) are two highly-related tasks, which both aim at identifying the referent according to a natural language expression. In this paper, we propose a novel Multi-task Collaborative Network (MCN) to achieve a joint learning of REC and RES for the first time. In MCN, RES can help REC to achieve better language-vision alignment, while REC can help RES to better locate the referent. In addition, we address a key challenge in this multi-task setup, i.e., the prediction conflict, with two innovative designs namely, Consistency Energy Maximization (CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM enables REC and RES to focus on similar visual regions by maximizing the consistency energy between two tasks. ASNLS supresses the response of unrelated regions in RES based on the prediction of REC. To validate our model, we conduct extensive experiments on three benchmark datasets of REC and RES, i.e., RefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant performance gains of MCN over all existing methods, i.e., up to +7.13% for REC and +11.50% for RES over SOTA, which well confirm the validity of our model for joint REC and RES learning. △ Less","19 March, 2020",https://arxiv.org/pdf/2003.08813
A Convolutional Neural Network-based Patent Image Retrieval Method for Design Ideation,Shuo Jiang;Jianxi Luo;Guillermo Ruiz Pava;Jie Hu;Christopher L. Magee,"The patent database is often used in searches of inspirational stimuli for innovative design opportunities because of its large size, extensive variety and rich design information in patent documents. However, most patent mining research only focuses on textual information and ignores visual information. Herein, we propose a convolutional neural network (CNN)-based patent image retrieval method. The core of this approach is a novel neural network architecture named Dual-VGG that is aimed to accomplish two tasks: visual material type prediction and international patent classification (IPC) class label prediction. In turn, the trained neural network provides the deep features in the image embedding vectors that can be utilized for patent image retrieval and visual mapping. The accuracy of both training tasks and patent image embedding space are evaluated to show the performance of our model. This approach is also illustrated in a case study of robot arm design retrieval. Compared to traditional keyword-based searching and Google image searching, the proposed method discovers more useful visual information for engineering design. △ Less","19 May, 2020",https://arxiv.org/pdf/2003.08741
Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions,Rui Wang;Joel Lehman;Aditya Rawal;Jiale Zhi;Yulun Li;Jeff Clune;Kenneth O. Stanley,"Creating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means. △ Less","13 April, 2020",https://arxiv.org/pdf/2003.08536
Convergence of Artificial Intelligence and High Performance Computing on NSF-supported Cyberinfrastructure,E. A. Huerta;Asad Khan;Edward Davis;Colleen Bushell;William D. Gropp;Daniel S. Katz;Volodymyr Kindratenko;Seid Koric;William T. C. Kramer;Brendan McGinty;Kenton McHenry;Aaron Saxton,"Significant investments to upgrade and construct large-scale scientific facilities demand commensurate investments in R&D to design algorithms and computing approaches to enable scientific and engineering breakthroughs in the big data era. Innovative Artificial Intelligence (AI) applications have powered transformational solutions for big data challenges in industry and technology that now drive a multi-billion dollar industry, and which play an ever increasing role shaping human social patterns. As AI continues to evolve into a computing paradigm endowed with statistical and mathematical rigor, it has become apparent that single-GPU solutions for training, validation, and testing are no longer sufficient for computational grand challenges brought about by scientific facilities that produce data at a rate and volume that outstrip the computing capabilities of available cyberinfrastructure platforms. This realization has been driving the confluence of AI and high performance computing (HPC) to reduce time-to-insight, and to enable a systematic study of domain-inspired AI architectures and optimization schemes to enable data-driven discovery. In this article we present a summary of recent developments in this field, and describe specific advances that authors in this article are spearheading to accelerate and streamline the use of HPC platforms to design and apply accelerated AI algorithms in academia and industry. △ Less","19 October, 2020",https://arxiv.org/pdf/2003.08394
Supporting Interoperability Between Open-Source Search Engines with the Common Index File Format,Jimmy Lin;Joel Mackenzie;Chris Kamphuis;Craig Macdonald;Antonio Mallia;Michał Siedlaczek;Andrew Trotman;Arjen de Vries,"There exists a natural tension between encouraging a diverse ecosystem of open-source search engines and supporting fair, replicable comparisons across those systems. To balance these two goals, we examine two approaches to providing interoperability between the inverted indexes of several systems. The first takes advantage of internal abstractions around index structures and building wrappers that allow one system to directly read the indexes of another. The second involves sharing indexes across systems via a data exchange specification that we have developed, called the Common Index File Format (CIFF). We demonstrate the first approach with the Java systems Anserini and Terrier, and the second approach with Anserini, JASSv2, OldDog, PISA, and Terrier. Together, these systems provide a wide range of implementations and features, with different research goals. Overall, we recommend CIFF as a low-effort approach to support independent innovation while enabling the types of fair evaluations that are critical for driving the field forward. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.08276
An Artificial Intelligence-Based System to Assess Nutrient Intake for Hospitalised Patients,Ya Lu;Thomai Stathopoulou;Maria F. Vasiloglou;Stergios Christodoulidis;Zeno Stanga;Stavroula Mougiakakou,"Regular monitoring of nutrient intake in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition. Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve data accuracy and reduce both the burden on participants and health costs. In this paper, we propose a novel system based on artificial intelligence (AI) to accurately estimate nutrient intake, by simply processing RGB Depth (RGB-D) image pairs captured before and after meal consumption. The system includes a novel multi-task contextual network for food segmentation, a few-shot learning-based classifier built by limited training samples for food recognition, and an algorithm for 3D surface construction. This allows sequential food segmentation, recognition, and estimation of the consumed food volume, permitting fully automatic estimation of the nutrient intake for each meal. For the development and evaluation of the system, a dedicated new database containing images and nutrient recipes of 322 meals is assembled, coupled to data annotation using innovative strategies. Experimental results demonstrate that the estimated nutrient intake is highly correlated (> 0.91) to the ground truth and shows very small mean relative errors (< 20%), outperforming existing techniques proposed for nutrient intake assessment. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.08273
Statistically Guided Divide-and-Conquer for Sparse Factorization of Large Matrix,Kun Chen;Ruipeng Dong;Wanwan Xu;Zemin Zheng,"The sparse factorization of a large matrix is fundamental in modern statistical learning. In particular, the sparse singular value decomposition and its variants have been utilized in multivariate regression, factor analysis, biclustering, vector time series modeling, among others. The appeal of this factorization is owing to its power in discovering a highly-interpretable latent association network, either between samples and variables or between responses and predictors. However, many existing methods are either ad hoc without a general performance guarantee, or are computationally intensive, rendering them unsuitable for large-scale studies. We formulate the statistical problem as a sparse factor regression and tackle it with a divide-and-conquer approach. In the first stage of division, we consider both sequential and parallel approaches for simplifying the task into a set of co-sparse unit-rank estimation (CURE) problems, and establish the statistical underpinnings of these commonly-adopted and yet poorly understood deflation methods. In the second stage of division, we innovate a contended stagewise learning technique, consisting of a sequence of simple incremental updates, to efficiently trace out the whole solution paths of CURE. Our algorithm has a much lower computational complexity than alternating convex search, and the choice of the step size enables a flexible and principled tradeoff between statistical accuracy and computational efficiency. Our work is among the first to enable stagewise learning for non-convex problems, and the idea can be applicable in many multi-convex problems. Extensive simulation studies and an application in genetics demonstrate the effectiveness and scalability of our approach. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.07898
NDE 4.0 From Design Thinking to Strategy,Johannes Vrana;Ripudaman Singh,"Cyber technologies are offering new horizons for quality control in manufacturing and safety assurance in-service of physical assets. The line between non-destructive evaluation (NDE) and Industry 4.0 is getting blurred since both are sensory data-driven domains. This multidisciplinary approach has led to the emergence of a new capability: NDE 4.0. The NDT community is coming together once again to define the purpose, chart the process, and address the adoption of emerging technologies. In this paper, the authors have taken a design thinking approach to spotlight proper objectives for research on this subject. It begins with qualitative research on twenty different perceptions of stakeholders and misconceptions around the current state of NDE. The interpretation is used to define ten value propositions or use cases under ""NDE for Industry 4.0"" and ""Industry 4.0 for NDE"" leading up to the clarity of purpose for NDE 4.0, enhanced safety and economic value for stakeholders. To pursue this worthy cause, the paper delves into some of the top adoption challenges, and proposes a journey of managed innovation, conscious skills development, and a new form of leadership required to succeed in the cyber-physical world. △ Less","17 November, 2020",https://arxiv.org/pdf/2003.07773
The Data Science Fire Next Time: Innovative strategies for mentoring in data science,Latifa Jackson;Heriberto Acosta Maestre,"As data mining research and applications continue to expand in to a variety of fields such as medicine, finance, security, etc., the need for talented and diverse individuals is clearly felt. This is particularly the case as Big Data initiatives have taken off in the federal, private and academic sectors, providing a wealth of opportunities, nationally and internationally. The Broadening Participation in Data Mining (BPDM) workshop was created more than 7 years ago with the goal of fostering mentorship, guidance, and connections for minority and underrepresented groups in the data science and machine learning community, while also enriching technical aptitude and exposure for a group of talented students. To date it has impacted the lives of more than 330 underrepresented trainees in data science. We provide a venue to connect talented students with innovative researchers in industry, academia, professional societies, and government. Our mission is to facilitate meaningful, lasting relationships between BPDM participants to ultimately increase diversity in data mining. This most recent workshop took place at Howard University in Washington, DC in February 2019. Here we report on the mentoring strategies that we undertook at the 2019 BPDM and how those were received. △ Less","29 February, 2020",https://arxiv.org/pdf/2003.07681
"Towards High Performance, Portability, and Productivity: Lightweight Augmented Neural Networks for Performance Prediction",Ajitesh Srivastava;Naifeng Zhang;Rajgopal Kannan;Viktor K. Prasanna,"Writing high-performance code requires significant expertise in the programming language, compiler optimizations, and hardware knowledge. This often leads to poor productivity and portability and is inconvenient for a non-programmer domain-specialist such as a Physicist. More desirable is a high-level language where the domain-specialist simply specifies the workload in terms of high-level operations (e.g., matrix-multiply(A, B)), and the compiler identifies the best implementation fully utilizing the heterogeneous platform. For creating a compiler that supports productivity, portability, and performance simultaneously, it is crucial to predict the performance of various available implementations (variants) of the dominant operations (kernels) contained in the workload on various hardware to decide (a) which variant should be chosen for each kernel in the workload, and (b) on which hardware resource the variant should run. To enable the performance prediction, we propose lightweight augmented neural networks for arbitrary combinations of kernel-variant-hardware. A key innovation is utilizing the mathematical complexity of the kernels as a feature to achieve higher accuracy. These models are compact to reduce training time and fast inference during compile-time and run-time. Using models with less than 75 parameters, and only 250 training data instances, we are able to obtain a low MAPE of 3%, significantly outperforming traditional feed-forward neural networks on 48 kernel-variant-hardware combinations. We further demonstrate that our variant-selection approach can be used in Halide implementations to obtain up to 1.7x speedup over Halide's auto-scheduler. △ Less","30 August, 2020",https://arxiv.org/pdf/2003.07497
Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection,Shi-Xue Zhang;Xiaobin Zhu;Jie-Bo Hou;Chang Liu;Chun Yang;Hongfa Wang;Xu-Cheng Yin,"Arbitrary shape text detection is a challenging task due to the high variety and complexity of scenes texts. In this paper, we propose a novel unified relational reasoning graph network for arbitrary shape text detection. In our method, an innovative local graph bridges a text proposal model via Convolutional Neural Network (CNN) and a deep relational reasoning network via Graph Convolutional Network (GCN), making our network end-to-end trainable. To be concrete, every text instance will be divided into a series of small rectangular components, and the geometry attributes (e.g., height, width, and orientation) of the small components will be estimated by our text proposal model. Given the geometry attributes, the local graph construction model can roughly establish linkages between different text components. For further reasoning and deducing the likelihood of linkages between the component and its neighbors, we adopt a graph-based network to perform deep relational reasoning on local graphs. Experiments on public available datasets demonstrate the state-of-the-art performance of our method. △ Less","30 August, 2020",https://arxiv.org/pdf/2003.07493
Understanding and Optimizing Persistent Memory Allocation,Wentao Cai;Haosen Wen;H. Alan Beadle;Chris Kjellqvist;Mohammad Hedayati;Michael L. Scott,"The proliferation of fast, dense, byte-addressable nonvolatile memory suggests that data might be kept in pointer-rich ""in-memory"" format across program runs and even process and system crashes. For full generality, such data requires dynamic memory allocation, and while the allocator could in principle ""rolled into"" each data structure, it is desirable to make it a separate abstraction. Toward this end, we introduce recoverability, a correctness criterion for persistent allocators, together with a nonblocking allocator, Ralloc, that satisfies this criterion. Ralloc is based on the LRMalloc of Leite and Rocha, with three key innovations. First, we persist just enough information during normal operation to permit correct reconstruction of the heap after a full-system crash. Our reconstruction mechanism performs garbage collection (GC) to identify and remedy any failure-induced memory leaks. Second, we introduce the notion of filter functions, which identify the locations of pointers within persistent blocks to mitigate the limitations of conservative GC. Third, to allow persistent regions to be mapped at an arbitrary address, we employ position-independent (offset-based) pointers for both data and metadata. Experiments show Ralloc to be performance-competitive with both Makalu, the state-of-the-art lock-based persistent allocator, and such transient allocators as LRMalloc and JEMalloc. In particular, reliance on GC and offline metadata reconstruction allows Ralloc to pay almost nothing for persistence during normal operation. △ Less","14 March, 2020",https://arxiv.org/pdf/2003.06718
High Quality Software for Planetary Science from Space,Francesco Lazzarotto;Gabriele Cremonese;Alice Lucchetti;Cristina Re;Emanuele Simioni;Maurizio Pajola;Pamela Cambianica;Giovanni Munaretto,"Planetary science space missions need high quality software ed efficient algorithms in order to extract innovative scientific results from flight data. Reliable and efficient software technologies are increasingly vital to improve and prolong the exploiting of the results of a mission, to allow the application of established algorithms and technologies also to future space missions and for the scientific analysis of archived data. Here after will be given an in-depth analysis study accompanied by implementation examples on ESA and ASI missions and some remarkable results fruit of decades of important experience reached by space agencies and research institutes in the field. Space applications software quality analysis is not different from other application contexts, among the hi-tech and hi-reliability fields. We describe here a Software Quality study in general, then we will focus on the quality of space mission software (s/w) with details on some notable cases. △ Less","12 April, 2020",https://arxiv.org/pdf/2003.06248
Needmining: Identifying micro blog data containing customer needs,Niklas Kühl;Jan Scheurenbrand;Gerhard Satzger,"The design of new products and services starts with the identification of needs of potential customers or users. Many existing methods like observations, surveys, and experiments draw upon specific efforts to elicit unsatisfied needs from individuals. At the same time, a huge amount of user-generated content in micro blogs is freely accessible at no cost. While this information is already analyzed to monitor sentiments towards existing offerings, it has not yet been tapped for the elicitation of needs. In this paper, we lay an important foundation for this endeavor: we propose a Machine Learning approach to identify those posts that do express needs. Our evaluation of tweets in the e-mobility domain demonstrates that the small share of relevant tweets can be identified with remarkable precision or recall results. Applied to huge data sets, the developed method should enable scalable need elicitation support for innovation managers - across thousands of users, and thus augment the service design tool set available to him. △ Less","12 March, 2020",https://arxiv.org/pdf/2003.05917
Cascade EF-GAN: Progressive Facial Expression Editing with Local Focuses,Rongliang Wu;Gongjie Zhang;Shijian Lu;Tao Chen,"Recent advances in Generative Adversarial Nets (GANs) have shown remarkable improvements for facial expression editing. However, current methods are still prone to generate artifacts and blurs around expression-intensive regions, and often introduce undesired overlapping artifacts while handling large-gap expression transformations such as transformation from furious to laughing. To address these limitations, we propose Cascade Expression Focal GAN (Cascade EF-GAN), a novel network that performs progressive facial expression editing with local expression focuses. The introduction of the local focus enables the Cascade EF-GAN to better preserve identity-related features and details around eyes, noses and mouths, which further helps reduce artifacts and blurs within the generated facial images. In addition, an innovative cascade transformation strategy is designed by dividing a large facial expression transformation into multiple small ones in cascade, which helps suppress overlapping artifacts and produce more realistic editing while dealing with large-gap expression transformations. Extensive experiments over two publicly available facial expression datasets show that our proposed Cascade EF-GAN achieves superior performance for facial expression editing. △ Less","25 March, 2020",https://arxiv.org/pdf/2003.05905
ZSTAD: Zero-Shot Temporal Activity Detection,Lingling Zhang;Xiaojun Chang;Jun Liu;Minnan Luo;Sen Wang;Zongyuan Ge;Alexander Hauptmann,"An integral part of video analysis and surveillance is temporal activity detection, which means to simultaneously recognize and localize activities in long untrimmed videos. Currently, the most effective methods of temporal activity detection are based on deep learning, and they typically perform very well with large scale annotated videos for training. However, these methods are limited in real applications due to the unavailable videos about certain activity classes and the time-consuming data annotation. To solve this challenging problem, we propose a novel task setting called zero-shot temporal activity detection (ZSTAD), where activities that have never been seen in training can still be detected. We design an end-to-end deep network based on R-C3D as the architecture for this solution. The proposed network is optimized with an innovative loss function that considers the embeddings of activity labels and their super-classes while learning the common semantics of seen and unseen activities. Experiments on both the THUMOS14 and the Charades datasets show promising performance in terms of detecting unseen activities. △ Less","11 March, 2020",https://arxiv.org/pdf/2003.05583
Machine Learning for Intelligent Optical Networks: A Comprehensive Survey,Rentao Gu;Zeyuan Yang;Yuefeng Ji,"With the rapid development of Internet and communication systems, both in services and technologies, communication networks have been suffering increasing complexity. It is imperative to improve intelligence in communication network, and several aspects have been incorporating with Artificial Intelligence (AI) and Machine Learning (ML). Optical network, which plays an important role both in core and access network in communication networks, also faces great challenges of system complexity and the requirement of manual operations. To overcome the current limitations and address the issues of future optical networks, it is essential to deploy more intelligence capability to enable autonomous and exible network operations. ML techniques are proved to have superiority on solving complex problems; and thus recently, ML techniques have been used for many optical network applications. In this paper, a detailed survey of existing applications of ML for intelligent optical networks is presented. The applications of ML are classified in terms of their use cases, which are categorized into optical network control and resource management, and optical networks monitoring and survivability. The use cases are analyzed and compared according to the used ML techniques. Besides, a tutorial for ML applications is provided from the aspects of the introduction of common ML algorithms, paradigms of ML, and motivations of applying ML. Lastly, challenges and possible solutions of ML application in optical networks are also discussed, which intends to inspire future innovations in leveraging ML to build intelligent optical networks. △ Less","11 March, 2020",https://arxiv.org/pdf/2003.05290
Label-Driven Reconstruction for Domain Adaptation in Semantic Segmentation,Jinyu Yang;Weizhi An;Sheng Wang;Xinliang Zhu;Chaochao Yan;Junzhou Huang,"Unsupervised domain adaptation enables to alleviate the need for pixel-wise annotation in the semantic segmentation. One of the most common strategies is to translate images from the source domain to the target domain and then align their marginal distributions in the feature space using adversarial learning. However, source-to-target translation enlarges the bias in translated images and introduces extra computations, owing to the dominant data size of the source domain. Furthermore, consistency of the joint distribution in source and target domains cannot be guaranteed through global feature alignment. Here, we present an innovative framework, designed to mitigate the image translation bias and align cross-domain features with the same category. This is achieved by 1) performing the target-to-source translation and 2) reconstructing both source and target images from their predicted labels. Extensive experiments on adapting from synthetic to real urban scene understanding demonstrate that our framework competes favorably against existing state-of-the-art methods. △ Less","23 August, 2020",https://arxiv.org/pdf/2003.04614
Neural Operator: Graph Kernel Network for Partial Differential Equations,Zongyi Li;Nikola Kovachki;Kamyar Azizzadenesheli;Burigede Liu;Kaushik Bhattacharya;Andrew Stuart;Anima Anandkumar,"The classical development of neural networks has been primarily for mappings between a finite-dimensional Euclidean space and a set of classes, or between two finite-dimensional Euclidean spaces. The purpose of this work is to generalize neural networks so that they can learn mappings between infinite-dimensional spaces (operators). The key innovation in our work is that a single set of network parameters, within a carefully designed network architecture, may be used to describe mappings between infinite-dimensional spaces and between different finite-dimensional approximations of those spaces. We formulate approximation of the infinite-dimensional mapping by composing nonlinear activation functions and a class of integral operators. The kernel integration is computed by message passing on graph networks. This approach has substantial practical consequences which we will illustrate in the context of mappings between input data to partial differential equations (PDEs) and their solutions. In this context, such learned networks can generalize among different approximation methods for the PDE (such as finite difference or finite element methods) and among approximations corresponding to different underlying levels of resolution and discretization. Experiments confirm that the proposed graph kernel network does have the desired properties and show competitive performance compared to the state of the art solvers. △ Less","6 March, 2020",https://arxiv.org/pdf/2003.03485
NYTWIT: A Dataset of Novel Words in the New York Times,Yuval Pinter;Cassandra L. Jacobs;Max Bittker,"We present the New York Times Word Innovation Types dataset, or NYTWIT, a collection of over 2,500 novel English words published in the New York Times between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). We present baseline results for both uncontextual and contextual prediction of novelty class, showing that there is room for improvement even for state-of-the-art NLP systems. We hope this resource will prove useful for linguists and NLP practitioners by providing a real-world environment of novel word appearance. △ Less","23 October, 2020",https://arxiv.org/pdf/2003.03444
Performance and energy footprint assessment of FPGAs and GPUs on HPC systems using Astrophysics application,David Goz;Georgios Ieronymakis;Vassilis Papaefstathiou;Nikolaos Dimou;Sara Bertocco;Francesco Simula;Antonio Ragagnin;Luca Tornatore;Igor Coretti;Giuliano Taffoni,"New challenges in Astronomy and Astrophysics (AA) are urging the need for a large number of exceptionally computationally intensive simulations. ""Exascale"" (and beyond) computational facilities are mandatory to address the size of theoretical problems and data coming from the new generation of observational facilities in AA. Currently, the High Performance Computing (HPC) sector is undergoing a profound phase of innovation, in which the primary challenge to the achievement of the ""Exascale"" is the power-consumption. The goal of this work is to give some insights about performance and energy footprint of contemporary architectures for a real astrophysical application in an HPC context. We use a state-of-the-art N-body application that we re-engineered and optimized to exploit the heterogeneous underlying hardware fully. We quantitatively evaluate the impact of computation on energy consumption when running on four different platforms. Two of them represent the current HPC systems (Intel-based and equipped with NVIDIA GPUs), one is a micro-cluster based on ARM-MPSoC, and one is a ""prototype towards Exascale"" equipped with ARM-MPSoCs tightly coupled with FPGAs. We investigate the behavior of the different devices where the high-end GPUs excel in terms of time-to-solution while MPSoC-FPGA systems outperform GPUs in power consumption. Our experience reveals that considering FPGAs for computationally intensive application seems very promising, as their performance is improving to meet the requirements of scientific applications. This work can be a reference for future platforms development for astrophysics applications where computationally intensive calculations are required. △ Less","10 April, 2020",https://arxiv.org/pdf/2003.03283
Statistical Context-Dependent Units Boundary Correction for Corpus-based Unit-Selection Text-to-Speech,Claudio Zito;Fabio Tesser;Mauro Nicolao;Piero Cosi,"In this study, we present an innovative technique for speaker adaptation in order to improve the accuracy of segmentation with application to unit-selection Text-To-Speech (TTS) systems. Unlike conventional techniques for speaker adaptation, which attempt to improve the accuracy of the segmentation using acoustic models that are more robust in the face of the speaker's characteristics, we aim to use only context dependent characteristics extrapolated with linguistic analysis techniques. In simple terms, we use the intuitive idea that context dependent information is tightly correlated with the related acoustic waveform. We propose a statistical model, which predicts correcting values to reduce the systematic error produced by a state-of-the-art Hidden Markov Model (HMM) based speech segmentation. Our approach consists of two phases: (1) identifying context-dependent phonetic unit classes (for instance, the class which identifies vowels as being the nucleus of monosyllabic words); and (2) building a regression model that associates the mean error value made by the ASR during the segmentation of a single speaker corpus to each class. The success of the approach is evaluated by comparing the corrected boundaries of units and the state-of-the-art HHM segmentation against a reference alignment, which is supposed to be the optimal solution. In conclusion, our work supplies a first analysis of a model sensitive to speaker-dependent characteristics, robust to defective and noisy information, and a very simple implementation which could be utilized as an alternative to either more expensive speaker-adaptation systems or of numerous manual correction sessions. △ Less","29 April, 2020",https://arxiv.org/pdf/2003.02837
InfDetect: a Large Scale Graph-based Fraud Detection System for E-Commerce Insurance,Cen Chen;Chen Liang;Jianbin Lin;Li Wang;Ziqi Liu;Xinxing Yang;Xiukun Wang;Jun Zhou;Yang Shuang;Yuan Qi,"The insurance industry has been creating innovative products around the emerging online shopping activities. Such e-commerce insurance is designed to protect buyers from potential risks such as impulse purchases and counterfeits. Fraudulent claims towards online insurance typically involve multiple parties such as buyers, sellers, and express companies, and they could lead to heavy financial losses. In order to uncover the relations behind organized fraudsters and detect fraudulent claims, we developed a large-scale insurance fraud detection system, i.e., InfDetect, which provides interfaces for commonly used graphs, standard data processing procedures, and a uniform graph learning platform. InfDetect is able to process big graphs containing up to 100 millions of nodes and billions of edges. In this paper, we investigate different graphs to facilitate fraudster mining, such as a device-sharing graph, a transaction graph, a friendship graph, and a buyer-seller graph. These graphs are fed to a uniform graph learning platform containing supervised and unsupervised graph learning algorithms. Cases on widely applied e-commerce insurance are described to demonstrate the usage and capability of our system. InfDetect has successfully detected thousands of fraudulent claims and saved over tens of thousands of dollars daily. △ Less","12 March, 2020",https://arxiv.org/pdf/2003.02833
A Deep Learning Method for Complex Human Activity Recognition Using Virtual Wearable Sensors,Fanyi Xiao;Ling Pei;Lei Chu;Danping Zou;Wenxian Yu;Yifan Zhu;Tao Li,"Sensor-based human activity recognition (HAR) is now a research hotspot in multiple application areas. With the rise of smart wearable devices equipped with inertial measurement units (IMUs), researchers begin to utilize IMU data for HAR. By employing machine learning algorithms, early IMU-based research for HAR can achieve accurate classification results on traditional classical HAR datasets, containing only simple and repetitive daily activities. However, these datasets rarely display a rich diversity of information in real-scene. In this paper, we propose a novel method based on deep learning for complex HAR in the real-scene. Specially, in the off-line training stage, the AMASS dataset, containing abundant human poses and virtual IMU data, is innovatively adopted for enhancing the variety and diversity. Moreover, a deep convolutional neural network with an unsupervised penalty is proposed to automatically extract the features of AMASS and improve the robustness. In the on-line testing stage, by leveraging advantages of the transfer learning, we obtain the final result by fine-tuning the partial neural network (optimizing the parameters in the fully-connected layers) using the real IMU data. The experimental results show that the proposed method can surprisingly converge in a few iterations and achieve an accuracy of 91.15% on a real IMU dataset, demonstrating the efficiency and effectiveness of the proposed method. △ Less","5 March, 2020",https://arxiv.org/pdf/2003.01874
A New MRAM-based Process In-Memory Accelerator for Efficient Neural Network Training with Floating Point Precision,Hongjie Wang;Yang Zhao;Chaojian Li;Yue Wang;Yingyan Lin,"The excellent performance of modern deep neural networks (DNNs) comes at an often prohibitive training cost, limiting the rapid development of DNN innovations and raising various environmental concerns. To reduce the dominant data movement cost of training, process in-memory (PIM) has emerged as a promising solution as it alleviates the need to access DNN weights. However, state-of-the-art PIM DNN training accelerators employ either analog/mixed signal computing which has limited precision or digital computing based on a memory technology that supports limited logic functions and thus requires complicated procedure to realize floating point computation. In this paper, we propose a spin orbit torque magnetic random access memory (SOT-MRAM) based digital PIM accelerator that supports floating point precision. Specifically, this new accelerator features an innovative (1) SOT-MRAM cell, (2) full addition design, and (3) floating point computation. Experiment results show that the proposed SOT-MRAM PIM based DNN training accelerator can achieve 3.3\times, 1.8\times, and 2.5\times improvement in terms of energy, latency, and area, respectively, compared with a state-of-the-art PIM based DNN training accelerator. △ Less","12 May, 2020",https://arxiv.org/pdf/2003.01551
The dominance of big teams in China's scientific output,Linlin Liu;Jianfei Yu;Junming Huang;Feng Xia;Tao Jia,"Modern science is dominated by scientific productions from teams. A recent finding shows that teams with both large and small sizes are essential in research, prompting us to analyze the extent to which a country's scientific work is carried out by big/small teams. Here, using over 26 million publications from Web of Science, we find that China's research output is more dominated by big teams than the rest of the world, which is particularly the case in fields of natural science. Despite the global trend that more papers are done by big teams, China's drop in small team output is much steeper. As teams in China shift from small to large size, the team diversity that is essential for innovative works does not increase as much as that in other countries. Using the national average as the baseline, we find that the National Natural Science Foundation of China (NSFC) supports fewer small team works than the National Science Foundation of U.S. (NSF) does, implying that big teams are more preferred by grant agencies in China. Our finding provides new insights into the concern of originality and innovation in China, which urges a need to balance small and big teams. △ Less","16 June, 2020",https://arxiv.org/pdf/2003.01108
Learning to Resolve Alliance Dilemmas in Many-Player Zero-Sum Games,Edward Hughes;Thomas W. Anthony;Tom Eccles;Joel Z. Leibo;David Balduzzi;Yoram Bachrach,"Zero-sum games have long guided artificial intelligence research, since they possess both a rich strategy space of best-responses and a clear evaluation metric. What's more, competition is a vital mechanism in many real-world multi-agent systems capable of generating intelligent innovations: Darwinian evolution, the market economy and the AlphaZero algorithm, to name a few. In two-player zero-sum games, the challenge is usually viewed as finding Nash equilibrium strategies, safeguarding against exploitation regardless of the opponent. While this captures the intricacies of chess or Go, it avoids the notion of cooperation with co-players, a hallmark of the major transitions leading from unicellular organisms to human civilization. Beyond two players, alliance formation often confers an advantage; however this requires trust, namely the promise of mutual cooperation in the face of incentives to defect. Successful play therefore requires adaptation to co-players rather than the pursuit of non-exploitability. Here we argue that a systematic study of many-player zero-sum games is a crucial element of artificial intelligence research. Using symmetric zero-sum matrix games, we demonstrate formally that alliance formation may be seen as a social dilemma, and empirically that naïve multi-agent reinforcement learning therefore fails to form alliances. We introduce a toy model of economic competition, and show how reinforcement learning may be augmented with a peer-to-peer contract mechanism to discover and enforce alliances. Finally, we generalize our agent model to incorporate temporally-extended contracts, presenting opportunities for further work. △ Less","27 February, 2020",https://arxiv.org/pdf/2003.00799
Distributed Joint Detection and Estimation: A Sequential Approach,Dominik Reinhard;Michael Fauß;Abdelhak M. Zoubir,"We investigate the problem of jointly testing two hypotheses and estimating a random parameter based on data that is observed sequentially by sensors in a distributed network. In particular, we assume the data to be drawn from a Gaussian distribution, whose random mean is to be estimated. Forgoing the need for a fusion center, the processing is performed locally and the sensors interact with their neighbors following the consensus+innovations approach. We design the test at the individual sensors such that the performance measures, namely, error probabilities and mean-squared error, do not exceed pre-defined levels while the average sample number is minimized. After converting the constrained problem to an unconstrained problem and the subsequent reduction to an optimal stopping problem, we solve the latter utilizing dynamic programming. The solution is shown to be characterized by a set of non-linear Bellman equations, parametrized by cost coefficients, which are then determined by linear programming as to fulfill the performance specifications. A numerical example validates the proposed theory. △ Less","3 March, 2020",https://arxiv.org/pdf/2003.00501
Resource Management Techniques for Cloud-Based IoT Environment,Syed Arshad Ali;Manzoor Ansari;Mansaf Alam,"Internet of Things (IoT) is an Internet-based environment of connected devices and applications. IoT creates an environment where physical devices and sensors are flawlessly combined into information nodes to deliver innovative and smart services for human-being to make their life easier and more efficient. The main objective of the IoT devices-network is to generate data, which are converted into useful information by the data analysis process, it also provides useful resources to the end users. IoT resource management is a key challenge to ensure the quality of end user experience. Many IoT smart devices and technologies like sensors, actuators, RFID, UMTS, 3G, and GSM etc. are used to develop IoT networks. Cloud Computing plays an important role in these networks deployment by providing physical resources as virtualized resources consist of memory, computation power, network bandwidth, virtualized system and device drivers in secure and pay as per use basis. One of the major concerns of Cloud-based IoT environment is resource management, which ensures efficient resource utilization, load balancing, reduce SLA violation, and improve the system performance by reducing operational cost and energy consumption. Many researchers have been proposed IoT based resource management techniques. The focus of this paper is to investigate these proposed resource allocation techniques and finds which parameters must be considered for improvement in resource allocation for IoT networks. Further, this paper also uncovered challenges and issues of Cloud-based resource allocation for IoT environment. △ Less","11 February, 2020",https://arxiv.org/pdf/2002.12729
"Target Detection, Tracking and Avoidance System for Low-cost UAVs using AI-Based Approaches",Vinorth Varatharasan;Alice Shuang Shuang Rao;Eric Toutounji;Ju-Hyeon Hong;Hyo-Sang Shin,"An onboard target detection, tracking and avoidance system has been developed in this paper, for low-cost UAV flight controllers using AI-Based approaches. The aim of the proposed system is that an ally UAV can either avoid or track an unexpected enemy UAV with a net to protect itself. In this point of view, a simple and robust target detection, tracking and avoidance system is designed. Two open-source tools were used for the aim: a state-of-the-art object detection technique called SSD and an API for MAVLink compatible systems called MAVSDK. The MAVSDK performs velocity control when a UAV is detected so that the manoeuvre is done simply and efficiently. The proposed system was verified with Software in the loop (SITL) and Hardware in the loop (HITL) simulators. The simplicity of this algorithm makes it innovative, and therefore it should be used in future applications needing robust performances with low-cost hardware such as delivery drone applications. △ Less","27 February, 2020",https://arxiv.org/pdf/2002.12461
Joint 2D-3D Breast Cancer Classification,Gongbo Liang;Xiaoqin Wang;Yu Zhang;Xin Xing;Hunter Blanton;Tawfiq Salem;Nathan Jacobs,"Breast cancer is the malignant tumor that causes the highest number of cancer deaths in females. Digital mammograms (DM or 2D mammogram) and digital breast tomosynthesis (DBT or 3D mammogram) are the two types of mammography imagery that are used in clinical practice for breast cancer detection and diagnosis. Radiologists usually read both imaging modalities in combination; however, existing computer-aided diagnosis tools are designed using only one imaging modality. Inspired by clinical practice, we propose an innovative convolutional neural network (CNN) architecture for breast cancer classification, which uses both 2D and 3D mammograms, simultaneously. Our experiment shows that the proposed method significantly improves the performance of breast cancer classification. By assembling three CNN classifiers, the proposed model achieves 0.97 AUC, which is 34.72% higher than the methods using only one imaging modality. △ Less","27 February, 2020",https://arxiv.org/pdf/2002.12392
Mechanism Design for Public Projects via Neural Networks,Guanhua Wang;Runqi Guo;Yuko Sakurai;Ali Babar;Mingyu Guo,"We study mechanism design for nonexcludable and excludable binary public project problems. We aim to maximize the expected number of consumers and the expected social welfare. For the nonexcludable public project model, we identify a sufficient condition on the prior distribution for the conservative equal costs mechanism to be the optimal strategy-proof and individually rational mechanism. For general distributions, we propose a dynamic program that solves for the optimal mechanism. For the excludable public project model, we identify a similar sufficient condition for the serial cost sharing mechanism to be optimal for 2 and 3 agents. We derive a numerical upper bound. Experiments show that for several common distributions, the serial cost sharing mechanism is close to optimality. The serial cost sharing mechanism is not optimal in general. We design better performing mechanisms via neural networks. Our approach involves several technical innovations that can be applied to mechanism design in general. We interpret the mechanisms as price-oriented rationing-free (PORF) mechanisms, which enables us to move the mechanism's complex (e.g., iterative) decision making off the network, to a separate program. We feed the prior distribution's analytical form into the cost function to provide quality gradients for training. We use supervision to manual mechanisms as a systematic way for initialization. Our approach of ""supervision and then gradient descent"" is effective for improving manual mechanisms' performances. It is also effective for fixing constraint violations for heuristic-based mechanisms that are infeasible. △ Less","26 February, 2020",https://arxiv.org/pdf/2002.11382
Sparse Sinkhorn Attention,Yi Tay;Dara Bahri;Liu Yang;Donald Metzler;Da-Cheng Juan,"We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.11296
DLSpec: A Deep Learning Task Exchange Specification,Abdul Dakkak;Cheng Li;Jinjun Xiong;Wen-Mei Hwu,"Deep Learning (DL) innovations are being introduced at a rapid pace. However, the current lack of standard specification of DL tasks makes sharing, running, reproducing, and comparing these innovations difficult. To address this problem, we propose DLSpec, a model-, dataset-, software-, and hardware-agnostic DL specification that captures the different aspects of DL tasks. DLSpec has been tested by specifying and running hundreds of DL tasks. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.11262
Software Engineering und Software Engineering Forschung im Zeitalter der Digitalisierung,Michael Felderer;Ralf Reussner;Bernhard Rumpe,"Digitization not only affects society, it also requires a redefinition of the location of computer science and computer scientists, as the science journalist Yogeshwar suggests. Since all official aspects of digitalization are based on software, this article is intended to attempt to redefine the role of software engineering and its research. Software-based products, systems or services are influencing all areas of life and are a critical component and central innovation driver of digitization in all areas of life. Scientifically, there are new opportunities and challenges for software engineering as a driving discipline in the development of any technical innovation. However, the chances must not be sacrificed to the competition for bibliometric numbers as an end in themselves. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.10835
A New Minimax Theorem for Randomized Algorithms,Shalev Ben-David;Eric Blais,"The celebrated minimax principle of Yao (1977) says that for any Boolean-valued function f with finite domain, there is a distribution μ over the domain of f such that computing f to error ε against inputs from μ is just as hard as computing f to error ε on worst-case inputs. Notably, however, the distribution μ depends on the target error level ε: the hard distribution which is tight for bounded error might be trivial to solve to small bias, and the hard distribution which is tight for a small bias level might be far from tight for bounded error levels. In this work, we introduce a new type of minimax theorem which can provide a hard distribution μ that works for all bias levels at once. We show that this works for randomized query complexity, randomized communication complexity, some randomized circuit models, quantum query and communication complexities, approximate polynomial degree, and approximate logrank. We also prove an improved version of Impagliazzo's hardcore lemma. Our proofs rely on two innovations over the classical approach of using Von Neumann's minimax theorem or linear programming duality. First, we use Sion's minimax theorem to prove a minimax theorem for ratios of bilinear functions representing the cost and score of algorithms. Second, we introduce a new way to analyze low-bias randomized algorithms by viewing them as ""forecasting algorithms"" evaluated by a proper scoring rule. The expected score of the forecasting version of a randomized algorithm appears to be a more fine-grained way of analyzing the bias of the algorithm. We show that such expected scores have many elegant mathematical properties: for example, they can be amplified linearly instead of quadratically. We anticipate forecasting algorithms will find use in future work in which a fine-grained analysis of small-bias algorithms is required. △ Less","17 September, 2020",https://arxiv.org/pdf/2002.10802
DeepPlume: Very High Resolution Real-Time Air Quality Mapping,Grégoire Jauvion;Thibaut Cassard;Boris Quennehen;David Lissmyr,"This paper presents an engine able to predict jointly the real-time concentration of the main pollutants harming people's health: nitrogen dioxyde (NO2), ozone (O3) and particulate matter (PM2.5 and PM10, which are respectively the particles whose size are below 2.5 um and 10 um). The engine covers a large part of the world and is fed with real-time official stations measures, atmospheric models' forecasts, land cover data, road networks and traffic estimates to produce predictions with a very high resolution in the range of a few dozens of meters. This resolution makes the engine adapted to very innovative applications like street-level air quality mapping or air quality adjusted routing. Plume Labs has deployed a similar prediction engine to build several products aiming at providing air quality data to individuals and businesses. For the sake of clarity and reproducibility, the engine presented here has been built specifically for this paper and differs quite significantly from the one used in Plume Labs' products. A major difference is in the data sources feeding the engine: in particular, this prediction engine does not include mobile sensors measurements. △ Less","14 February, 2020",https://arxiv.org/pdf/2002.10394
Physics-Informed Multi-LSTM Networks for Metamodeling of Nonlinear Structures,Ruiyang Zhang;Yang Liu;Hao Sun,"This paper introduces an innovative physics-informed deep learning framework for metamodeling of nonlinear structural systems with scarce data. The basic concept is to incorporate physics knowledge (e.g., laws of physics, scientific principles) into deep long short-term memory (LSTM) networks, which boosts the learning within a feasible solution space. The physics constraints are embedded in the loss function to enforce the model training which can accurately capture latent system nonlinearity even with very limited available training datasets. Specifically for dynamic structures, physical laws of equation of motion, state dependency and hysteretic constitutive relationship are considered to construct the physics loss. In particular, two physics-informed multi-LSTM network architectures are proposed for structural metamodeling. The satisfactory performance of the proposed framework is successfully demonstrated through two illustrative examples (e.g., nonlinear structures subjected to ground motion excitation). It turns out that the embedded physics can alleviate overfitting issues, reduce the need of big training datasets, and improve the robustness of the trained model for more reliable prediction. As a result, the physics-informed deep learning paradigm outperforms classical non-physics-guided data-driven neural networks. △ Less","18 February, 2020",https://arxiv.org/pdf/2002.10253
Fighting Fire with Light: A Case for Defending DDoS Attacks Using the Optical Layer,Matthew Hall;Ramakrishnan Durairajan;Vyas Sekar,"The DDoS attack landscape is growing at an unprecedented pace. Inspired by the recent advances in optical networking, we make a case for optical layer-aware DDoS defense (O-LAD) in this paper. Our approach leverages the optical layer to isolate attack traffic rapidly via dynamic reconfiguration of (backup) wavelengths using ROADMs---bridging the gap between (a) evolution of the DDoS attack landscape and (b) innovations in the optical layer (e.g., reconfigurable optics). We show that the physical separation of traffic profiles allows finer-grained handling of suspicious flows and offers better performance for benign traffic in the face of an attack. We present preliminary results modeling throughput and latency for legitimate flows while scaling the strength of attacks. We also identify a number of open problems for the security, optical, and systems communities: modeling diverse DDoS attacks (e.g., fixed vs. variable rate, detectable vs. undetectable), building a full-fledged defense system with optical advancements (e.g., OpenConfig), and optical layer-aware defenses for a broader class of attacks (e.g., network reconnaissance). △ Less","23 February, 2020",https://arxiv.org/pdf/2002.10009
PrivGen: Preserving Privacy of Sequences Through Data Generation,Sigal Shaked;Lior Rokach,"Sequential data is everywhere, and it can serve as a basis for research that will lead to improved processes. For example, road infrastructure can be improved by identifying bottlenecks in GPS data, or early diagnosis can be improved by analyzing patterns of disease progression in medical data. The main obstacle is that access and use of such data is usually limited or not permitted at all due to concerns about violating user privacy, and rightly so. Anonymizing sequence data is not a simple task, since a user creates an almost unique signature over time. Existing anonymization methods reduce the quality of information in order to maintain the level of anonymity required. Damage to quality may disrupt patterns that appear in the original data and impair the preservation of various characteristics. Since in many cases the researcher does not need the data as is and instead is only interested in the patterns that exist in the data, we propose PrivGen, an innovative method for generating data that maintains patterns and characteristics of the source data. We demonstrate that the data generation mechanism significantly limits the risk of privacy infringement. Evaluating our method with real-world datasets shows that its generated data preserves many characteristics of the data, including the sequential model, as trained based on the source data. This suggests that the data generated by our method could be used in place of actual data for various types of analysis, maintaining user privacy and the data's integrity at the same time. △ Less","23 February, 2020",https://arxiv.org/pdf/2002.09834
Longitudinal Support Vector Machines for High Dimensional Time Series,Kristiaan Pelckmans;Hong-Li Zeng,"We consider the problem of learning a classifier from observed functional data. Here, each data-point takes the form of a single time-series and contains numerous features. Assuming that each such series comes with a binary label, the problem of learning to predict the label of a new coming time-series is considered. Hereto, the notion of {\em margin} underlying the classical support vector machine is extended to the continuous version for such data. The longitudinal support vector machine is also a convex optimization problem and its dual form is derived as well. Empirical results for specified cases with significance tests indicate the efficacy of this innovative algorithm for analyzing such long-term multivariate data. △ Less","22 February, 2020",https://arxiv.org/pdf/2002.09763
Automatic Data Augmentation via Deep Reinforcement Learning for Effective Kidney Tumor Segmentation,Tiexin Qin;Ziyuan Wang;Kelei He;Yinghuan Shi;Yang Gao;Dinggang Shen,"Conventional data augmentation realized by performing simple pre-processing operations (\eg, rotation, crop, \etc) has been validated for its advantage in enhancing the performance for medical image segmentation. However, the data generated by these conventional augmentation methods are random and sometimes harmful to the subsequent segmentation. In this paper, we developed a novel automatic learning-based data augmentation method for medical image segmentation which models the augmentation task as a trial-and-error procedure using deep reinforcement learning (DRL). In our method, we innovatively combine the data augmentation module and the subsequent segmentation module in an end-to-end training manner with a consistent loss. Specifically, the best sequential combination of different basic operations is automatically learned by directly maximizing the performance improvement (\ie, Dice ratio) on the available validation set. We extensively evaluated our method on CT kidney tumor segmentation which validated the promising results of our method. △ Less","22 February, 2020",https://arxiv.org/pdf/2002.09703
A New Unified Deep Learning Approach with Decomposition-Reconstruction-Ensemble Framework for Time Series Forecasting,Guowei Zhang;Tao Ren;Yifan Yang,"A new variational mode decomposition (VMD) based deep learning approach is proposed in this paper for time series forecasting problem. Firstly, VMD is adopted to decompose the original time series into several sub-signals. Then, a convolutional neural network (CNN) is applied to learn the reconstruction patterns on the decomposed sub-signals to obtain several reconstructed sub-signals. Finally, a long short term memory (LSTM) network is employed to forecast the time series with the decomposed sub-signals and the reconstructed sub-signals as inputs. The proposed VMD-CNN-LSTM approach is originated from the decomposition-reconstruction-ensemble framework, and innovated by embedding the reconstruction, single forecasting, and ensemble steps in a unified deep learning approach. To verify the forecasting performance of the proposed approach, four typical time series datasets are introduced for empirical analysis. The empirical results demonstrate that the proposed approach outperforms consistently the benchmark approaches in terms of forecasting accuracy, and also indicate that the reconstructed sub-signals obtained by CNN is of importance for further improving the forecasting performance. △ Less","22 February, 2020",https://arxiv.org/pdf/2002.09695
Optimal Randomized First-Order Methods for Least-Squares Problems,Jonathan Lacotte;Mert Pilanci,"We provide an exact analysis of a class of randomized algorithms for solving overdetermined least-squares problems. We consider first-order methods, where the gradients are pre-conditioned by an approximation of the Hessian, based on a subspace embedding of the data matrix. This class of algorithms encompasses several randomized methods among the fastest solvers for least-squares problems. We focus on two classical embeddings, namely, Gaussian projections and subsampled randomized Hadamard transforms (SRHT). Our key technical innovation is the derivation of the limiting spectral density of SRHT embeddings. Leveraging this novel result, we derive the family of normalized orthogonal polynomials of the SRHT density and we find the optimal pre-conditioned first-order method along with its rate of convergence. Our analysis of Gaussian embeddings proceeds similarly, and leverages classical random matrix theory results. In particular, we show that for a given sketch size, SRHT embeddings exhibits a faster rate of convergence than Gaussian embeddings. Then, we propose a new algorithm by optimizing the computational complexity over the choice of the sketching dimension. To our knowledge, our resulting algorithm yields the best known complexity for solving least-squares problems with no condition number dependence. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.09488
Simplified Ray Tracing for the Millimeter Wave Channel: A Performance Evaluation,Mattia Lecci;Paolo Testolina;Marco Giordani;Michele Polese;Tanguy Ropitault;Camillo Gentile;Neeraj Varshney;Anuraag Bodi;Michele Zorzi,"Millimeter-wave (mmWave) communication is one of the cornerstone innovations of fifth-generation (5G) wireless networks, thanks to the massive bandwidth available in these frequency bands. To correctly assess the performance of such systems, however, it is essential to have reliable channel models, based on a deep understanding of the propagation characteristics of the mmWave signal. In this respect, ray tracers can provide high accuracy, at the expense of a significant computational complexity, which limits the scalability of simulations. To address this issue, in this paper we present possible simplifications that can reduce the complexity of ray tracing in the mmWave environment, without significantly affecting the accuracy of the model. We evaluate the effect of such simplifications on link-level metrics, testing different configuration parameters and propagation scenarios. △ Less","21 February, 2020",https://arxiv.org/pdf/2002.09179
CopyCat: Controlled Instruction-Level Attacks on Enclaves,Daniel Moghimi;Jo Van Bulck;Nadia Heninger;Frank Piessens;Berk Sunar,"The adversarial model presented by trusted execution environments (TEEs) has prompted researchers to investigate unusual attack vectors. One particularly powerful class of controlled-channel attacks abuses page-table modifications to reliably track enclave memory accesses at a page-level granularity. In contrast to noisy microarchitectural timing leakage, this line of deterministic controlled-channel attacks abuses indispensable architectural interfaces and hence cannot be mitigated by tweaking microarchitectural resources. We propose an innovative controlled-channel attack, named CopyCat, that deterministically counts the number of instructions executed within a single enclave code page. We show that combining the instruction counts harvested by CopyCat with traditional, coarse-grained page-level leakage allows the accurate reconstruction of enclave control flow at a maximal instruction-level granularity. CopyCat can identify intra-page and intra-cache line branch decisions that ultimately may only differ in a single instruction, underscoring that even extremely subtle control flow deviations can be deterministically leaked from secure enclaves. We demonstrate the improved resolution and practicality of CopyCat on Intel SGX in an extensive study of single-trace and deterministic attacks against cryptographic implementations, and give novel algorithmic attacks to perform single-trace key extraction that exploit subtle vulnerabilities in the latest versions of widely-used cryptographic libraries. Our findings highlight the importance of stricter verification of cryptographic implementations, especially in the context of TEEs. △ Less","25 June, 2020",https://arxiv.org/pdf/2002.08437
MLModelScope: A Distributed Platform for Model Evaluation and Benchmarking at Scale,Abdul Dakkak;Cheng Li;Jinjun Xiong;Wen-mei Hwu,"Machine Learning (ML) and Deep Learning (DL) innovations are being introduced at such a rapid pace that researchers are hard-pressed to analyze and study them. The complicated procedures for evaluating innovations, along with the lack of standard and efficient ways of specifying and provisioning ML/DL evaluation, is a major ""pain point"" for the community. This paper proposes MLModelScope, an open-source, framework/hardware agnostic, extensible and customizable design that enables repeatable, fair, and scalable model evaluation and benchmarking. We implement the distributed design with support for all major frameworks and hardware, and equip it with web, command-line, and library interfaces. To demonstrate MLModelScope's capabilities we perform parallel evaluation and show how subtle changes to model evaluation pipeline affects the accuracy and HW/SW stack choices affect performance. △ Less","19 February, 2020",https://arxiv.org/pdf/2002.08295
Molecule Attention Transformer,Łukasz Maziarka;Tomasz Danel;Sławomir Mucha;Krzysztof Rataj;Jacek Tabor;Stanisław Jastrzębski,"Designing a single neural network architecture that performs competitively across a range of molecule property prediction tasks remains largely an open challenge, and its solution may unlock a widespread use of deep learning in the drug discovery industry. To move towards this goal, we propose Molecule Attention Transformer (MAT). Our key innovation is to augment the attention mechanism in Transformer using inter-atomic distances and the molecular graph structure. Experiments show that MAT performs competitively on a diverse set of molecular prediction tasks. Most importantly, with a simple self-supervised pretraining, MAT requires tuning of only a few hyperparameter values to achieve state-of-the-art performance on downstream tasks. Finally, we show that attention weights learned by MAT are interpretable from the chemical point of view. △ Less","19 February, 2020",https://arxiv.org/pdf/2002.08264
Optical Rate-Splitting Multiple Access for Visible Light Communications,Shimaa Naser;Lina Bariah;Wael Jaafar;Sami Muhaidat;Paschalis C. Sofotasios;Mahmoud Al-Qutayri;Octavia A. Dobre,"The proliferation of connected devices and emergence of internet-of-everything represent a major challenge for broadband wireless networks. This requires a paradigm shift towards the development of innovative technologies for next generation wireless systems. One of the key challenges is the scarcity of spectrum, owing to the unprecedented broadband penetration rate in recent years. A promising solution is the proposal of visible light communications (VLC), which explores the unregulated visible light spectrum to enable high-speed communications, in addition to efficient lighting. This solution offers a wider bandwidth that can accommodate ubiquitous broadband connectivity to indoor users and offload data traffic from cellular networks. Although VLC is secure and able to overcome the shortcomings of RF systems, it suffers from several limitations, e.g., limited modulation bandwidth. In this respect, solutions have been proposed recently to overcome this limitation. In particular, most common orthogonal and non-orthogonal multiple access techniques initially proposed for RF systems, e.g., space-division multiple access (SDMA) and NOMA, have been considered in the context of VLC. In spite of their promising gains, the performance of these techniques is somewhat limited. Consequently, in this article a new and generalized multiple access technique, called rate-splitting multiple access (RSMA), is introduced and investigated for the first time in VLC networks. We first provide an overview of the key multiple access technologies used in VLC systems. Then, we propose the first comprehensive approach to the integration of RSMA with VLC systems. In our proposed framework, SINR expressions are derived and used to evaluate the weighted sum rate (WSR) of a two-user scenario. Our results illustrate the flexibility of RSMA in generalizing NOMA and SDMA, and its WSR superiority in the VLC context. △ Less","4 March, 2020",https://arxiv.org/pdf/2002.07583
Knowledge and Social Relatedness Shape Research Portfolio Diversification,Giorgio Tripodi;Francesca Chiaromonte;Fabrizio Lillo,"Scientific discovery is shaped by scientists' choices and thus by their career patterns. The increasing knowledge required to work at the frontier of science makes it harder for an individual to embark on unexplored paths. Yet collaborations can reduce learning costs -- albeit at the expense of increased coordination costs. In this article, we use data on the publication histories of a very large sample of physicists to measure the effects of knowledge and social relatedness on their diversification strategies. Using bipartite networks, we compute a measure of topics similarity and a measure of social proximity. We find that scientists' strategies are not random, and that they are significantly affected by both. Knowledge relatedness across topics explains \approx 10\% of logistic regression deviances and social relatedness as much as \approx 30\%, suggesting that science is an eminently social enterprise: when scientists move out of their core specialization, they do so through collaborations. Interestingly, we also find a significant negative interaction between knowledge and social relatedness, suggesting that the farther scientists move from their specialization, the more they rely on collaborations. Our results provide a starting point for broader quantitative analyses of scientific diversification strategies, which could also be extended to the domain of technological innovation -- offering insights from a comparative and policy perspective. △ Less","25 September, 2020",https://arxiv.org/pdf/2002.06419
"Sub-method, partial behavioral reflection with Reflectivity: Looking back on 10 years of use",Steven Costiou;Vincent Aranega;Marcus Denker,"Context. Refining or altering existing behavior is the daily work of every developer, but that cannot be always anticipated, and software sometimes cannot be stopped. In such cases, unanticipated adaptation of running systems is of interest for many scenarios, ranging from functional upgrades to on-the-fly debugging or monitoring of critical applications. Inquiry. A way of altering software at run time is using behavioral reflection, which is particularly well-suited for unanticipated adaptation of real-world systems. Partial behavioral reflection is not a new idea, and for years many efforts have been made to propose a practical way of expressing it. All these efforts resulted in practical solutions, but which introduced a semantic gap between the code that requires adaptation and the expression of the partial behavior. For example, in Aspect-Oriented Programming, a pointcut description is expressed in another language, which introduces a new distance between the behavior expression (the Advice) and the source code in itself. Approach. Ten years ago, the idea of closing the gap between the code and the expression of the partial behavior led to the implementation of the Reflectivity framework. Using Reflectivity, developers annotate Abstract Syntax Tree (AST) nodes with meta-behavior which is taken into account by the compiler to produce behavioral variations. In this paper, we present Reflectivity, its API, its implementation and its usage in Pharo. We reflect on ten years of use of Reflectivity, and show how it has been used as a basic building block of many innovative ideas. Knowledge. Reflectivity brings a practical way of working at the AST level, which is a high-level representation of the source code manipulated by software developers. It enables a powerful way of dynamically add and modify behavior. Reflectivity is also a flexible mean to bridge the gap between the expression of the meta-behavior and the source code. This ability to apply unanticipated adaptation and to provide behavioral reflection led to many experiments and projects during this last decade by external users. Existing work use Reflectivity to implement reflective libraries or languages extensions, featherweight code instrumentation, dynamic software update, debugging tools and visualization and software analysis tools. Grounding. Reflectivity is actively used in research projects. During the past ten years, it served as a support, either for implementation or as a fundamental base, for many research work including PhD theses, conference, journal and workshop papers. Reflectivity is now an important library of the Pharo language, and is integrated at the heart of the platform. Importance. Reflectivity exposes powerful abstractions to deal with partial behavioral adaptation, while providing a mature framework for unanticipated, non-intrusive and partial behavioral reflection based on AST annotation. Furthermore, even if Reflectivity found its home inside Pharo, it is not a pure Smalltalk-oriented solution. As validation over the practical use of Reflectivity in dynamic object-oriented languages, the API has been ported to Python. Finally, the AST annotation feature of Reflectivity opens new experimentation opportunities about the control that developers could gain on the behavior of their own software. △ Less","14 February, 2020",https://arxiv.org/pdf/2002.06182
Fairness through Experimentation: Inequality in A/B testing as an approach to responsible design,Guillaume Saint-Jacques;Amir Sepehri;Nicole Li;Igor Perisic,"As technology continues to advance, there is increasing concern about individuals being left behind. Many businesses are striving to adopt responsible design practices and avoid any unintended consequences of their products and services, ranging from privacy vulnerabilities to algorithmic bias. We propose a novel approach to fairness and inclusiveness based on experimentation. We use experimentation because we want to assess not only the intrinsic properties of products and algorithms but also their impact on people. We do this by introducing an inequality approach to A/B testing, leveraging the Atkinson index from the economics literature. We show how to perform causal inference over this inequality measure. We also introduce the concept of site-wide inequality impact, which captures the inclusiveness impact of targeting specific subpopulations for experiments, and show how to conduct statistical inference on this impact. We provide real examples from LinkedIn, as well as an open-source, highly scalable implementation of the computation of the Atkinson index and its variance in Spark/Scala. We also provide over a year's worth of learnings -- gathered by deploying our method at scale and analyzing thousands of experiments -- on which areas and which kinds of product innovations seem to inherently foster fairness through inclusiveness. △ Less","13 February, 2020",https://arxiv.org/pdf/2002.05819
Learning Occupational Task-Shares Dynamics for the Future of Work,Subhro Das;Sebastian Steffen;Wyatt Clarke;Prabhat Reddy;Erik Brynjolfsson;Martin Fleming,"The recent wave of AI and automation has been argued to differ from previous General Purpose Technologies (GPTs), in that it may lead to rapid change in occupations' underlying task requirements and persistent technological unemployment. In this paper, we apply a novel methodology of dynamic task shares to a large dataset of online job postings to explore how exactly occupational task demands have changed over the past decade of AI innovation, especially across high, mid and low wage occupations. Notably, big data and AI have risen significantly among high wage occupations since 2012 and 2016, respectively. We built an ARIMA model to predict future occupational task demands and showcase several relevant examples in Healthcare, Administration, and IT. Such task demands predictions across occupations will play a pivotal role in retraining the workforce of the future. △ Less","28 January, 2020",https://arxiv.org/pdf/2002.05655
Abnormal respiratory patterns classifier may contribute to large-scale screening of people infected with COVID-19 in an accurate and unobtrusive manner,Yunlu Wang;Menghan Hu;Qingli Li;Xiao-Ping Zhang;Guangtao Zhai;Nan Yao,"Research significance: The extended version of this paper has been accepted by IEEE Internet of Things journal (DOI: 10.1109/JIOT.2020.2991456), please cite the journal version. During the epidemic prevention and control period, our study can be helpful in prognosis, diagnosis and screening for the patients infected with COVID-19 (the novel coronavirus) based on breathing characteristics. According to the latest clinical research, the respiratory pattern of COVID-19 is different from the respiratory patterns of flu and the common cold. One significant symptom that occurs in the COVID-19 is Tachypnea. People infected with COVID-19 have more rapid respiration. Our study can be utilized to distinguish various respiratory patterns and our device can be preliminarily put to practical use. Demo videos of this method working in situations of one subject and two subjects can be downloaded online. Research details: Accurate detection of the unexpected abnormal respiratory pattern of people in a remote and unobtrusive manner has great significance. In this work, we innovatively capitalize on depth camera and deep learning to achieve this goal. The challenges in this task are twofold: the amount of real-world data is not enough for training to get the deep model; and the intra-class variation of different types of respiratory patterns is large and the outer-class variation is small. In this paper, considering the characteristics of actual respiratory signals, a novel and efficient Respiratory Simulation Model (RSM) is first proposed to fill the gap between the large amount of training data and scarce real-world data. The proposed deep model and the modeling ideas have the great potential to be extended to large scale applications such as public places, sleep scenario, and office environment. △ Less","20 December, 2020",https://arxiv.org/pdf/2002.05534
"LUCID: A Practical, Lightweight Deep Learning Solution for DDoS Attack Detection",Roberto Doriguzzi-Corin;Stuart Millar;Sandra Scott-Hayward;Jesus Martinez-del-Rincon;Domenico Siracusa,"Distributed Denial of Service (DDoS) attacks are one of the most harmful threats in today's Internet, disrupting the availability of essential services. The challenge of DDoS detection is the combination of attack approaches coupled with the volume of live traffic to be analysed. In this paper, we present a practical, lightweight deep learning DDoS detection system called LUCID, which exploits the properties of Convolutional Neural Networks (CNNs) to classify traffic flows as either malicious or benign. We make four main contributions; (1) an innovative application of a CNN to detect DDoS traffic with low processing overhead, (2) a dataset-agnostic preprocessing mechanism to produce traffic observations for online attack detection, (3) an activation analysis to explain LUCID's DDoS classification, and (4) an empirical validation of the solution on a resource-constrained hardware platform. Using the latest datasets, LUCID matches existing state-of-the-art detection accuracy whilst presenting a 40x reduction in processing time, as compared to the state-of-the-art. With our evaluation results, we prove that the proposed approach is suitable for effective DDoS detection in resource-constrained operational environments. △ Less","28 August, 2020",https://arxiv.org/pdf/2002.04902
CROFT: A scalable three-dimensional parallel Fast Fourier Transform (FFT) implementation for High Performance Clusters,Vivek Gavane;Supriya Prabhugawankar;Shivam Garg;Archana Achalere;Rajendra Joshi,"The FFT of three-dimensional (3D) input data is an important computational kernel of numerical simulations and is widely used in High Performance Computing (HPC) codes running on a large number of processors. Performance of many scientific applications such as Molecular Dynamic simulations depends on the underlying 3D parallel FFT library being used. In this paper, we present C-DACs three-dimensional Fast Fourier Transform (CROFT) library which implements three-dimensional parallel FFT using pencil decomposition. To exploit the hyperthreading capabilities of processor cores without affecting performance, CROFT is designed to use multithreading along with MPI. CROFT implementation has an innovative feature of overlapping compute and memory-I/O with MPI communication using multithreading. As opposed to other 3D FFT implementations, CROFT uses only two threads where one thread is dedicated for communication so that it can be effectively overlapped with computations. Thus, depending on the number of processes used, CROFT achieves performance improvement of about 51% to 42% as compared to FFTW3 library. △ Less","27 August, 2020",https://arxiv.org/pdf/2002.04896
Unveiling the research landscape of Sustainable Development Goals and their inclusion in Higher Education Institutions and Research Centers: major trends in 2000-2017,Nuria Bautista-Puig;Ana Marta Aleixo;Susana Leal;Ulisses Azeiteiro;Rodrigo Costas,"Sustainable Development Goals are the blueprint to achieve a better and more sustainable future for society. Its legacy is linked with the Millennium Development Goals, set up in 2000. A bibliometric analysis was conducted to 1) measure ""core"" research output from 2000-2017, with the aim to map the global research of sustainability goals, 2) describe thematic specialization based on keywords co-occurrence analysis and strongest citation burst, 3) present a methodology to classify scientific output (based on an ad-hoc glossary) and assess SDGs interconnections. Sustainability goals publications (core+expand based on direct citations) were identified in-house CWTS Web of Science by using search terms in titles, abstracts, and keywords. 25,299 bibliographic records were analyzed, from which 21,653 (85.59%) are from HEIs and research centres (RC). The purpose of this paper is to analyze the role of these organizations in sustainability research. The findings reveal the increasing participation of these organizations in this research (660 institutions in 2000-2005 to 1744 institutions involved in 2012-2017). In terms of specialization, some institutions present a higher production and specialization on the topic (e.g., London School of Hygiene & Tropical Medicine and World Health Organization); however, others present less production but higher specialization (e.g., Stockholm Environment Institute). Regarding the topics, health (especially in developing countries), women and socio-economic aspects are the most prominent ones. Moreover, it is observed the interlinked nature of SDGs between some SDGs in research output (e.g., SDG11 and SDG3). This study provides important orientation for HEIs and RCs in terms of Research, Development and Innovation (R&D+i) to respond to major societal challenges and could be useful for the policymakers in order to promote the research agenda on this topic. △ Less","12 February, 2020",https://arxiv.org/pdf/2002.04895
Validation and Optimization of Multi-Organ Segmentation on Clinical Imaging Archives,Yuchen Xu;Olivia Tang;Yucheng Tang;Ho Hin Lee;Yunqiang Chen;Dashan Gao;Shizhong Han;Riqiang Gao;Michael R. Savona;Richard G. Abramson;Yuankai Huo;Bennett A. Landman,"Segmentation of abdominal computed tomography(CT) provides spatial context, morphological properties, and a framework for tissue-specific radiomics to guide quantitative Radiological assessment. A 2015 MICCAI challenge spurred substantial innovation in multi-organ abdominal CT segmentation with both traditional and deep learning methods. Recent innovations in deep methods have driven performance toward levels for which clinical translation is appealing. However, continued cross-validation on open datasets presents the risk of indirect knowledge contamination and could result in circular reasoning. Moreover, 'real world' segmentations can be challenging due to the wide variability of abdomen physiology within patients. Herein, we perform two data retrievals to capture clinically acquired deidentified abdominal CT cohorts with respect to a recently published variation on 3D U-Net (baseline algorithm). First, we retrieved 2004 deidentified studies on 476 patients with diagnosis codes involving spleen abnormalities (cohort A). Second, we retrieved 4313 deidentified studies on 1754 patients without diagnosis codes involving spleen abnormalities (cohort B). We perform prospective evaluation of the existing algorithm on both cohorts, yielding 13% and 8% failure rate, respectively. Then, we identified 51 subjects in cohort A with segmentation failures and manually corrected the liver and gallbladder labels. We re-trained the model adding the manual labels, resulting in performance improvement of 9% and 6% failure rate for the A and B cohorts, respectively. In summary, the performance of the baseline on the prospective cohorts was similar to that on previously published datasets. Moreover, adding data from the first cohort substantively improved performance when evaluated on the second withheld validation cohort. △ Less","10 February, 2020",https://arxiv.org/pdf/2002.04102
Hierarchical Gaussian Process Priors for Bayesian Neural Network Weights,Theofanis Karaletsos;Thang D. Bui,"Probabilistic neural networks are typically modeled with independent weight priors, which do not capture weight correlations in the prior and do not provide a parsimonious interface to express properties in function space. A desirable class of priors would represent weights compactly, capture correlations between weights, facilitate calibrated reasoning about uncertainty, and allow inclusion of prior knowledge about the function space such as periodicity or dependence on contexts such as inputs. To this end, this paper introduces two innovations: (i) a Gaussian process-based hierarchical model for network weights based on unit embeddings that can flexibly encode correlated weight structures, and (ii) input-dependent versions of these weight priors that can provide convenient ways to regularize the function space through the use of kernels defined on contextual inputs. We show these models provide desirable test-time uncertainty estimates on out-of-distribution data, demonstrate cases of modeling inductive biases for neural networks with kernels which help both interpolation and extrapolation from training data, and demonstrate competitive predictive performance on an active learning benchmark. △ Less","10 February, 2020",https://arxiv.org/pdf/2002.04033
"PBE-CC: Congestion Control via Endpoint-Centric, Physical-Layer Bandwidth Measurements",Yaxiong Xie;Fan Yi;Kyle Jamieson,"Wireless networks are becoming ever more sophisticated and overcrowded, imposing the most delay, jitter, and throughput damage to end-to-end network flows in today's internet. We therefore argue for fine-grained mobile endpoint-based wireless measurements to inform a precise congestion control algorithm through a well-defined API to the mobile's wireless physical layer. Our proposed congestion control algorithm is based on Physical-Layer Bandwidth measurements taken at the Endpoint (PBE-CC), and captures the latest 5G New Radio innovations that increase wireless capacity, yet create abrupt rises and falls in available wireless capacity that the PBE-CC sender can react to precisely and very rapidly. We implement a proof-of-concept prototype of the PBE measurement module on software-defined radios and the PBE sender and receiver in C. An extensive performance evaluation compares PBE-CC head to head against the leading cellular-aware and wireless-oblivious congestion control protocols proposed in the research community and in deployment, in mobile and static mobile scenarios, and over busy and quiet networks. Results show 6.3% higher average throughput than BBR, while simultaneously reducing 95th percentile delay by 1.8x. △ Less","6 July, 2020",https://arxiv.org/pdf/2002.03475
"Robust Online Composition, Routing and NF Placement for NFV-enabled Services",Omar Alhussein;Weihua Zhuang,"Network function virtualization (NFV) fosters innovation in the networking field and reduces the complexity involved in managing modern-day conventional networks. Via NFV, the provisioning of a network service becomes more agile, whereby virtual network functions can be instantiated on commodity servers and data centers on demand. Network functions can be either mandatory or best-effort. The former type is strictly necessary for the correctness of a network service, whereas the latter is preferrable yet not necessary. In this paper, we study the online provisioning of NFV-enabled network services. We consider both unicast and multicast NFV-enabled services with multiple mandatory and best-effort NF instances. We propose a primal-dual based online approximation algorithm that allocates both processing and transmission resources to maximize a profit function, subject to resource constraints on physical links and NFV nodes. The online algorithm resembles a joint admission mechanism and an online composition, routing and NF placement framework. The online algorithm is derived from an offline formulation through a primal-dual based analysis. Such analysis offers direct insights and a fundamental understanding on the nature of the profit-maximization problem for NFV-enabled services with multiple resource types. △ Less","9 February, 2020",https://arxiv.org/pdf/2002.03464
Dynamic Inference: A New Approach Toward Efficient Video Action Recognition,Wenhao Wu;Dongliang He;Xiao Tan;Shifeng Chen;Yi Yang;Shilei Wen,"Though action recognition in videos has achieved great success recently, it remains a challenging task due to the massive computational cost. Designing lightweight networks is a possible solution, but it may degrade the recognition performance. In this paper, we innovatively propose a general dynamic inference idea to improve inference efficiency by leveraging the variation in the distinguishability of different videos. The dynamic inference approach can be achieved from aspects of the network depth and the number of input video frames, or even in a joint input-wise and network depth-wise manner. In a nutshell, we treat input frames and network depth of the computational graph as a 2-dimensional grid, and several checkpoints are placed on this grid in advance with a prediction module. The inference is carried out progressively on the grid by following some predefined route, whenever the inference process comes across a checkpoint, an early prediction can be made depending on whether the early stop criteria meets. For the proof-of-concept purpose, we instantiate three dynamic inference frameworks using two well-known backbone CNNs. In these instances, we overcome the drawback of limited temporal coverage resulted from an early prediction by a novel frame permutation scheme, and alleviate the conflict between progressive computation and video temporal relation modeling by introducing an online temporal shift module. Extensive experiments are conducted to thoroughly analyze the effectiveness of our ideas and to inspire future research efforts. Results on various datasets also evident the superiority of our approach. △ Less","9 February, 2020",https://arxiv.org/pdf/2002.03342
Ensemble of Deep Convolutional Neural Networks for Automatic Pavement Crack Detection and Measurement,Zhun Fan;Chong Li;Ying Chen;Paola Di Mascio;Xiaopeng Chen;Guijie Zhu;Giuseppe Loprencipe,"Automated pavement crack detection and measurement are important road issues. Agencies have to guarantee the improvement of road safety. Conventional crack detection and measurement algorithms can be extremely time-consuming and low efficiency. Therefore, recently, innovative algorithms have received increased attention from researchers. In this paper, we propose an ensemble of convolutional neural networks (without a pooling layer) based on probability fusion for automated pavement crack detection and measurement. Specifically, an ensemble of convolutional neural networks was employed to identify the structure of small cracks with raw images. Secondly, outputs of the individual convolutional neural network model for the ensemble were averaged to produce the final crack probability value of each pixel, which can obtain a predicted probability map. Finally, the predicted morphological features of the cracks were measured by using the skeleton extraction algorithm. To validate the proposed method, some experiments were performed on two public crack databases (CFD and AigleRN) and the results of the different state-of-the-art methods were compared. The experimental results show that the proposed method outperforms the other methods. For crack measurement, the crack length and width can be measure based on different crack types (complex, common, thin, and intersecting cracks.). The results show that the proposed algorithm can be effectively applied for crack measurement. △ Less","8 February, 2020",https://arxiv.org/pdf/2002.03241
Shipper Cooperation in Stochastic Drone Delivery: A Dynamic Bayesian Game Approach,Suttinee Sawadsitang;Dusit Niyato;Tan Puay Siew;Ping Wang;Sarana Nutanong,"With the recent technological innovation, unmanned aerial vehicles, known as drones, have found numerous applications including package and parcel delivery for shippers. Drone delivery offers benefits over conventional ground-based vehicle delivery in terms of faster speed, lower cost, more environment-friendly, and less manpower needed. However, most of existing studies on drone delivery planning and scheduling focus on a single shipper and ignore uncertainty factors. As such, in this paper, we consider a scenario that multiple shippers can cooperate to minimize their drone delivery cost. We propose the Bayesian Shipper Cooperation in Stochastic Drone Delivery (BCoSDD) framework. The framework is composed of three functions, i.e., package assignment, shipper cooperation formation and cost management. The uncertainties of drone breakdown and misbehavior of cooperative shippers are taken into account by using multistage stochastic programming optimization and dynamic Bayesian coalition formation game. We conduct extensive performance evaluation of the BCoSDD framework by using customer locations from Solomon benchmark suite and a real Singapore logistics industry. As a result, the framework can help the shippers plan and schedule their drone delivery effectively. △ Less","8 February, 2020",https://arxiv.org/pdf/2002.03118
Impact of the Interaction Network on the Dynamics of Word-of-Mouth with Information Seeking,Samuel Thiriot,"Word-of-Mouth refers to the dynamics of interpersonal communication occurring during the diffusion of innovations (novel practices, ideas or products). According to field studies, word-of-mouth is made of both information seeking and proactive communication: individuals first become aware of the existence of an innovation, then start actively seeking out for the expert knowledge required to evaluate the innovation; when they hold the expert knowledge, they might start promoting it pro-actively. Successful diffusion of innovation requires the individuals to hold both awareness and expert knowledge, so they can evaluate the innovation and use it properly. A computational model ""USA/IPK"" was recently proposed to study the role and impact of information seeking on the dynamics of word-of-mouth. We propose here an analysis of the impact of the network of interaction on the dynamics of this model. We compare the dynamics of the model over networks generated with different algorithms with the original dynamics. The results demonstrate the dynamics of the model are similar across tested networks, with the noticeable exception of the efficiency of the diffusion which varies between networks having similar densities and sizes. △ Less","7 February, 2020",https://arxiv.org/pdf/2002.02728
Faster On-Device Training Using New Federated Momentum Algorithm,Zhouyuan Huo;Qian Yang;Bin Gu;Lawrence Carin. Heng Huang,"Mobile crowdsensing has gained significant attention in recent years and has become a critical paradigm for emerging Internet of Things applications. The sensing devices continuously generate a significant quantity of data, which provide tremendous opportunities to develop innovative intelligent applications. To utilize these data to train machine learning models while not compromising user privacy, federated learning has become a promising solution. However, there is little understanding of whether federated learning algorithms are guaranteed to converge. We reconsider model averaging in federated learning and formulate it as a gradient-based method with biased gradients. This novel perspective assists analysis of its convergence rate and provides a new direction for more acceleration. We prove for the first time that the federated averaging algorithm is guaranteed to converge for non-convex problems, without imposing additional assumptions. We further propose a novel accelerated federated learning algorithm and provide a convergence guarantee. Simulated federated learning experiments are conducted to train deep neural networks on benchmark datasets, and experimental results show that our proposed method converges faster than previous approaches. △ Less","5 February, 2020",https://arxiv.org/pdf/2002.02090
FEA-Net: A Physics-guided Data-driven Model for Efficient Mechanical Response Prediction,Houpu Yao;Yi Gao;Yongming Liu,"An innovative physics-guided learning algorithm for predicting the mechanical response of materials and structures is proposed in this paper. The key concept of the proposed study is based on the fact that physics models are governed by Partial Differential Equation (PDE), and its loading/ response mapping can be solved using Finite Element Analysis (FEA). Based on this, a special type of deep convolutional neural network (DCNN) is proposed that takes advantage of our prior knowledge in physics to build data-driven models whose architectures are of physics meaning. This type of network is named as FEA-Net and is used to solve the mechanical response under external loading. Thus, the identification of a mechanical system parameters and the computation of its responses are treated as the learning and inference of FEA-Net, respectively. Case studies on multi-physics (e.g., coupled mechanical-thermal analysis) and multi-phase problems (e.g., composite materials with random micro-structures) are used to demonstrate and verify the theoretical and computational advantages of the proposed method. △ Less","31 January, 2020",https://arxiv.org/pdf/2002.01893
Zendoo: a zk-SNARK Verifiable Cross-Chain Transfer Protocol Enabling Decoupled and Decentralized Sidechains,Alberto Garoffolo;Dmytro Kaidalov;Roman Oliynykov,"Sidechains are an appealing innovation devised to enable blockchain scalability and extensibility. The basic idea is simple yet powerful: construct a parallel chain -- sidechain -- with desired features, and provide a way to transfer coins between the mainchain and the sidechain. In this paper, we introduce Zendoo, a construction for Bitcoin-like blockchain systems that allows the creation and communication with sidechains of different types without knowing their internal structure. We consider a parent-child relationship between the mainchain and sidechains, where sidechain nodes directly observe the mainchain while mainchain nodes only observe cryptographically authenticated certificates from sidechain maintainers. We use zk-SNARKs to construct a universal verifiable transfer mechanism that is used by sidechains. Moreover, we propose a specific sidechain construction, named Latus, that can be built on top of this infrastructure, and realizes a decentralized verifiable blockchain system for payments. We leverage the use of recursive composition of zk-SNARKs to generate succinct proofs of sidechain state progression that are used to generate certificates' validity proofs. This allows the mainchain to efficiently verify all operations performed in the sidechain without knowing any details about those operations. △ Less","5 February, 2020",https://arxiv.org/pdf/2002.01847
Illumination adaptive person reid based on teacher-student model and adversarial training,Ziyue Zhang;Richard YD Xu;Shuai Jiang;Yang Li;Congzhentao Huang;Chen Deng,"Most existing works in Person Re-identification (ReID) focus on settings where illumination either is kept the same or has very little fluctuation. However, the changes in the illumination degree may affect the robustness of a ReID algorithm significantly. To address this problem, we proposed a Two-Stream Network that can separate ReID features from lighting features to enhance ReID performance. Its innovations are threefold: (1) A discriminative entropy loss to ensure the ReID features contain no lighting information. (2) A ReID Teacher model trained by images under ""neutral"" lighting conditions to guide ReID classification. (3) An illumination Teacher model trained by the differences between the illumination-adjusted and original images to guide illumination classification. We construct two augmented datasets by synthetically changing a set of predefined lighting conditions in two of the most popular ReID benchmarks: Market1501 and DukeMTMC-ReID. Experiments demonstrate that our algorithm outperforms other state-of-the-art works and particularly potent in handling images under extremely low light. △ Less","26 May, 2020",https://arxiv.org/pdf/2002.01625
Beat the AI: Investigating Adversarial Human Annotation for Reading Comprehension,Max Bartolo;Alastair Roberts;Johannes Welbl;Sebastian Riedel;Pontus Stenetorp,"Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models. One recent trend to challenge current RC models is to involve a model in the annotation process: humans create questions adversarially, such that the model fails to answer them correctly. In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop. This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalisation to data collected without a model. We find that training on adversarially collected samples leads to strong generalisation to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop. Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop. When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F1 on questions that it cannot answer when trained on SQuAD - only marginally lower than when trained on data collected using RoBERTa itself (41.0F1). △ Less","22 September, 2020",https://arxiv.org/pdf/2002.00293
Deep Multi-View Enhancement Hashing for Image Retrieval,Chenggang Yan;Biao Gong;Yuxuan Wei;Yue Gao,"Hashing is an efficient method for nearest neighbor search in large-scale data space by embedding high-dimensional feature descriptors into a similarity preserving Hamming space with a low dimension. However, large-scale high-speed retrieval through binary code has a certain degree of reduction in retrieval accuracy compared to traditional retrieval methods. We have noticed that multi-view methods can well preserve the diverse characteristics of data. Therefore, we try to introduce the multi-view deep neural network into the hash learning field, and design an efficient and innovative retrieval model, which has achieved a significant improvement in retrieval performance. In this paper, we propose a supervised multi-view hash model which can enhance the multi-view information through neural networks. This is a completely new hash learning method that combines multi-view and deep learning methods. The proposed method utilizes an effective view stability evaluation method to actively explore the relationship among views, which will affect the optimization direction of the entire network. We have also designed a variety of multi-data fusion methods in the Hamming space to preserve the advantages of both convolution and multi-view. In order to avoid excessive computing resources on the enhancement procedure during retrieval, we set up a separate structure called memory network which participates in training together. The proposed method is systematically evaluated on the CIFAR-10, NUS-WIDE and MS-COCO datasets, and the results show that our method significantly outperforms the state-of-the-art single-view and multi-view hashing methods. △ Less","15 June, 2020",https://arxiv.org/pdf/2002.00169
ResilientDB: Global Scale Resilient Blockchain Fabric,Suyash Gupta;Sajjad Rahnama;Jelle Hellings;Mohammad Sadoghi,"Recent developments in blockchain technology have inspired innovative new designs in resilient distributed and database systems. At their core, these blockchain applications typically use Byzantine fault-tolerant consensus protocols to maintain a common state across all replicas, even if some replicas are faulty or malicious. Unfortunately, existing consensus protocols are not designed to deal with geo-scale deployments in which many replicas spread across a geographically large area participate in consensus. To address this, we present the Geo-Scale Byzantine FaultTolerant consensus protocol (GeoBFT). GeoBFT is designed for excellent scalability by using a topological-aware grouping of replicas in local clusters, by introducing parallelization of consensus at the local level, and by minimizing communication between clusters. To validate our vision of high-performance geo-scale resilient distributed systems, we implement GeoBFT in our efficient ResilientDB permissioned blockchain fabric. We show that GeoBFT is not only sound and provides great scalability, but also outperforms state-of-the-art consensus protocols by a factor of six in geo-scale deployments. △ Less","18 March, 2020",https://arxiv.org/pdf/2002.00160
Asymmetric Distribution Measure for Few-shot Learning,Wenbin Li;Lei Wang;Jing Huo;Yinghuan Shi;Yang Gao;Jiebo Luo,"The core idea of metric-based few-shot image classification is to directly measure the relations between query images and support classes to learn transferable feature embeddings. Previous work mainly focuses on image-level feature representations, which actually cannot effectively estimate a class's distribution due to the scarcity of samples. Some recent work shows that local descriptor based representations can achieve richer representations than image-level based representations. However, such works are still based on a less effective instance-level metric, especially a symmetric metric, to measure the relations between query images and support classes. Given the natural asymmetric relation between a query image and a support class, we argue that an asymmetric measure is more suitable for metric-based few-shot learning. To that end, we propose a novel Asymmetric Distribution Measure (ADM) network for few-shot learning by calculating a joint local and global asymmetric measure between two multivariate local distributions of queries and classes. Moreover, a task-aware Contrastive Measure Strategy (CMS) is proposed to further enhance the measure function. On popular miniImageNet and tieredImageNet, we achieve 3.02\% and 1.56\% gains over the state-of-the-art method on the 5-way 1-shot task, respectively, validating our innovative design of asymmetric distribution measures for few-shot learning. △ Less","1 February, 2020",https://arxiv.org/pdf/2002.00153
The Competitive Effects of Variance-based Pricing,Ludwig Dierks;Sven Seuken,"In many markets, like electricity or cloud computing markets, providers incur large costs for keeping sufficient capacity in reserve to accommodate demand fluctuations of a mostly fixed user base. These costs are significantly affected by the unpredictability of the users' demand. Nevertheless, standard mechanisms charge fixed per-unit prices that do not depend on the variability of the users' demand. In this paper, we study a variance-based pricing rule in a two-provider market setting and perform a game-theoretic analysis of the resulting competitive effects. We show that an innovative provider who employs variance-based pricing can choose a pricing strategy that guarantees himself a higher profit than using fixed per-unit prices for any individually rational response of a provider playing a fixed pricing strategy. We characterize all equilibria for the setting where both providers employ variance-based pricing strategies. We find that, while in equilibrium, the profits of the providers may increase or decrease depending on their cost functions, social welfare always weakly increases. △ Less","6 April, 2020",https://arxiv.org/pdf/2001.11769
Proposal of a standard of Knowledge Management and Technological Innovation for Mexico,Jorge Romero-Hidalgo,"The purpose of this work is to offer a methodology that allows to construct a standard in Knowledge Management and Technological Innovation which may be used in various organizations in México to improve the operation of their resources and productivity. Based on the review of the existing literature, a model is offered including several elements to enable organizations to establish their position in relation to both concepts. The following proposal is based on a systematic effort to understand and integrate models of Knowledge Management and Innovation published in recent years as well as the results of the experiences to propose standards of Knowledge Management and Technological Innovation. In order to elaborate the proposal, factors and their associated components have been analyzed through a review of the literature in order to build and validate a standard proposal. To test the research study, a six-stage research model has been constructed. For this purpose, an in-depth exploratory research study has been carried out in a public sector organization, in an area that allows the replicability of the model. The results have been analyzed to construct and empirically validate the Mexican Standard of Knowledge Management an Technological Innovation. Finally, after the statistical analysis, results obtained from the application of the validated instrument are shown , which supports the definition of the model. △ Less","27 January, 2020",https://arxiv.org/pdf/2001.11379
EEG-based Brain-Computer Interfaces (BCIs): A Survey of Recent Studies on Signal Sensing Technologies and Computational Intelligence Approaches and their Applications,Xiaotong Gu;Zehong Cao;Alireza Jolfaei;Peng Xu;Dongrui Wu;Tzyy-Ping Jung;Chin-Teng Lin,"Brain-Computer Interface (BCI) is a powerful communication tool between users and systems, which enhances the capability of the human brain in communicating and interacting with the environment directly. Advances in neuroscience and computer science in the past decades have led to exciting developments in BCI, thereby making BCI a top interdisciplinary research area in computational neuroscience and intelligence. Recent technological advances such as wearable sensing devices, real-time data streaming, machine learning, and deep learning approaches have increased interest in electroencephalographic (EEG) based BCI for translational and healthcare applications. Many people benefit from EEG-based BCIs, which facilitate continuous monitoring of fluctuations in cognitive states under monotonous tasks in the workplace or at home. In this study, we survey the recent literature of EEG signal sensing technologies and computational intelligence approaches in BCI applications, compensated for the gaps in the systematic summary of the past five years (2015-2019). In specific, we first review the current status of BCI and its significant obstacles. Then, we present advanced signal sensing and enhancement technologies to collect and clean EEG signals, respectively. Furthermore, we demonstrate state-of-art computational intelligence techniques, including interpretable fuzzy models, transfer learning, deep learning, and combinations, to monitor, maintain, or track human cognitive states and operating performance in prevalent applications. Finally, we deliver a couple of innovative BCI-inspired healthcare applications and discuss some future research directions in EEG-based BCIs. △ Less","28 January, 2020",https://arxiv.org/pdf/2001.11337
On the Convergence of Artificial Intelligence and Distributed Ledger Technology: A Scoping Review and Future Research Agenda,Konstantin D. Pandl;Scott Thiebes;Manuel Schmidt-Kraepelin;Ali Sunyaev,"Developments in Artificial Intelligence (AI) and Distributed Ledger Technology (DLT) currently lead to lively debates in academia and practice. AI processes data to perform tasks that were previously thought possible only for humans. DLT has the potential to create consensus over data among a group of participants in uncertain environments. In recent research, both technologies are used in similar and even the same systems. Examples include the design of secure distributed ledgers or the creation of allied learning systems distributed across multiple nodes. This can lead to technological convergence, which in the past, has paved the way for major innovations in information technology. Previous work highlights several potential benefits of the convergence of AI and DLT but only provides a limited theoretical framework to describe upcoming real-world integration cases of both technologies. We aim to contribute by conducting a systematic literature review on previous work and providing rigorously derived future research opportunities. This work helps researchers active in AI or DLT to overcome current limitations in their field, and practitioners to develop systems along with the convergence of both technologies. △ Less","5 February, 2020",https://arxiv.org/pdf/2001.11017
Lossless Compression of Mosaic Images with Convolutional Neural Network Prediction,Seyed Mehdi Ayyoubzadeh;Xiaolin Wu,"We present a CNN-based predictive lossless compression scheme for raw color mosaic images of digital cameras. This specialized application problem was previously understudied but it is now becoming increasingly important, because modern CNN methods for image restoration tasks (e.g., superresolution, low lighting enhancement, deblurring), must operate on original raw mosaic images to obtain the best possible results. The key innovation of this paper is a high-order nonlinear CNN predictor of spatial-spectral mosaic patterns. The deep learning prediction can model highly complex sample dependencies in spatial-spectral mosaic images more accurately and hence remove statistical redundancies more thoroughly than existing image predictors. Experiments show that the proposed CNN predictor achieves unprecedented lossless compression performance on camera raw images. △ Less","28 January, 2020",https://arxiv.org/pdf/2001.10484
Mining social media data for biomedical signals and health-related behavior,Rion Brattig Correia;Ian B. Wood;Johan Bollen;Luis M. Rocha,"Social media data has been increasingly used to study biomedical and health-related phenomena. From cohort level discussions of a condition to planetary level analyses of sentiment, social media has provided scientists with unprecedented amounts of data to study human behavior and response associated with a variety of health conditions and medical treatments. Here we review recent work in mining social media for biomedical, epidemiological, and social phenomena information relevant to the multilevel complexity of human health. We pay particular attention to topics where social media data analysis has shown the most progress, including pharmacovigilance, sentiment analysis especially for mental health, and other areas. We also discuss a variety of innovative uses of social media data for health-related applications and important limitations in social media data access and use. △ Less","28 January, 2020",https://arxiv.org/pdf/2001.10285
Competence Assessment as an Expert System for Human Resource Management: A Mathematical Approach,Mahdi Bohlouli;Nikolaos Mittas;George Kakarontzas;Theodosios Theodosiou;Lefteris Angelis;Madjid Fathi,"Efficient human resource management needs accurate assessment and representation of available competences as well as effective mapping of required competences for specific jobs and positions. In this regard, appropriate definition and identification of competence gaps express differences between acquired and required competences. Using a detailed quantification scheme together with a mathematical approach is a way to support accurate competence analytics, which can be applied in a wide variety of sectors and fields. This article describes the combined use of software technologies and mathematical and statistical methods for assessing and analyzing competences in human resource information systems. Based on a standard competence model, which is called a Professional, Innovative and Social competence tree, the proposed framework offers flexible tools to experts in real enterprise environments, either for evaluation of employees towards an optimal job assignment and vocational training or for recruitment processes. The system has been tested with real human resource data sets in the frame of the European project called ComProFITS. △ Less","16 January, 2020",https://arxiv.org/pdf/2001.09797
Knowledge Integration of Collaborative Product Design Using Cloud Computing Infrastructure,Mahdi Bohlouli;Alexander Holland;Madjid Fathi,"The pivotal key to the success of manufacturing enterprises is a sustainable and innovative product design and development. In collaborative design, stakeholders are heterogeneously distributed chain-like. Due to the growing volume of data and knowledge, effective management of the knowledge acquired in the product design and development is one of the key challenges facing most manufacturing enterprises. Opportunities for improving efficiency and performance of IT-based product design applications through centralization of resources such as knowledge and computation have increased in the last few years with the maturation of technologies such as SOA, virtualization, grid computing, and/or cloud computing. The main focus of this paper is the concept of ongoing research in providing the knowledge integration service for collaborative product design and development using cloud computing infrastructure. Potentials of the cloud computing to support the Knowledge integration functionalities as a Service by providing functionalities such as knowledge mapping, merging, searching, and transferring in product design procedure are described in this paper. Proposed knowledge integration services support users by giving real-time access to knowledge resources. The framework has the advantage of availability, efficiency, cost reduction, less time to result, and scalability. △ Less","16 January, 2020",https://arxiv.org/pdf/2001.09796
Practical Approach of Knowledge Management in Medical Science,Mahdi Bohlouli;Patrick Uhr;Fabian Merges;Sanaz Mohammad Hassani;Madjid Fathi,"Knowledge organization, infrastructure, and knowledge-based activities are all subjects that help in the creation of business strategies for the new enterprise. In this paper, the first basics of knowledge-based systems are studied. Practical issues and challenges of Knowledge Management (KM) implementations are then illustrated. Finally, a comparison of different knowledge-based projects is presented along with abstracted information on their implementation, techniques, and results. Most of these projects are in the field of medical science. Based on our study and evaluation of different KM projects, we conclude that KM is being used in every science, industry, and business. But its importance in medical science and assisted living projects are highlighted nowadays with the most of research institutes. Most medical centers are interested in using knowledge-based services like portals and learning techniques of knowledge for their future innovations and supports. △ Less","16 January, 2020",https://arxiv.org/pdf/2001.09795
"Autonomous Shuttle-as-a-Service (ASaaS): Challenges, Opportunities, and Social Implications",Antonio Bucchiarone;Sandro Battisti;Annapaola Marconi;Roberto Maldacea;Diego Cardona Ponce,"Modern cities are composed of complex socio-technical systems that exist to provide services effectively to their residents and visitors. In this context, smart mobility systems aim to support the efficient exploitation of the city transport facilities as well as sustainable mobility within the urban environment. People need to travel quickly and conveniently between locations at different scales, ranging from a trip of a few blocks within a city to a journey across cities or further. At the same time, goods need to be timely delivered considering the needs of both the users and the businesses. While most of the mobility and delivery solutions can cover significant distances and multiple requests, they suffer when the requests come from the growing neighborhoods and hard-to-reach areas such as city centers, corporate headquarters, and hospitals. In the last few years, several cities indicated interest in using Autonomous Vehicles (AV) for the ""last-mile"" mobility services. With them, it seems to be easier to get people and goods around using fewer vehicles. In this context, Autonomous Shuttles (AS) are beginning to be thought of as a new mobility/delivery service into the city center where narrow streets are not easily served by traditional buses. They allow them to serve critical areas with minimal new infrastructure and reducing noise and pollution. The goal of this article is to present an innovative vision on the introduction of the Autonomous Shuttles-as-a service (ASaaS) concept as the key pillar for the realization of innovative and sustainable proximity mobility. Through a set of real application scenarios, we present our view, and we discuss a set of challenges, opportunities, and social implications that this way to reimage the mobility of the future introduces. △ Less","14 January, 2020",https://arxiv.org/pdf/2001.09763
Towards a framework for understanding societal and ethical implications of Artificial Intelligence,Richard Benjamins;Idoia Salazar,"Artificial Intelligence (AI) is one of the most discussed technologies today. There are many innovative applications such as the diagnosis and treatment of cancer, customer experience, new business, education, contagious diseases propagation and optimization of the management of humanitarian catastrophes. However, with all those opportunities also comes great responsibility to ensure good and fair practice of AI. The objective of this paper is to identify the main societal and ethical challenges implied by a massive uptake of AI. We have surveyed the literature for the most common challenges and classified them in seven groups: 1) Non-desired effects, 2) Liability, 3) Unknown consequences, 4) Relation people-robots, 5) Concentration of power and wealth, 6) Intentional bad uses, and 7) AI for weapons and warfare. The challenges should be dealt with in different ways depending on their origin; some have technological solutions, while others require ethical, societal, or political answers. Depending on the origin, different stakeholders might need to act. Whatever the identified stakeholder, not treating those issues will lead to uncertainty and unforeseen consequences with potentially large negative societal impact, hurting especially the most vulnerable groups of societies. Technology is helping to take better decisions, and AI is promoting data-driven decisions in addition to experience- and intuition-based discussion, with many improvements happening. However, the negative side effects of this technology need to be well understood and acted upon before we launch them massively into the world. △ Less","3 January, 2020",https://arxiv.org/pdf/2001.09750
A Robust Real-Time Computing-based Environment Sensing System for Intelligent Vehicle,Qiwei Xie;Qian Long;Liming Zhang;Zhao Sun,"For intelligent vehicles, sensing the 3D environment is the first but crucial step. In this paper, we build a real-time advanced driver assistance system based on a low-power mobile platform. The system is a real-time multi-scheme integrated innovation system, which combines stereo matching algorithm with machine learning based obstacle detection approach and takes advantage of the distributed computing technology of a mobile platform with GPU and CPUs. First of all, a multi-scale fast MPV (Multi-Path-Viterbi) stereo matching algorithm is proposed, which can generate robust and accurate disparity map. Then a machine learning, which is based on fusion technology of monocular and binocular, is applied to detect the obstacles. We also advance an automatic fast calibration mechanism based on Zhang's calibration method. Finally, the distributed computing and reasonable data flow programming are applied to ensure the operational efficiency of the system. The experimental results show that the system can achieve robust and accurate real-time environment perception for intelligent vehicles, which can be directly used in the commercial real-time intelligent driving applications. △ Less","27 January, 2020",https://arxiv.org/pdf/2001.09678
TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network,Jiaming Shen;Zhihong Shen;Chenyan Xiong;Chi Wang;Kuansan Wang;Jiawei Han,"Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of <query concept, anchor concept> pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion. △ Less","26 January, 2020",https://arxiv.org/pdf/2001.09522
Software-Defined Location Privacy Protection for Vehicular Networks,Abdelwahab Boualouache;Ridha Soua;Thomas Engel,"While the adoption of connected vehicles is growing, security and privacy concerns are still the key barriers raised by society. These concerns mandate automakers and standardization groups to propose convenient solutions for privacy preservation. One of the main proposed solutions is the use of Pseudonym-Changing Strategies (PCSs). However, ETSI has recently published a technical report which highlights the absence of standardized and efficient PCSs [1]. This alarming situation mandates an innovative shift in the way that the privacy of end-users is protected during their journey. Software-Defined Networking (SDN) is emerging as a key 5G enabler to manage the network in a dynamic manner. SDN-enabled wireless networks are opening up new programmable and highly-flexible privacy-aware solutions. We exploit this paradigm to propose an innovative software-defined location privacy architecture for vehicular networks. The proposed architecture is context-aware, programmable, extensible, and able to encompass all existing and future pseudonym-changing strategies. To demonstrate the merit of our architecture, we consider a case study that involves four pseudonym-changing strategies, which we deploy over our architecture and compare with their static implementations. We also detail how the SDN controller dynamically switches between the strategies according to the context. △ Less","24 January, 2020",https://arxiv.org/pdf/2001.09170
The Impact of Content Commenting on User Continuance in Online Q&A Communities: An Affordance Perspective,Langtao Chen,"Online question-and-answer (Q&A) communities provide convenient and innovative ways for participants to share information and collaboratively solve problems with others. A growing challenge for those Q&A communities is to encourage and maintain ongoing user participation. From the perspective of motivational affordances, this study proposes a research framework to explain the effect of content commenting on user continuance behavior in online Q&A communities. The moderating role of participant's tenure in the relationship between content commenting and user continuance is also explored. Using a longitudinal panel dataset collected from a large online Q&A community, this research empirically tests the effect of content commenting on continued user participation in the Q&A community. The results show that both comment receipt and comment provisioning are important motivating factors for user continuance in the community. Specifically, received comments on questions submitted by a participant have a positive effect on the participant's continuance of posting questions, while answer comments both received and posted by a participant have positive impact on user continuance of posting answers in the community. In addition, tenure in the community is indeed found to have a significant negative moderating effect on the relationship between content commenting and user continuance. This research not only offers a more nuanced theoretical understanding of how content commenting affects continued user involvement and how participants' tenure in the community moderates the impact of content commenting, but also provides implications for improving user continuance in online Q&A communities. △ Less","24 January, 2020",https://arxiv.org/pdf/2001.08927
Chameleon: Adaptive Code Optimization for Expedited Deep Neural Network Compilation,Byung Hoon Ahn;Prannoy Pilligundla;Amir Yazdanbakhsh;Hadi Esmaeilzadeh,"Achieving faster execution with shorter compilation time can foster further diversity and innovation in neural networks. However, the current paradigm of executing neural networks either relies on hand-optimized libraries, traditional compilation heuristics, or very recently genetic algorithms and other stochastic methods. These methods suffer from frequent costly hardware measurements rendering them not only too time consuming but also suboptimal. As such, we devise a solution that can learn to quickly adapt to a previously unseen design space for code optimization, both accelerating the search and improving the output performance. This solution dubbed Chameleon leverages reinforcement learning whose solution takes fewer steps to converge, and develops an adaptive sampling algorithm that not only focuses on the costly samples (real hardware measurements) on representative points but also uses a domain-knowledge inspired logic to improve the samples itself. Experimentation with real hardware shows that Chameleon provides 4.45x speed up in optimization time over AutoTVM, while also improving inference time of the modern deep networks by 5.6%. △ Less","23 January, 2020",https://arxiv.org/pdf/2001.08743
Knowledge Graphs for Innovation Ecosystems,Alberto Tejero;Victor Rodriguez-Doncel;Ivan Pau,"Innovation ecosystems can be naturally described as a collection of networked entities, such as experts, institutions, projects, technologies and products. Representing in a machine-readable form these entities and their relations is not entirely attainable, due to the existence of abstract concepts such as knowledge and due to the confidential, non-public nature of this information, but even its partial depiction is of strong interest. The representation of innovation ecosystems incarnated as knowledge graphs would enable the generation of reports with new insights, the execution of advanced data analysis tasks. An ontology to capture the essential entities and relations is presented, as well as the description of data sources, which can be used to populate innovation knowledge graphs. Finally, the application case of the Universidad Politecnica de Madrid is presented, as well as an insight of future applications. △ Less","9 January, 2020",https://arxiv.org/pdf/2001.08615
Deep Technology Tracing for High-tech Companies,Han Wu;Kun Zhang;Guangyi Lv;Qi Liu;Runlong Yu;Weihao Zhao;Enhong Chen;Jianhui Ma,"Technological change and innovation are vitally important, especially for high-tech companies. However, factors influencing their future research and development (R&D) trends are both complicated and various, leading it a quite difficult task to make technology tracing for high-tech companies. To this end, in this paper, we develop a novel data-driven solution, i.e., Deep Technology Forecasting (DTF) framework, to automatically find the most possible technology directions customized to each high-tech company. Specially, DTF consists of three components: Potential Competitor Recognition (PCR), Collaborative Technology Recognition (CTR), and Deep Technology Tracing (DTT) neural network. For one thing, PCR and CTR aim to capture competitive relations among enterprises and collaborative relations among technologies, respectively. For another, DTT is designed for modeling dynamic interactions between companies and technologies with the above relations involved. Finally, we evaluate our DTF framework on real-world patent data, and the experimental results clearly prove that DTF can precisely help to prospect future technology emphasis of companies by exploiting hybrid factors. △ Less","2 January, 2020",https://arxiv.org/pdf/2001.08606
DDKSP: A Data-Driven Stochastic Programming Framework for Car-Sharing Relocation Problem,Xiaoming Li;Chun Wang;Xiao Huang,"Car-sharing issue is a popular research field in sharing economy. In this paper, we investigate the car-sharing relocation problem (CSRP) under uncertain demands. Normally, the real customer demands follow complicating probability distribution which cannot be described by parametric approaches. In order to overcome the problem, an innovative framework called Data-Driven Kernel Stochastic Programming (DDKSP) that integrates a non-parametric approach - kernel density estimation (KDE) and a two-stage stochastic programming (SP) model is proposed. Specifically, the probability distributions are derived from historical data by KDE, which are used as the input uncertain parameters for SP. Additionally, the CSRP is formulated as a two-stage SP model. Meanwhile, a Monte Carlo method called sample average approximation (SAA) and Benders decomposition algorithm are introduced to solve the large-scale optimization model. Finally, the numerical experimental validations which are based on New York taxi trip data sets show that the proposed framework outperforms the pure parametric approaches including Gaussian, Laplace and Poisson distributions with 3.72% , 4.58% and 11% respectively in terms of overall profits. △ Less","20 January, 2020",https://arxiv.org/pdf/2001.08109
Attention! A Lightweight 2D Hand Pose Estimation Approach,Nicholas Santavas;Ioannis Kansizoglou;Loukas Bampis;Evangelos Karakasis;Antonios Gasteratos,"Vision based human pose estimation is an non-invasive technology for Human-Computer Interaction (HCI). Direct use of the hand as an input device provides an attractive interaction method, with no need for specialized sensing equipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI is employed in various applications spreading in areas including manufacturing, surgery, entertainment industry and architecture, to mention a few. Deployment of vision based human pose estimation algorithms can give a breath of innovation to these applications. In this letter, we present a novel Convolutional Neural Network architecture, reinforced with a Self-Attention module that it can be deployed on an embedded system, due to its lightweight nature, with just 1.9 Million parameters. The source code and qualitative results are publicly available. △ Less","30 May, 2020",https://arxiv.org/pdf/2001.08047
DeepRS: Deep-learning Based Network-Adaptive FEC for Real-Time Video Communications,Sheng Cheng;Han Hu;Xinggong Zhang;Zongming Guo,This work proposes an innovative approach to handle packet loss in real-time video streaming scenarios in a more sophisticated way -- Predicting packet loss pattern on time field by deep learning model.,"21 January, 2020",https://arxiv.org/pdf/2001.07852
LOcAl DEcisions on Replicated States (LOADER) in programmable data planes: programming abstraction and experimental evaluation,German Sviridov;Marco Bonola;Angelo Tulumello;Paolo Giaccone;Andrea Bianco;Giuseppe Bianchi,"Programmable data planes recently emerged as a prominent innovation in Software Defined Networking (SDN), by permitting support of stateful flow processing functions over hardware network switches specifically designed for network processing. Unlike early SDN solutions such as OpenFlow, modern stateful data planes permit to keep (and dynamically update) local per-flow states inside network switches, thus dramatically improving reactiveness of network applications to state changes. Still, also in stateful data planes, the control and update of non-local states is assumed to be completely delegated to a centralized controller and thus accessed only at the price of extra delay. Our LOADER proposal aims at contrasting the apparent dichotomy between local states and global states. We do so by introducing a new possibility: permit to take localized (in-switch) decisions not only on local states but also on replicated global states, thus providing support for network-wide applications without incurring the drawbacks of classical approaches. To this purpose, i) we provide high-level programming abstractions devised to define the states and the update logic of a generic network-wide application, and ii) we detail the underlying low level state management and replication mechanisms. We then show LOADER's independence of the stateful data plane technology employed, by implementing it over two distinct stateful data planes (P4 switches and OPP - Open Packet Processor - switches), and by experimentally validating both implementations in an emulated testbed using a simple distributed Deny-of-Service (DoS) detection application. △ Less","11 November, 2020",https://arxiv.org/pdf/2001.07670
AI Trust in business processes: The need for process-aware explanations,Steve T. K. Jan;Vatche Ishakian;Vinod Muthusamy,"Business processes underpin a large number of enterprise operations including processing loan applications, managing invoices, and insurance claims. There is a large opportunity for infusing AI to reduce cost or provide better customer experience, and the business process management (BPM) literature is rich in machine learning solutions including unsupervised learning to gain insights on clusters of process traces, classification models to predict the outcomes, duration, or paths of partial process traces, extracting business process from documents, and models to recommend how to optimize a business process or navigate decision points. More recently, deep learning models including those from the NLP domain have been applied to process predictions. Unfortunately, very little of these innovations have been applied and adopted by enterprise companies. We assert that a large reason for the lack of adoption of AI models in BPM is that business users are risk-averse and do not implicitly trust AI models. There has, unfortunately, been little attention paid to explaining model predictions to business users with process context. We challenge the BPM community to build on the AI interpretability literature, and the AI Trust community to understand △ Less","21 January, 2020",https://arxiv.org/pdf/2001.07537
The evolution of knowledge within and across fields in modern physics,Ye Sun;Vito Latora,"The exchange of knowledge across different areas and disciplines plays a key role in the process of knowledge creation, and can stimulate innovation and the emergence of new fields. We develop here a quantitative framework to extract significant dependencies among scientific disciplines and turn them into a time-varying network whose nodes are the different fields, while the weighted links represent the flow of knowledge from one field to another at a given period of time. Drawing on a comprehensive data set on scientific production in modern physics and on the patterns of citations between articles published in the various fields in the last thirty years, we are then able to map, over time, how the ideas developed in a given field in a certain time period have influenced later discoveries in the same field or in other fields. The analysis of knowledge flows internal to each field displays a remarkable variety of temporal behaviours, with some fields of physics showing to be more self-referential than others. The temporal networks of knowledge exchanges across fields reveal cases of one field continuously absorbing knowledge from another field in the entire observed period, pairs of fields mutually influencing each other, but also cases of evolution from absorbing to mutual or even to back-nurture behaviors. △ Less","20 January, 2020",https://arxiv.org/pdf/2001.07199
Cyber Attack Detection thanks to Machine Learning Algorithms,Antoine Delplace;Sheryl Hermoso;Kristofer Anandita,"Cybersecurity attacks are growing both in frequency and sophistication over the years. This increasing sophistication and complexity call for more advancement and continuous innovation in defensive strategies. Traditional methods of intrusion detection and deep packet inspection, while still largely used and recommended, are no longer sufficient to meet the demands of growing security threats. As computing power increases and cost drops, Machine Learning is seen as an alternative method or an additional mechanism to defend against malwares, botnets, and other attacks. This paper explores Machine Learning as a viable solution by examining its capabilities to classify malicious traffic in a network. First, a strong data analysis is performed resulting in 22 extracted features from the initial Netflow datasets. All these features are then compared with one another through a feature selection process. Then, our approach analyzes five different machine learning algorithms against NetFlow dataset containing common botnets. The Random Forest Classifier succeeds in detecting more than 95% of the botnets in 8 out of 13 scenarios and more than 55% in the most difficult datasets. Finally, insight is given to improve and generalize the results, especially through a bootstrapping technique. △ Less","17 January, 2020",https://arxiv.org/pdf/2001.06309
Two-Phase Object-Based Deep Learning for Multi-temporal SAR Image Change Detection,Xinzheng Zhang;Guo Liu;Ce Zhang;Peter M Atkinson;Xiaoheng Tan;Xin Jian;Xichuan Zhou;Yongming Li,"Change detection is one of the fundamental applications of synthetic aperture radar (SAR) images. However, speckle noise presented in SAR images has a much negative effect on change detection. In this research, a novel two-phase object-based deep learning approach is proposed for multi-temporal SAR image change detection. Compared with traditional methods, the proposed approach brings two main innovations. One is to classify all pixels into three categories rather than two categories: unchanged pixels, changed pixels caused by strong speckle (false changes), and changed pixels formed by real terrain variation (real changes). The other is to group neighboring pixels into segmented into superpixel objects (from pixels) such as to exploit local spatial context. Two phases are designed in the methodology: 1) Generate objects based on the simple linear iterative clustering algorithm, and discriminate these objects into changed and unchanged classes using fuzzy c-means (FCM) clustering and a deep PCANet. The prediction of this Phase is the set of changed and unchanged superpixels. 2) Deep learning on the pixel sets over the changed superpixels only, obtained in the first phase, to discriminate real changes from false changes. SLIC is employed again to achieve new superpixels in the second phase. Low rank and sparse decomposition are applied to these new superpixels to suppress speckle noise significantly. A further clustering step is applied to these new superpixels via FCM. A new PCANet is then trained to classify two kinds of changed superpixels to achieve the final change maps. Numerical experiments demonstrate that, compared with benchmark methods, the proposed approach can distinguish real changes from false changes effectively with significantly reduced false alarm rates, and achieve up to 99.71% change detection accuracy using multi-temporal SAR imagery. △ Less","17 January, 2020",https://arxiv.org/pdf/2001.06252
Weakly Supervised Video Summarization by Hierarchical Reinforcement Learning,Yiyan Chen;Li Tao;Xueting Wang;Toshihiko Yamasaki,"Conventional video summarization approaches based on reinforcement learning have the problem that the reward can only be received after the whole summary is generated. Such kind of reward is sparse and it makes reinforcement learning hard to converge. Another problem is that labelling each frame is tedious and costly, which usually prohibits the construction of large-scale datasets. To solve these problems, we propose a weakly supervised hierarchical reinforcement learning framework, which decomposes the whole task into several subtasks to enhance the summarization quality. This framework consists of a manager network and a worker network. For each subtask, the manager is trained to set a subgoal only by a task-level binary label, which requires much fewer labels than conventional approaches. With the guide of the subgoal, the worker predicts the importance scores for video frames in the subtask by policy gradient according to both global reward and innovative defined sub-rewards to overcome the sparse problem. Experiments on two benchmark datasets show that our proposal has achieved the best performance, even better than supervised approaches. △ Less","29 February, 2020",https://arxiv.org/pdf/2001.05864
FGN: Fusion Glyph Network for Chinese Named Entity Recognition,Zhenyu Xuan;Rui Bao;Shengyi Jiang,"Chinese NER is a challenging task. As pictographs, Chinese characters contain latent glyph information, which is often overlooked. In this paper, we propose the FGN, Fusion Glyph Network for Chinese NER. Except for adding glyph information, this method may also add extra interactive information with the fusion mechanism. The major innovations of FGN include: (1) a novel CNN structure called CGS-CNN is proposed to capture both glyph information and interactive information between glyphs from neighboring characters. (2) we provide a method with sliding window and Slice-Attention to fuse the BERT representation and glyph representation for a character, which may capture potential interactive knowledge between context and glyph. Experiments are conducted on four NER datasets, showing that FGN with LSTM-CRF as tagger achieves new state-of-the-arts performance for Chinese NER. Further, more experiments are conducted to investigate the influences of various components and settings in FGN. △ Less","8 October, 2020",https://arxiv.org/pdf/2001.05272
Direct Visual-Inertial Ego-Motion Estimation via Iterated Extended Kalman Filter,Shangkun Zhong;Pakpong Chirarattananon,"This letter proposes a reactive navigation strategy for recovering the altitude, translational velocity and orientation of Micro Aerial Vehicles. The main contribution lies in the direct and tight fusion of Inertial Measurement Unit (IMU) measurements with monocular feedback under an assumption of a single planar scene. An Iterated Extended Kalman Filter (IEKF) scheme is employed. The state prediction makes use of IMU readings while the state update relies directly on photometric feedback as measurements. Unlike feature-based methods, the photometric difference for the innovation term renders an inherent and robust data association process in a single step. The proposed approach is validated using real-world datasets. The results show that the proposed method offers better robustness, accuracy, and efficiency than a feature-based approach. Further investigation suggests that the accuracy of the flight velocity estimates from the proposed approach is comparable to those of two state-of-the-art Visual Inertial Systems (VINS) while the proposed framework is \approx15-30 times faster thanks to the omission of reconstruction and mapping. △ Less","15 January, 2020",https://arxiv.org/pdf/2001.05215
"Disseminating Research News in HCI: Perceived Hazards, How-To's, and Opportunities for Innovation",C. Estelle Smith;Eduardo Nevarez;Haiyi Zhu,"Mass media afford researchers critical opportunities to disseminate research findings and trends to the general public. Yet researchers also perceive that their work can be miscommunicated in mass media, thus generating unintended understandings of HCI research by the general public. We conduct a Grounded Theory analysis of interviews with 12 HCI researchers and find that miscommunication can occur at four origins along the socio-technical infrastructure known as the Media Production Pipeline (MPP) for science news. Results yield researchers' perceived hazards of disseminating their work through mass media, as well as strategies for fostering effective communication of research. We conclude with implications for augmenting or innovating new MPP technologies. △ Less","14 January, 2020",https://arxiv.org/pdf/2001.04883
A Bayesian Filter for Multi-view 3D Multi-object Tracking with Occlusion Handling,Jonah Ong;Ba Tuong Vo;Ba Ngu Vo;Du Yong Kim;Sven Nordholm,"This paper proposes an online multi-camera multi-object tracker that only requires monocular detector training, independent of the multi-camera configurations, allowing seamless extension/deletion of cameras without retraining effort. The proposed algorithm has a linear complexity in the total number of detections across the cameras, and hence scales gracefully with the number of cameras. It operates in the 3D world frame, and provides 3D trajectory estimates of the objects. The key innovation is a high fidelity yet tractable 3D occlusion model, amenable to optimal Bayesian multi-view multi-object filtering, which seamlessly integrates, into a single Bayesian recursion, the sub-tasks of track management, state estimation, clutter rejection, and occlusion/misdetection handling. The proposed algorithm is evaluated on the latest WILDTRACKS dataset, and demonstrated to work in very crowded scenes on a new dataset. △ Less","27 October, 2020",https://arxiv.org/pdf/2001.04118
ReluDiff: Differential Verification of Deep Neural Networks,Brandon Paulsen;Jingbo Wang;Chao Wang,"As deep neural networks are increasingly being deployed in practice, their efficiency has become an important issue. While there are compression techniques for reducing the network's size, energy consumption and computational requirement, they only demonstrate empirically that there is no loss of accuracy, but lack formal guarantees of the compressed network, e.g., in the presence of adversarial examples. Existing verification techniques such as Reluplex, ReluVal, and DeepPoly provide formal guarantees, but they are designed for analyzing a single network instead of the relationship between two networks. To fill the gap, we develop a new method for differential verification of two closely related networks. Our method consists of a fast but approximate forward interval analysis pass followed by a backward pass that iteratively refines the approximation until the desired property is verified. We have two main innovations. During the forward pass, we exploit structural and behavioral similarities of the two networks to more accurately bound the difference between the output neurons of the two networks. Then in the backward pass, we leverage the gradient differences to more accurately compute the most beneficial refinement. Our experiments show that, compared to state-of-the-art verification tools, our method can achieve orders-of-magnitude speedup and prove many more properties than existing tools. △ Less","29 January, 2020",https://arxiv.org/pdf/2001.03662
Decentralized Optimization of Vehicle Route Planning -- A Cross-City Comparative Study,Brionna Davis;Grace Jennings;Taylor Pothast;Ilias Gerostathopoulos;Evangelos Pournaras;Raphael E. Stern,"New mobility concepts are at the forefront of research and innovation in smart cities. The introduction of connected and autonomous vehicles enables new possibilities in vehicle routing. Specifically, knowing the origin and destination of each agent in the network can allow for real-time routing of the vehicles to optimize network performance. However, this relies on individual vehicles being ""altruistic"" i.e., being willing to accept an alternative non-preferred route in order to achieve a network-level performance goal. In this work, we conduct a study to compare different levels of agent altruism and the resulting effect on the network-level traffic performance. Specifically, this study compares the effects of different underlying urban structures on the overall network performance, and investigates which characteristics of the network make it possible to realize routing improvements using a decentralized optimization router. The main finding is that, with increased vehicle altruism, it is possible to balance traffic flow among the links of the network. We show evidence that the decentralized optimization router is more effective with networks of high load while we study the influence of cities characteristics, in particular: networks with a higher number of nodes (intersections) or edges (roads) per unit area allow for more possible alternate routes, and thus higher potential to improve network performance. △ Less","10 January, 2020",https://arxiv.org/pdf/2001.03384
A3: An Automatic Topology-Aware Malfunction Detection and Fixation System in Data Center Networks,Che Zhang;Shiwei Zhang;Bo Jin;Weichao Li;Zhen Wang;Qing Li;Yi Wang,"Link failures and cable miswirings are not uncommon in building data center networks, which prevents the existing automatic address configuration methods from functioning correctly. However, accurately detecting such malfunctions is not an easy task because there could be no observable node degree changes. Fixing or correcting such malfunctions is even harder as almost no work can provide accurate fixation suggestions now. To solve the problems, we design and implement A3, an automatic topology-aware malfunction detection and fixation system. A3 innovatively formulates the problem of finding minimal fixation to the problem of computing minimum graph difference (NP-hard) and solves it in O(k^6) and O(k^3) for any less than k/2 and k/4 undirected link malfunctions for FatTree, respectively. Our evaluation demonstrates that for less than k/2 undirected link malfunctions, A3 is 100% accurate for malfunction detection and provides the minimum fixation result. For greater or equal to k/2 undirected link malfunctions, A3 still has accuracy of about 100% and provides the near optimal fixation result. △ Less","7 January, 2020",https://arxiv.org/pdf/2001.02163
An adaptive data-driven approach to solve real-world vehicle routing problems in logistics,Emir Zunic;Dzenana Donko;Emir Buza,"Transportation occupies one-third of the amount in the logistics costs, and accordingly transportation systems largely influence the performance of the logistics system. This work presents an adaptive data-driven innovative modular approach for solving the real-world Vehicle Routing Problems (VRP) in the field of logistics. The work consists of two basic units: (i) an innovative multi-step algorithm for successful and entirely feasible solving of the VRP problems in logistics, (ii) an adaptive approach for adjusting and setting up parameters and constants of the proposed algorithm. The proposed algorithm combines several data transformation approaches, heuristics and Tabu search. Moreover, as the performance of the algorithm depends on the set of control parameters and constants, a predictive model that adaptively adjusts these parameters and constants according to historical data is proposed. A comparison of the acquired results has been made using the Decision Support System with predictive models: Generalized Linear Models (GLM) and Support Vector Machine (SVM). The algorithm, along with the control parameters, which using the prediction method were acquired, was incorporated into a web-based enterprise system, which is in use in several big distribution companies in Bosnia and Herzegovina. The results of the proposed algorithm were compared with a set of benchmark instances and validated over real benchmark instances as well. The successful feasibility of the given routes, in a real environment, is also presented. △ Less","5 January, 2020",https://arxiv.org/pdf/2001.02094
Universal Wait-Free Memory Reclamation,Ruslan Nikolaev;Binoy Ravindran,"In this paper, we present a universal memory reclamation scheme, Wait-Free Eras (WFE), for deleted memory blocks in wait-free concurrent data structures. WFE's key innovation is that it is completely wait-free. Although some prior techniques provide similar guarantees for certain data structures, they lack support for arbitrary wait-free data structures. Consequently, developers are typically forced to marry their wait-free data structures with lock-free Hazard Pointers or (potentially blocking) epoch-based memory reclamation. Since both these schemes provide weaker progress guarantees, they essentially forfeit the strong progress guarantee of wait-free data structures. Though making the original Hazard Pointers scheme or epoch-based reclamation completely wait-free seems infeasible, we achieved this goal with a more recent, (lock-free) Hazard Eras scheme, which we extend to guarantee wait-freedom. As this extension is non-trivial, we discuss all challenges pertaining to the construction of universal wait-free memory reclamation. WFE is implementable on ubiquitous x86_64 and AArch64 (ARM) architectures. Its API is mostly compatible with Hazard Pointers, which allows easy transitioning of existing data structures into WFE. Our experimental evaluations show that WFE's performance is close to epoch-based reclamation and almost matches the original Hazard Eras scheme, while providing the stronger wait-free progress guarantee. △ Less","11 January, 2020",https://arxiv.org/pdf/2001.01999
Effective Scaling of Blockchain Beyond Consensus Innovations and Moore's Law,Yinqiu Liu;Kai Qian;Jianli Chen;Kun Wang;Lei He,"As an emerging technology, blockchain has achieved great success in numerous application scenarios, from intelligent healthcare to smart cities. However, a long-standing bottleneck hindering its further development is the massive resource consumption attributed to the distributed storage and computation methods. This makes blockchain suffer from insufficient performance and poor scalability. Here, we analyze the recent blockchain techniques and demonstrate that the potential of widely-adopted consensus-based scaling is seriously limited, especially in the current era when Moore's law-based hardware scaling is about to end. We achieve this by developing an open-source benchmarking tool, called Prism, for investigating the key factors causing low resource efficiency and then discuss various topology and hardware innovations which could help to scale up blockchain. To the best of our knowledge, this is the first in-depth study that explores the next-generation scaling strategies by conducting large-scale and comprehensive benchmarking. △ Less","20 April, 2020",https://arxiv.org/pdf/2001.01865
A Conceptual Paper on SERVQUAL-Framework for Assessing Quality of Internet of Things (IoT) Services,Sheikh Muhammad Hizam;Waqas Ahmed,"Service quality possesses the vital prominence in usability of innovative products and services. As technological innovation has made the life synchronized and effective, Internet of Things (IoT) is matter of discussion everywhere. From users' perspective, IoT services are always embraced by various system characteristics of security and performance. A service quality model can better present the preference of such technology customers. the study intends to project theoretical model of service quality for internet of things (IoT). Based on the existing models of service quality and the literature in internet of things, a framework is proposed to conceptualize and measure service quality for internet of things.This study established the IoT-Servqual model with four dimensions (i.e., Privacy, Functionality, Efficiency, and Tangibility) of multiple service quality models. These dimensions are essential and inclined towards the users' leaning of IoT Services. This paper contributes to research on internet of things services by development of a comprehensive framework for customers' quality apprehension. This model will previse the expression of information secrecy of users related with internet of things (IoT). This research will advance understanding of service quality in modern day technology and assist firms to devise the fruitful service structure. △ Less","6 January, 2020",https://arxiv.org/pdf/2001.01840
Unleashing the Potentials of Immersive Augmented Reality for Software Engineering,Leonel Merino;Mircea Lungu;Christoph Seidl,"In immersive augmented reality (IAR), users can wear a head-mounted display to see computer-generated images superimposed to their view of the world. IAR was shown to be beneficial across several domains, e.g., automotive, medicine, gaming and engineering, with positive impacts on, e.g., collaboration and communication. We think that IAR bears a great potential for software engineering but, as of yet, this research area has been neglected. In this vision paper, we elicit potentials and obstacles for the use of IAR in software engineering. We identify possible areas that can be supported with IAR technology by relating commonly discussed IAR improvements to typical software engineering tasks. We further demonstrate how innovative use of IAR technology may fundamentally improve typical activities of a software engineer through a comprehensive series of usage scenarios outlining practical application. Finally, we reflect on current limitations of IAR technology based on our scenarios and sketch research activities necessary to make our vision a reality. We consider this paper to be relevant to academia and industry alike in guiding the steps to innovative research and applications for IAR in software engineering. △ Less","5 January, 2020",https://arxiv.org/pdf/2001.01223
Social Science Guided Feature Engineering: A Novel Approach to Signed Link Analysis,Ghazaleh Beigi;Jiliang Tang;Huan Liu,"Many real-world relations can be represented by signed networks with positive links (e.g., friendships and trust) and negative links (e.g., foes and distrust). Link prediction helps advance tasks in social network analysis such as recommendation systems. Most existing work on link analysis focuses on unsigned social networks. The existence of negative links piques research interests in investigating whether properties and principles of signed networks differ from those of unsigned networks, and mandates dedicated efforts on link analysis for signed social networks. Recent findings suggest that properties of signed networks substantially differ from those of unsigned networks and negative links can be of significant help in signed link analysis in complementary ways. In this article, we center our discussion on a challenging problem of signed link analysis. Signed link analysis faces the problem of data sparsity, i.e. only a small percentage of signed links are given. This problem can even get worse when negative links are much sparser than positive ones as users are inclined more towards positive disposition rather than negative. We investigate how we can take advantage of other sources of information for signed link analysis. This research is mainly guided by three social science theories, Emotional Information, Diffusion of Innovations, and Individual Personality. Guided by these, we extract three categories of related features and leverage them for signed link analysis. Experiments show the significance of the features gleaned from social theories for signed link prediction and addressing the data sparsity challenge. △ Less","3 January, 2020",https://arxiv.org/pdf/2001.01015
Simulating Ankle Torque during Walking Using a new Bioinspired Muscle Model with Application for Controlling a Powered Exoskeleton,Safoura Sadegh Pour Aji Bishe;Dan Rivera;Katherine Strausser;Zachary Lerner;Kiisa Nishikawa,"Human-like motion is a primary goal for many robotic assistive devices. Emulating the strategy of the human neuromuscular system may aid the control of such powered devices, yet many challenges remain. In this study, we investigated the potential for using the winding filament model (WFM) of muscle to predict the net muscle moment of the ankle. The long-term goal is to use this model to improve ankle control of a commercial powered exoskeleton. The innovation aspects of this study are: First, there have been no commercialized active ankle exoskeletons available in the market. All the available exoskeletons have passive ankle joints, which cannot mimic human movement, especially in normal and fast walking [1]. Second, the Winding Filament Model Controller (WFMC) is the first control strategy based on a muscle model that does not use an electromyographic (EMG) signal as an input. The activation, which is calculated from EMG, is a crucial input parameter for almost all of the control strategies based on muscle modeling. However, the winding filament muscle model can predict muscle force by using muscle length as the primary input, and the activation input signal could be either a square wave [33] or a simple bell shape function like our study. This is one of the most important benefits of the WFMC strategy, since it makes this bioinspired strategy applicable for all patient populations, even those with impaired muscle activities or without muscle activities (e.g. stroke, spinal cord injury, Parkinson disease, etc.). Third, the WFMC is adaptive to different tasks like walking at different speeds, as well as walking over the incline and over stairs. △ Less","3 January, 2020",https://arxiv.org/pdf/2001.01011
A Coarse-to-Fine Adaptive Network for Appearance-Based Gaze Estimation,Yihua Cheng;Shiyao Huang;Fei Wang;Chen Qian;Feng Lu,"Human gaze is essential for various appealing applications. Aiming at more accurate gaze estimation, a series of recent works propose to utilize face and eye images simultaneously. Nevertheless, face and eye images only serve as independent or parallel feature sources in those works, the intrinsic correlation between their features is overlooked. In this paper we make the following contributions: 1) We propose a coarse-to-fine strategy which estimates a basic gaze direction from face image and refines it with corresponding residual predicted from eye images. 2) Guided by the proposed strategy, we design a framework which introduces a bi-gram model to bridge gaze residual and basic gaze direction, and an attention component to adaptively acquire suitable fine-grained feature. 3) Integrating the above innovations, we construct a coarse-to-fine adaptive network named CA-Net and achieve state-of-the-art performances on MPIIGaze and EyeDiap. △ Less","1 January, 2020",https://arxiv.org/pdf/2001.00187
Single-Bit Consensus with Finite-Time Convergence: Theory and Applications,Mohammadreza Doostmohammadian,"In this brief paper, a new consensus protocol based on the sign of innovations is proposed. Based on this protocol each agent only requires single-bit of information about its relative state to its neighboring agents. This is significant in real-time applications, since it requires less computation and/or communication load on agents. Using Lyapunov stability theorem the convergence is proved for networks having a spanning tree. Further, the convergence is shown to be in finite-time, which is significant as compared to most asymptotic protocols in the literature. Time-variant network topologies are also considered in this paper, and final consensus value is derived for undirected networks. Applications of the proposed consensus protocol in (i) 2D/3D rendezvous task, (ii) distributed estimation, (iii) distributed optimization, and (iv) formation control are considered and significance of applying this protocol is discussed. Numerical simulations are provided to compare the protocol with the existing protocols in the literature. △ Less","1 January, 2020",https://arxiv.org/pdf/2001.00141
backbone: An R Package for extracting the backbone of bipartite projections,Rachel Domagalski;Zachary Neal;Bruce Sagan,"Bipartite projections are used in a wide range of network contexts including politics (bill co-sponsorship), genetics (gene co-expression), economics (executive board co-membership), and innovation (patent co-authorship). However, because bipartite projections are always weighted graphs, which are inherently challenging to analyze and visualize, it is often useful to examine the 'backbone', an unweighted subgraph containing only the most significant edges. In this paper, we introduce the R package backbone for extracting the backbone of weighted bipartite projections, and use bill sponsorship data from the 114th session of the United States Senate to demonstrate its functionality. △ Less","14 December, 2020",https://arxiv.org/pdf/1912.12779
EVA: An Encrypted Vector Arithmetic Language and Compiler for Efficient Homomorphic Computation,Roshan Dathathri;Blagovesta Kostova;Olli Saarikivi;Wei Dai;Kim Laine;Madanlal Musuvathi,"Fully-Homomorphic Encryption (FHE) offers powerful capabilities by enabling secure offloading of both storage and computation, and recent innovations in schemes and implementations have made it all the more attractive. At the same time, FHE is notoriously hard to use with a very constrained programming model, a very unusual performance profile, and many cryptographic constraints. Existing compilers for FHE either target simpler but less efficient FHE schemes or only support specific domains where they can rely on expert-provided high-level runtimes to hide complications. This paper presents a new FHE language called Encrypted Vector Arithmetic (EVA), which includes an optimizing compiler that generates correct and secure FHE programs, while hiding all the complexities of the target FHE scheme. Bolstered by our optimizing compiler, programmers can develop efficient general-purpose FHE applications directly in EVA. For example, we have developed image processing applications using EVA, with a very few lines of code. EVA is designed to also work as an intermediate representation that can be a target for compiling higher-level domain-specific languages. To demonstrate this, we have re-targeted CHET, an existing domain-specific compiler for neural network inference, onto EVA. Due to the novel optimizations in EVA, its programs are on average 5.3x faster than those generated by CHET. We believe that EVA would enable a wider adoption of FHE by making it easier to develop FHE applications and domain-specific FHE compilers. △ Less","26 June, 2020",https://arxiv.org/pdf/1912.11951
The Windfall Clause: Distributing the Benefits of AI for the Common Good,Cullen O'Keefe;Peter Cihon;Ben Garfinkel;Carrick Flynn;Jade Leung;Allan Dafoe,"As the transformative potential of AI has become increasingly salient as a matter of public and political interest, there has been growing discussion about the need to ensure that AI broadly benefits humanity. This in turn has spurred debate on the social responsibilities of large technology companies to serve the interests of society at large. In response, ethical principles and codes of conduct have been proposed to meet the escalating demand for this responsibility to be taken seriously. As yet, however, few institutional innovations have been suggested to translate this responsibility into legal commitments which apply to companies positioned to reap large financial gains from the development and use of AI. This paper offers one potentially attractive tool for addressing such issues: the Windfall Clause, which is an ex ante commitment by AI firms to donate a significant amount of any eventual extremely large profits. By this we mean an early commitment that profits that a firm could not earn without achieving fundamental, economically transformative breakthroughs in AI capabilities will be donated to benefit humanity broadly, with particular attention towards mitigating any downsides from deployment of windfall-generating AI. △ Less","24 January, 2020",https://arxiv.org/pdf/1912.11595
Mining the Automotive Industry: A Network Analysis of Corporate Positioning and Technological Trends,Niklas Stoehr;Fabian Braesemann;Michael Frommelt;Shi Zhou,"The digital transformation is driving revolutionary innovations and new market entrants threaten established sectors of the economy such as the automotive industry. Following the need for monitoring shifting industries, we present a network-centred analysis of car manufacturer web pages. Solely exploiting publicly-available information, we construct large networks from web pages and hyperlinks. The network properties disclose the internal corporate positioning of the three largest automotive manufacturers, Toyota, Volkswagen and Hyundai with respect to innovative trends and their international outlook. We tag web pages concerned with topics like e-mobility and environment or autonomous driving, and investigate their relevance in the network. Sentiment analysis on individual web pages uncovers a relationship between page linking and use of positive language, particularly with respect to innovative trends. Web pages of the same country domain form clusters of different size in the network that reveal strong correlations with sales market orientation. Our approach maintains the web content's hierarchical structure imposed by the web page networks. It, thus, presents a method to reveal hierarchical structures of unstructured text content obtained from web scraping. It is highly transparent, reproducible and data driven, and could be used to gain complementary insights into innovative strategies of firms and competitive landscapes, which would not be detectable by the analysis of web content alone. △ Less","8 January, 2020",https://arxiv.org/pdf/1912.10097
LEO Small-Satellite Constellations for 5G and Beyond-5G Communications,Israel Leyva-Mayorga;Beatriz Soret;Maik Röper;Dirk Wübben;Bho Matthiesen;Armin Dekorsy;Petar Popovski,"The next frontier towards truly ubiquitous connectivity is the use of Low Earth Orbit (LEO) small-satellite constellations to support 5G and Beyond-5G (B5G) networks. Besides enhanced mobile broadband (eMBB) and massive machine-type communications (mMTC), LEO constellations can support ultra-reliable communications (URC) with relaxed latency requirements of a few tens of milliseconds. Small-satellite impairments and the use of low orbits pose major challenges to the design and performance of these networks, but also open new innovation opportunities. This paper provides a comprehensive overview of the physical and logical links, along with the essential architectural and technological components that enable the full integration of LEO constellations into 5G and B5G systems. Furthermore, we characterize and compare each physical link category and explore novel techniques to maximize the achievable data rates. △ Less","7 October, 2020",https://arxiv.org/pdf/1912.08110
To See in the Dark: N2DGAN for Background Modeling in Nighttime Scene,Zhenfeng Zhu;Yingying Meng;Deqiang Kong;Xingxing Zhang;Yandong Guo;Yao Zhao,"Due to the deteriorated conditions of \mbox{illumination} lack and uneven lighting, nighttime images have lower contrast and higher noise than their daytime counterparts of the same scene, which limits seriously the performances of conventional background modeling methods. For such a challenging problem of background modeling under nighttime scene, an innovative and reasonable solution is proposed in this paper, which paves a new way completely different from the existing ones. To make background modeling under nighttime scene performs as well as in daytime condition, we put forward a promising generation-based background modeling framework for foreground surveillance. With a pre-specified daytime reference image as background frame, the {\bfseries GAN} based generation model, called {\bfseries N2DGAN}, is trained to transfer each frame of {\bfseries n}ighttime video {\bfseries to} a virtual {\bfseries d}aytime image with the same scene to the reference image except for the foreground region. Specifically, to balance the preservation of background scene and the foreground object(s) in generating the virtual daytime image, we present a two-pathway generation model, in which the global and local sub-networks are well combined with spatial and temporal consistency constraints. For the sequence of generated virtual daytime images, a multi-scale Bayes model is further proposed to characterize pertinently the temporal variation of background. We evaluate on collected datasets with manually labeled ground truth, which provides a valuable resource for related research community. The impressive results illustrated in both the main paper and supplementary show efficacy of our proposed approach. △ Less","12 April, 2020",https://arxiv.org/pdf/1912.06556
Multi-Agent Task Allocation in Complementary Teams: A Hunter and Gatherer Approach,Mehdi Dadvar;Saeed Moazami;Harley R. Myler;Hassan Zargarzadeh,"Consider a dynamic task allocation problem, where tasks are unknowingly distributed over an environment. This paper considers each task comprised of two sequential subtasks: detection and completion, where each subtask can only be carried out by a certain type of agent. We address this problem using a novel nature-inspired approach called ""hunter and gatherer"". The proposed method employs two complementary teams of agents: one agile in detecting (hunters) and another skillful in completing (gatherers) the tasks. To minimize the collective cost of task accomplishments in a distributed manner, a game-theoretic solution is introduced to couple agents from complementary teams. We utilize market-based negotiation models to develop incentive-based decision-making algorithms relying on innovative notions of ""certainty and uncertainty profit margins"". The simulation results demonstrate that employing two complementary teams of hunters and gatherers can effectually improve the number of tasks completed by agents compared to conventional methods, while the collective cost of accomplishments is minimized. In addition, the stability and efficacy of the proposed solutions are studied using Nash equilibrium analysis and statistical analysis respectively. It is also numerically shown that the proposed solutions function fairly, i.e. for each type of agent, the overall workload is distributed equally. △ Less","26 March, 2020",https://arxiv.org/pdf/1912.05748
Naive Gabor Networks for Hyperspectral Image Classification,Chenying Liu;Jun Li;Lin He;Antonio J. Plaza;Shutao Li;Bo Li,"Recently, many convolutional neural network (CNN) methods have been designed for hyperspectral image (HSI) classification since CNNs are able to produce good representations of data, which greatly benefits from a huge number of parameters. However, solving such a high-dimensional optimization problem often requires a large amount of training samples in order to avoid overfitting. Additionally, it is a typical non-convex problem affected by many local minima and flat regions. To address these problems, in this paper, we introduce naive Gabor Networks or Gabor-Nets which, for the first time in the literature, design and learn CNN kernels strictly in the form of Gabor filters, aiming to reduce the number of involved parameters and constrain the solution space, and hence improve the performances of CNNs. Specifically, we develop an innovative phase-induced Gabor kernel, which is trickily designed to perform the Gabor feature learning via a linear combination of local low-frequency and high-frequency components of data controlled by the kernel phase. With the phase-induced Gabor kernel, the proposed Gabor-Nets gains the ability to automatically adapt to the local harmonic characteristics of the HSI data and thus yields more representative harmonic features. Also, this kernel can fulfill the traditional complex-valued Gabor filtering in a real-valued manner, hence making Gabor-Nets easily perform in a usual CNN thread. We evaluated our newly developed Gabor-Nets on three well-known HSIs, suggesting that our proposed Gabor-Nets can significantly improve the performance of CNNs, particularly with a small training set. △ Less","24 February, 2020",https://arxiv.org/pdf/1912.03991
ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks,Mohit Shridhar;Jesse Thomason;Daniel Gordon;Yonatan Bisk;Winson Han;Roozbeh Mottaghi;Luke Zettlemoyer;Dieter Fox,"We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like ""Rinse off a mug and place it in the coffee maker."" and low-level language instructions like ""Walk to the coffee maker on the right."" ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision-and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark. △ Less","30 March, 2020",https://arxiv.org/pdf/1912.01734
Scene recognition based on DNN and game theory with its applications in human-robot interaction,R. Q. Wang;W. Z. Wang;D. Z. Zhao;G. H. Chen;D. S. Luo,"Scene recognition model based on the DNN and game theory with its applications in human-robot interaction is proposed in this paper. The use of deep learning methods in the field of scene recognition is still in its infancy, but has become an important trend in the future. As the innovative idea of the paper, we propose the following novelties. (1) In this paper, the image registration problem is transformed into a problem of minimum energy in Markov Random Field to finalize the image pre-processing task. Game theory is used to find the optimal. (2) We select neighboring homogeneous sample features and the neighboring heterogeneous sample features for the extracted sample features to build a triple and modify the traditional neural network to propose the novel DNN for scene understanding. (3) The robot control is well combined to guide the robot vision for multiple tasks. The experiment is then conducted to validate the overall performance. △ Less","10 January, 2020",https://arxiv.org/pdf/1912.01293
MnasFPN: Learning Latency-aware Pyramid Architecture for Object Detection on Mobile Devices,Bo Chen;Golnaz Ghiasi;Hanxiao Liu;Tsung-Yi Lin;Dmitry Kalenichenko;Hartwig Adams;Quoc V. Le,"Despite the blooming success of architecture search for vision tasks in resource-constrained environments, the design of on-device object detection architectures have mostly been manual. The few automated search efforts are either centered around non-mobile-friendly search spaces or not guided by on-device latency. We propose MnasFPN, a mobile-friendly search space for the detection head, and combine it with latency-aware architecture search to produce efficient object detection models. The learned MnasFPN head, when paired with MobileNetV2 body, outperforms MobileNetV3+SSDLite by 1.8 mAP at similar latency on Pixel. It is also both 1.0 mAP more accurate and 10% faster than NAS-FPNLite. Ablation studies show that the majority of the performance gain comes from innovations in the search space. Further explorations reveal an interesting coupling between the search space design and the search algorithm, and that the complexity of MnasFPN search space may be at a local optimum. △ Less","30 July, 2020",https://arxiv.org/pdf/1912.01106
The AI Liability Puzzle and A Fund-Based Work-Around,Olivia J. Erdélyi;Gábor Erdélyi,"Certainty around the regulatory environment is crucial to enable responsible AI innovation and foster the social acceptance of these powerful new technologies. One notable source of uncertainty is, however, that the existing legal liability system is inapt to assign responsibility where a potentially harmful conduct and/or the harm itself are unforeseeable, yet some instantiations of AI and/or the harms they may trigger are not foreseeable in the legal sense. The unpredictability of how courts would handle such cases makes the risks involved in the investment and use of AI incalculable, creating an environment that is not conducive to innovation and may deprive society of some of the benefits AI could provide. To tackle this problem, we propose to draw insights from financial regulatory best-practices and establish a system of AI guarantee schemes. We envisage the system to form part of the broader market-structuring regulatory framework, with the primary function to provide a readily available, clear, and transparent funding mechanism to compensate claims that are either extremely hard or impossible to realize via conventional litigation. We propose it to be at least partially industry-funded, with funding arrangements depending on whether it would pursue other potential policy goals. We aim to engage in a high-level, comparative conceptual debate around the suitability of the foreseeability concept to limit legal liability rather than confronting the intricacies of the case law of specific jurisdictions. Recognizing the importance of the latter task, we leave this to further research in support of the legal system's incremental adaptation to the novel challenges of present and future AI technologies. △ Less","19 May, 2020",https://arxiv.org/pdf/1911.08005
SMART: Skeletal Motion Action Recognition aTtack,He Wang;Feixiang He;Zhexi Peng;Yongliang Yang;Tianjia Shao;Kun Zhou;David Hogg,"Adversarial attack has inspired great interest in computer vision, by showing that classification-based solutions are prone to imperceptible attack in many tasks. In this paper, we propose a method, SMART, to attack action recognizers which rely on 3D skeletal motions. Our method involves an innovative perceptual loss which ensures the imperceptibility of the attack. Empirical studies demonstrate that SMART is effective in both white-box and black-box scenarios. Its generalizability is evidenced on a variety of action recognizers and datasets. Its versatility is shown in different attacking strategies. Its deceitfulness is proven in extensive perceptual studies. Finally, SMART shows that adversarial attack on 3D skeletal motion, one type of time-series data, is significantly different from traditional adversarial attack problems. △ Less","10 March, 2020",https://arxiv.org/pdf/1911.07107
Testing linear-invariant properties,Jonathan Tidor;Yufei Zhao,"Fix a prime p and a positive integer R. We study the property testing of functions \mathbb F_p^n\to[R]. We say that a property is testable if there exists an oblivious tester for this property with one-sided error and constant query complexity. Furthermore, a property is proximity oblivious-testable (PO-testable) if the test is also independent of the proximity parameter ε. It is known that a number of natural properties such as linearity and being a low degree polynomial are PO-testable. These properties are examples of linear-invariant properties, meaning that they are preserved under linear automorphisms of the domain. Following work of Kaufman and Sudan, the study of linear-invariant properties has been an important problem in arithmetic property testing. A central conjecture in this field, proposed by Bhattacharyya, Grigorescu, and Shapira, is that a linear-invariant property is testable if and only if it is semi subspace-hereditary. We prove two results, the first resolves this conjecture and the second classifies PO-testable properties. (1) A linear-invariant property is testable if and only if it is semi subspace-hereditary. (2) A linear-invariant property is PO-testable if and only if it is locally characterized. Our innovations are two-fold. We give a more powerful version of the compactness argument first introduced by Alon and Shapira. This relies on a new strong arithmetic regularity lemma in which one mixes different levels of Gowers uniformity. This allows us to extend the work of Bhattacharyya, Fischer, Hatami, Hatami, and Lovett by removing the bounded complexity restriction in their work. Our second innovation is a novel recoloring technique called patching. This Ramsey-theoretic technique is critical for working in the linear-invariant setting and allows us to remove the translation-invariant restriction present in previous work. △ Less","3 July, 2020",https://arxiv.org/pdf/1911.06793
"Capturing the Production of the Innovative Ideas: An Online Social Network Experiment and ""Idea Geography"" Visualization",Yiding Cao;Yingjun Dong;Minjun Kim;Neil G. MacLaren;Ankita Kulkarni;Shelley D. Dionne;Francis J. Yammarino;Hiroki Sayama,"Collective design and innovation are crucial in organizations. To investigate how the collective design and innovation processes would be affected by the diversity of knowledge and background of collective individual members, we conducted three collaborative design task experiments which involved nearly 300 participants who worked together anonymously in a social network structure using a custom-made computer-mediated collaboration platform. We compared the idea generation activity among three different background distribution conditions (clustered, random, and dispersed) with the help of the ""doc2vec"" text representation machine learning algorithm. We also developed a new method called ""Idea Geography"" to visualize the idea utility terrain on a 2D problem domain. The results showed that groups with random background allocation tended to produce the best design idea with highest utility values. It was also suggested that the diversity of participants' backgrounds distribution on the network might interact with each other to affect the diversity of ideas generated. The proposed idea geography successfully visualized that the collective design processes did find the high utility area through exploration and exploitation in collaborative work. △ Less","24 February, 2020",https://arxiv.org/pdf/1911.06353
Tensor Decomposition with Relational Constraints for Predicting Multiple Types of MicroRNA-disease Associations,Feng Huang;Xiang Yue;Zhankun Xiong;Zhouxin Yu;Wen Zhang,"MicroRNAs (miRNAs) play crucial roles in multifarious biological processes associated with human diseases. Identifying potential miRNA-disease associations contributes to understanding the molecular mechanisms of miRNA-related diseases. Most of the existing computational methods mainly focus on predicting whether a miRNA-disease association exists or not. However, the roles of miRNAs in diseases are prominently diverged, for instance, Genetic variants of microRNA (mir-15) may affect expression level of miRNAs leading to B cell chronic lymphocytic leukemia, while circulating miRNAs (including mir-1246, mir-1307-3p, etc.) have potentials to detecting breast cancer in the early stage. In this paper, we aim to predict multi-type miRNA-disease associations instead of taking them as binary. To this end, we innovatively represent miRNA-disease-type triplets as a tensor and introduce Tensor Decomposition methods to solve the prediction task. Experimental results on two widely-adopted miRNA-disease datasets: HMDD v2.0 and HMDD v3.2 show that tensor decomposition methods improve a recent baseline in a large scale (up to 38% in top-1 F1). We further propose a novel method, Tensor Decomposition with Relational Constraints (TDRC), which incorporates biological features as relational constraints to further the existing tensor decomposition methods. Compared with two existing tensor decomposition methods, TDRC can produce better performance while being more efficient. △ Less","9 March, 2020",https://arxiv.org/pdf/1911.05584
Blockchain for Future Smart Grid: A Comprehensive Survey,Muhammad Baqer Mollah;Jun Zhao;Dusit Niyato;Kwok-Yan Lam;Xin Zhang;Amer M. Y. M. Ghias;Leong Hai Koh;Lei Yang,"The concept of smart grid has been introduced as a new vision of the conventional power grid to figure out an efficient way of integrating green and renewable energy technologies. In this way, Internet-connected smart grid, also called energy Internet, is also emerging as an innovative approach to ensure the energy from anywhere at any time. The ultimate goal of these developments is to build a sustainable society. However, integrating and coordinating a large number of growing connections can be a challenging issue for the traditional centralized grid system. Consequently, the smart grid is undergoing a transformation to the decentralized topology from its centralized form. On the other hand, blockchain has some excellent features which make it a promising application for smart grid paradigm. In this paper, we aim to provide a comprehensive survey on application of blockchain in smart grid. As such, we identify the significant security challenges of smart grid scenarios that can be addressed by blockchain. Then, we present a number of blockchain-based recent research works presented in different literatures addressing security issues in the area of smart grid. We also summarize several related practical projects, trials, and products that have been emerged recently. Finally, we discuss essential research challenges and future directions of applying blockchain to smart grid security issues. △ Less","13 May, 2020",https://arxiv.org/pdf/1911.03298
Fast Polynomial Approximation of Heat Kernel Convolution on Manifolds and Its Application to Brain Sulcal and Gyral Graph Pattern Analysis,Shih-Gu Huang;Ilwoo Lyu;Anqi Qiu;Moo K. Chung,"Heat diffusion has been widely used in brain imaging for surface fairing, mesh regularization and cortical data smoothing. Motivated by diffusion wavelets and convolutional neural networks on graphs, we present a new fast and accurate numerical scheme to solve heat diffusion on surface meshes. This is achieved by approximating the heat kernel convolution using high degree orthogonal polynomials in the spectral domain. We also derive the closed-form expression of the spectral decomposition of the Laplace-Beltrami operator and use it to solve heat diffusion on a manifold for the first time. The proposed fast polynomial approximation scheme avoids solving for the eigenfunctions of the Laplace-Beltrami operator, which is computationally costly for large mesh size, and the numerical instability associated with the finite element method based diffusion solvers. The proposed method is applied in localizing the male and female differences in cortical sulcal and gyral graph patterns obtained from MRI in an innovative way. The MATLAB code is available at http://www.stat.wisc.edu/~mchung/chebyshev. △ Less","17 January, 2020",https://arxiv.org/pdf/1911.02721
Program Sketching with Live Bidirectional Evaluation,Justin Lubin;Nick Collins;Cyrus Omar;Ravi Chugh,"We present a system called Smyth for program sketching in a typed functional language whereby the concrete evaluation of ordinary assertions gives rise to input-output examples, which are then used to guide the search to complete the holes. The key innovation, called live bidirectional evaluation, propagates examples ""backward"" through partially evaluated sketches. Live bidirectional evaluation enables Smyth to (a) synthesize recursive functions without trace-complete sets of examples and (b) specify and solve interdependent synthesis goals. Eliminating the trace-completeness requirement resolves a significant limitation faced by prior synthesis techniques when given partial specifications in the form of input-output examples. To assess the practical implications of our techniques, we ran several experiments on benchmarks used to evaluate Myth, a state-of-the-art example-based synthesis tool. First, given expert examples (and no partial implementations), we find that Smyth requires on average 66% of the number of expert examples required by Myth. Second, we find that Smyth is robust to randomly-generated examples, synthesizing many tasks with relatively few more random examples than those provided by an expert. Third, we create a suite of small sketching tasks by systematically employing a simple sketching strategy to the Myth benchmarks; we find that user-provided sketches in Smyth often further reduce the total specification burden (i.e. the combination of partial implementations and examples). Lastly, we find that Leon and Synquid, two state-of-the-art logic-based synthesis tools, fail to complete several tasks on which Smyth succeeds. △ Less","12 July, 2020",https://arxiv.org/pdf/1911.00583
RTOP: A Conceptual and Computational Framework for General Intelligence,Shilpesh Garg,"A novel general intelligence model is proposed with three types of learning. A unified sequence of the foreground percept trace and the command trace translates into direct and time-hop observation paths to form the basis of Raw learning. Raw learning includes the formation of image-image associations, which lead to the perception of temporal and spatial relationships among objects and object parts; and the formation of image-audio associations, which serve as the building blocks of language. Offline identification of similar segments in the observation paths and their subsequent reduction into a common segment through merging of memory nodes leads to Generalized learning. Generalization includes the formation of interpolated sensory nodes for robust and generic matching, the formation of sensory properties nodes for specific matching and superimposition, and the formation of group nodes for simpler logic pathways. Online superimposition of memory nodes across multiple predictions, primarily the superimposition of images on the internal projection canvas, gives rise to Innovative learning and thought. The learning of actions happens the same way as raw learning while the action determination happens through the utility model built into the raw learnings, the utility function being the pleasure and pain of the physical senses. △ Less","11 January, 2020",https://arxiv.org/pdf/1910.10393
"Digital Democracy: Episode IV -- A New Hope, How a Corporation for Public Software Could Transform Digital Engagement for Government and Civil Society",John Gastil;Todd Davies,"Though successive generations of digital technology have become increasingly powerful in the past twenty years, digital democracy has yet to realize its potential for deliberative transformation. The undemocratic exploitation of massive social media systems continued this trend, but it only worsened an existing problem of modern democracies, which were already struggling to develop deliberative infrastructure independent of digital technologies. There have been many creative conceptions of civic tech, but implementation has lagged behind innovation. This essay argues for implementing one such vision of digital democracy through the establishment of a public corporation. Modeled on the Corporation for Public Broadcasting in the U.S., this entity would foster the creation of new digital technology by providing a stable source of funding to nonprofit technologists, interest groups, civic organizations, government, researchers, private companies, and the public. Funded entities would produce and maintain software infrastructure for public benefit. The concluding sections identify what circumstances might create and sustain such an entity. △ Less","3 March, 2020",https://arxiv.org/pdf/1910.08604
Intelligent Traffic Monitoring Systems for Vehicle Classification: A Survey,Myounggyu Won,"A traffic monitoring system is an integral part of Intelligent Transportation Systems (ITS). It is one of the critical transportation infrastructures that transportation agencies invest a huge amount of money to collect and analyze the traffic data to better utilize the roadway systems, improve the safety of transportation, and establish future transportation plans. With recent advances in MEMS, machine learning, and wireless communication technologies, numerous innovative traffic monitoring systems have been developed. In this article, we present a review of state-of-the-art traffic monitoring systems focusing on the major functionality--vehicle classification. We organize various vehicle classification systems, examine research issues and technical challenges, and discuss hardware/software design, deployment experience, and system performance of vehicle classification systems. Finally, we discuss a number of critical open problems and future research directions in an aim to provide valuable resources to academia, industry, and government agencies for selecting appropriate technologies for their traffic monitoring applications. △ Less","30 April, 2020",https://arxiv.org/pdf/1910.04656
Graph Few-shot Learning via Knowledge Transfer,Huaxiu Yao;Chuxu Zhang;Ying Wei;Meng Jiang;Suhang Wang;Junzhou Huang;Nitesh V. Chawla;Zhenhui Li,"Towards the challenging problem of semi-supervised node classification, there have been extensive studies. As a frontier, Graph Neural Networks (GNNs) have aroused great interest recently, which update the representation of each node by aggregating information of its neighbors. However, most GNNs have shallow layers with a limited receptive field and may not achieve satisfactory performance especially when the number of labeled nodes is quite small. To address this challenge, we innovatively propose a graph few-shot learning (GFL) algorithm that incorporates prior knowledge learned from auxiliary graphs to improve classification accuracy on the target graph. Specifically, a transferable metric space characterized by a node embedding and a graph-specific prototype embedding function is shared between auxiliary graphs and the target, facilitating the transfer of structural knowledge. Extensive experiments and ablation studies on four real-world graph datasets demonstrate the effectiveness of our proposed model. △ Less","11 May, 2020",https://arxiv.org/pdf/1910.03053
The role of bipartite structure in R&D collaboration networks,D. Vasques Filho;Dion R. J. O'Neale,"A number of real-world networks are, in fact, one-mode projections of bipartite networks comprised of two types of nodes. For institutions engaging in collaboration for technological innovation, the underlying network is bipartite with institutions (agents) linked to the patents they have filed (artifacts), while the projection is the co-patenting network. Projected network topology is highly affected by the underlying bipartite structure, hence a lack of understanding of the bipartite network has consequences for the information that might be drawn from the one-mode co-patenting network. Here, we create an empirical bipartite network using data from 2.7 million patents. We project this network onto the agents (institutions) and look at properties of both the bipartite and projected networks that may play a role in knowledge sharing and collaboration. We compare these empirical properties to those of synthetic bipartite networks and their projections in order to understand the processes that might operate in the network formation. A good understanding of the topology is critical for investigating the potential flow of technological knowledge. We show how degree distributions and small cycles affect the topology of the one-mode projected network - specifically degree and clustering distributions, and assortativity. We propose new network-based metrics to quantify how collaborative agents are in the co-patenting network. We find that several large corporations that are the most collaborative agents in the network, however such organisations tend to have a low diversity of collaborators. In contrast, the most prolific institutions tend to collaborate relatively little but with a diverse set of collaborators. This indicates that they concentrate the knowledge of their core technical research, while seeking specific complementary knowledge via collaboration with smaller companies. △ Less","4 May, 2020",https://arxiv.org/pdf/1909.10977
Data Centers Job Scheduling with Deep Reinforcement Learning,Sisheng Liang;Zhou Yang;Fang Jin;Yong Chen,"Efficient job scheduling on data centers under heterogeneous complexity is crucial but challenging since it involves the allocation of multi-dimensional resources over time and space. To adapt the complex computing environment in data centers, we proposed an innovative Advantage Actor-Critic (A2C) deep reinforcement learning based approach called A2cScheduler for job scheduling. A2cScheduler consists of two agents, one of which, dubbed the actor, is responsible for learning the scheduling policy automatically and the other one, the critic, reduces the estimation error. Unlike previous policy gradient approaches, A2cScheduler is designed to reduce the gradient estimation variance and to update parameters efficiently. We show that the A2cScheduler can achieve competitive scheduling performance using both simulated workloads and real data collected from an academic data center. △ Less","1 March, 2020",https://arxiv.org/pdf/1909.07820
Psi-Calculi Revisited: Connectivity and Compositionality,Johannes Åman Pohjola,"Psi-calculi is a parametric framework for process calculi similar to popular pi-calculus extensions such as the explicit fusion calculus, the applied pi-calculus and the spi calculus. Mechanised proofs of standard algebraic and congruence properties of bisimilarity apply to all calculi within the framework. A limitation of psi-calculi is that communication channels must be symmetric and transitive. In this paper, we give a new operational semantics to psi-calculi that allows us to lift these restrictions and simplify some of the proofs. The key technical innovation is to annotate transitions with a provenance -- a description of the scope and channel they originate from. We give mechanised proofs that our extension is conservative, and that the standard algebraic and congruence properties of strong and weak bisimilarity are maintained. We show correspondence with a reduction semantics and barbed bisimulation. We show how a pi-calculus with preorders that was previously beyond the scope of psi-calculi can be captured, and how to encode mixed choice under very strong quality criteria. △ Less","14 December, 2020",https://arxiv.org/pdf/1909.06692
Selfie Drone Stick: A Natural Interface for Quadcopter Photography,Saif Alabachi;Gita Sukthankar;Rahul Sukthankar,"A physical selfie stick extends the user's reach, enabling the acquisition of personal photos that include more of the background scene. Similarly, a quadcopter can capture photos from vantage points unattainable by the user; but teleoperating a quadcopter to good viewpoints is a difficult task. This paper presents a natural interface for quadcopter photography, the SelfieDroneStick that allows the user to guide the quadcopter to the optimal vantage point based on the phone's sensors. Users specify the composition of their desired long-range selfies using their smartphone, and the quadcopter autonomously flies to a sequence of vantage points from where the desired shots can be taken. The robot controller is trained from a combination of real-world images and simulated flight data. This paper describes two key innovations required to deploy deep reinforcement learning models on a real robot: 1) an abstract state representation for transferring learning from simulation to the hardware platform, and 2) reward shaping and staging paradigms for training the controller. Both of these improvements were found to be essential in learning a robot controller from simulation that transfers successfully to the real robot. △ Less","18 August, 2020",https://arxiv.org/pdf/1909.06491
Adaptive Graph Representation Learning for Video Person Re-identification,Yiming Wu;Omar El Farouk Bourahla;Xi Li;Fei Wu;Qi Tian;Xue Zhou,"Recent years have witnessed the remarkable progress of applying deep learning models in video person re-identification (Re-ID). A key factor for video person Re-ID is to effectively construct discriminative and robust video feature representations for many complicated situations. Part-based approaches employ spatial and temporal attention to extract representative local features. While correlations between parts are ignored in the previous methods, to leverage the relations of different parts, we propose an innovative adaptive graph representation learning scheme for video person Re-ID, which enables the contextual interactions between relevant regional features. Specifically, we exploit the pose alignment connection and the feature affinity connection to construct an adaptive structure-aware adjacency graph, which models the intrinsic relations between graph nodes. We perform feature propagation on the adjacency graph to refine regional features iteratively, and the neighbor nodes' information is taken into account for part feature representation. To learn compact and discriminative representations, we further propose a novel temporal resolution-aware regularization, which enforces the consistency among different temporal resolutions for the same identities. We conduct extensive evaluations on four benchmarks, i.e. iLIDS-VID, PRID2011, MARS, and DukeMTMC-VideoReID, experimental results achieve the competitive performance which demonstrates the effectiveness of our proposed method. The code is available at https://github.com/weleen/AGRL.pytorch. △ Less","11 June, 2020",https://arxiv.org/pdf/1909.02240
The Diversity-Innovation Paradox in Science,Bas Hofstra;Vivek V. Kulkarni;Sebastian Munoz-Najar Galvez;Bryan He;Dan Jurafsky;Daniel A. McFarland,"Prior work finds a diversity paradox: diversity breeds innovation, and yet, underrepresented groups that diversify organizations have less successful careers within them. Does the diversity paradox hold for scientists as well? We study this by utilizing a near-population of ~1.2 million US doctoral recipients from 1977-2015 and following their careers into publishing and faculty positions. We use text analysis and machine learning to answer a series of questions: How do we detect scientific innovations? Are underrepresented groups more likely to generate scientific innovations? And are the innovations of underrepresented groups adopted and rewarded? Our analyses show that underrepresented groups produce higher rates of scientific novelty. However, their novel contributions are devalued and discounted: e.g., novel contributions by gender and racial minorities are taken up by other scholars at lower rates than novel contributions by gender and racial majorities, and equally impactful contributions of gender and racial minorities are less likely to result in successful scientific careers than for majority groups. These results suggest there may be unwarranted reproduction of stratification in academic careers that discounts diversity's role in innovation and partly explains the underrepresentation of some groups in academia. △ Less","15 January, 2020",https://arxiv.org/pdf/1909.02063
State Drug Policy Effectiveness: Comparative Policy Analysis of Drug Overdose Mortality,Jarrod Olson;Po-Hsu Allen Chen;Marissa White;Nicole Brennan;Ning Gong,"Opioid overdose rates have reached an epidemic level and state-level policy innovations have followed suit in an effort to prevent overdose deaths. State-level drug law is a set of policies that may reinforce or undermine each other, and analysts have a limited set of tools for handling the policy collinearity using statistical methods. This paper uses a machine learning method called hierarchical clustering to empirically generate ""policy bundles"" by grouping states with similar sets of policies in force at a given time together for analysis in a 50-state, 10-year interrupted time series regression with drug overdose deaths as the dependent variable. Policy clusters were generated from 138 binomial variables observed by state and year from the Prescription Drug Abuse Policy System. Clustering reduced the policies to a set of 10 bundles. The approach allows for ranking of the relative effect of different bundles and is a tool to recommend those most likely to succeed. This study shows that a set of policies balancing Medication Assisted Treatment, Naloxone Access, Good Samaritan Laws, Medication Assisted Treatment, Prescription Drug Monitoring Programs and legalization of medical marijuana leads to a reduced number of overdose deaths, but not until its second year in force. △ Less","5 October, 2020",https://arxiv.org/pdf/1909.01936
Optimal Causal Rate-Constrained Sampling of the Wiener Process,Nian Guo;Victoria Kostina,"We consider the following communication scenario. An encoder causally observes the Wiener process and decides when and what to transmit about it. A decoder makes real-time estimation of the process using causally received codewords. We determine the causal encoding and decoding policies that jointly minimize the mean-square estimation error, under the long-term communication rate constraint of R bits per second. We show that an optimal encoding policy can be implemented as a causal sampling policy followed by a causal compressing policy. We prove that the optimal encoding policy samples the Wiener process once the innovation passes either \sqrt{\frac{1}{R}} or -\sqrt{\frac{1}{R}}, and compresses the sign of the innovation (SOI) using a 1-bit codeword. The SOI coding scheme achieves the operational distortion-rate function, which is equal to D^{\mathrm{op}}(R)=\frac{1}{6R}. Surprisingly, this is significantly better than the distortion-rate tradeoff achieved in the limit of infinite delay by the best non-causal code. This is because the SOI coding scheme leverages the free timing information supplied by the zero-delay channel between the encoder and the decoder. The key to unlock that gain is the event-triggered nature of the SOI sampling policy. In contrast, the distortion-rate tradeoffs achieved with deterministic sampling policies are much worse: we prove that the causal informational distortion-rate function in that scenario is as high as D_{\mathrm{DET}}(R) = \frac{5}{6R}. It is achieved by the uniform sampling policy with the sampling interval \frac{1}{R}. In either case, the optimal strategy is to sample the process as fast as possible and to transmit 1-bit codewords to the decoder without delay. △ Less","13 May, 2020",https://arxiv.org/pdf/1909.01317
Robust Online Video Super-Resolution Using an Efficient Alternating Projections Scheme,Ricardo Augusto Borsoi,"Video super-resolution reconstruction (SRR) algorithms attempt to reconstruct high-resolution (HR) video sequences from low-resolution observations. Although recent progress in video SRR has significantly improved the quality of the reconstructed HR sequences, it remains challenging to design SRR algorithms that achieve good quality and robustness at a small computational complexity, being thus suitable for online applications. In this paper, we propose a new adaptive video SRR algorithm that achieves state-of-the-art performance at a very small computational cost. Using a nonlinear cost function constructed considering characteristics of typical innovation outliers in natural image sequences and an edge-preserving regularization strategy, we achieve state-of-the-art reconstructed image quality and robustness. This cost function is optimized using a specific alternating projections strategy over non-convex sets that is able to converge in a very few iterations. An accurate and very efficient approximation for the projection operations is also obtained using tools from multidimensional multirate signal processing. This solves the slow convergence issue of stochastic gradient-based methods while keeping a small computational complexity. Simulation results with both synthetic and real image sequences show that the performance of the proposed algorithm is similar or better than state-of-the-art SRR algorithms, while requiring only a small fraction of their computational cost. △ Less","10 March, 2020",https://arxiv.org/pdf/1909.00073
Dialog Intent Induction with Deep Multi-View Clustering,Hugh Perkins;Yi Yang,"We introduce the dialog intent induction task and present a novel deep multi-view clustering approach to tackle the problem. Dialog intent induction aims at discovering user intents from user query utterances in human-human conversations such as dialogs between customer support agents and customers. Motivated by the intuition that a dialog intent is not only expressed in the user query utterance but also captured in the rest of the dialog, we split a conversation into two independent views and exploit multi-view clustering techniques for inducing the dialog intent. In particular, we propose alternating-view k-means (AV-KMEANS) for joint multi-view representation learning and clustering analysis. The key innovation is that the instance-view representations are updated iteratively by predicting the cluster assignment obtained from the alternative view, so that the multi-view representations of the instances lead to similar cluster assignments. Experiments on two public datasets show that AV-KMEANS can induce better dialog intent clusters than state-of-the-art unsupervised representation learning methods and standard multi-view clustering approaches. △ Less","15 September, 2020",https://arxiv.org/pdf/1908.11487
Dedge-AGMNet:an effective stereo matching network optimized by depth edge auxiliary task,Weida Yang;Xindong Ai;Zuliu Yang;Yong Xu;Yong Zhao,"To improve the performance in ill-posed regions, this paper proposes an atrous granular multi-scale network based on depth edge subnetwork(Dedge-AGMNet). According to a general fact, the depth edge is the binary semantic edge of instance-sensitive. This paper innovatively generates the depth edge ground-truth by mining the semantic and instance dataset simultaneously. To incorporate the depth edge cues efficiently, our network employs the hard parameter sharing mechanism for the stereo matching branch and depth edge branch. The network modifies SPP to Dedge-SPP, which fuses the depth edge features to the disparity estimation network. The granular convolution is extracted and extends to 3D architecture. Then we design the AGM module to build a more suitable structure. This module could capture the multi-scale receptive field with fewer parameters. Integrating the ranks of different stereo datasets, our network outperforms other stereo matching networks and advances state-of-the-art performances on the Sceneflow, KITTI 2012 and KITTI 2015 benchmark datasets. △ Less","24 March, 2020",https://arxiv.org/pdf/1908.09346
Population-aware Hierarchical Bayesian Domain Adaptation via Multiple-component Invariant Learning,Vishwali Mhasawade;Nabeel Abdur Rehman;Rumi Chunara,"While machine learning is rapidly being developed and deployed in health settings such as influenza prediction, there are critical challenges in using data from one environment in another due to variability in features; even within disease labels there can be differences (e.g. ""fever"" may mean something different reported in a doctor's office versus in an online app). Moreover, models are often built on passive, observational data which contain different distributions of population subgroups (e.g. men or women). Thus, there are two forms of instability between environments in this observational transport problem. We first harness knowledge from health to conceptualize the underlying causal structure of this problem in a health outcome prediction task. Based on sources of stability in the model, we posit that for human-sourced data and health prediction tasks we can combine environment and population information in a novel population-aware hierarchical Bayesian domain adaptation framework that harnesses multiple invariant components through population attributes when needed. We study the conditions under which invariant learning fails, leading to reliance on the environment-specific attributes. Experimental results for an influenza prediction task on four datasets gathered from different contexts show the model can improve prediction in the case of largely unlabelled target data from a new environment and different constituent population, by harnessing both environment and population invariant information. This work represents a novel, principled way to address a critical challenge by blending domain (health) knowledge and algorithmic innovation. The proposed approach will have a significant impact in many social settings wherein who and where the data comes from matters. △ Less","9 March, 2020",https://arxiv.org/pdf/1908.09222
"Integration of Blockchain and Cloud of Things: Architecture, Applications and Challenges",Dinh C Nguyen;Pubudu N Pathirana;Ming Ding;Aruna Seneviratne,"The blockchain technology is taking the world by storm. Blockchain with its decentralized, transparent and secure nature has emerged as a disruptive technology for the next generation of numerous industrial applications. One of them is Cloud of Things enabled by the combination of cloud computing and Internet of Things. In this context, blockchain provides innovative solutions to address challenges in Cloud of Things in terms of decentralization, data privacy and network security, while Cloud of Things offer elasticity and scalability functionalities to improve the efficiency of blockchain operations. Therefore, a novel paradigm of blockchain and Cloud of Things integration, called BCoT, has been widely regarded as a promising enabler for a wide range of application scenarios. In this paper, we present a state-of-the-art review on the BCoT integration to provide general readers with an overview of the BCoT in various aspects, including background knowledge, motivation, and integrated architecture. Particularly, we also provide an in-depth survey of BCoT applications in different use-case domains such as smart healthcare, smart city, smart transportation and smart industry. Then, we review the recent BCoT developments with the emerging blockchain and cloud platforms, services, and research projects. Finally, some important research challenges and future directions are highlighted to spur further research in this promising area. △ Less","28 August, 2020",https://arxiv.org/pdf/1908.09058
"Decentralized Cooperative Online Estimation With Random Observation Matrices, Communication Graphs and Time Delays",Jiexiang Wang;Tao Li;Xiwei Zhang,"We analyze convergence of decentralized cooperative online estimation algorithms by a network of multiple nodes via information exchanging in an uncertain environment. Each node has a linear observation of an unknown parameter with randomly time-varying observation matrices. The underlying communication network is modeled by a sequence of random digraphs and is subjected to nonuniform random time-varying delays in channels. Each node runs an online estimation algorithm consisting of a consensus term taking a weighted sum of its own estimate and neighbours' delayed estimates, and an innovation term processing its own new measurement at each time step. By stochastic time-varying system, martingale convergence theories and the binomial expansion of random matrix products, we transform the convergence analysis of the algorithm into that of the mathematical expectation of random matrix products. Firstly, for the delay-free case, we show that the algorithm gains can be designed properly such that all nodes' estimates converge to the true parameter in mean square and almost surely if the observation matrices and communication graphs satisfy the stochastic spatiotemporal persistence of excitation condition. Secondly, for the case with time delays, we introduce delay matrices to model the random time-varying communication delays between nodes. It is shown that under the stochastic spatio-temporal persistence of excitation condition, for any given boundeddelays, proper algorithm gains can be designed to guarantee mean square convergence for the case with conditionally balanced digraphs. △ Less","7 December, 2020",https://arxiv.org/pdf/1908.08245
Instance Scale Normalization for image understanding,Zewen He;He Huang;Yudong Wu;Guan Huang;Wensheng Zhang,"Scale variation remains a challenging problem for object detection. Common paradigms usually adopt multiscale training & testing (image pyramid) or FPN (feature pyramid network) to process objects in a wide scale range. However, multi-scale methods aggravate more variations of scale that even deep convolution neural networks with FPN cannot handle well. In this work, we propose an innovative paradigm called Instance Scale Normalization (ISN) to resolve the above problem. ISN compresses the scale space of objects into a consistent range (ISN range), in both training and testing phases. This reassures the problem of scale variation fundamentally and reduces the difficulty of network optimization. Experiments show that ISN surpasses multi-scale counterpart significantly for object detection, instance segmentation, and multi-task human pose estimation, on several architectures. On COCO test-dev, our single model based on ISN achieves 46.5 mAP with a ResNet-101 backbone, which is among the state-of-the-art (SOTA) candidates for object detection. △ Less","9 June, 2020",https://arxiv.org/pdf/1908.07323
ZeroER: Entity Resolution using Zero Labeled Examples,Renzhi Wu;Sanya Chaba;Saurabh Sawlani;Xu Chu;Saravanan Thirumuruganathan,"Entity resolution (ER) refers to the problem of matching records in one or more relations that refer to the same real-world entity. While supervised machine learning (ML) approaches achieve the state-of-the-art results, they require a large amount of labeled examples that are expensive to obtain and often times infeasible. We investigate an important problem that vexes practitioners: is it possible to design an effective algorithm for ER that requires Zero labeled examples, yet can achieve performance comparable to supervised approaches? In this paper, we answer in the affirmative through our proposed approach dubbed ZeroER. Our approach is based on a simple observation -- the similarity vectors for matches should look different from that of unmatches. Operationalizing this insight requires a number of technical innovations. First, we propose a simple yet powerful generative model based on Gaussian Mixture Models for learning the match and unmatch distributions. Second, we propose an adaptive regularization technique customized for ER that ameliorates the issue of feature overfitting. Finally, we incorporate the transitivity property into the generative model in a novel way resulting in improved accuracy. On five benchmark ER datasets, we show that ZeroER greatly outperforms existing unsupervised approaches and achieves comparable performance to supervised approaches. △ Less","6 April, 2020",https://arxiv.org/pdf/1908.06049
"Fog Computing Systems: State of the Art, Research Issues and Future Trends, with a Focus on Resilience",Jose Moura;David Hutchison,"Many future innovative computing services will use Fog Computing Systems (FCS), integrated with Internet of Things (IoT) resources. These new services, built on the convergence of several distinct technologies, need to fulfil time-sensitive functions, provide variable levels of integration with their environment, and incorporate data storage, computation, communications, sensing, and control. There are, however, significant problems to be solved before such systems can be considered fit for purpose. The high heterogeneity, complexity, and dynamics of these resource-constrained systems bring new challenges to their robust and reliable operation, which implies the need for integral resilience management strategies. This paper surveys the state of the art in the relevant fields, and discusses the research issues and future trends that are emerging. We envisage future applications that have very stringent requirements, notably high-precision latency and synchronization between a large set of flows, where FCSs are key to supporting them. Thus, we hope to provide new insights into the design and management of resilient FCSs that are formed by IoT devices, edge computer servers and wireless sensor networks; these systems can be modelled using Game Theory, and flexibly programmed with the latest software and virtualization platforms. △ Less","17 July, 2020",https://arxiv.org/pdf/1908.05077
"Security in Brain-Computer Interfaces: State-of-the-art, opportunities, and future challenges",Sergio López Bernal;Alberto Huertas Celdrán;Gregorio Martínez Pérez;Michael Taynnan Barros;Sasitharan Balasubramaniam,"BCIs have significantly improved the patients' quality of life by restoring damaged hearing, sight, and movement capabilities. After evolving their application scenarios, the current trend of BCI is to enable new innovative brain-to-brain and brain-to-the-Internet communication paradigms. This technological advancement generates opportunities for attackers since users' personal information and physical integrity could be under tremendous risk. This work presents the existing versions of the BCI life-cycle and homogenizes them in a new approach that overcomes current limitations. After that, we offer a qualitative characterization of the security attacks affecting each phase of the BCI cycle to analyze their impacts and countermeasures documented in the literature. Finally, we reflect on lessons learned, highlighting research trends and future challenges concerning security on BCIs. △ Less","2 October, 2020",https://arxiv.org/pdf/1908.03536
Learning Densities in Feature Space for Reliable Segmentation of Indoor Scenes,Nicolas Marchal;Charlotte Moraldo;Roland Siegwart;Hermann Blum;Cesar Cadena;Abel Gawel,"Deep learning has enabled remarkable advances in scene understanding, particularly in semantic segmentation tasks. Yet, current state of the art approaches are limited to a closed set of classes, and fail when facing novel elements, also known as out of distribution (OoD) data. This is a problem as autonomous agents will inevitably come across a wide range of objects, all of which cannot be included during training. We propose a novel method to distinguish any object (foreground) from empty building structure (background) in indoor environments. We use normalizing flow to estimate the probability distribution of high-dimensional background descriptors. Foreground objects are therefore detected as areas in an image for which the descriptors are unlikely given the background distribution. As our method does not explicitly learn the representation of individual objects, its performance generalizes well outside of the training examples. Our model results in an innovative solution to reliably segment foreground from background in indoor scenes, which opens the way to a safer deployment of robots in human environments. △ Less","13 January, 2020",https://arxiv.org/pdf/1908.00448
Government as Network Catalyst: Accelerating Self-Organization in a Strategic Industry,Travis A. Whetsell;Michael D. Siciliano;Kaila G. K. Witkowski;Michael J. Leiblein,"Governments have long standing interests in preventing market failures and enhancing innovation in strategic industries. Public policy regarding domestic technology is critical to both national security and economic prosperity. Governments often seek to enhance their global competitiveness by promoting private sector cooperative activity at the inter-organizational level. Research on network governance has illuminated the structure of boundary-spanning collaboration mainly for programs with immediate public or non-profit objectives. Far less research has examined how governments might accelerate private sector cooperation to prevent market failures or to enhance innovation. The theoretical contribution of this research is to suggest that government programs might catalyze cooperative activity by accelerating the preferential attachment mechanism inherent in social networks. We analyze the long-term effects of a government program on the strategic alliance network of 451 organizations in the high-tech semiconductor industry between 1987 and 1999, using stochastic network analysis methods for longitudinal social networks. △ Less","16 January, 2020",https://arxiv.org/pdf/1907.13087
optimalFlow: Optimal-transport approach to flow cytometry gating and population matching,Eustasio del Barrio;Hristo Inouzhe;Jean-Michel Loubes;Carlos Matrán;Agustín Mayo-Íscar,"Data obtained from Flow Cytometry present pronounced variability due to biological and technical reasons. Biological variability is a well-known phenomenon produced by measurements on different individuals, with different characteristics such as illness, age, sex, etc. The use of different settings for measurement, the variation of the conditions during experiments and the different types of flow cytometers are some of the technical causes of variability. This mixture of sources of variability makes the use of supervised machine learning for identification of cell populations difficult. The present work is conceived as a combination of strategies to facilitate the task of supervised gating. We propose optimalFlowTemplates, based on a similarity distance and \text{Wasserstein barycenters}, which clusters cytometries and produces prototype cytometries for the different groups. We show that supervised learning, restricted to the new groups, performs better than the same techniques applied to the whole collection. We also present optimalFlowClassification, which uses a database of gated cytometries and optimalFlowTemplates to assign cell types to a new cytometry. We show that this procedure can outperform state of the art techniques in the proposed datasets. Our code is freely available as optimalFlow a Bioconductor R package at https://bioconductor.org/packages/optimalFlow. optimalFlowTemplates+optimalFlowClassification addresses the problem of using supervised learning while accounting for biological and technical variability. Our methodology provides a robust automated gating workflow that handles the intrinsic variability of flow cytometry data well. Our main innovation is the methodology itself and the optimal-transport techniques that we apply to flow cytometry analysis. △ Less","29 April, 2020",https://arxiv.org/pdf/1907.08006
Anonymous and confidential file sharing over untrusted clouds,Stefan Contiu;Sébastien Vaucher;Rafael Pires;Marcelo Pasin;Pascal Felber;Laurent Réveillère,"Using public cloud services for storing and sharing confidential data requires end users to cryptographically protect both the data and the access to the data. In some cases, the identity of end users needs to remain confidential against the cloud provider and fellow users accessing the data. As such, the underlying cryptographic access control mechanism needs to ensure the anonymity of both data producers and consumers. We introduce A-SKY, a cryptographic access control extension capable of providing confidentiality and anonymity guarantees, all while efficiently scaling to large organizations. A-SKY leverages trusted execution environments (TEEs) to address the impracticality of anonymous broadcast encryption (ANOBE) schemes, achieving faster execution times and shorter ciphertexts. The innovative design of A-SKY limits the usage of the TEE to the narrow set of data producing operations, and thus optimizes the dominant data consumption actions by not requiring a TEE. Furthermore, we propose a scalable implementation for A-SKY leveraging micro-services that preserves strong security guarantees while being able to efficiently manage realistic large user bases. Results highlight that the A-SKY cryptographic scheme is 3 orders of magnitude better than state of the art ANOBE, and an end-to-end system encapsulating A-SKY can elastically scale to support groups of 10 000 users while maintaining processing costs below 1 second. △ Less","6 April, 2020",https://arxiv.org/pdf/1907.06466
Seedless Graph Matching via Tail of Degree Distribution for Correlated Erdos-Renyi Graphs,Mahdi Bozorg;Saber Salehkaleybar;Matin Hashemi,"The network alignment (or graph matching) problem refers to recovering the node-to-node correspondence between two correlated networks. In this paper, we propose a network alignment algorithm which works without using a seed set of pre-matched node pairs or any other auxiliary information (e.g., node or edge labels) as an input. The algorithm assigns structurally innovative features to nodes based on the tail of empirical degree distribution of their neighbor nodes. Then, it matches the nodes according to these features. We evaluate the performance of proposed algorithm on both synthetic and real networks. For synthetic networks, we generate Erdos-Renyi graphs in the regions of Θ(\log(n)/n) and Θ(\log^{2}(n)/n), where a previous work theoretically showed that recovering is feasible in sparse Erdos-Renyi graphs if and only if the probability of having an edge between a pair of nodes in one of the graphs and also between the corresponding nodes in the other graph is in the order of Ω(\log(n)/n), where n is the number of nodes. Experiments on both real and synthetic networks show that it outperforms previous works in terms of probability of correct matching. △ Less","28 September, 2020",https://arxiv.org/pdf/1907.06334
ACTNET: end-to-end learning of feature activations and multi-stream aggregation for effective instance image retrieval,Syed Sameed Husain;Eng-Jon Ong;Miroslaw Bober,"We propose a novel CNN architecture called ACTNET for robust instance image retrieval from large-scale datasets. Our key innovation is a learnable activation layer designed to improve the signal-to-noise ratio (SNR) of deep convolutional feature maps. Further, we introduce a controlled multi-stream aggregation, where complementary deep features from different convolutional layers are optimally transformed and balanced using our novel activation layers, before aggregation into a global descriptor. Importantly, the learnable parameters of our activation blocks are explicitly trained, together with the CNN parameters, in an end-to-end manner minimising triplet loss. This means that our network jointly learns the CNN filters and their optimal activation and aggregation for retrieval tasks. To our knowledge, this is the first time parametric functions have been used to control and learn optimal aggregation. We conduct an in-depth experimental study on three non-linear activation functions: Sine-Hyperbolic, Exponential and modified Weibull, showing that while all bring significant gains the Weibull function performs best thanks to its ability to equalise strong activations. The results clearly demonstrate that our ACTNET architecture significantly enhances the discriminative power of deep features, improving significantly over the state-of-the-art retrieval results on all datasets. △ Less","23 October, 2020",https://arxiv.org/pdf/1907.05794
The Fog Development Kit: A Development Platform for SDN-based Edge-Fog Systems,Colton Powell;Christopher Desiniotis;Behnam Dezfouli,"With the rise of the Internet of Things (IoT), fog computing has emerged to help traditional cloud computing in meeting scalability demands. Fog computing makes it possible to fulfill real-time requirements of applications by bringing more processing, storage, and control power geographically closer to end-devices. However, since fog computing is a relatively new field, there is no standard platform for research and development in a realistic environment, and this dramatically inhibits innovation and development of fog-based applications. In response to these challenges, we propose the Fog Development Kit (FDK). By providing high-level interfaces for allocating computing and networking resources, the FDK abstracts the complexities of fog computing from developers and enables the rapid development of fog systems. In addition to supporting application development on a physical deployment, the FDK supports the use of emulation tools (e.g., GNS3 and Mininet) to create realistic environments, allowing fog application prototypes to be built with zero additional costs and enabling seamless portability to a physical infrastructure. Using a physical testbed and various kinds of applications running on it, we verify the operation and study the performance of the FDK. Specifically, we demonstrate that resource allocations are appropriately enforced and guaranteed, even amidst extreme network congestion. We also present simulation-based scalability analysis of the FDK versus the number of switches, the number of end-devices, and the number of fog-devices. △ Less","13 January, 2020",https://arxiv.org/pdf/1907.03081
A Survey of Maturity Models from Nolon to DevOps and Their Applications in Process Improvement,James J. Cusick,"This paper traces the history of Maturity Models and their impact on Process Improvement from the early work of Shewhart to their current usage with DevOps. The history of modern process improvement can be traced at least to Shewhart. From his foundational process contributions and those of other innovators a variety of methods and tools to aid in process quality advancement were developed. This paper begins by reviewing those early steps and then focuses on the emergence of Maturity Models in the 1970s with initial approach by Nolan. The broad adoption of Maturity Models that followed through the success of the CMM and then the CMMI approaches is detailed. This then leads to a general survey of additional models developed for such areas as IT Service Management, ITIL, Project Management, Agile Development, DevOps, CERT, and MDM among others. Finally, this paper discusses the application of these models in the support of process improvement and their limitations. Readers of this paper can expect to gain an appreciation for the origins of these models and surrounding methods as well as an ability to conduct comparative analysis of such models to aid in their selection and application. Keywords: Process Improvement, Process Engineering, Maturity Models, Capability Maturity Models, CMM, CMMI, ITSM, ITIL, Agile, DevOps, History of Science, History of Computing, Software Engineering, Quality. △ Less","10 October, 2020",https://arxiv.org/pdf/1907.01878
Proximal Distilled Evolutionary Reinforcement Learning,Cristian Bodnar;Ben Day;Pietro Lió,"Reinforcement Learning (RL) has achieved impressive performance in many complex environments due to the integration with Deep Neural Networks (DNNs). At the same time, Genetic Algorithms (GAs), often seen as a competing approach to RL, had limited success in scaling up to the DNNs required to solve challenging tasks. Contrary to this dichotomic view, in the physical world, evolution and learning are complementary processes that continuously interact. The recently proposed Evolutionary Reinforcement Learning (ERL) framework has demonstrated mutual benefits to performance when combining the two methods. However, ERL has not fully addressed the scalability problem of GAs. In this paper, we show that this problem is rooted in an unfortunate combination of a simple genetic encoding for DNNs and the use of traditional biologically-inspired variation operators. When applied to these encodings, the standard operators are destructive and cause catastrophic forgetting of the traits the networks acquired. We propose a novel algorithm called Proximal Distilled Evolutionary Reinforcement Learning (PDERL) that is characterised by a hierarchical integration between evolution and learning. The main innovation of PDERL is the use of learning-based variation operators that compensate for the simplicity of the genetic representation. Unlike traditional operators, our proposals meet the functional requirements of variation operators when applied on directly-encoded DNNs. We evaluate PDERL in five robot locomotion settings from the OpenAI gym. Our method outperforms ERL, as well as two state-of-the-art RL algorithms, PPO and TD3, in all tested environments. △ Less","7 July, 2020",https://arxiv.org/pdf/1906.09807
Benchmarking Regression Methods: A comparison with CGAN,Karan Aggarwal;Matthieu Kirchmeyer;Pranjul Yadav;S. Sathiya Keerthi;Patrick Gallinari,"In recent years, impressive progress has been made in the design of implicit probabilistic models via Generative Adversarial Networks (GAN) and its extension, the Conditional GAN (CGAN). Excellent solutions have been demonstrated mostly in image processing applications which involve large, continuous output spaces. There is almost no application of these powerful tools to problems having small dimensional output spaces. Regression problems involving the inductive learning of a map, y=f(x,z), z denoting noise, f:\mathbb{R}^n\times \mathbb{R}^k \rightarrow \mathbb{R}^m, with m small (e.g., m=1 or just a few) is one good case in point. The standard approach to solve regression problems is to probabilistically model the output y as the sum of a mean function m(x) and a noise term z; it is also usual to take the noise to be a Gaussian. These are done for convenience sake so that the likelihood of observed data is expressible in closed form. In the real world, on the other hand, stochasticity of the output is usually caused by missing or noisy input variables. Such a real world situation is best represented using an implicit model in which an extra noise vector, z is included with x as input. CGAN is naturally suited to design such implicit models. This paper makes the first step in this direction and compares the existing regression methods with CGAN. We notice however, that the existing methods like mixture density networks (MDN) and XGBoost do quite well compared to CGAN in terms of likelihood and mean absolute error, respectively. Both these methods are comparatively easier to train than CGANs. CGANs need more innovation to have a comparable modeling and ease-of-training with respect to the existing regression solvers. In summary, for modeling uncertainty MDNs are better while XGBoost is better for the cases where accurate prediction is more important. △ Less","4 February, 2020",https://arxiv.org/pdf/1905.12868
"The Who, What, How of Software Engineering Research: A Socio-Technical Framework",Margaret-Anne Storey;Neil A. Ernst;Courtney Williams;Eirini Kalliamvakou,"Software engineering is a socio-technical endeavor, and while many of our contributions focus on technical aspects, human stakeholders such as software developers are directly affected by and can benefit from our research and tool innovations. In this paper, we question how much of our research addresses human and social issues, and explore how much we study human and social aspects in our research designs. To answer these questions, we developed a socio-technical research framework to capture the main beneficiary of a research study (the who), the main type of research contribution produced (the what), and the research strategies used in the study (how we methodologically approach delivering relevant results given the who and what of our studies). We used this Who-What-How framework to analyze 151 papers from two well-cited publishing venues---the main technical track at the International Conference on Software Engineering, and the Empirical Software Engineering Journal by Springer---to assess how much this published research explicitly considers human aspects. We find that although a majority of these papers claim the contained research should benefit human stakeholders, most focus on technical contributions without engaging humans in their studies. Although our analysis is scoped to two venues, our results suggest a need for more diversification and triangulation of research strategies. In particular, there is a need for strategies that aim at a deeper understanding of human and social aspects of software development practice to balance the design and evaluation of technical innovations. We recommend that the framework should be used in the design of future studies in order to nudge software engineering research towards explicitly including human and social concerns in their designs, and to improve the relevance of our research for human stakeholders. △ Less","25 May, 2020",https://arxiv.org/pdf/1905.12841
Another Look at ALGORAND,Yongge Wang,"ALGORAND is a celebrated public ledger technology. In this paper, we identify several design flaws of the ALGORAND protocol. In particular, we show that the claimed (proved) fork-free property is not true and several assumptions in ALGORAND are not realistic in practice. The ALGORAND wiki page https://golden.com/wiki/Algorand claims that ""the probability of a fork in the protocol is estimated at 1/1,000,000,000 and therefore blocks can be considered final upon validation"". However, our first attack in this paper shows that a malicious adversary who controls less than 1/3 of the users (or money units) could fork the ALGORAND chain very easily. Our second attack shows that a malicious adversary could use a bribery attack to fork the ALGORAND chain very easily also. Furthermore, we show that the celebrated Byzantine Agreement component in ALGORAND is not necessary. The Byzantine Agreement is the most expensive part and one of the most innovative parts in the ALGORAND protocol. It is used to avoid forks in ALGORAND. We show that a simple majority vote could be used to achieve the same property that Byzantine Agreement achieves in ALGORAND under the same network assumption. △ Less","15 February, 2020",https://arxiv.org/pdf/1905.04463
Hadamard Matrix Guided Online Hashing,Mingbao Lin;Rongrong Ji;Hong Liu;Xiaoshuai Sun;Shen Chen;Qi Tian,"Online image hashing has attracted increasing research attention recently, which receives large-scale data in a streaming manner to update the hash functions on-the-fly. Its key challenge lies in the difficulty of balancing the learning timeliness and model accuracy. To this end, most works follow a supervised setting, i.e., using class labels to boost the hashing performance, which defects in two aspects: First, strong constraints, e.g., orthogonal or similarity preserving, are used, which however are typically relaxed and lead to large accuracy drop. Second, large amounts of training batches are required to learn the up-to-date hash functions, which largely increase the learning complexity. To handle the above challenges, a novel supervised online hashing scheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this paper. Our key innovation lies in introducing Hadamard matrix, which is an orthogonal binary matrix built via Sylvester method. In particular, to release the need of strong constraints, we regard each column of Hadamard matrix as the target code for each class label, which by nature satisfies several desired properties of hashing codes. To accelerate the online training, LSH is first adopted to align the lengths of target code and to-be-learned binary code. We then treat the learning of hash functions as a set of binary classification problems to fit the assigned target code. Finally, extensive experiments demonstrate the superior accuracy and efficiency of the proposed method over various state-of-the-art methods. Codes are available at https://github.com/lmbxmu/mycode. △ Less","22 January, 2020",https://arxiv.org/pdf/1905.04454
A Cloud-ready Architecture for Shared Medical Imaging Repository,Rui Lebre;Luís Bastião;Carlos Costa,"Background and Objective: Nowadays usage paradigms of medical imaging resources are requesting vendor-neutral archives, accessible through standard interfaces, with multi-repository support. Regional repositories shared by distinct institutions, teleradiology as a service at Cloud, teaching and research archives, are illustrative examples of this new reality. However, traditional production environments have a server archive instance per functional domain where every registered client application has access to all studies. This paper proposes an innovator ownership concept and access control mechanisms that provide a multi-repository environment and integrates well with standard protocols. Methods: A secure accounting mechanism for medical imaging repositories were designed and instantiated as an extension of a well-known open-source archive. A new Web services layer was implemented to provide a vendor-neutral solution complaint with modern DICOM-Web protocols for storage, search and retrieve of medical imaging data. Results: The concept validation was done through the integration of proposed architecture in an open-source solution. A quantitative assessment was performed for evaluating the impact of the mechanism in the usual DICOM Web operations. Conclusions: This article proposes a secure accounting architecture able to easily convert a standard medical imaging archive server in a multi-repository solution. The proposal validation was done through a set of tests that demonstrated its robustness and usage feasibility in a production environment. The proposed system offers new services, fundamental in a new era of Cloud-based operations, with acceptable temporal costs. △ Less","13 June, 2020",https://arxiv.org/pdf/1904.05795
"UG^{2+}
Track 2: A Collective Benchmark Effort for Evaluating and Advancing Image Understanding in Poor Visibility Environments",Ye Yuan;Wenhan Yang;Wenqi Ren;Jiaying Liu;Walter J. Scheirer;Zhangyang Wang,"The UG^{2+} challenge in IEEE CVPR 2019 aims to evoke a comprehensive discussion and exploration about how low-level vision techniques can benefit the high-level automatic visual recognition in various scenarios. In its second track, we focus on object or face detection in poor visibility enhancements caused by bad weathers (haze, rain) and low light conditions. While existing enhancement methods are empirically expected to help the high-level end task, that is observed to not always be the case in practice. To provide a more thorough examination and fair comparison, we introduce three benchmark sets collected in real-world hazy, rainy, and low-light conditions, respectively, with annotate objects/faces annotated. To our best knowledge, this is the first and currently largest effort of its kind. Baseline results by cascading existing enhancement and detection models are reported, indicating the highly challenging nature of our new data as well as the large room for further technical innovations. We expect a large participation from the broad research community to address these challenges together. △ Less","31 March, 2020",https://arxiv.org/pdf/1904.04474
A Survey of Distributed Consensus Protocols for Blockchain Networks,Yang Xiao;Ning Zhang;Wenjing Lou;Y. Thomas Hou,"Since the inception of Bitcoin, cryptocurrencies and the underlying blockchain technology have attracted an increasing interest from both academia and industry. Among various core components, consensus protocol is the defining technology behind the security and performance of blockchain. From incremental modifications of Nakamoto consensus protocol to innovative alternative consensus mechanisms, many consensus protocols have been proposed to improve the performance of the blockchain network itself or to accommodate other specific application needs. In this survey, we present a comprehensive review and analysis on the state-of-the-art blockchain consensus protocols. To facilitate the discussion of our analysis, we first introduce the key definitions and relevant results in the classic theory of fault tolerance which help to lay the foundation for further discussion. We identify five core components of a blockchain consensus protocol, namely, block proposal, block validation, information propagation, block finalization, and incentive mechanism. A wide spectrum of blockchain consensus protocols are then carefully reviewed accompanied by algorithmic abstractions and vulnerability analyses. The surveyed consensus protocols are analyzed using the five-component framework and compared with respect to different performance metrics. These analyses and comparisons provide us new insights in the fundamental differences of various proposals in terms of their suitable application scenarios, key assumptions, expected fault tolerance, scalability, drawbacks and trade-offs. We believe this survey will provide blockchain developers and researchers a comprehensive view on the state-of-the-art consensus protocols and facilitate the process of designing future protocols. △ Less","28 January, 2020",https://arxiv.org/pdf/1904.04098
IoT-Fog: A Communication Framework using Blockchain in the Internet of Things,Tanweer Alam,"In big cloud structures or large data structures, fog computing could be interpreted, referring critically to the growing issues and problems in accessing the information among the Internet of things (IoT) devices. Fog computing can be used to compute, store, control and connect smart devices to each other. IoT is an architecture of uniquely identified interrelated physical things, these physical things are able to communicate with each other and can transmit and receive information. This research presents a framework of the combination of the Internet of Things (IoT) and Fog computing. The blockchain is also the emerging technology that provides a hyper, distributed, public, authentic ledger to record the transactions. Blockchains technology is a secure technology that can be a great benefit to the next generation computing. The confluence of fog, blockchains, and IoT in this area introduces a new incentive. In this research work, the author mentions the convergence of blockchain, fog and IoT technological innovations to present an effective communication framework. The framework is implemented and tested using different scenarios. △ Less","3 June, 2020",https://arxiv.org/pdf/1904.00226
Benchmarking Web API Quality -- Revisited,David Bermbach;Erik Wittern,"Modern applications increasingly interact with web APIs -- reusable components, deployed and operated outside the application, and accessed over the network. Their existence, arguably, spurs application innovations, making it easy to integrate data or functionalities. While previous work has analyzed the ecosystem of web APIs and their design, little is known about web API quality at runtime. This gap is critical, as qualities including availability, latency, or provider security preferences can severely impact applications and user experience. In this paper, we revisit a 3-month, geo-distributed benchmark of popular web APIs, originally performed in 2015. We repeat this benchmark in 2018 and compare results from these two benchmarks regarding availability and latency. We furthermore introduce new results from assessing provider security preferences, collected both in 2015 and 2018, and results from our attempts to reach out to API providers with the results from our 2015 experiments. Our extensive experiments show that web API qualities vary 1.) based on the geo-distribution of clients, 2.) during our individual experiments, and 3.) between the two experiments. Our findings provide evidence to foster the discussion around web API quality, and can act as a basis for the creation of tools and approaches to mitigate quality issues. △ Less","3 July, 2020",https://arxiv.org/pdf/1903.07712
Smart Home Personal Assistants: A Security and Privacy Review,Jide S. Edu;Jose M. Such;Guillermo Suarez-Tangil,"Smart Home Personal Assistants (SPA) are an emerging innovation that is changing the way in which home users interact with the technology. However, there are a number of elements that expose these systems to various risks: i) the open nature of the voice channel they use, ii) the complexity of their architecture, iii) the AI features they rely on, and iv) their use of a wide-range of underlying technologies. This paper presents an in-depth review of the security and privacy issues in SPA, categorizing the most important attack vectors and their countermeasures. Based on this, we discuss open research challenges that can help steer the community to tackle and address current security and privacy issues in SPA. One of our key findings is that even though the attack surface of SPA is conspicuously broad and there has been a significant amount of recent research efforts in this area, research has so far focused on a small part of the attack surface, particularly on issues related to the interaction between the user and the SPA devices. We also point out that further research is needed to tackle issues related to authorization, speech recognition or profiling, to name a few. To the best of our knowledge, this is the first article to conduct such a comprehensive review and characterization of the security and privacy issues and countermeasures of SPA. △ Less","17 August, 2020",https://arxiv.org/pdf/1903.05593
Deep Learning in Medical Image Registration: A Survey,Grant Haskins;Uwe Kruger;Pingkun Yan,"The establishment of image correspondence through robust image registration is critical to many clinical tasks such as image fusion, organ atlas creation, and tumor growth monitoring, and is a very challenging problem. Since the beginning of the recent deep learning renaissance, the medical imaging research community has developed deep learning based approaches and achieved the state-of-the-art in many applications, including image registration. The rapid adoption of deep learning for image registration applications over the past few years necessitates a comprehensive summary and outlook, which is the main scope of this survey. This requires placing a focus on the different research areas as well as highlighting challenges that practitioners face. This survey, therefore, outlines the evolution of deep learning based medical image registration in the context of both research challenges and relevant innovations in the past few years. Further, this survey highlights future research directions to show how this field may be possibly moved forward to the next level. △ Less","21 January, 2020",https://arxiv.org/pdf/1903.02026
Automated Screening for Distress: A Perspective for the Future,Rajib Rana;Siddique Latif;Raj Gururajan;Anthony Gray;Geraldine Mackenzie;Gerald Humphris;Jeff Dunn,"Distress is a complex condition which affects a significant percentage of cancer patients and may lead to depression, anxiety, sadness, suicide and other forms of psychological morbidity. Compelling evidence supports screening for distress as a means of facilitating early intervention and subsequent improvements in psychological well-being and overall quality of life. Nevertheless, despite the existence of evidence based and easily administered screening tools, for example, the Distress Thermometer, routine screening for distress is yet to achieve widespread implementation. Efforts are intensifying to utilise innovative, cost effective methods now available through emerging technologies in the informatics and computational arenas. △ Less","27 July, 2020",https://arxiv.org/pdf/1902.09944
Zipper Stack: Shadow Stacks Without Shadow,Jinfeng Li;Liwei Chen;Qizhen Xu;Linan Tian;Gang Shi;Kai Chen;Dan Meng,"Return-Oriented Programming (ROP) is a typical attack technique that exploits return addresses to abuse existing code repeatedly. Most of the current return address protecting mechanisms (also known as the Backward-Edge Control-Flow Integrity) work only in limited threat models. For example, the attacker cannot break memory isolation, or the attacker has no knowledge of a secret key or random values. This paper presents a novel, lightweight mechanism protecting return addresses, Zipper Stack, which authenticates all return addresses by a chain structure using cryptographic message authentication codes (MACs). This innovative design can defend against the most powerful attackers who have full control over the program's memory and even know the secret key of the MAC function. This threat model is stronger than the one used in related work. At the same time, it produces low-performance overhead. We implemented Zipper Stack by extending the RISC-V instruction set architecture, and the evaluation on FPGA shows that the performance overhead of Zipper Stack is only 1.86%. Thus, we think Zipper Stack is suitable for actual deployment. △ Less","15 July, 2020",https://arxiv.org/pdf/1902.00888
Climate Change and Social Sciences: a bibliometric analysis,Flavio C. D. Moraes;Ana Lia Leonel;Pedro H. C. Torres;Pedro R. Jacobi;Sandra Momm,"The complexity of emergent wicked problems, such as climate change, culminates in a reformulation of how we think about society and mobilize scientists from various disciplines to seek solutions and perspectives on the problem. From an epistemological point of view, it is essential to evaluate how such topics can be developed inside the academic arena but, to do that, it is necessary to perform complex analysis of the great number of recent academic publications. In this work, we discuss how climate change has been addressed by social sciences in practice. Can we observe the development of a new epistemology by the emergence of the climate change debate? Are there contributions in academic journals within the field of social sciences addressing climate change? Which journals are these? Who are the authors? To answer these questions, we developed an innovative method combining different tools to search, filter, and analyze the impact of the academic production related to climate change in social sciences in the most relevant journals. △ Less","4 August, 2020",https://arxiv.org/pdf/1902.00712
Glyce: Glyph-vectors for Chinese Character Representations,Yuxian Meng;Wei Wu;Fei Wang;Xiaoya Li;Ping Nie;Fan Yin;Muyu Li;Qinghong Han;Xiaofei Sun;Jiwei Li,"It is intuitive that NLP tasks for logographic languages like Chinese should benefit from the use of the glyph information in those languages. However, due to the lack of rich pictographic evidence in glyphs and the weak generalization ability of standard computer vision models on character data, an effective way to utilize the glyph information remains to be found. In this paper, we address this gap by presenting Glyce, the glyph-vectors for Chinese character representations. We make three major innovations: (1) We use historical Chinese scripts (e.g., bronzeware script, seal script, traditional Chinese, etc) to enrich the pictographic evidence in characters; (2) We design CNN structures (called tianzege-CNN) tailored to Chinese character image processing; and (3) We use image-classification as an auxiliary task in a multi-task learning setup to increase the model's ability to generalize. We show that glyph-based models are able to consistently outperform word/char ID-based models in a wide range of Chinese NLP tasks. We are able to set new state-of-the-art results for a variety of Chinese NLP tasks, including tagging (NER, CWS, POS), sentence pair classification, single sentence classification tasks, dependency parsing, and semantic role labeling. For example, the proposed model achieves an F1 score of 80.6 on the OntoNotes dataset of NER, +1.5 over BERT; it achieves an almost perfect accuracy of 99.8\% on the Fudan corpus for text classification. Code found at https://github.com/ShannonAI/glyce. △ Less","21 May, 2020",https://arxiv.org/pdf/1901.10125
Bridging the Gap Between Computational Photography and Visual Recognition,Rosaura G. VidalMata;Sreya Banerjee;Brandon RichardWebster;Michael Albright;Pedro Davalos;Scott McCloskey;Ben Miller;Asong Tambo;Sushobhan Ghosh;Sudarshan Nagesh;Ye Yuan;Yueyu Hu;Junru Wu;Wenhan Yang;Xiaoshuai Zhang;Jiaying Liu;Zhangyang Wang;Hwann-Tzong Chen;Tzu-Wei Huang;Wen-Chi Chin;Yi-Chun Li;Mahmoud Lababidi;Charles Otto;Walter J. Scheirer,"What is the current state-of-the-art for image restoration and enhancement applied to degraded images acquired under less than ideal circumstances? Can the application of such algorithms as a pre-processing step to improve image interpretability for manual analysis or automatic visual recognition to classify scene content? While there have been important advances in the area of computational photography to restore or enhance the visual quality of an image, the capabilities of such techniques have not always translated in a useful way to visual recognition tasks. Consequently, there is a pressing need for the development of algorithms that are designed for the joint problem of improving visual appearance and recognition, which will be an enabling factor for the deployment of visual recognition tools in many real-world scenarios. To address this, we introduce the UG^2 dataset as a large-scale benchmark composed of video imagery captured under challenging conditions, and two enhancement tasks designed to test algorithmic impact on visual quality and automatic object recognition. Furthermore, we propose a set of metrics to evaluate the joint improvement of such tasks as well as individual algorithmic advances, including a novel psychophysics-based evaluation regime for human assessment and a realistic set of quantitative measures for object recognition performance. We introduce six new algorithms for image restoration or enhancement, which were created as part of the IARPA sponsored UG^2 Challenge workshop held at CVPR 2018. Under the proposed evaluation regime, we present an in-depth analysis of these algorithms and a host of deep learning-based and classic baseline approaches. From the observed results, it is evident that we are in the early days of building a bridge between computational photography and visual recognition, leaving many opportunities for innovation in this area. △ Less","19 February, 2020",https://arxiv.org/pdf/1901.09482
A Survey of the Recent Architectures of Deep Convolutional Neural Networks,Asifullah Khan;Anabia Sohail;Umme Zahoora;Aqsa Saeed Qureshi,"Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided. △ Less","10 May, 2020",https://arxiv.org/pdf/1901.06032
Optimization of Solidification in Die Casting using Numerical Simulations and Machine Learning,Shantanu Shahane;Narayana Aluru;Placid Ferreira;Shiv G Kapoor;Surya Pratap Vanka,"In this paper, we demonstrate the combination of machine learning and three dimensional numerical simulations for multi-objective optimization of low pressure die casting. The cooling of molten metal inside the mold is achieved typically by passing water through the cooling lines in the die. Depending on the cooling line location, coolant flow rate and die geometry, nonuniform temperatures are imposed on the molten metal at the mold wall. This boundary condition along with the initial molten metal temperature affect the product quality quantified in terms of micro-structure parameters and yield strength. A finite volume based numerical solver is used to determine the temperature-time history and correlate the inputs to outputs. The objective of this research is to develop and demonstrate a procedure to obtain the initial and wall temperatures so as to optimize the product quality. The non-dominated sorting genetic algorithm (NSGA-II) is used for multi-objective optimization in this work. The number of function evaluations required for NSGA-II can be of the order of millions and hence, the finite volume solver cannot be used directly for optimization. Therefore, a multilayer perceptron feed-forward neural network is first trained using the results from the numerical solution of the fluid flow and energy equations and is subsequently used as a surrogate model. As an assessment, simplified versions of the actual problem are designed to first verify results of the genetic algorithm. An innovative local sensitivity based approach is then used to rank the final Pareto optimal solutions and select a single best design. △ Less","3 January, 2020",https://arxiv.org/pdf/1901.02364
"Signal and System Design for Wireless Power Transfer : Prototype, Experiment and Validation",Junghoon Kim;Bruno Clerckx;Paul D. Mitcheson,"A new line of research on communications and signals design for Wireless Power Transfer (WPT) has recently emerged in the communication literature. Promising signal strategies to maximize the power transfer efficiency of WPT rely on (energy) beamforming, waveform, modulation and transmit diversity, and a combination thereof. To a great extent, the study of those strategies has so far been limited to theoretical performance analysis. In this paper, we study the real over-the-air performance of all the aforementioned signal strategies for WPT. To that end, we have designed, prototyped and experimented an innovative radiative WPT architecture based on Software-Defined Radio (SDR) that can operate in open-loop and closed-loop (with channel acquisition at the transmitter) modes. The prototype consists of three important blocks, namely the channel estimator, the signal generator, and the energy harvester. The experiments have been conducted in a variety of deployments, including frequency flat and frequency selective channels, under static and mobility conditions. Experiments highlight that a channeladaptive WPT architecture based on joint beamforming and waveform design offers significant performance improvements in harvested DC power over conventional single-antenna/multiantenna continuous wave systems. The experimental results fully validate the observations predicted from the theoretical signal designs and confirm the crucial and beneficial role played by the energy harvester nonlinearity. △ Less","2 August, 2020",https://arxiv.org/pdf/1901.01156
"Enumeration, structural and dimensional synthesis of robotic hands: theory and implementation",Ali Tamimi;Taher Deemyad;Alba Perez-Gracia,"Designing robotic hands for specific tasks could help in the creation of optimized end-effectors for grasping and manipulation. However the systematic design of robotic hands for a simultaneous task of all fingertips presents many challenges. In this work the algorithms and implementation of an overall synthesis process is presented, which could be a first step towards a complete design tool for robotic end-effectors. Type synthesis for a given task and number of fingers, solvability and dimensional synthesis for arbitrary topologies are developed and implemented. The resulting solver is a powerful tool that can aid in the creation of innovative robotic hands with arbitrary number of fingers and palms. Several examples of type synthesis, solvability calculations and dimensional synthesis are presented. △ Less","19 August, 2020",https://arxiv.org/pdf/1812.06348
Computing Bayes-Nash Equilibria in Combinatorial Auctions with Verification,Vitor Bosshard;Benedikt Bünz;Benjamin Lubin;Sven Seuken,"We present a new algorithm for computing pure-strategy \varepsilon-Bayes-Nash equilibria (\varepsilon-BNEs) in combinatorial auctions with continuous value and action spaces. An essential innovation of our algorithm is to separate the algorithm's search phase (for finding the \varepsilon-BNE) from the verification phase (for computing the \varepsilon). Using this approach, we obtain an algorithm that is both very fast and provides theoretical guarantees on the \varepsilon it finds. Our main technical contribution is a verification method which allows us to upper bound the \varepsilon across the whole continuous value space without making assumptions about the mechanism. Using our algorithm, we can now compute \varepsilon-BNEs in multi-minded domains that are significantly more complex than what was previously possible to solve. We release our code under an open-source license to enable researchers to perform algorithmic analyses of auctions, to enable bidders to analyze different strategies, and to facilitate many other applications. △ Less","24 July, 2020",https://arxiv.org/pdf/1812.01955
CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation,Junsong Fan;Zhaoxiang Zhang;Tieniu Tan;Chunfeng Song;Jun Xiao,"Weakly supervised semantic segmentation with only image-level labels saves large human effort to annotate pixel-level labels. Cutting-edge approaches rely on various innovative constraints and heuristic rules to generate the masks for every single image. Although great progress has been achieved by these methods, they treat each image independently and do not take account of the relationships across different images. In this paper, however, we argue that the cross-image relationship is vital for weakly supervised segmentation. Because it connects related regions across images, where supplementary representations can be propagated to obtain more consistent and integral regions. To leverage this information, we propose an end-to-end cross-image affinity module, which exploits pixel-level cross-image relationships with only image-level labels. By means of this, our approach achieves 64.3% and 65.3% mIoU on Pascal VOC 2012 validation and test set respectively, which is a new state-of-the-art result by only using image-level labels for weakly supervised semantic segmentation, demonstrating the superiority of our approach. △ Less","30 January, 2020",https://arxiv.org/pdf/1811.10842
"Decrypting Distributed Ledger Design -- Taxonomy, Classification and Blockchain Community Evaluation",Mark C. Ballandies;Marcus M. Dapp;Evangelos Pournaras,"More than 1000 distributed ledger technology (DLT) systems raising $600 billion in investment in 2016 feature the unprecedented and disruptive potential of blockchain technology. A systematic and data-driven analysis, comparison and rigorous evaluation of the different design choices of distributed ledgers and their implications is a challenge. The rapidly evolving nature of the blockchain landscape hinders reaching a common understanding of the techno-socio-economic design space of distributed ledgers and the cryptoeconomies they support. To fill this gap, this paper makes the following contributions: (i) A conceptual architecture of DLT systems with which (ii) a taxonomy is designed and (iii) a rigorous classification of DLT systems is made using real-world data and wisdom of the crowd. (iv) A DLT design guideline is the end result of applying machine learning methodologies on the classification data. Compared to related work and as defined in earlier taxonomy theory, the proposed taxonomy is highly comprehensive, robust, explanatory and extensible. The findings of this paper can provide new insights and better understanding of the key design choices evolving the modeling complexity of DLT systems, while identifying opportunities for new research contributions and business innovation. △ Less","16 January, 2020",https://arxiv.org/pdf/1811.03419
Category Trees,Kieran Greer,"This paper presents a batch classifier that has been improved from the earlier version and fixed a mistake in the earlier paper. Two important changes have been made. Each category is represented by a classifier, where each classifier classifies its own subset of data rows, using batch input values to represent the centroid. The first change is to use the category centroid as the desired category output. When the classifier represents more than one category, it creates a new layer and splits, to represent each category separately in the new layer. The second change therefore, is to allow the classifier to branch to new levels when there is a split in the data, or when some data rows are incorrectly classified. Each layer can therefore branch like a tree - not for distinguishing features, but for distinguishing categories. The paper then suggests further innovations, by adding fixed value ranges through bands, for each column or feature of the input dataset. When considering features, it is shown that some of the data can be classified directly through fixed value ranges, while the rest can be classified using the classifier technique. Tests show that the method can successfully classify a diverse set of benchmark datasets to better than the state-of-the-art. The paper also discusses a biological analogy with neurons and neuron links. △ Less","19 May, 2020",https://arxiv.org/pdf/1811.02617
"The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale",Alina Kuznetsova;Hassan Rom;Neil Alldrin;Jasper Uijlings;Ivan Krasin;Jordi Pont-Tuset;Shahab Kamali;Stefan Popov;Matteo Malloci;Alexander Kolesnikov;Tom Duerig;Vittorio Ferrari,"We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having unified annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection. △ Less","21 February, 2020",https://arxiv.org/pdf/1811.00982
"Reconciling λ
-Returns with Experience Replay",Brett Daley;Christopher Amato,"Modern deep reinforcement learning methods have departed from the incremental learning required for eligibility traces, rendering the implementation of the λ-return difficult in this context. In particular, off-policy methods that utilize experience replay remain problematic because their random sampling of minibatches is not conducive to the efficient calculation of λ-returns. Yet replay-based methods are often the most sample efficient, and incorporating λ-returns into them is a viable way to achieve new state-of-the-art performance. Towards this, we propose the first method to enable practical use of λ-returns in arbitrary replay-based methods without relying on other forms of decorrelation such as asynchronous gradient updates. By promoting short sequences of past transitions into a small cache within the replay memory, adjacent λ-returns can be efficiently precomputed by sharing Q-values. Computation is not wasted on experiences that are never sampled, and stored λ-returns behave as stable temporal-difference (TD) targets that replace the target network. Additionally, our method grants the unique ability to observe TD errors prior to sampling; for the first time, transitions can be prioritized by their true significance rather than by a proxy to it. Furthermore, we propose the novel use of the TD error to dynamically select λ-values that facilitate faster learning. We show that these innovations can enhance the performance of DQN when playing Atari 2600 games, even under partial observability. While our work specifically focuses on λ-returns, these ideas are applicable to any multi-step return estimator. △ Less","13 January, 2020",https://arxiv.org/pdf/1810.09967
A Robust Feature-aware Sparse Mesh Representation,Lizeth J. Fuentes Perez;Luciano A. Romero Calla;Anselmo A. Montenegro;Claudio Mura;Renato Pajarola,"The sparse representation of signals defined on Euclidean domains has been successfully applied in signal processing. Bringing the power of sparse representations to non-regular domains is still a challenge, but promising approaches have started emerging recently. In this paper, we investigate the problem of sparsely representing discrete surfaces and propose a new representation that is capable of providing tools for solving different geometry processing problems. The sparse discrete surface representation is obtained by combining innovative approaches into an integrated method. First, to deal with irregular mesh domains, we devised a new way to subdivide discrete meshes into a set of patches using a feature-aware seed sampling. Second, we achieve good surface approximation with over-fitting control by combining the power of a continuous global dictionary representation with a modified Orthogonal Marching Pursuit. The discrete surface approximation results produced were able to preserve the shape features while being robust to over-fitting. Our results show that the method is quite promising for applications like surface re-sampling and mesh compression. △ Less","24 November, 2020",https://arxiv.org/pdf/1810.08266
DeepCMB: Lensing Reconstruction of the Cosmic Microwave Background with Deep Neural Networks,João Caldeira;W. L. Kimmy Wu;Brian Nord;Camille Avestruz;Shubhendu Trivedi;Kyle T. Story,"Next-generation cosmic microwave background (CMB) experiments will have lower noise and therefore increased sensitivity, enabling improved constraints on fundamental physics parameters such as the sum of neutrino masses and the tensor-to-scalar ratio r. Achieving competitive constraints on these parameters requires high signal-to-noise extraction of the projected gravitational potential from the CMB maps. Standard methods for reconstructing the lensing potential employ the quadratic estimator (QE). However, the QE performs suboptimally at the low noise levels expected in upcoming experiments. Other methods, like maximum likelihood estimators (MLE), are under active development. In this work, we demonstrate reconstruction of the CMB lensing potential with deep convolutional neural networks (CNN) - ie, a ResUNet. The network is trained and tested on simulated data, and otherwise has no physical parametrization related to the physical processes of the CMB and gravitational lensing. We show that, over a wide range of angular scales, ResUNets recover the input gravitational potential with a higher signal-to-noise ratio than the QE method, reaching levels comparable to analytic approximations of MLE methods. We demonstrate that the network outputs quantifiably different lensing maps when given input CMB maps generated with different cosmologies. We also show we can use the reconstructed lensing map for cosmological parameter estimation. This application of CNN provides a few innovations at the intersection of cosmology and machine learning. First, while training and regressing on images, we predict a continuous-variable field rather than discrete classes. Second, we are able to establish uncertainty measures for the network output that are analogous to standard methods. We expect this approach to excel in capturing hard-to-model non-Gaussian astrophysical foreground and noise contributions. △ Less","12 June, 2020",https://arxiv.org/pdf/1810.01483
Automatic Foreground Extraction from Imperfect Backgrounds using Multi-Agent Consensus Equilibrium,Xiran Wang;Jason Juang;Stanley H. Chan,"Extracting accurate foreground objects from a scene is an essential step for many video applications. Traditional background subtraction algorithms can generate coarse estimates, but generating high quality masks requires professional softwares with significant human interventions, e.g., providing trimaps or labeling key frames. We propose an automatic foreground extraction method in applications where a static but imperfect background is available. Examples include filming and surveillance where the background can be captured before the objects enter the scene or after they leave the scene. Our proposed method is very robust and produces significantly better estimates than state-of-the-art background subtraction, video segmentation and alpha matting methods. The key innovation of our method is a novel information fusion technique. The fusion framework allows us to integrate the individual strengths of alpha matting, background subtraction and image denoising to produce an overall better estimate. Such integration is particularly important when handling complex scenes with imperfect background. We show how the framework is developed, and how the individual components are built. Extensive experiments and ablation studies are conducted to evaluate the proposed method. △ Less","31 May, 2020",https://arxiv.org/pdf/1808.08210
Learning Goal-Oriented Visual Dialog via Tempered Policy Gradient,Rui Zhao;Volker Tresp,"Learning goal-oriented dialogues by means of deep reinforcement learning has recently become a popular research topic. However, commonly used policy-based dialogue agents often end up focusing on simple utterances and suboptimal policies. To mitigate this problem, we propose a class of novel temperature-based extensions for policy gradient methods, which are referred to as Tempered Policy Gradients (TPGs). On a recent AI-testbed, i.e., the GuessWhat?! game, we achieve significant improvements with two innovations. The first one is an extension of the state-of-the-art solutions with Seq2Seq and Memory Network structures that leads to an improvement of 7%. The second one is the application of our newly developed TPG methods, which improves the performance additionally by around 5% and, even more importantly, helps produce more convincing utterances. △ Less","24 May, 2020",https://arxiv.org/pdf/1807.00737
Catering to Your Concerns: Automatic Generation of Personalised Security-Centric Descriptions for Android Apps,Tingmin Wu;Lihong Tang;Rongjunchen Zhang;Sheng Wen;Cecile Paris;Surya Nepal;Marthie Grobler;Yang Xiang,"Android users are increasingly concerned with the privacy of their data and security of their devices. To improve the security awareness of users, recent automatic techniques produce security-centric descriptions by performing program analysis. However, the generated text does not always address users' concerns as they are generally too technical to be understood by ordinary users. Moreover, different users have varied linguistic preferences, which do not match the text. Motivated by this challenge, we develop an innovative scheme to help users avoid malware and privacy-breaching apps by generating security descriptions that explain the privacy and security related aspects of an Android app in clear and understandable terms. We implement a prototype system, PERSCRIPTION, to generate personalised security-centric descriptions that automatically learn users' security concerns and linguistic preferences to produce user-oriented descriptions. We evaluate our scheme through experiments and user studies. The results clearly demonstrate the improvement on readability and users' security awareness of PERSCRIPTION's descriptions compared to existing description generators. △ Less","26 August, 2020",https://arxiv.org/pdf/1805.07070
Crossbar-Net: A Novel Convolutional Network for Kidney Tumor Segmentation in CT Images,Qian Yu;Yinghuan Shi;Jinquan Sun;Yang Gao;Yakang Dai;Jianbing Zhu,"Due to the irregular motion, similar appearance and diverse shape, accurate segmentation of kidney tumor in CT images is a difficult and challenging task. To this end, we present a novel automatic segmentation method, termed as Crossbar-Net, with the goal of accurate segmenting the kidney tumors. Firstly, considering that the traditional learning-based segmentation methods normally employ either whole images or squared patches as the training samples, we innovatively sample the orthogonal non-squared patches (namely crossbar patches), to fully cover the whole kidney tumors in either horizontal or vertical directions. These sampled crossbar patches could not only represent the detailed local information of kidney tumor as the traditional patches, but also describe the global appearance from either horizontal or vertical direction using contextual information. Secondly, with the obtained crossbar patches, we trained a convolutional neural network with two sub-models (i.e., horizontal sub-model and vertical sub-model) in a cascaded manner, to integrate the segmentation results from two directions (i.e., horizontal and vertical). This cascaded training strategy could effectively guarantee the consistency between sub-models, by feeding each other with the most difficult samples, for a better segmentation. In the experiment, we evaluate our method on a real CT kidney tumor dataset, collected from 94 different patients including 3,500 images. Compared with the state-of-the-art segmentation methods, the results demonstrate the superior results of our method on dice ratio score, true positive fraction, centroid distance and Hausdorff distance. Moreover, we have extended our crossbar-net to a different task: cardiac segmentation, showing the promising results for the better generalization. △ Less","2 April, 2020",https://arxiv.org/pdf/1804.10484
A New Channel Boosted Convolutional Neural Network using Transfer Learning,Asifullah Khan;Anabia Sohail;Amna Ali,"We present a novel architectural enhancement of Channel Boosting in a deep convolutional neural network (CNN). This idea of Channel Boosting exploits both the channel dimension of CNN (learning from multiple input channels) and Transfer learning (TL). TL is utilized at two different stages; channel generation and channel exploitation. In the proposed methodology, a deep CNN is boosted by various channels available through TL from already trained Deep Neural Networks, in addition to its original channel. The deep architecture of CNN then exploits the original and boosted channels down the stream for learning discriminative patterns. Churn prediction in telecom is a challenging task due to the high dimensionality and imbalanced nature of the data. Therefore, churn prediction data is used to evaluate the performance of the proposed Channel Boosted CNN (CB CNN). In the first phase, informative discriminative features are being extracted using a stacked autoencoder, and then in the second phase, these features are combined with the original features to form Channel Boosted images. Finally, the knowledge gained by a pretrained CNN is exploited by employing TL. The results are promising and show the ability of the Channel Boosting concept in learning complex classification problems by discerning even minute differences in churners and nonchurners. The proposed work validates the concept observed from the evolution of recent CNN architectures that the innovative restructuring of a CNN architecture may increase the networks representative capacity. △ Less","4 July, 2020",https://arxiv.org/pdf/1804.08528
The role of geography in the complex diffusion of innovations,Balázs Lengyel;Eszter Bokányi;Riccardo Di Clemente;János Kertész;Marta C. González,"The urban-rural divide is increasing in modern societies calling for geographical extensions of social influence modelling. Improved understanding of innovation diffusion across locations and through social connections can provide us with new insights into the spread of information, technological progress and economic development. In this work, we analyze the spatial adoption dynamics of iWiW, an Online Social Network (OSN) in Hungary and uncover empirical features about the spatial adoption in social networks. During its entire life cycle from 2002 to 2012, iWiW reached up to 300 million friendship ties of 3 million users. We find that the number of adopters as a function of town population follows a scaling law that reveals a strongly concentrated early adoption in large towns and a less concentrated late adoption. We also discover a strengthening distance decay of spread over the life-cycle indicating high fraction of distant diffusion in early stages but the dominance of local diffusion in late stages. The spreading process is modelled within the Bass diffusion framework that enables us to compare the differential equation version with an agent-based version of the model run on the empirical network. Although both models can capture the macro trend of adoption, they have limited capacity to describe the observed trends of urban scaling and distance decay. We find, however that incorporating adoption thresholds, defined by the fraction of social connections that adopt a technology before the individual adopts, improves the network model fit to the urban scaling of early adopters. Controlling for the threshold distribution enables us to eliminate the bias induced by local network structure on predicting local adoption peaks. Finally, we show that geographical features such as distance from the innovation origin and town size influence prediction of adoption peak at local scales. △ Less","27 August, 2020",https://arxiv.org/pdf/1804.01349
Sparse Principal Component Analysis via Variable Projection,N. Benjamin Erichson;Peng Zheng;Krithika Manohar;Steven L. Brunton;J. Nathan Kutz;Aleksandr Y. Aravkin,"Sparse principal component analysis (SPCA) has emerged as a powerful technique for modern data analysis, providing improved interpretation of low-rank structures by identifying localized spatial structures in the data and disambiguating between distinct time scales. We demonstrate a robust and scalable SPCA algorithm by formulating it as a value-function optimization problem. This viewpoint leads to a flexible and computationally efficient algorithm. Further, we can leverage randomized methods from linear algebra to extend the approach to the large-scale (big data) setting. Our proposed innovation also allows for a robust SPCA formulation which obtains meaningful sparse principal components in spite of grossly corrupted input data. The proposed algorithms are demonstrated using both synthetic and real world data, and show exceptional computational efficiency and diagnostic performance. △ Less","27 December, 2020",https://arxiv.org/pdf/1804.00341
Log-Scale Shrinkage Priors and Adaptive Bayesian Global-Local Shrinkage Estimation,Daniel F. Schmidt;Enes Makalic,"Global-local shrinkage hierarchies are an important innovation in Bayesian estimation. We propose the use of log-scale distributions as a novel basis for generating familes of prior distributions for local shrinkage hyperparameters. By varying the scale parameter one may vary the degree to which the prior distribution promotes sparsity in the coefficient estimates. By examining the class of distributions over the logarithm of the local shrinkage parameter that have log-linear, or sub-log-linear tails, we show that many standard prior distributions for local shrinkage parameters can be unified in terms of the tail behaviour and concentration properties of their corresponding marginal distributions over the coefficients β_j. We derive upper bounds on the rate of concentration around |β_j|=0, and the tail decay as |β_j| \to \infty, achievable by this wide class of prior distributions. We then propose a new type of ultra-heavy tailed prior, called the log-t prior with the property that, irrespective of the choice of associated scale parameter, the marginal distribution always diverges at β_j = 0, and always possesses super-Cauchy tails. We develop results demonstrating when prior distributions with (sub)-log-linear tails attain Kullback--Leibler super-efficiency and prove that the log-t prior distribution is always super-efficient. We show that the log-t prior is less sensitive to misspecification of the global shrinkage parameter than the horseshoe or lasso priors. By incorporating the scale parameter of the log-scale prior distributions into the Bayesian hierarchy we derive novel adaptive shrinkage procedures. Simulations show that the adaptive log-t procedure appears to always perform well, irrespective of the level of sparsity or signal-to-noise ratio of the underlying model. △ Less","30 January, 2020",https://arxiv.org/pdf/1801.02321
Robust and Precise Vehicle Localization based on Multi-sensor Fusion in Diverse City Scenes,Guowei Wan;Xiaolong Yang;Renlan Cai;Hao Li;Hao Wang;Shiyu Song,"We present a robust and precise localization system that achieves centimeter-level localization accuracy in disparate city scenes. Our system adaptively uses information from complementary sensors such as GNSS, LiDAR, and IMU to achieve high localization accuracy and resilience in challenging scenes, such as urban downtown, highways, and tunnels. Rather than relying only on LiDAR intensity or 3D geometry, we make innovative use of LiDAR intensity and altitude cues to significantly improve localization system accuracy and robustness. Our GNSS RTK module utilizes the help of the multi-sensor fusion framework and achieves a better ambiguity resolution success rate. An error-state Kalman filter is applied to fuse the localization measurements from different sources with novel uncertainty estimation. We validate, in detail, the effectiveness of our approaches, achieving 5-10cm RMS accuracy and outperforming previous state-of-the-art systems. Importantly, our system, while deployed in a large autonomous driving fleet, made our vehicles fully autonomous in crowded city streets despite road construction that occurred from time to time. A dataset including more than 60 km real traffic driving in various urban roads is used to comprehensively test our system. △ Less","5 March, 2020",https://arxiv.org/pdf/1711.05805
"Verifier-on-a-Leash: new schemes for verifiable delegated quantum computation, with quasilinear resources",Andrea Coladangelo;Alex Grilo;Stacey Jeffery;Thomas Vidick,"The problem of reliably certifying the outcome of a computation performed by a quantum device is rapidly gaining relevance. We present two protocols for a classical verifier to verifiably delegate a quantum computation to two non-communicating but entangled quantum provers. Our protocols have near-optimal complexity in terms of the total resources employed by the verifier and the honest provers, with the total number of operations of each party, including the number of entangled pairs of qubits required of the honest provers, scaling as O(g\log g) for delegating a circuit of size g. This is in contrast to previous protocols, which all require a prohibitively large polynomial overhead. Our first protocol requires a number of rounds that is linear in the depth of the circuit being delegated, and is blind, meaning neither prover can learn the circuit being delegated. The second protocol is not blind, but requires only a constant number of rounds of interaction. Our main technical innovation is an efficient rigidity theorem which allows a verifier to test that two entangled provers perform measurements specified by an arbitrary m-qubit tensor product of single-qubit Clifford observables on their respective halves of m shared EPR pairs, with a robustness that is independent of m. Our two-prover classical-verifier delegation protocols are obtained by combining this rigidity theorem with a single-prover quantum-verifier protocol for the verifiable delegation of a quantum computation, introduced by Broadbent (Theory of Computing, 2018). △ Less","9 January, 2020",https://arxiv.org/pdf/1708.07359
Heterogeneous domain adaptation: An unsupervised approach,Feng Liu;Guanquan Zhang;Jie Lu,"Domain adaptation leverages the knowledge in one domain - the source domain - to improve learning efficiency in another domain - the target domain. Existing heterogeneous domain adaptation research is relatively well-progressed, but only in situations where the target domain contains at least a few labeled instances. In contrast, heterogeneous domain adaptation with an unlabeled target domain has not been well-studied. To contribute to the research in this emerging field, this paper presents: (1) an unsupervised knowledge transfer theorem that guarantees the correctness of transferring knowledge; and (2) a principal angle-based metric to measure the distance between two pairs of domains: one pair comprises the original source and target domains and the other pair comprises two homogeneous representations of two domains. The theorem and the metric have been implemented in an innovative transfer model, called a Grassmann-Linear monotonic maps-geodesic flow kernel (GLG), that is specifically designed for heterogeneous unsupervised domain adaptation (HeUDA). The linear monotonic maps meet the conditions of the theorem and are used to construct homogeneous representations of the heterogeneous domains. The metric shows the extent to which the homogeneous representations have preserved the information in the original source and target domains. By minimizing the proposed metric, the GLG model learns the homogeneous representations of heterogeneous domains and transfers knowledge through these learned representations via a geodesic flow kernel. To evaluate the model, five public datasets were reorganized into ten HeUDA tasks across three applications: cancer detection, credit assessment, and text classification. The experiments demonstrate that the proposed model delivers superior performance over the existing baselines. △ Less","9 February, 2020",https://arxiv.org/pdf/1701.02511
Sound and Complete Bidirectional Typechecking for Higher-Rank Polymorphism with Existentials and Indexed Types,Jana Dunfield;Neelakantan R. Krishnaswami,"Bidirectional typechecking, in which terms either synthesize a type or are checked against a known type, has become popular for its applicability to a variety of type systems, its error reporting, and its ease of implementation. Following principles from proof theory, bidirectional typing can be applied to many type constructs. The principles underlying a bidirectional approach to indexed types (generalized algebraic datatypes) are less clear. Building on proof-theoretic treatments of equality, we give a declarative specification of typing based on focalization. This approach permits declarative rules for coverage of pattern matching, as well as support for first-class existential types using a focalized subtyping judgment. We use refinement types to avoid explicitly passing equality proofs in our term syntax, making our calculus similar to languages such as Haskell and OCaml. We also extend the declarative specification with an explicit rules for deducing when a type is principal, permitting us to give a complete declarative specification for a rich type system with significant type inference. We also give a set of algorithmic typing rules, and prove that it is sound and complete with respect to the declarative system. The proof requires a number of technical innovations, including proving soundness and completeness in a mutually recursive fashion. △ Less","19 September, 2020",https://arxiv.org/pdf/1601.05106
