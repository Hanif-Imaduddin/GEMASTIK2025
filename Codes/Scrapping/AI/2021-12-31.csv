title,authors,abstract,submitted_date,pdf_link
Machine Learning and Artificial Intelligence in Next-Generation Wireless Network,Wafeeq Iqbal;Wei Wang;Ting Zhu,"Due to the advancement in technologies, the next-generation wireless network will be very diverse, complicated, and according to the changed demands of the consumers. The current network operator methodologies and approaches are traditional and cannot help the next generation networks to utilize their resources most appropriately. The limited capability of the traditional tools will not allow the network providers to fulfill the demands of the network's subscribers in the future. Therefore, this paper will focus on machine learning, automation, artificial intelligence, and big data analytics for improving the capacity and effectiveness of next-generation wireless networks. The paper will discuss the role of these new technologies in improving the service and performance of the network providers in the future. The paper will find out that machine learning, big data analytics, and artificial intelligence will help in making the next-generation wireless network self-adaptive, self-aware, prescriptive, and proactive. At the end of the paper, it will be provided that future wireless network operators cannot work without shifting their operational framework to AI and machine learning technologies. △ Less","30 December, 2021",https://arxiv.org/pdf/2202.01690
Technological Approach to Mind Everywhere (TAME): an experimentally-grounded framework for understanding diverse bodies and minds,Michael Levin,"Synthetic biology and bioengineering provide the opportunity to create novel embodied cognitive systems (otherwise known as minds) in a very wide variety of chimeric architectures combining evolved and designed material and software. These advances are disrupting familiar concepts in the philosophy of mind, and require new ways of thinking about and comparing truly diverse intelligences, whose composition and origin are not like any of the available natural model species. In this Perspective, I introduce TAME - Technological Approach to Mind Everywhere - a framework for understanding and manipulating cognition in unconventional substrates. TAME formalizes a non-binary (continuous), empirically-based approach to strongly embodied agency. When applied to regenerating/developmental systems, TAME suggests a perspective on morphogenesis as an example of basal cognition. The deep symmetry between problem-solving in anatomical, physiological, transcriptional, and 3D (traditional behavioral) spaces drives specific hypotheses by which cognitive capacities can scale during evolution. An important medium exploited by evolution for joining active subunits into greater agents is developmental bioelectricity, implemented by pre-neural use of ion channels and gap junctions to scale cell-level feedback loops into anatomical homeostasis. This architecture of multi-scale competency of biological systems has important implications for plasticity of bodies and minds, greatly potentiating evolvability. Considering classical and recent data from the perspectives of computational science, evolutionary biology, and basal cognition, reveals a rich research program with many implications for cognitive science, evolutionary biology, regenerative medicine, and artificial intelligence. △ Less","24 December, 2021",https://arxiv.org/pdf/2201.10346
Computational Rational Engineering and Development: Synergies and Opportunities,Ramses Sala,"Research and development in computer technology and computational methods have resulted in a wide variety of valuable tools for Computer-Aided Engineering (CAE) and Industrial Engineering. However, despite the exponential increase in computational capabilities and Artificial Intelligence (AI) methods, many of the visionary perspectives on cybernetic automation of design, engineering, and development have not been successfully pursued or realized yet. While contemporary research trends and movements such as Industry 4.0 primarily target progress by connected automation in manufacturing and production, the objective of this paper is to survey progress and formulate perspectives targeted on the automation and autonomization of engineering development processes. Based on an interdisciplinary mini-review, this work identifies open challenges, synergies, and research opportunities towards the realization of resource-efficient cooperative engineering and development systems. In order to go beyond conventional human-centered, tool-based CAE approaches and realize Computational Intelligence Driven Development processes, it is suggested to extend the framework of Computational Rationality to challenges in design, engineering and development. △ Less","27 December, 2021",https://arxiv.org/pdf/2201.06922
Validation and Transparency in AI systems for pharmacovigilance: a case study applied to the medical literature monitoring of adverse events,Bruno Ohana;Jack Sullivan;Nicole Baker,"Recent advances in artificial intelligence applied to biomedical text are opening exciting opportunities for improving pharmacovigilance activities currently burdened by the ever growing volumes of real world data. To fully realize these opportunities, existing regulatory guidance and industry best practices should be taken into consideration in order to increase the overall trustworthiness of the system and enable broader adoption. In this paper we present a case study on how to operationalize existing guidance for validated AI systems in pharmacovigilance focusing on the specific task of medical literature monitoring (MLM) of adverse events from the scientific literature. We describe an AI system designed with the goal of reducing effort in MLM activities built in close collaboration with subject matter experts and considering guidance for validated systems in pharmacovigilance and AI transparency. In particular we make use of public disclosures as a useful risk control measure to mitigate system misuse and earn user trust. In addition we present experimental results showing the system can significantly remove screening effort while maintaining high levels of recall (filtering 55% of irrelevant articles on average, for a target recall of 0.99 on suspected adverse articles) and provide a robust method for tuning the desired recall to suit a particular risk profile. △ Less","21 December, 2021",https://arxiv.org/pdf/2201.00692
Relating Blindsight and AI: A Review,Joshua Bensemann;Qiming Bao;Gaël Gendron;Tim Hartill;Michael Witbrock,"Processes occurring in brains, a.k.a. biological neural networks, can and have been modeled within artificial neural network architectures. Due to this, we have conducted a review of research on the phenomenon of blindsight in an attempt to generate ideas for artificial intelligence models. Blindsight can be considered as a diminished form of visual experience. If we assume that artificial networks have no form of visual experience, then deficits caused by blindsight give us insights into the processes occurring within visual experience that we can incorporate into artificial neural networks. This article has been structured into three parts. Section 2 is a review of blindsight research, looking specifically at the errors occurring during this condition compared to normal vision. Section 3 identifies overall patterns from Section 2 to generate insights for computational models of vision. Section 4 demonstrates the utility of examining biological research to inform artificial intelligence research by examining computation models of visual attention relevant to one of the insights generated in Section 3. The research covered in Section 4 shows that incorporating one of our insights into computational vision does benefit those models. Future research will be required to determine whether our other insights are as valuable. △ Less","8 December, 2021",https://arxiv.org/pdf/2201.00616
Artificial Intelligence and Statistical Techniques in Short-Term Load Forecasting: A Review,Ali Bou Nassif;Bassel Soudan;Mohammad Azzeh;Imtinan Attilli;Omar AlMulla,"Electrical utilities depend on short-term demand forecasting to proactively adjust production and distribution in anticipation of major variations. This systematic review analyzes 240 works published in scholarly journals between 2000 and 2019 that focus on applying Artificial Intelligence (AI), statistical, and hybrid models to short-term load forecasting (STLF). This work represents the most comprehensive review of works on this subject to date. A complete analysis of the literature is conducted to identify the most popular and accurate techniques as well as existing gaps. The findings show that although Artificial Neural Networks (ANN) continue to be the most commonly used standalone technique, researchers have been exceedingly opting for hybrid combinations of different techniques to leverage the combined advantages of individual methods. The review demonstrates that it is commonly possible with these hybrid combinations to achieve prediction accuracy exceeding 99%. The most successful duration for short-term forecasting has been identified as prediction for a duration of one day at an hourly interval. The review has identified a deficiency in access to datasets needed for training of the models. A significant gap has been identified in researching regions other than Asia, Europe, North America, and Australia. △ Less","29 December, 2021",https://arxiv.org/pdf/2201.00437
"Scalar reward is not enough: A response to Silver, Singh, Precup and Sutton (2021)",Peter Vamplew;Benjamin J. Smith;Johan Kallstrom;Gabriel Ramos;Roxana Radulescu;Diederik M. Roijers;Conor F. Hayes;Fredrik Heintz;Patrick Mannion;Pieter J. K. Libin;Richard Dazeley;Cameron Foale,"The recent paper `""Reward is Enough"" by Silver, Singh, Precup and Sutton posits that the concept of reward maximisation is sufficient to underpin all intelligence, both natural and artificial. We contest the underlying assumption of Silver et al. that such reward can be scalar-valued. In this paper we explain why scalar rewards are insufficient to account for some aspects of both biological and computational intelligence, and argue in favour of explicitly multi-objective models of reward maximisation. Furthermore, we contend that even if scalar reward functions can trigger intelligent behaviour in specific cases, it is still undesirable to use this approach for the development of artificial general intelligence due to unacceptable risks of unsafe or unethical behaviour. △ Less","24 November, 2021",https://arxiv.org/pdf/2112.15422
Machine Learning Application Development: Practitioners' Insights,Md Saidur Rahman;Foutse Khomh;Alaleh Hamidi;Jinghui Cheng;Giuliano Antoniol;Hironori Washizaki,"Nowadays, intelligent systems and services are getting increasingly popular as they provide data-driven solutions to diverse real-world problems, thanks to recent breakthroughs in Artificial Intelligence (AI) and Machine Learning (ML). However, machine learning meets software engineering not only with promising potentials but also with some inherent challenges. Despite some recent research efforts, we still do not have a clear understanding of the challenges of developing ML-based applications and the current industry practices. Moreover, it is unclear where software engineering researchers should focus their efforts to better support ML application developers. In this paper, we report about a survey that aimed to understand the challenges and best practices of ML application development. We synthesize the results obtained from 80 practitioners (with diverse skills, experience, and application domains) into 17 findings; outlining challenges and best practices for ML application development. Practitioners involved in the development of ML-based software systems can leverage the summarized best practices to improve the quality of their system. We hope that the reported challenges will inform the research community about topics that need to be investigated to improve the engineering process and the quality of ML-based applications. △ Less","30 December, 2021",https://arxiv.org/pdf/2112.15277
Recent Trends in Artificial Intelligence-inspired Electronic Thermal Management,Aviral Chharia;Nishi Mehta;Shivam Gupta;Shivam Prajapati,"The rise of computation-based methods in thermal management has gained immense attention in recent years due to the ability of deep learning to solve complex 'physics' problems, which are otherwise difficult to be approached using conventional techniques. Thermal management is required in electronic systems to keep them from overheating and burning, enhancing their efficiency and lifespan. For a long time, numerical techniques have been employed to aid in the thermal management of electronics. However, they come with some limitations. To increase the effectiveness of traditional numerical approaches and address the drawbacks faced in conventional approaches, researchers have looked at using artificial intelligence at various stages of the thermal management process. The present study discusses in detail, the current uses of deep learning in the domain of 'electronic' thermal management. △ Less","26 December, 2021",https://arxiv.org/pdf/2112.14837
Anomaly Detection in Cyber-Physical Systems: Reconstruction of a Prediction Error Feature Space,Nuno Oliveira;Norberto Sousa;Jorge Oliveira;Isabel Praça,"Cyber-physical systems are infrastructures that use digital information such as network communications and sensor readings to control entities in the physical world. Many cyber-physical systems in airports, hospitals and nuclear power plants are regarded as critical infrastructures since a disruption of its normal functionality can result in negative consequences for the society. In the last few years, some security solutions for cyber-physical systems based on artificial intelligence have been proposed. Nevertheless, knowledge domain is required to properly setup and train artificial intelligence algorithms. Our work proposes a novel anomaly detection framework based on error space reconstruction, where genetic algorithms are used to perform hyperparameter optimization of machine learning methods. The proposed method achieved an F1-score of 87.89% in the SWaT dataset. △ Less","29 December, 2021",https://arxiv.org/pdf/2112.14821
Parallelized and Randomized Adversarial Imitation Learning for Safety-Critical Self-Driving Vehicles,Won Joon Yun;MyungJae Shin;Soyi Jung;Sean Kwon;Joongheon Kim,"Self-driving cars and autonomous driving research has been receiving considerable attention as major promising prospects in modern artificial intelligence applications. According to the evolution of advanced driver assistance system (ADAS), the design of self-driving vehicle and autonomous driving systems becomes complicated and safety-critical. In general, the intelligent system simultaneously and efficiently activates ADAS functions. Therefore, it is essential to consider reliable ADAS function coordination to control the driving system, safely. In order to deal with this issue, this paper proposes a randomized adversarial imitation learning (RAIL) algorithm. The RAIL is a novel derivative-free imitation learning method for autonomous driving with various ADAS functions coordination; and thus it imitates the operation of decision maker that controls autonomous driving with various ADAS functions. The proposed method is able to train the decision maker that deals with the LIDAR data and controls the autonomous driving in multi-lane complex highway environments. The simulation-based evaluation verifies that the proposed method achieves desired performance. △ Less","26 December, 2021",https://arxiv.org/pdf/2112.14710
On some Foundational Aspects of Human-Centered Artificial Intelligence,Luciano Serafini;Raul Barbosa;Jasmin Grosinger;Luca Iocchi;Christian Napoli;Salvatore Rinzivillo;Jacques Robin;Alessandro Saffiotti;Teresa Scantamburlo;Peter Schueller;Paolo Traverso;Javier Vazquez-Salceda,"The burgeoning of AI has prompted recommendations that AI techniques should be ""human-centered"". However, there is no clear definition of what is meant by Human Centered Artificial Intelligence, or for short, HCAI. This paper aims to improve this situation by addressing some foundational aspects of HCAI. To do so, we introduce the term HCAI agent to refer to any physical or software computational agent equipped with AI components and that interacts and/or collaborates with humans. This article identifies five main conceptual components that participate in an HCAI agent: Observations, Requirements, Actions, Explanations and Models. We see the notion of HCAI agent, together with its components and functions, as a way to bridge the technical and non-technical discussions on human-centered AI. In this paper, we focus our analysis on scenarios consisting of a single agent operating in dynamic environments in presence of humans. △ Less","29 December, 2021",https://arxiv.org/pdf/2112.14480
Federated Learning for Cross-block Oil-water Layer Identification,Bingyang Chena;Xingjie Zenga;Weishan Zhang,"Cross-block oil-water layer(OWL) identification is essential for petroleum development. Traditional methods are greatly affected by subjective factors due to depending mainly on the human experience. AI-based methods have promoted the development of OWL identification. However, because of the significant geological differences across blocks and the severe long-tailed distribution(class imbalanced), the identification effects of existing artificial intelligence(AI) models are limited. In this paper, we address this limitation by proposing a dynamic fusion-based federated learning(FL) for OWL identification. To overcome geological differences, we propose a dynamic weighted strategy to fuse models and train a general OWL identification model. In addition, an F1 score-based re-weighting scheme is designed and a novel loss function is derived theoretically to solve the data long-tailed problem. Further, a geological knowledge-based mask-attention mechanism is proposed to enhance model feature extraction. To our best knowledge, this is the first work to identify OWL using FL. We evaluate the proposed approach with an actual well logging dataset from the oil field and a public 3W dataset. Experimental results demonstrate that our approach significantly out-performs other AI methods. △ Less","28 December, 2021",https://arxiv.org/pdf/2112.14359
Towards continual task learning in artificial neural networks: current approaches and insights from neuroscience,David McCaffary,"The innate capacity of humans and other animals to learn a diverse, and often interfering, range of knowledge and skills throughout their lifespan is a hallmark of natural intelligence, with obvious evolutionary motivations. In parallel, the ability of artificial neural networks (ANNs) to learn across a range of tasks and domains, combining and re-using learned representations where required, is a clear goal of artificial intelligence. This capacity, widely described as continual learning, has become a prolific subfield of research in machine learning. Despite the numerous successes of deep learning in recent years, across domains ranging from image recognition to machine translation, such continual task learning has proved challenging. Neural networks trained on multiple tasks in sequence with stochastic gradient descent often suffer from representational interference, whereby the learned weights for a given task effectively overwrite those of previous tasks in a process termed catastrophic forgetting. This represents a major impediment to the development of more generalised artificial learning systems, capable of accumulating knowledge over time and task space, in a manner analogous to humans. A repository of selected papers and implementations accompanying this review can be found at https://github.com/mccaffary/continual-learning. △ Less","28 December, 2021",https://arxiv.org/pdf/2112.14146
Blockchain Meets AI for Resilient and Intelligent Internet of Vehicles,Pranav Kumar Singh;Sukumar Nandi;Sunit K. Nandi;Uttam Ghosh;Danda B. Rawat,"The Internet of Vehicles (IoV) is flourishing and offers various applications relating to road safety, traffic and fuel efficiency, and infotainment. Dealing with security and privacy threats and managing the trust (detecting malicious and misbehaving peers) in IoV remains the most significant concern. Artificial Intelligence is one of the most revolutionizing technologies, and the predictive power of its machine learning models can help detect intrusions and misbehaviors. Similarly, empowering the state-of-the-art IoV security framework with blockchain can make it secure and resilient. This article discusses joint AI and blockchain for security, privacy and trust-related risks in IoV. This paper also presents problems, challenges, requirements and solutions using ML and blockchain to address aforementioned issues in IoV. △ Less","28 December, 2021",https://arxiv.org/pdf/2112.14078
Abstractions of General Reinforcement Learning,Sultan J. Majeed,"The field of artificial intelligence (AI) is devoted to the creation of artificial decision-makers that can perform (at least) on par with the human counterparts on a domain of interest. Unlike the agents in traditional AI, the agents in artificial general intelligence (AGI) are required to replicate human intelligence in almost every domain of interest. Moreover, an AGI agent should be able to achieve this without (virtually any) further changes, retraining, or fine-tuning of the parameters. The real world is non-stationary, non-ergodic, and non-Markovian: we, humans, can neither revisit our past nor are the most recent observations sufficient statistics. Yet, we excel at a variety of complex tasks. Many of these tasks require longterm planning. We can associate this success to our natural faculty to abstract away task-irrelevant information from our overwhelming sensory experience. We make task-specific mental models of the world without much effort. Due to this ability to abstract, we can plan on a significantly compact representation of a task without much loss of performance. Not only this, we also abstract our actions to produce high-level plans: the level of action-abstraction can be anywhere between small muscle movements to a mental notion of ""doing an action"". It is natural to assume that any AGI agent competing with humans (at every plausible domain) should also have these abilities to abstract its experiences and actions. This thesis is an inquiry into the existence of such abstractions which aid efficient planing for a wide range of domains, and most importantly, these abstractions come with some optimality guarantees. △ Less","26 December, 2021",https://arxiv.org/pdf/2112.13404
Prevalence Threshold and bounds in the Accuracy of Binary Classification Systems,Jacques Balayla,"The accuracy of binary classification systems is defined as the proportion of correct predictions - both positive and negative - made by a classification model or computational algorithm. A value between 0 (no accuracy) and 1 (perfect accuracy), the accuracy of a classification model is dependent on several factors, notably: the classification rule or algorithm used, the intrinsic characteristics of the tool used to do the classification, and the relative frequency of the elements being classified. Several accuracy metrics exist, each with its own advantages in different classification scenarios. In this manuscript, we show that relative to a perfect accuracy of 1, the positive prevalence threshold (φ_e), a critical point of maximum curvature in the precision-prevalence curve, bounds the F{_β} score between 1 and 1.8/1.5/1.2 for β values of 0.5/1.0/2.0, respectively; the F_1 score between 1 and 1.5, and the Fowlkes-Mallows Index (FM) between 1 and \sqrt{2} \approx 1.414. We likewise describe a novel negative prevalence threshold (φ_n), the level of sharpest curvature for the negative predictive value-prevalence curve, such that φ_n > φ_e. The area between both these thresholds bounds the Matthews Correlation Coefficient (MCC) between \sqrt{2}/2 and \sqrt{2}. Conversely, the ratio of the maximum possible accuracy to that at any point below the prevalence threshold, φ_e, goes to infinity with decreasing prevalence. Though applications are numerous, the ideas herein discussed may be used in computational complexity theory, artificial intelligence, and medical screening, amongst others. Where computational time is a limiting resource, attaining the prevalence threshold in binary classification systems may be sufficient to yield levels of accuracy comparable to that under maximum prevalence. △ Less","25 December, 2021",https://arxiv.org/pdf/2112.13289
Evolutionary Generation of Visual Motion Illusions,Lana Sinapayen;Eiji Watanabe,"Why do we sometimes perceive static images as if they were moving? Visual motion illusions enjoy a sustained popularity, yet there is no definitive answer to the question of why they work. We present a generative model, the Evolutionary Illusion GENerator (EIGen), that creates new visual motion illusions. The structure of EIGen supports the hypothesis that illusory motion might be the result of perceiving the brain's own predictions rather than perceiving raw visual input from the eyes. The scientific motivation of this paper is to demonstrate that the perception of illusory motion could be a side effect of the predictive abilities of the brain. The philosophical motivation of this paper is to call attention to the untapped potential of ""motivated failures"", ways for artificial systems to fail as biological systems fail, as a worthy outlet for Artificial Intelligence and Artificial Life research. △ Less","25 December, 2021",https://arxiv.org/pdf/2112.13243
Explainable Artificial Intelligence for Pharmacovigilance: What Features Are Important When Predicting Adverse Outcomes?,Isaac Ronald Ward;Ling Wang;Juan lu;Mohammed Bennamoun;Girish Dwivedi;Frank M Sanfilippo,"Explainable Artificial Intelligence (XAI) has been identified as a viable method for determining the importance of features when making predictions using Machine Learning (ML) models. In this study, we created models that take an individual's health information (e.g. their drug history and comorbidities) as inputs, and predict the probability that the individual will have an Acute Coronary Syndrome (ACS) adverse outcome. Using XAI, we quantified the contribution that specific drugs had on these ACS predictions, thus creating an XAI-based technique for pharmacovigilance monitoring, using ACS as an example of the adverse outcome to detect. Individuals aged over 65 who were supplied Musculo-skeletal system (anatomical therapeutic chemical (ATC) class M) or Cardiovascular system (ATC class C) drugs between 1993 and 2009 were identified, and their drug histories, comorbidities, and other key features were extracted from linked Western Australian datasets. Multiple ML models were trained to predict if these individuals would have an ACS related adverse outcome (i.e., death or hospitalisation with a discharge diagnosis of ACS), and a variety of ML and XAI techniques were used to calculate which features -- specifically which drugs -- led to these predictions. The drug dispensing features for rofecoxib and celecoxib were found to have a greater than zero contribution to ACS related adverse outcome predictions (on average), and it was found that ACS related adverse outcomes can be predicted with 72% accuracy. Furthermore, the XAI libraries LIME and SHAP were found to successfully identify both important and unimportant features, with SHAP slightly outperforming LIME. ML models trained on linked administrative health datasets in tandem with XAI algorithms can successfully quantify feature importance, and with further development, could potentially be used as pharmacovigilance monitoring techniques. △ Less","25 December, 2021",https://arxiv.org/pdf/2112.13210
Deep Neuroevolution Squeezes More out of Small Neural Networks and Small Training Sets: Sample Application to MRI Brain Sequence Classification,Joseph N Stember;Hrithwik Shalu,"Purpose: Deep Neuroevolution (DNE) holds the promise of providing radiology artificial intelligence (AI) that performs well with small neural networks and small training sets. We seek to realize this potential via a proof-of-principle application to MRI brain sequence classification. Methods: We analyzed a training set of 20 patients, each with four sequences/weightings: T1, T1 post-contrast, T2, and T2-FLAIR. We trained the parameters of a relatively small convolutional neural network (CNN) as follows: First, we randomly mutated the CNN weights. We then measured the CNN training set accuracy, using the latter as the fitness evaluation metric. The fittest child CNNs were identified. We incorporated their mutations into the parent CNN. This selectively mutated parent became the next generation's parent CNN. We repeated this process for approximately 50,000 generations. Results: DNE achieved monotonic convergence to 100% training set accuracy. DNE also converged monotonically to 100% testing set accuracy. Conclusions: DNE can achieve perfect accuracy with small training sets and small CNNs. Particularly when combined with Deep Reinforcement Learning, DNE may provide a path forward in the quest to make radiology AI more human-like in its ability to learn. DNE may very well turn out to be a key component of the much-anticipated meta-learning regime of radiology AI algorithms that can adapt to new tasks and new image types, similar to human radiologists. △ Less","24 December, 2021",https://arxiv.org/pdf/2112.12990
Towards Understanding Human Functional Brain Development with Explainable Artificial Intelligence: Challenges and Perspectives,Mehrin Kiani;Javier Andreu-Perez;Hani Hagras;Silvia Rigato;Maria Laura Filippetti,"The last decades have seen significant advancements in non-invasive neuroimaging technologies that have been increasingly adopted to examine human brain development. However, these improvements have not necessarily been followed by more sophisticated data analysis measures that are able to explain the mechanisms underlying functional brain development. For example, the shift from univariate (single area in the brain) to multivariate (multiple areas in brain) analysis paradigms is of significance as it allows investigations into the interactions between different brain regions. However, despite the potential of multivariate analysis to shed light on the interactions between developing brain regions, artificial intelligence (AI) techniques applied render the analysis non-explainable. The purpose of this paper is to understand the extent to which current state-of-the-art AI techniques can inform functional brain development. In addition, a review of which AI techniques are more likely to explain their learning based on the processes of brain development as defined by developmental cognitive neuroscience (DCN) frameworks is also undertaken. This work also proposes that eXplainable AI (XAI) may provide viable methods to investigate functional brain development as hypothesised by DCN frameworks. △ Less","23 December, 2021",https://arxiv.org/pdf/2112.12910
Graph Few-shot Class-incremental Learning,Zhen Tan;Kaize Ding;Ruocheng Guo;Huan Liu,"The ability to incrementally learn new classes is vital to all real-world artificial intelligence systems. A large portion of high-impact applications like social media, recommendation systems, E-commerce platforms, etc. can be represented by graph models. In this paper, we investigate the challenging yet practical problem, Graph Few-shot Class-incremental (Graph FCL) problem, where the graph model is tasked to classify both newly encountered classes and previously learned classes. Towards that purpose, we put forward a Graph Pseudo Incremental Learning paradigm by sampling tasks recurrently from the base classes, so as to produce an arbitrary number of training episodes for our model to practice the incremental learning skill. Furthermore, we design a Hierarchical-Attention-based Graph Meta-learning framework, HAG-Meta. We present a task-sensitive regularizer calculated from task-level attention and node class prototypes to mitigate overfitting onto either novel or base classes. To employ the topological knowledge, we add a node-level attention module to adjust the prototype representation. Our model not only achieves greater stability of old knowledge consolidation, but also acquires advantageous adaptability to new knowledge with very limited data samples. Extensive experiments on three real-world datasets, including Amazon-clothing, Reddit, and DBLP, show that our framework demonstrates remarkable advantages in comparison with the baseline and other related state-of-the-art methods. △ Less","23 December, 2021",https://arxiv.org/pdf/2112.12819
AI-based Reconstruction for Fast MRI -- A Systematic Review and Meta-analysis,Yutong Chen;Carola-Bibiane Schönlieb;Pietro Liò;Tim Leiner;Pier Luigi Dragotti;Ge Wang;Daniel Rueckert;David Firmin;Guang Yang,"Compressed sensing (CS) has been playing a key role in accelerating the magnetic resonance imaging (MRI) acquisition process. With the resurgence of artificial intelligence, deep neural networks and CS algorithms are being integrated to redefine the state of the art of fast MRI. The past several years have witnessed substantial growth in the complexity, diversity, and performance of deep learning-based CS techniques that are dedicated to fast MRI. In this meta-analysis, we systematically review the deep learning-based CS techniques for fast MRI, describe key model designs, highlight breakthroughs, and discuss promising directions. We have also introduced a comprehensive analysis framework and a classification system to assess the pivotal role of deep learning in CS-based acceleration for MRI. △ Less","23 December, 2021",https://arxiv.org/pdf/2112.12744
"Beyond Low Earth Orbit: Biological Research, Artificial Intelligence, and Self-Driving Labs",Lauren M. Sanders;Jason H. Yang;Ryan T. Scott;Amina Ann Qutub;Hector Garcia Martin;Daniel C. Berrios;Jaden J. A. Hastings;Jon Rask;Graham Mackintosh;Adrienne L. Hoarfrost;Stuart Chalk;John Kalantari;Kia Khezeli;Erik L. Antonsen;Joel Babdor;Richard Barker;Sergio E. Baranzini;Afshin Beheshti;Guillermo M. Delgado-Aparicio;Benjamin S. Glicksberg;Casey S. Greene;Melissa Haendel;Arif A. Hamid;Philip Heller;Daniel Jamieson,"Space biology research aims to understand fundamental effects of spaceflight on organisms, develop foundational knowledge to support deep space exploration, and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem of plants, crops, microbes, animals, and humans for sustained multi-planetary life. To advance these aims, the field leverages experiments, platforms, data, and model organisms from both spaceborne and ground-analog studies. As research is extended beyond low Earth orbit, experiments and platforms must be maximally autonomous, light, agile, and intelligent to expedite knowledge discovery. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration on artificial intelligence, machine learning, and modeling applications which offer key solutions toward these space biology challenges. In the next decade, the synthesis of artificial intelligence into the field of space biology will deepen the biological understanding of spaceflight effects, facilitate predictive modeling and analytics, support maximally autonomous and reproducible experiments, and efficiently manage spaceborne data and metadata, all with the goal to enable life to thrive in deep space. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.12582
"Beyond Low Earth Orbit: Biomonitoring, Artificial Intelligence, and Precision Space Health",Ryan T. Scott;Erik L. Antonsen;Lauren M. Sanders;Jaden J. A. Hastings;Seung-min Park;Graham Mackintosh;Robert J. Reynolds;Adrienne L. Hoarfrost;Aenor Sawyer;Casey S. Greene;Benjamin S. Glicksberg;Corey A. Theriot;Daniel C. Berrios;Jack Miller;Joel Babdor;Richard Barker;Sergio E. Baranzini;Afshin Beheshti;Stuart Chalk;Guillermo M. Delgado-Aparicio;Melissa Haendel;Arif A. Hamid;Philip Heller;Daniel Jamieson;Katelyn J. Jarvis,"Human space exploration beyond low Earth orbit will involve missions of significant distance and duration. To effectively mitigate myriad space health hazards, paradigm shifts in data and space health systems are necessary to enable Earth-independence, rather than Earth-reliance. Promising developments in the fields of artificial intelligence and machine learning for biology and health can address these needs. We propose an appropriately autonomous and intelligent Precision Space Health system that will monitor, aggregate, and assess biomedical statuses; analyze and predict personalized adverse health outcomes; adapt and respond to newly accumulated data; and provide preventive, actionable, and timely insights to individual deep space crew members and iterative decision support to their crew medical officer. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration, on future applications of artificial intelligence in space biology and health. In the next decade, biomonitoring technology, biomarker science, spacecraft hardware, intelligent software, and streamlined data management must mature and be woven together into a Precision Space Health system to enable humanity to thrive in deep space. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.12554
Collaborative adversary nodes learning on the logs of IoT devices in an IoT network,Sandhya Aneja;Melanie Ang Xuan En;Nagender Aneja,"Artificial Intelligence (AI) development has encouraged many new research areas, including AI-enabled Internet of Things (IoT) network. AI analytics and intelligent paradigms greatly improve learning efficiency and accuracy. Applying these learning paradigms to network scenarios provide technical advantages of new networking solutions. In this paper, we propose an improved approach for IoT security from data perspective. The network traffic of IoT devices can be analyzed using AI techniques. The Adversary Learning (AdLIoTLog) model is proposed using Recurrent Neural Network (RNN) with attention mechanism on sequences of network events in the network traffic. We define network events as a sequence of the time series packets of protocols captured in the log. We have considered different packets TCP packets, UDP packets, and HTTP packets in the network log to make the algorithm robust. The distributed IoT devices can collaborate to cripple our world which is extending to Internet of Intelligence. The time series packets are converted into structured data by removing noise and adding timestamps. The resulting data set is trained by RNN and can detect the node pairs collaborating with each other. We used the BLEU score to evaluate the model performance. Our results show that the predicting performance of the AdLIoTLog model trained by our method degrades by 3-4% in the presence of attack in comparison to the scenario when the network is not under attack. AdLIoTLog can detect adversaries because when adversaries are present the model gets duped by the collaborative events and therefore predicts the next event with a biased event rather than a benign event. We conclude that AI can provision ubiquitous learning for the new generation of Internet of Things. △ Less","21 December, 2021",https://arxiv.org/pdf/2112.12546
Hybrid Human-AI Curriculum Development for Personalised Informal Learning Environments,Mohammadreza Tavakoli;Abdolali Faraji;Mohammadreza Molavi;Stefan T. Mol;Gábor Kismihók,"Informal learning procedures have been changing extremely fast over the recent decades not only due to the advent of online learning, but also due to changes in what humans need to learn to meet their various life and career goals. Consequently, online, educational platforms are expected to provide personalized, up-to-date curricula to assist learners. Therefore, in this paper, we propose an Artificial Intelligence (AI) and Crowdsourcing based approach to create and update curricula for individual learners. We show the design of this curriculum development system prototype, in which contributors receive AI-based recommendations to be able to define and update high-level learning goals, skills, and learning topics together with associated learning content. This curriculum development system was also integrated into our personalized online learning platform. To evaluate our prototype we compared experts' opinion with our system's recommendations, and resulted in 89%, 79%, and 93% F1-scores when recommending skills, learning topics, and educational materials respectively. Also, we interviewed eight senior level experts from educational institutions and career consulting organizations. Interviewees agreed that our curriculum development method has high potential to support authoring activities in dynamic, personalized learning environments. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.12100
Machine Learning for Computational Science and Engineering -- a brief introduction and some critical questions,Chennakesava Kadapa,"Artificial Intelligence (AI) is now entering every sub-field of science, technology, engineering, arts, and management. Thanks to the hype and availability of research funds, it is being adapted in many fields without much thought. Computational Science and Engineering (CS&E) is one such sub-field. By highlighting some critical questions around the issues and challenges in adapting Machine Learning (ML) for CS&E, most of which are often overlooked in journal papers, this contribution hopes to offer some insights into the adaptation of ML for applications in CS\&E and related fields. This is a general-purpose article written for a general audience and researchers new to the fields of ML and/or CS\&E. This work focuses only on the forward problems in computational science and engineering. Some basic equations and MATLAB code are also provided to help the reader understand the basics. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.12054
Catch Me If You GAN: Using Artificial Intelligence for Fake Log Generation,Christian Toemmel,"With artificial intelligence (AI) becoming relevant in various parts of everyday life, other technologies are already widely influenced by the new way of handling large amounts of data. Although widespread already, AI has had only punctual influences on the cybersecurity field specifically. Many techniques and technologies used by cybersecurity experts function through manual labor and barely draw on automation, e.g., logs are often reviewed manually by system admins for potentially malicious keywords. This work evaluates the use of a special type of AI called generative adversarial networks (GANs) for log generation. More precisely, three different generative adversarial networks, SeqGAN, MaliGAN, and CoT, are reviewed in this research regarding their performance, focusing on generating new logs as a means of deceiving system admins for red teams. Although static generators for fake logs have been around for a while, their produces are usually easy to reveal as such. Using AI as an approach to this problem has not been widely researched. Identified challenges consist of formatting, dates and times, and overall consistency. Summing up the results, GANs seem not to be a good fit for generating fake logs. Their capability to detect fake logs, however, might be of use in practical scenarios. △ Less","22 December, 2021",https://arxiv.org/pdf/2112.12006
Multigoal-oriented dual-weighted-residual error estimation using deep neural networks,Ayan Chakraborty;Thomas Wick;Xiaoying Zhuang;Timon Rabczuk,"Deep learning has shown successful application in visual recognition and certain artificial intelligence tasks. Deep learning is also considered as a powerful tool with high flexibility to approximate functions. In the present work, functions with desired properties are devised to approximate the solutions of PDEs. Our approach is based on a posteriori error estimation in which the adjoint problem is solved for the error localization to formulate an error estimator within the framework of neural network. An efficient and easy to implement algorithm is developed to obtain a posteriori error estimate for multiple goal functionals by employing the dual-weighted residual approach, which is followed by the computation of both primal and adjoint solutions using the neural network. The present study shows that such a data-driven model based learning has superior approximation of quantities of interest even with relatively less training data. The novel algorithmic developments are substantiated with numerical test examples. The advantages of using deep neural network over the shallow neural network are demonstrated and the convergence enhancing techniques are also presented △ Less","22 December, 2021",https://arxiv.org/pdf/2112.11360
Waveform-Defined Security: A Low-Cost Framework for Secure Communications,Tongyang Xu,"Communication security could be enhanced at physical layer but at the cost of complex algorithms and redundant hardware, which would render traditional physical layer security (PLS) techniques unsuitable for use with resource-constrained communication systems. This work investigates a waveform-defined security (WDS) framework, which differs fundamentally from traditional PLS techniques used in today's systems. The framework is not dependent on channel conditions such as signal power advantage and channel state information (CSI). Therefore, the framework is more reliable than channel dependent beamforming and artificial noise (AN) techniques. In addition, the framework is more than just increasing the cost of eavesdropping. By intentionally tuning waveform patterns to weaken signal feature diversity and enhance feature similarity, eavesdroppers will not be able to identify correctly signal formats. The wrong classification of signal formats would result in subsequent detection errors even when an eavesdropper uses brute-force detection techniques. To get a robust WDS framework, three impact factors, namely training data feature, oversampling factor and bandwidth compression factor (BCF) offset, are investigated. An optimal WDS waveform pattern is obtained at the end after a joint study of the three factors. To ensure a valid eavesdropping model, artificial intelligence (AI) dependent signal classifiers are designed followed by optimal performance achievable signal detectors. To show the compatibility in available communication systems, the WDS framework is successfully integrated in IEEE 802.11a with nearly no adding computational complexity. Finally, a low-cost software-defined radio (SDR) experiment is designed to verify the feasibility of the WDS framework in resource-constrained communications. △ Less","21 December, 2021",https://arxiv.org/pdf/2112.11350
Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems,Xu Kang;Bin Song;Jie Guo;Zhijin Qin;F. Richard Yu,"The vigorous developments of Internet of Things make it possible to extend its computing and storage capabilities to computing tasks in the aerial system with collaboration of cloud and edge, especially for artificial intelligence (AI) tasks based on deep learning (DL). Collecting a large amount of image/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent analysis tasks to the back-end mobile edge computing (MEC) server due to their limited storage and computing capabilities. How to efficiently transmit the most correlated information for the AI model is a challenging topic. Inspired by the task-oriented communication in recent years, we propose a new aerial image transmission paradigm for the scene classification task. A lightweight model is developed on the front-end UAV for semantic blocks transmission with perception of images and channel conditions. In order to achieve the tradeoff between transmission latency and classification accuracy, deep reinforcement learning (DRL) is used to explore the semantic blocks which have the best contribution to the back-end classifier under various channel conditions. Experimental results show that the proposed method can significantly improve classification accuracy compared to the fixed transmission strategy and traditional content perception methods. △ Less","20 December, 2021",https://arxiv.org/pdf/2112.10948
Adaptive Incentive Design with Multi-Agent Meta-Gradient Reinforcement Learning,Jiachen Yang;Ethan Wang;Rakshit Trivedi;Tuo Zhao;Hongyuan Zha,"Critical sectors of human society are progressing toward the adoption of powerful artificial intelligence (AI) agents, which are trained individually on behalf of self-interested principals but deployed in a shared environment. Short of direct centralized regulation of AI, which is as difficult an issue as regulation of human actions, one must design institutional mechanisms that indirectly guide agents' behaviors to safeguard and improve social welfare in the shared environment. Our paper focuses on one important class of such mechanisms: the problem of adaptive incentive design, whereby a central planner intervenes on the payoffs of an agent population via incentives in order to optimize a system objective. To tackle this problem in high-dimensional environments whose dynamics may be unknown or too complex to model, we propose a model-free meta-gradient method to learn an adaptive incentive function in the context of multi-agent reinforcement learning. Via the principle of online cross-validation, the incentive designer explicitly accounts for its impact on agents' learning and, through them, the impact on future social welfare. Experiments on didactic benchmark problems show that the proposed method can induce selfish agents to learn near-optimal cooperative behavior and significantly outperform learning-oblivious baselines. When applied to a complex simulated economy, the proposed method finds tax policies that achieve better trade-off between economic productivity and equality than baselines, a result that we interpret via a detailed behavioral analysis. △ Less","20 December, 2021",https://arxiv.org/pdf/2112.10859
Context-Based Music Recommendation Algorithm Evaluation,Marissa Baxter;Lisa Ha;Kirill Perfiliev;Natalie Sayre,"Artificial Intelligence (AI ) has been very successful in creating and predicting music playlists for online users based on their data; data received from users experience using the app such as searching the songs they like. There are lots of current technological advancements in AI due to the competition between music platform owners such as Spotify, Pandora, and more. In this paper, 6 machine learning algorithms and their individual accuracy for predicting whether a user will like a song are explored across 3 different platforms including Weka, SKLearn, and Orange. The algorithms explored include Logistic Regression, Naive Bayes, Sequential Minimal Optimization (SMO), Multilayer Perceptron (Neural Network), Nearest Neighbor, and Random Forest. With the analysis of the specific characteristics of each song provided by the Spotify API [1], Random Forest is the most successful algorithm for predicting whether a user will like a song with an accuracy of 84%. This is higher than the accuracy of 82.72% found by Mungekar using the Random Forest technique and slightly different characteristics of a song [2]. The characteristics in Mungekars Random Forest algorithm focus more on the artist and popularity rather than the sonic features of the songs. Removing the popularity aspect and focusing purely on the sonic qualities improve the accuracy of recommendations. Finally, this paper shows how song prediction can be accomplished without any monetary investments, and thus, inspires an idea of what amazing results can be accomplished with full financial research. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.10612
Supervised laser-speckle image sampling of skin tissue to detect very early stage of diabetes by its effects on skin subcellular properties,Ahmet Orun;Luke Vella Critien;Jennifer Carter;Martin Stacey,"This paper investigates the effectiveness of an expert system based on K-nearest neighbors algorithm for laser speckle image sampling applied to the early detection of diabetes. With the latest developments in artificial intelligent guided laser speckle imaging technologies, it may be possible to optimise laser parameters, such as wavelength, energy level and image texture measures in association with a suitable AI technique to interact effectively with the subcellular properties of a skin tissue to detect early signs of diabetes. The new approach is potentially more effective than the classical skin glucose level observation because of its optimised combination of laser physics and AI techniques, and additionally, it allows non-expert individuals to perform more frequent skin tissue tests for an early detection of diabetes. △ Less","18 December, 2021",https://arxiv.org/pdf/2112.10024
On the Evolution of the MCTS Upper Confidence Bounds for Trees by Means of Evolutionary Algorithms in the Game of Carcassonne,Edgar Galván;Gavin Simpson,"Monte Carlo Tree Search (MCTS) is a sampling best-first method to search for optimal decisions. The MCTS's popularity is based on its extraordinary results in the challenging two-player based game Go, a game considered much harder than Chess and that until very recently was considered infeasible for Artificial Intelligence methods. The success of MCTS depends heavily on how the tree is built and the selection process plays a fundamental role in this. One particular selection mechanism that has proved to be reliable is based on the Upper Confidence Bounds for Trees, commonly referred as UCT. The UCT attempts to nicely balance exploration and exploitation by considering the values stored in the statistical tree of the MCTS. However, some tuning of the MCTS UCT is necessary for this to work well. In this work, we use Evolutionary Algorithms (EAs) to evolve mathematical expressions with the goal to substitute the UCT mathematical expression. We compare our proposed approach, called Evolution Strategy in MCTS (ES-MCTS) against five variants of the MCTS UCT, three variants of the star-minimax family of algorithms as well as a random controller in the Game of Carcassonne. We also use a variant of our proposed EA-based controller, dubbed ES partially integrated in MCTS. We show how the ES-MCTS controller, is able to outperform all these 10 intelligent controllers, including robust MCTS UCT controllers. △ Less","17 December, 2021",https://arxiv.org/pdf/2112.09697
Learning Reward Machines: A Study in Partially Observable Reinforcement Learning,Rodrigo Toro Icarte;Ethan Waldie;Toryn Q. Klassen;Richard Valenzano;Margarita P. Castro;Sheila A. McIlraith,"Reinforcement learning (RL) is a central problem in artificial intelligence. This problem consists of defining artificial agents that can learn optimal behaviour by interacting with an environment -- where the optimal behaviour is defined with respect to a reward signal that the agent seeks to maximize. Reward machines (RMs) provide a structured, automata-based representation of a reward function that enables an RL agent to decompose an RL problem into structured subproblems that can be efficiently learned via off-policy learning. Here we show that RMs can be learned from experience, instead of being specified by the user, and that the resulting problem decomposition can be used to effectively solve partially observable RL problems. We pose the task of learning RMs as a discrete optimization problem where the objective is to find an RM that decomposes the problem into a set of subproblems such that the combination of their optimal memoryless policies is an optimal policy for the original problem. We show the effectiveness of this approach on three partially observable domains, where it significantly outperforms A3C, PPO, and ACER, and discuss its advantages, limitations, and broader potential. △ Less","17 December, 2021",https://arxiv.org/pdf/2112.09477
Towards Intelligent Context-Aware 6G Security,André N. Barreto;Stefan Köpsell;Arsenia Chorti;Bertram Poettering;Jens Jelitto;Julia Hesse;Jonathan Boole;Konrad Rieck;Marios Kountouris;Dave Singelee;Kumar Ashwinee,"Imagine interconnected objects with embedded artificial intelligence (AI), empowered to sense the environment, see it, hear it, touch it, interact with it, and move. As future networks of intelligent objects come to life, tremendous new challenges arise for security, but also new opportunities, allowing to address current, as well as future, pressing needs. In this paper we put forward a roadmap towards the realization of a new security paradigm that we articulate as intelligent context-aware security. The premise of this roadmap is that sensing and advanced AI will enable context awareness, which in turn can drive intelligent security mechanisms, such as adaptation and automation of security controls. This concept not only provides immediate answers to burning open questions, in particular with respect to non-functional requirements, such as energy or latency constraints, heterogeneity of radio frequency (RF) technologies and long life span of deployed devices, but also, more importantly, offers a viable answer to scalability by allowing such constraints to be met even in massive connectivity regimes. Furthermore, the proposed roadmap has to be designed ethically, by explicitly placing privacy concerns at its core. The path towards this vision and some of the challenges along the way are discussed in this contribution. △ Less","17 December, 2021",https://arxiv.org/pdf/2112.09411
AI-Empowered Persuasive Video Generation: A Survey,Chang Liu;Han Yu,"Promotional videos are rapidly becoming a popular medium for persuading people to change their behaviours in many settings (e.g., online shopping, social enterprise initiatives). Today, such videos are often produced by professionals, which is a time-, labour- and cost-intensive undertaking. In order to produce such contents to support a large applications (e.g., e-commerce), the field of artificial intelligence (AI)-empowered persuasive video generation (AIPVG) has gained traction in recent years. This field is interdisciplinary in nature, which makes it challenging for new researchers to grasp. Currently, there is no comprehensive survey of AIPVG available. In this paper, we bridge this gap by reviewing key AI techniques that can be utilized to automatically generate persuasive videos. We offer a first-of-its-kind taxonomy which divides AIPVG into three major steps: 1) visual material understanding, which extracts information from the visual materials (VMs) relevant to the target of promotion; 2) visual storyline generation, which shortlists and arranges high-quality VMs into a sequence in order to compose a storyline with persuasive power; and 3) post-production, which involves background music generation and still image animation to enhance viewing experience. We also introduce the evaluation metrics and datasets commonly adopted in the field of AIPVG. We analyze the advantages and disadvantages of the existing works belonging to the above-mentioned steps, and discuss interesting potential future research directions. △ Less","17 December, 2021",https://arxiv.org/pdf/2112.09401
Can Machine Learning Tools Support the Identification of Sustainable Design Leads From Product Reviews? Opportunities and Challenges,Michael Saidani;Harrison Kim;Bernard Yannou,"The increasing number of product reviews posted online is a gold mine for designers to know better about the products they develop, by capturing the voice of customers, and to improve these products accordingly. In the meantime, product design and development have an essential role in creating a more sustainable future. With the recent advance of artificial intelligence techniques in the field of natural language processing, this research aims to develop an integrated machine learning solution to obtain sustainable design insights from online product reviews automatically. In this paper, the opportunities and challenges offered by existing frameworks - including Python libraries, packages, as well as state-of-the-art algorithms like BERT - are discussed, illustrated, and positioned along an ad hoc machine learning process. This contribution discusses the opportunities to reach and the challenges to address for building a machine learning pipeline, in order to get insights from product reviews to design more sustainable products, including the five following stages, from the identification of sustainability-related reviews to the interpretation of sustainable design leads: data collection, data formatting, model training, model evaluation, and model deployment. Examples of sustainable design insights that can be produced out of product review mining and processing are given. Finally, promising lines for future research in the field are provided, including case studies putting in parallel standard products with their sustainable alternatives, to compare the features valued by customers and to generate in fine relevant sustainable design leads. △ Less","17 December, 2021",https://arxiv.org/pdf/2112.09391
Classification of diffraction patterns using a convolutional neural network in single particle imaging experiments performed at X-ray free-electron lasers,Dameli Assalauova;Alexandr Ignatenko;Fabian Isensee;Sergey Bobkov;Darya Trofimova;Ivan A. Vartanyants,"Single particle imaging (SPI) at X-ray free electron lasers (XFELs) is particularly well suited to determine the 3D structure of particles in their native environment. For a successful reconstruction, diffraction patterns originating from a single hit must be isolated from a large number of acquired patterns. We propose to formulate this task as an image classification problem and solve it using convolutional neural network (CNN) architectures. Two CNN configurations are developed: one that maximises the F1-score and one that emphasises high recall. We also combine the CNNs with expectation maximization (EM) selection as well as size filtering. We observed that our CNN selections have lower contrast in power spectral density functions relative to the EM selection, used in our previous work. However, the reconstruction of our CNN-based selections gives similar results. Introducing CNNs into SPI experiments allows streamlining the reconstruction pipeline, enables researchers to classify patterns on the fly, and, as a consequence, enables them to tightly control the duration of their experiments. We think that bringing non-standard artificial intelligence (AI) based solutions in a well-described SPI analysis workflow may be beneficial for the future development of the SPI experiments. △ Less","16 December, 2021",https://arxiv.org/pdf/2112.09020
COVID-19 Electrocardiograms Classification using CNN Models,Ismail Shahin;Ali Bou Nassif;Mohamed Bader Alsabek,"With the periodic rise and fall of COVID-19 and numerous countries being affected by its ramifications, there has been a tremendous amount of work that has been done by scientists, researchers, and doctors all over the world. Prompt intervention is keenly needed to tackle the unconscionable dissemination of the disease. The implementation of Artificial Intelligence (AI) has made a significant contribution to the digital health district by applying the fundamentals of deep learning algorithms. In this study, a novel approach is proposed to automatically diagnose the COVID-19 by the utilization of Electrocardiogram (ECG) data with the integration of deep learning algorithms, specifically the Convolutional Neural Network (CNN) models. Several CNN models have been utilized in this proposed framework, including VGG16, VGG19, InceptionResnetv2, InceptionV3, Resnet50, and Densenet201. The VGG16 model has outperformed the rest of the models, with an accuracy of 85.92%. Our results show a relatively low accuracy in the rest of the models compared to the VGG16 model, which is due to the small size of the utilized dataset, in addition to the exclusive utilization of the Grid search hyperparameters optimization approach for the VGG16 model only. Moreover, our results are preparatory, and there is a possibility to enhance the accuracy of all models by further expanding the dataset and adapting a suitable hyperparameters optimization technique. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.08931
Intelli-Paint: Towards Developing Human-like Painting Agents,Jaskirat Singh;Cameron Smith;Jose Echevarria;Liang Zheng,"The generation of well-designed artwork is often quite time-consuming and assumes a high degree of proficiency on part of the human painter. In order to facilitate the human painting process, substantial research efforts have been made on teaching machines how to ""paint like a human"", and then using the trained agent as a painting assistant tool for human users. However, current research in this direction is often reliant on a progressive grid-based division strategy wherein the agent divides the overall image into successively finer grids, and then proceeds to paint each of them in parallel. This inevitably leads to artificial painting sequences which are not easily intelligible to human users. To address this, we propose a novel painting approach which learns to generate output canvases while exhibiting a more human-like painting style. The proposed painting pipeline Intelli-Paint consists of 1) a progressive layering strategy which allows the agent to first paint a natural background scene representation before adding in each of the foreground objects in a progressive fashion. 2) We also introduce a novel sequential brushstroke guidance strategy which helps the painting agent to shift its attention between different image regions in a semantic-aware manner. 3) Finally, we propose a brushstroke regularization strategy which allows for ~60-80% reduction in the total number of required brushstrokes without any perceivable differences in the quality of the generated canvases. Through both quantitative and qualitative results, we show that the resulting agents not only show enhanced efficiency in output canvas generation but also exhibit a more natural-looking painting style which would better assist human users express their ideas through digital artwork. △ Less","16 December, 2021",https://arxiv.org/pdf/2112.08930
OptABC: an Optimal Hyperparameter Tuning Approach for Machine Learning Algorithms,Leila Zahedi;Farid Ghareh Mohammadi;M. Hadi Amini,"Hyperparameter tuning in machine learning algorithms is a computationally challenging task due to the large-scale nature of the problem. In order to develop an efficient strategy for hyper-parameter tuning, one promising solution is to use swarm intelligence algorithms. Artificial Bee Colony (ABC) optimization lends itself as a promising and efficient optimization algorithm for this purpose. However, in some cases, ABC can suffer from a slow convergence rate or execution time due to the poor initial population of solutions and expensive objective functions. To address these concerns, a novel algorithm, OptABC, is proposed to help ABC algorithm in faster convergence toward a near-optimum solution. OptABC integrates artificial bee colony algorithm, K-Means clustering, greedy algorithm, and opposition-based learning strategy for tuning the hyper-parameters of different machine learning models. OptABC employs these techniques in an attempt to diversify the initial population, and hence enhance the convergence ability without significantly decreasing the accuracy. In order to validate the performance of the proposed method, we compare the results with previous state-of-the-art approaches. Experimental results demonstrate the effectiveness of the OptABC compared to existing approaches in the literature. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.08511
"The Need for Ethical, Responsible, and Trustworthy Artificial Intelligence for Environmental Sciences",Amy McGovern;Imme Ebert-Uphoff;David John Gagne II;Ann Bostrom,"Given the growing use of Artificial Intelligence (AI) and machine learning (ML) methods across all aspects of environmental sciences, it is imperative that we initiate a discussion about the ethical and responsible use of AI. In fact, much can be learned from other domains where AI was introduced, often with the best of intentions, yet often led to unintended societal consequences, such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system. A common misconception is that the environmental sciences are immune to such unintended consequences when AI is being used, as most data come from observations, and AI algorithms are based on mathematical formulas, which are often seen as objective. In this article, we argue the opposite can be the case. Using specific examples, we demonstrate many ways in which the use of AI can introduce similar consequences in the environmental sciences. This article will stimulate discussion and research efforts in this direction. As a community, we should avoid repeating any foreseeable mistakes made in other domains through the introduction of AI. In fact, with proper precautions, AI can be a great tool to help {\it reduce} climate and environmental injustice. We primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.08453
Utilizing XAI technique to improve autoencoder based model for computer network anomaly detection with shapley additive explanation(SHAP),Khushnaseeb Roshan;Aasim Zafar,"Machine learning (ML) and Deep Learning (DL) methods are being adopted rapidly, especially in computer network security, such as fraud detection, network anomaly detection, intrusion detection, and much more. However, the lack of transparency of ML and DL based models is a major obstacle to their implementation and criticized due to its black-box nature, even with such tremendous results. Explainable Artificial Intelligence (XAI) is a promising area that can improve the trustworthiness of these models by giving explanations and interpreting its output. If the internal working of the ML and DL based models is understandable, then it can further help to improve its performance. The objective of this paper is to show that how XAI can be used to interpret the results of the DL model, the autoencoder in this case. And, based on the interpretation, we improved its performance for computer network anomaly detection. The kernel SHAP method, which is based on the shapley values, is used as a novel feature selection technique. This method is used to identify only those features that are actually causing the anomalous behaviour of the set of attack/anomaly instances. Later, these feature sets are used to train and validate the autoencoder but on benign data only. Finally, the built SHAP_Model outperformed the other two models proposed based on the feature selection method. This whole experiment is conducted on the subset of the latest CICIDS2017 network dataset. The overall accuracy and AUC of SHAP_Model is 94% and 0.969, respectively. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.08442
Towards Explainable Artificial Intelligence in Banking and Financial Services,Ambreen Hanif,"Artificial intelligence (AI) enables machines to learn from human experience, adjust to new inputs, and perform human-like tasks. AI is progressing rapidly and is transforming the way businesses operate, from process automation to cognitive augmentation of tasks and intelligent process/data analytics. However, the main challenge for human users would be to understand and appropriately trust the result of AI algorithms and methods. In this paper, to address this challenge, we study and analyze the recent work done in Explainable Artificial Intelligence (XAI) methods and tools. We introduce a novel XAI process, which facilitates producing explainable models while maintaining a high level of learning performance. We present an interactive evidence-based approach to assist human users in comprehending and trusting the results and output created by AI-enabled algorithms. We adopt a typical scenario in the Banking domain for analyzing customer transactions. We develop a digital dashboard to facilitate interacting with the algorithm results and discuss how the proposed XAI method can significantly improve the confidence of data scientists in understanding the result of AI-enabled algorithms. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.08441
"Est-ce que vous compute? Code-switching, cultural identity, and AI",Arianna Falbo;Travis LaCroix,"Cultural code-switching concerns how we adjust our overall behaviours, manners of speaking, and appearance in response to a perceived change in our social environment. We defend the need to investigate cultural code-switching capacities in artificial intelligence systems. We explore a series of ethical and epistemic issues that arise when bringing cultural code-switching to bear on artificial intelligence. Building upon Dotson's (2014) analysis of testimonial smothering, we discuss how emerging technologies in AI can give rise to epistemic oppression, and specifically, a form of self-silencing that we call 'cultural smothering'. By leaving the socio-dynamic features of cultural code-switching unaddressed, AI systems risk negatively impacting already-marginalised social groups by widening opportunity gaps and further entrenching social inequalities. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.08256
The exploitation of Multiple Feature Extraction Techniques for Speaker Identification in Emotional States under Disguised Voices,Noor Ahmad Al Hindawi;Ismail Shahin;Ali Bou Nassif,"Due to improvements in artificial intelligence, speaker identification (SI) technologies have brought a great direction and are now widely used in a variety of sectors. One of the most important components of SI is feature extraction, which has a substantial impact on the SI process and performance. As a result, numerous feature extraction strategies are thoroughly investigated, contrasted, and analyzed. This article exploits five distinct feature extraction methods for speaker identification in disguised voices under emotional environments. To evaluate this work significantly, three effects are used: high-pitched, low-pitched, and Electronic Voice Conversion (EVC). Experimental results reported that the concatenated Mel-Frequency Cepstral Coefficients (MFCCs), MFCCs-delta, and MFCCs-delta-delta is the best feature extraction method. △ Less","15 December, 2021",https://arxiv.org/pdf/2112.07940
Filling gaps in trustworthy development of AI,Shahar Avin;Haydn Belfield;Miles Brundage;Gretchen Krueger;Jasmine Wang;Adrian Weller;Markus Anderljung;Igor Krawczuk;David Krueger;Jonathan Lebensold;Tegan Maharaj;Noa Zilberman,"The range of application of artificial intelligence (AI) is vast, as is the potential for harm. Growing awareness of potential risks from AI systems has spurred action to address those risks, while eroding confidence in AI systems and the organizations that develop them. A 2019 study found over 80 organizations that published and adopted ""AI ethics principles'', and more have joined since. But the principles often leave a gap between the ""what"" and the ""how"" of trustworthy AI development. Such gaps have enabled questionable or ethically dubious behavior, which casts doubts on the trustworthiness of specific organizations, and the field more broadly. There is thus an urgent need for concrete methods that both enable AI developers to prevent harm and allow them to demonstrate their trustworthiness through verifiable behavior. Below, we explore mechanisms (drawn from arXiv:2004.07213) for creating an ecosystem where AI developers can earn trust - if they are trustworthy. Better assessment of developer trustworthiness could inform user choice, employee actions, investment decisions, legal recourse, and emerging governance regimes. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07773
Rushing and Strolling among Answer Sets -- Navigation Made Easy,Johannes K. Fichte;Sarah Alice Gaggl;Dominik Rusovac,"Answer set programming (ASP) is a popular declarative programming paradigm with a wide range of applications in artificial intelligence. Oftentimes, when modeling an AI problem with ASP, and in particular when we are interested beyond simple search for optimal solutions, an actual solution, differences between solutions, or number of solutions of the ASP program matter. For example, when a user aims to identify a specific answer set according to her needs, or requires the total number of diverging solutions to comprehend probabilistic applications such as reasoning in medical domains. Then, there are only certain problem specific and handcrafted encoding techniques available to navigate the solution space of ASP programs, which is oftentimes not enough. In this paper, we propose a formal and general framework for interactive navigation towards desired subsets of answer sets analogous to faceted browsing. Our approach enables the user to explore the solution space by consciously zooming in or out of sub-spaces of solutions at a certain configurable pace. We illustrate that weighted faceted navigation is computationally hard. Finally, we provide an implementation of our approach that demonstrates the feasibility of our framework for incomprehensible solution spaces. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07596
Levels of Autonomous Radiology,Suraj Ghuwalewala;Viraj Kulkarni;Richa Pant;Amit Kharat,"Radiology, being one of the younger disciplines of medicine with a history of just over a century, has witnessed tremendous technological advancements and has revolutionized the way we practice medicine today. In the last few decades, medical imaging modalities have generated seismic amounts of medical data. The development and adoption of Artificial Intelligence (AI) applications using this data will lead to the next phase of evolution in radiology. It will include automating laborious manual tasks such as annotations, report-generation, etc., along with the initial radiological assessment of cases to aid radiologists in their evaluation workflow. We propose a level-wise classification for the progression of automation in radiology, explaining AI assistance at each level with corresponding challenges and solutions. We hope that such discussions can help us address the challenges in a structured way and take the necessary steps to ensure the smooth adoption of new technologies in radiology. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07286
"Automatic COVID-19 disease diagnosis using 1D convolutional neural network and augmentation with human respiratory sound based on parameters: cough, breath, and voice",Kranthi Kumar Lella;Alphonse Pja,"The issue in respiratory sound classification has attained good attention from the clinical scientists and medical researcher's group in the last year to diagnosing COVID-19 disease. To date, various models of Artificial Intelligence (AI) entered into the real-world to detect the COVID-19 disease from human-generated sounds such as voice/speech, cough, and breath. The Convolutional Neural Network (CNN) model is implemented for solving a lot of real-world problems on machines based on Artificial Intelligence (AI). In this context, one dimension (1D) CNN is suggested and implemented to diagnose respiratory diseases of COVID-19 from human respiratory sounds such as a voice, cough, and breath. An augmentation-based mechanism is applied to improve the preprocessing performance of the COVID-19 sounds dataset and to automate COVID-19 disease diagnosis using the 1D convolutional network. Furthermore, a DDAE (Data De-noising Auto Encoder) technique is used to generate deep sound features such as the input function to the 1D CNN instead of adopting the standard input of MFCC (Mel-frequency cepstral coefficient), and it is performed better accuracy and performance than previous models. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07285
MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing Systems,Shreshth Tuli;Giuliano Casale;Nicholas R. Jennings,"Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user's service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07269
A real-time spatiotemporal AI model analyzes skill in open surgical videos,Emmett D. Goodman;Krishna K. Patel;Yilun Zhang;William Locke;Chris J. Kennedy;Rohan Mehrotra;Stephen Ren;Melody Y. Guan;Maren Downing;Hao Wei Chen;Jevin Z. Clark;Gabriel A. Brat;Serena Yeung,"Open procedures represent the dominant form of surgery worldwide. Artificial intelligence (AI) has the potential to optimize surgical practice and improve patient outcomes, but efforts have focused primarily on minimally invasive techniques. Our work overcomes existing data limitations for training AI models by curating, from YouTube, the largest dataset of open surgical videos to date: 1997 videos from 23 surgical procedures uploaded from 50 countries. Using this dataset, we developed a multi-task AI model capable of real-time understanding of surgical behaviors, hands, and tools - the building blocks of procedural flow and surgeon skill. We show that our model generalizes across diverse surgery types and environments. Illustrating this generalizability, we directly applied our YouTube-trained model to analyze open surgeries prospectively collected at an academic medical center and identified kinematic descriptors of surgical skill related to efficiency of hand motion. Our Annotated Videos of Open Surgery (AVOS) dataset and trained model will be made available for further development of surgical AI. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.07219
COVID-19 Pneumonia and Influenza Pneumonia Detection Using Convolutional Neural Networks,Julianna Antonchuk;Benjamin Prescott;Philip Melanchthon;Robin Singh,"In the research, we developed a computer vision solution to support diagnostic radiology in differentiating between COVID-19 pneumonia, influenza virus pneumonia, and normal biomarkers. The chest radiograph appearance of COVID-19 pneumonia is thought to be nonspecific, having presented a challenge to identify an optimal architecture of a convolutional neural network (CNN) that would classify with a high sensitivity among the pulmonary inflammation features of COVID-19 and non-COVID-19 types of pneumonia. Rahman (2021) states that COVID-19 radiography images observe unavailability and quality issues impacting the diagnostic process and affecting the accuracy of the deep learning detection models. A significant scarcity of COVID-19 radiography images introduced an imbalance in data motivating us to use over-sampling techniques. In the study, we include an extensive set of X-ray imaging of human lungs (CXR) with COVID-19 pneumonia, influenza virus pneumonia, and normal biomarkers to achieve an extensible and accurate CNN model. In the experimentation phase of the research, we evaluated a variety of convolutional network architectures, selecting a sequential convolutional network with two traditional convolutional layers and two pooling layers with maximum function. In its classification performance, the best performing model demonstrated a validation accuracy of 93% and an F1 score of 0.95. We chose the Azure Machine Learning service to perform network experimentation and solution deployment. The auto-scaling compute clusters offered a significant time reduction in network training. We would like to see scientists across fields of artificial intelligence and human biology collaborating and expanding on the proposed solution to provide rapid and comprehensive diagnostics, effectively mitigating the spread of the virus △ Less","13 December, 2021",https://arxiv.org/pdf/2112.07102
A Review: Challenges and Opportunities for Artificial Intelligence and Robotics in the Offshore Wind Sector,Daniel Mitchell;Jamie Blanche;Sam Harper;Theodore Lim;Ranjeetkumar Gupta;Osama Zaki;Wenshuo Tang;Valentin Robu;Simon Watson;David Flynn,"A global trend in increasing wind turbine size and distances from shore is emerging within the rapidly growing offshore wind farm market. In the UK, the offshore wind sector produced its highest amount of electricity in the UK in 2019, a 19.6% increase on the year before. Currently, the UK is set to increase production further, targeting a 74.7% increase of installed turbine capacity as reflected in recent Crown Estate leasing rounds. With such tremendous growth, the sector is now looking to Robotics and Artificial Intelligence (RAI) in order to tackle lifecycle service barriers as to support sustainable and profitable offshore wind energy production. Today, RAI applications are predominately being used to support short term objectives in operation and maintenance. However, moving forward, RAI has the potential to play a critical role throughout the full lifecycle of offshore wind infrastructure, from surveying, planning, design, logistics, operational support, training and decommissioning. This paper presents one of the first systematic reviews of RAI for the offshore renewable energy sector. The state-of-the-art in RAI is analyzed with respect to offshore energy requirements, from both industry and academia, in terms of current and future requirements. Our review also includes a detailed evaluation of investment, regulation and skills development required to support the adoption of RAI. The key trends identified through a detailed analysis of patent and academic publication databases provide insights to barriers such as certification of autonomous platforms for safety compliance and reliability, the need for digital architectures for scalability in autonomous fleets, adaptive mission planning for resilient resident operations and optimization of human machine interaction for trusted partnerships between people and autonomous assistants. △ Less","13 December, 2021",https://arxiv.org/pdf/2112.06620
A Complete Characterisation of ReLU-Invariant Distributions,Jan Macdonald;Stephan Wäldchen,"We give a complete characterisation of families of probability distributions that are invariant under the action of ReLU neural network layers. The need for such families arises during the training of Bayesian networks or the analysis of trained neural networks, e.g., in the context of uncertainty quantification (UQ) or explainable artificial intelligence (XAI). We prove that no invariant parametrised family of distributions can exist unless at least one of the following three restrictions holds: First, the network layers have a width of one, which is unreasonable for practical neural networks. Second, the probability measures in the family have finite support, which basically amounts to sampling distributions. Third, the parametrisation of the family is not locally Lipschitz continuous, which excludes all computationally feasible families. Finally, we show that these restrictions are individually necessary. For each of the three cases we can construct an invariant family exploiting exactly one of the restrictions but not the other two. △ Less","13 December, 2021",https://arxiv.org/pdf/2112.06532
Ex-Model: Continual Learning from a Stream of Trained Models,Antonio Carta;Andrea Cossu;Vincenzo Lomonaco;Davide Bacciu,"Learning continually from non-stationary data streams is a challenging research topic of growing popularity in the last few years. Being able to learn, adapt, and generalize continually in an efficient, effective, and scalable way is fundamental for a sustainable development of Artificial Intelligent systems. However, an agent-centric view of continual learning requires learning directly from raw data, which limits the interaction between independent agents, the efficiency, and the privacy of current approaches. Instead, we argue that continual learning systems should exploit the availability of compressed information in the form of trained models. In this paper, we introduce and formalize a new paradigm named ""Ex-Model Continual Learning"" (ExML), where an agent learns from a sequence of previously trained models instead of raw data. We further contribute with three ex-model continual learning algorithms and an empirical setting comprising three datasets (MNIST, CIFAR-10 and CORe50), and eight scenarios, where the proposed algorithms are extensively tested. Finally, we highlight the peculiarities of the ex-model paradigm and we point out interesting future research directions. △ Less","13 December, 2021",https://arxiv.org/pdf/2112.06511
On the Physical Layer Security Performance over RIS-aided Dual-hop RF-UOWC Mixed Network,T. Hossain;S. Shabab;A. S. M. Badrudduza;M. K. Kundu;I. S. Ansari,"Since security has been one of the crucial issues for high-yield communications such as 5G and 6G, the researchers continuously come up with newer techniques to enhance the security and performance of these progressive wireless communications. Reconfigurable intelligent surface (RIS) is one of those techniques that artificially rearrange and optimize the propagation environment of electromagnetic waves to improve both spectrum and energy efficiency of wireless networks. Besides, in underwater communication, underwater optical wireless communication (UOWC) is a better alternative/replacement for conventional acoustic and radio frequency (RF) technologies. Hence, mixed RIS-aided RF-UOWC can be treated as a promising technology for future wireless networks. This work focuses on the secrecy performance of mixed dual-hop RIS-aided RF-UOWC networks under the intercepting effort of a probable eavesdropper. The RF link operates under generalized Gamma fading distribution; likewise, the UOWC link experiences the mixture exponential generalized Gamma distribution. The secrecy analysis subsumes the derivations of closed-form expressions for average secrecy capacity, exact and lower bound of secrecy outage probability, and strictly positive secrecy capacity, all in terms of Meijer G functions. Capitalizing on these derivations, the effects of heterodyne and intensity modulation/direct detection systems, underwater turbulence resulting from air bubble levels, temperature gradients, and salinity gradients, are measured. Unlike conventional models that merely deal with thermally uniform scenarios, this proposed model is likely to be unique in terms of dealing with secrecy analysis of a temperature gradient RIS-aided RF-UOWC network. Lastly, the derivations are validated via Monte-Carlo simulations. △ Less","13 December, 2021",https://arxiv.org/pdf/2112.06487
Towards Autonomous Satellite Communications: An AI-based Framework to Address System-level Challenges,Juan Jose Garau-Luis;Skylar Eiskowitz;Nils Pachler;Edward Crawley;Bruce Cameron,"The next generation of satellite constellations is designed to better address the future needs of our connected society: highly-variable data demand, mobile connectivity, and reaching more under-served regions. Artificial Intelligence (AI) and learning-based methods are expected to become key players in the industry, given the poor scalability and slow reaction time of current resource allocation mechanisms. While AI frameworks have been validated for isolated communication tasks or subproblems, there is still not a clear path to achieve fully-autonomous satellite systems. Part of this issue results from the focus on subproblems when designing models, instead of the necessary system-level perspective. In this paper we try to bridge this gap by characterizing the system-level needs that must be met to increase satellite autonomy, and introduce three AI-based components (Demand Estimator, Offline Planner, and Real Time Engine) that jointly address them. We first do a broad literature review on the different subproblems and identify the missing links to the system-level goals. In response to these gaps, we outline the three necessary components and highlight their interactions. We also discuss how current models can be incorporated into the framework and possible directions of future work. △ Less","11 December, 2021",https://arxiv.org/pdf/2112.06055
Artificial Intellgence -- Application in Life Sciences and Beyond. The Upper Rhine Artificial Intelligence Symposium UR-AI 2021,Karl-Herbert Schäfer;Franz Quint,"The TriRhenaTech alliance presents the accepted papers of the 'Upper-Rhine Artificial Intelligence Symposium' held on October 27th 2021 in Kaiserslautern, Germany. Topics of the conference are applications of Artificial Intellgence in life sciences, intelligent systems, industry 4.0, mobility and others. The TriRhenaTech alliance is a network of universities in the Upper-Rhine Trinational Metropolitan Region comprising of the German universities of applied sciences in Furtwangen, Kaiserslautern, Karlsruhe, Offenburg and Trier, the Baden-Wuerttemberg Cooperative State University Loerrach, the French university network Alsace Tech (comprised of 14 'grandes écoles' in the fields of engineering, architecture and management) and the University of Applied Sciences and Arts Northwestern Switzerland. The alliance's common goal is to reinforce the transfer of knowledge, research, and technology, as well as the cross-border mobility of students. △ Less","10 December, 2021",https://arxiv.org/pdf/2112.05657
Reward-Based Environment States for Robot Manipulation Policy Learning,Cédérick Mouliets;Isabelle Ferrané;Heriberto Cuayáhuitl,"Training robot manipulation policies is a challenging and open problem in robotics and artificial intelligence. In this paper we propose a novel and compact state representation based on the rewards predicted from an image-based task success classifier. Our experiments, using the Pepper robot in simulation with two deep reinforcement learning algorithms on a grab-and-lift task, reveal that our proposed state representation can achieve up to 97% task success using our best policies. △ Less","10 December, 2021",https://arxiv.org/pdf/2112.05621
Paradigms of Computational Agency,Srinath Srinivasa;Jayati Deshmukh,"Agent-based models have emerged as a promising paradigm for addressing ever increasing complexity of information systems. In its initial days in the 1990s when object-oriented modeling was at its peak, an agent was treated as a special kind of ""object"" that had a persistent state and its own independent thread of execution. Since then, agent-based models have diversified enormously to even open new conceptual insights about the nature of systems in general. This paper presents a perspective on the disparate ways in which our understanding of agency, as well as computational models of agency have evolved. Advances in hardware like GPUs, that brought neural networks back to life, may also similarly infuse new life into agent-based models, as well as pave the way for advancements in research on Artificial General Intelligence (AGI). △ Less","10 December, 2021",https://arxiv.org/pdf/2112.05575
An Embarrassingly Pragmatic Introduction to Vision-based Autonomous Robots,Marcos V. Conde,"Autonomous robots are currently one of the most popular Artificial Intelligence problems, having experienced significant advances in the last decade, from Self-driving cars and humanoids to delivery robots and drones. Part of the problem is to get a robot to emulate the perception of human beings, our sense of sight, replacing the eyes with cameras and the brain with mathematical models such as Neural Networks. Developing an AI able to drive a car without human intervention and a small robot to deliver packages in the city may seem like different problems, nevertheless from the point of view of perception and vision, both problems have several similarities. The main solutions we currently find focus on the environment perception through visual information using Computer Vision techniques, Machine Learning, and various algorithms to make the robot understand the environment or scene, move, adapt its trajectory and perform its tasks (maintenance, exploration, etc.) without the need for human intervention. In this work, we develop a small-scale autonomous vehicle from scratch, capable of understanding the scene using only visual information, navigating through industrial environments, detecting people and obstacles, or performing simple maintenance tasks. We review the state-of-the-art of fundamental problems and demonstrate that many methods employed at small-scale are similar to the ones employed in real Self-driving cars from companies like Tesla or Lyft. Finally, we discuss the current state of Robotics and autonomous driving and the technological and ethical limitations that we can find in this field. △ Less","14 December, 2021",https://arxiv.org/pdf/2112.05534
Where is Memory Information Stored in the Brain?,James Tee;Desmond P. Taylor,"Within the scientific research community, memory information in the brain is commonly believed to be stored in the synapse - a hypothesis famously attributed to psychologist Donald Hebb. However, there is a growing minority who postulate that memory is stored inside the neuron at the molecular (RNA or DNA) level - an alternative postulation known as the cell-intrinsic hypothesis, coined by psychologist Randy Gallistel. In this paper, we review a selection of key experimental evidence from both sides of the argument. We begin with Eric Kandel's studies on sea slugs, which provided the first evidence in support of the synaptic hypothesis. Next, we touch on experiments in mice by John O'Keefe (declarative memory and the hippocampus) and Joseph LeDoux (procedural fear memory and the amygdala). Then, we introduce the synapse as the basic building block of today's artificial intelligence neural networks. After that, we describe David Glanzman's study on dissociating memory storage and synaptic change in sea slugs, and Susumu Tonegawa's experiment on reactivating retrograde amnesia in mice using laser. From there, we highlight Germund Hesslow's experiment on conditioned pauses in ferrets, and Beatrice Gelber's experiment on conditioning in single-celled organisms without synapses (Paramecium aurelia). This is followed by a description of David Glanzman's experiment on transplanting memory between sea slugs using RNA. Finally, we provide an overview of Brian Dias and Kerry Ressler's experiment on DNA transfer of fear in mice from parents to offspring. We conclude with some potential implications for the wider field of psychology. △ Less","10 December, 2021",https://arxiv.org/pdf/2112.05362
Bringing Atomistic Deep Learning to Prime Time,Nathan C. Frey;Siddharth Samsi;Bharath Ramsundar;Connor W. Coley;Vijay Gadepally,"Artificial intelligence has not yet revolutionized the design of materials and molecules. In this perspective, we identify four barriers preventing the integration of atomistic deep learning, molecular science, and high-performance computing. We outline focused research efforts to address the opportunities presented by these challenges. △ Less","9 December, 2021",https://arxiv.org/pdf/2112.04977
Artificial Intelligence and Design of Experiments for Assessing Security of Electricity Supply: A Review and Strategic Outlook,Jan Priesmann;Justin Münch;Elias Ridha;Thomas Spiegel;Marius Reich;Mario Adam;Lars Nolting;Aaron Praktiknjo,"Assessing the effects of the energy transition and liberalization of energy markets on resource adequacy is an increasingly important and demanding task. The rising complexity in energy systems requires adequate methods for energy system modeling leading to increased computational requirements. Furthermore, with complexity, uncertainty increases likewise calling for probabilistic assessments and scenario analyses. To adequately and efficiently address these various requirements, new methods from the field of data science are needed to accelerate current methods. With our systematic literature review, we want to close the gap between the three disciplines (1) assessment of security of electricity supply, (2) artificial intelligence, and (3) design of experiments. For this, we conduct a large-scale quantitative review on selected fields of application and methods and make a synthesis that relates the different disciplines to each other. Among other findings, we identify metamodeling of complex security of electricity supply models using AI methods and applications of AI-based methods for forecasts of storage dispatch and (non-)availabilities as promising fields of application that have not sufficiently been covered, yet. We end with deriving a new methodological pipeline for adequately and efficiently addressing the present and upcoming challenges in the assessment of security of electricity supply. △ Less","13 December, 2021",https://arxiv.org/pdf/2112.04889
Evaluating saliency methods on artificial data with different background types,Céline Budding;Fabian Eitel;Kerstin Ritter;Stefan Haufe,"Over the last years, many 'explainable artificial intelligence' (xAI) approaches have been developed, but these have not always been objectively evaluated. To evaluate the quality of heatmaps generated by various saliency methods, we developed a framework to generate artificial data with synthetic lesions and a known ground truth map. Using this framework, we evaluated two data sets with different backgrounds, Perlin noise and 2D brain MRI slices, and found that the heatmaps vary strongly between saliency methods and backgrounds. We strongly encourage further evaluation of saliency maps and xAI methods using this framework before applying these in clinical or other safety-critical settings. △ Less","9 December, 2021",https://arxiv.org/pdf/2112.04882
Co-evolutionary hybrid intelligence,Kirill Krinkin;Yulia Shichkina;Andrey Ignatyev,Artificial intelligence is one of the drivers of modern technological development. The current approach to the development of intelligent systems is data-centric. It has several limitations: it is fundamentally impossible to collect data for modeling complex objects and processes; training neural networks requires huge computational and energy resources; solutions are not explainable. The article discusses an alternative approach to the development of artificial intelligence systems based on human-machine hybridization and their co-evolution. △ Less,"9 December, 2021",https://arxiv.org/pdf/2112.04751
Application of Artificial Intelligence and Machine Learning in Libraries: A Systematic Review,Rajesh Kumar Das;Mohammad Sharif Ul Islam,"As the concept and implementation of cutting-edge technologies like artificial intelligence and machine learning has become relevant, academics, researchers and information professionals involve research in this area. The objective of this systematic literature review is to provide a synthesis of empirical studies exploring application of artificial intelligence and machine learning in libraries. To achieve the objectives of the study, a systematic literature review was conducted based on the original guidelines proposed by Kitchenham et al. (2009). Data was collected from Web of Science, Scopus, LISA and LISTA databases. Following the rigorous/ established selection process, a total of thirty-two articles were finally selected, reviewed and analyzed to summarize on the application of AI and ML domain and techniques which are most often used in libraries. Findings show that the current state of the AI and ML research that is relevant with the LIS domain mainly focuses on theoretical works. However, some researchers also emphasized on implementation projects or case studies. This study will provide a panoramic view of AI and ML in libraries for researchers, practitioners and educators for furthering the more technology-oriented approaches, and anticipating future innovation pathways. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.04573
Artificial Intelligence Powered Mobile Networks: From Cognition to Decision,Guiyang Luo;Quan Yuan;Jinglin Li;Shangguang Wang;Fangchun Yang,"Mobile networks (MN) are anticipated to provide unprecedented opportunities to enable a new world of connected experiences and radically shift the way people interact with everything. MN are becoming more and more complex, driven by ever-increasingly complicated configuration issues and blossoming new service requirements. This complexity poses significant challenges in deployment, management, operation, optimization, and maintenance, since they require a complete understanding and cognition of MN. Artificial intelligence (AI), which deals with the simulation of intelligent behavior in computers, has demonstrated enormous success in many application domains, suggesting its potential in cognizing the state of MN and making intelligent decisions. In this paper, we first propose an AI-powered mobile network architecture and discuss challenges in terms of cognition complexity, decisions with high-dimensional action space, and self-adaption to system dynamics. Then, potential solutions that are associated with AI are discussed. Finally, we propose a deep learning approach that directly maps the state of MN to perceived QoS, integrating cognition with the decision. Our proposed approach helps operators in making more intelligent decisions to guarantee QoS. Meanwhile, the effectiveness and advantages of our proposed approach are demonstrated on a real-world dataset, involving 31261 users over 77 stations within 5 days. △ Less","8 December, 2021",https://arxiv.org/pdf/2112.04263
The Origin and Value of Disagreement Among Data Labelers: A Case Study of the Individual Difference in Hate Speech Annotation,Yisi Sang;Jeffrey Stanton,"Human annotated data is the cornerstone of today's artificial intelligence efforts, yet data labeling processes can be complicated and expensive, especially when human labelers disagree with each other. The current work practice is to use majority-voted labels to overrule the disagreement. However, in the subjective data labeling tasks such as hate speech annotation, disagreement among individual labelers can be difficult to resolve. In this paper, we explored why such disagreements occur using a mixed-method approach - including interviews with experts, concept mapping exercises, and self-reporting items - to develop a multidimensional scale for distilling the process of how annotators label a hate speech corpus. We tested this scale with 170 annotators in a hate speech annotation task. Results showed that our scale can reveal facets of individual differences among annotators (e.g., age, personality, etc.), and these facets' relationships to an annotator's final label decision of an instance. We suggest that this work contributes to the understanding of how humans annotate data. The proposed scale can potentially improve the value of the currently discarded minority-vote labels. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.04030
Is Complexity Important for Philosophy of Mind?,Kristina Šekrst;Sandro Skansi,"Computational complexity has often been ignored in philosophy of mind, in philosophical artificial intelligence studies. The purpose of this paper is threefold. First and foremost, to show the importance of complexity rather than computability in philosophical and AI problems. Second, to rephrase the notion of computability in terms of solvability, i.e. treating computability as non-sufficient for establishing intelligence. The Church-Turing thesis is therefore revisited and rephrased in order to capture the ontological background of spatial and temporal complexity. Third, to emphasize ontological differences between different time complexities, which seem to provide a solid base towards better understanding of artificial intelligence in general. △ Less","2 November, 2021",https://arxiv.org/pdf/2112.03877
Qualitative Analysis for Human Centered AI,Orestis Papakyriakopoulos;Elizabeth Anne Watkins;Amy Winecoff;Klaudia Jaźwińska;Tithi Chattopadhyay,"Human-centered artificial intelligence (AI) posits that machine learning and AI should be developed and applied in a socially aware way. In this article, we argue that qualitative analysis (QA) can be a valuable tool in this process, supplementing, informing, and extending the possibilities of AI models. We show this by describing how QA can be integrated in the current prediction paradigm of AI, assisting scientists in the process of selecting data, variables, and model architectures. Furthermore, we argue that QA can be a part of novel paradigms towards Human Centered AI. QA can support scientists and practitioners in practical problem solving and situated model development. It can also promote participatory design approaches, reveal understudied and emerging issues in AI systems, and assist policy making. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.03784
In-Network Processing for Low-Latency Industrial Anomaly Detection in Softwarized Networks,Huanzhuo Wu;Jia He;Máté Tömösközi;Zuo Xiang;Frank H. P. Fitzek,"Modern manufacturers are currently undertaking the integration of novel digital technologies - such as 5G-based wireless networks, the Internet of Things (IoT), and cloud computing - to elevate their production process to a brand new level, the level of smart factories. In the setting of a modern smart factory, time-critical applications are increasingly important to facilitate efficient and safe production. However, these applications suffer from delays in data transmission and processing due to the high density of wireless sensors and the large volumes of data that they generate. As the advent of next-generation networks has made network nodes intelligent and capable of handling multiple network functions, the increased computational power of the nodes makes it possible to offload some of the computational overhead. In this paper, we show for the first time our IA-Net-Lite industrial anomaly detection system with the novel capability of in-network data processing. IA-Net-Lite utilizes intelligent network devices to combine data transmission and processing, as well as to progressively filter redundant data in order to optimize service latency. By testing in a practical network emulator, we showed that the proposed approach can reduce the service latency by up to 40%. Moreover, the benefits of our approach could potentially be exploited in other large-volume and artificial intelligence applications. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.03683
On Information Processing Limitations In Humans and Machines,Birgitta Dresp-Langley,"Information theory is concerned with the study of transmission, processing, extraction, and utilization of information. In its most abstract form, information is conceived as a means of resolving uncertainty. Shannon and Weaver (1949) were among the first to develop a conceptual framework for information theory. One of the key assumptions of the model is that uncertainty increases linearly with the amount of complexity (in bit units) of information transmitted or generated. A whole body of data from the cognitive neurosciences has shown since that the time of human response or action increases in a similar fashion as a function of information complexity. This paper will discuss some of the implications of what is known about the limitations of human information processing for the development of reliable Artificial Intelligence. It is concluded that novel conceptual frameworks are needed to inspire future studies on this complex problem space. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.03669
QKSA: Quantum Knowledge Seeking Agent -- resource-optimized reinforcement learning using quantum process tomography,Aritra Sarkar;Zaid Al-Ars;Harshitta Gandhi;Koen Bertels,"In this research, we extend the universal reinforcement learning (URL) agent models of artificial general intelligence to quantum environments. The utility function of a classical exploratory stochastic Knowledge Seeking Agent, KL-KSA, is generalized to distance measures from quantum information theory on density matrices. Quantum process tomography (QPT) algorithms form the tractable subset of programs for modeling environmental dynamics. The optimal QPT policy is selected based on a mutable cost function based on algorithmic complexity as well as computational resource complexity. Instead of Turing machines, we estimate the cost metrics on a high-level language to allow realistic experimentation. The entire agent design is encapsulated in a self-replicating quine which mutates the cost function based on the predictive value of the optimal policy choosing scheme. Thus, multiple agents with pareto-optimal QPT policies evolve using genetic programming, mimicking the development of physical theories each with different resource trade-offs. This formal framework is termed Quantum Knowledge Seeking Agent (QKSA). Despite its importance, few quantum reinforcement learning models exist in contrast to the current thrust in quantum machine learning. QKSA is the first proposal for a framework that resembles the classical URL models. Similar to how AIXI-tl is a resource-bounded active version of Solomonoff universal induction, QKSA is a resource-bounded participatory observer framework to the recently proposed algorithmic information-based reconstruction of quantum mechanics. QKSA can be applied for simulating and studying aspects of quantum information theory. Specifically, we demonstrate that it can be used to accelerate quantum variational algorithms which include tomographic reconstruction as its integral subroutine. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.03643
Do explanations increase the effectiveness of AI-crowd generated fake news warnings?,Ziv Epstein;Nicolò Foppiani;Sophie Hilgard;Sanjana Sharma;Elena Glassman;David Rand,"Social media platforms are increasingly deploying complex interventions to help users detect false news. Labeling false news using techniques that combine crowd-sourcing with artificial intelligence (AI) offers a promising way to inform users about potentially low-quality information without censoring content, but also can be hard for users to understand. In this study, we examine how users respond in their sharing intentions to information they are provided about a hypothetical human-AI hybrid system. We ask i) if these warnings increase discernment in social media sharing intentions and ii) if explaining how the labeling system works can boost the effectiveness of the warnings. To do so, we conduct a study (N=1473 Americans) in which participants indicated their likelihood of sharing content. Participants were randomly assigned to a control, a treatment where false content was labeled, or a treatment where the warning labels came with an explanation of how they were generated. We find clear evidence that both treatments increase sharing discernment, and directional evidence that explanations increase the warnings' effectiveness. Interestingly, we do not find that the explanations increase self-reported trust in the warning labels, although we do find some evidence that participants found the warnings with the explanations to be more informative. Together, these results have important implications for designing and deploying transparent misinformation warning labels, and AI-mediated systems more broadly. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.03450
JUSTICE: A Benchmark Dataset for Supreme Court's Judgment Prediction,Mohammad Alali;Shaayan Syed;Mohammed Alsayed;Smit Patel;Hemanth Bodala,"Artificial intelligence is being utilized in many domains as of late, and the legal system is no exception. However, as it stands now, the number of well-annotated datasets pertaining to legal documents from the Supreme Court of the United States (SCOTUS) is very limited for public use. Even though the Supreme Court rulings are public domain knowledge, trying to do meaningful work with them becomes a much greater task due to the need to manually gather and process that data from scratch each time. Hence, our goal is to create a high-quality dataset of SCOTUS court cases so that they may be readily used in natural language processing (NLP) research and other data-driven applications. Additionally, recent advances in NLP provide us with the tools to build predictive models that can be used to reveal patterns that influence court decisions. By using advanced NLP algorithms to analyze previous court cases, the trained models are able to predict and classify a court's judgment given the case's facts from the plaintiff and the defendant in textual format; in other words, the model is emulating a human jury by generating a final verdict. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.03414
Scalable Geometric Deep Learning on Molecular Graphs,Nathan C. Frey;Siddharth Samsi;Joseph McDonald;Lin Li;Connor W. Coley;Vijay Gadepally,"Deep learning in molecular and materials sciences is limited by the lack of integration between applied science, artificial intelligence, and high-performance computing. Bottlenecks with respect to the amount of training data, the size and complexity of model architectures, and the scale of the compute infrastructure are all key factors limiting the scaling of deep learning for molecules and materials. Here, we present \textit{LitMatter}, a lightweight framework for scaling molecular deep learning methods. We train four graph neural network architectures on over 400 GPUs and investigate the scaling behavior of these methods. Depending on the model architecture, training time speedups up to 60\times are seen. Empirical neural scaling relations quantify the model-dependent scaling and enable optimal compute resource allocation and the identification of scalable molecular geometric deep learning model implementations. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.03364
A Deep-Learning Intelligent System Incorporating Data Augmentation for Short-Term Voltage Stability Assessment of Power Systems,Yang Li;Meng Zhang;Chen Chen,"Facing the difficulty of expensive and trivial data collection and annotation, how to make a deep learning-based short-term voltage stability assessment (STVSA) model work well on a small training dataset is a challenging and urgent problem. Although a big enough dataset can be directly generated by contingency simulation, this data generation process is usually cumbersome and inefficient; while data augmentation provides a low-cost and efficient way to artificially inflate the representative and diversified training datasets with label preserving transformations. In this respect, this paper proposes a novel deep-learning intelligent system incorporating data augmentation for STVSA of power systems. First, due to the unavailability of reliable quantitative criteria to judge the stability status for a specific power system, semi-supervised cluster learning is leveraged to obtain labeled samples in an original small dataset. Second, to make deep learning applicable to the small dataset, conditional least squares generative adversarial networks (LSGAN)-based data augmentation is introduced to expand the original dataset via artificially creating additional valid samples. Third, to extract temporal dependencies from the post-disturbance dynamic trajectories of a system, a bi-directional gated recurrent unit with attention mechanism based assessment model is established, which bi-directionally learns the significant time dependencies and automatically allocates attention weights. The test results demonstrate the presented approach manages to achieve better accuracy and a faster response time with original small datasets. Besides classification accuracy, this work employs statistical measures to comprehensively examine the performance of the proposal. △ Less","5 December, 2021",https://arxiv.org/pdf/2112.03265
A Marketplace for Trading AI Models based on Blockchain and Incentives for IoT Data,Lam Duc Nguyen;Shashi Raj Pandey;Soret Beatriz;Arne Broering;Petar Popovski,"As Machine Learning (ML) models are becoming increasingly complex, one of the central challenges is their deployment at scale, such that companies and organizations can create value through Artificial Intelligence (AI). An emerging paradigm in ML is a federated approach where the learning model is delivered to a group of heterogeneous agents partially, allowing agents to train the model locally with their own data. However, the problem of valuation of models, as well the questions of incentives for collaborative training and trading of data/models, have received limited treatment in the literature. In this paper, a new ecosystem of ML model trading over a trusted Blockchain-based network is proposed. The buyer can acquire the model of interest from the ML market, and interested sellers spend local computations on their data to enhance that model's quality. In doing so, the proportional relation between the local data and the quality of trained models is considered, and the valuations of seller's data in training the models are estimated through the distributed Data Shapley Value (DSV). At the same time, the trustworthiness of the entire trading process is provided by the distributed Ledger Technology (DLT). Extensive experimental evaluation of the proposed approach shows a competitive run-time performance, with a 15\% drop in the cost of execution, and fairness in terms of incentives for the participants. △ Less","6 December, 2021",https://arxiv.org/pdf/2112.02870
Explainable Deep Learning in Healthcare: A Methodological Survey from an Attribution View,Di Jin;Elena Sergeeva;Wei-Hung Weng;Geeticka Chauhan;Peter Szolovits,"The increasing availability of large collections of electronic health record (EHR) data and unprecedented technical advances in deep learning (DL) have sparked a surge of research interest in developing DL based clinical decision support systems for diagnosis, prognosis, and treatment. Despite the recognition of the value of deep learning in healthcare, impediments to further adoption in real healthcare settings remain due to the black-box nature of DL. Therefore, there is an emerging need for interpretable DL, which allows end users to evaluate the model decision making to know whether to accept or reject predictions and recommendations before an action is taken. In this review, we focus on the interpretability of the DL models in healthcare. We start by introducing the methods for interpretability in depth and comprehensively as a methodological reference for future researchers or clinical practitioners in this field. Besides the methods' details, we also include a discussion of advantages and disadvantages of these methods and which scenarios each of them is suitable for, so that interested readers can know how to compare and choose among them for use. Moreover, we discuss how these methods, originally developed for solving general-domain problems, have been adapted and applied to healthcare problems and how they can help physicians better understand these data-driven technologies. Overall, we hope this survey can help researchers and practitioners in both artificial intelligence (AI) and clinical fields understand what methods we have for enhancing the interpretability of their DL models and choose the optimal one accordingly. △ Less","5 December, 2021",https://arxiv.org/pdf/2112.02625
Artificial Cognitively-inspired Generation of the Notion of Topological Group in the Context of Artificial Mathematical Intelligence,Danny A. J. Gomez-Ramirez;Yoe A. Herrera-Jaramillo;Florian Geismann,"The new computational paradigm of conceptual computation has been introduced in the research program of Artificial Mathematical Intelligence. We provide the explicit artificial generation (or conceptual computation) for the fundamental mathematical notion of topological groups. Specifically, we start with two basic notions belonging to topology and abstract algebra, and we describe recursively formal specifications in the Common Algebraic Specification Language (CASL). The notion of conceptual blending between such conceptual spaces can be materialized computationally in the Heterogeneous Tool Set (HETS). The fundamental notion of topological groups is explicitly generated through three different artificial specifications based on conceptual blending and conceptual identification, starting with the concepts of continuous functions and mathematical groups (described with minimal set-theoretical conditions). This constitutes in additional heuristic evidence for the third pillar of Artificial Mathematical Intelligence. △ Less","4 December, 2021",https://arxiv.org/pdf/2112.02457
Overcome Anterograde Forgetting with Cycled Memory Networks,Jian Peng;Dingqi Ye;Bo Tang;Yinjie Lei;Yu Liu;Haifeng Li,"Learning from a sequence of tasks for a lifetime is essential for an agent towards artificial general intelligence. This requires the agent to continuously learn and memorize new knowledge without interference. This paper first demonstrates a fundamental issue of lifelong learning using neural networks, named anterograde forgetting, i.e., preserving and transferring memory may inhibit the learning of new knowledge. This is attributed to the fact that the learning capacity of a neural network will be reduced as it keeps memorizing historical knowledge, and the fact that conceptual confusion may occur as it transfers irrelevant old knowledge to the current task. This work proposes a general framework named Cycled Memory Networks (CMN) to address the anterograde forgetting in neural networks for lifelong learning. The CMN consists of two individual memory networks to store short-term and long-term memories to avoid capacity shrinkage. A transfer cell is designed to connect these two memory networks, enabling knowledge transfer from the long-term memory network to the short-term memory network to mitigate the conceptual confusion, and a memory consolidation mechanism is developed to integrate short-term knowledge into the long-term memory network for knowledge accumulation. Experimental results demonstrate that the CMN can effectively address the anterograde forgetting on several task-related, task-conflict, class-incremental and cross-domain benchmarks. △ Less","4 December, 2021",https://arxiv.org/pdf/2112.02342
A Game-Theoretic Approach for AI-based Botnet Attack Defence,Hooman Alavizadeh;Julian Jang-Jaccard;Tansu Alpcan;Seyit A. Camtepe,"The new generation of botnets leverages Artificial Intelligent (AI) techniques to conceal the identity of botmasters and the attack intention to avoid detection. Unfortunately, there has not been an existing assessment tool capable of evaluating the effectiveness of existing defense strategies against this kind of AI-based botnet attack. In this paper, we propose a sequential game theory model that is capable to analyse the details of the potential strategies botnet attackers and defenders could use to reach Nash Equilibrium (NE). The utility function is computed under the assumption when the attacker launches the maximum number of DDoS attacks with the minimum attack cost while the defender utilises the maximum number of defense strategies with the minimum defense cost. We conduct a numerical analysis based on a various number of defense strategies involved on different (simulated) cloud-band sizes in relation to different attack success rate values. Our experimental results confirm that the success of defense highly depends on the number of defense strategies used according to careful evaluation of attack rates. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.02223
Could AI Democratise Education? Socio-Technical Imaginaries of an EdTech Revolution,Sahan Bulathwela;María Pérez-Ortiz;Catherine Holloway;John Shawe-Taylor,"Artificial Intelligence (AI) in Education has been said to have the potential for building more personalised curricula, as well as democratising education worldwide and creating a Renaissance of new ways of teaching and learning. Millions of students are already starting to benefit from the use of these technologies, but millions more around the world are not. If this trend continues, the first delivery of AI in Education could be greater educational inequality, along with a global misallocation of educational resources motivated by the current technological determinism narrative. In this paper, we focus on speculating and posing questions around the future of AI in Education, with the aim of starting the pressing conversation that would set the right foundations for the new generation of education that is permeated by technology. This paper starts by synthesising how AI might change how we learn and teach, focusing specifically on the case of personalised learning companions, and then move to discuss some socio-technical features that will be crucial for avoiding the perils of these AI systems worldwide (and perhaps ensuring their success). This paper also discusses the potential of using AI together with free, participatory and democratic resources, such as Wikipedia, Open Educational Resources and open-source tools. We also emphasise the need for collectively designing human-centered, transparent, interactive and collaborative AI-based algorithms that empower and give complete agency to stakeholders, as well as support new emerging pedagogies. Finally, we ask what would it take for this educational revolution to provide egalitarian and empowering access to education, beyond any political, cultural, language, geographical and learning ability barriers. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.02034
Modelling and optimization of nanovector synthesis for applications in drug delivery systems,Felipe J. Villaseñor-Cavazos;Daniel Torres-Valladares;Omar Lozano,"Nanovectors (NVs), based on nanostructured matter such as nanoparticles (NPs), have proven to perform as excellent drug delivery systems. However, due to the great variety of potential NVs, including NPs materials and their functionalization, in addition to the plethora of molecules that could transport, this fields presents a great challenge in terms of resources to find NVs with the most optimal physicochemical properties such as particle size and drug loading, where most of efforts rely on trial and error experimentation. In this regard, Artificial intelligence (AI) and metaheuristic algorithms offer efficient of the state-of-the-art modelling and optimization, respectively. This review focuses, through a systematic search, on the use of artificial intelligence and metaheuristic algorithms for nanoparticle synthesis in drug delivery systems. The main findings are: neural networks are better at modelling NVs properties than linear regression algorithms and response surface methodology, there is a very limited number of studies comparing AI or metaheuristic algorithm, and there is no information regarding the appropriateness of calculations of the sample size. Based on these findings, multilayer perceptron artificial neural network and adaptive neuro fuzzy inference system were tested for their modelling performance with a NV dataset; finding the latter the better algorithm. For metaheuristic algorithms, benchmark functions were optimized with cuckoo search, firefly algorithm, genetic algorithm and symbiotic organism search; finding cuckoo search and symbiotic organism search with the best performance. Finally, methods to estimate appropriate sample size for AI algorithms are discussed. △ Less","10 November, 2021",https://arxiv.org/pdf/2112.02002
REMR: A Reliability Evaluation Method for Dynamic Edge Computing Network under Time Constraints,Liang Chen;Jianpeng Qi;Xiao Su;Rui Wang,"While the concept of Artificial Intelligent Internet of Things\ (AIoT) is booming, computation and/or communication-intensive tasks accompanied by several sub-tasks are slowly moving from centralized deployment to edge-side deployment. The idea of edge computing also makes intelligent services sink locally. But in actual scenarios like dynamic edge computing networks (DECN), due to fluctuations in available computing resources of intermediate servers and changes in bandwidth during data transmission, service reliability becomes difficult to guarantee. Coupled with changes in the amount of data in a service, the above three problems all make the existing reliability evaluation methods no longer accurate. To study the effect of distributed service deployment strategies under such a background, this paper proposes a reliability evaluation method (REMR) based on lower boundary rule under time constraint to study the degree of the rationality of a service deployment plan combined with DECN. In this scenario, time delay is the main concern which would be affected by three quantitative factors: data packet storing and sending time, data transmission time and the calculation time of executing sub-tasks on the node devices, specially while the last two are in dynamic scenarios. In actual calculation, based on the idea of the minimal paths, the solution set would to be found that can meet requirements in the current deployment. Then the reliability of the service supported by the solution sets would be found out based on the principle of inclusion-exclusion combined with the distribution of available data transmission bandwidth and the distribution of node available computing resources. Besides a illustrative example was provided, to verify the calculated reliability of the designed service deployment plan, the NS3 is utilized along with Google cluster data set for simulation. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.01913
The Catalan Language CLUB,Carlos Rodriguez-Penagos;Carme Armentano-Oller;Marta Villegas;Maite Melero;Aitor Gonzalez;Ona de Gibert Bonet;Casimiro Carrino Pio,"The Catalan Language Understanding Benchmark (CLUB) encompasses various datasets representative of different NLU tasks that enable accurate evaluations of language models, following the General Language Understanding Evaluation (GLUE) example. It is part of AINA and PlanTL, two public funding initiatives to empower the Catalan language in the Artificial Intelligence era. △ Less","3 December, 2021",https://arxiv.org/pdf/2112.01894
Multi-modal application: Image Memes Generation,Zhiyuan Liu;Chuanzheng Sun;Yuxin Jiang;Shiqi Jiang;Mei Ming,"Meme is an interesting word. Internet memes offer unique insights into the changes in our perception of the world, the media and our own lives. If you surf the Internet for long enough, you will see it somewhere on the Internet. With the rise of social media platforms and convenient image dissemination, Image Meme has gained fame. Image memes have become a kind of pop culture and they play an important role in communication over social media, blogs, and open messages. With the development of artificial intelligence and the widespread use of deep learning, Natural Language Processing (NLP) and Computer Vision (CV) can also be used to solve more problems in life, including meme generation. An Internet meme commonly takes the form of an image and is created by combining a meme template (image) and a caption (natural language sentence). In our project, we propose an end-to-end encoder-decoder architecture meme generator. For a given input sentence, we use the Meme template selection model to determine the emotion it expresses and select the image template. Then generate captions and memes through to the meme caption generator. Code and models are available at github △ Less","2 December, 2021",https://arxiv.org/pdf/2112.01651
Training Efficiency and Robustness in Deep Learning,Fartash Faghri,"Deep Learning has revolutionized machine learning and artificial intelligence, achieving superhuman performance in several standard benchmarks. It is well-known that deep learning models are inefficient to train; they learn by processing millions of training data multiple times and require powerful computational resources to process large batches of data in parallel at the same time rather than sequentially. Deep learning models also have unexpected failure modes; they can be fooled into misbehaviour, producing unexpectedly incorrect predictions. In this thesis, we study approaches to improve the training efficiency and robustness of deep learning models. In the context of learning visual-semantic embeddings, we find that prioritizing learning on more informative training data increases convergence speed and improves generalization performance on test data. We formalize a simple trick called hard negative mining as a modification to the learning objective function with no computational overhead. Next, we seek improvements to optimization speed in general-purpose optimization methods in deep learning. We show that a redundancy-aware modification to the sampling of training data improves the training speed and develops an efficient method for detecting the diversity of training signal, namely, gradient clustering. Finally, we study adversarial robustness in deep learning and approaches to achieve maximal adversarial robustness without training with additional data. For linear models, we prove guaranteed maximal robustness achieved only by appropriate choice of the optimizer, regularization, or architecture. △ Less","2 December, 2021",https://arxiv.org/pdf/2112.01423
Homotopy Based Reinforcement Learning with Maximum Entropy for Autonomous Air Combat,Yiwen Zhu;Zhou Fang;Yuan Zheng;Wenya Wei,"The Intelligent decision of the unmanned combat aerial vehicle (UCAV) has long been a challenging problem. The conventional search method can hardly satisfy the real-time demand during high dynamics air combat scenarios. The reinforcement learning (RL) method can significantly shorten the decision time via using neural networks. However, the sparse reward problem limits its convergence speed and the artificial prior experience reward can easily deviate its optimal convergent direction of the original task, which raises great difficulties for the RL air combat application. In this paper, we propose a homotopy-based soft actor-critic method (HSAC) which focuses on addressing these problems via following the homotopy path between the original task with sparse reward and the auxiliary task with artificial prior experience reward. The convergence and the feasibility of this method are also proved in this paper. To confirm our method feasibly, we construct a detailed 3D air combat simulation environment for the RL-based methods training firstly, and we implement our method in both the attack horizontal flight UCAV task and the self-play confrontation task. Experimental results show that our method performs better than the methods only utilizing the sparse reward or the artificial prior experience reward. The agent trained by our method can reach more than 98.3% win rate in the attack horizontal flight UCAV task and average 67.4% win rate when confronted with the agents trained by the other two methods. △ Less","1 December, 2021",https://arxiv.org/pdf/2112.01328
Advancing Artificial Intelligence and Machine Learning in the U.S. Government Through Improved Public Competitions,Ezekiel J. Maier,"In the last two years, the U.S. government has emphasized the importance of accelerating artificial intelligence (AI) and machine learning (ML) within the government and across the nation. In particular, the National Artificial Intelligence Initiative Act of 2020, which became law on January 1, 2021, provides for a coordinated program across the entire federal government to accelerate AI research and application. The U.S. government can benefit from public artificial intelligence and machine learning challenges through the development of novel algorithms and participation in experiential training. Although the public, private, and non-profit sectors have a history of leveraging crowdsourcing initiatives to generate novel solutions to difficult problems and engage stakeholders, interest in public competitions has waned in recent years as a result of at least three major factors: (1) a lack of high-quality, high-impact data; (2) a narrow engagement focus on specialized groups; and (3) insufficient operationalization of challenge results. Herein we identify common issues and recommend approaches to increase the effectiveness of challenges. To address these barriers, enabling the use of public competitions for accelerating AI and ML practice, the U.S. government must leverage methods that protect sensitive data while enabling modelling, enable easier participation, empower deployment of validated models, and incentivize engagement from broad sections of the population. △ Less","29 November, 2021",https://arxiv.org/pdf/2112.01275
An AI-based Solution for Enhancing Delivery of Digital Learning for Future Teachers,Yong-Bin Kang;Abdur Rahim Mohammad Forkan;Prem Prakash Jayaraman;Natalie Wieland;Elizabeth Kollias;Hung Du;Steven Thomson;Yuan-Fang Li,"There has been a recent and rapid shift to digital learning hastened by the pandemic but also influenced by ubiquitous availability of digital tools and platforms now, making digital learning ever more accessible. An integral and one of the most difficult part of scaling digital learning and teaching is to be able to assess learner's knowledge and competency. An educator can record a lecture or create digital content that can be delivered to thousands of learners but assessing learners is extremely time consuming. In the paper, we propose an Artificial Intelligence (AI)-based solution namely VidVersityQG for generating questions automatically from pre-recorded video lectures. The solution can automatically generate different types of assessment questions (including short answer, multiple choice, true/false and fill in the blank questions) based on contextual and semantic information inferred from the videos. The proposed solution takes a human-centred approach, wherein teachers are provided the ability to modify/edit any AI generated questions. This approach encourages trust and engagement of teachers in the use and implementation of AI in education. The AI-based solution was evaluated for its accuracy in generating questions by 7 experienced teaching professionals and 117 education videos from multiple domains provided to us by our industry partner VidVersity. VidVersityQG solution showed promising results in generating high-quality questions automatically from video thereby significantly reducing the time and effort for educators in manual question generation. △ Less","7 December, 2021",https://arxiv.org/pdf/2112.01229
AI-Fuzzy Markup Language with Computational Intelligence for High-School Student Learning,Chang-Shing Lee;Mei-Hui Wang;Yusuke Nojima;Marek Reformat;Leo Guo,"Computational Intelligence (CI), which includes fuzzy logic (FL), neural network (NN), and evolutionary computation (EC), is an imperative branch of artificial intelligence (AI). As a core technology of AI, it plays a vital role in developing intelligent systems, such as games and game engines, neural-based systems including a variety of deep network models, evolutionary-based optimization methods, and advanced cognitive techniques. The 2021 IEEE CIS Summer School on CI for High-School Student Learning was held physically at the JanFuSun Resort Hotel, Taiwan, and virtually on Zoom, on August 10-12, 2021. The main contents of the Summer School were lectures focused on the basics of FL, NN, and EC and the workshop on AIoT (Artificial Intelligence of Things). Invited speakers gave nine courses covering topics like CI real-world applications, fundamentals of FL, and the introduction to NN and EC. The 2021 Summer School was supported by the 2021 IEEE CIS High School Outreach Subcommittee. We also invited students and teachers of high and elementary schools from Taiwan, Japan, and Indonesia. They attended the school and participated in AIoT workshop, gaining experience in applications of AIoT-FML learning tools. According to the short report and feedback from the involved students and teachers, we find out that most participants have quickly understood the principles of CI, FL, NN, and EC. In addition, one of the teachers sent the following remark to the organizers: ""This is a great event to introduce students to computational intelligence at a young age, stimulate them to be involved in rapidly evolving fields, and foster participation in future research adventures."" △ Less","6 November, 2021",https://arxiv.org/pdf/2112.01228
"Role of Artificial Intelligence, Clinicians & Policymakers in Clinical Decision Making: A Systems Viewpoint",Avishek Choudhury;Onur Asan;Mo Mansouri,"What is a system? Is one of those questions that is yet not clear to most individuals in this world. A system is an assemblage of interacting, interrelated and interdependent components forming a complex and integrated whole with an unambiguous and common goal. This paper emphasizes on the fact that all components of a complex system are inter-related and interdependent in some way and the behavior of that system depends on these independences. A health care system as portrayed in this article is widespread and complex. This encompasses not only hospitals but also governing bodies like the FDA, technologies such as AI, biomedical devices, Cloud computing and many more. The interactions between all these components govern the behavior and existence of the overall healthcare system. In this paper, we focus on the interaction of artificial intelligence, care providers and policymakers and analyze using systems thinking approach, their impact on clinical decision making △ Less","31 October, 2021",https://arxiv.org/pdf/2112.01226
Autonomous Vehicular Networks: Perspective and Open Issues,Tom H. Luan;Yao Zhang;Lin Cai;Yilong Hui;Changle Li;Nan Cheng,"The vehicular ad hoc networks (VANETs) have been researched for over twenty years. Although being a fundamental communication approach for vehicles, the conventional VANETs are challenged by the newly emerged autonomous vehicles (AVs) which introduce new features and challenges on communications. In the meantime, with the recent advances of artificial intelligence and 5G cellular networks, how should the fundamental framework of VANET evolve to utilize the new technologies? In this article, we reconsider the problem of vehicle-to-vehicle communications when the network is composed of AVs. We discuss the features and specific demands of AVs and how the conventional VANETs should adapt to fit them. △ Less","2 December, 2021",https://arxiv.org/pdf/2112.01154
Large-Scale Data Mining of Rapid Residue Detection Assay Data From HTML and PDF Documents: Improving Data Access and Visualization for Veterinarians,Majid Jaberi-Douraki;Soudabeh Taghian Dinani;Nuwan Indika Millagaha Gedara;Xuan Xu;Emily Richards;Fiona Maunsell;Nader Zad;Lisa Ann Tell,"Extra-label drug use in food animal medicine is authorized by the US Animal Medicinal Drug Use Clarification Act (AMDUCA), and estimated withdrawal intervals are based on published scientific pharmacokinetic data. Occasionally there is a paucity of scientific data on which to base a withdrawal interval or a large number of animals being treated, driving the need to test for drug residues. Rapid assay commercial farm-side tests are essential for monitoring drug residues in animal products to protect human health. Active ingredients, sensitivity, matrices, and species that have been evaluated for commercial rapid assay tests are typically reported on manufacturers' websites or in PDF documents that are available to consumers but may require a special access request. Additionally, this information is not always correlated with FDA-approved tolerances. Furthermore, parameter changes for these tests can be very challenging to regularly identify, especially those listed on websites or in documents that are not publicly available. Therefore, artificial intelligence plays a critical role in efficiently extracting the data and ensure current information. Extracting tables from PDF and HTML documents has been investigated both by academia and commercial tool builders. Research in text mining of such documents has become a widespread yet challenging arena in implementing natural language programming. However, techniques of extracting tables are still in their infancy and being investigated and improved by researchers. In this study, we developed and evaluated a data-mining method for automatically extracting rapid assay data from electronic documents. Our automatic electronic data extraction method includes a software package module, a developed pattern recognition tool, and a data mining engine. Assay details were provided by several commercial entities that produce these rapid drug residue assay △ Less","1 December, 2021",https://arxiv.org/pdf/2112.00962
Is the use of Deep Learning and Artificial Intelligence an appropriate means to locate debris in the ocean without harming aquatic wildlife?,Zoe Moorton;Zeyneb Kurt;Wai Lok Woo,"With the global issue of plastic debris ever expanding, it is about time that the technology industry stepped in. This study aims to assess whether deep learning can successfully distinguish between marine life and man-made debris underwater. The aim is to find if we are safely able to clean up our oceans with Artificial Intelligence without disrupting the delicate balance of the aquatic ecosystems. The research explores the use of Convolutional Neural Networks from the perspective of protecting the ecosystem, rather than primarily collecting rubbish. We did this by building a custom-built, deep learning model, with an original database including 1,644 underwater images and used a binary classification to sort synthesised material from aquatic life. We concluded that although it is possible to safely distinguish between debris and life, further exploration with a larger database and stronger CNN structure has the potential for much more promising results. △ Less","30 November, 2021",https://arxiv.org/pdf/2112.00190
The Effect of Iterativity on Adversarial Opinion Forming,Konstantinos Panagiotou;Simon Reisser,"Consider the following model to study adversarial effects on opinion forming. A set of initially selected experts form their binary opinion while being influenced by an adversary, who may convince some of them of the falsehood. All other participants in the network then take the opinion of the majority of their neighbouring experts. Can the adversary influence the experts in such a way that the majority of the network believes the falsehood? Alon et al. [1] conjectured that in this context an iterative dissemination process will always be beneficial to the adversary. This work provides a counterexample to that conjecture. [1] N. Alon, M. Feldman, O. Lev, and M. Tennenholtz. How Robust Is the Wisdom of the Crowds? In Proceedings of the 24th International Joint Conference on Artificial Intelligence (IJCAI 2015), pages 2055-2061, 2015. △ Less","1 December, 2021",https://arxiv.org/pdf/2111.15445
Emotions as abstract evaluation criteria in biological and artificial intelligences,Claudius Gros,"Biological as well as advanced artificial intelligences (AIs) need to decide which goals to pursue. We review nature's solution to the time allocation problem, which is based on a continuously readjusted categorical weighting mechanism we experience introspectively as emotions. One observes phylogenetically that the available number of emotional states increases hand in hand with the cognitive capabilities of animals and that raising levels of intelligence entail ever larger sets of behavioral options. Our ability to experience a multitude of potentially conflicting feelings is in this view not a leftover of a more primitive heritage, but a generic mechanism for attributing values to behavioral options that can not be specified at birth. In this view, emotions are essential for understanding the mind. For concreteness, we propose and discuss a framework which mimics emotions on a functional level. Based on time allocation via emotional stationarity (TAES), emotions are implemented as abstract criteria, such as satisfaction, challenge and boredom, which serve to evaluate activities that have been carried out. The resulting timeline of experienced emotions is compared with the `character' of the agent, which is defined in terms of a preferred distribution of emotional states. The long-term goal of the agent, to align experience with character, is achieved by optimizing the frequency for selecting individual tasks. Upon optimization, the statistics of emotion experience becomes stationary. △ Less","30 November, 2021",https://arxiv.org/pdf/2111.15275
Decoding the Protein-ligand Interactions Using Parallel Graph Neural Networks,Carter Knutson;Mridula Bontha;Jenna A. Bilbrey;Neeraj Kumar,"Protein-ligand interactions (PLIs) are fundamental to biochemical research and their identification is crucial for estimating biophysical and biochemical properties for rational therapeutic design. Currently, experimental characterization of these properties is the most accurate method, however, this is very time-consuming and labor-intensive. A number of computational methods have been developed in this context but most of the existing PLI prediction heavily depends on 2D protein sequence data. Here, we present a novel parallel graph neural network (GNN) to integrate knowledge representation and reasoning for PLI prediction to perform deep learning guided by expert knowledge and informed by 3D structural data. We develop two distinct GNN architectures, GNNF is the base implementation that employs distinct featurization to enhance domain-awareness, while GNNP is a novel implementation that can predict with no prior knowledge of the intermolecular interactions. The comprehensive evaluation demonstrated that GNN can successfully capture the binary interactions between ligand and proteins 3D structure with 0.979 test accuracy for GNNF and 0.958 for GNNP for predicting activity of a protein-ligand complex. These models are further adapted for regression tasks to predict experimental binding affinities and pIC50 is crucial for drugs potency and efficacy. We achieve a Pearson correlation coefficient of 0.66 and 0.65 on experimental affinity and 0.50 and 0.51 on pIC50 with GNNF and GNNP, respectively, outperforming similar 2D sequence-based models. Our method can serve as an interpretable and explainable artificial intelligence (AI) tool for predicted activity, potency, and biophysical properties of lead candidates. To this end, we show the utility of GNNP on SARS-Cov-2 protein targets by screening a large compound library and comparing our prediction with the experimentally measured data. △ Less","30 November, 2021",https://arxiv.org/pdf/2111.15144
Mesarovician Abstract Learning Systems,Tyler Cody,"The solution methods used to realize artificial general intelligence (AGI) may not contain the formalism needed to adequately model and characterize AGI. In particular, current approaches to learning hold notions of problem domain and problem task as fundamental precepts, but it is hardly apparent that an AGI encountered in the wild will be discernable into a set of domain-task pairings. Nor is it apparent that the outcomes of AGI in a system can be well expressed in terms of domain and task, or as consequences thereof. Thus, there is both a practical and theoretical use for meta-theories of learning which do not express themselves explicitly in terms of solution methods. General systems theory offers such a meta-theory. Herein, Mesarovician abstract systems theory is used as a super-structure for learning. Abstract learning systems are formulated. Subsequent elaboration stratifies the assumptions of learning systems into a hierarchy and considers the hierarchy such stratification projects onto learning theory. The presented Mesarovician abstract learning systems theory calls back to the founding motivations of artificial intelligence research by focusing on the thinking participants directly, in this case, learning systems, in contrast to the contemporary focus on the problems thinking participants solve. △ Less","29 November, 2021",https://arxiv.org/pdf/2111.14766
Explore the Potential Performance of Vision-and-Language Navigation Model: a Snapshot Ensemble Method,Wenda Qin;Teruhisa Misu;Derry Wijaya,"Vision-and-Language Navigation (VLN) is a challenging task in the field of artificial intelligence. Although massive progress has been made in this task over the past few years attributed to breakthroughs in deep vision and language models, it remains tough to build VLN models that can generalize as well as humans. In this paper, we provide a new perspective to improve VLN models. Based on our discovery that snapshots of the same VLN model behave significantly differently even when their success rates are relatively the same, we propose a snapshot-based ensemble solution that leverages predictions among multiple snapshots. Constructed on the snapshots of the existing state-of-the-art (SOTA) model \circlearrowrightBERT and our past-action-aware modification, our proposed ensemble achieves the new SOTA performance in the R2R dataset challenge in Navigation Error (NE) and Success weighted by Path Length (SPL). △ Less","28 November, 2021",https://arxiv.org/pdf/2111.14267
EffCNet: An Efficient CondenseNet for Image Classification on NXP BlueBox,Priyank Kalgaonkar;Mohamed El-Sharkawy,"Intelligent edge devices with built-in processors vary widely in terms of capability and physical form to perform advanced Computer Vision (CV) tasks such as image classification and object detection, for example. With constant advances in the field of autonomous cars and UAVs, embedded systems and mobile devices, there has been an ever-growing demand for extremely efficient Artificial Neural Networks (ANN) for real-time inference on these smart edge devices with constrained computational resources. With unreliable network connections in remote regions and an added complexity of data transmission, it is of an utmost importance to capture and process data locally instead of sending the data to cloud servers for remote processing. Edge devices on the other hand, offer limited processing power due to their inexpensive hardware, and limited cooling and computational resources. In this paper, we propose a novel deep convolutional neural network architecture called EffCNet which is an improved and an efficient version of CondenseNet Convolutional Neural Network (CNN) for edge devices utilizing self-querying data augmentation and depthwise separable convolutional strategies to improve real-time inference performance as well as reduce the final trained model size, trainable parameters, and Floating-Point Operations (FLOPs) of EffCNet CNN. Furthermore, extensive supervised image classification analyses are conducted on two benchmarking datasets: CIFAR-10 and CIFAR-100, to verify real-time inference performance of our proposed CNN. Finally, we deploy these trained weights on NXP BlueBox which is an intelligent edge development platform designed for self-driving vehicles and UAVs, and conclusions will be extrapolated accordingly. △ Less","28 November, 2021",https://arxiv.org/pdf/2111.14243
How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey,Zahra Khanjani;Gabrielle Watson;Vandana P. Janeja,"Deepfake is content or material that is synthetically generated or manipulated using artificial intelligence (AI) methods, to be passed off as real and can include audio, video, image, and text synthesis. This survey has been conducted with a different perspective compared to existing survey papers, that mostly focus on just video and image deepfakes. This survey not only evaluates generation and detection methods in the different deepfake categories, but mainly focuses on audio deepfakes that are overlooked in most of the existing surveys. This paper critically analyzes and provides a unique source of audio deepfake research, mostly ranging from 2016 to 2020. To the best of our knowledge, this is the first survey focusing on audio deepfakes in English. This survey provides readers with a summary of 1) different deepfake categories 2) how they could be created and detected 3) the most recent trends in this domain and shortcomings in detection methods 4) audio deepfakes, how they are created and detected in more detail which is the main focus of this paper. We found that Generative Adversarial Networks(GAN), Convolutional Neural Networks (CNN), and Deep Neural Networks (DNN) are common ways of creating and detecting deepfakes. In our evaluation of over 140 methods we found that the majority of the focus is on video deepfakes and in particular in the generation of video deepfakes. We found that for text deepfakes there are more generation methods but very few robust methods for detection, including fake news detection, which has become a controversial area of research because of the potential of heavy overlaps with human generation of fake content. This paper is an abbreviated version of the full survey and reveals a clear need to research audio deepfakes and particularly detection of audio deepfakes. △ Less","28 November, 2021",https://arxiv.org/pdf/2111.14203
Mapping Industry 4.0 Technologies: From Cyber-Physical Systems to Artificial Intelligence,Benjamin Meindl;Joana Mendonça,"The fourth industrial revolution is rapidly changing the manufacturing landscape. Due to the growing research and fast evolution in this field, no clear definitions of these concepts yet exist. This work provides a clear description of technological trends and gaps. We introduce a novel method to create a map of Industry 4.0 technologies, using natural language processing to extract technology terms from 14,667 research articles and applying network analysis. We identified eight clusters of Industry 4.0 technologies, which served as the basis for our analysis. Our results show that Industrial Internet of Things (IIoT) technologies have become the center of the Industry 4.0 technology map. This is in line with the initial definitions of Industry 4.0, which centered on IIoT. Given the recent growth in the importance of artificial intelligence (AI), we suggest accounting for AI's fundamental role in Industry 4.0 and understanding the fourth industrial revolution as an AI-powered natural collaboration between humans and machines. This article introduces a novel approach for literature reviews, and the results highlight trends and research gaps to guide future work and help these actors reap the benefits of digital transformations. △ Less","28 November, 2021",https://arxiv.org/pdf/2111.14168
Agility in Software 2.0 -- Notebook Interfaces and MLOps with Buttresses and Rebars,Markus Borg,"Artificial intelligence through machine learning is increasingly used in the digital society. Solutions based on machine learning bring both great opportunities, thus coined ""Software 2.0,"" but also great challenges for the engineering community to tackle. Due to the experimental approach used by data scientists when developing machine learning models, agility is an essential characteristic. In this keynote address, we discuss two contemporary development phenomena that are fundamental in machine learning development, i.e., notebook interfaces and MLOps. First, we present a solution that can remedy some of the intrinsic weaknesses of working in notebooks by supporting easy transitions to integrated development environments. Second, we propose reinforced engineering of AI systems by introducing metaphorical buttresses and rebars in the MLOps context. Machine learning-based solutions are dynamic in nature, and we argue that reinforced continuous engineering is required to quality assure the trustworthy AI systems of tomorrow. △ Less","28 November, 2021",https://arxiv.org/pdf/2111.14142
How Can Applications of Blockchain and Artificial Intelligence Improve Performance of Internet of Things? -- A Survey,Priyanka Bothra;Raja Karmakar;Sanjukta Bhattacharya;Sayantani De,"In the era of the Internet of Things (IoT), massive computing devices surrounding us operate and interact with each other to provide several significant services in industries, medical as well as in daily life activities at home, office, education sectors, and so on. The participating devices in an IoT network usually have resource constraints and the devices are prone to different cyber attacks, leading to the loopholes in the security and authentication. As a revolutionized and innovated technology, blockchain, that is applied in cryptocurrency, market prediction, etc., uses a distributed ledger that records transactions securely and efficiently. To utilize the great potential of blockchain, both industries and academia have paid a significant attention to integrate it with the IoT, as reported by several existing literature. On the other hand, Artificial Intelligence (AI) is able to embed intelligence in a system, and thus the AI can be integrated with IoT devices in order to automatically cope with different environments according to the demands. Furthermore, both blockchain and AI can be integrated with the IoT to design an automated secure and robust IoT model, as mentioned by numerous existing works. In this survey, we present a discussion on the IoT, blockchain, and AI, along with the descriptions of several research works that apply blockchain and AI in the IoT. In this direction, we point out strengths and limitations of the related existing researches. We also discuss different open challenges to exploit the full capacities of blockchain and AI in designing an IoT-based model. Therefore, the highlighted challenging issues can open the door for the development of future IoT models which will be intelligent and secure based on the integration of blockchain and AI with the IoT. △ Less","27 November, 2021",https://arxiv.org/pdf/2111.14018
A Topological Approach for Computing Supremal Sublanguages for Some Language Equations in Supervisory Control Theory,Liyong Lin;Rong Su,"In this paper, we shall present a topological approach for the computation of some supremal sublanguages, often specified by language equations, which arise from the study of the supervisory control theory. The basic idea is to identify the solutions of the language equations as open sets for some (semi)-topologies. Then, the supremal sublanguages naturally correspond to the supremal open subsets, i.e., the interiors. This provides an elementary and uniform approach for computing various supremal sublanguages encountered in the supervisory control theory and is closely related to a theory of approximation, known as the rough set theory, in artificial intelligence. △ Less","27 November, 2021",https://arxiv.org/pdf/2111.13840
A Fast Evolutionary adaptation for MCTS in Pommerman,Harsh Panwar;Saswata Chatterjee;Wil Dube,"Artificial Intelligence, when amalgamated with games makes the ideal structure for research and advancing the field. Multi-agent games have multiple controls for each agent which generates huge amounts of data while increasing search complexity. Thus, we need advanced search methods to find a solution and create an artificially intelligent agent. In this paper, we propose our novel Evolutionary Monte Carlo Tree Search (FEMCTS) agent which borrows ideas from Evolutionary Algorthims (EA) and Monte Carlo Tree Search (MCTS) to play the game of Pommerman. It outperforms Rolling Horizon Evolutionary Algorithm (RHEA) significantly in high observability settings and performs almost as well as MCTS for most game seeds, outperforming it in some cases. △ Less","26 November, 2021",https://arxiv.org/pdf/2111.13770
Interpreting Machine Learning Models for Room Temperature Prediction in Non-domestic Buildings,Jianqiao Mao;Grammenos Ryan,"An ensuing challenge in Artificial Intelligence (AI) is the perceived difficulty in interpreting sophisticated machine learning models, whose ever-increasing complexity makes it hard for such models to be understood, trusted and thus accepted by human beings. The lack, if not complete absence, of interpretability for these so-called black-box models can lead to serious economic and ethical consequences, thereby hindering the development and deployment of AI in wider fields, particularly in those involving critical and regulatory applications. Yet, the building services industry is a highly-regulated domain requiring transparency and decision-making processes that can be understood and trusted by humans. To this end, the design and implementation of autonomous Heating, Ventilation and Air Conditioning systems for the automatic but concurrently interpretable optimisation of energy efficiency and room thermal comfort is of topical interest. This work therefore presents an interpretable machine learning model aimed at predicting room temperature in non-domestic buildings, for the purpose of optimising the use of the installed HVAC system. We demonstrate experimentally that the proposed model can accurately forecast room temperatures eight hours ahead in real-time by taking into account historical RT information, as well as additional environmental and time-series features. In this paper, an enhanced feature engineering process is conducted based on the Exploratory Data Analysis results. Furthermore, beyond the commonly used Interpretable Machine Learning techniques, we propose a Permutation Feature-based Frequency Response Analysis (PF-FRA) method for quantifying the contributions of the different predictors in the frequency domain. Based on the generated reason codes, we find that the historical RT feature is the dominant factor that has most impact on the model prediction. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.13760
"Demystifying Ten Big Ideas and Rules Every Fire Scientist & Engineer Should Know About Blackbox, Whitebox & Causal Artificial Intelligence",M. Z. Naser,"Artificial intelligence (AI) is paving the way towards the fourth industrial revolution with the fire domain (Fire 4.0). As a matter of fact, the next few years will be elemental to how this technology will shape our academia, practice, and entrepreneurship. Despite the growing interest between fire research groups, AI remains absent of our curriculum, and we continue to lack a methodical framework to adopt, apply and create AI solutions suitable for our problems. The above is also true for parallel engineering domains (i.e., civil/mechanical engineering), and in order to negate the notion of history repeats itself (e.g., look at the continued debate with regard to modernizing standardized fire testing, etc.), it is the motivation behind this letter to the Editor to demystify some of the big ideas behind AI to jump-start prolific and strategic discussions on the front of AI & Fire. In addition, this letter intends to explain some of the most fundamental concepts and clear common misconceptions specific to the adoption of AI in fire engineering. This short letter is a companion to the Smart Systems in Fire Engineering special issue sponsored by Fire Technology. An in-depth review of AI algorithms [1] and success stories to the proper implementations of such algorithms can be found in the aforenoted special issue and collection of papers. This letter comprises two sections. The first section outlines big ideas pertaining to AI, and answers some of the burning questions with regard to the merit of adopting AI in our domain. The second section presents a set of rules or technical recommendations an AI user may deem helpful to practice whenever AI is used as an investigation methodology. The presented set of rules are complementary to the big ideas. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.13756
A Taxonomy of Anomalies in Log Data,Thorsten Wittkopp;Philipp Wiesner;Dominik Scheinert;Odej Kao,"Log data anomaly detection is a core component in the area of artificial intelligence for IT operations. However, the large amount of existing methods makes it hard to choose the right approach for a specific system. A better understanding of different kinds of anomalies, and which algorithms are suitable for detecting them, would support researchers and IT operators. Although a common taxonomy for anomalies already exists, it has not yet been applied specifically to log data, pointing out the characteristics and peculiarities in this domain. In this paper, we present a taxonomy for different kinds of log data anomalies and introduce a method for analyzing such anomalies in labeled datasets. We applied our taxonomy to the three common benchmark datasets Thunderbird, Spirit, and BGL, and trained five state-of-the-art unsupervised anomaly detection algorithms to evaluate their performance in detecting different kinds of anomalies. Our results show, that the most common anomaly type is also the easiest to predict. Moreover, deep learning-based approaches outperform data mining-based approaches in all anomaly types, but especially when it comes to detecting contextual anomalies. △ Less","26 November, 2021",https://arxiv.org/pdf/2111.13462
Error Bounds for a Matrix-Vector Product Approximation with Deep ReLU Neural Networks,Tilahun M. Getu,"Among the several paradigms of artificial intelligence (AI) or machine learning (ML), a remarkably successful paradigm is deep learning. Deep learning's phenomenal success has been hoped to be interpreted via fundamental research on the theory of deep learning. Accordingly, applied research on deep learning has spurred the theory of deep learning-oriented depth and breadth of developments. Inspired by such developments, we pose these fundamental questions: can we accurately approximate an arbitrary matrix-vector product using deep rectified linear unit (ReLU) feedforward neural networks (FNNs)? If so, can we bound the resulting approximation error? In light of these questions, we derive error bounds in Lebesgue and Sobolev norms that comprise our developed deep approximation theory. Guided by this theory, we have successfully trained deep ReLU FNNs whose test results justify our developed theory. The developed theory is also applicable for guiding and easing the training of teacher deep ReLU FNNs in view of the emerging teacher-student AI or ML paradigms that are essential for solving several AI or ML problems in wireless communications and signal processing; network science and graph signal processing; and network neuroscience and brain physics. △ Less","25 November, 2021",https://arxiv.org/pdf/2111.12963
"Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications",Khaled B. Letaief;Yuanming Shi;Jianmin Lu;Jianhua Lu,"The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from ""connected things"" to ""connected intelligence"". However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems. △ Less","24 November, 2021",https://arxiv.org/pdf/2111.12444
Mimicking Playstyle by Adapting Parameterized Behavior Trees in RTS Games,Andrzej Kozik;Tomasz Machalewski;Mariusz Marek;Adrian Ochmann,"The discovery of Behavior Trees (BTs) impacted the field of Artificial Intelligence (AI) in games, by providing flexible and natural representation of non-player characters (NPCs) logic, manageable by game-designers. Nevertheless, increased pressure on ever better NPCs AI-agents forced complexity of handcrafted BTs to became barely-tractable and error-prone. On the other hand, while many just-launched on-line games suffer from player-shortage, the existence of AI with a broad-range of capabilities could increase players retention. Therefore, to handle above challenges, recent trends in the field focused on automatic creation of AI-agents: from deep- and reinforcementlearning techniques to combinatorial (constrained) optimization and evolution of BTs. In this paper, we present a novel approach to semi-automatic construction of AI-agents, that mimic and generalize given human gameplays by adapting and tuning of expert-created BT under a developed similarity metric between source and BT gameplays. To this end, we formulated mixed discrete-continuous optimization problem, in which topological and functional changes of the BT are reflected in numerical variables, and constructed a dedicated hybrid-metaheuristic. The performance of presented approach was verified experimentally in a prototype real-time strategy game. Carried out experiments confirmed efficiency and perspectives of presented approach, which is going to be applied in a commercial game. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.12144
Reviewing continual learning from the perspective of human-level intelligence,Yifan Chang;Wenbo Li;Jian Peng;Bo Tang;Yu Kang;Yinjie Lei;Yuanmiao Gui;Qing Zhu;Yu Liu;Haifeng Li,"Humans' continual learning (CL) ability is closely related to Stability Versus Plasticity Dilemma that describes how humans achieve ongoing learning capacity and preservation for learned information. The notion of CL has always been present in artificial intelligence (AI) since its births. This paper proposes a comprehensive review of CL. Different from previous reviews that mainly focus on the catastrophic forgetting phenomenon in CL, this paper surveys CL from a more macroscopic perspective based on the Stability Versus Plasticity mechanism. Analogous to biological counterpart, ""smart"" AI agents are supposed to i) remember previously learned information (information retrospection); ii) infer on new information continuously (information prospection:); iii) transfer useful information (information transfer), to achieve high-level CL. According to the taxonomy, evaluation metrics, algorithms, applications as well as some open issues are then introduced. Our main contributions concern i) rechecking CL from the level of artificial general intelligence; ii) providing a detailed and extensive overview on CL topics; iii) presenting some novel ideas on the potential development of CL. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.11964
Explainable Deep Image Classifiers for Skin Lesion Diagnosis,Carlo Metta;Andrea Beretta;Riccardo Guidotti;Yuan Yin;Patrick Gallinari;Salvatore Rinzivillo;Fosca Giannotti,"A key issue in critical contexts such as medical diagnosis is the interpretability of the deep learning models adopted in decision-making systems. Research in eXplainable Artificial Intelligence (XAI) is trying to solve this issue. However, often XAI approaches are only tested on generalist classifier and do not represent realistic problems such as those of medical diagnosis. In this paper, we analyze a case study on skin lesion images where we customize an existing XAI approach for explaining a deep learning model able to recognize different types of skin lesions. The explanation is formed by synthetic exemplar and counter-exemplar images of skin lesion and offers the practitioner a way to highlight the crucial traits responsible for the classification decision. A survey conducted with domain experts, beginners and unskilled people proof that the usage of explanations increases the trust and confidence in the automatic decision system. Also, an analysis of the latent space adopted by the explainer unveils that some of the most frequent skin lesion classes are distinctly separated. This phenomenon could derive from the intrinsic characteristics of each class and, hopefully, can provide support in the resolution of the most frequent misclassifications by human experts. △ Less","22 November, 2021",https://arxiv.org/pdf/2111.11863
Spatio-Temporal Split Learning for Autonomous Aerial Surveillance using Urban Air Mobility (UAM) Networks,Yoo Jeong Ha;Soyi Jung;Jae-Hyun Kim;Marco Levorato;Joongheon Kim,"Autonomous surveillance unmanned aerial vehicles (UAVs) are deployed to observe the streets of the city for any suspicious activities. This paper utilizes surveillance UAVs for the purpose of detecting the presence of a fire in the streets. An extensive database is collected from UAV surveillance drones. With the aid of artificial intelligence (AI), fire stations can swiftly identify the presence of a fire emerging in the neighborhood. Spatio-temporal split learning is applied to this scenario to preserve privacy and globally train a fire classification model. Fires are hazardous natural disasters that can spread very quickly. Swift identification of fire is required to deploy firefighters to the scene. In order to do this, strong communication between the UAV and the central server where the deep learning process occurs is required. Improving communication resilience is integral to enhancing a safe experience on the roads. Therefore, this paper explores the adequate number of clients and data ratios for split learning in this UAV setting, as well as the required network infrastructure. △ Less","14 November, 2021",https://arxiv.org/pdf/2111.11856
Independent Learning in Stochastic Games,Asuman Ozdaglar;Muhammed O. Sayin;Kaiqing Zhang,"Reinforcement learning (RL) has recently achieved tremendous successes in many artificial intelligence applications. Many of the forefront applications of RL involve multiple agents, e.g., playing chess and Go games, autonomous driving, and robotics. Unfortunately, the framework upon which classical RL builds is inappropriate for multi-agent learning, as it assumes an agent's environment is stationary and does not take into account the adaptivity of other agents. In this review paper, we present the model of stochastic games for multi-agent learning in dynamic environments. We focus on the development of simple and independent learning dynamics for stochastic games: each agent is myopic and chooses best-response type actions to other agents' strategy without any coordination with her opponent. There has been limited progress on developing convergent best-response type independent learning dynamics for stochastic games. We present our recently proposed simple and independent learning dynamics that guarantee convergence in zero-sum stochastic games, together with a review of other contemporaneous algorithms for dynamic multi-agent learning in this setting. Along the way, we also reexamine some classical results from both the game theory and RL literature, to situate both the conceptual contributions of our independent learning dynamics, and the mathematical novelties of our analysis. We hope this review paper serves as an impetus for the resurgence of studying independent and natural learning dynamics in game theory, for the more challenging settings with a dynamic environment. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.11743
Machine Learning for Mars Exploration,Ali Momennasab,"Risk to human astronauts and interplanetary distance causing slow and limited communication drives scientists to pursue an autonomous approach to exploring distant planets, such as Mars. A portion of exploration of Mars has been conducted through the autonomous collection and analysis of Martian data by spacecraft such as the Mars rovers and the Mars Express Orbiter. The autonomy used on these Mars exploration spacecraft and on Earth to analyze data collected by these vehicles mainly consist of machine learning, a field of artificial intelligence where algorithms collect data and self-improve with the data. Additional applications of machine learning techniques for Mars exploration have potential to resolve communication limitations and human risks of interplanetary exploration. In addition, analyzing Mars data with machine learning has the potential to provide a greater understanding of Mars in numerous domains such as its climate, atmosphere, and potential future habitation. To explore further utilizations of machine learning techniques for Mars exploration, this paper will first summarize the general features and phenomena of Mars to provide a general overview of the planet, elaborate upon uncertainties of Mars that would be beneficial to explore and understand, summarize every current or previous usage of machine learning techniques in the exploration of Mars, explore implementations of machine learning that will be utilized in future Mars exploration missions, and explore machine learning techniques used in Earthly domains to provide solutions to the previously described uncertainties of Mars. △ Less","22 November, 2021",https://arxiv.org/pdf/2111.11537
Artificial Intelligence Technology analysis using Artificial Intelligence patent through Deep Learning model and vector space model,Yongmin Yoo;Dongjin Lim;Kyungsun Kim,"Thanks to rapid development of artificial intelligence technology in recent years, the current artificial intelligence technology is contributing to many part of society. Education, environment, medical care, military, tourism, economy, politics, etc. are having a very large impact on society as a whole. For example, in the field of education, there is an artificial intelligence tutoring system that automatically assigns tutors based on student's level. In the field of economics, there are quantitative investment methods that automatically analyze large amounts of data to find investment laws to create investment models or predict changes in financial markets. As such, artificial intelligence technology is being used in various fields. So, it is very important to know exactly what factors have an important influence on each field of artificial intelligence technology and how the relationship between each field is connected. Therefore, it is necessary to analyze artificial intelligence technology in each field. In this paper, we analyze patent documents related to artificial intelligence technology. We propose a method for keyword analysis within factors using artificial intelligence patent data sets for artificial intelligence technology analysis. This is a model that relies on feature engineering based on deep learning model named KeyBERT, and using vector space model. A case study of collecting and analyzing artificial intelligence patent data was conducted to show how the proposed model can be applied to real world problems. △ Less","7 November, 2021",https://arxiv.org/pdf/2111.11295
Comparing the Accuracy of Deep Neural Networks (DNN) and Convolutional Neural Network (CNN) in Music Genre Recognition (MGR): Experiments on Kurdish Music,Aza Zuhair;Hossein Hassani,"Musicologists use various labels to classify similar music styles under a shared title. But, non-specialists may categorize music differently. That could be through finding patterns in harmony, instruments, and form of the music. People usually identify a music genre solely by listening, but now computers and Artificial Intelligence (AI) can automate this process. The work on applying AI in the classification of types of music has been growing recently, but there is no evidence of such research on the Kurdish music genres. In this research, we developed a dataset that contains 880 samples from eight different Kurdish music genres. We evaluated two machine learning approaches, a Deep Neural Network (DNN) and a Convolutional Neural Network (CNN), to recognize the genres. The results showed that the CNN model outperformed the DNN by achieving 92% versus 90% accuracy. △ Less","22 November, 2021",https://arxiv.org/pdf/2111.11063
A Blockchain-Based Approach for Collaborative Formalization of Mathematics and Programs,Jin Xing Lim;Barnabé Monnot;Shaowei Lin;Georgios Piliouras,"Formalization of mathematics is the process of digitizing mathematical knowledge, which allows for formal proof verification as well as efficient semantic searches. Given the large and ever-increasing gap between the set of formalized and unformalized mathematical knowledge, there is a clear need to encourage more computer scientists and mathematicians to solve and formalize mathematical problems together. With blockchain technology, we are able to decentralize this process, provide time-stamped verification of authorship and encourage collaboration through implementation of incentive mechanisms via smart contracts. Currently, the formalization of mathematics is done through the use of proof assistants, which can be used to verify programs and protocols as well. Furthermore, with the advancement in artificial intelligence (AI), particularly machine learning, we can apply automated AI reasoning tools in these proof assistants and (at least partially) automate the process of synthesizing proofs. In our paper, we demonstrate a blockchain-based system for collaborative formalization of mathematics and programs incorporating both human labour as well as automated AI tools. We explain how Token-Curated Registries (TCR) and smart contracts are used to ensure appropriate documents are recorded and encourage collaboration through implementation of incentive mechanisms respectively. Using an illustrative example, we show how formalized proofs of different sorting algorithms can be produced collaboratively in our proposed blockchain system. △ Less","21 November, 2021",https://arxiv.org/pdf/2111.10824
Fueling the Next Quantum Leap in Cellular Networks: Embracing AI in 5G Evolution towards 6G,Xingqin Lin;Mingzhe Chen;Henrik Rydén;Jaeseong Jeong;Heunchul Lee;Mårten Sundberg;Roy Timo;Hazhir S. Razaghi;H. Vincent Poor,"Cellular networks, such as 5G systems, are becoming increasingly complex for supporting various deployment scenarios and applications. Embracing artificial intelligence (AI) in 5G evolution is critical to managing the complexity and fueling the next quantum leap in 6G cellular networks. In this article, we share our experience and best practices in applying AI in cellular networks. We first present a primer on the state of the art of AI in cellular networks, including basic concepts and recent key advances. Then we discuss 3GPP standardization aspects and share various design rationales influencing standardization. We also present case studies with real network data to showcase how AI can improve network performance and enable network automation. △ Less","20 November, 2021",https://arxiv.org/pdf/2111.10663
Quality and Computation Time in Optimization Problems,Zhicheng He,"Optimization problems are crucial in artificial intelligence. Optimization algorithms are generally used to adjust the performance of artificial intelligence models to minimize the error of mapping inputs to outputs. Current evaluation methods on optimization algorithms generally consider the performance in terms of quality. However, not all optimization algorithms for all test cases are evaluated equal from quality, the computation time should be also considered for optimization tasks. In this paper, we investigate the quality and computation time of optimization algorithms in optimization problems, instead of the one-for-all evaluation of quality. We select the well-known optimization algorithms (Bayesian optimization and evolutionary algorithms) and evaluate them on the benchmark test functions in terms of quality and computation time. The results show that BO is suitable to be applied in the optimization tasks that are needed to obtain desired quality in the limited function evaluations, and the EAs are suitable to search the optimal of the tasks that are allowed to find the optimal solution with enough function evaluations. This paper provides the recommendation to select suitable optimization algorithms for optimization problems with different numbers of function evaluations, which contributes to the efficiency that obtains the desired quality with less computation time for optimization problems. △ Less","20 November, 2021",https://arxiv.org/pdf/2111.10595
Satellite Based Computing Networks with Federated Learning,Hao Chen;Ming Xiao;Zhibo Pang,"Driven by the ever-increasing penetration and proliferation of data-driven applications, a new generation of wireless communication, the sixth-generation (6G) mobile system enhanced by artificial intelligence (AI), has attracted substantial research interests. Among various candidate technologies of 6G, low earth orbit (LEO) satellites have appealing characteristics of ubiquitous wireless access. However, the costs of satellite communication (SatCom) are still high, relative to counterparts of ground mobile networks. To support massively interconnected devices with intelligent adaptive learning and reduce expensive traffic in SatCom, we propose federated learning (FL) in LEO-based satellite communication networks. We first review the state-of-the-art LEO-based SatCom and related machine learning (ML) techniques, and then analyze four possible ways of combining ML with satellite networks. The learning performance of the proposed strategies is evaluated by simulation and results reveal that FL-based computing networks improve the performance of communication overheads and latency. Finally, we discuss future research topics along this research direction. △ Less","20 November, 2021",https://arxiv.org/pdf/2111.10586
Inter-Domain Fusion for Enhanced Intrusion Detection in Power Systems: An Evidence Theoretic and Meta-Heuristic Approach,Abhijeet Sahu;Katherine Davis,"False alerts due to misconfigured/ compromised IDS in ICS networks can lead to severe economic and operational damage. To solve this problem, research has focused on leveraging deep learning techniques that help reduce false alerts. However, a shortcoming is that these works often require or implicitly assume the physical and cyber sensors to be trustworthy. Implicit trust of data is a major problem with using artificial intelligence or machine learning for CPS security, because during critical attack detection time they are more at risk, with greater likelihood and impact, of also being compromised. To address this shortcoming, the problem is reframed on how to make good decisions given uncertainty. Then, the decision is detection, and the uncertainty includes whether the data used for ML-based IDS is compromised. Thus, this work presents an approach for reducing false alerts in CPS power systems by dealing uncertainty without the knowledge of prior distribution of alerts. Specifically, an evidence theoretic based approach leveraging Dempster Shafer combination rules are proposed for reducing false alerts. A multi-hypothesis mass function model is designed that leverages probability scores obtained from various supervised-learning classifiers. Using this model, a location-cum-domain based fusion framework is proposed and evaluated with different combination rules, that fuse multiple evidence from inter-domain and intra-domain sensors. The approach is demonstrated in a cyber-physical power system testbed with Man-In-The-Middle attack emulation in a large-scale synthetic electric grid. For evaluating the performance, plausibility, belief, pignistic, etc. metrics as decision functions are considered. To improve the performance, a multi-objective based genetic algorithm is proposed for feature selection considering the decision metrics as the fitness function. △ Less","19 November, 2021",https://arxiv.org/pdf/2111.10484
A Hybrid Approach for an Interpretable and Explainable Intrusion Detection System,Tiago Dias;Nuno Oliveira;Norberto Sousa;Isabel Praça;Orlando Sousa,"Cybersecurity has been a concern for quite a while now. In the latest years, cyberattacks have been increasing in size and complexity, fueled by significant advances in technology. Nowadays, there is an unavoidable necessity of protecting systems and data crucial for business continuity. Hence, many intrusion detection systems have been created in an attempt to mitigate these threats and contribute to a timelier detection. This work proposes an interpretable and explainable hybrid intrusion detection system, which makes use of artificial intelligence methods to achieve better and more long-lasting security. The system combines experts' written rules and dynamic knowledge continuously generated by a decision tree algorithm as new shreds of evidence emerge from network activity. △ Less","19 November, 2021",https://arxiv.org/pdf/2111.10280
Defeating Catastrophic Forgetting via Enhanced Orthogonal Weights Modification,Yanni Li;Bing Liu;Kaicheng Yao;Xiaoli Kou;Pengfan Lv;Yueshen Xu;Jiangtao Cui,"The ability of neural networks (NNs) to learn and remember multiple tasks sequentially is facing tough challenges in achieving general artificial intelligence due to their catastrophic forgetting (CF) issues. Fortunately, the latest OWM Orthogonal Weights Modification) and other several continual learning (CL) methods suggest some promising ways to overcome the CF issue. However, none of existing CL methods explores the following three crucial questions for effectively overcoming the CF issue: that is, what knowledge does it contribute to the effective weights modification of the NN during its sequential tasks learning? When the data distribution of a new learning task changes corresponding to the previous learned tasks, should a uniform/specific weight modification strategy be adopted or not? what is the upper bound of the learningable tasks sequentially for a given CL method? ect. To achieve this, in this paper, we first reveals the fact that of the weight gradient of a new learning task is determined by both the input space of the new task and the weight space of the previous learned tasks sequentially. On this observation and the recursive least square optimal method, we propose a new efficient and effective continual learning method EOWM via enhanced OWM. And we have theoretically and definitively given the upper bound of the learningable tasks sequentially of our EOWM. Extensive experiments conducted on the benchmarks demonstrate that our EOWM is effectiveness and outperform all of the state-of-the-art CL baselines. △ Less","19 November, 2021",https://arxiv.org/pdf/2111.10078
Explainable predictions of different machine learning algorithms used to predict Early Stage diabetes,V. Vakil;S. Pachchigar;C. Chavda;S. Soni,"Machine Learning and Artificial Intelligence can be widely used to diagnose chronic diseases so that necessary precautionary treatment can be done in critical time. Diabetes Mellitus which is one of the major diseases can be easily diagnosed by several Machine Learning algorithms. Early stage diagnosis is crucial to prevent dangerous consequences. In this paper we have made a comparative analysis of several machine learning algorithms viz. Random Forest, Decision Tree, Artificial Neural Networks, K Nearest Neighbor, Support Vector Machine, and XGBoost along with feature attribution using SHAP to identify the most important feature in predicting the diabetes on a dataset collected from Sylhet Hospital. As per the experimental results obtained, the Random Forest algorithm has outperformed all the other algorithms with an accuracy of 99 percent on this particular dataset. △ Less","18 November, 2021",https://arxiv.org/pdf/2111.09939
Reinforcement Learning on Human Decision Models for Uniquely Collaborative AI Teammates,Nicholas Kantack,"In 2021 the Johns Hopkins University Applied Physics Laboratory held an internal challenge to develop artificially intelligent (AI) agents that could excel at the collaborative card game Hanabi. Agents were evaluated on their ability to play with human players whom the agents had never previously encountered. This study details the development of the agent that won the challenge by achieving a human-play average score of 16.5, outperforming the current state-of-the-art for human-bot Hanabi scores. The winning agent's development consisted of observing and accurately modeling the author's decision making in Hanabi, then training with a behavioral clone of the author. Notably, the agent discovered a human-complementary play style by first mimicking human decision making, then exploring variations to the human-like strategy that led to higher simulated human-bot scores. This work examines in detail the design and implementation of this human compatible Hanabi teammate, as well as the existence and implications of human-complementary strategies and how they may be explored for more successful applications of AI in human machine teams. △ Less","18 November, 2021",https://arxiv.org/pdf/2111.09800
Hybrid Super Intelligence and Polymetric Analysis,Vladislav Dorofeev;Petro Trokhimchuk,The problem of possible applications Polymetric Analysis for the resolution problems of artificial Intelligence is discussed. As example the hybrid super intelligence system by N. Moiseev type was selected. The bond between polymetric analysis and hybrid super intelligence system was shown. In operational sense polymetric analysis is more general system. Therefore main principles of Moiseev concept may be unify with the help of polymetric analysis. Main peculiarities of this unification are analyzed. △ Less,"18 November, 2021",https://arxiv.org/pdf/2111.09762
Software Engineering for Responsible AI: An Empirical Study and Operationalised Patterns,Qinghua Lu;Liming Zhu;Xiwei Xu;Jon Whittle;David Douglas;Conrad Sanderson,"Although artificial intelligence (AI) is solving real-world challenges and transforming industries, there are serious concerns about its ability to behave and make decisions in a responsible way. Many AI ethics principles and guidelines for responsible AI have been recently issued by governments, organisations, and enterprises. However, these AI ethics principles and guidelines are typically high-level and do not provide concrete guidance on how to design and develop responsible AI systems. To address this shortcoming, we first present an empirical study where we interviewed 21 scientists and engineers to understand the practitioners' perceptions on AI ethics principles and their implementation. We then propose a template that enables AI ethics principles to be operationalised in the form of concrete patterns and suggest a list of patterns using the newly created template. These patterns provide concrete, operationalised guidance that facilitate the development of responsible AI systems. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09478
Advancing COVID-19 Diagnosis with Privacy-Preserving Collaboration in Artificial Intelligence,Xiang Bai;Hanchen Wang;Liya Ma;Yongchao Xu;Jiefeng Gan;Ziwei Fan;Fan Yang;Ke Ma;Jiehua Yang;Song Bai;Chang Shu;Xinyu Zou;Renhao Huang;Changzheng Zhang;Xiaowu Liu;Dandan Tu;Chuou Xu;Wenqing Zhang;Xi Wang;Anguo Chen;Yu Zeng;Dehua Yang;Ming-Wei Wang;Nagaraj Holalkere;Neil J. Halin,"Artificial intelligence (AI) provides a promising substitution for streamlining COVID-19 diagnoses. However, concerns surrounding security and trustworthiness impede the collection of large-scale representative medical data, posing a considerable challenge for training a well-generalised model in clinical practices. To address this, we launch the Unified CT-COVID AI Diagnostic Initiative (UCADI), where the AI model can be distributedly trained and independently executed at each host institution under a federated learning framework (FL) without data sharing. Here we show that our FL model outperformed all the local models by a large yield (test sensitivity /specificity in China: 0.973/0.951, in the UK: 0.730/0.942), achieving comparable performance with a panel of professional radiologists. We further evaluated the model on the hold-out (collected from another two hospitals leaving out the FL) and heterogeneous (acquired with contrast materials) data, provided visual explanations for decisions made by the model, and analysed the trade-offs between the model performance and the communication costs in the federated training process. Our study is based on 9,573 chest computed tomography scans (CTs) from 3,336 patients collected from 23 hospitals located in China and the UK. Collectively, our work advanced the prospects of utilising federated learning for privacy-preserving AI in digital health. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09461
Sustainable Artificial Intelligence through Continual Learning,Andrea Cossu;Marta Ziosi;Vincenzo Lomonaco,"The increasing attention on Artificial Intelligence (AI) regulation has led to the definition of a set of ethical principles grouped into the Sustainable AI framework. In this article, we identify Continual Learning, an active area of AI research, as a promising approach towards the design of systems compliant with the Sustainable AI principles. While Sustainable AI outlines general desiderata for ethical applications, Continual Learning provides means to put such desiderata into practice. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09437
Highly Accurate and Reliable Wireless Network Slicing in 5th Generation Networks: A Hybrid Deep Learning Approach,Sulaiman Khan;Suleman Khan;Yasir Ali;Muhammad Khalid;Zahid Ullah;Shahid Mumtaz,"In the current era, the next-generation networks like 5th generation (5G) and 6th generation (6G) networks require high security, low latency with a high reliable standards and capacity. In these networks, reconfigurable wireless network slicing is considered as one of the key elements for 5G and 6G networks. A reconfigurable slicing allows the operators to run various instances of the network using a single infrastructure for a better quality of services (QoS). The QoS can be achieved by reconfiguring and optimizing these networks using Artificial intelligence and machine learning algorithms. To develop a smart decision-making mechanism for network management and restricting network slice failures, machine learning-enabled reconfigurable wireless network solutions are required. In this paper, we propose a hybrid deep learning model that consists of a convolution neural network (CNN) and long short term memory (LSTM). The CNN performs resource allocation, network reconfiguration, and slice selection while the LSTM is used for statistical information (load balancing, error rate etc.) regarding network slices. The applicability of the proposed model is validated by using multiple unknown devices, slice failure, and overloading conditions. The overall accuracy of 95.17% is achieved by the proposed model that reflects its applicability. △ Less","7 October, 2021",https://arxiv.org/pdf/2111.09416
"Induce, Edit, Retrieve: Language Grounded Multimodal Schema for Instructional Video Retrieval",Yue Yang;Joongwon Kim;Artemis Panagopoulou;Mark Yatskar;Chris Callison-Burch,"Schemata are structured representations of complex tasks that can aid artificial intelligence by allowing models to break down complex tasks into intermediate steps. We propose a novel system that induces schemata from web videos and generalizes them to capture unseen tasks with the goal of improving video retrieval performance. Our system proceeds in three major phases: (1) Given a task with related videos, we construct an initial schema for a task using a joint video-text model to match video segments with text representing steps from wikiHow; (2) We generalize schemata to unseen tasks by leveraging language models to edit the text within existing schemata. Through generalization, we can allow our schemata to cover a more extensive range of tasks with a small amount of learning data; (3) We conduct zero-shot instructional video retrieval with the unseen task names as the queries. Our schema-guided approach outperforms existing methods for video retrieval, and we demonstrate that the schemata induced by our system are better than those generated by other models. △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09276
Oil and Gas Pipeline Monitoring during COVID-19 Pandemic via Unmanned Aerial Vehicle,Myssar Jabbar Hammood Al-Battbootti;Iuliana Marin;Nicolae Goga;Ramona Popa,"The vast network of oil and gas transmission pipelines requires periodic monitoring for maintenance and hazard inspection to avoid equipment failure and potential accidents. The severe COVID-19 pandemic situation forced the companies to shrink the size of their teams. One risk which is faced on-site is represented by the uncontrolled release of flammable oil and gas. Among many inspection methods, the unmanned aerial vehicle system contains flexibility and stability. Unmanned aerial vehicles can transfer data in real-time, while they are doing their monitoring tasks. The current article focuses on unmanned aerial vehicles equipped with optical sensing and artificial intelligence, especially image recognition with deep learning techniques for pipeline surveillance. Unmanned aerial vehicles can be used for regular patrolling duties to identify and capture images and videos of the area of interest. Places that are hard to reach will be accessed faster, cheaper and with less risk. The current paper is based on the idea of capturing video and images of drone-based inspections, which can discover several potential hazardous problems before they become dangerous. Damage can emerge as a weakening of the cladding on the external pipe insulation. There can also be the case when the thickness of piping through external corrosion can occur. The paper describes a survey completed by experts from the oil and gas industry done for finding the functional and non-functional requirements of the proposed system. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.09155
Airport Taxi Time Prediction and Alerting: A Convolutional Neural Network Approach,Erik Vargo;Alex Tien;Arian Jafari,"This paper proposes a novel approach to predict and determine whether the average taxi- out time at an airport will exceed a pre-defined threshold within the next hour of operations. Prior work in this domain has focused exclusively on predicting taxi-out times on a flight-by-flight basis, which requires significant efforts and data on modeling taxiing activities from gates to runways. Learning directly from surface radar information with minimal processing, a computer vision-based model is proposed that incorporates airport surface data in such a way that adaptation-specific information (e.g., runway configuration, the state of aircraft in the taxiing process) is inferred implicitly and automatically by Artificial Intelligence (AI). △ Less","17 November, 2021",https://arxiv.org/pdf/2111.09139
Generating Unrestricted 3D Adversarial Point Clouds,Xuelong Dai;Yanjie Li;Hua Dai;Bin Xiao,"Utilizing 3D point cloud data has become an urgent need for the deployment of artificial intelligence in many areas like facial recognition and self-driving. However, deep learning for 3D point clouds is still vulnerable to adversarial attacks, e.g., iterative attacks, point transformation attacks, and generative attacks. These attacks need to restrict perturbations of adversarial examples within a strict bound, leading to the unrealistic adversarial 3D point clouds. In this paper, we propose an Adversarial Graph-Convolutional Generative Adversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D point clouds from scratch. Specifically, we use a graph convolutional generator and a discriminator with an auxiliary classifier to generate realistic point clouds, which learn the latent distribution from the real 3D data. The unrestricted adversarial attack loss is incorporated in the special adversarial training of GAN, which enables the generator to generate the adversarial examples to spoof the target network. Compared with the existing state-of-art attack methods, the experiment results demonstrate the effectiveness of our unrestricted adversarial attack methods with a higher attack success rate and visual quality. Additionally, the proposed AdvGCGAN can achieve better performance against defense models and better transferability than existing attack methods with strong camouflage. △ Less","18 November, 2021",https://arxiv.org/pdf/2111.08973
Federated Learning for Smart Healthcare: A Survey,Dinh C. Nguyen;Quoc-Viet Pham;Pubudu N. Pathirana;Ming Ding;Aruna Seneviratne;Zihuai Lin;Octavia A. Dobre;Won-Joo Hwang,"Recent advances in communication technologies and Internet-of-Medical-Things have transformed smart healthcare enabled by artificial intelligence (AI). Traditionally, AI techniques require centralized data collection and processing that may be infeasible in realistic healthcare scenarios due to the high scalability of modern healthcare networks and growing data privacy concerns. Federated Learning (FL), as an emerging distributed collaborative AI paradigm, is particularly attractive for smart healthcare, by coordinating multiple clients (e.g., hospitals) to perform AI training without sharing raw data. Accordingly, we provide a comprehensive survey on the use of FL in smart healthcare. First, we present the recent advances in FL, the motivations, and the requirements of using FL in smart healthcare. The recent FL designs for smart healthcare are then discussed, ranging from resource-aware FL, secure and privacy-aware FL to incentive FL and personalized FL. Subsequently, we provide a state-of-the-art review on the emerging applications of FL in key healthcare domains, including health data management, remote health monitoring, medical imaging, and COVID-19 detection. Several recent FL-based smart healthcare projects are analyzed, and the key lessons learned from the survey are also highlighted. Finally, we discuss interesting research challenges and possible directions for future FL research in smart healthcare. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08834
How Mock Model Training Enhances User Perceptions of AI Systems,Amama Mahmood;Gopika Ajaykumar;Chien-Ming Huang,"Artificial Intelligence (AI) is an integral part of our daily technology use and will likely be a critical component of emerging technologies. However, negative user preconceptions may hinder adoption of AI-based decision making. Prior work has highlighted the potential of factors such as transparency and explainability in improving user perceptions of AI. We further contribute to work on improving user perceptions of AI by demonstrating that bringing the user in the loop through mock model training can improve their perceptions of an AI agent's capability and their comfort with the possibility of using technology employing the AI agent. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08830
Two-step adversarial debiasing with partial learning -- medical image case-studies,Ramon Correa;Jiwoong Jason Jeong;Bhavik Patel;Hari Trivedi;Judy W. Gichoya;Imon Banerjee,"The use of artificial intelligence (AI) in healthcare has become a very active research area in the last few years. While significant progress has been made in image classification tasks, only a few AI methods are actually being deployed in hospitals. A major hurdle in actively using clinical AI models currently is the trustworthiness of these models. More often than not, these complex models are black boxes in which promising results are generated. However, when scrutinized, these models begin to reveal implicit biases during the decision making, such as detecting race and having bias towards ethnic groups and subpopulations. In our ongoing study, we develop a two-step adversarial debiasing approach with partial learning that can reduce the racial disparity while preserving the performance of the targeted task. The methodology has been evaluated on two independent medical image case-studies - chest X-ray and mammograms, and showed promises in bias reduction while preserving the targeted performance. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08711
Words of Wisdom: Representational Harms in Learning From AI Communication,Amanda Buddemeyer;Erin Walker;Malihe Alikhani,"Many educational technologies use artificial intelligence (AI) that presents generated or produced language to the learner. We contend that all language, including all AI communication, encodes information about the identity of the human or humans who contributed to crafting the language. With AI communication, however, the user may index identity information that does not match the source. This can lead to representational harms if language associated with one cultural group is presented as ""standard"" or ""neutral"", if the language advantages one group over another, or if the language reinforces negative stereotypes. In this work, we discuss a case study using a Visual Question Generation (VQG) task involving gathering crowdsourced data from targeted demographic groups. Generated questions will be presented to human evaluators to understand how they index the identity behind the language, whether and how they perceive any representational harms, and how they would ideally address any such harms caused by AI communication. We reflect on the educational applications of this work as well as the implications for equality, diversity, and inclusion (EDI). △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08581
Use of machine learning in geriatric clinical care for chronic diseases: a systematic literature review,Avishek Choudhury;Emily Renjilian;Onur Asan,"Objectives-Geriatric clinical care is a multidisciplinary assessment designed to evaluate older patients (age 65 years and above) functional ability, physical health, and cognitive wellbeing. The majority of these patients suffer from multiple chronic conditions and require special attention. Recently, hospitals utilize various artificial intelligence (AI) systems to improve care for elderly patients. The purpose of this systematic literature review is to understand the current use of AI systems, particularly machine learning (ML), in geriatric clinical care for chronic diseases. Materials and Methods-We restricted our search to eight databases, namely PubMed, WorldCat, MEDLINE, ProQuest, ScienceDirect, SpringerLink, Wiley, and ERIC, to analyze research articles published in English between January 2010 and June 2019. We focused on studies that used ML algorithms in the care of geriatrics patients with chronic conditions. Results-We identified 35 eligible studies and classified in three groups-psychological disorder (n=22), eye diseases (n=6), and others (n=7). This review identified the lack of standardized ML evaluation metrics and the need for data governance specific to health care applications. Conclusion- More studies and ML standardization tailored to health care applications are required to confirm whether ML could aid in improving geriatric clinical care. △ Less","30 October, 2021",https://arxiv.org/pdf/2111.08441
An Empirical Study of Finding Similar Exercises,Tongwen Huang;Xihua Li,"Education artificial intelligence aims to profit tasks in the education domain such as intelligent test paper generation and consolidation exercises where the main technique behind is how to match the exercises, known as the finding similar exercises(FSE) problem. Most of these approaches emphasized their model abilities to represent the exercise, unfortunately there are still many challenges such as the scarcity of data, insufficient understanding of exercises and high label noises. We release a Chinese education pre-trained language model BERT_{Edu} for the label-scarce dataset and introduce the exercise normalization to overcome the diversity of mathematical formulas and terms in exercise. We discover new auxiliary tasks in an innovative way depends on problem-solving ideas and propose a very effective MoE enhanced multi-task model for FSE task to attain better understanding of exercises. In addition, confidence learning was utilized to prune train-set and overcome high noises in labeling data. Experiments show that these methods proposed in this paper are very effective. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08322
Deep Distilling: automated code generation using explainable deep learning,Paul J. Blazek;Kesavan Venkatesh;Milo M. Lin,"Human reasoning can distill principles from observed patterns and generalize them to explain and solve novel problems. The most powerful artificial intelligence systems lack explainability and symbolic reasoning ability, and have therefore not achieved supremacy in domains requiring human understanding, such as science or common sense reasoning. Here we introduce deep distilling, a machine learning method that learns patterns from data using explainable deep learning and then condenses it into concise, executable computer code. The code, which can contain loops, nested logical statements, and useful intermediate variables, is equivalent to the neural network but is generally orders of magnitude more compact and human-comprehensible. On a diverse set of problems involving arithmetic, computer vision, and optimization, we show that deep distilling generates concise code that generalizes out-of-distribution to solve problems orders-of-magnitude larger and more complex than the training data. For problems with a known ground-truth rule set, deep distilling discovers the rule set exactly with scalable guarantees. For problems that are ambiguous or computationally intractable, the distilled rules are similar to existing human-derived algorithms and perform at par or better. Our approach demonstrates that unassisted machine intelligence can build generalizable and intuitive rules explaining patterns in large datasets that would otherwise overwhelm human reasoning. △ Less","16 November, 2021",https://arxiv.org/pdf/2111.08275
An argument for the impossibility of machine intelligence,Jobst Landgrebe;Barry Smith,"Since the noun phrase `artificial intelligence' (AI) was coined, it has been debated whether humans are able to create intelligence using technology. We shed new light on this question from the point of view of themodynamics and mathematics. First, we define what it is to be an agent (device) that could be the bearer of AI. Then we show that the mainstream definitions of `intelligence' proposed by Hutter and others and still accepted by the AI community are too weak even to capture what is involved when we ascribe intelligence to an insect. We then summarise the highly useful definition of basic (arthropod) intelligence proposed by Rodney Brooks, and we identify the properties that an AI agent would need to possess in order to be the bearer of intelligence by this definition. Finally, we show that, from the perspective of the disciplines needed to create such an agent, namely mathematics and physics, these properties are realisable by neither implicit nor explicit mathematical design nor by setting up an environment in which an AI could evolve spontaneously. △ Less","20 October, 2021",https://arxiv.org/pdf/2111.07765
Interactive Medical Image Segmentation with Self-Adaptive Confidence Calibration,Wenhao Li;Qisen Xu;Chuyun Shen;Bin Hu;Fengping Zhu;Yuxin Li;Bo Jin;Xiangfeng Wang,"Medical image segmentation is one of the fundamental problems for artificial intelligence-based clinical decision systems. Current automatic medical image segmentation methods are often failed to meet clinical requirements. As such, a series of interactive segmentation algorithms are proposed to utilize expert correction information. However, existing methods suffer from some segmentation refining failure problems after long-term interactions and some cost problems from expert annotation, which hinder clinical applications. This paper proposes an interactive segmentation framework, called interactive MEdical segmentation with self-adaptive Confidence CAlibration (MECCA), by introducing the corrective action evaluation, which combines the action-based confidence learning and multi-agent reinforcement learning (MARL). The evaluation is established through a novel action-based confidence network, and the corrective actions are obtained from MARL. Based on the confidential information, a self-adaptive reward function is designed to provide more detailed feedback, and a simulated label generation mechanism is proposed on unsupervised data to reduce over-reliance on labeled data. Experimental results on various medical image datasets have shown the significant performance of the proposed algorithm. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.07716
Rationale production to support clinical decision-making,Niall Taylor;Lei Sha;Dan W Joyce;Thomas Lukasiewicz;Alejo Nevado-Holgado;Andrey Kormilitzin,"The development of neural networks for clinical artificial intelligence (AI) is reliant on interpretability, transparency, and performance. The need to delve into the black-box neural network and derive interpretable explanations of model output is paramount. A task of high clinical importance is predicting the likelihood of a patient being readmitted to hospital in the near future to enable efficient triage. With the increasing adoption of electronic health records (EHRs), there is great interest in applications of natural language processing (NLP) to clinical free-text contained within EHRs. In this work, we apply InfoCal, the current state-of-the-art model that produces extractive rationales for its predictions, to the task of predicting hospital readmission using hospital discharge notes. We compare extractive rationales produced by InfoCal to competitive transformer-based models pretrained on clinical text data and for which the attention mechanism can be used for interpretation. We find each presented model with selected interpretability or feature importance methods yield varying results, with clinical language domain expertise and pretraining critical to performance and subsequent interpretability. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.07611
"Confucius, Cyberpunk and Mr. Science: Comparing AI ethics between China and the EU",Pascale Fung;Hubert Etienne,"The exponential development and application of artificial intelligence triggered an unprecedented global concern for potential social and ethical issues. Stakeholders from different industries, international foundations, governmental organisations and standards institutions quickly improvised and created various codes of ethics attempting to regulate AI. A major concern is the large homogeneity and presumed consensualism around these principles. While it is true that some ethical doctrines, such as the famous Kantian deontology, aspire to universalism, they are however not universal in practice. In fact, ethical pluralism is more about differences in which relevant questions to ask rather than different answers to a common question. When people abide by different moral doctrines, they tend to disagree on the very approach to an issue. Even when people from different cultures happen to agree on a set of common principles, it does not necessarily mean that they share the same understanding of these concepts and what they entail. In order to better understand the philosophical roots and cultural context underlying ethical principles in AI, we propose to analyse and compare the ethical principles endorsed by the Chinese National New Generation Artificial Intelligence Governance Professional Committee (CNNGAIGPC) and those elaborated by the European High-level Expert Group on AI (HLEGAI). China and the EU have very different political systems and diverge in their cultural heritages. In our analysis, we wish to highlight that principles that seem similar a priori may actually have different meanings, derived from different approaches and reflect distinct goals. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.07555
Randomized Classifiers vs Human Decision-Makers: Trustworthy AI May Have to Act Randomly and Society Seems to Accept This,Gábor Erdélyi;Olivia J. Erdélyi;Vladimir Estivill-Castro,"As \emph{artificial intelligence} (AI) systems are increasingly involved in decisions affecting our lives, ensuring that automated decision-making is fair and ethical has become a top priority. Intuitively, we feel that akin to human decisions, judgments of artificial agents should necessarily be grounded in some moral principles. Yet a decision-maker (whether human or artificial) can only make truly ethical (based on any ethical theory) and fair (according to any notion of fairness) decisions if full information on all the relevant factors on which the decision is based are available at the time of decision-making. This raises two problems: (1) In settings, where we rely on AI systems that are using classifiers obtained with supervised learning, some induction/generalization is present and some relevant attributes may not be present even during learning. (2) Modeling such decisions as games reveals that any -- however ethical -- pure strategy is inevitably susceptible to exploitation. Moreover, in many games, a Nash Equilibrium can only be obtained by using mixed strategies, i.e., to achieve mathematically optimal outcomes, decisions must be randomized. In this paper, we argue that in supervised learning settings, there exist random classifiers that perform at least as well as deterministic classifiers, and may hence be the optimal choice in many circumstances. We support our theoretical results with an empirical study indicating a positive societal attitude towards randomized artificial decision-makers, and discuss some policy and implementation issues related to the use of random classifiers that relate to and are relevant for current AI policy and standardization initiatives. △ Less","15 November, 2021",https://arxiv.org/pdf/2111.07545
A Survey on AI Assurance,Feras A. Batarseh;Laura Freeman,"Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide library of algorithms for different problems. One important notion for the adoption of AI algorithms into operational decision process is the concept of assurance. The literature on assurance, unfortunately, conceals its outcomes within a tangled landscape of conflicting approaches, driven by contradicting motivations, assumptions, and intuitions. Accordingly, albeit a rising and novel area, this manuscript provides a systematic review of research works that are relevant to AI assurance, between years 1985 - 2021, and aims to provide a structured alternative to the landscape. A new AI assurance definition is adopted and presented and assurance methods are contrasted and tabulated. Additionally, a ten-metric scoring system is developed and introduced to evaluate and compare existing methods. Lastly, in this manuscript, we provide foundational insights, discussions, future directions, a roadmap, and applicable recommendations for the development and deployment of AI assurance. △ Less","14 November, 2021",https://arxiv.org/pdf/2111.07505
Computational Argumentation and Cognition,Emmanuelle Dietz;Antonis Kakas;Loizos Michael,"This paper examines the interdisciplinary research question of how to integrate Computational Argumentation, as studied in AI, with Cognition, as can be found in Cognitive Science, Linguistics, and Philosophy. It stems from the work of the 1st Workshop on Computational Argumentation and Cognition (COGNITAR), which was organized as part of the 24th European Conference on Artificial Intelligence (ECAI), and took place virtually on September 8th, 2020. The paper begins with a brief presentation of the scientific motivation for the integration of Computational Argumentation and Cognition, arguing that within the context of Human-Centric AI the use of theory and methods from Computational Argumentation for the study of Cognition can be a promising avenue to pursue. A short summary of each of the workshop presentations is given showing the wide spectrum of problems where the synthesis of the theory and methods of Computational Argumentation with other approaches that study Cognition can be applied. The paper presents the main problems and challenges in the area that would need to be addressed, both at the scientific level but also at the epistemological level, particularly in relation to the synthesis of ideas and approaches from the various disciplines involved. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06958
Visual Intelligence through Human Interaction,Ranjay Krishna;Mitchell Gordon;Li Fei-Fei;Michael Bernstein,"Over the last decade, Computer Vision, the branch of Artificial Intelligence aimed at understanding the visual world, has evolved from simply recognizing objects in images to describing pictures, answering questions about images, aiding robots maneuver around physical spaces and even generating novel visual content. As these tasks and applications have modernized, so too has the reliance on more data, either for model training or for evaluation. In this chapter, we demonstrate that novel interaction strategies can enable new forms of data collection and evaluation for Computer Vision. First, we present a crowdsourcing interface for speeding up paid data collection by an order of magnitude, feeding the data-hungry nature of modern vision models. Second, we explore a method to increase volunteer contributions using automated social interventions. Third, we develop a system to ensure human evaluation of generative vision models are reliable, affordable and grounded in psychophysics theory. We conclude with future opportunities for Human-Computer Interaction to aid Computer Vision. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06913
Influential Papers in Artificial Intelligence and Paediatrics: Assessing RPYS by Experts Review,Peter Kokol;Jernej Završnik;Helena Blažun Vošner,"The use of artificial intelligence in paediatrics has vastly increased in the last few years. Interestingly, no historical bibliometric study analysing the knowledge development in this specific paediatric field has been performed yet, thus our study aimed to close this gap. References Publication Years Spectrography (RPYS), more precisely CitedReferenceExplorer (CRE) software tool was employed to achieve this aim. We identified 28 influential papers and domain experts validation showed that both, the RPYS method and CRE tool performed adequately in the identification process. △ Less","27 October, 2021",https://arxiv.org/pdf/2111.06852
NRC-GAMMA: Introducing a Novel Large Gas Meter Image Dataset,Ashkan Ebadi;Patrick Paul;Sofia Auer;Stéphane Tremblay,"Automatic meter reading technology is not yet widespread. Gas, electricity, or water accumulation meters reading is mostly done manually on-site either by an operator or by the homeowner. In some countries, the operator takes a picture as reading proof to confirm the reading by checking offline with another operator and/or using it as evidence in case of conflicts or complaints. The whole process is time-consuming, expensive, and prone to errors. Automation can optimize and facilitate such labor-intensive and human error-prone processes. With the recent advances in the fields of artificial intelligence and computer vision, automatic meter reading systems are becoming more viable than ever. Motivated by the recent advances in the field of artificial intelligence and inspired by open-source open-access initiatives in the research community, we introduce a novel large benchmark dataset of real-life gas meter images, named the NRC-GAMMA dataset. The data were collected from an Itron 400A diaphragm gas meter on January 20, 2020, between 00:05 am and 11:59 pm. We employed a systematic approach to label the images, validate the labellings, and assure the quality of the annotations. The dataset contains 28,883 images of the entire gas meter along with 57,766 cropped images of the left and the right dial displays. We hope the NRC-GAMMA dataset helps the research community to design and implement accurate, innovative, intelligent, and reproducible automatic gas meter reading solutions. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06827
Monte Carlo dropout increases model repeatability,Andreanne Lemay;Katharina Hoebel;Christopher P. Bridge;Didem Egemen;Ana Cecilia Rodriguez;Mark Schiffman;John Peter Campbell;Jayashree Kalpathy-Cramer,"The integration of artificial intelligence into clinical workflows requires reliable and robust models. Among the main features of robustness is repeatability. Much attention is given to classification performance without assessing the model repeatability, leading to the development of models that turn out to be unusable in practice. In this work, we evaluate the repeatability of four model types on images from the same patient that were acquired during the same visit. We study the performance of binary, multi-class, ordinal, and regression models on three medical image analysis tasks: cervical cancer screening, breast density estimation, and retinopathy of prematurity classification. Moreover, we assess the impact of sampling Monte Carlo dropout predictions at test time on classification performance and repeatability. Leveraging Monte Carlo predictions significantly increased repeatability for all tasks on the binary, multi-class, and ordinal models leading to an average reduction of the 95% limits of agreement by 17% points. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06754
A Quantum Natural Language Processing Approach to Musical Intelligence,Eduardo Reck Miranda;Richie Yeung;Anna Pearson;Konstantinos Meichanetzidis;Bob Coecke,"There has been tremendous progress in Artificial Intelligence (AI) for music, in particular for musical composition and access to large databases for commercialisation through the Internet. We are interested in further advancing this field, focusing on composition. In contrast to current black-box AI methods, we are championing an interpretable compositional outlook on generative music systems. In particular, we are importing methods from the Distributional Compositional Categorical (DisCoCat) modelling framework for Natural Language Processing (NLP), motivated by musical grammars. Quantum computing is a nascent technology, which is very likely to impact the music industry in time to come. Thus, we are pioneering a Quantum Natural Language Processing (QNLP) approach to develop a new generation of intelligent musical systems. This work follows from previous experimental implementations of DisCoCat linguistic models on quantum hardware. In this chapter, we present Quanthoven, the first proof-of-concept ever built, which (a) demonstrates that it is possible to program a quantum computer to learn to classify music that conveys different meanings and (b) illustrates how such a capability might be leveraged to develop a system to compose meaningful pieces of music. After a discussion about our current understanding of music as a communication medium and its relationship to natural language, the chapter focuses on the techniques developed to (a) encode musical compositions as quantum circuits, and (b) design a quantum classifier. The chapter ends with demonstrations of compositions created with the system. △ Less","9 December, 2021",https://arxiv.org/pdf/2111.06741
"Towards 6G Internet of Things: Recent Advances, Use Cases, and Open Challenges",Zakria Qadir;Hafiz Suliman Munawar;Nasir Saeed;Khoa Le,"Smart services based on the Internet of Everything (IoE) are gaining considerable popularity due to the ever-increasing demands of wireless networks. This demands the appraisal of the wireless networks with enhanced properties as next-generation communication systems. Although 5G networks show great potential to support numerous IoE based services, it is not adequate to meet the complete requirements of the new smart applications. Therefore, there is an increased demand for envisioning the 6G wireless communication systems to overcome the major limitations in the existing 5G networks. Moreover, incorporating artificial intelligence in 6G will provide solutions for very complex problems relevant to network optimization. Furthermore, to add further value to the future 6G networks, researchers are investigating new technologies, such as THz and quantum communications. The requirements of future 6G wireless communications demand to support massive data-driven applications and the increasing number of users. This paper presents recent advances in the 6G wireless networks, including the evolution from 1G to 5G communications, the research trends for 6G, enabling technologies, and state-of-the-art 6G projects. △ Less","12 November, 2021",https://arxiv.org/pdf/2111.06596
DPLL(MAPF): an Integration of Multi-Agent Path Finding and SAT Solving Technologies,Martin Čapek;Pavel Surynek,"In multi-agent path finding (MAPF), the task is to find non-conflicting paths for multiple agents from their initial positions to given individual goal positions. MAPF represents a classical artificial intelligence problem often addressed by heuristic-search. An important alternative to search-based techniques is compilation of MAPF to a different formalism such as Boolean satisfiability (SAT). Contemporary SAT-based approaches to MAPF regard the SAT solver as an external tool whose task is to return an assignment of all decision variables of a Boolean model of input MAPF. We present in this short paper a novel compilation scheme called DPLL(MAPF) in which the consistency checking of partial assignments of decision variables with respect to the MAPF rules is integrated directly into the SAT solver. This scheme allows for far more automated compilation where the SAT solver and the consistency checking procedure work together simultaneously to create the Boolean model and to search for its satisfying assignment. △ Less","11 November, 2021",https://arxiv.org/pdf/2111.06494
Personalized multi-faceted trust modeling to determine trust links in social media and its potential for misinformation management,Alexandre Parmentier;Robin Cohen;Xueguang Ma;Gaurav Sahu;Queenie Chen,"In this paper, we present an approach for predicting trust links between peers in social media, one that is grounded in the artificial intelligence area of multiagent trust modeling. In particular, we propose a data-driven multi-faceted trust modeling which incorporates many distinct features for a comprehensive analysis. We focus on demonstrating how clustering of similar users enables a critical new functionality: supporting more personalized, and thus more accurate predictions for users. Illustrated in a trust-aware item recommendation task, we evaluate the proposed framework in the context of a large Yelp dataset. We then discuss how improving the detection of trusted relationships in social media can assist in supporting online users in their battle against the spread of misinformation and rumours, within a social networking environment which has recently exploded in popularity. We conclude with a reflection on a particularly vulnerable user base, older adults, in order to illustrate the value of reasoning about groups of users, looking to some future directions for integrating known preferences with insights gained through data analysis. △ Less","11 November, 2021",https://arxiv.org/pdf/2111.06440
Explainable AI (XAI): A Systematic Meta-Survey of Current Challenges and Future Opportunities,Waddah Saeed;Christian Omlin,"The past decade has seen significant progress in artificial intelligence (AI), which has resulted in algorithms being adopted for resolving a variety of problems. However, this success has been met by increasing model complexity and employing black-box AI models that lack transparency. In response to this need, Explainable AI (XAI) has been proposed to make AI more transparent and thus advance the adoption of AI in critical domains. Although there are several reviews of XAI topics in the literature that identified challenges and potential research directions in XAI, these challenges and research directions are scattered. This study, hence, presents a systematic meta-survey for challenges and future research directions in XAI organized in two themes: (1) general challenges and research directions in XAI and (2) challenges and research directions in XAI based on machine learning life cycle's phases: design, development, and deployment. We believe that our meta-survey contributes to XAI literature by providing a guide for future exploration in the XAI area. △ Less","11 November, 2021",https://arxiv.org/pdf/2111.06420
Governance of Ethical and Trustworthy AI Systems: Research Gaps in the ECCOLA Method,Mamia Agbese;Hanna-Kaisa Alanen;Jani Antikainen;Erika Halme;Hannakaisa Isomäki;Marianna Jantunen;Kai-Kristian Kemell;Rebekah Rousi;Heidi Vainio-Pekka;Ville Vakkuri,"Advances in machine learning (ML) technologies have greatly improved Artificial Intelligence (AI) systems. As a result, AI systems have become ubiquitous, with their application prevalent in virtually all sectors. However, AI systems have prompted ethical concerns, especially as their usage crosses boundaries in sensitive areas such as healthcare, transportation, and security. As a result, users are calling for better AI governance practices in ethical AI systems. Therefore, AI development methods are encouraged to foster these practices. This research analyzes the ECCOLA method for developing ethical and trustworthy AI systems to determine if it enables AI governance in development processes through ethical practices. The results demonstrate that while ECCOLA fully facilitates AI governance in corporate governance practices in all its processes, some of its practices do not fully foster data governance and information governance practices. This indicates that the method can be further improved. △ Less","11 November, 2021",https://arxiv.org/pdf/2111.06207
Implementation of Ethically Aligned Design with Ethical User stories in SMART terminal Digitalization project: Use case Passenger Flow,Erika Halme;Mamia Agbese;Hanna-Kaisa Alanen;Jani Antikainen;Marianna Jantunen;Arif Ali Khan;Kai-Kristian Kemell;Ville Vakkuri;Pekka Abrahamsson,"Digitalization and Smart systems are part of our everyday lives today. So far the development has been rapid and all the implications that comes after the deployment has not been able to foresee or even assess during the development, especially when ethics or trustworthiness is concerned. Artificial Intelligence (AI) and Autonomous Systems (AS) are the direction that software systems are taking today. It is witnessed in banks, stores, internet and it is proceeding to transportation as well as on traveling. Autonomous maritime industry has also taking this direction when taking under development in digitalization on fairway and port terminals. AI ethics has advanced profoundly since the machine learning develop during the last decade and is now being implemented in AI development and workflow of software engineers. It is not an easy task and tools are needed to make the ethical assessment easier. This paper will review a research in an industrial setting, where Ethically Aligned Design practice, Ethical User Stories are used to transfer ethical requirements to ethical user stories to form practical solutions for project use. This project is in the field of maritime industry and concentrates on digitalization of port terminals and this particular paper focuses on the passenger flow. Results are positive towards the practice of Ethical User Stories, drawn from a large empirical data set. △ Less","11 November, 2021",https://arxiv.org/pdf/2111.06116
Secure and Reliable Transfer Learning Framework for 6G-enabled Internet of Vehicles,Minrui Xu;Dinh Thai Hoang;Jiawen Kang;Dusit Niyato;Qiang Yan;Dong In Kim,"In the coming 6G era, Internet of Vehicles (IoV) has been evolving towards 6G-enabled IoV with super-high data rate, seamless networking coverage, and ubiquitous intelligence by Artificial Intelligence (AI). Transfer Learning (TL) has great potential to empower promising 6G-enabled IoV, such as smart driving assistance, with its outstanding features including enhancing quality and quantity of training data, speeding up learning processes and reducing computing demands. Although TL had been widely adopted in wireless applications (e.g., spectrum management and caching), its reliability and security in 6G-enabled IoV were still not well investigated. For instance, malicious vehicles in source domains may transfer and share untrustworthy models (i.e., knowledge) about connection availability to target domains, thus adversely affecting the performance of learning processes. Therefore, it is important to select and also incentivize trustworthy vehicles to participate in TL. In this article, we first introduce the integration of TL and 6G-enbaled IoV and provide TL applications for 6G-enabled IoV. We then design a secure and reliable transfer learning framework by using reputation to evaluate the reliability of pre-trained models and utilizing the consortium blockchain to achieve secure and efficient decentralized reputation management. Moreover, a deep learning-based auction scheme for the TL model market is designed to motivate high-reputation vehicles to participate in model sharing. Finally, the simulation results demonstrate that the proposed framework is secure and reliable with well-design incentives for TL in 6G-enabled IoV. △ Less","10 November, 2021",https://arxiv.org/pdf/2111.05804
PIMIP: An Open Source Platform for Pathology Information Management and Integration,Jialun Wu;Anyu Mao;Xinrui Bao;Haichuan Zhang;Zeyu Gao;Chunbao Wang;Tieliang Gong;Chen Li,"Digital pathology plays a crucial role in the development of artificial intelligence in the medical field. The digital pathology platform can make the pathological resources digital and networked, and realize the permanent storage of visual data and the synchronous browsing processing without the limitation of time and space. It has been widely used in various fields of pathology. However, there is still a lack of an open and universal digital pathology platform to assist doctors in the management and analysis of digital pathological sections, as well as the management and structured description of relevant patient information. Most platforms cannot integrate image viewing, annotation and analysis, and text information management. To solve the above problems, we propose a comprehensive and extensible platform PIMIP. Our PIMIP has developed the image annotation functions based on the visualization of digital pathological sections. Our annotation functions support multi-user collaborative annotation and multi-device annotation, and realize the automation of some annotation tasks. In the annotation task, we invited a professional pathologist for guidance. We introduce a machine learning module for image analysis. The data we collected included public data from local hospitals and clinical examples. Our platform is more clinical and suitable for clinical use. In addition to image data, we also structured the management and display of text information. So our platform is comprehensive. The platform framework is built in a modular way to support users to add machine learning modules independently, which makes our platform extensible. △ Less","9 November, 2021",https://arxiv.org/pdf/2111.05794
Statistical Perspectives on Reliability of Artificial Intelligence Systems,Yili Hong;Jiayi Lian;Li Xu;Jie Min;Yueyao Wang;Laura J. Freeman;Xinwei Deng,"Artificial intelligence (AI) systems have become increasingly popular in many areas. Nevertheless, AI technologies are still in their developing stages, and many issues need to be addressed. Among those, the reliability of AI systems needs to be demonstrated so that the AI systems can be used with confidence by the general public. In this paper, we provide statistical perspectives on the reliability of AI systems. Different from other considerations, the reliability of AI systems focuses on the time dimension. That is, the system can perform its designed functionality for the intended period. We introduce a so-called SMART statistical framework for AI reliability research, which includes five components: Structure of the system, Metrics of reliability, Analysis of failure causes, Reliability assessment, and Test planning. We review traditional methods in reliability data analysis and software reliability, and discuss how those existing methods can be transformed for reliability modeling and assessment of AI systems. We also describe recent developments in modeling and analysis of AI reliability and outline statistical research challenges in this area, including out-of-distribution detection, the effect of the training set, adversarial attacks, model accuracy, and uncertainty quantification, and discuss how those topics can be related to AI reliability, with illustrative examples. Finally, we discuss data collection and test planning for AI reliability assessment and how to improve system designs for higher AI reliability. The paper closes with some concluding remarks. △ Less","9 November, 2021",https://arxiv.org/pdf/2111.05391
"Towards Tractable Mathematical Reasoning: Challenges, Strategies, and Opportunities for Solving Math Word Problems",Keyur Faldu;Amit Sheth;Prashant Kikani;Manas Gaur;Aditi Avasthi,"Mathematical reasoning would be one of the next frontiers for artificial intelligence to make significant progress. The ongoing surge to solve math word problems (MWPs) and hence achieve better mathematical reasoning ability would continue to be a key line of research in the coming time. We inspect non-neural and neural methods to solve math word problems narrated in a natural language. We also highlight the ability of these methods to be generalizable, mathematically reasonable, interpretable, and explainable. Neural approaches dominate the current state of the art, and we survey them highlighting three strategies to MWP solving: (1) direct answer generation, (2) expression tree generation for inferring answers, and (3) template retrieval for answer computation. Moreover, we discuss technological approaches, review the evolution of intuitive design choices to solve MWPs, and examine them for mathematical reasoning ability. We finally identify several gaps that warrant the need for external knowledge and knowledge-infused learning, among several other opportunities in solving MWPs. △ Less","29 October, 2021",https://arxiv.org/pdf/2111.05364
Stain-free Detection of Embryo Polarization using Deep Learning,Cheng Shen;Adiyant Lamba;Meng Zhu;Ray Zhang;Changhuei Yang;Magdalena Zernicka Goetz,"Polarization of the mammalian embryo at the right developmental time is critical for its development to term and would be valuable in assessing the potential of human embryos. However, tracking polarization requires invasive fluorescence staining, impermissible in the in vitro fertilization clinic. Here, we report the use of artificial intelligence to detect polarization from unstained time-lapse movies of mouse embryos. We assembled a dataset of bright-field movie frames from 8-cell-stage embryos, side-by-side with corresponding images of fluorescent markers of cell polarization. We then used an ensemble learning model to detect whether any bright-field frame showed an embryo before or after onset of polarization. Our resulting model has an accuracy of 85% for detecting polarization, significantly outperforming human volunteers trained on the same data (61% accuracy). We discovered that our self-learning model focuses upon the angle between cells as one known cue for compaction, which precedes polarization, but it outperforms the use of this cue alone. By compressing three-dimensional time-lapsed image data into two-dimensions, we are able to reduce data to an easily manageable size for deep learning processing. In conclusion, we describe a method for detecting a key developmental feature of embryo development that avoids clinically impermissible fluorescence staining. △ Less","8 November, 2021",https://arxiv.org/pdf/2111.05315
Ethically aligned Deep Learning: Unbiased Facial Aesthetic Prediction,Michael Danner;Thomas Weber;Leping Peng;Tobias Gerlach;Xueping Su;Matthias Rätsch,"Facial beauty prediction (FBP) aims to develop a machine that automatically makes facial attractiveness assessment. In the past those results were highly correlated with human ratings, therefore also with their bias in annotating. As artificial intelligence can have racist and discriminatory tendencies, the cause of skews in the data must be identified. Development of training data and AI algorithms that are robust against biased information is a new challenge for scientists. As aesthetic judgement usually is biased, we want to take it one step further and propose an Unbiased Convolutional Neural Network for FBP. While it is possible to create network models that can rate attractiveness of faces on a high level, from an ethical point of view, it is equally important to make sure the model is unbiased. In this work, we introduce AestheticNet, a state-of-the-art attractiveness prediction network, which significantly outperforms competitors with a Pearson Correlation of 0.9601. Additionally, we propose a new approach for generating a bias-free CNN to improve fairness in machine learning. △ Less","9 November, 2021",https://arxiv.org/pdf/2111.05149
Tackling Morphological Analogies Using Deep Learning -- Extended Version,Safa Alsaidi;Amandine Decker;Esteban Marquer;Pierre-Alexandre Murena;Miguel Couceiro,"Analogical proportions are statements of the form ""A is to B as C is to D"". They constitute an inference tool that provides a logical framework to address learning, transfer, and explainability concerns and that finds useful applications in artificial intelligence and natural language processing. In this paper, we address two problems, namely, analogy detection and resolution in morphology. Multiple symbolic approaches tackle the problem of analogies in morphology and achieve competitive performance. We show that it is possible to use a data-driven strategy to outperform those models. We propose an approach using deep learning to detect and solve morphological analogies. It encodes structural properties of analogical proportions and relies on a specifically designed embedding model capturing morphological characteristics of words. We demonstrate our model's competitive performance on analogy detection and resolution over multiple languages. We provide an empirical study to analyze the impact of balancing training data and evaluate the robustness of our approach to input perturbation. △ Less","9 November, 2021",https://arxiv.org/pdf/2111.05147
Conformity Assessments and Post-market Monitoring: A Guide to the Role of Auditing in the Proposed European AI Regulation,Jakob Mokander;Maria Axente;Federico Casolari;Luciano Floridi,"The proposed European Artificial Intelligence Act (AIA) is the first attempt to elaborate a general legal framework for AI carried out by any major global economy. As such, the AIA is likely to become a point of reference in the larger discourse on how AI systems can (and should) be regulated. In this article, we describe and discuss the two primary enforcement mechanisms proposed in the AIA: the conformity assessments that providers of high-risk AI systems are expected to conduct, and the post-market monitoring plans that providers must establish to document the performance of high-risk AI systems throughout their lifetimes. We argue that AIA can be interpreted as a proposal to establish a Europe-wide ecosystem for conducting AI auditing, albeit in other words. Our analysis offers two main contributions. First, by describing the enforcement mechanisms included in the AIA in terminology borrowed from existing literature on AI auditing, we help providers of AI systems understand how they can prove adherence to the requirements set out in the AIA in practice. Second, by examining the AIA from an auditing perspective, we seek to provide transferable lessons from previous research about how to refine further the regulatory approach outlined in the AIA. We conclude by highlighting seven aspects of the AIA where amendments (or simply clarifications) would be helpful. These include, above all, the need to translate vague concepts into verifiable criteria and to strengthen the institutional safeguards concerning conformity assessments based on internal checks. △ Less","9 November, 2021",https://arxiv.org/pdf/2111.05071
An Interactive Visualization Tool for Understanding Active Learning,Zihan Wang;Jialin Lu;Oliver Snow;Martin Ester,"Despite recent progress in artificial intelligence and machine learning, many state-of-the-art methods suffer from a lack of explainability and transparency. The ability to interpret the predictions made by machine learning models and accurately evaluate these models is crucially important. In this paper, we present an interactive visualization tool to elucidate the training process of active learning. This tool enables one to select a sample of interesting data points, view how their prediction values change at different querying stages, and thus better understand when and how active learning works. Additionally, users can utilize this tool to compare different active learning strategies simultaneously and inspect why some strategies outperform others in certain contexts. With some preliminary experiments, we demonstrate that our visualization panel has a great potential to be used in various active learning experiments and help users evaluate their models appropriately. △ Less","8 November, 2021",https://arxiv.org/pdf/2111.04936
Building an AI-ready RSE Workforce,Ying Zhang;Matthew A. Gitzendanner;Dan S. Maxwell;Justin W. Richardson;Kaleb E. Smith;Eric A. Stubbs;Brian J. Stucky;Jingchao Zhang;Erik Deumens,"Artificial Intelligence has been transforming industries and academic research across the globe, and research software development is no exception. Machine learning and deep learning are being applied in every aspect of the research software development lifecycles, from new algorithm design paradigms to software development processes. In this paper, we discuss our views on today's challenges and opportunities that AI has presented on research software development and engineers, and the approaches we, at the University of Florida, are taking to prepare our workforce for the new era of AI. △ Less","8 November, 2021",https://arxiv.org/pdf/2111.04916
Improved security solutions for DDoS mitigation in 5G Multi-access Edge Computing,Marian Gusatu;Ruxandra F. Olimid,"Multi-access Edge Computing (MEC) is a 5G-enabling solution that aims to bring cloud-computing capabilities closer to the end-users. This paper focuses on mitigation techniques against Distributed Denial-of-Service (DDoS) attacks in the context of 5G MEC, providing solutions that involve the virtualized environment and the management entities from the MEC architecture. The proposed solutions aim to reduce the risk of affecting legitimate traffic in the context of DDoS attacks. Our work supports the idea of using a network flow collector that sends the data to an anomaly detection system based on artificial intelligence techniques and, as an improvement over the previous work, it contributes to redirecting detected anomalies for isolation to a separate virtual machine. This virtual machine uses deep packet inspection tools to analyze the traffic and provides services until the final verdict. We decrease the risk of compromising the virtual machine that provides services to legitimate users by isolating the suspicious traffic. The management entities of the MEC architecture allow to re-instantiate or reconfigure the virtual machines. Hence, if the machine inspecting the isolated traffic crashes because of an attack, the damaged machine can be restored while the services provided to legitimate users are not affected. △ Less","10 November, 2021",https://arxiv.org/pdf/2111.04801
BRACS: A Dataset for BReAst Carcinoma Subtyping in H&E Histology Images,Nadia Brancati;Anna Maria Anniciello;Pushpak Pati;Daniel Riccio;Giosuè Scognamiglio;Guillaume Jaume;Giuseppe De Pietro;Maurizio Di Bonito;Antonio Foncubierta;Gerardo Botti;Maria Gabrani;Florinda Feroce;Maria Frucci,"Breast cancer is the most commonly diagnosed cancer and registers the highest number of deaths for women with cancer. Recent advancements in diagnostic activities combined with large-scale screening policies have significantly lowered the mortality rates for breast cancer patients. However, the manual inspection of tissue slides by the pathologists is cumbersome, time-consuming, and is subject to significant inter- and intra-observer variability. Recently, the advent of whole-slide scanning systems have empowered the rapid digitization of pathology slides, and enabled to develop digital workflows. These advances further enable to leverage Artificial Intelligence (AI) to assist, automate, and augment pathological diagnosis. But the AI techniques, especially Deep Learning (DL), require a large amount of high-quality annotated data to learn from. Constructing such task-specific datasets poses several challenges, such as, data-acquisition level constrains, time-consuming and expensive annotations, and anonymization of private information. In this paper, we introduce the BReAst Carcinoma Subtyping (BRACS) dataset, a large cohort of annotated Hematoxylin & Eosin (H&E)-stained images to facilitate the characterization of breast lesions. BRACS contains 547 Whole-Slide Images (WSIs), and 4539 Regions of Interest (ROIs) extracted from the WSIs. Each WSI, and respective ROIs, are annotated by the consensus of three board-certified pathologists into different lesion categories. Specifically, BRACS includes three lesion types, i.e., benign, malignant and atypical, which are further subtyped into seven categories. It is, to the best of our knowledge, the largest annotated dataset for breast cancer subtyping both at WSI- and ROI-level. Further, by including the understudied atypical lesions, BRACS offers an unique opportunity for leveraging AI to better understand their characteristics. △ Less","8 November, 2021",https://arxiv.org/pdf/2111.04740
Flight Demand Forecasting with Transformers,Liya Wang;Amy Mykityshyn;Craig Johnson;Jillian Cheng,"Transformers have become the de-facto standard in the natural language processing (NLP) field. They have also gained momentum in computer vision and other domains. Transformers can enable artificial intelligence (AI) models to dynamically focus on certain parts of their input and thus reason more effectively. Inspired by the success of transformers, we adopted this technique to predict strategic flight departure demand in multiple horizons. This work was conducted in support of a MITRE-developed mobile application, Pacer, which displays predicted departure demand to general aviation (GA) flight operators so they can have better situation awareness of the potential for departure delays during busy periods. Field demonstrations involving Pacer's previously designed rule-based prediction method showed that the prediction accuracy of departure demand still has room for improvement. This research strives to improve prediction accuracy from two key aspects: better data sources and robust forecasting algorithms. We leveraged two data sources, Aviation System Performance Metrics (ASPM) and System Wide Information Management (SWIM), as our input. We then trained forecasting models with temporal fusion transformer (TFT) for five different airports. Case studies show that TFTs can perform better than traditional forecasting methods by large margins, and they can result in better prediction across diverse airports and with better interpretability. △ Less","4 November, 2021",https://arxiv.org/pdf/2111.04471
Systematic Review for AI-based Language Learning Tools,Jin Ha Woo;Heeyoul Choi,"The Second Language Acquisition field has been significantly impacted by a greater emphasis on individualized learning and rapid developments in artificial intelligence (AI). Although increasingly adaptive language learning tools are being developed with the application of AI to the Computer Assisted Language Learning field, there have been concerns regarding insufficient information and teacher preparation. To effectively utilize these tools, teachers need an in-depth overview on recently developed AI-based language learning tools. Therefore, this review synthesized information on AI tools that were developed between 2017 and 2020. A majority of these tools utilized machine learning and natural language processing, and were used to identify errors, provide feedback, and assess language abilities. After using these tools, learners demonstrated gains in their language abilities and knowledge. This review concludes by presenting pedagogical implications and emerging themes in the future research of AI-based language learning tools. △ Less","29 October, 2021",https://arxiv.org/pdf/2111.04455
AI Federalism: Shaping AI Policy within States in Germany,Anna Jobin;Licinia Guettel;Laura Liebig;Christian Katzenbach,"Recent AI governance research has focused heavily on the analysis of strategy papers and ethics guidelines for AI published by national governments and international bodies. Meanwhile, subnational institutions have also published documents on Artificial Intelligence, yet these have been largely absent from policy analyses. This is surprising because AI is connected to many policy areas, such as economic or research policy, where the competences are already distributed between the national and subnational level. To better understand the current dynamics of AI governance, it is essential to consider the context of policy making beyond the federal government. Although AI may be considered a new policy field, it is created, contested and ultimately shaped within existing political structures and dynamics. We therefore argue that more attention should be dedicated to subnational efforts to shape AI and present initial findings from our case study of Germany. Analyzing AI as a policy field on different levels of government will contribute to a better understanding of the developments and implementations of AI strategies in different national contexts. △ Less","28 October, 2021",https://arxiv.org/pdf/2111.04454
Artistic Autonomy in AI Art,Alayt Issak,"The concept of art has transposed meaning and medium across time, with its context being a deciding factor for its evolution. However, human beings' innermost functionality remains the same, and art, to this day, serves as an expression of the subconscious. Accelerated by the conception of GANs in 2014, automation has become a central medium in Artificial Intelligence (AI) Art. However, this raises concern over AI's influence on artistic autonomy within the process of creativity. This paper introduces the ethical responsibility of AI towards maintaining the artist's volition in exercising autonomy and utilizes principles of self-determination theory alongside fundamental limits of creativity to do so. △ Less","22 October, 2021",https://arxiv.org/pdf/2111.04437
Dense Representative Tooth Landmark/axis Detection Network on 3D Model,Guangshun Wei;Zhiming Cui;Jie Zhu;Lei Yang;Yuanfeng Zhou;Pradeep Singh;Min Gu;Wenping Wang,"Artificial intelligence (AI) technology is increasingly used for digital orthodontics, but one of the challenges is to automatically and accurately detect tooth landmarks and axes. This is partly because of sophisticated geometric definitions of them, and partly due to large variations among individual tooth and across different types of tooth. As such, we propose a deep learning approach with a labeled dataset by professional dentists to the tooth landmark/axis detection on tooth model that are crucial for orthodontic treatments. Our method can extract not only tooth landmarks in the form of point (e.g. cusps), but also axes that measure the tooth angulation and inclination. The proposed network takes as input a 3D tooth model and predicts various types of the tooth landmarks and axes. Specifically, we encode the landmarks and axes as dense fields defined on the surface of the tooth model. This design choice and a set of added components make the proposed network more suitable for extracting sparse landmarks from a given 3D tooth model. Extensive evaluation of the proposed method was conducted on a set of dental models prepared by experienced dentists. Results show that our method can produce tooth landmarks with high accuracy. Our method was examined and justified via comparison with the state-of-the-art methods as well as the ablation studies. △ Less","8 November, 2021",https://arxiv.org/pdf/2111.04212
AI challenges for predicting the impact of mutations on protein stability,Fabrizio Pucci;Martin Schwersensky;Marianne Rooman,"Stability is a key ingredient of protein fitness and its modification through targeted mutations has applications in various fields such as protein engineering, drug design and deleterious variant interpretation. Many studies have been devoted over the past decades to building new, more effective methods for predicting the impact of mutations on protein stability, based on the latest developments in artificial intelligence (AI). We discuss their features, algorithms, computational efficiency, and accuracy estimated on an independent test set. We focus on a critical analysis of their limitations, the recurrent biases towards the training set, their generalizability and interpretability. We found that the accuracy of the predictors has stagnated at around 1 kcal/mol for over 15 years. We conclude by discussing the challenges that need to be addressed to reach improved performance. △ Less","7 November, 2021",https://arxiv.org/pdf/2111.04208
On the Limits of Design: What Are the Conceptual Constraints on Designing Artificial Intelligence for Social Good?,Jakob Mokander,"Artificial intelligence AI can bring substantial benefits to society by helping to reduce costs, increase efficiency and enable new solutions to complex problems. Using Floridi's notion of how to design the 'infosphere' as a starting point, in this chapter I consider the question: what are the limits of design, i.e. what are the conceptual constraints on designing AI for social good? The main argument of this chapter is that while design is a useful conceptual tool to shape technologies and societies, collective efforts towards designing future societies are constrained by both internal and external factors. Internal constraints on design are discussed by evoking Hardin's thought experiment regarding 'the Tragedy of the Commons'. Further, Hayek's classical distinction between 'cosmos' and 'taxis' is used to demarcate external constraints on design. Finally, five design principles are presented which are aimed at helping policymakers manage the internal and external constraints on design. A successful approach to designing future societies needs to account for the emergent properties of complex systems by allowing space for serendipity and socio-technological coevolution. △ Less","7 November, 2021",https://arxiv.org/pdf/2111.04165
Global-Local Attention for Emotion Recognition,Nhat Le;Khanh Nguyen;Anh Nguyen;Bac Le,"Human emotion recognition is an active research area in artificial intelligence and has made substantial progress over the past few years. Many recent works mainly focus on facial regions to infer human affection, while the surrounding context information is not effectively utilized. In this paper, we proposed a new deep network to effectively recognize human emotions using a novel global-local attention mechanism. Our network is designed to extract features from both facial and context regions independently, then learn them together using the attention module. In this way, both the facial and contextual information is used to infer human emotions, therefore enhancing the discrimination of the classifier. The intensive experiments show that our method surpasses the current state-of-the-art methods on recent emotion datasets by a fair margin. Qualitatively, our global-local attention module can extract more meaningful attention maps than previous methods. The source code and trained model of our network are available at https://github.com/minhnhatvt/glamor-net △ Less","7 November, 2021",https://arxiv.org/pdf/2111.04129
Modelling and Optimisation of Resource Usage in an IoT Enabled Smart Campus,Thanchanok Sutjarittham,"University campuses are essentially a microcosm of a city. They comprise diverse facilities such as residences, sport centres, lecture theatres, parking spaces, and public transport stops. Universities are under constant pressure to improve efficiencies while offering a better experience to various stakeholders including students, staff, and visitors. Nonetheless, anecdotal evidence indicates that campus assets are not being utilised efficiently, often due to the lack of data collection and analysis, thereby limiting the ability to make informed decisions on the allocation and management of resources. Advances in the Internet of Things (IoT) technologies that can sense and communicate data from the physical world, coupled with data analytics and Artificial intelligence (AI) that can predict usage patterns, have opened up new opportunities for organisations to lower cost and improve user experience. This thesis explores this opportunity via theory and experimentation using UNSW Sydney as a living laboratory. △ Less","7 November, 2021",https://arxiv.org/pdf/2111.04085
Proposing an Interactive Audit Pipeline for Visual Privacy Research,Jasmine DeHart;Chenguang Xu;Lisa Egede;Christan Grant,"In an ideal world, deployed machine learning models will enhance our society. We hope that those models will provide unbiased and ethical decisions that will benefit everyone. However, this is not always the case; issues arise during the data preparation process throughout the steps leading to the models' deployment. The continued use of biased datasets and processes will adversely damage communities and increase the cost of fixing the problem later. In this work, we walk through the decision-making process that a researcher should consider before, during, and after a system deployment to understand the broader impacts of their research in the community. Throughout this paper, we discuss fairness, privacy, and ownership issues in the machine learning pipeline; we assert the need for a responsible human-over-the-loop methodology to bring accountability into the machine learning pipeline, and finally, reflect on the need to explore research agendas that have harmful societal impacts. We examine visual privacy research and draw lessons that can apply broadly to artificial intelligence. Our goal is to systematically analyze the machine learning pipeline for visual privacy and bias issues. We hope to raise stakeholder (e.g., researchers, modelers, corporations) awareness as these issues propagate in this pipeline's various machine learning phases. △ Less","23 November, 2021",https://arxiv.org/pdf/2111.03984
Disengagement Cause-and-Effect Relationships Extraction Using an NLP Pipeline,Yangtao Zhang;X. Jessie Yang;Feng Zhou,"The advancement in machine learning and artificial intelligence is promoting the testing and deployment of autonomous vehicles (AVs) on public roads. The California Department of Motor Vehicles (CA DMV) has launched the Autonomous Vehicle Tester Program, which collects and releases reports related to Autonomous Vehicle Disengagement (AVD) from autonomous driving. Understanding the causes of AVD is critical to improving the safety and stability of the AV system and provide guidance for AV testing and deployment. In this work, a scalable end-to-end pipeline is constructed to collect, process, model, and analyze the disengagement reports released from 2014 to 2020 using natural language processing deep transfer learning. The analysis of disengagement data using taxonomy, visualization and statistical tests revealed the trends of AV testing, categorized cause frequency, and significant relationships between causes and effects of AVD. We found that (1) manufacturers tested AVs intensively during the Spring and/or Winter, (2) test drivers initiated more than 80% of the disengagement while more than 75% of the disengagement were led by errors in perception, localization & mapping, planning and control of the AV system itself, and (3) there was a significant relationship between the initiator of AVD and the cause category. This study serves as a successful practice of deep transfer learning using pre-trained models and generates a consolidated disengagement database allowing further investigation for other researchers. △ Less","5 November, 2021",https://arxiv.org/pdf/2111.03511
Causal versus Marginal Shapley Values for Robotic Lever Manipulation Controlled using Deep Reinforcement Learning,Sindre Benjamin Remman;Inga Strümke;Anastasios M. Lekkas,"We investigate the effect of including domain knowledge about a robotic system's causal relations when generating explanations. To this end, we compare two methods from explainable artificial intelligence, the popular KernelSHAP and the recent causal SHAP, on a deep neural network trained using deep reinforcement learning on the task of controlling a lever using a robotic manipulator. A primary disadvantage of KernelSHAP is that its explanations represent only the features' direct effects on a model's output, not considering the indirect effects a feature can have on the output by affecting other features. Causal SHAP uses a partial causal ordering to alter KernelSHAP's sampling procedure to incorporate these indirect effects. This partial causal ordering defines the causal relations between the features, and we specify this using domain knowledge about the lever control task. We show that enabling an explanation method to account for indirect effects and incorporating some domain knowledge can lead to explanations that better agree with human intuition. This is especially favorable for a real-world robotics task, where there is considerable causality at play, and in addition, the required domain knowledge is often handily available. △ Less","4 November, 2021",https://arxiv.org/pdf/2111.02936
Attacking Deep Reinforcement Learning-Based Traffic Signal Control Systems with Colluding Vehicles,Ao Qu;Yihong Tang;Wei Ma,"The rapid advancements of Internet of Things (IoT) and artificial intelligence (AI) have catalyzed the development of adaptive traffic signal control systems (ATCS) for smart cities. In particular, deep reinforcement learning (DRL) methods produce the state-of-the-art performance and have great potentials for practical applications. In the existing DRL-based ATCS, the controlled signals collect traffic state information from nearby vehicles, and then optimal actions (e.g., switching phases) can be determined based on the collected information. The DRL models fully ""trust"" that vehicles are sending the true information to the signals, making the ATCS vulnerable to adversarial attacks with falsified information. In view of this, this paper first time formulates a novel task in which a group of vehicles can cooperatively send falsified information to ""cheat"" DRL-based ATCS in order to save their total travel time. To solve the proposed task, we develop CollusionVeh, a generic and effective vehicle-colluding framework composed of a road situation encoder, a vehicle interpreter, and a communication mechanism. We employ our method to attack established DRL-based ATCS and demonstrate that the total travel time for the colluding vehicles can be significantly reduced with a reasonable number of learning episodes, and the colluding effect will decrease if the number of colluding vehicles increases. Additionally, insights and suggestions for the real-world deployment of DRL-based ATCS are provided. The research outcomes could help improve the reliability and robustness of the ATCS and better protect the smart mobility systems. △ Less","4 November, 2021",https://arxiv.org/pdf/2111.02845
MUVINE: Multi-stage Virtual Network Embedding in Cloud Data Centers using Reinforcement Learning based Predictions,Hiren Kumar Thakkar;Chinmaya Kumar Dehury;Prasan Kumar Sahoo,"The recent advances in virtualization technology have enabled the sharing of computing and networking resources of cloud data centers among multiple users. Virtual Network Embedding (VNE) is highly important and is an integral part of the cloud resource management. The lack of historical knowledge on cloud functioning and inability to foresee the future resource demand are two fundamental shortcomings of the traditional VNE approaches. The consequence of those shortcomings is the inefficient embedding of virtual resources on Substrate Nodes (SNs). On the contrary, application of Artificial Intelligence (AI) in VNE is still in the premature stage and needs further investigation. Considering the underlying complexity of VNE that includes numerous parameters, intelligent solutions are required to utilize the cloud resources efficiently via careful selection of appropriate SNs for the VNE. In this paper, Reinforcement Learning based prediction model is designed for the efficient Multi-stage Virtual Network Embedding (MUVINE) among the cloud data centers. The proposed MUVINE scheme is extensively simulated and evaluated against the recent state-of-the-art schemes. The simulation outcomes show that the proposed MUVINE scheme consistently outperforms over the existing schemes and provides the promising results. △ Less","4 November, 2021",https://arxiv.org/pdf/2111.02737
Transparency of Deep Neural Networks for Medical Image Analysis: A Review of Interpretability Methods,Zohaib Salahuddin;Henry C Woodruff;Avishek Chatterjee;Philippe Lambin,"Artificial Intelligence has emerged as a useful aid in numerous clinical applications for diagnosis and treatment decisions. Deep neural networks have shown same or better performance than clinicians in many tasks owing to the rapid increase in the available data and computational power. In order to conform to the principles of trustworthy AI, it is essential that the AI system be transparent, robust, fair and ensure accountability. Current deep neural solutions are referred to as black-boxes due to a lack of understanding of the specifics concerning the decision making process. Therefore, there is a need to ensure interpretability of deep neural networks before they can be incorporated in the routine clinical workflow. In this narrative review, we utilized systematic keyword searches and domain expertise to identify nine different types of interpretability methods that have been used for understanding deep learning models for medical image analysis applications based on the type of generated explanations and technical similarities. Furthermore, we report the progress made towards evaluating the explanations produced by various interpretability methods. Finally we discuss limitations, provide guidelines for using interpretability methods and future directions concerning the interpretability of deep neural networks for medical imaging analysis. △ Less","31 October, 2021",https://arxiv.org/pdf/2111.02398
Exploring Explainable AI in the Financial Sector: Perspectives of Banks and Supervisory Authorities,Ouren Kuiper;Martin van den Berg;Joost van der Burgt;Stefan Leijnen,"Explainable artificial intelligence (xAI) is seen as a solution to making AI systems less of a black box. It is essential to ensure transparency, fairness, and accountability, which are especially paramount in the financial sector. The aim of this study was a preliminary investigation of the perspectives of supervisory authorities and regulated entities regarding the application of xAI in the fi-nancial sector. Three use cases (consumer credit, credit risk, and anti-money laundering) were examined using semi-structured interviews at three banks and two supervisory authorities in the Netherlands. We found that for the investigated use cases a disparity exists between supervisory authorities and banks regarding the desired scope of explainability of AI systems. We argue that the financial sector could benefit from clear differentiation between technical AI (model) ex-plainability requirements and explainability requirements of the broader AI system in relation to applicable laws and regulations. △ Less","3 November, 2021",https://arxiv.org/pdf/2111.02244
The Powerful Use of AI in the Energy Sector: Intelligent Forecasting,Erik Blasch;Haoran Li;Zhihao Ma;Yang Weng,"Artificial Intelligence (AI) techniques continue to broaden across governmental and public sectors, such as power and energy - which serve as critical infrastructures for most societal operations. However, due to the requirements of reliability, accountability, and explainability, it is risky to directly apply AI-based methods to power systems because society cannot afford cascading failures and large-scale blackouts, which easily cost billions of dollars. To meet society requirements, this paper proposes a methodology to develop, deploy, and evaluate AI systems in the energy sector by: (1) understanding the power system measurements with physics, (2) designing AI algorithms to forecast the need, (3) developing robust and accountable AI methods, and (4) creating reliable measures to evaluate the performance of the AI model. The goal is to provide a high level of confidence to energy utility users. For illustration purposes, the paper uses power system event forecasting (PEF) as an example, which carefully analyzes synchrophasor patterns measured by the Phasor Measurement Units (PMUs). Such a physical understanding leads to a data-driven framework that reduces the dimensionality with physics and forecasts the event with high credibility. Specifically, for dimensionality reduction, machine learning arranges physical information from different dimensions, resulting inefficient information extraction. For event forecasting, the supervised learning model fuses the results of different models to increase the confidence. Finally, comprehensive experiments demonstrate the high accuracy, efficiency, and reliability as compared to other state-of-the-art machine learning methods. △ Less","3 November, 2021",https://arxiv.org/pdf/2111.02026
Certifiable Artificial Intelligence Through Data Fusion,Erik Blasch;Junchi Bin;Zheng Liu,"This paper reviews and proposes concerns in adopting, fielding, and maintaining artificial intelligence (AI) systems. While the AI community has made rapid progress, there are challenges in certifying AI systems. Using procedures from design and operational test and evaluation, there are opportunities towards determining performance bounds to manage expectations of intended use. A notional use case is presented with image data fusion to support AI object recognition certifiability considering precision versus distance. △ Less","2 November, 2021",https://arxiv.org/pdf/2111.02001
A new method for binary classification of proteins with Machine Learning,Damiano Perri;Marco Simonetti;Andrea Lombardi;Noelia Faginas-Lago;Osvaldo Gervasi,"In this work we set out to find a method to classify protein structures using a Deep Learning methodology. Our Artificial Intelligence has been trained to recognize complex biomolecule structures extrapolated from the Protein Data Bank (PDB) database and reprocessed as images; for this purpose various tests have been conducted with pre-trained Convolutional Neural Networks, such as InceptionResNetV2 or InceptionV3, in order to extract significant features from these images and correctly classify the molecule. A comparative analysis of the performances of the various networks will therefore be produced. △ Less","2 November, 2021",https://arxiv.org/pdf/2111.01976
Predicting Cancer Using Supervised Machine Learning: Mesothelioma,Avishek Choudhury,"Background: Pleural Mesothelioma (PM) is an unusual, belligerent tumor that rapidly develops into cancer in the pleura of the lungs. Pleural Mesothelioma is a common type of Mesothelioma that accounts for about 75% of all Mesothelioma diagnosed yearly in the U.S. Diagnosis of Mesothelioma takes several months and is expensive. Given the risk and constraints associated with PM diagnosis, early identification of this ailment is essential for patient health. Objective: In this study, we use artificial intelligence algorithms recommending the best fit model for early diagnosis and prognosis of MPM. Methods: We retrospectively retrieved patients clinical data collected by Dicle University, Turkey, and applied multilayered perceptron (MLP), voted perceptron (VP), Clojure classifier (CC), kernel logistic regression (KLR), stochastic gradient decent SGD), adaptive boosting (AdaBoost), Hoeffding tree (VFDT), and primal estimated sub-gradient solver for support vector machine (s-Pegasos). We evaluated the models, compared and tested using paired T-test (corrected) at 0.05 significance based on their respective classification accuracy, f-measure, precision, recall, root mean squared error, receivers characteristic curve (ROC), and precision-recall curve (PRC). Results: In phase-1, SGD, AdaBoost. M1, KLR, MLP, VFDT generate optimal results with the highest possible performance measures. In phase 2, AdaBoost, with a classification accuracy of 71.29%, outperformed all other algorithms. C-reactive protein, platelet count, duration of symptoms, gender, and pleural protein were found to be the most relevant predictors that can prognosticate Mesothelioma. Conclusion: This study confirms that data obtained from Biopsy and imagining tests are strong predictors of Mesothelioma but are associated with a high cost; however, they can identify Mesothelioma with optimal accuracy. △ Less","31 October, 2021",https://arxiv.org/pdf/2111.01912
LogLAB: Attention-Based Labeling of Log Data Anomalies via Weak Supervision,Thorsten Wittkopp;Philipp Wiesner;Dominik Scheinert;Alexander Acker,"With increasing scale and complexity of cloud operations, automated detection of anomalies in monitoring data such as logs will be an essential part of managing future IT infrastructures. However, many methods based on artificial intelligence, such as supervised deep learning models, require large amounts of labeled training data to perform well. In practice, this data is rarely available because labeling log data is expensive, time-consuming, and requires a deep understanding of the underlying system. We present LogLAB, a novel modeling approach for automated labeling of log messages without requiring manual work by experts. Our method relies on estimated failure time windows provided by monitoring systems to produce precise labeled datasets in retrospect. It is based on the attention mechanism and uses a custom objective function for weak supervision deep learning techniques that accounts for imbalanced data. Our evaluation shows that LogLAB consistently outperforms nine benchmark approaches across three different datasets and maintains an F1-score of more than 0.98 even at large failure time windows. △ Less","25 November, 2021",https://arxiv.org/pdf/2111.01657
A Review of Dialogue Systems: From Trained Monkeys to Stochastic Parrots,Atharv Singh Patlan;Shiven Tripathi;Shubham Korde,"In spoken dialogue systems, we aim to deploy artificial intelligence to build automated dialogue agents that can converse with humans. Dialogue systems are increasingly being designed to move beyond just imitating conversation and also improve from such interactions over time. In this survey, we present a broad overview of methods developed to build dialogue systems over the years. Different use cases for dialogue systems ranging from task-based systems to open domain chatbots motivate and necessitate specific systems. Starting from simple rule-based systems, research has progressed towards increasingly complex architectures trained on a massive corpus of datasets, like deep learning systems. Motivated with the intuition of resembling human dialogues, progress has been made towards incorporating emotions into the natural language generator, using reinforcement learning. While we see a trend of highly marginal improvement on some metrics, we find that limited justification exists for the metrics, and evaluation practices are not uniform. To conclude, we flag these concerns and highlight possible research directions. △ Less","2 November, 2021",https://arxiv.org/pdf/2111.01414
Cognitive Load and Productivity Implications in Human-Chatbot Interaction,Johanna Schmidhuber;Stephan Schlögl;Christian Ploder,"The increasing progress in artificial intelligence and respective machine learning technology has fostered the proliferation of chatbots to the point where today they are being embedded into various human-technology interaction tasks. In enterprise contexts, the use of chatbots seeks to reduce labor costs and consequently increase productivity. For simple, repetitive customer service tasks such already proves beneficial, yet more complex collaborative knowledge work seems to require a better understanding of how the technology may best be integrated. Particularly, the additional mental burden which accompanies the use of these natural language based artificial assistants, often remains overlooked. To this end, cognitive load theory implies that unnecessary use of technology can induce additional extrinsic load and thus may have a contrary effect on users' productivity. The research presented in this paper thus reports on a study assessing cognitive load and productivity implications of human chatbot interaction in a realistic enterprise setting. A/B testing software-only vs. software + chatbot interaction, and the NASA TLX were used to evaluate and compare the cognitive load of two user groups. Results show that chatbot users experienced less cognitive load and were more productive than software-only users. Furthermore, they show lower frustration levels and better overall performance (i.e, task quality) despite their slightly longer average task completion time. △ Less","2 November, 2021",https://arxiv.org/pdf/2111.01400
On the Current and Emerging Challenges of Developing Fair and Ethical AI Solutions in Financial Services,Eren Kurshan;Jiahao Chen;Victor Storchan;Hongda Shen,"Artificial intelligence (AI) continues to find more numerous and more critical applications in the financial services industry, giving rise to fair and ethical AI as an industry-wide objective. While many ethical principles and guidelines have been published in recent years, they fall short of addressing the serious challenges that model developers face when building ethical AI solutions. We survey the practical and overarching issues surrounding model development, from design and implementation complexities, to the shortage of tools, and the lack of organizational constructs. We show how practical considerations reveal the gaps between high-level principles and concrete, deployed AI applications, with the aim of starting industry-wide conversations toward solution approaches. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.01306
SmartSplit: Latency-Energy-Memory Optimisation for CNN Splitting on Smartphone Environment,Ishan Prakash;Aniruddh Bansal;Rohit Verma;Rajeev Shorey,"Artificial Intelligence has now taken centre stage in the smartphone industry owing to the need of bringing all processing close to the user and addressing privacy concerns. Convolution Neural Networks (CNNs), which are used by several AI applications, are highly resource and computation intensive. Although new generation smartphones come with AI-enabled chips, minimal memory and energy utilisation is essential as many applications are run concurrently on a smartphone. In light of this, optimising the workload on the smartphone by offloading a part of the processing to a cloud server is an important direction of research. In this paper, we analyse the feasibility of splitting CNNs between smartphones and cloud server by formulating a multi-objective optimisation problem that optimises the end-to-end latency, memory utilisation, and energy consumption. We design SmartSplit, a Genetic Algorithm with decision analysis based approach to solve the optimisation problem. Our experiments run with multiple CNN models show that splitting a CNN between a smartphone and a cloud server is feasible. The proposed approach, SmartSplit fares better when compared to other state-of-the-art approaches. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.01077
Gomoku: analysis of the game and of the player Wine,Lorenzo Piazzo;Michele Scarpiniti;Enzo Baccarelli,"Gomoku, also known as five in a row, is a classical board game, ideally suited for quickly testing novel Artificial Intelligence (AI) techniques. With the aim of facilitating a developer willing to write a new Gomoku player, in this report we present an analysis of the main game concepts and strategies, which is wider and deeper than existing ones. Moreover, after discussing the general structure of an artificial player, we present and analyse a strong Gomoku player, named Wine, the code of which is freely available on the Internet and which is an excelent example of how a modern player is organised. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.01016
Modelling the transition to a low-carbon energy supply,Alexander Kell,"A transition to a low-carbon electricity supply is crucial to limit the impacts of climate change. Reducing carbon emissions could help prevent the world from reaching a tipping point, where runaway emissions are likely. Runaway emissions could lead to extremes in weather conditions around the world -- especially in problematic regions unable to cope with these conditions. However, the movement to a low-carbon energy supply can not happen instantaneously due to the existing fossil-fuel infrastructure and the requirement to maintain a reliable energy supply. Therefore, a low-carbon transition is required, however, the decisions various stakeholders should make over the coming decades to reduce these carbon emissions are not obvious. This is due to many long-term uncertainties, such as electricity, fuel and generation costs, human behaviour and the size of electricity demand. A well choreographed low-carbon transition is, therefore, required between all of the heterogenous actors in the system, as opposed to changing the behaviour of a single, centralised actor. The objective of this thesis is to create a novel, open-source agent-based model to better understand the manner in which the whole electricity market reacts to different factors using state-of-the-art machine learning and artificial intelligence methods. In contrast to other works, this thesis looks at both the long-term and short-term impact that different behaviours have on the electricity market by using these state-of-the-art methods. △ Less","25 September, 2021",https://arxiv.org/pdf/2111.00987
Smart Fashion: A Review of AI Applications in the Fashion & Apparel Industry,Seyed Omid Mohammadi;Ahmad Kalhor,"The fashion industry is on the verge of an unprecedented change. The implementation of machine learning, computer vision, and artificial intelligence (AI) in fashion applications is opening lots of new opportunities for this industry. This paper provides a comprehensive survey on this matter, categorizing more than 580 related articles into 22 well-defined fashion-related tasks. Such structured task-based multi-label classification of fashion research articles provides researchers with explicit research directions and facilitates their access to the related studies, improving the visibility of studies simultaneously. For each task, a time chart is provided to analyze the progress through the years. Furthermore, we provide a list of 86 public fashion datasets accompanied by a list of suggested applications and additional information for each. △ Less","2 November, 2021",https://arxiv.org/pdf/2111.00905
Artificial Intelligence in the Low-Level Realm -- A Survey,Vahid Mohammadi Safarzadeh;Hamed Ghasr Loghmani,"Resource-aware machine learning has been a trending topic in recent years, focusing on making ML computational aspects more exploitable by the edge devices in the Internet of Things. This paper attempts to review a conceptually and practically related area concentrated on efforts and challenges for applying ML in the operating systems' main tasks in a low-resource environment. Artificial Intelligence has been integrated into the operating system with applications such as voice or image recognition. However, this integration is only in user space. Here, we seek methods and efforts that exploit AI approaches, specifically machine learning, in the OSes' primary responsibilities. We provide the improvements that ML can bring to OS to make them more trustworthy. In other words, the main question to be answered is how AI has played/can play a role directly in improving the traditional OS kernel main tasks. Also, the challenges and limitations in the way of this combination are provided. △ Less","19 September, 2021",https://arxiv.org/pdf/2111.00881
"Reproducibility as a Mechanism for Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence",Ana Lucic;Maurits Bleeker;Sami Jullien;Samarth Bhargav;Maarten de Rijke,"In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility. The focal point of the course is a group project based on reproducing existing FACT-AI algorithms from top AI conferences and writing a corresponding report. In the first iteration of the course, we created an open source repository with the code implementations from the group projects. In the second iteration, we encouraged students to submit their group projects to the Machine Learning Reproducibility Challenge, resulting in 9 reports from our course being accepted for publication in the ReScience journal. We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs. We hope this can be a useful resource for instructors who want to set up similar courses in the future. △ Less","17 December, 2021",https://arxiv.org/pdf/2111.00826
Knowledge-driven Site Selection via Urban Knowledge Graph,Yu Liu;Jingtao Ding;Yong Li,"Site selection determines optimal locations for new stores, which is of crucial importance to business success. Especially, the wide application of artificial intelligence with multi-source urban data makes intelligent site selection promising. However, existing data-driven methods heavily rely on feature engineering, facing the issues of business generalization and complex relationship modeling. To get rid of the dilemma, in this work, we borrow ideas from knowledge graph (KG), and propose a knowledge-driven model for site selection, short for KnowSite. Specifically, motivated by distilled knowledge and rich semantics in KG, we firstly construct an urban KG (UrbanKG) with cities' key elements and semantic relationships captured. Based on UrbanKG, we employ pre-training techniques for semantic representations, which are fed into an encoder-decoder structure for site decisions. With multi-relational message passing and relation path-based attention mechanism developed, KnowSite successfully reveals the relationship between various businesses and site selection criteria. Extensive experiments on two datasets demonstrate that KnowSite outperforms representative baselines with both effectiveness and explainability achieved. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.00787
An AI-powered Smart Routing Solution for Payment Systems,Ramya Bygari;Aayush Gupta;Shashwat Raghuvanshi;Aakanksha Bapna;Birendra Sahu,"In the current era of digitization, online payment systems are attracting considerable interest. Improving the efficiency of a payment system is important since it has a substantial impact on revenues for businesses. A gateway is an integral component of a payment system through which every transaction is routed. In an online payment system, payment processors integrate with these gateways by means of various configurations such as pricing, methods, risk checks, etc. These configurations are called terminals. Each gateway can have multiple terminals associated with it. Routing a payment transaction through the best terminal is crucial to increase the probability of a payment transaction being successful. Machine learning (ML) and artificial intelligence (AI) techniques can be used to accurately predict the best terminals based on their previous performance and various payment-related attributes. We have devised a pipeline consisting of static and dynamic modules. The static module does the initial filtering of the terminals using static rules and a logistic regression model that predicts gateway downtimes. Subsequently, the dynamic module computes a lot of novel features based on success rate, payment attributes, time lag, etc. to model the terminal behaviour accurately. These features are updated using an adaptive time decay rate algorithm in real-time using a feedback loop and passed to a random forest classifier to predict the success probabilities for every terminal. This pipeline is currently in production at Razorpay routing millions of transactions through it in real-time and has given a 4-6\% improvement in success rate across all payment methods (credit card, debit card, UPI, net banking). This has made our payment system more resilient to performance drops, which has improved the user experience, instilled more trust in the merchants, and boosted the revenue of the business. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.00783
Explainable Artificial Intelligence for Smart City Application: A Secure and Trusted Platform,M. Humayn Kabir;Khondokar Fida Hasan;Mohammad Kamrul Hasan;Keyvan Ansari,"Artificial Intelligence (AI) is one of the disruptive technologies that is shaping the future. It has growing applications for data-driven decisions in major smart city solutions, including transportation, education, healthcare, public governance, and power systems. At the same time, it is gaining popularity in protecting critical cyber infrastructure from cyber threats, attacks, damages, or unauthorized access. However, one of the significant issues of those traditional AI technologies (e.g., deep learning) is that the rapid progress in complexity and sophistication propelled and turned out to be uninterpretable black boxes. On many occasions, it is very challenging to understand the decision and bias to control and trust systems' unexpected or seemingly unpredictable outputs. It is acknowledged that the loss of control over interpretability of decision-making becomes a critical issue for many data-driven automated applications. But how may it affect the system's security and trustworthiness? This chapter conducts a comprehensive study of machine learning applications in cybersecurity to indicate the need for explainability to address this question. While doing that, this chapter first discusses the black-box problems of AI technologies for Cybersecurity applications in smart city-based solutions. Later, considering the new technological paradigm, Explainable Artificial Intelligence (XAI), this chapter discusses the transition from black-box to white-box. This chapter also discusses the transition requirements concerning the interpretability, transparency, understandability, and Explainability of AI-based technologies in applying different autonomous systems in smart cities. Finally, it has presented some commercial XAI platforms that offer explainability over traditional AI technologies before presenting future challenges and opportunities. △ Less","31 October, 2021",https://arxiv.org/pdf/2111.00601
Identifying Functions and Behaviours of Social Robots during Learning Activities: Teachers' Perspective,Jessy Ceha;Edith Law;Dana Kulić;Pierre-Yves Oudeyer;Didier Roy,"With advances in artificial intelligence, research is increasingly exploring the potential functions that social robots can play in education. As teachers are a critical stakeholder in the use and application of educational technologies, we conducted a study to understand teachers' perspectives on how a social robot could support a variety of learning activities in the classroom. Through interviews, robot puppeteering, and group brainstorming sessions with five elementary and middle school teachers from a local school in Canada, we take a socio-technical perspective to conceptualize possible robot functions and behaviours, and the effects they may have on the current way learning activities are designed, planned, and executed. Overall, the teachers responded positively to the idea of introducing a social robot as a technological tool for learning activities, envisioning differences in usage for teacher-robot and student-robot interactions. Further, Engeström's Activity System Model -- a framework for analyzing human needs, tasks, and outcomes -- illustrated a number of tensions associated with learning activities in the classroom. We discuss the fine-grained robot functions and behaviours conceived by teachers, and how they address the current tensions -- providing suggestions for improving the design of social robots for learning activities. △ Less","30 October, 2021",https://arxiv.org/pdf/2111.00360
Visual Explanations for Convolutional Neural Networks via Latent Traversal of Generative Adversarial Networks,Amil Dravid;Aggelos K. Katsaggelos,"Lack of explainability in artificial intelligence, specifically deep neural networks, remains a bottleneck for implementing models in practice. Popular techniques such as Gradient-weighted Class Activation Mapping (Grad-CAM) provide a coarse map of salient features in an image, which rarely tells the whole story of what a convolutional neural network (CNN) learned. Using COVID-19 chest X-rays, we present a method for interpreting what a CNN has learned by utilizing Generative Adversarial Networks (GANs). Our GAN framework disentangles lung structure from COVID-19 features. Using this GAN, we can visualize the transition of a pair of COVID negative lungs in a chest radiograph to a COVID positive pair by interpolating in the latent space of the GAN, which provides fine-grained visualization of how the CNN responds to varying features within the lungs. △ Less","1 November, 2021",https://arxiv.org/pdf/2111.00116
Hidden Markov Based Mathematical Model dedicated to Extract Ingredients from Recipe Text,Zied Baklouti,"Natural Language Processing (NLP) is a branch of artificial intelligence that gives machines the ability to decode human languages. Partof-speech tagging (POS tagging) is a pre-processing task that requires an annotated corpus. Rule-based and stochastic methods showed remarkable results for POS tag prediction. On this work, I performed a mathematical model based on Hidden Markov structures and I obtained a high-level accuracy of ingredients extracted from text recipe with performances greater than what traditional methods could make without unknown words consideration. △ Less","28 September, 2021",https://arxiv.org/pdf/2110.15707
What makes us curious? analysis of a corpus of open-domain questions,Zhaozhen Xu;Amelia Howarth;Nicole Briggs;Nello Cristianini,"Every day people ask short questions through smart devices or online forums to seek answers to all kinds of queries. With the increasing number of questions collected it becomes difficult to provide answers to each of them, which is one of the reasons behind the growing interest in automated question answering. Some questions are similar to existing ones that have already been answered, while others could be answered by an external knowledge source such as Wikipedia. An important question is what can be revealed by analysing a large set of questions. In 2017, ""We the Curious"" science centre in Bristol started a project to capture the curiosity of Bristolians: the project collected more than 10,000 questions on various topics. As no rules were given during collection, the questions are truly open-domain, and ranged across a variety of topics. One important aim for the science centre was to understand what concerns its visitors had beyond science, particularly on societal and cultural issues. We addressed this question by developing an Artificial Intelligence tool that can be used to perform various processing tasks: detection of equivalence between questions; detection of topic and type; and answering of the question. As we focused on the creation of a ""generalist"" tool, we trained it with labelled data from different datasets. We called the resulting model QBERT. This paper describes what information we extracted from the automated analysis of the WTC corpus of open-domain questions. △ Less","28 October, 2021",https://arxiv.org/pdf/2110.15409
Lightweight Mobile Automated Assistant-to-physician for Global Lower-resource Areas,Chao Zhang;Hanxin Zhang;Atif Khan;Ted Kim;Olasubomi Omoleye;Oluwamayomikun Abiona;Amy Lehman;Christopher O. Olopade;Olufunmilayo I. Olopade;Pedro Lopes;Andrey Rzhetsky,"Importance: Lower-resource areas in Africa and Asia face a unique set of healthcare challenges: the dual high burden of communicable and non-communicable diseases; a paucity of highly trained primary healthcare providers in both rural and densely populated urban areas; and a lack of reliable, inexpensive internet connections. Objective: To address these challenges, we designed an artificial intelligence assistant to help primary healthcare providers in lower-resource areas document demographic and medical sign/symptom data and to record and share diagnostic data in real-time with a centralized database. Design: We trained our system using multiple data sets, including US-based electronic medical records (EMRs) and open-source medical literature and developed an adaptive, general medical assistant system based on machine learning algorithms. Main outcomes and Measure: The application collects basic information from patients and provides primary care providers with diagnoses and prescriptions suggestions. The application is unique from existing systems in that it covers a wide range of common diseases, signs, and medication typical in lower-resource countries; the application works with or without an active internet connection. Results: We have built and implemented an adaptive learning system that assists trained primary care professionals by means of an Android smartphone application, which interacts with a central database and collects real-time data. The application has been tested by dozens of primary care providers. Conclusions and Relevance: Our application would provide primary healthcare providers in lower-resource areas with a tool that enables faster and more accurate documentation of medical encounters. This application could be leveraged to automatically populate local or national EMR systems. △ Less","28 October, 2021",https://arxiv.org/pdf/2110.15127
Adaptive Multimodal and Multisensory Empathic Technologies for Enhanced Human Communication,Roxana Girju,"As digital social platforms and mobile technologies are becoming more prevalent and robust, the use of Artificial Intelligence (AI) in facilitating human communication will grow. This, in turn, will pave the way for the development of intuitive, adaptive, and effective empathic AI interfaces that better address the needs of socially and culturally diverse communities. I believe such developments must consider a principled framework that includes the human perceptual senses in the digital design process right from the start, for a more accurate, as well as a more aesthetic, memorable, and soothing experience. In this position paper, I suggest features, identify some challenges that need to be addressed in the process, and propose some future research directions that I think should be part of the design and implementation. Such an approach will allow various communities of practice to investigate the areas of intersection between artificial intelligence, on one side, and human communication, perceptual needs and social and cultural values, on the other. △ Less","24 October, 2021",https://arxiv.org/pdf/2110.15054
IMDB-WIKI-SbS: An Evaluation Dataset for Crowdsourced Pairwise Comparisons,Nikita Pavlichenko;Dmitry Ustalov,"Today, comprehensive evaluation of large-scale machine learning models is possible thanks to the open datasets produced using crowdsourcing, such as SQuAD, MS COCO, ImageNet, SuperGLUE, etc. These datasets capture objective responses, assuming the single correct answer, which does not allow to capture the subjective human perception. In turn, pairwise comparison tasks, in which one has to choose between only two options, allow taking peoples' preferences into account for very challenging artificial intelligence tasks, such as information retrieval and recommender system evaluation. Unfortunately, the available datasets are either small or proprietary, slowing down progress in gathering better feedback from human users. In this paper, we present IMDB-WIKI-SbS, a new large-scale dataset for evaluating pairwise comparisons. It contains 9,150 images appearing in 250,249 pairs annotated on a crowdsourcing platform. Our dataset has balanced distributions of age and gender using the well-known IMDB-WIKI dataset as ground truth. We describe how our dataset is built and then compare several baseline methods, indicating its suitability for model evaluation. △ Less","26 November, 2021",https://arxiv.org/pdf/2110.14990
Diagnosis of COVID-19 Using Machine Learning and Deep Learning: A review,M. Rubaiyat Hossain Mondal;Subrato Bharati;Prajoy Podder,"Background: This paper provides a systematic review of the application of Artificial Intelligence (AI) in the form of Machine Learning (ML) and Deep Learning (DL) techniques in fighting against the effects of novel coronavirus disease (COVID-19). Objective & Methods: The objective is to perform a scoping review on AI for COVID-19 using preferred reporting items of systematic reviews and meta-analysis (PRISMA) guidelines. A literature search was performed for relevant studies published from 1 January 2020 till 27 March 2021. Out of 4050 research papers available in reputed publishers, a full-text review of 440 articles was done based on the keywords of AI, COVID-19, ML, forecasting, DL, X-ray, and Computed Tomography (CT). Finally, 52 articles were included in the result synthesis of this paper. As part of the review, different ML regression methods were reviewed first in predicting the number of confirmed and death cases. Secondly, a comprehensive survey was carried out on the use of ML in classifying COVID-19 patients. Thirdly, different datasets on medical imaging were compared in terms of the number of images, number of positive samples and number of classes in the datasets. The different stages of the diagnosis, including preprocessing, segmentation and feature extraction were also reviewed. Fourthly, the performance results of different research papers were compared to evaluate the effectiveness of DL methods on different datasets. Results: Results show that residual neural network (ResNet-18) and densely connected convolutional network (DenseNet 169) exhibit excellent classification accuracy for X-ray images, while DenseNet-201 has the maximum accuracy in classifying CT scan images. This indicates that ML and DL are useful tools in assisting researchers and medical professionals in predicting, screening and detecting COVID-19. △ Less","28 October, 2021",https://arxiv.org/pdf/2110.14910
Towards Robust Reasoning over Knowledge Graphs,Zhaohan Xi;Ren Pang;Changjiang Li;Shouling Ji;Xiapu Luo;Xusheng Xiao;Ting Wang,"Answering complex logical queries over large-scale knowledge graphs (KGs) represents an important artificial intelligence task, entailing a range of applications. Recently, knowledge representation learning (KRL) has emerged as the state-of-the-art approach, wherein KG entities and the query are embedded into a latent space such that entities that answer the query are embedded close to the query. Yet, despite its surging popularity, the potential security risks of KRL are largely unexplored, which is concerning, given the increasing use of such capabilities in security-critical domains (e.g., cyber-security and healthcare). This work represents a solid initial step towards bridging this gap. We systematize the potential security threats to KRL according to the underlying attack vectors (e.g., knowledge poisoning and query perturbation) and the adversary's background knowledge. More importantly, we present ROAR(Reasoning Over Adversarial Representations), a new class of attacks that instantiate a variety of such threats. We demonstrate the practicality of ROAR in two representative use cases (i.e., cyber-threat hunting and drug repurposing). For instance, ROAR attains over 99% attack success rate in misleading the threat intelligence engine to give pre-defined answers for target queries, yet without any impact on non-target ones. Further, we discuss potential countermeasures against ROAR, including filtering of poisoning facts and robust training with adversarial queries, which leads to several promising research directions. △ Less","31 October, 2021",https://arxiv.org/pdf/2110.14693
Provably Robust Model-Centric Explanations for Critical Decision-Making,Cecilia G. Morales;Nicholas Gisolfi;Robert Edman;James K. Miller;Artur Dubrawski,"We recommend using a model-centric, Boolean Satisfiability (SAT) formalism to obtain useful explanations of trained model behavior, different and complementary to what can be gleaned from LIME and SHAP, popular data-centric explanation tools in Artificial Intelligence (AI). We compare and contrast these methods, and show that data-centric methods may yield brittle explanations of limited practical utility. The model-centric framework, however, can offer actionable insights into risks of using AI models in practice. For critical applications of AI, split-second decision making is best informed by robust explanations that are invariant to properties of data, the capability offered by model-centric frameworks. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.13937
"DASentimental: Detecting depression, anxiety and stress in texts via emotional recall, cognitive networks and machine learning",Asra Fatima;Li Ying;Thomas Hills;Massimo Stella,"Most current affect scales and sentiment analysis on written text focus on quantifying valence (sentiment) -- the most primary dimension of emotion. However, emotions are broader and more complex than valence. Distinguishing negative emotions of similar valence could be important in contexts such as mental health. This project proposes a semi-supervised machine learning model (DASentimental) to extract depression, anxiety and stress from written text. First, we trained the model to spot how sequences of recalled emotion words by N=200 individuals correlated with their responses to the Depression Anxiety Stress Scale (DASS-21). Within the framework of cognitive network science, we model every list of recalled emotions as a walk over a networked mental representation of semantic memory, with emotions connected according to free associations in people's memory. Among several tested machine learning approaches, we find that a multilayer perceptron neural network trained on word sequences and semantic network distances can achieve state-of-art, cross-validated predictions for depression (R = 0.7), anxiety (R = 0.44) and stress (R = 0.52). Though limited by sample size, this first-of-its-kind approach enables quantitative explorations of key semantic dimensions behind DAS levels. We find that semantic distances between recalled emotions and the dyad ""sad-happy"" are crucial features for estimating depression levels but are less important for anxiety and stress. We also find that semantic distance of recalls from ""fear"" can boost the prediction of anxiety but it becomes redundant when the ""sad-happy"" dyad is considered. Adopting DASentimental as a semi-supervised learning tool to estimate DAS in text, we apply it to a dataset of 142 suicide notes. We conclude by discussing key directions for future research enabled by artificial intelligence detecting stress, anxiety and depression. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.13710
GANash -- A GAN approach to steganography,Venkatesh Subramaniyan;Vignesh Sivakumar;A. K. Vagheesan;S. Sakthivelan;K. J. Jegadish Kumar;K. K. Nagarajan,"Data security is of the utmost concern of a communication system. Since the early days, many developments have been made to improve the performance of the system. PSNR of the received signal, secure transmission channel, quality of encoding used, etc. are some of the key attributes of a good system. To ensure security, the most commonly used technique is cryptography in which the message is altered with respect to a key and using the same, the encoded message is decoded at the receiver side. A complementary technique that is popularly used to insure security is steganography. The advancements in Artificial Intelligence(AI) have paved way for performing steganography in an intelligent, tamper-proof manner. The recent discovery by researchers in the field of Deep Learning(DL), an unsupervised learning network known as the Generative Adversarial Networks(GAN) has improved the performance of this technique exponentially. It has been demonstrated that deep neural networks are highly sensitive to tiny perturbations of input data, giving rise to adversarial examples. Though this property is usually considered a weakness of learned models, it could be beneficial if used appropriately. The work that has been accomplished by MIT for this purpose, a deep-neural model by the name of SteganoGAN, has shown obligation for using this technique for steganography. In this work, we have proposed a novel approach to improve the performance of the existing system using latent space compression on the encoded data. This theoretically would improve the performance exponentially. Thus, the algorithms used to improve the system's performance and the results obtained have been enunciated in this work. The results indicate the level of dominance this system could achieve to be able to diminish the difficulties in solving real-time problems in terms of security, deployment and database management. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.13650
Automated Support for Unit Test Generation: A Tutorial Book Chapter,Afonso Fontes;Gregory Gay;Francisco Gomes de Oliveira Neto;Robert Feldt,"Unit testing is a stage of testing where the smallest segment of code that can be tested in isolation from the rest of the system - often a class - is tested. Unit tests are typically written as executable code, often in a format provided by a unit testing framework such as pytest for Python. Creating unit tests is a time and effort-intensive process with many repetitive, manual elements. To illustrate how AI can support unit testing, this chapter introduces the concept of search-based unit test generation. This technique frames the selection of test input as an optimization problem - we seek a set of test cases that meet some measurable goal of a tester - and unleashes powerful metaheuristic search algorithms to identify the best possible test cases within a restricted timeframe. This chapter introduces two algorithms that can generate pytest-formatted unit tests, tuned towards coverage of source code statements. The chapter concludes by discussing more advanced concepts and gives pointers to further reading for how artificial intelligence can support developers and testers when unit testing software. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.13575
Emotion recognition in talking-face videos using persistent entropy and neural networks,Eduardo Paluzo-Hidalgo;Guillermo Aguirre-Carrazana;Rocio Gonzalez-Diaz,"The automatic recognition of a person's emotional state has become a very active research field that involves scientists specialized in different areas such as artificial intelligence, computer vision or psychology, among others. Our main objective in this work is to develop a novel approach, using persistent entropy and neural networks as main tools, to recognise and classify emotions from talking-face videos. Specifically, we combine audio-signal and image-sequence information to compute a topology signature(a 9-dimensional vector) for each video. We prove that small changes in the video produce small changes in the signature. These topological signatures are used to feed a neural network to distinguish between the following emotions: neutral, calm, happy, sad, angry, fearful, disgust, and surprised. The results reached are promising and competitive, beating the performance reached in other state-of-the-art works found in the literature. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.13571
How Should AI Interpret Rules? A Defense of Minimally Defeasible Interpretive Argumentation,John Licato,"Can artificially intelligent systems follow rules? The answer might seem an obvious `yes', in the sense that all (current) AI strictly acts in accordance with programming code constructed from highly formalized and well-defined rulesets. But here I refer to the kinds of rules expressed in human language that are the basis of laws, regulations, codes of conduct, ethical guidelines, and so on. The ability to follow such rules, and to reason about them, is not nearly as clear-cut as it seems on first analysis. Real-world rules are unavoidably rife with open-textured terms, which imbue rules with a possibly infinite set of possible interpretations. Narrowing down this set requires a complex reasoning process that is not yet within the scope of contemporary AI. This poses a serious problem for autonomous AI: If one cannot reason about open-textured terms, then one cannot reason about (or in accordance with) real-world rules. And if one cannot reason about real-world rules, then one cannot: follow human laws, comply with regulations, act in accordance with written agreements, or even obey mission-specific commands that are anything more than trivial. But before tackling these problems, we must first answer a more fundamental question: Given an open-textured rule, what is its correct interpretation? Or more precisely: How should our artificially intelligent systems determine which interpretation to consider correct? In this essay, I defend the following answer: Rule-following AI should act in accordance with the interpretation best supported by minimally defeasible interpretive arguments (MDIA). △ Less","25 October, 2021",https://arxiv.org/pdf/2110.13341
Decomposed Inductive Procedure Learning,Daniel Weitekamp;Christopher MacLellan;Erik Harpstead;Kenneth Koedinger,"Recent advances in machine learning have made it possible to train artificially intelligent agents that perform with super-human accuracy on a great diversity of complex tasks. However, the process of training these capabilities often necessitates millions of annotated examples -- far more than humans typically need in order to achieve a passing level of mastery on similar tasks. Thus, while contemporary methods in machine learning can produce agents that exhibit super-human performance, their rate of learning per opportunity in many domains is decidedly lower than human-learning. In this work we formalize a theory of Decomposed Inductive Procedure Learning (DIPL) that outlines how different forms of inductive symbolic learning can be used in combination to build agents that learn educationally relevant tasks such as mathematical, and scientific procedures, at a rate similar to human learners. We motivate the construction of this theory along Marr's concepts of the computational, algorithmic, and implementation levels of cognitive modeling, and outline at the computational-level six learning capacities that must be achieved to accurately model human learning. We demonstrate that agents built along the DIPL theory are amenable to satisfying these capacities, and demonstrate, both empirically and theoretically, that DIPL enables the creation of agents that exhibit human-like learning performance. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.13233
Debiasing Credit Scoring using Evolutionary Algorithms,Nigel Kingsman,"This paper investigates the application of machine learning when training a credit decision model over real, publicly available data whilst accounting for ""bias objectives"". We use the term ""bias objective"" to describe the requirement that a trained model displays discriminatory bias against a given groups of individuals that doesn't exceed a prescribed level, where such level can be zero. This research presents an empirical study examining the tension between competing model training objectives which in all cases include one or more bias objectives. This work is motivated by the observation that the parties associated with creditworthiness models have requirements that can not certainly be fully met simultaneously. The research herein seeks to highlight the impracticality of satisfying all parties' objectives, demonstrating the need for ""trade-offs"" to be made. The results and conclusions presented by this paper are of particular importance for all stakeholders within the credit scoring industry that rely upon artificial intelligence (AI) models as part of the decision-making process when determining the creditworthiness of individuals. This paper provides an exposition of the difficulty of training AI models that are able to simultaneously satisfy multiple bias objectives whilst maintaining acceptable levels of accuracy. Stakeholders should be aware of this difficulty and should acknowledge that some degree of discriminatory bias, across a number of protected characteristics and formulations of bias, cannot be avoided. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.12838
Random Matrix based Physical Layer Secret Key Generation in Static Channels,Zhuangkun Wei;Weisi Guo,"Physical layer secret key generation exploits the reciprocal channel randomness for key generation and has proven to be an effective addition security layer in wireless communications. However, static or scarcely random channels require artificially induced dynamics to improve the secrecy performance, e.g., using intelligent reflecting surface (IRS). One key challenge is that the induced random phase from IRS is also reflected in the direction to eavesdroppers (Eve). This leakage enables Eve nodes to estimate the legitimate channels and secret key via a globally known pilot sequence. To mitigate the secret key leakage issue, we propose to exploit random matrix theory to inform the design of a new physical layer secret key generation (PL-SKG) algorithm. We prove that, when sending appropriate random Gaussian matrices, the singular values of Alice's and Bob's received signals follow a similar probability distribution. Leveraging these common singular values, we propose a random Gaussian matrix based PL-SKG (RGM PL-SKG), which avoids the usages of the globally known pilot and thereby prevents the aforementioned leakage issue. Our results show the following: (i) high noise resistance which leads to superior secret key rate (SKR) improvement (up to 300%) in low SNR regime, and (ii) general improved SKR performance against multiple colluded Eves. We believe our combination of random matrix theory and PL-SKG shows a new paradigm to secure the wireless communication channels. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.12785
Machine Learning in Finance-Emerging Trends and Challenges,Jaydip Sen;Rajdeep Sen;Abhishek Dutta,"The paradigm of machine learning and artificial intelligence has pervaded our everyday life in such a way that it is no longer an area for esoteric academics and scientists putting their effort to solve a challenging research problem. The evolution is quite natural rather than accidental. With the exponential growth in processing speed and with the emergence of smarter algorithms for solving complex and challenging problems, organizations have found it possible to harness a humongous volume of data in realizing solutions that have far-reaching business values. This introductory chapter highlights some of the challenges and barriers that organizations in the financial services sector at the present encounter in adopting machine learning and artificial intelligence-based models and applications in their day-to-day operations. △ Less","8 October, 2021",https://arxiv.org/pdf/2110.11999
Gapoera: Application Programming Interface for AI Environment of Indonesian Board Game,Rian Adam Rajagede;Galang Prihadi Mahardhika,"Currently, the development of computer games has shown a tremendous surge. The ease and speed of internet access today have also influenced the development of computer games, especially computer games that are played online. Internet technology has allowed computer games to be played in multiplayer mode. Interaction between players in a computer game can be built in several ways, one of which is by providing balanced opponents. Opponents can be developed using intelligent agents. On the other hand, research on developing intelligent agents is also growing rapidly. In computer game development, one of the easiest ways to measure the performance of an intelligent agent is to develop a virtual environment that allows the intelligent agent to interact with other players. In this research, we try to develop an intelligent agent and virtual environment for the board game. To be easily accessible, the intelligent agent and virtual environment are then developed into an Application Programming Interface (API) service called Gapoera API. The Gapoera API service that is built is expected to help game developers develop a game without having to think much about the artificial intelligence that will be embedded in the game. This service provides a basic multilevel intelligent agent that can provide users with playing board games commonly played in Indonesia. Although the Gapoera API can be used for various types of games, in this paper, we will focus on the discussion on a popular traditional board game in Indonesia, namely Mancala. The test results conclude that the multilevel agent concept developed has worked as expected. On the other hand, the development of the Gapoera API service has also been successfully accessed on several game platforms. △ Less","22 October, 2021",https://arxiv.org/pdf/2110.11924
Measuring the Non-Transitivity in Chess,Ricky Sanjaya;Jun Wang;Yaodong Yang,"It has long been believed that Chess is the \emph{Drosophila} of Artificial Intelligence (AI). Studying Chess can productively provide valid knowledge about complex systems. Although remarkable progress has been made on solving Chess, the geometrical landscape of Chess in the strategy space is still mysterious. Judging on AI-generated strategies, researchers hypothesised that the strategy space of Chess possesses a spinning top geometry, with the upright axis representing the \emph{transitive} dimension (e.g., A beats B, B beats C, A beats C), and the radial axis representing the \emph{non-transitive} dimension (e.g., A beats B, B beats C, C beats A). However, it is unclear whether such a hypothesis holds for real-world strategies. In this paper, we quantify the non-transitivity in Chess through real-world data from human players. Specifically, we performed two ways of non-transitivity quantifications -- Nash Clustering and counting the number of Rock-Paper-Scissor cycles -- on over one billion match data from Lichess and FICS. Our findings positively indicate that the strategy space occupied by real-world Chess strategies demonstrates a spinning top geometry, and more importantly, there exists a strong connection between the degree of non-transitivity and the progression of a Chess player's rating. In particular, high degrees of non-transitivity tend to prevent human players from making progress on their Elo rating, whereas progressions are easier to make at the level of ratings where the degree of non-transitivity is lower. Additionally, we also investigate the implication of the degree of non-transitivity for population-based training methods. By considering \emph{fixed-memory Fictitious Play} as a proxy, we reach the conclusion that maintaining large-size and diverse populations of strategies is imperative to training effective AI agents in solving Chess types of games. △ Less","22 October, 2021",https://arxiv.org/pdf/2110.11737
"Neural-guided, Bidirectional Program Search for Abstraction and Reasoning",Simon Alford;Anshula Gandhi;Akshay Rangamani;Andrzej Banburski;Tony Wang;Sylee Dandekar;John Chin;Tomaso Poggio;Peter Chin,"One of the challenges facing artificial intelligence research today is designing systems capable of utilizing systematic reasoning to generalize to new tasks. The Abstraction and Reasoning Corpus (ARC) measures such a capability through a set of visual reasoning tasks. In this paper we report incremental progress on ARC and lay the foundations for two approaches to abstraction and reasoning not based in brute-force search. We first apply an existing program synthesis system called DreamCoder to create symbolic abstractions out of tasks solved so far, and show how it enables solving of progressively more challenging ARC tasks. Second, we design a reasoning algorithm motivated by the way humans approach ARC. Our algorithm constructs a search graph and reasons over this graph structure to discover task solutions. More specifically, we extend existing execution-guided program synthesis approaches with deductive reasoning based on function inverse semantics to enable a neural-guided bidirectional search algorithm. We demonstrate the effectiveness of the algorithm on three domains: ARC, 24-Game tasks, and a 'double-and-add' arithmetic puzzle. △ Less","26 October, 2021",https://arxiv.org/pdf/2110.11536
Center Loss Regularization for Continual Learning,Kaustubh Olpadkar;Ekta Gavas,"The ability to learn different tasks sequentially is essential to the development of artificial intelligence. In general, neural networks lack this capability, the major obstacle being catastrophic forgetting. It occurs when the incrementally available information from non-stationary data distributions is continually acquired, disrupting what the model has already learned. Our approach remembers old tasks by projecting the representations of new tasks close to that of old tasks while keeping the decision boundaries unchanged. We employ the center loss as a regularization penalty that enforces new tasks' features to have the same class centers as old tasks and makes the features highly discriminative. This, in turn, leads to the least forgetting of already learned information. This method is easy to implement, requires minimal computational and memory overhead, and allows the neural network to maintain high performance across many sequentially encountered tasks. We also demonstrate that using the center loss in conjunction with the memory replay outperforms other replay-based strategies. Along with standard MNIST variants for continual learning, we apply our method to continual domain adaptation scenarios with the Digits and PACS datasets. We demonstrate that our approach is scalable, effective, and gives competitive performance compared to state-of-the-art continual learning methods. △ Less","21 October, 2021",https://arxiv.org/pdf/2110.11314
On games and simulators as a platform for development of artificial intelligence for command and control,Vinicius G. Goecks;Nicholas Waytowich;Derrik E. Asher;Song Jun Park;Mark Mittrick;John Richardson;Manuel Vindiola;Anne Logie;Mark Dennison;Theron Trout;Priya Narayanan;Alexander Kott,"Games and simulators can be a valuable platform to execute complex multi-agent, multiplayer, imperfect information scenarios with significant parallels to military applications: multiple participants manage resources and make decisions that command assets to secure specific areas of a map or neutralize opposing forces. These characteristics have attracted the artificial intelligence (AI) community by supporting development of algorithms with complex benchmarks and the capability to rapidly iterate over new ideas. The success of artificial intelligence algorithms in real-time strategy games such as StarCraft II have also attracted the attention of the military research community aiming to explore similar techniques in military counterpart scenarios. Aiming to bridge the connection between games and military applications, this work discusses past and current efforts on how games and simulators, together with the artificial intelligence algorithms, have been adapted to simulate certain aspects of military missions and how they might impact the future battlefield. This paper also investigates how advances in virtual reality and visual augmentation systems open new possibilities in human interfaces with gaming platforms and their military parallels. △ Less","21 October, 2021",https://arxiv.org/pdf/2110.11305
A Survey on Methods and Metrics for the Assessment of Explainability under the Proposed AI Act,Francesco Sovrano;Salvatore Sapienza;Monica Palmirani;Fabio Vitali,"This study discusses the interplay between metrics used to measure the explainability of the AI systems and the proposed EU Artificial Intelligence Act. A standardisation process is ongoing: several entities (e.g. ISO) and scholars are discussing how to design systems that are compliant with the forthcoming Act and explainability metrics play a significant role. This study identifies the requirements that such a metric should possess to ease compliance with the AI Act. It does so according to an interdisciplinary approach, i.e. by departing from the philosophical concept of explainability and discussing some metrics proposed by scholars and standardisation entities through the lenses of the explainability obligations set by the proposed AI Act. Our analysis proposes that metrics to measure the kind of explainability endorsed by the proposed AI Act shall be risk-focused, model-agnostic, goal-aware, intelligible & accessible. This is why we discuss the extent to which these requirements are met by the metrics currently under discussion. △ Less","21 October, 2021",https://arxiv.org/pdf/2110.11168
Evaluation of Various Open-Set Medical Imaging Tasks with Deep Neural Networks,Zongyuan Ge;Xin Wang,"The current generation of deep neural networks has achieved close-to-human results on ""closed-set"" image recognition; that is, the classes being evaluated overlap with the training classes. Many recent methods attempt to address the importance of the unknown, which are termed ""open-set"" recognition algorithms, try to reject unknown classes as well as maintain high recognition accuracy on known classes. However, it is still unclear how different general domain-trained open-set methods from ImageNet would perform on a different but more specific domain, such as the medical domain. Without principled and formal evaluations to measure the effectiveness of those general open-set methods, artificial intelligence (AI)-based medical diagnostics would experience ineffective adoption and increased risks of bad decision making. In this paper, we conduct rigorous evaluations amongst state-of-the-art open-set methods, exploring different open-set scenarios from ""similar-domain"" to ""different-domain"" scenarios and comparing them on various general and medical domain datasets. We summarise the results and core ideas and explain how the models react to various degrees of openness and different distributions of open classes. We show the main difference between general domain-trained and medical domain-trained open-set models with our quantitative and qualitative analysis of the results. We also identify aspects of model robustness in real clinical workflow usage according to confidence calibration and the inference efficiency. △ Less","21 October, 2021",https://arxiv.org/pdf/2110.10888
Frontiers in Evolutionary Computation: A Workshop Report,Tyler Millhouse;Melanie Moses;Melanie Mitchell,"In July of 2021, the Santa Fe Institute hosted a workshop on evolutionary computation as part of its Foundations of Intelligence in Natural and Artificial Systems project. This project seeks to advance the field of artificial intelligence by promoting interdisciplinary research on the nature of intelligence. The workshop brought together computer scientists and biologists to share their insights about the nature of evolution and the future of evolutionary computation. In this report, we summarize each of the talks and the subsequent discussions. We also draw out a number of key themes and identify important frontiers for future research. △ Less","19 October, 2021",https://arxiv.org/pdf/2110.10320
What is Learned in Knowledge Graph Embeddings?,Michael R. Douglas;Michael Simkin;Omri Ben-Eliezer;Tianqi Wu;Peter Chin;Trung V. Dang;Andrew Wood,"A knowledge graph (KG) is a data structure which represents entities and relations as the vertices and edges of a directed graph with edge types. KGs are an important primitive in modern machine learning and artificial intelligence. Embedding-based models, such as the seminal TransE [Bordes et al., 2013] and the recent PairRE [Chao et al., 2020] are among the most popular and successful approaches for representing KGs and inferring missing edges (link completion). Their relative success is often credited in the literature to their ability to learn logical rules between the relations. In this work, we investigate whether learning rules between relations is indeed what drives the performance of embedding-based methods. We define motif learning and two alternative mechanisms, network learning (based only on the connectivity of the KG, ignoring the relation types), and unstructured statistical learning (ignoring the connectivity of the graph). Using experiments on synthetic KGs, we show that KG models can learn motifs and how this ability is degraded by non-motif (noise) edges. We propose tests to distinguish the contributions of the three mechanisms to performance, and apply them to popular KG benchmarks. We also discuss an issue with the standard performance testing protocol and suggest an improvement. To appear in the proceedings of Complex Networks 2021. △ Less","19 October, 2021",https://arxiv.org/pdf/2110.09978
Protecting Anonymous Speech: A Generative Adversarial Network Methodology for Removing Stylistic Indicators in Text,Rishi Balakrishnan;Stephen Sloan;Anil Aswani,"With Internet users constantly leaving a trail of text, whether through blogs, emails, or social media posts, the ability to write and protest anonymously is being eroded because artificial intelligence, when given a sample of previous work, can match text with its author out of hundreds of possible candidates. Existing approaches to authorship anonymization, also known as authorship obfuscation, often focus on protecting binary demographic attributes rather than identity as a whole. Even those that do focus on obfuscating identity require manual feedback, lose the coherence of the original sentence, or only perform well given a limited subset of authors. In this paper, we develop a new approach to authorship anonymization by constructing a generative adversarial network that protects identity and optimizes for three different losses corresponding to anonymity, fluency, and content preservation. Our fully automatic method achieves comparable results to other methods in terms of content preservation and fluency, but greatly outperforms baselines in regards to anonymization. Moreover, our approach is able to generalize well to an open-set context and anonymize sentences from authors it has not encountered before. △ Less","18 October, 2021",https://arxiv.org/pdf/2110.09495
Goal Agnostic Planning using Maximum Likelihood Paths in Hypergraph World Models,Christopher Robinson,"In this paper, we present a hypergraph--based machine learning algorithm, a datastructure--driven maintenance method, and a planning algorithm based on a probabilistic application of Dijkstra's algorithm. Together, these form a goal agnostic automated planning engine for an autonomous learning agent which incorporates beneficial properties of both classical Machine Learning and traditional Artificial Intelligence. We prove that the algorithm determines optimal solutions within the problem space, mathematically bound learning performance, and supply a mathematical model analyzing system state progression through time yielding explicit predictions for learning curves, goal achievement rates, and response to abstractions and uncertainty. To validate performance, we exhibit results from applying the agent to three archetypal planning problems, including composite hierarchical domains, and highlight empirical findings which illustrate properties elucidated in the analysis. △ Less","18 October, 2021",https://arxiv.org/pdf/2110.09442
Streaming Machine Learning and Online Active Learning for Automated Visual Inspection,Jože M. Rožanec;Elena Trajkova;Paulien Dam;Blaž Fortuna;Dunja Mladenić,"Quality control is a key activity performed by manufacturing companies to verify product conformance to the requirements and specifications. Standardized quality control ensures that all the products are evaluated under the same criteria. The decreased cost of sensors and connectivity enabled an increasing digitalization of manufacturing and provided greater data availability. Such data availability has spurred the development of artificial intelligence models, which allow higher degrees of automation and reduced bias when inspecting the products. Furthermore, the increased speed of inspection reduces overall costs and time required for defect inspection. In this research, we compare five streaming machine learning algorithms applied to visual defect inspection with real-world data provided by Philips Consumer Lifestyle BV. Furthermore, we compare them in a streaming active learning context, which reduces the data labeling effort in a real-world context. Our results show that active learning reduces the data labeling effort by almost 15% on average for the worst case, while keeping an acceptable classification performance. The use of machine learning models for automated visual inspection are expected to speed up the quality inspection up to 40%. △ Less","9 December, 2021",https://arxiv.org/pdf/2110.09396
Neuro-Symbolic Forward Reasoning,Hikaru Shindo;Devendra Singh Dhami;Kristian Kersting,"Reasoning is an essential part of human intelligence and thus has been a long-standing goal in artificial intelligence research. With the recent success of deep learning, incorporating reasoning with deep learning systems, i.e., neuro-symbolic AI has become a major field of interest. We propose the Neuro-Symbolic Forward Reasoner (NSFR), a new approach for reasoning tasks taking advantage of differentiable forward-chaining using first-order logic. The key idea is to combine differentiable forward-chaining reasoning with object-centric (deep) learning. Differentiable forward-chaining reasoning computes logical entailments smoothly, i.e., it deduces new facts from given facts and rules in a differentiable manner. The object-centric learning approach factorizes raw inputs into representations in terms of objects. Thus, it allows us to provide a consistent framework to perform the forward-chaining inference from raw inputs. NSFR factorizes the raw inputs into the object-centric representations, converts them into probabilistic ground atoms, and finally performs differentiable forward-chaining inference using weighted rules for inference. Our comprehensive experimental evaluations on object-centric reasoning data sets, 2D Kandinsky patterns and 3D CLEVR-Hans, and a variety of tasks show the effectiveness and advantage of our approach. △ Less","18 October, 2021",https://arxiv.org/pdf/2110.09383
Comparative Analysis of Deep Learning Algorithms for Classification of COVID-19 X-Ray Images,Unsa Maheen;Khawar Iqbal Malik;Gohar Ali,"The Coronavirus was first emerged in December, in the city of China named Wuhan in 2019 and spread quickly all over the world. It has very harmful effects all over the global economy, education, social, daily living and general health of humans. To restrict the quick expansion of the disease initially, main difficulty is to explore the positive corona patients as quickly as possible. As there are no automatic tool kits accessible the requirement for supplementary diagnostic tools has risen up. Previous studies have findings acquired from radiological techniques proposed that this kind of images have important details related to the coronavirus. The usage of modified Artificial Intelligence (AI) system in combination with radio-graphical images can be fruitful for the precise and exact solution of this virus and can also be helpful to conquer the issue of deficiency of professional physicians in distant villages. In our research, we analyze the different techniques for the detection of COVID-19 using X-Ray radiographic images of the chest, we examined the different pre-trained CNN models AlexNet, VGG-16, MobileNet-V2, SqeezeNet, ResNet-34, ResNet-50 and COVIDX-Net to correct analytics for classification system of COVID-19. Our study shows that the pre trained CNN Model with ResNet-34 technique gives the higher accuracy rate of 98.33, 96.77% precision, and 98.36 F1-score, which is better than other CNN techniques. Our model may be helpful for the researchers to fine train the CNN model for the the quick screening of COVID patients. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.09294
Foundations for the Future: Institution building for the purpose of Artificial Intelligence governance,Charlotte Stix,"Governance efforts for artificial intelligence (AI) are taking on increasingly more concrete forms, drawing on a variety of approaches and instruments from hard regulation to standardisation efforts, aimed at mitigating challenges from high-risk AI systems. To implement these and other efforts, new institutions will need to be established on a national and international level. This paper sketches a blueprint of such institutions, and conducts in-depth investigations of three key components of any future AI governance institutions, exploring benefits and associated drawbacks: (1) purpose, relating to the institution's overall goals and scope of work or mandate; (2) geography, relating to questions of participation and the reach of jurisdiction; and (3) capacity, the infrastructural and human make-up of the institution. Subsequently, the paper highlights noteworthy aspects of various institutional roles specifically around questions of institutional purpose, and frames what these could look like in practice, by placing these debates in a European context and proposing different iterations of a European AI Agency. Finally, conclusions and future research directions are proposed. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.09238
Conceptual Modeling and Artificial Intelligence: Mutual Benefits from Complementary Worlds,Dominik Bork,"Conceptual modeling (CM) applies abstraction to reduce the complexity of a system under study (e.g., an excerpt of reality). As a result of the conceptual modeling process a human interpretable, formalized representation (i.e., a conceptual model) is derived which enables understanding and communication among humans, and processing by machines. Artificial Intelligence (AI) algorithms are also applied to complex realities (regularly represented by vast amounts of data) to identify patterns or to classify entities in the data. Aside from the commonalities of both approaches, a significant difference can be observed by looking at the results. While conceptual models are comprehensible, reproducible, and explicit knowledge representations, AI techniques are capable of efficiently deriving an output from a given input while acting as a black box. AI solutions often lack comprehensiveness and reproducibility. Even the developers of AI systems can't explain why a certain output is derived. In the Conceptual Modeling meets Artificial Intelligence (CMAI) workshop, we are interested in tackling the intersection of the two, thus far, mostly isolated approached disciplines of CM and AI. The workshop embraces the assumption, that manifold mutual benefits can be realized by i) investigating what Conceptual Modeling (CM) can contribute to AI, and ii) the other way around, what Artificial Intelligence (AI) can contribute to CM. △ Less","16 October, 2021",https://arxiv.org/pdf/2110.08637
Automated Quality Control of Vacuum Insulated Glazing by Convolutional Neural Network Image Classification,Henrik Riedel;Sleheddine Mokdad;Isabell Schulz;Cenk Kocer;Philipp Rosendahl;Jens Schneider;Michael A. Kraus;Michael Drass,"Vacuum Insulated Glazing (VIG) is a highly thermally insulating window technology, which boasts an extremely thin profile and lower weight as compared to gas-filled insulated glazing units of equivalent performance. The VIG is a double-pane configuration with a submillimeter vacuum gap between the panes and therefore under constant atmospheric pressure over their service life. Small pillars are positioned between the panes to maintain the gap, which can damage the glass reducing the lifetime of the VIG unit. To efficiently assess any surface damage on the glass, an automated damage detection system is highly desirable. For the purpose of classifying the damage, we have developed, trained, and tested a deep learning computer vision system using convolutional neural networks. The classification model flawlessly classified the test dataset with an area under the curve (AUC) for the receiver operating characteristic (ROC) of 100%. We have automatically cropped the images down to their relevant information by using Faster-RCNN to locate the position of the pillars. We employ the state-of-the-art methods Grad-CAM and Score-CAM of explainable Artificial Intelligence (XAI) to provide an understanding of the internal mechanisms and were able to show that our classifier outperforms ResNet50V2 for identification of crack locations and geometry. The proposed methods can therefore be used to detect systematic defects even without large amounts of training data. Further analyses of our model's predictive capabilities demonstrates its superiority over state-of-the-art models (ResNet50V2, ResNet101V2 and ResNet152V2) in terms of convergence speed, accuracy, precision at 100% recall and AUC for ROC. △ Less","15 October, 2021",https://arxiv.org/pdf/2110.08079
Span Detection for Aspect-Based Sentiment Analysis in Vietnamese,Kim Thi-Thanh Nguyen;Sieu Khai Huynh;Luong Luc Phan;Phuc Huynh Pham;Duc-Vu Nguyen;Kiet Van Nguyen,"Aspect-based sentiment analysis plays an essential role in natural language processing and artificial intelligence. Recently, researchers only focused on aspect detection and sentiment classification but ignoring the sub-task of detecting user opinion span, which has enormous potential in practical applications. In this paper, we present a new Vietnamese dataset (UIT-ViSD4SA) consisting of 35,396 human-annotated spans on 11,122 feedback comments for evaluating the span detection in aspect-based sentiment analysis. Besides, we also propose a novel system using Bidirectional Long Short-Term Memory (BiLSTM) with a Conditional Random Field (CRF) layer (BiLSTM-CRF) for the span detection task in Vietnamese aspect-based sentiment analysis. The best result is a 62.76% F1 score (macro) for span detection using BiLSTM-CRF with embedding fusion of syllable embedding, character embedding, and contextual embedding from XLM-RoBERTa. In future work, span detection will be extended in many NLP tasks such as constructive detection, emotion recognition, complaint analysis, and opinion mining. Our dataset is freely available at https://github.com/kimkim00/UIT-ViSD4SA for research purposes. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.07833
Adversarial Scene Reconstruction and Object Detection System for Assisting Autonomous Vehicle,Md Foysal Haque;Hay-Youn Lim;Dae-Seong Kang,"In the current computer vision era classifying scenes through video surveillance systems is a crucial task. Artificial Intelligence (AI) Video Surveillance technologies have been advanced remarkably while artificial intelligence and deep learning ascended into the system. Adopting the superior compounds of deep learning visual classification methods achieved enormous accuracy in classifying visual scenes. However, the visual classifiers face difficulties examining the scenes in dark visible areas, especially during the nighttime. Also, the classifiers face difficulties in identifying the contexts of the scenes. This paper proposed a deep learning model that reconstructs dark visual scenes to clear scenes like daylight, and the method recognizes visual actions for the autonomous vehicle. The proposed model achieved 87.3 percent accuracy for scene reconstruction and 89.2 percent in scene understanding and detection tasks. △ Less","13 October, 2021",https://arxiv.org/pdf/2110.07716
Predicting Solar Flares with Remote Sensing and Machine Learning,Erik Larsen,"High energy solar flares and coronal mass ejections have the potential to destroy Earth's ground and satellite infrastructures, causing trillions of dollars in damage and mass human suffering. Destruction of these critical systems would disable power grids and satellites, crippling communications and transportation. This would lead to food shortages and an inability to respond to emergencies. A solution to this impending problem is proposed herein using satellites in solar orbit that continuously monitor the Sun, use artificial intelligence and machine learning to calculate the probability of massive solar explosions from this sensed data, and then signal defense mechanisms that will mitigate the threat. With modern technology there may be only safeguards that can be implemented with enough warning, which is why the best algorithm must be identified and continuously trained with existing and new data to maximize true positive rates while minimizing false negatives. This paper conducts a survey of current machine learning models using open source solar flare prediction data. The rise of edge computing allows machine learning hardware to be placed on the same satellites as the sensor arrays, saving critical time by not having to transmit remote sensing data across the vast distances of space. A system of systems approach will allow enough warning for safety measures to be put into place mitigating the risk of disaster. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.07658
3D Structure from 2D Microscopy images using Deep Learning,Benjamin J. Blundell;Christian Sieben;Suliana Manley;Ed Rosten;QueeLim Ch'ng;Susan Cox,"Understanding the structure of a protein complex is crucial indetermining its function. However, retrieving accurate 3D structures from microscopy images is highly challenging, particularly as many imaging modalities are two-dimensional. Recent advances in Artificial Intelligence have been applied to this problem, primarily using voxel based approaches to analyse sets of electron microscopy images. Herewe present a deep learning solution for reconstructing the protein com-plexes from a number of 2D single molecule localization microscopy images, with the solution being completely unconstrained. Our convolutional neural network coupled with a differentiable renderer predicts pose and derives a single structure. After training, the network is dis-carded, with the output of this method being a structural model which fits the data-set. We demonstrate the performance of our system on two protein complexes: CEP152 (which comprises part of the proximal toroid of the centriole) and centrioles. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.07608
Connection Management xAPP for O-RAN RIC: A Graph Neural Network and Reinforcement Learning Approach,Oner Orhan;Vasuki Narasimha Swamy;Thomas Tetzlaff;Marcel Nassar;Hosein Nikopour;Shilpa Talwar,"Connection management is an important problem for any wireless network to ensure smooth and well-balanced operation throughout. Traditional methods for connection management (specifically user-cell association) consider sub-optimal and greedy solutions such as connection of each user to a cell with maximum receive power. However, network performance can be improved by leveraging machine learning (ML) and artificial intelligence (AI) based solutions. The next generation software defined 5G networks defined by the Open Radio Access Network (O-RAN) alliance facilitates the inclusion of ML/AI based solutions for various network problems. In this paper, we consider intelligent connection management based on the O-RAN network architecture to optimize user association and load balancing in the network. We formulate connection management as a combinatorial graph optimization problem. We propose a deep reinforcement learning (DRL) solution that uses the underlying graph to learn the weights of the graph neural networks (GNN) for optimal user-cell association. We consider three candidate objective functions: sum user throughput, cell coverage, and load balancing. Our results show up to 10% gain in throughput, 45-140% gain cell coverage, 20-45% gain in load balancing depending on network deployment configurations compared to baseline greedy techniques. △ Less","20 October, 2021",https://arxiv.org/pdf/2110.07525
Federated Learning for COVID-19 Detection with Generative Adversarial Networks in Edge Cloud Computing,Dinh C. Nguyen;Ming Ding;Pubudu N. Pathirana;Aruna Seneviratne;Albert Y. Zomaya,"COVID-19 has spread rapidly across the globe and become a deadly pandemic. Recently, many artificial intelligence-based approaches have been used for COVID-19 detection, but they often require public data sharing with cloud datacentres and thus remain privacy concerns. This paper proposes a new federated learning scheme, called FedGAN, to generate realistic COVID-19 images for facilitating privacy-enhanced COVID-19 detection with generative adversarial networks (GANs) in edge cloud computing. Particularly, we first propose a GAN where a discriminator and a generator based on convolutional neural networks (CNNs) at each edge-based medical institution alternatively are trained to mimic the real COVID-19 data distribution. Then, we propose a new federated learning solution which allows local GANs to collaborate and exchange learned parameters with a cloud server, aiming to enrich the global GAN model for generating realistic COVID-19 images without the need for sharing actual data. To enhance the privacy in federated COVID-19 data analytics, we integrate a differential privacy solution at each hospital institution. Moreover, we propose a new blockchain-based FedGAN framework for secure COVID-19 data analytics, by decentralizing the FL process with a new mining solution for low running latency. Simulations results demonstrate the superiority of our approach for COVID-19 detection over the state-of-the-art schemes. △ Less","13 October, 2021",https://arxiv.org/pdf/2110.07136
An algorithm for a fairer and better voting system,Gabriel-Claudiu Grama,"The major finding, of this article, is an ensemble method, but more exactly, a novel, better ranked voting system (and other variations of it), that aims to solve the problem of finding the best candidate to represent the voters. We have the source code on GitHub, for making realistic simulations of elections, based on artificial intelligence for comparing different variations of the algorithm, and other already known algorithms. We have convincing evidence that our algorithm is better than Instant-Runoff Voting, Preferential Block Voting, Single Transferable Vote, and First Past The Post (if certain, natural conditions are met, to support the wisdom of the crowds). By also comparing with the best voter, we demonstrated the wisdom of the crowds, suggesting that democracy (distributed system) is a better option than dictatorship (centralized system), if those certain, natural conditions are met. Voting systems are not restricted to politics, they are ensemble methods for artificial intelligence, but the context of this article is natural intelligence. It is important to find a system that is fair (e.g. freedom of expression on the ballot exists), especially when the outcome of the voting system has social impact: some voting systems have the unfair inevitability to trend (over time) towards the same two major candidates (Duverger's law). △ Less","13 October, 2021",https://arxiv.org/pdf/2110.07066
Investigating Health-Aware Smart-Nudging with Machine Learning to Help People Pursue Healthier Eating-Habits,Mansura A Khan;Khalil Muhammad;Barry Smyth;David Coyle,"Food-choices and eating-habits directly contribute to our long-term health. This makes the food recommender system a potential tool to address the global crisis of obesity and malnutrition. Over the past decade, artificial-intelligence and medical researchers became more invested in researching tools that can guide and help people make healthy and thoughtful decisions around food and diet. In many typical (Recommender System) RS domains, smart nudges have been proven effective in shaping users' consumption patterns. In recent years, knowledgeable nudging and incentifying choices started getting attention in the food domain as well. To develop smart nudging for promoting healthier food choices, we combined Machine Learning and RS technology with food-healthiness guidelines from recognized health organizations, such as the World Health Organization, Food Standards Agency, and the National Health Service United Kingdom. In this paper, we discuss our research on, persuasive visualization for making users aware of the healthiness of the recommended recipes. Here, we propose three novel nudging technology, the WHO-BubbleSlider, the FSA-ColorCoading, and the DRCI-MLCP, that encourage users to choose healthier recipes. We also propose a Topic Modeling based portion-size recommendation algorithm. To evaluate our proposed smart-nudges, we conducted an online user study with 96 participants and 92250 recipes. Results showed that, during the food decision-making process, appropriate healthiness cues make users more likely to click, browse, and choose healthier recipes over less healthy ones. △ Less","5 October, 2021",https://arxiv.org/pdf/2110.07045
A Novel Clustering-Based Algorithm for Continuous and Non-invasive Cuff-Less Blood Pressure Estimation,Ali Farki;Reza Baradaran Kazemzadeh;Elham Akhondzadeh Noughabi,"Extensive research has been performed on continuous, non-invasive, cuffless blood pressure (BP) measurement using artificial intelligence algorithms. This approach involves extracting certain features from physiological signals like ECG, PPG, ICG, BCG, etc. as independent variables and extracting features from Arterial Blood Pressure (ABP) signals as dependent variables, and then using machine learning algorithms to develop a blood pressure estimation model based on these data. The greatest challenge of this field is the insufficient accuracy of estimation models. This paper proposes a novel blood pressure estimation method with a clustering step for accuracy improvement. The proposed method involves extracting Pulse Transit Time (PTT), PPG Intensity Ratio (PIR), and Heart Rate (HR) features from Electrocardiogram (ECG) and Photoplethysmogram (PPG) signals as the inputs of clustering and regression, extracting Systolic Blood Pressure (SBP) and Diastolic Blood Pressure (DBP) features from ABP signals as dependent variables, and finally developing regression models by applying Gradient Boosting Regression (GBR), Random Forest Regression (RFR), and Multilayer Perceptron Regression (MLP) on each cluster. The method was implemented using the MIMICII dataset with the silhouette criterion used to determine the optimal number of clusters. The results showed that because of the inconsistency, high dispersion, and multi-trend behavior of the extracted features vectors, the accuracy can be significantly improved by running a clustering algorithm and then developing a regression model on each cluster, and finally weighted averaging of the results based on the error of each cluster. When implemented with 5 clusters and GBR, this approach yielded an MAE of 2.56 for SBP estimates and 2.23 for DBP estimates, which were significantly better than the best results without clustering (DBP: 6.27, SBP: 6.36). △ Less","27 December, 2021",https://arxiv.org/pdf/2110.06996
Systematic Inequalities in Language Technology Performance across the World's Languages,Damián Blasi;Antonios Anastasopoulos;Graham Neubig,"Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world's 6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as more linguistic NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. △ Less","13 October, 2021",https://arxiv.org/pdf/2110.06733
Logic Constraints to Feature Importances,Nicola Picchiotti;Marco Gori,"In recent years, Artificial Intelligence (AI) algorithms have been proven to outperform traditional statistical methods in terms of predictivity, especially when a large amount of data was available. Nevertheless, the ""black box"" nature of AI models is often a limit for a reliable application in high-stakes fields like diagnostic techniques, autonomous guide, etc. Recent works have shown that an adequate level of interpretability could enforce the more general concept of model trustworthiness. The basic idea of this paper is to exploit the human prior knowledge of the features' importance for a specific task, in order to coherently aid the phase of the model's fitting. This sort of ""weighted"" AI is obtained by extending the empirical loss with a regularization term encouraging the importance of the features to follow predetermined constraints. This procedure relies on local methods for the feature importance computation, e.g. LRP, LIME, etc. that are the link between the model weights to be optimized and the user-defined constraints on feature importance. In the fairness area, promising experimental results have been obtained for the Adult dataset. Many other possible applications of this model agnostic theoretical framework are described. △ Less","13 October, 2021",https://arxiv.org/pdf/2110.06596
A Brief Introduction to Automatic Differentiation for Machine Learning,Davan Harrison,"Machine learning and neural network models in particular have been improving the state of the art performance on many artificial intelligence related tasks. Neural network models are typically implemented using frameworks that perform gradient based optimization methods to fit a model to a dataset. These frameworks use a technique of calculating derivatives called automatic differentiation (AD) which removes the burden of performing derivative calculations from the model designer. In this report we describe AD, its motivations, and different implementation approaches. We briefly describe dataflow programming as it relates to AD. Lastly, we present example programs that are implemented with Tensorflow and PyTorch, which are two commonly used AD frameworks. △ Less","14 October, 2021",https://arxiv.org/pdf/2110.06209
Hotel Preference Rank based on Online Customer Review,Muhammad Apriandito Arya Saputra;Andry Alamsyah;Fajar Ibnu Fatihan,"Topline hotels are now shifting into the digital way in how they understand their customers to maintain and ensuring satisfaction. Rather than the conventional way which uses written reviews or interviews, the hotel is now heavily investing in Artificial Intelligence particularly Machine Learning solutions. Analysis of online customer reviews changes the way companies make decisions in a more effective way than using conventional analysis. The purpose of this research is to measure hotel service quality. The proposed approach emphasizes service quality dimensions reviews of the top-5 luxury hotel in Indonesia that appear on the online travel site TripAdvisor based on section Best of 2018. In this research, we use a model based on a simple Bayesian classifier to classify each customer review into one of the service quality dimensions. Our model was able to separate each classification properly by accuracy, kappa, recall, precision, and F-measure measurements. To uncover latent topics in the customer's opinion we use Topic Modeling. We found that the common issue that occurs is about responsiveness as it got the lowest percentage compared to others. Our research provides a faster outlook of hotel rank based on service quality to end customers based on a summary of the previous online review. △ Less","10 October, 2021",https://arxiv.org/pdf/2110.06133
Downtime-Aware O-RAN VNF Deployment Strategy for Optimized Self-Healing in the O-Cloud,Ibrahim Tamim;Anas Saci;Manar Jammal;Abdallah Shami,"Due to the huge surge in the traffic of IoT devices and applications, mobile networks require a new paradigm shift to handle such demand roll out. With the 5G economics, those networks should provide virtualized multi-vendor and intelligent systems that can scale and efficiently optimize the investment of the underlying infrastructure. Therefore, the market stakeholders have proposed the Open Radio Access Network (O-RAN) as one of the solutions to improve the network performance, agility, and time-to-market of new applications. O-RAN harnesses the power of artificial intelligence, cloud computing, and new network technologies (NFV and SDN) to allow operators to manage their infrastructure in a cost-efficient manner. Therefore, it is necessary to address the O-RAN performance and availability challenges autonomously while maintaining the quality of service. In this work, we propose an optimized deployment strategy for the virtualized O-RAN units in the O-Cloud to minimize the network's outage while complying with the performance and operational requirements. The model's evaluation provides an optimal deployment strategy that maximizes the network's overall availability and adheres to the O-RAN-specific requirements. △ Less","12 November, 2021",https://arxiv.org/pdf/2110.06060
A Deployment Model to Extend Ethically Aligned AI Implementation Method ECCOLA,Jani Antikainen;Mamia Agbese;Hanna-Kaisa Alanen;Erika Halme;Hannakaisa Isomäki;Marianna Jantunen;Kai-Kristian Kemell;Rebekah Rousi;Heidi Vainio-Pekka;Ville Vakkuri,"There is a struggle in Artificial intelligence (AI) ethics to gain ground in actionable methods and models to be utilized by practitioners while developing and implementing ethically sound AI systems. AI ethics is a vague concept without a consensus of definition or theoretical grounding and bearing little connection to practice. Practice involving primarily technical tasks like software development is not aptly equipped to process and decide upon ethical considerations. Efforts to create tools and guidelines to help people working with AI development have been concentrating almost solely on the technical aspects of AI. A few exceptions do apply, such as the ECCOLA method for creating ethically aligned AI -systems. ECCOLA has proven results in terms of increased ethical considerations in AI systems development. Yet, it is a novel innovation, and room for development still exists. This study aims to extend ECCOLA with a deployment model to drive the adoption of ECCOLA, as any method, no matter how good, is of no value without adoption and use. The model includes simple metrics to facilitate the communication of ethical gaps or outcomes of ethical AI development. It offers the opportunity to assess any AI system at any given lifecycle phase, e.g., opening possibilities like analyzing the ethicality of an AI system under acquisition. △ Less","12 October, 2021",https://arxiv.org/pdf/2110.05933
HUNTER: AI based Holistic Resource Management for Sustainable Cloud Computing,Shreshth Tuli;Sukhpal Singh Gill;Minxian Xu;Peter Garraghan;Rami Bahsoon;Schahram Dustdar;Rizos Sakellariou;Omer Rana;Rajkumar Buyya;Giuliano Casale;Nicholas R. Jennings,"The worldwide adoption of cloud data centers (CDCs) has given rise to the ubiquitous demand for hosting application services on the cloud. Further, contemporary data-intensive industries have seen a sharp upsurge in the resource requirements of modern applications. This has led to the provisioning of an increased number of cloud servers, giving rise to higher energy consumption and, consequently, sustainability concerns. Traditional heuristics and reinforcement learning based algorithms for energy-efficient cloud resource management address the scalability and adaptability related challenges to a limited extent. Existing work often fails to capture dependencies across thermal characteristics of hosts, resource consumption of tasks and the corresponding scheduling decisions. This leads to poor scalability and an increase in the compute resource requirements, particularly in environments with non-stationary resource demands. To address these limitations, we propose an artificial intelligence (AI) based holistic resource management technique for sustainable cloud computing called HUNTER. The proposed model formulates the goal of optimizing energy efficiency in data centers as a multi-objective scheduling problem, considering three important models: energy, thermal and cooling. HUNTER utilizes a Gated Graph Convolution Network as a surrogate model for approximating the Quality of Service (QoS) for a system state and generating optimal scheduling decisions. Experiments on simulated and physical cloud environments using the CloudSim toolkit and the COSCO framework show that HUNTER outperforms state-of-the-art baselines in terms of energy consumption, SLA violation, scheduling time, cost and temperature by up to 12, 35, 43, 54 and 3 percent respectively. △ Less","28 October, 2021",https://arxiv.org/pdf/2110.05529
An In-depth Summary of Recent Artificial Intelligence Applications in Drug Design,Yi Zhang,"As a promising tool to navigate in the vast chemical space, artificial intelligence (AI) is leveraged for drug design. From the year 2017 to 2021, the number of applications of several recent AI models (i.e. graph neural network (GNN), recurrent neural network (RNN), variation autoencoder (VAE), generative adversarial network (GAN), flow and reinforcement learning (RL)) in drug design increases significantly. Many relevant literature reviews exist. However, none of them provides an in-depth summary of many applications of the recent AI models in drug design. To complement the existing literature, this survey includes the theoretical development of the previously mentioned AI models and detailed summaries of 42 recent applications of AI in drug design. Concretely, 13 of them leverage GNN for molecular property prediction and 29 of them use RL and/or deep generative models for molecule generation and optimization. In most cases, the focus of the summary is the models, their variants, and modifications for specific tasks in drug design. Moreover, 60 additional applications of AI in molecule generation and optimization are briefly summarized in a table. Finally, this survey provides a holistic discussion of the abundant applications so that the tasks, potential solutions, and challenges in AI-based drug design become evident. △ Less","9 October, 2021",https://arxiv.org/pdf/2110.05478
"All One Needs to Know about Metaverse: A Complete Survey on Technological Singularity, Virtual Ecosystem, and Research Agenda",Lik-Hang Lee;Tristan Braud;Pengyuan Zhou;Lin Wang;Dianlei Xu;Zijun Lin;Abhishek Kumar;Carlos Bermejo;Pan Hui,"Since the popularisation of the Internet in the 1990s, the cyberspace has kept evolving. We have created various computer-mediated virtual environments including social networks, video conferencing, virtual 3D worlds (e.g., VR Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and unconnected, have bought us various degrees of digital transformation. The term `metaverse' has been coined to further facilitate the digital transformation in every aspect of our physical lives. At the core of the metaverse stands the vision of an immersive Internet as a gigantic, unified, persistent, and shared realm. While the metaverse may seem futuristic, catalysed by emerging technologies such as Extended Reality, 5G, and Artificial Intelligence, the digital `big bang' of our cyberspace is not far away. This survey paper presents the first effort to offer a comprehensive framework that examines the latest metaverse development under the dimensions of state-of-the-art technologies and metaverse ecosystems, and illustrates the possibility of the digital `big bang'. First, technologies are the enablers that drive the transition from the current Internet to the metaverse. We thus examine eight enabling technologies rigorously - Extended Reality, User Interactivity (Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks. In terms of applications, the metaverse ecosystem allows human users to live and play within a self-sustaining, persistent, and shared realm. Therefore, we discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy, Social Acceptability, Security and Privacy, and Trust and Accountability. Finally, we propose a concrete research agenda for the development of the metaverse. △ Less","3 November, 2021",https://arxiv.org/pdf/2110.05352
"Compositionality as we see it, everywhere around us",Bob Coecke,"There are different meanings of the term ""compositionality"" within science: what one researcher would call compositional, is not at all compositional for another researcher. The most established conception is usually attributed to Frege, and is characterised by a bottom-up flow of meanings: the meaning of the whole can be derived from the meanings of the parts, and how these parts are structured together. Inspired by work on compositionality in quantum theory, and categorical quantum mechanics in particular, we propose the notions of Schrodinger, Whitehead, and complete compositionality. Accounting for recent important developments in quantum technology and artificial intelligence, these do not have the bottom-up meaning flow as part of their definitions. Schrodinger compositionality accommodates quantum theory, and also meaning-as-context. Complete compositionality further strengthens Schrodinger compositionality in order to single out theories like ZX-calculus, that are complete with regard to the intended model. All together, our new notions aim to capture the fact that compositionality is at its best when it is `real', `non-trivial', and even more when it also is `complete'. At this point we only put forward the intuitive and/or restricted formal definitions, and leave a fully comprehensive definition to future collaborative work. △ Less","25 October, 2021",https://arxiv.org/pdf/2110.05327
A Survey on Proactive Customer Care: Enabling Science and Steps to Realize it,Viswanath Ganapathy;Sauptik Dhar;Olimpiya Saha;Pelin Kurt Garberson;Javad Heydari;Mohak Shah,"In recent times, advances in artificial intelligence (AI) and IoT have enabled seamless and viable maintenance of appliances in home and building environments. Several studies have shown that AI has the potential to provide personalized customer support which could predict and avoid errors more reliably than ever before. In this paper, we have analyzed the various building blocks needed to enable a successful AI-driven predictive maintenance use-case. Unlike, existing surveys which mostly provide a deep dive into the recent AI algorithms for Predictive Maintenance (PdM), our survey provides the complete view; starting from business impact to recent technology advancements in algorithms as well as systems research and model deployment. Furthermore, we provide exemplar use-cases on predictive maintenance of appliances using publicly available data sets. Our survey can serve as a template needed to design a successful predictive maintenance use-case. Finally, we touch upon existing public data sources and provide a step-wise breakdown of an AI-driven proactive customer care (PCC) use-case, starting from generic anomaly detection to fault prediction and finally root-cause analysis. We highlight how such a step-wise approach can be advantageous for accurate model building and helpful for gaining insights into predictive maintenance of electromechanical appliances. △ Less","11 October, 2021",https://arxiv.org/pdf/2110.05015
Perceptions and attitudes of Children and Young People to Artificial Intelligence in Medicine,Sheena Visram;Deirdre Leyden;Oceiah Annesley;Dauda Bappa;Neil J Sebire,"There is increasing interest in Artificial Intelligence and its application to medicine. Perceptions are less well-known, notably amongst children and young people. 21 members of a Young Persons Advisory Group for research, recommend creating an enabling environment with children and young people, through educational workshops with practical examples that use Artificial Intelligence to help, but not replace humans, address issues, build trust, and effectively communicate about potential opportunities. △ Less","10 October, 2021",https://arxiv.org/pdf/2110.04890
Using Human-Guided Causal Knowledge for More Generalized Robot Task Planning,Semir Tatlidil;Yanqi Liu;Emily Sheetz;R. Iris Bahar;Steven Sloman,"A major challenge in research involving artificial intelligence (AI) is the development of algorithms that can find solutions to problems that can generalize to different environments and tasks. Unlike AI, humans are adept at finding solutions that can transfer. We hypothesize this is because their solutions are informed by causal models. We propose to use human-guided causal knowledge to help robots find solutions that can generalize to a new environment. We develop and test the feasibility of a language interface that naïve participants can use to communicate these causal models to a planner. We find preliminary evidence that participants are able to use our interface and generate causal models that achieve near-generalization. We outline an experiment aimed at testing far-generalization using our interface and describe our longer terms goals for these causal models. △ Less","9 October, 2021",https://arxiv.org/pdf/2110.04664
Embed Everything: A Method for Efficiently Co-Embedding Multi-Modal Spaces,Sarah Di;Robin Yu;Amol Kapoor,"Any general artificial intelligence system must be able to interpret, operate on, and produce data in a multi-modal latent space that can represent audio, imagery, text, and more. In the last decade, deep neural networks have seen remarkable success in unimodal data distributions, while transfer learning techniques have seen a massive expansion of model reuse across related domains. However, training multi-modal networks from scratch remains expensive and illusive, while heterogeneous transfer learning (HTL) techniques remain relatively underdeveloped. In this paper, we propose a novel and cost-effective HTL strategy for co-embedding multi-modal spaces. Our method avoids cost inefficiencies by preprocessing embeddings using pretrained models for all components, without passing gradients through these models. We prove the use of this system in a joint image-audio embedding task. Our method has wide-reaching applications, as successfully bridging the gap between different latent spaces could provide a framework for the promised ""universal"" embedding. △ Less","9 October, 2021",https://arxiv.org/pdf/2110.04599
EnsembleNTLDetect: An Intelligent Framework for Electricity Theft Detection in Smart Grid,Yogesh Kulkarni;Sayf Hussain Z;Krithi Ramamritham;Nivethitha Somu,"Artificial intelligence-based techniques applied to the electricity consumption data generated from the smart grid prove to be an effective solution in reducing Non Technical Loses (NTLs), thereby ensures safety, reliability, and security of the smart energy systems. However, imbalanced data, consecutive missing values, large training times, and complex architectures hinder the real time application of electricity theft detection models. In this paper, we present EnsembleNTLDetect, a robust and scalable electricity theft detection framework that employs a set of efficient data pre-processing techniques and machine learning models to accurately detect electricity theft by analysing consumers' electricity consumption patterns. This framework utilises an enhanced Dynamic Time Warping Based Imputation (eDTWBI) algorithm to impute missing values in the time series data and leverages the Near-miss undersampling technique to generate balanced data. Further, stacked autoencoder is introduced for dimensionality reduction and to improve training efficiency. A Conditional Generative Adversarial Network (CTGAN) is used to augment the dataset to ensure robust training and a soft voting ensemble classifier is designed to detect the consumers with aberrant consumption patterns. Furthermore, experiments were conducted on the real-time electricity consumption data provided by the State Grid Corporation of China (SGCC) to validate the reliability and efficiency of EnsembleNTLDetect over the state-of-the-art electricity theft detection models in terms of various quality metrics. △ Less","9 October, 2021",https://arxiv.org/pdf/2110.04502
Towards AI Logic for Social Reasoning,Huimin Dong;Réka Markovich;Leendert van der Torre,"Artificial Intelligence (AI) logic formalizes the reasoning of intelligent agents. In this paper, we discuss how an argumentation-based AI logic could be used also to formalize important aspects of social reasoning. Besides reasoning about the knowledge and actions of individual agents, social AI logic can reason also about social dependencies among agents using the rights, obligations and permissions of the agents. We discuss four aspects of social AI logic. First, we discuss how rights represent relations between the obligations and permissions of intelligent agents. Second, we discuss how to argue about the right-to-know, a central issue in the recent discussion of privacy and ethics. Third, we discuss how a wide variety of conflicts among intelligent agents can be identified and (sometimes) resolved by comparing formal arguments. Importantly, to cover a wide range of arguments occurring in daily life, also fallacious arguments can be represented and reasoned about. Fourth, we discuss how to argue about the freedom to act for intelligent agents. Examples from social, legal and ethical reasoning highlight the challenges in developing social AI logic. The discussion of the four challenges leads to a research program for argumentation-based social AI logic, contributing towards the future development of AI logic. △ Less","9 October, 2021",https://arxiv.org/pdf/2110.04452
Multi-Agent Autonomy: Advancements and Challenges in Subterranean Exploration,Michael T. Ohradzansky;Eugene R. Rush;Danny G. Riley;Andrew B. Mills;Shakeeb Ahmad;Steve McGuire;Harel Biggie;Kyle Harlow;Michael J. Miles;Eric W. Frew;Christoffer Heckman;J. Sean Humbert,"Artificial intelligence has undergone immense growth and maturation in recent years, though autonomous systems have traditionally struggled when fielded in diverse and previously unknown environments. DARPA is seeking to change that with the Subterranean Challenge, by providing roboticists the opportunity to support civilian and military first responders in complex and high-risk underground scenarios. The subterranean domain presents a handful of challenges, such as limited communication, diverse topology and terrain, and degraded sensing. Team MARBLE proposes a solution for autonomous exploration of unknown subterranean environments in which coordinated agents search for artifacts of interest. The team presents two navigation algorithms in the form of a metric-topological graph-based planner and a continuous frontier-based planner. To facilitate multi-agent coordination, agents share and merge new map information and candidate goal-points. Agents deploy communication beacons at different points in the environment, extending the range at which maps and other information can be shared. Onboard autonomy reduces the load on human supervisors, allowing agents to detect and localize artifacts and explore autonomously outside established communication networks. Given the scale, complexity, and tempo of this challenge, a range of lessons were learned, most importantly, that frequent and comprehensive field testing in representative environments is key to rapidly refining system performance. △ Less","8 October, 2021",https://arxiv.org/pdf/2110.04390
Landslide Detection in Real-Time Social Media Image Streams,Ferda Ofli;Muhammad Imran;Umair Qazi;Julien Roch;Catherine Pennington;Vanessa J. Banks;Remy Bossu,"Lack of global data inventories obstructs scientific modeling of and response to landslide hazards which are oftentimes deadly and costly. To remedy this limitation, new approaches suggest solutions based on citizen science that requires active participation. However, as a non-traditional data source, social media has been increasingly used in many disaster response and management studies in recent years. Inspired by this trend, we propose to capitalize on social media data to mine landslide-related information automatically with the help of artificial intelligence (AI) techniques. Specifically, we develop a state-of-the-art computer vision model to detect landslides in social media image streams in real time. To that end, we create a large landslide image dataset labeled by experts and conduct extensive model training experiments. The experimental results indicate that the proposed model can be deployed in an online fashion to support global landslide susceptibility maps and emergency response. △ Less","3 October, 2021",https://arxiv.org/pdf/2110.04080
Proposal of Analog In-Memory Computing with Magnified Tunnel Magnetoresistance Ratio and Universal STT-MRAM Cell,Hao Cai;Yanan Guo;Bo Liu;Mingyang Zhou;Juntong Chen;Xinning Liu;Jun Yang,"In-memory computing (IMC) is an effectual solution for energy-efficient artificial intelligence applications. Analog IMC amortizes the power consumption of multiple sensing amplifiers with analog-to-digital converter (ADC), and simultaneously completes the calculation of multi-line data with high parallelism degree. Based on a universal one-transistor one-magnetic tunnel junction (MTJ) spin transfer torque magnetic RAM (STT-MRAM) cell, this paper demonstrates a novel tunneling magnetoresistance (TMR) ratio magnifying method to realize analog IMC. Previous concerns include low TMR ratio and analog calculation nonlinearity are addressed using device-circuit interaction. Peripheral circuits are minimally modified to enable in-memory matrix-vector multiplication. A current mirror with feedback structure is implemented to enhance analog computing linearity and calculation accuracy. The proposed design maximumly supports 1024 2-bit input and 1-bit weight multiply-and-accumulate (MAC) computations simultaneously. The 2-bit input is represented by the width of the input (IN) pulses, while the 1-bit weight is stored in STT-MRAM and the x7500 magnified TMR (m-TMR) ratio is obtained by latching. The proposal is simulated using 28-nm CMOS process and MTJ compact model. The integral nonlinearity is reduced by 57.6% compared with the conventional structure. 9.47-25.4 TOPS/W is realized with 2-bit input, 1-bit weight and 4-bit output convolution neural network (CNN). △ Less","8 October, 2021",https://arxiv.org/pdf/2110.03937
Developing Medical AI : a cloud-native audio-visual data collection study,Sagi Schein;Greg Arutiunian;Vitaly Burshtein;Gal Sadeh;Michelle Townshend;Bruce Friedman;Shada Sadr-azodi,"Designing Artificial Intelligence (AI) solutions that can operate in real-world situations is a highly complex task. Deploying such solutions in the medical domain is even more challenging. The promise of using AI to improve patient care and reduce cost has encouraged many companies to undertake such endeavours. For our team, the goal has been to improve early identification of deteriorating patients in the hospital. Identifying patient deterioration in lower acuity wards relies, to a large degree on the attention and intuition of clinicians, rather than on the presence of physiological monitoring devices. In these care areas, an automated tool which could continuously observe patients and notify the clinical staff of suspected deterioration, would be extremely valuable. In order to develop such an AI-enabled tool, a large collection of patient images and audio correlated with corresponding vital signs, past medical history and clinical outcome would be indispensable. To the best of our knowledge, no such public or for-pay data set currently exists. This lack of audio-visual data led to the decision to conduct exactly such study. The main contributions of this paper are, the description of a protocol for audio-visual data collection study, a cloud-architecture for efficiently processing and consuming such data, and the design of a specific data collection device. △ Less","17 August, 2021",https://arxiv.org/pdf/2110.03660
Human in the Loop for Machine Creativity,Neo Christopher Chung,"Artificial intelligence (AI) is increasingly utilized in synthesizing visuals, texts, and audio. These AI-based works, often derived from neural networks, are entering the mainstream market, as digital paintings, songs, books, and others. We conceptualize both existing and future human-in-the-loop (HITL) approaches for creative applications and to develop more expressive, nuanced, and multimodal models. Particularly, how can our expertise as curators and collaborators be encoded in AI models in an interactive manner? We examine and speculate on long term implications for models, interfaces, and machine creativity. Our selection, creation, and interpretation of AI art inherently contain our emotional responses, cultures, and contexts. Therefore, the proposed HITL may help algorithms to learn creative processes that are much harder to codify or quantify. We envision multimodal HITL processes, where texts, visuals, sounds, and other information are coupled together, with automated analysis of humans and environments. Overall, these HITL approaches will increase interaction between human and AI, and thus help the future AI systems to better understand our own creative and emotional processes. △ Less","7 October, 2021",https://arxiv.org/pdf/2110.03569
Data-Driven Methods for Balancing Fairness and Efficiency in Ride-Pooling,Naveen Raman;Sanket Shah;John Dickerson,"Rideshare and ride-pooling platforms use artificial intelligence-based matching algorithms to pair riders and drivers. However, these platforms can induce inequality either through an unequal income distribution or disparate treatment of riders. We investigate two methods to reduce forms of inequality in ride-pooling platforms: (1) incorporating fairness constraints into the objective function and (2) redistributing income to drivers to reduce income fluctuation and inequality. To evaluate our solutions, we use the New York City taxi data set. For the first method, we find that optimizing for driver-side fairness outperforms state-of-the-art models on the number of riders serviced, both in the worst-off neighborhood and overall, showing that optimizing for fairness can assist profitability in certain circumstances. For the second method, we explore income redistribution as a way to combat income inequality by having drivers keep an r fraction of their income, and contributing the rest to a redistribution pool. For certain values of r, most drivers earn near their Shapley value, while still incentivizing drivers to maximize value, thereby avoiding the free-rider problem and reducing income variability. The first method can be extended to many definitions of fairness and the second method provably improves fairness without affecting profitability. △ Less","7 October, 2021",https://arxiv.org/pdf/2110.03524
Design of an Intelligent Vision Algorithm for Recognition and Classification of Apples in an Orchard Scene,Hamid Majidi Balanji;Alaeedin Rahmani Didar;Mohamadali Hadad Derafshi,"Apple is one of the remarkable fresh fruit that contains a high degree of nutritious and medicinal value. Hand harvesting of apples by seasonal farmworkers increases physical damages on the surface of these fruits, which causes a great loss in marketing quality. The main objective of this study is focused on designing a robust vision algorithm for robotic apple harvesters. The proposed algorithm is able to recognize and classify 4-classes of objects found in an orchard scene including apples, leaves, trunk and branches, and sky into two apples and non-apples classes. 100 digital images of Red Delicious apples and 100 digital images of Golden Delicious apples were selected among 1000 captured images of apples from 18 apple gardens in West Azerbaijan, Iran. An image processing algorithm is proposed for segmentation and extraction of the image classes based on the color characteristics of mentioned classes. Invariant-Momentums were chosen as the extracted features from the segmented classes, e.g. apples. Multilayer Feedforward Neural Networks, MFNNs, were used as an artificial intelligence tool for the recognition and classification of image classes. △ Less","7 October, 2021",https://arxiv.org/pdf/2110.03232
Towards efficient end-to-end speech recognition with biologically-inspired neural networks,Thomas Bohnstingl;Ayush Garg;Stanisław Woźniak;George Saon;Evangelos Eleftheriou;Angeliki Pantazi,"Automatic speech recognition (ASR) is a capability which enables a program to process human speech into a written form. Recent developments in artificial intelligence (AI) have led to high-accuracy ASR systems based on deep neural networks, such as the recurrent neural network transducer (RNN-T). However, the core components and the performed operations of these approaches depart from the powerful biological counterpart, i.e., the human brain. On the other hand, the current developments in biologically-inspired ASR models, based on spiking neural networks (SNNs), lag behind in terms of accuracy and focus primarily on small scale applications. In this work, we revisit the incorporation of biologically-plausible models into deep learning and we substantially enhance their capabilities, by taking inspiration from the diverse neural and synaptic dynamics found in the brain. In particular, we introduce neural connectivity concepts emulating the axo-somatic and the axo-axonic synapses. Based on this, we propose novel deep learning units with enriched neuro-synaptic dynamics and integrate them into the RNN-T architecture. We demonstrate for the first time, that a biologically realistic implementation of a large-scale ASR model can yield competitive performance levels compared to the existing deep learning models. Specifically, we show that such an implementation bears several advantages, such as a reduced computational cost and a lower latency, which are critical for speech recognition applications. △ Less","4 November, 2021",https://arxiv.org/pdf/2110.02743
Bach Style Music Authoring System based on Deep Learning,Minghe Kong;Lican Huang,"With the continuous improvement in various aspects in the field of artificial intelligence, the momentum of artificial intelligence with deep learning capabilities into the field of music is coming. The research purpose of this paper is to design a Bach style music authoring system based on deep learning. We use a LSTM neural network to train serialized and standardized music feature data. By repeated experiments, we find the optimal LSTM model which can generate imitation of Bach music. Finally the generated music is comprehensively evaluated in the form of online audition and Turing test. The repertoires which the music generation system constructed in this article are very close to the style of Bach's original music, and it is relatively difficult for ordinary people to distinguish the musics Bach authored and AI created. △ Less","6 October, 2021",https://arxiv.org/pdf/2110.02640
Runtime Interchange for Adaptive Re-use of Intelligent Cyber-Physical System Controllers,Hammond Pearce;Xin Yang;Srinivas Pinisetty;Partha S. Roop,"Cyber-Physical Systems (CPSs) such as those found within autonomous vehicles are increasingly adopting Artificial Neural Network (ANN)-based controllers. To ensure the safety of these controllers, there is a spate of recent activity to formally verify the ANN-based designs. There are two challenges with these approaches: (1) The verification of such systems is difficult and time consuming. (2) These verified controllers are not able to adapt to frequent requirements changes, which are typical in situations like autonomous driving. This raises the question: how can trained and verified controllers, which have gone through expensive training and verification processes, be re-used to deal with requirement changes? This paper addresses this challenge for the first time by proposing a new framework that is capable of dealing with requirement changes at runtime through a mechanism we term runtime interchange. Our approach functions via a continual exchange and selection process of multiple pre-verified controllers. It represents a key step on the way to component-oriented engineering for intelligent designs, as it preserves the behaviours of the original controllers while introducing additional functionality. To demonstrate the efficacy of our approach we utilise an existing autonomous driving case study as well as a set of smaller benchmarks. These show that introduced overheads are extremely minimal and that the approach is very scalable. △ Less","23 September, 2021",https://arxiv.org/pdf/2110.01974
"Compression, The Fermi Paradox and Artificial Super-Intelligence",Michael Timothy Bennett,"The following briefly discusses possible difficulties in communication with and control of an AGI (artificial general intelligence), building upon an explanation of The Fermi Paradox and preceding work on symbol emergence and artificial general intelligence. The latter suggests that to infer what someone means, an agent constructs a rationale for the observed behaviour of others. Communication then requires two agents labour under similar compulsions and have similar experiences (construct similar solutions to similar tasks). Any non-human intelligence may construct solutions such that any rationale for their behaviour (and thus the meaning of their signals) is outside the scope of what a human is inclined to notice or comprehend. Further, the more compressed a signal, the closer it will appear to random noise. Another intelligence may possess the ability to compress information to the extent that, to us, their signals would appear indistinguishable from noise (an explanation for The Fermi Paradox). To facilitate predictive accuracy an AGI would tend to more compressed representations of the world, making any rationale for their behaviour more difficult to comprehend for the same reason. Communication with and control of an AGI may subsequently necessitate not only human-like compulsions and experiences, but imposed cognitive impairment. △ Less","5 October, 2021",https://arxiv.org/pdf/2110.01835
"The Artificial Scientist: Logicist, Emergentist, and Universalist Approaches to Artificial General Intelligence",Michael Timothy Bennett;Yoshihiro Maruyama,"We attempt to define what is necessary to construct an Artificial Scientist, explore and evaluate several approaches to artificial general intelligence (AGI) which may facilitate this, conclude that a unified or hybrid approach is necessary and explore two theories that satisfy this requirement to some degree. △ Less","5 October, 2021",https://arxiv.org/pdf/2110.01831
Ultrafast Neuromorphic Photonic Image Processing with a VCSEL Neuron,Joshua Robertson;Paul Kirkland;Juan Arturo Alanis;Matěj Hejda;Julián Bueno;Gaetano Di Caterina;Antonio Hurtado,"The ever-increasing demand for Artificial Intelligence (AI) systems is underlining a significant requirement for new, AI-optimised hardware. Neuromorphic (brain-like) processors are one highly-promising solution, with photonic-enabled realizations receiving increasing attention. Among these, approaches based upon Vertical Cavity Surface Emitting Lasers (VCSELs) are attracting interest given their favourable attributes and mature technology. Here, we demonstrate a hardware-friendly neuromorphic photonic spike processor, using a single VCSEL, for all-optical image edge-feature detection. This exploits the ability of a VCSEL-based photonic neuron to integrate temporally-encoded pixel data at high speed; and fire fast (100ps-long) optical spikes upon detecting desired image features. Furthermore, the photonic system is combined with a software-implemented spiking neural network yielding a full platform for complex image classification tasks. This work therefore highlights the potentials of VCSEL-based platforms for novel, ultrafast, all-optical neuromorphic processors interfacing with current computation and communication systems for use in future light-enabled AI and computer vision functionalities. △ Less","4 October, 2021",https://arxiv.org/pdf/2110.01617
"A curated, ontology-based, large-scale knowledge graph of artificial intelligence tasks and benchmarks",Kathrin Blagec;Adriano Barbosa-Silva;Simon Ott;Matthias Samwald,"Research in artificial intelligence (AI) is addressing a growing number of tasks through a rapidly growing number of models and methodologies. This makes it difficult to keep track of where novel AI methods are successfully -- or still unsuccessfully -- applied, how progress is measured, how different advances might synergize with each other, and how future research should be prioritized. To help address these issues, we created the Intelligence Task Ontology and Knowledge Graph (ITO), a comprehensive, richly structured and manually curated resource on artificial intelligence tasks, benchmark results and performance metrics. The current version of ITO contain 685,560 edges, 1,100 classes representing AI processes and 1,995 properties representing performance metrics. The goal of ITO is to enable precise and network-based analyses of the global landscape of AI tasks and capabilities. ITO is based on technologies that allow for easy integration and enrichment with external data, automated inference and continuous, collaborative expert curation of underlying ontological models. We make the ITO dataset and a collection of Jupyter notebooks utilising ITO openly available. △ Less","6 October, 2021",https://arxiv.org/pdf/2110.01434
Collective eXplainable AI: Explaining Cooperative Strategies and Agent Contribution in Multiagent Reinforcement Learning with Shapley Values,Alexandre Heuillet;Fabien Couthouis;Natalia Díaz-Rodríguez,"While Explainable Artificial Intelligence (XAI) is increasingly expanding more areas of application, little has been applied to make deep Reinforcement Learning (RL) more comprehensible. As RL becomes ubiquitous and used in critical and general public applications, it is essential to develop methods that make it better understood and more interpretable. This study proposes a novel approach to explain cooperative strategies in multiagent RL using Shapley values, a game theory concept used in XAI that successfully explains the rationale behind decisions taken by Machine Learning algorithms. Through testing common assumptions of this technique in two cooperation-centered socially challenging multi-agent environments environments, this article argues that Shapley values are a pertinent way to evaluate the contribution of players in a cooperative multi-agent RL context. To palliate the high overhead of this method, Shapley values are approximated using Monte Carlo sampling. Experimental results on Multiagent Particle and Sequential Social Dilemmas show that Shapley values succeed at estimating the contribution of each agent. These results could have implications that go beyond games in economics, (e.g., for non-discriminatory decision making, ethical and responsible AI-derived decisions or policy making under fairness constraints). They also expose how Shapley values only give general explanations about a model and cannot explain a single run, episode nor justify precise actions taken by agents. Future work should focus on addressing these critical aspects. △ Less","4 October, 2021",https://arxiv.org/pdf/2110.01307
Classification of Viral Pneumonia X-ray Images with the Aucmedi Framework,Pia Schneider;Dominik Müller;Frank Kramer,"In this work we use the AUCMEDI-Framework to train a deep neural network to classify chest X-ray images as either normal or viral pneumonia. Stratified k-fold cross-validation with k=3 is used to generate the validation-set and 15% of the data are set aside for the evaluation of the models of the different folds and ensembles each. A random-forest ensemble as well as a Soft-Majority-Vote ensemble are built from the predictions of the different folds. Evaluation metrics (Classification-Report, macro f1-scores, Confusion-Matrices, ROC-Curves) of the individual folds and the ensembles show that the classifier works well. Finally Grad-CAM and LIME explainable artificial intelligence (XAI) algorithms are applied to visualize the image features that are most important for the prediction. For Grad-CAM the heatmaps of the three folds are furthermore averaged for all images in order to calculate a mean XAI-heatmap. As the heatmaps of the different folds for most images differ only slightly this averaging procedure works well. However, only medical professionals can evaluate the quality of the features marked by the XAI. A comparison of the evaluation metrics with metrics of standard procedures such as PCR would also be important. Further limitations are discussed. △ Less","3 October, 2021",https://arxiv.org/pdf/2110.01017
Artificial Intelligence For Breast Cancer Detection: Trends & Directions,Shahid Munir Shah;Rizwan Ahmed Khan;Sheeraz Arif;Unaiza Sajid,"In the last decade, researchers working in the domain of computer vision and Artificial Intelligence (AI) have beefed up their efforts to come up with the automated framework that not only detects but also identifies stage of breast cancer. The reason for this surge in research activities in this direction are mainly due to advent of robust AI algorithms (deep learning), availability of hardware that can train those robust and complex AI algorithms and accessibility of large enough dataset required for training AI algorithms. Different imaging modalities that have been exploited by researchers to automate the task of breast cancer detection are mammograms, ultrasound, magnetic resonance imaging, histopathological images or any combination of them. This article analyzes these imaging modalities and presents their strengths, limitations and enlists resources from where their datasets can be accessed for research purpose. This article then summarizes AI and computer vision based state-of-the-art methods proposed in the last decade, to detect breast cancer using various imaging modalities. Generally, in this article we have focused on to review frameworks that have reported results using mammograms as it is most widely used breast imaging modality that serves as first test that medical practitioners usually prescribe for the detection of breast cancer. Second reason of focusing on mammogram imaging modalities is the availability of its labeled datasets. Datasets availability is one of the most important aspect for the development of AI based frameworks as such algorithms are data hungry and generally quality of dataset affects performance of AI based algorithms. In a nutshell, this research article will act as a primary resource for the research community working in the field of automated breast imaging analysis. △ Less","3 October, 2021",https://arxiv.org/pdf/2110.00942
Artificial intelligence for Sustainable Energy: A Contextual Topic Modeling and Content Analysis,Tahereh Saheb;Mohammad Dehghani,"Parallel to the rising debates over sustainable energy and artificial intelligence solutions, the world is currently discussing the ethics of artificial intelligence and its possible negative effects on society and the environment. In these arguments, sustainable AI is proposed, which aims at advancing the pathway toward sustainability, such as sustainable energy. In this paper, we offered a novel contextual topic modeling combining LDA, BERT, and Clustering. We then combined these computational analyses with content analysis of related scientific publications to identify the main scholarly topics, sub-themes, and cross-topic themes within scientific research on sustainable AI in energy. Our research identified eight dominant topics including sustainable buildings, AI-based DSSs for urban water management, climate artificial intelligence, Agriculture 4, the convergence of AI with IoT, AI-based evaluation of renewable technologies, smart campus and engineering education, and AI-based optimization. We then recommended 14 potential future research strands based on the observed theoretical gaps. Theoretically, this analysis contributes to the existing literature on sustainable AI and sustainable energy, and practically, it intends to act as a general guide for energy engineers and scientists, AI scientists, and social scientists to widen their knowledge of sustainability in AI and energy convergence research. △ Less","2 October, 2021",https://arxiv.org/pdf/2110.00828
Making Things Explainable vs Explaining: Requirements and Challenges under the GDPR,Francesco Sovrano;Fabio Vitali;Monica Palmirani,"The European Union (EU) through the High-Level Expert Group on Artificial Intelligence (AI-HLEG) and the General Data Protection Regulation (GDPR) has recently posed an interesting challenge to the eXplainable AI (XAI) community, by demanding a more user-centred approach to explain Automated Decision-Making systems (ADMs). Looking at the relevant literature, XAI is currently focused on producing explainable software and explanations that generally follow an approach we could term One-Size-Fits-All, that is unable to meet a requirement of centring on user needs. One of the causes of this limit is the belief that making things explainable alone is enough to have pragmatic explanations. Thus, insisting on a clear separation between explainabilty (something that can be explained) and explanations, we point to explanatorY AI (YAI) as an alternative and more powerful approach to win the AI-HLEG challenge. YAI builds over XAI with the goal to collect and organize explainable information, articulating it into something we called user-centred explanatory discourses. Through the use of explanatory discourses/narratives we represent the problem of generating explanations for Automated Decision-Making systems (ADMs) into the identification of an appropriate path over an explanatory space, allowing explainees to interactively explore it and produce the explanation best suited to their needs. △ Less","2 October, 2021",https://arxiv.org/pdf/2110.00758
Sparse Deep Learning: A New Framework Immune to Local Traps and Miscalibration,Yan Sun;Wenjun Xiong;Faming Liang,"Deep learning has powered recent successes of artificial intelligence (AI). However, the deep neural network, as the basic model of deep learning, has suffered from issues such as local traps and miscalibration. In this paper, we provide a new framework for sparse deep learning, which has the above issues addressed in a coherent way. In particular, we lay down a theoretical foundation for sparse deep learning and propose prior annealing algorithms for learning sparse neural networks. The former has successfully tamed the sparse deep neural network into the framework of statistical modeling, enabling prediction uncertainty correctly quantified. The latter can be asymptotically guaranteed to converge to the global optimum, enabling the validity of the down-stream statistical inference. Numerical result indicates the superiority of the proposed method compared to the existing ones. △ Less","2 December, 2021",https://arxiv.org/pdf/2110.00653
Natural language understanding for logical games,Adrian Groza;Cristian Nitu,"We developed a system able to automatically solve logical puzzles in natural language. Our solution is composed by a parser and an inference module. The parser translates the text into first order logic (FOL), while the MACE4 model finder is used to compute the models of the given FOL theory. We also empower our software agent with the capability to provide Yes/No answers to natural language questions related to each puzzle. Moreover, in line with Explainalbe Artificial Intelligence (XAI), the agent can back its answer, providing a graphical representation of the proof. The advantage of using reasoning for Natural Language Understanding (NLU) instead of Machine learning is that the user can obtain an explanation of the reasoning chain. We illustrate how the system performs on various types of natural language puzzles, including 382 knights and knaves puzzles. These features together with the overall performance rate of 80.89\% makes the proposed solution an improvement upon similar solvers for natural language understanding in the puzzles domain. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00558
"Self-Supervised Decomposition, Disentanglement and Prediction of Video Sequences while Interpreting Dynamics: A Koopman Perspective",Armand Comas;Sandesh Ghimire;Haolin Li;Mario Sznaier;Octavia Camps,"Human interpretation of the world encompasses the use of symbols to categorize sensory inputs and compose them in a hierarchical manner. One of the long-term objectives of Computer Vision and Artificial Intelligence is to endow machines with the capacity of structuring and interpreting the world as we do. Towards this goal, recent methods have successfully been able to decompose and disentangle video sequences into their composing objects and dynamics, in a self-supervised fashion. However, there has been a scarce effort in giving interpretation to the dynamics of the scene. We propose a method to decompose a video into moving objects and their attributes, and model each object's dynamics with linear system identification tools, by means of a Koopman embedding. This allows interpretation, manipulation and extrapolation of the dynamics of the different objects by employing the Koopman operator K. We test our method in various synthetic datasets and successfully forecast challenging trajectories while interpreting them. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00547
PhiNets: a scalable backbone for low-power AI at the edge,Francesco Paissan;Alberto Ancilotto;Elisabetta Farella,"In the Internet of Things era, where we see many interconnected and heterogeneous mobile and fixed smart devices, distributing the intelligence from the cloud to the edge has become a necessity. Due to limited computational and communication capabilities, low memory and limited energy budget, bringing artificial intelligence algorithms to peripheral devices, such as the end-nodes of a sensor network, is a challenging task and requires the design of innovative methods. In this work, we present PhiNets, a new scalable backbone optimized for deep-learning-based image processing on resource-constrained platforms. PhiNets are based on inverted residual blocks specifically designed to decouple the computational cost, working memory, and parameter memory, thus exploiting all the available resources. With a YoloV2 detection head and Simple Online and Realtime Tracking, the proposed architecture has achieved the state-of-the-art results in (i) detection on the COCO and VOC2012 benchmarks, and (ii) tracking on the MOT15 benchmark. PhiNets reduce the parameter count of 87% to 93% with respect to previous state-of-the-art models (EfficientNetv1, MobileNetv2) and achieve better performance with lower computational cost. Moreover, we demonstrate our approach on a prototype node based on a STM32H743 microcontroller (MCU) with 2MB of internal Flash and 1MB of RAM and achieve power requirements in the order of 10 mW. The code for the PhiNets is publicly available on GitHub. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00337
Q-Net: A Quantitative Susceptibility Mapping-based Deep Neural Network for Differential Diagnosis of Brain Iron Deposition in Hemochromatosis,Soheil Zabihi;Elahe Rahimian;Soumya Sharma;Sean K. Sethi;Sara Gharabaghi;Amir Asif;E. Mark Haacke;Mandar S. Jog;Arash Mohammadi,"Brain iron deposition, in particular deep gray matter nuclei, increases with advancing age. Hereditary Hemochromatosis (HH) is the most common inherited disorder of systemic iron excess in Europeans and recent studies claimed high brain iron accumulation in patient with Hemochromatosis. In this study, we focus on Artificial Intelligence (AI)-based differential diagnosis of brain iron deposition in HH via Quantitative Susceptibility Mapping (QSM), which is an established Magnetic Resonance Imaging (MRI) technique to study the distribution of iron in the brain. Our main objective is investigating potentials of AI-driven frameworks to accurately and efficiently differentiate individuals with Hemochromatosis from those of the healthy control group. More specifically, we developed the Q-Net framework, which is a data-driven model that processes information on iron deposition in the brain obtained from multi-echo gradient echo imaging data and anatomical information on T1-Weighted images of the brain. We illustrate that the Q-Net framework can assist in differentiating between someone with HH and Healthy control (HC) of the same age, something that is not possible by just visualizing images. The study is performed based on a unique dataset that was collected from 52 subjects with HH and 47 HC. The Q-Net provides a differential diagnosis accuracy of 83.16% and 80.37% in the scan-level and image-level classification, respectively. △ Less","1 October, 2021",https://arxiv.org/pdf/2110.00203
Development of the algorithm for differentiating bone metastases and trauma of the ribs in bone scintigraphy and demonstration of visual evidence of the algorithm -- Using only anterior bone scan view of thorax,Shigeaki Higashiyama;Yukino Ohta;Yutaka Katayama;Atsushi Yoshida;Joji Kawabe,"Background: Although there are many studies on the application of artificial intelligence (AI) models to medical imaging, there is no report of an AI model that determines the accumulation of ribs in bone metastases and trauma only using the anterior image of thorax of bone scintigraphy. In recent years, a method for visualizing diagnostic grounds called Gradient-weighted Class Activation Mapping (Grad-CAM) has been proposed in the area of diagnostic images using Deep Convolutional Neural Network (DCNN). As far as we have investigated, there are no reports of visualization of the diagnostic basis in bone scintigraphy. Our aim is to visualize the area of interest of DCNN, in addition to developing an algorithm to classify and diagnose whether RI accumulation on the ribs is bone metastasis or trauma using only anterior bone scan view of thorax. Material and Methods: For this retrospective study, we used 838 patients who underwent bone scintigraphy to search for bone metastases at our institution. A frontal chest image of bone scintigraphy was used to create the algorithm. We used 437 cases with bone metastases on the ribs and 401 cases with abnormal RI accumulation due to trauma. Result: AI model was able to detect bone metastasis lesion with a sensitivity of 90.00% and accuracy of 86.5%. And it was possible to visualize the part that the AI model focused on with Grad-CAM. △ Less","30 September, 2021",https://arxiv.org/pdf/2110.00130
AIive: Interactive Visualization and Sonification of Neural Networks in Virtual Reality,Zhuoyue Lyu;Jiannan Li;Bryan Wang,"Artificial Intelligence (AI), especially Neural Networks (NNs), has become increasingly popular. However, people usually treat AI as a tool, focusing on improving outcome, accuracy, and performance while paying less attention to the representation of AI itself. We present AIive, an interactive visualization of AI in Virtual Reality (VR) that brings AI ""alive"". AIive enables users to manipulate the parameters of NNs with virtual hands and provides auditory feedback for the real-time values of loss, accuracy, and hyperparameters. Thus, AIive contributes an artistic and intuitive way to represent AI by integrating visualization, sonification, and direct manipulation in VR, potentially targeting a wide range of audiences. △ Less","21 October, 2021",https://arxiv.org/pdf/2109.15193
Transfer Learning Based Multi-Objective Genetic Algorithm for Dynamic Community Detection,Jungang Zou;Fan Lin;Siyu Gao;Gaoshan Deng;Wenhua Zeng;Gil Alterovitz,"Dynamic community detection is the hotspot and basic problem of complex network and artificial intelligence research in recent years. It is necessary to maximize the accuracy of clustering as the network structure changes, but also to minimize the two consecutive clustering differences between the two results. There is a trade-off relationship between these two objectives. In this paper, we propose a Feature Transfer Based Multi-Objective Optimization Genetic Algorithm (TMOGA) based on transfer learning and traditional multi-objective evolutionary algorithm framework. The main idea is to extract stable features from past community structures, retain valuable feature information, and integrate this feature information into current optimization processes to improve the evolutionary algorithms. Additionally, a new theoretical framework is proposed in this paper to analyze community detection problem based on information theory. Then, we exploit this framework to prove the rationality of TMOGA. Finally, the experimental results show that our algorithm can achieve better clustering effects compared with the state-of-the-art dynamic network community detection algorithms in diverse test problems. △ Less","8 October, 2021",https://arxiv.org/pdf/2109.15136
Dynamical symmetry breaking through AI: The dimer self-trapping transition,G. P. Tsironis;G. D. Barmparis;D. K. Campbell,"The nonlinear dimer obtained through the nonlinear Schr{ö}dinger equation has been a workhorse for the discovery the role nonlinearity plays in strongly interacting systems. While the analysis of the stationary states demonstrates the onset of a symmetry broken state for some degree of nonlinearity, the full dynamics maps the system into an effective φ^4 model. In this latter context, the self-trapping transition is an initial condition dependent transfer of a classical particle over a barrier set by the nonlinear term. This transition has been investigated analytically and mathematically it is expressed through the hyperbolic limit of Jacobian elliptic functions. The aim of the present work is to recapture this transition through the use of methods of Artificial Intelligence (AI). Specifically, we used a physics motivated machine learning model that is shown to be able to capture the original dynamic self-trapping transition and its dependence on initial conditions. Exploitation of this result in the case of the non-degenerate nonlinear dimer gives additional information on the more general dynamics and helps delineate linear from nonlinear localization. This work shows how AI methods may be embedded in physics and provide useful tools for discovery. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.15057
The Artificial Intelligence behind the winning entry to the 2019 AI Robotic Racing Competition,Christophe De Wagter;Federico Paredes-Vallés;Nilay Sheth;Guido de Croon,"Robotics is the next frontier in the progress of Artificial Intelligence (AI), as the real world in which robots operate represents an enormous, complex, continuous state space with inherent real-time requirements. One extreme challenge in robotics is currently formed by autonomous drone racing. Human drone racers can fly through complex tracks at speeds of up to 190 km/h. Achieving similar speeds with autonomous drones signifies tackling fundamental problems in AI under extreme restrictions in terms of resources. In this article, we present the winning solution of the first AI Robotic Racing (AIRR) Circuit, a competition consisting of four races in which all participating teams used the same drone, to which they had limited access. The core of our approach is inspired by how human pilots combine noisy observations of the race gates with their mental model of the drone's dynamics to achieve fast control. Our approach has a large focus on gate detection with an efficient deep neural segmentation network and active vision. Further, we make contributions to robust state estimation and risk-based control. This allowed us to reach speeds of ~9.2m/s in the last race, unrivaled by previous autonomous drone race competitions. Although our solution was the fastest and most robust, it still lost against one of the best human pilots, Gab707. The presented approach indicates a promising direction to close the gap with human drone pilots, forming an important step in bringing AI to the real world. △ Less","30 September, 2021",https://arxiv.org/pdf/2109.14985
Private sampling: a noiseless approach for generating differentially private synthetic data,March Boedihardjo;Thomas Strohmer;Roman Vershynin,"In a world where artificial intelligence and data science become omnipresent, data sharing is increasingly locking horns with data-privacy concerns. Differential privacy has emerged as a rigorous framework for protecting individual privacy in a statistical database, while releasing useful statistical information about the database. The standard way to implement differential privacy is to inject a sufficient amount of noise into the data. However, in addition to other limitations of differential privacy, this process of adding noise will affect data accuracy and utility. Another approach to enable privacy in data sharing is based on the concept of synthetic data. The goal of synthetic data is to create an as-realistic-as-possible dataset, one that not only maintains the nuances of the original data, but does so without risk of exposing sensitive information. The combination of differential privacy with synthetic data has been suggested as a best-of-both-worlds solutions. In this work, we propose the first noisefree method to construct differentially private synthetic data; we do this through a mechanism called ""private sampling"". Using the Boolean cube as benchmark data model, we derive explicit bounds on accuracy and privacy of the constructed synthetic data. The key mathematical tools are hypercontractivity, duality, and empirical processes. A core ingredient of our private sampling mechanism is a rigorous ""marginal correction"" method, which has the remarkable property that importance reweighting can be utilized to exactly match the marginals of the sample to the marginals of the population. △ Less","30 September, 2021",https://arxiv.org/pdf/2109.14839
An Automated Scanning Transmission Electron Microscope Guided by Sparse Data Analytics,Matthew Olszta;Derek Hopkins;Kevin R. Fiedler;Marjolein Oostrom;Sarah Akers;Steven R. Spurgeon,"Artificial intelligence (AI) promises to reshape scientific inquiry and enable breakthrough discoveries in areas such as energy storage, quantum computing, and biomedicine. Scanning transmission electron microscopy (STEM), a cornerstone of the study of chemical and materials systems, stands to benefit greatly from AI-driven automation. However, present barriers to low-level instrument control, as well as generalizable and interpretable feature detection, make truly automated microscopy impractical. Here, we discuss the design of a closed-loop instrument control platform guided by emerging sparse data analytics. We demonstrate how a centralized controller, informed by machine learning combining limited a priori knowledge and task-based discrimination, can drive on-the-fly experimental decision-making. This platform unlocks practical, automated analysis of a variety of material features, enabling new high-throughput and statistical studies. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14772
The Overlap Gap Property: a Geometric Barrier to Optimizing over Random Structures,David Gamarnik,"The problem of optimizing over random structures emerges in many areas of science and engineering, ranging from statistical physics to machine learning and artificial intelligence. For many such structures finding optimal solutions by means of fast algorithms is not known and often is believed not possible. At the same time the formal hardness of these problems in form of say complexity-theoretic NP-hardness is lacking. In this introductory article a new approach for algorithmic intractability in random structures is described, which is based on the topological disconnectivity property of the set of pair-wise distances of near optimal solutions, called the Overlap Gap Property. The article demonstrates how this property a) emerges in most models known to exhibit an apparent algorithmic hardness b) is consistent with the hardness/tractability phase transition for many models analyzed to the day, and importantly c) allows to mathematically rigorously rule out large classes of algorithms as potential contenders, in particular the algorithms exhibiting the input stability (insensitivity). △ Less","1 August, 2021",https://arxiv.org/pdf/2109.14409
Understanding Relations Between Perception of Fairness and Trust in Algorithmic Decision Making,Jianlong Zhou;Sunny Verma;Mudit Mittal;Fang Chen,"Algorithmic processes are increasingly employed to perform managerial decision making, especially after the tremendous success in Artificial Intelligence (AI). This paradigm shift is occurring because these sophisticated AI techniques are guaranteeing the optimality of performance metrics. However, this adoption is currently under scrutiny due to various concerns such as fairness, and how does the fairness of an AI algorithm affects user's trust is much legitimate to pursue. In this regard, we aim to understand the relationship between induced algorithmic fairness and its perception in humans. In particular, we are interested in whether these two are positively correlated and reflect substantive fairness. Furthermore, we also study how does induced algorithmic fairness affects user trust in algorithmic decision making. To understand this, we perform a user study to simulate candidate shortlisting by introduced (manipulating mathematical) fairness in a human resource recruitment setting. Our experimental results demonstrate that different levels of introduced fairness are positively related to human perception of fairness, and simultaneously it is also positively related to user trust in algorithmic decision making. Interestingly, we also found that users are more sensitive to the higher levels of introduced fairness than the lower levels of introduced fairness. Besides, we summarize the theoretical and practical implications of this research with a discussion on perception of fairness. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14345
Formulation and validation of a car-following model based on deep reinforcement learning,Fabian Hart;Ostap Okhrin;Martin Treiber,"We propose and validate a novel car following model based on deep reinforcement learning. Our model is trained to maximize externally given reward functions for the free and car-following regimes rather than reproducing existing follower trajectories. The parameters of these reward functions such as desired speed, time gap, or accelerations resemble that of traditional models such as the Intelligent Driver Model (IDM) and allow for explicitly implementing different driving styles. Moreover, they partially lift the black-box nature of conventional neural network models. The model is trained on leading speed profiles governed by a truncated Ornstein-Uhlenbeck process reflecting a realistic leader's kinematics. This allows for arbitrary driving situations and an infinite supply of training data. For various parameterizations of the reward functions, and for a wide variety of artificial and real leader data, the model turned out to be unconditionally string stable, comfortable, and crash-free. String stability has been tested with a platoon of five followers following an artificial and a real leading trajectory. A cross-comparison with the IDM calibrated to the goodness-of-fit of the relative gaps showed a higher reward compared to the traditional model and a better goodness-of-fit. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14268
"BLEU, METEOR, BERTScore: Evaluation of Metrics Performance in Assessing Critical Translation Errors in Sentiment-oriented Text",Hadeel Saadany;Constantin Orasan,"Social media companies as well as authorities make extensive use of artificial intelligence (AI) tools to monitor postings of hate speech, celebrations of violence or profanity. Since AI software requires massive volumes of data to train computers, Machine Translation (MT) of the online content is commonly used to process posts written in several languages and hence augment the data needed for training. However, MT mistakes are a regular occurrence when translating sentiment-oriented user-generated content (UGC), especially when a low-resource language is involved. The adequacy of the whole process relies on the assumption that the evaluation metrics used give a reliable indication of the quality of the translation. In this paper, we assess the ability of automatic quality metrics to detect critical machine translation errors which can cause serious misunderstanding of the affect message. We compare the performance of three canonical metrics on meaningless translations where the semantic content is seriously impaired as compared to meaningful translations with a critical error which exclusively distorts the sentiment of the source text. We conclude that there is a need for fine-tuning of automatic metrics to make them more robust in detecting sentiment critical errors. △ Less","29 September, 2021",https://arxiv.org/pdf/2109.14250
Semantic Communications With AI Tasks,Yang Yang;Caili Guo;Fangfang Liu;Chuanhong Liu;Lunan Sun;Qizheng Sun;Jiujiu Chen,"A radical paradigm shift of wireless networks from ``connected things'' to ``connected intelligence'' undergoes, which coincides with the Shanno and Weaver's envisions: Communications will transform from the technical level to the semantic level. This article proposes a semantic communication method with artificial intelligence tasks (SC-AIT). First, the architecture of SC-AIT is elaborated. Then, based on the proposed architecture, we implement SC-AIT for a image classifications task. A prototype of SC-AIT is also established for surface defect detection, is conducted. Experimental results show that SC-AIT has much lower bandwidth requirements, and can achieve more than 40\% classification accuracy gains compared with the communications at the technical level. Future trends and key challenges for semantic communications are also identified. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.14170
A framework for quantitative analysis of Computed Tomography images of viral pneumonitis: radiomic features in COVID and non-COVID patients,Giulia Zorzi;Luca Berta;Stefano Carrazza;Alberto Torresin,"Purpose: to optimize a pipeline of clinical data gathering and CT images processing implemented during the COVID-19 pandemic crisis and to develop artificial intelligence model for different of viral pneumonia. Methods: 1028 chest CT image of patients with positive swab were segmented automatically for lung extraction. A Gaussian model developed in Python language was applied to calculate quantitative metrics (QM) describing well-aerated and ill portions of the lungs from the histogram distribution of lung CT numbers in both lungs of each image and in four geometrical subdivision. Furthermore, radiomic features (RF) of first and second order were extracted from bilateral lungs using PyRadiomic tools. QM and RF were used to develop 4 different Multi-Layer Perceptron (MLP) classifier to discriminate images of patients with COVID (n=646) and non-COVID (n=382) viral pneumonia. Results: The Gaussian model applied to lung CT histogram correctly described healthy parenchyma 94% of the patients. The resulting accuracy of the models for COVID diagnosis were in the range 0.76-0.87, as the integral of the receiver operating curve. The best diagnostic performances were associated to the model based on RF of first and second order, with 21 relevant features after LASSO regression and an accuracy of 0.81\pm0.02 after 4-fold cross validation Conclusions: Despite these results were obtained with CT images from a single center, a platform for extracting useful quantitative metrics from CT images was developed and optimized. Four artificial intelligence-based models for classifying patients with COVID and non-COVID viral pneumonia were developed and compared showing overall good diagnostic performances △ Less","28 September, 2021",https://arxiv.org/pdf/2109.13931
The eDiscovery Medicine Show,Maura R. Grossman;Gordon V. Cormack,"The practice of bloodletting gradually fell into disfavor as a growing body of scientific evidence showed its ineffectiveness and demonstrated the effectiveness of various pharmaceuticals for the prevention and treatment of certain diseases. At the same time, the patent medicine industry promoted ineffective remedies at medicine shows featuring entertainment, testimonials, and pseudo-scientific claims with all the trappings--but none of the methodology--of science. Today, many producing parties and eDiscovery vendors similarly promote obsolete technology as well as unvetted tools labeled ""artificial intelligence"" or ""technology-assisted review,"" along with unsound validation protocols. This situation will end only when eDiscovery technologies and tools are subject to testing using the methods of information retrieval. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.13908
Intelligence Complements from the Built Environment: A review of Smart Building Technologies for Cognitively Declined Occupants,Saeid Alimoradi;Xinghua Gao,"Traditionally, caregivers, whether formal or informal, have taken the responsibility of providing assistance and care to patients with cognitive decline. Usually, both the caregiver and the patient are subjected to financial and emotional burdens, which impact the patient's life quality. To overcome this issue, Ambient Assistive Living (AAL) technologies have been adopted to replace the caregivers and complement patients' lack of intelligence. Technologies such as Internet of Things (IoT) and Artificial Intelligence (AI) have enabled intelligent ubiquitous learning for smart buildings to monitor the cognitively declined occupants and provide in-home assistive services and solutions. This paper aims to summarize and evaluate the intelligence complements provided by smart buildings that can increase the cognitively declined occupants' quality of life and autonomy. Through a systematic literature review, the authors find that most of the existing contributions are towards identifying the occupants' behavior, and thus, to determine corresponding assistive services and solutions. Five key research gaps are identified, including the lack of adequate adoption of technological interventions to fully support the occupants' autonomy and independence. The authors also propose a conceptual framework to highlight the research gaps in smart building applications for cognitively declined occupants and to map the future research directions. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.13852
Not Color Blind: AI Predicts Racial Identity from Black and White Retinal Vessel Segmentations,Aaron S. Coyner;Praveer Singh;James M. Brown;Susan Ostmo;R. V. Paul Chan;Michael F. Chiang;Jayashree Kalpathy-Cramer;J. Peter Campbell,"Background: Artificial intelligence (AI) may demonstrate racial bias when skin or choroidal pigmentation is present in medical images. Recent studies have shown that convolutional neural networks (CNNs) can predict race from images that were not previously thought to contain race-specific features. We evaluate whether grayscale retinal vessel maps (RVMs) of patients screened for retinopathy of prematurity (ROP) contain race-specific features. Methods: 4095 retinal fundus images (RFIs) were collected from 245 Black and White infants. A U-Net generated RVMs from RFIs, which were subsequently thresholded, binarized, or skeletonized. To determine whether RVM differences between Black and White eyes were physiological, CNNs were trained to predict race from color RFIs, raw RVMs, and thresholded, binarized, or skeletonized RVMs. Area under the precision-recall curve (AUC-PR) was evaluated. Findings: CNNs predicted race from RFIs near perfectly (image-level AUC-PR: 0.999, subject-level AUC-PR: 1.000). Raw RVMs were almost as informative as color RFIs (image-level AUC-PR: 0.938, subject-level AUC-PR: 0.995). Ultimately, CNNs were able to detect whether RFIs or RVMs were from Black or White babies, regardless of whether images contained color, vessel segmentation brightness differences were nullified, or vessel segmentation widths were normalized. Interpretation: AI can detect race from grayscale RVMs that were not thought to contain racial information. Two potential explanations for these findings are that: retinal vessels physiologically differ between Black and White babies or the U-Net segments the retinal vasculature differently for various fundus pigmentations. Either way, the implications remain the same: AI algorithms have potential to demonstrate racial bias in practice, even when preliminary attempts to remove such information from the underlying images appear to be successful. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.13845
An Automated Data Engineering Pipeline for Anomaly Detection of IoT Sensor Data,Xinze Li;Baixi Zou,"The rapid development in the field of System of Chip (SoC) technology, Internet of Things (IoT), cloud computing, and artificial intelligence has brought more possibilities of improving and solving the current problems. With data analytics and the use of machine learning/deep learning, it is made possible to learn the underlying patterns and make decisions based on what was learned from massive data generated from IoT sensors. When combined with cloud computing, the whole pipeline can be automated, and free of manual controls and operations. In this paper, an implementation of an automated data engineering pipeline for anomaly detection of IoT sensor data is studied and proposed. The process involves the use of IoT sensors, Raspberry Pis, Amazon Web Services (AWS) and multiple machine learning techniques with the intent to identify anomalous cases for the smart home security system. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.13828
CateCom: a practical data-centric approach to categorization of computational models,Alexander Zech;Timur Bazhirov,"The advent of data-driven science in the 21st century brought about the need for well-organized structured data and associated infrastructure able to facilitate the applications of Artificial Intelligence and Machine Learning. We present an effort aimed at organizing the diverse landscape of physics-based and data-driven computational models in order to facilitate the storage of associated information as structured data. We apply object-oriented design concepts and outline the foundations of an open-source collaborative framework that is: (1) capable of uniquely describing the approaches in structured data, (2) flexible enough to cover the majority of widely used models, and (3) utilizes collective intelligence through community contributions. We present example database schemas and corresponding data structures and explain how these are deployed in software at the time of this writing. △ Less","27 September, 2021",https://arxiv.org/pdf/2109.13452
Trustworthy AI and Robotics and the Implications for the AEC Industry: A Systematic Literature Review and Future Potentials,Newsha Emaminejad;Reza Akhavian,"Human-technology interaction deals with trust as an inevitable requirement for user acceptance. As the applications of artificial intelligence (AI) and robotics emerge and with their ever-growing socio-economic influence in various fields of research and practice, there is an imminent need to study trust in such systems. With the opaque work mechanism of AI-based systems and the prospect of intelligent robots as workers' companions, context-specific interdisciplinary studies on trust are key in increasing their adoption. Through a thorough systematic literature review on (1) trust in AI and robotics (AIR) and (2) AIR applications in the architecture, engineering, and construction (AEC) industry, this study identifies common trust dimensions in the literature and uses them to organize the paper. Furthermore, the connections of the identified dimensions to the existing and potential AEC applications are determined and discussed. Finally, major future directions on trustworthy AI and robotics in AEC research and practice are outlined. △ Less","27 September, 2021",https://arxiv.org/pdf/2109.13373
On deploying the Artificial Sport Trainer into practice,Iztok Fister Jr.;Iztok Fister;Andres Iglesias;Akemi Galvez;Suash Deb;Dušan Fister,"Computational Intelligence methods for automatic generation of sport training plans in individual sport disciplines have achieved a mature phase. In order to confirm their added value, they have been deployed into practice. As a result, several methods have been developed for generating well formulated training plans on computers automatically that, typically, depend on the collection of past sport activities. However, monitoring the realization of the performed training sessions still represents a bottleneck in automating the process of sport training as a whole. The objective of this paper is to present a new low-cost and efficient embedded device for monitoring the realization of sport training sessions that is dedicated to monitor cycling training sessions. We designed and developed a new bike computer, i.e. the AST-Monitor, that can be mounted easily on almost every bicycle. The aforementioned bike computer is based on the Raspberry Pi device that supports different external sensors for capturing the data during the realization of sport training sessions. An adjusted GUI tailored to the needs of athletes is developed, along with the hardware. The proof of concept study, using the AST-Monitor in practice, revealed the potential of the proposed solution for monitoring of realized sport training sessions automatically. The new device also opens the door for the future utilization of Artificial Intelligence in a wide variety of sports. △ Less","3 September, 2021",https://arxiv.org/pdf/2109.13334
Bayesian Transfer Learning: An Overview of Probabilistic Graphical Models for Transfer Learning,Junyu Xuan;Jie Lu;Guangquan Zhang,"Transfer learning where the behavior of extracting transferable knowledge from the source domain(s) and reusing this knowledge to target domain has become a research area of great interest in the field of artificial intelligence. Probabilistic graphical models (PGMs) have been recognized as a powerful tool for modeling complex systems with many advantages, e.g., the ability to handle uncertainty and possessing good interpretability. Considering the success of these two aforementioned research areas, it seems natural to apply PGMs to transfer learning. However, although there are already some excellent PGMs specific to transfer learning in the literature, the potential of PGMs for this problem is still grossly underestimated. This paper aims to boost the development of PGMs for transfer learning by 1) examining the pilot studies on PGMs specific to transfer learning, i.e., analyzing and summarizing the existing mechanisms particularly designed for knowledge transfer; 2) discussing examples of real-world transfer problems where existing PGMs have been successfully applied; and 3) exploring several potential research directions on transfer learning using PGM. △ Less","26 September, 2021",https://arxiv.org/pdf/2109.13233
Palimpsest Memories Stored in Memristive Synapses,Christos Giotis;Alexander Serb;Vasileios Manouras;Spyros Stathopoulos;Themis Prodromakis,"Biological synapses store multiple memories on top of each other in a palimpsest fashion and at different timescales. Palimpsest consolidation is facilitated by the interaction of hidden biochemical processes that govern synaptic efficacy during varying lifetimes. This arrangement allows idle memories to be temporarily overwritten without being forgotten, in favour of new memories utilised in the short-term. While embedded artificial intelligence can greatly benefit from such functionality, a practical demonstration in hardware is still missing. Here, we show how the intrinsic properties of metal-oxide volatile memristors emulate the hidden processes that support biological palimpsest consolidation. Our memristive synapses exhibit an expanded doubled capacity which can protect a consolidated long-term memory while up to hundreds of uncorrelated short-term memories temporarily overwrite it. The synapses can also implement familiarity detection of previously forgotten memories. Crucially, palimpsest operation is achieved automatically and without the need for specialised instructions. We further demonstrate a practical adaptation of this technology in the context of image vision. This showcases the use of emerging memory technologies to efficiently expand the capacity of artificial intelligence hardware towards more generalised learning memories. △ Less","22 September, 2021",https://arxiv.org/pdf/2109.13198
Automated Mining of Leaderboards for Empirical AI Research,Salomon Kabongo;Jennifer D'Souza;Sören Auer,"With the rapid growth of research publications, empowering scientists to keep oversight over the scientific progress is of paramount importance. In this regard, the Leaderboards facet of information organization provides an overview on the state-of-the-art by aggregating empirical results from various studies addressing the same research challenge. Crowdsourcing efforts like PapersWithCode among others are devoted to the construction of Leaderboards predominantly for various subdomains in Artificial Intelligence. Leaderboards provide machine-readable scholarly knowledge that has proven to be directly useful for scientists to keep track of research progress. The construction of Leaderboards could be greatly expedited with automated text mining. This study presents a comprehensive approach for generating Leaderboards for knowledge-graph-based scholarly information organization. Specifically, we investigate the problem of automated Leaderboard construction using state-of-the-art transformer models, viz. Bert, SciBert, and XLNet. Our analysis reveals an optimal approach that significantly outperforms existing baselines for the task with evaluation scores above 90% in F1. This, in turn, offers new state-of-the-art results for Leaderboard extraction. As a result, a vast share of empirical AI research can be organized in the next-generation digital libraries as knowledge graphs. △ Less","31 August, 2021",https://arxiv.org/pdf/2109.13089
A User-Centred Framework for Explainable Artificial Intelligence in Human-Robot Interaction,Marco Matarese;Francesco Rea;Alessandra Sciutti,"State of the art Artificial Intelligence (AI) techniques have reached an impressive complexity. Consequently, researchers are discovering more and more methods to use them in real-world applications. However, the complexity of such systems requires the introduction of methods that make those transparent to the human user. The AI community is trying to overcome the problem by introducing the Explainable AI (XAI) field, which is tentative to make AI algorithms less opaque. However, in recent years, it became clearer that XAI is much more than a computer science problem: since it is about communication, XAI is also a Human-Agent Interaction problem. Moreover, AI came out of the laboratories to be used in real life. This implies the need for XAI solutions tailored to non-expert users. Hence, we propose a user-centred framework for XAI that focuses on its social-interactive aspect taking inspiration from cognitive and social sciences' theories and findings. The framework aims to provide a structure for interactive XAI solutions thought for non-expert users. △ Less","5 November, 2021",https://arxiv.org/pdf/2109.12912
A Multi-Agent System for Autonomous Mobile Robot Coordination,Norberto Sousa;Nuno Oliveira;Isabel Praça,"The automation of internal logistics and inventory-related tasks is one of the main challenges of modern-day manufacturing corporations since it allows a more effective application of their human resources. Nowadays, Autonomous Mobile Robots (AMR) are state of the art technologies for such applications due to their great adaptability in dynamic environments, replacing more traditional solutions such as Automated Guided Vehicles (AGV), which are quite limited in terms of flexibility and require expensive facility updates for their installation. The application of Artificial Intelligence (AI) to increase AMRs capabilities has been contributing for the development of more sophisticated and efficient robots. Nevertheless, multi-robot coordination and cooperation for solving complex tasks is still a hot research line with increasing interest. This work proposes a Multi-Agent System for coordinating multiple TIAGo robots in tasks related to the manufacturing ecosystem such as the transportation and dispatching of raw materials, finished products and tools. Furthermore, the system is showcased in a realistic simulation using both Gazebo and Robot Operating System (ROS). △ Less","25 September, 2021",https://arxiv.org/pdf/2109.12386
NUMA-aware FFT-based Convolution on ARMv8 Many-core CPUs,Xiandong Huang;Qinglin Wang;Shuyu Lu;Ruochen Hao;Songzhu Mei;Jie Liu,"Convolutional Neural Networks (CNNs), one of the most representative algorithms of deep learning, are widely used in various artificial intelligence applications. Convolution operations often take most of the computational overhead of CNNs. The FFT-based algorithm can improve the efficiency of convolution by reducing its algorithm complexity, there are a lot of works about the high-performance implementation of FFT-based convolution on many-core CPUs. However, there is no optimization for the non-uniform memory access (NUMA) characteristics in many-core CPUs. In this paper, we present a NUMA-aware FFT-based convolution implementation on ARMv8 many-core CPUs with NUMA architectures. The implementation can reduce a number of remote memory access through the data reordering of FFT transformations and the three-level parallelization of the complex matrix multiplication. The experiment results on a ARMv8 many-core CPU with NUMA architectures demonstrate that our NUMA-aware implementation has much better performance than the state-of-the-art work in most cases. △ Less","24 September, 2021",https://arxiv.org/pdf/2109.12259
AI Explainability 360: Impact and Design,Vijay Arya;Rachel K. E. Bellamy;Pin-Yu Chen;Amit Dhurandhar;Michael Hind;Samuel C. Hoffman;Stephanie Houde;Q. Vera Liao;Ronny Luss;Aleksandra Mojsilovic;Sami Mourad;Pablo Pedemonte;Ramya Raghavendra;John Richards;Prasanna Sattigeri;Karthikeyan Shanmugam;Moninder Singh;Kush R. Varshney;Dennis Wei;Yunfeng Zhang,"As artificial intelligence and machine learning algorithms become increasingly prevalent in society, multiple stakeholders are calling for these algorithms to provide explanations. At the same time, these stakeholders, whether they be affected citizens, government regulators, domain experts, or system developers, have different explanation needs. To address these needs, in 2019, we created AI Explainability 360 (Arya et al. 2020), an open source software toolkit featuring ten diverse and state-of-the-art explainability methods and two evaluation metrics. This paper examines the impact of the toolkit with several case studies, statistics, and community feedback. The different ways in which users have experienced AI Explainability 360 have resulted in multiple types of impact and improvements in multiple metrics, highlighted by the adoption of the toolkit by the independent LF AI & Data Foundation. The paper also describes the flexible design of the toolkit, examples of its use, and the significant educational material and documentation available to its users. △ Less","24 September, 2021",https://arxiv.org/pdf/2109.12151
A Bayesian-Based Approach to Human Operator Intent Recognition in Remote Mobile Robot Navigation,Dimitris Panagopoulos;Giannis Petousakis;Rustam Stolkin;Grigoris Nikolaou;Manolis Chiou,"This paper addresses the problem of human operator intent recognition during teleoperated robot navigation. In this context, recognition of the operator's intended navigational goal, could enable an artificial intelligence (AI) agent to assist the operator in an advanced human-robot interaction framework. We propose a Bayesian Operator Intent Recognition (BOIR) probabilistic method that utilizes: (i) an observation model that fuses information as a weighting combination of multiple observation sources providing geometric information; (ii) a transition model that indicates the evolution of the state; and (iii) an action model, the Active Intent Recognition Model (AIRM), that enables the operator to communicate their explicit intent asynchronously. The proposed method is evaluated in an experiment where operators controlling a remote mobile robot are tasked with navigation and exploration under various scenarios with different map and obstacle layouts. Results demonstrate that BOIR outperforms two related methods from literature in terms of accuracy and uncertainty of the intent recognition. △ Less","24 September, 2021",https://arxiv.org/pdf/2109.12045
Towards a Governance Framework for Brain Data,Marcello Ienca;Joseph J. Fins;Ralf J. Jox;Fabrice Jotterand;Silja Voeneky;Roberto Andorno;Tonio Ball;Claude Castelluccia;Ricardo Chavarriaga;Hervé Chneiweiss;Agata Ferretti;Orsolya Friedrich;Samia Hurst;Grischa Merkel;Fruzsina Molnar-Gabor;Jean-Marc Rickli;James Scheibner;Effy Vayena;Rafael Yuste;Philipp Kellmeyer,"The increasing availability of brain data within and outside the biomedical field, combined with the application of artificial intelligence (AI) to brain data analysis, poses a challenge for ethics and governance. We identify distinctive ethical implications of brain data acquisition and processing, and outline a multi-level governance framework. This framework is aimed at maximizing the benefits of facilitated brain data collection and further processing for science and medicine whilst minimizing risks and preventing harmful use. The framework consists of four primary areas of regulatory intervention: binding regulation, ethics and soft law, responsible innovation, and human rights. △ Less","28 September, 2021",https://arxiv.org/pdf/2109.11960
Document Automation Architectures and Technologies: A Survey,Mohammad Ahmadi Achachlouei;Omkar Patil;Tarun Joshi;Vijayan N. Nair,"This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in artificial intelligence and deep neural networks. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11603
Chess AI: Competing Paradigms for Machine Intelligence,Shiva Maharaj;Nick Polson;Alex Turk,"Endgame studies have long served as a tool for testing human creativity and intelligence. We find that they can serve as a tool for testing machine ability as well. Two of the leading chess engines, Stockfish and Leela Chess Zero (LCZero), employ significantly different methods during play. We use Plaskett's Puzzle, a famous endgame study from the late 1970s, to compare the two engines. Our experiments show that Stockfish outperforms LCZero on the puzzle. We examine the algorithmic differences between the engines and use our observations as a basis for carefully interpreting the test results. Drawing inspiration from how humans solve chess problems, we ask whether machines can possess a form of imagination. On the theoretical side, we describe how Bellman's equation may be applied to optimize the probability of winning. To conclude, we discuss the implications of our work on artificial intelligence (AI) and artificial general intelligence (AGI), suggesting possible avenues for future research. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11602
LSTM Hyper-Parameter Selection for Malware Detection: Interaction Effects and Hierarchical Selection Approach,Mohit Sewak;Sanjay K. Sahay;Hemant Rathore,"Long-Short-Term-Memory (LSTM) networks have shown great promise in artificial intelligence (AI) based language modeling. Recently, LSTM networks have also become popular for designing AI-based Intrusion Detection Systems (IDS). However, its applicability in IDS is studied largely in the default settings as used in language models. Whereas security applications offer distinct conditions and hence warrant careful consideration while applying such recurrent networks. Therefore, we conducted one of the most exhaustive works on LSTM hyper-parameters for IDS and experimented with approx. 150 LSTM configurations to determine its hyper-parameters relative importance, interaction effects, and optimal selection approach for designing an IDS. We conducted multiple analyses of the results of these experiments and empirically controlled for the interaction effects of different hyper-parameters covariate levels. We found that for security applications, especially for designing an IDS, neither similar relative importance as applicable to language models is valid, nor is the standard linear method for hyper-parameter selection ideal. We ascertained that the interaction effect plays a crucial role in determining the relative importance of hyper-parameters. We also discovered that after controlling for the interaction effect, the correct relative importance for LSTMs for an IDS is batch-size, followed by dropout ratio and padding. The findings are significant because when LSTM was first used for language models, the focus had mostly been on increasing the number of layers to enhance performance. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11500
Exploring Machine Teaching with Children,Utkarsh Dwivedi;Jaina Gandhi;Raj Parikh;Merijke Coenraad;Elizabeth Bonsignore;Hernisa Kacorri,"Iteratively building and testing machine learning models can help children develop creativity, flexibility, and comfort with machine learning and artificial intelligence. We explore how children use machine teaching interfaces with a team of 14 children (aged 7-13 years) and adult co-designers. Children trained image classifiers and tested each other's models for robustness. Our study illuminates how children reason about ML concepts, offering these insights for designing machine teaching experiences for children: (i) ML metrics (e.g. confidence scores) should be visible for experimentation; (ii) ML activities should enable children to exchange models for promoting reflection and pattern recognition; and (iii) the interface should allow quick data inspection (e.g. images vs. gestures). △ Less","27 September, 2021",https://arxiv.org/pdf/2109.11434
An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System,Pornpat Sirithumgul;Pimpaka Prasertsilp;Lorne Olfman,"This research is aimed to propose an artificial intelligence algorithm comprising an ontology-based design, text mining, and natural language processing for automatically generating gap-fill multiple choice questions (MCQs). The simulation of this research demonstrated an application of the algorithm in generating gap-fill MCQs about software testing. The simulation results revealed that by using 103 online documents as inputs, the algorithm could automatically produce more than 16 thousand valid gap-fill MCQs covering a variety of topics in the software testing domain. Finally, in the discussion section of this paper we suggest how the proposed algorithm should be applied to produce gap-fill MCQs being collected in a question pool used by a knowledge expert system. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.11421
Nine Challenges in Artificial Intelligence and Wireless Communications for 6G,Wen Tong;Geoffrey Ye Li,"In recent years, techniques developed in artificial intelligence (AI), especially those in machine learning (ML), have been successfully applied in various areas, leading to a widespread belief that AI will collectively play an important role in future wireless communications. To accomplish the aspiration, we present nine challenges to be addressed by the interdisciplinary areas of AI/ML and wireless communications, with particular focus towards the sixth generation (6G) wireless networks. Specifically, this article classifies the nine challenges into computation in AI, distributed neural networks and learning, and ML enabled semantic communications. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11320
Multi-view Contrastive Self-Supervised Learning of Accounting Data Representations for Downstream Audit Tasks,Marco Schreyer;Timur Sattarov;Damian Borth,"International audit standards require the direct assessment of a financial statement's underlying accounting transactions, referred to as journal entries. Recently, driven by the advances in artificial intelligence, deep learning inspired audit techniques have emerged in the field of auditing vast quantities of journal entry data. Nowadays, the majority of such methods rely on a set of specialized models, each trained for a particular audit task. At the same time, when conducting a financial statement audit, audit teams are confronted with (i) challenging time-budget constraints, (ii) extensive documentation obligations, and (iii) strict model interpretability requirements. As a result, auditors prefer to harness only a single preferably `multi-purpose' model throughout an audit engagement. We propose a contrastive self-supervised learning framework designed to learn audit task invariant accounting data representations to meet this requirement. The framework encompasses deliberate interacting data augmentation policies that utilize the attribute characteristics of journal entry data. We evaluate the framework on two real-world datasets of city payments and transfer the learned representations to three downstream audit tasks: anomaly detection, audit sampling, and audit documentation. Our experimental results provide empirical evidence that the proposed framework offers the ability to increase the efficiency of audits by learning rich and interpretable `multi-task' representations. △ Less","23 September, 2021",https://arxiv.org/pdf/2109.11201
Mapping and Validating a Point Neuron Model on Intel's Neuromorphic Hardware Loihi,Srijanie Dey;Alexander Dimitrov,"Neuromorphic hardware is based on emulating the natural biological structure of the brain. Since its computational model is similar to standard neural models, it could serve as a computational acceleration for research projects in the field of neuroscience and artificial intelligence, including biomedical applications. However, in order to exploit this new generation of computer chips, rigorous simulation and consequent validation of brain-based experimental data is imperative. In this work, we investigate the potential of Intel's fifth generation neuromorphic chip - `Loihi', which is based on the novel idea of Spiking Neural Networks (SNNs) emulating the neurons in the brain. The work is implemented in context of simulating the Leaky Integrate and Fire (LIF) models based on the mouse primary visual cortex matched to a rich data set of anatomical, physiological and behavioral constraints. Simulations on the classical hardware serve as the validation platform for the neuromorphic implementation. We find that Loihi replicates classical simulations very efficiently and scales notably well in terms of both time and energy performance as the networks get larger. △ Less","22 September, 2021",https://arxiv.org/pdf/2109.10835
Application of Video-to-Video Translation Networks to Computational Fluid Dynamics,Hiromitsu Kigure,"In recent years, the evolution of artificial intelligence, especially deep learning, has been remarkable, and its application to various fields has been growing rapidly. In this paper, I report the results of the application of generative adversarial networks (GANs), specifically video-to-video translation networks, to computational fluid dynamics (CFD) simulations. The purpose of this research is to reduce the computational cost of CFD simulations with GANs. The architecture of GANs in this research is a combination of the image-to-image translation networks (the so-called ""pix2pix"") and Long Short-Term Memory (LSTM). It is shown that the results of high-cost and high-accuracy simulations (with high-resolution computational grids) can be estimated from those of low-cost and low-accuracy simulations (with low-resolution grids). In particular, the time evolution of density distributions in the cases of a high-resolution grid is reproduced from that in the cases of a low-resolution grid through GANs, and the density inhomogeneity estimated from the image generated by GANs recovers the ground truth with good accuracy. Qualitative and quantitative comparisons of the results of the proposed method with those of several super-resolution algorithms are also presented. △ Less","11 September, 2021",https://arxiv.org/pdf/2109.10679
DialogueBERT: A Self-Supervised Learning based Dialogue Pre-training Encoder,Zhenyu Zhang;Tao Guo;Meng Chen,"With the rapid development of artificial intelligence, conversational bots have became prevalent in mainstream E-commerce platforms, which can provide convenient customer service timely. To satisfy the user, the conversational bots need to understand the user's intention, detect the user's emotion, and extract the key entities from the conversational utterances. However, understanding dialogues is regarded as a very challenging task. Different from common language understanding, utterances in dialogues appear alternately from different roles and are usually organized as hierarchical structures. To facilitate the understanding of dialogues, in this paper, we propose a novel contextual dialogue encoder (i.e. DialogueBERT) based on the popular pre-trained language model BERT. Five self-supervised learning pre-training tasks are devised for learning the particularity of dialouge utterances. Four different input embeddings are integrated to catch the relationship between utterances, including turn embedding, role embedding, token embedding and position embedding. DialogueBERT was pre-trained with 70 million dialogues in real scenario, and then fine-tuned in three different downstream dialogue understanding tasks. Experimental results show that DialogueBERT achieves exciting results with 88.63% accuracy for intent recognition, 94.25% accuracy for emotion recognition and 97.04% F1 score for named entity recognition, which outperforms several strong baselines by a large margin. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.10480
Learning through structure: towards deep neuromorphic knowledge graph embeddings,Victor Caceres Chian;Marcel Hildebrandt;Thomas Runkler;Dominik Dold,"Computing latent representations for graph-structured data is an ubiquitous learning task in many industrial and academic applications ranging from molecule synthetization to social network analysis and recommender systems. Knowledge graphs are among the most popular and widely used data representations related to the Semantic Web. Next to structuring factual knowledge in a machine-readable format, knowledge graphs serve as the backbone of many artificial intelligence applications and allow the ingestion of context information into various learning algorithms. Graph neural networks attempt to encode graph structures in low-dimensional vector spaces via a message passing heuristic between neighboring nodes. Over the recent years, a multitude of different graph neural network architectures demonstrated ground-breaking performances in many learning tasks. In this work, we propose a strategy to map deep graph learning architectures for knowledge graph reasoning to neuromorphic architectures. Based on the insight that randomly initialized and untrained (i.e., frozen) graph neural networks are able to preserve local graph structures, we compose a frozen neural network with shallow knowledge graph embedding models. We experimentally show that already on conventional computing hardware, this leads to a significant speedup and memory reduction while maintaining a competitive performance level. Moreover, we extend the frozen architecture to spiking neural networks, introducing a novel, event-based and highly sparse knowledge graph embedding algorithm that is suitable for implementation in neuromorphic hardware. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.10376
Learning offline: memory replay in biological and artificial reinforcement learning,Emma L. Roscow;Raymond Chua;Rui Ponte Costa;Matt W. Jones;Nathan Lepora,"Learning to act in an environment to maximise rewards is among the brain's key functions. This process has often been conceptualised within the framework of reinforcement learning, which has also gained prominence in machine learning and artificial intelligence (AI) as a way to optimise decision-making. A common aspect of both biological and machine reinforcement learning is the reactivation of previously experienced episodes, referred to as replay. Replay is important for memory consolidation in biological neural networks, and is key to stabilising learning in deep neural networks. Here, we review recent developments concerning the functional roles of replay in the fields of neuroscience and AI. Complementary progress suggests how replay might support learning processes, including generalisation and continual learning, affording opportunities to transfer knowledge across the two fields to advance the understanding of biological and artificial learning and memory. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.10034
Assessing clinical utility of Machine Learning and Artificial Intelligence approaches to analyze speech recordings in Multiple Sclerosis: A Pilot Study,Emil Svoboda;Tomáš Bořil;Jan Rusz;Tereza Tykalová;Dana Horáková;Charles R. G. Guttman;Krastan B. Blagoev;Hiroto Hatabu;Vlad I. Valtchinov,"Background: An early diagnosis together with an accurate disease progression monitoring of multiple sclerosis is an important component of successful disease management. Prior studies have established that multiple sclerosis is correlated with speech discrepancies. Early research using objective acoustic measurements has discovered measurable dysarthria. Objective: To determine the potential clinical utility of machine learning and deep learning/AI approaches for the aiding of diagnosis, biomarker extraction and progression monitoring of multiple sclerosis using speech recordings. Methods: A corpus of 65 MS-positive and 66 healthy individuals reading the same text aloud was used for targeted acoustic feature extraction utilizing automatic phoneme segmentation. A series of binary classification models was trained, tuned, and evaluated regarding their Accuracy and area-under-curve. Results: The Random Forest model performed best, achieving an Accuracy of 0.82 on the validation dataset and an area-under-curve of 0.76 across 5 k-fold cycles on the training dataset. 5 out of 7 acoustic features were statistically significant. Conclusion: Machine learning and artificial intelligence in automatic analyses of voice recordings for aiding MS diagnosis and progression tracking seems promising. Further clinical validation of these methods and their mapping onto multiple sclerosis progression is needed, as well as a validating utility for English-speaking populations. △ Less","27 September, 2021",https://arxiv.org/pdf/2109.09844
Prediction of severe thunderstorm events with ensemble deep learning and radar data,Sabrina Guastavino;Michele Piana;Marco Tizzi;Federico Cassola;Antonio Iengo;Davide Sacchetti;Enrico Solazzo;Federico Benvenuto,"The problem of nowcasting extreme weather events can be addressed by applying either numerical methods for the solution of dynamic model equations or data-driven artificial intelligence algorithms. Within this latter framework, the present paper illustrates how a deep learning method, exploiting videos of radar reflectivity frames as input, can be used to realize a warning machine able to sound timely alarms of possible severe thunderstorm events. From a technical viewpoint, the computational core of this approach is the use of a value-weighted skill score for both transforming the probabilistic outcomes of the deep neural network into binary classification and assessing the forecasting performances. The warning machine has been validated against weather radar data recorded in the Liguria region, in Italy, △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09791
Configuring Multiple Instances with Multi-Configuration,Alexander Felfernig;Andrei Popescu;Mathias Uta;Viet-Man Le;Seda Polat-Erdeniz;Martin Stettinger;Müslüm Atas;Thi Ngoc Trang Tran,"Configuration is a successful application area of Artificial Intelligence. In the majority of the cases, configuration systems focus on configuring one solution (configuration) that satisfies the preferences of a single user or a group of users. In this paper, we introduce a new configuration approach - multi-configuration - that focuses on scenarios where the outcome of a configuration process is a set of configurations. Example applications thereof are the configuration of personalized exams for individual students, the configuration of project teams, reviewer-to-paper assignment, and hotel room assignments including individualized city trips for tourist groups. For multi-configuration scenarios, we exemplify a constraint satisfaction problem representation in the context of configuring exams. The paper is concluded with a discussion of open issues for future work. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09696
Actionable Approaches to Promote Ethical AI in Libraries,Helen Bubinger;Jesse David Dinneen,"The widespread use of artificial intelligence (AI) in many domains has revealed numerous ethical issues from data and design to deployment. In response, countless broad principles and guidelines for ethical AI have been published, and following those, specific approaches have been proposed for how to encourage ethical outcomes of AI. Meanwhile, library and information services too are seeing an increase in the use of AI-powered and machine learning-powered information systems, but no practical guidance currently exists for libraries to plan for, evaluate, or audit the ethics of intended or deployed AI. We therefore report on several promising approaches for promoting ethical AI that can be adapted from other contexts to AI-powered information services and in different stages of the software lifecycle. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09672
Emulation of Synaptic Plasticity on Cobalt based Synaptic Transistor for Neuromorphic Computing,P. Monalisha;P. S. Anil Kumar;X. Renshaw Wang;S. N. Piramanayagam,"Neuromorphic Computing (NC), which emulates neural activities of the human brain, is considered for low-power implementation of artificial intelligence. Towards realizing NC, fabrication, and investigations of hardware elements such as synaptic devices and neurons are essential. Electrolyte gating has been widely used for conductance modulation by massive carrier injections and has proven to be an effective way of emulating biological synapses. Synaptic devices, in the form of synaptic transistors, have been studied using a wide variety of materials. However, studies on metallic channel based synaptic transistors remain vastly unexplored. Here, we have demonstrated a three-terminal cobalt-based synaptic transistor to emulate biological synapse. We realized gating controlled multilevel, nonvolatile conducting states in the proposed device. The device could successfully emulate essential synaptic functions demonstrating short-term and long-term plasticity. A transition from short-term memory to long-term memory has been realized by tuning gate pulse amplitude and duration. The crucial cognitive behavior viz., learning, forgetting, and relearning, has been emulated, showing resemblance to the human brain. Along with learning and memory, the device showed dynamic filtering behavior. These results provide an insight into the design of metallic channel based synaptic transistors for neuromorphic computing. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09613
Real-Time Trash Detection for Modern Societies using CCTV to Identifying Trash by utilizing Deep Convolutional Neural Network,Syed Muhammad Raza;Syed Muhammad Ghazi Hassan;Syed Ali Hassan;Soo Young Shin,"To protect the environment from trash pollution, especially in societies, and to take strict action against the red-handed people who throws the trash. As modern societies are developing and these societies need a modern solution to make the environment clean. Artificial intelligence (AI) evolution, especially in Deep Learning, gives an excellent opportunity to develop real-time trash detection using CCTV cameras. The inclusion of this project is real-time trash detection using a deep model of Convolutional Neural Network (CNN). It is used to obtain eight classes mask, tissue papers, shoppers, boxes, automobile parts, pampers, bottles, and juices boxes. After detecting the trash, the camera records the video of that person for ten seconds who throw trash in society. The challenging part of this paper is preparing a complex custom dataset that took too much time. The dataset consists of more than 2100 images. The CNN model was created, labeled, and trained. The detection time accuracy and average mean precision (mAP) benchmark both models' performance. In experimental phase the mAP performance and accuracy of the improved CNN model was superior in all aspects. The model is used on a CCTV camera to detect trash in real-time. △ Less","21 September, 2021",https://arxiv.org/pdf/2109.09611
Some Critical and Ethical Perspectives on the Empirical Turn of AI Interpretability,Jean-Marie John-Mathews,"We consider two fundamental and related issues currently faced by Artificial Intelligence (AI) development: the lack of ethics and interpretability of AI decisions. Can interpretable AI decisions help to address ethics in AI? Using a randomized study, we experimentally show that the empirical and liberal turn of the production of explanations tends to select AI explanations with a low denunciatory power. Under certain conditions, interpretability tools are therefore not means but, paradoxically, obstacles to the production of ethical AI since they can give the illusion of being sensitive to ethical incidents. We also show that the denunciatory power of AI explanations is highly dependent on the context in which the explanation takes place, such as the gender or education level of the person to whom the explication is intended for. AI ethics tools are therefore sometimes too flexible and self-regulation through the liberal production of explanations do not seem to be enough to address ethical issues. We then propose two scenarios for the future development of ethical AI: more external regulation or more liberalization of AI explanations. These two opposite paths will play a major role on the future development of ethical AI. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09586
Incremental Learning Techniques for Online Human Activity Recognition,Meysam Vakili;Masoumeh Rezaei,"Unobtrusive and smart recognition of human activities using smartphones inertial sensors is an interesting topic in the field of artificial intelligence acquired tremendous popularity among researchers, especially in recent years. A considerable challenge that needs more attention is the real-time detection of physical activities, since for many real-world applications such as health monitoring and elderly care, it is required to recognize users' activities immediately to prevent severe damages to individuals' wellness. In this paper, we propose a human activity recognition (HAR) approach for the online prediction of physical movements, benefiting from the capabilities of incremental learning algorithms. We develop a HAR system containing monitoring software and a mobile application that collects accelerometer and gyroscope data and send them to a remote server via the Internet for classification and recognition operations. Six incremental learning algorithms are employed and evaluated in this work and compared with several batch learning algorithms commonly used for developing offline HAR systems. The Final results indicated that considering all performance evaluation metrics, Incremental K-Nearest Neighbors and Incremental Naive Bayesian outperformed other algorithms, exceeding a recognition accuracy of 95% in real-time. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09435
Improved AI-based segmentation of apical and basal slices from clinical cine CMR,Jorge Mariscal-Harana;Naomi Kifle;Reza Razavi;Andrew P. King;Bram Ruijsink;Esther Puyol-Antón,"Current artificial intelligence (AI) algorithms for short-axis cardiac magnetic resonance (CMR) segmentation achieve human performance for slices situated in the middle of the heart. However, an often-overlooked fact is that segmentation of the basal and apical slices is more difficult. During manual analysis, differences in the basal segmentations have been reported as one of the major sources of disagreement in human interobserver variability. In this work, we aim to investigate the performance of AI algorithms in segmenting basal and apical slices and design strategies to improve their segmentation. We trained all our models on a large dataset of clinical CMR studies obtained from two NHS hospitals (n=4,228) and evaluated them against two external datasets: ACDC (n=100) and M&Ms (n=321). Using manual segmentations as a reference, CMR slices were assigned to one of four regions: non-cardiac, base, middle, and apex. Using the nnU-Net framework as a baseline, we investigated two different approaches to reduce the segmentation performance gap between cardiac regions: (1) non-uniform batch sampling, which allows us to choose how often images from different regions are seen during training; and (2) a cardiac-region classification model followed by three (i.e. base, middle, and apex) region-specific segmentation models. We show that the classification and segmentation approach was best at reducing the performance gap across all datasets. We also show that improvements in the classification performance can subsequently lead to a significantly better performance in the segmentation task. △ Less","20 September, 2021",https://arxiv.org/pdf/2109.09421
"Ti_3
C_2
T_x
MXene Enabled All-Optical Nonlinear Activation Function for On-Chip Photonic Deep Neural Networks",Adir Hazan;Barak Ratzker;Danzhen Zhang;Aviad Katiyi;Nachum Frage;Maxim Sokol;Yury Gogotsi;Alina Karabchevsky,"Neural networks are one of the first major milestones in developing artificial intelligence systems. The utilisation of integrated photonics in neural networks offers a promising alternative approach to microelectronic and hybrid optical-electronic implementations due to improvements in computational speed and low energy consumption in machine-learning tasks. However, at present, most of the neural network hardware systems are still electronic-based due to a lack of optical realisation of the nonlinear activation function. Here, we experimentally demonstrate two novel approaches for implementing an all-optical neural nonlinear activation function based on utilising unique light-matter interactions in 2D Ti_3C_2T_x (MXene) in the infrared (IR) range in two configurations: 1) a saturable absorber made of MXene thin film, and 2) a silicon waveguide with MXene flakes overlayer. These configurations may serve as nonlinear units in photonic neural networks, while their nonlinear transfer function can be flexibly designed to optimise the performance of different neuromorphic tasks, depending on the operating wavelength. The proposed configurations are reconfigurable and can therefore be adjusted for various applications without the need to modify the physical structure. We confirm the capability and feasibility of the obtained results in machine-learning applications via an Modified National Institute of Standards and Technology (MNIST) handwritten digit classifications task, with near 99% accuracy. Our developed concept for an all-optical neuron is expected to constitute a major step towards the realization of all-optically implemented deep neural networks. △ Less","19 September, 2021",https://arxiv.org/pdf/2109.09177
Towards Resilient Artificial Intelligence: Survey and Research Issues,Oliver Eigner;Sebastian Eresheim;Peter Kieseberg;Lukas Daniel Klausner;Martin Pirker;Torsten Priebe;Simon Tjoa;Fiammetta Marulli;Francesco Mercaldo,"Artificial intelligence (AI) systems are becoming critical components of today's IT landscapes. Their resilience against attacks and other environmental influences needs to be ensured just like for other IT assets. Considering the particular nature of AI, and machine learning (ML) in particular, this paper provides an overview of the emerging field of resilient AI and presents research issues the authors identify as potential future work. △ Less","18 September, 2021",https://arxiv.org/pdf/2109.08904
Computational Imaging and Artificial Intelligence: The Next Revolution of Mobile Vision,Jinli Suo;Weihang Zhang;Jin Gong;Xin Yuan;David J. Brady;Qionghai Dai,"Signal capture stands in the forefront to perceive and understand the environment and thus imaging plays the pivotal role in mobile vision. Recent explosive progresses in Artificial Intelligence (AI) have shown great potential to develop advanced mobile platforms with new imaging devices. Traditional imaging systems based on the ""capturing images first and processing afterwards"" mechanism cannot meet this unprecedented demand. Differently, Computational Imaging (CI) systems are designed to capture high-dimensional data in an encoded manner to provide more information for mobile vision systems.Thanks to AI, CI can now be used in real systems by integrating deep learning algorithms into the mobile vision platform to achieve the closed loop of intelligent acquisition, processing and decision making, thus leading to the next revolution of mobile vision.Starting from the history of mobile vision using digital cameras, this work first introduces the advances of CI in diverse applications and then conducts a comprehensive review of current research topics combining CI and AI. Motivated by the fact that most existing studies only loosely connect CI and AI (usually using AI to improve the performance of CI and only limited works have deeply connected them), in this work, we propose a framework to deeply integrate CI and AI by using the example of self-driving vehicles with high-speed communication, edge computing and traffic planning. Finally, we outlook the future of CI plus AI by investigating new materials, brain science and new computing techniques to shed light on new directions of mobile vision systems. △ Less","18 September, 2021",https://arxiv.org/pdf/2109.08880
Multi-terminal memristive devices enabling tunable synaptic plasticity in neuromorphic hardware: a mini-review,Yann Beilliard;Fabien Alibart,"Neuromorphic computing based on spiking neural networks has the potential to significantly improve on-line learning capabilities and energy efficiency of artificial intelligence, specially for edge computing. Recent progress in computational neuroscience have demonstrated the importance of heterosynaptic plasticity for network activity regulation and memorization. Implementing heterosynaptic plasticity in hardware is thus highly desirable, but important materials and engineering challenges remain, calling for breakthroughs in neuromorphic devices. In this mini-review, we propose an overview of the latest advances in multi-terminal memristive devices on silicon with tunable synaptic plasticity, enabling heterosynaptic plasticity in hardware. The scalability and compatibility of the devices with industrial complementary metal oxide semiconductor (CMOS) technologies are discussed. △ Less","17 September, 2021",https://arxiv.org/pdf/2109.08720
Comprehensive Multi-Agent Epistemic Planning,Francesco Fabiano,"Over the last few years, the concept of Artificial Intelligence has become central in different tasks concerning both our daily life and several working scenarios. Among these tasks automated planning has always been central in the AI research community. In particular, this manuscript is focused on a specialized kind of planning known as Multi-agent Epistemic Planning (MEP). Epistemic Planning (EP) refers to an automated planning setting where the agent reasons in the space of knowledge/beliefs states and tries to find a plan to reach a desirable state from a starting one. Its general form, the MEP problem, involves multiple agents who need to reason about both the state of the world and the information flows between agents. To tackle the MEP problem several tools have been developed and, while the diversity of approaches has led to a deeper understanding of the problem space, each proposed tool lacks some abilities and does not allow for a comprehensive investigation of the information flows. That is why, the objective of our work is to formalize an environment where a complete characterization of the agents' knowledge/beliefs interaction and update is possible. In particular, we aim to achieve such goal by defining a new action-based language for multi-agent epistemic planning and to implement an epistemic planner based on it. This solver should provide a tool flexible enough to reason on different domains, e.g., economy, security, justice and politics, where considering others' knowledge/beliefs could lead to winning strategies. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.08301
"CompilerGym: Robust, Performant Compiler Optimization Environments for AI Research",Chris Cummins;Bram Wasti;Jiadong Guo;Brandon Cui;Jason Ansel;Sahir Gomez;Somya Jain;Jia Liu;Olivier Teytaud;Benoit Steiner;Yuandong Tian;Hugh Leather,"Interest in applying Artificial Intelligence (AI) techniques to compiler optimizations is increasing rapidly, but compiler research has a high entry barrier. Unlike in other domains, compiler and AI researchers do not have access to the datasets and frameworks that enable fast iteration and development of ideas, and getting started requires a significant engineering investment. What is needed is an easy, reusable experimental infrastructure for real world compiler optimization tasks that can serve as a common benchmark for comparing techniques, and as a platform to accelerate progress in the field. We introduce CompilerGym, a set of environments for real world compiler optimization tasks, and a toolkit for exposing new optimization tasks to compiler researchers. CompilerGym enables anyone to experiment on production compiler optimization problems through an easy-to-use package, regardless of their experience with compilers. We build upon the popular OpenAI Gym interface enabling researchers to interact with compilers using Python and a familiar API. We describe the CompilerGym architecture and implementation, characterize the optimization spaces and computational efficiencies of three included compiler environments, and provide extensive empirical evaluations. Compared to prior works, CompilerGym offers larger datasets and optimization spaces, is 27x more computationally efficient, is fault-tolerant, and capable of detecting reproducibility bugs in the underlying compilers. In making it easy for anyone to experiment with compilers - irrespective of their background - we aim to accelerate progress in the AI and compiler research domains. △ Less","22 December, 2021",https://arxiv.org/pdf/2109.08267
Deep Algorithmic Question Answering: Towards a Compositionally Hybrid AI for Algorithmic Reasoning,Kwabena Nuamah,"An important aspect of artificial intelligence (AI) is the ability to reason in a step-by-step ""algorithmic"" manner that can be inspected and verified for its correctness. This is especially important in the domain of question answering (QA). We argue that the challenge of algorithmic reasoning in QA can be effectively tackled with a ""systems"" approach to AI which features a hybrid use of symbolic and sub-symbolic methods including deep neural networks. Additionally, we argue that while neural network models with end-to-end training pipelines perform well in narrow applications such as image classification and language modelling, they cannot, on their own, successfully perform algorithmic reasoning, especially if the task spans multiple domains. We discuss a few notable exceptions and point out how they are still limited when the QA problem is widened to include other intelligence-requiring tasks. However, deep learning, and machine learning in general, do play important roles as components in the reasoning process. We propose an approach to algorithm reasoning for QA, Deep Algorithmic Question Answering (DAQA), based on three desirable properties: interpretability, generalizability, and robustness which such an AI system should possess, and conclude that they are best achieved with a combination of hybrid and compositional AI. △ Less","5 November, 2021",https://arxiv.org/pdf/2109.08006
Behavior of Keyword Spotting Networks Under Noisy Conditions,Anwesh Mohanty;Adrian Frischknecht;Christoph Gerum;Oliver Bringmann,"Keyword spotting (KWS) is becoming a ubiquitous need with the advancement in artificial intelligence and smart devices. Recent work in this field have focused on several different architectures to achieve good results on datasets with low to moderate noise. However, the performance of these models deteriorates under high noise conditions as shown by our experiments. In our paper, we present an extensive comparison between state-of-the-art KWS networks under various noisy conditions. We also suggest adaptive batch normalization as a technique to improve the performance of the networks when the noise files are unknown during the training phase. The results of such high noise characterization enable future work in developing models that perform better in the aforementioned conditions. △ Less","15 September, 2021",https://arxiv.org/pdf/2109.07930
Risk Management of AI/ML Software as a Medical Device (SaMD): On ISO 14971 and Related Standards and Guidances,Stephen G. Odaibo,"Safety and efficacy are the paramount objectives of medical device regulation. And in line with the medical ethos of non-maleficence, first do no harm, safety is the primary goal of regulation also. As such, risk management is the underlying principle that governs the regulation of medical devices, whether traditional devices or Software as a Medical Device (SaMD). In this article, I review how Risk Management Standard ISO 14971:2019 both connects with and serves as a foundation for the other parts of the Artificial Intelligence (AI)/Machine Learning (ML) SaMD regulatory framework. △ Less","11 September, 2021",https://arxiv.org/pdf/2109.07905
Automated risk classification of colon biopsies based on semantic segmentation of histopathology images,John-Melle Bokhorst;Iris D. Nagtegaal;Filippo Fraggetta;Simona Vatrano;Wilma Mesker;Michael Vieth;Jeroen van der Laak;Francesco Ciompi,"Artificial Intelligence (AI) can potentially support histopathologists in the diagnosis of a broad spectrum of cancer types. In colorectal cancer (CRC), AI can alleviate the laborious task of characterization and reporting on resected biopsies, including polyps, the numbers of which are increasing as a result of CRC population screening programs, ongoing in many countries all around the globe. Here, we present an approach to address two major challenges in automated assessment of CRC histopathology whole-slide images. First, we present an AI-based method to segment multiple tissue compartments in the H\&E-stained whole-slide image, which provides a different, more perceptible picture of tissue morphology and composition. We test and compare a panel of state-of-the-art loss functions available for segmentation models, and provide indications about their use in histopathology image segmentation, based on the analysis of a) a multi-centric cohort of CRC cases from five medical centers in the Netherlands and Germany, and b) two publicly available datasets on segmentation in CRC. Second, we use the best performing AI model as the basis for a computer-aided diagnosis system (CAD) that classifies colon biopsies into four main categories that are relevant pathologically. We report the performance of this system on an independent cohort of more than 1,000 patients. The results show the potential of such an AI-based system to assist pathologists in diagnosis of CRC in the context of population screening. We have made the segmentation model available for research use on https://grand-challenge.org/algorithms/colon-tissue-segmentation/. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.07892
Humanly Certifying Superhuman Classifiers,Qiongkai Xu;Christian Walder;Chenchen Xu,"Estimating the performance of a machine learning system is a longstanding challenge in artificial intelligence research. Today, this challenge is especially relevant given the emergence of systems which appear to increasingly outperform human beings. In some cases, this ""superhuman"" performance is readily demonstrated; for example by defeating legendary human players in traditional two player games. On the other hand, it can be challenging to evaluate classification models that potentially surpass human performance. Indeed, human annotations are often treated as a ground truth, which implicitly assumes the superiority of the human over any models trained on human annotations. In reality, human annotators can make mistakes and be subjective. Evaluating the performance with respect to a genuine oracle may be more objective and reliable, even when querying the oracle is expensive or impossible. In this paper, we first raise the challenge of evaluating the performance of both humans and models with respect to an oracle which is unobserved. We develop a theory for estimating the accuracy compared to the oracle, using only imperfect human annotations for reference. Our analysis provides a simple recipe for detecting and certifying superhuman performance in this setting, which we believe will assist in understanding the stage of current research on classification. We validate the convergence of the bounds and the assumptions of our theory on carefully designed toy experiments with known oracles. Moreover, we demonstrate the utility of our theory by meta-analyzing large-scale natural language processing tasks, for which an oracle does not exist, and show that under our assumptions a number of models from recent years are with high probability superhuman. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.07867
AI video editing tools. What editors want and how far is AI from delivering?,Than Htut Soe,"Video editing can be a very tedious task, so unsurprisingly Artificial Intelligence has been increasingly used to streamline the workflow or automate away tedious tasks. However, it is very difficult to get an overview of what intelligent video editing tools are in the research literature and needs for automation from the video editors. So, we identified the field of intelligent video editing tools in research, and we survey the opinions of professional video editors. We have also summarized current state of the art in artificial intelligence research with the intention of identifying what are the possibilities and current technical limits towards truly intelligent video editing tools. The findings contribute towards understanding of the field of intelligent video editing tools, highlights unaddressed automation needs by the survey and provides general suggestions for further research in intelligent video editing tools. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.07809
Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning,Shikha Dubey;Farrukh Olimov;Muhammad Aasim Rafique;Joonmo Kim;Moongu Jeon,"Automatic transcription of scene understanding in images and videos is a step towards artificial general intelligence. Image captioning is a nomenclature for describing meaningful information in an image using computer vision techniques. Automated image captioning techniques utilize encoder and decoder architecture, where the encoder extracts features from an image and the decoder generates a transcript. In this work, we investigate two unexplored ideas for image captioning using transformers: First, we demonstrate the enforcement of using objects' relevance in the surrounding environment. Second, learning an explicit association between labels and language constructs. We propose label-attention Transformer with geometrically coherent objects (LATGeO). The proposed technique acquires a proposal of geometrically coherent objects using a deep neural network (DNN) and generates captions by investigating their relationships using a label-attention module. Object coherence is defined using the localized ratio of the geometrical properties of the proposals. The label-attention module associates the extracted objects classes to the available dictionary using self-attention layers. The experimentation results show that objects' relevance in surroundings and binding of their visual feature with their geometrically localized ratios combined with its associated labels help in defining meaningful captions. The proposed framework is tested on the MSCOCO dataset, and a thorough evaluation resulting in overall better quantitative scores pronounces its superiority. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.07799
Beyond 5G RIS mmWave Systems: Where Communication and Localization Meet,Jiguang He;Fan Jiang;Kamran Keykhosravi;Joonas Kokkoniemi;Henk Wymeersch;Markku Juntti,"Upcoming beyond fifth generation (5G) communications systems aim at further enhancing key performance indicators and fully supporting brand new use cases by embracing emerging techniques, e.g., reconfigurable intelligent surface (RIS), integrated communication, localization, and sensing, and mmWave/THz communications. The wireless intelligence empowered by state-of-the-art artificial intelligence techniques has been widely considered at the transceivers, and now the paradigm is deemed to be shifted to the smart control of radio propagation environment by virtue of RISs. In this article, we argue that to harness the full potential of RISs, localization and communication must be tightly coupled. This is in sharp contrast to 5G and earlier generations, where localization was a minor additional service. To support this, we first introduce the fundamentals of RIS mmWave channel modeling, followed by RIS channel state information acquisition and link establishment. Then, we deal with the connection between localization and communications, from a separate and joint perspective. △ Less","16 September, 2021",https://arxiv.org/pdf/2109.07729
Network representation learning systematic review: ancestors and current development state,Amina Amara;Mohamed Ali Hadj Taieb;Mohamed Ben Aouicha,"Real-world information networks are increasingly occurring across various disciplines including online social networks and citation networks. These network data are generally characterized by sparseness, nonlinearity and heterogeneity bringing different challenges to the network analytics task to capture inherent properties from network data. Artificial intelligence and machine learning have been recently leveraged as powerful systems to learn insights from network data and deal with presented challenges. As part of machine learning techniques, graph embedding approaches are originally conceived for graphs constructed from feature represented datasets, like image dataset, in which links between nodes are explicitly defined. These traditional approaches cannot cope with network data challenges. As a new learning paradigm, network representation learning has been proposed to map a real-world information network into a low-dimensional space while preserving inherent properties of the network. In this paper, we present a systematic comprehensive survey of network representation learning, known also as network embedding, from birth to the current development state. Through the undertaken survey, we provide a comprehensive view of reasons behind the emergence of network embedding and, types of settings and models used in the network embedding pipeline. Thus, we introduce a brief history of representation learning and word representation learning ancestor of network embedding. We provide also formal definitions of basic concepts required to understand network representation learning followed by a description of network embedding pipeline. Most commonly used downstream tasks to evaluate embeddings, their evaluation metrics and popular datasets are highlighted. Finally, we present the open-source libraries for network embedding. △ Less","14 September, 2021",https://arxiv.org/pdf/2109.07583
Estimation of Warfarin Dosage with Reinforcement Learning,Arpita Vats,"In this paper, it has attempted to use Reinforcement learning to model the proper dosage of Warfarin for patients.The paper first examines two baselines: a fixed model of 35 mg/week dosages and a linear model that relies on patient data. We implemented a LinUCB bandit that improved performance measured on regret and percent incorrect. On top of the LinUCB bandit, we experimented with online supervised learning and reward reshaping to boost performance. Our results clearly beat the baselines and show the promise of using multi-armed bandits and artificial intelligence to aid physicians in deciding proper dosages. △ Less","15 September, 2021",https://arxiv.org/pdf/2109.07564
Can Machines Read Coding Manuals Yet? -- A Benchmark for Building Better Language Models for Code Understanding,Ibrahim Abdelaziz;Julian Dolby;Jamie McCusker;Kavitha Srinivas,"Code understanding is an increasingly important application of Artificial Intelligence. A fundamental aspect of understanding code is understanding text about code, e.g., documentation and forum discussions. Pre-trained language models (e.g., BERT) are a popular approach for various NLP tasks, and there are now a variety of benchmarks, such as GLUE, to help improve the development of such models for natural language understanding. However, little is known about how well such models work on textual artifacts about code, and we are unaware of any systematic set of downstream tasks for such an evaluation. In this paper, we derive a set of benchmarks (BLANCA - Benchmarks for LANguage models on Coding Artifacts) that assess code understanding based on tasks such as predicting the best answer to a question in a forum post, finding related forum posts, or predicting classes related in a hierarchy from class documentation. We evaluate the performance of current state-of-the-art language models on these tasks and show that there is a significant improvement on each task from fine tuning. We also show that multi-task training over BLANCA tasks helps build better language models for code understanding. △ Less","15 September, 2021",https://arxiv.org/pdf/2109.07452
NBcoded: network attack classifiers based on Encoder and Naive Bayes model for resource limited devices,Lander Segurola-Gil;Francesco Zola;Xabier Echeberria-Barrio;Raul Orduna-Urrutia,"In the recent years, cybersecurity has gained high relevance, converting the detection of attacks or intrusions into a key task. In fact, a small breach in a system, application, or network, can cause huge damage for the companies. However, when this attack detection encounters the Artificial Intelligence paradigm, it can be addressed using high-quality classifiers which often need high resource demands in terms of computation or memory usage. This situation has a high impact when the attack classifiers need to be used with limited resourced devices or without overloading the performance of the devices, as it happens for example in IoT devices, or in industrial systems. For overcoming this issue, NBcoded, a novel light attack classification tool is proposed in this work. NBcoded works in a pipeline combining the removal of noisy data properties of the encoders with the low resources and timing consuming obtained by the Naive Bayes classifier. This work compares three different NBcoded implementations based on three different Naive Bayes likelihood distribution assumptions (Gaussian, Complement and Bernoulli). Then, the best NBcoded is compared with state of the art classifiers like Multilayer Perceptron and Random Forest. Our implementation shows to be the best model reducing the impact of training time and disk usage, even if it is outperformed by the other two in terms of Accuracy and F1-score (~ 2%). △ Less","15 September, 2021",https://arxiv.org/pdf/2109.07273
"Agile, Antifragile, Artificial-Intelligence-Enabled, Command and Control",Jacob Simpson;Rudolph Oosthuizen;Sondoss El Sawah;Hussein Abbass,"Artificial Intelligence (AI) is rapidly becoming integrated into military Command and Control (C2) systems as a strategic priority for many defence forces. The successful implementation of AI is promising to herald a significant leap in C2 agility through automation. However, realistic expectations need to be set on what AI can achieve in the foreseeable future. This paper will argue that AI could lead to a fragility trap, whereby the delegation of C2 functions to an AI could increase the fragility of C2, resulting in catastrophic strategic failures. This calls for a new framework for AI in C2 to avoid this trap. We will argue that antifragility along with agility should form the core design principles for AI-enabled C2 systems. This duality is termed Agile, Antifragile, AI-Enabled Command and Control (A3IC2). An A3IC2 system continuously improves its capacity to perform in the face of shocks and surprises through overcompensation from feedback during the C2 decision-making cycle. An A3IC2 system will not only be able to survive within a complex operational environment, it will also thrive, benefiting from the inevitable shocks and volatility of war. △ Less","14 September, 2021",https://arxiv.org/pdf/2109.06874
Deep Convolutional Generative Modeling for Artificial Microstructure Development of Aluminum-Silicon Alloy,Akshansh Mishra;Tarushi Pathak,"Machine learning which is a sub-domain of an Artificial Intelligence which is finding various applications in manufacturing and material science sectors. In the present study, Deep Generative Modeling which a type of unsupervised machine learning technique has been adapted for the constructing the artificial microstructure of Aluminium-Silicon alloy. Deep Generative Adversarial Networks has been used for developing the artificial microstructure of the given microstructure image dataset. The results obtained showed that the developed models had learnt to replicate the lining near the certain images of the microstructures. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.06635
A Novel Data Encryption Method Inspired by Adversarial Attacks,Praveen Fernando;Jin Wei-Kocsis,"Due to the advances of sensing and storage technologies, a tremendous amount of data becomes available and, it supports the phenomenal growth of artificial intelligence (AI) techniques especially, deep learning (DL), in various application domains. While the data sources become valuable assets for enabling the success of autonomous decision-making, they also lead to critical vulnerabilities in privacy and security. For example, data leakage can be exploited via querying and eavesdropping in the exploratory phase for black-box attacks against DL-based autonomous decision-making systems. To address this issue, in this work, we propose a novel data encryption method, called AdvEncryption, by exploiting the principle of adversarial attacks. Different from existing encryption technologies, the AdvEncryption method is not developed to prevent attackers from exploiting the dataset. Instead, our proposed method aims to trap the attackers in a misleading feature distillation of the data. To achieve this goal, our AdvEncryption method consists of two essential components: 1) an adversarial attack-inspired encryption mechanism to encrypt the data with stealthy adversarial perturbation, and 2) a decryption mechanism that minimizes the impact of the perturbations on the effectiveness of autonomous decision making. In the performance evaluation section, we evaluate the performance of our proposed AdvEncryption method through case studies considering different scenarios. △ Less","14 September, 2021",https://arxiv.org/pdf/2109.06634
Open-World Active Learning with Stacking Ensemble for Self-Driving Cars,Paulo R. Vieira;Pedro D. Félix;Luis Macedo,"The environments, in which autonomous cars act, are high-risky, dynamic, and full of uncertainty, demanding a continuous update of their sensory information and knowledge bases. The frequency of facing an unknown object is too high making hard the usage of Artificial Intelligence (AI) classical classification models that usually rely on the close-world assumption. This problem of classifying objects in this domain is better faced with and open-world AI approach. We propose an algorithm to identify not only all the known entities that may appear in front of the car, but also to detect and learn the classes of those unknown objects that may be rare to stand on an highway (e.g., a lost box from a truck). Our approach relies on the DOC algorithm from Lei Shu et. al. as well as on the Query-by-Committee algorithm. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.06628
Deep hierarchical reinforcement agents for automated penetration testing,Khuong Tran;Ashlesha Akella;Maxwell Standen;Junae Kim;David Bowman;Toby Richer;Chin-Teng Lin,"Penetration testing the organised attack of a computer system in order to test existing defences has been used extensively to evaluate network security. This is a time consuming process and requires in-depth knowledge for the establishment of a strategy that resembles a real cyber-attack. This paper presents a novel deep reinforcement learning architecture with hierarchically structured agents called HA-DRL, which employs an algebraic action decomposition strategy to address the large discrete action space of an autonomous penetration testing simulator where the number of actions is exponentially increased with the complexity of the designed cybersecurity network. The proposed architecture is shown to find the optimal attacking policy faster and more stably than a conventional deep Q-learning agent which is commonly used as a method to apply artificial intelligence in automatic penetration testing. △ Less","14 September, 2021",https://arxiv.org/pdf/2109.06449
Neuro-Symbolic AI: An Emerging Class of AI Workloads and their Characterization,Zachary Susskind;Bryce Arden;Lizy K. John;Patrick Stockton;Eugene B. John,"Neuro-symbolic artificial intelligence is a novel area of AI research which seeks to combine traditional rules-based AI approaches with modern deep learning techniques. Neuro-symbolic models have already demonstrated the capability to outperform state-of-the-art deep learning models in domains such as image and video reasoning. They have also been shown to obtain high accuracy with significantly less training data than traditional models. Due to the recency of the field's emergence and relative sparsity of published results, the performance characteristics of these models are not well understood. In this paper, we describe and analyze the performance characteristics of three recent neuro-symbolic models. We find that symbolic models have less potential parallelism than traditional neural models due to complex control flow and low-operational-intensity operations, such as scalar multiplication and tensor addition. However, the neural aspect of computation dominates the symbolic part in cases where they are clearly separable. We also find that data movement poses a potential bottleneck, as it does in many ML workloads. △ Less","13 September, 2021",https://arxiv.org/pdf/2109.06133
Internet of Things in Space: A Review of Opportunities and Challenges from Satellite-Aided Computing to Digitally-Enhanced Space Living,Jonathan Kua;Chetan Arora;Seng W. Loke;Niroshinie Fernando;Chathurika Ranaweera,"Recent scientific and technological advancements driven by the Internet of Things (IoT), Machine Learning (ML) and Artificial Intelligence (AI), distributed computing and data communication technologies have opened up a vast range of opportunities in many scientific fields - spanning from fast, reliable and efficient data communication to large-scale cloud/edge computing and intelligent big data analytics. Technological innovations and developments in these areas have also enabled many opportunities in the space industry. The successful Mars landing of NASA's Perseverance rover on February 18, 2021 represents another giant leap for mankind in space exploration. Emerging research and developments of connectivity and computing technologies in IoT for space/non-terrestrial environments is expected to yield significant benefits in the near future. This survey paper presents a broad overview of the area and provides a look-ahead of the opportunities made possible by IoT and space-based technologies. We first survey the current developments of IoT and space industry, and identify key challenges and opportunities in these areas. We then review the state-of-the-art and discuss future opportunities for IoT developments, deployment and integration to support future endeavours in space exploration. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.05971
Physics-AI Symbiosis,Bahram Jalali;Achuta Kadambi;Vwani Roychowdhury,"The phenomenal success of physics in explaining nature and designing hardware is predicated on efficient computational models. A universal codebook of physical laws defines the computational rules and a physical system is an interacting ensemble governed by these rules. Led by deep neural networks, artificial intelligence (AI) has introduced an alternate end-to-end data-driven computational framework, with astonishing performance gains in image classification and speech recognition and fueling hopes for a novel approach to discovering physics itself. These gains, however, come at the expense of interpretability and also computational efficiency; a trend that is on a collision course with the expected end of semiconductor scaling known as the Moore's Law. With focus on photonic applications, this paper argues how an emerging symbiosis of physics and artificial intelligence can overcome such formidable challenges, thereby not only extending the latter's spectacular rise but also transforming the direction of physical science. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.05959
On Solving a Stochastic Shortest-Path Markov Decision Process as Probabilistic Inference,Mohamed Baioumy;Bruno Lacerda;Paul Duckworth;Nick Hawes,"Previous work on planning as active inference addresses finite horizon problems and solutions valid for online planning. We propose solving the general Stochastic Shortest-Path Markov Decision Process (SSP MDP) as probabilistic inference. Furthermore, we discuss online and offline methods for planning under uncertainty. In an SSP MDP, the horizon is indefinite and unknown a priori. SSP MDPs generalize finite and infinite horizon MDPs and are widely used in the artificial intelligence community. Additionally, we highlight some of the differences between solving an MDP using dynamic programming approaches widely used in the artificial intelligence community and approaches used in the active inference community. △ Less","13 September, 2021",https://arxiv.org/pdf/2109.05866
Cyber-Security in the Emerging World of Smart Everything,Elochukwu A. Ukwandu;Ephraim N. C. Okafor;Charles Ikerionwu;Comfort Olebara;Celestine Ugwu,"The fourth industrial revolution (4IR) is a revolution many authors believe have come to stay. It is a revolution that has been fast blurring the line between physical, digital and biological technologies. These disruptive technologies largely rely on high-speed internet connectivity, Cloud technologies, Augmented Reality, Additive Manufacturing, Data science and Artificial Intelligence. Most developed economies have embraced the it while the developing economies are struggling to adopt 4IR because they lack the requisite skills, knowledge and technology. Thus, this study investigates Nigeria as one of the developing economies to understand her readiness for 4IR and the level of preparedness to mitigate the sophisticated cyber-attacks that comes with it. The investigation adopted quantitative research approach and developed an online questionnaire that was shared amongst the population of interest that includes academic, industry experts and relevant stakeholders. The questionnaire returned 116 valid responses which were analysed with descriptive statistical tools in SPSS. Results suggest that 60 of the respondents opined that Nigerian government at are not showing enough evidence to demonstrate her preparedness to leverage these promised potentials by developing 4IR relevant laws, strong institutional frameworks and policies. They lack significant development capacity to mitigate risks associated with digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the universities, 52 of the courses offered at the undergraduate and 42 at the post-graduate levels are relevant in the development of skills required in the revolution. The study recommends that the government at all levels make adequate efforts in developing the countrys intangible assets. In all, this paper posits that successful implementation of these could equip Nigeria to embrace the 4IR in all its aspects. △ Less","13 September, 2021",https://arxiv.org/pdf/2109.05821
Perceptions of Fairness and Trustworthiness Based on Explanations in Human vs. Automated Decision-Making,Jakob Schoeffer;Yvette Machowski;Niklas Kuehl,"Automated decision systems (ADS) have become ubiquitous in many high-stakes domains. Those systems typically involve sophisticated yet opaque artificial intelligence (AI) techniques that seldom allow for full comprehension of their inner workings, particularly for affected individuals. As a result, ADS are prone to deficient oversight and calibration, which can lead to undesirable (e.g., unfair) outcomes. In this work, we conduct an online study with 200 participants to examine people's perceptions of fairness and trustworthiness towards ADS in comparison to a scenario where a human instead of an ADS makes a high-stakes decision -- and we provide thorough identical explanations regarding decisions in both cases. Surprisingly, we find that people perceive ADS as fairer than human decision-makers. Our analyses also suggest that people's AI literacy affects their perceptions, indicating that people with higher AI literacy favor ADS more strongly over human decision-makers, whereas low-AI-literacy people exhibit no significant differences in their perceptions. △ Less","13 September, 2021",https://arxiv.org/pdf/2109.05792
Data Analytics for Smart cities: Challenges and Promises,Farid Ghareh Mohammadi;Farzan Shenavarmasouleh;M. Hadi Amini;Hamid R. Arabnia,"The explosion of advancements in artificial intelligence, sensor technologies, and wireless communication activates ubiquitous sensing through distributed sensors. These sensors are various domains of networks that lead us to smart systems in healthcare, transportation, environment, and other relevant branches/networks. Having collaborative interaction among the smart systems connects end-user devices to each other which enables achieving a new integrated entity called Smart Cities. The goal of this study is to provide a comprehensive survey of data analytics in smart cities. In this paper, we aim to focus on one of the smart cities important branches, namely Smart Mobility, and its positive ample impact on the smart cities decision-making process. Intelligent decision-making systems in smart mobility offer many advantages such as saving energy, relaying city traffic, and more importantly, reducing air pollution by offering real-time useful information and imperative knowledge. Making a decision in smart cities in time is challenging due to various and high dimensional factors and parameters, which are not frequently collected. In this paper, we first address current challenges in smart cities and provide an overview of potential solutions to these challenges. Then, we offer a framework of these solutions, called universal smart cities decision making, with three main sections of data capturing, data analysis, and decision making to optimize the smart mobility within smart cities. With this framework, we elaborate on fundamental concepts of big data, machine learning, and deep leaning algorithms that have been applied to smart cities and discuss the role of these algorithms in decision making for smart mobility in smart cities. △ Less","12 September, 2021",https://arxiv.org/pdf/2109.05581
Benchmarking Processor Performance by Multi-Threaded Machine Learning Algorithms,Muhammad Fahad Saleem,"Machine learning algorithms have enabled computers to predict things by learning from previous data. The data storage and processing power are increasing rapidly, thus increasing machine learning and Artificial intelligence applications. Much of the work is done to improve the accuracy of the models built in the past, with little research done to determine the computational costs of machine learning acquisitions. In this paper, I will proceed with this later research work and will make a performance comparison of multi-threaded machine learning clustering algorithms. I will be working on Linear Regression, Random Forest, and K-Nearest Neighbors to determine the performance characteristics of the algorithms as well as the computation costs to the obtained results. I will be benchmarking system hardware performance by running these multi-threaded algorithms to train and test the models on a dataset to note the differences in performance matrices of the algorithms. In the end, I will state the best performing algorithms concerning the performance efficiency of these algorithms on my system. △ Less","11 September, 2021",https://arxiv.org/pdf/2109.05276
How Can Subgroup Discovery Help AIOps?,Youcef Remil,"The genuine supervision of modern IT systems brings new challenges as it requires higher standards of scalability, reliability and efficiency when analysing and monitoring big data streams. Rule-based inference engines are a key component of maintenance systems in detecting anomalies and automating their resolution. However, they remain confined to simple and general rules and cannot handle the huge amount of data, nor the large number of alerts raised by IT systems, a lesson learned from expert systems era. Artificial Intelligence for Operation Systems (AIOps) proposes to take advantage of advanced analytics and machine learning on big data to improve and automate every step of supervision systems and aid incident management in detecting outages, identifying root causes and applying appropriate healing actions. Nevertheless, the best AIOps techniques rely on opaque models, strongly limiting their adoption. As a part of this PhD thesis, we study how Subgroup Discovery can help AIOps. This promising data mining technique offers possibilities to extract interesting hypothesis from data and understand the underlying process behind predictive models. To ensure relevancy of our propositions, this project involves both data mining researchers and practitioners from Infologic, a French software editor. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.04909
Emerging AI Security Threats for Autonomous Cars -- Case Studies,Shanthi Lekkala;Tanya Motwani;Manojkumar Parmar;Amit Phadke,"Artificial Intelligence has made a significant contribution to autonomous vehicles, from object detection to path planning. However, AI models require a large amount of sensitive training data and are usually computationally intensive to build. The commercial value of such models motivates attackers to mount various attacks. Adversaries can launch model extraction attacks for monetization purposes or step-ping-stone towards other attacks like model evasion. In specific cases, it even results in destroying brand reputation, differentiation, and value proposition. In addition, IP laws and AI-related legalities are still evolving and are not uniform across countries. We discuss model extraction attacks in detail with two use-cases and a generic kill-chain that can compromise autonomous cars. It is essential to investigate strategies to manage and mitigate the risk of model theft. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.04865
From Philosophy to Interfaces: an Explanatory Method and a Tool Inspired by Achinstein's Theory of Explanation,Francesco Sovrano;Fabio Vitali,"We propose a new method for explanations in Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. Among the mainstream philosophical theories of explanation we identified one that in our view is more easily applicable as a practical model for user-centric tools: Achinstein's Theory of Explanation. With this work we aim to prove that the theory proposed by Achinstein can be actually adapted for being implemented into a concrete software application, as an interactive process answering questions. To this end we found a way to handle the generic (archetypal) questions that implicitly characterise an explanatory processes as preliminary overviews rather than as answers to explicit questions, as commonly understood. To show the expressive power of this approach we designed and implemented a pipeline of AI algorithms for the generation of interactive explanations under the form of overviews, focusing on this aspect of explanations rather than on existing interfaces and presentation logic layers for question answering. We tested our hypothesis on a well-known XAI-powered credit approval system by IBM, comparing CEM, a static explanatory tool for post-hoc explanations, with an extension we developed adding interactive explanations based on our model. The results of the user study, involving more than 100 participants, showed that our proposed solution produced a statistically relevant improvement on effectiveness (U=931.0, p=0.036) over the baseline, thus giving evidence in favour of our theory. △ Less","9 September, 2021",https://arxiv.org/pdf/2109.04171
Modelling GDPR-Compliant Explanations for Trustworthy AI,Francesco Sovrano;Fabio Vitali;Monica Palmirani,"Through the General Data Protection Regulation (GDPR), the European Union has set out its vision for Automated Decision- Making (ADM) and AI, which must be reliable and human-centred. In particular we are interested on the Right to Explanation, that requires industry to produce explanations of ADM. The High-Level Expert Group on Artificial Intelligence (AI-HLEG), set up to support the implementation of this vision, has produced guidelines discussing the types of explanations that are appropriate for user-centred (interactive) Explanatory Tools. In this paper we propose our version of Explanatory Narratives (EN), based on user-centred concepts drawn from ISO 9241, as a model for user-centred explanations aligned with the GDPR and the AI-HLEG guidelines. Through the use of ENs we convert the problem of generating explanations for ADM into the identification of an appropriate path over an Explanatory Space, allowing explainees to interactively explore it and produce the explanation best suited to their needs. To this end we list suitable exploration heuristics, we study the properties and structure of explanations, and discuss the proposed model identifying its weaknesses and strengths. △ Less","9 September, 2021",https://arxiv.org/pdf/2109.04165
Risk-Averse Decision Making Under Uncertainty,Mohamadreza Ahmadi;Ugo Rosolia;Michel D. Ingham;Richard M. Murray;Aaron D. Ames,"A large class of decision making under uncertainty problems can be described via Markov decision processes (MDPs) or partially observable MDPs (POMDPs), with application to artificial intelligence and operations research, among others. Traditionally, policy synthesis techniques are proposed such that a total expected cost or reward is minimized or maximized. However, optimality in the total expected cost sense is only reasonable if system behavior in the large number of runs is of interest, which has limited the use of such policies in practical mission-critical scenarios, wherein large deviations from the expected behavior may lead to mission failure. In this paper, we consider the problem of designing policies for MDPs and POMDPs with objectives and constraints in terms of dynamic coherent risk measures, which we refer to as the constrained risk-averse problem. For MDPs, we reformulate the problem into a infsup problem via the Lagrangian framework and propose an optimization-based method to synthesize Markovian policies. For MDPs, we demonstrate that the formulated optimization problems are in the form of difference convex programs (DCPs) and can be solved by the disciplined convex-concave programming (DCCP) framework. We show that these results generalize linear programs for constrained MDPs with total discounted expected costs and constraints. For POMDPs, we show that, if the coherent risk measures can be defined as a Markov risk transition mapping, an infinite-dimensional optimization can be used to design Markovian belief-based policies. For stochastic finite-state controllers (FSCs), we show that the latter optimization simplifies to a (finite-dimensional) DCP and can be solved by the DCCP framework. We incorporate these DCPs in a policy iteration algorithm to design risk-averse FSCs for POMDPs. △ Less","9 September, 2021",https://arxiv.org/pdf/2109.04082
Attributing Fair Decisions with Attention Interventions,Ninareh Mehrabi;Umang Gupta;Fred Morstatter;Greg Ver Steeg;Aram Galstyan,"The widespread use of Artificial Intelligence (AI) in consequential domains, such as healthcare and parole decision-making systems, has drawn intense scrutiny on the fairness of these methods. However, ensuring fairness is often insufficient as the rationale for a contentious decision needs to be audited, understood, and defended. We propose that the attention mechanism can be used to ensure fair outcomes while simultaneously providing feature attributions to account for how a decision was made. Toward this goal, we design an attention-based model that can be leveraged as an attribution framework. It can identify features responsible for both performance and fairness of the model through attention interventions and attention weight manipulation. Using this attribution framework, we then design a post-processing bias mitigation strategy and compare it with a suite of baselines. We demonstrate the versatility of our approach by conducting experiments on two distinct data types, tabular and textual. △ Less","8 September, 2021",https://arxiv.org/pdf/2109.03952
A Clustering-aided Ensemble Method for Predicting Ridesourcing Demand in Chicago,Xiaojian Zhang;Xilei Zhao,"Accurately forecasting ridesourcing demand is important for effective transportation planning and policy-making. With the rise of Artificial Intelligence (AI), researchers have started to utilize machine learning models to forecast travel demand, which, in many cases, can produce higher prediction accuracy than statistical models. However, most existing machine-learning studies used a global model to predict the demand and ignored the influence of spatial heterogeneity (i.e., the spatial variations in the impacts of explanatory variables). Spatial heterogeneity can drive the parameter estimations varying over space; failing to consider the spatial variations may limit the model's prediction performance. To account for spatial heterogeneity, this study proposes a Clustering-aided Ensemble Method (CEM) to forecast the zone-to-zone (census-tract-to-census-tract) travel demand for ridesourcing services. Specifically, we develop a clustering framework to split the origin-destination pairs into different clusters and ensemble the cluster-specific machine learning models for prediction. We implement and test the proposed methodology by using the ridesourcing-trip data in Chicago. The results show that, with a more transparent and flexible model structure, the CEM significantly improves the prediction accuracy than the benchmark models (i.e., global machine-learning and statistical models directly trained on all observations). This study offers transportation researchers and practitioners a new methodology of travel demand forecasting, especially for new travel modes like ridesourcing and micromobility. △ Less","8 September, 2021",https://arxiv.org/pdf/2109.03433
Computing on Functions Using Randomized Vector Representations,E. Paxon Frady;Denis Kleyko;Christopher J. Kymn;Bruno A. Olshausen;Friedrich T. Sommer,"Vector space models for symbolic processing that encode symbols by random vectors have been proposed in cognitive science and connectionist communities under the names Vector Symbolic Architecture (VSA), and, synonymously, Hyperdimensional (HD) computing. In this paper, we generalize VSAs to function spaces by mapping continuous-valued data into a vector space such that the inner product between the representations of any two data points represents a similarity kernel. By analogy to VSA, we call this new function encoding and computing framework Vector Function Architecture (VFA). In VFAs, vectors can represent individual data points as well as elements of a function space (a reproducing kernel Hilbert space). The algebraic vector operations, inherited from VSA, correspond to well-defined operations in function space. Furthermore, we study a previously proposed method for encoding continuous data, fractional power encoding (FPE), which uses exponentiation of a random base vector to produce randomized representations of data points and fulfills the kernel properties for inducing a VFA. We show that the distribution from which elements of the base vector are sampled determines the shape of the FPE kernel, which in turn induces a VFA for computing with band-limited functions. In particular, VFAs provide an algebraic framework for implementing large-scale kernel machines with random features, extending Rahimi and Recht, 2007. Finally, we demonstrate several applications of VFA models to problems in image recognition, density estimation and nonlinear regression. Our analyses and results suggest that VFAs constitute a powerful new framework for representing and manipulating functions in distributed neural systems, with myriad applications in artificial intelligence. △ Less","8 September, 2021",https://arxiv.org/pdf/2109.03429
"Visual Sensation and Perception Computational Models for Deep Learning: State of the art, Challenges and Prospects",Bing Wei;Yudi Zhao;Kuangrong Hao;Lei Gao,"Visual sensation and perception refers to the process of sensing, organizing, identifying, and interpreting visual information in environmental awareness and understanding. Computational models inspired by visual perception have the characteristics of complexity and diversity, as they come from many subjects such as cognition science, information science, and artificial intelligence. In this paper, visual perception computational models oriented deep learning are investigated from the biological visual mechanism and computational vision theory systematically. Then, some points of view about the prospects of the visual perception computational models are presented. Finally, this paper also summarizes the current challenges of visual perception and predicts its future development trends. Through this survey, it will provide a comprehensive reference for research in this direction. △ Less","7 September, 2021",https://arxiv.org/pdf/2109.03391
"Smart Automotive Technology Adherence to the Law: (De)Constructing Road Rules for Autonomous System Development, Verification and Safety",Scott McLachlan;Martin Neil;Kudakwashe Dube;Ronny Bogani;Norman Fenton;Burkhard Schaffer,"Driving is an intuitive task that requires skills, constant alertness and vigilance for unexpected events. The driving task also requires long concentration spans focusing on the entire task for prolonged periods, and sophisticated negotiation skills with other road users, including wild animals. These requirements are particularly important when approaching intersections, overtaking, giving way, merging, turning and while adhering to the vast body of road rules. Modern motor vehicles now include an array of smart assistive and autonomous driving systems capable of subsuming some, most, or in limited cases, all of the driving task. The UK Department of Transport's response to the Safe Use of Automated Lane Keeping System consultation proposes that these systems are tested for compliance with relevant traffic rules. Building these smart automotive systems requires software developers with highly technical software engineering skills, and now a lawyer's in-depth knowledge of traffic legislation as well. These skills are required to ensure the systems are able to safely perform their tasks while being observant of the law. This paper presents an approach for deconstructing the complicated legalese of traffic law and representing its requirements and flow. The approach (de)constructs road rules in legal terminology and specifies them in structured English logic that is expressed as Boolean logic for automation and Lawmaps for visualisation. We demonstrate an example using these tools leading to the construction and validation of a Bayesian Network model. We strongly believe these tools to be approachable by programmers and the general public, and capable of use in developing Artificial Intelligence to underpin motor vehicle smart systems, and in validation to ensure these systems are considerate of the law when making decisions. △ Less","10 September, 2021",https://arxiv.org/pdf/2109.02956
The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning,Yujin Tang;David Ha,"In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artificial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identical neural networks, each with no fixed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/ △ Less","28 September, 2021",https://arxiv.org/pdf/2109.02869
Symbolic Computation in Software Science: My Personal View,Bruno Buchberger,"In this note, I develop my personal view on the scope and relevance of symbolic computation in software science. For this, I discuss the interaction and differences between symbolic computation, software science, automatic programming, mathematical knowledge management, artificial intelligence, algorithmic intelligence, numerical computation, and machine learning. In the discussion of these notions, I allow myself to refer also to papers (1982, 1985, 2001, 2003, 2013) of mine in which I expressed my views on these areas at early stages of some of these fields. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.02806
An Intelligent Material with Chemical Pathway Networks,Li Lin;Michael Keidar,"A new type of material with embedded intelligence, namely 'intelligent plasma', is introduced. Such new material exhibits programmable chemical pathway networks resembling artificial neural networks. As a Markov process of chemistry, the chemical pathway network can be customized and thus the intelligent plasmas can be programmed to make their own decisions to react to the dynamic external and internal conditions. It finally can accomplish complex missions without any external controls from the humans while relying on its preprogrammed chemical network topology before the mission. To that end, only basic data input and readings are required without any external controls during the mission. The approach to 'if' conditions and 'while' loops of the programmable intelligent plasmas are also discussed with examples of applications including automatic workflows, and signal processing. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.02735
Estimation of Bivariate Structural Causal Models by Variational Gaussian Process Regression Under Likelihoods Parametrised by Normalising Flows,Nico Reick;Felix Wiewel;Alexander Bartler;Bin Yang,"One major drawback of state-of-the-art artificial intelligence is its lack of explainability. One approach to solve the problem is taking causality into account. Causal mechanisms can be described by structural causal models. In this work, we propose a method for estimating bivariate structural causal models using a combination of normalising flows applied to density estimation and variational Gaussian process regression for post-nonlinear models. It facilitates causal discovery, i.e. distinguishing cause and effect, by either the independence of cause and residual or a likelihood ratio test. Our method which estimates post-nonlinear models can better explain a variety of real-world cause-effect pairs than a simple additive noise model. Though it remains difficult to exploit this benefit regarding all pairs from the Tübingen benchmark database, we demonstrate that combining the additive noise model approach with our method significantly enhances causal discovery. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.02521
Active Learning for Automated Visual Inspection of Manufactured Products,Elena Trajkova;Jože M. Rožanec;Paulien Dam;Blaž Fortuna;Dunja Mladenić,"Quality control is a key activity performed by manufacturing enterprises to ensure products meet quality standards and avoid potential damage to the brand's reputation. The decreased cost of sensors and connectivity enabled an increasing digitalization of manufacturing. In addition, artificial intelligence enables higher degrees of automation, reducing overall costs and time required for defect inspection. In this research, we compare three active learning approaches and five machine learning algorithms applied to visual defect inspection with real-world data provided by Philips Consumer Lifestyle BV. Our results show that active learning reduces the data labeling effort without detriment to the models' performance. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.02469
Improved RAMEN: Towards Domain Generalization for Visual Question Answering,Bhanuka Manesha Samarasekara Vitharana Gamage;Lim Chern Hong,"Currently nearing human-level performance, Visual Question Answering (VQA) is an emerging area in artificial intelligence. Established as a multi-disciplinary field in machine learning, both computer vision and natural language processing communities are working together to achieve state-of-the-art (SOTA) performance. However, there is a gap between the SOTA results and real world applications. This is due to the lack of model generalisation. The RAMEN model \cite{Shrestha2019} aimed to achieve domain generalization by obtaining the highest score across two main types of VQA datasets. This study provides two major improvements to the early/late fusion module and aggregation module of the RAMEN architecture, with the objective of further strengthening domain generalization. Vector operations based fusion strategies are introduced for the fusion module and the transformer architecture is introduced for the aggregation module. Improvements of up to five VQA datasets from the experiments conducted are evident. Following the results, this study analyses the effects of both the improvements on the domain generalization problem. The code is available on GitHub though the following link \url{https://github.com/bhanukaManesha/ramen}. △ Less","6 September, 2021",https://arxiv.org/pdf/2109.02370
Explaining Autonomous Decisions in Swarms of Human-on-the-Loop Small Unmanned Aerial Systems,Ankit Agrawal;Jane Cleland-Huang,"Rapid advancements in Artificial Intelligence have shifted the focus from traditional human-directed robots to fully autonomous ones that do not require explicit human control. These are commonly referred to as Human-on-the-Loop (HotL) systems. Transparency of HotL systems necessitates clear explanations of autonomous behavior so that humans are aware of what is happening in the environment and can understand why robots behave in a certain way. However, in complex multi-robot environments, especially those in which the robots are autonomous, mobile, and require intermittent interventions, humans may struggle to maintain situational awareness. Presenting humans with rich explanations of autonomous behavior tends to overload them with too much information and negatively affect their understanding of the situation. Therefore, explaining the autonomous behavior or autonomy of multiple robots creates a design tension that demands careful investigation. This paper examines the User Interface (UI) design trade-offs associated with providing timely and detailed explanations of autonomous behavior for swarms of small Unmanned Aerial Systems (sUAS) or drones. We analyze the impact of UI design choices on human awareness of the situation. We conducted multiple user studies with both inexperienced and expert sUAS operators to present our design solution and provide initial guidelines for designing the HotL multi-sUAS interface. △ Less","5 September, 2021",https://arxiv.org/pdf/2109.02077
Tolerating Adversarial Attacks and Byzantine Faults in Distributed Machine Learning,Yusen Wu;Hao Chen;Xin Wang;Chao Liu;Phuong Nguyen;Yelena Yesha,"Adversarial attacks attempt to disrupt the training, retraining and utilizing of artificial intelligence and machine learning models in large-scale distributed machine learning systems. This causes security risks on its prediction outcome. For example, attackers attempt to poison the model by either presenting inaccurate misrepresentative data or altering the models' parameters. In addition, Byzantine faults including software, hardware, network issues occur in distributed systems which also lead to a negative impact on the prediction outcome. In this paper, we propose a novel distributed training algorithm, partial synchronous stochastic gradient descent (ParSGD), which defends adversarial attacks and/or tolerates Byzantine faults. We demonstrate the effectiveness of our algorithm under three common adversarial attacks again the ML models and a Byzantine fault during the training phase. Our results show that using ParSGD, ML models can still produce accurate predictions as if it is not being attacked nor having failures at all when almost half of the nodes are being compromised or failed. We will report the experimental evaluations of ParSGD in comparison with other algorithms. △ Less","5 September, 2021",https://arxiv.org/pdf/2109.02018
Eden: A Unified Environment Framework for Booming Reinforcement Learning Algorithms,Ruizhi Chen;Xiaoyu Wu;Yansong Pan;Kaizhao Yuan;Ling Li;TianYun Ma;JiYuan Liang;Rui Zhang;Kai Wang;Chen Zhang;Shaohui Peng;Xishan Zhang;Zidong Du;Qi Guo;Yunji Chen,"With AlphaGo defeats top human players, reinforcement learning(RL) algorithms have gradually become the code-base of building stronger artificial intelligence(AI). The RL algorithm design firstly needs to adapt to the specific environment, so the designed environment guides the rapid and profound development of RL algorithms. However, the existing environments, which can be divided into real world games and customized toy environments, have obvious shortcomings. For real world games, it is designed for human entertainment, and too much difficult for most of RL researchers. For customized toy environments, there is no widely accepted unified evaluation standard for all RL algorithms. Therefore, we introduce the first virtual user-friendly environment framework for RL. In this framework, the environment can be easily configured to realize all kinds of RL tasks in the mainstream research. Then all the mainstream state-of-the-art(SOTA) RL algorithms can be conveniently evaluated and compared. Therefore, our contributions mainly includes the following aspects: 1.single configured environment for all classification of SOTA RL algorithms; 2.combined environment of more than one classification RL algorithms; 3.the evaluation standard for all kinds of RL algorithms. With all these efforts, a possibility for breeding an AI with capability of general competency in a variety of tasks is provided, and maybe it will open up a new chapter for AI. △ Less","3 September, 2021",https://arxiv.org/pdf/2109.01768
Will bots take over the supply chain? Revisiting Agent-based supply chain automation,Liming Xu;Stephen Mak;Alexandra Brintrup,"Agent-based systems have the capability to fuse information from many distributed sources and create better plans faster. This feature makes agent-based systems naturally suitable to address the challenges in Supply Chain Management (SCM). Although agent-based supply chains systems have been proposed since early 2000; industrial uptake of them has been lagging. The reasons quoted include the immaturity of the technology, a lack of interoperability with supply chain information systems, and a lack of trust in Artificial Intelligence (AI). In this paper, we revisit the agent-based supply chain and review the state of the art. We find that agent-based technology has matured, and other supporting technologies that are penetrating supply chains; are filling in gaps, leaving the concept applicable to a wider range of functions. For example, the ubiquity of IoT technology helps agents ""sense"" the state of affairs in a supply chain and opens up new possibilities for automation. Digital ledgers help securely transfer data between third parties, making agent-based information sharing possible, without the need to integrate Enterprise Resource Planning (ERP) systems. Learning functionality in agents enables agents to move beyond automation and towards autonomy. We note this convergence effect through conceptualising an agent-based supply chain framework, reviewing its components, and highlighting research challenges that need to be addressed in moving forward. △ Less","3 September, 2021",https://arxiv.org/pdf/2109.01703
Data science and Machine learning in the Clouds: A Perspective for the Future,Hrishav Bakul Barua,"As we are fast approaching the beginning of a paradigm shift in the field of science, Data driven science (the so called fourth science paradigm) is going to be the driving force in research and innovation. From medicine to biodiversity and astronomy to geology, all these terms are somehow going to be affected by this paradigm shift. The huge amount of data to be processed under this new paradigm will be a major concern in the future and one will strongly require cloud based services in all the aspects of these computations (from storage to compute and other services). Another aspect will be energy consumption and performance of prediction jobs and tasks within such a scientific paradigm which will change the way one sees computation. Data science has heavily impacted or rather triggered the emergence of Machine Learning, Signal/Image/Video processing related algorithms, Artificial intelligence, Robotics, health informatics, geoinformatics, and many more such areas of interest. Hence, we envisage an era where Data science can deliver its promises with the help of the existing cloud based platforms and services with the addition of new services. In this article, we discuss about data driven science and Machine learning and how they are going to be linked through cloud based services in the future. It also discusses the rise of paradigms like approximate computing, quantum computing and many more in recent times and their applicability in big data processing, data science, analytics, prediction and machine learning in the cloud environments. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.01661
Artificial Intelligence in Dry Eye Disease,Andrea M. Storås;Inga Strümke;Michael A. Riegler;Jakob Grauslund;Hugo L. Hammer;Anis Yazidi;Pål Halvorsen;Kjell G. Gundersen;Tor P. Utheim;Catherine Jackson,"Dry eye disease (DED) has a prevalence of between 5 and 50\%, depending on the diagnostic criteria used and population under study. However, it remains one of the most underdiagnosed and undertreated conditions in ophthalmology. Many tests used in the diagnosis of DED rely on an experienced observer for image interpretation, which may be considered subjective and result in variation in diagnosis. Since artificial intelligence (AI) systems are capable of advanced problem solving, use of such techniques could lead to more objective diagnosis. Although the term `AI' is commonly used, recent success in its applications to medicine is mainly due to advancements in the sub-field of machine learning, which has been used to automatically classify images and predict medical outcomes. Powerful machine learning techniques have been harnessed to understand nuances in patient data and medical images, aiming for consistent diagnosis and stratification of disease severity. This is the first literature review on the use of AI in DED. We provide a brief introduction to AI, report its current use in DED research and its potential for application in the clinic. Our review found that AI has been employed in a wide range of DED clinical tests and research applications, primarily for interpretation of interferometry, slit-lamp and meibography images. While initial results are promising, much work is still needed on model development, clinical testing and standardisation. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.01658
Symbol Emergence and The Solutions to Any Task,Michael Timothy Bennett,"The following defines intent, an arbitrary task and its solutions, and then argues that an agent which always constructs what is called an Intensional Solution would qualify as artificial general intelligence. We then explain how natural language may emerge and be acquired by such an agent, conferring the ability to model the intent of other individuals labouring under similar compulsions, because an abstract symbol system and the solution to a task are one and the same. △ Less","4 October, 2021",https://arxiv.org/pdf/2109.01281
Non-functional Requirements for Machine Learning: Understanding Current Use and Challenges in Industry,Khan Mohammad Habibullah;Jennifer Horkoff,"Machine Learning (ML) is an application of Artificial Intelligence (AI) that uses big data to produce complex predictions and decision-making systems, which would be challenging to obtain otherwise. To ensure the success of ML-enabled systems, it is essential to be aware of certain qualities of ML solutions (performance, transparency, fairness), known from a Requirement Engineering (RE) perspective as non-functional requirements (NFRs). However, when systems involve ML, NFRs for traditional software may not apply in the same ways; some NFRs may become more prominent or less important; NFRs may be defined over the ML model, data, or the entire system; and NFRs for ML may be measured differently. In this work, we aim to understand the state-of-the-art and challenges of dealing with NFRs for ML in industry. We interviewed ten engineering practitioners working with NFRs and ML. We find examples of (1) the identification and measurement of NFRs for ML, (2) identification of more and less important NFRs for ML, and (3) the challenges associated with NFRs and ML in the industry. This knowledge paints a picture of how ML-related NFRs are treated in practice and helps to guide future RE for ML efforts. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.00872
Self-timed Reinforcement Learning using Tsetlin Machine,Adrian Wheeldon;Alex Yakovlev;Rishad Shafik,"We present a hardware design for the learning datapath of the Tsetlin machine algorithm, along with a latency analysis of the inference datapath. In order to generate a low energy hardware which is suitable for pervasive artificial intelligence applications, we use a mixture of asynchronous design techniques - including Petri nets, signal transition graphs, dual-rail and bundled-data. The work builds on previous design of the inference hardware, and includes an in-depth breakdown of the automaton feedback, probability generation and Tsetlin automata. Results illustrate the advantages of asynchronous design in applications such as personalized healthcare and battery-powered internet of things devices, where energy is limited and latency is an important figure of merit. Challenges of static timing analysis in asynchronous circuits are also addressed. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.00846
Adherence and Constancy in LIME-RS Explanations for Recommendation,Vito Walter Anelli;Alejandro Bellogín;Tommaso Di Noia;Francesco Maria Donini;Vincenzo Paparella;Claudio Pomo,"Explainable Recommendation has attracted a lot of attention due to a renewed interest in explainable artificial intelligence. In particular, post-hoc approaches have proved to be the most easily applicable ones to increasingly complex recommendation models, which are then treated as black-boxes. The most recent literature has shown that for post-hoc explanations based on local surrogate models, there are problems related to the robustness of the approach itself. This consideration becomes even more relevant in human-related tasks like recommendation. The explanation also has the arduous task of enhancing increasingly relevant aspects of user experience such as transparency or trustworthiness. This paper aims to show how the characteristics of a classical post-hoc model based on surrogates is strongly model-dependent and does not prove to be accountable for the explanations generated. △ Less","8 October, 2021",https://arxiv.org/pdf/2109.00818
Evaluating the Single-Shot MultiBox Detector and YOLO Deep Learning Models for the Detection of Tomatoes in a Greenhouse,Sandro A. Magalhães;Luís Castro;Germano Moreira;Filipe N. Santos;mário Cunha;Jorge Dias;António P. Moreira,"The development of robotic solutions for agriculture requires advanced perception capabilities that can work reliably in any crop stage. For example, to automatise the tomato harvesting process in greenhouses, the visual perception system needs to detect the tomato in any life cycle stage (flower to the ripe tomato). The state-of-the-art for visual tomato detection focuses mainly on ripe tomato, which has a distinctive colour from the background. This paper contributes with an annotated visual dataset of green and reddish tomatoes. This kind of dataset is uncommon and not available for research purposes. This will enable further developments in edge artificial intelligence for in situ and in real-time visual tomato detection required for the development of harvesting robots. Considering this dataset, five deep learning models were selected, trained and benchmarked to detect green and reddish tomatoes grown in greenhouses. Considering our robotic platform specifications, only the Single-Shot MultiBox Detector (SSD) and YOLO architectures were considered. The results proved that the system can detect green and reddish tomatoes, even those occluded by leaves. SSD MobileNet v2 had the best performance when compared against SSD Inception v2, SSD ResNet 50, SSD ResNet 101 and YOLOv4 Tiny, reaching an F1-score of 66.15%, an mAP of 51.46% and an inference time of 16.44 ms with the NVIDIA Turing Architecture platform, an NVIDIA Tesla T4, with 12 GB. YOLOv4 Tiny also had impressive results, mainly concerning inferring times of about 5 ms. △ Less","2 September, 2021",https://arxiv.org/pdf/2109.00810
Bio-inspired robot perception coupled with robot-modeled human perception,Tobias Fischer,"My overarching research goal is to provide robots with perceptional abilities that allow interactions with humans in a human-like manner. To develop these perceptional abilities, I believe that it is useful to study the principles of the human visual system. I use these principles to develop new computer vision algorithms and validate their effectiveness in intelligent robotic systems. I am enthusiastic about this approach as it offers the dual benefit of uncovering principles inherent in the human visual system, as well as applying these principles to its artificial counterpart. Fig. 1 contains a depiction of my research. △ Less","31 August, 2021",https://arxiv.org/pdf/2109.00097
Informing Autonomous Deception Systems with Cyber Expert Performance Data,Maxine Major;Brian Souza;Joseph DiVita;Kimberly Ferguson-Walter,"The performance of artificial intelligence (AI) algorithms in practice depends on the realism and correctness of the data, models, and feedback (labels or rewards) provided to the algorithm. This paper discusses methods for improving the realism and ecological validity of AI used for autonomous cyber defense by exploring the potential to use Inverse Reinforcement Learning (IRL) to gain insight into attacker actions, utilities of those actions, and ultimately decision points which cyber deception could thwart. The Tularosa study, as one example, provides experimental data of real-world techniques and tools commonly used by attackers, from which core data vectors can be leveraged to inform an autonomous cyber defense system. △ Less","31 August, 2021",https://arxiv.org/pdf/2109.00066
An Artificial Intelligence Life Cycle: From Conception to Production,Daswin De Silva;Damminda Alahakoon,"Drawing on our experience of more than a decade of AI in academic research, technology development, industry engagement, postgraduate teaching, doctoral supervision and organisational consultancy, we present the 'CDAC AI Life Cycle', a comprehensive life cycle for the design, development and deployment of Artificial Intelligence (AI) systems and solutions. It consists of three phases, Design, Develop and Deploy, and 17 constituent stages across the three phases from conception to production of any AI initiative. The 'Design' phase highlights the importance of contextualising a problem description by reviewing public domain and service-based literature on state-of-the-art AI applications, algorithms, pre-trained models and equally importantly ethics guidelines and frameworks, which then informs the data, or Big Data, acquisition and preparation. The 'Develop' phase is technique-oriented, as it transforms data and algorithms into AI models that are benchmarked, evaluated and explained. The 'Deploy' phase evaluates computational performance, which then apprises pipelines for model operationalisation, culminating in the hyperautomation of a process or system as a complete AI solution, that is continuously monitored and evaluated to inform the next iteration of the life cycle. An ontological mapping of AI algorithms to applications, followed by an organisational context for the AI life cycle are further contributions of this article. △ Less","29 August, 2021",https://arxiv.org/pdf/2108.13861
Towards a Common Testing Terminology for Software Engineering and Data Science Experts,Lisa Jöckel;Thomas Bauer;Michael Kläs;Marc P. Hauer;Janek Groß,"Analytical quality assurance, especially testing, is an integral part of software-intensive system development. With the increased usage of Artificial Intelligence (AI) and Machine Learning (ML) as part of such systems, this becomes more difficult as well-understood software testing approaches cannot be applied directly to the AI-enabled parts of the system. The required adaptation of classical testing approaches and the development of new concepts for AI would benefit from a deeper understanding and exchange between AI and software engineering experts. We see the different terminologies used in the two communities as a major obstacle on this way. As we consider a mutual understanding of the testing terminology a key, this paper contributes a mapping between the most important concepts from classical software testing and AI testing. In the mapping, we highlight differences in the relevance and naming of the mapped concepts. △ Less","6 October, 2021",https://arxiv.org/pdf/2108.13837
DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation,Lijie Wang;Hao Liu;Shuyuan Peng;Hongxuan Tang;Xinyan Xiao;Ying Chen;Hua Wu;Haifeng Wang,"While deep learning models have greatly improved the performance of most artificial intelligence tasks, they are often criticized to be untrustworthy due to the black-box problem. Consequently, many works have been proposed to study the trustworthiness of deep learning. However, as most open datasets are designed for evaluating the accuracy of model outputs, there is still a lack of appropriate datasets for evaluating the inner workings of neural networks. The lack of datasets obviously hinders the development of trustworthiness research. Therefore, in order to systematically evaluate the factors for building trustworthy systems, we propose a novel and well-annotated sentiment analysis dataset to evaluate robustness and interpretability. To evaluate these factors, our dataset contains diverse annotations about the challenging distribution of instances, manual adversarial instances and sentiment explanations. Several evaluation metrics are further proposed for interpretability and robustness. Based on the dataset and metrics, we conduct comprehensive comparisons for the trustworthiness of three typical models, and also study the relations between accuracy, robustness and interpretability. We release this trustworthiness evaluation dataset at \url{https://github/xyz} and hope our work can facilitate the progress on building more trustworthy systems for real-world applications. △ Less","7 September, 2021",https://arxiv.org/pdf/2108.13140
Efficient Visual Recognition with Deep Neural Networks: A Survey on Recent Advances and New Directions,Yang Wu;Dingheng Wang;Xiaotong Lu;Fan Yang;Guoqi Li;Weisheng Dong;Jianbo Shi,"Visual recognition is currently one of the most important and active research areas in computer vision, pattern recognition, and even the general field of artificial intelligence. It has great fundamental importance and strong industrial needs. Deep neural networks (DNNs) have largely boosted their performances on many concrete tasks, with the help of large amounts of training data and new powerful computation resources. Though recognition accuracy is usually the first concern for new progresses, efficiency is actually rather important and sometimes critical for both academic research and industrial applications. Moreover, insightful views on the opportunities and challenges of efficiency are also highly required for the entire community. While general surveys on the efficiency issue of DNNs have been done from various perspectives, as far as we are aware, scarcely any of them focused on visual recognition systematically, and thus it is unclear which progresses are applicable to it and what else should be concerned. In this paper, we present the review of the recent advances with our suggestions on the new possible directions towards improving the efficiency of DNN-related visual recognition approaches. We investigate not only from the model but also the data point of view (which is not the case in existing surveys), and focus on three most studied data types (images, videos and points). This paper attempts to provide a systematic summary via a comprehensive survey which can serve as a valuable reference and inspire both researchers and practitioners who work on visual recognition problems. △ Less","8 September, 2021",https://arxiv.org/pdf/2108.13055
Goal-driven text descriptions for images,Ruotian Luo,"A big part of achieving Artificial General Intelligence(AGI) is to build a machine that can see and listen like humans. Much work has focused on designing models for image classification, video classification, object detection, pose estimation, speech recognition, etc., and has achieved significant progress in recent years thanks to deep learning. However, understanding the world is not enough. An AI agent also needs to know how to talk, especially how to communicate with a human. While perception (vision, for example) is more common across animal species, the use of complicated language is unique to humans and is one of the most important aspects of intelligence. In this thesis, we focus on generating textual output given visual input. In Chapter 3, we focus on generating the referring expression, a text description for an object in the image so that a receiver can infer which object is being described. We use a comprehension machine to directly guide the generated referring expressions to be more discriminative. In Chapter 4, we introduce a method that encourages discriminability in image caption generation. We show that more discriminative captioning models generate more descriptive captions. In Chapter 5, we study how training objectives and sampling methods affect the models' ability to generate diverse captions. We find that a popular captioning training strategy will be detrimental to the diversity of generated captions. In Chapter 6, we propose a model that can control the length of generated captions. By changing the desired length, one can influence the style and descriptiveness of the captions. Finally, in Chapter 7, we rank/generate informative image tags according to their information utility. The proposed method better matches what humans think are the most important tags for the images. △ Less","28 August, 2021",https://arxiv.org/pdf/2108.12575
Why and How Governments Should Monitor AI Development,Jess Whittlestone;Jack Clark,"In this paper we outline a proposal for improving the governance of artificial intelligence (AI) by investing in government capacity to systematically measure and monitor the capabilities and impacts of AI systems. If adopted, this would give governments greater information about the AI ecosystem, equipping them to more effectively direct AI development and deployment in the most societally and economically beneficial directions. It would also create infrastructure that could rapidly identify potential threats or harms that could occur as a consequence of changes in the AI ecosystem, such as the emergence of strategically transformative capabilities, or the deployment of harmful systems. We begin by outlining the problem which motivates this proposal: in brief, traditional governance approaches struggle to keep pace with the speed of progress in AI. We then present our proposal for addressing this problem: governments must invest in measurement and monitoring infrastructure. We discuss this proposal in detail, outlining what specific things governments could focus on measuring and monitoring, and the kinds of benefits this would generate for policymaking. Finally, we outline some potential pilot projects and some considerations for implementing this in practice. △ Less","31 August, 2021",https://arxiv.org/pdf/2108.12427
Integrating Heuristics and Learning in a Computational Architecture for Cognitive Trading,Remo Pareschi;Federico Zappone,"The successes of Artificial Intelligence in recent years in areas such as image analysis, natural language understanding and strategy games have sparked interest from the world of finance. Specifically, there are high expectations, and ongoing engineering projects, regarding the creation of artificial agents, known as robotic traders, capable of juggling the financial markets with the skill of experienced human traders. Obvious economic implications aside, this is certainly an area of great scientific interest, due to the challenges that such a real context poses to the use of AI techniques. Precisely for this reason, we must be aware that artificial agents capable of operating at such levels are not just round the corner, and that there will be no simple answers, but rather a concurrence of various technologies and methods to the success of the effort. In the course of this article, we review the issues inherent in the design of effective robotic traders as well as the consequently applicable solutions, having in view the general objective of bringing the current state of the art of robo-trading up to the next level of intelligence, which we refer to as Cognitive Trading. Key to our approach is the joining of two methodological and technological directions which, although both deeply rooted in the disciplinary field of artificial intelligence, have so far gone their separate ways: heuristics and learning. △ Less","27 August, 2021",https://arxiv.org/pdf/2108.12333
Deep learning models are not robust against noise in clinical text,Milad Moradi;Kathrin Blagec;Matthias Samwald,"Artificial Intelligence (AI) systems are attracting increasing interest in the medical domain due to their ability to learn complicated tasks that require human intelligence and expert knowledge. AI systems that utilize high-performance Natural Language Processing (NLP) models have achieved state-of-the-art results on a wide variety of clinical text processing benchmarks. They have even outperformed human accuracy on some tasks. However, performance evaluation of such AI systems have been limited to accuracy measures on curated and clean benchmark datasets that may not properly reflect how robustly these systems can operate in real-world situations. In order to address this challenge, we introduce and implement a wide variety of perturbation methods that simulate different types of noise and variability in clinical text data. While noisy samples produced by these perturbation methods can often be understood by humans, they may cause AI systems to make erroneous decisions. Conducting extensive experiments on several clinical text processing tasks, we evaluated the robustness of high-performance NLP models against various types of character-level and word-level noise. The results revealed that the NLP models performance degrades when the input contains small amounts of noise. This study is a significant step towards exposing vulnerabilities of AI models utilized in clinical text processing systems. The proposed perturbation methods can be used in performance evaluation tests to assess how robustly clinical NLP models can operate on noisy data, in real-world settings. △ Less","27 August, 2021",https://arxiv.org/pdf/2108.12242
JUWELS Booster -- A Supercomputer for Large-Scale AI Research,Stefan Kesselheim;Andreas Herten;Kai Krajsek;Jan Ebert;Jenia Jitsev;Mehdi Cherti;Michael Langguth;Bing Gong;Scarlet Stadtler;Amirpasha Mozaffari;Gabriele Cavallaro;Rocco Sedona;Alexander Schug;Alexandre Strube;Roshni Kamath;Martin G. Schultz;Morris Riedel;Thomas Lippert,"In this article, we present JUWELS Booster, a recently commissioned high-performance computing system at the Jülich Supercomputing Center. With its system architecture, most importantly its large number of powerful Graphics Processing Units (GPUs) and its fast interconnect via InfiniBand, it is an ideal machine for large-scale Artificial Intelligence (AI) research and applications. We detail its system architecture, parallel, distributed model training, and benchmarks indicating its outstanding performance. We exemplify its potential for research application by presenting large-scale AI research highlights from various scientific fields that require such a facility. △ Less","30 June, 2021",https://arxiv.org/pdf/2108.11976
Security and privacy for 6G: A survey on prospective technologies and challenges,Van-Linh Nguyen;Po-Ching Lin;Bo-Chao Cheng;Ren-Hung Hwang;Ying-Dar Lin,"Sixth-generation (6G) mobile networks will have to cope with diverse threats on a space-air-ground integrated network environment, novel technologies, and an accessible user information explosion. However, for now, security and privacy issues for 6G remain largely in concept. This survey provides a systematic overview of security and privacy issues based on prospective technologies for 6G in the physical, connection, and service layers, as well as through lessons learned from the failures of existing security architectures and state-of-the-art defenses. Two key lessons learned are as follows. First, other than inheriting vulnerabilities from the previous generations, 6G has new threat vectors from new radio technologies, such as the exposed location of radio stripes in ultra-massive MIMO systems at Terahertz bands and attacks against pervasive intelligence. Second, physical layer protection, deep network slicing, quantum-safe communications, artificial intelligence (AI) security, platform-agnostic security, real-time adaptive security, and novel data protection mechanisms such as distributed ledgers and differential privacy are the top promising techniques to mitigate the attack magnitude and personal data breaches substantially. △ Less","31 August, 2021",https://arxiv.org/pdf/2108.11861
AI at work -- Mitigating safety and discriminatory risk with technical standards,Nikolas Becker;Pauline Junginger;Lukas Martinez;Daniel Krupka;Leonie Beining,"The use of artificial intelligence (AI) and AI methods in the workplace holds both great opportunities as well as risks to occupational safety and discrimination. In addition to legal regulation, technical standards will play a key role in mitigating such risk by defining technical requirements for development and testing of AI systems. This paper provides an overview and assessment of existing international, European and German standards as well as those currently under development. The paper is part of the research project ""ExamAI - Testing and Auditing of AI systems"" and focusses on the use of AI in an industrial production environment as well as in the realm of human resource management (HR). △ Less","26 August, 2021",https://arxiv.org/pdf/2108.11844
Technological Approaches to Detecting Online Disinformation and Manipulation,Aleš Horák;Vít Baisa;Ondřej Herman,"The move of propaganda and disinformation to the online environment is possible thanks to the fact that within the last decade, digital information channels radically increased in popularity as a news source. The main advantage of such media lies in the speed of information creation and dissemination. This, on the other hand, inevitably adds pressure, accelerating editorial work, fact-checking, and the scrutiny of source credibility. In this chapter, an overview of computer-supported approaches to detecting disinformation and manipulative techniques based on several criteria is presented. We concentrate on the technical aspects of automatic methods which support fact-checking, topic identification, text style analysis, or message filtering on social media channels. Most of the techniques employ artificial intelligence and machine learning with feature extraction combining available information resources. The following text firstly specifies the tasks related to computer detection of manipulation and disinformation spreading. The second section presents concrete methods of solving the tasks of the analysis, and the third sections enlists current verification and benchmarking datasets published and used in this area for evaluation and comparison. △ Less","26 August, 2021",https://arxiv.org/pdf/2108.11669
ML-Assisted UE Positioning: Performance Analysis and 5G Architecture Enhancements,M. Majid Butt;Anna Pantelidou;István Z. Kovács,"Artificial intelligence and data-driven networks will be integral part of 6G systems. In this article, we comprehensively discuss implementation challenges and need for architectural changes in 5G radio access networks for integrating machine learning (ML) solutions. As an example use case, we investigate user equipment (UE) positioning assisted by deep learning (DL) in 5G and beyond networks. As compared to state of the art positioning algorithms used in today's networks, radio signal fingerprinting and machine learning (ML) assisted positioning requires smaller additional feedback overhead; and the positioning estimates are made directly inside the radio access network (RAN), thereby assisting in radio resource management. In this regard, we study ML-assisted positioning methods and evaluate their performance using system level simulations for an outdoor scenario. The study is based on the use of raytracing tool, a 3GPP 5G NR compliant system level simulator and DL framework to estimate positioning accuracy of the UE. We evaluate and compare performance of various DL models and show mean positioning error in the range of 1-1.5m for a 2-hidden layer DL architecture with appropriate feature-modeling. Building on our performance analysis, we discuss pros and cons of various architectures to implement ML solutions for future networks and draw conclusions on the most suitable architecture. △ Less","25 August, 2021",https://arxiv.org/pdf/2108.11365
Quantum Artificial Intelligence for the Science of Climate Change,Manmeet Singh;Chirag Dhara;Adarsh Kumar;Sukhpal Singh Gill;Steve Uhlig,"Climate change has become one of the biggest global problems increasingly compromising the Earth's habitability. Recent developments such as the extraordinary heat waves in California & Canada, and the devastating floods in Germany point to the role of climate change in the ever-increasing frequency of extreme weather. Numerical modelling of the weather and climate have seen tremendous improvements in the last five decades, yet stringent limitations remain to be overcome. Spatially and temporally localized forecasting is the need of the hour for effective adaptation measures towards minimizing the loss of life and property. Artificial Intelligence-based methods are demonstrating promising results in improving predictions, but are still limited by the availability of requisite hardware and software required to process the vast deluge of data at a scale of the planet Earth. Quantum computing is an emerging paradigm that has found potential applicability in several fields. In this opinion piece, we argue that new developments in Artificial Intelligence algorithms designed for quantum computers - also known as Quantum Artificial Intelligence (QAI) - may provide the key breakthroughs necessary to furthering the science of climate change. The resultant improvements in weather and climate forecasts are expected to cascade to numerous societal benefits. △ Less","10 December, 2021",https://arxiv.org/pdf/2108.10855
Autoencoder-based Semantic Novelty Detection: Towards Dependable AI-based Systems,Andreas Rausch;Azarmidokht Motamedi Sedeh;Meng Zhang,"Many autonomous systems, such as driverless taxis, perform safety critical functions. Autonomous systems employ artificial intelligence (AI) techniques, specifically for the environment perception. Engineers cannot completely test or formally verify AI-based autonomous systems. The accuracy of AI-based systems depends on the quality of training data. Thus, novelty detection - identifying data that differ in some respect from the data used for training - becomes a safety measure for system development and operation. In this paper, we propose a new architecture for autoencoder-based semantic novelty detection with two innovations: architectural guidelines for a semantic autoencoder topology and a semantic error calculation as novelty criteria. We demonstrate that such a semantic novelty detection outperforms autoencoder-based novelty detection approaches known from literature by minimizing false negatives. △ Less","25 August, 2021",https://arxiv.org/pdf/2108.10851
Federated Learning for Privacy-Preserving Open Innovation Future on Digital Health,Guodong Long;Tao Shen;Yue Tan;Leah Gerrard;Allison Clarke;Jing Jiang,"Privacy protection is an ethical issue with broad concern in Artificial Intelligence (AI). Federated learning is a new machine learning paradigm to learn a shared model across users or organisations without direct access to the data. It has great potential to be the next-general AI model training framework that offers privacy protection and therefore has broad implications for the future of digital health and healthcare informatics. Implementing an open innovation framework in the healthcare industry, namely open health, is to enhance innovation and creative capability of health-related organisations by building a next-generation collaborative framework with partner organisations and the research community. In particular, this game-changing collaborative framework offers knowledge sharing from diverse data with a privacy-preserving. This chapter will discuss how federated learning can enable the development of an open health ecosystem with the support of AI. Existing challenges and solutions for federated learning will be discussed. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10761
Interpretable deep-learning models to help achieve the Sustainable Development Goals,Ricardo Vinuesa;Beril Sirmacek,"We discuss our insights into interpretable artificial-intelligence (AI) models, and how they are essential in the context of developing ethical AI systems, as well as data-driven solutions compliant with the Sustainable Development Goals (SDGs). We highlight the potential of extracting truly-interpretable models from deep-learning methods, for instance via symbolic models obtained through inductive biases, to ensure a sustainable development of AI. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10744
Improvement of a Prediction Model for Heart Failure Survival through Explainable Artificial Intelligence,Pedro A. Moreno-Sanchez,"Cardiovascular diseases and their associated disorder of heart failure are one of the major death causes globally, being a priority for doctors to detect and predict its onset and medical consequences. Artificial Intelligence (AI) allows doctors to discover clinical indicators and enhance their diagnosis and treatments. Specifically, explainable AI offers tools to improve the clinical prediction models that experience poor interpretability of their results. This work presents an explainability analysis and evaluation of a prediction model for heart failure survival by using a dataset that comprises 299 patients who suffered heart failure. The model employs a data workflow pipeline able to select the best ensemble tree algorithm as well as the best feature selection technique. Moreover, different post-hoc techniques have been used for the explainability analysis of the model. The paper's main contribution is an explainability-driven approach to select the best prediction model for HF survival based on an accuracy-explainability balance. Therefore, the most balanced explainable prediction model implements an Extra Trees classifier over 5 selected features (follow-up time, serum creatinine, ejection fraction, age and diabetes) out of 12, achieving a balanced-accuracy of 85.1% and 79.5% with cross-validation and new unseen data respectively. The follow-up time is the most influencing feature followed by serum-creatinine and ejection-fraction. The explainable prediction model for HF survival presented in this paper would improve a further adoption of clinical prediction models by providing doctors with intuitions to better understand the reasoning of, usually, black-box AI clinical solutions, and make more reasonable and data-driven decisions. △ Less","20 August, 2021",https://arxiv.org/pdf/2108.10717
Energy time series forecasting-Analytical and empirical assessment of conventional and machine learning models,Hala Hamdoun;Alaa Sagheer;Hassan Youness,"Machine learning methods have been adopted in the literature as contenders to conventional methods to solve the energy time series forecasting (TSF) problems. Recently, deep learning methods have been emerged in the artificial intelligence field attaining astonishing performance in a wide range of applications. Yet, the evidence about their performance in to solve the energy TSF problems, in terms of accuracy and computational requirements, is scanty. Most of the review articles that handle the energy TSF problem are systematic reviews, however, a qualitative and quantitative study for the energy TSF problem is not yet available in the literature. The purpose of this paper is twofold, first it provides a comprehensive analytical assessment for conventional,machine learning, and deep learning methods that can be utilized to solve various energy TSF problems. Second, the paper carries out an empirical assessment for many selected methods through three real-world datasets. These datasets related to electrical energy consumption problem, natural gas problem, and electric power consumption of an individual household problem.The first two problems are univariate TSF and the third problem is a multivariate TSF. Com-pared to both conventional and machine learning contenders, the deep learning methods attain a significant improvement in terms of accuracy and forecasting horizons examined. In the mean-time, their computational requirements are notably greater than other contenders. Eventually,the paper identifies a number of challenges, potential research directions, and recommendations to the research community may serve as a basis for further research in the energy forecasting domain. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10663
GrADE: A graph based data-driven solver for time-dependent nonlinear partial differential equations,Yash Kumar;Souvik Chakraborty,"The physical world is governed by the laws of physics, often represented in form of nonlinear partial differential equations (PDEs). Unfortunately, solution of PDEs is non-trivial and often involves significant computational time. With recent developments in the field of artificial intelligence and machine learning, the solution of PDEs using neural network has emerged as a domain with huge potential. However, most of the developments in this field are based on either fully connected neural networks (FNN) or convolutional neural networks (CNN). While FNN is computationally inefficient as the number of network parameters can be potentially huge, CNN necessitates regular grid and simpler domain. In this work, we propose a novel framework referred to as the Graph Attention Differential Equation (GrADE) for solving time dependent nonlinear PDEs. The proposed approach couples FNN, graph neural network, and recently developed Neural ODE framework. The primary idea is to use graph neural network for modeling the spatial domain, and Neural ODE for modeling the temporal domain. The attention mechanism identifies important inputs/features and assign more weightage to the same; this enhances the performance of the proposed framework. Neural ODE, on the other hand, results in constant memory cost and allows trading of numerical precision for speed. We also propose depth refinement as an effective technique for training the proposed architecture in lesser time with better accuracy. The effectiveness of the proposed framework is illustrated using 1D and 2D Burgers' equations. Results obtained illustrate the capability of the proposed framework in modeling PDE and its scalability to larger domains without the need for retraining. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10639
A generative adversarial approach to facilitate archival-quality histopathologic diagnoses from frozen tissue sections,Kianoush Falahkheirkhah;Tao Guo;Michael Hwang;Pheroze Tamboli;Christopher G Wood;Jose A Karam;Kanishka Sircar;Rohit Bhargava,"In clinical diagnostics and research involving histopathology, formalin fixed paraffin embedded (FFPE) tissue is almost universally favored for its superb image quality. However, tissue processing time (more than 24 hours) can slow decision-making. In contrast, fresh frozen (FF) processing (less than 1 hour) can yield rapid information but diagnostic accuracy is suboptimal due to lack of clearing, morphologic deformation and more frequent artifacts. Here, we bridge this gap using artificial intelligence. We synthesize FFPE-like images ,virtual FFPE, from FF images using a generative adversarial network (GAN) from 98 paired kidney samples derived from 40 patients. Five board-certified pathologists evaluated the results in a blinded test. Image quality of the virtual FFPE data was assessed to be high and showed a close resemblance to real FFPE images. Clinical assessments of disease on the virtual FFPE images showed a higher inter-observer agreement compared to FF images. The nearly instantaneously generated virtual FFPE images can not only reduce time to information but can facilitate more precise diagnosis from routine FF images without extraneous costs and effort. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10550
Longitudinal Distance: Towards Accountable Instance Attribution,Rosina O. Weber;Prateek Goel;Shideh Amiri;Gideon Simpson,"Previous research in interpretable machine learning (IML) and explainable artificial intelligence (XAI) can be broadly categorized as either focusing on seeking interpretability in the agent's model (i.e., IML) or focusing on the context of the user in addition to the model (i.e., XAI). The former can be categorized as feature or instance attribution. Example- or sample-based methods such as those using or inspired by case-based reasoning (CBR) rely on various approaches to select instances that are not necessarily attributing instances responsible for an agent's decision. Furthermore, existing approaches have focused on interpretability and explainability but fall short when it comes to accountability. Inspired in case-based reasoning principles, this paper introduces a pseudo-metric we call Longitudinal distance and its use to attribute instances to a neural network agent's decision that can be potentially used to build accountable CBR agents. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.10437
Knowledge-based XAI through CBR: There is more to explanations than models can tell,Rosina Weber;Manil Shrestha;Adam J Johs,"The underlying hypothesis of knowledge-based explainable artificial intelligence is the data required for data-centric artificial intelligence agents (e.g., neural networks) are less diverse in contents than the data required to explain the decisions of such agents to humans. The idea is that a classifier can attain high accuracy using data that express a phenomenon from one perspective whereas the audience of explanations can entail multiple stakeholders and span diverse perspectives. We hence propose to use domain knowledge to complement the data used by agents. We formulate knowledge-based explainable artificial intelligence as a supervised data classification problem aligned with the CBR methodology. In this formulation, the inputs are case problems composed of both the inputs and outputs of the data-centric agent and case solutions, the outputs, are explanation categories obtained from domain knowledge and subject matter experts. This formulation does not typically lead to an accurate classification, preventing the selection of the correct explanation category. Knowledge-based explainable artificial intelligence extends the data in this formulation by adding features aligned with domain knowledge that can increase accuracy when selecting explanation categories. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.10363
A study on Machine Learning Approaches for Player Performance and Match Results Prediction,Harsh Mittal;Deepak Rikhari;Jitendra Kumar;Ashutosh Kumar Singh,"Cricket is unarguably one of the most popular sports in the world. Predicting the outcome of a cricket match has become a fundamental problem as we are advancing in the field of machine learning. Multiple researchers have tried to predict the outcome of a cricket match or a tournament, or to predict the performance of players during a match, or to predict the players who should be selected as per their current performance, form, morale, etc. using machine learning and artificial intelligence techniques keeping in mind extensive detailing, features, and parameters. We discuss some of these techniques along with a brief comparison among these techniques. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.10125
Deep neural networks approach to microbial colony detection -- a comparative analysis,Sylwia Majchrowska;Jarosław Pawłowski;Natalia Czerep;Aleksander Górecki;Jakub Kuciński;Tomasz Golan,"Counting microbial colonies is a fundamental task in microbiology and has many applications in numerous industry branches. Despite this, current studies towards automatic microbial counting using artificial intelligence are hardly comparable due to the lack of unified methodology and the availability of large datasets. The recently introduced AGAR dataset is the answer to the second need, but the research carried out is still not exhaustive. To tackle this problem, we compared the performance of three well-known deep learning approaches for object detection on the AGAR dataset, namely two-stage, one-stage and transformer based neural networks. The achieved results may serve as a benchmark for future experiments. △ Less","24 August, 2021",https://arxiv.org/pdf/2108.10103
Artificial Intelligence in the Global South (AI4D): Potential and Risks,P. J. Wall;Deepak Saxena;Suzana Brown,"Artificial intelligence is becoming more widely available in all parts of the world. This has created many previously unforeseen possibilities for addressing the challenges outlined in the Sustainable Development Goals in the Global South. However, the use of AI in such contexts brings with it a unique set of risks and challenges. Among these are the potential for Governments to use such technologies to suppress their own people, and the ethical questions arising from implementing AI primarily designed and developed in the Global North into vastly different social, cultural, and political environments in the Global South. This paper examines the key issues and questions arising in the emerging sub-field of AI for global development (AI4D) and the potential and risks associated with using such technologies in the Global South. We propose that although there are many risks associated with the use of AI, the potential benefits are enough to warrant detailed research and investigation of the most appropriate and effective ways to design, develop, implement, and use such technologies in the Global South. We conclude by calling for the wider ICT4D community to continue to conduct detailed research and investigation of all aspects of AI4D. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.10093
Development of A Fully Data-Driven Artificial Intelligence and Deep Learning for URLLC Application in 6G Wireless Systems: A Survey,Adeeb Salh;Lukman Audah;Qazwan Abdullah;Abdullah Noorsaliza;Nor Shahida Mohd Shah;Jameel Mukred;Shipun Hamzah,"The full future of the sixth generation will develop a fully data-driven that provide terabit rate per second, and adopt an average of 1000+ massive number of connections per person in 10 years 2030 virtually instantaneously. Data-driven for ultra-reliable and low latency communication is a new service paradigm provided by a new application of future sixth-generation wireless communication and network architecture, involving 100+ Gbps data rates with one millisecond latency. The key constraint is the amount of computing power available to spread massive data and well-designed artificial neural networks. Artificial Intelligence provides a new technique to design wireless networks by apply learning, predicting, and make decisions to manage the stream of big data training individuals, which provides more the capacity to transform that expert learning to develop the performance of wireless networks. We study the developing technologies that will be the driving force are artificial intelligence, communication systems to guarantee low latency. This paper aims to discuss the efficiency of the developing network and alleviate the great challenge for application scenarios and study Holographic radio, enhanced wireless channel coding, enormous Internet of Things integration, and haptic communication for virtual and augmented reality provide new services on the 6G network. Furthermore, improving a multi-level architecture for ultra-reliable and low latency in deep Learning allows for data-driven AI and 6G networks for device intelligence, as well as allowing innovations based on effective learning capabilities. These difficulties must be solved in order to meet the needs of future smart networks. Furthermore, this research categorizes various unexplored research gaps between machine learning and the sixth generation. △ Less","3 August, 2021",https://arxiv.org/pdf/2108.10076
Artificial Intelligence Ethics: An Inclusive Global Discourse?,Cathy Roche;Dave Lewis;P. J. Wall,"It is widely accepted that technology is ubiquitous across the planet and has the potential to solve many of the problems existing in the Global South. Moreover, the rapid advancement of artificial intelligence (AI) brings with it the potential to address many of the challenges outlined in the Sustainable Development Goals (SDGs) in ways which were never before possible. However, there are many questions about how such advanced technologies should be managed and governed, and whether or not the emerging ethical frameworks and standards for AI are dominated by the Global North. This research examines the growing body of documentation on AI ethics to examine whether or not there is equality of participation in the ongoing global discourse. Specifically, it seeks to discover if both countries in the Global South and women are underrepresented in this discourse. Findings indicate a dearth of references to both of these themes in the AI ethics documents, suggesting that the associated ethical implications and risks are being neglected. Without adequate input from both countries in the Global South and from women, such ethical frameworks and standards may be discriminatory with the potential to reinforce marginalisation. △ Less","23 August, 2021",https://arxiv.org/pdf/2108.09959
SAI-BA-IoMT: Secure AI-Based Blockchain-Assisted Internet of Medical Things Tool to Moderate the Outbreak of COVID-19 Crisis,Mahender Kumar;Ruby Rani,"Recently, an infectious disease, coronavirus disease 2019 (COVID-19), has been reported in Wuhan, China, and subsequently spread worldwide within a couple of months. On May 16, 2020, the COVID-19 pandemic has affected several countries in the world. In this article, we present how the amalgamation of blockchain, cryptography, internet of medical things (IoMT), and artificial intelligence (AI) technologies can address such an issue in the event of the COVID-19 pandemic. Further, we propose a secure AI-based blockchain-assisted IoMT (SAI-BA-IoMT) model for the healthcare system in the COVID-19 crisis. The paper also examines the post-corona crisis that the world could be experienced after the pandemic. Additionally, we exhibit the potential applications of the proposed model to resolve the difficulties originated from coronavirus. △ Less","21 August, 2021",https://arxiv.org/pdf/2108.09539
DeepEdgeBench: Benchmarking Deep Neural Networks on Edge Devices,Stephan Patrick Baller;Anshul Jindal;Mohak Chadha;Michael Gerndt,"EdgeAI (Edge computing based Artificial Intelligence) has been most actively researched for the last few years to handle variety of massively distributed AI applications to meet up the strict latency requirements. Meanwhile, many companies have released edge devices with smaller form factors (low power consumption and limited resources) like the popular Raspberry Pi and Nvidia's Jetson Nano for acting as compute nodes at the edge computing environments. Although the edge devices are limited in terms of computing power and hardware resources, they are powered by accelerators to enhance their performance behavior. Therefore, it is interesting to see how AI-based Deep Neural Networks perform on such devices with limited resources. In this work, we present and compare the performance in terms of inference time and power consumption of the four Systems on a Chip (SoCs): Asus Tinker Edge R, Raspberry Pi 4, Google Coral Dev Board, Nvidia Jetson Nano, and one microcontroller: Arduino Nano 33 BLE, on different deep learning models and frameworks. We also provide a method for measuring power consumption, inference time and accuracy for the devices, which can be easily extended to other devices. Our results showcase that, for Tensorflow based quantized model, the Google Coral Dev Board delivers the best performance, both for inference time and power consumption. For a low fraction of inference computation time, i.e. less than 29.3% of the time for MobileNetV2, the Jetson Nano performs faster than the other devices. △ Less","21 August, 2021",https://arxiv.org/pdf/2108.09457
Safe Transformative AI via a Windfall Clause,Paolo Bova;Jonas Emanuel Müller;Benjamin Harack,"Society could soon see transformative artificial intelligence (TAI). Models of competition for TAI show firms face strong competitive pressure to deploy TAI systems before they are safe. This paper explores a proposed solution to this problem, a Windfall Clause, where developers commit to donating a significant portion of any eventual extremely large profits to good causes. However, a key challenge for a Windfall Clause is that firms must have reason to join one. Firms must also believe these commitments are credible. We extend a model of TAI competition with a Windfall Clause to show how firms and policymakers can design a Windfall Clause which overcomes these challenges. Encouragingly, firms benefit from joining a Windfall Clause under a wide range of scenarios. We also find that firms join the Windfall Clause more often when the competition is more dangerous. Even when firms learn each other's capabilities, firms rarely wish to withdraw their support for the Windfall Clause. These three findings strengthen the case for using a Windfall Clause to promote the safe development of TAI. △ Less","28 August, 2021",https://arxiv.org/pdf/2108.09404
"OSRM-CCTV: Open-source CCTV-aware routing and navigation system for privacy, anonymity and safety (Preprint)",Lauri Sintonen;Hannu Turtiainen;Andrei Costin;Timo Hamalainen;Tuomo Lahtinen,"For the last several decades, the increased, widespread, unwarranted, and unaccountable use of Closed-Circuit TeleVision (CCTV) cameras globally has raised concerns about privacy risks. Additional recent features of many CCTV cameras, such as Internet of Things (IoT) connectivity and Artificial Intelligence (AI)-based facial recognition, only increase concerns among privacy advocates. Therefore, on par \emph{CCTV-aware solutions} must exist that provide privacy, safety, and cybersecurity features. We argue that an important step forward is to develop solutions addressing privacy concerns via routing and navigation systems (e.g., OpenStreetMap, Google Maps) that provide both privacy and safety options for areas where cameras are known to be present. However, at present no routing and navigation system, whether online or offline, provide corresponding CCTV-aware functionality. In this paper we introduce OSRM-CCTV -- the first and only CCTV-aware routing and navigation system designed and built for privacy, anonymity and safety applications. We validate and demonstrate the effectiveness and usability of the system on a handful of synthetic and real-world examples. To help validate our work as well as to further encourage the development and wide adoption of the system, we release OSRM-CCTV as open-source. △ Less","20 August, 2021",https://arxiv.org/pdf/2108.09369
MHealth: An Artificial Intelligence Oriented Mobile Application for Personal Healthcare Support,Ismail Ali Afrah;Utku Kose,"Main objective of this study is to introduce an expert system-based mHealth application that takes Artificial Intelligence support by considering previously introduced solutions from the literature and employing possible requirements for a better solution. Thanks to that research study, a mobile software system having Artificial Intelligence support and providing dynamic support against the common health problems in daily life was designed-developed and it was evaluated via survey and diagnosis-based evaluation tasks. Evaluation tasks indicated positive outcomes for the mHealth system. △ Less","18 August, 2021",https://arxiv.org/pdf/2108.09277
Explainable Reinforcement Learning for Broad-XAI: A Conceptual Framework and Survey,Richard Dazeley;Peter Vamplew;Francisco Cruz,"Broad Explainable Artificial Intelligence moves away from interpreting individual decisions based on a single datum and aims to provide integrated explanations from multiple machine learning algorithms into a coherent explanation of an agent's behaviour that is aligned to the communication needs of the explainee. Reinforcement Learning (RL) methods, we propose, provide a potential backbone for the cognitive model required for the development of Broad-XAI. RL represents a suite of approaches that have had increasing success in solving a range of sequential decision-making problems. However, these algorithms all operate as black-box problem solvers, where they obfuscate their decision-making policy through a complex array of values and functions. EXplainable RL (XRL) is relatively recent field of research that aims to develop techniques to extract concepts from the agent's: perception of the environment; intrinsic/extrinsic motivations/beliefs; Q-values, goals and objectives. This paper aims to introduce a conceptual framework, called the Causal XRL Framework (CXF), that unifies the current XRL research and uses RL as a backbone to the development of Broad-XAI. Additionally, we recognise that RL methods have the ability to incorporate a range of technologies to allow agents to adapt to their environment. CXF is designed for the incorporation of many standard RL extensions and integrated with external ontologies and communication facilities so that the agent can answer questions that explain outcomes and justify its decisions. △ Less","20 August, 2021",https://arxiv.org/pdf/2108.09003
Deep Sequence Modeling: Development and Applications in Asset Pricing,Lin William Cong;Ke Tang;Jingyuan Wang;Yang Zhang,"We predict asset returns and measure risk premia using a prominent technique from artificial intelligence -- deep sequence modeling. Because asset returns often exhibit sequential dependence that may not be effectively captured by conventional time series models, sequence modeling offers a promising path with its data-driven approach and superior performance. In this paper, we first overview the development of deep sequence models, introduce their applications in asset pricing, and discuss their advantages and limitations. We then perform a comparative analysis of these methods using data on U.S. equities. We demonstrate how sequence modeling benefits investors in general through incorporating complex historical path dependence, and that Long- and Short-term Memory (LSTM) based models tend to have the best out-of-sample performance. △ Less","20 August, 2021",https://arxiv.org/pdf/2108.08999
Retrieval and Localization with Observation Constraints,Yuhao Zhou;Huanhuan Fan;Shuang Gao;Yuchen Yang;Xudong Zhang;Jijunnan Li;Yandong Guo,"Accurate visual re-localization is very critical to many artificial intelligence applications, such as augmented reality, virtual reality, robotics and autonomous driving. To accomplish this task, we propose an integrated visual re-localization method called RLOCS by combining image retrieval, semantic consistency and geometry verification to achieve accurate estimations. The localization pipeline is designed as a coarse-to-fine paradigm. In the retrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ DBSCAN followed by spatial verification to obtain a better initial coarse pose. We design a module called observation constraints, which combines geometry information and semantic consistency for filtering outliers. Comprehensive experiments are conducted on open datasets, including retrieval on R-Oxford5k and R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen Day-Night and InLoc. By creatively modifying separate modules in the total pipeline, our method achieves many performance improvements on the challenging localization benchmarks. △ Less","19 August, 2021",https://arxiv.org/pdf/2108.08516
"Edge AI without Compromise: Efficient, Versatile and Accurate Neurocomputing in Resistive Random-Access Memory",Weier Wan;Rajkumar Kubendran;Clemens Schaefer;S. Burc Eryilmaz;Wenqiang Zhang;Dabin Wu;Stephen Deiss;Priyanka Raina;He Qian;Bin Gao;Siddharth Joshi;Huaqiang Wu;H. -S. Philip Wong;Gert Cauwenberghs,"Realizing today's cloud-level artificial intelligence functionalities directly on devices distributed at the edge of the internet calls for edge hardware capable of processing multiple modalities of sensory data (e.g. video, audio) at unprecedented energy-efficiency. AI hardware architectures today cannot meet the demand due to a fundamental ""memory wall"": data movement between separate compute and memory units consumes large energy and incurs long latency. Resistive random-access memory (RRAM) based compute-in-memory (CIM) architectures promise to bring orders of magnitude energy-efficiency improvement by performing computation directly within memory. However, conventional approaches to CIM hardware design limit its functional flexibility necessary for processing diverse AI workloads, and must overcome hardware imperfections that degrade inference accuracy. Such trade-offs between efficiency, versatility and accuracy cannot be addressed by isolated improvements on any single level of the design. By co-optimizing across all hierarchies of the design from algorithms and architecture to circuits and devices, we present NeuRRAM - the first multimodal edge AI chip using RRAM CIM to simultaneously deliver a high degree of versatility for diverse model architectures, record energy-efficiency 5\times - 8\times better than prior art across various computational bit-precisions, and inference accuracy comparable to software models with 4-bit weights on all measured standard AI benchmarks including accuracy of 99.0% on MNIST and 85.7% on CIFAR-10 image classification, 84.7% accuracy on Google speech command recognition, and a 70% reduction in image reconstruction error on a Bayesian image recovery task. This work paves a way towards building highly efficient and reconfigurable edge AI hardware platforms for the more demanding and heterogeneous AI applications of the future. △ Less","17 August, 2021",https://arxiv.org/pdf/2108.07879
A Framework for Understanding AI-Induced Field Change: How AI Technologies are Legitimized and Institutionalized,Benjamin Cedric Larsen,"Artificial intelligence (AI) systems operate in increasingly diverse areas, from healthcare to facial recognition, the stock market, autonomous vehicles, and so on. While the underlying digital infrastructure of AI systems is developing rapidly, each area of implementation is subject to different degrees and processes of legitimization. By combining elements from institutional theory and information systems-theory, this paper presents a conceptual framework to analyze and understand AI-induced field-change. The introduction of novel AI-agents into new or existing fields creates a dynamic in which algorithms (re)shape organizations and institutions while existing institutional infrastructures determine the scope and speed at which organizational change is allowed to occur. Where institutional infrastructure and governance arrangements, such as standards, rules, and regulations, still are unelaborate, the field can move fast but is also more likely to be contested. The institutional infrastructure surrounding AI-induced fields is generally little elaborated, which could be an obstacle to the broader institutionalization of AI-systems going forward. △ Less","18 August, 2021",https://arxiv.org/pdf/2108.07804
Harnessing value from data science in business: ensuring explainability and fairness of solutions,Krzysztof Chomiak;Michał Miktus,"The paper introduces concepts of fairness and explainability (XAI) in artificial intelligence, oriented to solve a sophisticated business problems. For fairness, the authors discuss the bias-inducing specifics, as well as relevant mitigation methods, concluding with a set of recipes for introducing fairness in data-driven organizations. Additionally, for XAI, the authors audit specific algorithms paired with demonstrational business use-cases, discuss a plethora of techniques of explanations quality quantification and provide an overview of future research avenues. △ Less","10 August, 2021",https://arxiv.org/pdf/2108.07714
Explainability Auditing for Intelligent Systems: A Rationale for Multi-Disciplinary Perspectives,Markus Langer;Kevin Baum;Kathrin Hartmann;Stefan Hessel;Timo Speith;Jonas Wahl,"National and international guidelines for trustworthy artificial intelligence (AI) consider explainability to be a central facet of trustworthy systems. This paper outlines a multi-disciplinary rationale for explainability auditing. Specifically, we propose that explainability auditing can ensure the quality of explainability of systems in applied contexts and can be the basis for certification as a means to communicate whether systems meet certain explainability standards and requirements. Moreover, we emphasize that explainability auditing needs to take a multi-disciplinary perspective, and we provide an overview of four perspectives (technical, psychological, ethical, legal) and their respective benefits with respect to explainability auditing. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.07711
Implementation of Sprouts: a graph drawing game,Tomáš Čížek;Martin Balko,"Sprouts is a two-player pencil-and-paper game invented by John Conway and Michael Paterson in 1967. In the game, the players take turns in joining dots by curves according to simple rules, until one player cannot make a move. The game of Sprouts is very popular and simple-looking, so it may come as a surprise that there are essentially no AI Sprouts players available. This lack of computer opponents is caused by the fact that the game hides a surprisingly high combinatorial complexity and implementing it involves fascinating programming challenges. We overcome all the implementation barriers and create the first user-friendly Sprouts application with a strong artificial intelligence after more than 50 years of the existence of the game. In particular, we combine results from the theory of nimbers with new methods based on Delaunay triangulations and crossing-preserving force-directed algorithms to develop an AI Sprouts player which plays a perfect game on up to 11 spots. △ Less","3 October, 2021",https://arxiv.org/pdf/2108.07671
Independent Ethical Assessment of Text Classification Models: A Hate Speech Detection Case Study,Amitoj Singh;Jingshu Chen;Lihao Zhang;Amin Rasekh;Ilana Golbin;Anand Rao,"An independent ethical assessment of an artificial intelligence system is an impartial examination of the system's development, deployment, and use in alignment with ethical values. System-level qualitative frameworks that describe high-level requirements and component-level quantitative metrics that measure individual ethical dimensions have been developed over the past few years. However, there exists a gap between the two, which hinders the execution of independent ethical assessments in practice. This study bridges this gap and designs a holistic independent ethical assessment process for a text classification model with a special focus on the task of hate speech detection. The assessment is further augmented with protected attributes mining and counterfactual-based analysis to enhance bias assessment. It covers assessments of technical performance, data bias, embedding bias, classification bias, and interpretability. The proposed process is demonstrated through an assessment of a deep hate speech detection model. △ Less","19 July, 2021",https://arxiv.org/pdf/2108.07627
FARF: A Fair and Adaptive Random Forests Classifier,Wenbin Zhang;Albert Bifet;Xiangliang Zhang;Jeremy C. Weiss;Wolfgang Nejdl,"As Artificial Intelligence (AI) is used in more applications, the need to consider and mitigate biases from the learned models has followed. Most works in developing fair learning algorithms focus on the offline setting. However, in many real-world applications data comes in an online fashion and needs to be processed on the fly. Moreover, in practical application, there is a trade-off between accuracy and fairness that needs to be accounted for, but current methods often have multiple hyperparameters with non-trivial interaction to achieve fairness. In this paper, we propose a flexible ensemble algorithm for fair decision-making in the more challenging context of evolving online settings. This algorithm, called FARF (Fair and Adaptive Random Forests), is based on using online component classifiers and updating them according to the current distribution, that also accounts for fairness and a single hyperparameters that alters fairness-accuracy balance. Experiments on real-world discriminated data streams demonstrate the utility of FARF. △ Less","21 August, 2021",https://arxiv.org/pdf/2108.07403
Supply of engineering techniques and software design patterns in psychoanalysis and psychometrics sciences,Omid Shokrollahi,"The purpose of this study is to introduce software technologies and models and artificial intelligence algorithms to improve the weaknesses of CBT (Cognitive Behavior Therapy) method in psychotherapy. The presentation method for this purpose is the implementation of psychometric experiments in which the hidden human variables are inferred from the answers of tests. In this report, we describe the various models of Item Response Theory and measure the hidden components of ability and complementary parameters of the reality of the individual's situation. Psychometrics, selecting the appropriate model and estimating its parameters have been introduced and implemented using R language developed libraries. Due to the high flexibility of the Multi variant Rasch mixture Model, machine learning has been applied to this method of data modeling. BIC and CML were used to determine the number of hidden classes of the model and its parameters respectively, to obtain Measurement Invariance. The sensitivity of items to hidden attributes varies between groups (DIF), so methods for detecting it are introduced. This simulation is done based on the Verbal Aggression Dataset. We also analyze and compile a reference model based on this certificate based on the discovered patterns of software engineering. Other achievements of this study are related to providing a solution to explain the reengineering problems of the mind, by preparing an identity card for the clients by an ontology. Finally, applying the developed knowledge in the form of system thinking and recommended patterns in software engineering during the treatment process is pointed out. △ Less","16 August, 2021",https://arxiv.org/pdf/2108.06963
Challenges for cognitive decoding using deep learning methods,Armin W. Thomas;Christopher Ré;Russell A. Poldrack,"In cognitive decoding, researchers aim to characterize a brain region's representations by identifying the cognitive states (e.g., accepting/rejecting a gamble) that can be identified from the region's activity. Deep learning (DL) methods are highly promising for cognitive decoding, with their unmatched ability to learn versatile representations of complex data. Yet, their widespread application in cognitive decoding is hindered by their general lack of interpretability as well as difficulties in applying them to small datasets and in ensuring their reproducibility and robustness. We propose to approach these challenges by leveraging recent advances in explainable artificial intelligence and transfer learning, while also providing specific recommendations on how to improve the reproducibility and robustness of DL modeling results. △ Less","16 August, 2021",https://arxiv.org/pdf/2108.06896
Evolution Toward 6G Wireless Networks: A Resource Management Perspective,Mehdi Rasti;Shiva Kazemi Taskou;Hina Tabassum;Ekram Hossain,"In this article, we first present the vision, key performance indicators, key enabling techniques (KETs), and services of 6G wireless networks. Then, we highlight a series of general resource management (RM) challenges as well as unique RM challenges corresponding to each KET. The unique RM challenges in 6G necessitate the transformation of existing optimization-based solutions to artificial intelligence/machine learning-empowered solutions. In the sequel, we formulate a joint network selection and subchannel allocation problem for 6G multi-band network that provides both further enhanced mobile broadband (FeMBB) and extreme ultra reliable low latency communication (eURLLC) services to the terrestrial and aerial users. Our solution highlights the efficacy of multi-band network and demonstrates the robustness of dueling deep Q-learning in obtaining efficient RM solution with faster convergence rate compared to deep-Q network and double deep Q-network algorithms. △ Less","14 August, 2021",https://arxiv.org/pdf/2108.06527
Towards artificially intelligent recycling Improving image processing for waste classification,Youpeng Yu;Ryan Grammenos,"The ever-increasing amount of global refuse is overwhelming the waste and recycling management industries. The need for smart systems for environmental monitoring and the enhancement of recycling processes is thus greater than ever. Amongst these efforts lies IBM's Wastenet project which aims to improve recycling by using artificial intelligence for waste classification. The work reported in this paper builds on this project through the use of transfer learning and data augmentation techniques to ameliorate classification accuracy. Starting with a convolutional neural network (CNN), a systematic approach is followed for selecting appropriate splitting ratios and for tuning multiple training parameters including learning rate schedulers, layers freezing, batch sizes and loss functions, in the context of the given scenario which requires classification of waste into different recycling types. Results are compared and contrasted using 10-fold cross validation and demonstrate that the model developed achieves a 91.21% test accuracy. Subsequently, a range of data augmentation techniques are then incorporated into this work including flipping, rotation, shearing, zooming, and brightness control. Results show that these augmentation techniques further improve the test accuracy of the final model to 95.40%. Unlike other work reported in the field, this paper provides full details regarding the training of the model. Furthermore, the code for this work has been made open-source and we have demonstrated that the model can perform successful real-time classification of recycling waste items using a standard computer webcam. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.06274
"MAIR: Framework for mining relationships between research articles, strategies, and regulations in the field of explainable artificial intelligence",Stanisław Gizinski;Michał Kuzba;Bartosz Pielinski;Julian Sienkiewicz;Stanisław Łaniewski;Przemysław Biecek,"The growing number of AI applications, also for high-stake decisions, increases the interest in Explainable and Interpretable Machine Learning (XI-ML). This trend can be seen both in the increasing number of regulations and strategies for developing trustworthy AI and the growing number of scientific papers dedicated to this topic. To ensure the sustainable development of AI, it is essential to understand the dynamics of the impact of regulation on research papers as well as the impact of scientific discourse on AI-related policies. This paper introduces a novel framework for joint analysis of AI-related policy documents and eXplainable Artificial Intelligence (XAI) research papers. The collected documents are enriched with metadata and interconnections, using various NLP methods combined with a methodology inspired by Institutional Grammar. Based on the information extracted from collected documents, we showcase a series of analyses that help understand interactions, similarities, and differences between documents at different stages of institutionalization. To the best of our knowledge, this is the first work to use automatic language analysis tools to understand the dynamics between XI-ML methods and regulations. We believe that such a system contributes to better cooperation between XAI researchers and AI policymakers. △ Less","29 July, 2021",https://arxiv.org/pdf/2108.06216
Competency Model Approach to AI Literacy: Research-based Path from Initial Framework to Model,Farhana Faruqe;Ryan Watkins;Larry Medsker,"The recent developments in Artificial Intelligence (AI) technologies challenge educators and educational institutions to respond with curriculum and resources that prepare students of all ages with the foundational knowledge and skills for success in the AI workplace. Research on AI Literacy could lead to an effective and practical platform for developing these skills. We propose and advocate for a pathway for developing AI Literacy as a pragmatic and useful tool for AI education. Such a discipline requires moving beyond a conceptual framework to a multi-level competency model with associated competency assessments. This approach to an AI Literacy could guide future development of instructional content as we prepare a range of groups (i.e., consumers, co-workers, collaborators, and creators). We propose here a research matrix as an initial step in the development of a roadmap for AI Literacy research, which requires a systematic and coordinated effort with the support of publication outlets and research funding, to expand the areas of competency and assessments. △ Less","12 August, 2021",https://arxiv.org/pdf/2108.05809
Networked Twins and Twins of Networks: an Overview on the Relationship Between Digital Twins and 6G,Hamed Ahmadi;Avishek Nag;Zaheer Khan;Kamran Sayrafian;Susanto Rahadrja,"Digital Twin (DT) is a promising technology for the new immersive digital life with a variety of applications in areas such as Industry 4.0, aviation, and healthcare. Proliferation of this technology requires higher data rates, reliability, resilience, and lower latency beyond what is currently offered by 5G. Thus, DT can become a major driver for 6G research and development. Alternatively, 6G network development can benefit from Digital Twin technology and its powerful features such as modularity and remote intelligence. Using DT, a 6G network (or some of its components) will have the opportunity to use Artificial Intelligence more proactively in order to enhance its resilience. DT's application in telecommunications is still in its infancy. In this article we highlight some of the most promising research and development directions for this technology. △ Less","12 August, 2021",https://arxiv.org/pdf/2108.05781
A Mathematical Approach to Constraining Neural Abstraction and the Mechanisms Needed to Scale to Higher-Order Cognition,Ananta Nair,"Artificial intelligence has made great strides in the last decade but still falls short of the human brain, the best-known example of intelligence. Not much is known of the neural processes that allow the brain to make the leap to achieve so much from so little beyond its ability to create knowledge structures that can be flexibly and dynamically combined, recombined, and applied in new and novel ways. This paper proposes a mathematical approach using graph theory and spectral graph theory, to hypothesize how to constrain these neural clusters of information based on eigen-relationships. This same hypothesis is hierarchically applied to scale up from the smallest to the largest clusters of knowledge that eventually lead to model building and reasoning. △ Less","11 August, 2021",https://arxiv.org/pdf/2108.05494
"Intelligence as information processing: brains, swarms, and computers",Carlos Gershenson,"There is no agreed definition of intelligence, so it is problematic to simply ask whether brains, swarms, computers, or other systems are intelligent or not. To compare the potential intelligence exhibited by different cognitive systems, I use the common approach used by artificial intelligence and artificial life: Instead of studying the substrate of systems, let us focus on their organization. This organization can be measured with information. Thus, I apply an informationist epistemology to describe cognitive systems, including brains and computers. This allows me to frame the usefulness and limitations of the brain-computer analogy in different contexts. I also use this perspective to discuss the evolution and ecology of intelligence. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.05349
Logic Explained Networks,Gabriele Ciravegna;Pietro Barbiero;Francesco Giannini;Marco Gori;Pietro Lió;Marco Maggini;Stefano Melacci,"The large and still increasing popularity of deep learning clashes with a major limit of neural network architectures, that consists in their lack of capability in providing human-understandable motivations of their decisions. In situations in which the machine is expected to support the decision of human experts, providing a comprehensible explanation is a feature of crucial importance. The language used to communicate the explanations must be formal enough to be implementable in a machine and friendly enough to be understandable by a wide audience. In this paper, we propose a general approach to Explainable Artificial Intelligence in the case of neural architectures, showing how a mindful design of the networks leads to a family of interpretable deep learning models called Logic Explained Networks (LENs). LENs only require their inputs to be human-understandable predicates, and they provide explanations in terms of simple First-Order Logic (FOL) formulas involving such predicates. LENs are general enough to cover a large number of scenarios. Amongst them, we consider the case in which LENs are directly used as special classifiers with the capability of being explainable, or when they act as additional networks with the role of creating the conditions for making a black-box classifier explainable by FOL formulas. Despite supervised learning problems are mostly emphasized, we also show that LENs can learn and provide explanations in unsupervised learning settings. Experimental results on several datasets and tasks show that LENs may yield better classifications than established white-box models, such as decision trees and Bayesian rule lists, while providing more compact and meaningful explanations. △ Less","11 August, 2021",https://arxiv.org/pdf/2108.05149
Prioritized SIPP for Multi-Agent Path Finding With Kinematic Constraints,Zain Alabedeen Ali;Konstantin Yakovlev,"Multi-Agent Path Finding (MAPF) is a long-standing problem in Robotics and Artificial Intelligence in which one needs to find a set of collision-free paths for a group of mobile agents (robots) operating in the shared workspace. Due to its importance, the problem is well-studied and multiple optimal and approximate algorithms are known. However, many of them abstract away from the kinematic constraints and assume that the agents can accelerate/decelerate instantaneously. This complicates the application of the algorithms on the real robots. In this paper, we present a method that mitigates this issue to a certain extent. The suggested solver is essentially, a prioritized planner based on the well-known Safe Interval Path Planning (SIPP) algorithm. Within SIPP we explicitly reason about the speed and the acceleration thus the constructed plans directly take kinematic constraints of agents into account. We suggest a range of heuristic functions for that setting and conduct a thorough empirical evaluation of the suggested algorithm. △ Less","11 August, 2021",https://arxiv.org/pdf/2108.05145
Snakes AI Competition 2020 and 2021 Report,Joseph Alexander Brown;Luiz Jonata Pires de Araujo;Alexandr Grichshenko,"The Snakes AI Competition was held by the Innopolis University and was part of the IEEE Conference on Games2020 and 2021 editions. It aimed to create a sandbox for learning and implementing artificial intelligence algorithms in agents in a ludic manner. Competitors of several countries participated in both editions of the competition, which was streamed to create asynergy between organizers and the community. The high-quality submissions and the enthusiasm around the developed framework create an exciting scenario for future extensions. △ Less","11 August, 2021",https://arxiv.org/pdf/2108.05136
Examining correlation between trust and transparency with explainable artificial intelligence,Arnav Kartikeya,"Trust between humans and artificial intelligence(AI) is an issue which has implications in many fields of human computer interaction. The current issue with artificial intelligence is a lack of transparency into its decision making, and literature shows that increasing transparency increases trust. Explainable artificial intelligence has the ability to increase transparency of AI, which could potentially increase trust for humans. This paper attempts to use the task of predicting yelp review star ratings with assistance from an explainable and non explainable artificial intelligence to see if trust is increased with increased transparency. Results show that for these tasks, explainable artificial intelligence provided significant increase in trust as a measure of influence. △ Less","10 August, 2021",https://arxiv.org/pdf/2108.04770
Logical Information Cells I,Jean-Claude Belfiore;Daniel Bennequin;Xavier Giraud,"In this study we explore the spontaneous apparition of visible intelligible reasoning in simple artificial networks, and we connect this experimental observation with a notion of semantic information. We start with the reproduction of a DNN model of natural neurons in monkeys, studied by Neromyliotis and Moschovakis in 2017 and 2018, to explain how ""motor equivalent neurons"", coding only for the action of pointing, are supplemented by other neurons for specifying the actor of the action, the eye E, the hand H, or the eye and the hand together EH. There appear inner neurons performing a logical work, making intermediary proposition, for instance E V EH. Then, we remarked that adding a second hidden layer and choosing a symmetric metric for learning, the activities of the neurons become almost quantized and more informative. Using the work of Carnap and Bar-Hillel 1952, we define a measure of the logical value for collections of such cells. The logical score growths with the depth of the layer, i.e. the information on the output decision increases, which confirms a kind of bottleneck principle. Then we study a bit more complex tasks, a priori involving predicate logic. We compare the logic and the measured weights. This shows, for groups of neurons, a neat correlation between the logical score and the size of the weights. It exhibits a form of sparsity between the layers. The most spectacular result concerns the triples which can conclude for all conditions: when applying their weight matrices to their logical matrix, we recover the classification. This shows that weights precisely perform the proofs. △ Less","10 August, 2021",https://arxiv.org/pdf/2108.04751
Industrial Digital Twins at the Nexus of NextG Wireless Networks and Computational Intelligence: A Survey,Shah Zeb;Aamir Mahmood;Syed Ali Hassan;MD. Jalil Piran;Mikael Gidlund;Mohsen Guizani,"By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry~4.0 promotes integrating cyber-physical worlds through cyber-physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT is an emerging but conceptually different construct than CPS. Like CPS, DT relies on communication to create a highly-consistent, synchronized digital mirror image of the objects or physical processes. DT, in addition, uses built-in models on this precise image to simulate, analyze, predict, and optimize their real-time operation using feedback. DT is rapidly diffusing in the industries with recent advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research in next-generation (NextG) wireless technologies (e.g., 5G and beyond networks), various tools (e.g., age of information, federated learning, data analytics), and other promising trends in networked computing (e.g., edge and cloud computing). Moreover, we discuss the DT deployment strategies at different industrial communication layers to meet the monitoring and control requirements of industrial applications. We also outline several key reflections and future research challenges and directions to facilitate industrial DT's adoption. △ Less","10 August, 2021",https://arxiv.org/pdf/2108.04465
Detecting Visual Design Principles in Art and Architecture through Deep Convolutional Neural Networks,Gozdenur Demir;Asli Cekmis;Vahit Bugra Yesilkaynak;Gozde Unal,"Visual design is associated with the use of some basic design elements and principles. Those are applied by the designers in the various disciplines for aesthetic purposes, relying on an intuitive and subjective process. Thus, numerical analysis of design visuals and disclosure of the aesthetic value embedded in them are considered as hard. However, it has become possible with emerging artificial intelligence technologies. This research aims at a neural network model, which recognizes and classifies the design principles over different domains. The domains include artwork produced since the late 20th century; professional photos; and facade pictures of contemporary buildings. The data collection and curation processes, including the production of computationally-based synthetic dataset, is genuine. The proposed model learns from the knowledge of myriads of original designs, by capturing the underlying shared patterns. It is expected to consolidate design processes by providing an aesthetic evaluation of the visual compositions with objectivity. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.04048
Knowledge accumulating: The general pattern of learning,Zhuoran Xu;Hao Liu,"Artificial Intelligence has been developed for decades with the achievement of great progress. Recently, deep learning shows its ability to solve many real world problems, e.g. image classification and detection, natural language processing, playing GO. Theoretically speaking, an artificial neural network can fit any function and reinforcement learning can learn from any delayed reward. But in solving real world tasks, we still need to spend a lot of effort to adjust algorithms to fit task unique features. This paper proposes that the reason of this phenomenon is the sparse feedback feature of the nature, and a single algorithm, no matter how we improve it, can only solve dense feedback tasks or specific sparse feedback tasks. This paper first analyses how sparse feedback affects algorithm perfomance, and then proposes a pattern that explains how to accumulate knowledge to solve sparse feedback problems. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.03988
A Neural Approach for Detecting Morphological Analogies,Safa Alsaidi;Amandine Decker;Puthineath Lay;Esteban Marquer;Pierre-Alexandre Murena;Miguel Couceiro,"Analogical proportions are statements of the form ""A is to B as C is to D"" that are used for several reasoning and classification tasks in artificial intelligence and natural language processing (NLP). For instance, there are analogy based approaches to semantics as well as to morphology. In fact, symbolic approaches were developed to solve or to detect analogies between character strings, e.g., the axiomatic approach as well as that based on Kolmogorov complexity. In this paper, we propose a deep learning approach to detect morphological analogies, for instance, with reinflexion or conjugation. We present empirical results that show that our framework is competitive with the above-mentioned state of the art symbolic approaches. We also explore empirically its transferability capacity across languages, which highlights interesting similarities between them. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.03945
On the Transferability of Neural Models of Morphological Analogies,Safa Alsaidi;Amandine Decker;Puthineath Lay;Esteban Marquer;Pierre-Alexandre Murena;Miguel Couceiro,"Analogical proportions are statements expressed in the form ""A is to B as C is to D"" and are used for several reasoning and classification tasks in artificial intelligence and natural language processing (NLP). In this paper, we focus on morphological tasks and we propose a deep learning approach to detect morphological analogies. We present an empirical study to see how our framework transfers across languages, and that highlights interesting similarities and differences between these languages. In view of these results, we also discuss the possibility of building a multilingual morphological model. △ Less","9 August, 2021",https://arxiv.org/pdf/2108.03938
Toward Human-Level Artificial Intelligence,Deokgun Park,"In this paper, we present our research on programming human-level artificial intelligence (HLAI), including 1) a definition of HLAI, 2) an environment to develop and test HLAI, and 3) a cognitive architecture for HLAI. The term AI is used in a broad meaning, and HLAI is not clearly defined. I claim that the essence of Human-Level Intelligence to be the capability to learn from others' experiences via language. The key is that the event described by language has the same effect as if the agent experiences it firsthand for the update of the behavior policy. To develop and test models with such a capability, we are developing a simulated environment called SEDRo. There is a 3D Home, and a mother character takes care of the baby (the learning agent) and teaches languages. The environment provides comparable experiences to that of a human baby from birth to one year. Finally, I propose a cognitive architecture of HLAI called Modulated Heterarchical Prediction Memory (mHPM). In mHPM, there are three components: a universal module that learns to predict the next vector given the sequence of vector signals, a heterarchical network of those modules, and a reward-based modulation of learning. mHPM models the workings of the neocortex but the innate auxiliary units such hippocampus, reward system, instincts, and amygdala play critical roles, too. △ Less","8 August, 2021",https://arxiv.org/pdf/2108.03793
"COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging",Alexander MacLean;Saad Abbasi;Ashkan Ebadi;Andy Zhao;Maya Pavlova;Hayden Gunraj;Pengcheng Xi;Sonny Kohli;Alexander Wong,"The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of life globally, and a critical factor in mitigating its effects is screening individuals for infections, thereby allowing for both proper treatment for those individuals as well as action to be taken to prevent further spread of the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a screening tool as it is a much cheaper and easier to apply imaging modality than others that are traditionally used for pulmonary examinations, namely chest x-ray and computed tomography. Given the scarcity of expert radiologists for interpreting POCUS examinations in many highly affected regions around the world, low-cost deep learning-driven clinical decision support solutions can have a large impact during the on-going pandemic. Motivated by this, we introduce COVID-Net US, a highly efficient, self-attention deep convolutional neural network design tailored for COVID-19 screening from lung POCUS images. Experimental results show that the proposed COVID-Net US can achieve an AUC of over 0.98 while achieving 353X lower architectural complexity, 62X lower computational complexity, and 14.3X faster inference times on a Raspberry Pi. Clinical validation was also conducted, where select cases were reviewed and reported on by a practicing clinician (20 years of clinical practice) specializing in intensive care (ICU) and 15 years of expertise in POCUS interpretation. To advocate affordable healthcare and artificial intelligence for resource-constrained environments, we have made COVID-Net US open source and publicly available as part of the COVID-Net open source initiative. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.03131
Molecule Generation Experience: An Open Platform of Material Design for Public Users,Seiji Takeda;Toshiyuki Hama;Hsiang-Han Hsu;Akihiro Kishimoto;Makoto Kogoh;Takumi Hongo;Kumiko Fujieda;Hideaki Nakashika;Dmitry Zubarev;Daniel P. Sanders;Jed W. Pitera;Junta Fuchiwaki;Daiju Nakano,"Artificial Intelligence (AI)-driven material design has been attracting great attentions as a groundbreaking technology across a wide spectrum of industries. Molecular design is particularly important owing to its broad application domains and boundless creativity attributed to progresses in generative models. The recent maturity of molecular generative models has stimulated expectations for practical use among potential users, who are not necessarily familiar with coding or scripting, such as experimental engineers and students in chemical domains. However, most of the existing molecular generative models are Python libraries on GitHub, that are accessible for only IT-savvy users. To fill this gap, we newly developed a graphical user interface (GUI)-based web application of molecular generative models, Molecule Generation Experience, that is open to the general public. This is the first web application of molecular generative models enabling users to work with built-in datasets to carry out molecular design. In this paper, we describe the background technology extended from our previous work. Our new online evaluation and structural filtering algorithms significantly improved the generation speed by 30 to 1,000 times with a wider structural variety, satisfying chemical stability and synthetic reality. We also describe in detail our Kubernetes-based scalable cloud architecture and user-oriented GUI that are necessary components to achieve a public service. Finally, we present actual use cases in industrial research to design new photoacid generators (PAGs) as well as release cases in educational events. △ Less","6 August, 2021",https://arxiv.org/pdf/2108.03044
Interpretable Summaries of Black Box Incident Triaging with Subgroup Discovery,Youcef Remil;Anes Bendimerad;Marc Plantevit;Céline Robardet;Mehdi Kaytoue,"The need of predictive maintenance comes with an increasing number of incidents reported by monitoring systems and equipment/software users. In the front line, on-call engineers (OCEs) have to quickly assess the degree of severity of an incident and decide which service to contact for corrective actions. To automate these decisions, several predictive models have been proposed, but the most efficient models are opaque (say, black box), strongly limiting their adoption. In this paper, we propose an efficient black box model based on 170K incidents reported to our company over the last 7 years and emphasize on the need of automating triage when incidents are massively reported on thousands of servers running our product, an ERP. Recent developments in eXplainable Artificial Intelligence (XAI) help in providing global explanations to the model, but also, and most importantly, with local explanations for each model prediction/outcome. Sadly, providing a human with an explanation for each outcome is not conceivable when dealing with an important number of daily predictions. To address this problem, we propose an original data-mining method rooted in Subgroup Discovery, a pattern mining technique with the natural ability to group objects that share similar explanations of their black box predictions and provide a description for each group. We evaluate this approach and present our preliminary results which give us good hope towards an effective OCE's adoption. We believe that this approach provides a new way to address the problem of model agnostic outcome explanation. △ Less","6 August, 2021",https://arxiv.org/pdf/2108.03013
Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene,Qi Wu;Cheng-Ju Wu;Yixin Zhu;Jungseock Joo,"Human-robot collaboration is an essential research topic in artificial intelligence (AI), enabling researchers to devise cognitive AI systems and affords an intuitive means for users to interact with the robot. Of note, communication plays a central role. To date, prior studies in embodied agent navigation have only demonstrated that human languages facilitate communication by instructions in natural languages. Nevertheless, a plethora of other forms of communication is left unexplored. In fact, human communication originated in gestures and oftentimes is delivered through multimodal cues, e.g. ""go there"" with a pointing gesture. To bridge the gap and fill in the missing dimension of communication in embodied agent navigation, we propose investigating the effects of using gestures as the communicative interface instead of verbal cues. Specifically, we develop a VR-based 3D simulation environment, named Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human player is placed in the same virtual scene and shepherds the artificial agent using only gestures. The agent is tasked to solve the navigation problem guided by natural gestures with unknown semantics; we do not use any predefined gestures due to the diversity and versatile nature of human gestures. We argue that learning the semantics of natural gestures is mutually beneficial to learning the navigation task--learn to communicate and communicate to learn. In a series of experiments, we demonstrate that human gesture cues, even without predefined semantics, improve the object-goal navigation for an embodied agent, outperforming various state-of-the-art methods. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.02846
Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence,Jaber Rezaei Mianroodi;Shahed Rezaei;Nima H. Siboni;Bai-Xiang Xu;Dierk Raabe,"The elastic properties of materials derive from their electronic and atomic nature. However, simulating bulk materials fully at these scales is not feasible, so that typically homogenized continuum descriptions are used instead. A seamless and lossless transition of the constitutive description of the elastic response of materials between these two scales has been so far elusive. Here we show how this problem can be overcome by using Artificial Intelligence (AI). A Convolutional Neural Network (CNN) model is trained, by taking the structure image of a nanoporous material as input and the corresponding elasticity tensor, calculated from Molecular Statics (MS), as output. Trained with the atomistic data, the CNN model captures the size- and pore-dependency of the material's elastic properties which, on the physics side, can stem from surfaces and non-local effects. Such effects are often ignored in upscaling from atomistic to classical continuum theory. To demonstrate the accuracy and the efficiency of the trained CNN model, a Finite Element Method (FEM) based result of an elastically deformed nanoporous beam equipped with the CNN as constitutive law is compared with that by a full atomistic simulation. The good agreement between the atomistic simulations and the FEM-AI combination for a system with size and surface effects establishes a new lossless scale bridging approach to such problems. The trained CNN model deviates from the atomistic result by 9.6\% for porosity scenarios of up to 90\% but it is about 230 times faster than the MS calculation and does not require to change simulation methods between different scales. The efficiency of the CNN evaluation together with the preservation of important atomistic effects makes the trained model an effective atomistically-informed constitutive model for macroscopic simulations of nanoporous materials and solving of inverse problems. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.02837
Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering,E. William Webb;Peter J. H. Scott,"Artificial intelligence and machine learning are poised to disrupt PET imaging from bench to clinic. In this perspective we offer insights into how the technology could be applied to improve the design and synthesis of new radiopharmaceuticals for PET imaging, including identification of an optimal labeling approach as well as strategies for radiolabeling reaction optimization. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.02814
Using a Collated Cybersecurity Dataset for Machine Learning and Artificial Intelligence,Erik Hemberg;Una-May O'Reilly,"Artificial Intelligence (AI) and Machine Learning (ML) algorithms can support the span of indicator-level, e.g. anomaly detection, to behavioral level cyber security modeling and inference. This contribution is based on a dataset named BRON which is amalgamated from public threat and vulnerability behavioral sources. We demonstrate how BRON can support prediction of related threat techniques and attack patterns. We also discuss other AI and ML uses of BRON to exploit its behavioral knowledge. △ Less","5 August, 2021",https://arxiv.org/pdf/2108.02618
A Method for Medical Data Analysis Using the LogNNet for Clinical Decision Support Systems and Edge Computing in Healthcare,Andrei Velichko,"Edge computing is a fast-growing and much needed technology in healthcare. The problem of implementing artificial intelligence on edge devices is the complexity and high resource intensity of the most known neural network data analysis methods and algorithms. The difficulty of implementing these methods on low-power microcontrollers with small memory size calls for the development of new effective algorithms for neural networks. This study presents a new method for analyzing medical data based on the LogNNet neural network, which uses chaotic mappings to transform input information. The method effectively solves classification problems and calculates risk factors for the presence of a disease in a patient according to a set of medical health indicators. The efficiency of LogNNet in assessing perinatal risk is illustrated on cardiotocogram data obtained from the UC Irvine machine learning repository. The classification accuracy reaches ~91% with the ~3-10 kB of RAM used on the Arduino microcontroller. Using the LogNNet network trained on a publicly available database of the Israeli Ministry of Health, a service concept for COVID-19 express testing is provided. A classification accuracy of ~95% is achieved, and ~0.6 kB of RAM is used. In all examples, the model is tested using standard classification quality metrics: precision, recall, and F1-measure. The LogNNet architecture allows the implementation of artificial intelligence on medical peripherals of the Internet of Things with low RAM resources and can be used in clinical decision support systems. △ Less","24 September, 2021",https://arxiv.org/pdf/2108.02428
Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations,Xing Chen;Flavio Abreu Araujo;Mathieu Riou;Jacob Torrejon;Dafiné Ravelosona;Wang Kang;Weisheng Zhao;Julie Grollier;Damien Querlioz,"Deep learning has an increasing impact to assist research, allowing, for example, the discovery of novel materials. Until now, however, these artificial intelligence techniques have fallen short of discovering the full differential equation of an experimental physical system. Here we show that a dynamical neural network, trained on a minimal amount of data, can predict the behavior of spintronic devices with high accuracy and an extremely efficient simulation time, compared to the micromagnetic simulations that are usually employed to model them. For this purpose, we re-frame the formalism of Neural Ordinary Differential Equations (ODEs) to the constraints of spintronics: few measured outputs, multiple inputs and internal parameters. We demonstrate with Spin-Neural ODEs an acceleration factor over 200 compared to micromagnetic simulations for a complex problem -- the simulation of a reservoir computer made of magnetic skyrmions (20 minutes compared to three days). In a second realization, we show that we can predict the noisy response of experimental spintronic nano-oscillators to varying inputs after training Spin-Neural ODEs on five milliseconds of their measured response to different excitations. Spin-Neural ODE is a disruptive tool for developing spintronic applications in complement to micromagnetic simulations, which are time-consuming and cannot fit experiments when noise or imperfections are present. Spin-Neural ODE can also be generalized to other electronic devices involving dynamics. △ Less","23 July, 2021",https://arxiv.org/pdf/2108.02318
"From ""Analogue"" Science to AI-powered Digital Science",Angelina Lesnikova,"Phase transition from the human-limited, ""analogue"" way of research enquiry to the silicon-based, artificial intelligence (AI)-powered digital science is forthcoming. To facilitate this transition, I propose three project ideas: 1) CryptoScience platform, aimed to provide tools for endemic digitalization & open access of the research data, with attribution of the work credit of all involved individuals and parties using blockchain technology; 2) Computational Publication Standard, designed for publishing research findings in an AI-friendly format of knowledge graphs; 3) data modelling from publications, meant to extract key pieces of knowledge (entities and their relationships) contained in manuscript publications using natural language processing and image recognition, to enable their incorporation into the digital knowledge databases. These three ideas could be implemented separately by different research groups and institutions, however, they will strongly benefit from each other's synergistic effect. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.02265
The MIT Supercloud Dataset,Siddharth Samsi;Matthew L Weiss;David Bestor;Baolin Li;Michael Jones;Albert Reuther;Daniel Edelman;William Arcand;Chansup Byun;John Holodnack;Matthew Hubbell;Jeremy Kepner;Anna Klein;Joseph McDonald;Adam Michaleas;Peter Michaleas;Lauren Milechin;Julia Mullen;Charles Yee;Benjamin Price;Andrew Prout;Antonio Rosa;Allan Vanterpool;Lindsey McEvoy;Anson Cheng,"Artificial intelligence (AI) and Machine learning (ML) workloads are an increasingly larger share of the compute workloads in traditional High-Performance Computing (HPC) centers and commercial cloud systems. This has led to changes in deployment approaches of HPC clusters and the commercial cloud, as well as a new focus on approaches to optimized resource usage, allocations and deployment of new AI frame- works, and capabilities such as Jupyter notebooks to enable rapid prototyping and deployment. With these changes, there is a need to better understand cluster/datacenter operations with the goal of developing improved scheduling policies, identifying inefficiencies in resource utilization, energy/power consumption, failure prediction, and identifying policy violations. In this paper we introduce the MIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the analysis of large scale HPC and datacenter/cloud operations. We provide detailed monitoring logs from the MIT Supercloud system, which include CPU and GPU usage by jobs, memory usage, file system logs, and physical monitoring data. This paper discusses the details of the dataset, collection methodology, data availability, and discusses potential challenge problems being developed using this data. Datasets and future challenge announcements will be available via https://dcc.mit.edu. △ Less","4 August, 2021",https://arxiv.org/pdf/2108.02037
On the Importance of Domain-specific Explanations in AI-based Cybersecurity Systems (Technical Report),Jose N. Paredes;Juan Carlos L. Teze;Gerardo I. Simari;Maria Vanina Martinez,"With the availability of large datasets and ever-increasing computing power, there has been a growing use of data-driven artificial intelligence systems, which have shown their potential for successful application in diverse areas. However, many of these systems are not able to provide information about the rationale behind their decisions to their users. Lack of understanding of such decisions can be a major drawback, especially in critical domains such as those related to cybersecurity. In light of this problem, in this paper we make three contributions: (i) proposal and discussion of desiderata for the explanation of outputs generated by AI-based cybersecurity systems; (ii) a comparative analysis of approaches in the literature on Explainable Artificial Intelligence (XAI) under the lens of both our desiderata and further dimensions that are typically used for examining XAI approaches; and (iii) a general architecture that can serve as a roadmap for guiding research efforts towards the development of explainable AI-based cybersecurity systems -- at its core, this roadmap proposes combinations of several research lines in a novel way towards tackling the unique challenges that arise in this context. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.02006
Roadmap of Designing Cognitive Metrics for Explainable Artificial Intelligence (XAI),Janet Hui-wen Hsiao;Hilary Hei Ting Ngai;Luyu Qiu;Yi Yang;Caleb Chen Cao,"More recently, Explainable Artificial Intelligence (XAI) research has shifted to focus on a more pragmatic or naturalistic account of understanding, that is, whether the stakeholders understand the explanation. This point is especially important for research on evaluation methods for XAI systems. Thus, another direction where XAI research can benefit significantly from cognitive science and psychology research is ways to measure understanding of users, responses and attitudes. These measures can be used to quantify explanation quality and as feedback to the XAI system to improve the explanations. The current report aims to propose suitable metrics for evaluating XAI systems from the perspective of the cognitive states and processes of stakeholders. We elaborate on 7 dimensions, i.e., goodness, satisfaction, user understanding, curiosity & engagement, trust & reliance, controllability & interactivity, and learning curve & productivity, together with the recommended subjective and objective psychological measures. We then provide more details about how we can use the recommended measures to evaluate a visual classification XAI system according to the recommended cognitive metrics. △ Less","20 July, 2021",https://arxiv.org/pdf/2108.01737
SwarmPlay: Interactive Tic-tac-toe Board Game with Swarm of Nano-UAVs driven by Reinforcement Learning,Ekaterina Karmanova;Valerii Serpiva;Stepan Perminov;Aleksey Fedoseev;Dzmitry Tsetserukou,"Reinforcement learning (RL) methods have been actively applied in the field of robotics, allowing the system itself to find a solution for a task otherwise requiring a complex decision-making algorithm. In this paper, we present a novel RL-based Tic-tac-toe scenario, i.e. SwarmPlay, where each playing component is presented by an individual drone that has its own mobility and swarm intelligence to win against a human player. Thus, the combination of challenging swarm strategy and human-drone collaboration aims to make the games with machines tangible and interactive. Although some research on AI for board games already exists, e.g., chess, the SwarmPlay technology has the potential to offer much more engagement and interaction with the user as it proposes a multi-agent swarm instead of a single interactive robot. We explore user's evaluation of RL-based swarm behavior in comparison with the game theory-based behavior. The preliminary user study revealed that participants were highly engaged in the game with drones (70% put a maximum score on the Likert scale) and found it less artificial compared to the regular computer-based systems (80%). The affection of the user's game perception from its outcome was analyzed and put under discussion. User study revealed that SwarmPlay has the potential to be implemented in a wider range of games, significantly improving human-drone interactivity. △ Less","3 August, 2021",https://arxiv.org/pdf/2108.01593
The application of artificial intelligence in software engineering: a review challenging conventional wisdom,Feras A. Batarseh;Rasika Mohod;Abhinav Kumar;Justin Bui,"The field of artificial intelligence (AI) is witnessing a recent upsurge in research, tools development, and deployment of applications. Multiple software companies are shifting their focus to developing intelligent systems; and many others are deploying AI paradigms to their existing processes. In parallel, the academic research community is injecting AI paradigms to provide solutions to traditional engineering problems. Similarly, AI has evidently been proved useful to software engineering (SE). When one observes the SE phases (requirements, design, development, testing, release, and maintenance), it becomes clear that multiple AI paradigms (such as neural networks, machine learning, knowledge-based systems, natural language processing) could be applied to improve the process and eliminate many of the major challenges that the SE field has been facing. This survey chapter is a review of the most commonplace methods of AI applied to SE. The review covers methods between years 1975-2017, for the requirements phase, 46 major AI-driven methods are found, 19 for design, 15 for development, 68 for testing, and 15 for release and maintenance. Furthermore, the purpose of this chapter is threefold; firstly, to answer the following questions: is there sufficient intelligence in the SE lifecycle? What does applying AI to SE entail? Secondly, to measure, formulize, and evaluate the overlap of SE phases and AI disciplines. Lastly, this chapter aims to provide serious questions to challenging the current conventional wisdom (i.e., status quo) of the state-of-the-art, craft a call for action, and to redefine the path forward. △ Less","3 August, 2021",https://arxiv.org/pdf/2108.01591
Efficacy of Statistical and Artificial Intelligence-based False Information Cyberattack Detection Models for Connected Vehicles,Sakib Mahmud Khan;Gurcan Comert;Mashrur Chowdhury,"Connected vehicles (CVs), because of the external connectivity with other CVs and connected infrastructure, are vulnerable to cyberattacks that can instantly compromise the safety of the vehicle itself and other connected vehicles and roadway infrastructure. One such cyberattack is the false information attack, where an external attacker injects inaccurate information into the connected vehicles and eventually can cause catastrophic consequences by compromising safety-critical applications like the forward collision warning. The occurrence and target of such attack events can be very dynamic, making real-time and near-real-time detection challenging. Change point models, can be used for real-time anomaly detection caused by the false information attack. In this paper, we have evaluated three change point-based statistical models; Expectation Maximization, Cumulative Summation, and Bayesian Online Change Point Algorithms for cyberattack detection in the CV data. Also, data-driven artificial intelligence (AI) models, which can be used to detect known and unknown underlying patterns in the dataset, have the potential of detecting a real-time anomaly in the CV data. We have used six AI models to detect false information attacks and compared the performance for detecting the attacks with our developed change point models. Our study shows that change points models performed better in real-time false information attack detection compared to the performance of the AI models. Change point models having the advantage of no training requirements can be a feasible and computationally efficient alternative to AI models for false information attack detection in connected vehicles. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.01124
Time-based Dynamic Controllability of Disjunctive Temporal Networks with Uncertainty: A Tree Search Approach with Graph Neural Network Guidance,Kevin Osanlou;Jeremy Frank;J. Benton;Andrei Bursuc;Christophe Guettier;Eric Jacopin;Tristan Cazenave,"Scheduling in the presence of uncertainty is an area of interest in artificial intelligence due to the large number of applications. We study the problem of dynamic controllability (DC) of disjunctive temporal networks with uncertainty (DTNU), which seeks a strategy to satisfy all constraints in response to uncontrollable action durations. We introduce a more restricted, stronger form of controllability than DC for DTNUs, time-based dynamic controllability (TDC), and present a tree search approach to determine whether or not a DTNU is TDC. Moreover, we leverage the learning capability of a message passing neural network (MPNN) as a heuristic for tree search guidance. Finally, we conduct experiments for which the tree search shows superior results to state-of-the-art timed-game automata (TGA) based approaches. We observe that using an MPNN for tree search guidance leads to a significant increase in solving performance and scalability to harder DTNU problems. △ Less","2 August, 2021",https://arxiv.org/pdf/2108.01068
AI Techniques for Software Requirements Prioritization,Alexander Felfernig,"Aspects such as limited resources, frequently changing market demands, and different technical restrictions regarding the implementation of software requirements (features) often demand for the prioritization of requirements. The task of prioritization is the ranking and selection of requirements that should be included in future software releases. In this context, an intelligent prioritization decision support is extremely important. The prioritization approaches discussed in this paper are based on different Artificial Intelligence (AI) techniques that can help to improve the overall quality of requirements prioritization processes △ Less","2 August, 2021",https://arxiv.org/pdf/2108.00832
SwarmPlay: A Swarm of Nano-Quadcopters Playing Tic-tac-toe Board Game against a Human,Ekaterina Karmanova;Valerii Serpiva;Stepan Perminov;Roman Ibrahimov;Aleksey Fedoseev;Dzmitry Tsetserukou,"We present a new paradigm of games, i.e. SwarmPlay, where each playing component is presented by an individual drone that has its own mobility and swarm intelligence to win against a human player. The motivation behind the research is to make the games with machines tangible and interactive. Although some research on the robotic players for board games already exists, e.g., chess, the SwarmPlay technology has the potential to offer much more engagement and interaction with a human as it proposes a multi-agent swarm instead of a single interactive robot. The proposed system consists of a robotic swarm, a workstation, a computer vision (CV), and Game Theory-based algorithms. A novel game algorithm was developed to provide a natural game experience to the user. The preliminary user study revealed that participants were highly engaged in the game with drones (69% put a maximum score on the Likert scale) and found it less artificial compared to the regular computer-based systems (77% put maximum score). The affection of the user's game perception from its outcome was analyzed and put under discussion. User study revealed that SwarmPlay has the potential to be implemented in a wider range of games, significantly improving human-drone interactivity. △ Less","1 August, 2021",https://arxiv.org/pdf/2108.00488
A Survey on Audio Synthesis and Audio-Visual Multimodal Processing,Zhaofeng Shi,"With the development of deep learning and artificial intelligence, audio synthesis has a pivotal role in the area of machine learning and shows strong applicability in the industry. Meanwhile, significant efforts have been dedicated by researchers to handle multimodal tasks at present such as audio-visual multimodal processing. In this paper, we conduct a survey on audio synthesis and audio-visual multimodal processing, which helps understand current research and future trends. This review focuses on text to speech(TTS), music generation and some tasks that combine visual and acoustic information. The corresponding technical methods are comprehensively classified and introduced, and their future development trends are prospected. This survey can provide some guidance for researchers who are interested in the areas like audio synthesis and audio-visual multimodal processing. △ Less","1 August, 2021",https://arxiv.org/pdf/2108.00443
Automated Pest Detection with DNN on the Edge for Precision Agriculture,Andrea Albanese;Matteo Nardello;Davide Brunelli,"Artificial intelligence has smoothly penetrated several economic activities, especially monitoring and control applications, including the agriculture sector. However, research efforts toward low-power sensing devices with fully functional machine learning (ML) on-board are still fragmented and limited in smart farming. Biotic stress is one of the primary causes of crop yield reduction. With the development of deep learning in computer vision technology, autonomous detection of pest infestation through images has become an important research direction for timely crop disease diagnosis. This paper presents an embedded system enhanced with ML functionalities, ensuring continuous detection of pest infestation inside fruit orchards. The embedded solution is based on a low-power embedded sensing system along with a Neural Accelerator able to capture and process images inside common pheromone-based traps. Three different ML algorithms have been trained and deployed, highlighting the capabilities of the platform. Moreover, the proposed approach guarantees an extended battery life thanks to the integration of energy harvesting functionalities. Results show how it is possible to automate the task of pest infestation for unlimited time without the farmer's intervention. △ Less","1 August, 2021",https://arxiv.org/pdf/2108.00421
Citations or dollars? Early signals of a firm's research success,Shuqi Xu;Manuel S. Mariani;Linyuan Lü;Lorenzo Napolitano;Emanuele Pugliese;Andrea Zaccaria,"Scientific and technological progress is largely driven by firms in many domains, including artificial intelligence and vaccine development. However, we do not know yet whether the success of firms' research activities exhibits dynamic regularities and some degree of predictability. By inspecting the research lifecycles of 7,440 firms, we find that the economic value of a firm's early patents is an accurate predictor of various dimensions of a firm's future research success. At the same time, a smaller set of future top-performers do not generate early patents of high economic value, but they are detectable via the technological value of their early patents. Importantly, the observed predictability cannot be explained by a cumulative advantage mechanism, and the observed heterogeneity of the firms' temporal success patterns markedly differs from patterns previously observed for individuals' research careers. Our results uncover the dynamical regularities of the research success of firms, and they could inform managerial strategies as well as policies to promote entrepreneurship and accelerate human progress. △ Less","31 July, 2021",https://arxiv.org/pdf/2108.00200
Secure solutions for Smart City Command Control Centre using AIOT,Balachandar. S;Chinnaiyan. R,"To build a robust secure solution for smart city IOT network from any Cyber attacks using Artificial Intelligence. In Smart City IOT network, data collected from different log collectors or direct sources from cloud or edge should harness the potential of AI. The smart city command and control center team will leverage these models and deploy it in different city IOT network to help on intrusion prediction, network packet surge, potential botnet attacks from external network. Some of the vital use cases considered based on the users of command-and-control center △ Less","29 July, 2021",https://arxiv.org/pdf/2108.00003
"Seeing poverty from space, how much can it be tuned?",Tomas Sako;Arturo Jr M. Martinez,"Since the United Nations launched the Sustainable Development Goals (SDG) in 2015, numerous universities, NGOs and other organizations have attempted to develop tools for monitoring worldwide progress in achieving them. Led by advancements in the fields of earth observation techniques, data sciences and the emergence of artificial intelligence, a number of research teams have developed innovative tools for highlighting areas of vulnerability and tracking the implementation of SDG targets. In this paper we demonstrate that individuals with no organizational affiliation and equipped only with common hardware, publicly available datasets and cloud-based computing services can participate in the improvement of predicting machine-learning-based approaches to predicting local poverty levels in a given agro-ecological environment. The approach builds upon several pioneering efforts over the last five years related to mapping poverty by deep learning to process satellite imagery and ""ground-truth"" data from the field to link features with incidence of poverty in a particular context. The approach employs new methods for object identification in order to optimize the modeled results and achieve significantly high accuracy. A key goal of the project was to intentionally keep costs as low as possible - by using freely available resources - so that citizen scientists, students and organizations could replicate the method in other areas of interest. Moreover, for simplicity, the input data used were derived from just a handful of sources (involving only earth observation and population headcounts). The results of the project could therefore certainly be strengthened further through the integration of proprietary data from social networks, mobile phone providers, and other sources. △ Less","30 July, 2021",https://arxiv.org/pdf/2107.14700
Digital Twin As A Cost Reduction Method,Suleyman Yukcu;Omer Aydin,"Many fields have been affected by the introduction of concepts such as sensors, industry 4.0, internet of things, machine learning and artificial intelligence in recent years. As a result of the interaction of cyber physical systems with these concepts, digital twin model has emerged. The concept of digital twin has been used in many areas with its emergence. The use of this model has made significant gains, especially in decision making processes. The gains in decision making processes contribute to every field and cause changes in terms of cost. In this study, the historical development of the concept of digital twin has been mentioned and general information about the usage areas of digital twin has been given. In the light of this information, the cost effect of the digital twin model, therefore its appearance from the cost accounting window and its use as a cost reduction method were evaluated. This study was carried out in order to shed light on the studies with the insufficient resources in the Turkish literature and the cost accounting perspective. △ Less","10 July, 2021",https://arxiv.org/pdf/2107.14109
"The ghost of AI governance past, present and future: AI governance in the European Union",Charlotte Stix,"The received wisdom is that artificial intelligence (AI) is a competition between the US and China. In this chapter, the author will examine how the European Union (EU) fits into that mix and what it can offer as a third way to govern AI. The chapter presents this by exploring the past, present and future of AI governance in the EU. Section 1 serves to explore and evidence the EUs coherent and comprehensive approach to AI governance. In short, the EU ensures and encourages ethical, trustworthy and reliable technological development. This will cover a range of key documents and policy tools that lead to the most crucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive towards digital sovereignty through the lens of regulation and infrastructure. This covers topics such as the trustworthiness of AI systems, cloud, compute and foreign direct investment. In Section 3, the chapter concludes by offering several considerations to achieve good AI governance in the EU. △ Less","8 July, 2021",https://arxiv.org/pdf/2107.14099
Audit and Assurance of AI Algorithms: A framework to ensure ethical algorithmic practices in Artificial Intelligence,Ramya Akula;Ivan Garibay,"Algorithms are becoming more widely used in business, and businesses are becoming increasingly concerned that their algorithms will cause significant reputational or financial damage. We should emphasize that any of these damages stem from situations in which the United States lacks strict legislative prohibitions or specified protocols for measuring damages. As a result, governments are enacting legislation and enforcing prohibitions, regulators are fining businesses, and the judiciary is debating whether or not to make artificially intelligent computer models as the decision-makers in the eyes of the law. From autonomous vehicles and banking to medical care, housing, and legal decisions, there will soon be enormous amounts of algorithms that make decisions with limited human interference. Governments, businesses, and society would have an algorithm audit, which would have systematic verification that algorithms are lawful, ethical, and secure, similar to financial audits. A modern market, auditing, and assurance of algorithms developed to professionalize and industrialize AI, machine learning, and related algorithms. Stakeholders of this emerging field include policymakers and regulators, along with industry experts and entrepreneurs. In addition, we foresee audit thresholds and frameworks providing valuable information to all who are concerned with governance and standardization. This paper aims to review the critical areas required for auditing and assurance and spark discussion in this novel field of study and practice. △ Less","14 July, 2021",https://arxiv.org/pdf/2107.14046
The brain is a computer is a brain: neuroscience's internal debate and the social significance of the Computational Metaphor,Alexis T. Baria;Keith Cross,"The Computational Metaphor, comparing the brain to the computer and vice versa, is the most prominent metaphor in neuroscience and artificial intelligence (AI). Its appropriateness is highly debated in both fields, particularly with regards to whether it is useful for the advancement of science and technology. Considerably less attention, however, has been devoted to how the Computational Metaphor is used outside of the lab, and particularly how it may shape society's interactions with AI. As such, recently publicized concerns over AI's role in perpetuating racism, genderism, and ableism suggest that the term ""artificial intelligence"" is misplaced, and that a new lexicon is needed to describe these computational systems. Thus, there is an essential question about the Computational Metaphor that is rarely asked by neuroscientists: whom does it help and whom does it harm? This essay invites the neuroscience community to consider the social implications of the field's most controversial metaphor. △ Less","18 July, 2021",https://arxiv.org/pdf/2107.14042
Artificial Intelligence (AI) and Big Data for Coronavirus (COVID-19) Pandemic: A Survey on the State-of-the-Arts,Quoc-Viet Pham;Dinh C. Nguyen;Thien Huynh-The;Won-Joo Hwang;Pubudu N Pathirana,"The very first infected novel coronavirus case (COVID-19) was found in Hubei, China in Dec. 2019. The COVID-19 pandemic has spread over 214 countries and areas in the world, and has significantly affected every aspect of our daily lives. At the time of writing this article, the numbers of infected cases and deaths still increase significantly and have no sign of a well-controlled situation, e.g., as of 13 July 2020, from a total number of around 13.1 million positive cases, 571, 527 deaths were reported in the world. Motivated by recent advances and applications of artificial intelligence (AI) and big data in various areas, this paper aims at emphasizing their importance in responding to the COVID-19 outbreak and preventing the severe effects of the COVID-19 pandemic. We firstly present an overview of AI and big data, then identify the applications aimed at fighting against COVID-19, next highlight challenges and issues associated with state-of-the-art solutions, and finally come up with recommendations for the communications to effectively control the COVID-19 situation. It is expected that this paper provides researchers and communities with new insights into the ways AI and big data improve the COVID-19 situation, and drives further studies in stopping the COVID-19 outbreak. △ Less","17 July, 2021",https://arxiv.org/pdf/2107.14040
A Checklist for Explainable AI in the Insurance Domain,Olivier Koster;Ruud Kosman;Joost Visser,"Artificial intelligence (AI) is a powerful tool to accomplish a great many tasks. This exciting branch of technology is being adopted increasingly across varying sectors, including the insurance domain. With that power arise several complications. One of which is a lack of transparency and explainability of an algorithm for experts and non-experts alike. This brings into question both the usefulness as well as the accuracy of the algorithm, coupled with an added difficulty to assess potential biases within the data or the model. In this paper, we investigate the current usage of AI algorithms in the Dutch insurance industry and the adoption of explainable artificial intelligence (XAI) techniques. Armed with this knowledge we design a checklist for insurance companies that should help assure quality standards regarding XAI and a solid foundation for cooperation between organisations. This checklist extends an existing checklist of SIVI, the standardisation institute for digital cooperation and innovation in Dutch insurance. △ Less","18 July, 2021",https://arxiv.org/pdf/2107.14039
Resisting Out-of-Distribution Data Problem in Perturbation of XAI,Luyu Qiu;Yi Yang;Caleb Chen Cao;Jing Liu;Yueyuan Zheng;Hilary Hei Ting Ngai;Janet Hsiao;Lei Chen,"With the rapid development of eXplainable Artificial Intelligence (XAI), perturbation-based XAI algorithms have become quite popular due to their effectiveness and ease of implementation. The vast majority of perturbation-based XAI techniques face the challenge of Out-of-Distribution (OoD) data -- an artifact of randomly perturbed data becoming inconsistent with the original dataset. OoD data leads to the over-confidence problem in model predictions, making the existing XAI approaches unreliable. To our best knowledge, the OoD data problem in perturbation-based XAI algorithms has not been adequately addressed in the literature. In this work, we address this OoD data problem by designing an additional module quantifying the affinity between the perturbed data and the original dataset distribution, which is integrated into the process of aggregation. Our solution is shown to be compatible with the most popular perturbation-based XAI algorithms, such as RISE, OCCLUSION, and LIME. Experiments have confirmed that our methods demonstrate a significant improvement in general cases using both computational and cognitive metrics. Especially in the case of degradation, our proposed approach demonstrates outstanding performance comparing to baselines. Besides, our solution also resolves a fundamental problem with the faithfulness indicator, a commonly used evaluation metric of XAI algorithms that appears to be sensitive to the OoD issue. △ Less","27 July, 2021",https://arxiv.org/pdf/2107.14000
Video Generation from Text Employing Latent Path Construction for Temporal Modeling,Amir Mazaheri;Mubarak Shah,"Video generation is one of the most challenging tasks in Machine Learning and Computer Vision fields of study. In this paper, we tackle the text to video generation problem, which is a conditional form of video generation. Humans can listen/read natural language sentences, and can imagine or visualize what is being described; therefore, we believe that video generation from natural language sentences will have an important impact on Artificial Intelligence. Video generation is relatively a new field of study in Computer Vision, which is far from being solved. The majority of recent works deal with synthetic datasets or real datasets with very limited types of objects, scenes, and emotions. To the best of our knowledge, this is the very first work on the text (free-form sentences) to video generation on more realistic video datasets like Actor and Action Dataset (A2D) or UCF101. We tackle the complicated problem of video generation by regressing the latent representations of the first and last frames and employing a context-aware interpolation method to build the latent representations of in-between frames. We propose a stacking ``upPooling'' block to sequentially generate RGB frames out of each latent representation and progressively increase the resolution. Moreover, our proposed Discriminator encodes videos based on single and multiple frames. We provide quantitative and qualitative results to support our arguments and show the superiority of our method over well-known baselines like Recurrent Neural Network (RNN) and Deconvolution (as known as Convolutional Transpose) based video generation methods. △ Less","29 July, 2021",https://arxiv.org/pdf/2107.13766
An Ethical Framework for Guiding the Development of Affectively-Aware Artificial Intelligence,Desmond C. Ong,"The recent rapid advancements in artificial intelligence research and deployment have sparked more discussion about the potential ramifications of socially- and emotionally-intelligent AI. The question is not if research can produce such affectively-aware AI, but when it will. What will it mean for society when machines -- and the corporations and governments they serve -- can ""read"" people's minds and emotions? What should developers and operators of such AI do, and what should they not do? The goal of this article is to pre-empt some of the potential implications of these developments, and propose a set of guidelines for evaluating the (moral and) ethical consequences of affectively-aware AI, in order to guide researchers, industry professionals, and policy-makers. We propose a multi-stakeholder analysis framework that separates the ethical responsibilities of AI Developers vis-à-vis the entities that deploy such AI -- which we term Operators. Our analysis produces two pillars that clarify the responsibilities of each of these stakeholders: Provable Beneficence, which rests on proving the effectiveness of the AI, and Responsible Stewardship, which governs responsible collection, use, and storage of data and the decisions made from such data. We end with recommendations for researchers, developers, operators, as well as regulators and law-makers. △ Less","28 July, 2021",https://arxiv.org/pdf/2107.13734
Toward Integrated Human-machine Intelligence for Civil Engineering: An Interdisciplinary Perspective,Cheng Zhang;Jinwoo Kim;JungHo Jeon;Jinding Xing;Changbum Ahn;Pingbo Tang;Hubo Cai,"The purpose of this paper is to examine the opportunities and barriers of Integrated Human-Machine Intelligence (IHMI) in civil engineering. Integrating artificial intelligence's high efficiency and repeatability with humans' adaptability in various contexts can advance timely and reliable decision-making during civil engineering projects and emergencies. Successful cases in other domains, such as biomedical science, healthcare, and transportation, showed the potential of IHMI in data-driven, knowledge-based decision-making in numerous civil engineering applications. However, whether the industry and academia are ready to embrace the era of IHMI and maximize its benefit to the industry is still questionable due to several knowledge gaps. This paper thus calls for future studies in exploring the value, method, and challenges of applying IHMI in civil engineering. Our systematic review of the literature and motivating cases has identified four knowledge gaps in achieving effective IHMI in civil engineering. First, it is unknown what types of tasks in the civil engineering domain can be assisted by AI and to what extent. Second, the interface between human and AI in civil engineering-related tasks need more precise and formal definition. Third, the barriers that impede collecting detailed behavioral data from humans and contextual environments deserve systematic classification and prototyping. Lastly, it is unknown what expected and unexpected impacts will IHMI have on the AEC industry and entrepreneurship. Analyzing these knowledge gaps led to a list of identified research questions. This paper will lay the foundation for identifying relevant studies to form a research roadmap to address the four knowledge gaps identified. △ Less","28 July, 2021",https://arxiv.org/pdf/2107.13498
A Proof-of-Concept Study of Artificial Intelligence Assisted Contour Revision,Ti Bai;Anjali Balagopal;Michael Dohopolski;Howard E. Morgan;Rafe McBeth;Jun Tan;Mu-Han Lin;David J. Sher;Dan Nguyen;Steve Jiang,"Automatic segmentation of anatomical structures is critical for many medical applications. However, the results are not always clinically acceptable and require tedious manual revision. Here, we present a novel concept called artificial intelligence assisted contour revision (AIACR) and demonstrate its feasibility. The proposed clinical workflow of AIACR is as follows given an initial contour that requires a clinicians revision, the clinician indicates where a large revision is needed, and a trained deep learning (DL) model takes this input to update the contour. This process repeats until a clinically acceptable contour is achieved. The DL model is designed to minimize the clinicians input at each iteration and to minimize the number of iterations needed to reach acceptance. In this proof-of-concept study, we demonstrated the concept on 2D axial images of three head-and-neck cancer datasets, with the clinicians input at each iteration being one mouse click on the desired location of the contour segment. The performance of the model is quantified with Dice Similarity Coefficient (DSC) and 95th percentile of Hausdorff Distance (HD95). The average DSC/HD95 (mm) of the auto-generated initial contours were 0.82/4.3, 0.73/5.6 and 0.67/11.4 for three datasets, which were improved to 0.91/2.1, 0.86/2.4 and 0.86/4.7 with three mouse clicks, respectively. Each DL-based contour update requires around 20 ms. We proposed a novel AIACR concept that uses DL models to assist clinicians in revising contours in an efficient and effective way, and we demonstrated its feasibility by using 2D axial CT images from three head-and-neck cancer datasets. △ Less","28 July, 2021",https://arxiv.org/pdf/2107.13465
Artificial Intelligence in Healthcare: Lost In Translation?,Vince I. Madai;David C. Higgins,"Artificial intelligence (AI) in healthcare is a potentially revolutionary tool to achieve improved healthcare outcomes while reducing overall health costs. While many exploratory results hit the headlines in recent years there are only few certified and even fewer clinically validated products available in the clinical setting. This is a clear indication of failing translation due to shortcomings of the current approach to AI in healthcare. In this work, we highlight the major areas, where we observe current challenges for translation in AI in healthcare, namely precision medicine, reproducible science, data issues and algorithms, causality, and product development. For each field, we outline possible solutions for these challenges. Our work will lead to improved translation of AI in healthcare products into the clinical setting △ Less","28 July, 2021",https://arxiv.org/pdf/2107.13454
"Meaning Versus Information, Prediction Versus Memory, and Question Versus Answer",Yoonsuck Choe,"Brain science and artificial intelligence have made great progress toward the understanding and engineering of the human mind. The progress has accelerated significantly since the turn of the century thanks to new methods for probing the brain (both structure and function), and rapid development in deep learning research. However, despite these new developments, there are still many open questions, such as how to understand the brain at the system level, and various robustness issues and limitations of deep learning. In this informal essay, I will talk about some of the concepts that are central to brain science and artificial intelligence, such as information and memory, and discuss how a different view on these concepts can help us move forward, beyond current limits of our understanding in these fields. △ Less","29 June, 2021",https://arxiv.org/pdf/2107.13393
Experimenting with Self-Supervision using Rotation Prediction for Image Captioning,Ahmed Elhagry;Karima Kadaoui,"Image captioning is a task in the field of Artificial Intelligence that merges between computer vision and natural language processing. It is responsible for generating legends that describe images, and has various applications like descriptions used by assistive technology or indexing images (for search engines for instance). This makes it a crucial topic in AI that is undergoing a lot of research. This task however, like many others, is trained on large images labeled via human annotation, which can be very cumbersome: it needs manual effort, both financial and temporal costs, it is error-prone and potentially difficult to execute in some cases (e.g. medical images). To mitigate the need for labels, we attempt to use self-supervised learning, a type of learning where models use the data contained within the images themselves as labels. It is challenging to accomplish though, since the task is two-fold: the images and captions come from two different modalities and usually handled by different types of networks. It is thus not obvious what a completely self-supervised solution would look like. How it would achieve captioning in a comparable way to how self-supervision is applied today on image recognition tasks is still an ongoing research topic. In this project, we are using an encoder-decoder architecture where the encoder is a convolutional neural network (CNN) trained on OpenImages dataset and learns image features in a self-supervised fashion using the rotation pretext task. The decoder is a Long Short-Term Memory (LSTM), and it is trained, along within the image captioning model, on MS COCO dataset and is responsible of generating captions. Our GitHub repository can be found: https://github.com/elhagry1/SSL_ImageCaptioning_RotationPrediction △ Less","27 July, 2021",https://arxiv.org/pdf/2107.13111
Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI,ennifer Chubba;Sondess Missaouib;Shauna Concannonc;Liam Maloneyb;James Alfred Walker,"Conversational Artificial Intelligence (CAI) systems and Intelligent Personal Assistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming ubiquitous in our lives, including those of children, the implications of which is receiving increased attention, specifically with respect to the effects of these systems on children's cognitive, social and linguistic development. Recent advances address the implications of CAI with respect to privacy, safety, security, and access. However, there is a need to connect and embed the ethical and technical aspects in the design. Using a case-study of a research and development project focused on the use of CAI in storytelling for children, this paper reflects on the social context within a specific case of technology development, as substantiated and supported by argumentation from within the literature. It describes the decision making process behind the recommendations made on this case for their adoption in the creative industries. Further research that engages with developers and stakeholders in the ethics of storytelling through CAI is highlighted as a matter of urgency. △ Less","20 July, 2021",https://arxiv.org/pdf/2107.13076
Neuromorphic scaling advantages for energy-efficient random walk computation,J. Darby Smith;Aaron J. Hill;Leah E. Reeder;Brian C. Franke;Richard B. Lehoucq;Ojas Parekh;William Severa;James B. Aimone,"Computing stands to be radically improved by neuromorphic computing (NMC) approaches inspired by the brain's incredible efficiency and capabilities. Most NMC research, which aims to replicate the brain's computational structure and architecture in man-made hardware, has focused on artificial intelligence; however, less explored is whether this brain-inspired hardware can provide value beyond cognitive tasks. We demonstrate that high-degree parallelism and configurability of spiking neuromorphic architectures makes them well-suited to implement random walks via discrete time Markov chains. Such random walks are useful in Monte Carlo methods, which represent a fundamental computational tool for solving a wide range of numerical computing tasks. Additionally, we show how the mathematical basis for a probabilistic solution involving a class of stochastic differential equations can leverage those simulations to provide solutions for a range of broadly applicable computational tasks. Despite being in an early development stage, we find that NMC platforms, at a sufficient scale, can drastically reduce the energy demands of high-performance computing (HPC) platforms. △ Less","27 July, 2021",https://arxiv.org/pdf/2107.13057
The social dilemma in artificial intelligence development and why we have to solve it,Inga Strümke;Marija Slavkovik;Vince I. Madai,"While the demand for ethical artificial intelligence (AI) systems increases, the number of unethical uses of AI accelerates, even though there is no shortage of ethical guidelines. We argue that a possible underlying cause for this is that AI developers face a social dilemma in AI development ethics, preventing the widespread adaptation of ethical best practices. We define the social dilemma for AI development and describe why the current crisis in AI development ethics cannot be solved without relieving AI developers of their social dilemma. We argue that AI development must be professionalised to overcome the social dilemma, and discuss how medicine can be used as a template in this process. △ Less","17 November, 2021",https://arxiv.org/pdf/2107.12977
Edge service resource allocation strategy based on intelligent prediction,Yujie Wamg;Xin Du;Xuzhao Chen;Zhihui Lu,"Artificial intelligence is one of the important technologies for industrial applications, but it requires a lot of computing resources and sensor data to support it. With the development of edge computing and the Internet of Things, artificial intelligence are playing an increasingly important role in the field of edge services. Therefore, how to make intelligent algorithms provide better services and the development of the Internet of Things has become an increasingly important topic. This paper focuses on the application of edge service distribution strategy, and proposes an edge service distribution strategy based on intelligent prediction, which reduces the bandwidth consumption of edge service providers and minimizes the cost of edge service providers. In addition, this article uses the real data provided by the Wangsu Technology Company and an improved long and short term memory prediction method to dynamically change the bandwidth, and achieves better optimization of resources allocation comparing with actual industrial applications.The simulation results show that our intelligent prediction can achieve good results, and the mechanism can achieve higher resource utilization. △ Less","27 July, 2021",https://arxiv.org/pdf/2107.12740
Decision Making Using Rough Set based Spanning Sets for a Decision System,Nidhika Yadav,"Rough Set based concepts of Span and Spanning Sets were recently proposed to deal with uncertainties in data. Here, this paper, presents novel concepts for generic decision-making process using Rough Set based span for a decision table. Majority of problems in Artificial Intelligence deal with decision making. This paper provides real life applications of proposed Rough Set based span for decision tables. Here, novel concept of span for a decision table is proposed, illustrated with real life example of flood relief and rescue team assignment. Its uses, applications and properties are explored. The key contribution of paper is primarily to study decision making using Rough Set based Span for a decision tables, as against an information system in prior works. Here, the main contribution is that decision classes are automatically learned by the technique of Rough Set based span, for a particular problem, hence automating the decision-making process. These decision-making tools based on span can guide an expert in taking decisions in tough and time-bound situations. △ Less","21 July, 2021",https://arxiv.org/pdf/2107.12477
Systematic Literature Review of Validation Methods for AI Systems,Lalli Myllyaho;Mikko Raatikainen;Tomi Männistö;Tommi Mikkonen;Jukka K. Nurminen,"Context: Artificial intelligence (AI) has made its way into everyday activities, particularly through new techniques such as machine learning (ML). These techniques are implementable with little domain knowledge. This, combined with the difficulty of testing AI systems with traditional methods, has made system trustworthiness a pressing issue. Objective: This paper studies the methods used to validate practical AI systems reported in the literature. Our goal is to classify and describe the methods that are used in realistic settings to ensure the dependability of AI systems. Method: A systematic literature review resulted in 90 papers. Systems presented in the papers were analysed based on their domain, task, complexity, and applied validation methods. Results: The validation methods were synthesized into a taxonomy consisting of trial, simulation, model-centred validation, and expert opinion. Failure monitors, safety channels, redundancy, voting, and input and output restrictions are methods used to continuously validate the systems after deployment. Conclusions: Our results clarify existing strategies applied to validation. They form a basis for the synthesization, assessment, and refinement of AI system validation in research and guidelines for validating individual systems in practice. While various validation strategies have all been relatively widely applied, only few studies report on continuous validation. Keywords: artificial intelligence, machine learning, validation, testing, V&V, systematic literature review. △ Less","26 July, 2021",https://arxiv.org/pdf/2107.12190
Measuring Ethics in AI with AI: A Methodology and Dataset Construction,Pedro H. C. Avelar;Rafael B. Audibert;Anderson R. Tavares;Luís C. Lamb,"Recently, the use of sound measures and metrics in Artificial Intelligence has become the subject of interest of academia, government, and industry. Efforts towards measuring different phenomena have gained traction in the AI community, as illustrated by the publication of several influential field reports and policy documents. These metrics are designed to help decision takers to inform themselves about the fast-moving and impacting influences of key advances in Artificial Intelligence in general and Machine Learning in particular. In this paper we propose to use such newfound capabilities of AI technologies to augment our AI measuring capabilities. We do so by training a model to classify publications related to ethical issues and concerns. In our methodology we use an expert, manually curated dataset as the training set and then evaluate a large set of research papers. Finally, we highlight the implications of AI metrics, in particular their contribution towards developing trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI Fairness; AI Measurement. Ethics in Computer Science. △ Less","20 September, 2021",https://arxiv.org/pdf/2107.11913
Deep Learning-based Frozen Section to FFPE Translation,Kutsev Bengisu Ozyoruk;Sermet Can;Guliz Irem Gokceler;Kayhan Basak;Derya Demir;Gurdeniz Serin;Uguray Payam Hacisalihoglu;Emirhan Kurtuluş;Berkan Darbaz;Ming Y. Lu;Tiffany Y. Chen;Drew F. K. Williamson;Funda Yilmaz;Faisal Mahmood;Mehmet Turan,"Frozen sectioning (FS) is the preparation method of choice for microscopic evaluation of tissues during surgical operations. The high speed of the procedure allows pathologists to rapidly assess the key microscopic features, such as tumour margins and malignant status to guide surgical decision-making and minimise disruptions to the course of the operation. However, FS is prone to introducing many misleading artificial structures (histological artefacts), such as nuclear ice crystals, compression, and cutting artefacts, hindering timely and accurate diagnostic judgement of the pathologist. Additional training and prolonged experience is often required to make highly effective and time-critical diagnosis on frozen sections. On the other hand, the gold standard tissue preparation technique of formalin-fixation and paraffin-embedding (FFPE) provides significantly superior image quality, but is a very time-consuming process (12-48 hours), making it unsuitable for intra-operative use. In this paper, we propose an artificial intelligence (AI) method that improves FS image quality by computationally transforming frozen-sectioned whole-slide images (FS-WSIs) into whole-slide FFPE-style images in minutes. AI-FFPE rectifies FS artefacts with the guidance of an attention mechanism that puts a particular emphasis on artefacts while utilising a self-regularization mechanism established between FS input image and synthesized FFPE-style image that preserves clinically relevant features. As a result, AI-FFPE method successfully generates FFPE-style images without significantly extending tissue processing time and consequently improves diagnostic accuracy. We demonstrate the efficacy of AI-FFPE on lung and brain frozen sections using a variety of different qualitative and quantitative metrics including visual Turing tests from 20 board certified pathologists. △ Less","2 November, 2021",https://arxiv.org/pdf/2107.11786
Caveats for the use of Web of Science Core Collection in old literature retrieval and historical bibliometric analysis,Weishu Liu,"By using publications from Web of Science Core Collection (WoSCC), Fosso Wamba and his colleagues published an interesting and comprehensive paper in Technological Forecasting and Social Change to explore the structure and dynamics of artificial intelligence (AI) scholarship. Data demonstrated in Fosso Wamba's study implied that the year 1991 seemed to be a ""watershed"" of AI research. This research note tried to uncover the 1991 phenomenon from the perspective of database limitation by probing the limitations of search in abstract/author keywords/keywords plus fields of WoSCC empirically. The low availability rates of abstract/author keywords/keywords plus information in WoSCC found in this study can explain the ""watershed"" phenomenon of AI scholarship in 1991 to a large extent. Some other caveats for the use of WoSCC in old literature retrieval and historical bibliometric analysis were also mentioned in the discussion section. This research note complements Fosso Wamba and his colleagues' study and also helps avoid improper interpretation in the use of WoSCC in old literature retrieval and historical bibliometric analysis. △ Less","23 July, 2021",https://arxiv.org/pdf/2107.11521
Using NLP to analyze whether customer statements comply with their inner belief,Fabian Thaler;Stefan Faußer;Heiko Gewald,"Customers' emotions play a vital role in the service industry. The better frontline personnel understand the customer, the better the service they can provide. As human emotions generate certain (unintentional) bodily reactions, such as increase in heart rate, sweating, dilation, blushing and paling, which are measurable, artificial intelligence (AI) technologies can interpret these signals. Great progress has been made in recent years to automatically detect basic emotions like joy, anger etc. Complex emotions, consisting of multiple interdependent basic emotions, are more difficult to identify. One complex emotion which is of great interest to the service industry is difficult to detect: whether a customer is telling the truth or just a story. This research presents an AI-method for capturing and sensing emotional data. With an accuracy of around 98 %, the best trained model was able to detect whether a participant of a debating challenge was arguing for or against her/his conviction, using speech analysis. The data set was collected in an experimental setting with 40 participants. The findings are applicable to a wide range of service processes and specifically useful for all customer interactions that take place via telephone. The algorithm presented can be applied in any situation where it is helpful for the agent to know whether a customer is speaking to her/his conviction. This could, for example, lead to a reduction in doubtful insurance claims, or untruthful statements in job interviews. This would not only reduce operational losses for service companies, but also encourage customers to be more truthful. △ Less","11 August, 2021",https://arxiv.org/pdf/2107.11175
Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments,Manish Bhattarai,"We present a new four-pronged approach to build firefighter's situational awareness for the first time in the literature. We construct a series of deep learning frameworks built on top of one another to enhance the safety, efficiency, and successful completion of rescue missions conducted by firefighters in emergency first response settings. First, we used a deep Convolutional Neural Network (CNN) system to classify and identify objects of interest from thermal imagery in real-time. Next, we extended this CNN framework for object detection, tracking, segmentation with a Mask RCNN framework, and scene description with a multimodal natural language processing(NLP) framework. Third, we built a deep Q-learning-based agent, immune to stress-induced disorientation and anxiety, capable of making clear navigation decisions based on the observed and stored facts in live-fire environments. Finally, we used a low computational unsupervised learning technique called tensor decomposition to perform meaningful feature extraction for anomaly detection in real-time. With these ad-hoc deep learning structures, we built the artificial intelligence system's backbone for firefighters' situational awareness. To bring the designed system into usage by firefighters, we designed a physical structure where the processed results are used as inputs in the creation of an augmented reality capable of advising firefighters of their location and key features around them, which are vital to the rescue operation at hand, as well as a path planning feature that acts as a virtual guide to assist disoriented first responders in getting back to safety. When combined, these four approaches present a novel approach to information understanding, transfer, and synthesis that could dramatically improve firefighter response and efficacy and reduce life loss. △ Less","8 November, 2021",https://arxiv.org/pdf/2107.11043
User Perception of Privacy with Ubiquitous Devices,Priyam Rajkhowa;Pradipta Biswas,"Privacy is important for all individuals in everyday life. With emerging technologies, smartphones with AR, various social networking applications and artificial intelligence driven modes of surveillance, they tend to intrude privacy. This study aimed to explore and discover various concerns related to perception of privacy in this era of ubiquitous technologies. It employed online survey questionnaire to study user perspectives of privacy. Purposive sampling was used to collect data from 60 participants. Inductive thematic analysis was used to analyze data. Our study discovered key themes like attitude towards privacy in public and private spaces, privacy awareness, consent seeking, dilemmas/confusions related to various technologies, impact of attitude and beliefs on individuals actions regarding how to protect oneself from invasion of privacy in both public and private spaces. These themes interacted amongst themselves and influenced formation of various actions. They were like core principles that molded actions that prevented invasion of privacy for both participant and bystander. Findings of this study would be helpful to improve privacy and personalization of various emerging technologies. This study contributes to privacy by design and positive design by considering psychological needs of users. This is suggestive that the findings can be applied in the areas of experience design, positive technologies, social computing and behavioral interventions. △ Less","23 July, 2021",https://arxiv.org/pdf/2107.11029
Explainable artificial intelligence (XAI) in deep learning-based medical image analysis,Bas H. M. van der Velden;Hugo J. Kuijf;Kenneth G. A. Gilhuijs;Max A. Viergever,"With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of eXplainable Artificial Intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis. △ Less","22 July, 2021",https://arxiv.org/pdf/2107.10912
Establishing Digital Recognition and Identification of Microscopic Objects for Implementation of Artificial Intelligence (AI) Guided Microassembly,Tuo Zhou;Shih-Yuan Yu;Matthew Michaels;Fangzhou Du;Lawrence Kulinsky;Mohammad Abdullah Al Faruque,"s miniaturization of electrical and mechanical components used in modern technology progresses, there is an increasing need for high-throughput and low-cost micro-scale assembly techniques. Many current micro-assembly methods are serial in nature, resulting in unfeasibly low throughput. Additionally, the need for increasingly smaller tools to pick and place individual microparts makes these methods cost prohibitive. Alternatively, parallel self-assembly or directed-assembly techniques can be employed by utilizing forces dominant at the micro and nano scales such as electro-kinetic, thermal, and capillary forces. However, these forces are governed by complex equations and often act on microparts simultaneously and competitively, making modeling and simulation difficult. The research in this paper presents a novel phenomenological approach to directed micro-assembly through the use of artificial intelligence to correlate micro-particle movement via dielectrophoretic and electro-osmotic forces in response to varying frequency of an applied non-uniform electric field. This research serves as a proof of concept of the application of artificial intelligence to create high yield low-cost micro-assembly techniques, which will prove useful in a variety of fields including micro-electrical-mechanical systems (MEMS), biotechnology, and tissue engineering. △ Less","22 July, 2021",https://arxiv.org/pdf/2107.10823
Philosophical Specification of Empathetic Ethical Artificial Intelligence,Michael Timothy Bennett;Yoshihiro Maruyama,"In order to construct an ethical artificial intelligence (AI) two complex problems must be overcome. Firstly, humans do not consistently agree on what is or is not ethical. Second, contemporary AI and machine learning methods tend to be blunt instruments which either search for solutions within the bounds of predefined rules, or mimic behaviour. An ethical AI must be capable of inferring unspoken rules, interpreting nuance and context, possess and be able to infer intent, and explain not just its actions but its intent. Using enactivism, semiotics, perceptual symbol systems and symbol emergence, we specify an agent that learns not just arbitrary relations between signs but their meaning in terms of the perceptual states of its sensorimotor system. Subsequently it can learn what is meant by a sentence and infer the intent of others in terms of its own experiences. It has malleable intent because the meaning of symbols changes as it learns, and its intent is represented symbolically as a goal. As such it may learn a concept of what is most likely to be considered ethical by the majority within a population of humans, which may then be used as a goal. The meaning of abstract symbols is expressed using perceptual symbols of raw sensorimotor stimuli as the weakest (consistent with Ockham's Razor) necessary and sufficient concept, an intensional definition learned from an ostensive definition, from which the extensional definition or category of all ethical decisions may be obtained. Because these abstract symbols are the same for both situation and response, the same symbol is used when either performing or observing an action. This is akin to mirror neurons in the human brain. Mirror symbols may allow the agent to empathise, because its own experiences are associated with the symbol, which is also associated with the observation of another agent experiencing something that symbol represents. △ Less","22 July, 2021",https://arxiv.org/pdf/2107.10715
Abstract Reasoning via Logic-guided Generation,Sihyun Yu;Sangwoo Mo;Sungsoo Ahn;Jinwoo Shin,"Abstract reasoning, i.e., inferring complicated patterns from given observations, is a central building block of artificial general intelligence. While humans find the answer by either eliminating wrong candidates or first constructing the answer, prior deep neural network (DNN)-based methods focus on the former discriminative approach. This paper aims to design a framework for the latter approach and bridge the gap between artificial and human intelligence. To this end, we propose logic-guided generation (LoGe), a novel generative DNN framework that reduces abstract reasoning as an optimization problem in propositional logic. LoGe is composed of three steps: extract propositional variables from images, reason the answer variables with a logic layer, and reconstruct the answer image from the variables. We demonstrate that LoGe outperforms the black box DNN frameworks for generative abstract reasoning under the RAVEN benchmark, i.e., reconstructing answers based on capturing correct rules of various attributes from observations. △ Less","11 August, 2021",https://arxiv.org/pdf/2107.10493
Inter and Intra-Annual Spatio-Temporal Variability of Habitat Suitability for Asian Elephants in India: A Random Forest Model-based Analysis,P. Anjali;Deepak N. Subramani,"We develop a Random Forest model to estimate the species distribution of Asian elephants in India and study the inter and intra-annual spatiotemporal variability of habitats suitable for them. Climatic, topographic variables and satellite-derived Land Use/Land Cover (LULC), Net Primary Productivity (NPP), Leaf Area Index (LAI), and Normalized Difference Vegetation Index (NDVI) are used as predictors, and the species sighting data of Asian elephants from Global Biodiversity Information Reserve is used to develop the Random Forest model. A careful hyper-parameter tuning and training-validation-testing cycle are completed to identify the significant predictors and develop a final model that gives precision and recall of 0.78 and 0.77. The model is applied to estimate the spatial and temporal variability of suitable habitats. We observe that seasonal reduction in the suitable habitat may explain the migration patterns of Asian elephants and the increasing human-elephant conflict. Further, the total available suitable habitat area is observed to have reduced, which exacerbates the problem. This machine learning model is intended to serve as an input to the Agent-Based Model that we are building as part of our Artificial Intelligence-driven decision support tool to reduce human-wildlife conflict. △ Less","22 July, 2021",https://arxiv.org/pdf/2107.10478
Shedding some light on Light Up with Artificial Intelligence,Libo Sun;James Browning;Roberto Perera,"The Light-Up puzzle, also known as the AKARI puzzle, has never been solved using modern artificial intelligence (AI) methods. Currently, the most widely used computational technique to autonomously develop solutions involve evolution theory algorithms. This project is an effort to apply new AI techniques for solving the Light-up puzzle faster and more computationally efficient. The algorithms explored for producing optimal solutions include hill climbing, simulated annealing, feed-forward neural network (FNN), and convolutional neural network (CNN). Two algorithms were developed for hill climbing and simulated annealing using 2 actions (add and remove light bulb) versus 3 actions(add, remove, or move light-bulb to a different cell). Both hill climbing and simulated annealing algorithms showed a higher accuracy for the case of 3 actions. The simulated annealing showed to significantly outperform hill climbing, FNN, CNN, and an evolutionary theory algorithm achieving 100% accuracy in 30 unique board configurations. Lastly, while FNN and CNN algorithms showed low accuracies, computational times were significantly faster compared to the remaining algorithms. The GitHub repository for this project can be found at https://github.com/rperera12/AKARI-LightUp-GameSolver-with-DeepNeuralNetworks-and-HillClimb-or-SimulatedAnnealing. △ Less","21 July, 2021",https://arxiv.org/pdf/2107.10429
Evaluation of In-Person Counseling Strategies To Develop Physical Activity Chatbot for Women,Kai-Hui Liang;Patrick Lange;Yoo Jung Oh;Jingwen Zhang;Yoshimi Fukuoka;Zhou Yu,"Artificial intelligence chatbots are the vanguard in technology-based intervention to change people's behavior. To develop intervention chatbots, the first step is to understand natural language conversation strategies in human conversation. This work introduces an intervention conversation dataset collected from a real-world physical activity intervention program for women. We designed comprehensive annotation schemes in four dimensions (domain, strategy, social exchange, and task-focused exchange) and annotated a subset of dialogs. We built a strategy classifier with context information to detect strategies from both trainers and participants based on the annotation. To understand how human intervention induces effective behavior changes, we analyzed the relationships between the intervention strategies and the participants' changes in the barrier and social support for physical activity. We also analyzed how participant's baseline weight correlates to the amount of occurrence of the corresponding strategy. This work lays the foundation for developing a personalized physical activity intervention bot. The dataset and code are available at https://github.com/KaihuiLiang/physical-activity-counseling △ Less","21 July, 2021",https://arxiv.org/pdf/2107.10410
A Sparsity Algorithm with Applications to Corporate Credit Rating,Dan Wang;Zhi Chen;Ionut Florescu,"In Artificial Intelligence, interpreting the results of a Machine Learning technique often termed as a black box is a difficult task. A counterfactual explanation of a particular ""black box"" attempts to find the smallest change to the input values that modifies the prediction to a particular output, other than the original one. In this work we formulate the problem of finding a counterfactual explanation as an optimization problem. We propose a new ""sparsity algorithm"" which solves the optimization problem, while also maximizing the sparsity of the counterfactual explanation. We apply the sparsity algorithm to provide a simple suggestion to publicly traded companies in order to improve their credit ratings. We validate the sparsity algorithm with a synthetically generated dataset and we further apply it to quarterly financial statements from companies in financial, healthcare and IT sectors of the US market. We provide evidence that the counterfactual explanation can capture the nature of the real statement features that changed between the current quarter and the following quarter when ratings improved. The empirical results show that the higher the rating of a company the greater the ""effort"" required to further improve credit rating. △ Less","21 July, 2021",https://arxiv.org/pdf/2107.10306
Multi-institution encrypted medical imaging AI validation without data sharing,Arjun Soin;Pratik Bhatu;Rohit Takhar;Nishanth Chandran;Divya Gupta;Javier Alvarez-Valle;Rahul Sharma;Vidur Mahajan;Matthew P Lungren,"Adoption of artificial intelligence medical imaging applications is often impeded by barriers between healthcare systems and algorithm developers given that access to both private patient data and commercial model IP is important to perform pre-deployment evaluation. This work investigates a framework for secure, privacy-preserving and AI-enabled medical imaging inference using CrypTFlow2, a state-of-the-art end-to-end compiler allowing cryptographically secure 2-party Computation (2PC) protocols between the machine learning model vendor and target patient data owner. A common DenseNet-121 chest x-ray diagnosis model was evaluated on multi-institutional chest radiographic imaging datasets both with and without CrypTFlow2 on two test sets spanning seven sites across the US and India, and comprising 1,149 chest x-ray images. We measure comparative AUROC performance between secure and insecure inference in multiple pathology classification tasks, and explore model output distributional shifts and resource constraints introduced by secure model inference. Secure inference with CrypTFlow2 demonstrated no significant difference in AUROC for all diagnoses, and model outputs from secure and insecure inference methods were distributionally equivalent. The use of CrypTFlow2 may allow off-the-shelf secure 2PC between healthcare systems and AI model vendors for medical imaging, without changes in performance, and can facilitate scalable pre-deployment infrastructure for real-world secure model evaluation without exposure to patient data or model IP. △ Less","13 August, 2021",https://arxiv.org/pdf/2107.10230
GLIME: A new graphical methodology for interpretable model-agnostic explanations,Zoumpolia Dikopoulou;Serafeim Moustakidis;Patrik Karlsson,"Explainable artificial intelligence (XAI) is an emerging new domain in which a set of processes and tools allow humans to better comprehend the decisions generated by black box models. However, most of the available XAI tools are often limited to simple explanations mainly quantifying the impact of individual features to the models' output. Therefore, human users are not able to understand how the features are related to each other to make predictions, whereas the inner workings of the trained models remain hidden. This paper contributes to the development of a novel graphical explainability tool that not only indicates the significant features of the model but also reveals the conditional relationships between features and the inference capturing both the direct and indirect impact of features to the models' decision. The proposed XAI methodology, termed as gLIME, provides graphical model-agnostic explanations either at the global (for the entire dataset) or the local scale (for specific data points). It relies on a combination of local interpretable model-agnostic explanations (LIME) with graphical least absolute shrinkage and selection operator (GLASSO) producing undirected Gaussian graphical models. Regularization is adopted to shrink small partial correlation coefficients to zero providing sparser and more interpretable graphical explanations. Two well-known classification datasets (BIOPSY and OAI) were selected to confirm the superiority of gLIME over LIME in terms of both robustness and consistency over multiple permutations. Specifically, gLIME accomplished increased stability over the two datasets with respect to features' importance (76%-96% compared to 52%-77% using LIME). gLIME demonstrates a unique potential to extend the functionality of the current state-of-the-art in XAI by providing informative graphically given explanations that could unlock black boxes. △ Less","21 July, 2021",https://arxiv.org/pdf/2107.09927
MEGEX: Data-Free Model Extraction Attack against Gradient-Based Explainable AI,Takayuki Miura;Satoshi Hasegawa;Toshiki Shibahara,"The advance of explainable artificial intelligence, which provides reasons for its predictions, is expected to accelerate the use of deep neural networks in the real world like Machine Learning as a Service (MLaaS) that returns predictions on queried data with the trained model. Deep neural networks deployed in MLaaS face the threat of model extraction attacks. A model extraction attack is an attack to violate intellectual property and privacy in which an adversary steals trained models in a cloud using only their predictions. In particular, a data-free model extraction attack has been proposed recently and is more critical. In this attack, an adversary uses a generative model instead of preparing input data. The feasibility of this attack, however, needs to be studied since it requires more queries than that with surrogate datasets. In this paper, we propose MEGEX, a data-free model extraction attack against a gradient-based explainable AI. In this method, an adversary uses the explanations to train the generative model and reduces the number of queries to steal the model. Our experiments show that our proposed method reconstructs high-accuracy models -- 0.97\times and 0.98\times the victim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries, respectively. This implies that there is a trade-off between the interpretability of models and the difficulty of stealing them. △ Less","19 July, 2021",https://arxiv.org/pdf/2107.08909
Data Partition and Rate Control for Learning and Energy Efficient Edge Intelligence,Xiaoyang Li;Shuai Wang;Guangxu Zhu;Ziqin Zhou;Kaibin Huang;Yi Gong,"The rapid development of artificial intelligence together with the powerful computation capabilities of the advanced edge servers make it possible to deploy learning tasks at the wireless network edge, which is dubbed as edge intelligence (EI). The communication bottleneck between the data resource and the server results in deteriorated learning performance as well as tremendous energy consumption. To tackle this challenge, we explore a new paradigm called learning-and-energy-efficient (LEE) EI, which simultaneously maximizes the learning accuracies and energy efficiencies of multiple tasks via data partition and rate control. Mathematically, this results in a multi-objective optimization problem. Moreover, the continuous varying rates over the whole transmission duration introduce infinite variables. To solve this complex problem, we consider the case with infinite server buffer capacity and one-shot data arrival at sensor. First, the number of variables are reduced to a finite level by exploiting the optimality of constant-rate transmission in each epoch. Second, the optimal solution is found by applying stratified sequencing or objectives merging. By assuming higher priority of learning efficiency in stratified sequencing, the closed form of optimal data partition is derived by the Lagrange method, while the optimal rate control is proved to have the structure of directional water filling (DWF), based on which a string-pulling (SP) algorithm is proposed to obtain the numerical values. The DWF structure of rate control is also proved to be optimal in objectives merging via weighted summation. By exploiting the optimal rate changing properties, the SP algorithm is further extended to account for the cases with limited server buffer capacity or bursty data arrival at sensor. The performance of the proposed design is examined by extensive experiments based on public datasets. △ Less","19 July, 2021",https://arxiv.org/pdf/2107.08884
EvilModel: Hiding Malware Inside of Neural Network Models,Zhi Wang;Chaoge Liu;Xiang Cui,"Delivering malware covertly and evasively is critical to advanced malware campaigns. In this paper, we present a new method to covertly and evasively deliver malware through a neural network model. Neural network models are poorly explainable and have a good generalization ability. By embedding malware in neurons, the malware can be delivered covertly, with minor or no impact on the performance of neural network. Meanwhile, because the structure of the neural network model remains unchanged, it can pass the security scan of antivirus engines. Experiments show that 36.9MB of malware can be embedded in a 178MB-AlexNet model within 1% accuracy loss, and no suspicion is raised by anti-virus engines in VirusTotal, which verifies the feasibility of this method. With the widespread application of artificial intelligence, utilizing neural networks for attacks becomes a forwarding trend. We hope this work can provide a reference scenario for the defense on neural network-assisted attacks. △ Less","5 August, 2021",https://arxiv.org/pdf/2107.08590
A pattern recognition approach for distinguishing between prose and poetry,Henrique F. de Arruda;Sandro M. Reia;Filipi N. Silva;Diego R. Amancio;Luciano da F. Costa,"Poetry and prose are written artistic expressions that help us to appreciate the reality we live. Each of these styles has its own set of subjective properties, such as rhyme and rhythm, which are easily caught by a human reader's eye and ear. With the recent advances in artificial intelligence, the gap between humans and machines may have decreased, and today we observe algorithms mastering tasks that were once exclusively performed by humans. In this paper, we propose an automated method to distinguish between poetry and prose based solely on aural and rhythmic properties. In other to compare prose and poetry rhythms, we represent the rhymes and phones as temporal sequences and thus we propose a procedure for extracting rhythmic features from these sequences. The classification of the considered texts using the set of features extracted resulted in a best accuracy of 0.78, obtained with a neural network. Interestingly, by using an approach based on complex networks to visualize the similarities between the different texts considered, we found that the patterns of poetry vary much more than prose. Consequently, a much richer and complex set of rhythmic possibilities tends to be found in that modality. △ Less","18 July, 2021",https://arxiv.org/pdf/2107.08512
Model Uncertainty and Correctability for Directed Graphical Models,Panagiota Birmpa;Jinchao Feng;Markos A. Katsoulakis;Luc Rey-Bellet,"Probabilistic graphical models are a fundamental tool in probabilistic modeling, machine learning and artificial intelligence. They allow us to integrate in a natural way expert knowledge, physical modeling, heterogeneous and correlated data and quantities of interest. For exactly this reason, multiple sources of model uncertainty are inherent within the modular structure of the graphical model. In this paper we develop information-theoretic, robust uncertainty quantification methods and non-parametric stress tests for directed graphical models to assess the effect and the propagation through the graph of multi-sourced model uncertainties to quantities of interest. These methods allow us to rank the different sources of uncertainty and correct the graphical model by targeting its most impactful components with respect to the quantities of interest. Thus, from a machine learning perspective, we provide a mathematically rigorous approach to correctability that guarantees a systematic selection for improvement of components of a graphical model while controlling potential new errors created in the process in other parts of the model. We demonstrate our methods in two physico-chemical examples, namely quantum scale-informed chemical kinetics and materials screening to improve the efficiency of fuel cells. △ Less","17 July, 2021",https://arxiv.org/pdf/2107.08179
Uncertainty Prediction for Machine Learning Models of Material Properties,Francesca Tavazza;Brian De Cost;Kamal Choudhary,"Uncertainty quantification in Artificial Intelligence (AI)-based predictions of material properties is of immense importance for the success and reliability of AI applications in material science. While confidence intervals are commonly reported for machine learning (ML) models, prediction intervals, i.e., the evaluation of the uncertainty on each prediction, are seldomly available. In this work we compare 3 different approaches to obtain such individual uncertainty, testing them on 12 ML-physical properties. Specifically, we investigated using the Quantile loss function, machine learning the prediction intervals directly and using Gaussian Processes. We identify each approachs advantages and disadvantages and end up slightly favoring the modeling of the individual uncertainties directly, as it is the easiest to fit and, in most cases, minimizes over-and under-estimation of the predicted errors. All data for training and testing were taken from the publicly available JARVIS-DFT database, and the codes developed for computing the prediction intervals are available through JARVIS-Tools. △ Less","16 July, 2021",https://arxiv.org/pdf/2107.07997
Multiclass Permanent Magnets Superstructure for Indoor Localization using Artificial Intelligence,Amir Ivry;Elad Fisher;Roger Alimi;Idan Mosseri;Kanna Nahir,"Smartphones have become a popular tool for indoor localization and position estimation of users. Existing solutions mainly employ Wi-Fi, RFID, and magnetic sensing techniques to track movements in crowded venues. These are highly sensitive to magnetic clutters and depend on local ambient magnetic fields, which frequently degrades their performance. Also, these techniques often require pre-known mapping surveys of the area, or the presence of active beacons, which are not always available. We embed small-volume and large-moment magnets in pre-known locations and arrange them in specific geometric constellations that create magnetic superstructure patterns of supervised magnetic signatures. These signatures constitute an unambiguous magnetic environment with respect to the moving sensor carrier. The localization algorithm learns the unique patterns of the scattered magnets during training and detects them from the ongoing streaming of data during localization. Our contribution is twofold. First, we deploy passive permanent magnets that do not require a power supply, in contrast to active magnetic transmitters. Second, we perform localization based on smartphone motion rather than on static positioning of the magnetometer. In our previous study, we considered a single superstructure pattern. Here, we present an extended version of that algorithm for multi-superstructure localization, which covers a broader localization area of the user. Experimental results demonstrate localization accuracy of 95% with a mean localization error of less than 1m using artificial intelligence. △ Less","14 July, 2021",https://arxiv.org/pdf/2107.07425
Learning Mixed-Integer Linear Programs from Contextual Examples,Mohit Kumar;Samuel Kolb;Luc De Raedt;Stefano Teso,"Mixed-integer linear programs (MILPs) are widely used in artificial intelligence and operations research to model complex decision problems like scheduling and routing. Designing such programs however requires both domain and modelling expertise. In this paper, we study the problem of acquiring MILPs from contextual examples, a novel and realistic setting in which examples capture solutions and non-solutions within a specific context. The resulting learning problem involves acquiring continuous parameters -- namely, a cost vector and a feasibility polytope -- but has a distinctly combinatorial flavor. To solve this complex problem, we also contribute MISSLE, an algorithm for learning MILPs from contextual examples. MISSLE uses a variant of stochastic local search that is guided by the gradient of a continuous surrogate loss function. Our empirical evaluation on synthetic data shows that MISSLE acquires better MILPs faster than alternatives based on stochastic local search and gradient descent. △ Less","15 July, 2021",https://arxiv.org/pdf/2107.07136
DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based EEG Signals with Convolutional Autoencoder,Dae-Hyeok Lee;Sung-Jin Kim;Seong-Whan Lee,"Brain-computer interface (BCI) is one of the tools which enables the communication between humans and devices by reflecting intention and status of humans. With the development of artificial intelligence, the interest in communication between humans and drones using electroencephalogram (EEG) is increased. Especially, in the case of controlling drone swarms such as direction or formation, there are many advantages compared with controlling a drone unit. Imagined speech is one of the endogenous BCI paradigms, which can identify intentions of users. When conducting imagined speech, the users imagine the pronunciation as if actually speaking. In contrast, overt speech is a task in which the users directly pronounce the words. When controlling drone swarms using imagined speech, complex commands can be delivered more intuitively, but decoding performance is lower than that of other endogenous BCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of overt speech for imagined speech-based EEG signals classification. To the best of our knowledge, this study is the first attempt to use EEG features of overt speech to decode imagined speech-based EEG signals with an autoencoder. A total of eight subjects participated in the experiment. When classifying four words, the average accuracy of the DAL was 48.41%. In addition, when comparing the performance between w/o and w/ EEG features of overt speech, there was a performance improvement of 7.42% when including EEG features of overt speech. Hence, we demonstrated that EEG features of overt speech could improve the decoding performance of imagined speech. △ Less","14 July, 2021",https://arxiv.org/pdf/2107.07064
Explainable AI: current status and future directions,Prashant Gohel;Priyanka Singh;Manoranjan Mohanty,"Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other ""wh"" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided. △ Less","12 July, 2021",https://arxiv.org/pdf/2107.07045
Tortured phrases: A dubious writing style emerging in science. Evidence of critical issues affecting established journals,Guillaume Cabanac;Cyril Labbé;Alexander Magazinov,"Probabilistic text generators have been used to produce fake scientific papers for more than a decade. Such nonsensical papers are easily detected by both human and machine. Now more complex AI-powered generation techniques produce texts indistinguishable from that of humans and the generation of scientific texts from a few keywords has been documented. Our study introduces the concept of tortured phrases: unexpected weird phrases in lieu of established ones, such as 'counterfeit consciousness' instead of 'artificial intelligence.' We combed the literature for tortured phrases and study one reputable journal where these concentrated en masse. Hypothesising the use of advanced language models we ran a detector on the abstracts of recent articles of this journal and on several control sets. The pairwise comparisons reveal a concentration of abstracts flagged as 'synthetic' in the journal. We also highlight irregularities in its operation, such as abrupt changes in editorial timelines. We substantiate our call for investigation by analysing several individual dubious articles, stressing questionable features: tortured writing style, citation of non-existent literature, and unacknowledged image reuse. Surprisingly, some websites offer to rewrite texts for free, generating gobbledegook full of tortured phrases. We believe some authors used rewritten texts to pad their manuscripts. We wish to raise the awareness on publications containing such questionable AI-generated or rewritten texts that passed (poor) peer review. Deception with synthetic texts threatens the integrity of the scientific literature. △ Less","12 July, 2021",https://arxiv.org/pdf/2107.06751
Artificial Intelligence in PET: an Industry Perspective,Arkadiusz Sitek;Sangtae Ahn;Evren Asma;Adam Chandler;Alvin Ihsani;Sven Prevrhal;Arman Rahmim;Babak Saboury;Kris Thielemans,"Artificial intelligence (AI) has significant potential to positively impact and advance medical imaging, including positron emission tomography (PET) imaging applications. AI has the ability to enhance and optimize all aspects of the PET imaging chain from patient scheduling, patient setup, protocoling, data acquisition, detector signal processing, reconstruction, image processing and interpretation. AI poses industry-specific challenges which will need to be addressed and overcome to maximize the future potentials of AI in PET. This paper provides an overview of these industry-specific challenges for the development, standardization, commercialization, and clinical adoption of AI, and explores the potential enhancements to PET imaging brought on by AI in the near future. In particular, the combination of on-demand image reconstruction, AI, and custom designed data processing workflows may open new possibilities for innovation which would positively impact the industry and ultimately patients. △ Less","14 July, 2021",https://arxiv.org/pdf/2107.06747
Trustworthy AI: A Computational Perspective,Haochen Liu;Yiqi Wang;Wenqi Fan;Xiaorui Liu;Yaxin Li;Shaili Jain;Yunhao Liu;Anil K. Jain;Jiliang Tang,"In the past few decades, artificial intelligence (AI) technology has experienced swift developments, changing everyone's daily life and profoundly altering the course of human society. The intention of developing AI is to benefit humans, by reducing human labor, bringing everyday convenience to human lives, and promoting social good. However, recent research and AI applications show that AI can cause unintentional harm to humans, such as making unreliable decisions in safety-critical scenarios or undermining fairness by inadvertently discriminating against one group. Thus, trustworthy AI has attracted immense attention recently, which requires careful consideration to avoid the adverse effects that AI may bring to humans, so that humans can fully trust and live in harmony with AI technologies. Recent years have witnessed a tremendous amount of research on trustworthy AI. In this survey, we present a comprehensive survey of trustworthy AI from a computational perspective, to help readers understand the latest technologies for achieving trustworthy AI. Trustworthy AI is a large and complex area, involving various dimensions. In this work, we focus on six of the most crucial dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii) Non-discrimination & Fairness, (iii) Explainability, (iv) Privacy, (v) Accountability & Auditability, and (vi) Environmental Well-Being. For each dimension, we review the recent related technologies according to a taxonomy and summarize their applications in real-world systems. We also discuss the accordant and conflicting interactions among different dimensions and discuss potential aspects for trustworthy AI to investigate in the future. △ Less","18 August, 2021",https://arxiv.org/pdf/2107.06641
BRIMA: low-overhead BRowser-only IMage Annotation tool (Preprint),Tuomo Lahtinen;Hannu Turtiainen;Andrei Costin,"Image annotation and large annotated datasets are crucial parts within the Computer Vision and Artificial Intelligence fields.At the same time, it is well-known and acknowledged by the research community that the image annotation process is challenging, time-consuming and hard to scale. Therefore, the researchers and practitioners are always seeking ways to perform the annotations easier, faster, and at higher quality. Even though several widely used tools exist and the tools' landscape evolved considerably, most of the tools still require intricate technical setups and high levels of technical savviness from its operators and crowdsource contributors. In order to address such challenges, we develop and present BRIMA -- a flexible and open-source browser extension that allows BRowser-only IMage Annotation at considerably lower overheads. Once added to the browser, it instantly allows the user to annotate images easily and efficiently directly from the browser without any installation or setup on the client-side. It also features cross-browser and cross-platform functionality thus presenting itself as a neat tool for researchers within the Computer Vision, Artificial Intelligence, and privacy-related fields. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06351
Predictive models for wind speed using artificial intelligence and copula,Md Amimul Ehsan,"Electricity generation from burning fossil fuels is one of the major contributors to global warming. Renewable energy sources are a viable alternative to produce electrical energy and to reduce the emission from the power industry. These energy sources are the building blocks of green energy, which all have different characteristics. Their availabilities are also diverse, depending on geographical locations and other parameters. Low implementation cost and distributed availability all over the world uplifts their popularity exponentially. Therefore, it has unlocked opportunities for consumers to produce electricity locally and use it on-site, which reduces dependency on centralized utility companies. The research considers two main objectives: the prediction of wind speed that simplifies wind farm planning and feasibility study. Secondly, the need to understand the dependency structure of the wind speeds of multiple distant locations. To address the first objective, twelve artificial intelligence algorithms were used for wind speed prediction from collected meteorological parameters. The model performances were compared to determine the wind speed prediction accuracy. The results show a deep learning approach, long short-term memory (LSTM) outperforms other models with the highest accuracy of 97.8%. For dependency, a multivariate cumulative distribution function, Copula, was used to find the joint distribution of two or more distant location wind speeds, followed by a case study. We found that the appropriate copula family and the parameters vary based on the distance in between. For the case study, Joe-Frank (BB8) copula shows an efficient joint distribution fit for a wind speed pair with a standard error of 0.0094. Finally, some insights about the uncertainty aspects of wind speed dependency were addressed. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.06182
aiSTROM -- A roadmap for developing a successful AI strategy,Dorien Herremans,"A total of 34% of AI research and development projects fails or are abandoned, according to a recent survey by Rackspace Technology of 1,870 companies. We propose a new strategic framework, aiSTROM, that empowers managers to create a successful AI strategy based on a thorough literature review. This provides a unique and integrated approach that guides managers and lead developers through the various challenges in the implementation process. In the aiSTROM framework, we start by identifying the top n potential projects (typically 3-5). For each of those, seven areas of focus are thoroughly analysed. These areas include creating a data strategy that takes into account unique cross-departmental machine learning data requirements, security, and legal requirements. aiSTROM then guides managers to think about how to put together an interdisciplinary artificial intelligence (AI) implementation team given the scarcity of AI talent. Once an AI team strategy has been established, it needs to be positioned within the organization, either cross-departmental or as a separate division. Other considerations include AI as a service (AIaas), or outsourcing development. Looking at new technologies, we have to consider challenges such as bias, legality of black-box-models, and keeping humans in the loop. Next, like any project, we need value-based key performance indicators (KPIs) to track and validate the progress. Depending on the company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities, and threats) can help further classify the shortlisted projects. Finally, we should make sure that our strategy includes continuous education of employees to enable a culture of adoption. This unique and comprehensive framework offers a valuable, literature supported, tool for managers and lead developers. △ Less","15 November, 2021",https://arxiv.org/pdf/2107.06071
Tales of a City: Sentiment Analysis of Urban Green Space in Dublin,Mohammadhossein Ghahramani;Nadina Galle;Carlo Ratti;Francesco Pilla,"Social media services such as TripAdvisor and Foursquare can provide opportunities for users to exchange their opinions about urban green space (UGS). Visitors can exchange their experiences with parks, woods, and wetlands in social communities via social networks. In this work, we implement a unified topic modeling approach to reveal UGS characteristics. Leveraging Artificial Intelligence techniques for opinion mining using the mentioned platforms (e.g., TripAdvisor and Foursquare) reviews is a novel application to UGS quality assessments. We show how specific characteristics of different green spaces can be explored by using a tailor-optimized sentiment analysis model. Such an application can support local authorities and stakeholders in understanding--and justification for--future urban green space investments. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06041
A Deep Generative Artificial Intelligence system to decipher species coexistence patterns,J. Hirn;J. E. García;A. Montesinos-Navarro;R. Sanchez-Martín;V. Sanz;M. Verdú,"1. Deciphering coexistence patterns is a current challenge to understanding diversity maintenance, especially in rich communities where the complexity of these patterns is magnified through indirect interactions that prevent their approximation with classical experimental approaches. 2. We explore cutting-edge Machine Learning techniques called Generative Artificial Intelligence (GenAI) to decipher species coexistence patterns in vegetation patches, training generative adversarial networks (GAN) and variational AutoEncoders (VAE) that are then used to unravel some of the mechanisms behind community assemblage. 3. The GAN accurately reproduces the species composition of real patches as well as the affinity of plant species to different soil types, and the VAE also reaches a high level of accuracy, above 99%. Using the artificially generated patches, we found that high order interactions tend to suppress the positive effects of low order interactions. Finally, by reconstructing successional trajectories we could identify the pioneer species with larger potential to generate a high diversity of distinct patches in terms of species composition. 4. Understanding the complexity of species coexistence patterns in diverse ecological communities requires new approaches beyond heuristic rules. Generative Artificial Intelligence can be a powerful tool to this end as it allows to overcome the inherent dimensionality of this challenge. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.06020
A Classification of Artificial Intelligence Systems for Mathematics Education,Steven Van Vaerenbergh;Adrián Pérez-Suay,"This chapter provides an overview of the different Artificial Intelligence (AI) systems that are being used in contemporary digital tools for Mathematics Education (ME). It is aimed at researchers in AI and Machine Learning (ML), for whom we shed some light on the specific technologies that are being used in educational applications; and at researchers in ME, for whom we clarify: i) what the possibilities of the current AI technologies are, ii) what is still out of reach and iii) what is to be expected in the near future. We start our analysis by establishing a high-level taxonomy of AI tools that are found as components in digital ME applications. Then, we describe in detail how these AI tools, and in particular ML, are being used in two key applications, specifically AI-based calculators and intelligent tutoring systems. We finish the chapter with a discussion about student modeling systems and their relationship to artificial general intelligence. △ Less","20 October, 2021",https://arxiv.org/pdf/2107.06015
Recent Advances in Leveraging Human Guidance for Sequential Decision-Making Tasks,Ruohan Zhang;Faraz Torabi;Garrett Warnell;Peter Stone,"A longstanding goal of artificial intelligence is to create artificial agents capable of learning to perform tasks that require sequential decision making. Importantly, while it is the artificial agent that learns and acts, it is still up to humans to specify the particular task to be performed. Classical task-specification approaches typically involve humans providing stationary reward functions or explicit demonstrations of the desired tasks. However, there has recently been a great deal of research energy invested in exploring alternative ways in which humans may guide learning agents that may, e.g., be more suitable for certain tasks or require less human effort. This survey provides a high-level overview of five recent machine learning frameworks that primarily rely on human guidance apart from pre-specified reward functions or conventional, step-by-step action demonstrations. We review the motivation, assumptions, and implementation of each framework, and we discuss possible future research directions. △ Less","12 July, 2021",https://arxiv.org/pdf/2107.05825
GPTPU: Accelerating Applications using Edge Tensor Processing Units,Kuan-Chieh Hsu;Hung-Wei Tseng,"Neural network (NN) accelerators have been integrated into a wide-spectrum of computer systems to accommodate the rapidly growing demands for artificial intelligence (AI) and machine learning (ML) applications. NN accelerators share the idea of providing native hardware support for operations on multidimensional tensor data. Therefore, NN accelerators are theoretically tensor processors that can improve system performance for any problem that uses tensors as inputs/outputs. Unfortunately, commercially available NN accelerators only expose computation capabilities through AI/ML-specific interfaces. Furthermore, NN accelerators reveal very few hardware design details, so applications cannot easily leverage the tensor operations NN accelerators provide. This paper introduces General-Purpose Computing on Edge Tensor Processing Units (GPTPU), an open-source, open-architecture framework that allows the developer and research communities to discover opportunities that NN accelerators enable for applications. GPTPU includes a powerful programming interface with efficient runtime system-level support -- similar to that of CUDA/OpenCL in GPGPU computing -- to bridge the gap between application demands and mismatched hardware/software interfaces. We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which are widely available and representative of many commercial NN accelerators. We identified several novel use cases and revisited the algorithms. By leveraging the underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our results reveal that GPTPU can achieve a 2.46x speedup over high-end CPUs and reduce energy consumption by 40%. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.05473
SimDem A Multi-agent Simulation Environment to Model Persons with Dementia and their Assistance,Muhammad Salman Shaukat;Bjarne Christian Hiller;Sebastian Bader;Thomas Kirste,"Developing artificial intelligence based assistive systems to aid Persons with Dementia (PwD) requires large amounts of training data. However, data collection poses ethical, legal, economic, and logistic issues. Synthetic data generation tools, in this regard, provide a potential solution. However, we believe that already available such tools do not adequately reflect cognitive deficiencies in behavior simulation. To counter these issues we propose a simulation model (SimDem ) that primarily focuses on cognitive impairments suffered by PwD and can be easily configured and adapted by the users to model and evaluate assistive solutions. △ Less","12 July, 2021",https://arxiv.org/pdf/2107.05346
Machine Learning Challenges and Opportunities in the African Agricultural Sector -- A General Perspective,Racine Ly,"The improvement of computers' capacities, advancements in algorithmic techniques, and the significant increase of available data have enabled the recent developments of Artificial Intelligence (AI) technology. One of its branches, called Machine Learning (ML), has shown strong capacities in mimicking characteristics attributed to human intelligence, such as vision, speech, and problem-solving. However, as previous technological revolutions suggest, their most significant impacts could be mostly expected on other sectors that were not traditional users of that technology. The agricultural sector is vital for African economies; improving yields, mitigating losses, and effective management of natural resources are crucial in a climate change era. Machine Learning is a technology with an added value in making predictions, hence the potential to reduce uncertainties and risk across sectors, in this case, the agricultural sector. The purpose of this paper is to contextualize and discuss barriers to ML-based solutions for African agriculture. In the second section, we provided an overview of ML technology from a historical and technical perspective and its main driving force. In the third section, we provided a brief review of the current use of ML in agriculture. Finally, in section 4, we discuss ML growing interest in Africa and the potential barriers to creating and using ML-based solutions in the agricultural sector. △ Less","11 July, 2021",https://arxiv.org/pdf/2107.05101
Bayesian Convolutional Neural Networks for Seven Basic Facial Expression Classifications,Yuan Tai;Yihua Tan;Wei Gong;Hailan Huang,"The seven basic facial expression classifications are a basic way to express complex human emotions and are an important part of artificial intelligence research. Based on the traditional Bayesian neural network framework, the ResNet18_BNN network constructed in this paper has been improved in the following three aspects: (1) A new objective function is proposed, which is composed of the KL loss of uncertain parameters and the intersection of specific parameters. Entropy loss composition. (2) Aiming at a special objective function, a training scheme for alternately updating these two parameters is proposed. (3) Only model the parameters of the last convolution group. Through testing on the FER2013 test set, we achieved 71.5% and 73.1% accuracy in PublicTestSet and PrivateTestSet, respectively. Compared with traditional Bayesian neural networks, our method brings the highest classification accuracy gain. △ Less","13 July, 2021",https://arxiv.org/pdf/2107.04834
Lifelong Teacher-Student Network Learning,Fei Ye;Adrian G. Bors,"A unique cognitive capability of humans consists in their ability to acquire new knowledge and skills from a sequence of experiences. Meanwhile, artificial intelligence systems are good at learning only the last given task without being able to remember the databases learnt in the past. We propose a novel lifelong learning methodology by employing a Teacher-Student network framework. While the Student module is trained with a new given database, the Teacher module would remind the Student about the information learnt in the past. The Teacher, implemented by a Generative Adversarial Network (GAN), is trained to preserve and replay past knowledge corresponding to the probabilistic representations of previously learn databases. Meanwhile, the Student module is implemented by a Variational Autoencoder (VAE) which infers its latent variable representation from both the output of the Teacher module as well as from the newly available database. Moreover, the Student module is trained to capture both continuous and discrete underlying data representations across different domains. The proposed lifelong learning framework is applied in supervised, semi-supervised and unsupervised training. The code is available~: \url{https://github.com/dtuzi123/Lifelong-Teacher-Student-Network-Learning} △ Less","9 July, 2021",https://arxiv.org/pdf/2107.04689
Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs and Convolutional Networks,Marwan Dhuheir;Emna Baccour;Aiman Erbad;Sinan Sabeeh;Mounir Hamdi,"Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention due to their outstanding ability to be used in different sectors and serve in difficult and dangerous areas. Moreover, the advancements in computer vision and artificial intelligence have increased the use of UAVs in various applications and solutions, such as forest fires detection and borders monitoring. However, using deep neural networks (DNNs) with UAVs introduces several challenges of processing deeper networks and complex models, which restricts their on-board computation. In this work, we present a strategy aiming at distributing inference requests to a swarm of resource-constrained UAVs that classifies captured images on-board and finds the minimum decision-making latency. We formulate the model as an optimization problem that minimizes the latency between acquiring images and making the final decisions. The formulated optimization solution is an NP-hard problem. Hence it is not adequate for online resource allocation. Therefore, we introduce an online heuristic solution, namely DistInference, to find the layers placement strategy that gives the best latency among the available UAVs. The proposed approach is general enough to be used for different low decision-latency applications as well as for all CNN types organized into the pipeline of layers (e.g., VGG) or based on residual blocks (e.g., ResNet). △ Less","9 July, 2021",https://arxiv.org/pdf/2107.04648
Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction,Jiahua Rao;Shuangjia Zheng;Yuedong Yang,"Advances in machine learning have led to graph neural network-based methods for drug discovery, yielding promising results in molecular design, chemical synthesis planning, and molecular property prediction. However, current graph neural networks (GNNs) remain of limited acceptance in drug discovery is limited due to their lack of interpretability. Although this major weakness has been mitigated by the development of explainable artificial intelligence (XAI) techniques, the ""ground truth"" assignment in most explainable tasks ultimately rests with subjective judgments by humans so that the quality of model interpretation is hard to evaluate in quantity. In this work, we first build three levels of benchmark datasets to quantitatively assess the interpretability of the state-of-the-art GNN models. Then we implemented recent XAI methods in combination with different GNN algorithms to highlight the benefits, limitations, and future opportunities for drug discovery. As a result, GradInput and IG generally provide the best model interpretability for GNNs, especially when combined with GraphNet and CMPNN. The integrated and developed XAI package is fully open-sourced and can be used by practitioners to train new models on other drug discovery tasks. △ Less","12 July, 2021",https://arxiv.org/pdf/2107.04119
"Smart Healthcare in the Age of AI: Recent Advances, Challenges, and Future Prospects",Mahmoud Nasr;MD. Milon Islam;Shady Shehata;Fakhri Karray;Yuri Quintana,"The significant increase in the number of individuals with chronic ailments (including the elderly and disabled) has dictated an urgent need for an innovative model for healthcare systems. The evolved model will be more personalized and less reliant on traditional brick-and-mortar healthcare institutions such as hospitals, nursing homes, and long-term healthcare centers. The smart healthcare system is a topic of recently growing interest and has become increasingly required due to major developments in modern technologies, especially in artificial intelligence (AI) and machine learning (ML). This paper is aimed to discuss the current state-of-the-art smart healthcare systems highlighting major areas like wearable and smartphone devices for health monitoring, machine learning for disease diagnosis, and the assistive frameworks, including social robots developed for the ambient assisted living environment. Additionally, the paper demonstrates software integration architectures that are very significant to create smart healthcare systems, integrating seamlessly the benefit of data analytics and other tools of AI. The explained developed systems focus on several facets: the contribution of each developed framework, the detailed working procedure, the performance as outcomes, and the comparative merits and limitations. The current research challenges with potential future directions are addressed to highlight the drawbacks of existing systems and the possible methods to introduce novel frameworks, respectively. This review aims at providing comprehensive insights into the recent developments of smart healthcare systems to equip experts to contribute to the field. △ Less","24 June, 2021",https://arxiv.org/pdf/2107.03924
Artificial intelligence across company borders,Olga Fink;Torbjørn Netland;Stefan Feuerriegel,"Artificial intelligence (AI) has become a valued technology in many companies. At the same time, a substantial potential for utilizing AI \emph{across} company borders has remained largely untapped. An inhibiting factor concerns disclosure of data to external parties, which raises legitimate concerns about intellectual property rights, privacy issues, and cybersecurity risks. Combining federated learning with domain adaptation can provide a solution to this problem by enabling effective cross-company AI without data disclosure. In this Viewpoint, we discuss the use, value, and implications of this approach in a cross-company setting. △ Less","21 June, 2021",https://arxiv.org/pdf/2107.03912
AI and the future of pharmaceutical research,Adam Zielinski,"This paper examines how pharmaceutical Artificial Intelligence advancements may affect the development of new drugs in the coming years. The question was answered by reviewing a rich body of source material, including industry literature, research journals, AI studies, market reports, market projections, discussion papers, press releases, and organizations' websites. The paper argues that continued innovation in pharmaceutical AI will enable rapid development of safe and effective therapies for previously untreatable diseases. A series of major points support this conclusion: The pharmaceutical industry is in a significant productivity crisis today, and AI-enabled research methods can be directly applied to reduce the time and cost of drug discovery projects. The industry already reported results such as a 10-fold reduction in drug molecule discovery times. Numerous AI alliances between industry, governments, and academia enabled utilizing proprietary data and led to outcomes such as the largest molecule toxicity database to date or more than 200 drug safety predictive models. The momentum was recently increased by the involvement of tech giants combined with record rounds of funding. The long-term effects will range from safer and more effective therapies, through the diminished role of pharmaceutical patents, to large-scale collaboration and new business strategies oriented around currently untreatable diseases. The paper notes that while many reviewed resources seem to have overly optimistic future expectations, even a fraction of these developments would alleviate the productivity crisis. Finally, the paper concludes that the focus on pharmaceutical AI put the industry on a trajectory towards another significant disruption: open data sharing and collaboration. △ Less","25 June, 2021",https://arxiv.org/pdf/2107.03896
Semantic Intelligence in Big Data Applications,Valentina Janev,"Today, data is growing at a tremendous rate and, according to the International Data Corporation, it is expected to reach 175 zettabytes by 2025. The International Data Corporation also forecasts that more than 150B devices will be connected across the globe by 2025, most of which will be creating data in real-time, while 90 zettabytes of data will be created by the Internet of Things devices. This vast amount of data creates several new opportunities for modern enterprises, especially for analysing the enterprise value chains in a broader sense. To leverage the potential of real data and build smart applications on top of sensory data, IoT-based systems integrate domain knowledge and context-relevant information. Semantic Intelligence is the process of bridging the semantic gap between human and computer comprehension by teaching a machine to think in terms of object-oriented concepts in the same way as a human does. Semantic intelligence technologies are the most important component in developing artificially intelligent knowledge-based systems since they assist machines in contextually and intelligently integrating and processing resources. This Chapter aims at demystifying semantic intelligence in distributed, enterprise and web-based information systems. It also discusses prominent tools that leverage semantics, handle large data at scale and address challenges (e.g. heterogeneity, interoperability, machine learning explainability) in different industrial applications. △ Less","8 July, 2021",https://arxiv.org/pdf/2107.03853
Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification,Gonzalo Nápoles;Yamisleydi Salgueiro;Isel Grau;Maikel Leon Espinosa,"Machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry. However, the lack of transparency and accountability of most accurate models often hinders their safe use. Thus, there is a clear need for developing explainable artificial intelligence mechanisms. There exist model-agnostic methods that summarize feature contributions, but their interpretability is limited to predictions made by black-box models. An open challenge is to develop models that have intrinsic interpretability and produce their own explanations, even for classes of models that are traditionally considered black boxes like (recurrent) neural networks. In this paper, we propose a Long-Term Cognitive Network for interpretable pattern classification of structured data. Our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process. For supporting the interpretability without affecting the performance, the model incorporates more flexibility through a quasi-nonlinear reasoning rule that allows controlling nonlinearity. Besides, we propose a recurrence-aware decision model that evades the issues posed by the unique fixed point while introducing a deterministic learning algorithm to compute the tunable parameters. The simulations show that our interpretable model obtains competitive results when compared to state-of-the-art white and black-box models. △ Less","23 December, 2021",https://arxiv.org/pdf/2107.03423
Trans4E: Link Prediction on Scholarly Knowledge Graphs,Mojtaba Nayyeri;Gokce Muge Cil;Sahar Vahdati;Francesco Osborne;Mahfuzur Rahman;Simone Angioni;Angelo Salatino;Diego Reforgiato Recupero;Nadezhda Vassilyeva;Enrico Motta;Jens Lehmann,"The incompleteness of Knowledge Graphs (KGs) is a crucial issue affecting the quality of AI-based services. In the scholarly domain, KGs describing research publications typically lack important information, hindering our ability to analyse and predict research dynamics. In recent years, link prediction approaches based on Knowledge Graph Embedding models became the first aid for this issue. In this work, we present Trans4E, a novel embedding model that is particularly fit for KGs which include N to M relations with N\ggM. This is typical for KGs that categorize a large number of entities (e.g., research articles, patents, persons) according to a relatively small set of categories. Trans4E was applied on two large-scale knowledge graphs, the Academia/Industry DynAmics (AIDA) and Microsoft Academic Graph (MAG), for completing the information about Fields of Study (e.g., 'neural networks', 'machine learning', 'artificial intelligence'), and affiliation types (e.g., 'education', 'company', 'government'), improving the scope and accuracy of the resulting data. We evaluated our approach against alternative solutions on AIDA, MAG, and four other benchmarks (FB15k, FB15k-237, WN18, and WN18RR). Trans4E outperforms the other models when using low embedding dimensions and obtains competitive results in high dimensions. △ Less","3 July, 2021",https://arxiv.org/pdf/2107.03297
Levels of explainable artificial intelligence for human-aligned conversational explanations,Richard Dazeley;Peter Vamplew;Cameron Foale;Charlotte Young;Sunil Aryal;Francisco Cruz,"Over the last few years there has been rapid research growth into eXplainable Artificial Intelligence (XAI) and the closely aligned Interpretable Machine Learning (IML). Drivers for this growth include recent legislative changes and increased investments by industry and governments, along with increased concern from the general public. People are affected by autonomous decisions every day and the public need to understand the decision-making process to accept the outcomes. However, the vast majority of the applications of XAI/IML are focused on providing low-level `narrow' explanations of how an individual decision was reached based on a particular datum. While important, these explanations rarely provide insights into an agent's: beliefs and motivations; hypotheses of other (human, animal or AI) agents' intentions; interpretation of external cultural expectations; or, processes used to generate its own explanation. Yet all of these factors, we propose, are essential to providing the explanatory depth that people require to accept and trust the AI's decision-making. This paper aims to define levels of explanation and describe how they can be integrated to create a human-aligned conversational explanation system. In so doing, this paper will survey current approaches and discuss the integration of different technologies to achieve these levels with Broad eXplainable Artificial Intelligence (Broad-XAI), and thereby move towards high-level `strong' explanations. △ Less","7 July, 2021",https://arxiv.org/pdf/2107.03178
"Cellular, Wide-Area, and Non-Terrestrial IoT: A Survey on 5G Advances and the Road Towards 6G",Mojtaba Vaezi;Amin Azari;Saeed R. Khosravirad;Mahyar Shirvanimoghaddam;M. Mahdi Azari;Danai Chasaki;Petar Popovski,"The next wave of wireless technologies is proliferating in connecting things among themselves as well as to humans. In the era of the Internet of things (IoT), billions of sensors, machines, vehicles, drones, and robots will be connected, making the world around us smarter. The IoT will encompass devices that must wirelessly communicate a diverse set of data gathered from the environment for myriad new applications. The ultimate goal is to extract insights from this data and develop solutions that improve quality of life and generate new revenue. Providing large-scale, long-lasting, reliable, and near real-time connectivity is the major challenge in enabling a smart connected world. This paper provides a comprehensive survey on existing and emerging communication solutions for serving IoT applications in the context of cellular, wide-area, as well as non-terrestrial networks. Specifically, wireless technology enhancements for providing IoT access in fifth-generation (5G) and beyond cellular networks, and communication networks over the unlicensed spectrum are presented. Aligned with the main key performance indicators of 5G and beyond 5G networks, we investigate solutions and standards that enable energy efficiency, reliability, low latency, and scalability (connection density) of current and future IoT networks. The solutions include grant-free access and channel coding for short-packet communications, non-orthogonal multiple access, and on-device intelligence. Further, a vision of new paradigm shifts in communication networks in the 2030s is provided, and the integration of the associated new technologies like artificial intelligence, non-terrestrial networks, and new spectra is elaborated. Finally, future research directions toward beyond 5G IoT networks are pointed out. △ Less","7 July, 2021",https://arxiv.org/pdf/2107.03059
Towards Achieving Trust Through Transparency and Ethics (Pre-Print),David Kwan;Luiz Marcio Cysneiros;Julio Cesar Sampaio do Prado Leite,"The ubiquitous presence of software in the products we use, together with Artificial Intelligence in these products, has led to an increasing need for consumer trust. Consumers often lose faith in products, and the lack of Trust propagates to the companies behind them. This is even more so in mission-critical systems such as autonomous vehicles and clinical support systems. This paper follows grounded theory principles to elicit knowledge related to Trust, Ethics, and Transparency. We approach these qualities as Non-Functional Requirements (NFRs), aiming to build catalogs to subsidize the construction of Socially Responsible Software. The corpus we have used was built on a selected collection of literature on Corporate Social Responsibility, with an emphasis on Business Ethics. Our challenge is how to encode the social perspective knowledge, mainly through the view of Corporate Social Responsibility, on how organizations or institutions achieve trustworthiness. Since our ground perspective is that of NFRs, results are presented by a catalogue of Trust as a Non-Functional Requirement, represented as a Softgoal Interdependency Graph (SIG). The SIG language helps software engineers in understanding alternatives they have to improve Trust in software products. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.02959
Principles for Evaluation of AI/ML Model Performance and Robustness,Olivia Brown;Andrew Curtis;Justin Goodwin,"The Department of Defense (DoD) has significantly increased its investment in the design, evaluation, and deployment of Artificial Intelligence and Machine Learning (AI/ML) capabilities to address national security needs. While there are numerous AI/ML successes in the academic and commercial sectors, many of these systems have also been shown to be brittle and nonrobust. In a complex and ever-changing national security environment, it is vital that the DoD establish a sound and methodical process to evaluate the performance and robustness of AI/ML models before these new capabilities are deployed to the field. This paper reviews the AI/ML development process, highlights common best practices for AI/ML model evaluation, and makes recommendations to DoD evaluators to ensure the deployment of robust AI/ML capabilities for national security needs. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.02868
Machine Learning for Network-based Intrusion Detection Systems: an Analysis of the CIDDS-001 Dataset,José Carneiro;Nuno Oliveira;Norberto Sousa;Eva Maia;Isabel Praça,"With the increasing amount of reliance on digital data and computer networks by corporations and the public in general, the occurrence of cyber attacks has become a great threat to the normal functioning of our society. Intrusion detection systems seek to address this threat by preemptively detecting attacks in real time while attempting to block them or minimizing their damage. These systems can function in many ways being some of them based on artificial intelligence methods. Datasets containing both normal network traffic and cyber attacks are used for training these algorithms so that they can learn the underlying patterns of network-based data. The CIDDS-001 is one of the most used datasets for network-based intrusion detection research. Regarding this dataset, in the majority of works published so far, the Class label was used for training machine learning algorithms. However, there is another label in the CIDDS-001, AttackType, that seems very promising for this purpose and remains considerably unexplored. This work seeks to make a comparison between two machine learning models, K-Nearest Neighbours and Random Forest, which were trained with both these labels in order to ascertain whether AttackType can produce reliable results in comparison with the Class label. △ Less","2 July, 2021",https://arxiv.org/pdf/2107.02753
A Model-Driven Approach to Machine Learning and Software Modeling for the IoT,Armin Moin;Moharram Challenger;Atta Badii;Stephan Günnemann,"Models are used in both Software Engineering (SE) and Artificial Intelligence (AI). SE models may specify the architecture at different levels of abstraction and for addressing different concerns at various stages of the software development life-cycle, from early conceptualization and design, to verification, implementation, testing and evolution. However, AI models may provide smart capabilities, such as prediction and decision-making support. For instance, in Machine Learning (ML), which is currently the most popular sub-discipline of AI, mathematical models may learn useful patterns in the observed data and can become capable of making predictions. The goal of this work is to create synergy by bringing models in the said communities together and proposing a holistic approach to model-driven software development for intelligent systems that require ML. We illustrate how software models can become capable of creating and dealing with ML models in a seamless manner. The main focus is on the domain of the Internet of Things (IoT), where both ML and model-driven SE play a key role. In the context of the need to take a Cyber-Physical System-of-Systems perspective of the targeted architecture, an integrated design environment for both SE and ML sub-systems would best support the optimization and overall efficiency of the implementation of the resulting system. In particular, we implement the proposed approach, called ML-Quadrat, based on ThingML, and validate it using a case study from the IoT domain, as well as through an empirical user evaluation. It transpires that the proposed approach is not only feasible, but may also contribute to the performance leap of software development for smart Cyber-Physical Systems (CPS) which are connected to the IoT, as well as an enhanced user experience of the practitioners who use the proposed modeling solution. △ Less","23 November, 2021",https://arxiv.org/pdf/2107.02689
Does Dataset Complexity Matters for Model Explainers?,José Ribeiro;Raíssa Silva;Lucas Cardoso;Ronnie Alves,"Strategies based on Explainable Artificial Intelligence - XAI have emerged in computing to promote a better understanding of predictions made by black box models. Most XAI measures used today explain these types of models, generating attribute rankings aimed at explaining the model, that is, the analysis of Attribute Importance of Model. There is no consensus on which XAI measure generates an overall explainability rank. For this reason, several proposals for tools have emerged (Ciu, Dalex, Eli5, Lofo, Shap and Skater). An experimental benchmark of explainable AI techniques capable of producing global explainability ranks based on tabular data related to different problems and ensemble models are presented herein. Seeking to answer questions such as ""Are the explanations generated by the different measures the same, similar or different?"" and ""How does data complexity play along model explainability?"" The results from the construction of 82 computational models and 592 ranks shed some light on the other side of the problem of explainability: dataset complexity! △ Less","17 November, 2021",https://arxiv.org/pdf/2107.02661
Understanding Consumer Preferences for Explanations Generated by XAI Algorithms,Yanou Ramon;Tom Vermeire;Olivier Toubia;David Martens;Theodoros Evgeniou,"Explaining firm decisions made by algorithms in customer-facing applications is increasingly required by regulators and expected by customers. While the emerging field of Explainable Artificial Intelligence (XAI) has mainly focused on developing algorithms that generate such explanations, there has not yet been sufficient consideration of customers' preferences for various types and formats of explanations. We discuss theoretically and study empirically people's preferences for explanations of algorithmic decisions. We focus on three main attributes that describe automatically-generated explanations from existing XAI algorithms (format, complexity, and specificity), and capture differences across contexts (online targeted advertising vs. loan applications) as well as heterogeneity in users' cognitive styles. Despite their popularity among academics, we find that counterfactual explanations are not popular among users, unless they follow a negative outcome (e.g., loan application was denied). We also find that users are willing to tolerate some complexity in explanations. Finally, our results suggest that preferences for specific (vs. more abstract) explanations are related to the level at which the decision is construed by the user, and to the deliberateness of the user's cognitive style. △ Less","6 July, 2021",https://arxiv.org/pdf/2107.02624
CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation,Minha Kim;Shahroz Tariq;Simon S. Woo,"Over the last few decades, artificial intelligence research has made tremendous strides, but it still heavily relies on fixed datasets in stationary environments. Continual learning is a growing field of research that examines how AI systems can learn sequentially from a continuous stream of linked data in the same way that biological systems do. Simultaneously, fake media such as deepfakes and synthetic face images have emerged as significant to current multimedia technologies. Recently, numerous method has been proposed which can detect deepfakes with high accuracy. However, they suffer significantly due to their reliance on fixed datasets in limited evaluation settings. Therefore, in this work, we apply continuous learning to neural networks' learning dynamics, emphasizing its potential to increase data efficiency significantly. We propose Continual Representation using Distillation (CoReD) method that employs the concept of Continual Learning (CL), Representation Learning (RL), and Knowledge Distillation (KD). We design CoReD to perform sequential domain adaptation tasks on new deepfake and GAN-generated synthetic face datasets, while effectively minimizing the catastrophic forgetting in a teacher-student model setting. Our extensive experimental results demonstrate that our method is efficient at domain adaptation to detect low-quality deepfakes videos and GAN-generated images from several datasets, outperforming the-state-of-art baseline methods. △ Less","5 August, 2021",https://arxiv.org/pdf/2107.02408
Total Nitrogen Estimation in Agricultural Soils via Aerial Multispectral Imaging and LIBS,Md Abir Hossen;Prasoon K Diwaka;Shankarachary Ragi,"Measuring soil health indicators is an important and challenging task that affects farmers' decisions on timing, placement, and quantity of fertilizers applied in the farms. Most existing methods to measure soil health indicators (SHIs) are in-lab wet chemistry or spectroscopy-based methods, which require significant human input and effort, time-consuming, costly, and are low-throughput in nature. To address this challenge, we develop an artificial intelligence (AI)-driven near real-time unmanned aerial vehicle (UAV)-based multispectral sensing (UMS) solution to estimate total nitrogen (TN) of the soil, an important macro-nutrient or SHI that directly affects the crop health. Accurate prediction of soil TN can significantly increase crop yield through informed decision making on the timing of seed planting, and fertilizer quantity and timing. We train two machine learning models including multi-layer perceptron and support vector machine to predict the soil nitrogen using a suite of data classes including multispectral characteristics of the soil and crops in red, near-infrared, and green spectral bands, computed vegetation indices, and environmental variables including air temperature and relative humidity. To generate the ground-truth data or the training data for the machine learning models, we measure the total nitrogen of the soil samples (collected from a farm) using laser-induced breakdown spectroscopy (LIBS). △ Less","5 July, 2021",https://arxiv.org/pdf/2107.02355
Domain Adaptation via CycleGAN for Retina Segmentation in Optical Coherence Tomography,Ricky Chen;Timothy T. Yu;Gavin Xu;Da Ma;Marinko V. Sarunic;Mirza Faisal Beg,"With the FDA approval of Artificial Intelligence (AI) for point-of-care clinical diagnoses, model generalizability is of the utmost importance as clinical decision-making must be domain-agnostic. A method of tackling the problem is to increase the dataset to include images from a multitude of domains; while this technique is ideal, the security requirements of medical data is a major limitation. Additionally, researchers with developed tools benefit from the addition of open-sourced data, but are limited by the difference in domains. Herewith, we investigated the implementation of a Cycle-Consistent Generative Adversarial Networks (CycleGAN) for the domain adaptation of Optical Coherence Tomography (OCT) volumes. This study was done in collaboration with the Biomedical Optics Research Group and Functional & Anatomical Imaging & Shape Analysis Lab at Simon Fraser University. In this study, we investigated a learning-based approach of adapting the domain of a publicly available dataset, UK Biobank dataset (UKB). To evaluate the performance of domain adaptation, we utilized pre-existing retinal layer segmentation tools developed on a different set of RETOUCH OCT data. This study provides insight on state-of-the-art tools for domain adaptation compared to traditional processing techniques as well as a pipeline for adapting publicly available retinal data to the domains previously used by our collaborators. △ Less","5 July, 2021",https://arxiv.org/pdf/2107.02345
A Review of Explainable Artificial Intelligence in Manufacturing,Georgios Sofianidis;Jože M. Rožanec;Dunja Mladenić;Dimosthenis Kyriazis,"The implementation of Artificial Intelligence (AI) systems in the manufacturing domain enables higher production efficiency, outstanding performance, and safer operations, leveraging powerful tools such as deep learning and reinforcement learning techniques. Despite the high accuracy of these models, they are mostly considered black boxes: they are unintelligible to the human. Opaqueness affects trust in the system, a factor that is critical in the context of decision-making. We present an overview of Explainable Artificial Intelligence (XAI) techniques as a means of boosting the transparency of models. We analyze different metrics to evaluate these techniques and describe several application scenarios in the manufacturing domain. △ Less","5 July, 2021",https://arxiv.org/pdf/2107.02295
Agents that Listen: High-Throughput Reinforcement Learning with Multiple Sensory Systems,Shashank Hegde;Anssi Kanervisto;Aleksei Petrenko,"Humans and other intelligent animals evolved highly sophisticated perception systems that combine multiple sensory modalities. On the other hand, state-of-the-art artificial agents rely mostly on visual inputs or structured low-dimensional observations provided by instrumented environments. Learning to act based on combined visual and auditory inputs is still a new topic of research that has not been explored beyond simple scenarios. To facilitate progress in this area we introduce a new version of VizDoom simulator to create a highly efficient learning environment that provides raw audio observations. We study the performance of different model architectures in a series of tasks that require the agent to recognize sounds and execute instructions given in natural language. Finally, we train our agent to play the full game of Doom and find that it can consistently defeat a traditional vision-based adversary. We are currently in the process of merging the augmented simulator with the main ViZDoom code repository. Video demonstrations and experiment code can be found at https://sites.google.com/view/sound-rl. △ Less","5 July, 2021",https://arxiv.org/pdf/2107.02195
Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated,Felix Biessmann;Dionysius Refiano,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability. △ Less","1 July, 2021",https://arxiv.org/pdf/2107.02033
QKSA: Quantum Knowledge Seeking Agent,Aritra Sarkar,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modelling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modelling principle is presented. The various background ideas and a baseline formalism are discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development. △ Less","3 July, 2021",https://arxiv.org/pdf/2107.01429
Memory and attention in deep learning,Hung Le,"Intelligence necessitates memory. Without memory, humans fail to perform various nontrivial tasks such as reading novels, playing games or solving maths. As the ultimate goal of machine learning is to derive intelligent systems that learn and act automatically just like human, memory construction for machine is inevitable. Artificial neural networks model neurons and synapses in the brain by interconnecting computational units via weights, which is a typical class of machine learning algorithms that resembles memory structure. Their descendants with more complicated modeling techniques (a.k.a deep learning) have been successfully applied to many practical problems and demonstrated the importance of memory in the learning process of machinery systems. Recent progresses on modeling memory in deep learning have revolved around external memory constructions, which are highly inspired by computational Turing models and biological neuronal systems. Attention mechanisms are derived to support acquisition and retention operations on the external memory. Despite the lack of theoretical foundations, these approaches have shown promises to help machinery systems reach a higher level of intelligence. The aim of this thesis is to advance the understanding on memory and attention in deep learning. Its contributions include: (i) presenting a collection of taxonomies for memory, (ii) constructing new memory-augmented neural networks (MANNs) that support multiple control and memory units, (iii) introducing variability via memory in sequential generative models, (iv) searching for optimal writing operations to maximise the memorisation capacity in slot-based memory networks, and (v) simulating the Universal Turing Machine via Neural Stored-program Memory-a new kind of external memory for neural networks. △ Less","3 July, 2021",https://arxiv.org/pdf/2107.01390
Collaborative Visual Navigation,Haiyang Wang;Wenguan Wang;Xizhou Zhu;Jifeng Dai;Liwei Wang,"As a fundamental problem for Artificial Intelligence, multi-agent system (MAS) is making rapid progress, mainly driven by multi-agent reinforcement learning (MARL) techniques. However, previous MARL methods largely focused on grid-world like or game environments; MAS in visually rich environments has remained less explored. To narrow this gap and emphasize the crucial role of perception in MAS, we propose a large-scale 3D dataset, CollaVN, for multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed to cooperatively navigate across photo-realistic environments to reach target locations. Diverse MAVN variants are explored to make our problem more general. Moreover, a memory-augmented communication framework is proposed. Each agent is equipped with a private, external memory to persistently store communication information. This allows agents to make better use of their past communication information, enabling more efficient collaboration and robust long-term planning. In our experiments, several baselines and evaluation metrics are designed. We also empirically verify the efficacy of our proposed MARL approach across different MAVN task settings. △ Less","20 July, 2021",https://arxiv.org/pdf/2107.01151
Toward 6G: From New Hardware Design to Wireless Semantic and Goal-Oriented Communication Paradigms,Emilio Calvanese Strinati;Didier Belot;Alexis Falempin;Jean-Baptiste Doré,"Several speculative visions are conjecturing on what 6G services will be able to offer at the horizon of 2030. Nevertheless, the 6G design process is at its preliminary stages. The reality today is that hardware, technologies and new materials required to effectively meet the unprecedented performance targets required for future 6G services and network operation, have not been designed, tested or even do not exist yet. Today, a solid vision on the cost-benefit trade-offs of machine learning and artificial intelligence support for 6G network and services operation optimization is missing. This includes the possible support from hardware efficiency, operation effectiveness and, the immeasurable cost due to data acquisition-transfer-processing. The contribution of this paper is three-fold. This is the first paper deriving crucial 6G key performance indicators on hardware and technology design. Second, we present a new hardware technologies design methodology conceived to enable the effective software-hardware components integration required to meet the challenging performance envisioned for future 6G networks. Third, we suggest a paradigm shift towards goal-oriented and semantic communications, in which a totally new opportunity of joint design of hardware, artificial intelligence and effective communication is offered. The proposed vision is consolidated by our recent results on hardware, technology and machine learning performance. △ Less","25 June, 2021",https://arxiv.org/pdf/2107.01019
Reinforcement Learning Provides a Flexible Approach for Realistic Supply Chain Safety Stock Optimisation,Edward Elson Kosasih;Alexandra Brintrup,"Although safety stock optimisation has been studied for more than 60 years, most companies still use simplistic means to calculate necessary safety stock levels, partly due to the mismatch between existing analytical methods' emphases on deriving provably optimal solutions and companies' preferences to sacrifice optimal results in favour of more realistic problem settings. A newly emerging method from the field of Artificial Intelligence (AI), namely Reinforcement Learning (RL), offers promise in finding optimal solutions while accommodating more realistic problem features. Unlike analytical-based models, RL treats the problem as a black-box simulation environment mitigating against the problem of oversimplifying reality. As such, assumptions on stock keeping policy can be relaxed and a higher number of problem variables can be accommodated. While RL has been popular in other domains, its applications in safety stock optimisation remain scarce. In this paper, we investigate three RL methods, namely, Q-Learning, Temporal Difference Advantage Actor-Critic and Multi-agent Temporal Difference Advantage Actor-Critic for optimising safety stock in a linear chain of independent agents. We find that RL can simultaneously optimise both safety stock level and order quantity parameters of an inventory policy, unlike classical safety stock optimisation models where only safety stock level is optimised while order quantity is predetermined based on simple rules. This allows RL to model more complex supply chain procurement behaviour. However, RL takes longer time to arrive at solutions, necessitating future research on identifying and improving trade-offs between the use of AI and mathematical models are needed. △ Less","2 July, 2021",https://arxiv.org/pdf/2107.00913
Almost Tight Approximation Algorithms for Explainable Clustering,Hossein Esfandiari;Vahab Mirrokni;Shyam Narayanan,"Recently, due to an increasing interest for transparency in artificial intelligence, several methods of explainable machine learning have been developed with the simultaneous goal of accuracy and interpretability by humans. In this paper, we study a recent framework of explainable clustering first suggested by Dasgupta et al.~\cite{dasgupta2020explainable}. Specifically, we focus on the k-means and k-medians problems and provide nearly tight upper and lower bounds. First, we provide an O(\log k \log \log k)-approximation algorithm for explainable k-medians, improving on the best known algorithm of O(k)~\cite{dasgupta2020explainable} and nearly matching the known Ω(\log k) lower bound~\cite{dasgupta2020explainable}. In addition, in low-dimensional spaces d \ll \log k, we show that our algorithm also provides an O(d \log^2 d)-approximate solution for explainable k-medians. This improves over the best known bound of O(d \log k) for low dimensions~\cite{laber2021explainable}, and is a constant for constant dimensional spaces. To complement this, we show a nearly matching Ω(d) lower bound. Next, we study the k-means problem in this context and provide an O(k \log k)-approximation algorithm for explainable k-means, improving over the O(k^2) bound of Dasgupta et al. and the O(d k \log k) bound of \cite{laber2021explainable}. To complement this we provide an almost tight Ω(k) lower bound, improving over the Ω(\log k) lower bound of Dasgupta et al. Given an approximate solution to the classic k-means and k-medians, our algorithm for k-medians runs in time O(kd \log^2 k ) and our algorithm for k-means runs in time O(k^2 d). △ Less","15 July, 2021",https://arxiv.org/pdf/2107.00774
iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis,Xin Liu;Henglin Shi;Haoyu Chen;Zitong Yu;Xiaobai Li;Guoying Zhaoz?,"We introduce a new dataset for the emotional artificial intelligence research: identity-free video dataset for Micro-Gesture Understanding and Emotion analysis (iMiGUE). Different from existing public datasets, iMiGUE focuses on nonverbal body gestures without using any identity information, while the predominant researches of emotion analysis concern sensitive biometric data, like face and speech. Most importantly, iMiGUE focuses on micro-gestures, i.e., unintentional behaviors driven by inner feelings, which are different from ordinary scope of gestures from other gesture datasets which are mostly intentionally performed for illustrative purposes. Furthermore, iMiGUE is designed to evaluate the ability of models to analyze the emotional states by integrating information of recognized micro-gesture, rather than just recognizing prototypes in the sequences separately (or isolatedly). This is because the real need for emotion AI is to understand the emotional states behind gestures in a holistic way. Moreover, to counter for the challenge of imbalanced sample distribution of this dataset, an unsupervised learning method is proposed to capture latent representations from the micro-gesture sequences themselves. We systematically investigate representative methods on this dataset, and comprehensive experimental results reveal several interesting insights from the iMiGUE, e.g., micro-gesture-based analysis can promote emotion understanding. We confirm that the new iMiGUE dataset could advance studies of micro-gesture and emotion AI. △ Less","1 July, 2021",https://arxiv.org/pdf/2107.00285
Automated Detection and Diagnosis of Diabetic Retinopathy: A Comprehensive Survey,Vasudevan Lakshminarayanan;Hoda Kherdfallah;Arya Sarkar;J. Jothi Balaji,"Diabetic Retinopathy (DR) is a leading cause of vision loss in the world,. In the past few Diabetic Retinopathy (DR) is a leading cause of vision loss in the world. In the past few years, Artificial Intelligence (AI) based approaches have been used to detect and grade DR. Early detection enables appropriate treatment and thus prevents vision loss, Both fundus and optical coherence tomography (OCT) images are used to image the retina. With deep learning/machine learning apprroaches it is possible to extract features from the images and detect the presence of DR. Multiple strategies are implemented to detect and grade the presence of DR using classification, segmentation, and hybrid techniques. This review covers the literature dealing with AI approaches to DR that have been published in the open literature over a five year span (2016-2021). In addition a comprehensive list of available DR datasets is reported. Both the PICO (P-patient, I-intervention, C-control O-outcome) and Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA)2009 search strategies were employed. We summarize a total of 114 published articles which conformed to the scope of the review. In addition a list of 43 major datasets is presented. △ Less","30 June, 2021",https://arxiv.org/pdf/2107.00115
6G V2X Technologies and Orchestrated Sensing for Autonomous Driving,Marouan Mizmizi;Mattia Brambilla;Dario Tagliaferri;Christian Mazzucco;Merouane Debbah;Tomasz Mach;Rino Simeone;Silvio Mandelli;Valerio Frascolla;Renato Lombardi;Maurizio Magarini;Monica Nicoli;Umberto Spagnolini,"6G technology targets to revolutionize the mobility industry by revamping the role of wireless connections. In this article, we draw out our vision on an intelligent, cooperative, and sustainable mobility environment of the future, discussing how 6G will positively impact mobility services and applications. The scenario in focus is a densely populated area by smart connected entities that are mutually connected over a 6G virtual bus, which enables access to an extensive and always up-to-date set of context-sensitive information. The augmented dataset is functional to let vehicles engage in adaptive and cooperative learning mechanisms, enabling fully automated functionalities with higher communication integrity and reduced risk of accidents while being a sentient and collaborative processing node of the same ecosystem. Smart sensing and communication technologies are discussed herein, and their convergence is devised by the pervasiveness of artificial intelligence in centralized or distributed and federated network architectures. △ Less","22 May, 2021",https://arxiv.org/pdf/2106.16146
A Comprehensive Survey on STP Approach to Finite Games,Daizhan Cheng;Yuhu Wu;Guodong Zhao;Shihua Fu,"Nowadays the semi-tensor product (STP) approach to finite games has become a promising new direction. This paper provides a comprehensive survey on this prosperous field. After a brief introduction for STP and finite (networked) games, a description for the principle and fundamental technique of STP approach to finite games is presented. Then several problems and recent results about theory and applications of finite games via STP are presented. A brief comment about the potential use of STP to artificial intelligence is also proposed. △ Less","30 June, 2021",https://arxiv.org/pdf/2106.16086
Ethical AI-Powered Regression Test Selection,Per Erik Strandberg;Mirgita Frasheri;Eduard Paul Enoiu,"Test automation is common in software development; often one tests repeatedly to identify regressions. If the amount of test cases is large, one may select a subset and only use the most important test cases. The regression test selection (RTS) could be automated and enhanced with Artificial Intelligence (AI-RTS). This however could introduce ethical challenges. While such challenges in AI are in general well studied, there is a gap with respect to ethical AI-RTS. By exploring the literature and learning from our experiences of developing an industry AI-RTS tool, we contribute to the literature by identifying three challenges (assigning responsibility, bias in decision-making and lack of participation) and three approaches (explicability, supervision and diversity). Additionally, we provide a checklist for ethical AI-RTS to help guide the decision-making of the stakeholders involved in the process. △ Less","30 June, 2021",https://arxiv.org/pdf/2106.16050
Communication conditions in virtual acoustic scenes in an underground station,Ľuboš Hládek;Stephan D. Ewert;Bernhard U. Seeber,"Underground stations are a common communication situation in towns: we talk with friends or colleagues, listen to announcements or shop for titbits while background noise and reverberation are challenging communication. Here, we perform an acoustical analysis of two communication scenes in an underground station in Munich and test speech intelligibility. The acoustical conditions were measured in the station and are compared to simulations in the real-time Simulated Open Field Environment (rtSOFE). We compare binaural room impulse responses measured with an artificial head in the station to modeled impulse responses for free-field auralization via 60 loudspeakers in the rtSOFE. We used the image source method to model early reflections and a set of multi-microphone recordings to model late reverberation. The first communication scene consists of 12 equidistant (1.6 m) horizontally spaced source positions around a listener, simulating different direction-dependent spatial unmasking conditions. The second scene mimics an approaching speaker across six radially spaced source positions (from 1 m to 10 m) with varying direct sound level and thus direct-to-reverberant energy. The acoustic parameters of the underground station show a moderate amount of reverberation (T30 in octave bands was between 2.3 s and 0.6 s and early-decay times between 1.46 s and 0.46 s). The binaural and energetic parameters of the auralization were in a close match to the measurement. Measured speech reception thresholds were within the error of the speech test, letting us to conclude that the auralized simulation reproduces acoustic and perceptually relevant parameters for speech intelligibility with high accuracy. △ Less","2 November, 2021",https://arxiv.org/pdf/2106.15916
A Survey on Neural Speech Synthesis,Xu Tan;Tao Qin;Frank Soong;Tie-Yan Liu,"Text to speech (TTS), or speech synthesis, which aims to synthesize intelligible and natural speech given text, is a hot research topic in speech, language, and machine learning communities and has broad applications in the industry. As the development of deep learning and artificial intelligence, neural network-based TTS has significantly improved the quality of synthesized speech in recent years. In this paper, we conduct a comprehensive survey on neural TTS, aiming to provide a good understanding of current research and future trends. We focus on the key components in neural TTS, including text analysis, acoustic models and vocoders, and several advanced topics, including fast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc. We further summarize resources related to TTS (e.g., datasets, opensource implementations) and discuss future research directions. This survey can serve both academic researchers and industry practitioners working on TTS. △ Less","23 July, 2021",https://arxiv.org/pdf/2106.15561
Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue,Shoya Matsumori;Kosuke Shingyouchi;Yuki Abe;Yosuke Fukuchi;Komei Sugiura;Michita Imai,"Building an interactive artificial intelligence that can ask questions about the real world is one of the biggest challenges for vision and language problems. In particular, goal-oriented visual dialogue, where the aim of the agent is to seek information by asking questions during a turn-taking dialogue, has been gaining scholarly attention recently. While several existing models based on the GuessWhat?! dataset have been proposed, the Questioner typically asks simple category-based questions or absolute spatial questions. This might be problematic for complex scenes where the objects share attributes or in cases where descriptive questions are required to distinguish objects. In this paper, we propose a novel Questioner architecture, called Unified Questioner Transformer (UniQer), for descriptive question generation with referring expressions. In addition, we build a goal-oriented visual dialogue task called CLEVR Ask. It synthesizes complex scenes that require the Questioner to generate descriptive questions. We train our model with two variants of CLEVR Ask datasets. The results of the quantitative and qualitative evaluations show that UniQer outperforms the baseline. △ Less","29 June, 2021",https://arxiv.org/pdf/2106.15550
Coach2vec: autoencoding the playing style of soccer coaches,Paolo Cintia;Luca Pappalardo,"Capturing the playing style of professional soccer coaches is a complex, and yet barely explored, task in sports analytics. Nowadays, the availability of digital data describing every relevant spatio-temporal aspect of soccer matches, allows for capturing and analyzing the playing style of players, teams, and coaches in an automatic way. In this paper, we present coach2vec, a workflow to capture the playing style of professional coaches using match event streams and artificial intelligence. Coach2vec extracts ball possessions from each match, clusters them based on their similarity, and reconstructs the typical ball possessions of coaches. Then, it uses an autoencoder, a type of artificial neural network, to obtain a concise representation (encoding) of the playing style of each coach. Our experiments, conducted on soccer-logs describing the last four seasons of the Italian first division, reveal interesting similarities between prominent coaches, paving the road to the simulation of playing styles and the quantitative comparison of professional coaches. △ Less","29 June, 2021",https://arxiv.org/pdf/2106.15444
High-dimensional separability for one- and few-shot learning,Alexander N. Gorban;Bogdan Grechuk;Evgeny M. Mirkes;Sergey V. Stasenko;Ivan Y. Tyukin,"This work is driven by a practical question: corrections of Artificial Intelligence (AI) errors. These corrections should be quick and non-iterative. To solve this problem without modification of a legacy AI system, we propose special `external' devices, correctors. Elementary correctors consist of two parts, a classifier that separates the situations with high risk of error from the situations in which the legacy AI system works well and a new decision for situations with potential errors. Input signals for the correctors can be the inputs of the legacy AI system, its internal signals, and outputs. If the intrinsic dimensionality of data is high enough then the classifiers for correction of small number of errors can be very simple. According to the blessing of dimensionality effects, even simple and robust Fisher's discriminants can be used for one-shot learning of AI correctors. Stochastic separation theorems provide the mathematical basis for this one-short learning. However, as the number of correctors needed grows, the cluster structure of data becomes important and a new family of stochastic separation theorems is required. We refuse the classical hypothesis of the regularity of the data distribution and assume that the data can have a fine-grained structure with many clusters and peaks in the probability density. New stochastic separation theorems for data with fine-grained structure are formulated and proved. The multi-correctors for granular data are proposed. The advantages of the multi-corrector technology were demonstrated by examples of correcting errors and learning new classes of objects by a deep convolutional neural network on the CIFAR-10 dataset. The key problems of the non-classical high-dimensional data analysis are reviewed together with the basic preprocessing steps including supervised, semi-supervised and domain adaptation Principal Component Analysis. △ Less","22 October, 2021",https://arxiv.org/pdf/2106.15416
Scalable Gaussian Processes for Data-Driven Design using Big Data with Categorical Factors,Liwei Wang;Suraj Yerramilli;Akshay Iyer;Daniel Apley;Ping Zhu;Wei Chen,"Scientific and engineering problems often require the use of artificial intelligence to aid understanding and the search for promising designs. While Gaussian processes (GP) stand out as easy-to-use and interpretable learners, they have difficulties in accommodating big datasets, categorical inputs, and multiple responses, which has become a common challenge for a growing number of data-driven design applications. In this paper, we propose a GP model that utilizes latent variables and functions obtained through variational inference to address the aforementioned challenges simultaneously. The method is built upon the latent variable Gaussian process (LVGP) model where categorical factors are mapped into a continuous latent space to enable GP modeling of mixed-variable datasets. By extending variational inference to LVGP models, the large training dataset is replaced by a small set of inducing points to address the scalability issue. Output response vectors are represented by a linear combination of independent latent functions, forming a flexible kernel structure to handle multiple responses that might have distinct behaviors. Comparative studies demonstrate that the proposed method scales well for large datasets with over 10^4 data points, while outperforming state-of-the-art machine learning methods without requiring much hyperparameter tuning. In addition, an interpretable latent space is obtained to draw insights into the effect of categorical factors, such as those associated with building blocks of architectures and element choices in metamaterial and materials design. Our approach is demonstrated for machine learning of ternary oxide materials and topology optimization of a multiscale compliant mechanism with aperiodic microstructures and multiple materials. △ Less","29 June, 2021",https://arxiv.org/pdf/2106.15356
Deep Learning for Multi-View Stereo via Plane Sweep: A Survey,Qingtian Zhu;Chen Min;Zizhuang Wei;Yisong Chen;Guoping Wang,"3D reconstruction has lately attracted increasing attention due to its wide application in many areas, such as autonomous driving, robotics and virtual reality. As a dominant technique in artificial intelligence, deep learning has been successfully adopted to solve various computer vision problems. However, deep learning for 3D reconstruction is still at its infancy due to its unique challenges and varying pipelines. To stimulate future research, this paper presents a review of recent progress in deep learning methods for Multi-view Stereo (MVS), which is considered as a crucial task of image-based 3D reconstruction. It also presents comparative results on several publicly available datasets, with insightful observations and inspiring future research directions. △ Less","29 July, 2021",https://arxiv.org/pdf/2106.15328
Artificial Intelligence in Minimally Invasive Interventional Treatment,Daniel Ruijters,Minimally invasive image guided treatment procedures often employ advanced image processing algorithms. The recent developments of artificial intelligence algorithms harbor potential to further enhance this domain. In this article we explore several application areas within the minimally invasive treatment space and discuss the deployment of artificial intelligence within these areas. △ Less,"8 June, 2021",https://arxiv.org/pdf/2106.15306
On Board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery,Maria Pia Del Rosso;Alessandro Sebastianelli;Dario Spiller;Pierre Philippe Mathieu;Silvia Liberata Ullo,"In recent years, the growth of Machine Learning (ML) algorithms has raised the number of studies including their applicability in a variety of different scenarios. Among all, one of the hardest ones is the aerospace, due to its peculiar physical requirements. In this context, a feasibility study and a first prototype for an Artificial Intelligence (AI) model to be deployed on board satellites are presented in this work. As a case study, the detection of volcanic eruptions has been investigated as a method to swiftly produce alerts and allow immediate interventions. Two Convolutional Neural Networks (CNNs) have been proposed and designed, showing how to efficiently implement them for identifying the eruptions and at the same time adapting their complexity in order to fit on board requirements. △ Less","28 July, 2021",https://arxiv.org/pdf/2106.15281
FallDeF5: A Fall Detection Framework Using 5G-based Deep Gated Recurrent Unit Networks,Mabrook S. Al-Rakhami;Abdu Gumaei1;Meteb Altaf;Mohammad Mehedi Hassan;Bader Fahad Alkhamees;Khan Muhammad;Giancarlo Fortino,"Fall prevalence is high among elderly people, which is challenging due to the severe consequences of falling. This is why rapid assistance is a critical task. Ambient assisted living (AAL) uses recent technologies such as 5G networks and the internet of medical things (IoMT) to address this research area. Edge computing can reduce the cost of cloud communication, including high latency and bandwidth use, by moving conventional healthcare services and applications closer to end-users. Artificial intelligence (AI) techniques such as deep learning (DL) have been used recently for automatic fall detection, as well as supporting healthcare services. However, DL requires a vast amount of data and substantial processing power to improve its performance for the IoMT linked to the traditional edge computing environment. This research proposes an effective fall detection framework based on DL algorithms and mobile edge computing (MEC) within 5G wireless networks, the aim being to empower IoMT-based healthcare applications. We also propose the use of a deep gated recurrent unit (DGRU) neural network to improve the accuracy of existing DL-based fall detection methods. DGRU has the advantage of dealing with time-series IoMT data, and it can reduce the number of parameters and avoid the vanishing gradient problem. The experimental results on two public datasets show that the DGRU model of the proposed framework achieves higher accuracy rates compared to the current related works on the same datasets. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.15049
How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures,Stylianos I. Venieris;Ioannis Panopoulos;Ilias Leontiadis;Iakovos S. Venieris,"The unprecedented performance of deep neural networks (DNNs) has led to large strides in various Artificial Intelligence (AI) inference tasks, such as object and speech recognition. Nevertheless, deploying such AI models across commodity devices faces significant challenges: large computational cost, multiple performance objectives, hardware heterogeneity and a common need for high accuracy, together pose critical problems to the deployment of DNNs across the various embedded and mobile devices in the wild. As such, we have yet to witness the mainstream usage of state-of-the-art deep learning algorithms across consumer devices. In this paper, we provide preliminary answers to this potentially game-changing question by presenting an array of design techniques for efficient AI systems. We start by examining the major roadblocks when targeting both programmable processors and custom accelerators. Then, we present diverse methods for achieving real-time performance following a cross-stack approach. These span model-, system- and hardware-level techniques, and their combination. Our findings provide illustrative examples of AI systems that do not overburden mobile hardware, while also indicating how they can improve inference accuracy. Moreover, we showcase how custom ASIC- and FPGA-based accelerators can be an enabling factor for next-generation AI applications, such as multi-DNN systems. Collectively, these results highlight the critical need for further exploration as to how the various cross-stack solutions can be best combined in order to bring the latest advances in deep learning close to users, in a robust and efficient manner. △ Less","21 June, 2021",https://arxiv.org/pdf/2106.15021
Early Mobility Recognition for Intensive Care Unit Patients Using Accelerometers,Rex Liu;Sarina A Fazio;Huanle Zhang;Albara Ah Ramli;Xin Liu;Jason Yeates Adams,"With the development of the Internet of Things(IoT) and Artificial Intelligence(AI) technologies, human activity recognition has enabled various applications, such as smart homes and assisted living. In this paper, we target a new healthcare application of human activity recognition, early mobility recognition for Intensive Care Unit(ICU) patients. Early mobility is essential for ICU patients who suffer from long-time immobilization. Our system includes accelerometer-based data collection from ICU patients and an AI model to recognize patients' early mobility. To improve the model accuracy and stability, we identify features that are insensitive to sensor orientations and propose a segment voting process that leverages a majority voting strategy to recognize each segment's activity. Our results show that our system improves model accuracy from 77.78\% to 81.86\% and reduces the model instability (standard deviation) from 16.69\% to 6.92\%, compared to the same AI model without our feature engineering and segment voting process. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.15017
A Survey on Trust Metrics for Autonomous Robotic Systems,Vincenzo DiLuoffo;William R. Michalson,"This paper surveys the area of Trust Metrics related to security for autonomous robotic systems. As the robotics industry undergoes a transformation from programmed, task oriented, systems to Artificial Intelligence-enabled learning, these autonomous systems become vulnerable to several security risks, making a security assessment of these systems of critical importance. Therefore, our focus is on a holistic approach for assessing system trust which requires incorporating system, hardware, software, cognitive robustness, and supplier level trust metrics into a unified model of trust. We set out to determine if there were already trust metrics that defined such a holistic system approach. While there are extensive writings related to various aspects of robotic systems such as, risk management, safety, security assurance and so on, each source only covered subsets of an overall system and did not consistently incorporate the relevant costs in their metrics. This paper attempts to put this prior work into perspective, and to show how it might be extended to develop useful system-level trust metrics for evaluating complex robotic (and other) systems. △ Less","30 June, 2021",https://arxiv.org/pdf/2106.15015
Blockchain and AI-based Solutions to Combat Coronavirus (COVID-19)-like Epidemics: A Survey,Dinh C. Nguyen;Ming Ding;Pubudu N. Pathirana;Aruna Seneviratne,"The beginning of 2020 has seen the emergence of coronavirus outbreak caused by a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled worldwide spread of COVID-19 show the limitations of existing healthcare systems in timely handling public health emergencies. In such contexts, innovative technologies such as blockchain and Artificial Intelligence (AI) have emerged as promising solutions for fighting coronavirus epidemic. In particular, blockchain can combat pandemics by enabling early detection of outbreaks, ensuring the ordering of medical data, and ensuring reliable medical supply chain during the outbreak tracing. Moreover, AI provides intelligent solutions for identifying symptoms caused by coronavirus for treatments and supporting drug manufacturing. Therefore, we present an extensive survey on the use of blockchain and AI for combating COVID-19 epidemics. First, we introduce a new conceptual architecture which integrates blockchain and AI for fighting COVID-19. Then, we survey the latest research efforts on the use of blockchain and AI for fighting COVID-19 in various applications. The newly emerging projects and use cases enabled by these technologies to deal with coronavirus pandemic are also presented. A case study is also provided using federated AI for COVID-19 detection. Finally, we point out challenges and future directions that motivate more research efforts to deal with future coronavirus-like epidemics. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.14631
Software quality: A Historical and Synthetic Content Analysis,Peter Kokol,"Interconnected computers and software systems have become an indispensable part of people's lives, therefore software quality research is becoming more and more important. There have been multiple attempts to synthesize knowledge gained in software quality research, however, they were focused mainly on single aspects of software quality and not to structure the knowledge in a holistic way. The aim of our study was to close this gap. The software quality publications were harvested from the Scopus bibliographic database. The metadata was exported first to CRexlporer, which was employed to identify historical roots, and next to VOSViewer, which was used as a part of the synthetic content analysis. In our study we defined synthetic context analysis as a triangulation of bibliometrics and content analysis. Our search resulted in 14451 publications. The performance bibliometric study showed that the production of research publications relating to software quality is currently following an exponential growth trend and that the software quality research community is growing. The most productive country was the United States and the most productive Institution The Florida Atlantic University. The synthetic content analysis revealed that the published knowledge can be structured into 10 themes, the most important being the themes regarding software quality improvement with enhancing software engineering, advanced software testing, and improved defect and fault prediction with machine learning and data mining. According to the analysis of the hot topics, it seems that future research will be directed into developing and using a full specter of new artificial intelligence tools (not just machine learning and data mining) and focusing on how to assure software quality in agile development paradigms. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.14598
Integrate-and-Fire Neurons for Low-Powered Pattern Recognition,Florian Bacho;Dominique Chu,"Embedded systems acquire information about the real world from sensors and process it to make decisions and/or for transmission. In some situations, the relationship between the data and the decision is complex and/or the amount of data to transmit is large (e.g. in biologgers). Artificial Neural Networks (ANNs) can efficiently detect patterns in the input data which makes them suitable for decision making or compression of information for data transmission. However, ANNs require a substantial amount of energy which reduces the lifetime of battery-powered devices. Therefore, the use of Spiking Neural Networks can improve such systems by providing a way to efficiently process sensory data without being too energy-consuming. In this work, we introduce a low-powered neuron model called Integrate-and-Fire which exploits the charge and discharge properties of the capacitor. Using parallel and series RC circuits, we developed a trainable neuron model that can be expressed in a recurrent form. Finally, we trained its simulation with an artificially generated dataset of dog postures and implemented it as hardware that showed promising energetic properties. This paper is the full text of the research, presented at the 20th International Conference on Artificial Intelligence and Soft Computing Web System (ICAISC 2021) △ Less","28 June, 2021",https://arxiv.org/pdf/2106.14596
Modelling Monotonic and Non-Monotonic Attribute Dependencies with Embeddings: A Theoretical Analysis,Steven Schockaert,"During the last decade, entity embeddings have become ubiquitous in Artificial Intelligence. Such embeddings essentially serve as compact but semantically meaningful representations of the entities of interest. In most approaches, vectors are used for representing the entities themselves, as well as for representing their associated attributes. An important advantage of using attribute embeddings is that (some of the) semantic dependencies between the attributes can thus be captured. However, little is known about what kinds of semantic dependencies can be modelled in this way. The aim of this paper is to shed light on this question, focusing on settings where the embedding of an entity is obtained by pooling the embeddings of its known attributes. Our particular focus is on studying the theoretical limitations of different embedding strategies, rather than their ability to effectively learn attribute dependencies in practice. We first show a number of negative results, revealing that some of the most popular embedding models are not able to capture even basic Horn rules. However, we also find that some embedding strategies are capable, in principle, of modelling both monotonic and non-monotonic attribute dependencies. △ Less","14 September, 2021",https://arxiv.org/pdf/2106.14431
Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning,Yihuang Kang;Yi-Wen Chiu;Ming-Yen Lin;Fang-yi Su;Sheng-Tai Huang,"Machine Learning (ML) and its applications have been transforming our lives but it is also creating issues related to the development of fair, accountable, transparent, and ethical Artificial Intelligence. As the ML models are not fully comprehensible yet, it is obvious that we still need humans to be part of algorithmic decision-making processes. In this paper, we consider a ML framework that may accelerate model learning and improve its interpretability by incorporating human experts into the model learning loop. We propose a novel human-in-the-loop ML framework aimed at dealing with learning problems that the cost of data annotation is high and the lack of appropriate data to model the association between the target tasks and the input features. With an application to precision dosing, our experimental results show that the approach can learn interpretable rules from data and may potentially lower experts' workload by replacing data annotation with rule representation editing. The approach may also help remove algorithmic bias by introducing experts' feedback into the iterative model learning process. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.14384
Reward-Based 1-bit Compressed Federated Distillation on Blockchain,Leon Witt;Usama Zafar;KuoYeh Shen;Felix Sattler;Dan Li;Wojciech Samek,"The recent advent of various forms of Federated Knowledge Distillation (FD) paves the way for a new generation of robust and communication-efficient Federated Learning (FL), where mere soft-labels are aggregated, rather than whole gradients of Deep Neural Networks (DNN) as done in previous FL schemes. This security-per-design approach in combination with increasingly performant Internet of Things (IoT) and mobile devices opens up a new realm of possibilities to utilize private data from industries as well as from individuals as input for artificial intelligence model training. Yet in previous FL systems, lack of trust due to the imbalance of power between workers and a central authority, the assumption of altruistic worker participation and the inability to correctly measure and compare contributions of workers hinder this technology from scaling beyond small groups of already entrusted entities towards mass adoption. This work aims to mitigate the aforementioned issues by introducing a novel decentralized federated learning framework where heavily compressed 1-bit soft-labels, resembling 1-hot label predictions, are aggregated on a smart contract. In a context where workers' contributions are now easily comparable, we modify the Peer Truth Serum for Crowdsourcing mechanism (PTSC) for FD to reward honest participation based on peer consistency in an incentive compatible fashion. Due to heavy reductions of both computational complexity and storage, our framework is a fully on-blockchain FL system that is feasible on simple smart contracts and therefore blockchain agnostic. We experimentally test our new framework and validate its theoretical properties. △ Less","27 June, 2021",https://arxiv.org/pdf/2106.14265
Detecting race and gender bias in visual representation of AI on web search engines,Mykola Makhortykh;Aleksandra Urman;Roberto Ulloa,"Web search engines influence perception of social reality by filtering and ranking information. However, their outputs are often subjected to bias that can lead to skewed representation of subjects such as professional occupations or gender. In our paper, we use a mixed-method approach to investigate presence of race and gender bias in representation of artificial intelligence (AI) in image search results coming from six different search engines. Our findings show that search engines prioritize anthropomorphic images of AI that portray it as white, whereas non-white images of AI are present only in non-Western search engines. By contrast, gender representation of AI is more diverse and less skewed towards a specific gender that can be attributed to higher awareness about gender bias in search outputs. Our observations indicate both the the need and the possibility for addressing bias in representation of societally relevant subjects, such as technological innovation, and emphasize the importance of designing new approaches for detecting bias in information retrieval systems. △ Less","26 June, 2021",https://arxiv.org/pdf/2106.14072
Quantum Computing for Artificial Intelligence Based Mobile Network Optimization,Furqan Ahmed;Petri Mähönen,"In this paper, we discuss how certain radio access network optimization problems can be modelled using the concept of constraint satisfaction problems in artificial intelligence, and solved at scale using a quantum computer. As a case study, we discuss root sequence index (RSI) assignment problem - an important LTE/NR physical random access channel configuration related automation use-case. We formulate RSI assignment as quadratic unconstrained binary optimization (QUBO) problem constructed using data ingested from a commercial mobile network, and solve it using a cloud-based commercially available quantum computing platform. Results show that quantum annealing solver can successfully assign conflict-free RSIs. Comparison with well-known heuristics reveals that some classic algorithms are even more effective in terms of solution quality and computation time. The non-quantum advantage is due to the fact that current implementation is a semi-quantum proof-of-concept algorithm. Also, the results depend on the type of quantum computer used. Nevertheless, the proposed framework is highly flexible and holds tremendous potential for harnessing the power of quantum computing in mobile network automation. △ Less","25 June, 2021",https://arxiv.org/pdf/2106.13917
Building Bridges: Generative Artworks to Explore AI Ethics,Ramya Srinivasan;Devi Parikh,"In recent years, there has been an increased emphasis on understanding and mitigating adverse impacts of artificial intelligence (AI) technologies on society. Across academia, industry, and government bodies, a variety of endeavours are being pursued towards enhancing AI ethics. A significant challenge in the design of ethical AI systems is that there are multiple stakeholders in the AI pipeline, each with their own set of constraints and interests. These different perspectives are often not understood, due in part to communication gaps.For example, AI researchers who design and develop AI models are not necessarily aware of the instability induced in consumers' lives by the compounded effects of AI decisions. Educating different stakeholders about their roles and responsibilities in the broader context becomes necessary. In this position paper, we outline some potential ways in which generative artworks can play this role by serving as accessible and powerful educational tools for surfacing different perspectives. We hope to spark interdisciplinary discussions about computational creativity broadly as a tool for enhancing AI ethics. △ Less","25 June, 2021",https://arxiv.org/pdf/2106.13901
Semantic annotation for computational pathology: Multidisciplinary experience and best practice recommendations,Noorul Wahab;Islam M Miligy;Katherine Dodd;Harvir Sahota;Michael Toss;Wenqi Lu;Mostafa Jahanifar;Mohsin Bilal;Simon Graham;Young Park;Giorgos Hadjigeorghiou;Abhir Bhalerao;Ayat Lashen;Asmaa Ibrahim;Ayaka Katayama;Henry O Ebili;Matthew Parkin;Tom Sorell;Shan E Ahmed Raza;Emily Hero;Hesham Eldaly;Yee Wah Tsang;Kishore Gopalakrishnan;David Snead;Emad Rakha,"Recent advances in whole slide imaging (WSI) technology have led to the development of a myriad of computer vision and artificial intelligence (AI) based diagnostic, prognostic, and predictive algorithms. Computational Pathology (CPath) offers an integrated solution to utilize information embedded in pathology WSIs beyond what we obtain through visual assessment. For automated analysis of WSIs and validation of machine learning (ML) models, annotations at the slide, tissue and cellular levels are required. The annotation of important visual constructs in pathology images is an important component of CPath projects. Improper annotations can result in algorithms which are hard to interpret and can potentially produce inaccurate and inconsistent results. Despite the crucial role of annotations in CPath projects, there are no well-defined guidelines or best practices on how annotations should be carried out. In this paper, we address this shortcoming by presenting the experience and best practices acquired during the execution of a large-scale annotation exercise involving a multidisciplinary team of pathologists, ML experts and researchers as part of the Pathology image data Lake for Analytics, Knowledge and Education (PathLAKE) consortium. We present a real-world case study along with examples of different types of annotations, diagnostic algorithm, annotation data dictionary and annotation constructs. The analyses reported in this work highlight best practice recommendations that can be used as annotation guidelines over the lifecycle of a CPath project. △ Less","25 June, 2021",https://arxiv.org/pdf/2106.13689
Creating and Implementing a Smart Speaker,Sanskar Jethi;Avinash Kumar Choudhary;Yash Gupta;Abhishek Chaudhary,"We have seen significant advancements in Artificial Intelligence and Machine Learning in the 21st century. It has enabled a new technology where we can have a human-like conversation with the machines. The most significant use of this speech recognition and contextual understanding technology exists in the form of a Smart Speaker. We have a wide variety of Smart Speaker products available to us. This paper aims to decode its creation and explain the technology that makes these Speakers, ""Smart."" △ Less","30 May, 2021",https://arxiv.org/pdf/2106.13675
Post-Selections in AI and How to Avoid Them,Juyang Weng,"Neural network based Artificial Intelligence (AI) has reported increasing scales in experiments. However, this paper raises a rarely reported stage in such experiments called Post-Selection alter the reader to several possible protocol flaws that may result in misleading results. All AI methods fall into two broad schools, connectionist and symbolic. The Post-Selection fall into two kinds, Post-Selection Using Validation Sets (PSUVS) and Post-Selection Using Test Sets (PSUTS). Each kind has two types of post-selectors, machines and humans. The connectionist school received criticisms for its ""black box"" and now the Post-Selection; but the seemingly ""clean"" symbolic school seems more brittle because of its human PSUTS. This paper first presents a controversial view: all static ""big data"" are non-scalable. We then analyze why error-backprop from randomly initialized weights suffers from severe local minima, why PSUVS lacks cross-validation, why PSUTS violates well-established protocols, and why every paper involved should transparently report the Post-Selection stage. To avoid future pitfalls in AI competitions, this paper proposes a new AI metrics, called developmental errors for all networks trained, under Three Learning Conditions: (1) an incremental learning architecture (due to a ""big data"" flaw), (2) a training experience and (3) a limited amount of computational resources. Developmental Networks avoid Post-Selections because they automatically discover context-rules on the fly by generating emergent Turing machines (not black boxes) that are optimal in the sense of maximum-likelihood across lifetime, conditioned on the Three Learning Conditions. △ Less","14 September, 2021",https://arxiv.org/pdf/2106.13233
CCC/Code 8.7: Applying AI in the Fight Against Modern Slavery,Nadya Bliss;Mark Briers;Alice Eckstein;James Goulding;Daniel P. Lopresti;Anjali Mazumder;Gavin Smith,"On any given day, tens of millions of people find themselves trapped in instances of modern slavery. The terms ""human trafficking,"" ""trafficking in persons,"" and ""modern slavery"" are sometimes used interchangeably to refer to both sex trafficking and forced labor. Human trafficking occurs when a trafficker compels someone to provide labor or services through the use of force, fraud, and/or coercion. The wide range of stakeholders in human trafficking presents major challenges. Direct stakeholders are law enforcement, NGOs and INGOs, businesses, local/planning government authorities, and survivors. Viewed from a very high level, all stakeholders share in a rich network of interactions that produce and consume enormous amounts of information. The problems of making efficient use of such information for the purposes of fighting trafficking while at the same time adhering to community standards of privacy and ethics are formidable. At the same time they help us, technologies that increase surveillance of populations can also undermine basic human rights. In early March 2020, the Computing Community Consortium (CCC), in collaboration with the Code 8.7 Initiative, brought together over fifty members of the computing research community along with anti-slavery practitioners and survivors to lay out a research roadmap. The primary goal was to explore ways in which long-range research in artificial intelligence (AI) could be applied to the fight against human trafficking. Building on the kickoff Code 8.7 conference held at the headquarters of the United Nations in February 2019, the focus for this workshop was to link the ambitious goals outlined in the A 20-Year Community Roadmap for Artificial Intelligence Research in the US (AI Roadmap) to challenges vital in achieving the UN's Sustainable Development Goal Target 8.7, the elimination of modern slavery. △ Less","24 June, 2021",https://arxiv.org/pdf/2106.13186
Towards Fully Interpretable Deep Neural Networks: Are We There Yet?,Sandareka Wickramanayake;Wynne Hsu;Mong Li Lee,"Despite the remarkable performance, Deep Neural Networks (DNNs) behave as black-boxes hindering user trust in Artificial Intelligence (AI) systems. Research on opening black-box DNN can be broadly categorized into post-hoc methods and inherently interpretable DNNs. While many surveys have been conducted on post-hoc interpretation methods, little effort is devoted to inherently interpretable DNNs. This paper provides a review of existing methods to develop DNNs with intrinsic interpretability, with a focus on Convolutional Neural Networks (CNNs). The aim is to understand the current progress towards fully interpretable DNNs that can cater to different interpretation requirements. Finally, we identify gaps in current work and suggest potential research directions. △ Less","24 June, 2021",https://arxiv.org/pdf/2106.13164
Human-in-the-loop model explanation via verbatim boundary identification in generated neighborhoods,Xianlong Zeng;Fanghao Song;Zhongen Li;Krerkkiat Chusap;Chang Liu,"The black-box nature of machine learning models limits their use in case-critical applications, raising faithful and ethical concerns that lead to trust crises. One possible way to mitigate this issue is to understand how a (mispredicted) decision is carved out from the decision boundary. This paper presents a human-in-the-loop approach to explain machine learning models using verbatim neighborhood manifestation. Contrary to most of the current eXplainable Artificial Intelligence (XAI) systems, which provide hit-or-miss approximate explanations, our approach generates the local decision boundary of the given instance and enables human intelligence to conclude the model behavior. Our method can be divided into three stages: 1) a neighborhood generation stage, which generates instances based on the given sample; 2) a classification stage, which yields classifications on the generated instances to carve out the local decision boundary and delineate the model behavior; and 3) a human-in-the-loop stage, which involves human to refine and explore the neighborhood of interest. In the generation stage, a generative model is used to generate the plausible synthetic neighbors around the given instance. After the classification stage, the classified neighbor instances provide a multifaceted understanding of the model behavior. Three intervention points are provided in the human-in-the-loop stage, enabling humans to leverage their own intelligence to interpret the model behavior. Several experiments on two datasets are conducted, and the experimental results demonstrate the potential of our proposed approach for boosting human understanding of the complex machine learning model. △ Less","24 June, 2021",https://arxiv.org/pdf/2106.13093
A Systematic Collection of Medical Image Datasets for Deep Learning,Johann Li;Guangming Zhu;Cong Hua;Mingtao Feng;BasheerBennamoun;Ping Li;Xiaoyuan Lu;Juan Song;Peiyi Shen;Xu Xu;Lin Mei;Liang Zhang;Syed Afaq Ali Shah;Mohammed Bennamoun,"The astounding success made by artificial intelligence (AI) in healthcare and other fields proves that AI can achieve human-like performance. However, success always comes with challenges. Deep learning algorithms are data-dependent and require large datasets for training. The lack of data in the medical imaging field creates a bottleneck for the application of deep learning to medical image analysis. Medical image acquisition, annotation, and analysis are costly, and their usage is constrained by ethical restrictions. They also require many resources, such as human expertise and funding. That makes it difficult for non-medical researchers to have access to useful and large medical data. Thus, as comprehensive as possible, this paper provides a collection of medical image datasets with their associated challenges for deep learning research. We have collected information of around three hundred datasets and challenges mainly reported between 2013 and 2020 and categorized them into four categories: head & neck, chest & abdomen, pathology & blood, and ``others''. Our paper has three purposes: 1) to provide a most up to date and complete list that can be used as a universal reference to easily find the datasets for clinical image analysis, 2) to guide researchers on the methodology to test and evaluate their methods' performance and robustness on relevant datasets, 3) to provide a ``route'' to relevant algorithms for the relevant medical topics, and challenge leaderboards. △ Less","24 June, 2021",https://arxiv.org/pdf/2106.12864
How Well do Feature Visualizations Support Causal Understanding of CNN Activations?,Roland S. Zimmermann;Judy Borowski;Robert Geirhos;Matthias Bethge;Thomas S. A. Wallis;Wieland Brendel,"A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These synthetic feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating natural dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task (68 \pm 4% accuracy; baseline performance without any visualizations is 60 \pm 3%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance (66\pm3% to 67 \pm3% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better ""causal understanding"" of unit activations than simple alternative visualizations. △ Less","12 November, 2021",https://arxiv.org/pdf/2106.12447
Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation,Esther Puyol-Anton;Bram Ruijsink;Stefan K. Piechnik;Stefan Neubauer;Steffen E. Petersen;Reza Razavi;Andrew P. King,"The subject of ""fairness"" in artificial intelligence (AI) refers to assessing AI algorithms for potential bias based on demographic characteristics such as race and gender, and the development of algorithms to address this bias. Most applications to date have been in computer vision, although some work in healthcare has started to emerge. The use of deep learning (DL) in cardiac MR segmentation has led to impressive results in recent years, and such techniques are starting to be translated into clinical practice. However, no work has yet investigated the fairness of such models. In this work, we perform such an analysis for racial/gender groups, focusing on the problem of training data imbalance, using a nnU-Net model trained and evaluated on cine short axis cardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from 6 different racial groups. We find statistically significant differences in Dice performance between different racial groups. To reduce the racial bias, we investigated three strategies: (1) stratified batch sampling, in which batch sampling is stratified to ensure balance between racial groups; (2) fair meta-learning for segmentation, in which a DL classifier is trained to classify race and jointly optimized with the segmentation model; and (3) protected group models, in which a different segmentation model is trained for each racial group. We also compared the results to the scenario where we have a perfectly balanced database. To assess fairness we used the standard deviation (SD) and skewed error ratio (SER) of the average Dice values. Our results demonstrate that the racial bias results from the use of imbalanced training data, and that all proposed bias mitigation strategies improved fairness, with the best SD and SER resulting from the use of protected group models. △ Less","1 July, 2021",https://arxiv.org/pdf/2106.12387
"PatentNet: A Large-Scale Incomplete Multiview, Multimodal, Multilabel Industrial Goods Image Database",Fangyuan Lei;Da Huang;Jianjian Jiang;Ruijun Ma;Senhong Wang;Jiangzhong Cao;Yusen Lin;Qingyun Dai,"In deep learning area, large-scale image datasets bring a breakthrough in the success of object recognition and retrieval. Nowadays, as the embodiment of innovation, the diversity of the industrial goods is significantly larger, in which the incomplete multiview, multimodal and multilabel are different from the traditional dataset. In this paper, we introduce an industrial goods dataset, namely PatentNet, with numerous highly diverse, accurate and detailed annotations of industrial goods images, and corresponding texts. In PatentNet, the images and texts are sourced from design patent. Within over 6M images and corresponding texts of industrial goods labeled manually checked by professionals, PatentNet is the first ongoing industrial goods image database whose varieties are wider than industrial goods datasets used previously for benchmarking. PatentNet organizes millions of images into 32 classes and 219 subclasses based on the Locarno Classification Agreement. Through extensive experiments on image classification, image retrieval and incomplete multiview clustering, we demonstrate that our PatentNet is much more diverse, complex, and challenging, enjoying higher potentials than existing industrial image datasets. Furthermore, the characteristics of incomplete multiview, multimodal and multilabel in PatentNet are able to offer unparalleled opportunities in the artificial intelligence community and beyond. △ Less","22 June, 2021",https://arxiv.org/pdf/2106.12139
High Performance Optimization at the Door of the Exascale,Claude Tadonki,"quest for processing speed potential. In fact, we always get a fraction of the technically available computing power (so-called {\em theoretical peak}), and the gap is likely to go hand-to-hand with the hardware complexity of the target system. Among the key aspects of this complexity, we have: the {\em heterogeneity} of the computing units, the {\em memory hierarchy and partitioning} including the non-uniform memory access (NUMA) configuration, and the {\em interconnect} for data exchanges among the computing nodes. Scientific investigations and cutting-edge technical activities should ideally scale-up with respect to sustained performance. The special case of quantitative approaches for solving (large-scale) problems deserves a special focus. Indeed, most of common real-life problems, even when considering the artificial intelligence paradigm, rely on optimization techniques for the main kernels of algorithmic solutions. Mathematical programming and pure combinatorial methods are not easy to implement efficiently on large-scale supercomputers because of {\em irregular control flow}, {\em complex memory access patterns}, {\em heterogeneous kernels}, {\em numerical issues}, to name a few. We describe and examine our thoughts from the standpoint of large-scale supercomputers. △ Less","22 June, 2021",https://arxiv.org/pdf/2106.11819
Trinity: A No-Code AI platform for complex spatial datasets,C. V. Krishnakumar Iyer;Feili Hou;Henry Wang;Yonghong Wang;Kay Oh;Swetava Ganguli;Vipul Pandey,"We present a no-code Artificial Intelligence (AI) platform called Trinity with the main design goal of enabling both machine learning researchers and non-technical geospatial domain experts to experiment with domain-specific signals and datasets for solving a variety of complex problems on their own. This versatility to solve diverse problems is achieved by transforming complex Spatio-temporal datasets to make them consumable by standard deep learning models, in this case, Convolutional Neural Networks (CNNs), and giving the ability to formulate disparate problems in a standard way, eg. semantic segmentation. With an intuitive user interface, a feature store that hosts derivatives of complex feature engineering, a deep learning kernel, and a scalable data processing mechanism, Trinity provides a powerful platform for domain experts to share the stage with scientists and engineers in solving business-critical problems. It enables quick prototyping, rapid experimentation and reduces the time to production by standardizing model building and deployment. In this paper, we present our motivation behind Trinity and its design along with showcasing sample applications to motivate the idea of lowering the bar to using AI. △ Less","1 July, 2021",https://arxiv.org/pdf/2106.11756
ESR: Ethics and Society Review of Artificial Intelligence Research,Michael S. Bernstein;Margaret Levi;David Magnus;Betsy Rajala;Debra Satz;Charla Waeiss,"Artificial intelligence (AI) research is routinely criticized for its real and potential impacts on society, and we lack adequate institutional responses to this criticism and to the responsibility that it reflects. AI research often falls outside the purview of existing feedback mechanisms such as the Institutional Review Board (IRB), which are designed to evaluate harms to human subjects rather than harms to human society. In response, we have developed the Ethics and Society Review board (ESR), a feedback panel that works with researchers to mitigate negative ethical and societal aspects of AI research. The ESR's main insight is to serve as a requirement for funding: researchers cannot receive grant funding from a major AI funding program at our university until the researchers complete the ESR process for the proposal. In this article, we describe the ESR as we have designed and run it over its first year across 41 proposals. We analyze aggregate ESR feedback on these proposals, finding that the panel most commonly identifies issues of harms to minority groups, inclusion of diverse stakeholders in the research plan, dual use, and representation in data. Surveys and interviews of researchers who interacted with the ESR found that 58% felt that it had influenced the design of their research project, 100% are willing to continue submitting future projects to the ESR, and that they sought additional scaffolding for reasoning through ethics and society issues. △ Less","9 July, 2021",https://arxiv.org/pdf/2106.11521
A Logical Neural Network Structure With More Direct Mapping From Logical Relations,Gang Wang,"Logical relations widely exist in human activities. Human use them for making judgement and decision according to various conditions, which are embodied in the form of \emph{if-then} rules. As an important kind of cognitive intelligence, it is prerequisite of representing and storing logical relations rightly into computer systems so as to make automatic judgement and decision, especially for high-risk domains like medical diagnosis. However, current numeric ANN (Artificial Neural Network) models are good at perceptual intelligence such as image recognition while they are not good at cognitive intelligence such as logical representation, blocking the further application of ANN. To solve it, researchers have tried to design logical ANN models to represent and store logical relations. Although there are some advances in this research area, recent works still have disadvantages because the structures of these logical ANN models still don't map more directly with logical relations which will cause the corresponding logical relations cannot be read out from their network structures. Therefore, in order to represent logical relations more clearly by the neural network structure and to read out logical relations from it, this paper proposes a novel logical ANN model by designing the new logical neurons and links in demand of logical representation. Compared with the recent works on logical ANN models, this logical ANN model has more clear corresponding with logical relations using the more direct mapping method herein, thus logical relations can be read out following the connection patterns of the network structure. Additionally, less neurons are used. △ Less","21 June, 2021",https://arxiv.org/pdf/2106.11463
A Turing Test for Transparency,Felix Biessmann;Viktor Treu,"A central goal of explainable artificial intelligence (XAI) is to improve the trust relationship in human-AI interaction. One assumption underlying research in transparent AI systems is that explanations help to better assess predictions of machine learning (ML) models, for instance by enabling humans to identify wrong predictions more efficiently. Recent empirical evidence however shows that explanations can have the opposite effect: When presenting explanations of ML predictions humans often tend to trust ML predictions even when these are wrong. Experimental evidence suggests that this effect can be attributed to how intuitive, or human, an AI or explanation appears. This effect challenges the very goal of XAI and implies that responsible usage of transparent AI methods has to consider the ability of humans to distinguish machine generated from human explanations. Here we propose a quantitative metric for XAI methods based on Turing's imitation game, a Turing Test for Transparency. A human interrogator is asked to judge whether an explanation was generated by a human or by an XAI method. Explanations of XAI methods that can not be detected by humans above chance performance in this binary classification task are passing the test. Detecting such explanations is a requirement for assessing and calibrating the trust relationship in human-AI interaction. We present experimental results on a crowd-sourced text classification task demonstrating that even for basic ML models and XAI approaches most participants were not able to differentiate human from machine generated explanations. We discuss ethical and practical implications of our results for applications of transparent ML. △ Less","21 June, 2021",https://arxiv.org/pdf/2106.11394
BEyond observation: an approach for ObjectNav,Daniel V. Ruiz;Eduardo Todt,"With the rise of automation, unmanned vehicles became a hot topic both as commercial products and as a scientific research topic. It composes a multi-disciplinary field of robotics that encompasses embedded systems, control theory, path planning, Simultaneous Localization and Mapping (SLAM), scene reconstruction, and pattern recognition. In this work, we present our exploratory research of how sensor data fusion and state-of-the-art machine learning algorithms can perform the Embodied Artificial Intelligence (E-AI) task called Visual Semantic Navigation. This task, a.k.a Object-Goal Navigation (ObjectNav) consists of autonomous navigation using egocentric visual observations to reach an object belonging to the target semantic class without prior knowledge of the environment. Our method reached fourth place on the Habitat Challenge 2021 ObjectNav on the Minival phase and the Test-Standard Phase. △ Less","21 June, 2021",https://arxiv.org/pdf/2106.11379
Techniques for Symbol Grounding with SATNet,Sever Topan;David Rolnick;Xujie Si,"Many experts argue that the future of artificial intelligence is limited by the field's ability to integrate symbolic logical reasoning into deep learning architectures. The recently proposed differentiable MAXSAT solver, SATNet, was a breakthrough in its capacity to integrate with a traditional neural network and solve visual reasoning problems. For instance, it can learn the rules of Sudoku purely from image examples. Despite its success, SATNet was shown to succumb to a key challenge in neurosymbolic systems known as the Symbol Grounding Problem: the inability to map visual inputs to symbolic variables without explicit supervision (""label leakage""). In this work, we present a self-supervised pre-training pipeline that enables SATNet to overcome this limitation, thus broadening the class of problems that SATNet architectures can solve to include datasets where no intermediary labels are available at all. We demonstrate that our method allows SATNet to attain full accuracy even with a harder problem setup that prevents any label leakage. We additionally introduce a proofreading method that further improves the performance of SATNet architectures, beating the state-of-the-art on Visual Sudoku. △ Less","16 June, 2021",https://arxiv.org/pdf/2106.11072
Paradigm selection for Data Fusion of SAR and Multispectral Sentinel data applied to Land-Cover Classification,Alessandro Sebastianelli;Maria Pia Del Rosso;Pierre Philippe Mathieu;Silvia Liberata Ullo,"Data fusion is a well-known technique, becoming more and more popular in the Artificial Intelligence for Earth Observation (AI4EO) domain mainly due to its ability of reinforcing AI4EO applications by combining multiple data sources and thus bringing better results. On the other hand, like other methods for satellite data analysis, data fusion itself is also benefiting and evolving thanks to the integration of Artificial Intelligence (AI). In this letter, four data fusion paradigms, based on Convolutional Neural Networks (CNNs), are analyzed and implemented. The goals are to provide a systematic procedure for choosing the best data fusion framework, resulting in the best classification results, once the basic structure for the CNN has been defined, and to help interested researchers in their work when data fusion applied to remote sensing is involved. The procedure has been validated for land-cover classification but it can be transferred to other cases. △ Less","18 June, 2021",https://arxiv.org/pdf/2106.11056
Institutionalising Ethics in AI through Broader Impact Requirements,Carina Prunkl;Carolyn Ashurst;Markus Anderljung;Helena Webb;Jan Leike;Allan Dafoe,"Turning principles into practice is one of the most pressing challenges of artificial intelligence (AI) governance. In this article, we reflect on a novel governance initiative by one of the world's largest AI conferences. In 2020, the Conference on Neural Information Processing Systems (NeurIPS) introduced a requirement for submitting authors to include a statement on the broader societal impacts of their research. Drawing insights from similar governance initiatives, including institutional review boards (IRBs) and impact requirements for funding applications, we investigate the risks, challenges and potential benefits of such an initiative. Among the challenges, we list a lack of recognised best practice and procedural transparency, researcher opportunity costs, institutional and social pressures, cognitive biases, and the inherently difficult nature of the task. The potential benefits, on the other hand, include improved anticipation and identification of impacts, better communication with policy and governance experts, and a general strengthening of the norms around responsible research. To maximise the chance of success, we recommend measures to increase transparency, improve guidance, create incentives to engage earnestly with the process, and facilitate public deliberation on the requirement's merits and future. Perhaps the most important contribution from this analysis are the insights we can gain regarding effective community-based governance and the role and responsibility of the AI research community more broadly. △ Less","30 May, 2021",https://arxiv.org/pdf/2106.11039
Teaching Machine Learning in K-12 Computing Education: Potential and Pitfalls,Matti Tedre;Tapani Toivonen;Juho Kahila;Henriikka Vartiainen;Teemu Valtonen;Ilkka Jormanainen;Arnold Pears,"Over the past decades, numerous practical applications of machine learning techniques have shown the potential of data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based ""traditional"" programming is a central aspect and building block in developing next generation computational thinking. △ Less","2 June, 2021",https://arxiv.org/pdf/2106.11034
Hard Choices in Artificial Intelligence,Roel Dobbe;Thomas Krendl Gilbert;Yonatan Mintz,"As AI systems are integrated into high stakes social domains, researchers now examine how to design and operate them in a safe and ethical manner. However, the criteria for identifying and diagnosing safety risks in complex social contexts remain unclear and contested. In this paper, we examine the vagueness in debates about the safety and ethical behavior of AI systems. We show how this vagueness cannot be resolved through mathematical formalism alone, instead requiring deliberation about the politics of development as well as the context of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness in terms of distinct design challenges at key stages in AI system development. The resulting framework of Hard Choices in Artificial Intelligence (HCAI) empowers developers by 1) identifying points of overlap between design decisions and major sociotechnical challenges; 2) motivating the creation of stakeholder feedback channels so that safety issues can be exhaustively addressed. As such, HCAI contributes to a timely debate about the status of AI development in democratic societies, arguing that deliberation should be the goal of AI Safety, not just the procedure by which it is ensured. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.11022
Online Handbook of Argumentation for AI: Volume 2,OHAAI Collaboration;Andreas Brannstrom;Federico Castagna;Theo Duchatelle;Matt Foulis;Timotheus Kampik;Isabelle Kuhlmann;Lars Malmqvist;Mariela Morveli-Espinoza;Jack Mumford;Stipe Pandzic;Robin Schaefer;Luke Thorburn;Andreas Xydis;Antonio Yuste-Ginel;Heng Zheng,"This volume contains revised versions of the papers selected for the second volume of the Online Handbook of Argumentation for AI (OHAAI). Previously, formal theories of argument and argument interaction have been proposed and studied, and this has led to the more recent study of computational models of argument. Argumentation, as a field within artificial intelligence (AI), is highly relevant for researchers interested in symbolic representations of knowledge and defeasible reasoning. The purpose of this handbook is to provide an open access and curated anthology for the argumentation research community. OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI. △ Less","23 June, 2021",https://arxiv.org/pdf/2106.10832
"MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant placements of relay nodes in mission critical wireless networks",Quian Matteo Chen;Alberto Finzi;Toni Mancini;Igor Melatti;Enrico Tronci,"In critical infrastructures like airports, much care has to be devoted in protecting radio communication networks from external electromagnetic interference. Protection of such mission-critical radio communication networks is usually tackled by exploiting radiogoniometers: at least three suitably deployed radiogoniometers, and a gateway gathering information from them, permit to monitor and localise sources of electromagnetic emissions that are not supposed to be present in the monitored area. Typically, radiogoniometers are connected to the gateway through relay nodes. As a result, some degree of fault-tolerance for the network of relay nodes is essential in order to offer a reliable monitoring. On the other hand, deployment of relay nodes is typically quite expensive. As a result, we have two conflicting requirements: minimise costs while guaranteeing a given fault-tolerance. In this paper, we address the problem of computing a deployment for relay nodes that minimises the relay node network cost while at the same time guaranteeing proper working of the network even when some of the relay nodes (up to a given maximum number) become faulty (fault-tolerance). We show that, by means of a computation-intensive pre-processing on a HPC infrastructure, the above optimisation problem can be encoded as a 0/1 Linear Program, becoming suitable to be approached with standard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT solvers. Our problem formulation enables us to present experimental results comparing the performance of these three solving technologies on a real case study of a relay node network deployment in areas of the Leonardo da Vinci Airport in Rome, Italy. △ Less","20 June, 2021",https://arxiv.org/pdf/2106.10685
TinyML: Analysis of Xtensa LX6 microprocessor for Neural Network Applications by ESP32 SoC,Md Ziaul Haque Zim,"In recent decades, Machine Learning (ML) has become extremely important for many computing applications. The pervasiveness of ultra-low-power embedded devices such as ESP32 or ESP32 Cam with tiny Machine Learning (tinyML) applications will enable the mass proliferation of Artificial Intelligent powered Embedded IoT Devices. In the last few years, the microcontroller device (Espressif ESP32) became powerful enough to be used for small/tiny machine learning (tinyML) tasks. The ease of use of platforms like Arduino IDE, MicroPython and TensorFlow Lite (TF) with tinyML application make it an indispensable topic of research for mobile robotics, modern computer science and electrical engineering. The goal of this paper is to analyze the speed of the Xtensa dual core 32-bit LX6 microprocessor by running a neural network application. The different number of inputs (9, 36, 144 and 576) inputted through the different number of neurons in neural networks with one and two hidden layers. Xtensa LX6 microprocessor has been analyzed because it comes inside with Espressif ESP32 and ESP32 Cam which are very easy to use, plug and play IoT device. In this paper speed of the Xtensa LX6 microprocessor in feed-forward mode has been analyzed. △ Less","20 June, 2021",https://arxiv.org/pdf/2106.10652
Vulnerability Detection with Fine-grained Interpretations,Yi Li;Shaohua Wang;Tien N. Nguyen,"Despite the successes of machine learning (ML) and deep learning (DL) based vulnerability detectors (VD), they are limited to providing only the decision on whether a given code is vulnerable or not, without details on what part of the code is relevant to the detected vulnerability. We present IVDetect an interpretable vulnerability detector with the philosophy of using Artificial Intelligence (AI) to detect vulnerabilities, while using Intelligence Assistant (IA) via providing VD interpretations in terms of vulnerable statements. For vulnerability detection, we separately consider the vulnerable statements and their surrounding contexts via data and control dependencies. This allows our model better discriminate vulnerable statements than using the mixture of vulnerable code and~contextual code as in existing approaches. In addition to the coarse-grained vulnerability detection result, we leverage interpretable AI to provide users with fine-grained interpretations that include the sub-graph in the Program Dependency Graph (PDG) with the crucial statements that are relevant to the detected vulnerability. Our empirical evaluation on vulnerability databases shows that IVDetect outperforms the existing DL-based approaches by 43%--84% and 105%--255% in top-10 nDCG and MAP ranking scores. IVDetect correctly points out the vulnerable statements relevant to the vulnerability via its interpretation~in 67% of the cases with a top-5 ranked list. It improves over baseline interpretation models by 12.3%--400% and 9%--400% in accuracy. △ Less","19 June, 2021",https://arxiv.org/pdf/2106.10478
A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents,Muhammad Monjurul Karim;Yu Li;Ruwen Qin;Zhaozheng Yin,"The rapid advancement of sensor technologies and artificial intelligence are creating new opportunities for traffic safety enhancement. Dashboard cameras (dashcams) have been widely deployed on both human driving vehicles and automated driving vehicles. A computational intelligence model that can accurately and promptly predict accidents from the dashcam video will enhance the preparedness for accident prevention. The spatial-temporal interaction of traffic agents is complex. Visual cues for predicting a future accident are embedded deeply in dashcam video data. Therefore, the early anticipation of traffic accidents remains a challenge. Inspired by the attention behavior of humans in visually perceiving accident risks, this paper proposes a Dynamic Spatial-Temporal Attention (DSTA) network for the early accident anticipation from dashcam videos. The DSTA-network learns to select discriminative temporal segments of a video sequence with a Dynamic Temporal Attention (DTA) module. It also learns to focus on the informative spatial regions of frames with a Dynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) is trained jointly with the attention modules to predict the probability of a future accident. The evaluation of the DSTA-network on two benchmark datasets confirms that it has exceeded the state-of-the-art performance. A thorough ablation study that assesses the DSTA-network at the component level reveals how the network achieves such performance. Furthermore, this paper proposes a method to fuse the prediction scores from two complementary models and verifies its effectiveness in further boosting the performance of early accident anticipation. △ Less","20 December, 2021",https://arxiv.org/pdf/2106.10197
All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers,Carmelo Scribano;Davide Sapienza;Giorgia Franchini;Micaela Verucchi;Marko Bertogna,"Combining Natural Language with Vision represents a unique and interesting challenge in the domain of Artificial Intelligence. The AI City Challenge Track 5 for Natural Language-Based Vehicle Retrieval focuses on the problem of combining visual and textual information, applied to a smart-city use case. In this paper, we present All You Can Embed (AYCE), a modular solution to correlate single-vehicle tracking sequences with natural language. The main building blocks of the proposed architecture are (i) BERT to provide an embedding of the textual descriptions, (ii) a convolutional backbone along with a Transformer model to embed the visual information. For the training of the retrieval model, a variation of the Triplet Margin Loss is proposed to learn a distance measure between the visual and language embeddings. The code is publicly available at https://github.com/cscribano/AYCE_2021. △ Less","18 June, 2021",https://arxiv.org/pdf/2106.10153
Enhancing user creativity: Semantic measures for idea generation,Georgi V. Georgiev;Danko D. Georgiev,"Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez-Batet) and semantic similarity (Lin/Sánchez-Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence. △ Less","18 June, 2021",https://arxiv.org/pdf/2106.10131
Towards interpreting computer vision based on transformation invariant optimization,Chen Li;Jinzhe Jiang;Xin Zhang;Tonghuan Zhang;Yaqian Zhao;Dongdong Jiang;RenGang Li,"Interpreting how does deep neural networks (DNNs) make predictions is a vital field in artificial intelligence, which hinders wide applications of DNNs. Visualization of learned representations helps we humans understand the vision of DNNs. In this work, visualized images that can activate the neural network to the target classes are generated by back-propagation method. Here, rotation and scaling operations are applied to introduce the transformation invariance in the image generating process, which we find a significant improvement on visualization effect. Finally, we show some cases that such method can help us to gain insight into neural networks. △ Less","18 June, 2021",https://arxiv.org/pdf/2106.09982
AI-Enabled Ultra-Low-Dose CT Reconstruction,Weiwen Wu;Chuang Niu;Shadi Ebrahimian;Hengyong Yu;Mannu Kalra;Ge Wang,"By the ALARA (As Low As Reasonably Achievable) principle, ultra-low-dose CT reconstruction is a holy grail to minimize cancer risks and genetic damages, especially for children. With the development of medical CT technologies, the iterative algorithms are widely used to reconstruct decent CT images from a low-dose scan. Recently, artificial intelligence (AI) techniques have shown a great promise in further reducing CT radiation dose to the next level. In this paper, we demonstrate that AI-powered CT reconstruction offers diagnostic image quality at an ultra-low-dose level comparable to that of radiography. Specifically, here we develop a Split Unrolled Grid-like Alternative Reconstruction (SUGAR) network, in which deep learning, physical modeling and image prior are integrated. The reconstruction results from clinical datasets show that excellent images can be reconstructed using SUGAR from 36 projections. This approach has a potential to change future healthcare. △ Less","17 June, 2021",https://arxiv.org/pdf/2106.09834
Exploring deterministic frequency deviations with explainable AI,Johannes Kruse;Benjamin Schäfer;Dirk Witthaut,"Deterministic frequency deviations (DFDs) critically affect power grid frequency quality and power system stability. A better understanding of these events is urgently needed as frequency deviations have been growing in the European grid in recent years. DFDs are partially explained by the rapid adjustment of power generation following the intervals of electricity trading, but this intuitive picture fails especially before and around noonday. In this article, we provide a detailed analysis of DFDs and their relation to external features using methods from explainable Artificial Intelligence. We establish a machine learning model that well describes the daily cycle of DFDs and elucidate key interdependencies using SHapley Additive exPlanations (SHAP). Thereby, we identify solar ramps as critical to explain patterns in the Rate of Change of Frequency (RoCoF). △ Less","14 June, 2021",https://arxiv.org/pdf/2106.09538
DeepInsight: Interpretability Assisting Detection of Adversarial Samples on Graphs,Junhao Zhu;Yalu Shan;Jinhuan Wang;Shanqing Yu;Guanrong Chen;Qi Xuan,"With the rapid development of artificial intelligence, a number of machine learning algorithms, such as graph neural networks have been proposed to facilitate network analysis or graph data mining. Although effective, recent studies show that these advanced methods may suffer from adversarial attacks, i.e., they may lose effectiveness when only a small fraction of links are unexpectedly changed. This paper investigates three well-known adversarial attack methods, i.e., Nettack, Meta Attack, and GradArgmax. It is found that different attack methods have their specific attack preferences on changing the target network structures. Such attack pattern are further verified by experimental results on some real-world networks, revealing that generally the top four most important network attributes on detecting adversarial samples suffice to explain the preference of an attack method. Based on these findings, the network attributes are utilized to design machine learning models for adversarial sample detection and attack method recognition with outstanding performance. △ Less","23 June, 2021",https://arxiv.org/pdf/2106.09501
Importance measures derived from random forests: characterisation and extension,Antonio Sutera,"Nowadays new technologies, and especially artificial intelligence, are more and more established in our society. Big data analysis and machine learning, two sub-fields of artificial intelligence, are at the core of many recent breakthroughs in many application fields (e.g., medicine, communication, finance, ...), including some that are strongly related to our day-to-day life (e.g., social networks, computers, smartphones, ...). In machine learning, significant improvements are usually achieved at the price of an increasing computational complexity and thanks to bigger datasets. Currently, cutting-edge models built by the most advanced machine learning algorithms typically became simultaneously very efficient and profitable but also extremely complex. Their complexity is to such an extent that these models are commonly seen as black-boxes providing a prediction or a decision which can not be interpreted or justified. Nevertheless, whether these models are used autonomously or as a simple decision-making support tool, they are already being used in machine learning applications where health and human life are at stake. Therefore, it appears to be an obvious necessity not to blindly believe everything coming out of those models without a detailed understanding of their predictions or decisions. Accordingly, this thesis aims at improving the interpretability of models built by a specific family of machine learning algorithms, the so-called tree-based methods. Several mechanisms have been proposed to interpret these models and we aim along this thesis to improve their understanding, study their properties, and define their limitations. △ Less","21 June, 2021",https://arxiv.org/pdf/2106.09473
Conference proceedings KI4Industry AI for SMEs -- The online congress for practical entry into AI for SMEs,Michael Arnemann;Per Olof Beckemeier;Thomas Bertram;Michael Eder;Maximilian Erschig;Matthias Feiner;Francisco Javier Fernandez Garcia;Frederic Foerster;Ruediger Haas;Martin Kipfmueller;Jan Kotschenreuther;Bernd Langer;Ivan Lozada Rodriguez;Thomas Meibert;Simon Ottenhaus;Stefan Paschek;Lars Pfotzer;Michael M. Roth;Tim Schanz;Philip Scherer;Janine Schwienke;Martin Simon;Robin Tenscher-Philipp,"The Institute of Materials and Processes, IMP, of the University of Applied Sciences in Karlsruhe, Germany in cooperation with VDI Verein Deutscher Ingenieure e.V, AEN Automotive Engineering Network and their cooperation partners present their competences of AI-based solution approaches in the production engineering field. The online congress KI 4 Industry on November 12 and 13, 2020, showed what opportunities the use of artificial intelligence offers for medium-sized manufacturing companies, SMEs, and where potential fields of application lie. The main purpose of KI 4 Industry is to increase the transfer of knowledge, research and technology from universities to small and medium-sized enterprises, to demystify the term AI and to encourage companies to use AI-based solutions in their own value chain or in their products. △ Less","5 August, 2021",https://arxiv.org/pdf/2106.09455
RHNAS: Realizable Hardware and Neural Architecture Search,Yash Akhauri;Adithya Niranjan;J. Pablo Muñoz;Suvadeep Banerjee;Abhijit Davare;Pasquale Cocchini;Anton A. Sorokin;Ravi Iyer;Nilesh Jain,"The rapidly evolving field of Artificial Intelligence necessitates automated approaches to co-design neural network architecture and neural accelerators to maximize system efficiency and address productivity challenges. To enable joint optimization of this vast space, there has been growing interest in differentiable NN-HW co-design. Fully differentiable co-design has reduced the resource requirements for discovering optimized NN-HW configurations, but fail to adapt to general hardware accelerator search spaces. This is due to the existence of non-synthesizable (invalid) designs in the search space of many hardware accelerators. To enable efficient and realizable co-design of configurable hardware accelerators with arbitrary neural network search spaces, we introduce RHNAS. RHNAS is a method that combines reinforcement learning for hardware optimization with differentiable neural architecture search. RHNAS discovers realizable NN-HW designs with 1.84x lower latency and 1.86x lower energy-delay product (EDP) on ImageNet and 2.81x lower latency and 3.30x lower EDP on CIFAR-10 over the default hardware accelerator design. △ Less","16 June, 2021",https://arxiv.org/pdf/2106.09180
An Intelligent Question Answering System based on Power Knowledge Graph,Yachen Tang;Haiyun Han;Xianmao Yu;Jing Zhao;Guangyi Liu;Longfei Wei,"The intelligent question answering (IQA) system can accurately capture users' search intention by understanding the natural language questions, searching relevant content efficiently from a massive knowledge-base, and returning the answer directly to the user. Since the IQA system can save inestimable time and workforce in data search and reasoning, it has received more and more attention in data science and artificial intelligence. This article introduced a domain knowledge graph using the graph database and graph computing technologies from massive heterogeneous data in electric power. It then proposed an IQA system based on the electrical power knowledge graph to extract the intent and constraints of natural interrogation based on the natural language processing (NLP) method, to construct graph data query statements via knowledge reasoning, and to complete the accurate knowledge search and analysis to provide users with an intuitive visualization. This method thoroughly combined knowledge graph and graph computing characteristics, realized high-speed multi-hop knowledge correlation reasoning analysis in tremendous knowledge. The proposed work can also provide a basis for the context-aware intelligent question and answer. △ Less","16 June, 2021",https://arxiv.org/pdf/2106.09013
Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions,Luke Guerdan;Alex Raymond;Hatice Gunes,"As machine learning approaches are increasingly used to augment human decision-making, eXplainable Artificial Intelligence (XAI) research has explored methods for communicating system behavior to humans. However, these approaches often fail to account for the emotional responses of humans as they interact with explanations. Facial affect analysis, which examines human facial expressions of emotions, is one promising lens for understanding how users engage with explanations. Therefore, in this work, we aim to (1) identify which facial affect features are pronounced when people interact with XAI interfaces, and (2) develop a multitask feature embedding for linking facial affect signals with participants' use of explanations. Our analyses and results show that the occurrence and values of facial AU1 and AU4, and Arousal are heightened when participants fail to use explanations effectively. This suggests that facial affect analysis should be incorporated into XAI to personalize explanations to individuals' interaction styles and to adapt explanations based on the difficulty of the task performed. △ Less","15 October, 2021",https://arxiv.org/pdf/2106.08761
A Dataset-Level Geometric Framework for Ensemble Classifiers,Shengli Wu;Weimin Ding,"Ensemble classifiers have been investigated by many in the artificial intelligence and machine learning community. Majority voting and weighted majority voting are two commonly used combination schemes in ensemble learning. However, understanding of them is incomplete at best, with some properties even misunderstood. In this paper, we present a group of properties of these two schemes formally under a dataset-level geometric framework. Two key factors, every component base classifier's performance and dissimilarity between each pair of component classifiers are evaluated by the same metric - the Euclidean distance. Consequently, ensembling becomes a deterministic problem and the performance of an ensemble can be calculated directly by a formula. We prove several theorems of interest and explain their implications for ensembles. In particular, we compare and contrast the effect of the number of component classifiers on these two types of ensemble schemes. Empirical investigation is also conducted to verify the theoretical results when other metrics such as accuracy are used. We believe that the results from this paper are very useful for us to understand the fundamental properties of these two combination schemes and the principles of ensemble classifiers in general. The results are also helpful for us to investigate some issues in ensemble classifiers, such as ensemble performance prediction, selecting a small number of base classifiers to obtain efficient and effective ensembles. △ Less","16 June, 2021",https://arxiv.org/pdf/2106.08658
Structured DropConnect for Uncertainty Inference in Image Classification,Wenqing Zheng;Jiyang Xie;Weidong Liu;Zhanyu Ma,"With the complexity of the network structure, uncertainty inference has become an important task to improve the classification accuracy for artificial intelligence systems. For image classification tasks, we propose a structured DropConnect (SDC) framework to model the output of a deep neural network by a Dirichlet distribution. We introduce a DropConnect strategy on weights in the fully connected layers during training. In test, we split the network into several sub-networks, and then model the Dirichlet distribution by match its moments with the mean and variance of the outputs of these sub-networks. The entropy of the estimated Dirichlet distribution is finally utilized for uncertainty inference. In this paper, this framework is implemented on LeNet5 and VGG16 models for misclassification detection and out-of-distribution detection on MNIST and CIFAR-10 datasets. Experimental results show that the performance of the proposed SDC can be comparable to other uncertainty inference methods. Furthermore, the SDC is adapted well to different network structures with certain generalization capabilities and research prospects. △ Less","1 August, 2021",https://arxiv.org/pdf/2106.08624
Rinascimento: searching the behaviour space of Splendor,Ivan Bravi;Simon Lucas,"The use of Artificial Intelligence (AI) for play-testing is still on the sidelines of main applications of AI in games compared to performance-oriented game-playing. One of the main purposes of play-testing a game is gathering data on the gameplay, highlighting good and bad features of the design of the game, providing useful insight to the game designers for improving the design. Using AI agents has the potential of speeding the process dramatically. The purpose of this research is to map the behavioural space (BSpace) of a game by using a general method. Using the MAP-Elites algorithm we search the hyperparameter space Rinascimento AI agents and map it to the BSpace defined by several behavioural metrics. This methodology was able to highlight both exemplary and degenerated behaviours in the original game design of Splendor and two variations. In particular, the use of event-value functions has generally shown a remarkable improvement in the coverage of the BSpace compared to agents based on classic score-based reward signals. △ Less","15 June, 2021",https://arxiv.org/pdf/2106.08371
"Identifying Roles, Requirements and Responsibilities in Trustworthy AI Systems",Iain Barclay;Will Abramson,"Artificial Intelligence (AI) systems are being deployed around the globe in critical fields such as healthcare and education. In some cases, expert practitioners in these domains are being tasked with introducing or using such systems, but have little or no insight into what data these complex systems are based on, or how they are put together. In this paper, we consider an AI system from the domain practitioner's perspective and identify key roles that are involved in system deployment. We consider the differing requirements and responsibilities of each role, and identify a tension between transparency and privacy that needs to be addressed so that domain practitioners are able to intelligently assess whether a particular AI system is appropriate for use in their domain. △ Less","15 June, 2021",https://arxiv.org/pdf/2106.08258
Zero-sample surface defect detection and classification based on semantic feedback neural network,Yibo Guo;Yiming Fan;Zhiyang Xiang;Haidi Wang;Wenhua Meng;Mingliang Xu,"Defect detection and classification technology has changed from traditional artificial visual inspection to current intelligent automated inspection, but most of the current defect detection methods are training related detection models based on a data-driven approach, taking into account the difficulty of collecting some sample data in the industrial field. We apply zero-shot learning technology to the industrial field. Aiming at the problem of the existing ""Latent Feature Guide Attribute Attention"" (LFGAA) zero-shot image classification network, the output latent attributes and artificially defined attributes are different in the semantic space, which leads to the problem of model performance degradation, proposed an LGFAA network based on semantic feedback, and improved model performance by constructing semantic embedded modules and feedback mechanisms. At the same time, for the common domain shift problem in zero-shot learning, based on the idea of co-training algorithm using the difference information between different views of data to learn from each other, we propose an Ensemble Co-training algorithm, which adaptively reduces the prediction error in image tag embedding from multiple angles. Various experiments conducted on the zero-shot dataset and the cylinder liner dataset in the industrial field provide competitive results. △ Less","15 June, 2021",https://arxiv.org/pdf/2106.07959
Diagnosing the Impact of AI on Radiology in China,Niklas Muennighoff,"Artificial Intelligence will significantly impact the work environment of radiologists. I suggest that up to 50% of a radiologists work in 2021 will be performed by AI-models in 2025. However, it won't increase beyond that 50% level, as radiologists remain key for human-centered aspects of their job. I project that few to no radiologists will be laid off in China due to the existing supply shortage of radiology services in 2021. The application of AI in radiology could contribute 1.7 billion USD to China's GDP in 2025. It will further allow radiologists to start productive work up to four years earlier. AI in radiology will positively impact the health of patients and radiologists themselves. △ Less","15 June, 2021",https://arxiv.org/pdf/2106.07921
Counterfactual Explanations as Interventions in Latent Space,Riccardo Crupi;Alessandro Castelnovo;Daniele Regoli;Beatriz San Miguel Gonzalez,"Explainable Artificial Intelligence (XAI) is a set of techniques that allows the understanding of both technical and non-technical aspects of Artificial Intelligence (AI) systems. XAI is crucial to help satisfying the increasingly important demand of \emph{trustworthy} Artificial Intelligence, characterized by fundamental characteristics such as respect of human autonomy, prevention of harm, transparency, accountability, etc. Within XAI techniques, counterfactual explanations aim to provide to end users a set of features (and their corresponding values) that need to be changed in order to achieve a desired outcome. Current approaches rarely take into account the feasibility of actions needed to achieve the proposed explanations, and in particular they fall short of considering the causal impact of such actions. In this paper, we present Counterfactual Explanations as Interventions in Latent Space (CEILS), a methodology to generate counterfactual explanations capturing by design the underlying causal relations from the data, and at the same time to provide feasible recommendations to reach the proposed profile. Moreover, our methodology has the advantage that it can be set on top of existing counterfactuals generator algorithms, thus minimising the complexity of imposing additional causal constrains. We demonstrate the effectiveness of our approach with a set of different experiments using synthetic and real datasets (including a proprietary dataset of the financial domain). △ Less","8 November, 2021",https://arxiv.org/pdf/2106.07754
Can Explainable AI Explain Unfairness? A Framework for Evaluating Explainable AI,Kiana Alikhademi;Brianna Richardson;Emma Drobina;Juan E. Gilbert,"Many ML models are opaque to humans, producing decisions too complex for humans to easily understand. In response, explainable artificial intelligence (XAI) tools that analyze the inner workings of a model have been created. Despite these tools' strength in translating model behavior, critiques have raised concerns about the impact of XAI tools as a tool for `fairwashing` by misleading users into trusting biased or incorrect models. In this paper, we created a framework for evaluating explainable AI tools with respect to their capabilities for detecting and addressing issues of bias and fairness as well as their capacity to communicate these results to their users clearly. We found that despite their capabilities in simplifying and explaining model behavior, many prominent XAI tools lack features that could be critical in detecting bias. Developers can use our framework to suggest modifications needed in their toolkits to reduce issues likes fairwashing. △ Less","14 June, 2021",https://arxiv.org/pdf/2106.07483
i-Pulse: A NLP based novel approach for employee engagement in logistics organization,Rachit Garg;Arvind W Kiwelekar;Laxman D Netak;Akshay Ghodake,"Although most logistics and freight forwarding organizations, in one way or another, claim to have core values. The engagement of employees is a vast structure that affects almost every part of the company's core environmental values. There is little theoretical knowledge about the relationship between firms and the engagement of employees. Based on research literature, this paper aims to provide a novel approach for insight around employee engagement in a logistics organization by implementing deep natural language processing concepts. The artificial intelligence-enabled solution named Intelligent Pulse (I-Pulse) can evaluate hundreds and thousands of pulse survey comments and provides the actionable insights and gist of employee feedback. I-Pulse allows the stakeholders to think in new ways in their organization, helping them to have a powerful influence on employee engagement, retention, and efficiency. This study is of corresponding interest to researchers and practitioners. △ Less","24 May, 2021",https://arxiv.org/pdf/2106.07341
"Pre-Trained Models: Past, Present and Future",Xu Han;Zhengyan Zhang;Ning Ding;Yuxian Gu;Xiao Liu;Yuqi Huo;Jiezhong Qiu;Yuan Yao;Ao Zhang;Liang Zhang;Wentao Han;Minlie Huang;Qin Jin;Yanyan Lan;Yang Liu;Zhiyuan Liu;Zhiwu Lu;Xipeng Qiu;Ruihua Song;Jie Tang;Ji-Rong Wen;Jinhui Yuan;Wayne Xin Zhao;Jun Zhu,"Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs. △ Less","11 August, 2021",https://arxiv.org/pdf/2106.07139
User Acceptance of Gender Stereotypes in Automated Career Recommendations,Clarice Wang;Kathryn Wang;Andrew Bian;Rashidul Islam;Kamrun Naher Keya;James Foulds;Shimei Pan,"Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g. along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we show that a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using career recommendation as a case study, we build a fair AI career recommender by employing gender debiasing machine learning techniques. Our offline evaluation showed that the debiased recommender makes fairer career recommendations without sacrificing its accuracy. Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that perceived gender disparity is a determining factor for the acceptance of a recommendation. In other words, our results demonstrate we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. △ Less","28 July, 2021",https://arxiv.org/pdf/2106.07112
HistoTransfer: Understanding Transfer Learning for Histopathology,Yash Sharma;Lubaina Ehsan;Sana Syed;Donald E. Brown,"Advancement in digital pathology and artificial intelligence has enabled deep learning-based computer vision techniques for automated disease diagnosis and prognosis. However, WSIs present unique computational and algorithmic challenges. WSIs are gigapixel-sized, making them infeasible to be used directly for training deep neural networks. Hence, for modeling, a two-stage approach is adopted: Patch representations are extracted first, followed by the aggregation for WSI prediction. These approaches require detailed pixel-level annotations for training the patch encoder. However, obtaining these annotations is time-consuming and tedious for medical experts. Transfer learning is used to address this gap and deep learning architectures pre-trained on ImageNet are used for generating patch-level representation. Even though ImageNet differs significantly from histopathology data, pre-trained networks have been shown to perform impressively on histopathology data. Also, progress in self-supervised and multi-task learning coupled with the release of multiple histopathology data has led to the release of histopathology-specific networks. In this work, we compare the performance of features extracted from networks trained on ImageNet and histopathology data. We use an attention pooling network over these extracted features for slide-level aggregation. We investigate if features learned using more complex networks lead to gain in performance. We use a simple top-k sampling approach for fine-tuning framework and study the representation similarity between frozen and fine-tuned networks using Centered Kernel Alignment. Further, to examine if intermediate block representation is better suited for feature extraction and ImageNet architectures are unnecessarily large for histopathology, we truncate the blocks of ResNet18 and DenseNet121 and examine the performance. △ Less","13 June, 2021",https://arxiv.org/pdf/2106.07068
RCURRENCY: Live Digital Asset Trading Using a Recurrent Neural Network-based Forecasting System,Yapeng Jasper Hu;Ralph van Gurp;Ashay Somai;Hugo Kooijman;Jan S. Rellermeyer,"Consistent alpha generation, i.e., maintaining an edge over the market, underpins the ability of asset traders to reliably generate profits. Technical indicators and trading strategies are commonly used tools to determine when to buy/hold/sell assets, yet these are limited by the fact that they operate on known values. Over the past decades, multiple studies have investigated the potential of artificial intelligence in stock trading in conventional markets, with some success. In this paper, we present RCURRENCY, an RNN-based trading engine to predict data in the highly volatile digital asset market which is able to successfully manage an asset portfolio in a live environment. By combining asset value prediction and conventional trading tools, RCURRENCY determines whether to buy, hold or sell digital currencies at a given point in time. Experimental results show that, given the data of an interval t, a prediction with an error of less than 0.5\% of the data at the subsequent interval t+1 can be obtained. Evaluation of the system through backtesting shows that RCURRENCY can be used to successfully not only maintain a stable portfolio of digital assets in a simulated live environment using real historical trading data but even increase the portfolio value over time. △ Less","13 June, 2021",https://arxiv.org/pdf/2106.06972
How Crucial Is It for 6G Networks to Be Autonomous?,Nadia Adem;Ahmed Benfaid;Ramy Harib;Anas Alarabi,"The sixth generation (6G), unlike any of the previous generations, is envisioned by 2030 to connect everything. Moreover, in addition to the new use cases, 6G is expected to support, it will need to provide a superior performance over 5G. The global connectivity, large network dimensions, users heterogeneity, extremely low-power consumption, high throughput, ultrahigh reliability, efficient network operation and maintenance, and low-latency requirements to be met by future networks inevitably necessitate the autonomy of 6G. Intelligence, facilitated mainly by the advancement of artificial intelligence (AI) techniques, is a key to achieve autonomy. In this paper, we provide a bird's-eye view of 6G, its vision, progress, and objectives. Furthermore, we present some technologies that would be mainly enabling intelligent globally connected world. In addition to discussing the role of AI for future wireless communications, we, unlike any other review papers, provide our original results which give early evidence for the viability of achieving 6G networks autonomy through leveraging AI advances. Furthermore, we, very importantly, identify 6G implementation challenges and key innovative techniques that promise to solve them. This article serves as a starting point for learners to acquire more knowledge about 6G and also for researchers to promote more development to the field. △ Less","13 August, 2021",https://arxiv.org/pdf/2106.06949
An Interaction-based Convolutional Neural Network (ICNN) Towards Better Understanding of COVID-19 X-ray Images,Shaw-Hwa Lo;Yiqiao Yin,"The field of Explainable Artificial Intelligence (XAI) aims to build explainable and interpretable machine learning (or deep learning) methods without sacrificing prediction performance. Convolutional Neural Networks (CNNs) have been successful in making predictions, especially in image classification. However, these famous deep learning models use tens of millions of parameters based on a large number of pre-trained filters which have been repurposed from previous data sets. We propose a novel Interaction-based Convolutional Neural Network (ICNN) that does not make assumptions about the relevance of local information. Instead, we use a model-free Influence Score (I-score) to directly extract the influential information from images to form important variable modules. We demonstrate that the proposed method produces state-of-the-art prediction performance of 99.8% on a real-world data set classifying COVID-19 Chest X-ray images without sacrificing the explanatory power of the model. This proposed design can efficiently screen COVID-19 patients before human diagnosis, and will be the benchmark for addressing future XAI problems in large-scale data sets. △ Less","13 June, 2021",https://arxiv.org/pdf/2106.06911
FeSHI: Feature Map Based Stealthy Hardware Intrinsic Attack,Tolulope Odetola;Faiq Khalid;Travis Sandefur;Hawzhin Mohammed;Syed Rafay Hasan,"To reduce the time-to-market and access to state-of-the-art techniques, CNN hardware mapping and deployment on embedded accelerators are often outsourced to untrusted third parties, which is going to be more prevalent in futuristic artificial intelligence of things (AIoT) systems. These AIoT systems anticipate horizontal collaboration among different resource-constrained AIoT node devices, where CNN layers are partitioned and these devices collaboratively compute complex CNN tasks. This horizontal collaboration opens another attack surface to the CNN-based application, like inserting the hardware Trojans (HT) into the embedded accelerators designed for the CNN. Therefore, there is a dire need to explore this attack surface for designing secure embedded hardware accelerators for CNNs. Towards this goal, in this paper, we exploited this attack surface to propose an HT-based attack called FeSHI. Since in horizontal collaboration of RC AIoT devices different sections of CNN architectures are outsourced to different untrusted third parties, the attacker may not know the input image, but it has access to the layer-by-layer output feature maps information for the assigned sections of the CNN architecture. This attack exploits the statistical distribution, i.e., Gaussian distribution, of the layer-by-layer feature maps of the CNN to design two triggers for stealthy HT with a very low probability of triggering. Also, three different novel, stealthy and effective trigger designs are proposed. △ Less","25 August, 2021",https://arxiv.org/pdf/2106.06895
Explaining the Deep Natural Language Processing by Mining Textual Interpretable Features,Francesco Ventura;Salvatore Greco;Daniele Apiletti;Tania Cerquitelli,"Despite the high accuracy offered by state-of-the-art deep natural-language models (e.g. LSTM, BERT), their application in real-life settings is still widely limited, as they behave like a black-box to the end-user. Hence, explainability is rapidly becoming a fundamental requirement of future-generation data-driven systems based on deep-learning approaches. Several attempts to fulfill the existing gap between accuracy and interpretability have been done. However, robust and specialized xAI (Explainable Artificial Intelligence) solutions tailored to deep natural-language models are still missing. We propose a new framework, named T-EBAnO, which provides innovative prediction-local and class-based model-global explanation strategies tailored to black-box deep natural-language models. Given a deep NLP model and the textual input data, T-EBAnO provides an objective, human-readable, domain-specific assessment of the reasons behind the automatic decision-making process. Specifically, the framework extracts sets of interpretable features mining the inner knowledge of the model. Then, it quantifies the influence of each feature during the prediction process by exploiting the novel normalized Perturbation Influence Relation index at the local level and the novel Global Absolute Influence and Global Relative Influence indexes at the global level. The effectiveness and the quality of the local and global explanations obtained with T-EBAnO are proved on (i) a sentiment analysis task performed by a fine-tuned BERT model, and (ii) a toxic comment classification task performed by an LSTM model. △ Less","12 June, 2021",https://arxiv.org/pdf/2106.06697
Robust Representation Learning via Perceptual Similarity Metrics,Saeid Asgari Taghanaki;Kristy Choi;Amir Khasahmadi;Anirudh Goyal,"A fundamental challenge in artificial intelligence is learning useful representations of data that yield good performance on a downstream task, without overfitting to spurious input features. Extracting such task-relevant predictive information is particularly difficult for real-world datasets. In this work, we propose Contrastive Input Morphing (CIM), a representation learning framework that learns input-space transformations of the data to mitigate the effect of irrelevant input features on downstream performance. Our method leverages a perceptual similarity metric via a triplet loss to ensure that the transformation preserves task-relevant information.Empirically, we demonstrate the efficacy of our approach on tasks which typically suffer from the presence of spurious correlations: classification with nuisance information, out-of-distribution generalization, and preservation of subgroup accuracies. We additionally show that CIM is complementary to other mutual information-based representation learning techniques, and demonstrate that it improves the performance of variational information bottleneck (VIB) when used together. △ Less","11 June, 2021",https://arxiv.org/pdf/2106.06620
An adaptive cognitive sensor node for ECG monitoring in the Internet of Medical Things,Matteo Antonio Scrugli;Daniela Loi;Luigi Raffo;Paolo Meloni,"The Internet of Medical Things (IoMT) paradigm is becoming mainstream in multiple clinical trials and healthcare procedures. Cardiovascular diseases monitoring, usually involving electrocardiogram (ECG) traces analysis, is one of the most promising and high-impact applications. Nevertheless, to fully exploit the potential of IoMT in this domain, some steps forward are needed. First, the edge-computing paradigm must be added to the picture. A certain level of near-sensor processing has to be enabled, to improve the scalability, portability, reliability, responsiveness of the IoMT nodes. Second, novel, increasingly accurate, data analysis algorithms, such as those based on artificial intelligence and Deep Learning, must be exploited. To reach these objectives, designers and programmers of IoMT nodes, have to face challenging optimization tasks, in order to execute fairly complex computing tasks on low-power wearable and portable processing systems, with tight power and battery lifetime budgets. In this work, we explore the implementation of a cognitive data analysis algorithm, based on a convolutional neural network trained to classify ECG waveforms, on a resource-constrained microcontroller-based computing platform. To minimize power consumption, we add an adaptivity layer that dynamically manages the hardware and software configuration of the device to adapt it at runtime to the required operating mode. Our experimental results show that adapting the node setup to the workload at runtime can save up to 50% power consumption. Our optimized and quantized neural network reaches an accuracy value higher than 97% for arrhythmia disorders detection on MIT-BIH Arrhythmia dataset. △ Less","23 July, 2021",https://arxiv.org/pdf/2106.06498
AugNet: End-to-End Unsupervised Visual Representation Learning with Image Augmentation,Mingxiang Chen;Zhanguo Chang;Haonan Lu;Bitao Yang;Zhuang Li;Liufang Guo;Zhecheng Wang,"Most of the achievements in artificial intelligence so far were accomplished by supervised learning which requires numerous annotated training data and thus costs innumerable manpower for labeling. Unsupervised learning is one of the effective solutions to overcome such difficulties. In our work, we propose AugNet, a new deep learning training paradigm to learn image features from a collection of unlabeled pictures. We develop a method to construct the similarities between pictures as distance metrics in the embedding space by leveraging the inter-correlation between augmented versions of samples. Our experiments demonstrate that the method is able to represent the image in low dimensional space and performs competitively in downstream tasks such as image classification and image similarity comparison. Specifically, we achieved over 60% and 27% accuracy on the STL10 and CIFAR100 datasets with unsupervised clustering, respectively. Moreover, unlike many deep-learning-based image retrieval algorithms, our approach does not require access to external annotated datasets to train the feature extractor, but still shows comparable or even better feature representation ability and easy-to-use characteristics. In our evaluations, the method outperforms all the state-of-the-art image retrieval algorithms on some out-of-domain image datasets. The code for the model implementation is available at https://github.com/chenmingxiang110/AugNet. △ Less","11 June, 2021",https://arxiv.org/pdf/2106.06250
AI Empowered Resource Management for Future Wireless Networks,Yifei Shen;Jun Zhang;S. H. Song;Khaled B. Letaief,"Resource management plays a pivotal role in wireless networks, which, unfortunately, leads to challenging NP-hard problems. Artificial Intelligence (AI), especially deep learning techniques, has recently emerged as a disruptive technology to solve such challenging problems in a real-time manner. However, although promising results have been reported, practical design guidelines and performance guarantees of AI-based approaches are still missing. In this paper, we endeavor to address two fundamental questions: 1) What are the main advantages of AI-based methods compared with classical techniques; and 2) Which neural network should we choose for a given resource management task. For the first question, four advantages are identified and discussed. For the second question, \emph{optimality gap}, i.e., the gap to the optimal performance, is proposed as a measure for selecting model architectures, as well as, for enabling a theoretical comparison between different AI-based approaches. Specifically, for K-user interference management problem, we theoretically show that graph neural networks (GNNs) are superior to multi-layer perceptrons (MLPs), and the performance gap between these two methods grows with \sqrt{K}. △ Less","11 June, 2021",https://arxiv.org/pdf/2106.06178
Studying the characteristics of scientific communities using individual-level bibliometrics: the case of Big Data research,Xiaozan Lyu;Rodrigo Costas,"Unlike most bibliometric studies focusing on publications, taking Big Data research as a case study, we introduce a novel bibliometric approach to unfold the status of a given scientific community from an individual level perspective. We study the academic age, production, and research focus of the community of authors active in Big Data research. Artificial Intelligence (AI) is selected as a reference area for comparative purposes. Results show that the academic realm of ""Big Data"" is a growing topic with an expanding community of authors, particularly of new authors every year. Compared to AI, Big Data attracts authors with a longer academic age, who can be regarded to have accumulated some publishing experience before entering the community. Despite the highly skewed distribution of productivity amongst researchers in both communities, Big Data authors have higher values of both research focus and production than those of AI. Considering the community size, overall academic age, and persistence of publishing on the topic, our results support the idea of Big Data as a research topic with attractiveness for researchers. We argue that the community-focused indicators proposed in this study could be generalized to investigate the development and dynamics of other research fields and topics. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.05581
CALTeC: Content-Adaptive Linear Tensor Completion for Collaborative Intelligence,Ashiv Dhondea;Robert A. Cohen;Ivan V. Bajić,"In collaborative intelligence, an artificial intelligence (AI) model is typically split between an edge device and the cloud. Feature tensors produced by the edge sub-model are sent to the cloud via an imperfect communication channel. At the cloud side, parts of the feature tensor may be missing due to packet loss. In this paper we propose a method called Content-Adaptive Linear Tensor Completion (CALTeC) to recover the missing feature data. The proposed method is fast, data-adaptive, does not require pre-training, and produces better results than existing methods for tensor data recovery in collaborative intelligence. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.05531
"Brittle AI, Causal Confusion, and Bad Mental Models: Challenges and Successes in the XAI Program",Jeff Druce;James Niehaus;Vanessa Moody;David Jensen;Michael L. Littman,"The advances in artificial intelligence enabled by deep learning architectures are undeniable. In several cases, deep neural network driven models have surpassed human level performance in benchmark autonomy tasks. The underlying policies for these agents, however, are not easily interpretable. In fact, given their underlying deep models, it is impossible to directly understand the mapping from observations to actions for any reasonably complex agent. Producing this supporting technology to ""open the black box"" of these AI systems, while not sacrificing performance, was the fundamental goal of the DARPA XAI program. In our journey through this program, we have several ""big picture"" takeaways: 1) Explanations need to be highly tailored to their scenario; 2) many seemingly high performing RL agents are extremely brittle and are not amendable to explanation; 3) causal models allow for rich explanations, but how to present them isn't always straightforward; and 4) human subjects conjure fantastically wrong mental models for AIs, and these models are often hard to break. This paper discusses the origins of these takeaways, provides amplifying information, and suggestions for future work. △ Less","10 June, 2021",https://arxiv.org/pdf/2106.05506
Artificial Intelligence in Drug Discovery: Applications and Techniques,Jianyuan Deng;Zhibo Yang;Iwao Ojima;Dimitris Samaras;Fusheng Wang,"Artificial intelligence (AI) has been transforming the practice of drug discovery in the past decade. Various AI techniques have been used in a wide range of applications, such as virtual screening and drug design. In this survey, we first give an overview on drug discovery and discuss related applications, which can be reduced to two major tasks, i.e., molecular property prediction and molecule generation. We then discuss common data resources, molecule representations and benchmark platforms. Furthermore, to summarize the progress of AI in drug discovery, we present the relevant AI techniques including model architectures and learning paradigms in the papers surveyed. We expect that this survey will serve as a guide for researchers who are interested in working at the interface of artificial intelligence and drug discovery. We also provide a GitHub repository (https://github.com/dengjianyuan/Survey_AI_Drug_Discovery) with the collection of papers and codes, if applicable, as a learning resource, which is regularly updated. △ Less","2 November, 2021",https://arxiv.org/pdf/2106.05386
DravidianMultiModality: A Dataset for Multi-modal Sentiment Analysis in Tamil and Malayalam,Bharathi Raja Chakravarthi;Jishnu Parameswaran P. K;Premjith B;K. P Soman;Rahul Ponnusamy;Prasanna Kumar Kumaresan;Kingston Pal Thamburaj;John P. McCrae,"Human communication is inherently multimodal and asynchronous. Analyzing human emotions and sentiment is an emerging field of artificial intelligence. We are witnessing an increasing amount of multimodal content in local languages on social media about products and other topics. However, there are not many multimodal resources available for under-resourced Dravidian languages. Our study aims to create a multimodal sentiment analysis dataset for the under-resourced Tamil and Malayalam languages. First, we downloaded product or movies review videos from YouTube for Tamil and Malayalam. Next, we created captions for the videos with the help of annotators. Then we labelled the videos for sentiment, and verified the inter-annotator agreement using Fleiss's Kappa. This is the first multimodal sentiment analysis dataset for Tamil and Malayalam by volunteer annotators. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.04853
Self-Adaptive Swarm System (SASS),Qin Yang,"Distributed artificial intelligence (DAI) studies artificial intelligence entities working together to reason, plan, solve problems, organize behaviors and strategies, make collective decisions and learn. This Ph.D. research proposes a principled Multi-Agent Systems (MAS) cooperation framework -- Self-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap between perception, communication, planning, execution, decision-making, and learning. △ Less","19 August, 2021",https://arxiv.org/pdf/2106.04679
A Survey of Transformers,Tianyang Lin;Yuxin Wang;Xiangyang Liu;Xipeng Qiu,"Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research. △ Less","15 June, 2021",https://arxiv.org/pdf/2106.04554
KIGLIS: Smart Networks for Smart Cities,Daniel Bogdoll;Patrick Matalla;Christoph Füllner;Christian Raack;Shi Li;Tobias Käfer;Stefan Orf;Marc René Zofka;Finn Sartoris;Christoph Schweikert;Thomas Pfeiffer;André Richter;Sebastian Randel;Rene Bonk,"Smart cities will be characterized by a variety of intelligent and networked services, each with specific requirements for the underlying network infrastructure. While smart city architectures and services have been studied extensively, little attention has been paid to the network technology. The KIGLIS research project, consisting of a consortium of companies, universities and research institutions, focuses on artificial intelligence for optimizing fiber-optic networks of a smart city, with a special focus on future mobility applications, such as automated driving. In this paper, we present early results on our process of collecting smart city requirements for communication networks, which will lead towards reference infrastructure and architecture solutions. Finally, we suggest directions in which artificial intelligence will improve smart city networks. △ Less","28 September, 2021",https://arxiv.org/pdf/2106.04549
Optimization of Service Addition in Multilevel Index Model for Edge Computing,Jiayan Gu;Yan Wu;Ashiq Anjum;John Panneerselvam;Yao Lu;Bo Yuan,"With the development of Edge Computing and Artificial Intelligence (AI) technologies, edge devices are witnessed to generate data at unprecedented volume. The Edge Intelligence (EI) has led to the emergence of edge devices in various application domains. The EI can provide efficient services to delay-sensitive applications, where the edge devices are deployed as edge nodes to host the majority of execution, which can effectively manage services and improve service discovery efficiency. The multilevel index model is a well-known model used for indexing service, such a model is being introduced and optimized in the edge environments to efficiently services discovery whilst managing large volumes of data. However, effectively updating the multilevel index model by adding new services timely and precisely in the dynamic Edge Computing environments is still a challenge. Addressing this issue, this paper proposes a designated key selection method to improve the efficiency of adding services in the multilevel index models. Our experimental results show that in the partial index and the full index of multilevel index model, our method reduces the service addition time by around 84% and 76%, respectively when compared with the original key selection method and by around 78% and 66%, respectively when compared with the random selection method. Our proposed method significantly improves the service addition efficiency in the multilevel index model, when compared with existing state-of-the-art key selection methods, without compromising the service retrieval stability to any notable level. △ Less","19 June, 2021",https://arxiv.org/pdf/2106.04494
Computer-Assisted Analysis of Biomedical Images,Leonardo Rundo,"Nowadays, the amount of heterogeneous biomedical data is increasing more and more thanks to novel sensing techniques and high-throughput technologies. In reference to biomedical image analysis, the advances in image acquisition modalities and high-throughput imaging experiments are creating new challenges. This huge information ensemble could overwhelm the analytic capabilities needed by physicians in their daily decision-making tasks as well as by biologists investigating complex biochemical systems. In particular, quantitative imaging methods convey scientifically and clinically relevant information in prediction, prognosis or treatment response assessment, by also considering radiomics approaches. Therefore, the computational analysis of medical and biological images plays a key role in radiology and laboratory applications. In this regard, frameworks based on advanced Machine Learning and Computational Intelligence can significantly improve traditional Image Processing and Pattern Recognition approaches. However, conventional Artificial Intelligence techniques must be tailored to address the unique challenges concerning biomedical imaging data. This thesis aims at proposing novel and advanced computer-assisted methods for biomedical image analysis, also as an instrument in the development of Clinical Decision Support Systems, by always keeping in mind the clinical feasibility of the developed solutions. In conclusion, the ultimate goal of these research studies is to gain clinically and biologically useful insights that can guide differential diagnosis and therapies, leading towards biomedical data integration for personalized medicine. As a matter of fact, the proposed computer-assisted bioimage analysis methods can be beneficial for the definition of imaging biomarkers, as well as for quantitative medicine and biology. △ Less","4 June, 2021",https://arxiv.org/pdf/2106.04381
Defining definition: a Text mining Approach to Define Innovative Technological Fields,Vito Giordano;Filippo Chiarello;Elena Cervelli,"One of the first task of an innovative project is delineating the scope of the project itself or of the product/service to be developed. A wrong scope definition can determine (in the worst case) project failure. A good scope definition become even more relevant in technological intensive innovation projects, nowadays characterized by a highly dynamic multidisciplinary, turbulent and uncertain environment. In these cases, the boundaries of the project are not easily detectable and it is difficult to decide what it is in-scope and out-of-scope. The present work proposes a tool for the scope delineation process, that automatically define an innovative technological field or a new technology. The tool is based on Text Mining algorithm that exploits Elsevier's Scopus abstracts in order to the extract relevant data to define a technological scope. The automatic definition tool is then applied on four case studies: Artificial Intelligence and Data Science. The results show how the tool can provide many crucial information in the definition process of a technological field. In particular for the target technological field (or technology), it provides the definition and other elements related to the target. △ Less","8 June, 2021",https://arxiv.org/pdf/2106.04210
AutoPtosis,Abdullah Aleem;Manoj Prabhakar Nallabothula;Pete Setabutr;Joelle A. Hallak;Darvin Yi,"Blepharoptosis, or ptosis as it is more commonly referred to, is a condition of the eyelid where the upper eyelid droops. The current diagnosis for ptosis involves cumbersome manual measurements that are time-consuming and prone to human error. In this paper, we present AutoPtosis, an artificial intelligence based system with interpretable results for rapid diagnosis of ptosis. We utilize a diverse dataset collected from the Illinois Ophthalmic Database Atlas (I-ODA) to develop a robust deep learning model for prediction and also develop a clinically inspired model that calculates the marginal reflex distance and iris ratio. AutoPtosis achieved 95.5% accuracy on physician verified data that had an equal class balance. The proposed algorithm can help in the rapid and timely diagnosis of ptosis, significantly reduce the burden on the healthcare system, and save the patients and clinics valuable resources. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.03905
Deterministic Iteratively Built KD-Tree with KNN Search for Exact Applications,Aryan Naim;Joseph Bowkett;Sisir Karumanchi;Peyman Tavallali;Brett Kennedy,"K-Nearest Neighbors (KNN) search is a fundamental algorithm in artificial intelligence software with applications in robotics, and autonomous vehicles. These wide-ranging applications utilize KNN either directly for simple classification or combine KNN results as input to other algorithms such as Locally Weighted Learning (LWL). Similar to binary trees, kd-trees become unbalanced as new data is added in online applications which can lead to rapid degradation in search performance unless the tree is rebuilt. Although approximate methods are suitable for graphics applications, which prioritize query speed over query accuracy, they are unsuitable for certain applications in autonomous systems, aeronautics, and robotic manipulation where exact solutions are desired. In this paper, we will attempt to assess the performance of non-recursive deterministic kd-tree functions and KNN functions. We will also present a ""forest of interval kd-trees"" which reduces the number of tree rebuilds, without compromising the exactness of query results. △ Less","7 June, 2021",https://arxiv.org/pdf/2106.03799
Explainable Artificial Intelligence (XAI) for Increasing User Trust in Deep Reinforcement Learning Driven Autonomous Systems,Jeff Druce;Michael Harradon;James Tittle,"We consider the problem of providing users of deep Reinforcement Learning (RL) based systems with a better understanding of when their output can be trusted. We offer an explainable artificial intelligence (XAI) framework that provides a three-fold explanation: a graphical depiction of the systems generalization and performance in the current game state, how well the agent would play in semantically similar environments, and a narrative explanation of what the graphical information implies. We created a user-interface for our XAI framework and evaluated its efficacy via a human-user experiment. The results demonstrate a statistically significant increase in user trust and acceptance of the AI system with explanation, versus the AI system without explanation. △ Less","7 June, 2021",https://arxiv.org/pdf/2106.03775
Mechanism Design for Facility Location Problems: A Survey,Hau Chan;Aris Filos-Ratsikas;Bo Li;Minming Li;Chenhao Wang,"The study of approximate mechanism design for facility location problems has been in the center of research at the intersection of artificial intelligence and economics for the last decades, largely due to its practical importance in various domains, such as social planning and clustering. At a high level, the goal is to design mechanisms to select a set of locations on which to build a set of facilities, aiming to optimize some social objective and ensure desirable properties based on the preferences of strategic agents, who might have incentives to misreport their private information such as their locations. This paper presents a comprehensive survey of the significant progress that has been made since the introduction of the problem, highlighting the different variants and methodologies, as well as the most interesting directions for future research. △ Less","9 June, 2021",https://arxiv.org/pdf/2106.03457
Online Trading Models with Deep Reinforcement Learning in the Forex Market Considering Transaction Costs,Koya Ishikawa;Kazuhide Nakata,"In recent years, a wide range of investment models have been created using artificial intelligence. Automatic trading by artificial intelligence can expand the range of trading methods, such as by conferring the ability to operate 24 hours a day and the ability to trade with high frequency. Automatic trading can also be expected to trade with more information than is available to humans if it can sufficiently consider past data. In this paper, we propose an investment agent based on a deep reinforcement learning model, which is an artificial intelligence model. The model considers the transaction costs involved in actual trading and creates a framework for trading over a long period of time so that it can make a large profit on a single trade. In doing so, it can maximize the profit while keeping transaction costs low. In addition, in consideration of actual operations, we use online learning so that the system can continue to learn by constantly updating the latest online data instead of learning with static data. This makes it possible to trade in non-stationary financial markets by always incorporating current market trend information. △ Less","15 December, 2021",https://arxiv.org/pdf/2106.03035
AI Driven Road Maintenance Inspection,Ratnajit Mukherjee;Haris Iqbal;Shabbir Marzban;Ahmed Badar;Terence Brouns;Shruthi Gowda;Elahe Arani;Bahram Zonooz,"Road infrastructure maintenance inspection is typically a labour-intensive and critical task to ensure the safety of all the road users. In this work, we propose a detailed methodology to use state-of-the-art techniques in artificial intelligence and computer vision to automate a sizeable portion of the maintenance inspection subtasks and reduce the labour costs. The proposed methodology uses state-of-the-art computer vision techniques such as object detection and semantic segmentation to automate inspections on primary road structures such as the road surface, markings, barriers (guardrails) and traffic signs. The models are mostly trained on commercially viable datasets and augmented with proprietary data. We demonstrate that our AI models can not only automate and scale maintenance inspections on primary road structures but also result in higher recall compared to traditional manual inspections. △ Less","4 June, 2021",https://arxiv.org/pdf/2106.02567
Towards Fairness Certification in Artificial Intelligence,Tatiana Tommasi;Silvia Bucci;Barbara Caputo;Pietro Asinari,"Thanks to the great progress of machine learning in the last years, several Artificial Intelligence (AI) techniques have been increasingly moving from the controlled research laboratory settings to our everyday life. AI is clearly supportive in many decision-making scenarios, but when it comes to sensitive areas such as health care, hiring policies, education, banking or justice, with major impact on individuals and society, it becomes crucial to establish guidelines on how to design, develop, deploy and monitor this technology. Indeed the decision rules elaborated by machine learning models are data-driven and there are multiple ways in which discriminatory biases can seep into data. Algorithms trained on those data incur the risk of amplifying prejudices and societal stereotypes by over associating protected attributes such as gender, ethnicity or disabilities with the prediction task. Starting from the extensive experience of the National Metrology Institute on measurement standards and certification roadmaps, and of Politecnico di Torino on machine learning as well as methods for domain bias evaluation and mastering, we propose a first joint effort to define the operational steps needed for AI fairness certification. Specifically we will overview the criteria that should be met by an AI system before coming into official service and the conformity assessment procedures useful to monitor its functioning for fair decisions. △ Less","4 June, 2021",https://arxiv.org/pdf/2106.02498
Classification of Audio Segments in Call Center Recordings using Convolutional Recurrent Neural Networks,Şükrü Ozan,"Detailed statistical analysis of call center recordings is critical in the customer relationship management point of view. With the recent advances in artificial intelligence, many tasks regarding the calculation of call statistics are now performed automatically. This work proposes a neural network framework where the aim is to correctly identify audio segments and classify them as either customer or agent sections. Accurately identifying these sections gives a fair metric for evaluating agents' performances. We inherited the convolutional recurrent neural network (CRNN) architecture commonly used for such problems as music genre classification. We also tested the same architecture's performance, where the previous class information and the gender information of speakers are also added to the training data labels. We saw that CRNN could generalize the training data and perform well on validation data for this problem with and without the gender information. Moreover, even the training was performed using Turkish speech samples; the trained network was proven to achieve high accuracy for call center recordings in other languages like German and English. △ Less","4 June, 2021",https://arxiv.org/pdf/2106.02422
A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals,Ju Sun;Le Peng;Taihui Li;Dyah Adila;Zach Zaiman;Genevieve B. Melton;Nicholas Ingraham;Eric Murray;Daniel Boley;Sean Switzer;John L. Burns;Kun Huang;Tadashi Allen;Scott D. Steenburg;Judy Wawira Gichoya;Erich Kummerfeld;Christopher Tignanelli,"Importance: An artificial intelligence (AI)-based model to predict COVID-19 likelihood from chest x-ray (CXR) findings can serve as an important adjunct to accelerate immediate clinical decision making and improve clinical decision making. Despite significant efforts, many limitations and biases exist in previously developed AI diagnostic models for COVID-19. Utilizing a large set of local and international CXR images, we developed an AI model with high performance on temporal and external validation. Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct, but not replacement, for clinical decision support of COVID-19 diagnosis, which largely hinges on exposure history, signs, and symptoms. While AI-based tools have not yet reached full diagnostic potential in COVID-19, they may still offer valuable information to clinicians taken into consideration along with clinical signs and symptoms. △ Less","6 June, 2021",https://arxiv.org/pdf/2106.02118
Cloud-Enabled High-Altitude Platform Systems: Challenges and Opportunities,Khaleel Mershad;Hayssam Dahrouj;Hadi Sarieddeen;Basem Shihada;Tareq Al-Naffouri;Mohamed-Slim Alouini,"Augmenting ground-level communications with flying networks, such as the high-altitude platform system (HAPS), is among the major innovative initiatives of the next generation of wireless systems (6G). Given HAPS quasi-static positioning at the stratosphere, HAPS-to-ground and HAPS-to-air connectivity frameworks are expected to be prolific in terms of data acquisition and computing, especially given the mild weather and quasi-constant wind speed characteristics of the stratospheric layer. This paper explores the opportunities stemming from the realization of cloud-enabled HAPS in the context of telecommunications applications and services. The paper first advocates for the potential physical advantages of deploying HAPS as flying data-centers, also known as super-macro base stations. The paper then describes various cloud services that can be offered from the HAPS and the merits that can be achieved by this integration, such as enhancing the quality, speed, and range of the offered services. The proposed services span a wide range of fields, including satellites, Internet of Things (IoT), ad hoc networks (such as sensor; vehicular; and aerial networks), gaming, and social networks. For each service, the paper illustrates the methods that would be used by cloud providers to offload the service data to the HAPS and enable the cloud customers to consume the service. The paper further sheds light on the challenges that need to be addressed for realizing practical cloud-enabled HAPS, mainly, those related to high energy, processing power, quality of service (QoS), and security considerations. Finally, the paper discusses some open issues on the topic, namely, HAPS mobility and message routing, HAPS security via blockchain and machine learning, artificial intelligence-based resource allocation in cloud-enabled HAPS, and integration with vertical heterogeneous networks. △ Less","28 June, 2021",https://arxiv.org/pdf/2106.02006
Individual vs. Joint Perception: a Pragmatic Model of Pointing as Communicative Smithian Helping,Kaiwen Jiang;Stephanie Stacy;Chuyu Wei;Adelpha Chan;Federico Rossano;Yixin Zhu;Tao Gao,"The simple gesture of pointing can greatly augment ones ability to comprehend states of the world based on observations. It triggers additional inferences relevant to ones task at hand. We model an agents update to its belief of the world based on individual observations using a partially observable Markov decision process (POMDP), a mainstream artificial intelligence (AI) model of how to act rationally according to beliefs formed through observation. On top of that, we model pointing as a communicative act between agents who have a mutual understanding that the pointed observation must be relevant and interpretable. Our model measures relevance by defining a Smithian Value of Information (SVI) as the utility improvement of the POMDP agent before and after receiving the pointing. We model that agents calculate SVI by using the cognitive theory of Smithian helping as a principle of coordinating separate beliefs for action prediction and action evaluation. We then import SVI into rational speech act (RSA) as the utility function of an utterance. These lead us to a pragmatic model of pointing allowing for contextually flexible interpretations. We demonstrate the power of our Smithian pointing model by extending the Wumpus world, a classic AI task where a hunter hunts a monster with only partial observability of the world. We add another agent as a guide who can only help by marking an observation already perceived by the hunter with a pointing or not, without providing new observations or offering any instrumental help. Our results show that this severely limited and overloaded communication nevertheless significantly improves the hunters performance. The advantage of pointing is indeed due to a computation of relevance based on Smithian helping, as it disappears completely when the task is too difficult or too easy for the guide to help. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.02003
Toward Explainable Users: Using NLP to Enable AI to Understand Users' Perceptions of Cyber Attacks,Faranak Abri;Luis Felipe Gutierrez;Chaitra T. Kulkarni;Akbar Siami Namin;Keith S. Jones,"To understand how end-users conceptualize consequences of cyber security attacks, we performed a card sorting study, a well-known technique in Cognitive Sciences, where participants were free to group the given consequences of chosen cyber attacks into as many categories as they wished using rationales they see fit. The results of the open card sorting study showed a large amount of inter-participant variation making the research team wonder how the consequences of security attacks were comprehended by the participants. As an exploration of whether it is possible to explain user's mental model and behavior through Artificial Intelligence (AI) techniques, the research team compared the card sorting data with the outputs of a number of Natural Language Processing (NLP) techniques with the goal of understanding how participants perceived and interpreted the consequences of cyber attacks written in natural languages. The results of the NLP-based exploration methods revealed an interesting observation implying that participants had mostly employed checking individual keywords in each sentence to group cyber attack consequences together and less considered the semantics behind the description of consequences of cyber attacks. The results reported in this paper are seemingly useful and important for cyber attacks comprehension from user's perspectives. To the best of our knowledge, this paper is the first introducing the use of AI techniques in explaining and modeling users' behavior and their perceptions about a context. The novel idea introduced here is about explaining users using AI. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.01998
EmoDNN: Understanding emotions from short texts through a deep neural network ensemble,Sara Kamran;Raziyeh Zall;Mohammad Reza Kangavari;Saeid Hosseini;Sana Rahmani;Wen Hua,"The latent knowledge in the emotions and the opinions of the individuals that are manifested via social networks are crucial to numerous applications including social management, dynamical processes, and public security. Affective computing, as an interdisciplinary research field, linking artificial intelligence to cognitive inference, is capable to exploit emotion-oriented knowledge from brief contents. The textual contents convey hidden information such as personality and cognition about corresponding authors that can determine both correlations and variations between users. Emotion recognition from brief contents should embrace the contrast between authors where the differences in personality and cognition can be traced within emotional expressions. To tackle this challenge, we devise a framework that, on the one hand, infers latent individual aspects, from brief contents and, on the other hand, presents a novel ensemble classifier equipped with dynamic dropout convnets to extract emotions from textual context. To categorize short text contents, our proposed method conjointly leverages cognitive factors and exploits hidden information. We utilize the outcome vectors in a novel embedding model to foster emotion-pertinent features that are collectively assembled by lexicon inductions. Experimental results show that compared to other competitors, our proposed model can achieve a higher performance in recognizing emotion from noisy contents. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.01706
Data-Driven Design-by-Analogy: State of the Art and Future Directions,Shuo Jiang;Jie Hu;Kristin L. Wood;Jianxi Luo,"Design-by-Analogy (DbA) is a design methodology wherein new solutions, opportunities or designs are generated in a target domain based on inspiration drawn from a source domain; it can benefit designers in mitigating design fixation and improving design ideation outcomes. Recently, the increasingly available design databases and rapidly advancing data science and artificial intelligence technologies have presented new opportunities for developing data-driven methods and tools for DbA support. In this study, we survey existing data-driven DbA studies and categorize individual studies according to the data, methods, and applications in four categories, namely, analogy encoding, retrieval, mapping, and evaluation. Based on both nuanced organic review and structured analysis, this paper elucidates the state of the art of data-driven DbA research to date and benchmarks it with the frontier of data science and AI research to identify promising research opportunities and directions for the field. Finally, we propose a future conceptual data-driven DbA system that integrates all propositions. △ Less","3 June, 2021",https://arxiv.org/pdf/2106.01592
Gender-Specific Patterns in the Artificial Intelligence Scientific Ecosystem,Anahita Hajibabaei;Andrea Schiffauerova;Ashkan Ebadi,"Gender disparity in science is one of the most focused debating points among authorities and the scientific community. Over the last few decades, numerous initiatives have endeavored to accelerate gender equity in academia and research society. However, despite the ongoing efforts, gaps persist across the world, and more measures need to be taken. Using social network analysis, natural language processing, and machine learning, in this study, we comprehensively analyzed gender-specific patterns in the highly interdisciplinary and evolving field of artificial intelligence for the period of 2000-2019. Our findings suggest an overall increasing rate of mixed-gender collaborations. From the observed gender-specific collaborative patterns, the existence of disciplinary homophily at both dyadic and team levels is confirmed. However, a higher preference was observed for female researchers to form homophilous collaborative links. Our core-periphery analysis indicated a significant positive association between having diverse collaboration and scientific performance and experience. We found evidence in support of expecting the rise of new female superstar researchers in the artificial intelligence field. △ Less","2 June, 2021",https://arxiv.org/pdf/2106.01446
Grasp stability prediction with time series data based on STFT and LSTM,Tao Wang;Frank Kirchner,"With an increasing demand for robots, robotic grasping will has a more important role in future applications. This paper takes grasp stability prediction as the key technology for grasping and tries to solve the problem with time series data inputs including the force and pressure data. Widely applied to more fields to predict unstable grasping with time series data, algorithms can significantly promote the application of artificial intelligence in traditional industries. This research investigates models that combine short-time Fourier transform (STFT) and long short-term memory (LSTM) and then tested generalizability with dexterous hand and suction cup gripper. The experiments suggest good results for grasp stability prediction with the force data and the generalized results in the pressure data. Among the 4 models, (Data + STFT) & LSTM delivers the best performance. We plan to perform more work on grasp stability prediction, generalize the findings to different types of sensors, and apply the grasp stability prediction in more grasping use cases in real life. △ Less","2 June, 2021",https://arxiv.org/pdf/2106.01272
COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences,Shikhar Singh;Nuan Wen;Yu Hou;Pegah Alipoormolabashi;Te-Lin Wu;Xuezhe Ma;Nanyun Peng,"Commonsense reasoning is intuitive for humans but has been a long-term challenge for artificial intelligence (AI). Recent advancements in pretrained language models have shown promising results on several commonsense benchmark datasets. However, the reliability and comprehensiveness of these benchmarks towards assessing model's commonsense reasoning ability remains unclear. To this end, we introduce a new commonsense reasoning benchmark dataset comprising natural language true/false statements, with each sample paired with its complementary counterpart, resulting in 4k sentence pairs. We propose a pairwise accuracy metric to reliably measure an agent's ability to perform commonsense reasoning over a given situation. The dataset is crowdsourced and enhanced with an adversarial model-in-the-loop setup to incentivize challenging samples. To facilitate a systematic analysis of commonsense capabilities, we design our dataset along the dimensions of knowledge domains, reasoning scenarios and numeracy. Experimental results demonstrate that our strongest baseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and ~51% pairwise accuracy, well below human performance (~95% for both metrics). The dataset is available at https://github.com/PlusLabNLP/Com2Sense. △ Less","2 June, 2021",https://arxiv.org/pdf/2106.00969
Conversational Question Answering: A Survey,Munazza Zaib;Wei Emma Zhang;Quan Z. Sheng;Adnan Mahmood;Yang Zhang,"Question answering (QA) systems provide a way of querying the information available in various formats including, but not limited to, unstructured and structured data in natural languages. It constitutes a considerable part of conversational artificial intelligence (AI) which has led to the introduction of a special research topic on Conversational Question Answering (CQA), wherein a system is required to understand the given context and then engages in multi-turn QA to satisfy the user's information needs. Whilst the focus of most of the existing research work is subjected to single-turn QA, the field of multi-turn QA has recently grasped attention and prominence owing to the availability of large-scale, multi-turn QA datasets and the development of pre-trained language models. With a good amount of models and research papers adding to the literature every year recently, there is a dire need of arranging and presenting the related work in a unified manner to streamline future research. This survey, therefore, is an effort to present a comprehensive review of the state-of-the-art research trends of CQA primarily based on reviewed papers from 2016-2021. Our findings show that there has been a trend shift from single-turn to multi-turn QA which empowers the field of Conversational AI from different perspectives. This survey is intended to provide an epitome for the research community with the hope of laying a strong foundation for the field of CQA. △ Less","2 June, 2021",https://arxiv.org/pdf/2106.00874
Leveraging Pre-Images to Discover Nonlinear Relationships in Multivariate Environments,M. Ali Vosoughi;Axel Wismuller,"Causal discovery, beyond the inference of a network as a collection of connected dots, offers a crucial functionality in scientific discovery using artificial intelligence. The questions that arise in multiple domains, such as physics, physiology, the strategic decision in uncertain environments with multiple agents, climatology, among many others, have roots in causality and reasoning. It became apparent that many real-world temporal observations are nonlinearly related to each other. While the number of observations can be as high as millions of points, the number of temporal samples can be minimal due to ethical or practical reasons, leading to the curse-of-dimensionality in large-scale systems. This paper proposes a novel method using kernel principal component analysis and pre-images to obtain nonlinear dependencies of multivariate time-series data. We show that our method outperforms state-of-the-art causal discovery methods when the observations are restricted by time and are nonlinearly related. Extensive simulations on both real-world and synthetic datasets with various topologies are provided to evaluate our proposed methods. △ Less","1 June, 2021",https://arxiv.org/pdf/2106.00842
"Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition",Da Ma;Vincent Chow;Karteek Popuri;Mirza Faisal Beg,"The latest advances in computer-assisted precision medicine are making it feasible to move from population-wide models that are useful to discover aggregate patterns that hold for group-based analysis to patient-specific models that can drive patient-specific decisions with regard to treatment choices, and predictions of outcomes of treatment. Body Composition is recognized as an important driver and risk factor for a wide variety of diseases, as well as a predictor of individual patient-specific clinical outcomes to treatment choices or surgical interventions. 3D CT images are routinely acquired in the oncological worklows and deliver accurate rendering of internal anatomy and therefore can be used opportunistically to assess the amount of skeletal muscle and adipose tissue compartments. Powerful tools of artificial intelligence such as deep learning are making it feasible now to segment the entire 3D image and generate accurate measurements of all internal anatomy. These will enable the overcoming of the severe bottleneck that existed previously, namely, the need for manual segmentation, which was prohibitive to scale to the hundreds of 2D axial slices that made up a 3D volumetric image. Automated tools such as presented here will now enable harvesting whole-body measurements from 3D CT or MRI images, leading to a new era of discovery of the drivers of various diseases based on individual tissue, organ volume, shape, and functional status. These measurements were hitherto unavailable thereby limiting the field to a very small and limited subset. These discoveries and the potential to perform individual image segmentation with high speed and accuracy are likely to lead to the incorporation of these 3D measures into individual specific treatment planning models related to nutrition, aging, chemotoxicity, surgery and survival after the onset of a major disease such as cancer. △ Less","26 July, 2021",https://arxiv.org/pdf/2106.00652
To trust or not to trust an explanation: using LEAF to evaluate local linear XAI methods,Elvio G. Amparore;Alan Perotti;Paolo Bajardi,"The main objective of eXplainable Artificial Intelligence (XAI) is to provide effective explanations for black-box classifiers. The existing literature lists many desirable properties for explanations to be useful, but there is no consensus on how to quantitatively evaluate explanations in practice. Moreover, explanations are typically used only to inspect black-box models, and the proactive use of explanations as a decision support is generally overlooked. Among the many approaches to XAI, a widely adopted paradigm is Local Linear Explanations - with LIME and SHAP emerging as state-of-the-art methods. We show that these methods are plagued by many defects including unstable explanations, divergence of actual implementations from the promised theoretical properties, and explanations for the wrong label. This highlights the need to have standard and unbiased evaluation procedures for Local Linear Explanations in the XAI field. In this paper we address the problem of identifying a clear and unambiguous set of metrics for the evaluation of Local Linear Explanations. This set includes both existing and novel metrics defined specifically for this class of explanations. All metrics have been included in an open Python framework, named LEAF. The purpose of LEAF is to provide a reference for end users to evaluate explanations in a standardised and unbiased way, and to guide researchers towards developing improved explainable techniques. △ Less","1 June, 2021",https://arxiv.org/pdf/2106.00461
AI-Ethics by Design. Evaluating Public Perception on the Importance of Ethical Design Principles of AI,Kimon Kieslich;Birte Keller;Christopher Starke,"Despite the immense societal importance of ethically designing artificial intelligence (AI), little research on the public perceptions of ethical AI principles exists. This becomes even more striking when considering that ethical AI development has the aim to be human-centric and of benefit for the whole society. In this study, we investigate how ethical principles (explainability, fairness, security, accountability, accuracy, privacy, machine autonomy) are weighted in comparison to each other. This is especially important, since simultaneously considering ethical principles is not only costly, but sometimes even impossible, as developers must make specific trade-off decisions. In this paper, we give first answers on the relative importance of ethical principles given a specific use case - the use of AI in tax fraud detection. The results of a large conjoint survey (n=1099) suggest that, by and large, German respondents found the ethical principles equally important. However, subsequent cluster analysis shows that different preference models for ethically designed systems exist among the German population. These clusters substantially differ not only in the preferred attributes, but also in the importance level of the attributes themselves. We further describe how these groups are constituted in terms of sociodemographics as well as opinions on AI. Societal implications as well as design challenges are discussed. △ Less","1 June, 2021",https://arxiv.org/pdf/2106.00326
Watching Smartly from the Bottom: Intrusion Detection revamped through Programmable Networks and Artificial Intelligence,Sergio Armando Gutiérrez;John Willian Branch;Luciano Paschoal Gaspary;Juan Felipe Botero,"The advent of Programmable Data Planes represents an outstanding evolution and complete revolution of the Software- Defined Networking paradigm. The capacity to define the entire behavior of forwarding devices by controlling the packet parsing procedures and executing custom operations enables offloading functionalities traditionally performed at the control plane. A recent research line has explored the possibility of even offloading to the data plane part of Artificial Intelligence algorithms, and more specifically, Machine Learning ones, to increase their accuracy and responsiveness (by having more detailed visibility of the traffic). This introduces a significant opportunity for evolution in the critical field of Intrusion Detection. However, offloading functionalities to the data plane is not a straightforward task. In this paper, we discuss how Programmable Data Planes might complement different stages of an Intrusion Detection System based on Machine Learning. We present two use cases that make evident the feasibility of this approach and highlight aspects that must be considered when addressing the challenge of deploying solutions leveraging data-plane functionalities. △ Less","1 June, 2021",https://arxiv.org/pdf/2106.00239
The Social Responsibility of Game AI,Michael Cook,"Over the last decade we have watched as artificial intelligence has been transformed into one of the most important issues of our time, and games have grown into the biggest entertainment industry. As a result, game AI research as a field has enjoyed increased access to funding, exposure in the press, and influence with governments and some of the largest technology firms in the world. At this pivotal moment in the history of our field, this paper argues that this privileged position brings with it an important set of responsibilities which we have largely failed to meet. We show to whom we are responsible, identify some of these responsibilities, and suggest actions we can take as a community to leverage this power for good. △ Less","23 May, 2021",https://arxiv.org/pdf/2105.15122
"Applications of Artificial Intelligence, Machine Learning and related techniques for Computer Networking Systems",Krishna M. Sivalingam,"This article presents a primer/overview of applications of Artificial Intelligence and Machine Learning (AI/ML) techniques to address problems in the domain of computer networking. In particular, the techniques have been used to support efficient and accurate traffic prediction, traffic classification, anomaly detection, network management, network security, network resource allocation and optimization, network scheduling algorithms, fault diagnosis and many more such applications. The article first summarizes some of the key networking concepts and a few representative machine learning techniques and algorithms. The article then presents details regarding the availability of data sets for networking applications and machine learning software and toolkits for processing these data sets. Highlights of some of the standards activities, pursued by ITU-T and ETSI, which are related to AI/ML for networking, are also presented. Finally, the article discusses a small set of representative networking problems where AI/ML techniques have been successfully applied. △ Less","21 April, 2021",https://arxiv.org/pdf/2105.15103
Bounded logit attention: Learning to explain image classifiers,Thomas Baumhauer;Djordje Slijepcevic;Matthias Zeppelzauer,"Explainable artificial intelligence is the attempt to elucidate the workings of systems too complex to be directly accessible to human cognition through suitable side-information referred to as ""explanations"". We present a trainable explanation module for convolutional image classifiers we call bounded logit attention (BLA). The BLA module learns to select a subset of the convolutional feature map for each input instance, which then serves as an explanation for the classifier's prediction. BLA overcomes several limitations of the instancewise feature selection method ""learning to explain"" (L2X) introduced by Chen et al. (2018): 1) BLA scales to real-world sized image classification problems, and 2) BLA offers a canonical way to learn explanations of variable size. Due to its modularity BLA lends itself to transfer learning setups and can also be employed as a post-hoc add-on to trained classifiers. Beyond explainability, BLA may serve as a general purpose method for differentiable approximation of subset selection. In a user study we find that BLA explanations are preferred over explanations generated by the popular (Grad-)CAM method. △ Less","31 May, 2021",https://arxiv.org/pdf/2105.14824
Federated Learning for Industrial Internet of Things in Future Industries,Dinh C. Nguyen;Ming Ding;Pubudu N. Pathirana;Aruna Seneviratne;Jun Li;Dusit Niyato;H. Vincent Poor,"The Industrial Internet of Things (IIoT) offers promising opportunities to transform the operation of industrial systems and becomes a key enabler for future industries. Recently, artificial intelligence (AI) has been widely utilized for realizing intelligent IIoT applications where AI techniques require centralized data collection and processing. However, this is not always feasible in realistic scenarios due to the high scalability of modern IIoT networks and growing industrial data confidentiality. Federated Learning (FL), as an emerging collaborative AI approach, is particularly attractive for intelligent IIoT networks by coordinating multiple IIoT devices and machines to perform AI training at the network edge while helping protect user privacy. In this article, we provide a detailed overview and discussions of the emerging applications of FL in key IIoT services and applications. A case study is also provided to demonstrate the feasibility of FL in IIoT. Finally, we highlight a range of interesting open research topics that need to be addressed for the full realization of FL-IIoT in industries. △ Less","30 May, 2021",https://arxiv.org/pdf/2105.14659
Human-level COVID-19 Diagnosis from Low-dose CT Scans Using a Two-stage Time-distributed Capsule Network,Parnian Afshar;Moezedin Javad Rafiee;Farnoosh Naderkhani;Shahin Heidarian;Nastaran Enshaei;Anastasia Oikonomou;Faranak Babaki Fard;Reut Anconina;Keyvan Farahani;Konstantinos N. Plataniotis;Arash Mohammadi,"Reverse transcription-polymerase chain reaction (RT-PCR) is currently the gold standard in COVID-19 diagnosis. It can, however, take days to provide the diagnosis, and false negative rate is relatively high. Imaging, in particular chest computed tomography (CT), can assist with diagnosis and assessment of this disease. Nevertheless, it is shown that standard dose CT scan gives significant radiation burden to patients, especially those in need of multiple scans. In this study, we consider low-dose and ultra-low-dose (LDCT and ULDCT) scan protocols that reduce the radiation exposure close to that of a single X-Ray, while maintaining an acceptable resolution for diagnosis purposes. Since thoracic radiology expertise may not be widely available during the pandemic, we develop an Artificial Intelligence (AI)-based framework using a collected dataset of LDCT/ULDCT scans, to study the hypothesis that the AI model can provide human-level performance. The AI model uses a two stage capsule network architecture and can rapidly classify COVID-19, community acquired pneumonia (CAP), and normal cases, using LDCT/ULDCT scans. The AI model achieves COVID-19 sensitivity of 89.5% +\- 0.11, CAP sensitivity of 95% +\- 0.11, normal cases sensitivity (specificity) of 85.7% +\- 0.16, and accuracy of 90% +\- 0.06. By incorporating clinical data (demographic and symptoms), the performance further improves to COVID-19 sensitivity of 94.3% +\- pm 0.05, CAP sensitivity of 96.7% +\- 0.07, normal cases sensitivity (specificity) of 91% +\- 0.09 , and accuracy of 94.1% +\- 0.03. The proposed AI model achieves human-level diagnosis based on the LDCT/ULDCT scans with reduced radiation exposure. We believe that the proposed AI model has the potential to assist the radiologists to accurately and promptly diagnose COVID-19 infection and help control the transmission chain during the pandemic. △ Less","1 December, 2021",https://arxiv.org/pdf/2105.14656
Gradient-Free Neural Network Training via Synaptic-Level Reinforcement Learning,Aman Bhargava;Mohammad R. Rezaei;Milad Lankarany,"An ongoing challenge in neural information processing is: how do neurons adjust their connectivity to improve task performance over time (i.e., actualize learning)? It is widely believed that there is a consistent, synaptic-level learning mechanism in specific brain regions that actualizes learning. However, the exact nature of this mechanism remains unclear. Here we propose an algorithm based on reinforcement learning (RL) to generate and apply a simple synaptic-level learning policy for multi-layer perceptron (MLP) models. In this algorithm, the action space for each MLP synapse consists of a small increase, decrease, or null action on the synapse weight, and the state for each synapse consists of the last two actions and reward signals. A binary reward signal indicates improvement or deterioration in task performance. The static policy produces superior training relative to the adaptive policy and is agnostic to activation function, network shape, and task. Trained MLPs yield character recognition performance comparable to identically shaped networks trained with gradient descent. 0 hidden unit character recognition tests yielded an average validation accuracy of 88.28%, 1.86\pm0.47% higher than the same MLP trained with gradient descent. 32 hidden unit character recognition tests yielded an average validation accuracy of 88.45%, 1.11\pm0.79% lower than the same MLP trained with gradient descent. The robustness and lack of reliance on gradient computations opens the door for new techniques for training difficult-to-differentiate artificial neural networks such as spiking neural networks (SNNs) and recurrent neural networks (RNNs). Further, the method's simplicity provides a unique opportunity for further development of local rule-driven multi-agent connectionist models for machine intelligence analogous to cellular automata. △ Less","29 May, 2021",https://arxiv.org/pdf/2105.14383
Machine Learning for Performance Prediction of Channel Bonding in Next-Generation IEEE 802.11 WLANs,Francesc Wilhelmi;David Góez;Paola Soto;Ramon Vallés;Mohammad Alfaifi;Abdulrahman Algunayah;Jorge Martin-Pérez;Luigi Girletti;Rajasekar Mohan;K Venkat Ramnan;Boris Bellalta,"With the advent of Artificial Intelligence (AI)-empowered communications, industry, academia, and standardization organizations are progressing on the definition of mechanisms and procedures to address the increasing complexity of future 5G and beyond communications. In this context, the International Telecommunication Union (ITU) organized the first AI for 5G Challenge to bring industry and academia together to introduce and solve representative problems related to the application of Machine Learning (ML) to networks. In this paper, we present the results gathered from Problem Statement~13 (PS-013), organized by Universitat Pompeu Fabra (UPF), which primary goal was predicting the performance of next-generation Wireless Local Area Networks (WLANs) applying Channel Bonding (CB) techniques. In particular, we overview the ML models proposed by participants (including Artificial Neural Networks, Graph Neural Networks, Random Forest regression, and gradient boosting) and analyze their performance on an open dataset generated using the IEEE 802.11ax-oriented Komondor network simulator. The accuracy achieved by the proposed methods demonstrates the suitability of ML for predicting the performance of WLANs. Moreover, we discuss the importance of abstracting WLAN interactions to achieve better results, and we argue that there is certainly room for improvement in throughput prediction through ML. △ Less","29 May, 2021",https://arxiv.org/pdf/2105.14219
A Survey on Anomaly Detection for Technical Systems using LSTM Networks,Benjamin Lindemann;Benjamin Maschler;Nada Sahlab;Michael Weyrich,"Anomalies represent deviations from the intended system operation and can lead to decreased efficiency as well as partial or complete system failure. As the causes of anomalies are often unknown due to complex system dynamics, efficient anomaly detection is necessary. Conventional detection approaches rely on statistical and time-invariant methods that fail to address the complex and dynamic nature of anomalies. With advances in artificial intelligence and increasing importance for anomaly detection and prevention in various domains, artificial neural network approaches enable the detection of more complex anomaly types while considering temporal and contextual characteristics. In this article, a survey on state-of-the-art anomaly detection using deep neural and especially long short-term memory networks is conducted. The investigated approaches are evaluated based on the application scenario, data and anomaly types as well as further metrics. To highlight the potential of upcoming anomaly detection techniques, graph-based and transfer learning approaches are also included in the survey, enabling the analysis of heterogeneous data as well as compensating for its shortage and improving the handling of dynamic processes. △ Less","28 May, 2021",https://arxiv.org/pdf/2105.13810
The Trusted Edge,Christian Meurisch,"Edge computing promises to reshape the centralized nature of today's cloud-based applications by bringing computing resources, at least in part, closer to the user. Reasons include the increasing need for real-time (short-delay, reliably-connected) computing and resource-demanding artificial intelligence (AI) algorithms that overstrain mobile devices' batteries or compute power but are too bandwidth-demanding to be offloaded to a distant cloud. However, companies may need to run their protected business logic on (untrusted) third-party edge devices, which can lead to serious issues due to weaker security measures than in cloud environments. This article makes the case for trusted edge computing (TEC), which focuses on developing concepts and methods for protecting application providers' business logic (and thus their intellectual property) specifically tailored to open edge infrastructures. This article further discusses open challenges in TEC to be addressed in the future. Otherwise, edge computing risks being a non-starter for most businesses due to the inadequate and neglected protection of intellectual property. △ Less","28 May, 2021",https://arxiv.org/pdf/2105.13601
Quantization and Deployment of Deep Neural Networks on Microcontrollers,Pierre-Emmanuel Novac;Ghouthi Boukli Hacene;Alain Pegatoquet;Benoît Miramond;Vincent Gripon,"Embedding Artificial Intelligence onto low-power devices is a challenging task that has been partly overcome with recent advances in machine learning and hardware design. Presently, deep neural networks can be deployed on embedded targets to perform different tasks such as speech recognition,object detection or Human Activity Recognition. However, there is still room for optimization of deep neural networks onto embedded devices. These optimizations mainly address power consumption,memory and real-time constraints, but also an easier deployment at the edge. Moreover, there is still a need for a better understanding of what can be achieved for different use cases. This work focuses on quantization and deployment of deep neural networks onto low-power 32-bit microcontrollers. The quantization methods, relevant in the context of an embedded execution onto a microcontroller, are first outlined. Then, a new framework for end-to-end deep neural networks training, quantization and deployment is presented. This framework, called MicroAI, is designed as an alternative to existing inference engines (TensorFlow Lite for Microcontrollers and STM32CubeAI). Our framework can indeed be easily adjusted and/or extended for specific use cases. Execution using single precision 32-bit floating-point as well as fixed-point on 8- and 16-bit integers are supported. The proposed quantization method is evaluated with three different datasets (UCI-HAR, Spoken MNIST and GTSRB). Finally, a comparison study between MicroAI and both existing embedded inference engines is provided in terms of memory and power efficiency. On-device evaluation is done using ARM Cortex-M4F-based microcontrollers (Ambiq Apollo3 and STM32L452RE). △ Less","23 September, 2021",https://arxiv.org/pdf/2105.13331
Characterizing Impacts of Storage Faults on HPC Applications: A Methodology and Insights,Bo Fang;Daoce Wang;Sian Jin;Quincey Koziol;Zhao Zhang;Qiang Guan;Suren Byna;Sriram Krishnamoorthy;Dingwen Tao,"In recent years, the increasing complexity in scientific simulations and emerging demands for training heavy artificial intelligence models require massive and fast data accesses, which urges high-performance computing (HPC) platforms to equip with more advanced storage infrastructures such as solid-state disks (SSDs). While SSDs offer high-performance I/O, the reliability challenges faced by the HPC applications under the SSD-related failures remains unclear, in particular for failures resulting in data corruptions. The goal of this paper is to understand the impact of SSD-related faults on the behaviors of complex HPC applications. To this end, we propose FFIS, a FUSE-based fault injection framework that systematically introduces storage faults into the application layer to model the errors originated from SSDs. FFIS is able to plant different I/O related faults into the data returned from underlying file systems, which enables the investigation on the error resilience characteristics of the scientific file format. We demonstrate the use of FFIS with three representative real HPC applications, showing how each application reacts to the data corruptions, and provide insights on the error resilience of the widely adopted HDF5 file format for the HPC applications. △ Less","2 August, 2021",https://arxiv.org/pdf/2105.12929
ATRIA: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-DRAM CNN Processing,Supreeth Mysore Shivanandamurthy;Ishan. G. Thakkar;Sayed Ahmad Salehi,"With the rapidly growing use of Convolutional Neural Networks (CNNs) in real-world applications related to machine learning and Artificial Intelligence (AI), several hardware accelerator designs for CNN inference and training have been proposed recently. In this paper, we present ATRIA, a novel bit-pArallel sTochastic aRithmetic based In-DRAM Accelerator for energy-efficient and high-speed inference of CNNs. ATRIA employs light-weight modifications in DRAM cell arrays to implement bit-parallel stochastic arithmetic based acceleration of multiply-accumulate (MAC) operations inside DRAM. ATRIA significantly improves the latency, throughput, and efficiency of processing CNN inferences by performing 16 MAC operations in only five consecutive memory operation cycles. We mapped the inference tasks of four benchmark CNNs on ATRIA to compare its performance with five state-of-the-art in-DRAM CNN accelerators from prior work. The results of our analysis show that ATRIA exhibits only 3.5% drop in CNN inference accuracy and still achieves improvements of up to 3.2x in frames-per-second (FPS) and up to 10x in efficiency (FPS/W/mm2), compared to the best-performing in-DRAM accelerator from prior work. △ Less","26 May, 2021",https://arxiv.org/pdf/2105.12781
Towards Transparent Application of Machine Learning in Video Processing,Luka Murn;Marc Gorriz Blanch;Maria Santamaria;Fiona Rivera;Marta Mrak,"Machine learning techniques for more efficient video compression and video enhancement have been developed thanks to breakthroughs in deep learning. The new techniques, considered as an advanced form of Artificial Intelligence (AI), bring previously unforeseen capabilities. However, they typically come in the form of resource-hungry black-boxes (overly complex with little transparency regarding the inner workings). Their application can therefore be unpredictable and generally unreliable for large-scale use (e.g. in live broadcast). The aim of this work is to understand and optimise learned models in video processing applications so systems that incorporate them can be used in a more trustworthy manner. In this context, the presented work introduces principles for simplification of learned models targeting improved transparency in implementing machine learning for video production and distribution applications. These principles are demonstrated on video compression examples, showing how bitrate savings and reduced complexity can be achieved by simplifying relevant deep learning models. △ Less","27 May, 2021",https://arxiv.org/pdf/2105.12700
Database Workload Characterization with Query Plan Encoders,Debjyoti Paul;Jie Cao;Feifei Li;Vivek Srikumar,"Smart databases are adopting artificial intelligence (AI) technologies to achieve {\em instance optimality}, and in the future, databases will come with prepackaged AI models within their core components. The reason is that every database runs on different workloads, demands specific resources, and settings to achieve optimal performance. It prompts the necessity to understand workloads running in the system along with their features comprehensively, which we dub as workload characterization. To address this workload characterization problem, we propose our query plan encoders that learn essential features and their correlations from query plans. Our pretrained encoders capture the {\em structural} and the {\em computational performance} of queries independently. We show that our pretrained encoders are adaptable to workloads that expedite the transfer learning process. We performed independent assessments of structural encoder and performance encoders with multiple downstream tasks. For the overall evaluation of our query plan encoders, we architect two downstream tasks (i) query latency prediction and (ii) query classification. These tasks show the importance of feature-based workload characterization. We also performed extensive experiments on individual encoders to verify the effectiveness of representation learning and domain adaptability. △ Less","25 May, 2021",https://arxiv.org/pdf/2105.12287
Some Pragmatic Prevention's Guidelines regarding SARS-CoV-2 and COVID-19 in Latin-America inspired by mixed Machine Learning Techniques and Artificial Mathematical Intelligence. Case Study: Colombia,Danny A. J. Gomez-Ramirez;Yoe A. Herrera-Jaramillo;Johana C. Ortega-Giraldo;Alex M. Ardila-Garcia,"We use an enhanced methodology combining specific forms of AI techniques, opinion mining and artificial mathematical intelligence (AMI), with public data on the spread of the coronavirus SARS-CoV-2 and the incidence of COVID-19 disease in Colombia during the first three months since the first reported positive case. The results obtained, together with conceptual tools coming from the global taxonomy of fundamental cognitive mechanisms emerging in AMI and with suitable contextual information from Colombian public health and mainstream social media, allowed us to stating specific preventive guidelines for a better restructuring of initial safe and stable life conditions in Colombia, and in an extended manner in similar Latin American Countries. More specifically, we describe three major guidelines: 1) regular creative visualization and effective planning, 2) the continuous use of constructive linguistic frameworks, and 3) frequent and moderate use of kinesthetic routines. They should be understood as effective tools from a cognitive and behavioural perspective, rather than from a biological one. Even more, the first two guidelines should be acknowledged in integral cooperation with the third one regarding the global effect of COVID-19 in human beings as a whole, this includes the mind and body. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.12213
From Motor Control to Team Play in Simulated Humanoid Football,Siqi Liu;Guy Lever;Zhe Wang;Josh Merel;S. M. Ali Eslami;Daniel Hennes;Wojciech M. Czarnecki;Yuval Tassa;Shayegan Omidshafiei;Abbas Abdolmaleki;Noah Y. Siegel;Leonard Hasenclever;Luke Marris;Saran Tunyasuvunakool;H. Francis Song;Markus Wulfmeier;Paul Muller;Tuomas Haarnoja;Brendan D. Tracey;Karl Tuyls;Thore Graepel;Nicolas Heess,"Intelligent behaviour in the physical world exhibits structure at multiple spatial and temporal scales. Although movements are ultimately executed at the level of instantaneous muscle tensions or joint torques, they must be selected to serve goals defined on much longer timescales, and in terms of relations that extend far beyond the body itself, ultimately involving coordination with other agents. Recent research in artificial intelligence has shown the promise of learning-based approaches to the respective problems of complex movement, longer-term planning and multi-agent coordination. However, there is limited research aimed at their integration. We study this problem by training teams of physically simulated humanoid avatars to play football in a realistic virtual environment. We develop a method that combines imitation learning, single- and multi-agent reinforcement learning and population-based training, and makes use of transferable representations of behaviour for decision making at different levels of abstraction. In a sequence of stages, players first learn to control a fully articulated body to perform realistic, human-like movements such as running and turning; they then acquire mid-level football skills such as dribbling and shooting; finally, they develop awareness of others and play as a team, bridging the gap between low-level motor control at a timescale of milliseconds, and coordinated goal-directed behaviour as a team at the timescale of tens of seconds. We investigate the emergence of behaviours at different levels of abstraction, as well as the representations that underlie these behaviours using several analysis techniques, including statistics from real-world sports analytics. Our work constitutes a complete demonstration of integrated decision-making at multiple scales in a physically embodied multi-agent setting. See project video at https://youtu.be/KHMwq9pv7mg. △ Less","25 May, 2021",https://arxiv.org/pdf/2105.12196
Optical coherent dot-product chip for sophisticated deep learning regression,Shaofu Xu;Jing Wang;Haowen Shu;Zhike Zhang;Sicheng Yi;Bowen Bai;Xingjun Wang;Jianguo Liu;Weiwen Zou,"Optical implementations of neural networks (ONNs) herald the next-generation high-speed and energy-efficient deep learning computing by harnessing the technical advantages of large bandwidth and high parallelism of optics. However, due to the problems of incomplete numerical domain, limited hardware scale, or inadequate numerical accuracy, the majority of existing ONNs were studied for basic classification tasks. Given that regression is a fundamental form of deep learning and accounts for a large part of current artificial intelligence applications, it is necessary to master deep learning regression for further development and deployment of ONNs. Here, we demonstrate a silicon-based optical coherent dot-product chip (OCDC) capable of completing deep learning regression tasks. The OCDC adopts optical fields to carry out operations in complete real-value domain instead of in only positive domain. Via reusing, a single chip conducts matrix multiplications and convolutions in neural networks of any complexity. Also, hardware deviations are compensated via in-situ backpropagation control provided the simplicity of chip architecture. Therefore, the OCDC meets the requirements for sophisticated regression tasks and we successfully demonstrate a representative neural network, the AUTOMAP (a cutting-edge neural network model for image reconstruction). The quality of reconstructed images by the OCDC and a 32-bit digital computer is comparable. To the best of our knowledge, there is no precedent of performing such state-of-the-art regression tasks on ONN chip. It is anticipated that the OCDC can promote novel accomplishment of ONNs in modern AI applications including autonomous driving, natural language processing, and scientific study. △ Less","15 December, 2021",https://arxiv.org/pdf/2105.12122
IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task,Baban Gain;Dibyanayan Bandyopadhyay;Arkadipta De;Tanik Saikh;Asif Ekbal,"In this article, we present a description of our systems as a part of our participation in the shared task namely Artificial Intelligence for Legal Assistance (AILA 2019). This is an integral event of Forum for Information Retrieval Evaluation-2019. The outcomes of this track would be helpful for the automation of the working process of the Indian Judiciary System. The manual working procedures and documentation at any level (from lower to higher court) of the judiciary system are very complex in nature. The systems produced as a part of this track would assist the law practitioners. It would be helpful for common men too. This kind of track also opens the path of research of Natural Language Processing (NLP) in the judicial domain. This track defined two problems such as Task 1: Identifying relevant prior cases for a given situation and Task 2: Identifying the most relevant statutes for a given situation. We tackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As per the results declared by the task organizers, we are in 3rd and a modest position in Task 1 and Task 2 respectively. △ Less","26 June, 2021",https://arxiv.org/pdf/2105.11347
Coupling Power Laws Offers a Powerful Method for Problems such as Biodiversity and COVID-19 Fatality Predictions,Sam Ma,"Power laws have been found to describe a wide variety of natural (physical, biological, astronomic, meteorological, geological) and man-made (social, financial, computational) phenomena over a wide range of magnitudes, although their underlying mechanisms are not always clear. In statistics, power law distribution is often found to fit data exceptionally well when the normal (Gaussian) distribution fails. Nevertheless, predicting power law phenomena is notoriously difficult because some of its idiosyncratic properties such as lack of well-defined average value, and potentially unbounded variance. TPL (Taylor's power law), a power law first discovered to characterize the spatial and/or temporal distribution of biological populations and recently extended to describe the spatiotemporal heterogeneities (distributions) of human microbiomes and other natural and artificial systems such as fitness distribution in computational (artificial) intelligence. The power law with exponential cutoff (PLEC) is a variant of power-law function that tapers off the exponential growth of power-law function ultimately and can be particularly useful for certain predictive problems such as biodiversity estimation and turning-point prediction for COVID-19 infection/fatality. Here, we propose coupling (integration) of TPL and PLEC to offer improved prediction quality of certain power-law phenomena. The coupling takes advantages of variance prediction using TPL and the asymptote estimation using PLEC and delivers confidence interval for the asymptote. We demonstrate the integrated approach to the estimation of potential (dark) biodiversity and turning point of COVID-19 fatality. We expect this integrative approach should have wide applications given the duel relationship between power law and normal statistical distributions. △ Less","23 May, 2021",https://arxiv.org/pdf/2105.11002
Towards Knowledge Organization Ecosystems,Mayukh Bagchi,"It is needless to mention the (already established) overarching importance of knowledge organization and its tried-and-tested high-quality schemes in knowledge-based Artificial Intelligence (AI) systems. But equally, it is also hard to ignore that, increasingly, standalone KOSs are becoming functionally ineffective components for such systems, given their inability to capture the continuous facetization and drift of domains. The paper proposes a radical re-conceptualization of KOSs as a first step to solve such an inability, and, accordingly, contributes in the form of the following dimensions: (i) an explicit characterization of Knowledge Organization Ecosystems (KOEs) (possibly for the first time) and their positioning as pivotal components in realizing sustainable knowledge-based AI solutions, (ii) as a consequence of such a novel characterization, a first examination and characterization of KOEs as Socio-Technical Systems (STSs), thus opening up an entirely new stream of research in knowledge-based AI, and (iii) motivating KOEs not to be mere STSs but STSs which are grounded in Ethics and Responsible Artificial Intelligence cardinals from their very genesis. The paper grounds the above contributions in relevant research literature in a distributed fashion throughout the paper, and finally concludes by outlining the future research possibilities. △ Less","23 May, 2021",https://arxiv.org/pdf/2105.10923
An Efficient Application of Neuroevolution for Competitive Multiagent Learning,Unnikrishnan Rajendran Menon;Anirudh Rajiv Menon,Multiagent systems provide an ideal environment for the evaluation and analysis of real-world problems using reinforcement learning algorithms. Most traditional approaches to multiagent learning are affected by long training periods as well as high computational complexity. NEAT (NeuroEvolution of Augmenting Topologies) is a popular evolutionary strategy used to obtain the best performing neural network architecture often used to tackle optimization problems in the field of artificial intelligence. This paper utilizes the NEAT algorithm to achieve competitive multiagent learning on a modified pong game environment in an efficient manner. The competing agents abide by different rules while having similar observation space parameters. The proposed algorithm utilizes this property of the environment to define a singular neuroevolutionary procedure that obtains the optimal policy for all the agents. The compiled results indicate that the proposed implementation achieves ideal behaviour in a very short training period when compared to existing multiagent reinforcement learning models. △ Less,"23 May, 2021",https://arxiv.org/pdf/2105.10907
Towards Artificial Intelligence Enabled Financial Crime Detection,Zeinab Rouhollahi,"Recently, financial institutes have been dealing with an increase in financial crimes. In this context, financial services firms started to improve their vigilance and use new technologies and approaches to identify and predict financial fraud and crime possibilities. This task is challenging as institutions need to upgrade their data and analytics capabilities to enable new technologies such as Artificial Intelligence (AI) to predict and detect financial crimes. In this paper, we put a step towards AI-enabled financial crime detection in general and money laundering detection in particular to address this challenge. We study and analyse the recent works done in financial crime detection and present a novel model to detect money laundering cases with minimum human intervention needs. △ Less","23 May, 2021",https://arxiv.org/pdf/2105.10866
Cybercosm: New Foundations for a Converged Science Data Ecosystem,Mark Asch;François Bodin;Micah Beck;Terry Moore;Michela Taufer;Martin Swany;Jean-Pierre Vilotte,"Scientific communities naturally tend to organize around data ecosystems created by the combination of their observational devices, their data repositories, and the workflows essential to carry their research from observation to discovery. However, these legacy data ecosystems are now breaking down under the pressure of the exponential growth in the volume and velocity of these workflows, which are further complicated by the need to integrate the highly data intensive methods of the Artificial Intelligence revolution. Enabling ground breaking science that makes full use of this new, data saturated research environment will require distributed systems that support dramatically improved resource sharing, workflow portability and composability, and data ecosystem convergence. The Cybercosm vision presented in this white paper describes a radically different approach to the architecture of distributed systems for data-intensive science and its application workflows. As opposed to traditional models that restrict interoperability by hiving off storage, networking, and computing resources in separate technology silos, Cybercosm defines a minimally sufficient hypervisor as a spanning layer for its data plane that virtualizes and converges the local resources of the system's nodes in a fully interoperable manner. By building on a common, universal interface into which the problems that infect today's data-intensive workflows can be decomposed and attacked, Cybercosm aims to support scalable, portable and composable workflows that span and merge the distributed data ecosystems that characterize leading edge research communities today. △ Less","29 June, 2021",https://arxiv.org/pdf/2105.10680
Towards Realization of Augmented Intelligence in Dermatology: Advances and Future Directions,Roxana Daneshjou;Carrie Kovarik;Justin M Ko,"Artificial intelligence (AI) algorithms using deep learning have advanced the classification of skin disease images; however these algorithms have been mostly applied ""in silico"" and not validated clinically. Most dermatology AI algorithms perform binary classification tasks (e.g. malignancy versus benign lesions), but this task is not representative of dermatologists' diagnostic range. The American Academy of Dermatology Task Force on Augmented Intelligence published a position statement emphasizing the importance of clinical validation to create human-computer synergy, termed augmented intelligence (AuI). Liu et al's recent paper, ""A deep learning system for differential diagnosis of skin diseases"" represents a significant advancement of AI in dermatology, bringing it closer to clinical impact. However, significant issues must be addressed before this algorithm can be integrated into clinical workflow. These issues include accurate and equitable model development, defining and assessing appropriate clinical outcomes, and real-world integration. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.10477
Object-Process Methodology for Intelligent System Development,Vladislav Dorofeev,"Development of the new artificial systems with unique characteristics is very challenging task. In this paper the application of the hybrid super intelligence concept with object-process methodology to develop unique high-performance computational systems is considered. The methodological approach how to design new intelligent components for existing high-performance computing development systems is proposed on the example of system requirements creation for ""MicroAI"" and ""Artificial Electronic"" systems. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.10387
AI Certification: Advancing Ethical Practice by Reducing Information Asymmetries,Peter Cihon;Moritz J. Kleinaltenkamp;Jonas Schuett;Seth D. Baum,"As artificial intelligence (AI) systems are increasingly deployed, principles for ethical AI are also proliferating. Certification offers a method to both incentivize adoption of these principles and substantiate that they have been implemented in practice. This paper draws from management literature on certification and reviews current AI certification programs and proposals. Successful programs rely on both emerging technical methods and specific design considerations. In order to avoid two common failures of certification, program designs should ensure that the symbol of the certification is substantially implemented in practice and that the program achieves its stated goals. The review indicates that the field currently focuses on self-certification and third-party certification of systems, individuals, and organizations - to the exclusion of process management certifications. Additionally, the paper considers prospects for future AI certification programs. Ongoing changes in AI technology suggest that AI certification regimes should be designed to emphasize governance criteria of enduring value, such as ethics training for AI developers, and to adjust technical criteria as the technology changes. Overall, certification can play a valuable mix in the portfolio of AI governance tools. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.10356
Error Resilient Collaborative Intelligence via Low-Rank Tensor Completion,Lior Bragilevsky;Ivan V. Bajić,"In the race to bring Artificial Intelligence (AI) to the edge, collaborative intelligence has emerged as a promising way to lighten the computation load on edge devices that run applications based on Deep Neural Networks (DNNs). Typically, a deep model is split at a certain layer into edge and cloud sub-models. The deep feature tensor produced by the edge sub-model is transmitted to the cloud, where the remaining computationally intensive workload is performed by the cloud sub-model. The communication channel between the edge and cloud is imperfect, which will result in missing data in the deep feature tensor received at the cloud side. In this study, we examine the effectiveness of four low-rank tensor completion methods in recovering missing data in the deep feature tensor. We consider both sparse tensors, such as those produced by the VGG16 model, as well as non-sparse tensors, such as those produced by ResNet34 model. We study tensor completion effectiveness in both conplexity-constrained and unconstrained scenario. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.10341
Multi-Agent Deep Reinforcement Learning using Attentive Graph Neural Architectures for Real-Time Strategy Games,Won Joon Yun;Sungwon Yi;Joongheon Kim,"In real-time strategy (RTS) game artificial intelligence research, various multi-agent deep reinforcement learning (MADRL) algorithms are widely and actively used nowadays. Most of the research is based on StarCraft II environment because it is the most well-known RTS games in world-wide. In our proposed MADRL-based algorithm, distributed MADRL is fundamentally used that is called QMIX. In addition to QMIX-based distributed computation, we consider state categorization which can reduce computational complexity significantly. Furthermore, self-attention mechanisms are used for identifying the relationship among agents in the form of graphs. Based on these approaches, we propose a categorized state graph attention policy (CSGA-policy). As observed in the performance evaluation of our proposed CSGA-policy with the most well-known StarCraft II simulation environment, our proposed algorithm works well in various settings, as expected. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.10211
Wide & Deep neural network model for patch aggregation in CNN-based prostate cancer detection systems,Lourdes Duran-Lopez;Juan P. Dominguez-Morales;Daniel Gutierrez-Galan;Antonio Rios-Navarro;Angel Jimenez-Fernandez;Saturnino Vicente-Diaz;Alejandro Linares-Barranco,"Prostate cancer (PCa) is one of the most commonly diagnosed cancer and one of the leading causes of death among men, with almost 1.41 million new cases and around 375,000 deaths in 2020. Artificial Intelligence algorithms have had a huge impact in medical image analysis, including digital histopathology, where Convolutional Neural Networks (CNNs) are used to provide a fast and accurate diagnosis, supporting experts in this task. To perform an automatic diagnosis, prostate tissue samples are first digitized into gigapixel-resolution whole-slide images. Due to the size of these images, neural networks cannot use them as input and, therefore, small subimages called patches are extracted and predicted, obtaining a patch-level classification. In this work, a novel patch aggregation method based on a custom Wide & Deep neural network model is presented, which performs a slide-level classification using the patch-level classes obtained from a CNN. The malignant tissue ratio, a 10-bin malignant probability histogram, the least squares regression line of the histogram, and the number of malignant connected components are used by the proposed model to perform the classification. An accuracy of 94.24% and a sensitivity of 98.87% were achieved, proving that the proposed system could aid pathologists by speeding up the screening process and, thus, contribute to the fight against PCa. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09974
Prospects and applications of photonic neural networks,Chaoran Huang;Volker J. Sorger;Mario Miscuglio;Mohammed Al-Qadasi;Avilash Mukherjee;Sudip Shekhar;Lukas Chrostowski;Lutz Lampe;Mitchell Nichols;Mable P. Fok;Daniel Brunner;Alexander N. Tait;Thomas Ferreira de Lima;Bicky A. Marquez;Paul R. Prucnal;Bhavin J. Shastri,"Neural networks have enabled applications in artificial intelligence through machine learning, and neuromorphic computing. Software implementations of neural networks on conventional computers that have separate memory and processor (and that operate sequentially) are limited in speed and energy efficiency. Neuromorphic engineering aims to build processors in which hardware mimics neurons and synapses in the brain for distributed and parallel processing. Neuromorphic engineering enabled by photonics (optical physics) can offer sub-nanosecond latencies and high bandwidth with low energies to extend the domain of artificial intelligence and neuromorphic computing applications to machine learning acceleration, nonlinear programming, intelligent signal processing, etc. Photonic neural networks have been demonstrated on integrated platforms and free-space optics depending on the class of applications being targeted. Here, we discuss the prospects and demonstrated applications of these photonic neural networks. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09943
Detecting and Counting Oysters,Behzad Sadrfaridpour;Yiannis Aloimonos;Miao Yu;Yang Tao;Donald Webster,"Oysters are an essential species in the Chesapeake Bay living ecosystem. Oysters are filter feeders and considered the vacuum cleaners of the Chesapeake Bay that can considerably improve the Bay's water quality. Many oyster restoration programs have been initiated in the past decades and continued to date. Advancements in robotics and artificial intelligence have opened new opportunities for aquaculture. Drone-like ROVs with high maneuverability are getting more affordable and, if equipped with proper sensory devices, can monitor the oysters. This work presents our efforts for videography of the Chesapeake bay bottom using an ROV, constructing a database of oysters, implementing Mask R-CNN for detecting oysters, and counting their number in a video by tracking them. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09758
Covid-19 Detection from Chest X-ray and Patient Metadata using Graph Convolutional Neural Networks,Thosini Bamunu Mudiyanselage;Nipuna Senanayake;Chunyan Ji;Yi Pan;Yanqing Zhang,"The novel corona virus (Covid-19) has introduced significant challenges due to its rapid spreading nature through respiratory transmission. As a result, there is a huge demand for Artificial Intelligence (AI) based quick disease diagnosis methods as an alternative to high demand tests such as Polymerase Chain Reaction (PCR). Chest X-ray (CXR) Image analysis is such cost-effective radiography technique due to resource availability and quick screening. But, a sufficient and systematic data collection that is required by complex deep leaning (DL) models is more difficult and hence there are recent efforts that utilize transfer learning to address this issue. Still these transfer learnt models suffer from lack of generalization and increased bias to the training dataset resulting poor performance for unseen data. Limited correlation of the transferred features from the pre-trained model to a specific medical imaging domain like X-ray and overfitting on fewer data can be reasons for this circumstance. In this work, we propose a novel Graph Convolution Neural Network (GCN) that is capable of identifying bio-markers of Covid-19 pneumonia from CXR images and meta information about patients. The proposed method exploits important relational knowledge between data instances and their features using graph representation and applies convolution to learn the graph data which is not possible with conventional convolution on Euclidean domain. The results of extensive experiments of proposed model on binary (Covid vs normal) and three class (Covid, normal, other pneumonia) classification problems outperform different benchmark transfer learnt models, hence overcoming the aforementioned drawbacks. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.09720
Bidirectional LSTM-CRF Attention-based Model for Chinese Word Segmentation,Chen Jin;Zhuangwei Shi;Weihua Li;Yanbu Guo,"Chinese word segmentation (CWS) is the basic of Chinese natural language processing (NLP). The quality of word segmentation will directly affect the rest of NLP tasks. Recently, with the artificial intelligence tide rising again, Long Short-Term Memory (LSTM) neural network, as one of easily modeling in sequence, has been widely utilized in various kinds of NLP tasks, and functions well. Attention mechanism is an ingenious method to solve the memory compression problem on LSTM. Furthermore, inspired by the powerful abilities of bidirectional LSTM models for modeling sequence and CRF model for decoding, we propose a Bidirectional LSTM-CRF Attention-based Model in this paper. Experiments on PKU and MSRA benchmark datasets show that our model performs better than the baseline methods modeling by other neural networks. △ Less","20 May, 2021",https://arxiv.org/pdf/2105.09681
Futuristic Intelligent Transportation System,Yilong Hui;Zhou Su;Tom H. Luan;Nan Cheng,"The emerging autonomous vehicles (AVs) will inevitably revolutionize the transportation systems. This is because of a key feature of AVs; instead of being managed by human drivers as the conventional vehicles, AVs are of the complete capability to manage the driving by themselves. As a result, the futuristic intelligent transportation system (FITS) can be a centrally managed and optimized system with the fully coordinated driving of vehicles, which is impossible by the current transportation systems controlled by humans. In this article, we envision the operation of such FITS when AVs, advanced vehicular networks (VANETs) and artificial intelligence (AI) are adopted. Specifically, we first develop the autonomous vehicular networks (AVNs) based on the advanced development of AVs and heterogeneous vehicular communication technologies to achieve global data collection and real-time data sharing. With this network architecture, we then integrate AVNs and AI based on the intelligent digital twin (IDT) to design the FITS with the target of setting up an accurate and efficient global traffic scheduling system. After that, compared with the conventional schemes, a customized path planning case is studied to evaluate the performance of the proposed FITS. Finally, we highlight the emerging issues related to the FITS for future research. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09493
Designing AI-based Conversational Agent for Diabetes Care in a Multilingual Context,Thuy-Trinh Nguyen;Kellie Sim;Anthony To Yiu Kuen;Ronald R. O'donnell;Suan Tee Lim;Wenru Wang;Hoang D. Nguyen,"Conversational agents (CAs) represent an emerging research field in health information systems, where there are great potentials in empowering patients with timely information and natural language interfaces. Nevertheless, there have been limited attempts in establishing prescriptive knowledge on designing CAs in the healthcare domain in general, and diabetes care specifically. In this paper, we conducted a Design Science Research project and proposed three design principles for designing health-related CAs that embark on artificial intelligence (AI) to address the limitations of existing solutions. Further, we instantiated the proposed design and developed AMANDA - an AI-based multilingual CA in diabetes care with state-of-the-art technologies for natural-sounding localised accent. We employed mean opinion scores and system usability scale to evaluate AMANDA's speech quality and usability, respectively. This paper provides practitioners with a blueprint for designing CAs in diabetes care with concrete design guidelines that can be extended into other healthcare domains. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09490
Social Behaviour Understanding using Deep Neural Networks: Development of Social Intelligence Systems,Ethan Lim Ding Feng;Zhi-Wei Neo;Aaron William De Silva;Kellie Sim;Hong-Ray Tan;Thi-Thanh Nguyen;Karen Wei Ling Koh;Wenru Wang;Hoang D. Nguyen,"With the rapid development in artificial intelligence, social computing has evolved beyond social informatics toward the birth of social intelligence systems. This paper, therefore, takes initiatives to propose a social behaviour understanding framework with the use of deep neural networks for social and behavioural analysis. The integration of information fusion, person and object detection, social signal understanding, behaviour understanding, and context understanding plays a harmonious role to elicit social behaviours. Three systems, including depression detection, activity recognition and cognitive impairment screening, are developed to evidently demonstrate the importance of social intelligence. The study considerably contributes to the cumulative development of social computing and health informatics. It also provides a number of implications for academic bodies, healthcare practitioners, and developers of socially intelligent agents. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09489
Federated Artificial Intelligence for Unified Credit Assessment,Minh-Duc Hoang;Linh Le;Anh-Tuan Nguyen;Trang Le;Hoang D. Nguyen,"With the rapid adoption of Internet technologies, digital footprints have become ubiquitous and versatile to revolutionise the financial industry in digital transformation. This paper takes initiatives to investigate a new paradigm of the unified credit assessment with the use of federated artificial intelligence. We conceptualised digital human representation which consists of social, contextual, financial and technological dimensions to assess the commercial creditworthiness and social reputation of both banked and unbanked individuals. A federated artificial intelligence platform is proposed with a comprehensive set of system design for efficient and effective credit scoring. The study considerably contributes to the cumulative development of financial intelligence and social computing. It also provides a number of implications for academic bodies, practitioners, and developers of financial technologies. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09484
Mobile Reconfigurable Intelligent Surfaces for NOMA Networks: Federated Learning Approaches,Ruikang Zhong;Xiao Liu;Yuanwei Liu;Yue Chen;Zhu Han,"A novel framework of reconfigurable intelligent surfaces (RISs)-enhanced indoor wireless networks is proposed, where an RIS mounted on the robot is invoked to enable mobility of the RIS and enhance the service quality for mobile users. Meanwhile, non-orthogonal multiple access (NOMA) techniques are adopted to further increase the spectrum efficiency since RISs are capable to provide NOMA with artificial controlled channel conditions, which can be seen as a beneficial operation condition to obtain NOMA gains. To optimize the sum rate of all users, a deep deterministic policy gradient (DDPG) algorithm is invoked to optimize the deployment and phase shifts of the mobile RIS as well as the power allocation policy. In order to improve the efficiency and effectiveness of agent training for the DDPG agents, a federated learning (FL) concept is adopted to enable multiple agents to simultaneously explore similar environments and exchange experiences. We also proved that with the same random exploring policy, the FL armed deep reinforcement learning (DRL) agents can theoretically obtain a reward gain compare to the independent agents. Our simulation results indicate that the mobile RIS scheme can significantly outperform the fixed RIS paradigm, which provides about three times data rate gain compare to the fixed RIS paradigm. Moreover, the NOMA scheme is capable to achieve a gain of 42% in contrast with the OMA scheme in terms of sum rate. Finally, the multi-cell simulation proved that the FL enhanced DDPG algorithm has a superior convergence rate and optimization performance than the independent training framework. △ Less","20 March, 2021",https://arxiv.org/pdf/2105.09462
Explainable Health Risk Predictor with Transformer-based Medicare Claim Encoder,Chuhong Lahlou;Ancil Crayton;Caroline Trier;Evan Willett,"In 2019, The Centers for Medicare and Medicaid Services (CMS) launched an Artificial Intelligence (AI) Health Outcomes Challenge seeking solutions to predict risk in value-based care for incorporation into CMS Innovation Center payment and service delivery models. Recently, modern language models have played key roles in a number of health related tasks. This paper presents, to the best of our knowledge, the first application of these models to patient readmission prediction. To facilitate this, we create a dataset of 1.2 million medical history samples derived from the Limited Dataset (LDS) issued by CMS. Moreover, we propose a comprehensive modeling solution centered on a deep learning framework for this data. To demonstrate the framework, we train an attention-based Transformer to learn Medicare semantics in support of performing downstream prediction tasks thereby achieving 0.91 AUC and 0.91 recall on readmission classification. We also introduce a novel data pre-processing pipeline and discuss pertinent deployment considerations surrounding model explainability and bias. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09428
Diversity in Kemeny Rank Aggregation: A Parameterized Approach,Emmanuel Arrighi;Henning Fernau;Daniel Lokshtanov;Mateus de Oliveira Oliveira;Petra Wolf,"In its most traditional setting, the main concern of optimization theory is the search for optimal solutions for instances of a given computational problem. A recent trend of research in artificial intelligence, called solution diversity, has focused on the development of notions of optimality that may be more appropriate in settings where subjectivity is essential. The idea is that instead of aiming at the development of algorithms that output a single optimal solution, the goal is to investigate algorithms that output a small set of sufficiently good solutions that are sufficiently diverse from one another. In this way, the user has the opportunity to choose the solution that is most appropriate to the context at hand. It also displays the richness of the solution space. When combined with techniques from parameterized complexity theory, the paradigm of diversity of solutions offers a powerful algorithmic framework to address problems of practical relevance. In this work, we investigate the impact of this combination in the field of Kemeny Rank Aggregation, a well-studied class of problems lying in the intersection of order theory and social choice theory and also in the field of order theory itself. In particular, we show that the Kemeny Rank Aggregation problem is fixed-parameter tractable with respect to natural parameters providing natural formalizations of the notions of diversity and of the notion of a sufficiently good solution. Our main results work both when considering the traditional setting of aggregation over linearly ordered votes, and in the more general setting where votes are partially ordered. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09413
"Geographic Question Answering: Challenges, Uniqueness, Classification, and Future Directions",Gengchen Mai;Krzysztof Janowicz;Rui Zhu;Ling Cai;Ni Lao,"As an important part of Artificial Intelligence (AI), Question Answering (QA) aims at generating answers to questions phrased in natural language. While there has been substantial progress in open-domain question answering, QA systems are still struggling to answer questions which involve geographic entities or concepts and that require spatial operations. In this paper, we discuss the problem of geographic question answering (GeoQA). We first investigate the reasons why geographic questions are difficult to answer by analyzing challenges of geographic questions. We discuss the uniqueness of geographic questions compared to general QA. Then we review existing work on GeoQA and classify them by the types of questions they can address. Based on this survey, we provide a generic classification framework for geographic questions. Finally, we conclude our work by pointing out unique future research directions for GeoQA. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.09392
Learning Pareto-Frontier Resource Management Policies for Heterogeneous SoCs: An Information-Theoretic Approach,Aryan Deshwal;Syrine Belakaria;Ganapati Bhat;Janardhan Rao Doppa;Partha Pratim Pande,"Mobile system-on-chips (SoCs) are growing in their complexity and heterogeneity (e.g., Arm's Big-Little architecture) to meet the needs of emerging applications, including games and artificial intelligence. This makes it very challenging to optimally manage the resources (e.g., controlling the number and frequency of different types of cores) at runtime to meet the desired trade-offs among multiple objectives such as performance and energy. This paper proposes a novel information-theoretic framework referred to as PaRMIS to create Pareto-optimal resource management policies for given target applications and design objectives. PaRMIS specifies parametric policies to manage resources and learns statistical models from candidate policy evaluation data in the form of target design objective values. The key idea is to select a candidate policy for evaluation in each iteration guided by statistical models that maximize the information gain about the true Pareto front. Experiments on a commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts and is easily usable to optimize complex objectives (e.g., performance per Watt) when compared to prior methods. △ Less","14 April, 2021",https://arxiv.org/pdf/2105.09282
"Beyond ""Fairness:"" Structural (In)justice Lenses on AI for Education",Michael Madaio;Su Lin Blodgett;Elijah Mayfield;Ezekiel Dixon-Román,"Educational technologies, and the systems of schooling in which they are deployed, enact particular ideologies about what is important to know and how learners should learn. As artificial intelligence technologies -- in education and beyond -- may contribute to inequitable outcomes for marginalized communities, various approaches have been developed to evaluate and mitigate the harmful impacts of AI. However, we argue in this paper that the dominant paradigm of evaluating fairness on the basis of performance disparities in AI models is inadequate for confronting the systemic inequities that educational AI systems (re)produce. We draw on a lens of structural injustice informed by critical theory and Black feminist scholarship to critically interrogate several widely-studied and widely-adopted categories of educational AI and explore how they are bound up in and reproduce historical legacies of structural injustice and inequity, regardless of the parity of their models' performance. We close with alternative visions for a more equitable future for educational AI. △ Less","1 November, 2021",https://arxiv.org/pdf/2105.08847
DID-eFed: Facilitating Federated Learning as a Service with Decentralized Identities,Jiahui Geng;Neel Kanwal;Martin Gilje Jaatun;Chunming Rong,"We have entered the era of big data, and it is considered to be the ""fuel"" for the flourishing of artificial intelligence applications. The enactment of the EU General Data Protection Regulation (GDPR) raises concerns about individuals' privacy in big data. Federated learning (FL) emerges as a functional solution that can help build high-performance models shared among multiple parties while still complying with user privacy and data confidentiality requirements. Although FL has been intensively studied and used in real applications, there is still limited research related to its prospects and applications as a FLaaS (Federated Learning as a Service) to interested 3rd parties. In this paper, we present a FLaaS system: DID-eFed, where FL is facilitated by decentralized identities (DID) and a smart contract. DID enables a more flexible and credible decentralized access management in our system, while the smart contract offers a frictionless and less error-prone process. We describe particularly the scenario where our DID-eFed enables the FLaaS among hospitals and research institutions. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.08671
AI-Native Network Slicing for 6G Networks,Wen Wu;Conghao Zhou;Mushu Li;Huaqing Wu;Haibo Zhou;Ning Zhang;Xuemin;Shen;Weihua Zhuang,"With the global roll-out of the fifth generation (5G) networks, it is necessary to look beyond 5G and envision the 6G networks. The 6G networks are expected to have space-air-ground integrated networks, advanced network virtualization, and ubiquitous intelligence. This article presents an artificial intelligence (AI)-native network slicing architecture for 6G networks to enable the synergy of AI and network slicing, thereby facilitating intelligent network management and supporting emerging AI services. AI-based solutions are first discussed across network slicing lifecycle to intelligently manage network slices, i.e., AI for slicing. Then, network slicing solutions are studied to support emerging AI services by constructing AI instances and performing efficient resource management, i.e., slicing for AI. Finally, a case study is presented, followed by a discussion of open research issues that are essential for AI-native network slicing in 6G networks. △ Less","5 November, 2021",https://arxiv.org/pdf/2105.08576
AI Based Landscape Sensing Using Radio Signals,Vijaya Yajnanarayana;Dongdong Huang;Deep Shrestha;Yi Geng;Ali Behravan;Erik Dahlman,"In many sensing applications, typically radio signals are emitted by a radar and from the bounced reflections of the obstacles, inference about the environment is made. Even though radars can be used to sense the landscapes around the user-equipment (UE) such as whether UE is in the forested region, inside buildings, etc., it is not suitable in many wireless applications as many UEs does not have radars in them. Using radar will also increase the cost and power requirements on the UEs in applications requiring sensing of the landscapes. In this paper, we provide a mechanism where basestation (BS) is able to sense the UE's landscape without the use of a radar. We propose an artificial intelligence (AI) based approach with suitable choice of the features derived from the wireless channel to infer the landscape of the UEs. Results for the proposed methods when applied to practical environments such as London city scenario yields a precision score of more than 95 percent. △ Less","18 May, 2021",https://arxiv.org/pdf/2105.08436
Modeling the EdNet Dataset with Logistic Regression,Philip I. Pavlik Jr;Luke G. Eglington,"Many of these challenges are won by neural network models created by full-time artificial intelligence scientists. Due to this origin, they have a black-box character that makes their use and application less clear to learning scientists. We describe our experience with competition from the perspective of educational data mining, a field founded in the learning sciences and connected with roots in psychology and statistics. We describe our efforts from the perspectives of learning scientists and the challenges to our methods, some real and some imagined. We also discuss some basic results in the Kaggle system and our thoughts on how those results may have been improved. Finally, we describe how learner model predictions are used to make pedagogical decisions for students. Their practical use entails a) model predictions and b) a decision rule (based on the predictions). We point out how increased model accuracy can be of limited practical utility, especially when paired with simple decision rules and argue instead for the need to further investigate optimal decision rules. △ Less","17 May, 2021",https://arxiv.org/pdf/2105.08150
COVID-19 Lung Lesion Segmentation Using a Sparsely Supervised Mask R-CNN on Chest X-rays Automatically Computed from Volumetric CTs,Vignav Ramesh;Blaine Rister;Daniel L. Rubin,"Chest X-rays of coronavirus disease 2019 (COVID-19) patients are frequently obtained to determine the extent of lung disease and are a valuable source of data for creating artificial intelligence models. Most work to date assessing disease severity on chest imaging has focused on segmenting computed tomography (CT) images; however, given that CTs are performed much less frequently than chest X-rays for COVID-19 patients, automated lung lesion segmentation on chest X-rays could be clinically valuable. There currently exists a universal shortage of chest X-rays with ground truth COVID-19 lung lesion annotations, and manually contouring lung opacities is a tedious, labor-intensive task. To accelerate severity detection and augment the amount of publicly available chest X-ray training data for supervised deep learning (DL) models, we leverage existing annotated CT images to generate frontal projection ""chest X-ray"" images for training COVID-19 chest X-ray models. In this paper, we propose an automated pipeline for segmentation of COVID-19 lung lesions on chest X-rays comprised of a Mask R-CNN trained on a mixed dataset of open-source chest X-rays and coronal X-ray projections computed from annotated volumetric CTs. On a test set containing 40 chest X-rays of COVID-19 positive patients, our model achieved IoU scores of 0.81 \pm 0.03 and 0.79 \pm 0.03 when trained on a dataset of 60 chest X-rays and on a mixed dataset of 10 chest X-rays and 50 projections from CTs, respectively. Our model far outperforms current baselines with limited supervised training and may assist in automated COVID-19 severity quantification on chest X-rays. △ Less","19 May, 2021",https://arxiv.org/pdf/2105.08147
MUSER: MUltimodal Stress Detection using Emotion Recognition as an Auxiliary Task,Yiqun Yao;Michalis Papakostas;Mihai Burzo;Mohamed Abouelenien;Rada Mihalcea,"The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER -- a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluations on the Multimodal Stressed Emotion (MuSE) dataset show that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results. △ Less","17 May, 2021",https://arxiv.org/pdf/2105.08146
AI Enabled Data Quality Monitoring with Hydra,Thomas Britton;David Lawrence;Kishansingh Rajput,"Data quality monitoring is critical to all experiments impacting the quality of any physics results. Traditionally, this is done through an alarm system, which detects low level faults, leaving higher level monitoring to human crews. Artificial Intelligence is beginning to find its way into scientific applications, but comes with difficulties, relying on the acquisition of new skill sets, either through education or acquisition, in data science. This paper will discuss the development and deployment of the Hydra monitoring system in production at Gluex. It will show how ""off-the-shelf"" technologies can be rapidly developed, as well as discuss what sociological hurdles must be overcome to successfully deploy such a system. Early results from production running of Hydra will also be shared as well as a future outlook for development of Hydra. △ Less","28 April, 2021",https://arxiv.org/pdf/2105.07948
A Review on Explainability in Multimodal Deep Neural Nets,Gargi Joshi;Rahee Walambe;Ketan Kotecha,"Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain △ Less","18 May, 2021",https://arxiv.org/pdf/2105.07878
The challenges and realities of retailing in a COVID-19 world: Identifying trending and Vital During Crisis keywords during Covid-19 using Machine Learning (Austria as a case study),Reda Mastouri,"From global pandemics to geopolitical turmoil, leaders in logistics, product allocation, procurement and operations are facing increasing difficulty with safeguarding their organizations against supply chain vulnerabilities. It is recommended to opt for forecasting against trending based benchmark because auditing a future forecast puts more focus on seasonality. The forecasting models provide with end-to-end, real time oversight of the entire supply chain, while utilizing predictive analytics and artificial intelligence to identify potential disruptions before they occur. By combining internal and external data points, coming up with an AI-enabled modelling engine can greatly reduce risk by helping retail companies proactively respond to supply and demand variability. This research paper puts focus on creating an ingenious way to tackle the impact of COVID19 on Supply chain, product allocation, trending and seasonality. Key words: Supply chain, covid-19, forecasting, coronavirus, manufacturing, seasonality, trending, retail. △ Less","10 May, 2021",https://arxiv.org/pdf/2105.07876
An Extensive Analytical Approach on Human Resources using Random Forest Algorithm,Swarajya lakshmi v papineni;A. Mallikarjuna Reddy;Sudeepti yarlagadda;Snigdha Yarlagadda;Haritha Akkinen,"The current job survey shows that most software employees are planning to change their job role due to high pay for recent jobs such as data scientists, business analysts and artificial intelligence fields. The survey also indicated that work life imbalances, low pay, uneven shifts and many other factors also make employees think about changing their work life. In this paper, for an efficient organisation of the company in terms of human resources, the proposed system designed a model with the help of a random forest algorithm by considering different employee parameters. This helps the HR department retain the employee by identifying gaps and helping the organisation to run smoothly with a good employee retention ratio. This combination of HR and data science can help the productivity, collaboration and well-being of employees of the organisation. It also helps to develop strategies that have an impact on the performance of employees in terms of external and social factors. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.07855
Hard Choices and Hard Limits for Artificial Intelligence,Bryce Goodman,"Artificial intelligence (AI) is supposed to help us make better choices. Some of these choices are small, like what route to take to work, or what music to listen to. Others are big, like what treatment to administer for a disease or how long to sentence someone for a crime. If AI can assist with these big decisions, we might think it can also help with hard choices, cases where alternatives are neither better, worse nor equal but on a par. The aim of this paper, however, is to show that this view is mistaken: the fact of parity shows that there are hard limits on AI in decision making and choices that AI cannot, and should not, resolve. △ Less","4 May, 2021",https://arxiv.org/pdf/2105.07852
"Does ""AI"" stand for augmenting inequality in the era of covid-19 healthcare?",David Leslie;Anjali Mazumder;Aidan Peppin;Maria Wolters;Alexa Hagerty,"Among the most damaging characteristics of the covid-19 pandemic has been its disproportionate effect on disadvantaged communities. As the outbreak has spread globally, factors such as systemic racism, marginalisation, and structural inequality have created path dependencies that have led to poor health outcomes. These social determinants of infectious disease and vulnerability to disaster have converged to affect already disadvantaged communities with higher levels of economic instability, disease exposure, infection severity, and death. Artificial intelligence (AI) technologies are an important part of the health informatics toolkit used to fight contagious disease. AI is well known, however, to be susceptible to algorithmic biases that can entrench and augment existing inequality. Uncritically deploying AI in the fight against covid-19 thus risks amplifying the pandemic's adverse effects on vulnerable groups, exacerbating health inequity. In this paper, we claim that AI systems can introduce or reflect bias and discrimination in three ways: in patterns of health discrimination that become entrenched in datasets, in data representativeness, and in human choices made during the design, development, and deployment of these systems. We highlight how the use of AI technologies threaten to exacerbate the disparate effect of covid-19 on marginalised, under-represented, and vulnerable groups, particularly black, Asian, and other minoritised ethnic people, older populations, and those of lower socioeconomic status. We conclude that, to mitigate the compounding effects of AI on inequalities associated with covid-19, decision makers, technology developers, and health officials must account for the potential biases and inequities at all stages of the AI process. △ Less","30 April, 2021",https://arxiv.org/pdf/2105.07844
An Experimental Analysis of Work-Life Balance Among The Employees using Machine Learning Classifiers,Karampudi Radha;Mekala Rohith,"Researchers today have found out the importance of Artificial Intelligence, and Machine Learning in our daily lives, as well as they can be used to improve the quality of our lives as well as the cities and nations alike. An example of this is that it is currently speculated that ML can provide ways to relieve workers as it can predict effective working schedules and patterns which increase the efficiency of the workers. Ultimately this is leading to a Work-Life Balance for the workers. But how is this possible? It is practically possible with the Machine Learning algorithms to predict, calculate the factors affecting the feelings of the worker's work-life balance. In order to actually do this, a sizeable amount of 12,756 people's data has been taken under consideration. Upon analysing the data and calculating under various factors, we have found out the correlation of various factors and WLB(Work-Life Balance in short). There are some factors that have to be taken into serious consideration as they play a major role in WLB. We have trained 80% of our data with Random Forest Classifier, SVM and Naive Bayes algorithms. Upon testing, the algorithms predict the WLB with 71.5% as the best accuracy. △ Less","28 April, 2021",https://arxiv.org/pdf/2105.07837
Designer-User Communication for XAI: An epistemological approach to discuss XAI design,Juliana Jansen Ferreira;Mateus Monteiro,"Artificial Intelligence is becoming part of any technology we use nowadays. If the AI informs people's decisions, the explanation about AI's outcomes, results, and behavior becomes a necessary capability. However, the discussion of XAI features with various stakeholders is not a trivial task. Most of the available frameworks and methods for XAI focus on data scientists and ML developers as users. Our research is about XAI for end-users of AI systems. We argue that we need to discuss XAI early in the AI-system design process and with all stakeholders. In this work, we aimed at investigating how to operationalize the discussion about XAI scenarios and opportunities among designers and developers of AI and its end-users. We took the Signifying Message as our conceptual tool to structure and discuss XAI scenarios. We experiment with its use for the discussion of a healthcare AI-System. △ Less","17 May, 2021",https://arxiv.org/pdf/2105.07804
Automated Biodesign Engineering by Abductive Meta-Interpretive Learning,Wang-Zhou Dai;Liam Hallett;Stephen H. Muggleton;Geoff S. Baldwin,"The application of Artificial Intelligence (AI) to synthetic biology will provide the foundation for the creation of a high throughput automated platform for genetic design, in which a learning machine is used to iteratively optimise the system through a design-build-test-learn (DBTL) cycle. However, mainstream machine learning techniques represented by deep learning lacks the capability to represent relational knowledge and requires prodigious amounts of annotated training data. These drawbacks strongly restrict AI's role in synthetic biology in which experimentation is inherently resource and time intensive. In this work, we propose an automated biodesign engineering framework empowered by Abductive Meta-Interpretive Learning (Meta_{Abd}), a novel machine learning approach that combines symbolic and sub-symbolic machine learning, to further enhance the DBTL cycle by enabling the learning machine to 1) exploit domain knowledge and learn human-interpretable models that are expressed by formal languages such as first-order logic; 2) simultaneously optimise the structure and parameters of the models to make accurate numerical predictions; 3) reduce the cost of experiments and effort on data annotation by actively generating hypotheses and examples. To verify the effectiveness of Meta_{Abd}, we have modelled a synthetic dataset for the production of proteins from a three gene operon in a microbial host, which represents a common synthetic biology problem. △ Less","17 May, 2021",https://arxiv.org/pdf/2105.07758
Private Facial Diagnosis as an Edge Service for Parkinson's DBS Treatment Valuation,Richard Jiang;Paul Chazot;Danny Crookes;Ahmed Bouridane;M Emre Celebi,"Facial phenotyping has recently been successfully exploited for medical diagnosis as a novel way to diagnose a range of diseases, where facial biometrics has been revealed to have rich links to underlying genetic or medical causes. In this paper, taking Parkinson's Diseases (PD) as a case study, we proposed an Artificial-Intelligence-of-Things (AIoT) edge-oriented privacy-preserving facial diagnosis framework to analyze the treatment of Deep Brain Stimulation (DBS) on PD patients. In the proposed framework, a new edge-based information theoretically secure framework is proposed to implement private deep facial diagnosis as a service over a privacy-preserving AIoT-oriented multi-party communication scheme, where partial homomorphic encryption (PHE) is leveraged to enable privacy-preserving deep facial diagnosis directly on encrypted facial patterns. In our experiments with a collected facial dataset from PD patients, for the first time, we demonstrated that facial patterns could be used to valuate the improvement of PD patients undergoing DBS treatment. We further implemented a privacy-preserving deep facial diagnosis framework that can achieve the same accuracy as the non-encrypted one, showing the potential of our privacy-preserving facial diagnosis as an trustworthy edge service for grading the severity of PD in patients. △ Less","16 May, 2021",https://arxiv.org/pdf/2105.07533
A brain basis of dynamical intelligence for AI and computational neuroscience,Joseph D. Monaco;Kanaka Rajan;Grace M. Hwang,"The deep neural nets of modern artificial intelligence (AI) have not achieved defining features of biological intelligence, including abstraction, causal learning, and energy-efficiency. While scaling to larger models has delivered performance improvements for current applications, more brain-like capacities may demand new theories, models, and methods for designing artificial learning systems. Here, we argue that this opportunity to reassess insights from the brain should stimulate cooperation between AI research and theory-driven computational neuroscience (CN). To motivate a brain basis of neural computation, we present a dynamical view of intelligence from which we elaborate concepts of sparsity in network structure, temporal dynamics, and interactive learning. In particular, we suggest that temporal dynamics, as expressed through neural synchrony, nested oscillations, and flexible sequences, provide a rich computational layer for reading and updating hierarchical models distributed in long-term memory networks. Moreover, embracing agent-centered paradigms in AI and CN will accelerate our understanding of the complex dynamics and behaviors that build useful world models. A convergence of AI/CN theories and objectives will reveal dynamical principles of intelligence for brains and engineered learning systems. This article was inspired by our symposium on dynamical neuroscience and machine learning at the 6th Annual US/NIH BRAIN Initiative Investigators Meeting. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.07284
The Paradigm of Digital Twin Communications,Tom H. Luan;Ruhan Liu;Longxiang Gao;Rui Li;Haibo Zhou,"With the fast evolving of cloud computing and artificial intelligence (AI), the concept of digital twin (DT) has recently been proposed and finds broad applications in industrial Internet, IoT, smart city, etc. The DT builds a mirror integrated multi-physics of the physical system in the digital space. By doing so, the DT can utilize the rich computing power and AI at the cloud to operate on the mirror physical system, and accordingly provides feedbacks to help the real-world physical system in their practical task completion. The existing literature mainly considers DT as a simulation/emulation approach, whereas the communication framework for DT has not been clearly defined and discussed. In this article, we describe the basic DT communication models and present the open research issues. By combining wireless communications, artificial intelligence (AI) and cloud computing, we show that the DT communication provides a novel framework for futuristic mobile agent systems. △ Less","15 May, 2021",https://arxiv.org/pdf/2105.07182
Slicing-Based AI Service Provisioning on Network Edge,Mushu Li;Jie Gao;Conghao Zhou;Xuemin;Shen;Weihua Zhuang,"Edge intelligence leverages computing resources on network edge to provide artificial intelligence (AI) services close to network users. As it enables fast inference and distributed learning, edge intelligence is envisioned to be an important component of 6G networks. In this article, we investigate AI service provisioning for supporting edge intelligence. First, we present the features and requirements of AI services. Then, we introduce AI service data management, and customize network slicing for AI services. Specifically, we propose a novel resource pooling method to jointly manage service data and network resources for AI services. A trace-driven case study demonstrates the effectiveness of the proposed resource pooling method. Through this study, we illustrate the necessity, challenge, and potential of AI service provisioning on network edge. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.07052
Waste detection in Pomerania: non-profit project for detecting waste in environment,Sylwia Majchrowska;Agnieszka Mikołajczyk;Maria Ferlin;Zuzanna Klawikowska;Marta A. Plantykow;Arkadiusz Kwasigroch;Karol Majek,"Waste pollution is one of the most significant environmental issues in the modern world. The importance of recycling is well known, either for economic or ecological reasons, and the industry demands high efficiency. Our team conducted comprehensive research on Artificial Intelligence usage in waste detection and classification to fight the world's waste pollution problem. As a result an open-source framework that enables the detection and classification of litter was developed. The final pipeline consists of two neural networks: one that detects litter and a second responsible for litter classification. Waste is classified into seven categories: bio, glass, metal and plastic, non-recyclable, other, paper and unknown. Our approach achieves up to 70% of average precision in waste detection and around 75% of classification accuracy on the test dataset. The code used in the studies is publicly available online. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.06808
Building Affordance Relations for Robotic Agents - A Review,Paola Ardón;Èric Pairet;Katrin S. Lohan;Subramanian Ramamoorthy;Ronald P. A. Petrick,"Affordances describe the possibilities for an agent to perform actions with an object. While the significance of the affordance concept has been previously studied from varied perspectives, such as psychology and cognitive science, these approaches are not always sufficient to enable direct transfer, in the sense of implementations, to artificial intelligence (AI)-based systems and robotics. However, many efforts have been made to pragmatically employ the concept of affordances, as it represents great potential for AI agents to effectively bridge perception to action. In this survey, we review and find common ground amongst different strategies that use the concept of affordances within robotic tasks, and build on these methods to provide guidance for including affordances as a mechanism to improve autonomy. To this end, we outline common design choices for building representations of affordance relations, and their implications on the generalisation capabilities of an agent when facing previously unseen scenarios. Finally, we identify and discuss a range of interesting research directions involving affordances that have the potential to improve the capabilities of an AI agent. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.06706
Physical Artificial Intelligence: The Concept Expansion of Next-Generation Artificial Intelligence,Yingbo Li;Yucong Duan;Anamaria-Beatrice Spulber;Haoyang Che;Zakaria Maamar;Zhao Li;Chen Yang;Yu lei,"Artificial Intelligence has been a growth catalyst to our society and is cosidered across all idustries as a fundamental technology. However, its development has been limited to the signal processing domain that relies on the generated and collected data from other sensors. In recent research, concepts of Digital Artificial Intelligence and Physicial Artifical Intelligence have emerged and this can be considered a big step in the theoretical development of Artifical Intelligence. In this paper we explore the concept of Physicial Artifical Intelligence and propose two subdomains: Integrated Physicial Artifical Intelligence and Distributed Physicial Artifical Intelligence. The paper will also examine the trend and governance of Physicial Artifical Intelligence. △ Less","16 May, 2021",https://arxiv.org/pdf/2105.06564
Axes for Sociotechnical Inquiry in AI Research,Sarah Dean;Thomas Krendl Gilbert;Nathan Lambert;Tom Zick,"The development of artificial intelligence (AI) technologies has far exceeded the investigation of their relationship with society. Sociotechnical inquiry is needed to mitigate the harms of new technologies whose potential impacts remain poorly understood. To date, subfields of AI research develop primarily individual views on their relationship with sociotechnics, while tools for external investigation, comparison, and cross-pollination are lacking. In this paper, we propose four directions for inquiry into new and evolving areas of technological development: value--what progress and direction does a field promote, optimization--how the defined system within a problem formulation relates to broader dynamics, consensus--how agreement is achieved and who is included in building it, and failure--what methods are pursued when the problem specification is found wanting. The paper provides a lexicon for sociotechnical inquiry and illustrates it through the example of consumer drone technology. △ Less","26 April, 2021",https://arxiv.org/pdf/2105.06551
What Clinical Trials Can Teach Us about the Development of More Resilient AI for Cybersecurity,Edmon Begoli;Robert A. Bridges;Sean Oesch;Kathryn E. Knight,"Policy-mandated, rigorously administered scientific testing is needed to provide transparency into the efficacy of artificial intelligence-based (AI-based) cyber defense tools for consumers and to prioritize future research and development. In this article, we propose a model that is informed by our experience, urged forward by massive scale cyberattacks, and inspired by parallel developments in the biomedical field and the unprecedentedly fast development of new vaccines to combat global pathogens. △ Less","13 May, 2021",https://arxiv.org/pdf/2105.06545
Advances in Machine and Deep Learning for Modeling and Real-time Detection of Multi-Messenger Sources,E. A. Huerta;Zhizhen Zhao,"We live in momentous times. The science community is empowered with an arsenal of cosmic messengers to study the Universe in unprecedented detail. Gravitational waves, electromagnetic waves, neutrinos and cosmic rays cover a wide range of wavelengths and time scales. Combining and processing these datasets that vary in volume, speed and dimensionality requires new modes of instrument coordination, funding and international collaboration with a specialized human and technological infrastructure. In tandem with the advent of large-scale scientific facilities, the last decade has experienced an unprecedented transformation in computing and signal processing algorithms. The combination of graphics processing units, deep learning, and the availability of open source, high-quality datasets, have powered the rise of artificial intelligence. This digital revolution now powers a multi-billion dollar industry, with far-reaching implications in technology and society. In this chapter we describe pioneering efforts to adapt artificial intelligence algorithms to address computational grand challenges in Multi-Messenger Astrophysics. We review the rapid evolution of these disruptive algorithms, from the first class of algorithms introduced in early 2017, to the sophisticated algorithms that now incorporate domain expertise in their architectural design and optimization schemes. We discuss the importance of scientific visualization and extreme-scale computing in reducing time-to-insight and obtaining new knowledge from the interplay between models and data. △ Less","1 October, 2021",https://arxiv.org/pdf/2105.06479
Providing Assurance and Scrutability on Shared Data and Machine Learning Models with Verifiable Credentials,Iain Barclay;Alun Preece;Ian Taylor;Swapna K. Radha;Jarek Nabrzyski,"Adopting shared data resources requires scientists to place trust in the originators of the data. When shared data is later used in the development of artificial intelligence (AI) systems or machine learning (ML) models, the trust lineage extends to the users of the system, typically practitioners in fields such as healthcare and finance. Practitioners rely on AI developers to have used relevant, trustworthy data, but may have limited insight and recourse. This paper introduces a software architecture and implementation of a system based on design patterns from the field of self-sovereign identity. Scientists can issue signed credentials attesting to qualities of their data resources. Data contributions to ML models are recorded in a bill of materials (BOM), which is stored with the model as a verifiable credential. The BOM provides a traceable record of the supply chain for an AI system, which facilitates on-going scrutiny of the qualities of the contributing components. The verified BOM, and its linkage to certified data qualities, is used in the AI Scrutineer, a web-based tool designed to offer practitioners insight into ML model constituents and highlight any problems with adopted datasets, should they be found to have biased data or be otherwise discredited. △ Less","13 May, 2021",https://arxiv.org/pdf/2105.06370
A Pragmatic Approach to Regulating Artificial Intelligence: A Technology Regulator's Perspective,Joshua Ellul;Stephen McCarthy;Trevor Sammut;Juanita Brockdorff;Matthew Scerri;Gordon J. Pace,"Artificial Intelligence (AI) and the regulation thereof is a topic that is increasingly being discussed within various fora. Various proposals have been made in literature for defining regulatory bodies and/or related regulation. In this paper, we present a pragmatic approach for providing a technology assurance regulatory framework. To the best knowledge of the authors this work presents the first national AI technology assurance legal and regulatory framework that has been implemented by a national authority empowered through law to do so. In aim of both providing assurances where required and not stifling innovation yet supporting it, herein it is proposed that such regulation should not be mandated for all AI-based systems and that rather it should primarily provide a voluntary framework and only be mandated in sectors and activities where required and as deemed necessary by other authorities for regulated and critical areas. △ Less","15 April, 2021",https://arxiv.org/pdf/2105.06267
Playing Codenames with Language Graphs and Word Embeddings,Divya Koyyalagunta;Anna Sun;Rachel Lea Draelos;Cynthia Rudin,"Although board games and video games have been studied for decades in artificial intelligence research, challenging word games remain relatively unexplored. Word games are not as constrained as games like chess or poker. Instead, word game strategy is defined by the players' understanding of the way words relate to each other. The word game Codenames provides a unique opportunity to investigate common sense understanding of relationships between words, an important open challenge. We propose an algorithm that can generate Codenames clues from the language graph BabelNet or from any of several embedding methods - word2vec, GloVe, fastText or BERT. We introduce a new scoring function that measures the quality of clues, and we propose a weighting term called DETECT that incorporates dictionary-based word representations and document frequency to improve clue selection. We develop BabelNet-Word Selection Framework (BabelNet-WSF) to improve BabelNet clue quality and overcome the computational barriers that previously prevented leveraging language graphs for Codenames. Extensive experiments with human evaluators demonstrate that our proposed innovations yield state-of-the-art performance, with up to 102.8% improvement in precision@2 in some cases. Overall, this work advances the formal study of word games and approaches for common sense language understanding. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.05885
The FeatureCloud AI Store for Federated Learning in Biomedicine and Beyond,Julian Matschinske;Julian Späth;Reza Nasirigerdeh;Reihaneh Torkzadehmahani;Anne Hartebrodt;Balázs Orbán;Sándor Fejér;Olga Zolotareva;Mohammad Bakhtiari;Béla Bihari;Marcus Bloice;Nina C Donner;Walid Fdhila;Tobias Frisch;Anne-Christin Hauschild;Dominik Heider;Andreas Holzinger;Walter Hötzendorfer;Jan Hospes;Tim Kacprowski;Markus Kastelitz;Markus List;Rudolf Mayer;Mónika Moga;Heimo Müller,"Machine Learning (ML) and Artificial Intelligence (AI) have shown promising results in many areas and are driven by the increasing amount of available data. However, this data is often distributed across different institutions and cannot be shared due to privacy concerns. Privacy-preserving methods, such as Federated Learning (FL), allow for training ML models without sharing sensitive data, but their implementation is time-consuming and requires advanced programming skills. Here, we present the FeatureCloud AI Store for FL as an all-in-one platform for biomedical research and other applications. It removes large parts of this complexity for developers and end-users by providing an extensible AI Store with a collection of ready-to-use apps. We show that the federated apps produce similar results to centralized ML, scale well for a typical number of collaborators and can be combined with Secure Multiparty Computation (SMPC), thereby making FL algorithms safely and easily applicable in biomedical and clinical environments. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.05734
Interpretable performance analysis towards offline reinforcement learning: A dataset perspective,Chenyang Xi;Bo Tang;Jiajun Shen;Xinfu Liu;Feiyu Xiong;Xueying Li,"Offline reinforcement learning (RL) has increasingly become the focus of the artificial intelligent research due to its wide real-world applications where the collection of data may be difficult, time-consuming, or costly. In this paper, we first propose a two-fold taxonomy for existing offline RL algorithms from the perspective of exploration and exploitation tendency. Secondly, we derive the explicit expression of the upper bound of extrapolation error and explore the correlation between the performance of different types of algorithms and the distribution of actions under states. Specifically, we relax the strict assumption on the sufficiently large amount of state-action tuples. Accordingly, we provably explain why batch constrained Q-learning (BCQ) performs better than other existing techniques. Thirdly, after identifying the weakness of BCQ on dataset of low mean episode returns, we propose a modified variant based on top return selection mechanism, which is proved to be able to gain state-of-the-art performance on various datasets. Lastly, we create a benchmark platform on the Atari domain, entitled RL easy go (RLEG), at an estimated cost of more than 0.3 million dollars. We make it open-source for fair and comprehensive competitions between offline RL algorithms with complete datasets and checkpoints being provided. △ Less","12 May, 2021",https://arxiv.org/pdf/2105.05473
Neuro-Symbolic Artificial Intelligence: Current Trends,Md Kamruzzaman Sarker;Lu Zhou;Aaron Eberhart;Pascal Hitzler,"Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods with methods that are based on artificial neural networks -- has a long-standing history. In this article, we provide a structured overview of current trends, by means of categorizing recent publications from key conferences. The article is meant to serve as a convenient starting point for research on the general topic. △ Less","14 May, 2021",https://arxiv.org/pdf/2105.05330
Intelligent interactive technologies for mental health and well-being,Mladjan Jovanovic;Aleksandar Jevremovic;Milica Pejovic-Milovancevic,"Mental healthcare has seen numerous benefits from interactive technologies and artificial intelligence. Various interventions have successfully used intelligent technologies to automate the assessment and evaluation of psychological treatments and mental well-being and functioning. These technologies include different types of robots, video games, and conversational agents. The paper critically analyzes existing solutions with the outlooks for their future. In particular, we: i)give an overview of the technology for mental health, ii) critically analyze the technology against the proposed criteria, and iii) provide the design outlooks for these technologies. △ Less","11 May, 2021",https://arxiv.org/pdf/2105.05306
ANDREAS: Artificial intelligence traiNing scheDuler foR accElerAted resource clusterS,Federica Filippini;Danilo Ardagna;Marco Lattuada;Edoardo Amaldi;Michele Ciavotta;Maciek Riedl;Katarzyna Materka;Paweł Skrzypek;Fabrizio Magugliani;Marco Cicala,"Artificial Intelligence (AI) and Deep Learning (DL) algorithms are currently applied to a wide range of products and solutions. DL training jobs are highly resource demanding and they experience great benefits when exploiting AI accelerators (e.g., GPUs). However, the effective management of GPU-powered clusters comes with great challenges. Among these, efficient scheduling and resource allocation solutions are crucial to maximize performance and minimize Data Centers operational costs. In this paper we propose ANDREAS, an advanced scheduling solution that tackles these problems jointly, aiming at optimizing DL training runtime workloads and their energy consumption in accelerated clusters. Experiments based on simulation demostrate that we can achieve a cost reduction between 30 and 62% on average with respect to first-principle methods while the validation on a real cluster shows a worst case deviation below 13% between actual and predicted costs, proving the effectiveness of ANDREAS solution in practical scenarios. △ Less","11 May, 2021",https://arxiv.org/pdf/2105.05080
Applications of Deep Learning Techniques for Automated Multiple Sclerosis Detection Using Magnetic Resonance Imaging: A Review,Afshin Shoeibi;Marjane Khodatars;Mahboobeh Jafari;Parisa Moridian;Mitra Rezaei;Roohallah Alizadehsani;Fahime Khozeimeh;Juan Manuel Gorriz;Jónathan Heras;Maryam Panahiazar;Saeid Nahavandi;U. Rajendra Acharya,"Multiple Sclerosis (MS) is a type of brain disease which causes visual, sensory, and motor problems for people with a detrimental effect on the functioning of the nervous system. In order to diagnose MS, multiple screening methods have been proposed so far; among them, magnetic resonance imaging (MRI) has received considerable attention among physicians. MRI modalities provide physicians with fundamental information about the structure and function of the brain, which is crucial for the rapid diagnosis of MS lesions. Diagnosing MS using MRI is time-consuming, tedious, and prone to manual errors. Hence, computer aided diagnosis systems (CADS) based on artificial intelligence (AI) methods have been proposed in recent years for accurate diagnosis of MS using MRI neuroimaging modalities. In the AI field, automated MS diagnosis is being conducted using (i) conventional machine learning and (ii) deep learning (DL) techniques. The conventional machine learning approach is based on feature extraction and selection by trial and error. In DL, these steps are performed by the DL model itself. In this paper, a complete review of automated MS diagnosis methods performed using DL techniques with MRI neuroimaging modalities are discussed. Also, each work is thoroughly reviewed and discussed. Finally, the most important challenges and future directions in the automated MS diagnosis using DL techniques coupled with MRI modalities are presented in detail. △ Less","9 August, 2021",https://arxiv.org/pdf/2105.04881
What Will the Future of UAV Cellular Communications Be? A Flight from 5G to 6G,Giovanni Geraci;Adrian Garcia-Rodriguez;M. Mahdi Azari;Angel Lozano;Marco Mezzavilla;Symeon Chatzinotas;Yun Chen;Sundeep Rangan;Marco Di Renzo,"What will the future of UAV cellular communications be? In this tutorial article, we address such a compelling yet difficult question by embarking on a journey from 5G to 6G and sharing a large number of realistic case studies supported by original results. We start by overviewing the status quo on UAV communications from an industrial standpoint, providing fresh updates from the 3GPP and detailing new 5G NR features in support of aerial devices. We then show the potential and the limitations of such features. In particular, we demonstrate how sub-6 GHz massive MIMO can successfully tackle cell selection and interference challenges, we showcase encouraging mmWave coverage evaluations in both urban and suburban/rural settings, and we examine the peculiarities of direct device-to-device communications in the sky. Moving on, we sneak a peek at next-generation UAV communications, listing some of the use cases envisioned for the 2030s. We identify the most promising 6G enablers for UAV communication, those expected to take the performance and reliability to the next level. For each of these disruptive new paradigms (non-terrestrial networks, cell-free architectures, artificial intelligence, reconfigurable intelligent surfaces, and THz communications), we gauge the prospective benefits for UAVs and discuss the main technological hurdles that stand in the way. All along, we distil our numerous findings into essential takeaways, and we identify key open problems worthy of further study. △ Less","11 May, 2021",https://arxiv.org/pdf/2105.04842
Unpacking the Expressed Consequences of AI Research in Broader Impact Statements,Priyanka Nanayakkara;Jessica Hullman;Nicholas Diakopoulos,"The computer science research community and the broader public have become increasingly aware of negative consequences of algorithmic systems. In response, the top-tier Neural Information Processing Systems (NeurIPS) conference for machine learning and artificial intelligence research required that authors include a statement of broader impact to reflect on potential positive and negative consequences of their work. We present the results of a qualitative thematic analysis of a sample of statements written for the 2020 conference. The themes we identify broadly fall into categories related to how consequences are expressed (e.g., valence, specificity, uncertainty), areas of impacts expressed (e.g., bias, the environment, labor, privacy), and researchers' recommendations for mitigating negative consequences in the future. In light of our results, we offer perspectives on how the broader impact statement can be implemented in future iterations to better align with potential goals. △ Less","22 May, 2021",https://arxiv.org/pdf/2105.04760
"Neuroscience-inspired perception-action in robotics: applying active inference for state estimation, control and self-perception",Pablo Lanillos;Marcel van Gerven,"Unlike robots, humans learn, adapt and perceive their bodies by interacting with the world. Discovering how the brain represents the body and generates actions is of major importance for robotics and artificial intelligence. Here we discuss how neuroscience findings open up opportunities to improve current estimation and control algorithms in robotics. In particular, how active inference, a mathematical formulation of how the brain resists a natural tendency to disorder, provides a unified recipe to potentially solve some of the major challenges in robotics, such as adaptation, robustness, flexibility, generalization and safe interaction. This paper summarizes some experiments and lessons learned from developing such a computational model on real embodied platforms, i.e., humanoid and industrial robots. Finally, we showcase the limitations and challenges that we are still facing to give robots human-like perception △ Less","10 May, 2021",https://arxiv.org/pdf/2105.04261
Fast constraint satisfaction problem and learning-based algorithm for solving Minesweeper,Yash Pratyush Sinha;Pranshu Malviya;Rupaj Kumar Nayak,"Minesweeper is a popular spatial-based decision-making game that works with incomplete information. As an exemplary NP-complete problem, it is a major area of research employing various artificial intelligence paradigms. The present work models this game as Constraint Satisfaction Problem (CSP) and Markov Decision Process (MDP). We propose a new method named as dependents from the independent set using deterministic solution search (DSScsp) for the faster enumeration of all solutions of a CSP based Minesweeper game and improve the results by introducing heuristics. Using MDP, we implement machine learning methods on these heuristics. We train the classification model on sparse data with results from CSP formulation. We also propose a new rewarding method for applying a modified deep Q-learning for better accuracy and versatile learning in the Minesweeper game. The overall results have been analyzed for different kinds of Minesweeper games and their accuracies have been recorded. Results from these experiments show that the proposed method of MDP based classification model and deep Q-learning overall is the best methods in terms of accuracy for games with given mine densities. △ Less","10 May, 2021",https://arxiv.org/pdf/2105.04120
Swarm Differential Privacy for Purpose Driven Data-Information-Knowledge-Wisdom Architecture,Yingbo Li;Yucong Duan;Zakaria Maama;Haoyang Che;Anamaria-Beatrice Spulber;Stelios Fuentes,"Privacy protection has recently been in the spotlight of attention to both academia and industry. Society protects individual data privacy through complex legal frameworks. The increasing number of applications of data science and artificial intelligence has resulted in a higher demand for the ubiquitous application of the data. The privacy protection of the broad Data-Information-Knowledge-Wisdom (DIKW) landscape, the next generation of information organization, has taken a secondary role. In this paper, we will explore DIKW architecture through the applications of the popular swarm intelligence and differential privacy. As differential privacy proved to be an effective data privacy approach, we will look at it from a DIKW domain perspective. Swarm Intelligence can effectively optimize and reduce the number of items in DIKW used in differential privacy, thus accelerating both the effectiveness and the efficiency of differential privacy for crossing multiple modals of conceptual DIKW. The proposed approach is demonstrated through the application of personalized data that is based on the open-sourse IRIS dataset. This experiment demonstrates the efficiency of Swarm Intelligence in reducing computing complexity. △ Less","29 June, 2021",https://arxiv.org/pdf/2105.04045
Security Concerns on Machine Learning Solutions for 6G Networks in mmWave Beam Prediction,Ferhat Ozgur Catak;Evren Catak;Murat Kuzlu;Umit Cali;Devrim Unal,"6G -- sixth generation -- is the latest cellular technology currently under development for wireless communication systems. In recent years, machine learning algorithms have been applied widely in various fields, such as healthcare, transportation, energy, autonomous car, and many more. Those algorithms have been also using in communication technologies to improve the system performance in terms of frequency spectrum usage, latency, and security. With the rapid developments of machine learning techniques, especially deep learning, it is critical to take the security concern into account when applying the algorithms. While machine learning algorithms offer significant advantages for 6G networks, security concerns on Artificial Intelligent (AI) models is typically ignored by the scientific community so far. However, security is also a vital part of the AI algorithms, this is because the AI model itself can be poisoned by attackers. This paper proposes a mitigation method for adversarial attacks against proposed 6G machine learning models for the millimeter-wave (mmWave) beam prediction using adversarial learning. The main idea behind adversarial attacks against machine learning models is to produce faulty results by manipulating trained deep learning models for 6G applications for mmWave beam prediction. We also present the adversarial learning mitigation method's performance for 6G security in mmWave beam prediction application with fast gradient sign method attack. The mean square errors (MSE) of the defended model under attack are very close to the undefended model without attack. △ Less","23 July, 2021",https://arxiv.org/pdf/2105.03905
Lawformer: A Pre-trained Language Model for Chinese Legal Long Documents,Chaojun Xiao;Xueyu Hu;Zhiyuan Liu;Cunchao Tu;Maosong Sun,"Legal artificial intelligence (LegalAI) aims to benefit legal systems with the technology of artificial intelligence, especially natural language processing (NLP). Recently, inspired by the success of pre-trained language models (PLMs) in the generic domain, many LegalAI researchers devote their effort to apply PLMs to legal tasks. However, utilizing PLMs to address legal tasks is still challenging, as the legal documents usually consist of thousands of tokens, which is far longer than the length that mainstream PLMs can process. In this paper, we release the Longformer-based pre-trained language model, named as Lawformer, for Chinese legal long documents understanding. We evaluate Lawformer on a variety of LegalAI tasks, including judgment prediction, similar case retrieval, legal reading comprehension, and legal question answering. The experimental results demonstrate that our model can achieve promising improvement on tasks with long documents as inputs. △ Less","9 May, 2021",https://arxiv.org/pdf/2105.03887
Reinforcement Learning with Expert Trajectory For Quantitative Trading,Sihang Chen;Weiqi Luo;Chao Yu,"In recent years, quantitative investment methods combined with artificial intelligence have attracted more and more attention from investors and researchers. Existing related methods based on the supervised learning are not very suitable for learning problems with long-term goals and delayed rewards in real futures trading. In this paper, therefore, we model the price prediction problem as a Markov decision process (MDP), and optimize it by reinforcement learning with expert trajectory. In the proposed method, we employ more than 100 short-term alpha factors instead of price, volume and several technical factors in used existing methods to describe the states of MDP. Furthermore, unlike DQN (deep Q-learning) and BC (behavior cloning) in related methods, we introduce expert experience in training stage, and consider both the expert-environment interaction and the agent-environment interaction to design the temporal difference error so that the agents are more adaptable for inevitable noise in financial data. Experimental results evaluated on share price index futures in China, including IF (CSI 300) and IC (CSI 500), show that the advantages of the proposed method compared with three typical technical analysis and two deep leaning based methods. △ Less","9 May, 2021",https://arxiv.org/pdf/2105.03844
A parallel-network continuous quantitative trading model with GARCH and PPO,Zhishun Wang;Wei Lu;Kaixin Zhang;Tianhao Li;Zixi Zhao,"It is a difficult task for both professional investors and individual traders continuously making profit in stock market. With the development of computer science and deep reinforcement learning, Buy\&Hold (B\&H) has been oversteped by many artificial intelligence trading algorithms. However, the information and process are not enough, which limit the performance of reinforcement learning algorithms. Thus, we propose a parallel-network continuous quantitative trading model with GARCH and PPO to enrich the basical deep reinforcement learning model, where the deep learning parallel network layers deal with 3 different frequencies data (including GARCH information) and proximal policy optimization (PPO) algorithm interacts actions and rewards with stock trading environment. Experiments in 5 stocks from Chinese stock market show our method achieves more extra profit comparing with basical reinforcement learning methods and bench models. △ Less","21 May, 2021",https://arxiv.org/pdf/2105.03625
The Synergy of Complex Event Processing and Tiny Machine Learning in Industrial IoT,Haoyu Ren;Darko Anicic;Thomas Runkler,"Focusing on comprehensive networking, big data, and artificial intelligence, the Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness in factory operations. Various sensors and field devices play a central role, as they generate a vast amount of real-time data that can provide insights into manufacturing. The synergy of complex event processing (CEP) and machine learning (ML) has been developed actively in the last years in IIoT to identify patterns in heterogeneous data streams and fuse raw data into tangible facts. In a traditional compute-centric paradigm, the raw field data are continuously sent to the cloud and processed centrally. As IIoT devices become increasingly pervasive and ubiquitous, concerns are raised since transmitting such amount of data is energy-intensive, vulnerable to be intercepted, and subjected to high latency. The data-centric paradigm can essentially solve these problems by empowering IIoT to perform decentralized on-device ML and CEP, keeping data primarily on edge devices and minimizing communications. However, this is no mean feat because most IIoT edge devices are designed to be computationally constrained with low power consumption. This paper proposes a framework that exploits ML and CEP's synergy at the edge in distributed sensor networks. By leveraging tiny ML and micro CEP, we shift the computation from the cloud to the power-constrained IIoT devices and allow users to adapt the on-device ML model and the CEP reasoning logic flexibly on the fly without requiring to reupload the whole program. Lastly, we evaluate the proposed solution and show its effectiveness and feasibility using an industrial use case of machine safety monitoring. △ Less","4 May, 2021",https://arxiv.org/pdf/2105.03371
Finding the unicorn: Predicting early stage startup success through a hybrid intelligence method,Dominik Dellermann;Nikolaus Lipusch;Philipp Ebel;Karl Michael Popp;Jan Marco Leimeister,"Artificial intelligence is an emerging topic and will soon be able to perform decisions better than humans. In more complex and creative contexts such as innovation, however, the question remains whether machines are superior to humans. Machines fail in two kinds of situations: processing and interpreting soft information (information that cannot be quantified) and making predictions in unknowable risk situations of extreme uncertainty. In such situations, the machine does not have representative information for a certain outcome. Thereby, humans are still the gold standard for assessing soft signals and make use of intuition. To predict the success of startups, we, thus, combine the complementary capabilities of humans and machines in a Hybrid Intelligence method. To reach our aim, we follow a design science research approach to develop a Hybrid Intelligence method that combines the strength of both machine and collective intelligence to demonstrate its utility for predictions under extreme uncertainty. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03360
The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems,Dominik Dellermann;Adrian Calma;Nikolaus Lipusch;Thorsten Weber;Sascha Weigel;Philipp Ebel,"Recent technological advances, especially in the field of machine learning, provide astonishing progress on the road towards artificial general intelligence. However, tasks in current real-world business applications cannot yet be solved by machines alone. We, therefore, identify the need for developing socio-technological ensembles of humans and machines. Such systems possess the ability to accomplish complex goals by combining human and artificial intelligence to collectively achieve superior results and continuously improve by learning from each other. Thus, the need for structured design knowledge for those systems arises. Following a taxonomy development method, this article provides three main contributions: First, we present a structured overview of interdisciplinary research on the role of humans in the machine learning pipeline. Second, we envision hybrid intelligence systems and conceptualize the relevant dimensions for system design for the first time. Finally, we offer useful guidance for system developers during the implementation of such applications. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03354
Energy-Efficient AI over a Virtualized Cloud Fog Network,Barzan A. Yosuf;Sanaa H. Mohamed;Mohamed Alenazi;Taisir E. H. El-Gorashi;Jaafar M. H. Elmirghani,"Deep Neural Networks (DNNs) have served as a catalyst in introducing a plethora of next-generation services in the era of Internet of Things (IoT), thanks to the availability of massive amounts of data collected by the objects on the edge. Currently, DNN models are used to deliver many Artificial Intelligence (AI) services that include image and natural language processing, speech recognition, and robotics. Accordingly, such services utilize various DNN models that make it computationally intensive for deployment on the edge devices alone. Thus, most AI models are offloaded to distant cloud data centers (CDCs), which tend to consolidate large amounts of computing and storage resources into one or more CDCs. Deploying services in the CDC will inevitably lead to excessive latencies and overall increase in power consumption. Instead, fog computing allows for cloud services to be extended to the edge of the network, which allows for data processing to be performed closer to the end-user device. However, different from cloud data centers, fog nodes have limited computational power and are highly distributed in the network. In this paper, using Mixed Integer Linear Programming (MILP), we formulate the placement of DNN inference models, which is abstracted as a network embedding problem in a Cloud Fog Network (CFN) architecture, where power savings are introduced through trade-offs between processing and networking. We study the performance of the CFN architecture by comparing the energy savings when compared to the baseline approach which is the CDC. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03221
AI in (and for) Games,Kostas Karpouzis;George Tsatiris,"This chapter outlines the relation between artificial intelligence (AI) / machine learning (ML) algorithms and digital games. This relation is two-fold: on one hand, AI/ML researchers can generate large, in-the-wild datasets of human affective activity, player behaviour (i.e. actions within the game world), commercial behaviour, interaction with graphical user interface elements or messaging with other players, while games can utilise intelligent algorithms to automate testing of game levels, generate content, develop intelligent and responsive non-player characters (NPCs) or predict and respond player behaviour across a wide variety of player cultures. In this work, we discuss some of the most common and widely accepted uses of AI/ML in games and how intelligent systems can benefit from those, elaborating on estimating player experience based on expressivity and performance, and on generating proper and interesting content for a language learning game. △ Less","7 May, 2021",https://arxiv.org/pdf/2105.03123
Deep reinforcement learning-designed radiofrequency waveform in MRI,Dongmyung Shin;Younghoon Kim;Chungseok Oh;Hongjun An;Juhyung Park;Jiye Kim;Jongho Lee,"Carefully engineered radiofrequency (RF) pulses play a key role in a number of systems such as mobile phone, radar, and magnetic resonance imaging. The design of an RF waveform, however, is often posed as an inverse problem with no general solution. As a result, various design methods each with a specific purpose have been developed based on the intuition of human experts. In this work, we propose an artificial intelligence (AI)-powered RF pulse design framework, DeepRF, which utilizes the self-learning characteristics of deep reinforcement learning to generate a novel RF pulse. The effectiveness of DeepRF is demonstrated using four types of RF pulses that are commonly used. The DeepRF-designed pulses successfully satisfy the design criteria while reporting reduced energy. Analyses demonstrate the pulses utilize new mechanisms of magnetization manipulation, suggesting the potentials of DeepRF in discovering unseen design dimensions beyond human intuition. This work may lay the foundation for an emerging field of AI-driven RF waveform design. △ Less","18 November, 2021",https://arxiv.org/pdf/2105.03061
fAshIon after fashion: A Report of AI in Fashion,Xingxing Zou;Waikeung Wong,"In this independent report fAshIon after fashion, we examine the development of fAshIon (artificial intelligence (AI) in fashion) and explore its potentiality to become a major disruptor of the fashion industry in the near future. To do this, we investigate AI technologies used in the fashion industry through several lenses. We summarise fAshIon studies conducted over the past decade and categorise them into seven groups: Overview, Evaluation, Basic Tech, Selling, Styling, Design, and Buying. The datasets mentioned in fAshIon research have been consolidated on one GitHub page for ease of use. We analyse the authors' backgrounds and the geographic regions treated in these studies to determine the landscape of fAshIon research. The results of our analysis are presented with an aim to provide researchers with a holistic view of research in fAshIon. As part of our primary research, we also review a wide range of cases of applied fAshIon in the fashion industry and analyse their impact on the industry, markets and individuals. We also identify the challenges presented by fAshIon and suggest that these may form the basis for future research. We finally exhibit that many potential opportunities exist for the use of AI in fashion which can transform the fashion industry embedded with AI technologies and boost profits. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.03050
Recognition of handwritten MNIST digits on low-memory 2 Kb RAM Arduino board using LogNNet reservoir neural network,Y. A. Izotov;A. A. Velichko;A. A. Ivshin;R. E. Novitskiy,"The presented compact algorithm for recognizing handwritten digits of the MNIST database, created on the LogNNet reservoir neural network, reaches the recognition accuracy of 82%. The algorithm was tested on a low-memory Arduino board with 2 Kb static RAM low-power microcontroller. The dependences of the accuracy and time of image recognition on the number of neurons in the reservoir have been investigated. The memory allocation demonstrates that the algorithm stores all the necessary information in RAM without using additional data storage, and operates with original images without preliminary processing. The simple structure of the algorithm, with appropriate training, can be adapted for wide practical application, for example, for creating mobile biosensors for early diagnosis of adverse events in medicine. The study results are important for the implementation of artificial intelligence on peripheral constrained IoT devices and for edge computing. △ Less","20 April, 2021",https://arxiv.org/pdf/2105.02953
AI Risk Skepticism,Roman V. Yampolskiy,"In this work, we survey skepticism regarding AI risk and show parallels with other types of scientific skepticism. We start by classifying different types of AI Risk skepticism and analyze their root causes. We conclude by suggesting some intervention approaches, which may be successful in reducing AI risk skepticism, at least amongst artificial intelligence researchers. △ Less","17 July, 2021",https://arxiv.org/pdf/2105.02704
Quantification of pulmonary involvement in COVID-19 pneumonia by means of a cascade oftwo U-nets: training and assessment on multipledatasets using different annotation criteria,Francesca Lizzi;Abramo Agosti;Francesca Brero;Raffaella Fiamma Cabini;Maria Evelina Fantacci;Silvia Figini;Alessandro Lascialfari;Francesco Laruina;Piernicola Oliva;Stefano Piffer;Ian Postuma;Lisa Rinaldi;Cinzia Talamonti;Alessandra Retico,"The automatic assignment of a severity score to the CT scans of patients affected by COVID-19 pneumonia could reduce the workload in radiology departments. This study aims at exploiting Artificial intelligence (AI) for the identification, segmentation and quantification of COVID-19 pulmonary lesions. We investigated the effects of using multiple datasets, heterogeneously populated and annotated according to different criteria. We developed an automated analysis pipeline, the LungQuant system, based on a cascade of two U-nets. The first one (U-net_1) is devoted to the identification of the lung parenchyma, the second one (U-net_2) acts on a bounding box enclosing the segmented lungs to identify the areas affected by COVID-19 lesions. Different public datasets were used to train the U-nets and to evaluate their segmentation performances, which have been quantified in terms of the Dice index. The accuracy in predicting the CT-Severity Score (CT-SS) of the LungQuant system has been also evaluated. Both Dice and accuracy showed a dependency on the quality of annotations of the available data samples. On an independent and publicly available benchmark dataset, the Dice values measured between the masks predicted by LungQuant system and the reference ones were 0.95\pm0.01 and 0.66\pm0.13 for the segmentation of lungs and COVID-19 lesions, respectively. The accuracy of 90% in the identification of the CT-SS on this benchmark dataset was achieved. We analysed the impact of using data samples with different annotation criteria in training an AI-based quantification system for pulmonary involvement in COVID-19 pneumonia. In terms of the Dice index, the U-net segmentation quality strongly depends on the quality of the lesion annotations. Nevertheless, the CT-SS can be accurately predicted on independent validation sets, demonstrating the satisfactory generalization ability of the LungQuant. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02566
Federated Face Recognition,Fan Bai;Jiaxiang Wu;Pengcheng Shen;Shaoxin Li;Shuigeng Zhou,"Face recognition has been extensively studied in computer vision and artificial intelligence communities in recent years. An important issue of face recognition is data privacy, which receives more and more public concerns. As a common privacy-preserving technique, Federated Learning is proposed to train a model cooperatively without sharing data between parties. However, as far as we know, it has not been successfully applied in face recognition. This paper proposes a framework named FedFace to innovate federated learning for face recognition. Specifically, FedFace relies on two major innovative algorithms, Partially Federated Momentum (PFM) and Federated Validation (FV). PFM locally applies an estimated equivalent global momentum to approximating the centralized momentum-SGD efficiently. FV repeatedly searches for better federated aggregating weightings via testing the aggregated models on some private validation datasets, which can improve the model's generalization ability. The ablation study and extensive experiments validate the effectiveness of the FedFace method and show that it is comparable to or even better than the centralized baseline in performance. △ Less","6 May, 2021",https://arxiv.org/pdf/2105.02501
Explainable Artificial Intelligence for Human Decision-Support System in Medical Domain,Samanta Knapič;Avleen Malhi;Rohit Saluja;Kary Främling,"In the present paper we present the potential of Explainable Artificial Intelligence methods for decision-support in medical image analysis scenarios. With three types of explainable methods applied to the same medical image data set our aim was to improve the comprehensibility of the decisions provided by the Convolutional Neural Network (CNN). The visual explanations were provided on in-vivo gastral images obtained from a Video capsule endoscopy (VCE), with the goal of increasing the health professionals' trust in the black box predictions. We implemented two post-hoc interpretable machine learning methods LIME and SHAP and the alternative explanation approach CIU, centered on the Contextual Value and Utility (CIU). The produced explanations were evaluated using human evaluation. We conducted three user studies based on the explanations provided by LIME, SHAP and CIU. Users from different non-medical backgrounds carried out a series of tests in the web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n=20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We have found that, as hypothesized, the CIU explainable method performed better than both LIME and SHAP methods in terms of increasing support for human decision-making as well as being more transparent and thus understandable to users. Additionally, CIU outperformed LIME and SHAP by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that can with future improvements in implementation be generalized on different medical data sets and can provide great decision-support for medical experts. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.02357
Foundations of Intelligence in Natural and Artificial Systems: A Workshop Report,Tyler Millhouse;Melanie Moses;Melanie Mitchell,"In March of 2021, the Santa Fe Institute hosted a workshop as part of its Foundations of Intelligence in Natural and Artificial Systems project. This project seeks to advance the field of artificial intelligence by promoting interdisciplinary research on the nature of intelligence. During the workshop, speakers from diverse disciplines gathered to develop a taxonomy of intelligence, articulating their own understanding of intelligence and how their research has furthered that understanding. In this report, we summarize the insights offered by each speaker and identify the themes that emerged during the talks and subsequent discussions. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.02198
Ethics and Governance of Artificial Intelligence: Evidence from a Survey of Machine Learning Researchers,Baobao Zhang;Markus Anderljung;Lauren Kahn;Noemi Dreksler;Michael C. Horowitz;Allan Dafoe,"Machine learning (ML) and artificial intelligence (AI) researchers play an important role in the ethics and governance of AI, including taking action against what they perceive to be unethical uses of AI (Belfield, 2020; Van Noorden, 2020). Nevertheless, this influential group's attitudes are not well understood, which undermines our ability to discern consensuses or disagreements between AI/ML researchers. To examine these researchers' views, we conducted a survey of those who published in the top AI/ML conferences (N = 524). We compare these results with those from a 2016 survey of AI/ML researchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey of the US public (Zhang & Dafoe, 2020). We find that AI/ML researchers place high levels of trust in international organizations and scientific organizations to shape the development and use of AI in the public interest; moderate trust in most Western tech companies; and low trust in national militaries, Chinese tech companies, and Facebook. While the respondents were overwhelmingly opposed to AI/ML researchers working on lethal autonomous weapons, they are less opposed to researchers working on other military applications of AI, particularly logistics algorithms. A strong majority of respondents think that AI safety research should be prioritized and that ML institutions should conduct pre-publication review to assess potential harms. Being closer to the technology itself, AI/ML re-searchers are well placed to highlight new risks and develop technical solutions, so this novel attempt to measure their attitudes has broad relevance. The findings should help to improve how researchers, private sector executives, and policymakers think about regulations, governance frameworks, guiding principles, and national and international governance strategies for AI. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.02117
Schematic Memory Persistence and Transience for Efficient and Robust Continual Learning,Yuyang Gao;Giorgio A. Ascoli;Liang Zhao,"Continual learning is considered a promising step towards next-generation Artificial Intelligence (AI), where deep neural networks (DNNs) make decisions by continuously learning a sequence of different tasks akin to human learning processes. It is still quite primitive, with existing works focusing primarily on avoiding (catastrophic) forgetting. However, since forgetting is inevitable given bounded memory and unbounded task loads, 'how to reasonably forget' is a problem continual learning must address in order to reduce the performance gap between AIs and humans, in terms of 1) memory efficiency, 2) generalizability, and 3) robustness when dealing with noisy data. To address this, we propose a novel ScheMAtic memory peRsistence and Transience (SMART) framework for continual learning with external memory that builds on recent advances in neuroscience. The efficiency and generalizability are enhanced by a novel long-term forgetting mechanism and schematic memory, using sparsity and 'backward positive transfer' constraints with theoretical guarantees on the error bound. Robust enhancement is achieved using a novel short-term forgetting mechanism inspired by background information-gated learning. Finally, an extensive experimental analysis on both benchmark and real-world datasets demonstrates the effectiveness and efficiency of our model. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.02085
XAI-KG: knowledge graph to support XAI and decision-making in manufacturing,Jože M. Rožanec;Patrik Zajec;Klemen Kenda;Inna Novalija;Blaž Fortuna;Dunja Mladenić,"The increasing adoption of artificial intelligence requires accurate forecasts and means to understand the reasoning of artificial intelligence models behind such a forecast. Explainable Artificial Intelligence (XAI) aims to provide cues for why a model issued a certain prediction. Such cues are of utmost importance to decision-making since they provide insights on the features that influenced most certain forecasts and let the user decide if the forecast can be trusted. Though many techniques were developed to explain black-box models, little research was done on assessing the quality of those explanations and their influence on decision-making. We propose an ontology and knowledge graph to support collecting feedback regarding forecasts, forecast explanations, recommended decision-making options, and user actions. This way, we provide means to improve forecasting models, explanations, and recommendations of decision-making options. We tailor the knowledge graph for the domain of demand forecasting and validate it on real-world data. △ Less","5 May, 2021",https://arxiv.org/pdf/2105.01929
Envisioning Communities: A Participatory Approach Towards AI for Social Good,Elizabeth Bondi;Lily Xu;Diana Acosta-Navas;Jackson A. Killian,"Research in artificial intelligence (AI) for social good presupposes some definition of social good, but potential definitions have been seldom suggested and never agreed upon. The normative question of what AI for social good research should be ""for"" is not thoughtfully elaborated, or is frequently addressed with a utilitarian outlook that prioritizes the needs of the majority over those who have been historically marginalized, brushing aside realities of injustice and inequity. We argue that AI for social good ought to be assessed by the communities that the AI system will impact, using as a guide the capabilities approach, a framework to measure the ability of different policies to improve human welfare equity. Furthermore, we lay out how AI research has the potential to catalyze social progress by expanding and equalizing capabilities. We show how the capabilities approach aligns with a participatory approach for the design and implementation of AI for social good research in a framework we introduce called PACT, in which community members affected should be brought in as partners and their input prioritized throughout the project. We conclude by providing an incomplete set of guiding questions for carrying out such participatory AI research in a way that elicits and respects a community's own definition of social good. △ Less","18 June, 2021",https://arxiv.org/pdf/2105.01774
Towards Error Measures which Influence a Learners Inductive Bias to the Ground Truth,A. I. Parkes;A. J. Sobey;D. A. Hudson,"Artificial intelligence is applied in a range of sectors, and is relied upon for decisions requiring a high level of trust. For regression methods, trust is increased if they approximate the true input-output relationships and perform accurately outside the bounds of the training data. But often performance off-test-set is poor, especially when data is sparse. This is because the conditional average, which in many scenarios is a good approximation of the `ground truth', is only modelled with conventional Minkowski-r error measures when the data set adheres to restrictive assumptions, with many real data sets violating these. To combat this there are several methods that use prior knowledge to approximate the `ground truth'. However, prior knowledge is not always available, and this paper investigates how error measures affect the ability for a regression method to model the `ground truth' in these scenarios. Current error measures are shown to create an unhelpful bias and a new error measure is derived which does not exhibit this behaviour. This is tested on 36 representative data sets with different characteristics, showing that it is more consistent in determining the `ground truth' and in giving improved predictions in regions beyond the range of the training data. △ Less","4 May, 2021",https://arxiv.org/pdf/2105.01567
Archives and AI: An Overview of Current Debates and Future Perspectives,Giovanni Colavizza;Tobias Blanke;Charles Jeurgens;Julia Noordegraaf,"The digital transformation is turning archives, both old and new, into data. As a consequence, automation in the form of artificial intelligence techniques is increasingly applied both to scale traditional recordkeeping activities, and to experiment with novel ways to capture, organise and access records. We survey recent developments at the intersection of Artificial Intelligence and archival thinking and practice. Our overview of this growing body of literature is organised through the lenses of the Records Continuum model. We find four broad themes in the literature on archives and artificial intelligence: theoretical and professional considerations, the automation of recordkeeping processes, organising and accessing archives, and novel forms of digital archives. We conclude by underlining emerging trends and directions for future work, which include the application of recordkeeping principles to the very data and processes which power modern artificial intelligence, and a more structural, yet critically-aware, integration of artificial intelligence into archival systems and practice. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.01117
Algorithms are not neutral: Bias in collaborative filtering,Catherine Stinson,"Discussions of algorithmic bias tend to focus on examples where either the data or the people building the algorithms are biased. This gives the impression that clean data and good intentions could eliminate bias. The neutrality of the algorithms themselves is defended by prominent Artificial Intelligence researchers. However, algorithms are not neutral. In addition to biased data and biased algorithm makers, AI algorithms themselves can be biased. This is illustrated with the example of collaborative filtering, which is known to suffer from popularity, and homogenizing biases. Iterative information filtering algorithms in general create a selection bias in the course of learning from user responses to documents that the algorithm recommended. These are not merely biases in the statistical sense; these statistical biases can cause discriminatory outcomes. Data points on the margins of distributions of human data tend to correspond to marginalized people. Popularity and homogenizing biases have the effect of further marginalizing the already marginal. This source of bias warrants serious attention given the ubiquity of algorithmic decision-making. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.01031
Hierarchical Reinforcement Learning for Air-to-Air Combat,Adrian P. Pope;Jaime S. Ide;Daria Micovic;Henry Diaz;David Rosenbluth;Lee Ritholtz;Jason C. Twedt;Thayne T. Walker;Kevin Alcedo;Daniel Javorsek,"Artificial Intelligence (AI) is becoming a critical component in the defense industry, as recently demonstrated by DARPA`s AlphaDogfight Trials (ADT). ADT sought to vet the feasibility of AI algorithms capable of piloting an F-16 in simulated air-to-air combat. As a participant in ADT, Lockheed Martin`s (LM) approach combines a hierarchical architecture with maximum-entropy reinforcement learning (RL), integrates expert knowledge through reward shaping, and supports modularity of policies. This approach achieved a 2^{nd} place finish in the final ADT event (among eight total competitors) and defeated a graduate of the US Air Force's (USAF) F-16 Weapons Instructor Course in match play. △ Less","11 June, 2021",https://arxiv.org/pdf/2105.00990
LiveStyle -- An Application to Transfer Artistic Styles,Amogh G. Warkhandkar;Omkar B. Bhambure,"Art is a variety of human activities that include the production of visual, auditory, or performing objects that express the creativity, creative concepts, or technological abilities of the artist, intended primarily for their beauty or emotional power to be appreciated. The renaissance of historic and forgotten art has been made possible by modern developments in Artificial Intelligence. Techniques for Computer Vision have long been related to such arts. Style Transfer using Neural Networks refers to optimization techniques, where a content image and a style image are taken and blended such that it feels like the content image is reconstructed in the style image color palette. This paper implements the Style Transfer using three different Neural Networks in form of an application that is accessible to the general population thereby reviving interest in lost art styles. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.00865
Elo Ratings for Large Tournaments of Software Agents in Asymmetric Games,Ben Wise,"The Elo rating system has been used world wide for individual sports and team sports, as exemplified by the European Go Federation (EGF), International Chess Federation (FIDE), International Federation of Association Football (FIFA), and many others. To evaluate the performance of artificial intelligence agents, it is natural to evaluate them on the same Elo scale as humans, such as the rating of 5185 attributed to AlphaGo Zero. There are several fundamental differences between humans and AI that suggest modifications to the system, which in turn require revisiting Elo's fundamental rationale. AI is typically trained on many more games than humans play, and we have little a-priori information on newly created AI agents. Further, AI is being extended into games which are asymmetric between the players, and which could even have large complex boards with different setup in every game, such as commercial paper strategy games. We present a revised rating system, and guidelines for tournaments, to reflect these differences. △ Less","23 April, 2021",https://arxiv.org/pdf/2105.00839
Natural Language Generation Using Link Grammar for General Conversational Intelligence,Vignav Ramesh;Anton Kolonin,"Many current artificial general intelligence (AGI) and natural language processing (NLP) architectures do not possess general conversational intelligence--that is, they either do not deal with language or are unable to convey knowledge in a form similar to the human language without manual, labor-intensive methods such as template-based customization. In this paper, we propose a new technique to automatically generate grammatically valid sentences using the Link Grammar database. This natural language generation method far outperforms current state-of-the-art baselines and may serve as the final component in a proto-AGI question answering pipeline that understandably handles natural language material. △ Less","19 April, 2021",https://arxiv.org/pdf/2105.00830
Graph Learning: A Survey,Feng Xia;Ke Sun;Shuo Yu;Abdul Aziz;Liangtian Wan;Shirui Pan;Huan Liu,"Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.00696
Hybrid Intelligence,Dominik Dellermann;Philipp Ebel;Matthias Soellner;Jan Marco Leimeister,"Research has a long history of discussing what is superior in predicting certain outcomes: statistical methods or the human brain. This debate has repeatedly been sparked off by the remarkable technological advances in the field of artificial intelligence (AI), such as solving tasks like object and speech recognition, achieving significant improvements in accuracy through deep-learning algorithms (Goodfellow et al. 2016), or combining various methods of computational intelligence, such as fuzzy logic, genetic algorithms, and case-based reasoning (Medsker 2012). One of the implicit promises that underlie these advancements is that machines will 1 day be capable of performing complex tasks or may even supersede humans in performing these tasks. This triggers new heated debates of when machines will ultimately replace humans (McAfee and Brynjolfsson 2017). While previous research has proved that AI performs well in some clearly defined tasks such as playing chess, playing Go or identifying objects on images, it is doubted that the development of an artificial general intelligence (AGI) which is able to solve multiple tasks at the same time can be achieved in the near future (e.g., Russell and Norvig 2016). Moreover, the use of AI to solve complex business problems in organizational contexts occurs scarcely, and applications for AI that solve complex problems remain mainly in laboratory settings instead of being implemented in practice. Since the road to AGI is still a long one, we argue that the most likely paradigm for the division of labor between humans and machines in the next decades is Hybrid Intelligence. This concept aims at using the complementary strengths of human intelligence and AI, so that they can perform better than each of the two could separately (e.g., Kamar 2016). △ Less","3 May, 2021",https://arxiv.org/pdf/2105.00691
Explaining how your AI system is fair,Boris Ruf;Marcin Detyniecki,"To implement fair machine learning in a sustainable way, choosing the right fairness objective is key. Since fairness is a concept of justice which comes in various, sometimes conflicting definitions, this is not a trivial task though. The most appropriate fairness definition for an artificial intelligence (AI) system is a matter of ethical standards and legal requirements, and the right choice depends on the particular use case and its context. In this position paper, we propose to use a decision tree as means to explain and justify the implemented kind of fairness to the end users. Such a structure would first of all support AI practitioners in mapping ethical principles to fairness definitions for a concrete application and therefore make the selection a straightforward and transparent process. However, this approach would also help document the reasoning behind the decision making. Due to the general complexity of the topic of fairness in AI, we argue that specifying ""fairness"" for a given use case is the best way forward to maintain confidence in AI systems. In this case, this could be achieved by sharing the reasons and principles expressed during the decision making process with the broader audience. △ Less","3 May, 2021",https://arxiv.org/pdf/2105.00667
AI-Assisted MAC for Reconfigurable Intelligent Surface-Aided Wireless Networks: Challenges and Opportunities,Xuelin Cao;Bo Yang;Chongwen Huang;Chau Yuen;Marco Di Renzo;Zhu Han;Dusit Niyato;H. Vincent Poor;Lajos Hanzo,"Recently, significant research attention has been devoted to the study of reconfigurable intelligent surfaces (RISs), which are capable of reconfiguring the wireless propagation environment by exploiting the unique properties of metamaterials-based integrated large arrays of inexpensive antennas. Existing research demonstrates that RISs significantly improve the physical layer performance, including the wireless coverage, achievable data rate and energy efficiency. However, the medium access control (MAC) of multiple users accessing an RIS-enabled channel is still in its infancy, while many open issues remain to be addressed. In this article, we present four typical RIS-aided multi-user scenarios with special emphasis on the MAC schemes. We then propose and elaborate upon centralized, distributed and hybrid artificial-intelligence (AI)-assisted MAC architectures in RIS-aided multi-user communications systems. Finally, we discuss some challenges, perspectives and potential applications of RISs as they are related to MAC design. △ Less","2 May, 2021",https://arxiv.org/pdf/2105.00437
AGMB-Transformer: Anatomy-Guided Multi-Branch Transformer Network for Automated Evaluation of Root Canal Therapy,Yunxiang Li;Guodong Zeng;Yifan Zhang;Jun Wang;Qianni Zhang;Qun Jin;Lingling Sun;Qisi Lian;Neng Xia;Ruizi Peng;Kai Tang;Yaqi Wang;Shuai Wang,"Accurate evaluation of the treatment result on X-ray images is a significant and challenging step in root canal therapy since the incorrect interpretation of the therapy results will hamper timely follow-up which is crucial to the patients' treatment outcome. Nowadays, the evaluation is performed in a manual manner, which is time-consuming, subjective, and error-prone. In this paper, we aim to automate this process by leveraging the advances in computer vision and artificial intelligence, to provide an objective and accurate method for root canal therapy result assessment. A novel anatomy-guided multi-branch Transformer (AGMB-Transformer) network is proposed, which first extracts a set of anatomy features and then uses them to guide a multi-branch Transformer network for evaluation. Specifically, we design a polynomial curve fitting segmentation strategy with the help of landmark detection to extract the anatomy features. Moreover, a branch fusion module and a multi-branch structure including our progressive Transformer and Group Multi-Head Self-Attention (GMHSA) are designed to focus on both global and local features for an accurate diagnosis. To facilitate the research, we have collected a large-scale root canal therapy evaluation dataset with 245 root canal therapy X-ray images, and the experiment results show that our AGMB-Transformer can improve the diagnosis accuracy from 57.96% to 90.20% compared with the baseline network. The proposed AGMB-Transformer can achieve a highly accurate evaluation of root canal therapy. To our best knowledge, our work is the first to perform automatic root canal therapy evaluation and has important clinical value to reduce the workload of endodontists. △ Less","28 October, 2021",https://arxiv.org/pdf/2105.00381
Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary Results,Harish Panneer Selvam;Yan Li;Pengyue Wang;William F. Northrop;Shashi Shekhar,"Given an on-board diagnostics (OBD) dataset and a physics-based emissions prediction model, this paper aims to develop an accurate and computational-efficient AI (Artificial Intelligence) method that predicts vehicle emissions. The problem is of societal importance because vehicular emissions lead to climate change and impact human health. This problem is challenging because the OBD data does not contain enough parameters needed by high-order physics models. Conversely, related work has shown that low-order physics models have poor predictive accuracy when using available OBD data. This paper uses a divergent window co-occurrence pattern detection method to develop a spatiotemporal variability-aware AI model for predicting emission values from the OBD datasets. We conducted a case study using real-world OBD data from a local public transportation agency. Results show that the proposed AI method has approximately 65% improved predictive accuracy than a non-AI low-order physics model and is approximately 35% more accurate than a baseline model. △ Less","1 May, 2021",https://arxiv.org/pdf/2105.00375
AI-enabled Efficient and Safe Food Supply Chain,Ilianna Kollia;Jack Stevenson;Stefanos Kollias,"This paper provides a review of an emerging field in the food processing sector, referring to efficient and safe food supply chains, from farm to fork, as enabled by Artificial Intelligence (AI). Recent advances in machine and deep learning are used for effective food production, energy management and food labeling. Appropriate deep neural architectures are adopted and used for this purpose, including Fully Convolutional Networks, Long Short-Term Memories and Recurrent Neural Networks, Auto-Encoders and Attention mechanisms, Latent Variable extraction and clustering, as well as Domain Adaptation. Three experimental studies are presented, illustrating the ability of these AI methodologies to produce state-of-the-art performance in the whole food supply chain. In particular, these concern: (i) predicting plant growth and tomato yield in greenhouses, thus matching food production to market needs and reducing food waste or food unavailability; (ii) optimizing energy consumption across large networks of food retail refrigeration systems, through optimal selection of systems that can get shut-down and through prediction of the respective food de-freezing times, during peaks of power demand load; (iii) optical recognition and verification of food consumption expiry date in automatic inspection of retail packaged food, thus ensuring safety of food and people's health. △ Less","1 May, 2021",https://arxiv.org/pdf/2105.00333
A Peek Into the Reasoning of Neural Networks: Interpreting with Structural Visual Concepts,Yunhao Ge;Yao Xiao;Zhi Xu;Meng Zheng;Srikrishna Karanam;Terrence Chen;Laurent Itti;Ziyan Wu,"Despite substantial progress in applying neural networks (NN) to a wide variety of areas, they still largely suffer from a lack of transparency and interpretability. While recent developments in explainable artificial intelligence attempt to bridge this gap (e.g., by visualizing the correlation between input pixels and final outputs), these approaches are limited to explaining low-level relationships, and crucially, do not provide insights on error correction. In this work, we propose a framework (VRX) to interpret classification NNs with intuitive structural visual concepts. Given a trained classification model, the proposed VRX extracts relevant class-specific visual concepts and organizes them using structural concept graphs (SCG) based on pairwise concept relationships. By means of knowledge distillation, we show VRX can take a step towards mimicking the reasoning process of NNs and provide logical, concept-level explanations for final model decisions. With extensive experiments, we empirically show VRX can meaningfully answer ""why"" and ""why not"" questions about the prediction, providing easy-to-understand insights about the reasoning process. We also show that these insights can potentially provide guidance on improving NN's performance. △ Less","1 May, 2021",https://arxiv.org/pdf/2105.00290
Lecture Notes on Voting Theory,Davide Grossi,These lecture notes have been developed for the course Computational Social Choice of the Artificial Intelligence MSc programme at the University of Groningen. They cover mathematical and algorithmic aspects of voting theory.,"1 May, 2021",https://arxiv.org/pdf/2105.00216
Emotion Recognition of the Singing Voice: Toward a Real-Time Analysis Tool for Singers,Daniel Szelogowski,"Current computational-emotion research has focused on applying acoustic properties to analyze how emotions are perceived mathematically or used in natural language processing machine learning models. While recent interest has focused on analyzing emotions from the spoken voice, little experimentation has been performed to discover how emotions are recognized in the singing voice -- both in noiseless and noisy data (i.e., data that is either inaccurate, difficult to interpret, has corrupted/distorted/nonsense information like actual noise sounds in this case, or has a low ratio of usable/unusable information). Not only does this ignore the challenges of training machine learning models on more subjective data and testing them with much noisier data, but there is also a clear disconnect in progress between advancing the development of convolutional neural networks and the goal of emotionally cognizant artificial intelligence. By training a new model to include this type of information with a rich comprehension of psycho-acoustic properties, not only can models be trained to recognize information within extremely noisy data, but advancement can be made toward more complex biofeedback applications -- including creating a model which could recognize emotions given any human information (language, breath, voice, body, posture) and be used in any performance medium (music, speech, acting) or psychological assistance for patients with disorders such as BPD, alexithymia, autism, among others. This paper seeks to reflect and expand upon the findings of related research and present a stepping-stone toward this end goal. △ Less","4 July, 2021",https://arxiv.org/pdf/2105.00173
Revisiting Citizen Science Through the Lens of Hybrid Intelligence,Janet Rafner;Miroslav Gajdacz;Gitte Kragh;Arthur Hjorth;Anna Gander;Blanka Palfi;Aleks Berditchevskaia;François Grey;Kobi Gal;Avi Segal;Mike Walmsley;Josh Aaron Miller;Dominik Dellerman;Muki Haklay;Pietro Michelucci;Jacob Sherson,"Artificial Intelligence (AI) can augment and sometimes even replace human cognition. Inspired by efforts to value human agency alongside productivity, we discuss the benefits of solving Citizen Science (CS) tasks with Hybrid Intelligence (HI), a synergetic mixture of human and artificial intelligence. Currently there is no clear framework or methodology on how to create such an effective mixture. Due to the unique participant-centered set of values and the abundance of tasks drawing upon both human common sense and complex 21st century skills, we believe that the field of CS offers an invaluable testbed for the development of HI and human-centered AI of the 21st century, while benefiting CS as well. In order to investigate this potential, we first relate CS to adjacent computational disciplines. Then, we demonstrate that CS projects can be grouped according to their potential for HI-enhancement by examining two key dimensions: the level of digitization and the amount of knowledge or experience required for participation. Finally, we propose a framework for types of human-AI interaction in CS based on established criteria of HI. This ""HI lens"" provides the CS community with an overview of several ways to utilize the combination of AI and human intelligence in their projects. It also allows the AI community to gain ideas on how developing AI in CS projects can further their own field. △ Less","30 April, 2021",https://arxiv.org/pdf/2104.14961
Crack Semantic Segmentation using the U-Net with Full Attention Strategy,Fangzheng Lin;Jiesheng Yang;Jiangpeng Shu;Raimar J. Scherer,"Structures suffer from the emergence of cracks, therefore, crack detection is always an issue with much concern in structural health monitoring. Along with the rapid progress of deep learning technology, image semantic segmentation, an active research field, offers another solution, which is more effective and intelligent, to crack detection Through numerous artificial neural networks have been developed to address the preceding issue, corresponding explorations are never stopped improving the quality of crack detection. This paper presents a novel artificial neural network architecture named Full Attention U-net for image semantic segmentation. The proposed architecture leverages the U-net as the backbone and adopts the Full Attention Strategy, which is a synthesis of the attention mechanism and the outputs from each encoding layer in skip connection. Subject to the hardware in training, the experiments are composed of verification and validation. In verification, 4 networks including U-net, Attention U-net, Advanced Attention U-net, and Full Attention U-net are tested through cell images for a competitive study. With respect to mean intersection-over-unions and clarity of edge identification, the Full Attention U-net performs best in verification, and is hence applied for crack semantic segmentation in validation to demonstrate its effectiveness. △ Less","29 April, 2021",https://arxiv.org/pdf/2104.14586
Explainable AI For COVID-19 CT Classifiers: An Initial Comparison Study,Qinghao Ye;Jun Xia;Guang Yang,"Artificial Intelligence (AI) has made leapfrogs in development across all the industrial sectors especially when deep learning has been introduced. Deep learning helps to learn the behaviour of an entity through methods of recognising and interpreting patterns. Despite its limitless potential, the mystery is how deep learning algorithms make a decision in the first place. Explainable AI (XAI) is the key to unlocking AI and the black-box for deep learning. XAI is an AI model that is programmed to explain its goals, logic, and decision making so that the end users can understand. The end users can be domain experts, regulatory agencies, managers and executive board members, data scientists, users that use AI, with or without awareness, or someone who is affected by the decisions of an AI model. Chest CT has emerged as a valuable tool for the clinical diagnostic and treatment management of the lung diseases associated with COVID-19. AI can support rapid evaluation of CT scans to differentiate COVID-19 findings from other lung diseases. However, how these AI tools or deep learning algorithms reach such a decision and which are the most influential features derived from these neural networks with typically deep layers are not clear. The aim of this study is to propose and develop XAI strategies for COVID-19 classification models with an investigation of comparison. The results demonstrate promising quantification and qualitative visualisations that can further enhance the clinician's understanding and decision making with more granular information from the results given by the learned XAI models. △ Less","25 April, 2021",https://arxiv.org/pdf/2104.14506
Connecting AI Learning and Blockchain Mining in 6G Systems,Yunkai Wei;Zixian An;Supeng Leng;Kun Yang,"The sixth generation (6G) systems are generally recognized to be established on ubiquitous Artificial Intelligence (AI) and distributed ledger such as blockchain. However, the AI training demands tremendous computing resource, which is limited in most 6G devices. Meanwhile, miners in Proof-of-Work (PoW) based blockchains devote massive computing power to block mining, and are widely criticized for the waste of computation. To address this dilemma, we propose an Evolved-Proof-of-Work (E-PoW) consensus that can integrate the matrix computations, which are widely existed in AI training, into the process of brute-force searches in the block mining. Consequently, E-PoW can connect AI learning and block mining via the multiply used common computing resource. Experimental results show that E-PoW can salvage by up to 80 percent computing power from pure block mining for parallel AI training in 6G systems. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.14088
Applications of Artificial Intelligence to aid detection of dementia: a narrative review on current capabilities and future directions,Renjie Li;Xinyi Wang;Katherine Lawler;Saurabh Garg;Quan Bai;Jane Alty,"With populations ageing, the number of people with dementia worldwide is expected to triple to 152 million by 2050. Seventy percent of cases are due to Alzheimer's disease (AD) pathology and there is a 10-20 year 'pre-clinical' period before significant cognitive decline occurs. We urgently need, cost effective, objective methods to detect AD, and other dementias, at an early stage. Risk factor modification could prevent 40% of cases and drug trials would have greater chances of success if participants are recruited at an earlier stage. Currently, detection of dementia is largely by pen and paper cognitive tests but these are time consuming and insensitive to pre-clinical phases. Specialist brain scans and body fluid biomarkers can detect the earliest stages of dementia but are too invasive or expensive for widespread use. With the advancement of technology, Artificial Intelligence (AI) shows promising results in assisting with detection of early-stage dementia. Existing AI-aided methods and potential future research directions are reviewed and discussed. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.14073
A Study of the Mathematics of Deep Learning,Anirbit Mukherjee,"""Deep Learning""/""Deep Neural Nets"" is a technological marvel that is now increasingly deployed at the cutting-edge of artificial intelligence tasks. This dramatic success of deep learning in the last few years has been hinged on an enormous amount of heuristics and it has turned out to be a serious mathematical challenge to be able to rigorously explain them. In this thesis, submitted to the Department of Applied Mathematics and Statistics, Johns Hopkins University we take several steps towards building strong theoretical foundations for these new paradigms of deep-learning. In chapter 2 we show new circuit complexity theorems for deep neural functions and prove classification theorems about these function spaces which in turn lead to exact algorithms for empirical risk minimization for depth 2 ReLU nets. We also motivate a measure of complexity of neural functions to constructively establish the existence of high-complexity neural functions. In chapter 3 we give the first algorithm which can train a ReLU gate in the realizable setting in linear time in an almost distribution free set up. In chapter 4 we give rigorous proofs towards explaining the phenomenon of autoencoders being able to do sparse-coding. In chapter 5 we give the first-of-its-kind proofs of convergence for stochastic and deterministic versions of the widely used adaptive gradient deep-learning algorithms, RMSProp and ADAM. This chapter also includes a detailed empirical study on autoencoders of the hyper-parameter values at which modern algorithms have a significant advantage over classical acceleration based methods. In the last chapter 6 we give new and improved PAC-Bayesian bounds for the risk of stochastic neural nets. This chapter also includes an experimental investigation revealing new geometric properties of the paths in weight space that are traced out by the net during the training. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.14033
Applying Convolutional Neural Networks for Stock Market Trends Identification,Ekaterina Zolotareva,"In this paper we apply a specific type ANNs - convolutional neural networks (CNNs) - to the problem of finding start and endpoints of trends, which are the optimal points for entering and leaving the market. We aim to explore long-term trends, which last several months, not days. The key distinction of our model is that its labels are fully based on expert opinion data. Despite the various models based solely on stock price data, some market experts still argue that traders are able to see hidden opportunities. The labelling was done via the GUI interface, which means that the experts worked directly with images, not numerical data. This fact makes CNN the natural choice of algorithm. The proposed framework requires the sequential interaction of three CNN submodels, which identify the presence of a changepoint in a window, locate it and finally recognize the type of new tendency - upward, downward or flat. These submodels have certain pitfalls, therefore the calibration of their hyperparameters is the main direction of further research. The research addresses such issues as imbalanced datasets and contradicting labels, as well as the need for specific quality metrics to keep up with practical applicability. This paper is the full text of the research, presented at the 20th International Conference on Artificial Intelligence and Soft Computing Web System (ICAISC 2021) △ Less","19 April, 2021",https://arxiv.org/pdf/2104.13948
Investigating Perceptions of Social Intelligence in Simulated Human-Chatbot Interactions,Natascha Mariacher;Stephan Schlögl;Alexander Monz,"With the ongoing penetration of conversational user interfaces, a better understanding of social and emotional characteristic inherent to dialogue is required. Chatbots in particular face the challenge of conveying human-like behaviour while being restricted to one channel of interaction, i.e., text. The goal of the presented work is thus to investigate whether characteristics of social intelligence embedded in human-chatbot interactions are perceivable by human interlocutors and if yes, whether such influences the experienced interaction quality. Focusing on the social intelligence dimensions Authenticity, Clarity and Empathy, we first used a questionnaire survey evaluating the level of perception in text utterances, and then conducted a Wizard of Oz study to investigate the effects of these utterances in a more interactive setting. Results show that people have great difficulties perceiving elements of social intelligence in text. While on the one hand they find anthropomorphic behaviour pleasant and positive for the naturalness of a dialogue, they may also perceive it as frightening and unsuitable when expressed by an artificial agent in the wrong way or at the wrong time. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.13823
The Algonauts Project 2021 Challenge: How the Human Brain Makes Sense of a World in Motion,R. M. Cichy;K. Dwivedi;B. Lahner;A. Lascelles;P. Iamshchinina;M. Graumann;A. Andonian;N. A. R. Murty;K. Kay;G. Roig;A. Oliva,"The sciences of natural and artificial intelligence are fundamentally connected. Brain-inspired human-engineered AI are now the standard for predicting human brain responses during vision, and conversely, the brain continues to inspire invention in AI. To promote even deeper connections between these fields, we here release the 2021 edition of the Algonauts Project Challenge: How the Human Brain Makes Sense of a World in Motion (http://algonauts.csail.mit.edu/). We provide whole-brain fMRI responses recorded while 10 human participants viewed a rich set of over 1,000 short video clips depicting everyday events. The goal of the challenge is to accurately predict brain responses to these video clips. The format of our challenge ensures rapid development, makes results directly comparable and transparent, and is open to all. In this way it facilitates interdisciplinary collaboration towards a common goal of understanding visual intelligence. The 2021 Algonauts Project is conducted in collaboration with the Cognitive Computational Neuroscience (CCN) conference. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.13714
Continual Learning Approach for Improving the Data and Computation Mapping in Near-Memory Processing System,Pritam Majumder;Jiayi Huang;Sungkeun Kim;Abdullah Muzahid;Dylan Siegers;Chia-Che Tsai;Eun Jung Kim,"The resurgence of near-memory processing (NMP) with the advent of big data has shifted the computation paradigm from processor-centric to memory-centric computing. To meet the bandwidth and capacity demands of memory-centric computing, 3D memory has been adopted to form a scalable memory-cube network. Along with NMP and memory system development, the mapping for placing data and guiding computation in the memory-cube network has become crucial in driving the performance improvement in NMP. However, it is very challenging to design a universal optimal mapping for all applications due to unique application behavior and intractable decision space. In this paper, we propose an artificially intelligent memory mapping scheme, AIMM, that optimizes data placement and resource utilization through page and computation remapping. Our proposed technique involves continuously evaluating and learning the impact of mapping decisions on system performance for any application. AIMM uses a neural network to achieve a near-optimal mapping during execution, trained using a reinforcement learning algorithm that is known to be effective for exploring a vast design space. We also provide a detailed AIMM hardware design that can be adopted as a plugin module for various NMP systems. Our experimental evaluation shows that AIMM improves the baseline NMP performance in single and multiple program scenario by up to 70% and 50%, respectively. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.13671
Preventing and Controlling Epidemics through Blockchain-Assisted AI-Enabled Networks,Safa Otoum;Ismaeel Al Ridhawi;Hussein T. Mouftah,"The COVID-19 pandemic, which spread rapidly in late 2019, has revealed that the use of computing and communication technologies provides significant aid in preventing, controlling, and combating infectious diseases. With the ongoing research in next-generation networking (NGN), the use of secure and reliable communication and networking is of utmost importance when dealing with users' health records and other sensitive information. Through the adaptation of Artificial Intelligence (AI)-enabled NGN, the shape of healthcare systems can be altered to achieve smart and secure healthcare capable of coping with epidemics that may emerge at any given moment. In this article, we envision a cooperative and distributed healthcare framework that relies on state-of-the-art computing, communication, and intelligence capabilities, namely, Federated Learning (FL), mobile edge computing (MEC), and Blockchain, to enable epidemic (or suspicious infectious disease) discovery, remote monitoring, and fast health-authority response. The introduced framework can also enable secure medical data exchange at the edge and between different health entities. Such a technique, coupled with the low latency and high bandwidth functionality of 5G and beyond networks, would enable mass surveillance, monitoring and analysis to occur at the edge. Challenges, issues, and design guidelines are also discussed in this article with highlights on some trending solutions. △ Less","25 April, 2021",https://arxiv.org/pdf/2104.13224
Controlling earthquake-like instabilities using artificial intelligence,Efthymios Papachristos;Ioannis Stefanou,"Earthquakes are lethal and costly. This study aims at avoiding these catastrophic events by the application of injection policies retrieved through reinforcement learning. With the rapid growth of artificial intelligence, prediction-control problems are all the more tackled by function approximation models that learn how to control a specific task, even for systems with unmodeled/unknown dynamics and important uncertainties. Here, we show for the first time the possibility of controlling earthquake-like instabilities using state-of-the-art deep reinforcement learning techniques. The controller is trained using a reduced model of the physical system, i.e, the spring-slider model, which embodies the main dynamics of the physical problem for a given earthquake magnitude. Its robustness to unmodeled dynamics is explored through a parametric study. Our study is a first step towards minimizing seismicity in industrial projects (geothermal energy, hydrocarbons production, CO2 sequestration) while, in a second step for inspiring techniques for natural earthquakes control and prevention. △ Less","27 April, 2021",https://arxiv.org/pdf/2104.13180
"Watershed of Artificial Intelligence: Human Intelligence, Machine Intelligence, and Biological Intelligence",Li Weigang;Liriam Enamoto;Denise Leyi Li;Geraldo Pereira Rocha Filho,"This article reviews the ""Once learning"" mechanism that was proposed 23 years ago and the subsequent successes of ""One-shot learning"" in image classification and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current development of Artificial Intelligence (AI), the proposal is that AI should be clearly divided into the following categories: Artificial Human Intelligence (AHI), Artificial Machine Intelligence (AMI), and Artificial Biological Intelligence (ABI), which will also be the main directions of theory and application development for AI. As a watershed for the branches of AI, some classification standards and methods are discussed: 1) Human-oriented, machine-oriented, and biological-oriented AI R&D; 2) Information input processed by Dimensionality-up or Dimensionality-reduction; 3) The use of one/few or large samples for knowledge learning. △ Less","7 May, 2021",https://arxiv.org/pdf/2104.13155
Why AI is Harder Than We Think,Melanie Mitchell,"Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment (""AI spring"") and periods of disappointment, loss of confidence, and reduced funding (""AI winter""). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense. △ Less","28 April, 2021",https://arxiv.org/pdf/2104.12871
Cloud computing as a platform for monetizing data services: A two-sided game business model,Ahmed Saleh Bataineh;Jamal Bentahar;Rabeb Mizouni;Omar Abdel Wahab;Gaith Rjoub;May El Barachi,"With the unprecedented reliance on cloud computing as the backbone for storing today's big data, we argue in this paper that the role of the cloud should be reshaped from being a passive virtual market to become an active platform for monetizing the big data through Artificial Intelligence (AI) services. The objective is to enable the cloud to be an active platform that can help big data service providers reach a wider set of customers and cloud users (i.e., data consumers) to be exposed to a larger and richer variety of data to run their data analytic tasks. To achieve this vision, we propose a novel game theoretical model, which consists of a mix of cooperative and competitive strategies. The players of the game are the big data service providers, cloud computing platform, and cloud users. The strategies of the players are modeled using the two-sided market theory that takes into consideration the network effects among involved parties, while integrating the externalities between the cloud resources and consumer demands into the design of the game. Simulations conducted using Amazon and google clustered data show that the proposed model improves the total surplus of all the involved parties in terms of cloud resources provision and monetary profits compared to the current merchant model. △ Less","26 April, 2021",https://arxiv.org/pdf/2104.12762
A Framework for Ethical AI at the United Nations,Lambert Hogenhout,"This paper aims to provide an overview of the ethical concerns in artificial intelligence (AI) and the framework that is needed to mitigate those risks, and to suggest a practical path to ensure the development and use of AI at the United Nations (UN) aligns with our ethical values. The overview discusses how AI is an increasingly powerful tool with potential for good, albeit one with a high risk of negative side-effects that go against fundamental human rights and UN values. It explains the need for ethical principles for AI aligned with principles for data governance, as data and AI are tightly interwoven. It explores different ethical frameworks that exist and tools such as assessment lists. It recommends that the UN develop a framework consisting of ethical principles, architectural standards, assessment methods, tools and methodologies, and a policy to govern the implementation and adherence to this framework, accompanied by an education program for staff. △ Less","9 April, 2021",https://arxiv.org/pdf/2104.12547
The Effects of Air Quality on the Spread of the COVID-19 Pandemic in Italy: An Artificial Intelligence Approach,Andrea Loreggia;Anna Passarelli;Maria Silvia Pini,"The COVID-19 pandemic considerably affects public health systems around the world. The lack of knowledge about the virus, the extension of this phenomenon, and the speed of the evolution of the infection are all factors that highlight the necessity of employing new approaches to study these events. Artificial intelligence techniques may be useful in analyzing data related to areas affected by the virus. The aim of this work is to investigate any possible relationships between air quality and confirmed cases of COVID-19 in Italian districts. Specifically, we report an analysis of the correlation between daily COVID-19 cases and environmental factors, such as temperature, relative humidity, and atmospheric pollutants. Our analysis confirms a significant association of some environmental parameters with the spread of the virus. This suggests that machine learning models trained on the environmental parameters to predict the number of future infected cases may be accurate. Predictive models may be useful for helping institutions in making decisions for protecting the population and contrasting the pandemic. △ Less","31 August, 2021",https://arxiv.org/pdf/2104.12546
Fostering learners' self-regulation and collaboration skills and strategies for mobile language learning beyond the classroom,Olga Viberg;Agnes Kukulska-Hulme,"Many language learners need to be supported in acquiring a second or foreign language quickly and effectively across learning environments beyond the classroom. The chapter argues that support should focus on the development of two vital learning skills, namely being able to self-regulate and to collaborate effectively in the learning process. We base our argumentation on the theoretical lenses of self-regulated learning (SRL) and collaborative learning in the context of mobile situated learning that can take place in a variety of settings. The chapter examines a sample of selected empirical studies within the field of mobile-assisted language learning with a twofold aim. Firstly, the studies are analyzed in order to understand the role of learner self-regulation and collaboration while acquiring a new language beyond the classroom. Secondly, we aim to provide a deeper understanding of any mechanisms provided to develop or support language learners' self-regulated and collaborative learning skills. Finally, we propose that fostering SRL and collaborative learning skills and strategies will benefit from recent advances in the fields of learning analytics and artificial intelligence, coupled with the use of mobile technologies and self-monitoring mechanisms. The ultimate aim is to enable the provision of individual adaptive learning paths to facilitate language learning beyond the classroom. △ Less","20 March, 2021",https://arxiv.org/pdf/2104.12486
What Makes a Message Persuasive? Identifying Adaptations Towards Persuasiveness in Nine Exploratory Case Studies,Sebastian Duerr;Krystian Teodor Lange;Peter A. Gloor,"The ability to persuade others is critical to professional and personal success. However, crafting persuasive messages is demanding and poses various challenges. We conducted nine exploratory case studies to identify adaptations that professional and non-professional writers make in written scenarios to increase their subjective persuasiveness. Furthermore, we identified challenges that those writers faced and identified strategies to resolve them with persuasive natural language generation, i.e., artificial intelligence. Our findings show that humans can achieve high degrees of persuasiveness (more so for professional-level writers), and artificial intelligence can complement them to achieve increased celerity and alignment in the process. △ Less","26 April, 2021",https://arxiv.org/pdf/2104.12454
Explainable Artificial Intelligence Reveals Novel Insight into Tumor Microenvironment Conditions Linked with Better Prognosis in Patients with Breast Cancer,Debaditya Chakraborty;Cristina Ivan;Paola Amero;Maliha Khan;Cristian Rodriguez-Aguayo;Hakan Başağaoğlu;Gabriel Lopez-Berestein,"We investigated the data-driven relationship between features in the tumor microenvironment (TME) and the overall and 5-year survival in triple-negative breast cancer (TNBC) and non-TNBC (NTNBC) patients by using Explainable Artificial Intelligence (XAI) models. We used clinical information from patients with invasive breast carcinoma from The Cancer Genome Atlas and from two studies from the cbioPortal, the PanCanAtlas project and the GDAC Firehose study. In this study, we used a normalized RNA sequencing data-driven cohort from 1,015 breast cancer patients, alive or deceased, from the UCSC Xena data set and performed integrated deconvolution with the EPIC method to estimate the percentage of seven different immune and stromal cells from RNA sequencing data. Novel insights derived from our XAI model showed that CD4+ T cells and B cells are more critical than other TME features for enhanced prognosis for both TNBC and NTNBC patients. Our XAI model revealed the critical inflection points (i.e., threshold fractions) of CD4+ T cells and B cells above or below which 5-year survival rates improve. Subsequently, we ascertained the conditional probabilities of \geq 5-year survival in both TNBC and NTNBC patients under specific conditions inferred from the inflection points. In particular, the XAI models revealed that a B-cell fraction exceeding 0.018 in the TME could ensure 100% 5-year survival for NTNBC patients. The findings from this research could lead to more accurate clinical predictions and enhanced immunotherapies and to the design of innovative strategies to reprogram the TME of breast cancer patients. △ Less","24 April, 2021",https://arxiv.org/pdf/2104.12021
"Compilation-based Solvers for Multi-Agent Path Finding: a Survey, Discussion, and Future Opportunities",Pavel Surynek,"Multi-agent path finding (MAPF) attracts considerable attention in artificial intelligence community as well as in robotics, and other fields such as warehouse logistics. The task in the standard MAPF is to find paths through which agents can navigate from their starting positions to specified individual goal positions. The combination of two additional requirements makes the problem computationally challenging: (i) agents must not collide with each other and (ii) the paths must be optimal with respect to some objective. Two major approaches to optimal MAPF solving include (1) dedicated search-based methods, which solve MAPF directly, and (2) compilation-based methods that reduce a MAPF instance to an instance in a different well established formalism, for which an efficient solver exists. The compilation-based MAPF solving can benefit from advancements accumulated during the development of the target solver often decades long. We summarize and compare contemporary compilation-based solvers for MAPF using formalisms like ASP, MIP, and SAT. We show the lessons learned from past developments and current trends in the topic and discuss its wider impact. △ Less","23 April, 2021",https://arxiv.org/pdf/2104.11809
Becoming Good at AI for Good,Meghana Kshirsagar;Caleb Robinson;Siyu Yang;Shahrzad Gholami;Ivan Klyuzhin;Sumit Mukherjee;Md Nasir;Anthony Ortiz;Felipe Oviedo;Darren Tanner;Anusua Trivedi;Yixi Xu;Ming Zhong;Bistra Dilkina;Rahul Dodhia;Juan M. Lavista Ferres,"AI for good (AI4G) projects involve developing and applying artificial intelligence (AI) based solutions to further goals in areas such as sustainability, health, humanitarian aid, and social justice. Developing and deploying such solutions must be done in collaboration with partners who are experts in the domain in question and who already have experience in making progress towards such goals. Based on our experiences, we detail the different aspects of this type of collaboration broken down into four high-level categories: communication, data, modeling, and impact, and distill eleven takeaways to guide such projects in the future. We briefly describe two case studies to illustrate how some of these takeaways were applied in practice during our past collaborations. △ Less","3 May, 2021",https://arxiv.org/pdf/2104.11757
Secure Artificial Intelligence of Things for Implicit Group Recommendations,Keping Yu;Zhiwei Guo;Yu Shen;Wei Wang;Jerry Chun-Wei Lin;Takuro Sato,"The emergence of Artificial Intelligence of Things (AIoT) has provided novel insights for many social computing applications such as group recommender systems. As distance among people has been greatly shortened, it has been a more general demand to provide personalized services to groups instead of individuals. In order to capture group-level preference features from individuals, existing methods were mostly established via aggregation and face two aspects of challenges: secure data management workflow is absent, and implicit preference feedbacks is ignored. To tackle current difficulties, this paper proposes secure Artificial Intelligence of Things for implicit Group Recommendations (SAIoT-GR). As for hardware module, a secure IoT structure is developed as the bottom support platform. As for software module, collaborative Bayesian network model and non-cooperative game are can be introduced as algorithms. Such a secure AIoT architecture is able to maximize the advantages of the two modules. In addition, a large number of experiments are carried out to evaluate the performance of the SAIoT-GR in terms of efficiency and robustness. △ Less","23 April, 2021",https://arxiv.org/pdf/2104.11699
Learning in Deep Neural Networks Using a Biologically Inspired Optimizer,Giorgia Dellaferrera;Stanislaw Wozniak;Giacomo Indiveri;Angeliki Pantazi;Evangelos Eleftheriou,"Plasticity circuits in the brain are known to be influenced by the distribution of the synaptic weights through the mechanisms of synaptic integration and local regulation of synaptic strength. However, the complex interplay of stimulation-dependent plasticity with local learning signals is disregarded by most of the artificial neural network training algorithms devised so far. Here, we propose a novel biologically inspired optimizer for artificial (ANNs) and spiking neural networks (SNNs) that incorporates key principles of synaptic integration observed in dendrites of cortical neurons: GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals). GRAPES implements a weight-distribution dependent modulation of the error signal at each node of the neural network. We show that this biologically inspired mechanism leads to a systematic improvement of the convergence rate of the network, and substantially improves classification accuracy of ANNs and SNNs with both feedforward and recurrent architectures. Furthermore, we demonstrate that GRAPES supports performance scalability for models of increasing complexity and mitigates catastrophic forgetting by enabling networks to generalize to unseen tasks based on previously acquired knowledge. The local characteristics of GRAPES minimize the required memory resources, making it optimally suited for dedicated hardware implementations. Overall, our work indicates that reconciling neurophysiology insights with machine intelligence is key to boosting the performance of neural networks. △ Less","23 April, 2021",https://arxiv.org/pdf/2104.11604
A Picture is Worth a Collaboration: Accumulating Design Knowledge for Computer-Vision-based Hybrid Intelligence Systems,Patrick Zschech;Jannis Walk;Kai Heinrich;Michael Vössing;Niklas Kühl,"Computer vision (CV) techniques try to mimic human capabilities of visual perception to support labor-intensive and time-consuming tasks like the recognition and localization of critical objects. Nowadays, CV increasingly relies on artificial intelligence (AI) to automatically extract useful information from images that can be utilized for decision support and business process automation. However, the focus of extant research is often exclusively on technical aspects when designing AI-based CV systems while neglecting socio-technical facets, such as trust, control, and autonomy. For this purpose, we consider the design of such systems from a hybrid intelligence (HI) perspective and aim to derive prescriptive design knowledge for CV-based HI systems. We apply a reflective, practice-inspired design science approach and accumulate design knowledge from six comprehensive CV projects. As a result, we identify four design-related mechanisms (i.e., automation, signaling, modification, and collaboration) that inform our derived meta-requirements and design principles. This can serve as a basis for further socio-technical research on CV-based HI systems. △ Less","23 April, 2021",https://arxiv.org/pdf/2104.11600
Intensional Artificial Intelligence: From Symbol Emergence to Explainable and Empathetic AI,Michael Timothy Bennett;Yoshihiro Maruyama,"We argue that an explainable artificial intelligence must possess a rationale for its decisions, be able to infer the purpose of observed behaviour, and be able to explain its decisions in the context of what its audience understands and intends. To address these issues we present four novel contributions. Firstly, we define an arbitrary task in terms of perceptual states, and discuss two extremes of a domain of possible solutions. Secondly, we define the intensional solution. Optimal by some definitions of intelligence, it describes the purpose of a task. An agent possessed of it has a rationale for its decisions in terms of that purpose, expressed in a perceptual symbol system grounded in hardware. Thirdly, to communicate that rationale requires natural language, a means of encoding and decoding perceptual states. We propose a theory of meaning in which, to acquire language, an agent should model the world a language describes rather than the language itself. If the utterances of humans are of predictive value to the agent's goals, then the agent will imbue those utterances with meaning in terms of its own goals and perceptual states. In the context of Peircean semiotics, a community of agents must share rough approximations of signs, referents and interpretants in order to communicate. Meaning exists only in the context of intent, so to communicate with humans an agent must have comparable experiences and goals. An agent that learns intensional solutions, compelled by objective functions somewhat analogous to human motivators such as hunger and pain, may be capable of explaining its rationale not just in terms of its own intent, but in terms of what its audience understands and intends. It forms some approximation of the perceptual states of humans. △ Less","23 April, 2021",https://arxiv.org/pdf/2104.11573
Deep learning for detecting bid rigging: Flagging cartel participants based on convolutional neural networks,Martin Huber;David Imhof,"Adding to the literature on the data-driven detection of bid-rigging cartels, we propose a novel approach based on deep learning (a subfield of artificial intelligence) that flags cartel participants based on their pairwise bidding interactions with other firms. More concisely, we combine a so-called convolutional neural network for image recognition with graphs that in a pairwise manner plot the normalized bid values of some reference firm against the normalized bids of any other firms participating in the same tenders as the reference firm. Based on Japanese and Swiss procurement data, we construct such graphs for both collusive and competitive episodes (i.e when a bid-rigging cartel is or is not active) and use a subset of graphs to train the neural network such that it learns distinguishing collusive from competitive bidding patterns. We use the remaining graphs to test the neural network's out-of-sample performance in correctly classifying collusive and competitive bidding interactions. We obtain a very decent average accuracy of around 90% or slightly higher when either applying the method within Japanese, Swiss, or mixed data (in which Swiss and Japanese graphs are pooled). When using data from one country for training to test the trained model's performance in the other country (i.e. transnationally), predictive performance decreases (likely due to institutional differences in procurement procedures across countries), but often remains satisfactorily high. All in all, the generally quite high accuracy of the convolutional neural network despite being trained in a rather small sample of a few 100 graphs points to a large potential of deep learning approaches for flagging and fighting bid-rigging cartels. △ Less","22 April, 2021",https://arxiv.org/pdf/2104.11142
A learning gap between neuroscience and reinforcement learning,Samuel T. Wauthier;Pietro Mazzaglia;Ozan Çatal;Cedric De Boom;Tim Verbelen;Bart Dhoedt,"Historically, artificial intelligence has drawn much inspiration from neuroscience to fuel advances in the field. However, current progress in reinforcement learning is largely focused on benchmark problems that fail to capture many of the aspects that are of interest in neuroscience today. We illustrate this point by extending a T-maze task from neuroscience for use with reinforcement learning algorithms, and show that state-of-the-art algorithms are not capable of solving this problem. Finally, we point out where insights from neuroscience could help explain some of the issues encountered. △ Less","4 May, 2021",https://arxiv.org/pdf/2104.10995
Blockchain based Privacy-Preserved Federated Learning for Medical Images: A Case Study of COVID-19 CT Scans,Rajesh Kumar;WenYong Wang;Cheng Yuan;Jay Kumar;Zakria;He Qing;Ting Yang;Abdullah Aman Khan,"Medical health care centers are envisioned as a promising paradigm to handle the massive volume of data of COVID-19 patients using artificial intelligence (AI). Traditionally, AI techniques often require centralized data collection and training the model in a single organization, which is most common weakness due to the privacy and security of raw data communication. To solve this challenging task, we propose a blockchain-based federated learning framework that provides collaborative data training solutions by coordinating multiple hospitals to train and share encrypted federated models without leakage of data privacy. The blockchain ledger technology provides the decentralization of federated learning model without any central server. The proposed homomorphic encryption scheme encrypts and decrypts the gradients of model to preserve the privacy. More precisely, the proposed framework: i) train the local model by a novel capsule network to segmentation and classify COVID-19 images, ii) then use the homomorphic encryption scheme to secure the local model that encrypts and decrypts the gradients, and finally the model is shared over a decentralized platform through proposed blockchain-based federated learning algorithm. The integration of blockchain and federated learning leads to a new paradigm for medical image data sharing in the decentralized network. The conducted experimental resultsdemonstrate the performance of the proposed scheme. △ Less","31 May, 2021",https://arxiv.org/pdf/2104.10903
Explainable artificial intelligence for mechanics: physics-informing neural networks for constitutive models,Arnd Koeppe;Franz Bamer;Michael Selzer;Britta Nestler;Bernd Markert,"(Artificial) neural networks have become increasingly popular in mechanics to accelerate computations with model order reduction techniques and as universal models for a wide variety of materials. However, the major disadvantage of neural networks remains: their numerous parameters are challenging to interpret and explain. Thus, neural networks are often labeled as black boxes, and their results often elude human interpretation. In mechanics, the new and active field of physics-informed neural networks attempts to mitigate this disadvantage by designing deep neural networks on the basis of mechanical knowledge. By using this a priori knowledge, deeper and more complex neural networks became feasible, since the mechanical assumptions could be explained. However, the internal reasoning and explanation of neural network parameters remain mysterious. Complementary to the physics-informed approach, we propose a first step towards a physics-informing approach, which explains neural networks trained on mechanical data a posteriori. This novel explainable artificial intelligence approach aims at elucidating the black box of neural networks and their high-dimensional representations. Therein, the principal component analysis decorrelates the distributed representations in cell states of RNNs and allows the comparison to known and fundamental functions. The novel approach is supported by a systematic hyperparameter search strategy that identifies the best neural network architectures and training parameters. The findings of three case studies on fundamental constitutive models (hyperelasticity, elastoplasticity, and viscoelasticity) imply that the proposed strategy can help identify numerical and analytical closed-form solutions to characterize new materials. △ Less","12 July, 2021",https://arxiv.org/pdf/2104.10683
TransICD: Transformer Based Code-wise Attention Model for Explainable ICD Coding,Biplob Biswas;Thai-Hoang Pham;Ping Zhang,"International Classification of Disease (ICD) coding procedure which refers to tagging medical notes with diagnosis codes has been shown to be effective and crucial to the billing system in medical sector. Currently, ICD codes are assigned to a clinical note manually which is likely to cause many errors. Moreover, training skilled coders also requires time and human resources. Therefore, automating the ICD code determination process is an important task. With the advancement of artificial intelligence theory and computational hardware, machine learning approach has emerged as a suitable solution to automate this process. In this project, we apply a transformer-based architecture to capture the interdependence among the tokens of a document and then use a code-wise attention mechanism to learn code-specific representations of the entire document. Finally, they are fed to separate dense layers for corresponding code prediction. Furthermore, to handle the imbalance in the code frequency of clinical datasets, we employ a label distribution aware margin (LDAM) loss function. The experimental results on the MIMIC-III dataset show that our proposed model outperforms other baselines by a significant margin. In particular, our best setting achieves a micro-AUC score of 0.923 compared to 0.868 of bidirectional recurrent neural networks. We also show that by using the code-wise attention mechanism, the model can provide more insights about its prediction, and thus it can support clinicians to make reliable decisions. Our code is available online (https://github.com/biplob1ly/TransICD) △ Less","28 March, 2021",https://arxiv.org/pdf/2104.10652
Learning future terrorist targets through temporal meta-graphs,Gian Maria Campedelli;Mihovil Bartulovic;Kathleen M. Carley,"In the last 20 years, terrorism has led to hundreds of thousands of deaths and massive economic, political, and humanitarian crises in several regions of the world. Using real-world data on attacks occurred in Afghanistan and Iraq from 2001 to 2018, we propose the use of temporal meta-graphs and deep learning to forecast future terrorist targets. Focusing on three event dimensions, i.e., employed weapons, deployed tactics and chosen targets, meta-graphs map the connections among temporally close attacks, capturing their operational similarities and dependencies. From these temporal meta-graphs, we derive 2-day-based time series that measure the centrality of each feature within each dimension over time. Formulating the problem in the context of the strategic behavior of terrorist actors, these multivariate temporal sequences are then utilized to learn what target types are at the highest risk of being chosen. The paper makes two contributions. First, it demonstrates that engineering the feature space via temporal meta-graphs produces richer knowledge than shallow time-series that only rely on frequency of feature occurrences. Second, the performed experiments reveal that bi-directional LSTM networks achieve superior forecasting performance compared to other algorithms, calling for future research aiming at fully discovering the potential of artificial intelligence to counter terrorist violence. △ Less","21 April, 2021",https://arxiv.org/pdf/2104.10398
Measuring economic activity from space: a case study using flying airplanes and COVID-19,Mauricio Pamplona Segundo;Allan Pinto;Rodrigo Minetto;Ricardo da Silva Torres;Sudeep Sarkar,"This work introduces a novel solution to measure economic activity through remote sensing for a wide range of spatial areas. We hypothesized that disturbances in human behavior caused by major life-changing events leave signatures in satellite imagery that allows devising relevant image-based indicators to estimate their impacts and support decision-makers. We present a case study for the COVID-19 coronavirus outbreak, which imposed severe mobility restrictions and caused worldwide disruptions, using flying airplane detection around the 30 busiest airports in Europe to quantify and analyze the lockdown's effects and post-lockdown recovery. Our solution won the Rapid Action Coronavirus Earth observation (RACE) upscaling challenge, sponsored by the European Space Agency and the European Commission, and now integrates the RACE dashboard. This platform combines satellite data and artificial intelligence to promote a progressive and safe reopening of essential activities. Code and CNN models are available at https://github.com/maups/covid19-custom-script-contest △ Less","21 April, 2021",https://arxiv.org/pdf/2104.10345
Network Defense is Not a Game,Andres Molina-Markham;Ransom K. Winder;Ahmad Ridley,"Research seeks to apply Artificial Intelligence (AI) to scale and extend the capabilities of human operators to defend networks. A fundamental problem that hinders the generalization of successful AI approaches -- i.e., beating humans at playing games -- is that network defense cannot be defined as a single game with a fixed set of rules. Our position is that network defense is better characterized as a collection of games with uncertain and possibly drifting rules. Hence, we propose to define network defense tasks as distributions of network environments, to: (i) enable research to apply modern AI techniques, such as unsupervised curriculum learning and reinforcement learning for network defense; and, (ii) facilitate the design of well-defined challenges that can be used to compare approaches for autonomous cyberdefense. To demonstrate that an approach for autonomous network defense is practical it is important to be able to reason about the boundaries of its applicability. Hence, we need to be able to define network defense tasks that capture sets of adversarial tactics, techniques, and procedures (TTPs); quality of service (QoS) requirements; and TTPs available to defenders. Furthermore, the abstractions to define these tasks must be extensible; must be backed by well-defined semantics that allow us to reason about distributions of environments; and should enable the generation of data and experiences from which an agent can learn. Our approach named Network Environment Design for Autonomous Cyberdefense inspired the architecture of FARLAND, a Framework for Advanced Reinforcement Learning for Autonomous Network Defense, which we use at MITRE to develop RL network defenders that perform blue actions from the MITRE Shield matrix against attackers with TTPs that drift from MITRE ATT&CK TTPs. △ Less","20 April, 2021",https://arxiv.org/pdf/2104.10262
Episodic Memory Model for Learning Robotic Manipulation Tasks,Sanaz Behbahani;Siddharth Chhatpar;Said Zahrai;Vishakh Duggal;Mohak Sukhwani,"Machine learning, artificial intelligence and especially deep learning based approaches are often used to simplify or eliminate the burden of programming industrial robots. Using these approaches robots inherently learn a skill instead of being programmed using strict and tedious programming instructions. While deep learning is effective in making robots learn skills, it does not offer a practical route for teaching a complete task, such as assembly or machine tending, where a complex logic must be understood and related sub-tasks need to be performed. We present a model similar to an episodic memory that allows robots to comprehend sequences of actions using single demonstration and perform them properly and accurately. The algorithm identifies and recognizes the changes in the states of the system and memorizes how to execute the necessary tasks in order to make those changes. This allows the robot to decompose the tasks into smaller sub-tasks, retain the essential steps, and remember how they have been performed. △ Less","20 April, 2021",https://arxiv.org/pdf/2104.10218
Prospective Artificial Intelligence Approaches for Active Cyber Defence,Neil Dhir;Henrique Hoeltgebaum;Niall Adams;Mark Briers;Anthony Burke;Paul Jones,"Cybercriminals are rapidly developing new malicious tools that leverage artificial intelligence (AI) to enable new classes of adaptive and stealthy attacks. New defensive methods need to be developed to counter these threats. Some cybersecurity professionals are speculating AI will enable corresponding new classes of active cyber defence measures -- is this realistic, or currently mostly hype? The Alan Turing Institute, with expert guidance from the UK National Cyber Security Centre and Defence Science Technology Laboratory, published a research roadmap for AI for ACD last year. This position paper updates the roadmap for two of the most promising AI approaches -- reinforcement learning and causal inference - and describes why they could help tip the balance back towards defenders. △ Less","20 April, 2021",https://arxiv.org/pdf/2104.09981
Subsentence Extraction from Text Using Coverage-Based Deep Learning Language Models,JongYoon Lim;Inkyu Sa;Ho Seok Ahn;Norina Gasteiger;Sanghyub John Lee;Bruce MacDonald,"Sentiment prediction remains a challenging and unresolved task in various research fields, including psychology, neuroscience, and computer science. This stems from its high degree of subjectivity and limited input sources that can effectively capture the actual sentiment. This can be even more challenging with only text-based input. Meanwhile, the rise of deep learning and an unprecedented large volume of data have paved the way for artificial intelligence to perform impressively accurate predictions or even human-level reasoning. Drawing inspiration from this, we propose a coverage-based sentiment and subsentence extraction system that estimates a span of input text and recursively feeds this information back to the networks. The predicted subsentence consists of auxiliary information expressing a sentiment. This is an important building block for enabling vivid and epic sentiment delivery (within the scope of this paper) and for other natural language processing tasks such as text summarisation and Q&A. Our approach outperforms the state-of-the-art approaches by a large margin in subsentence prediction (i.e., Average Jaccard scores from 0.72 to 0.89). For the evaluation, we designed rigorous experiments consisting of 24 ablation studies. Finally, our learned lessons are returned to the community by sharing software packages and a public dataset that can reproduce the results presented in this paper. △ Less","6 May, 2021",https://arxiv.org/pdf/2104.09777
Semantic Knowledge Discovery and Discussion Mining of Incel Online Community: Topic modeling,Hamed Jelodar;Richard Frank,"Online forums provide a unique opportunity for online users to share comments and exchange information on a particular topic. Understanding user behaviour is valuable to organizations and has applications for social and security strategies, for instance, identifying user opinions within a community or predicting future behaviour. Discovering the semantic aspects in Incel forums are the main goal of this research; we apply Natural language processing techniques based on topic modeling to latent topic discovery and opinion mining of users from a popular online Incel discussion forum. To prepare the input data for our study, we extracted the comments from Incels.co. The research experiments show that Artificial Intelligence (AI) based on NLP models can be effective for semantic and emotion knowledge discovery and retrieval of useful information from the Incel community. For example, we discovered semantic-related words that describe issues within a large volume of Incel comments, which is difficult with manual methods. △ Less","21 April, 2021",https://arxiv.org/pdf/2104.09586
Artificial Intelligence in Open Radio Access Network,Paul H. Masur;Jeffrey H. Reed,"This tutorial seeks to outline the proposed Open Radio Access Network (O-RAN) deployment for Fifth generation (5G) wireless networks. O-RAN seeks to supplant hardware-specific Radio Access Network (RAN) components (e.g., the mobility management entity (MME) or base station (gNB)) with generic hardware, specialized software, and open signaling interfaces. The virtualization and network slicing features of 5G allow for software to replace previously hardware specific functions. Software further provides faster analytics, thus supporting 5Gs latency requirements and advanced usage scenarios (i.e., enhanced mobile broadband (eMBB), massive machine type communications (mMTC), and ultra-reliable low latency communications (uRLLC)). Furthermore, as software annexes control of the RAN, there is freedom to integrate Artificial Intelligence/Machine Learning (AI/ML) algorithms into RAN management (particularly at the user plane). This integration is one of the goals of O-RAN. Lastly, relying on generic hardware and specialized, open-source software eliminates reliance upon specific device manufacturers. This paper will provide questions regarding the future of O-RAN, with a focus on 5G network device security. △ Less","27 April, 2021",https://arxiv.org/pdf/2104.09445
Aiding Long-Term Investment Decisions with XGBoost Machine Learning Model,Ekaterina Zolotareva,"The ability to identify stock market trends has obvious advantages for investors. Buying stock on an upward trend (as well as selling it in case of downward movement) results in profit. Accordingly, the start and end-points of the trend are the optimal points for entering and leaving the market. The research concentrates on recognizing stock market long-term upward and downward trends. The key results are obtained with the use of gradient boosting algorithms, XGBoost in particular. The raw data is represented by time series with basic stock market quotes with periods labelled by experts as Trend or Flat. The features are then obtained via various data transformations, aiming to catch implicit factors resulting in a change of stock direction. Modelling is done in two stages: stage one aims to detect endpoints of tendencies (i.e. sliding windows), stage two recognizes the tendency itself inside the window. The research addresses such issues as imbalanced datasets and contradicting labels, as well as the need for specific quality metrics to keep up with practical applicability. The model can be used to design an investment strategy though further research in feature engineering and fine calibration is required.This paper is the full text of the research, presented at the 20th International Conference on Artificial Intelligence and Soft Computing Web System (ICAISC 2021) △ Less","19 April, 2021",https://arxiv.org/pdf/2104.09341
"Comprehensive systematic review into combinations of artificial intelligence, human factors, and automation",Reza Khani-Shekarab;Alireza khani-shekarab,"Artificial intelligence (AI)-based models used to improve different fields including healthcare, and finance. One of the field that receive advantages of AI is automation. However, it is important to consider human factors in application of AI in automation. This paper reports on a systematic review of the published studies used to investigate the application of AI in PM. This comprehensive systematic review used ScienceDirect to identify relevant articles. Of the 422 articles found, 40 met the inclusion and exclusion criteria and were used in the review. Selected articles were classified based on categories of human factors and areas of application. The results indicated that application of AI in automation with respect to human factors could be divided into three areas of physical ergonomics, cognitive ergonomic and organizational ergonomics. The main areas of application in physical and cognitive ergonomics are including transportation, User experience, and human-machine interactions. △ Less","9 April, 2021",https://arxiv.org/pdf/2104.09233
DA-DGCEx: Ensuring Validity of Deep Guided Counterfactual Explanations With Distribution-Aware Autoencoder Loss,Jokin Labaien;Ekhi Zugasti;Xabier De Carlos,"Deep Learning has become a very valuable tool in different fields, and no one doubts the learning capacity of these models. Nevertheless, since Deep Learning models are often seen as black boxes due to their lack of interpretability, there is a general mistrust in their decision-making process. To find a balance between effectiveness and interpretability, Explainable Artificial Intelligence (XAI) is gaining popularity in recent years, and some of the methods within this area are used to generate counterfactual explanations. The process of generating these explanations generally consists of solving an optimization problem for each input to be explained, which is unfeasible when real-time feedback is needed. To speed up this process, some methods have made use of autoencoders to generate instant counterfactual explanations. Recently, a method called Deep Guided Counterfactual Explanations (DGCEx) has been proposed, which trains an autoencoder attached to a classification model, in order to generate straightforward counterfactual explanations. However, this method does not ensure that the generated counterfactual instances are close to the data manifold, so unrealistic counterfactual instances may be generated. To overcome this issue, this paper presents Distribution Aware Deep Guided Counterfactual Explanations (DA-DGCEx), which adds a term to the DGCEx cost function that penalizes out of distribution counterfactual instances. △ Less","22 April, 2021",https://arxiv.org/pdf/2104.09062
RingCNN: Exploiting Algebraically-Sparse Ring Tensors for Energy-Efficient CNN-Based Computational Imaging,Chao-Tsung Huang,"In the era of artificial intelligence, convolutional neural networks (CNNs) are emerging as a powerful technique for computational imaging. They have shown superior quality for reconstructing fine textures from badly-distorted images and have potential to bring next-generation cameras and displays to our daily life. However, CNNs demand intensive computing power for generating high-resolution videos and defy conventional sparsity techniques when rendering dense details. Therefore, finding new possibilities in regular sparsity is crucial to enable large-scale deployment of CNN-based computational imaging. In this paper, we consider a fundamental but yet well-explored approach -- algebraic sparsity -- for energy-efficient CNN acceleration. We propose to build CNN models based on ring algebra that defines multiplication, addition, and non-linearity for n-tuples properly. Then the essential sparsity will immediately follow, e.g. n-times reduction for the number of real-valued weights. We define and unify several variants of ring algebras into a modeling framework, RingCNN, and make comparisons in terms of image quality and hardware complexity. On top of that, we further devise a novel ring algebra which minimizes complexity with component-wise product and achieves the best quality using directional ReLU. Finally, we implement an accelerator, eRingCNN, in two settings, n=2 and 4 (50% and 75% sparsity), with 40 nm technology to support advanced denoising and super-resolution at up to 4K UHD 30 fps. Layout results show that they can deliver equivalent 41 TOPS using 3.76 W and 2.22 W, respectively. Compared to the real-valued counterpart, our ring convolution engines for n=2 achieve 2.00x energy efficiency and 2.08x area efficiency with similar or even better image quality. With n=4, the efficiency gains of energy and area are further increased to 3.84x and 3.77x with 0.11 dB drop of PSNR. △ Less","19 April, 2021",https://arxiv.org/pdf/2104.09056
Modular Procedural Generation for Voxel Maps,Adarsh Pyarelal;Aditya Banerjee;Kobus Barnard,"Task environments developed in Minecraft are becoming increasingly popular for artificial intelligence (AI) research. However, most of these are currently constructed manually, thus failing to take advantage of procedural content generation (PCG), a capability unique to virtual task environments. In this paper, we present mcg, an open-source library to facilitate implementing PCG algorithms for voxel-based environments such as Minecraft. The library is designed with human-machine teaming research in mind, and thus takes a 'top-down' approach to generation, simultaneously generating low and high level machine-readable representations that are suitable for empirical research. These can be consumed by downstream AI applications that consider human spatial cognition. The benefits of this approach include rapid, scalable, and efficient development of virtual environments, the ability to control the statistics of the environment at a semantic level, and the ability to generate novel environments in response to player actions in real time. △ Less","18 April, 2021",https://arxiv.org/pdf/2104.08890
IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT,Baban Gain;Dibyanayan Bandyopadhyay;Tanik Saikh;Asif Ekbal,"Natural Language Processing (NLP) and Information Retrieval (IR) in the judicial domain is an essential task. With the advent of availability domain-specific data in electronic form and aid of different Artificial intelligence (AI) technologies, automated language processing becomes more comfortable, and hence it becomes feasible for researchers and developers to provide various automated tools to the legal community to reduce human burden. The Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in association with the International Conference on Artificial Intelligence and Law (ICAIL)-2019 has come up with few challenging tasks. The shared defined four sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to provide few automated systems to the judicial system. The paper presents our working note on the experiments carried out as a part of our participation in all the sub-tasks defined in this shared task. We make use of different Information Retrieval(IR) and deep learning based approaches to tackle these problems. We obtain encouraging results in all these four sub-tasks. △ Less","24 June, 2021",https://arxiv.org/pdf/2104.08653
Zero-shot Slot Filling with DPR and RAG,Michael Glass;Gaetano Rossiello;Alfio Gliozzo,"The ability to automatically extract Knowledge Graphs (KG) from a given collection of documents is a long-standing problem in Artificial Intelligence. One way to assess this capability is through the task of slot filling. Given an entity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot by generating or extracting the missing value from a relevant passage or passages. This capability is crucial to create systems for automatic knowledge base population, which is becoming in ever-increasing demand, especially in enterprise applications. Recently, there has been a promising direction in evaluating language models in the same way we would evaluate knowledge bases, and the task of slot filling is the most suitable to this intent. The recent advancements in the field try to solve this task in an end-to-end fashion using retrieval-based language models. Models like Retrieval Augmented Generation (RAG) show surprisingly good performance without involving complex information extraction pipelines. However, the results achieved by these models on the two slot filling tasks in the KILT benchmark are still not at the level required by real-world information extraction systems. In this paper, we describe several strategies we adopted to improve the retriever and the generator of RAG in order to make it a better slot filler. Our KGI0 system (available at https://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position on the KILT leaderboard on both T-REx and zsRE dataset with a large margin. △ Less","17 April, 2021",https://arxiv.org/pdf/2104.08610
ALF -- A Fitness-Based Artificial Life Form for Evolving Large-Scale Neural Networks,Rune Krauss;Marcel Merten;Mirco Bockholt;Rolf Drechsler,"Machine Learning (ML) is becoming increasingly important in daily life. In this context, Artificial Neural Networks (ANNs) are a popular approach within ML methods to realize an artificial intelligence. Usually, the topology of ANNs is predetermined. However, there are problems where it is difficult to find a suitable topology. Therefore, Topology and Weight Evolving Artificial Neural Network (TWEANN) algorithms have been developed that can find ANN topologies and weights using genetic algorithms. A well-known downside for large-scale problems is that TWEANN algorithms often evolve inefficient ANNs and require long runtimes. To address this issue, we propose a new TWEANN algorithm called Artificial Life Form (ALF) with the following technical advancements: (1) speciation via structural and semantic similarity to form better candidate solutions, (2) dynamic adaptation of the observed candidate solutions for better convergence properties, and (3) integration of solution quality into genetic reproduction to increase the probability of optimization success. Experiments on large-scale ML problems confirm that these approaches allow the fast solving of these problems and lead to efficient evolved ANNs. △ Less","16 April, 2021",https://arxiv.org/pdf/2104.08252
Multiple feature fusion-based video face tracking for IoT big data,Tianping Li;Zhifeng Liu;Jianping Qiao,"With the advancement of IoT and artificial intelligence technologies, and the need for rapid application growth in fields such as security entrance control and financial business trade, facial information processing has become an important means for achieving identity authentication and information security. In this paper, we propose a multi-feature fusion algorithm based on integral histograms and a real-time update tracking particle filtering module. First, edge and colour features are extracted, weighting methods are used to weight the colour histogram and edge features to describe facial features, and fusion of colour and edge features is made adaptive by using fusion coefficients to improve face tracking reliability. Then, the integral histogram is integrated into the particle filtering algorithm to simplify the calculation steps of complex particles. Finally, the tracking window size is adjusted in real time according to the change in the average distance from the particle centre to the edge of the current model and the initial model to reduce the drift problem and achieve stable tracking with significant changes in the target dimension. The results show that the algorithm improves video tracking accuracy, simplifies particle operation complexity, improves the speed, and has good anti-interference ability and robustness. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.08096
CyberLearning: Effectiveness Analysis of Machine Learning Security Modeling to Detect Cyber-Anomalies and Multi-Attacks,Iqbal H. Sarker,"Detecting cyber-anomalies and attacks are becoming a rising concern these days in the domain of cybersecurity. The knowledge of artificial intelligence, particularly, the machine learning techniques can be used to tackle these issues. However, the effectiveness of a learning-based security model may vary depending on the security features and the data characteristics. In this paper, we present ""CyberLearning"", a machine learning-based cybersecurity modeling with correlated-feature selection, and a comprehensive empirical analysis on the effectiveness of various machine learning based security models. In our CyberLearning modeling, we take into account a binary classification model for detecting anomalies, and multi-class classification model for various types of cyber-attacks. To build the security model, we first employ the popular ten machine learning classification techniques, such as naive Bayes, Logistic regression, Stochastic gradient descent, K-nearest neighbors, Support vector machine, Decision Tree, Random Forest, Adaptive Boosting, eXtreme Gradient Boosting, as well as Linear discriminant analysis. We then present the artificial neural network-based security model considering multiple hidden layers. The effectiveness of these learning-based security models is examined by conducting a range of experiments utilizing the two most popular security datasets, UNSW-NB15 and NSL-KDD. Overall, this paper aims to serve as a reference point for data-driven security modeling through our experimental analysis and findings in the context of cybersecurity. △ Less","28 March, 2021",https://arxiv.org/pdf/2104.08080
Ontology-based Feature Selection: A Survey,Konstantinos Sikelis;George E Tsekouras;Konstantinos I Kotis,"The SemanticWeb emerged as an extension to the traditional Web, towards adding meaning to a distributed Web of structured and linked data. At its core, the concept of ontology provides the means to semantically describe and structure information and data and expose it to software and human agents in a machine and human-readable form. For software agents to be realized, it is crucial to develop powerful artificial intelligence and machine learning techniques, able to extract knowledge from information and data sources and represent it in the underlying ontology. This survey aims to provide insight into key aspects of ontology-based knowledge extraction, from various sources such as text, images, databases and human expertise, with emphasis on the task of feature selection. First, some of the most common classification and feature selection algorithms are briefly presented. Then, selected methodologies, which utilize ontologies to represent features and perform feature selection and classification, are described. The presented examples span diverse application domains, e.g., medicine, tourism, mechanical and civil engineering, and demonstrate the feasibility and applicability of such methods. △ Less","30 April, 2021",https://arxiv.org/pdf/2104.07720
"Decentralized Federated Learning for UAV Networks: Architecture, Challenges, and Opportunities",Yuben Qu;Haipeng Dai;Yan Zhuang;Jiafa Chen;Chao Dong;Fan Wu;Song Guo,"Unmanned aerial vehicles (UAVs), or say drones, are envisioned to support extensive applications in next-generation wireless networks in both civil and military fields. Empowering UAVs networks intelligence by artificial intelligence (AI) especially machine learning (ML) techniques is inevitable and appealing to enable the aforementioned applications. To solve the problems of traditional cloud-centric ML for UAV networks such as privacy concern, unacceptable latency, and resource burden, a distributed ML technique, \textit(i.e.), federated learning (FL), has been recently proposed to enable multiple UAVs to collaboratively train ML model without letting out raw data. However, almost all existing FL paradigms are still centralized, \textit{i.e.}, a central entity is in charge of ML model aggregation and fusion over the whole network, which could result in the issue of a single point of failure and are inappropriate to UAV networks with both unreliable nodes and links. Thus motivated, in this article, we propose a novel architecture called DFL-UN (\underline{D}ecentralized \underline{F}ederated \underline{L}earning for \underline{U}AV \underline{N}etworks), which enables FL within UAV networks without a central entity. We also conduct a preliminary simulation study to validate the feasibility and effectiveness of the DFL-UN architecture. Finally, we discuss the main challenges and potential research directions in the DFL-UN. △ Less","18 August, 2021",https://arxiv.org/pdf/2104.07557
Street-Map Based Validation of Semantic Segmentation in Autonomous Driving,Laura von Rueden;Tim Wirtz;Fabian Hueger;Jan David Schneider;Nico Piatkowski;Christian Bauckhage,"Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness, which motivates the thorough validation of learned models. However, current validation approaches mostly require ground truth data and are thus both cost-intensive and limited in their applicability. We propose to overcome these limitations by a model agnostic validation using a-priori knowledge from street maps. In particular, we show how to validate semantic segmentation masks and demonstrate the potential of our approach using OpenStreetMap. We introduce validation metrics that indicate false positive or negative road segments. Besides the validation approach, we present a method to correct the vehicle's GPS position so that a more accurate localization can be used for the street-map based validation. Lastly, we present quantitative results on the Cityscapes dataset indicating that our validation approach can indeed uncover errors in semantic segmentation masks. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.07538
"Estimation of atrial fibrillation from lead-I ECGs: Comparison with cardiologists and machine learning model (CurAlive), a clinical validation study",N. Korucuk;C. Polat;E. S. Gunduz;O. Karaman;V. Tosun;M. Onac;N. Yildirim;Y. Cete;K. Polat,"Electrocardiogram recognition of cardiac arrhythmias is critical for cardiac abnormality diagnosis. Because of their strong prediction characteristics, artificial neural networks are the preferred method in medical diagnosis systems. This study presents a method to detect atrial fibrillation with lead-I ECGs using artificial intelligence. The aim of the study is to compare the accuracy of the diagnoses estimated by cardiologists and artificial intelligence over lead-I ECGs using 12-lead ECGs as references. To evaluate the performance of the proposed model, dataset were collected from China Physiological Signal Challenge 2018. In the study, diagnoses were examined in three groups as normal sinus rhythm, atrial fibrillation and OTHER. All rhythm and beat types except NSR and AFIB were labeled as OTHER super-class. OTHER contains First-degree atrioventricular blocks, Conduction disturbances, Left bundle branch block, Right bundle branch block, Premature atrial contraction, Premature ventricular contraction, ST-segment depression and ST-segment elevated type ECGs. CurAlive A.I. model which is using DenseNet as a CNN architecture and continuous wavelet transform as feature extraction method, showed a great performance on classifying ECGs from only lead-I compared to cardiologists. The AI model reached the weighted average precision, recall, F1-score and total accuracy 94.1%, 93.6%, 93.7% and 93.6% respectively, and the average of each of the three cardiologists has reached weighted average precision, recall, F1-score and total accuracy 82.2%, 54.6%, 57.5% and 54.6% respectively. This study showed that the proposed CNN model CurAlive, can be used to accurately diagnose AFIB, NSR, and OTHER rhythm using lead-I ECGs to accelerate the early detection of AFIB as a cardiologist assistant. It is also able to identify patients into different risk groups as part of remote patient monitoring systems. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.07427
Human-in-the-Loop Deep Reinforcement Learning with Application to Autonomous Driving,Jingda Wu;Zhiyu Huang;Chao Huang;Zhongxu Hu;Peng Hang;Yang Xing;Chen Lv,"Due to the limited smartness and abilities of machine intelligence, currently autonomous vehicles are still unable to handle all kinds of situations and completely replace drivers. Because humans exhibit strong robustness and adaptability in complex driving scenarios, it is of great importance to introduce humans into the training loop of artificial intelligence, leveraging human intelligence to further advance machine learning algorithms. In this study, a real-time human-guidance-based deep reinforcement learning (Hug-DRL) method is developed for policy training of autonomous driving. Leveraging a newly designed control transfer mechanism between human and automation, human is able to intervene and correct the agent's unreasonable actions in real time when necessary during the model training process. Based on this human-in-the-loop guidance mechanism, an improved actor-critic architecture with modified policy and value networks is developed. The fast convergence of the proposed Hug-DRL allows real-time human guidance actions to be fused into the agent's training loop, further improving the efficiency and performance of deep reinforcement learning. The developed method is validated by human-in-the-loop experiments with 40 subjects and compared with other state-of-the-art learning approaches. The results suggest that the proposed method can effectively enhance the training efficiency and performance of the deep reinforcement learning algorithm under human guidance, without imposing specific requirements on participant expertise and experience. △ Less","15 April, 2021",https://arxiv.org/pdf/2104.07246
Skilled and Mobile: Survey Evidence of AI Researchers' Immigration Preferences,Remco Zwetsloot;Baobao Zhang;Noemi Dreksler;Lauren Kahn;Markus Anderljung;Allan Dafoe;Michael C. Horowitz,"Countries, companies, and universities are increasingly competing over top-tier artificial intelligence (AI) researchers. Where are these researchers likely to immigrate and what affects their immigration decisions? We conducted a survey (n = 524) of the immigration preferences and motivations of researchers that had papers accepted at one of two prestigious AI conferences: the Conference on Neural Information Processing Systems (NeurIPS) and the International Conference on Machine Learning (ICML). We find that the U.S. is the most popular destination for AI researchers, followed by the U.K., Canada, Switzerland, and France. A country's professional opportunities stood out as the most common factor that influences immigration decisions of AI researchers, followed by lifestyle and culture, the political climate, and personal relations. The destination country's immigration policies were important to just under half of the researchers surveyed, while around a quarter noted current immigration difficulties to be a deciding factor. Visa and immigration difficulties were perceived to be a particular impediment to conducting AI research in the U.S., the U.K., and Canada. Implications of the findings for the future of AI talent policies and governance are discussed. △ Less","5 May, 2021",https://arxiv.org/pdf/2104.07237
Text Guide: Improving the quality of long text classification by a text selection method based on feature importance,Krzysztof Fiok;Waldemar Karwowski;Edgar Gutierrez;Mohammad Reza Davahli;Maciej Wilamowski;Tareq Ahram;Awad Al-Juaid;Jozef Zurada,"The performance of text classification methods has improved greatly over the last decade for text instances of less than 512 tokens. This limit has been adopted by most state-of-the-research transformer models due to the high computational cost of analyzing longer text instances. To mitigate this problem and to improve classification for longer texts, researchers have sought to resolve the underlying causes of the computational cost and have proposed optimizations for the attention mechanism, which is the key element of every transformer model. In our study, we are not pursuing the ultimate goal of long text classification, i.e., the ability to analyze entire text instances at one time while preserving high performance at a reasonable computational cost. Instead, we propose a text truncation method called Text Guide, in which the original text length is reduced to a predefined limit in a manner that improves performance over naive and semi-naive approaches while preserving low computational costs. Text Guide benefits from the concept of feature importance, a notion from the explainable artificial intelligence domain. We demonstrate that Text Guide can be used to improve the performance of recent language models specifically designed for long text classification, such as Longformer. Moreover, we discovered that parameter optimization is the key to Text Guide performance and must be conducted before the method is deployed. Future experiments may reveal additional benefits provided by this new method. △ Less","25 October, 2021",https://arxiv.org/pdf/2104.07225
A Large-Scale Analysis of Mixed Initiative in Information-Seeking Dialogues for Conversational Search,Svitlana Vakulenko;Evangelos Kanoulas;Maarten de Rijke,"Conversational search is a relatively young area of research that aims at automating an information-seeking dialogue. In this paper we help to position it with respect to other research areas within conversational Artificial Intelligence (AI) by analysing the structural properties of an information-seeking dialogue. To this end, we perform a large-scale dialogue analysis of more than 150K transcripts from 16 publicly available dialogue datasets. These datasets were collected to inform different dialogue-based tasks including conversational search. We extract different patterns of mixed initiative from these dialogue transcripts and use them to compare dialogues of different types. Moreover, we contrast the patterns found in information-seeking dialogues that are being used for research purposes with the patterns found in virtual reference interviews that were conducted by professional librarians. The insights we provide (1) establish close relations between conversational search and other conversational AI tasks; and (2) uncover limitations of existing conversational datasets to inform future data collection tasks. △ Less","8 June, 2021",https://arxiv.org/pdf/2104.07096
"Towards a framework for evaluating the safety, acceptability and efficacy of AI systems for health: an initial synthesis",Jessica Morley;Caroline Morton;Kassandra Karpathakis;Mariarosaria Taddeo;Luciano Floridi,"The potential presented by Artificial Intelligence (AI) for healthcare has long been recognised by the technical community. More recently, this potential has been recognised by policymakers, resulting in considerable public and private investment in the development of AI for healthcare across the globe. Despite this, excepting limited success stories, real-world implementation of AI systems into front-line healthcare has been limited. There are numerous reasons for this, but a main contributory factor is the lack of internationally accepted, or formalised, regulatory standards to assess AI safety and impact and effectiveness. This is a well-recognised problem with numerous ongoing research and policy projects to overcome it. Our intention here is to contribute to this problem-solving effort by seeking to set out a minimally viable framework for evaluating the safety, acceptability and efficacy of AI systems for healthcare. We do this by conducting a systematic search across Scopus, PubMed and Google Scholar to identify all the relevant literature published between January 1970 and November 2020 related to the evaluation of: output performance; efficacy; and real-world use of AI systems, and synthesising the key themes according to the stages of evaluation: pre-clinical (theoretical phase); exploratory phase; definitive phase; and post-market surveillance phase (monitoring). The result is a framework to guide AI system developers, policymakers, and regulators through a sufficient evaluation of an AI system designed for use in healthcare. △ Less","14 April, 2021",https://arxiv.org/pdf/2104.06910
Enabling Machine Learning Algorithms for Credit Scoring -- Explainable Artificial Intelligence (XAI) methods for clear understanding complex predictive models,Przemysław Biecek;Marcin Chlebus;Janusz Gajda;Alicja Gosiewska;Anna Kozak;Dominik Ogonowski;Jakub Sztachelski;Piotr Wojewnik,"Rapid development of advanced modelling techniques gives an opportunity to develop tools that are more and more accurate. However as usually, everything comes with a price and in this case, the price to pay is to loose interpretability of a model while gaining on its accuracy and precision. For managers to control and effectively manage credit risk and for regulators to be convinced with model quality the price to pay is too high. In this paper, we show how to take credit scoring analytics in to the next level, namely we present comparison of various predictive models (logistic regression, logistic regression with weight of evidence transformations and modern artificial intelligence algorithms) and show that advanced tree based models give best results in prediction of client default. What is even more important and valuable we also show how to boost advanced models using techniques which allow to interpret them and made them more accessible for credit risk practitioners, resolving the crucial obstacle in widespread deployment of more complex, 'black box' models like random forests, gradient boosted or extreme gradient boosted trees. All this will be shown on the large dataset obtained from the Polish Credit Bureau to which all the banks and most of the lending companies in the country do report the credit files. In this paper the data from lending companies were used. The paper then compares state of the art best practices in credit risk modelling with new advanced modern statistical tools boosted by the latest developments in the field of interpretability and explainability of artificial intelligence algorithms. We believe that this is a valuable contribution when it comes to presentation of different modelling tools but what is even more important it is showing which methods might be used to get insight and understanding of AI methods in credit risk context. △ Less","14 April, 2021",https://arxiv.org/pdf/2104.06735
Trust and Safety,S. K. Devitt;R. Horne;Z. Assaad;E. Broad;H. Kurniawati;B. Cardier;A. Scott;S. Lazar;M. Gould;C. Adamson;C. Karl;F. Schrever;S. Keay;K. Tranter;E. Shellshear;D. Hunter;M. Brady;T. Putland,"Robotics in Australia have a long history of conforming with safety standards and risk managed practices. This chapter articulates the current state of trust and safety in robotics including society's expectations, safety management systems and system safety as well as emerging issues and methods for ensuring safety in increasingly autonomous robotics. The future of trust and safety will combine standards with iterative, adaptive and responsive regulatory and assurance methods for diverse applications of robotics, autonomous systems and artificial intelligence (RAS-AI). Robotics will need novel technical and social approaches to achieve assurance, particularly for game-changing innovations. The ability for users to easily update algorithms and software, which alters the performance of a system, implies that traditional machine assurance performed prior to deployment or sale, will no longer be viable. Moreover, the high frequency of updates implies that traditional certification that requires substantial time will no longer be practical. To alleviate these difficulties, automation of assurance will likely be needed; something like 'ASsurance-as-a-Service' (ASaaS), where APIs constantly ping RAS-AI to ensure abidance with various rules, frameworks and behavioural expectations. There are exceptions to this, such as in contested or communications denied environments, or in underground or undersea mining; and these systems need their own risk assessments and limitations imposed. Indeed, self-monitors are already operating within some systems. To ensure safe operations of future robotics systems, Australia needs to invest in RAS-AI assurance research, stakeholder engagement and continued development and refinement of robust frameworks, methods, guidelines and policy in order to educate and prepare its technology developers, certifiers, and general population. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06512
Neuro-Symbolic VQA: A review from the perspective of AGI desiderata,Ian Berlot-Attwell,"An ultimate goal of the AI and ML fields is artificial general intelligence (AGI); although such systems remain science fiction, various models exhibit aspects of AGI. In this work, we look at neuro-symbolic (NS)approaches to visual question answering (VQA) from the perspective of AGI desiderata. We see how well these systems meet these desiderata, and how the desiderata often pull the scientist in opposing directions. It is my hope that through this work we can temper model evaluation on benchmarks with a discussion of the properties of these systems and their potential for future extension. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06365
Agents for Automated User Experience Testing,Pedro M. Fernandes;Manuel Lopes;Rui Prada,"The automation of functional testing in software has allowed developers to continuously check for negative impacts on functionality throughout the iterative phases of development. This is not the case for User eXperience (UX), which has hitherto relied almost exclusively on testing with real users. User testing is a slow endeavour that can become a bottleneck for development of interactive systems. To address this problem, we here propose an agent based approach for automatic UX testing. We develop agents with basic problem solving skills and a core affect model, allowing us to model an artificial affective state as they traverse different levels of a game. Although this research is still at a primordial state, we believe the results here presented make a strong case for the use of intelligent agents endowed with affective computing models for automating UX testing. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06220
LioNets: A Neural-Specific Local Interpretation Technique Exploiting Penultimate Layer Information,Ioannis Mollas;Nick Bassiliades;Grigorios Tsoumakas,"Artificial Intelligence (AI) has a tremendous impact on the unexpected growth of technology in almost every aspect. AI-powered systems are monitoring and deciding about sensitive economic and societal issues. The future is towards automation, and it must not be prevented. However, this is a conflicting viewpoint for a lot of people, due to the fear of uncontrollable AI systems. This concern could be reasonable if it was originating from considerations associated with social issues, like gender-biased, or obscure decision-making systems. Explainable AI (XAI) is recently treated as a huge step towards reliable systems, enhancing the trust of people to AI. Interpretable machine learning (IML), a subfield of XAI, is also an urgent topic of research. This paper presents a small but significant contribution to the IML community, focusing on a local-based, neural-specific interpretation process applied to textual and time-series data. The proposed methodology introduces new approaches to the presentation of feature importance based interpretations, as well as the production of counterfactual words on textual datasets. Eventually, an improved evaluation metric is introduced for the assessment of interpretation techniques, which supports an extensive set of qualitative and quantitative experiments. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.06057
An Adaptive Synaptic Array using Fowler-Nordheim Dynamic Analog Memory,Darshit Mehta;Kenji Aono;Shantanu Chakrabartty,"In this paper we present a synaptic array that uses dynamical states to implement an analog memory for energy-efficient training of machine learning (ML) systems. Each of the analog memory elements is a micro-dynamical system that is driven by the physics of Fowler-Nordheim (FN) quantum tunneling, whereas the system level learning modulates the state trajectory of the memory ensembles towards the optimal solution. We show that the extrinsic energy required for modulation can be matched to the dynamics of learning and weight decay leading to a significant reduction in the energy-dissipated during ML training. With the energy-dissipation as low as 5 fJ per memory update and a programming resolution up to 14 bits, the proposed synapse array could be used to address the energy-efficiency imbalance between the training and the inference phases observed in artificial intelligence (AI) systems. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.05926
Visual Goal-Step Inference using wikiHow,Yue Yang;Artemis Panagopoulou;Qing Lyu;Li Zhang;Mark Yatskar;Chris Callison-Burch,"Understanding what sequence of steps are needed to complete a goal can help artificial intelligence systems reason about human activities. Past work in NLP has examined the task of goal-step inference for text. We introduce the visual analogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model is given a textual goal and must choose which of four images represents a plausible step towards that goal. With a new dataset harvested from wikiHow consisting of 772,277 images representing human actions, we show that our task is challenging for state-of-the-art multimodal models. Moreover, the multimodal representation learned from our data can be effectively transferred to other datasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task will facilitate multimodal reasoning about procedural events. △ Less","9 September, 2021",https://arxiv.org/pdf/2104.05845
Fruit Quality and Defect Image Classification with Conditional GAN Data Augmentation,Jordan J. Bird;Chloe M. Barnes;Luis J. Manso;Anikó Ekárt;Diego R. Faria,"Contemporary Artificial Intelligence technologies allow for the employment of Computer Vision to discern good crops from bad, providing a step in the pipeline of selecting healthy fruit from undesirable fruit, such as those which are mouldy or gangrenous. State-of-the-art works in the field report high accuracy results on small datasets (<1000 images), which are not representative of the population regarding real-world usage. The goals of this study are to further enable real-world usage by improving generalisation with data augmentation as well as to reduce overfitting and energy usage through model pruning. In this work, we suggest a machine learning pipeline that combines the ideas of fine-tuning, transfer learning, and generative model-based training data augmentation towards improving fruit quality image classification. A linear network topology search is performed to tune a VGG16 lemon quality classification model using a publicly-available dataset of 2690 images. We find that appending a 4096 neuron fully connected layer to the convolutional layers leads to an image classification accuracy of 83.77%. We then train a Conditional Generative Adversarial Network on the training data for 2000 epochs, and it learns to generate relatively realistic images. Grad-CAM analysis of the model trained on real photographs shows that the synthetic images can exhibit classifiable characteristics such as shape, mould, and gangrene. A higher image classification accuracy of 88.75% is then attained by augmenting the training with synthetic images, arguing that Conditional Generative Adversarial Networks have the ability to produce new data to alleviate issues of data scarcity. Finally, model pruning is performed via polynomial decay, where we find that the Conditional GAN-augmented classification network can retain 81.16% classification accuracy when compressed to 50% of its original size. △ Less","12 April, 2021",https://arxiv.org/pdf/2104.05647
Deep Transformer Networks for Time Series Classification: The NPP Safety Case,Bing Zha;Alessandro Vanni;Yassin Hassan;Tunc Aldemir;Alper Yilmaz,"A challenging part of dynamic probabilistic risk assessment for nuclear power plants is the need for large amounts of temporal simulations given various initiating events and branching conditions from which representative feature extraction becomes complicated for subsequent applications. Artificial Intelligence techniques have been shown to be powerful tools in time-dependent sequential data processing to automatically extract and yield complex features from large data. An advanced temporal neural network referred to as the Transformer is used within a supervised learning fashion to model the time-dependent NPP simulation data and to infer whether a given sequence of events leads to core damage or not. The training and testing datasets for the Transformer are obtained by running 10,000 RELAP5-3D NPP blackout simulations with the list of variables obtained from the RAVEN software. Each simulation is classified as ""OK"" or ""CORE DAMAGE"" based on the consequence. The results show that the Transformer can learn the characteristics of the sequential data and yield promising performance with approximately 99% classification accuracy on the testing dataset. △ Less","18 April, 2021",https://arxiv.org/pdf/2104.05448
Machine learning and deep learning,Christian Janiesch;Patrick Zschech;Kai Heinrich,"Today, intelligent systems that offer artificial intelligence capabilities often rely on machine learning. Machine learning describes the capacity of systems to learn from problem-specific training data to automate the process of analytical model building and solve associated tasks. Deep learning is a machine learning concept based on artificial neural networks. For many applications, deep learning models outperform shallow machine learning models and traditional data analysis approaches. In this article, we summarize the fundamentals of machine learning and deep learning to generate a broader understanding of the methodical underpinning of current intelligent systems. In particular, we provide a conceptual distinction between relevant terms and concepts, explain the process of automated analytical model building through machine learning and deep learning, and discuss the challenges that arise when implementing such intelligent systems in the field of electronic markets and networked business. These naturally go beyond technological aspects and highlight issues in human-machine interaction and artificial intelligence servitization. △ Less","14 April, 2021",https://arxiv.org/pdf/2104.05314
Distributed Learning Systems with First-order Methods,Ji Liu;Ce Zhang,"Scalable and efficient distributed learning is one of the main driving forces behind the recent rapid advancement of machine learning and artificial intelligence. One prominent feature of this topic is that recent progresses have been made by researchers in two communities: (1) the system community such as database, data management, and distributed systems, and (2) the machine learning and mathematical optimization community. The interaction and knowledge sharing between these two communities has led to the rapid development of new distributed learning systems and theory. In this work, we hope to provide a brief introduction of some distributed learning techniques that have recently been developed, namely lossy communication compression (e.g., quantization and sparsification), asynchronous communication, and decentralized communication. One special focus in this work is on making sure that it can be easily understood by researchers in both communities -- On the system side, we rely on a simplified system model hiding many system details that are not necessary for the intuition behind the system speedups; while, on the theory side, we rely on minimal assumptions and significantly simplify the proof of some recent work to achieve comparable results. △ Less","12 April, 2021",https://arxiv.org/pdf/2104.05245
Artificial Intelligence Methods Based Hierarchical Classification of Frontotemporal Dementia to Improve Diagnostic Predictability,Km Poonam;Rajlakshmi Guha;Partha P Chakrabarti,"Patients with Frontotemporal Dementia (FTD) have impaired cognitive abilities, executive and behavioral traits, loss of language ability, and decreased memory capabilities. Based on the distinct patterns of cortical atrophy and symptoms, the FTD spectrum primarily includes three variants: behavioral variant FTD (bvFTD), non-fluent variant primary progressive aphasia (nfvPPA), and semantic variant primary progressive aphasia (svPPA). The purpose of this study is to classify MRI images of every single subject into one of the spectrums of the FTD in a hierarchical order by applying data-driven techniques of Artificial Intelligence (AI) on cortical thickness data. This data is computed by FreeSurfer software. We used the Smallest Univalue Segment Assimilating Nucleus (SUSAN) technique to minimize the noise in cortical thickness data. Specifically, we took 204 subjects from the frontotemporal lobar degeneration neuroimaging initiative (NIFTD) database to validate this approach, and each subject was diagnosed in one of the diagnostic categories (bvFTD, svPPA, nfvPPA and cognitively normal). Our proposed automated classification model yielded classification accuracy of 86.5, 76, and 72.7 with support vector machine (SVM), linear discriminant analysis (LDA), and Naive Bayes methods, respectively, in 10-fold cross-validation analysis, which is a significant improvement on a traditional single multi-class model with an accuracy of 82.7, 73.4, and 69.2. △ Less","12 April, 2021",https://arxiv.org/pdf/2104.05235
Accelerating science with human versus alien artificial intelligences,Jamshid Sourati;James Evans,"Data-driven artificial intelligence models fed with published scientific findings have been used to create powerful prediction engines for scientific and technological advance, such as the discovery of novel materials with desired properties and the targeted invention of new therapies and vaccines. These AI approaches typically ignore the distribution of human prediction engines -- scientists and inventor -- who continuously alter the landscape of discovery and invention. As a result, AI hypotheses are designed to substitute for human experts, failing to complement them for punctuated collective advance. Here we show that incorporating the distribution of human expertise into self-supervised models by training on inferences cognitively available to experts dramatically improves AI prediction of future human discoveries and inventions. Including expert-awareness into models that propose (a) valuable energy-relevant materials increases the precision of materials predictions by ~100%, (b) repurposing thousands of drugs to treat new diseases increases precision by 43%, and (c) COVID-19 vaccine candidates examined in clinical trials by 260%. These models succeed by predicting human predictions and the scientists who will make them. By tuning AI to avoid the crowd, however, it generates scientifically promising ""alien"" hypotheses unlikely to be imagined or pursued without intervention, not only accelerating but punctuating scientific advance. By identifying and correcting for collective human bias, these models also suggest opportunities to improve human prediction by reformulating science education for discovery. △ Less","11 April, 2021",https://arxiv.org/pdf/2104.05188
Towards a Collective Agenda on AI for Earth Science Data Analysis,Devis Tuia;Ribana Roscher;Jan Dirk Wegner;Nathan Jacobs;Xiao Xiang Zhu;Gustau Camps-Valls,"In the last years we have witnessed the fields of geosciences and remote sensing and artificial intelligence to become closer. Thanks to both the massive availability of observational data, improved simulations, and algorithmic advances, these disciplines have found common objectives and challenges to advance the modeling and understanding of the Earth system. Despite such great opportunities, we also observed a worrying tendency to remain in disciplinary comfort zones applying recent advances from artificial intelligence on well resolved remote sensing problems. Here we take a position on research directions where we think the interface between these fields will have the most impact and become potential game changers. In our declared agenda for AI on Earth sciences, we aim to inspire researchers, especially the younger generations, to tackle these challenges for a real advance of remote sensing and the geosciences. △ Less","11 April, 2021",https://arxiv.org/pdf/2104.05107
A review of artificial intelligence methods combined with Raman spectroscopy to identify the composition of substances,Liangrui Pan;Peng Zhang;Chalongrat Daengngam;Mitchai Chongcheawchamnan,"In general, most of the substances in nature exist in mixtures, and the noninvasive identification of mixture composition with high speed and accuracy remains a difficult task. However, the development of Raman spectroscopy, machine learning, and deep learning techniques have paved the way for achieving efficient analytical tools capable of identifying mixture components, making an apparent breakthrough in the identification of mixtures beyond the traditional chemical analysis methods. This article summarizes the work of Raman spectroscopy in identifying the composition of substances as well as provides detailed reviews on the preprocessing process of Raman spectroscopy, the analysis methods and applications of artificial intelligence. This review summarizes the work of Raman spectroscopy in identifying the composition of substances and reviews the preprocessing process of Raman spectroscopy, the analysis methods and applications of artificial intelligence. Finally, the advantages and disadvantages and development prospects of Raman spectroscopy are discussed in detail. △ Less","4 April, 2021",https://arxiv.org/pdf/2104.04599
"Artificial intelligence, human rights, democracy, and the rule of law: a primer",David Leslie;Christopher Burr;Mhairi Aitken;Josh Cowls;Michael Katell;Morgan Briggs,"In September 2019, the Council of Europe's Committee of Ministers adopted the terms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI). The CAHAI is charged with examining the feasibility and potential elements of a legal framework for the design, development, and deployment of AI systems that accord with Council of Europe standards across the interrelated areas of human rights, democracy, and the rule of law. As a first and necessary step in carrying out this responsibility, the CAHAI's Feasibility Study, adopted by its plenary in December 2020, has explored options for an international legal response that fills existing gaps in legislation and tailors the use of binding and non-binding legal instruments to the specific risks and opportunities presented by AI systems. The Study examines how the fundamental rights and freedoms that are already codified in international human rights law can be used as the basis for such a legal framework. The purpose of this primer is to introduce the main concepts and principles presented in the CAHAI's Feasibility Study for a general, non-technical audience. It also aims to provide some background information on the areas of AI innovation, human rights law, technology policy, and compliance mechanisms covered therein. In keeping with the Council of Europe's commitment to broad multi-stakeholder consultations, outreach, and engagement, this primer has been designed to help facilitate the meaningful and informed participation of an inclusive group of stakeholders as the CAHAI seeks feedback and guidance regarding the essential issues raised by the Feasibility Study. △ Less","2 April, 2021",https://arxiv.org/pdf/2104.04147
Characterization of Time-variant and Time-invariant Assessment of Suicidality on Reddit using C-SSRS,Manas Gaur;Vamsi Aribandi;Amanuel Alambo;Ugur Kursuncu;Krishnaprasad Thirunarayan;Jonanthan Beich;Jyotishman Pathak;Amit Sheth,"Suicide is the 10th leading cause of death in the U.S (1999-2019). However, predicting when someone will attempt suicide has been nearly impossible. In the modern world, many individuals suffering from mental illness seek emotional support and advice on well-known and easily-accessible social media platforms such as Reddit. While prior artificial intelligence research has demonstrated the ability to extract valuable information from social media on suicidal thoughts and behaviors, these efforts have not considered both severity and temporality of risk. The insights made possible by access to such data have enormous clinical potential - most dramatically envisioned as a trigger to employ timely and targeted interventions (i.e., voluntary and involuntary psychiatric hospitalization) to save lives. In this work, we address this knowledge gap by developing deep learning algorithms to assess suicide risk in terms of severity and temporality from Reddit data based on the Columbia Suicide Severity Rating Scale (C-SSRS). In particular, we employ two deep learning approaches: time-variant and time-invariant modeling, for user-level suicide risk assessment, and evaluate their performance against a clinician-adjudicated gold standard Reddit corpus annotated based on the C-SSRS. Our results suggest that the time-variant approach outperforms the time-invariant method in the assessment of suicide-related ideations and supportive behaviors (AUC:0.78), while the time-invariant model performed better in predicting suicide-related behaviors and suicide attempt (AUC:0.64). The proposed approach can be integrated with clinical diagnostic interviews for improving suicide risk assessments. △ Less","8 April, 2021",https://arxiv.org/pdf/2104.04140
An artificial intelligence and Internet of things based automated irrigation system,Ömer Aydin;Cem Ali Kandemir;Umut Kiraç;Feriştah Dalkiliç,"It is not hard to see that the need for clean water is growing by considering the decrease of the water sources day by day in the world. Potable fresh water is also used for irrigation, so it should be planned to decrease freshwater wastage. With the development of technology and the availability of cheaper and more effective solutions, the efficiency of irrigation increased and the water loss can be reduced. In particular, Internet of things (IoT) devices has begun to be used in all areas. We can easily and precisely collect temperature, humidity and mineral values from the irrigation field with the IoT devices and sensors. Most of the operations and decisions about irrigation are carried out by people. For people, it is hard to have all the real-time data such as temperature, moisture and mineral levels in the decision-making process and make decisions by considering them. People usually make decisions with their experience. In this study, a wide range of information from the irrigation field was obtained by using IoT devices and sensors. Data collected from IoT devices and sensors sent via communication channels and stored on MongoDB. With the help of Weka software, the data was normalized and the normalized data was used as a learning set. As a result of the examinations, a decision tree (J48) algorithm with the highest accuracy was chosen and an artificial intelligence model was created. Decisions are used to manage operations such as starting, maintaining and stopping the irrigation. The accuracy of the decisions was evaluated and the irrigation system was tested with the results. There are options to manage, view the system remotely and manually and also see the system s decisions with the created mobile application. △ Less","1 April, 2021",https://arxiv.org/pdf/2104.04076
Towards a Rigorous Evaluation of Explainability for Multivariate Time Series,Rohit Saluja;Avleen Malhi;Samanta Knapič;Kary Främling;Cicek Cavdar,"Machine learning-based systems are rapidly gaining popularity and in-line with that there has been a huge research surge in the field of explainability to ensure that machine learning models are reliable, fair, and can be held liable for their decision-making process. Explainable Artificial Intelligence (XAI) methods are typically deployed to debug black-box machine learning models but in comparison to tabular, text, and image data, explainability in time series is still relatively unexplored. The aim of this study was to achieve and evaluate model agnostic explainability in a time series forecasting problem. This work focused on proving a solution for a digital consultancy company aiming to find a data-driven approach in order to understand the effect of their sales related activities on the sales deals closed. The solution involved framing the problem as a time series forecasting problem to predict the sales deals and the explainability was achieved using two novel model agnostic explainability techniques, Local explainable model-agnostic explanations (LIME) and Shapley additive explanations (SHAP) which were evaluated using human evaluation of explainability. The results clearly indicate that the explanations produced by LIME and SHAP greatly helped lay humans in understanding the predictions made by the machine learning model. The presented work can easily be extended to any time △ Less","6 April, 2021",https://arxiv.org/pdf/2104.04075
Towards a New Participatory Approach for Designing Artificial Intelligence and Data-Driven Technologies,Soaad Hossain;Syed Ishtiaque Ahmed,"With there being many technical and ethical issues with artificial intelligence (AI) that involve marginalized communities, there is a growing interest for design methods used with marginalized people that may be transferable to the design of AI technologies. Participatory design (PD) is a design method that is often used with marginalized communities for the design of social development, policy, IT and other matters and solutions. However, there are issues with the current PD, raising concerns when it is applied to the design of technologies, including AI technologies. This paper argues for the use of PD for the design of AI technologies, and introduces and proposes a new PD, which we call agile participatory design, that not only can could be used for the design of AI and data-driven technologies, but also overcomes issues surrounding current PD and its use in the design of such technologies. △ Less","30 March, 2021",https://arxiv.org/pdf/2104.04072
Transient Information Adaptation of Artificial Intelligence: Towards Sustainable Data Processes in Complex Projects,Nicholas Dacre;Fredrik Kockum;PK Senyo,"Large scale projects increasingly operate in complicated settings whilst drawing on an array of complex data-points, which require precise analysis for accurate control and interventions to mitigate possible project failure. Coupled with a growing tendency to rely on new information systems and processes in change projects, 90% of megaprojects globally fail to achieve their planned objectives. Renewed interest in the concept of Artificial Intelligence (AI) against a backdrop of disruptive technological innovations, seeks to enhance project managers cognitive capacity through the project lifecycle and enhance project excellence. However, despite growing interest there remains limited empirical insights on project managers ability to leverage AI for cognitive load enhancement in complex settings. As such this research adopts an exploratory sequential linear mixed methods approach to address unresolved empirical issues on transient adaptations of AI in complex projects, and the impact on cognitive load enhancement. Initial thematic findings from semi-structured interviews with domain experts, suggest that in order to leverage AI technologies and processes for sustainable cognitive load enhancement with complex data over time, project managers require improved knowledge and access to relevant technologies that mediate data processes in complex projects, but equally reflect application across different project phases. These initial findings support further hypothesis testing through a larger quantitative study incorporating structural equation modelling to examine the relationship between artificial intelligence and project managers cognitive load with project data in complex contexts. △ Less","18 April, 2021",https://arxiv.org/pdf/2104.04067
AlephBERT:A Hebrew Large Pre-Trained Language Model to Start-off your Hebrew NLP Application With,Amit Seker;Elron Bandel;Dan Bareket;Idan Brusilovsky;Refael Shaked Greenfeld;Reut Tsarfaty,"Large Pre-trained Language Models (PLMs) have become ubiquitous in the development of language understanding technology and lie at the heart of many artificial intelligence advances. While advances reported for English using PLMs are unprecedented, reported advances using PLMs in Hebrew are few and far between. The problem is twofold. First, Hebrew resources available for training NLP models are not at the same order of magnitude as their English counterparts. Second, there are no accepted tasks and benchmarks to evaluate the progress of Hebrew PLMs on. In this work we aim to remedy both aspects. First, we present AlephBERT, a large pre-trained language model for Modern Hebrew, which is trained on larger vocabulary and a larger dataset than any Hebrew PLM before. Second, using AlephBERT we present new state-of-the-art results on multiple Hebrew tasks and benchmarks, including: Segmentation, Part-of-Speech Tagging, full Morphological Tagging, Named-Entity Recognition and Sentiment Analysis. We make our AlephBERT model publicly available, providing a single point of entry for the development of Hebrew NLP applications. △ Less","8 April, 2021",https://arxiv.org/pdf/2104.04052
Enabling Cross-Domain Communication: How to Bridge the Gap between AI and HW Engineers,Michael J. Klaiber;Axel J. Acosta;Ingo Feldner;Falk Rehm,"A key issue in system design is the lack of communication between hardware, software and domain expert. Recent research work shows progress in automatic HW/SW co-design flows of neural accelerators that seems to make this kind of communication obsolete. Most real-world systems, however, are a composition of multiple processing units, communication networks and memories. A HW/SW co-design process of (reconfigurable) neural accelerators, therefore, is an important sub-problem towards a common co-design methodology. The ultimate challenge is to define the constraints for the design space exploration on system level - a task which requires deep knowledge and understanding of hardware architectures, mapping of workloads onto hardware and the application domain, e.g. artificial intelligence. For most projects, these skills are distributed among several people or even different teams which is one of the major reasons why there is no established end-to-end development methodology for digital systems. This position paper discusses possibilities how to establish such a methodology for systems that include (reconfigurable) dedicated accelerators and outlines the central role that languages and tools play in the process. △ Less","8 April, 2021",https://arxiv.org/pdf/2104.03780
Voluntary safety commitments provide an escape from over-regulation in AI development,The Anh Han;Tom Lenaerts;Francisco C. Santos;Luis Moniz Pereira,"With the introduction of Artificial Intelligence (AI) and related technologies in our daily lives, fear and anxiety about their misuse as well as the hidden biases in their creation have led to a demand for regulation to address such issues. Yet blindly regulating an innovation process that is not well understood, may stifle this process and reduce benefits that society may gain from the generated technology, even under the best intentions. In this paper, starting from a baseline model that captures the fundamental dynamics of a race for domain supremacy using AI technology, we demonstrate how socially unwanted outcomes may be produced when sanctioning is applied unconditionally to risk-taking, i.e. potentially unsafe, behaviours. As an alternative to resolve the detrimental effect of over-regulation, we propose a voluntary commitment approach wherein technologists have the freedom of choice between independently pursuing their course of actions or establishing binding agreements to act safely, with sanctioning of those that do not abide to what they pledged. Overall, this work reveals for the first time how voluntary commitments, with sanctions either by peers or an institution, leads to socially beneficial outcomes in all scenarios envisageable in a short-term race towards domain supremacy through AI technology. These results are directly relevant for the design of governance and regulatory policies that aim to ensure an ethical and responsible AI technology development process. △ Less","8 April, 2021",https://arxiv.org/pdf/2104.03741
Quantum Enhanced Filter: QFilter,Parfait Atchade-Adelomou;Guillermo Alonso-Linaje,"Convolutional Neural Networks (CNN) are used mainly to treat problems with many images characteristic of Deep Learning. In this work, we propose a hybrid image classification model to take advantage of quantum and classical computing. The method will use the potential that convolutional networks have shown in artificial intelligence by replacing classical filters with variational quantum filters. Similarly, this work will compare with other classification methods and the system's execution on different servers. The algorithm's quantum feasibility is modelled and tested on Amazon Braket Notebook instances and experimented on the Pennylane's philosophy and framework. △ Less","7 April, 2021",https://arxiv.org/pdf/2104.03418
Nanosecond machine learning event classification with boosted decision trees in FPGA for high energy physics,Tae Min Hong;Benjamin Carlson;Brandon Eubanks;Stephen Racz;Stephen Roche;Joerg Stelzer;Daniel Stumpp,"We present a novel implementation of classification using the machine learning / artificial intelligence method called boosted decision trees (BDT) on field programmable gate arrays (FPGA). The firmware implementation of binary classification requiring 100 training trees with a maximum depth of 4 using four input variables gives a latency value of about 10 ns, independent of the clock speed from 100 to 320 MHz in our setup. The low timing values are achieved by restructuring the BDT layout and reconfiguring its parameters. The FPGA resource utilization is also kept low at a range from 0.01% to 0.2% in our setup. A software package called fwXmachina achieves this implementation. Our intended user is an expert of custom electronics-based trigger systems in high energy physics experiments or anyone that needs decisions at the lowest latency values for real-time event classification. Two problems from high energy physics are considered, in the separation of electrons vs. photons and in the selection of vector boson fusion-produced Higgs bosons vs. the rejection of the multijet processes. △ Less","17 August, 2021",https://arxiv.org/pdf/2104.03408
Triplot: model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure,Katarzyna Pekala;Katarzyna Woznica;Przemyslaw Biecek,"One of the key elements of explanatory analysis of a predictive model is to assess the importance of individual variables. Rapid development of the area of predictive model exploration (also called explainable artificial intelligence or interpretable machine learning) has led to the popularization of methods for local (instance level) and global (dataset level) methods, such as Permutational Variable Importance, Shapley Values (SHAP), Local Interpretable Model Explanations (LIME), Break Down and so on. However, these methods do not use information about the correlation between features which significantly reduce the explainability of the model behaviour. In this work, we propose new methods to support model analysis by exploiting the information about the correlation between variables. The dataset level aspect importance measure is inspired by the block permutations procedure, while the instance level aspect importance measure is inspired by the LIME method. We show how to analyze groups of variables (aspects) both when they are proposed by the user and when they should be determined automatically based on the hierarchical structure of correlations between variables. Additionally, we present the new type of model visualisation, triplot, which exploits a hierarchical structure of variable grouping to produce a high information density model visualisation. This visualisation provides a consistent illustration for either local or global model and data exploration. We also show an example of real-world data with 5k instances and 37 features in which a significant correlation between variables affects the interpretation of the effect of variable importance. The proposed method is, to our knowledge, the first to allow direct use of the correlation between variables in exploratory model analysis. △ Less","7 April, 2021",https://arxiv.org/pdf/2104.03403
Enabling Integration and Interaction for Decentralized Artificial Intelligence in Airline Disruption Management,Kolawole Ogunsina;Daniel DeLaurentis,"Airline disruption management traditionally seeks to address three problem dimensions: aircraft scheduling, crew scheduling, and passenger scheduling, in that order. However, current efforts have, at most, only addressed the first two problem dimensions concurrently and do not account for the propagative effects that uncertain scheduling outcomes in one dimension can have on another dimension. In addition, existing approaches for airline disruption management include human specialists who decide on necessary corrective actions for airline schedule disruptions on the day of operation. However, human specialists are limited in their ability to process copious amounts of information imperative for making robust decisions that simultaneously address all problem dimensions during disruption management. Therefore, there is a need to augment the decision-making capabilities of a human specialist with quantitative and qualitative tools that can rationalize complex interactions amongst all dimensions in airline disruption management, and provide objective insights to the specialists in the airline operations control center. To that effect, we provide a discussion and demonstration of an agnostic and systematic paradigm for enabling expeditious simultaneously-integrated recovery of all problem dimensions during airline disruption management, through an intelligent multi-agent system that employs principles from artificial intelligence and distributed ledger technology. Results indicate that our paradigm for simultaneously-integrated recovery executes in polynomial time and is effective when all the flights in the airline route network are disrupted. △ Less","20 December, 2021",https://arxiv.org/pdf/2104.03349
"TB-Net: A Tailored, Self-Attention Deep Convolutional Neural Network Design for Detection of Tuberculosis Cases from Chest X-ray Images",Alexander Wong;James Ren Hou Lee;Hadi Rahmat-Khah;Ali Sabri;Amer Alaref,"Tuberculosis (TB) remains a global health problem, and is the leading cause of death from an infectious disease. A crucial step in the treatment of tuberculosis is screening high risk populations and the early detection of the disease, with chest x-ray (CXR) imaging being the most widely-used imaging modality. As such, there has been significant recent interest in artificial intelligence-based TB screening solutions for use in resource-limited scenarios where there is a lack of trained healthcare workers with expertise in CXR interpretation. Motivated by this pressing need and the recent recommendation by the World Health Organization (WHO) for the use of computer-aided diagnosis of TB, we introduce TB-Net, a self-attention deep convolutional neural network tailored for TB case screening. More specifically, we leveraged machine-driven design exploration to build a highly customized deep neural network architecture with attention condensers. We conducted an explainability-driven performance validation process to validate TB-Net's decision-making behaviour. Experiments on CXR data from a multi-national patient cohort showed that the proposed TB-Net is able to achieve accuracy/sensitivity/specificity of 99.86%/100.0%/99.71%. Radiologist validation was conducted on select cases by two board-certified radiologists with over 10 and 19 years of experience, respectively, and showed consistency between radiologist interpretation and critical factors leveraged by TB-Net for TB case detection for the case where radiologists identified anomalies. While not a production-ready solution, we hope that the open-source release of TB-Net as part of the COVID-Net initiative will support researchers, clinicians, and citizen data scientists in advancing this field in the fight against this global public health crisis. △ Less","13 April, 2021",https://arxiv.org/pdf/2104.03165
AI perspectives in Smart Cities and Communities to enable road vehicle automation and smart traffic control,Cristofer Englund;Eren Erdal Aksoy;Fernando Alonso-Fernandez;Martin Daniel Cooney;Sepideh Pashami;Bjorn Astrand,"Smart Cities and Communities (SCC) constitute a new paradigm in urban development. SCC ideates on a data-centered society aiming at improving efficiency by automating and optimizing activities and utilities. Information and communication technology along with internet of things enables data collection and with the help of artificial intelligence (AI) situation awareness can be obtained to feed the SCC actors with enriched knowledge. This paper describes AI perspectives in SCC and gives an overview of AI-based technologies used in traffic to enable road vehicle automation and smart traffic control. Perception, Smart Traffic Control and Driver Modelling are described along with open research challenges and standardization to help introduce advanced driver assistance systems and automated vehicle functionality in traffic. To fully realize the potential of SCC, to create a holistic view on a city level, the availability of data from different stakeholders is need. Further, though AI technologies provide accurate predictions and classifications there is an ambiguity regarding the correctness of their outputs. This can make it difficult for the human operator to trust the system. Today there are no methods that can be used to match function requirements with the level of detail in data annotation in order to train an accurate model. Another challenge related to trust is explainability, while the models have difficulties explaining how they come to a certain conclusion it is difficult for humans to trust it. △ Less","12 May, 2021",https://arxiv.org/pdf/2104.03150
Bootstrapping Your Own Positive Sample: Contrastive Learning With Electronic Health Record Data,Tingyi Wanyan;Jing Zhang;Ying Ding;Ariful Azad;Zhangyang Wang;Benjamin S Glicksberg,"Electronic Health Record (EHR) data has been of tremendous utility in Artificial Intelligence (AI) for healthcare such as predicting future clinical events. These tasks, however, often come with many challenges when using classical machine learning models due to a myriad of factors including class imbalance and data heterogeneity (i.e., the complex intra-class variances). To address some of these research gaps, this paper leverages the exciting contrastive learning framework and proposes a novel contrastive regularized clinical classification model. The contrastive loss is found to substantially augment EHR-based prediction: it effectively characterizes the similar/dissimilar patterns (by its ""push-and-pull"" form), meanwhile mitigating the highly skewed class distribution by learning more balanced feature spaces (as also echoed by recent findings). In particular, when naively exporting the contrastive learning to the EHR data, one hurdle is in generating positive samples, since EHR data is not as amendable to data augmentation as image data. To this end, we have introduced two unique positive sampling strategies specifically tailored for EHR data: a feature-based positive sampling that exploits the feature space neighborhood structure to reinforce the feature learning; and an attribute-based positive sampling that incorporates pre-generated patient similarity metrics to define the sample proximity. Both sampling approaches are designed with an awareness of unique high intra-class variance in EHR data. Our overall framework yields highly competitive experimental results in predicting the mortality risk on real-world COVID-19 EHR data with a total of 5,712 patients admitted to a large, urban health system. Specifically, our method reaches a high AUROC prediction score of 0.959, which outperforms other baselines and alternatives: cross-entropy(0.873) and focal loss(0.931). △ Less","7 April, 2021",https://arxiv.org/pdf/2104.02932
"I-ODA, Real-World Multi-modal Longitudinal Data for OphthalmicApplications",Nooshin Mojab;Vahid Noroozi;Abdullah Aleem;Manoj P. Nallabothula;Joseph Baker;Dimitri T. Azar;Mark Rosenblatt;RV Paul Chan;Darvin Yi;Philip S. Yu;Joelle A. Hallak,"Data from clinical real-world settings is characterized by variability in quality, machine-type, setting, and source. One of the primary goals of medical computer vision is to develop and validate artificial intelligence (AI) based algorithms on real-world data enabling clinical translations. However, despite the exponential growth in AI based applications in healthcare, specifically in ophthalmology, translations to clinical settings remain challenging. Limited access to adequate and diverse real-world data inhibits the development and validation of translatable algorithms. In this paper, we present a new multi-modal longitudinal ophthalmic imaging dataset, the Illinois Ophthalmic Database Atlas (I-ODA), with the goal of advancing state-of-the-art computer vision applications in ophthalmology, and improving upon the translatable capacity of AI based applications across different clinical settings. We present the infrastructure employed to collect, annotate, and anonymize images from multiple sources, demonstrating the complexity of real-world retrospective data and its limitations. I-ODA includes 12 imaging modalities with a total of 3,668,649 ophthalmic images of 33,876 individuals from the Department of Ophthalmology and Visual Sciences at the Illinois Eye and Ear Infirmary of the University of Illinois Chicago (UIC) over the course of 12 years. △ Less","29 March, 2021",https://arxiv.org/pdf/2104.02609
Future of work: ethics,David Pastor-Escuredo,"Work must be reshaped in the upcoming new era characterized by new challenges and the presence of new technologies and computational tools. Over-automation seems to be the driver of the digitalization process. Substitution is the paradigm leading Artificial Intelligence and robotics development against human cognition. Digital technology should be designed to enhance human skills and make more productive use of human cognition and capacities. Digital technology is characterized also by scalability because of its easy and inexpensive deployment. Thus, automation can lead to the absence of jobs and scalable negative impact in human development and the performance of business. A look at digitalization from the lens of Sustainable Development Goals can tell us how digitalization impact in different sectors and areas considering society as a complex interconnected system. Here, reflections on how AI and Data impact future of work and sustainable development are provided grounded on an ethical core that comprises human-level principles and also systemic principles. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.02580
End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning,Tal Feldman;Ashley Peake,"Machine Learning models have been deployed across many different aspects of society, often in situations that affect social welfare. Although these models offer streamlined solutions to large problems, they may contain biases and treat groups or individuals unfairly based on protected attributes such as gender. In this paper, we introduce several examples of machine learning gender bias in practice followed by formalizations of fairness. We provide a survey of fairness research by detailing influential pre-processing, in-processing, and post-processing bias mitigation algorithms. We then propose an end-to-end bias mitigation framework, which employs a fusion of pre-, in-, and post-processing methods to leverage the strengths of each individual technique. We test this method, along with the standard techniques we review, on a deep neural network to analyze bias mitigation in a deep learning setting. We find that our end-to-end bias mitigation framework outperforms the baselines with respect to several fairness metrics, suggesting its promise as a method for improving fairness. As society increasingly relies on artificial intelligence to help in decision-making, addressing gender biases present in deep learning models is imperative. To provide readers with the tools to assess the fairness of machine learning models and mitigate the biases present in them, we discuss multiple open source packages for fairness in AI. △ Less","20 June, 2021",https://arxiv.org/pdf/2104.02532
A Novel Approach for Semiconductor Etching Process with Inductive Biases,Sanghoon Myung;Hyunjae Jang;Byungseon Choi;Jisu Ryu;Hyuk Kim;Sang Wuk Park;Changwook Jeong;Dae Sin Kim,"The etching process is one of the most important processes in semiconductor manufacturing. We have introduced the state-of-the-art deep learning model to predict the etching profiles. However, the significant problems violating physics have been found through various techniques such as explainable artificial intelligence and representation of prediction uncertainty. To address this problem, this paper presents a novel approach to apply the inductive biases for etching process. We demonstrate that our approach fits the measurement faster than physical simulator while following the physical behavior. Our approach would bring a new opportunity for better etching process with higher accuracy and lower cost. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.02468
"The Duo of Artificial Intelligence and Big Data for Industry 4.0: Review of Applications, Techniques, Challenges, and Future Research Directions",Senthil Kumar Jagatheesaperumal;Mohamed Rahouti;Kashif Ahmad;Ala Al-Fuqaha;Mohsen Guizani,"The increasing need for economic, safe, and sustainable smart manufacturing combined with novel technological enablers, has paved the way for Artificial Intelligence (AI) and Big Data in support of smart manufacturing. This implies a substantial integration of AI, Industrial Internet of Things (IIoT), Robotics, Big data, Blockchain, 5G communications, in support of smart manufacturing and the dynamical processes in modern industries. In this paper, we provide a comprehensive overview of different aspects of AI and Big Data in Industry 4.0 with a particular focus on key applications, techniques, the concepts involved, key enabling technologies, challenges, and research perspective towards deployment of Industry 5.0. In detail, we highlight and analyze how the duo of AI and Big Data is helping in different applications of Industry 4.0. We also highlight key challenges in a successful deployment of AI and Big Data methods in smart industries with a particular emphasis on data-related issues, such as availability, bias, auditing, management, interpretability, communication, and different adversarial attacks and security issues. In a nutshell, we have explored the significance of AI and Big data towards Industry 4.0 applications through panoramic reviews and discussions. We believe, this work will provide a baseline for future research in the domain. △ Less","7 April, 2021",https://arxiv.org/pdf/2104.02425
Text-guided Legal Knowledge Graph Reasoning,Luoqiu Li;Zhen Bi;Hongbin Ye;Shumin Deng;Hui Chen;Huaixiao Tou,"Recent years have witnessed the prosperity of legal artificial intelligence with the development of technologies. In this paper, we propose a novel legal application of legal provision prediction (LPP), which aims to predict the related legal provisions of affairs. We formulate this task as a challenging knowledge graph completion problem, which requires not only text understanding but also graph reasoning. To this end, we propose a novel text-guided graph reasoning approach. We collect amounts of real-world legal provision data from the Guangdong government service website and construct a legal dataset called LegalLPP. Extensive experimental results on the dataset show that our approach achieves better performance compared with baselines. The code and dataset are available in \url{https://github.com/zxlzr/LegalPP} for reproducibility. △ Less","22 August, 2021",https://arxiv.org/pdf/2104.02284
"A clinical validation of VinDr-CXR, an AI system for detecting abnormal chest radiographs",Ngoc Huy Nguyen;Ha Quy Nguyen;Nghia Trung Nguyen;Thang Viet Nguyen;Hieu Huy Pham;Tuan Ngoc-Minh Nguyen,"Computer-Aided Diagnosis (CAD) systems for chest radiographs using artificial intelligence (AI) have recently shown a great potential as a second opinion for radiologists. The performances of such systems, however, were mostly evaluated on a fixed dataset in a retrospective manner and, thus, far from the real performances in clinical practice. In this work, we demonstrate a mechanism for validating an AI-based system for detecting abnormalities on X-ray scans, VinDr-CXR, at the Phu Tho General Hospital - a provincial hospital in the North of Vietnam. The AI system was directly integrated into the Picture Archiving and Communication System (PACS) of the hospital after being trained on a fixed annotated dataset from other sources. The performance of the system was prospectively measured by matching and comparing the AI results with the radiology reports of 6,285 chest X-ray examinations extracted from the Hospital Information System (HIS) over the last two months of 2020. The normal/abnormal status of a radiology report was determined by a set of rules and served as the ground truth. Our system achieves an F1 score - the harmonic average of the recall and the precision - of 0.653 (95% CI 0.635, 0.671) for detecting any abnormalities on chest X-rays. Despite a significant drop from the in-lab performance, this result establishes a high level of confidence in applying such a system in real-life situations. △ Less","6 April, 2021",https://arxiv.org/pdf/2104.02256
"In-Line Image Transformations for Imbalanced, Multiclass Computer Vision Classification of Lung Chest X-Rays",Alexandrea K. Ramnarine,"Artificial intelligence (AI) is disrupting the medical field as advances in modern technology allow common household computers to learn anatomical and pathological features that distinguish between healthy and disease with the accuracy of highly specialized, trained physicians. Computer vision AI applications use medical imaging, such as lung chest X-Rays (LCXRs), to facilitate diagnoses by providing second-opinions in addition to a physician's or radiologist's interpretation. Considering the advent of the current Coronavirus disease (COVID-19) pandemic, LCXRs may provide rapid insights to indirectly aid in infection containment, however generating a reliably labeled image dataset for a novel disease is not an easy feat, nor is it of highest priority when combating a global pandemic. Deep learning techniques such as convolutional neural networks (CNNs) are able to select features that distinguish between healthy and disease states for other lung pathologies; this study aims to leverage that body of literature in order to apply image transformations that would serve to balance the lack of COVID-19 LCXR data. Furthermore, this study utilizes a simple CNN architecture for high-performance multiclass LCXR classification at 94 percent accuracy. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.02238
Intelligent Building Control Systems for Thermal Comfort and Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted Techniques,Ghezlane Halhoul Merabet;Mohamed Essaaidi;Mohamed Ben Haddou;Basheer Qolomany;Junaid Qadir;Muhammad Anan;Ala Al-Fuqaha;Mohamed Riduan Abid;Driss Benhaddou,"Building operations represent a significant percentage of the total primary energy consumed in most countries due to the proliferation of Heating, Ventilation and Air-Conditioning (HVAC) installations in response to the growing demand for improved thermal comfort. Reducing the associated energy consumption while maintaining comfortable conditions in buildings are conflicting objectives and represent a typical optimization problem that requires intelligent system design. Over the last decade, different methodologies based on the Artificial Intelligence (AI) techniques have been deployed to find the sweet spot between energy use in HVAC systems and suitable indoor comfort levels to the occupants. This paper performs a comprehensive and an in-depth systematic review of AI-based techniques used for building control systems by assessing the outputs of these techniques, and their implementations in the reviewed works, as well as investigating their abilities to improve the energy-efficiency, while maintaining thermal comfort conditions. This enables a holistic view of (1) the complexities of delivering thermal comfort to users inside buildings in an energy-efficient way, and (2) the associated bibliographic material to assist researchers and experts in the field in tackling such a challenge. Among the 20 AI tools developed for both energy consumption and comfort control, functions such as identification and recognition patterns, optimization, predictive control. Based on the findings of this work, the application of AI technology in building control is a promising area of research and still an ongoing, i.e., the performance of AI-based control is not yet completely satisfactory. This is mainly due in part to the fact that these algorithms usually need a large amount of high-quality real-world data, which is lacking in the building or, more precisely, the energy sector. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.02214
An Artificial Intelligence Framework for Bidding Optimization with Uncertainty in Multiple Frequency Reserve Markets,Thimal Kempitiya;Seppo Sierla;Daswin De Silva;Matti Yli-Ojanpera;Damminda Alahakoon;Valeriy Vyatkin,"The global ambitions of a carbon-neutral society necessitate a stable and robust smart grid that capitalises on frequency reserves of renewable energy. Frequency reserves are resources that adjust power production or consumption in real time to react to a power grid frequency deviation. Revenue generation motivates the availability of these resources for managing such deviations. However, limited research has been conducted on data-driven decisions and optimal bidding strategies for trading such capacities in multiple frequency reserves markets. We address this limitation by making the following research contributions. Firstly, a generalised model is designed based on an extensive study of critical characteristics of global frequency reserves markets. Secondly, three bidding strategies are proposed, based on this market model, to capitalise on price peaks in multi-stage markets. Two strategies are proposed for non-reschedulable loads, in which case the bidding strategy aims to select the market with the highest anticipated price, and the third bidding strategy focuses on rescheduling loads to hours on which highest reserve market prices are anticipated. The third research contribution is an Artificial Intelligence (AI) based bidding optimization framework that implements these three strategies, with novel uncertainty metrics that supplement data-driven price prediction. Finally, the framework is evaluated empirically using a case study of multiple frequency reserves markets in Finland. The results from this evaluation confirm the effectiveness of the proposed bidding strategies and the AI-based bidding optimization framework in terms of cumulative revenue generation, leading to an increased availability of frequency reserves. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.01865
Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and Defenses,Yao Deng;Tiehua Zhang;Guannan Lou;Xi Zheng;Jiong Jin;Qing-Long Han,"The rapid development of artificial intelligence, especially deep learning technology, has advanced autonomous driving systems (ADSs) by providing precise control decisions to counterpart almost any driving event, spanning from anti-fatigue safe driving to intelligent route planning. However, ADSs are still plagued by increasing threats from different attacks, which could be categorized into physical attacks, cyberattacks and learning-based adversarial attacks. Inevitably, the safety and security of deep learning-based autonomous driving are severely challenged by these attacks, from which the countermeasures should be analyzed and studied comprehensively to mitigate all potential risks. This survey provides a thorough analysis of different attacks that may jeopardize ADSs, as well as the corresponding state-of-the-art defense mechanisms. The analysis is unrolled by taking an in-depth overview of each step in the ADS workflow, covering adversarial attacks for various deep learning models and attacks in both physical and cyber context. Furthermore, some promising research directions are suggested in order to improve deep learning-based autonomous driving safety, including model robustness training, model testing and verification, and anomaly detection based on cloud/edge servers. △ Less","9 April, 2021",https://arxiv.org/pdf/2104.01789
Federated Learning Meets Blockchain in Edge Computing: Opportunities and Challenges,Dinh C. Nguyen;Ming Ding;Quoc-Viet Pham;Pubudu N. Pathirana;Long Bao Le;Aruna Seneviratne;Jun Li;Dusit Niyato;H. Vincent Poor,"Mobile edge computing (MEC) has been envisioned as a promising paradigm to handle the massive volume of data generated from ubiquitous mobile devices for enabling intelligent services with the help of artificial intelligence (AI). Traditionally, AI techniques often require centralized data collection and training in a single entity, e.g., an MEC server, which is now becoming a weak point due to data privacy concerns and high data communication overheads. In this context, federated learning (FL) has been proposed to provide collaborative data training solutions, by coordinating multiple mobile devices to train a shared AI model without exposing their data, which enjoys considerable privacy enhancement. To improve the security and scalability of FL implementation, blockchain as a ledger technology is attractive for realizing decentralized FL training without the need for any central server. Particularly, the integration of FL and blockchain leads to a new paradigm, called FLchain, which potentially transforms intelligent MEC networks into decentralized, secure, and privacy-enhancing systems. This article presents an overview of the fundamental concepts and explores the opportunities of FLchain in MEC networks. We identify several main topics in FLchain design, including communication cost, resource allocation, incentive mechanism, security and privacy protection. The key solutions for FLchain design are provided, and the lessons learned as well as the outlooks are also discussed. Then, we investigate the applications of FLchain in popular MEC domains, such as edge data sharing, edge content caching and edge crowdsensing. Finally, important research challenges and future directions are also highlighted. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.01776
A Dynamics Perspective of Pursuit-Evasion Games of Intelligent Agents with the Ability to Learn,Hao Xiong;Huanhui Cao;Lin Zhang;Wenjie Lu,"Pursuit-evasion games are ubiquitous in nature and in an artificial world. In nature, pursuer(s) and evader(s) are intelligent agents that can learn from experience, and dynamics (i.e., Newtonian or Lagrangian) is vital for the pursuer and the evader in some scenarios. To this end, this paper addresses the pursuit-evasion game of intelligent agents from the perspective of dynamics. A bio-inspired dynamics formulation of a pursuit-evasion game and baseline pursuit and evasion strategies are introduced at first. Then, reinforcement learning techniques are used to mimic the ability of intelligent agents to learn from experience. Based on the dynamics formulation and reinforcement learning techniques, the effects of improving both pursuit and evasion strategies based on experience on pursuit-evasion games are investigated at two levels 1) individual runs and 2) ranges of the parameters of pursuit-evasion games. Results of the investigation are consistent with nature observations and the natural law - survival of the fittest. More importantly, with respect to the result of a pursuit-evasion game of agents with baseline strategies, this study achieves a different result. It is shown that, in a pursuit-evasion game with a dynamics formulation, an evader is not able to escape from a slightly faster pursuer with an effective learned pursuit strategy, based on agile maneuvers and an effective learned evasion strategy. △ Less","3 April, 2021",https://arxiv.org/pdf/2104.01445
Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing,Ioannis Kakogeorgiou;Konstantinos Karantzalos,"Although deep neural networks hold the state-of-the-art in several remote sensing tasks, their black-box operation hinders the understanding of their decisions, concealing any bias and other shortcomings in datasets and model performance. To this end, we have applied explainable artificial intelligence (XAI) methods in remote sensing multi-label classification tasks towards producing human-interpretable explanations and improve transparency. In particular, we utilized and trained deep learning models with state-of-the-art performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods were employed towards understanding and interpreting models' predictions, along with quantitative metrics to assess and compare their performance. Numerous experiments were performed to assess the overall performance of XAI methods for straightforward prediction cases, competing multiple labels, as well as misclassification cases. According to our findings, Occlusion, Grad-CAM and Lime were the most interpretable and reliable XAI methods. However, none delivers high-resolution outputs, while apart from Grad-CAM, both Lime and Occlusion are computationally expensive. We also highlight different aspects of XAI performance and elaborate with insights on black-box decisions in order to improve transparency, understand their behavior and reveal, as well, datasets' particularities. △ Less","20 September, 2021",https://arxiv.org/pdf/2104.01375
"A Review of AI-enabled Routing Protocols for UAV Networks: Trends, Challenges, and Future Outlook",Arnau Rovira-Sugranes;Abolfazl Razi;Fatemeh Afghah;Jacob Chakareski,"Unmanned Aerial Vehicles (UAVs), as a recently emerging technology, enabled a new breed of unprecedented applications in different domains. This technology's ongoing trend is departing from large remotely-controlled drones to networks of small autonomous drones to collectively complete intricate tasks time and cost-effectively. An important challenge is developing efficient sensing, communication, and control algorithms that can accommodate the requirements of highly dynamic UAV networks with heterogeneous mobility levels. Recently, the use of Artificial Intelligence (AI) in learning-based networking has gained momentum to harness the learning power of cognizant nodes to make more intelligent networking decisions by integrating computational intelligence into UAV networks. An important example of this trend is developing learning-powered routing protocols, where machine learning methods are used to model and predict topology evolution, channel status, traffic mobility, and environmental factors for enhanced routing. This paper reviews AI-enabled routing protocols designed primarily for aerial networks, including topology-predictive and self-adaptive learning-based routing algorithms, with an emphasis on accommodating highly-dynamic network topology. To this end, we justify the importance and adaptation of AI into UAV network communications. We also address, with an AI emphasis, the closely related topics of mobility and networking models for UAV networks, simulation tools and public datasets, and relations to UAV swarming, which serve to choose the right algorithm for each scenario. We conclude by presenting future trends, and the remaining challenges in AI-based UAV networking, for different aspects of routing, connectivity, topology control, security and privacy, energy efficiency, and spectrum sharing. △ Less","8 November, 2021",https://arxiv.org/pdf/2104.01283
Designing for human-AI complementarity in K-12 education,Kenneth Holstein;Vincent Aleven,"Recent work has explored how complementary strengths of humans and artificial intelligence (AI) systems might be productively combined. However, successful forms of human-AI partnership have rarely been demonstrated in real-world settings. We present the iterative design and evaluation of Lumilo, smart glasses that help teachers help their students in AI-supported classrooms by presenting real-time analytics about students' learning, metacognition, and behavior. Results from a field study conducted in K-12 classrooms indicate that students learn more when teachers and AI tutors work together during class. We discuss implications of this research for the design of human-AI partnerships. We argue for more participatory approaches to research and design in this area, in which practitioners and other stakeholders are deeply, meaningfully involved throughout the process. Furthermore, we advocate for theory-building and for principled approaches to the study of human-AI decision-making in real-world contexts. △ Less","10 July, 2021",https://arxiv.org/pdf/2104.01266
STARdom: an architecture for trusted and secure human-centered manufacturing systems,Jože M. Rožanec;Patrik Zajec;Klemen Kenda;Inna Novalija;Blaž Fortuna;Dunja Mladenić;Entso Veliou;Dimitrios Papamartzivanos;Thanassis Giannetsos;Sofia Anna Menesidou;Rubén Alonso;Nino Cauli;Diego Reforgiato Recupero;Dimosthenis Kyriazis;Georgios Sofianidis;Spyros Theodoropoulos;John Soldatos,"There is a lack of a single architecture specification that addresses the needs of trusted and secure Artificial Intelligence systems with humans in the loop, such as human-centered manufacturing systems at the core of the evolution towards Industry 5.0. To realize this, we propose an architecture that integrates forecasts, Explainable Artificial Intelligence, supports collecting users' feedback, and uses Active Learning and Simulated Reality to enhance forecasts and provide decision-making recommendations. The architecture security is addressed as a general concern. We align the proposed architecture with the Big Data Value Association Reference Architecture Model. We tailor it for the domain of demand forecasting and validate it on a real-world case study. △ Less","2 April, 2021",https://arxiv.org/pdf/2104.00983
Low Dose Helical CBCT denoising by using domain filtering with deep reinforcement learning,Wooram Kang;Mayank Patwari,"Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT imaging. Especially, The Low Dose CT imaging is one of possible options to protect organs of patients when conducting CT imaging. Therefore Low Dose CT imaging can be an alternative instead of Standard dose CT imaging. However Low Dose CT imaging has a fundamental issue with noises within results compared to Standard Dose CT imaging. Currently, there are lots of attempts to erase the noises. Most of methods with artificial intelligence have many parameters and unexplained layers or a kind of black-box methods. Therefore, our research has purposes related to these issues. Our approach has less parameters than usual methods by having Iterative learn-able bilateral filtering approach with Deep reinforcement learning. And we applied The Iterative learn-able filtering approach with deep reinforcement learning to sinograms and reconstructed volume domains. The method and the results of the method can be much more explainable than The other black box AI approaches. And we applied the method to Helical Cone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested this method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA dataset. The results and the performances of our approach overtake the results of the other previous methods. △ Less","2 April, 2021",https://arxiv.org/pdf/2104.00889
A Comparative Analysis of Machine Learning and Grey Models,Gang He;Khwaja Mutahir Ahmad;Wenxin Yu;Xiaochuan Xu;Jay Kumar,"Artificial Intelligence (AI) has recently shown its capabilities for almost every field of life. Machine Learning, which is a subset of AI, is a `HOT' topic for researchers. Machine Learning outperforms other classical forecasting techniques in almost all-natural applications. It is a crucial part of modern research. As per this statement, Modern Machine Learning algorithms are hungry for big data. Due to the small datasets, the researchers may not prefer to use Machine Learning algorithms. To tackle this issue, the main purpose of this survey is to illustrate, demonstrate related studies for significance of a semi-parametric Machine Learning framework called Grey Machine Learning (GML). This kind of framework is capable of handling large datasets as well as small datasets for time series forecasting likely outcomes. This survey presents a comprehensive overview of the existing semi-parametric machine learning techniques for time series forecasting. In this paper, a primer survey on the GML framework is provided for researchers. To allow an in-depth understanding for the readers, a brief description of Machine Learning, as well as various forms of conventional grey forecasting models are discussed. Moreover, a brief description on the importance of GML framework is presented. △ Less","4 December, 2021",https://arxiv.org/pdf/2104.00871
Daisen: A Framework for Visualizing Detailed GPU Execution,Yifan Sun;Yixuan Zhang;Ali Mosallaei;Michael D. Shah;Cody Dunne;David Kaeli,"Graphics Processing Units (GPUs) have been widely used to accelerate artificial intelligence, physics simulation, medical imaging, and information visualization applications. To improve GPU performance, GPU hardware designers need to identify performance issues by inspecting a huge amount of simulator-generated traces. Visualizing the execution traces can reduce the cognitive burden of users and facilitate making sense of behaviors of GPU hardware components. In this paper, we first formalize the process of GPU performance analysis and characterize the design requirements of visualizing execution traces based on a survey study and interviews with GPU hardware designers. We contribute data and task abstraction for GPU performance analysis. Based on our task analysis, we propose Daisen, a framework that supports data collection from GPU simulators and provides visualization of the simulator-generated GPU execution traces. Daisen features a data abstraction and trace format that can record simulator-generated GPU execution traces. Daisen also includes a web-based visualization tool that helps GPU hardware designers examine GPU execution traces, identify performance bottlenecks, and verify performance improvement. Our qualitative evaluation with GPU hardware designers demonstrates that the design of Daisen reflects the typical workflow of GPU hardware designers. Using Daisen, participants were able to effectively identify potential performance bottlenecks and opportunities for performance improvement. The open-sourced implementation of Daisen can be found at gitlab.com/akita/vis. Supplemental materials including a demo video, survey questions, evaluation study guide, and post-study evaluation survey are available at osf.io/j5ghq. △ Less","1 April, 2021",https://arxiv.org/pdf/2104.00828
Deep Reinforcement Learning for Constrained Field Development Optimization in Subsurface Two-phase Flow,Yusuf Nasir;Jincong He;Chaoshun Hu;Shusei Tanaka;Kainan Wang;XianHuan Wen,"We present a deep reinforcement learning-based artificial intelligence agent that could provide optimized development plans given a basic description of the reservoir and rock/fluid properties with minimal computational cost. This artificial intelligence agent, comprising of a convolutional neural network, provides a mapping from a given state of the reservoir model, constraints, and economic condition to the optimal decision (drill/do not drill and well location) to be taken in the next stage of the defined sequential field development planning process. The state of the reservoir model is defined using parameters that appear in the governing equations of the two-phase flow. A feedback loop training process referred to as deep reinforcement learning is used to train an artificial intelligence agent with such a capability. The training entails millions of flow simulations with varying reservoir model descriptions (structural, rock and fluid properties), operational constraints, and economic conditions. The parameters that define the reservoir model, operational constraints, and economic conditions are randomly sampled from a defined range of applicability. Several algorithmic treatments are introduced to enhance the training of the artificial intelligence agent. After appropriate training, the artificial intelligence agent provides an optimized field development plan instantly for new scenarios within the defined range of applicability. This approach has advantages over traditional optimization algorithms (e.g., particle swarm optimization, genetic algorithm) that are generally used to find a solution for a specific field development scenario and typically not generalizable to different scenarios. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00527
Learning Rates for Multi-task Regularization Networks,Jie Gui;Haizhang Zhang,"Multi-task learning is an important trend of machine learning in facing the era of artificial intelligence and big data. Despite a large amount of researches on learning rate estimates of various single-task machine learning algorithms, there is little parallel work for multi-task learning. We present mathematical analysis on the learning rate estimate of multi-task learning based on the theory of vector-valued reproducing kernel Hilbert spaces and matrix-valued reproducing kernels. For the typical multi-task regularization networks, an explicit learning rate dependent both on the number of sample data and the number of tasks is obtained. It reveals that the generalization ability of multi-task learning algorithms is indeed affected as the number of tasks increases. △ Less","28 September, 2021",https://arxiv.org/pdf/2104.00453
WakaVT: A Sequential Variational Transformer for Waka Generation,Yuka Takeishi;Mingxuan Niu;Jing Luo;Zhong Jin;Xinyu Yang,"Poetry generation has long been a challenge for artificial intelligence. In the scope of Japanese poetry generation, many researchers have paid attention to Haiku generation, but few have focused on Waka generation. To further explore the creative potential of natural language generation systems in Japanese poetry creation, we propose a novel Waka generation model, WakaVT, which automatically produces Waka poems given user-specified keywords. Firstly, an additive mask-based approach is presented to satisfy the form constraint. Secondly, the structures of Transformer and variational autoencoder are integrated to enhance the quality of generated content. Specifically, to obtain novelty and diversity, WakaVT employs a sequence of latent variables, which effectively captures word-level variability in Waka data. To improve linguistic quality in terms of fluency, coherence, and meaningfulness, we further propose the fused multilevel self-attention mechanism, which properly models the hierarchical linguistic structure of Waka. To the best of our knowledge, we are the first to investigate Waka generation with models based on Transformer and/or variational autoencoder. Both objective and subjective evaluation results demonstrate that our model outperforms baselines significantly. △ Less","1 April, 2021",https://arxiv.org/pdf/2104.00426
Taking Stock of the Present and Future of Smart Technologies for Older Adults and Caregivers,Christina N. Harrington;Ben Jelen;Amanda Lazar;Aqueasha Martin-Hammond;Alisha Pradhan;Blaine Reeder;Katie Siek,"Technology has the opportunity to assist older adults as they age in place, coordinate caregiving resources, and meet unmet needs through access to resources. Currently, older adults use consumer technologies to support everyday life, however these technologies are not always accessible or as useful as they can be. Indeed, industry has attempted to create smart home technologies with older adults as a target user group, however these solutions are often more focused on the technical aspects and are short lived. In this paper, we advocate for older adults being involved in the design process - from initial ideation to product development to deployment. We encourage federally funded researchers and industry to create compensated, diverse older adult advisory boards to address stereotypes about aging while ensuring their needs are considered. We envision artificial intelligence systems that augment resources instead of replacing them - especially in under-resourced communities. Older adults rely on their caregiver networks and community organizations for social, emotional, and physical support; thus, AI should be used to coordinate resources better and lower the burden of connecting with these resources. Although sociotechnical smart systems can help identify needs of older adults, the lack of affordable research infrastructure and translation of findings into consumer technology perpetuates inequities in designing for diverse older adults. In addition, there is a disconnect between the creation of smart sensing systems and creating understandable, actionable data for older adults and caregivers to utilize. We ultimately advocate for a well-coordinated research effort across the United States that connects older adults, caregivers, community organizations, and researchers together to catalyze innovative and practical research for all stakeholders. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00096
"Imagine All the People: Citizen Science, Artificial Intelligence, and Computational Research",Lea A. Shanley;Lucy Fortson;Tanya Berger-Wolf;Kevin Crowston;Pietro Michelucci,"Machine learning, artificial intelligence, and deep learning have advanced significantly over the past decade. Nonetheless, humans possess unique abilities such as creativity, intuition, context and abstraction, analytic problem solving, and detecting unusual events. To successfully tackle pressing scientific and societal challenges, we need the complementary capabilities of both humans and machines. The Federal Government could accelerate its priorities on multiple fronts through judicious integration of citizen science and crowdsourcing with artificial intelligence (AI), Internet of Things (IoT), and cloud strategies. △ Less","5 April, 2021",https://arxiv.org/pdf/2104.00093
Ultra-Reliable Indoor Millimeter Wave Communications using Multiple Artificial Intelligence-Powered Intelligent Surfaces,Mehdi Naderi Soorki;Walid Saad;Mehdi Bennis;Choong Seon Hong,"In this paper, a novel framework for guaranteeing ultra-reliable millimeter wave (mmW) communications using multiple artificial intelligence (AI)-enabled reconfigurable intelligent surfaces (RISs) is proposed. The use of multiple AI-powered RISs allows changing the propagation direction of the signals transmitted from a mmW access point (AP) thereby improving coverage particularly for non-line-of-sight (NLoS) areas. However, due to the possibility of highly stochastic blockage over mmW links, designing an intelligent controller to jointly optimize the mmW AP beam and RIS phase shifts is a daunting task. In this regard, first, a parametric risk-sensitive episodic return is proposed to maximize the expected bit rate and mitigate the risk of mmW link blockage. Then, a closed-form approximation of the policy gradient of the risk-sensitive episodic return is analytically derived. Next, the problem of joint beamforming for mmW AP and phase shift control for mmW RISs is modeled as an identical payoff stochastic game within a cooperative multi-agent environment, in which the agents are the mmW AP and the RISs. Two centralized and distributed controllers are proposed to control the policies of the mmW AP and RISs. To directly find an optimal solution, the parametric functional-form policies for these controllers are modeled using deep recurrent neural networks (RNNs). Simulation results show that the error between policies of the optimal and the RNN-based controllers is less than 1.5%. Moreover, the variance of the achievable rates resulting from the deep RNN-based controllers is 60% less than the variance of the risk-averse baseline. △ Less","19 August, 2021",https://arxiv.org/pdf/2104.00075
Why is AI hard and Physics simple?,Daniel A. Roberts,"We discuss why AI is hard and why physics is simple. We discuss how physical intuition and the approach of theoretical physics can be brought to bear on the field of artificial intelligence and specifically machine learning. We suggest that the underlying project of machine learning and the underlying project of physics are strongly coupled through the principle of sparsity, and we call upon theoretical physicists to work on AI as physicists. As a first step in that direction, we discuss an upcoming book on the principles of deep learning theory that attempts to realize this approach. △ Less","31 March, 2021",https://arxiv.org/pdf/2104.00008
Classification of Hematoma: Joint Learning of Semantic Segmentation and Classification,Hokuto Hirano;Tsuyoshi Okita,"Cerebral hematoma grows rapidly in 6-24 hours and misprediction of the growth can be fatal if it is not operated by a brain surgeon. There are two types of cerebral hematomas: one that grows rapidly and the other that does not grow rapidly. We are developing the technique of artificial intelligence to determine whether the CT image includes the cerebral hematoma which leads to the rapid growth. This problem has various difficulties: the few positive cases in this classification problem of cerebral hematoma and the targeted hematoma has deformable object. Other difficulties include the imbalance classification, the covariate shift, the small data, and the spurious correlation problems. It is difficult with the plain CNN classification such as VGG. This paper proposes the joint learning of semantic segmentation and classification and evaluate the performance of this. △ Less","31 March, 2021",https://arxiv.org/pdf/2103.17172
Trusted Artificial Intelligence: Towards Certification of Machine Learning Applications,Philip Matthias Winter;Sebastian Eder;Johannes Weissenböck;Christoph Schwald;Thomas Doms;Tom Vogt;Sepp Hochreiter;Bernhard Nessler,"Artificial Intelligence is one of the fastest growing technologies of the 21st century and accompanies us in our daily lives when interacting with technical applications. However, reliance on such technical systems is crucial for their widespread applicability and acceptance. The societal tools to express reliance are usually formalized by lawful regulations, i.e., standards, norms, accreditations, and certificates. Therefore, the TÜV AUSTRIA Group in cooperation with the Institute for Machine Learning at the Johannes Kepler University Linz, proposes a certification process and an audit catalog for Machine Learning applications. We are convinced that our approach can serve as the foundation for the certification of applications that use Machine Learning and Deep Learning, the techniques that drive the current revolution in Artificial Intelligence. While certain high-risk areas, such as fully autonomous robots in workspaces shared with humans, are still some time away from certification, we aim to cover low-risk applications with our certification procedure. Our holistic approach attempts to analyze Machine Learning applications from multiple perspectives to evaluate and verify the aspects of secure software development, functional requirements, data quality, data protection, and ethics. Inspired by existing work, we introduce four criticality levels to map the criticality of a Machine Learning application regarding the impact of its decisions on people, environment, and organizations. Currently, the audit catalog can be applied to low-risk applications within the scope of supervised learning as commonly encountered in industry. Guided by field experience, scientific developments, and market demands, the audit catalog will be extended and modified accordingly. △ Less","31 March, 2021",https://arxiv.org/pdf/2103.16910
Collaborative construction of lexicographic and parallel datasets for African languages: first assessment,Elvis Mboning Tchiaze,"Faced with a considerable lack of resources in African languages to carry out work in Natural Language Processing (NLP), Natural Language Understanding (NLU) and artificial intelligence, the research teams of NTeALan association has set itself the objective of building open-source platforms for the collaborative construction of lexicographic data in African languages. In this article, we present our first reports after 2 years of collaborative construction of lexicographic resources useful for African NLP tools. △ Less","30 March, 2021",https://arxiv.org/pdf/2103.16712
Enabling Design Methodologies and Future Trends for Edge AI: Specialization and Co-design,Cong Hao;Jordan Dotzel;Jinjun Xiong;Luca Benini;Zhiru Zhang;Deming Chen,"Artificial intelligence (AI) technologies have dramatically advanced in recent years, resulting in revolutionary changes in people's lives. Empowered by edge computing, AI workloads are migrating from centralized cloud architectures to distributed edge systems, introducing a new paradigm called edge AI. While edge AI has the promise of bringing significant increases in autonomy and intelligence into everyday lives through common edge devices, it also raises new challenges, especially for the development of its algorithms and the deployment of its services, which call for novel design methodologies catered to these unique challenges. In this paper, we provide a comprehensive survey of the latest enabling design methodologies that span the entire edge AI development stack. We suggest that the key methodologies for effective edge AI development are single-layer specialization and cross-layer co-design. We discuss representative methodologies in each category in detail, including on-device training methods, specialized software design, dedicated hardware design, benchmarking and design automation, software/hardware co-design, software/compiler co-design, and compiler/hardware co-design. Moreover, we attempt to reveal hidden cross-layer design opportunities that can further boost the solution quality of future edge AI and provide insights into future directions and emerging areas that require increased research focus. △ Less","30 March, 2021",https://arxiv.org/pdf/2103.15750
Towards An Ethics-Audit Bot,Siani Pearson;Martin Lloyd;Vivek Nallur,"In this paper we focus on artificial intelligence (AI) for governance, not governance for AI, and on just one aspect of governance, namely ethics audit. Different kinds of ethical audit bots are possible, but who makes the choices and what are the implications? In this paper, we do not provide ethical/philosophical solutions, but rather focus on the technical aspects of what an AI-based solution for validating the ethical soundness of a target system would be like. We propose a system that is able to conduct an ethical audit of a target system, given certain socio-technical conditions. To be more specific, we propose the creation of a bot that is able to support organisations in ensuring that their software development lifecycles contain processes that meet certain ethical standards. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.15746
Automation: An Essential Component Of Ethical AI?,Vivek Nallur;Martin Lloyd;Siani Pearson,"Ethics is sometimes considered to be too abstract to be meaningfully implemented in artificial intelligence (AI). In this paper, we reflect on other aspects of computing that were previously considered to be very abstract. Yet, these are now accepted as being done very well by computers. These tasks have ranged from multiple aspects of software engineering to mathematics to conversation in natural language with humans. This was done by automating the simplest possible step and then building on it to perform more complex tasks. We wonder if ethical AI might be similarly achieved and advocate the process of automation as key step in making AI take ethical decisions. The key contribution of this paper is to reflect on how automation was introduced into domains previously considered too abstract for computers. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.15739
Self-Constructing Neural Networks Through Random Mutation,Samuel Schmidgall,"The search for neural architecture is producing many of the most exciting results in artificial intelligence. It has increasingly become apparent that task-specific neural architecture plays a crucial role for effectively solving problems. This paper presents a simple method for learning neural architecture through random mutation. This method demonstrates 1) neural architecture may be learned during the agent's lifetime, 2) neural architecture may be constructed over a single lifetime without any initial connections or neurons, and 3) architectural modifications enable rapid adaptation to dynamic and novel task scenarios. Starting without any neurons or connections, this method constructs a neural architecture capable of high-performance on several tasks. The lifelong learning capabilities of this method are demonstrated in an environment without episodic resets, even learning with constantly changing morphology, limb disablement, and changing task goals all without losing locomotion capabilities. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.15692
A Model-Based Approach to Synthetic Data Set Generation for Patient-Ventilator Waveforms for Machine Learning and Educational Use,A. van Diepen;T. H. G. F. Bakkes;A. J. R. De Bie;S. Turco;R. A. Bouwman;P. H. Woerlee;M. Mischi,"Although mechanical ventilation is a lifesaving intervention in the ICU, it has harmful side-effects, such as barotrauma and volutrauma. These harms can occur due to asynchronies. Asynchronies are defined as a mismatch between the ventilator timing and patient respiratory effort. Automatic detection of these asynchronies, and subsequent feedback, would improve lung ventilation and reduce the probability of lung damage. Neural networks to detect asynchronies provide a promising new approach but require large annotated data sets, which are difficult to obtain and require complex monitoring of inspiratory effort. In this work, we propose a model-based approach to generate a synthetic data set for machine learning and educational use by extending an existing lung model with a first-order ventilator model. The physiological nature of the derived lung model allows adaptation to various disease archetypes, resulting in a diverse data set. We generated a synthetic data set using 9 different patient archetypes, which are derived from measurements in the literature. The model and synthetic data quality have been verified by comparison with clinical data, review by a clinical expert, and an artificial intelligence model that was trained on experimental data. The evaluation showed it was possible to generate patient-ventilator waveforms including asynchronies that have the most important features of experimental patient-ventilator waveforms. △ Less","7 May, 2021",https://arxiv.org/pdf/2103.15684
A Survey of Hybrid Human-Artificial Intelligence for Social Computing,Wenxi Wang;Huansheng Ning;Feifei Shi;Sahraoui Dhelim;Weishan Zhang;Liming Chen,"Along with the development of modern computing technology and social sciences, both theoretical research and practical applications of social computing have been continuously extended. In particular with the boom of artificial intelligence (AI), social computing is significantly influenced by AI. However, the conventional technologies of AI have drawbacks in dealing with more complicated and dynamic problems. Such deficiency can be rectified by hybrid human-artificial intelligence (H-AI) which integrates both human intelligence and AI into one unity, forming a new enhanced intelligence. H-AI in dealing with social problems shows the advantages that AI can not surpass. This paper firstly introduces the concept of H-AI. AI is the intelligence in the transition stage of H-AI, so the latest research progresses of AI in social computing are reviewed. Secondly, it summarizes typical challenges faced by AI in social computing, and makes it possible to introduce H-AI to solve these challenges. Finally, the paper proposes a holistic framework of social computing combining with H-AI, which consists of four layers: object layer, base layer, analysis layer, and application layer. It represents H-AI has significant advantages over AI in solving social problems. △ Less","17 March, 2021",https://arxiv.org/pdf/2103.15558
"Connectionism, Complexity, and Living Systems: a comparison of Artificial and Biological Neural Networks",Krishna Katyal;Jesse Parent;Bradly Alicea,"While Artificial Neural Networks (ANNs) have yielded impressive results in the realm of simulated intelligent behavior, it is important to remember that they are but sparse approximations of Biological Neural Networks (BNNs). We go beyond comparison of ANNs and BNNs to introduce principles from BNNs that might guide the further development of ANNs as embodied neural models. These principles include representational complexity, complex network structure/energetics, and robust function. We then consider these principles in ways that might be implemented in the future development of ANNs. In conclusion, we consider the utility of this comparison, particularly in terms of building more robust and dynamic ANNs. This even includes constructing a morphology and sensory apparatus to create an embodied ANN, which when complemented with the organizational and functional advantages of BNNs unlocks the adaptive potential of lifelike networks. △ Less","14 March, 2021",https://arxiv.org/pdf/2103.15553
Situated Case Studies for a Human-Centered Design of Explanation User Interfaces,Claudia Müller-Birn;Katrin Glinka;Peter Sörries;Michael Tebbe;Susanne Michl,"Researchers and practitioners increasingly consider a human-centered perspective in the design of machine learning-based applications, especially in the context of Explainable Artificial Intelligence (XAI). However, clear methodological guidance in this context is still missing because each new situation seems to require a new setup, which also creates different methodological challenges. Existing case study collections in XAI inspired us; therefore, we propose a similar collection of case studies for human-centered XAI that can provide methodological guidance or inspiration for others. We want to showcase our idea in this workshop by describing three case studies from our research. These case studies are selected to highlight how apparently small differences require a different set of methods and considerations. With this workshop contribution, we would like to engage in a discussion on how such a collection of case studies can provide a methodological guidance and critical reflection. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.15462
"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest Value for us?",Bin Liu,"AI has surpassed humans across a variety of tasks such as image classification, playing games (e.g., go, ""Starcraft"" and poker), and protein structure prediction. However, at the same time, AI is also bearing serious controversies. Many researchers argue that little substantial progress has been made for AI in recent decades. In this paper, the author (1) explains why controversies about AI exist; (2) discriminates two paradigms of AI research, termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3) clarifies how to judge which paradigm a research work should be classified into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance to develop into ""strong AI"". △ Less","28 March, 2021",https://arxiv.org/pdf/2103.15294
Holographic photonic neuron,Vincent R. Daria,"The promise of artificial intelligence (AI) to process complex datasets has brought about innovative computing paradigms. While recent developments in quantum-photonic computing have reached significant feats, mimicking our brain's ability to recognize images are poorly integrated in these ventures. Here, I incorporate orbital angular momentum (OAM) states in a classical Vander Lugt optical correlator to create the holographic photonic neuron. The photonic neuron can memorize an array of matched filters in a phase-hologram, which is derived by linking OAM states with elements in the array. Successful correlation is independent of intensity and yields photons with OAM states, which can be used as a transmission protocol or qudits for quantum computing. The unique OAM identifier establishes the photonic neuron as a fundamental AI device for pattern recognition that can be scaled and integrated with other computing platforms to build-up a neuromorphic quantum-photonic processor that mimics the brain. △ Less","28 March, 2021",https://arxiv.org/pdf/2103.15272
The General Theory of General Intelligence: A Pragmatic Patternist Perspective,Ben Goertzel,"A multi-decade exploration into the theoretical foundations of artificial and natural general intelligence, which has been expressed in a series of books and papers and used to guide a series of practical and research-prototype software systems, is reviewed at a moderate level of detail. The review covers underlying philosophies (patternist philosophy of mind, foundational phenomenological and logical ontology), formalizations of the concept of intelligence, and a proposed high level architecture for AGI systems partly driven by these formalizations and philosophies. The implementation of specific cognitive processes such as logical reasoning, program learning, clustering and attention allocation in the context and language of this high level architecture is considered, as is the importance of a common (e.g. typed metagraph based) knowledge representation for enabling ""cognitive synergy"" between the various processes. The specifics of human-like cognitive architecture are presented as manifestations of these general principles, and key aspects of machine consciousness and machine ethics are also treated in this context. Lessons for practical implementation of advanced AGI in frameworks such as OpenCog Hyperon are briefly considered. △ Less","4 April, 2021",https://arxiv.org/pdf/2103.15100
Playing Against the Board: Rolling Horizon Evolutionary Algorithms Against Pandemic,Konstantinos Sfikas;Antonios Liapis,"Competitive board games have provided a rich and diverse testbed for artificial intelligence. This paper contends that collaborative board games pose a different challenge to artificial intelligence as it must balance short-term risk mitigation with long-term winning strategies. Collaborative board games task all players to coordinate their different powers or pool their resources to overcome an escalating challenge posed by the board and a stochastic ruleset. This paper focuses on the exemplary collaborative board game Pandemic and presents a rolling horizon evolutionary algorithm designed specifically for this game. The complex way in which the Pandemic game state changes in a stochastic but predictable way required a number of specially designed forward models, macro-action representations for decision-making, and repair functions for the genetic operations of the evolutionary algorithm. Variants of the algorithm which explore optimistic versus pessimistic game state evaluations, different mutation rates and event horizons are compared against a baseline hierarchical policy agent. Results show that an evolutionary approach via short-horizon rollouts can better account for the future dangers that the board may introduce, and guard against them. Results highlight the types of challenges that collaborative board games pose to artificial intelligence, especially for handling multi-player collaboration interactions. △ Less","28 March, 2021",https://arxiv.org/pdf/2103.15090
IUP: An Intelligent Utility Prediction Scheme for Solid-State Fermentation in 5G IoT,Min Wang;Shanchen Pang;Tong Ding;Sibo Qiao;Xue Zhai;Shuo Wang;Neal N. Xiong;Zhengwen Huang,"At present, SOILD-STATE Fermentation (SSF) is mainly controlled by artificial experience, and the product quality and yield are not stable. Accurately predicting the quality and yield of SSF is of great significance for improving human food security and supply. In this paper, we propose an Intelligent Utility Prediction (IUP) scheme for SSF in 5G Industrial Internet of Things (IoT), including parameter collection and utility prediction of SSF process. This IUP scheme is based on the environmental perception and intelligent learning algorithms of the 5G Industrial IoT. We build a workflow model based on rewritable petri net to verify the correctness of the system model function and process. In addition, we design a utility prediction model for SSF based on the Generative Adversarial Networks (GAN) and Fully Connected Neural Network (FCNN). We design a GAN with constraint of mean square error (MSE-GAN) to solve the problem of few-shot learning of SSF, and then combine with the FCNN to realize the utility prediction (usually use the alcohol) of SSF. Based on the production of liquor in laboratory, the experiments show that the proposed method is more accurate than the other prediction methods in the utility prediction of SSF, and provide the basis for the numerical analysis of the proportion of preconfigured raw materials and the appropriate setting of cellar temperature. △ Less","28 March, 2021",https://arxiv.org/pdf/2103.15073
eXtended Artificial Intelligence: New Prospects of Human-AI Interaction Research,Carolin Wienrich;Marc Erich Latoschik,"Artificial Intelligence (AI) covers a broad spectrum of computational problems and use cases. Many of those implicate profound and sometimes intricate questions of how humans interact or should interact with AIs. Moreover, many users or future users do have abstract ideas of what AI is, significantly depending on the specific embodiment of AI applications. Human-centered-design approaches would suggest evaluating the impact of different embodiments on human perception of and interaction with AI. An approach that is difficult to realize due to the sheer complexity of application fields and embodiments in reality. However, here XR opens new possibilities to research human-AI interactions. The article's contribution is twofold: First, it provides a theoretical treatment and model of human-AI interaction based on an XR-AI continuum as a framework for and a perspective of different approaches of XR-AI combinations. It motivates XR-AI combinations as a method to learn about the effects of prospective human-AI interfaces and shows why the combination of XR and AI fruitfully contributes to a valid and systematic investigation of human-AI interactions and interfaces. Second, the article provides two exemplary experiments investigating the aforementioned approach for two distinct AI-systems. The first experiment reveals an interesting gender effect in human-robot interaction, while the second experiment reveals an Eliza effect of a recommender system. Here the article introduces two paradigmatic implementations of the proposed XR testbed for human-AI interactions and interfaces and shows how a valid and systematic investigation can be conducted. In sum, the article opens new perspectives on how XR benefits human-centered AI design and development. △ Less","5 April, 2021",https://arxiv.org/pdf/2103.15004
NMRPy: a novel NMR scripting system to implement artificial intelligence and advanced applications,Zao Liu;Kan Song;Zhiwei Chen,"Background: Software is an important windows to offer a variety of complex instrument control and data processing for nuclear magnetic resonance (NMR) spectrometer. NMR software should allow researchers to flexibly implement various functionality according to the requirement of applications. Scripting system can offer an open environment for NMR users to write custom programs with basic libraries. Emerging technologies, especially multivariate statistical analysis and artificial intelligence, have been successfully applied to NMR applications such as metabolomics and biomacromolecules. Scripting system should support more complex NMR libraries, which will enable the emerging technologies to be easily implemented in the scripting environment. Result: Here, a novel NMR scripting system named ""NMRPy"" is introduced. In the scripting system, both Java based NMR methods and original CPython based libraries are supported. A module was built as a bridge to integrate the runtime environment of Java and CPython. It works as an extension in CPython environment, as well as interacts with Java part by Java Native Interface. Leveraging the bridge, Java based instrument control and data processing methods can be called as a CPython style. Compared with traditional scripting system, NMRPy is easier for NMR researchers to develop complex functionality with fast numerical computation, multivariate statistical analysis, deep learning etc. Non-uniform sampling and protein structure prediction methods based on deep learning can be conveniently integrated into NMRPy. Conclusion: NMRPy offers a user-friendly environment to implement custom functionality leveraging its powerful basic NMR and rich CPython libraries. NMR applications with emerging technologies can be easily integrated. The scripting system is free of charge and can be downloaded by visiting http://www.spinstudioj.net/nmrpy. △ Less","27 March, 2021",https://arxiv.org/pdf/2103.14988
On the benefits of robust models in modulation recognition,Javier Maroto;Gérôme Bovet;Pascal Frossard,"Given the rapid changes in telecommunication systems and their higher dependence on artificial intelligence, it is increasingly important to have models that can perform well under different, possibly adverse, conditions. Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in many tasks in communications. However, in other domains, like image classification, DNNs have been shown to be vulnerable to adversarial perturbations, which consist of imperceptible crafted noise that when added to the data fools the model into misclassification. This puts into question the security of DNNs in communication tasks, and in particular in modulation recognition. We propose a novel framework to test the robustness of current state-of-the-art models where the adversarial perturbation strength is dependent on the signal strength and measured with the ""signal to perturbation ratio"" (SPR). We show that current state-of-the-art models are susceptible to these perturbations. In contrast to current research on the topic of image classification, modulation recognition allows us to have easily accessible insights on the usefulness of the features learned by DNNs by looking at the constellation space. When analyzing these vulnerable models we found that adversarial perturbations do not shift the symbols towards the nearest classes in constellation space. This shows that DNNs do not base their decisions on signal statistics that are important for the Bayes-optimal modulation recognition model, but spurious correlations in the training data. Our feature analysis and proposed framework can help in the task of finding better models for communication systems. △ Less","27 March, 2021",https://arxiv.org/pdf/2103.14977
COVID-19 personal protective equipment detection using real-time deep learning methods,Shayan Khosravipour;Erfan Taghvaei;Nasrollah Moghadam Charkari,"The exponential spread of COVID-19 in over 215 countries has led WHO to recommend face masks and gloves for a safe return to school or work. We used artificial intelligence and deep learning algorithms for automatic face masks and gloves detection in public areas. We investigated and assessed the efficacy of two popular deep learning algorithms of YOLO (You Only Look Once) and SSD MobileNet for the detection and proper wearing of face masks and gloves trained over a data set of 8250 images imported from the internet. YOLOv3 is implemented using the DarkNet framework, and the SSD MobileNet algorithm is applied for the development of accurate object detection. The proposed models have been developed to provide accurate multi-class detection (Mask vs. No-Mask vs. Gloves vs. No-Gloves vs. Improper). When people wear their masks improperly, the method detects them as an improper class. The introduced models provide accuracies of (90.6% for YOLO and 85.5% for SSD) for multi-class detection. The systems' results indicate the efficiency and validity of detecting people who do not wear masks and gloves in public. △ Less","27 March, 2021",https://arxiv.org/pdf/2103.14878
Alignment of Language Agents,Zachary Kenton;Tom Everitt;Laura Weidinger;Iason Gabriel;Vladimir Mikulik;Geoffrey Irving,"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues. △ Less","26 March, 2021",https://arxiv.org/pdf/2103.14659
Local Explanations via Necessity and Sufficiency: Unifying Theory and Practice,David Watson;Limor Gultchin;Ankur Taly;Luciano Floridi,"Necessity and sufficiency are the building blocks of all successful explanations. Yet despite their importance, these notions have been conceptually underdeveloped and inconsistently applied in explainable artificial intelligence (XAI), a fast-growing research area that is so far lacking in firm theoretical foundations. Building on work in logic, probability, and causality, we establish the central role of necessity and sufficiency in XAI, unifying seemingly disparate methods in a single formal framework. We provide a sound and complete algorithm for computing explanatory factors with respect to a given context, and demonstrate its flexibility and competitive performance against state of the art alternatives on various tasks. △ Less","10 June, 2021",https://arxiv.org/pdf/2103.14651
"Randomization-based Machine Learning in Renewable Energy Prediction Problems: Critical Literature Review, New Results and Perspectives",Javier Del Ser;David Casillas-Perez;Laura Cornejo-Bueno;Luis Prieto-Godino;Julia Sanz-Justo;Carlos Casanova-Mateo;Sancho Salcedo-Sanz,"Randomization-based Machine Learning methods for prediction are currently a hot topic in Artificial Intelligence, due to their excellent performance in many prediction problems, with a bounded computation time. The application of randomization-based approaches to renewable energy prediction problems has been massive in the last few years, including many different types of randomization-based approaches, their hybridization with other techniques and also the description of new versions of classical randomization-based algorithms, including deep and ensemble approaches. In this paper we review the most important characteristics of randomization-based machine learning approaches and their application to renewable energy prediction problems. We describe the most important methods and algorithms of this family of modeling methods, and perform a critical literature review, examining prediction problems related to solar, wind, marine/ocean and hydro-power renewable sources. We support our critical analysis with an extensive experimental study, comprising real-world problems related to solar, wind and hydro-power energy, where randomization-based algorithms are found to achieve superior results at a significantly lower computational cost than other modeling counterparts. We end our survey with a prospect of the most important challenges and research directions that remain open this field, along with an outlook motivating further research efforts in this exciting research field. △ Less","26 March, 2021",https://arxiv.org/pdf/2103.14624
Classification of Pneumonia and Tuberculosis from Chest X-rays,M. Abubakar;I. Shah;W. Ali;F. bashir,"Artificial intelligence (AI) and specifically machine learning is making inroads into number of fields. Machine learning is replacing and/or complementing humans in a certain type of domain to make systems perform tasks more efficiently and independently. Healthcare is a worthy domain to merge with AI and Machine learning to get things to work smoother and efficiently. The X-ray based detection and classification of diseases related to chest is much needed in this modern era due to the low number of quality radiologists. This thesis focuses on the classification of Pneumonia and Tuberculosis two major chest diseases from the chest X-rays. This system provides an opinion to the user whether one is having a disease or not, thereby helping doctors and medical staff to make a quick and informed decision about the presence of disease. As compared to previous work our model can detect two types of abnormality. Our model can detect whether X-ray is normal or having abnormality which can be pneumonia and tuberculosis 92.97% accurately. △ Less","25 March, 2021",https://arxiv.org/pdf/2103.14562
Socio-Technical Grounded Theory for Software Engineering,Rashina Hoda,"Grounded Theory (GT), a sociological research method designed to study social phenomena, is increasingly being used to investigate the human and social aspects of software engineering (SE). However, being written by and for sociologists, GT is often challenging for a majority of SE researchers to understand and apply. Additionally, SE researchers attempting ad hoc adaptations of traditional GT guidelines for modern socio-technical (ST) contexts often struggle in the absence of clear and relevant guidelines to do so, resulting in poor quality studies. To overcome these research community challenges and leverage modern research opportunities, this paper presents Socio-Technical Grounded Theory (STGT) designed to ease application and achieve quality outcomes. It defines what exactly is meant by an ST research context and presents the STGT guidelines that expand GT's philosophical foundations, provide increased clarity and flexibility in its methodological steps and procedures, define possible scope and contexts of application, encourage frequent reporting of a variety of interim, preliminary, and mature outcomes, and introduce nuanced evaluation guidelines for different outcomes. It is hoped that the SE research community and related ST disciplines such as computer science, data science, artificial intelligence, information systems, human computer/robot/AI interaction, human-centered emerging technologies (and increasingly other disciplines being transformed by rapid digitalisation and AI-based augmentation), will benefit from applying STGT to conduct quality research studies and systematically produce rich findings and mature theories with confidence. △ Less","9 September, 2021",https://arxiv.org/pdf/2103.14235
ACRE: Abstract Causal REasoning Beyond Covariation,Chi Zhang;Baoxiong Jia;Mark Edmonds;Song-Chun Zhu;Yixin Zhu,"Causal induction, i.e., identifying unobservable mechanisms that lead to the observable relations among variables, has played a pivotal role in modern scientific discovery, especially in scenarios with only sparse and limited data. Humans, even young toddlers, can induce causal relationships surprisingly well in various settings despite its notorious difficulty. However, in contrast to the commonplace trait of human cognition is the lack of a diagnostic benchmark to measure causal induction for modern Artificial Intelligence (AI) systems. Therefore, in this work, we introduce the Abstract Causal REasoning (ACRE) dataset for systematic evaluation of current vision systems in causal induction. Motivated by the stream of research on causal discovery in Blicket experiments, we query a visual reasoning system with the following four types of questions in either an independent scenario or an interventional scenario: direct, indirect, screening-off, and backward-blocking, intentionally going beyond the simple strategy of inducing causal relationships by covariation. By analyzing visual reasoning architectures on this testbed, we notice that pure neural models tend towards an associative strategy under their chance-level performance, whereas neuro-symbolic combinations struggle in backward-blocking reasoning. These deficiencies call for future research in models with a more comprehensive capability of causal induction. △ Less","25 March, 2021",https://arxiv.org/pdf/2103.14232
Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and Execution,Chi Zhang;Baoxiong Jia;Song-Chun Zhu;Yixin Zhu,"Spatial-temporal reasoning is a challenging task in Artificial Intelligence (AI) due to its demanding but unique nature: a theoretic requirement on representing and reasoning based on spatial-temporal knowledge in mind, and an applied requirement on a high-level cognitive system capable of navigating and acting in space and time. Recent works have focused on an abstract reasoning task of this kind -- Raven's Progressive Matrices (RPM). Despite the encouraging progress on RPM that achieves human-level performance in terms of accuracy, modern approaches have neither a treatment of human-like reasoning on generalization, nor a potential to generate answers. To fill in this gap, we propose a neuro-symbolic Probabilistic Abduction and Execution (PrAE) learner; central to the PrAE learner is the process of probabilistic abduction and execution on a probabilistic scene representation, akin to the mental manipulation of objects. Specifically, we disentangle perception and reasoning from a monolithic model. The neural visual perception frontend predicts objects' attributes, later aggregated by a scene inference engine to produce a probabilistic scene representation. In the symbolic logical reasoning backend, the PrAE learner uses the representation to abduce the hidden rules. An answer is predicted by executing the rules on the probabilistic representation. The entire system is trained end-to-end in an analysis-by-synthesis manner without any visual attribute annotations. Extensive experiments demonstrate that the PrAE learner improves cross-configuration generalization and is capable of rendering an answer, in contrast to prior works that merely make a categorical choice from candidates. △ Less","13 May, 2021",https://arxiv.org/pdf/2103.14230
Engineering an Intelligent Essay Scoring and Feedback System: An Experience Report,Akriti Chadda;Kelly Song;Raman Chandrasekar;Ian Gorton,"Artificial Intelligence (AI) / Machine Learning (ML)-based systems are widely sought-after commercial solutions that can automate and augment core business services. Intelligent systems can improve the quality of services offered and support scalability through automation. In this paper we describe our experience in engineering an exploratory system for assessing the quality of essays supplied by customers of a specialized recruitment support service. The problem domain is challenging because the open-ended customer-supplied source text has considerable scope for ambiguity and error, making models for analysis hard to build. There is also a need to incorporate specialized business domain knowledge into the intelligent processing systems. To address these challenges, we experimented with and exploited a number of cloud-based machine learning models and composed them into an application-specific processing pipeline. This design allows for modification of the underlying algorithms as more data and improved techniques become available. We describe our design, and the main challenges we faced, namely keeping a check on the quality control of the models, testing the software and deploying the computationally expensive ML models on the cloud. △ Less","24 March, 2021",https://arxiv.org/pdf/2103.13590
Artificial Intelligence in Tumor Subregion Analysis Based on Medical Imaging: A Review,Mingquan Lin;Jacob Wynne;Yang Lei;Tonghe Wang;Walter J. Curran;Tian Liu;Xiaofeng Yang,"Medical imaging is widely used in cancer diagnosis and treatment, and artificial intelligence (AI) has achieved tremendous success in various tasks of medical image analysis. This paper reviews AI-based tumor subregion analysis in medical imaging. We summarize the latest AI-based methods for tumor subregion analysis and their applications. Specifically, we categorize the AI-based methods by training strategy: supervised and unsupervised. A detailed review of each category is presented, highlighting important contributions and achievements. Specific challenges and potential AI applications in tumor subregion analysis are discussed. △ Less","24 March, 2021",https://arxiv.org/pdf/2103.13588
The Duality of Data and Knowledge Across the Three Waves of AI,Amit Sheth;Krishnaprasad Thirunarayan,"We discuss how over the last 30 to 50 years, Artificial Intelligence (AI) systems that focused only on data have been handicapped, and how knowledge has been critical in developing smarter, intelligent, and more effective systems. In fact, the vast progress in AI can be viewed in terms of the three waves of AI as identified by DARPA. During the first wave, handcrafted knowledge has been at the center-piece, while during the second wave, the data-driven approaches supplanted knowledge. Now we see a strong role and resurgence of knowledge fueling major breakthroughs in the third wave of AI underpinning future intelligent systems as they attempt human-like decision making, and seek to become trusted assistants and companions for humans. We find a wider availability of knowledge created from diverse sources, using manual to automated means both by repurposing as well as by extraction. Using knowledge with statistical learning is becoming increasingly indispensable to help make AI systems more transparent and auditable. We will draw a parallel with the role of knowledge and experience in human intelligence based on cognitive science, and discuss emerging neuro-symbolic or hybrid AI systems in which knowledge is the critical enabler for combining capabilities of the data-intensive statistical AI systems with those of symbolic AI systems, resulting in more capable AI systems that support more human-like intelligence. △ Less","14 April, 2021",https://arxiv.org/pdf/2103.13520
"A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based Finger Control",Anh Tuan Nguyen;Markus W. Drealan;Diu Khue Luu;Ming Jiang;Jian Xu;Jonathan Cheng;Qi Zhao;Edward W. Keefer;Zhi Yang,"Objective: Deep learning-based neural decoders have emerged as the prominent approach to enable dexterous and intuitive control of neuroprosthetic hands. Yet few studies have materialized the use of deep learning in clinical settings due to its high computational requirements. Methods: Recent advancements of edge computing devices bring the potential to alleviate this problem. Here we present the implementation of a neuroprosthetic hand with embedded deep learning-based control. The neural decoder is designed based on the recurrent neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a compacted yet powerful edge computing platform for deep learning inference. This enables the implementation of the neuroprosthetic hand as a portable and self-contained unit with real-time control of individual finger movements. Results: The proposed system is evaluated on a transradial amputee using peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes. The experiment results demonstrate the system's capabilities of providing robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of individual finger movements in various laboratory and real-world environments. Conclusion: Modern edge computing platforms enable the effective use of deep learning-based neural decoders for neuroprosthesis control as an autonomous system. Significance: This work helps pioneer the deployment of deep neural networks in clinical applications underlying a new class of wearable biomedical devices with embedded artificial intelligence. △ Less","24 March, 2021",https://arxiv.org/pdf/2103.13452
MONAIfbs: MONAI-based fetal brain MRI deep learning segmentation,Marta B. M. Ranzini;Lucas Fidon;Sébastien Ourselin;Marc Modat;Tom Vercauteren,"In fetal Magnetic Resonance Imaging, Super Resolution Reconstruction (SRR) algorithms are becoming popular tools to obtain high-resolution 3D volume reconstructions from low-resolution stacks of 2D slices, acquired at different orientations. To be effective, these algorithms often require accurate segmentation of the region of interest, such as the fetal brain in suspected pathological cases. In the case of Spina Bifida, Ebner, Wang et al. (NeuroImage, 2020) combined their SRR algorithm with a 2-step segmentation pipeline (2D localisation followed by a 2D segmentation network). However, if the localisation step fails, the second network is not able to recover a correct brain mask, thus requiring manual corrections for an effective SRR. In this work, we aim at improving the fetal brain segmentation for SRR in Spina Bifida. We hypothesise that a well-trained single-step UNet can achieve accurate performance, avoiding the need of a 2-step approach. We propose a new tool for fetal brain segmentation called MONAIfbs, which takes advantage of the Medical Open Network for Artificial Intelligence (MONAI) framework. Our network is based on the dynamic UNet (dynUNet), an adaptation of the nnU-Net framework. When compared to the original 2-step approach proposed in Ebner-Wang, and the same Ebner-Wang approach retrained with the expanded dataset available for this work, the dynUNet showed to achieve higher performance using a single step only. It also showed to reduce the number of outliers, as only 28 stacks obtained Dice score less than 0.9, compared to 68 for Ebner-Wang and 53 Ebner-Wang expanded. The proposed dynUNet model thus provides an improvement of the state-of-the-art fetal brain segmentation techniques, reducing the need for manual correction in automated SRR pipelines. Our code and our trained model are made publicly available at https://github.com/gift-surg/MONAIfbs. △ Less","21 March, 2021",https://arxiv.org/pdf/2103.13314
Actionable Cognitive Twins for Decision Making in Manufacturing,Jože M. Rožanec;Jinzhi Lu;Jan Rupnik;Maja Škrjanc;Dunja Mladenić;Blaž Fortuna;Xiaochen Zheng;Dimitris Kiritsis,"Actionable Cognitive Twins are the next generation Digital Twins enhanced with cognitive capabilities through a knowledge graph and artificial intelligence models that provide insights and decision-making options to the users. The knowledge graph describes the domain-specific knowledge regarding entities and interrelationships related to a manufacturing setting. It also contains information on possible decision-making options that can assist decision-makers, such as planners or logisticians. In this paper, we propose a knowledge graph modeling approach to construct actionable cognitive twins for capturing specific knowledge related to demand forecasting and production planning in a manufacturing plant. The knowledge graph provides semantic descriptions and contextualization of the production lines and processes, including data identification and simulation or artificial intelligence algorithms and forecasts used to support them. Such semantics provide ground for inferencing, relating different knowledge types: creative, deductive, definitional, and inductive. To develop the knowledge graph models for describing the use case completely, systems thinking approach is proposed to design and verify the ontology, develop a knowledge graph and build an actionable cognitive twin. Finally, we evaluate our approach in two use cases developed for a European original equipment manufacturer related to the automotive industry as part of the European Horizon 2020 project FACTLOG. △ Less","23 March, 2021",https://arxiv.org/pdf/2103.12854
Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid Human-Artificial Intelligence Scheme,Dapeng Wu;Ruili Bao;Zhidu Li;Honggang Wang;Ruyan Wang,"In this paper, a video service enhancement strategy is investigated under an edge-cloud collaboration framework, where video caching and delivery decisions are made in the cloud and edge respectively. We aim to guarantee the user fairness in terms of video coding rate under statistical delay constraint and edge caching capacity constraint. A hybrid human-artificial intelligence approach is developed to improve the user hit rate for video caching. Specifically, individual user interest is first characterized by merging factorization machine (FM) model and multi-layer perceptron (MLP) model, where both low-order and high-order features can be well learned simultaneously. Thereafter, a social aware similarity model is constructed to transferred individual user interest to group interest, based on which, videos can be selected to cache. Furthermore, a double bisection exploration scheme is proposed to optimize wireless resource allocation and video coding rate. The effectiveness of the proposed video caching scheme and video delivery scheme is finally validated by extensive experiments with a real-world data set. △ Less","14 January, 2021",https://arxiv.org/pdf/2103.12516
The Digital Agricultural Revolution: a Bibliometric Analysis Literature Review,Riccardo Bertoglio;Chiara Corbo;Filippo M. Renga;Matteo Matteucci,"The application of digital technologies in agriculture can improve traditional practices to adapt to climate change, reduce Greenhouse Gases (GHG) emissions, and promote a sustainable intensification for food security. Some authors argued that we are experiencing a Digital Agricultural Revolution (DAR) that will boost sustainable farming. This study aims to find evidence of the ongoing DAR process and clarify its roots, what it means, and where it is heading. We investigated the scientific literature with bibliometric analysis tools to produce an objective and reproducible literature review. We retrieved 4995 articles by querying the Web of Science database in the timespan 2012-2019, and we analyzed the obtained dataset to answer three specific research questions: i) what is the spectrum of the DAR-related terminology?; ii) what are the key articles and the most influential journals, institutions, and countries?; iii) what are the main research streams and the emerging topics? By grouping the authors' keywords reported on publications, we identified five main research streams: Climate-Smart Agriculture (CSA), Site-Specific Management (SSM), Remote Sensing (RS), Internet of Things (IoT), and Artificial Intelligence (AI). To provide a broad overview of each of these topics, we analyzed relevant review articles, and we present here the main achievements and the ongoing challenges. Finally, we showed the trending topics of the last three years (2017, 2018, 2019). △ Less","18 October, 2021",https://arxiv.org/pdf/2103.12488
Multimodal Data Fusion for Power-On-and-Go Robotic Systems in Retail,Shubham Sonawani;Kailas Maneparambil;Heni Ben Amor,"Robotic systems for retail have gained a lot of attention due to the labor-intensive nature of such business environments. Many tasks have the potential to be automated via intelligent robotic systems that have manipulation capabilities. For example, empty shelves can be replenished, stray products can be picked up or new items can be delivered. However, many challenges make the realization of this vision a challenge. In particular, robots are still too expensive and do not work out of the box. In this paper, we discuss a work-in-progress approach for enabling power-on-and-go robots in retail environments through a combination of active, physical sensors and passive, artificial sensors. In particular, we use low-cost hardware sensors in conjunction with machine learning techniques in order to generate high-quality environmental information. More specifically, we present a setup in which a standard monocular camera and Bluetooth low-energy yield a reliable robot system that can immediately be used after placing a couple of sensors in the environment. The camera information is used to synthesize accurate 3D point clouds, whereas the BLE data is used to integrate the data into a complex map of the environment. The combination of active and passive sensing enables high-quality sensing capabilities at a fraction of the costs traditionally associated with such tasks. △ Less","3 May, 2021",https://arxiv.org/pdf/2103.12241
Special Session: Reliability Analysis for ML/AI Hardware,Shamik Kundu;Kanad Basu;Mehdi Sadi;Twisha Titirsha;Shihao Song;Anup Das;Ujjwal Guin,"Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive in today's applications, such as autonomous vehicles, healthcare, aerospace, cybersecurity, and many critical applications. Ensuring the reliability and robustness of the underlying AI/ML hardware becomes our paramount importance. In this paper, we explore and evaluate the reliability of different AI/ML hardware. The first section outlines the reliability issues in a commercial systolic array-based ML accelerator in the presence of faults engendering from device-level non-idealities in the DRAM. Next, we quantified the impact of circuit-level faults in the MSB and LSB logic cones of the Multiply and Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we present two key reliability issues -- circuit aging and endurance in emerging neuromorphic hardware platforms and present our system-level approach to mitigate them. △ Less","29 March, 2021",https://arxiv.org/pdf/2103.12166
Automated and Autonomous Experiment in Electron and Scanning Probe Microscopy,Sergei V. Kalinin;Maxim A. Ziatdinov;Jacob Hinkle;Stephen Jesse;Ayana Ghosh;Kyle P. Kelley;Andrew R. Lupini;Bobby G. Sumpter;Rama K. Vasudevan,"Machine learning and artificial intelligence (ML/AI) are rapidly becoming an indispensable part of physics research, with domain applications ranging from theory and materials prediction to high-throughput data analysis. In parallel, the recent successes in applying ML/AI methods for autonomous systems from robotics through self-driving cars to organic and inorganic synthesis are generating enthusiasm for the potential of these techniques to enable automated and autonomous experiment (AE) in imaging. Here, we aim to analyze the major pathways towards AE in imaging methods with sequential image formation mechanisms, focusing on scanning probe microscopy (SPM) and (scanning) transmission electron microscopy ((S)TEM). We argue that automated experiments should necessarily be discussed in a broader context of the general domain knowledge that both informs the experiment and is increased as the result of the experiment. As such, this analysis should explore the human and ML/AI roles prior to and during the experiment, and consider the latencies, biases, and knowledge priors of the decision-making process. Similarly, such discussion should include the limitations of the existing imaging systems, including intrinsic latencies, non-idealities and drifts comprising both correctable and stochastic components. We further pose that the role of the AE in microscopy is not the exclusion of human operators (as is the case for autonomous driving), but rather automation of routine operations such as microscope tuning, etc., prior to the experiment, and conversion of low latency decision making processes on the time scale spanning from image acquisition to human-level high-order experiment planning. △ Less","22 March, 2021",https://arxiv.org/pdf/2103.12165
Edge Intelligence for Empowering IoT-based Healthcare Systems,Vahideh Hayyolalam;Moayad Aloqaily;Oznur Ozkasap;Mohsen Guizani,"The demand for real-time, affordable, and efficient smart healthcare services is increasing exponentially due to the technological revolution and burst of population. To meet the increasing demands on this critical infrastructure, there is a need for intelligent methods to cope with the existing obstacles in this area. In this regard, edge computing technology can reduce latency and energy consumption by moving processes closer to the data sources in comparison to the traditional centralized cloud and IoT-based healthcare systems. In addition, by bringing automated insights into the smart healthcare systems, artificial intelligence (AI) provides the possibility of detecting and predicting high-risk diseases in advance, decreasing medical costs for patients, and offering efficient treatments. The objective of this article is to highlight the benefits of the adoption of edge intelligent technology, along with AI in smart healthcare systems. Moreover, a novel smart healthcare model is proposed to boost the utilization of AI and edge technology in smart healthcare systems. Additionally, the paper discusses issues and research directions arising when integrating these different technologies together. △ Less","22 March, 2021",https://arxiv.org/pdf/2103.12144
Explaining Black-Box Algorithms Using Probabilistic Contrastive Counterfactuals,Sainyam Galhotra;Romila Pradhan;Babak Salimi,"There has been a recent resurgence of interest in explainable artificial intelligence (XAI) that aims to reduce the opaqueness of AI-based decision-making systems, allowing humans to scrutinize and trust them. Prior work in this context has focused on the attribution of responsibility for an algorithm's decisions to its inputs wherein responsibility is typically approached as a purely associational concept. In this paper, we propose a principled causality-based approach for explaining black-box decision-making systems that addresses limitations of existing methods in XAI. At the core of our framework lies probabilistic contrastive counterfactuals, a concept that can be traced back to philosophical, cognitive, and social foundations of theories on how humans generate and select explanations. We show how such counterfactuals can quantify the direct and indirect influences of a variable on decisions made by an algorithm, and provide actionable recourse for individuals negatively affected by the algorithm's decision. Unlike prior work, our system, LEWIS: (1)can compute provably effective explanations and recourse at local, global and contextual levels (2)is designed to work with users with varying levels of background knowledge of the underlying causal model and (3)makes no assumptions about the internals of an algorithmic system except for the availability of its input-output data. We empirically evaluate LEWIS on three real-world datasets and show that it generates human-understandable explanations that improve upon state-of-the-art approaches in XAI, including the popular LIME and SHAP. Experiments on synthetic data further demonstrate the correctness of LEWIS's explanations and the scalability of its recourse algorithm. △ Less","23 June, 2021",https://arxiv.org/pdf/2103.11972
Artificial Intelligence Narratives: An Objective Perspective on Current Developments,Noah Klarmann,"This work provides a starting point for researchers interested in gaining a deeper understanding of the big picture of artificial intelligence (AI). To this end, a narrative is conveyed that allows the reader to develop an objective view on current developments that is free from false promises that dominate public communication. An essential takeaway for the reader is that AI must be understood as an umbrella term encompassing a plethora of different methods, schools of thought, and their respective historical movements. Consequently, a bottom-up strategy is pursued in which the field of AI is introduced by presenting various aspects that are characteristic of the subject. This paper is structured in three parts: (i) Discussion of current trends revealing false public narratives, (ii) an introduction to the history of AI focusing on recurring patterns and main characteristics, and (iii) a critical discussion on the limitations of current methods in the context of the potential emergence of a strong(er) AI. It should be noted that this work does not cover any of these aspects holistically; rather, the content addressed is a selection made by the author and subject to a didactic strategy. △ Less","18 March, 2021",https://arxiv.org/pdf/2103.11961
SuSketch: Surrogate Models of Gameplay as a Design Assistant,Panagiotis Migkotzidis;Antonios Liapis,"This paper introduces SuSketch, a design tool for first person shooter levels. SuSketch provides the designer with gameplay predictions for two competing players of specific character classes. The interface allows the designer to work side-by-side with an artificially intelligent creator and to receive varied types of feedback such as path information, predicted balance between players in a complete playthrough, or a predicted heatmap of the locations of player deaths. The system also proactively designs alternatives to the level and class pairing, and presents them to the designer as suggestions that improve the predicted balance of the game. SuSketch offers a new way of integrating machine learning into mixed-initiative co-creation tools, as a surrogate of human play trained on a large corpus of artificial playtraces. A user study with 16 game developers indicated that the tool was easy to use, but also highlighted a need to make SuSketch more accessible and more explainable. △ Less","22 March, 2021",https://arxiv.org/pdf/2103.11726
"Blockchain-based Digital Twins: Research Trends, Issues, and Future Challenges",Sabah Suhail;Rasheed Hussain;Raja Jurdak;Alma Oracevic;Khaled Salah;Raimundas Matulevičius;Choong Seon Hong,"Industrial processes rely on sensory data for decision-making processes, risk assessment, and performance evaluation. Extracting actionable insights from the collected data calls for an infrastructure that can ensure the dissemination of trustworthy data. For the physical data to be trustworthy, it needs to be cross-validated through multiple sensor sources with overlapping fields of view. Cross-validated data can then be stored on the blockchain, to maintain its integrity and trustworthiness. Once trustworthy data is recorded on the blockchain, product lifecycle events can be fed into data-driven systems for process monitoring, diagnostics, and optimized control. In this regard, Digital Twins (DTs) can be leveraged to draw intelligent conclusions from data by identifying the faults and recommending precautionary measures ahead of critical events. Empowering DTs with blockchain in industrial use-cases targets key challenges of disparate data repositories, untrustworthy data dissemination, and the need for predictive maintenance. In this survey, while highlighting the key benefits of using blockchain-based DTs, we present a comprehensive review of the state-of-the-art research results for blockchain-based DTs. Based on the current research trends, we discuss a trustworthy blockchain-based DTs framework. We highlight the role of Artificial Intelligence (AI) in blockchain-based DTs. Furthermore, we discuss current and future research and deployment challenges of blockchain-supported DTs that require further investigation. △ Less","6 September, 2021",https://arxiv.org/pdf/2103.11585
Responsible AI: Gender bias assessment in emotion recognition,Artem Domnich;Gholamreza Anbarjafari,"Rapid development of artificial intelligence (AI) systems amplify many concerns in society. These AI algorithms inherit different biases from humans due to mysterious operational flow and because of that it is becoming adverse in usage. As a result, researchers have started to address the issue by investigating deeper in the direction towards Responsible and Explainable AI. Among variety of applications of AI, facial expression recognition might not be the most important one, yet is considered as a valuable part of human-AI interaction. Evolution of facial expression recognition from the feature based methods to deep learning drastically improve quality of such algorithms. This research work aims to study a gender bias in deep learning methods for facial expression recognition by investigating six distinct neural networks, training them, and further analysed on the presence of bias, according to the three definition of fairness. The main outcomes show which models are gender biased, which are not and how gender of subject affects its emotion recognition. More biased neural networks show bigger accuracy gap in emotion recognition between male and female test sets. Furthermore, this trend keeps for true positive and false positive rates. In addition, due to the nature of the research, we can observe which types of emotions are better classified for men and which for women. Since the topic of biases in facial expression recognition is not well studied, a spectrum of continuation of this research is truly extensive, and may comprise detail analysis of state-of-the-art methods, as well as targeting other biases. △ Less","21 March, 2021",https://arxiv.org/pdf/2103.11436
Collaborative Agent Gameplay in the Pandemic Board Game,Konstantinos Sfikas;Antonios Liapis,"While artificial intelligence has been applied to control players' decisions in board games for over half a century, little attention is given to games with no player competition. Pandemic is an exemplar collaborative board game where all players coordinate to overcome challenges posed by events occurring during the game's progression. This paper proposes an artificial agent which controls all players' actions and balances chances of winning versus risk of losing in this highly stochastic environment. The agent applies a Rolling Horizon Evolutionary Algorithm on an abstraction of the game-state that lowers the branching factor and simulates the game's stochasticity. Results show that the proposed algorithm can find winning strategies more consistently in different games of varying difficulty. The impact of a number of state evaluation metrics is explored, balancing between optimistic strategies that favor winning and pessimistic strategies that guard against losing. △ Less","21 March, 2021",https://arxiv.org/pdf/2103.11388
Semantic 3D Map Change Detection and Update based on Smartphone Visual Positioning System,Max Jwo Lem Lee;Li-Ta Hsu,"Accurate localization and 3D maps are increasingly needed for various artificial intelligence based IoT applications such as augmented reality, intelligent transportation, crowd monitoring, robotics, etc. This article proposes a novel semantic 3D map change detection and update based on a smartphone visual positioning system (VPS) for the outdoor and indoor environments. The proposed method presents an alternate solution to SLAM for map update in terms of efficiency, cost, availability, and map reuse. Building on existing 3D maps of recent years, a system is designed to use artificial intelligence to identify high-level semantics in images for positioning and map change detection. Then, a virtual LIDAR that estimates the depth of objects in the 3D map is used to generate a compact point cloud to update changes in the scene. We present an excellent performance of localization with respect to other state-of-the-art smartphone positioning solutions to accurately update semantic 3D maps. It is shown that the proposed solution can position users within 1.9m, and update objects with an average error of 2.1m. △ Less","21 March, 2021",https://arxiv.org/pdf/2103.11311
Predictive Maintenance -- Bridging Artificial Intelligence and IoT,G. G. Samatas;S. S. Moumgiakmas;G. A. Papakostas,"This paper highlights the trends in the field of predictive maintenance with the use of machine learning. With the continuous development of the Fourth Industrial Revolution, through IoT, the technologies that use artificial intelligence are evolving. As a result, industries have been using these technologies to optimize their production. Through scientific research conducted for this paper, conclusions were drawn about the trends in Predictive Maintenance applications with the use of machine learning bridging Artificial Intelligence and IoT. These trends are related to the types of industries in which Predictive Maintenance was applied, the models of artificial intelligence were implemented, mainly of machine learning and the types of sensors that are applied through the IoT to the applications. Six sectors were presented and the production sector was dominant as it accounted for 54.54% of total publications. In terms of artificial intelligence models, the most prevalent among ten were the Artificial Neural Networks, Support Vector Machine and Random Forest with 27.84%, 17.72% and 13.92% respectively. Finally, twelve categories of sensors emerged, of which the most widely used were the sensors of temperature and vibration with percentages of 60.71% and 46.42% correspondingly. △ Less","20 April, 2021",https://arxiv.org/pdf/2103.11148
Fully Onboard AI-powered Human-Drone Pose Estimation on Ultra-low Power Autonomous Flying Nano-UAVs,Daniele Palossi;Nicky Zimmerman;Alessio Burrello;Francesco Conti;Hanna Müller;Luca Maria Gambardella;Luca Benini;Alessandro Giusti;Jérôme Guzzi,"Artificial intelligence-powered pocket-sized air robots have the potential to revolutionize the Internet-of-Things ecosystem, acting as autonomous, unobtrusive, and ubiquitous smart sensors. With a few cm^{2} form-factor, nano-sized unmanned aerial vehicles (UAVs) are the natural befit for indoor human-drone interaction missions, as the pose estimation task we address in this work. However, this scenario is challenged by the nano-UAVs' limited payload and computational power that severely relegates the onboard brain to the sub-100 mW microcontroller unit-class. Our work stands at the intersection of the novel parallel ultra-low-power (PULP) architectural paradigm and our general development methodology for deep neural network (DNN) visual pipelines, i.e., covering from perception to control. Addressing the DNN model design, from training and dataset augmentation to 8-bit quantization and deployment, we demonstrate how a PULP-based processor, aboard a nano-UAV, is sufficient for the real-time execution (up to 135 frame/s) of our novel DNN, called PULP-Frontnet. We showcase how, scaling our model's memory and computational requirement, we can significantly improve the onboard inference (top energy efficiency of 0.43 mJ/frame) with no compromise in the quality-of-result vs. a resource-unconstrained baseline (i.e., full-precision DNN). Field experiments demonstrate a closed-loop top-notch autonomous navigation capability, with a heavily resource-constrained 27-gram Crazyflie 2.1 nano-quadrotor. Compared against the control performance achieved using an ideal sensing setup, onboard relative pose inference yields excellent drone behavior in terms of median absolute errors, such as positional (onboard: 41 cm, ideal: 26 cm) and angular (onboard: 3.7^{\circ}, ideal: 4.1^{\circ}). △ Less","19 March, 2021",https://arxiv.org/pdf/2103.10873
Deep Reinforcement Learning-Aided RAN Slicing Enforcement for B5G Latency Sensitive Services,Sergio Martiradonna;Andrea Abrardo;Marco Moretti;Giuseppe Piro;Gennaro Boggia,"The combination of cloud computing capabilities at the network edge and artificial intelligence promise to turn future mobile networks into service- and radio-aware entities, able to address the requirements of upcoming latency-sensitive applications. In this context, a challenging research goal is to exploit edge intelligence to dynamically and optimally manage the Radio Access Network Slicing (that is a less mature and more complex technology than fifth-generation Network Slicing) and Radio Resource Management, which is a very complex task due to the mostly unpredictably nature of the wireless channel. This paper presents a novel architecture that leverages Deep Reinforcement Learning at the edge of the network in order to address Radio Access Network Slicing and Radio Resource Management optimization supporting latency-sensitive applications. The effectiveness of our proposal against baseline methodologies is investigated through computer simulation, by considering an autonomous-driving use-case. △ Less","18 March, 2021",https://arxiv.org/pdf/2103.10277
Requirement Engineering Challenges for AI-intense Systems Development,Hans-Martin Heyn;Eric Knauss;Amna Pir Muhammad;Olof Eriksson;Jennifer Linder;Padmini Subbiah;Shameer Kumar Pradhan;Sagar Tungal,"Availability of powerful computation and communication technology as well as advances in artificial intelligence enable a new generation of complex, AI-intense systems and applications. Such systems and applications promise exciting improvements on a societal level, yet they also bring with them new challenges for their development. In this paper we argue that significant challenges relate to defining and ensuring behaviour and quality attributes of such systems and applications. We specifically derive four challenge areas from relevant use cases of complex, AI-intense systems and applications related to industry, transportation, and home automation: understanding, determining, and specifying (i) contextual definitions and requirements, (ii) data attributes and requirements, (iii) performance definition and monitoring, and (iv) the impact of human factors on system acceptance and success. Solving these challenges will imply process support that integrates new requirements engineering methods into development approaches for complex, AI-intense systems and applications. We present these challenges in detail and propose a research roadmap. △ Less","22 March, 2021",https://arxiv.org/pdf/2103.10270
Systematic Mapping Study on the Machine Learning Lifecycle,Yuanhao Xie;Luís Cruz;Petra Heck;Jan S. Rellermeyer,"The development of artificial intelligence (AI) has made various industries eager to explore the benefits of AI. There is an increasing amount of research surrounding AI, most of which is centred on the development of new AI algorithms and techniques. However, the advent of AI is bringing an increasing set of practical problems related to AI model lifecycle management that need to be investigated. We address this gap by conducting a systematic mapping study on the lifecycle of AI model. Through quantitative research, we provide an overview of the field, identify research opportunities, and provide suggestions for future research. Our study yields 405 publications published from 2005 to 2020, mapped in 5 different main research topics, and 31 sub-topics. We observe that only a minority of publications focus on data management and model production problems, and that more studies should address the AI lifecycle from a holistic perspective. △ Less","11 March, 2021",https://arxiv.org/pdf/2103.10248
COVIDx-US -- An open-access benchmark dataset of ultrasound imaging data for AI-driven COVID-19 analytics,Ashkan Ebadi;Pengcheng Xi;Alexander MacLean;Stéphane Tremblay;Sonny Kohli;Alexander Wong,"The COVID-19 pandemic continues to have a devastating effect on the health and well-being of the global population. Apart from the global health crises, the pandemic has also caused significant economic and financial difficulties and socio-physiological implications. Effective screening, triage, treatment planning, and prognostication of outcome plays a key role in controlling the pandemic. Recent studies have highlighted the role of point-of-care ultrasound imaging for COVID-19 screening and prognosis, particularly given that it is non-invasive, globally available, and easy-to-sanitize. Motivated by these attributes and the promise of artificial intelligence tools to aid clinicians, we introduce COVIDx-US, an open-access benchmark dataset of COVID-19 related ultrasound imaging data. The COVIDx-US dataset was curated from multiple sources and its current version, i.e., v1.2., consists of 150 lung ultrasound videos and 12,943 processed images of patients infected with COVID-19 infection, non-COVID-19 infection, other lung diseases/conditions, as well as normal control cases. The COVIDx-US is the largest open-access fully-curated dataset of its kind that has been systematically curated, processed, and validated specifically for the purpose of building and evaluating artificial intelligence algorithms and models. △ Less","20 April, 2021",https://arxiv.org/pdf/2103.10003
A deep learning theory for neural networks grounded in physics,Benjamin Scellier,"In the last decade, deep learning has become a major component of artificial intelligence. The workhorse of deep learning is the optimization of loss functions by stochastic gradient descent (SGD). Traditionally in deep learning, neural networks are differentiable mathematical functions, and the loss gradients required for SGD are computed with the backpropagation algorithm. However, the computer architectures on which these neural networks are implemented and trained suffer from speed and energy inefficiency issues, due to the separation of memory and processing in these architectures. To solve these problems, the field of neuromorphic computing aims at implementing neural networks on hardware architectures that merge memory and processing, just like brains do. In this thesis, we argue that building large, fast and efficient neural networks on neuromorphic architectures also requires rethinking the algorithms to implement and train them. We present an alternative mathematical framework, also compatible with SGD, which offers the possibility to design neural networks in substrates that directly exploit the laws of physics. Our framework applies to a very broad class of models, namely those whose state or dynamics are described by variational equations. This includes physical systems whose equilibrium state minimizes an energy function, and physical systems whose trajectory minimizes an action functional. We present a simple procedure to compute the loss gradients in such systems, called equilibrium propagation (EqProp), which requires solely locally available information for each trainable parameter. Since many models in physics and engineering can be described by variational principles, our framework has the potential to be applied to a broad variety of physical systems whose applications extend to various fields of engineering, beyond neuromorphic computing. △ Less","23 April, 2021",https://arxiv.org/pdf/2103.09985
Fused Deep Features Based Classification Framework for COVID-19 Classification with Optimized MLP,Saban Ozturk;Enes Yigit;Umut Ozkaya,"The new type of Coronavirus disease called COVID-19 continues to spread quite rapidly. Although it shows some specific symptoms, this disease, which can show different symptoms in almost every individual, has caused hundreds of thousands of patients to die. Although healthcare professionals work hard to prevent further loss of life, the rate of disease spread is very high. For this reason, the help of computer aided diagnosis (CAD) and artificial intelligence (AI) algorithms is vital. In this study, a method based on optimization of convolutional neural network (CNN) architecture, which is the most effective image analysis method of today, is proposed to fulfill the mentioned COVID-19 detection needs. First, COVID-19 images are trained using ResNet-50 and VGG-16 architectures. Then, features in the last layer of these two architectures are combined with feature fusion. These new image features matrices obtained with feature fusion are classified for COVID detection. A multi-layer perceptron (MLP) structure optimized by the whale optimization algorithm is used for the classification process. The obtained results show that the performance of the proposed framework is almost 4.5% higher than VGG-16 performance and almost 3.5% higher than ResNet-50 performance. △ Less","15 March, 2021",https://arxiv.org/pdf/2103.09904
Characterizing Technical Debt and Antipatterns in AI-Based Systems: A Systematic Mapping Study,Justus Bogner;Roberto Verdecchia;Ilias Gerostathopoulos,"Background: With the rising popularity of Artificial Intelligence (AI), there is a growing need to build large and complex AI-based systems in a cost-effective and manageable way. Like with traditional software, Technical Debt (TD) will emerge naturally over time in these systems, therefore leading to challenges and risks if not managed appropriately. The influence of data science and the stochastic nature of AI-based systems may also lead to new types of TD or antipatterns, which are not yet fully understood by researchers and practitioners. Objective: The goal of our study is to provide a clear overview and characterization of the types of TD (both established and new ones) that appear in AI-based systems, as well as the antipatterns and related solutions that have been proposed. Method: Following the process of a systematic mapping study, 21 primary studies are identified and analyzed. Results: Our results show that (i) established TD types, variations of them, and four new TD types (data, model, configuration, and ethics debt) are present in AI-based systems, (ii) 72 antipatterns are discussed in the literature, the majority related to data and model deficiencies, and (iii) 46 solutions have been proposed, either to address specific TD types, antipatterns, or TD in general. Conclusions: Our results can support AI professionals with reasoning about and communicating aspects of TD present in their systems. Additionally, they can serve as a foundation for future research to further our understanding of TD in AI-based systems. △ Less","17 March, 2021",https://arxiv.org/pdf/2103.09783
Understanding and Modeling AI-Intensive System Development,Luigi Lavazza;Sandro Morasca,"Developers of AI-Intensive Systems--i.e., systems that involve both ""traditional"" software and Artificial Intelligence""are recognizing the need to organize development systematically and use engineered methods and tools. Since an AI-Intensive System (AIIS) relies heavily on software, it is expected that Software Engineering (SE) methods and tools can help. However, AIIS development differs from the development of ""traditional"" software systems in a few substantial aspects. Hence, traditional SE methods and tools are not suitable or sufficient by themselves and need to be adapted and extended. A quest for ""SE for AI"" methods and tools has started. We believe that, in this effort, we should learn from experience and avoid repeating some of the mistakes made in the quest for SE in past years. To this end, a fundamental instrument is a set of concepts and a notation to deal with AIIS and the problems that characterize their development processes. In this paper, we propose to describe AIIS via a notation that was proposed for SE and embeds a set of concepts that are suitable to represent AIIS as well. We demonstrate the usage of the notation by modeling some characteristics that are particularly relevant for AIIS. △ Less","16 March, 2021",https://arxiv.org/pdf/2103.09191
A Multilingual African Embedding for FAQ Chatbots,Aymen Ben Elhaj Mabrouk;Moez Ben Haj Hmida;Chayma Fourati;Hatem Haddad;Abir Messaoudi,"Searching for an available, reliable, official, and understandable information is not a trivial task due to scattered information across the internet, and the availability lack of governmental communication channels communicating with African dialects and languages. In this paper, we introduce an Artificial Intelligence Powered chatbot for crisis communication that would be omnichannel, multilingual and multi dialectal. We present our work on modified StarSpace embedding tailored for African dialects for the question-answering task along with the architecture of the proposed chatbot system and a description of the different layers. English, French, Arabic, Tunisian, Igbo,Yorùbá, and Hausa are used as languages and dialects. Quantitative and qualitative evaluation results are obtained for our real deployed Covid-19 chatbot. Results show that users are satisfied and the conversation with the chatbot is meeting customer needs. △ Less","16 March, 2021",https://arxiv.org/pdf/2103.09185
Exploring the Assessment List for Trustworthy AI in the Context of Advanced Driver-Assistance Systems,Markus Borg;Joshua Bronson;Linus Christensson;Fredrik Olsson;Olof Lennartsson;Elias Sonnsjö;Hamid Ebabi;Martin Karsberg,"Artificial Intelligence (AI) is increasingly used in critical applications. Thus, the need for dependable AI systems is rapidly growing. In 2018, the European Commission appointed experts to a High-Level Expert Group on AI (AI-HLEG). AI-HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3) robust and specified seven corresponding key requirements. To help development organizations, AI-HLEG recently published the Assessment List for Trustworthy AI (ALTAI). We present an illustrative case study from applying ALTAI to an ongoing development project of an Advanced Driver-Assistance System (ADAS) that relies on Machine Learning (ML). Our experience shows that ALTAI is largely applicable to ADAS development, but specific parts related to human agency and transparency can be disregarded. Moreover, bigger questions related to societal and environmental impact cannot be tackled by an ADAS supplier in isolation. We present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we provide three recommendations for the next revision of ALTAI, i.e., life-cycle variants, domain-specific adaptations, and removed redundancy. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.09051
MLOps Challenges in Multi-Organization Setup: Experiences from Two Real-World Cases,Tuomas Granlund;Aleksi Kopponen;Vlad Stirbu;Lalli Myllyaho;Tommi Mikkonen,"The emerging age of connected, digital world means that there are tons of data, distributed to various organizations and their databases. Since this data can be confidential in nature, it cannot always be openly shared in seek of artificial intelligence (AI) and machine learning (ML) solutions. Instead, we need integration mechanisms, analogous to integration patterns in information systems, to create multi-organization AI/ML systems. In this paper, we present two real-world cases. First, we study integration between two organizations in detail. Second, we address scaling of AI/ML to multi-organization context. The setup we assume is that of continuous deployment, often referred to DevOps in software development. When also ML components are deployed in a similar fashion, term MLOps is used. Towards the end of the paper, we list the main observations and draw some final conclusions. Finally, we propose some directions for future work. △ Less","16 March, 2021",https://arxiv.org/pdf/2103.08937
Quick Learning Mechanism with Cross-Domain Adaptation for Intelligent Fault Diagnosis,Arun K. Sharma;Nishchal K. Verma,"The fault diagnostic model trained for a laboratory case machine fails to perform well on the industrial machines running under variable operating conditions. For every new operating condition of such machines, a new diagnostic model has to be trained which is a time-consuming and uneconomical process. Therefore, we propose a quick learning mechanism that can transform the existing diagnostic model into a new model suitable for industrial machines operating in different conditions. The proposed method uses the Net2Net transformation followed by a fine-tuning to cancel/minimize the maximum mean discrepancy between the new data and the previous one. The fine-tuning of the model requires a very less amount of labelled target samples and very few iterations of training. Therefore, the proposed method is capable of learning the new target data pattern quickly. The effectiveness of the proposed fault diagnosis method has been demonstrated on the Case Western Reserve University dataset, Intelligent Maintenance Systems bearing dataset, and Paderborn university dataset under the wide variations of the operating conditions. It has been validated that the diagnostic model trained on artificially damaged fault datasets can be used to quickly train another model for a real damage dataset. △ Less","6 September, 2021",https://arxiv.org/pdf/2103.08889
Interpretability of a Deep Learning Model in the Application of Cardiac MRI Segmentation with an ACDC Challenge Dataset,Adrianna Janik;Jonathan Dodd;Georgiana Ifrim;Kris Sankaran;Kathleen Curran,"Cardiac Magnetic Resonance (CMR) is the most effective tool for the assessment and diagnosis of a heart condition, which malfunction is the world's leading cause of death. Software tools leveraging Artificial Intelligence already enhance radiologists and cardiologists in heart condition assessment but their lack of transparency is a problem. This project investigates if it is possible to discover concepts representative for different cardiac conditions from the deep network trained to segment crdiac structures: Left Ventricle (LV), Right Ventricle (RV) and Myocardium (MYO), using explainability methods that enhances classification system by providing the score-based values of qualitative concepts, along with the key performance metrics. With introduction of a need of explanations in GDPR explainability of AI systems is necessary. This study applies Discovering and Testing with Concept Activation Vectors (D-TCAV), an interpretaibilty method to extract underlying features important for cardiac disease diagnosis from MRI data. The method provides a quantitative notion of concept importance for disease classified. In previous studies, the base method is applied to the classification of cardiac disease and provides clinically meaningful explanations for the predictions of a black-box deep learning classifier. This study applies a method extending TCAV with a Discovering phase (D-TCAV) to cardiac MRI analysis. The advantage of the D-TCAV method over the base method is that it is user-independent. The contribution of this study is a novel application of the explainability method D-TCAV for cardiac MRI anlysis. D-TCAV provides a shorter pre-processing time for clinicians than the base method. △ Less","15 March, 2021",https://arxiv.org/pdf/2103.08590
Towards the evaluation of automatic simultaneous speech translation from a communicative perspective,Claudio Fantinuoli;Bianca Prandi,"In recent years, automatic speech-to-speech and speech-to-text translation has gained momentum thanks to advances in artificial intelligence, especially in the domains of speech recognition and machine translation. The quality of such applications is commonly tested with automatic metrics, such as BLEU, primarily with the goal of assessing improvements of releases or in the context of evaluation campaigns. However, little is known about how the output of such systems is perceived by end users or how they compare to human performances in similar communicative tasks. In this paper, we present the results of an experiment aimed at evaluating the quality of a real-time speech translation engine by comparing it to the performance of professional simultaneous interpreters. To do so, we adopt a framework developed for the assessment of human interpreters and use it to perform a manual evaluation on both human and machine performances. In our sample, we found better performance for the human interpreters in terms of intelligibility, while the machine performs slightly better in terms of informativeness. The limitations of the study and the possible enhancements of the chosen framework are discussed. Despite its intrinsic limitations, the use of this framework represents a first step towards a user-centric and communication-oriented methodology for evaluating real-time automatic speech translation. △ Less","30 June, 2021",https://arxiv.org/pdf/2103.08364
Explaining Credit Risk Scoring through Feature Contribution Alignment with Expert Risk Analysts,Ayoub El Qadi;Natalia Diaz-Rodriguez;Maria Trocan;Thomas Frossard,"Credit assessments activities are essential for financial institutions and allow the global economy to grow. Building robust, solid and accurate models that estimate the probability of a default of a company is mandatory for credit insurance companies, moreover when it comes to bridging the trade finance gap. Automating the risk assessment process will allow credit risk experts to reduce their workload and focus on the critical and complex cases, as well as to improve the loan approval process by reducing the time to process the application. The recent developments in Artificial Intelligence are offering new powerful opportunities. However, most AI techniques are labelled as blackbox models due to their lack of explainability. For both users and regulators, in order to deploy such technologies at scale, being able to understand the model logic is a must to grant accurate and ethical decision making. In this study, we focus on companies credit scoring and we benchmark different machine learning models. The aim is to build a model to predict whether a company will experience financial problems in a given time horizon. We address the black box problem using eXplainable Artificial Techniques in particular, post-hoc explanations using SHapley Additive exPlanations. We bring light by providing an expert-aligned feature relevance score highlighting the disagreement between a credit risk expert and a model feature attribution explanation in order to better quantify the convergence towards a better human-aligned decision making. △ Less","15 March, 2021",https://arxiv.org/pdf/2103.08359
Crossing the Tepper Line: An Emerging Ontology for Describing the Dynamic Sociality of Embodied AI,Katie Seaborn;Peter Pennefather;Norihisa P. Miyake;Mihoko Otake-Matsuura,"Artificial intelligences (AI) are increasingly being embodied and embedded in the world to carry out tasks and support decision-making with and for people. Robots, recommender systems, voice assistants, virtual humans - do these disparate types of embodied AI have something in common? Here we show how they can manifest as ""socially embodied AI."" We define this as the state that embodied AI ""circumstantially"" take on within interactive contexts when perceived as both social and agentic by people. We offer a working ontology that describes how embodied AI can dynamically transition into socially embodied AI. We propose an ontological heuristic for describing the threshold: the Tepper line. We reinforce our theoretical work with expert insights from a card sort workshop. We end with two case studies to illustrate the dynamic and contextual nature of this heuristic. △ Less","14 March, 2021",https://arxiv.org/pdf/2103.08079
AIR4Children: Artificial Intelligence and Robotics for Children,Rocio Montenegro;Elva Corona;Donato Badillo-Perez;Angel Mandujano;Leticia Vazquez;Dago Cruz;Miguel Xochicale,"We introduce AIR4Children, Artificial Intelligence for Children, as a way to (a) tackle aspects for inclusion, accessibility, transparency, equity, fairness and participation and (b) to create affordable child-centred materials in AI and Robotics (AIR). We present current challenges and opportunities for a child-centred approaches for AIR. Similarly, we touch on open-sourced software and hardware technologies to make a more inclusive, affordable and fair participation of children in areas of AIR. Then, we describe the avenues that AIR4Children can take with the development of open-sourced software and hardware based on our initial pilots and experiences. Similarly, we propose to follow the philosophy of Montessori education to help children to not only develop computational thinking but also to internalise new concepts and learning skills through activities of movement and repetition. Finally, we conclude with the opportunities of our work and mainly we pose the future work of putting in practice what is proposed here to evaluate the potential impact on AIR to children, instructors, parents and their community. △ Less","13 March, 2021",https://arxiv.org/pdf/2103.07637
Challenges and Governance Solutions for Data Science Services based on Open Data and APIs,Juha-Pekka Joutsenlahti;Timo Lehtonen;Mikko Raatikainen;Elina Kettunen;Tommi Mikkonen,"Increasingly common open data and open application programming interfaces (APIs) together with the progress of data science -- such as artificial intelligence (AI) and especially machine learning (ML) -- create opportunities to build novel services by combining data from different sources. In this experience report, we describe our firsthand experiences on open data and in the domain of marine traffic in Finland and Sweden and identified technological opportunities for novel services. We enumerate five challenges that we have encountered with the application of open data: relevant data, historical data, licensing, runtime quality, and API evolution. These challenges affect both business model and technical implementation. We discuss how these challenges could be alleviated by better governance practices for provided open APIs and data. △ Less","12 March, 2021",https://arxiv.org/pdf/2103.07290
Robust and generalizable embryo selection based on artificial intelligence and time-lapse image sequences,Jørgen Berntsen;Jens Rimestad;Jacob Theilgaard Lassen;Dang Tran;Mikkel Fly Kragh,"Assessing and selecting the most viable embryos for transfer is an essential part of in vitro fertilization (IVF). In recent years, several approaches have been made to improve and automate the procedure using artificial intelligence (AI) and deep learning. Based on images of embryos with known implantation data (KID), AI models have been trained to automatically score embryos related to their chance of achieving a successful implantation. However, as of now, only limited research has been conducted to evaluate how embryo selection models generalize to new clinics and how they perform in subgroup analyses across various conditions. In this paper, we investigate how a deep learning-based embryo selection model using only time-lapse image sequences performs across different patient ages and clinical conditions, and how it correlates with traditional morphokinetic parameters. The model was trained and evaluated based on a large dataset from 18 IVF centers consisting of 115,832 embryos, of which 14,644 embryos were transferred KID embryos. In an independent test set, the AI model sorted KID embryos with an area under the curve (AUC) of a receiver operating characteristic curve of 0.67 and all embryos with an AUC of 0.95. A clinic hold-out test showed that the model generalized to new clinics with an AUC range of 0.60-0.75 for KID embryos. Across different subgroups of age, insemination method, incubation time, and transfer protocol, the AUC ranged between 0.63 and 0.69. Furthermore, model predictions correlated positively with blastocyst grading and negatively with direct cleavages. The fully automated iDAScore v1.0 model was shown to perform at least as good as a state-of-the-art manual embryo selection model. Moreover, full automatization of embryo scoring implies fewer manual evaluations and eliminates biases due to inter- and intraobserver variation. △ Less","12 March, 2021",https://arxiv.org/pdf/2103.07262
Auction Based Clustered Federated Learning in Mobile Edge Computing System,Renhao Lu;Weizhe Zhang;Qiong Li;Xiaoxiong Zhong;Athanasios V. Vasilakos,"In recent years, mobile clients' computing ability and storage capacity have greatly improved, efficiently dealing with some applications locally. Federated learning is a promising distributed machine learning solution that uses local computing and local data to train the Artificial Intelligence (AI) model. Combining local computing and federated learning can train a powerful AI model under the premise of ensuring local data privacy while making full use of mobile clients' resources. However, the heterogeneity of local data, that is, Non-independent and identical distribution (Non-IID) and imbalance of local data size, may bring a bottleneck hindering the application of federated learning in mobile edge computing (MEC) system. Inspired by this, we propose a cluster-based clients selection method that can generate a federated virtual dataset that satisfies the global distribution to offset the impact of data heterogeneity and proved that the proposed scheme could converge to an approximate optimal solution. Based on the clustering method, we propose an auction-based clients selection scheme within each cluster that fully considers the system's energy heterogeneity and gives the Nash equilibrium solution of the proposed scheme for balance the energy consumption and improving the convergence rate. The simulation results show that our proposed selection methods and auction-based federated learning can achieve better performance with the Convolutional Neural Network model (CNN) under different data distributions. △ Less","12 March, 2021",https://arxiv.org/pdf/2103.07150
Wandering and getting lost: the architecture of an app activating local communities on dementia issues,Nicklas Sindlev Andersen;Marco Chiarandini;Jacopo Mauro,"We describe the architecture of Sammen Om Demens (SOD), an application for portable devices aiming at helping persons with dementia when wandering and getting lost through the involvement of caregivers, family members, and ordinary citizens who volunteer. To enable the real-time detection of a person with dementia that has lost orientation, we transfer location data at high frequency from a frontend on the smartphone of a person with dementia to a backend system. The backend system must be able to cope with the high throughput data and carry out possibly heavy computations for the detection of anomalous behavior via artificial intelligence techniques. This sets certain performance and architectural requirements on the design of the backend. In the paper, we discuss our design and implementation choices for the backend of SOD that involve microservices and serverless services to achieve efficiency and scalability. We give evidence of the achieved goals by deploying the SOD backend on a public cloud and measuring the performance on simulated load tests. △ Less","25 October, 2021",https://arxiv.org/pdf/2103.06777
Intelligent behavior depends on the ecological niche: Scaling up AI to human-like intelligence in socio-cultural environments,Manfred Eppe;Pierre-Yves Oudeyer,"This paper outlines a perspective on the future of AI, discussing directions for machines models of human-like intelligence. We explain how developmental and evolutionary theories of human cognition should further inform artificial intelligence. We emphasize the role of ecological niches in sculpting intelligent behavior, and in particular that human intelligence was fundamentally shaped to adapt to a constantly changing socio-cultural environment. We argue that a major limit of current work in AI is that it is missing this perspective, both theoretically and experimentally. Finally, we discuss the promising approach of developmental artificial intelligence, modeling infant development through multi-scale interaction between intrinsically motivated learning, embodiment and a fastly changing socio-cultural environment. This paper takes the form of an interview of Pierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI - K{ü}nstliche Intelligenz special issue in developmental robotics. △ Less","11 March, 2021",https://arxiv.org/pdf/2103.06769
"Where is your place, Visual Place Recognition?",Sourav Garg;Tobias Fischer;Michael Milford,"Visual Place Recognition (VPR) is often characterized as being able to recognize the same place despite significant changes in appearance and viewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling robotic platforms and intelligent augmentation platforms such as augmented reality devices to perceive and understand the physical world. In this paper, we observe that there are three ""drivers"" that impose requirements on spatially intelligent agents and thus VPR systems: 1) the particular agent including its sensors and computational resources, 2) the operating environment of this agent, and 3) the specific task that the artificial agent carries out. In this paper, we characterize and survey key works in the VPR area considering those drivers, including their place representation and place matching choices. We also provide a new definition of VPR based on the visual overlap -- akin to spatial view cells in the brain -- that enables us to find similarities and differences to other research areas in the robotics and computer vision fields. We identify numerous open challenges and suggest areas that require more in-depth attention in future works. △ Less","8 November, 2021",https://arxiv.org/pdf/2103.06443
The AI Index 2021 Annual Report,Daniel Zhang;Saurabh Mishra;Erik Brynjolfsson;John Etchemendy;Deep Ganguli;Barbara Grosz;Terah Lyons;James Manyika;Juan Carlos Niebles;Michael Sellitto;Yoav Shoham;Jack Clark;Raymond Perrault,"Welcome to the fourth edition of the AI Index Report. This year we significantly expanded the amount of data available in the report, worked with a broader set of external organizations to calibrate our data, and deepened our connections with the Stanford Institute for Human-Centered Artificial Intelligence (HAI). The AI Index Report tracks, collates, distills, and visualizes data related to artificial intelligence. Its mission is to provide unbiased, rigorously vetted, and globally sourced data for policymakers, researchers, executives, journalists, and the general public to develop intuitions about the complex field of AI. The report aims to be the most credible and authoritative source for data and insights about AI in the world. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.06312
The whole brain architecture approach: Accelerating the development of artificial general intelligence by referring to the brain,Hiroshi Yamakawa,"The vastness of the design space created by the combination of a large number of computational mechanisms, including machine learning, is an obstacle to creating an artificial general intelligence (AGI). Brain-inspired AGI development, in other words, cutting down the design space to look more like a biological brain, which is an existing model of a general intelligence, is a promising plan for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain because the neuroscientific data required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA) -- the flow of information and the diagram of corresponding components -- and the task of developing each component using the BRA. This is called BRA-driven development. Another difficulty lies in the extraction of the operating principles necessary for reproducing the cognitive-behavioral function of the brain from neuroscience data. Therefore, this study proposes the Structure-constrained Interface Decomposition (SCID) method, which is a hypothesis-building method for creating a hypothetical component diagram consistent with neuroscientific findings. The application of this approach has begun for building various regions of the brain. Moving forward, we will examine methods of evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be merged, associated with the same regions of the brain. △ Less","5 March, 2021",https://arxiv.org/pdf/2103.06123
The AI Arena: A Framework for Distributed Multi-Agent Reinforcement Learning,Edward W. Staley;Corban G. Rivera;Ashley J. Llorens,"Advances in reinforcement learning (RL) have resulted in recent breakthroughs in the application of artificial intelligence (AI) across many different domains. An emerging landscape of development environments is making powerful RL techniques more accessible for a growing community of researchers. However, most existing frameworks do not directly address the problem of learning in complex operating environments, such as dense urban settings or defense-related scenarios, that incorporate distributed, heterogeneous teams of agents. To help enable AI research for this important class of applications, we introduce the AI Arena: a scalable framework with flexible abstractions for distributed multi-agent reinforcement learning. The AI Arena extends the OpenAI Gym interface to allow greater flexibility in learning control policies across multiple agents with heterogeneous learning strategies and localized views of the environment. To illustrate the utility of our framework, we present experimental results that demonstrate performance gains due to a distributed multi-agent learning approach over commonly-used RL techniques in several different learning environments. △ Less","9 March, 2021",https://arxiv.org/pdf/2103.05737
Robust Black-box Watermarking for Deep NeuralNetwork using Inverse Document Frequency,Mohammad Mehdi Yadollahi;Farzaneh Shoeleh;Sajjad Dadkhah;Ali A. Ghorbani,"Deep learning techniques are one of the most significant elements of any Artificial Intelligence (AI) services. Recently, these Machine Learning (ML) methods, such as Deep Neural Networks (DNNs), presented exceptional achievement in implementing human-level capabilities for various predicaments, such as Natural Processing Language (NLP), voice recognition, and image processing, etc. Training these models are expensive in terms of computational power and the existence of enough labelled data. Thus, ML-based models such as DNNs establish genuine business value and intellectual property (IP) for their owners. Therefore the trained models need to be protected from any adversary attacks such as illegal redistribution, reproducing, and derivation. Watermarking can be considered as an effective technique for securing a DNN model. However, so far, most of the watermarking algorithm focuses on watermarking the DNN by adding noise to an image. To this end, we propose a framework for watermarking a DNN model designed for a textual domain. The watermark generation scheme provides a secure watermarking method by combining Term Frequency (TF) and Inverse Document Frequency (IDF) of a particular word. The proposed embedding procedure takes place in the model's training time, making the watermark verification stage straightforward by sending the watermarked document to the trained model. The experimental results show that watermarked models have the same accuracy as the original ones. The proposed framework accurately verifies the ownership of all surrogate models without impairing the performance. The proposed algorithm is robust against well-known attacks such as parameter pruning and brute force attack. △ Less","9 March, 2021",https://arxiv.org/pdf/2103.05590
Low-level cognitive skill transfer between two individuals' minds via computer game-based framework,Ahmet Orun,"The novel technique introduced here aims to accomplish the first stage of transferring low-level cognitive skills between two individuals (e.g. from expert to learner) to ease the consecutive higher level declarative learning process for the target ""learner"" individual in a game environment. Such low-level cognitive skill is associated with the procedural knowledge and established at low-level of mind which can be unveiled and transferred by only a novel technique (rather than by a traditional educational environment ) like a highly interactive computer game domain in which a user exposes his/her unconscious mind behaviors via the game-hero non-deliberately during the game sessions. The cognitive data exposed by the game-hero would be recorded, and then be modelled by the artificial intelligence technique like Bayesian networks for an early stage of cognitive skill transfer and the cognitive stimuli are also generated to be used as game agents to train the learner. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.05563
MirrorME: Implementation of an IoT based Smart Mirror through Facial Recognition and Personalized Information Recommendation Algorithm,Khandaker Mohammad Mohi Uddin;Samrat Kumar Dey;Gias Uddin Parvez;Ayesha Siddika Mukta;Uzzal Kumar Acharjee,"We are living in the era of the fourth industrial revolution, which also treated as 4IR or Industry 4.0. Generally, 4IR considered as the mixture of robotics, artificial intelligence (AI), quantum computing, the Internet of Things (IoT) and other frontier technologies. It is obvious that nowadays a plethora of smart devices is providing services to make the daily life of human easier. However, in the morning most people around the globe use a traditional mirror while preparing themselves for daily task. The aim is to build a low-cost intelligent mirror system that can display a variety of details based on user recommendations. Therefore, in this article, Internet of Things (IoT) and AI-based smart mirror is introduced that will support the users to receive the necessary daily update of weather information, date, time, calendar, to-do list, updated news headlines, traffic updates, COVID-19 cases status and so on. Moreover, a face detection method also implemented with the smart mirror to construct the architecture more secure. Our proposed MirrorME application provides a success rate of nearly 87% in interacting with the features of face recognition and voice input. The mirror is capable of delivering multimedia facilities while maintaining high levels of security within the device. △ Less","28 February, 2021",https://arxiv.org/pdf/2103.05562
"Did Chatbots Miss Their 'Apollo Moment'? A Survey of the Potential, Gaps and Lessons from Using Collaboration Assistants During COVID-19",Biplav Srivastava,"Artificial Intelligence (AI) technologies have long been positioned as a tool to provide crucial data-driven decision support to people. In this survey paper, we look at how AI in general, and collaboration assistants (CAs or chatbots for short) in particular, have been used during a true global exigency - the COVID-19 pandemic. The key observation is that chatbots missed their ""Apollo moment"" when they could have really provided contextual, personalized, reliable decision support at scale that the state-of-the-art makes possible. We review the existing capabilities that are feasible and methods, identify the potential that chatbots could have met, the use-cases they were deployed on, the challenges they faced and gaps that persisted, and draw lessons that, if implemented, would make them more relevant in future health emergencies. △ Less","27 February, 2021",https://arxiv.org/pdf/2103.05561
When is it permissible for artificial intelligence to lie? A trust-based approach,Tae Wan Kim;Tong;Lu;Kyusong Lee;Zhaoqi Cheng;Yanhan Tang;John Hooker,"Conversational Artificial Intelligence (AI) used in industry settings can be trained to closely mimic human behaviors, including lying and deception. However, lying is often a necessary part of negotiation. To address this, we develop a normative framework for when it is ethical or unethical for a conversational AI to lie to humans, based on whether there is what we call ""invitation of trust"" in a particular scenario. Importantly, cultural norms play an important role in determining whether there is invitation of trust across negotiation settings, and thus an AI trained in one culture may not be generalizable to others. Moreover, individuals may have different expectations regarding the invitation of trust and propensity to lie for human vs. AI negotiators, and these expectations may vary across cultures as well. Finally, we outline how a conversational chatbot can be trained to negotiate ethically by applying autoregressive models to large dialog and negotiations datasets. △ Less","13 March, 2021",https://arxiv.org/pdf/2103.05434
Towards Artefact-based Requirements Engineering for Data-Centric Systems,Tatiana Chuprina;Daniel Mendez;Krzysztof Wnuk,"Many modern software-intensive systems employ artificial intelligence / machine-learning (AI/ML) components and are, thus, inherently data-centric. The behaviour of such systems depends on typically large amounts of data processed at run-time rendering such non-deterministic systems as complex. This complexity growth affects our understanding on needs and practices in Requirements Engineering (RE). There is, however, still little guidance on how to handle requirements for such systems effectively: What are, for example, typical quality requirements classes? What modelling concepts do we rely on or which levels of abstraction do we need to consider? In fact, how to integrate such concepts into approaches for a more traditional RE still needs profound investigations. In this research preview paper, we report on ongoing efforts to establish an artefact-based RE approach for the development of datacentric systems (DCSs). To this end, we sketch a DCS development process with the newly proposed requirements categories and data-centric artefacts and briefly report on an ongoing investigation of current RE challenges in industry developing data-centric systems. △ Less","9 March, 2021",https://arxiv.org/pdf/2103.05233
An Introduction to Deep Generative Modeling,Lars Ruthotto;Eldad Haber,"Deep generative models (DGM) are neural networks with many hidden layers trained to approximate complicated, high-dimensional probability distributions using a large number of samples. When trained successfully, we can use the DGMs to estimate the likelihood of each observation and to create new samples from the underlying distribution. Developing DGMs has become one of the most hotly researched fields in artificial intelligence in recent years. The literature on DGMs has become vast and is growing rapidly. Some advances have even reached the public sphere, for example, the recent successes in generating realistic-looking images, voices, or movies; so-called deep fakes. Despite these successes, several mathematical and practical issues limit the broader use of DGMs: given a specific dataset, it remains challenging to design and train a DGM and even more challenging to find out why a particular model is or is not effective. To help advance the theoretical understanding of DGMs, we introduce DGMs and provide a concise mathematical framework for modeling the three most popular approaches: normalizing flows (NF), variational autoencoders (VAE), and generative adversarial networks (GAN). We illustrate the advantages and disadvantages of these basic approaches using numerical experiments. Our goal is to enable and motivate the reader to contribute to this proliferating research area. Our presentation also emphasizes relations between generative modeling and optimal transport. △ Less","11 April, 2021",https://arxiv.org/pdf/2103.05180
Explanations in Autonomous Driving: A Survey,Daniel Omeiza;Helena Webb;Marina Jirotka;Lars Kunze,"The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environments and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society becomes paramount and may largely depend on their degree of transparency, trustworthiness, and compliance with regulations. The assessment of the compliance of AVs to these acceptance requirements can be facilitated through the provision of explanations for AVs' behaviour. Explainability is therefore seen as an important requirement for AVs. AVs should be able to explain what they have 'seen', done, and might do in environments in which they operate. In this paper, we provide a comprehensive survey of the existing body of work around explainable autonomous driving. First, we open with a motivation for explanations by highlighting and emphasising the importance of transparency, accountability, and trust in AVs; and examining existing regulations and standards related to AVs. Second, we identify and categorise the different stakeholders involved in the development, use, and regulation of AVs and elicit their explanation requirements for AV. Third, we provide a rigorous review of previous work on explanations for the different AV operations (i.e., perception, localisation, planning, control, and system management). Finally, we identify pertinent challenges and provide recommendations, such as a conceptual framework for AV explainability. This survey aims to provide the fundamental knowledge required of researchers who are interested in explainability in AVs. △ Less","9 November, 2021",https://arxiv.org/pdf/2103.05154
Urdu Handwritten Text Recognition Using ResNet18,Muhammad Kashif,"Handwritten text recognition is an active research area in the field of deep learning and artificial intelligence to convert handwritten text into machine-understandable. A lot of work has been done for other languages, especially for English, but work for the Urdu language is very minimal due to the cursive nature of Urdu characters. The need for Urdu HCR systems is increasing because of the advancement of technology. In this paper, we propose a ResNet18 model for handwritten text recognition using Urdu Nastaliq Handwritten Dataset (UNHD) which contains 3,12000 words written by 500 candidates. △ Less","19 February, 2021",https://arxiv.org/pdf/2103.05105
Hardware error correction for programmable photonics,Saumil Bandyopadhyay;Ryan Hamerly;Dirk Englund,"Programmable photonic circuits of reconfigurable interferometers can be used to implement arbitrary operations on optical modes, facilitating a flexible platform for accelerating tasks in quantum simulation, signal processing, and artificial intelligence. A major obstacle to scaling up these systems is static fabrication error, where small component errors within each device accrue to produce significant errors within the circuit computation. Mitigating this error usually requires numerical optimization dependent on real-time feedback from the circuit, which can greatly limit the scalability of the hardware. Here we present a deterministic approach to correcting circuit errors by locally correcting hardware errors within individual optical gates. We apply our approach to simulations of large scale optical neural networks and infinite impulse response filters implemented in programmable photonics, finding that they remain resilient to component error well beyond modern day process tolerances. Our results highlight a new avenue for scaling up programmable photonics to hundreds of modes within current day fabrication processes. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.04993
labelCloud: A Lightweight Domain-Independent Labeling Tool for 3D Object Detection in Point Clouds,Christoph Sager;Patrick Zschech;Niklas Kühl,"Within the past decade, the rise of applications based on artificial intelligence (AI) in general and machine learning (ML) in specific has led to many significant contributions within different domains. The applications range from robotics over medical diagnoses up to autonomous driving. However, nearly all applications rely on trained data. In case this data consists of 3D images, it is of utmost importance that the labeling is as accurate as possible to ensure high-quality outcomes of the ML models. Labeling in the 3D space is mostly manual work performed by expert workers, where they draw 3D bounding boxes around target objects the ML model should later automatically identify, e.g., pedestrians for autonomous driving or cancer cells within radiography. While a small range of recent 3D labeling tools exist, they all share three major shortcomings: (i) they are specified for autonomous driving applications, (ii) they lack convenience and comfort functions, and (iii) they have high dependencies and little flexibility in data format. Therefore, we propose a novel labeling tool for 3D object detection in point clouds to address these shortcomings. △ Less","5 March, 2021",https://arxiv.org/pdf/2103.04970
A Comparative Approach to Explainable Artificial Intelligence Methods in Application to High-Dimensional Electronic Health Records: Examining the Usability of XAI,Jamie Andrew Duell,"Explainable Artificial Intelligence (XAI) is a rising field in AI. It aims to produce a demonstrative factor of trust, which for human subjects is achieved through communicative means, which Machine Learning (ML) algorithms cannot solely produce, illustrating the necessity of an extra layer producing support to the model output. When approaching the medical field, we can see challenges arise when dealing with the involvement of human-subjects, the ideology behind trusting a machine to tend towards the livelihood of a human poses an ethical conundrum - leaving trust as the basis of the human-expert in acceptance to the machines decision. The aim of this paper is to apply XAI methods to demonstrate the usability of explainable architectures as a tertiary layer for the medical domain supporting ML predictions and human-expert opinion, XAI methods produce visualization of the feature contribution towards a given models output on both a local and global level. The work in this paper uses XAI to determine feature importance towards high-dimensional data-driven questions to inform domain-experts of identifiable trends with a comparison of model-agnostic methods in application to ML algorithms. The performance metrics for a glass-box method is also provided as a comparison against black-box capability for tabular data. Future work will aim to produce a user-study using metrics to evaluate human-expert usability and opinion of the given models. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.04951
A Case for 3D Integrated System Design for Neuromorphic Computing & AI Applications,Eren Kurshan;Hai Li;Mingoo Seok;Yuan Xie,"Over the last decade, artificial intelligence has found many applications areas in the society. As AI solutions have become more sophistication and the use cases grew, they highlighted the need to address performance and energy efficiency challenges faced during the implementation process. To address these challenges, there has been growing interest in neuromorphic chips. Neuromorphic computing relies on non von Neumann architectures as well as novel devices, circuits and manufacturing technologies to mimic the human brain. Among such technologies, 3D integration is an important enabler for AI hardware and the continuation of the scaling laws. In this paper, we overview the unique opportunities 3D integration provides in neuromorphic chip design, discuss the emerging opportunities in next generation neuromorphic architectures and review the obstacles. Neuromorphic architectures, which relied on the brain for inspiration and emulation purposes, face grand challenges due to the limited understanding of the functionality and the architecture of the human brain. Yet, high-levels of investments are dedicated to develop neuromorphic chips. We argue that 3D integration not only provides strategic advantages to the cost-effective and flexible design of neuromorphic chips, it may provide design flexibility in incorporating advanced capabilities to further benefits the designs in the future. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.04852
Spatial Equalization Before Reception: Reconfigurable Intelligent Surfaces for Multi-path Mitigation,Hongliang Zhang;Lingyang Song;Zhu Han;H. Vincent Poor,"Reconfigurable intelligent surfaces (RISs), which enable tunable anomalous reflection, have appeared as a promising method to enhance wireless systems. In this paper, we propose to use an RIS as a spatial equalizer to address the well-known multi-path fading phenomenon. By introducing some controllable paths artificially against the multi-path fading through the RIS, we can perform equalization during the transmission process instead of at the receiver, and thus all the users can share the same equalizer. Unlike the beamforming application of the RIS, which aims to maximize the received energy at receivers, the objective of the equalization application is to reduce the inter-symbol interference (ISI), which makes phase shifts at the RIS different. To this end, we formulate the phase shift optimization problem and propose an iterative algorithm to solve it. Simulation results show that the multi-path fading effect can be eliminated effectively compared to benchmark schemes. △ Less","8 March, 2021",https://arxiv.org/pdf/2103.04784
A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic Platforms,Wilkie Olin-Ammentorp;Yury Sokolov;Maxim Bazhenov,"Reinforcement learning (RL) is a foundation of learning in biological systems and provides a framework to address numerous challenges with real-world artificial intelligence applications. Efficient implementations of RL techniques could allow for agents deployed in edge-use cases to gain novel abilities, such as improved navigation, understanding complex situations and critical decision making. Towards this goal, we describe a flexible architecture to carry out reinforcement learning on neuromorphic platforms. This architecture was implemented using an Intel neuromorphic processor and demonstrated solving a variety of tasks using spiking dynamics. Our study proposes a usable energy efficient solution for real-world RL applications and demonstrates applicability of the neuromorphic platforms for RL problems. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.04780
Helicopter Track Identification with Autoencoder,Liya Wang;Panta Lucic;Keith Campbell;Craig Wanke,"Computing power, big data, and advancement of algorithms have led to a renewed interest in artificial intelligence (AI), especially in deep learning (DL). The success of DL largely lies on data representation because different representations can indicate to a degree the different explanatory factors of variation behind the data. In the last few year, the most successful story in DL is supervised learning. However, to apply supervised learning, one challenge is that data labels are expensive to get, noisy, or only partially available. With consideration that we human beings learn in an unsupervised way; self-supervised learning methods have garnered a lot of attention recently. A dominant force in self-supervised learning is the autoencoder, which has multiple uses (e.g., data representation, anomaly detection, denoise). This research explored the application of an autoencoder to learn effective data representation of helicopter flight track data, and then to support helicopter track identification. Our testing results are promising. For example, at Phoenix Deer Valley (DVT) airport, where 70% of recorded flight tracks have missing aircraft types, the autoencoder can help to identify twenty-two times more helicopters than otherwise detectable using rule-based methods; for Grand Canyon West Airport (1G4) airport, the autoencoder can identify thirteen times more helicopters than a current rule-based approach. Our approach can also identify mislabeled aircraft types in the flight track data and find true types for records with pseudo aircraft type labels such as HELO. With improved labelling, studies using these data sets can produce more reliable results. △ Less","3 March, 2021",https://arxiv.org/pdf/2103.04768
"AI-enabled Future Wireless Networks: Challenges, Opportunities and Open Issues",Medhat Elsayed;Melike Erol-Kantarci,"A plethora of demanding services and use cases mandate a revolutionary shift in the management of future wireless network resources. Indeed, when tight quality of service demands of applications are combined with increased complexity of the network, legacy network management routines will become unfeasible in 6G. Artificial Intelligence (AI) is emerging as a fundamental enabler to orchestrate the network resources from bottom to top. AI-enabled radio access and AI-enabled core will open up new opportunities for automated configuration of 6G. On the other hand, there are many challenges in AI-enabled networks that need to be addressed. Long convergence time, memory complexity, and complex behaviour of machine learning algorithms under uncertainty as well as highly dynamic channel, traffic and mobility conditions of the network contribute to the challenges. In this paper, we survey the state-of-art research in utilizing machine learning techniques in improving the performance of wireless networks. In addition, we identify challenges and open issues to provide a roadmap for the researchers. △ Less","7 March, 2021",https://arxiv.org/pdf/2103.04536
Expert System Gradient Descent Style Training: Development of a Defensible Artificial Intelligence Technique,Jeremy Straub,"Artificial intelligence systems, which are designed with a capability to learn from the data presented to them, are used throughout society. These systems are used to screen loan applicants, make sentencing recommendations for criminal defendants, scan social media posts for disallowed content and more. Because these systems don't assign meaning to their complex learned correlation network, they can learn associations that don't equate to causality, resulting in non-optimal and indefensible decisions being made. In addition to making decisions that are sub-optimal, these systems may create legal liability for their designers and operators by learning correlations that violate anti-discrimination and other laws regarding what factors can be used in different types of decision making. This paper presents the use of a machine learning expert system, which is developed with meaning-assigned nodes (facts) and correlations (rules). Multiple potential implementations are considered and evaluated under different conditions, including different network error and augmentation levels and different training levels. The performance of these systems is compared to random and fully connected networks. △ Less","7 March, 2021",https://arxiv.org/pdf/2103.04314
"Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications",Yu-Liang Chou;Catarina Moreira;Peter Bruza;Chun Ouyang;Joaquim Jorge,"There has been a growing interest in model-agnostic methods that can make deep learning models more transparent and explainable to a user. Some researchers recently argued that for a machine to achieve a certain degree of human-level explainability, this machine needs to provide human causally understandable explanations, also known as causability. A specific class of algorithms that have the potential to provide causability are counterfactuals. This paper presents an in-depth systematic review of the diverse existing body of literature on counterfactuals and causability for explainable artificial intelligence. We performed an LDA topic modelling analysis under a PRISMA framework to find the most relevant literature articles. This analysis resulted in a novel taxonomy that considers the grounding theories of the surveyed algorithms, together with their underlying properties and applications in real-world data. This research suggests that current model-agnostic counterfactual algorithms for explainable AI are not grounded on a causal theoretical formalism and, consequently, cannot promote causability to a human decision-maker. Our findings suggest that the explanations derived from major algorithms in the literature provide spurious correlations rather than cause/effects relationships, leading to sub-optimal, erroneous or even biased explanations. This paper also advances the literature with new directions and challenges on promoting causability in model-agnostic approaches for explainable artificial intelligence. △ Less","8 June, 2021",https://arxiv.org/pdf/2103.04244
Estimating and Improving Fairness with Adversarial Learning,Xiaoxiao Li;Ziteng Cui;Yifan Wu;Lin Gu;Tatsuya Harada,"Fairness and accountability are two essential pillars for trustworthy Artificial Intelligence (AI) in healthcare. However, the existing AI model may be biased in its decision marking. To tackle this issue, we propose an adversarial multi-task training strategy to simultaneously mitigate and detect bias in the deep learning-based medical image analysis system. Specifically, we propose to add a discrimination module against bias and a critical module that predicts unfairness within the base classification model. We further impose an orthogonality regularization to force the two modules to be independent during training. Hence, we can keep these deep learning tasks distinct from one another, and avoid collapsing them into a singular point on the manifold. Through this adversarial training method, the data from the underprivileged group, which is vulnerable to bias because of attributes such as sex and skin tone, are transferred into a domain that is neutral relative to these attributes. Furthermore, the critical module can predict fairness scores for the data with unknown sensitive attributes. We evaluate our framework on a large-scale public-available skin lesion dataset under various fairness evaluation metrics. The experiments demonstrate the effectiveness of our proposed method for estimating and improving fairness in the deep learning-based medical image analysis system. △ Less","11 May, 2021",https://arxiv.org/pdf/2103.04243
EVEREST: A design environment for extreme-scale big data analytics on heterogeneous platforms,Christian Pilato;Stanislav Bohm;Fabien Brocheton;Jeronimo Castrillon;Riccardo Cevasco;Vojtech Cima;Radim Cmar;Dionysios Diamantopoulos;Fabrizio Ferrandi;Jan Martinovic;Gianluca Palermo;Michele Paolino;Antonio Parodi;Lorenzo Pittaluga;Daniel Raho;Francesco Regazzoni;Katerina Slaninova;Christoph Hagleitner,"High-Performance Big Data Analytics (HPDA) applications are characterized by huge volumes of distributed and heterogeneous data that require efficient computation for knowledge extraction and decision making. Designers are moving towards a tight integration of computing systems combining HPC, Cloud, and IoT solutions with artificial intelligence (AI). Matching the application and data requirements with the characteristics of the underlying hardware is a key element to improve the predictions thanks to high performance and better use of resources. We present EVEREST, a novel H2020 project started on October 1st, 2020 that aims at developing a holistic environment for the co-design of HPDA applications on heterogeneous, distributed, and secure platforms. EVEREST focuses on programmability issues through a data-driven design approach, the use of hardware-accelerated AI, and an efficient runtime monitoring with virtualization support. In the different stages, EVEREST combines state-of-the-art programming models, emerging communication standards, and novel domain-specific extensions. We describe the EVEREST approach and the use cases that drive our research. △ Less","6 March, 2021",https://arxiv.org/pdf/2103.04185
The Prevalence of Code Smells in Machine Learning projects,Bart van Oort;Luís Cruz;Maurício Aniche;Arie van Deursen,"Artificial Intelligence (AI) and Machine Learning (ML) are pervasive in the current computer science landscape. Yet, there still exists a lack of software engineering experience and best practices in this field. One such best practice, static code analysis, can be used to find code smells, i.e., (potential) defects in the source code, refactoring opportunities, and violations of common coding standards. Our research set out to discover the most prevalent code smells in ML projects. We gathered a dataset of 74 open-source ML projects, installed their dependencies and ran Pylint on them. This resulted in a top 20 of all detected code smells, per category. Manual analysis of these smells mainly showed that code duplication is widespread and that the PEP8 convention for identifier naming style may not always be applicable to ML code due to its resemblance with mathematical notation. More interestingly, however, we found several major obstructions to the maintainability and reproducibility of ML projects, primarily related to the dependency management of Python projects. We also found that Pylint cannot reliably check for correct usage of imported dependencies, including prominent ML libraries such as PyTorch. △ Less","6 March, 2021",https://arxiv.org/pdf/2103.04146
A Real-time Low-cost Artificial Intelligence System for Autonomous Spraying in Palm Plantations,Zhenwang Qin;Wensheng Wang;Karl-Heinz Dammer;Leifeng Guo;Zhen Cao,"In precision crop protection, (target-orientated) object detection in image processing can help navigate Unmanned Aerial Vehicles (UAV, crop protection drones) to the right place to apply the pesticide. Unnecessary application of non-target areas could be avoided. Deep learning algorithms dominantly use in modern computer vision tasks which require high computing time, memory footprint, and power consumption. Based on the Edge Artificial Intelligence, we investigate the main three paths that lead to dealing with this problem, including hardware accelerators, efficient algorithms, and model compression. Finally, we integrate them and propose a solution based on a light deep neural network (DNN), called Ag-YOLO, which can make the crop protection UAV have the ability to target detection and autonomous operation. This solution is restricted in size, cost, flexible, fast, and energy-effective. The hardware is only 18 grams in weight and 1.5 watts in energy consumption, and the developed DNN model needs only 838 kilobytes of disc space. We tested the developed hardware and software in comparison to the tiny version of the state-of-art YOLOv3 framework, known as YOLOv3-Tiny to detect individual palm in a plantation. An average F1 score of 0.9205 at the speed of 36.5 frames per second (in comparison to similar accuracy at 18 frames per second and 8.66 megabytes of the YOLOv3-Tiny algorithm) was reached. This developed detection system is easily plugged into any machines already purchased as long as the machines have USB ports and run Linux Operating System. △ Less","6 March, 2021",https://arxiv.org/pdf/2103.04132
ODIN: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-Situ Neural Network Processing in Phase Change RAM,Supreeth Mysore Shivanandamurthy;Ishan. G. Thakkar;Sayed Ahmad Salehi,"Due to the very rapidly growing use of Artificial Neural Networks (ANNs) in real-world applications related to machine learning and Artificial Intelligence (AI), several hardware accelerator de-signs for ANNs have been proposed recently. In this paper, we present a novel processing-in-memory (PIM) engine called ODIN that employs hybrid binary-stochastic bit-parallel arithmetic in-side phase change RAM (PCRAM) to enable a low-overhead in-situ acceleration of all essential ANN functions such as multiply-accumulate (MAC), nonlinear activation, and pooling. We mapped four ANN benchmark applications on ODIN to compare its performance with a conventional processor-centric design and a crossbar-based in-situ ANN accelerator from prior work. The results of our analysis for the considered ANN topologies indicate that our ODIN accelerator can be at least 5.8x faster and 23.2x more energy-efficient, and up to 90.8x faster and 1554x more energy-efficient, compared to the crossbar-based in-situ ANN accelerator from prior work. △ Less","5 March, 2021",https://arxiv.org/pdf/2103.03953
"Deep Hedging, Generative Adversarial Networks, and Beyond",Hyunsu Kim,"This paper introduces a potential application of deep learning and artificial intelligence in finance, particularly its application in hedging. The major goal encompasses two objectives. First, we present a framework of a direct policy search reinforcement agent replicating a simple vanilla European call option and use the agent for the model-free delta hedging. Through the first part of this paper, we demonstrate how the RNN-based direct policy search RL agents can perform delta hedging better than the classic Black-Scholes model in Q-world based on parametrically generated underlying scenarios, particularly minimizing tail exposures at higher values of the risk aversion parameter. In the second part of this paper, with the non-parametric paths generated by time-series GANs from multi-variate temporal space, we illustrate its delta hedging performance on various values of the risk aversion parameter via the basic RNN-based RL agent introduced in the first part of the paper, showing that we can potentially achieve higher average profits with a rather evident risk-return trade-off. We believe that this RL-based hedging framework is a more efficient way of performing hedging in practice, addressing some of the inherent issues with the classic models, providing promising/intuitive hedging results, and rendering a flexible framework that can be easily paired with other AI-based models for many other purposes. △ Less","5 March, 2021",https://arxiv.org/pdf/2103.03913
Reducing cybersickness in 360-degree virtual reality,Iqra Arshad;Paulo De Mello;Martin Ender;Jason D. McEwen;Elisa R. Ferré,"Despite the technological advancements in Virtual Reality (VR), users are constantly combating feelings of nausea and disorientation, the so called cybersickness. Cybersickness symptoms cause severe discomfort and hinder the immersive VR experience. Here we investigated cybersickness in 360-degree head-mounted display VR. In traditional 360-degree VR experiences, translational movement in the real world is not reflected in the virtual world, and therefore self-motion information is not corroborated by matching visual and vestibular cues, which may trigger symptoms of cybersickness. We have evaluated whether a new Artificial Intelligence (AI) software designed to supplement the 360-degree VR experience with artificial 6-degrees-of-freedom motion may reduce cybersickness. Explicit (simulator sickness questionnaire and fast motion sickness rating) and implicit (heart rate) measurements were used to evaluate cybersickness symptoms during and after 360-degree VR exposure. Simulator sickness scores showed a significant reduction in feelings of nausea during the AI supplemented 6-degrees-of-freedom motion VR compared to traditional 360-degree VR. However, 6-degrees-of-freedom motion VR did not reduce oculomotor or disorientation measures of sickness. No changes have been observed in fast motion sickness and heart rate measures. Improving the congruency between visual and vestibular cues in 360-degree VR, as provided by the AI supplemented 6-degrees-of-freedom motion system considered, is essential to provide a more engaging, immersive and safe VR, which is critical for educational, cultural and entertainment applications. △ Less","17 November, 2021",https://arxiv.org/pdf/2103.03898
On Channel Reciprocity in Reconfigurable Intelligent Surface Assisted Wireless Network,Wankai Tang;Xiangyu Chen;Ming Zheng Chen;Jun Yan Dai;Yu Han;Shi Jin;Qiang Cheng;Geoffrey Ye Li;Tie Jun Cui,"Channel reciprocity greatly facilitates downlink precoding in time-division duplexing (TDD) multiple-input multiple-output (MIMO) communications without the need for channel state information (CSI) feedback. Recently, reconfigurable intelligent surfaces (RISs) emerge as a promising technology to enhance the performance of future wireless networks. However, since the artificial electromagnetic characteristics of RISs do not strictly follow the normal laws of nature, it brings up a question: does the channel reciprocity hold in RIS-assisted TDD wireless networks? After briefly reviewing the reciprocity theorem, in this article, we show that there still exists channel reciprocity for RIS-assisted wireless networks satisfying certain conditions. We also experimentally demonstrate the reciprocity at the sub-6 GHz and the millimeter-wave frequency bands by using two fabricated RISs. Furthermore, we introduce several RIS-assisted approaches to realizing nonreciprocal channels. Finally, potential opportunities brought by reciprocal/nonreciprocal RISs and future research directions are outlined. △ Less","8 September, 2021",https://arxiv.org/pdf/2103.03753
Suicide Classificaction for News Media Using Convolutional Neural Network,Hugo J. Bello;Nora Palomar-Ciria;Enrique Baca-García;Celia Lozano,"Currently, the process of evaluating suicides is highly subjective, which limits the efficacy and accuracy of prevention efforts. Artificial intelligence (AI) has emerged as a means of investigating large datasets to identify patterns within ""big data"" that can determine the factors on suicide outcomes. Here, we use AI tools to extract the topic from (press and social) media text. However, news media articles lack of suicide tags. Using tweets with hashtags related to sucide, we train a neuronal model which identifies if a given text has a suicidade-related contagion. Our results suggest a high level of the impact of mediatic into suicide cases, and a intrinsic thematic relationship of suicide news. These results pave the way to build more interpretable suicide data, which may help to better track, understand its origin, and improve prevention strategies. △ Less","18 February, 2021",https://arxiv.org/pdf/2103.03727
Uncertainty Quantification and Software Risk Analysis for Digital Twins in the Nearly Autonomous Management and Control Systems: A Review,Linyu Lin;Han Bao;Nam Dinh,"A nearly autonomous management and control (NAMAC) system is designed to furnish recommendations to operators for achieving particular goals based on NAMAC's knowledge base. As a critical component in a NAMAC system, digital twins (DTs) are used to extract information from the knowledge base to support decision-making in reactor control and management during all modes of plant operations. With the advancement of artificial intelligence and data-driven methods, machine learning algorithms are used to build DTs of various functions in the NAMAC system. To evaluate the uncertainty of DTs and its impacts on the reactor digital instrumentation and control systems, uncertainty quantification (UQ) and software risk analysis is needed. As a comprehensive overview of prior research and a starting point for new investigations, this study selects and reviews relevant UQ techniques and software hazard and software risk analysis methods that may be suitable for DTs in the NAMAC system. △ Less","28 January, 2021",https://arxiv.org/pdf/2103.03680
A framework for fostering transparency in shared artificial intelligence models by increasing visibility of contributions,Iain Barclay;Harrison Taylor;Alun Preece;Ian Taylor;Dinesh Verma;Geeth de Mel,"Increased adoption of artificial intelligence (AI) systems into scientific workflows will result in an increasing technical debt as the distance between the data scientists and engineers who develop AI system components and scientists, researchers and other users grows. This could quickly become problematic, particularly where guidance or regulations change and once-acceptable best practice becomes outdated, or where data sources are later discredited as biased or inaccurate. This paper presents a novel method for deriving a quantifiable metric capable of ranking the overall transparency of the process pipelines used to generate AI systems, such that users, auditors and other stakeholders can gain confidence that they will be able to validate and trust the data sources and contributors in the AI systems that they rely on. The methodology for calculating the metric, and the type of criteria that could be used to make judgements on the visibility of contributions to systems are evaluated through models published at ModelHub and PyTorch Hub, popular archives for sharing science resources, and is found to be helpful in driving consideration of the contributions made to generating AI systems and approaches towards effective documentation and improving transparency in machine learning assets shared within scientific communities. △ Less","5 March, 2021",https://arxiv.org/pdf/2103.03610
Advances in Multi-turn Dialogue Comprehension: A Survey,Zhuosheng Zhang;Hai Zhao,"Training machines to understand natural language and interact with humans is an elusive and essential task of artificial intelligence. A diversity of dialogue systems has been designed with the rapid development of deep learning techniques, especially the recent pre-trained language models (PrLMs). Among these studies, the fundamental yet challenging type of task is dialogue comprehension whose role is to teach the machines to read and comprehend the dialogue context before responding. In this paper, we review the previous methods from the technical perspective of dialogue modeling for the dialogue comprehension task. We summarize the characteristics and challenges of dialogue comprehension in contrast to plain-text reading comprehension. Then, we discuss three typical patterns of dialogue modeling. In addition, we categorize dialogue-related pre-training techniques which are employed to enhance PrLMs in dialogue scenarios. Finally, we highlight the technical advances in recent years and point out the lessons from the empirical analysis and the prospects towards a new frontier of researches. △ Less","12 October, 2021",https://arxiv.org/pdf/2103.03125
Detecting Spurious Correlations with Sanity Tests for Artificial Intelligence Guided Radiology Systems,Usman Mahmood;Robik Shrestha;David D. B. Bates;Lorenzo Mannelli;Giuseppe Corrias;Yusuf Erdi;Christopher Kanan,"Artificial intelligence (AI) has been successful at solving numerous problems in machine perception. In radiology, AI systems are rapidly evolving and show progress in guiding treatment decisions, diagnosing, localizing disease on medical images, and improving radiologists' efficiency. A critical component to deploying AI in radiology is to gain confidence in a developed system's efficacy and safety. The current gold standard approach is to conduct an analytical validation of performance on a generalization dataset from one or more institutions, followed by a clinical validation study of the system's efficacy during deployment. Clinical validation studies are time-consuming, and best practices dictate limited re-use of analytical validation data, so it is ideal to know ahead of time if a system is likely to fail analytical or clinical validation. In this paper, we describe a series of sanity tests to identify when a system performs well on development data for the wrong reasons. We illustrate the sanity tests' value by designing a deep learning system to classify pancreatic cancer seen in computed tomography scans. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.03048
The Dota 2 Bot Competition,Jose M. Font;Tobias Mahlmann,"Multiplayer Online Battle Area (MOBA) games are a recent huge success both in the video game industry and the international eSports scene. These games encourage team coordination and cooperation, short and long-term planning, within a real-time combined action and strategy gameplay. Artificial Intelligence and Computational Intelligence in Games research competitions offer a wide variety of challenges regarding the study and application of AI techniques to different game genres. These events are widely accepted by the AI/CI community as a sort of AI benchmarking that strongly influences many other research areas in the field. This paper presents and describes in detail the Dota 2 Bot competition and the Dota 2 AI framework that supports it. This challenge aims to join both, MOBAs and AI/CI game competitions, inviting participants to submit AI controllers for the successful MOBA \textit{Defense of the Ancients 2} (Dota 2) to play in 1v1 matches, which aims for fostering research on AI techniques for real-time games. The Dota 2 AI framework makes use of the actual Dota 2 game modding capabilities to enable to connect external AI controllers to actual Dota 2 game matches using the original Free-to-Play game.se of the actual Dota 2 game modding capabilities to enable to connect external AI controllers to actual Dota 2 game matches using the original Free-to-Play game. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.02943
FootApp: an AI-Powered System for Football Match Annotation,Silvio Barra;Salvatore M. Carta;Alessandro Giuliani;Alessia Pisu;Alessandro Sebastian Podda;DanieleRiboni,"In the last years, scientific and industrial research has experienced a growing interest in acquiring large annotated data sets to train artificial intelligence algorithms for tackling problems in different domains. In this context, we have observed that even the market for football data has substantially grown. The analysis of football matches relies on the annotation of both individual players' and team actions, as well as the athletic performance of players. Consequently, annotating football events at a fine-grained level is a very expensive and error-prone task. Most existing semi-automatic tools for football match annotation rely on cameras and computer vision. However, those tools fall short in capturing team dynamics, and in extracting data of players who are not visible in the camera frame. To address these issues, in this manuscript we present FootApp, an AI-based system for football match annotation. First, our system relies on an advanced and mixed user interface that exploits both vocal and touch interaction. Second, the motor performance of players is captured and processed by applying machine learning algorithms to data collected from inertial sensors worn by players. Artificial intelligence techniques are then used to check the consistency of generated labels, including those regarding the physical activity of players, to automatically recognize annotation errors. Notably, we implemented a full prototype of the proposed system, performing experiments to show its effectiveness in a real-world adoption scenario. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.02938
Visual Question Answering: which investigated applications?,Silvio Barra;Carmen Bisogni;Maria De Marsico;Stefano Ricciardi,"Visual Question Answering (VQA) is an extremely stimulating and challenging research area where Computer Vision (CV) and Natural Language Processig (NLP) have recently met. In image captioning and video summarization, the semantic information is completely contained in still images or video dynamics, and it has only to be mined and expressed in a human-consistent way. Differently from this, in VQA semantic information in the same media must be compared with the semantics implied by a question expressed in natural language, doubling the artificial intelligence-related effort. Some recent surveys about VQA approaches have focused on methods underlying either the image-related processing or the verbal-related one, or on the way to consistently fuse the conveyed information. Possible applications are only suggested, and, in fact, most cited works rely on general-purpose datasets that are used to assess the building blocks of a VQA system. This paper rather considers the proposals that focus on real-world applications, possibly using as benchmarks suitable data bound to the application domain. The paper also reports about some recent challenges in VQA research. △ Less","4 March, 2021",https://arxiv.org/pdf/2103.02937
"Toward Native Artificial Intelligence in 6G Networks: System Design, Architectures, and Paradigms",Jianjun Wu;Rongpeng Li;Xueli An;Chenghui Peng;Zhe Liu;Jon Crowcroft;Honggang Zhang,"The mobile communication system has transformed to be the fundamental infrastructure to support digital demands from all industry sectors, and 6G is envisioned to go far beyond the communication-only purpose. There is coming to a consensus that 6G will treat Artificial Intelligence (AI) as the cornerstone and has a potential capability to provide ""intelligence inclusion"", which implies to enable the access of AI services at anytime and anywhere by anyone. Apparently, the intelligent inclusion vision produces far-reaching influence on the corresponding network architecture design in 6G and deserves a clean-slate rethink. In this article, we propose an end-to-end system architecture design scope for 6G, and talk about the necessity to incorporate an independent data plane and a novel intelligent plane with particular emphasis on end-to-end AI workflow orchestration, management and operation. We also highlight the advantages to provision converged connectivity and computing services at the network function plane. Benefiting from these approaches, we believe that 6G will turn to an ""everything as a service"" (XaaS) platform with significantly enhanced business merits. △ Less","3 March, 2021",https://arxiv.org/pdf/2103.02823
A toolbox for neuromorphic sensing in robotics,Julien Dupeyroux;Stein Stroobants;Guido de Croon,"The third generation of artificial intelligence (AI) introduced by neuromorphic computing is revolutionizing the way robots and autonomous systems can sense the world, process the information, and interact with their environment. The promises of high flexibility, energy efficiency, and robustness of neuromorphic systems is widely supported by software tools for simulating spiking neural networks, and hardware integration (neuromorphic processors). Yet, while efforts have been made on neuromorphic vision (event-based cameras), it is worth noting that most of the sensors available for robotics remain inherently incompatible with neuromorphic computing, where information is encoded into spikes. To facilitate the use of traditional sensors, we need to convert the output signals into streams of spikes, i.e., a series of events (+1, -1) along with their corresponding timestamps. In this paper, we propose a review of the coding algorithms from a robotics perspective and further supported by a benchmark to assess their performance. We also introduce a ROS (Robot Operating System) toolbox to encode and decode input signals coming from any type of sensor available on a robot. This initiative is meant to stimulate and facilitate robotic integration of neuromorphic AI, with the opportunity to adapt traditional off-the-shelf sensors to spiking neural nets within one of the most powerful robotic tools, ROS. △ Less","5 October, 2021",https://arxiv.org/pdf/2103.02751
Optimal Kidney Exchange with Immunosuppressants,Haris Aziz;Agnes Cseh;John P. Dickerson;Duncan C. McElfresh,"Algorithms for exchange of kidneys is one of the key successful applications in market design, artificial intelligence, and operations research. Potent immunosuppressant drugs suppress the body's ability to reject a transplanted organ up to the point that a transplant across blood- or tissue-type incompatibility becomes possible. In contrast to the standard kidney exchange problem, we consider a setting that also involves the decision about which recipients receive from the limited supply of immunosuppressants that make them compatible with originally incompatible kidneys. We firstly present a general computational framework to model this problem. Our main contribution is a range of efficient algorithms that provide flexibility in terms of meeting meaningful objectives. Motivated by the current reality of kidney exchanges using sophisticated mathematical-programming-based clearing algorithms, we then present a general but scalable approach to optimal clearing with immunosuppression; we validate our approach on realistic data from a large fielded exchange. △ Less","3 March, 2021",https://arxiv.org/pdf/2103.02253
Convergence and Inequality in Research Globalization,Saurabh Mishra;Kuansan Wang,"The catch-up effect and the Matthew effect offer opposing characterizations of globalization: the former predicts an eventual convergence as the poor can grow faster than the rich due to free exchanges of complementary resources, while the latter, a deepening inequality between the rich and the poor. To understand these effects on the globalization of research, we conduct an in-depth study based on scholarly and patent publications covering STEM research from 218 countries/regions over the past four decades, covering more than 55 million scholarly articles and 1.7 billion citations. Unique to this investigation is the simultaneous examination of both the research output and its impact in the same data set, using a novel machine learning based measure, called saliency, to mitigate the intrinsic biases in quantifying the research impact. The results show that the two effects are in fact co-occurring: there are clear indications of convergence among the high income and upper middle income countries across the STEM fields, but a widening gap is developing that segregates the lower middle and low income regions from the higher income regions. Furthermore, the rate of convergence varies notably among the STEM sub-fields, with the highly strategic area of Artificial Intelligence (AI) sandwiched between fields such as Medicine and Materials Science that occupy the opposite ends of the spectrum. The data support the argument that a leading explanation of the Matthew effect, namely, the preferential attachment theory, can actually foster the catch-up effect when organizations from lower income countries forge substantial research collaborations with those already dominant. The data resoundingly show such collaborations benefit all parties involved, and a case of role reversal can be seen in the Materials Science field where the most advanced signs of convergence are observed. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.02052
Medical Imaging and Machine Learning,Rohan Shad;John P. Cunningham;Euan A. Ashley;Curtis P. Langlotz;William Hiesinger,"Advances in computing power, deep learning architectures, and expert labelled datasets have spurred the development of medical imaging artificial intelligence systems that rival clinical experts in a variety of scenarios. The National Institutes of Health in 2018 identified key focus areas for the future of artificial intelligence in medical imaging, creating a foundational roadmap for research in image acquisition, algorithms, data standardization, and translatable clinical decision support systems. Among the key issues raised in the report: data availability, need for novel computing architectures and explainable AI algorithms, are still relevant despite the tremendous progress made over the past few years alone. Furthermore, translational goals of data sharing, validation of performance for regulatory approval, generalizability and mitigation of unintended bias must be accounted for early in the development process. In this perspective paper we explore challenges unique to high dimensional clinical imaging data, in addition to highlighting some of the technical and ethical considerations in developing high-dimensional, multi-modality, machine learning systems for clinical decision support. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.01938
Virufy: A Multi-Branch Deep Learning Network for Automated Detection of COVID-19,Ahmed Fakhry;Xinyi Jiang;Jaclyn Xiao;Gunvant Chaudhari;Asriel Han;Amil Khanzada,"Fast and affordable solutions for COVID-19 testing are necessary to contain the spread of the global pandemic and help relieve the burden on medical facilities. Currently, limited testing locations and expensive equipment pose difficulties for individuals trying to be tested, especially in low-resource settings. Researchers have successfully presented models for detecting COVID-19 infection status using audio samples recorded in clinical settings [5, 15], suggesting that audio-based Artificial Intelligence models can be used to identify COVID-19. Such models have the potential to be deployed on smartphones for fast, widespread, and low-resource testing. However, while previous studies have trained models on cleaned audio samples collected mainly from clinical settings, audio samples collected from average smartphones may yield suboptimal quality data that is different from the clean data that models were trained on. This discrepancy may add a bias that affects COVID-19 status predictions. To tackle this issue, we propose a multi-branch deep learning network that is trained and tested on crowdsourced data where most of the data has not been manually processed and cleaned. Furthermore, the model achieves state-of-art results for the COUGHVID dataset [16]. After breaking down results for each category, we have shown an AUC of 0.99 for audio samples with COVID-19 positive labels. △ Less","16 March, 2021",https://arxiv.org/pdf/2103.01806
IoT-Enabled Social Relationships Meet Artificial Social Intelligence,Sahraoui Dhelim;Huansheng Ning;Fadi Farha;Liming Chen;Luigi Atzori;Mahmoud Daneshmand,"With the recent advances of the Internet of Things, and the increasing accessibility of ubiquitous computing resources and mobile devices, the prevalence of rich media contents, and the ensuing social, economic, and cultural changes, computing technology and applications have evolved quickly over the past decade. They now go beyond personal computing, facilitating collaboration and social interactions in general, causing a quick proliferation of social relationships among IoT entities. The increasing number of these relationships and their heterogeneous social features have led to computing and communication bottlenecks that prevent the IoT network from taking advantage of these relationships to improve the offered services and customize the delivered content, known as relationship explosion. On the other hand, the quick advances in artificial intelligence applications in social computing have led to the emerging of a promising research field known as Artificial Social Intelligence (ASI) that has the potential to tackle the social relationship explosion problem. This paper discusses the role of IoT in social relationships detection and management, the problem of social relationships explosion in IoT and reviews the proposed solutions using ASI, including social-oriented machine-learning and deep-learning techniques. △ Less","25 May, 2021",https://arxiv.org/pdf/2103.01776
Sparse Training Theory for Scalable and Efficient Agents,Decebal Constantin Mocanu;Elena Mocanu;Tiago Pinto;Selima Curci;Phuong H. Nguyen;Madeleine Gibescu;Damien Ernst;Zita A. Vale,"A fundamental task for artificial intelligence is learning. Deep Neural Networks have proven to cope perfectly with all learning paradigms, i.e. supervised, unsupervised, and reinforcement learning. Nevertheless, traditional deep learning approaches make use of cloud computing facilities and do not scale well to autonomous agents with low computational resources. Even in the cloud, they suffer from computational and memory limitations, and they cannot be used to model adequately large physical worlds for agents which assume networks with billions of neurons. These issues are addressed in the last few years by the emerging topic of sparse training, which trains sparse networks from scratch. This paper discusses sparse training state-of-the-art, its challenges and limitations while introducing a couple of new theoretical research directions which has the potential of alleviating sparse training limitations to push deep learning scalability well beyond its current boundaries. Nevertheless, the theoretical advancements impact in complex multi-agents settings is discussed from a real-world perspective, using the smart grid case study. △ Less","2 March, 2021",https://arxiv.org/pdf/2103.01636
Natural interaction with traffic control cameras through multimodal interfaces,Marco Grazioso;Alessandro Sebastian Podda;Silvio Barra;Francesco Cutugno,"Human-Computer Interfaces have always played a fundamental role in usability and commands' interpretability of the modern software systems. With the explosion of the Artificial Intelligence concept, such interfaces have begun to fill the gap between the user and the system itself, further evolving in Adaptive User Interfaces (AUI). Meta Interfaces are a further step towards the user, and they aim at supporting the human activities in an ambient interactive space; in such a way, the user can control the surrounding space and interact with it. This work aims at proposing a meta user interface that exploits the Put That There paradigm to enable the user to fast interaction by employing natural language and gestures. The application scenario is a video surveillance control room, in which the speed of actions and reactions is fundamental for urban safety and driver and pedestrian security. The interaction is oriented towards three environments: the first is the control room itself, in which the operator can organize the views of the monitors related to the cameras on site by vocal commands and gestures, as well as conveying the audio on the headset or in the speakers of the room. The second one is related to the control of the video, in order to go back and forth to a particular scene showing specific events, or zoom in/out a particular camera; the third allows the operator to send rescue vehicle in a particular street, in case of need. The gestures data are acquired through a Microsoft Kinect 2 which captures pointing and gestures allowing the user to interact multimodally thus increasing the naturalness of the interaction; the related module maps the movement information to a particular instruction, also supported by vocal commands which enable its execution. (cont...) △ Less","2 March, 2021",https://arxiv.org/pdf/2103.01518
Interpretable Hyperspectral AI: When Non-Convex Modeling meets Hyperspectral Remote Sensing,Danfeng Hong;Wei He;Naoto Yokoya;Jing Yao;Lianru Gao;Liangpei Zhang;Jocelyn Chanussot;Xiao Xiang Zhu,"Hyperspectral imaging, also known as image spectrometry, is a landmark technique in geoscience and remote sensing (RS). In the past decade, enormous efforts have been made to process and analyze these hyperspectral (HS) products mainly by means of seasoned experts. However, with the ever-growing volume of data, the bulk of costs in manpower and material resources poses new challenges on reducing the burden of manual labor and improving efficiency. For this reason, it is, therefore, urgent to develop more intelligent and automatic approaches for various HS RS applications. Machine learning (ML) tools with convex optimization have successfully undertaken the tasks of numerous artificial intelligence (AI)-related applications. However, their ability in handling complex practical problems remains limited, particularly for HS data, due to the effects of various spectral variabilities in the process of HS imaging and the complexity and redundancy of higher dimensional HS signals. Compared to the convex models, non-convex modeling, which is capable of characterizing more complex real scenes and providing the model interpretability technically and theoretically, has been proven to be a feasible solution to reduce the gap between challenging HS vision tasks and currently advanced intelligent data processing models. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01449
Noncoding RNAs and deep learning neural network discriminate multi-cancer types,Anyou Wang;Rong Hai;Paul J Rider;Qianchuan He,"Detecting cancers at early stages can dramatically reduce mortality rates. Therefore, practical cancer screening at the population level is needed. Here, we develop a comprehensive detection system to classify all common cancer types. By integrating artificial intelligence deep learning neural network and noncoding RNA biomarkers selected from massive data, our system can accurately detect cancer vs healthy object with 96.3% of AUC of ROC (Area Under Curve of a Receiver Operating Characteristic curve). Intriguinely, with no more than 6 biomarkers, our approach can easily discriminate any individual cancer type vs normal with 99% to 100% AUC. Furthermore, a comprehensive marker panel can simultaneously multi-classify all common cancers with a stable 78% of accuracy at heterological cancerous tissues and conditions. This provides a valuable framework for large scale cancer screening. The AI models and plots of results were available in https://combai.org/ai/cancerdetection/ △ Less","26 March, 2021",https://arxiv.org/pdf/2103.01179
Reflections on the Clinical Acceptance of Artificial Intelligence,Jens Schneider;Marco Agus,"In this chapter, we reflect on the use of Artificial Intelligence (AI) and its acceptance in clinical environments. We develop a general view of hindrances for clinical acceptance in the form of a pipeline model combining AI and clinical practise. We then link each challenge to the relevant stage in the pipeline and discuss the necessary requirements in order to overcome each challenge. We complement this discussion with an overview of opportunities for AI, which we currently see at the periphery of clinical workflows. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01149
Extending Prolog for Quantified Boolean Horn Formulas,Anish Mallick;Anil Shukla,"Prolog is a well known declarative programming language based on propositional Horn formulas. It is useful in various areas, including artificial intelligence, automated theorem proving, mathematical logic and so on. An active research area for many years is to extend Prolog to larger classes of logic. Some important extensions of it includes the constraint logic programming, and the object oriented logic programming. However, it cannot solve problems having arbitrary quantified Horn formulas. To be precise, the facts, rules and queries in Prolog are not allowed to have arbitrary quantified variables. The paper overcomes this major limitations of Prolog by extending it for the quantified Boolean Horn formulas. We achieved this by extending the SLD-resolution proof system for quantified Boolean Horn formulas, followed by proposing an efficient model for implementation. The paper shows that the proposed implementation also supports the first-order predicate Horn logic with arbitrary quantified variables. The paper also introduces for the first time, a declarative programming for the quantified Boolean Horn formulas. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01046
Machine learning on small size samples: A synthetic knowledge synthesis,Peter Kokol;Marko Kokol;Sašo Zagoranski,"One of the increasingly important technologies dealing with the growing complexity of the digitalization of almost all human activities is Artificial intelligence, more precisely machine learning Despite the fact, that we live in a Big data world where almost everything is digitally stored, there are many real-world situations, where researchers are faced with small data samples. The present study aim is to answer the following research question namely What is the small data problem in machine learning and how it is solved?. Our bibliometric study showed a positive trend in the number of research publications concerning the use of small datasets and substantial growth of the research community dealing with the small dataset problem, indicating that the research field is moving toward higher maturity levels. Despite notable international cooperation, the regional concentration of research literature production in economically more developed countries was observed. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.01002
Explainable AI in Credit Risk Management,Branka Hadji Misheva;Joerg Osterrieder;Ali Hirsa;Onkar Kulkarni;Stephen Fung Lin,"Artificial Intelligence (AI) has created the single biggest technology revolution the world has ever seen. For the finance sector, it provides great opportunities to enhance customer experience, democratize financial services, ensure consumer protection and significantly improve risk management. While it is easier than ever to run state-of-the-art machine learning models, designing and implementing systems that support real-world finance applications have been challenging. In large part because they lack transparency and explainability which are important factors in establishing reliable technology and the research on this topic with a specific focus on applications in credit risk management. In this paper, we implement two advanced post-hoc model agnostic explainability techniques called Local Interpretable Model Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) to machine learning (ML)-based credit scoring models applied to the open-access data set offered by the US-based P2P Lending Platform, Lending Club. Specifically, we use LIME to explain instances locally and SHAP to get both local and global explanations. We discuss the results in detail and present multiple comparison scenarios by using various kernels available for explaining graphs generated using SHAP values. We also discuss the practical challenges associated with the implementation of these state-of-art eXplainabale AI (XAI) methods and document them for future reference. We have made an effort to document every technical aspect of this research, while at the same time providing a general summary of the conclusions. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.00949
Rethinking complexity for software code structures: A pioneering study on Linux kernel code repository,Wenhe Zhang;Jin He;Kevin Song,"The recent progress of artificial intelligence(AI) has shown great potentials for alleviating human burden in various complex tasks. From the view of software engineering, AI techniques can be seen in many fundamental aspects of development, such as source code comprehension, in which state-of-the-art models are implemented to extract and express the meaning of code snippets automatically. However, such technologies are still struggling to tackle and comprehend the complex structures within industrial code, thus far from real-world applications. In the present work, we built an innovative and systematical framework, emphasizing the problem of complexity in code comprehension and further software engineering. Upon automatic data collection from the latest Linux kernel source code, we modeled code structures as complex networks through token extraction and relation parsing. Comprehensive analysis of complexity further revealed the density and scale of network-based code representations. Our work constructed the first large-scale dataset from industrial-strength software code for downstream software engineering tasks including code comprehension, and incorporated complex network theory into code-level investigations of software development for the first time. In the longer term, the proposed methodology could play significant roles in the entire software engineering process, powering software design, coding, debugging, testing, and sustaining by redefining and embracing complexity. △ Less","1 March, 2021",https://arxiv.org/pdf/2103.00821
"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence",Atoosa Kasirzadeh,"The societal and ethical implications of the use of opaque artificial intelligence systems for consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholder groups, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by (1) identifying the types of explanations that are most pertinent to artificial intelligence predictions, (2) recognizing the relevance and importance of social and ethical values for the evaluation of these explanations, and (3) demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems. △ Less","28 February, 2021",https://arxiv.org/pdf/2103.00752
Digital History and History Teaching in the Digital Age,Maria Papadopoulou;Zacharoula Smyrnaiou,"Digital technologies, such as the Internet and Artificial Intelligence, are part of our daily lives, influencing broader aspects of our way of life, as well as the way we interact with the past. Having dramatically changed the ways in which knowledge is produced and consumed, the algorithmic age has also radically changed the relationship that the general public has with History. Fields of History such as Public and Oral History have particularly benefitted from the rise of digital culture. How does our digital culture affect the way we think, study, research and teach the past, as historical evidence spreads rapidly in the public sphere? How do digital technologies promote the study, writing and teaching of History? What should historians, students of history and pre-service history teachers be critically aware of, when swarmed with digitized or born-digital content, constantly growing on the Internet? And while these changes are now visible globally, how is the discipline of History situated within the digital transformation rapidly advancing in Greece? Finally, what are the consequences of these changes for History as a subject taught at Greek secondary schools? These are some of the issues raised in the text that follows, which is part of the course materials of the undergraduate course offered during winter semester 2020-2021 at the School University of Athens, School of Philosophy, Pedagogy, Psychology. Course Title: 'Pedagogics of History: Theory and Practice', Academic Institution: School of Philosophy-Pedagogy-Psychology, University of Athens. △ Less","28 February, 2021",https://arxiv.org/pdf/2103.00473
Predicting post-operative right ventricular failure using video-based deep learning,Rohan Shad;Nicolas Quach;Robyn Fong;Patpilai Kasinpila;Cayley Bowles;Miguel Castro;Ashrith Guha;Eddie Suarez;Stefan Jovinge;Sangjin Lee;Theodore Boeve;Myriam Amsallem;Xiu Tang;Francois Haddad;Yasuhiro Shudo;Y. Joseph Woo;Jeffrey Teuteberg;John P. Cunningham;Curt P. Langlotz;William Hiesinger,"Non-invasive and cost effective in nature, the echocardiogram allows for a comprehensive assessment of the cardiac musculature and valves. Despite progressive improvements over the decades, the rich temporally resolved data in echocardiography videos remain underutilized. Human reads of echocardiograms reduce the complex patterns of cardiac wall motion, to a small list of measurements of heart function. Furthermore, all modern echocardiography artificial intelligence (AI) systems are similarly limited by design - automating measurements of the same reductionist metrics rather than utilizing the wealth of data embedded within each echo study. This underutilization is most evident in situations where clinical decision making is guided by subjective assessments of disease acuity, and tools that predict disease onset within clinically actionable timeframes are unavailable. Predicting the likelihood of developing post-operative right ventricular failure (RV failure) in the setting of mechanical circulatory support is one such clinical example. To address this, we developed a novel video AI system trained to predict post-operative right ventricular failure (RV failure), using the full spatiotemporal density of information from pre-operative echocardiography scans. We achieve an AUC of 0.729, specificity of 52% at 80% sensitivity and 46% sensitivity at 80% specificity. Furthermore, we show that our ML system significantly outperforms a team of human experts tasked with predicting RV failure on independent clinical evaluation. Finally, the methods we describe are generalizable to any cardiac clinical decision support application where treatment or patient selection is guided by qualitative echocardiography assessments. △ Less","27 February, 2021",https://arxiv.org/pdf/2103.00364
CXR-Net: An Artificial Intelligence Pipeline for Quick Covid-19 Screening of Chest X-Rays,Haikal Abdulah;Benjamin Huber;Sinan Lal;Hassan Abdallah;Luigi L. Palese;Hamid Soltanian-Zadeh;Domenico L. Gatti,"CXR-Net is a two-module Artificial Intelligence pipeline for the quick detection of SARS-CoV-2 from chest X-rays (CXRs). Module 1 was trained on a public dataset of 6395 CXRs with radiologist annotated lung contours to generate masks of the lungs that overlap the heart and large vasa. Module 2 is a hybrid convnet in which the first convolutional layer with learned coefficients is replaced by a layer with fixed coefficients provided by the Wavelet Scattering Transform (WST). Module 2 takes as inputs the patients CXRs and corresponding lung masks calculated by Module 1, and produces as outputs a class assignment (Covid vs. non-Covid) and high resolution heat maps that identify the SARS associated lung regions. Module 2 was trained on a dataset of CXRs from non-Covid and RT-PCR confirmed Covid patients acquired at the Henry Ford Health System (HFHS) Hospital in Detroit. All non-Covid CXRs were from pre-Covid era (2018-2019), and included images from both normal lungs and lungs affected by non-Covid pathologies. Training and test sets consisted of 2265 CXRs (1417 Covid negative, 848 Covid positive), and 1532 CXRs (945 Covid negative, 587 Covid positive), respectively. Six distinct cross-validation models, each trained on 1887 images and validated against 378 images, were combined into an ensemble model that was used to classify the CXR images of the test set with resulting Accuracy = 0.789, Precision = 0.739, Recall = 0.693, F1 score = 0.715, ROC(AUC) = 0.852. △ Less","26 February, 2021",https://arxiv.org/pdf/2103.00087
Evolution of collective fairness in complex networks through degree-based role assignment,Andreia Sofia Teixeira;Francisco C. Santos;Alexandre P. Francisco;Fernando P. Santos,"From social contracts to climate agreements, individuals engage in groups that must collectively reach decisions with varying levels of equality and fairness. These dilemmas also pervade Distributed Artificial Intelligence, in domains such as automated negotiation, conflict resolution or resource allocation. As evidenced by the well-known Ultimatum Game -- where a Proposer has to divide a resource with a Responder -- payoff-maximizing outcomes are frequently at odds with fairness. Eliciting equality in populations of self-regarding agents requires judicious interventions. Here we use knowledge about agents' social networks to implement fairness mechanisms, in the context of Multiplayer Ultimatum Games. We focus on network-based role assignment and show that preferentially attributing the role of Proposer to low-connected nodes increases the fairness levels in a population. We evaluate the effectiveness of low-degree Proposer assignment considering networks with different average connectivity, group sizes, and group voting rules when accepting proposals (e.g. majority or unanimity). We further show that low-degree Proposer assignment is efficient, not only optimizing fairness, but also the average payoff level in the population. Finally, we show that stricter voting rules (i.e., imposing an accepting consensus as requirement for collectives to accept a proposal) attenuates the unfairness that results from situations where high-degree nodes (hubs) are the natural candidates to play as Proposers. Our results suggest new routes to use role assignment and voting mechanisms to prevent unfair behaviors from spreading on complex networks. △ Less","26 February, 2021",https://arxiv.org/pdf/2102.13597
Benchmarking and Survey of Explanation Methods for Black Box Models,Francesco Bodria;Fosca Giannotti;Riccardo Guidotti;Francesca Naretto;Dino Pedreschi;Salvatore Rinzivillo,"The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking. △ Less","25 February, 2021",https://arxiv.org/pdf/2102.13076
TELESTO: A Graph Neural Network Model for Anomaly Classification in Cloud Services,Dominik Scheinert;Alexander Acker,"Deployment, operation and maintenance of large IT systems becomes increasingly complex and puts human experts under extreme stress when problems occur. Therefore, utilization of machine learning (ML) and artificial intelligence (AI) is applied on IT system operation and maintenance - summarized in the term AIOps. One specific direction aims at the recognition of re-occurring anomaly types to enable remediation automation. However, due to IT system specific properties, especially their frequent changes (e.g. software updates, reconfiguration or hardware modernization), recognition of reoccurring anomaly types is challenging. Current methods mainly assume a static dimensionality of provided data. We propose a method that is invariant to dimensionality changes of given data. Resource metric data such as CPU utilization, allocated memory and others are modelled as multivariate time series. The extraction of temporal and spatial features together with the subsequent anomaly classification is realized by utilizing TELESTO, our novel graph convolutional neural network (GCNN) architecture. The experimental evaluation is conducted in a real-world cloud testbed deployment that is hosting two applications. Classification results of injected anomalies on a cassandra database node show that TELESTO outperforms the alternative GCNNs and achieves an overall classification accuracy of 85.1%. Classification results for the other nodes show accuracy values between 85% and 60%. △ Less","29 July, 2021",https://arxiv.org/pdf/2102.12877
"Modular Object-Oriented Games: A Task Framework for Reinforcement Learning, Psychology, and Neuroscience",Nicholas Watters;Joshua Tenenbaum;Mehrdad Jazayeri,"In recent years, trends towards studying simulated games have gained momentum in the fields of artificial intelligence, cognitive science, psychology, and neuroscience. The intersections of these fields have also grown recently, as researchers increasing study such games using both artificial agents and human or animal subjects. However, implementing games can be a time-consuming endeavor and may require a researcher to grapple with complex codebases that are not easily customized. Furthermore, interdisciplinary researchers studying some combination of artificial intelligence, human psychology, and animal neurophysiology face additional challenges, because existing platforms are designed for only one of these domains. Here we introduce Modular Object-Oriented Games, a Python task framework that is lightweight, flexible, customizable, and designed for use by machine learning, psychology, and neurophysiology researchers. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12616
"A Large-Scale, Automated Study of Language Surrounding Artificial Intelligence",Autumn Toney,"This work presents a large-scale analysis of artificial intelligence (AI) and machine learning (ML) references within news articles and scientific publications between 2011 and 2019. We implement word association measurements that automatically identify shifts in language co-occurring with AI/ML and quantify the strength of these word associations. Our results highlight the evolution of perceptions and definitions around AI/ML and detect emerging application areas, models, and systems (e.g., blockchain and cybersecurity). Recent small-scale, manual studies have explored AI/ML discourse within the general public, the policymaker community, and researcher community, but are limited in their scalability and longevity. Our methods provide new views into public perceptions and subject-area expert discussions of AI/ML and greatly exceed the explanative power of prior work. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12516
Actionable Principles for Artificial Intelligence Policy: Three Pathways,Charlotte Stix,"In the development of governmental policy for artificial intelligence (AI) that is informed by ethics, one avenue currently pursued is that of drawing on AI Ethics Principles. However, these AI Ethics Principles often fail to be actioned in governmental policy. This paper proposes a novel framework for the development of Actionable Principles for AI. The approach acknowledges the relevance of AI Ethics Principles and homes in on methodological elements to increase their practical implementability in policy processes. As a case study, elements are extracted from the development process of the Ethics Guidelines for Trustworthy AI of the European Commissions High Level Expert Group on AI. Subsequently, these elements are expanded on and evaluated in light of their ability to contribute to a prototype framework for the development of Actionable Principles for AI. The paper proposes the following three propositions for the formation of such a prototype framework: (1) preliminary landscape assessments; (2) multi-stakeholder participation and cross-sectoral feedback; and, (3) mechanisms to support implementation and operationalizability. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12406
Wirelessly Powered Federated Edge Learning: Optimal Tradeoffs Between Convergence and Power Transfer,Qunsong Zeng;Yuqing Du;Kaibin Huang,"Federated edge learning (FEEL) is a widely adopted framework for training an artificial intelligence (AI) model distributively at edge devices to leverage their data while preserving their data privacy. The execution of a power-hungry learning task at energy-constrained devices is a key challenge confronting the implementation of FEEL. To tackle the challenge, we propose the solution of powering devices using wireless power transfer (WPT). To derive guidelines on deploying the resultant wirelessly powered FEEL (WP-FEEL) system, this work aims at the derivation of the tradeoff between the model convergence and the settings of power sources in two scenarios: 1) the transmission power and density of power-beacons (dedicated charging stations) if they are deployed, or otherwise 2) the transmission power of a server (access-point). The development of the proposed analytical framework relates the accuracy of distributed stochastic gradient estimation to the WPT settings, the randomness in both communication and WPT links, and devices' computation capacities. Furthermore, the local-computation at devices (i.e., mini-batch size and processor clock frequency) is optimized to efficiently use the harvested energy for gradient estimation. The resultant learning-WPT tradeoffs reveal the simple scaling laws of the model-convergence rate with respect to the transferred energy as well as the devices' computational energy efficiencies. The results provide useful guidelines on WPT provisioning to provide a guaranteer on learning performance. They are corroborated by experimental results using a real dataset. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12357
"Functional neural network for decision processing, a racing network of programmable neurons with fuzzy logic where the target operating model relies on the network itself",Frederic Jumelle;Kelvin So;Didan Deng,"In this paper, we are introducing a novel model of artificial intelligence, the functional neural network for modeling of human decision-making processes. This neural network is composed of multiple artificial neurons racing in the network. Each of these neurons has a similar structure programmed independently by the users and composed of an intention wheel, a motor core and a sensory core representing the user itself and racing at a specific velocity. The mathematics of the neuron's formulation and the racing mechanism of multiple nodes in the network will be discussed, and the group decision process with fuzzy logic and the transformation of these conceptual methods into practical methods of simulation and in operations will be developed. Eventually, we will describe some possible future research directions in the fields of finance, education and medicine including the opportunity to design an intelligent learning agent with application in business operations supervision. We believe that this functional neural network has a promising potential to transform the way we can compute decision-making and lead to a new generation of neuromorphic chips for seamless human-machine interactions. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12339
Efficient Low-Latency Dynamic Licensing for Deep Neural Network Deployment on Edge Devices,Toan Pham Van;Ngoc N. Tran;Hoang Pham Minh;Tam Nguyen Minh;Thanh Ta Minh,"Along with the rapid development in the field of artificial intelligence, especially deep learning, deep neural network applications are becoming more and more popular in reality. To be able to withstand the heavy load from mainstream users, deployment techniques are essential in bringing neural network models from research to production. Among the two popular computing topologies for deploying neural network models in production are cloud-computing and edge-computing. Recent advances in communication technologies, along with the great increase in the number of mobile devices, has made edge-computing gradually become an inevitable trend. In this paper, we propose an architecture to solve deploying and processing deep neural networks on edge-devices by leveraging their synergy with the cloud and the access-control mechanisms of the database. Adopting this architecture allows low-latency DNN model updates on devices. At the same time, with only one model deployed, we can easily make different versions of it by setting access permissions on the model weights. This method allows for dynamic model licensing, which benefits commercial applications. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12165
From Universal Language Model to Downstream Task: Improving RoBERTa-Based Vietnamese Hate Speech Detection,Quang Huu Pham;Viet Anh Nguyen;Linh Bao Doan;Ngoc N. Tran;Ta Minh Thanh,"Natural language processing is a fast-growing field of artificial intelligence. Since the Transformer was introduced by Google in 2017, a large number of language models such as BERT, GPT, and ELMo have been inspired by this architecture. These models were trained on huge datasets and achieved state-of-the-art results on natural language understanding. However, fine-tuning a pre-trained language model on much smaller datasets for downstream tasks requires a carefully-designed pipeline to mitigate problems of the datasets such as lack of training data and imbalanced data. In this paper, we propose a pipeline to adapt the general-purpose RoBERTa language model to a specific text classification task: Vietnamese Hate Speech Detection. We first tune the PhoBERT on our dataset by re-training the model on the Masked Language Model task; then, we employ its encoder for text classification. In order to preserve pre-trained weights while learning new feature representations, we further utilize different training techniques: layer freezing, block-wise learning rate, and label smoothing. Our experiments proved that our proposed pipeline boosts the performance significantly, achieving a new state-of-the-art on Vietnamese Hate Speech Detection campaign with 0.7221 F1 score. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12162
Perspective: Purposeful Failure in Artificial Life and Artificial Intelligence,Lana Sinapayen,"Complex systems fail. I argue that failures can be a blueprint characterizing living organisms and biological intelligence, a control mechanism to increase complexity in evolutionary simulations, and an alternative to classical fitness optimization. Imitating biological successes in Artificial Life and Artificial Intelligence can be misleading; imitating failures offers a path towards understanding and emulating life it in artificial systems. △ Less","24 February, 2021",https://arxiv.org/pdf/2102.12076
Artificial Intelligence as an Anti-Corruption Tool (AI-ACT) -- Potentials and Pitfalls for Top-down and Bottom-up Approaches,Nils Köbis;Christopher Starke;Iyad Rahwan,"Corruption continues to be one of the biggest societal challenges of our time. New hope is placed in Artificial Intelligence (AI) to serve as an unbiased anti-corruption agent. Ever more available (open) government data paired with unprecedented performance of such algorithms render AI the next frontier in anti-corruption. Summarizing existing efforts to use AI-based anti-corruption tools (AI-ACT), we introduce a conceptual framework to advance research and policy. It outlines why AI presents a unique tool for top-down and bottom-up anti-corruption approaches. For both approaches, we outline in detail how AI-ACT present different potentials and pitfalls for (a) input data, (b) algorithmic design, and (c) institutional implementation. Finally, we venture a look into the future and flesh out key questions that need to be addressed to develop AI-ACT while considering citizens' views, hence putting ""society in the loop"". △ Less","23 February, 2021",https://arxiv.org/pdf/2102.11567
Decision Rule Elicitation for Domain Adaptation,Alexander Nikitin;Samuel Kaski,"Human-in-the-loop machine learning is widely used in artificial intelligence (AI) to elicit labels for data points from experts or to provide feedback on how close the predicted results are to the target. This simplifies away all the details of the decision-making process of the expert. In this work, we allow the experts to additionally produce decision rules describing their decision-making; the rules are expected to be imperfect but to give additional information. In particular, the rules can extend to new distributions, and hence enable significantly improving performance for cases where the training and testing distributions differ, such as in domain adaptation. We apply the proposed method to lifelong learning and domain adaptation problems and discuss applications in other branches of AI, such as knowledge acquisition problems in expert systems. In simulated and real-user studies, we show that decision rule elicitation improves domain adaptation of the algorithm and helps to propagate expert's knowledge to the AI model. △ Less","23 February, 2021",https://arxiv.org/pdf/2102.11539
Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications using Emotional Intelligence,Falguni Patel;NirmalKumar Patel;Santosh Kumar Bharti,"Veracity is an essential key in research and development of innovative products. Live Emotion analysis and verification nullify deceit made to complainers on live chat, corroborate messages of both ends in messaging apps and promote an honest conversation between users. The main concept behind this emotion artificial intelligent verifier is to license or decline message accountability by comparing variegated emotions of chat app users recognized through facial expressions and text prediction. In this paper, a proposed emotion intelligent live detector acts as an honest arbiter who distributes facial emotions into labels namely, Happiness, Sadness, Surprise, and Hate. Further, it separately predicts a label of messages through text classification. Finally, it compares both labels and declares the message as a fraud or a bonafide. For emotion detection, we deployed Convolutional Neural Network (CNN) using a miniXception model and for text prediction, we selected Support Vector Machine (SVM) natural language processing probability classifier due to receiving the best accuracy on training dataset after applying Support Vector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and Logistic regression. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.11318
Software Architecture for Next-Generation AI Planning Systems,Sebastian Graef;Ilche Georgievski,"Artificial Intelligence (AI) planning is a flourishing research and development discipline that provides powerful tools for searching a course of action that achieves some user goal. While these planning tools show excellent performance on benchmark planning problems, they represent challenging software systems when it comes to their use and integration in real-world applications. In fact, even in-depth understanding of their internal mechanisms does not guarantee that one can successfully set up, use and manipulate existing planning tools. We contribute toward alleviating this situation by proposing a service-oriented planning architecture to be at the core of the ability to design, develop and use next-generation AI planning systems. We collect and classify common planning capabilities to form the building blocks of the planning architecture. We incorporate software design principles and patterns into the architecture to allow for usability, interoperability and reusability of the planning capabilities. Our prototype planning system demonstrates the potential of our approach for rapid prototyping and flexibility of system composition. Finally, we provide insight into the qualitative advantages of our approach when compared to a typical planning tool. △ Less","22 February, 2021",https://arxiv.org/pdf/2102.10985
Fair and Responsible AI: A Focus on the Ability to Contest,Henrietta Lyons;Eduardo Velloso;Tim Miller,"As the use of artificial intelligence (AI) in high-stakes decision-making increases, the ability to contest such decisions is being recognised in AI ethics guidelines as an important safeguard for individuals. Yet, there is little guidance on how AI systems can be designed to support contestation. In this paper we explain that the design of a contestation process is important due to its impact on perceptions of fairness and satisfaction. We also consider design challenges, including a lack of transparency as well as the numerous design options that decision-making entities will be faced with. We argue for a human-centred approach to designing for contestability to ensure that the needs of decision subjects, and the community, are met. △ Less","22 February, 2021",https://arxiv.org/pdf/2102.10787
Tchebichef Transform Domain-based Deep Learning Architecture for Image Super-resolution,Ahlad Kumar;Harsh Vardhan Singh,"The recent outbreak of COVID-19 has motivated researchers to contribute in the area of medical imaging using artificial intelligence and deep learning. Super-resolution (SR), in the past few years, has produced remarkable results using deep learning methods. The ability of deep learning methods to learn the non-linear mapping from low-resolution (LR) images to their corresponding high-resolution (HR) images leads to compelling results for SR in diverse areas of research. In this paper, we propose a deep learning based image super-resolution architecture in Tchebichef transform domain. This is achieved by integrating a transform layer into the proposed architecture through a customized Tchebichef convolutional layer (TCL). The role of TCL is to convert the LR image from the spatial domain to the orthogonal transform domain using Tchebichef basis functions. The inversion of the aforementioned transformation is achieved using another layer known as the Inverse Tchebichef convolutional Layer (ITCL), which converts back the LR images from the transform domain to the spatial domain. It has been observed that using the Tchebichef transform domain for the task of SR takes the advantage of high and low-frequency representation of images that makes the task of super-resolution simplified. We, further, introduce transfer learning approach to enhance the quality of Covid based medical images. It is shown that our architecture enhances the quality of X-ray and CT images of COVID-19, providing a better image quality that helps in clinical diagnosis. Experimental results obtained using the proposed Tchebichef transform domain super-resolution (TTDSR) architecture provides competitive results when compared with most of the deep learning methods employed using a fewer number of trainable parameters. △ Less","22 February, 2021",https://arxiv.org/pdf/2102.10640
AI-Augmented Behavior Analysis for Children with Developmental Disabilities: Building Towards Precision Treatment,Shadi Ghafghazi;Amarie Carnett;Leslie Neely;Arun Das;Paul Rad,"Autism spectrum disorder is a developmental disorder characterized by significant social, communication, and behavioral challenges. Individuals diagnosed with autism, intellectual, and developmental disabilities (AUIDD) typically require long-term care and targeted treatment and teaching. Effective treatment of AUIDD relies on efficient and careful behavioral observations done by trained applied behavioral analysts (ABAs). However, this process overburdens ABAs by requiring the clinicians to collect and analyze data, identify the problem behaviors, conduct pattern analysis to categorize and predict categorical outcomes, hypothesize responsiveness to treatments, and detect the effects of treatment plans. Successful integration of digital technologies into clinical decision-making pipelines and the advancements in automated decision-making using Artificial Intelligence (AI) algorithms highlights the importance of augmenting teaching and treatments using novel algorithms and high-fidelity sensors. In this article, we present an AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to provide personalized treatment and learning plans to AUIDD individuals. By defining systematic experiments along with automated data collection and analysis, AI-ABA can promote self-regulative behavior using reinforcement-based augmented or virtual reality and other mobile platforms. Thus, AI-ABA could assist clinicians to focus on making precise data-driven decisions and increase the quality of individualized interventions for individuals with AUIDD. △ Less","28 June, 2021",https://arxiv.org/pdf/2102.10635
Mapping Surgeon's Hand/Finger Motion During Conventional Microsurgery to Enhance Intuitive Surgical Robot Teleoperation,Mohammad Fattahi Sani;Raimondo Ascione;Sanja Dogramadzi,"Purpose: Recent developments in robotics and artificial intelligence (AI) have led to significant advances in healthcare technologies enhancing robot-assisted minimally invasive surgery (RAMIS) in some surgical specialties. However, current human-robot interfaces lack intuitive teleoperation and cannot mimic surgeon's hand/finger sensing and fine motion. These limitations make tele-operated robotic surgery not suitable for micro-surgery and difficult to learn for established surgeons. We report a pilot study showing an intuitive way of recording and mapping surgeon's gross hand motion and the fine synergic motion during cardiac micro-surgery as a way to enhance future intuitive teleoperation. Methods: We set to develop a prototype system able to train a Deep Neural Net-work (DNN) by mapping wrist, hand and surgical tool real-time data acquisition(RTDA) inputs during mock-up heart micro-surgery procedures. The trained network was used to estimate the tools poses from refined hand joint angles. Results: Based on surgeon's feedback during mock micro-surgery, the developed wearable system with light-weight sensors for motion tracking did not interfere with the surgery and instrument handling. The wearable motion tracking system used 15 finger-thumb-wrist joint angle sensors to generate meaningful data-sets representing inputs of the DNN network with new hand joint angles added as necessary based on comparing the estimated tool poses against measured tool pose. The DNN architecture was optimized for the highest estimation accuracy and the ability to determine the tool pose with the least mean squared error. This novel approach showed that the surgical instrument's pose, an essential requirement for teleoperation, can be accurately estimated from recorded surgeon's hand/finger movements with a mean squared error (MSE) less than 0.3% △ Less","21 February, 2021",https://arxiv.org/pdf/2102.10585
Customized Slicing for 6G: Enforcing Artificial Intelligence on Resource Management,Wanqing Guan;Haijun Zhang;Victor C. M. Leung,"Next generation wireless networks are expected to support diverse vertical industries and offer countless emerging use cases. To satisfy stringent requirements of diversified services, network slicing is developed, which enables service-oriented resource allocation by tailoring the infrastructure network into multiple logical networks. However, there are still some challenges in cross-domain multi-dimensional resource management for end-to-end (E2E) slices under the dynamic and uncertain environment. Trading off the revenue and cost of resource allocation while guaranteeing service quality is significant to tenants. Therefore, this article introduces a hierarchical resource management framework, utilizing deep reinforcement learning in admission control of resource requests from different tenants and resource adjustment within admitted slices for each tenant. Particularly, we first discuss the challenges in customized resource management of 6G. Second, the motivation and background are presented to explain why artificial intelligence (AI) is applied in resource customization of multi-tenant slicing. Third, E2E resource management is decomposed into two problems, multi-dimensional resource allocation decision based on slice-level feedback and real-time slice adaption aimed at avoiding service quality degradation. Simulation results demonstrate the effectiveness of AI-based customized slicing. Finally, several significant challenges that need to be addressed in practical implementation are investigated. △ Less","20 February, 2021",https://arxiv.org/pdf/2102.10498
Cybersecurity Awareness Platform with Virtual Coach and Automated Challenge Assessment,Tiago Espinha Gasiba;Ulrike Lechner;Maria Pinto-Albuquerque;Anmoal Porwal,"Over the last years, the number of cyber-attacks on industrial control systems has been steadily increasing. Among several factors, proper software development plays a vital role in keeping these systems secure. To achieve secure software, developers need to be aware of secure coding guidelines and secure coding best practices. This work presents a platform geared towards software developers in the industry that aims to increase awareness of secure software development. The authors also introduce an interactive game component, a virtual coach, which implements a simple artificial intelligence engine based on the laddering technique for interviews. Through a survey, a preliminary evaluation of the implemented artifact with real-world players (from academia and industry) shows a positive acceptance of the developed platform. Furthermore, the players agree that the platform is adequate for training their secure coding skills. The impact of our work is to introduce a new automatic challenge evaluation method together with a virtual coach to improve existing cybersecurity awareness training programs. These training workshops can be easily held remotely or off-line. △ Less","20 February, 2021",https://arxiv.org/pdf/2102.10430
Automated identification of transiting exoplanet candidates in NASA Transiting Exoplanets Survey Satellite (TESS) data with machine learning methods,Leon Ofman;Amir Averbuch;Adi Shliselberg;Idan Benaun;David Segev;Aron Rissman,"A novel artificial intelligence (AI) technique that uses machine learning (ML) methodologies combines several algorithms, which were developed by ThetaRay, Inc., is applied to NASA's Transiting Exoplanets Survey Satellite (TESS) dataset to identify exoplanetary candidates. The AI/ML ThetaRay system is trained initially with Kepler exoplanetary data and validated with confirmed exoplanets before its application to TESS data. Existing and new features of the data, based on various observational parameters, are constructed and used in the AI/ML analysis by employing semi-supervised and unsupervised machine learning techniques. By the application of ThetaRay system to 10,803 light curves of threshold crossing events (TCEs) produced by the TESS mission, obtained from the Mikulski Archive for Space Telescopes, the algorithm yields about 50 targets for further analysis, and we uncover three new exoplanetary candidates by further manual vetting. This study demonstrates for the first time the successful application of the particular combined multiple AI/ML-based methodologies to a large astrophysical dataset for rapid automated classification of TCEs. △ Less","27 August, 2021",https://arxiv.org/pdf/2102.10326
Artificial Intelligence Enhanced Rapid and Efficient Diagnosis of Mycoplasma Pneumoniae Pneumonia in Children Patients,Chenglin Pan;Kuan Yan;Xiao Liu;Yanjie Chen;Yanyan Luo;Xiaoming Li;Zhenguo Nie;Xinjun Liu,"Artificial intelligence methods have been increasingly turning into a potentially powerful tool in the diagnosis and management of diseases. In this study, we utilized logistic regression (LR), decision tree (DT), gradient boosted decision tree (GBDT), support vector machine (SVM), and multilayer perceptron (MLP) as machine learning models to rapidly diagnose the mycoplasma pneumoniae pneumonia (MPP) in children patients. The classification task was carried out after applying the preprocessing procedure to the MPP dataset. The most efficient results are obtained by GBDT. It provides the best performance with an accuracy of 93.7%. In contrast to standard raw feature weighting, the feature importance takes the underlying correlation structure of the features into account. The most crucial feature of GBDT is the ""pulmonary infiltrates range"" with a score of 0.5925, followed by ""cough"" (0.0953) and ""pleural effusion"" (0.0492). We publicly share our full implementation with the dataset and trained models at https://github.com/zhenguonie/2021_AI4MPP. △ Less","20 February, 2021",https://arxiv.org/pdf/2102.10284
Making a Case for Federated Learning in the Internet of Vehicles and Intelligent Transportation Systems,Dimitrios Michael Manias;Abdallah Shami,"With the incoming introduction of 5G networks and the advancement in technologies, such as Network Function Virtualization and Software Defined Networking, new and emerging networking technologies and use cases are taking shape. One such technology is the Internet of Vehicles (IoV), which describes an interconnected system of vehicles and infrastructure. Coupled with recent developments in artificial intelligence and machine learning, the IoV is transformed into an Intelligent Transportation System (ITS). There are, however, several operational considerations that hinder the adoption of ITS systems, including scalability, high availability, and data privacy. To address these challenges, Federated Learning, a collaborative and distributed intelligence technique, is suggested. Through an ITS case study, the ability of a federated model deployed on roadside infrastructure throughout the network to recover from faults by leveraging group intelligence while reducing recovery time and restoring acceptable system performance is highlighted. With a multitude of use cases and benefits, Federated Learning is a key enabler for ITS and is poised to achieve widespread implementation in 5G and beyond networks and applications. △ Less","19 February, 2021",https://arxiv.org/pdf/2102.10142
Image Classification using CNN for Traffic Signs in Pakistan,Abdul Azeem Sikander;Hamza Ali,"The autonomous automotive industry is one of the largest and most conventional projects worldwide, with many technology companies effectively designing and orienting their products towards automobile safety and accuracy. These products are performing very well over the roads in developed countries. But can fail in the first minute in an underdeveloped country because there is much difference between a developed country environment and an underdeveloped country environment. The following study proposed to train these Artificial intelligence models in environment space in an underdeveloped country like Pakistan. The proposed approach on image classification uses convolutional neural networks for image classification for the model. For model pre-training German traffic signs data set was selected then fine-tuned on Pakistan's dataset. The experimental setup showed the best results and accuracy from the previously conducted experiments. In this work to increase the accuracy, more dataset was collected to increase the size of images in every class in the data set. In the future, a low number of classes are required to be further increased where more images for traffic signs are required to be collected to get more accuracy on the training of the model over traffic signs of Pakistan's most used and popular roads motorway and national highway, whose traffic signs color, size, and shapes are different from common traffic signs. △ Less","19 February, 2021",https://arxiv.org/pdf/2102.10130
Deep learning-based COVID-19 pneumonia classification using chest CT images: model generalizability,Dan Nguyen;Fernando Kay;Jun Tan;Yulong Yan;Yee Seng Ng;Puneeth Iyengar;Ron Peshock;Steve Jiang,"Since the outbreak of the COVID-19 pandemic, worldwide research efforts have focused on using artificial intelligence (AI) technologies on various medical data of COVID-19-positive patients in order to identify or classify various aspects of the disease, with promising reported results. However, concerns have been raised over their generalizability, given the heterogeneous factors in training datasets. This study aims to examine the severity of this problem by evaluating deep learning (DL) classification models trained to identify COVID-19-positive patients on 3D computed tomography (CT) datasets from different countries. We collected one dataset at UT Southwestern (UTSW), and three external datasets from different countries: CC-CCII Dataset (China), COVID-CTset (Iran), and MosMedData (Russia). We divided the data into 2 classes: COVID-19-positive and COVID-19-negative patients. We trained nine identical DL-based classification models by using combinations of the datasets with a 72% train, 8% validation, and 20% test data split. The models trained on a single dataset achieved accuracy/area under the receiver operating characteristics curve (AUC) values of 0.87/0.826 (UTSW), 0.97/0.988 (CC-CCCI), and 0.86/0.873 (COVID-CTset) when evaluated on their own dataset. The models trained on multiple datasets and evaluated on a test set from one of the datasets used for training performed better. However, the performance dropped close to an AUC of 0.5 (random guess) for all models when evaluated on a different dataset outside of its training datasets. Including the MosMedData, which only contained positive labels, into the training did not necessarily help the performance on the other datasets. Multiple factors likely contribute to these results, such as patient demographics and differences in image acquisition or reconstruction, causing a data shift among different study cohorts. △ Less","18 February, 2021",https://arxiv.org/pdf/2102.09616
iX-BSP: Incremental Belief Space Planning,Elad I. Farhi;Vadim Indelman,"Deciding what's next? is a fundamental problem in robotics and Artificial Intelligence. Under belief space planning (BSP), in a partially observable setting, it involves calculating the expected accumulated belief-dependent reward, where the expectation is with respect to all future measurements. Since solving this general un-approximated problem quickly becomes intractable, state of the art approaches turn to approximations while still calculating planning sessions from scratch. In this work we propose a novel paradigm, Incremental BSP (iX-BSP), based on the key insight that calculations across planning sessions are similar in nature and can be appropriately re-used. We calculate the expectation incrementally by utilizing Multiple Importance Sampling techniques for selective re-sampling and re-use of measurement from previous planning sessions. The formulation of our approach considers general distributions and accounts for data association aspects. We demonstrate how iX-BSP could benefit existing approximations of the general problem, introducing iML-BSP, which re-uses calculations across planning sessions under the common Maximum Likelihood assumption. We evaluate both methods and demonstrate a substantial reduction in computation time while statistically preserving accuracy. The evaluation includes both simulation and real-world experiments considering autonomous vision-based navigation and SLAM. As a further contribution, we introduce to iX-BSP the non-integral wildfire approximation, allowing one to trade accuracy for computational performance by averting from updating re-used beliefs when they are ""close enough"". We evaluate iX-BSP under wildfire demonstrating a substantial reduction in computation time while controlling the accuracy sacrifice. We also provide analytical and empirical bounds of the effect wildfire holds over the objective value. △ Less","28 February, 2021",https://arxiv.org/pdf/2102.09539
"Artificial Intelligence Technologies in Education: Benefits, Challenges and Strategies of Implementation",Mieczysław L. Owoc;Agnieszka Sawicka;Paweł Weichbroth,"Since the education sector is associated with highly dynamic business environments which are controlled and maintained by information systems, recent technological advancements and the increasing pace of adopting artificial intelligence (AI) technologies constitute a need to identify and analyze the issues regarding their implementation in education sector. However, a study of the contemporary literature reveled that relatively little research has been undertaken in this area. To fill this void, we have identified the benefits and challenges of implementing artificial intelligence in the education sector, preceded by a short discussion on the concepts of AI and its evolution over time. Moreover, we have also reviewed modern AI technologies for learners and educators, currently available on the software market, evaluating their usefulness. Last but not least, we have developed a strategy implementation model, described by a five-stage, generic process, along with the corresponding configuration guide. To verify and validate their design, we separately developed three implementation strategies for three different higher education organizations. We believe that the obtained results will contribute to better understanding the specificities of AI systems, services and tools, and afterwards pave a smooth way in their implementation. △ Less","11 February, 2021",https://arxiv.org/pdf/2102.09365
Ethics as a service: a pragmatic operationalisation of AI Ethics,Jessica Morley;Anat Elhalal;Francesca Garcia;Libby Kinsey;Jakob Mokander;Luciano Floridi,"As the range of potential uses for Artificial Intelligence (AI), in particular machine learning (ML), has increased, so has awareness of the associated ethical issues. This increased awareness has led to the realisation that existing legislation and regulation provides insufficient protection to individuals, groups, society, and the environment from AI harms. In response to this realisation, there has been a proliferation of principle-based ethics codes, guidelines and frameworks. However, it has become increasingly clear that a significant gap exists between the theory of AI ethics principles and the practical design of AI systems. In previous work, we analysed whether it is possible to close this gap between the what and the how of AI ethics through the use of tools and methods designed to help AI developers, engineers, and designers translate principles into practice. We concluded that this method of closure is currently ineffective as almost all existing translational tools and methods are either too flexible (and thus vulnerable to ethics washing) or too strict (unresponsive to context). This raised the question: if, even with technical guidance, AI ethics is challenging to embed in the process of algorithmic design, is the entire pro-ethical design endeavour rendered futile? And, if no, then how can AI ethics be made useful for AI practitioners? This is the question we seek to address here by exploring why principles and technical translational tools are still needed even if they are limited, and how these limitations can be potentially overcome by providing theoretical grounding of a concept that has been termed Ethics as a Service. △ Less","11 February, 2021",https://arxiv.org/pdf/2102.09364
All-optical spiking neurosynaptic networks with self-learning capabilities,J. Feldmann;N. Youngblood;C. D. Wright;H. Bhaskaran;W. H. P. Pernice,"Software-implementation, via neural networks, of brain-inspired computing approaches underlie many important modern-day computational tasks, from image processing to speech recognition, artificial intelligence and deep learning applications. Yet, differing from real neural tissue, traditional computing architectures physically separate the core computing functions of memory and processing, making fast, efficient and low-energy brain-like computing difficult to achieve. To overcome such limitations, an attractive and alternative goal is to design direct hardware mimics of brain neurons and synapses which, when connected in appropriate networks (or neuromorphic systems), process information in a way more fundamentally analogous to that of real brains. Here we present an all-optical approach to achieving such a goal. Specifically, we demonstrate an all-optical spiking neuron device and connect it, via an integrated photonics network, to photonic synapses to deliver a small-scale all-optical neurosynaptic system capable of supervised and unsupervised learning. Moreover, we exploit wavelength division multiplexing techniques to implement a scalable circuit architecture for photonic neural networks, successfully demonstrating pattern recognition directly in the optical domain using a photonic system comprising 140 elements. Such optical implementations of neurosynaptic networks promise access to the high speed and bandwidth inherent to optical systems, which would be very attractive for the direct processing of telecommunication and visual data in the optical domain. △ Less","18 February, 2021",https://arxiv.org/pdf/2102.09360
FedMood: Federated Learning on Mobile Health Data for Mood Detection,Xiaohang Xu;Hao Peng;Lichao Sun;Md Zakirul Alam Bhuiyan;Lianzhong Liu;Lifang He,"Depression is one of the most common mental illness problems, and the symptoms shown by patients are not consistent, making it difficult to diagnose in the process of clinical practice and pathological research. Although researchers hope that artificial intelligence can contribute to the diagnosis and treatment of depression, the traditional centralized machine learning needs to aggregate patient data, and the data privacy of patients with mental illness needs to be strictly confidential, which hinders machine learning algorithms clinical application. To solve the problem of privacy of the medical history of patients with depression, we implement federated learning to analyze and diagnose depression. First, we propose a general multi-view federated learning framework using multi-source data, which can extend any traditional machine learning model to support federated learning across different institutions or parties. Secondly, we adopt late fusion methods to solve the problem of inconsistent time series of multi-view data. Finally, we compare the federated framework with other cooperative learning frameworks in performance and discuss the related results. △ Less","20 May, 2021",https://arxiv.org/pdf/2102.09342
Testing Lotka's Law and Pattern of Author Productivity in the Scholarly Publications of Artificial Intelligence,Muneer Ahmad;Dr M Sadik Batcha;S Roselin Jahina,"Artificial intelligence has changed our day to day life in multitude ways. AI technology is rearing itself as a driving force to be reckoned with in the largest industries in the world. AI has already engulfed our educational system, our businesses and our financial establishments. The future is definite that machines with artificial intelligence will soon be captivating over trained manual work that now is mostly cared by humans. Machines can carry out human-like tasks by new inputs as artificial intelligence makes it possible for machines to learn from experience. AI data from web of science database from 2008 to 2017 have been mapped to depict the average growth rate, relative growth rate, contribution made by authors in the view of research productivity, authorship pattern and collaboration of AI literature. The Lotka's law on authorship productivity of AI literature has been tested to confirm the applicability of the law to the present data set. A K-S test was applied to measure the degree of agreement between the distribution of the observed set of data against the inverse general power relationship and the theoretical value of α =2. It is found that the inverse square law of Lotka follow as such. △ Less","18 February, 2021",https://arxiv.org/pdf/2102.09182
Learning Fair Representations for Recommendation: A Graph-based Perspective,Le Wu;Lei Chen;Pengyang Shao;Richang Hong;Xiting Wang;Meng Wang,"As a key application of artificial intelligence, recommender systems are among the most pervasive computer aided systems to help users find potential items of interests. Recently, researchers paid considerable attention to fairness issues for artificial intelligence applications. Most of these approaches assumed independence of instances, and designed sophisticated models to eliminate the sensitive information to facilitate fairness. However, recommender systems differ greatly from these approaches as users and items naturally form a user-item bipartite graph, and are collaboratively correlated in the graph structure. In this paper, we propose a novel graph based technique for ensuring fairness of any recommendation models. Here, the fairness requirements refer to not exposing sensitive feature set in the user modeling process. Specifically, given the original embeddings from any recommendation models, we learn a composition of filters that transform each user's and each item's original embeddings into a filtered embedding space based on the sensitive feature set. For each user, this transformation is achieved under the adversarial learning of a user-centric graph, in order to obfuscate each sensitive feature between both the filtered user embedding and the sub graph structures of this user. Finally, extensive experimental results clearly show the effectiveness of our proposed model for fair recommendation. We publish the source code at https://github.com/newlei/FairGo. △ Less","23 April, 2021",https://arxiv.org/pdf/2102.09140
Understanding algorithmic collusion with experience replay,Bingyan Han,"In an infinitely repeated pricing game, pricing algorithms based on artificial intelligence (Q-learning) may consistently learn to charge supra-competitive prices even without communication. Although concerns on algorithmic collusion have arisen, little is known on underlying factors. In this work, we experimentally analyze the dynamics of algorithms with three variants of experience replay. Algorithmic collusion still has roots in human preferences. Randomizing experience yields prices close to the static Bertrand equilibrium and higher prices are easily restored by favoring the latest experience. Moreover, relative performance concerns also stabilize the collusion. Finally, we investigate the scenarios with heterogeneous agents and test robustness on various factors. △ Less","21 March, 2021",https://arxiv.org/pdf/2102.09139
Understanding and Creating Art with AI: Review and Outlook,Eva Cetinic;James She,"Technologies related to artificial intelligence (AI) have a strong impact on the changes of research and creative practices in visual arts. The growing number of research initiatives and creative applications that emerge in the intersection of AI and art, motivates us to examine and discuss the creative and explorative potentials of AI technologies in the context of art. This paper provides an integrated review of two facets of AI and art: 1) AI is used for art analysis and employed on digitized artwork collections; 2) AI is used for creative purposes and generating novel artworks. In the context of AI-related research for art understanding, we present a comprehensive overview of artwork datasets and recent works that address a variety of tasks such as classification, object detection, similarity retrieval, multimodal representations, computational aesthetics, etc. In relation to the role of AI in creating art, we address various practical and theoretical aspects of AI Art and consolidate related works that deal with those topics in detail. Finally, we provide a concise outlook on the future progression and potential impact of AI technologies on our understanding and creation of art. △ Less","17 February, 2021",https://arxiv.org/pdf/2102.09109
Towards AIOps in Edge Computing Environments,Soeren Becker;Florian Schmidt;Anton Gulenko;Alexander Acker;Odej Kao,"Edge computing was introduced as a technical enabler for the demanding requirements of new network technologies like 5G. It aims to overcome challenges related to centralized cloud computing environments by distributing computational resources to the edge of the network towards the customers. The complexity of the emerging infrastructures increases significantly, together with the ramifications of outages on critical use cases such as self-driving cars or health care. Artificial Intelligence for IT Operations (AIOps) aims to support human operators in managing complex infrastructures by using machine learning methods. This paper describes the system design of an AIOps platform which is applicable in heterogeneous, distributed environments. The overhead of a high-frequency monitoring solution on edge devices is evaluated and performance experiments regarding the applicability of three anomaly detection algorithms on edge devices are conducted. The results show, that it is feasible to collect metrics with a high frequency and simultaneously run specific anomaly detection algorithms directly on edge devices with a reasonable overhead on the resource utilization. △ Less","12 February, 2021",https://arxiv.org/pdf/2102.09001
An Implementation of Vector Quantization using the Genetic Algorithm Approach,Maha Mohammed Khan,"The application of machine learning(ML) and genetic programming(GP) to the image compression domain has produced promising results in many cases. The need for compression arises due to the exorbitant size of data shared on the internet. Compression is required for text, videos, or images, which are used almost everywhere on web be it news articles, social media posts, blogs, educational platforms, medical domain, government services, and many other websites, need packets for transmission and hence compression is necessary to avoid overwhelming the network. This paper discusses some of the implementations of image compression algorithms that use techniques such as Artificial Neural Networks, Residual Learning, Fuzzy Neural Networks, Convolutional Neural Nets, Deep Learning, Genetic Algorithms. The paper also describes an implementation of Vector Quantization using GA to generate codebook which is used for Lossy image compression. All these approaches prove to be very contrasting to the standard approaches to processing images due to the highly parallel and computationally extensive nature of machine learning algorithms. Such non-linear abilities of ML and GP make it widely popular for use in multiple domains. Traditional approaches are also combined with artificially intelligent systems, leading to hybrid systems, to achieve better results. △ Less","15 February, 2021",https://arxiv.org/pdf/2102.08893
"An asymptotic analysis of probabilistic logic programming, with implications for expressing projective families of distributions",Felix Weitkämper,"Probabilistic logic programming is a major part of statistical relational artificial intelligence, where approaches from logic and probability are brought together to reason about and learn from relational domains in a setting of uncertainty. However, the behaviour of statistical relational representations across variable domain sizes is complex, and scaling inference and learning to large domains remains a significant challenge. In recent years, connections have emerged between domain size dependence, lifted inference and learning from sampled subpopulations. The asymptotic behaviour of statistical relational representations has come under scrutiny, and projectivity was investigated as the strongest form of domain-size dependence, in which query marginals are completely independent of the domain size. In this contribution we show that every probabilistic logic program under the distribution semantics is asymptotically equivalent to an acyclic probabilistic logic program consisting only of determinate clauses over probabilistic facts. We conclude that every probabilistic logic program inducing a projective family of distributions is in fact everywhere equivalent to a program from this fragment, and we investigate the consequences for the projective families of distributions expressible by probabilistic logic programs. To facilitate the application of classical results from finite model theory, we introduce the abstract distribution semantics, defined as an arbitrary logical theory over probabilistic facts. This bridges the gap to the distribution semantics underlying probabilistic logic programming. In this representation, determinate logic programs correspond to quantifier-free theories, making asymptotic quantifier elimination results available for the setting of probabilistic logic programming. This paper is under consideration for acceptance in TPLP. △ Less","18 August, 2021",https://arxiv.org/pdf/2102.08777
Towards the Right Kind of Fairness in AI,Boris Ruf;Marcin Detyniecki,"Fairness is a concept of justice. Various definitions exist, some of them conflicting with each other. In the absence of an uniformly accepted notion of fairness, choosing the right kind for a specific situation has always been a central issue in human history. When it comes to implementing sustainable fairness in artificial intelligence systems, this old question plays a key role once again: How to identify the most appropriate fairness metric for a particular application? The answer is often a matter of context, and the best choice depends on ethical standards and legal requirements. Since ethics guidelines on this topic are kept rather general for now, we aim to provide more hands-on guidance with this document. Therefore, we first structure the complex landscape of existing fairness metrics and explain the different options by example. Furthermore, we propose the ""Fairness Compass"", a tool which formalises the selection process and makes identifying the most appropriate fairness definition for a given system a simple, straightforward procedure. Because this process also allows to document the reasoning behind the respective decisions, we argue that this approach can help to build trust from the user through explaining and justifying the implemented fairness. △ Less","30 September, 2021",https://arxiv.org/pdf/2102.08453
Finding the Gap: Neuromorphic Motion Vision in Cluttered Environments,Thorben Schoepe;Ella Janotte;Moritz B. Milde;Olivier J. N. Bertrand;Martin Egelhaaf;Elisabetta Chicca,"Many animals meander in environments and avoid collisions. How the underlying neuronal machinery can yield robust behaviour in a variety of environments remains unclear. In the fly brain, motion-sensitive neurons indicate the presence of nearby objects and directional cues are integrated within an area known as the central complex. Such neuronal machinery, in contrast with the traditional stream-based approach to signal processing, uses an event-based approach, with events occurring when changes are sensed by the animal. Contrary to von Neumann computing architectures, event-based neuromorphic hardware is designed to process information in an asynchronous and distributed manner. Inspired by the fly brain, we model, for the first time, a neuromorphic closed-loop system mimicking essential behaviours observed in flying insects, such as meandering in clutter and gap crossing, which are highly relevant for autonomous vehicles. We implemented our system both in software and on neuromorphic hardware. While moving through an environment, our agent perceives changes in its surroundings and uses this information for collision avoidance. The agent's manoeuvres result from a closed action-perception loop implementing probabilistic decision-making processes. This loop-closure is thought to have driven the development of neural circuitry in biological agents since the Cambrian explosion. In the fundamental quest to understand neural computation in artificial agents, we come closer to understanding and modelling biological intelligence by closing the loop also in neuromorphic systems. As a closed-loop system, our system deepens our understanding of processing in neural networks and computations in biological and artificial systems. With these investigations, we aim to set the foundations for neuromorphic intelligence in the future, moving towards leveraging the full potential of neuromorphic systems. △ Less","16 February, 2021",https://arxiv.org/pdf/2102.08417
"Design a Technology Based on the Fusion of Genetic Algorithm, Neural network and Fuzzy logic",Raid R. Al-Nima;Fawaz S. Abdullah;Ali N. Hamoodi,"This paper describes the design and development of a prototype technique for artificial intelligence based on the fusion of genetic algorithm, neural network and fuzzy logic. It starts by establishing a relationship between the neural network and fuzzy logic. Then, it combines the genetic algorithm with them. Information fusions are at the confidence level, where matching scores can be reported and discussed. The technique is called the Genetic Neuro-Fuzzy (GNF). It can be used for high accuracy real-time environments. △ Less","16 February, 2021",https://arxiv.org/pdf/2102.08035
"A Mental Trespass? Unveiling Truth, Exposing Thoughts and Threatening Civil Liberties with Non-Invasive AI Lie Detection",Taylan Sen;Kurtis Haut;Denis Lomakin;Ehsan Hoque,"Imagine an app on your phone or computer that can tell if you are being dishonest, just by processing affective features of your facial expressions, body movements, and voice. People could ask about your political preferences, your sexual orientation, and immediately determine which of your responses are honest and which are not. In this paper we argue why artificial intelligence-based, non-invasive lie detection technologies are likely to experience a rapid advancement in the coming years, and that it would be irresponsible to wait any longer before discussing its implications. Legal and popular perspectives are reviewed to evaluate the potential for these technologies to cause societal harm. To understand the perspective of a reasonable person, we conducted a survey of 129 individuals, and identified consent and accuracy as the major factors in their decision-making process regarding the use of these technologies. In our analysis, we distinguish two types of lie detection technology, accurate truth metering and accurate thought exposing. We generally find that truth metering is already largely within the scope of existing US federal and state laws, albeit with some notable exceptions. In contrast, we find that current regulation of thought exposing technologies is ambiguous and inadequate to safeguard civil liberties. In order to rectify these shortcomings, we introduce the legal concept of mental trespass and use this concept as the basis for proposed regulation. △ Less","16 February, 2021",https://arxiv.org/pdf/2102.08004
What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research,Markus Langer;Daniel Oster;Timo Speith;Holger Hermanns;Lena Kästner;Eva Schmidt;Andreas Sesing;Kevin Baum,"Previous research in Explainable Artificial Intelligence (XAI) suggests that a main aim of explainability approaches is to satisfy specific interests, goals, expectations, needs, and demands regarding artificial systems (we call these stakeholders' desiderata) in a variety of contexts. However, the literature on XAI is vast, spreads out across multiple largely disconnected disciplines, and it often remains unclear how explainability approaches are supposed to achieve the goal of satisfying stakeholders' desiderata. This paper discusses the main classes of stakeholders calling for explainability of artificial systems and reviews their desiderata. We provide a model that explicitly spells out the main concepts and relations necessary to consider and investigate when evaluating, adjusting, choosing, and developing explainability approaches that aim to satisfy stakeholders' desiderata. This model can serve researchers from the variety of different disciplines involved in XAI as a common ground. It emphasizes where there is interdisciplinary potential in the evaluation and the development of explainability approaches. △ Less","15 February, 2021",https://arxiv.org/pdf/2102.07817
"A survey of recommender systems for energy efficiency in buildings: Principles, challenges and prospects",Yassine Himeur;Abdullah Alsalemi;Ayman Al-Kababji;Faycal Bensaali;Abbes Amira;Christos Sardianos;George Dimitrakopoulos;Iraklis Varlamis,"Recommender systems have significantly developed in recent years in parallel with the witnessed advancements in both internet of things (IoT) and artificial intelligence (AI) technologies. Accordingly, as a consequence of IoT and AI, multiple forms of data are incorporated in these systems, e.g. social, implicit, local and personal information, which can help in improving recommender systems' performance and widen their applicability to traverse different disciplines. On the other side, energy efficiency in the building sector is becoming a hot research topic, in which recommender systems play a major role by promoting energy saving behavior and reducing carbon emissions. However, the deployment of the recommendation frameworks in buildings still needs more investigations to identify the current challenges and issues, where their solutions are the keys to enable the pervasiveness of research findings, and therefore, ensure a large-scale adoption of this technology. Accordingly, this paper presents, to the best of the authors' knowledge, the first timely and comprehensive reference for energy-efficiency recommendation systems through (i) surveying existing recommender systems for energy saving in buildings; (ii) discussing their evolution; (iii) providing an original taxonomy of these systems based on specified criteria, including the nature of the recommender engine, its objective, computing platforms, evaluation metrics and incentive measures; and (iv) conducting an in-depth, critical analysis to identify their limitations and unsolved issues. The derived challenges and areas of future implementation could effectively guide the energy research community to improve the energy-efficiency in buildings and reduce the cost of developed recommender systems-based solutions. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.07654
Real-time tracking of COVID-19 and coronavirus research updates through text mining,Yutong Jin;Jie Li;Xinyu Wang;Peiyao Li;Jinjiang Guo;Junfeng Wu;Dawei Leng;Lurong Pan,"The novel coronavirus (SARS-CoV-2) which causes COVID-19 is an ongoing pandemic. There are ongoing studies with up to hundreds of publications uploaded to databases daily. We are exploring the use-case of artificial intelligence and natural language processing in order to efficiently sort through these publications. We demonstrate that clinical trial information, preclinical studies, and a general topic model can be used as text mining data intelligence tools for scientists all over the world to use as a resource for their own research. To evaluate our method, several metrics are used to measure the information extraction and clustering results. In addition, we demonstrate that our workflow not only have a use-case for COVID-19, but for other disease areas as well. Overall, our system aims to allow scientists to more efficiently research coronavirus. Our automatically updating modules are available on our information portal at https://ghddi-ailab.github.io/Targeting2019-nCoV/ for public viewing. △ Less","8 February, 2021",https://arxiv.org/pdf/2102.07640
Accelerating COVID-19 research with graph mining and transformer-based learning,Ilya Tyagin;Ankit Kulshrestha;Justin Sybrandt;Krish Matta;Michael Shtutman;Ilya Safro,"In 2020, the White House released the, ""Call to Action to the Tech Community on New Machine Readable COVID-19 Dataset,"" wherein artificial intelligence experts are asked to collect data and develop text mining techniques that can help the science community answer high-priority scientific questions related to COVID-19. The Allen Institute for AI and collaborators announced the availability of a rapidly growing open dataset of publications, the COVID-19 Open Research Dataset (CORD-19). As the pace of research accelerates, biomedical scientists struggle to stay current. To expedite their investigations, scientists leverage hypothesis generation systems, which can automatically inspect published papers to discover novel implicit connections. We present an automated general purpose hypothesis generation systems AGATHA-C and AGATHA-GP for COVID-19 research. The systems are based on graph-mining and the transformer model. The systems are massively validated using retrospective information rediscovery and proactive analysis involving human-in-the-loop expert analysis. Both systems achieve high-quality predictions across domains (in some domains up to 0.97% ROC AUC) in fast computational time and are released to the broad scientific community to accelerate biomedical research. In addition, by performing the domain expert curated study, we show that the systems are able to discover on-going research findings such as the relationship between COVID-19 and oxytocin hormone. △ Less","29 September, 2021",https://arxiv.org/pdf/2102.07631
A Reference Model for IoT Embodied Agents Controlled by Neural Networks,Nathalia Nascimento;Paulo Alencar;Donald Cowan;Carlos Lucena,"Embodied agents is a term used to denote intelligent agents, which are a component of devices belonging to the Internet of Things (IoT) domain. Each agent is provided with sensors and actuators to interact with the environment, and with a 'controller' that usually contains an artificial neural network (ANN). In previous publications, we introduced three software approaches to design, implement and test IoT embodied agents. In this paper, we propose a reference model based on statecharts that offers abstractions tailored to the development of IoT applications. The model represents embodied agents that are controlled by neural networks. Our model includes the ANN training process, represented as a reconfiguration step such as changing agent features or neural net connections. Our contributions include the identification of the main characteristics of IoT embodied agents, a reference model specification based on statecharts, and an illustrative application of the model to support autonomous street lights. The proposal aims to support the design and implementation of IoT applications by providing high-level design abstractions and models, thus enabling the designer to have a uniform approach to conceiving, designing and explaining such applications. △ Less","15 February, 2021",https://arxiv.org/pdf/2102.07589
The corruptive force of AI-generated advice,Margarita Leib;Nils C. Köbis;Rainer Michael Rilke;Marloes Hagens;Bernd Irlenbusch,"Artificial Intelligence (AI) is increasingly becoming a trusted advisor in people's lives. A new concern arises if AI persuades people to break ethical rules for profit. Employing a large-scale behavioural experiment (N = 1,572), we test whether AI-generated advice can corrupt people. We further test whether transparency about AI presence, a commonly proposed policy, mitigates potential harm of AI-generated advice. Using the Natural Language Processing algorithm, GPT-2, we generated honesty-promoting and dishonesty-promoting advice. Participants read one type of advice before engaging in a task in which they could lie for profit. Testing human behaviour in interaction with actual AI outputs, we provide first behavioural insights into the role of AI as an advisor. Results reveal that AI-generated advice corrupts people, even when they know the source of the advice. In fact, AI's corrupting force is as strong as humans'. △ Less","15 February, 2021",https://arxiv.org/pdf/2102.07536
Why Talking about ethics is not enough: a proposal for Fintech's AI ethics,Cristina Godoy Bernardo de Oliveira;Evandro Eduardo Seron Ruiz,"As the potential applications of Artificial Intelligence (AI) in the financial sector increases, ethical issues become gradually latent. The distrust of individuals, social groups, and governments about the risks arising from Fintech's activities is growing. Due to this scenario, the preparation of recommendations and Ethics Guidelines is increasing and the risks of being chosen the principles and ethical values most appropriate to companies are high. Thus, this exploratory research aims to analyze the benefits of the application of the stakeholder theory and the idea of Social License to build an environment of trust and for the realization of ethical principles by Fintech. The formation of a Fintech association for the creation of a Social License will allow early-stage Fintech to participate from the beginning of its activities in the elaboration of a dynamic ethical code and with the participation of stakeholders. △ Less","14 February, 2021",https://arxiv.org/pdf/2102.07213
Parametric Optimization of Violin Top Plates using Machine Learning,Davide Salvi;Sebastian Gonzalez;Fabio Antonacci;Augusto Sarti,"We recently developed a neural network that receives as input the geometrical and mechanical parameters that define a violin top plate and gives as output its first ten eigenfrequencies computed in free boundary conditions. In this manuscript, we use the network to optimize several error functions, with the goal of analyzing the relationship between the eigenspectrum problem for violin top plates and their geometry. First, we focus on the violin outline. Given a vibratory feature, we find which is the best geometry of the plate to obtain it. Second, we investigate whether, from the vibrational point of view, a change in the outline shape can be compensated by one in the thickness distribution and vice versa. Finally, we analyze how to modify the violin shape to keep its response constant as its material properties vary. This is an original technique in musical acoustics, where artificial intelligence is not widely used yet. It allows us to both compute the vibrational behavior of an instrument from its geometry and optimize its shape for a given response. Furthermore, this method can be of great help to violin makers, who can thus easily understand the effects of the geometry changes in the violins they build, shedding light on one of the most relevant and, at the same time, less understood aspects of the construction process of musical instruments. △ Less","18 February, 2021",https://arxiv.org/pdf/2102.07133
Machine Learning Methods for the Design and Operation of Liquid Rocket Engines -- Research Activities at the DLR Institute of Space Propulsion,Günther Waxenegger-Wilfing;Kai Dresia;Jan Deeken;Michael Oschwald,"The last years have witnessed an enormous interest in the use of artificial intelligence methods, especially machine learning algorithms. This also has a major impact on aerospace engineering in general, and the design and operation of liquid rocket engines in particular, and research in this area is growing rapidly. The paper describes current machine learning applications at the DLR Institute of Space Propulsion. Not only applications in the field of modeling are presented, but also convincing results that prove the capabilities of machine learning methods for control and condition monitoring are described in detail. Furthermore, the advantages and disadvantages of the presented methods as well as current and future research directions are discussed. △ Less","14 February, 2021",https://arxiv.org/pdf/2102.07109
Hybrid Artificial Intelligence Methods for Predicting Air Demand in Dam Bottom Outlet,Aliakbar Narimani;Mahdi Moghimi;Amir Mosavi,"In large infrastructures such as dams, which have a relatively high economic value, ensuring the proper operation of the associated hydraulic facilities in different operating conditions is of utmost importance. To ensure the correct and successful operation of the dam's hydraulic equipment and prevent possible damages, including gates and downstream tunnel, to build laboratory models and perform some tests are essential (the advancement of the smart sensors based on artificial intelligence is essential). One of the causes of damage to dam bottom outlets is cavitation in downstream and between the gates, which can impact on dam facilities, and air aeration can be a solution to improve it. In the present study, six dams in different provinces in Iran has been chosen to evaluate the air entrainment in the downstream tunnel experimentally. Three artificial neural networks (ANN) based machine learning (ML) algorithms are used to model and predict the air aeration in the bottom outlet. The proposed models are trained with genetic algorithms (GA), particle swarm optimization (PSO), i.e., ANN-GA, ANN-PSO, and ANFIS-PSO. Two hydrodynamic variables, namely volume rate and opening percentage of the gate, are used as inputs into all bottom outlet models. The results showed that the most optimal model is ANFIS-PSO to predict the dependent value compared with ANN-GA and ANN-PSO. The importance of the volume rate and opening percentage of the dams' gate parameters is more effective for suitable air aeration. △ Less","13 February, 2021",https://arxiv.org/pdf/2102.06929
"Fusion of convolution neural network, support vector machine and Sobel filter for accurate detection of COVID-19 patients using X-ray images",Danial Sharifrazi;Roohallah Alizadehsani;Mohamad Roshanzamir;Javad Hassannataj Joloudari;Afshin Shoeibi;Mahboobeh Jafari;Sadiq Hussain;Zahra Alizadeh Sani;Fereshteh Hasanzadeh;Fahime Khozeimeh;Abbas Khosravi;Saeid Nahavandi;Maryam Panahiazar;Assef Zare;Sheikh Mohammed Shariful Islam;U Rajendra Acharya,"The coronavirus (COVID-19) is currently the most common contagious disease which is prevalent all over the world. The main challenge of this disease is the primary diagnosis to prevent secondary infections and its spread from one person to another. Therefore, it is essential to use an automatic diagnosis system along with clinical procedures for the rapid diagnosis of COVID-19 to prevent its spread. Artificial intelligence techniques using computed tomography (CT) images of the lungs and chest radiography have the potential to obtain high diagnostic performance for Covid-19 diagnosis. In this study, a fusion of convolutional neural network (CNN), support vector machine (SVM), and Sobel filter is proposed to detect COVID-19 using X-ray images. A new X-ray image dataset was collected and subjected to high pass filter using a Sobel filter to obtain the edges of the images. Then these images are fed to CNN deep learning model followed by SVM classifier with ten-fold cross validation strategy. This method is designed so that it can learn with not many data. Our results show that the proposed CNN-SVM with Sobel filtering (CNN-SVM+Sobel) achieved the highest classification accuracy of 99.02% in accurate detection of COVID-19. It showed that using Sobel filter can improve the performance of CNN. Unlike most of the other researches, this method does not use a pre-trained network. We have also validated our developed model using six public databases and obtained the highest performance. Hence, our developed model is ready for clinical application △ Less","13 February, 2021",https://arxiv.org/pdf/2102.06883
Leveraging Artificial Intelligence to Analyze Citizens' Opinions on Urban Green Space,Mohammadhossein Ghahramani;Nadina J. Galle;Fabio Duarte;Carlo Ratti;Francesco Pilla,"Continued population growth and urbanization is shifting research to consider the quality of urban green space over the quantity of these parks, woods, and wetlands. The quality of urban green space has been hitherto measured by expert assessments, including in-situ observations, surveys, and remote sensing analyses. Location data platforms, such as TripAdvisor, can provide people's opinion on many destinations and experiences, including UGS. This paper leverages Artificial Intelligence techniques for opinion mining and text classification using such platform's reviews as a novel approach to urban green space quality assessments. Natural Language Processing is used to analyze contextual information given supervised scores of words by implementing computational analysis. Such an application can support local authorities and stakeholders in their understanding of and justification for future investments in urban green space. △ Less","12 February, 2021",https://arxiv.org/pdf/2102.06659
VitrAI -- Applying Explainable AI in the Real World,Marc Hanussek;Falko Kötter;Maximilien Kintz;Jens Drawehn,"With recent progress in the field of Explainable Artificial Intelligence (XAI) and increasing use in practice, the need for an evaluation of different XAI methods and their explanation quality in practical usage scenarios arises. For this purpose, we present VitrAI, which is a web-based service with the goal of uniformly demonstrating four different XAI algorithms in the context of three real life scenarios and evaluating their performance and comprehensibility for humans. This work reveals practical obstacles when adopting XAI methods and gives qualitative estimates on how well different approaches perform in said scenarios. △ Less","12 February, 2021",https://arxiv.org/pdf/2102.06518
Uncertainty-Aware Semi-Supervised Method Using Large Unlabeled and Limited Labeled COVID-19 Data,Roohallah Alizadehsani;Danial Sharifrazi;Navid Hoseini Izadi;Javad Hassannataj Joloudari;Afshin Shoeibi;Juan M. Gorriz;Sadiq Hussain;Juan E. Arco;Zahra Alizadeh Sani;Fahime Khozeimeh;Abbas Khosravi;Saeid Nahavandi;Sheikh Mohammed Shariful Islam;U Rajendra Acharya,"The new coronavirus has caused more than one million deaths and continues to spread rapidly. This virus targets the lungs, causing respiratory distress which can be mild or severe. The X-ray or computed tomography (CT) images of lungs can reveal whether the patient is infected with COVID-19 or not. Many researchers are trying to improve COVID-19 detection using artificial intelligence. Our motivation is to develop an automatic method that can cope with scenarios in which preparing labeled data is time consuming or expensive. In this article, we propose a Semi-supervised Classification using Limited Labeled Data (SCLLD) relying on Sobel edge detection and Generative Adversarial Networks (GANs) to automate the COVID-19 diagnosis. The GAN discriminator output is a probabilistic value which is used for classification in this work. The proposed system is trained using 10,000 CT scans collected from Omid Hospital, whereas a public dataset is also used for validating our system. The proposed method is compared with other state-of-the-art supervised methods such as Gaussian processes. To the best of our knowledge, this is the first time a semi-supervised method for COVID-19 detection is presented. Our system is capable of learning from a mixture of limited labeled and unlabeled data where supervised learners fail due to a lack of sufficient amount of labeled data. Thus, our semi-supervised training method significantly outperforms the supervised training of Convolutional Neural Network (CNN) when labeled training data is scarce. The 95% confidence intervals for our method in terms of accuracy, sensitivity, and specificity are 99.56 +- 0.20%, 99.88 +- 0.24%, and 99.40 +- 0.18%, respectively, whereas intervals for the CNN (trained supervised) are 68.34 +- 4.11%, 91.2 +- 6.15%, and 46.40 +- 5.21%. △ Less","24 December, 2021",https://arxiv.org/pdf/2102.06388
Hedging of Financial Derivative Contracts via Monte Carlo Tree Search,Oleg Szehr,"The construction of approximate replication strategies for pricing and hedging of derivative contracts in incomplete markets is a key problem of financial engineering. Recently Reinforcement Learning algorithms for hedging under realistic market conditions have attracted significant interest. While research in the derivatives area mostly focused on variations of Q-learning, in artificial intelligence Monte Carlo Tree Search is the recognized state-of-the-art method for various planning problems, such as the games of Hex, Chess, Go,... This article introduces Monte Carlo Tree Search as a method to solve the stochastic optimal control problem behind the pricing and hedging tasks. As compared to Q-learning it combines Reinforcement Learning with tree search techniques. As a consequence Monte Carlo Tree Search has higher sample efficiency, is less prone to over-fitting to specific market models and generally learns stronger policies faster. In our experiments we find that Monte Carlo Tree Search, being the world-champion in games like Chess and Go, is easily capable of maximizing the utility of investor's terminal wealth without setting up an auxiliary mathematical framework. △ Less","19 April, 2021",https://arxiv.org/pdf/2102.06274
Artificial Intelligence Advances for De Novo Molecular Structure Modeling in Cryo-EM,Dong Si;Andrew Nakamura;Runbang Tang;Haowen Guan;Jie Hou;Ammaar Firozi;Renzhi Cao;Kyle Hippe;Minglei Zhao,"Cryo-electron microscopy (cryo-EM) has become a major experimental technique to determine the structures of large protein complexes and molecular assemblies, as evidenced by the 2017 Nobel Prize. Although cryo-EM has been drastically improved to generate high-resolution three-dimensional (3D) maps that contain detailed structural information about macromolecules, the computational methods for using the data to automatically build structure models are lagging far behind. The traditional cryo-EM model building approach is template-based homology modeling. Manual de novo modeling is very time-consuming when no template model is found in the database. In recent years, de novo cryo-EM modeling using machine learning (ML) and deep learning (DL) has ranked among the top-performing methods in macromolecular structure modeling. Deep-learning-based de novo cryo-EM modeling is an important application of artificial intelligence, with impressive results and great potential for the next generation of molecular biomedicine. Accordingly, we systematically review the representative ML/DL-based de novo cryo-EM modeling methods. And their significances are discussed from both practical and methodological viewpoints. We also briefly describe the background of cryo-EM data processing workflow. Overall, this review provides an introductory guide to modern research on artificial intelligence (AI) for de novo molecular structure modeling and future directions in this emerging field. △ Less","23 February, 2021",https://arxiv.org/pdf/2102.06125
Artificial Intelligence based Autonomous Molecular Design for Medical Therapeutic: A Perspective,Rajendra P. Joshi;Neeraj Kumar,"Domain-aware machine learning (ML) models have been increasingly adopted for accelerating small molecule therapeutic design in the recent years. These models have been enabled by significant advancement in state-of-the-art artificial intelligence (AI) and computing infrastructures. Several ML architectures are pre-dominantly and independently used either for predicting the properties of small molecules, or for generating lead therapeutic candidates. Synergetically using these individual components along with robust representation and data generation techniques autonomously in closed loops holds enormous promise for accelerated drug design which is a time consuming and expensive task otherwise. In this perspective, we present the most recent breakthrough achieved by each of the components, and how such autonomous AI and ML workflow can be realized to radically accelerate the hit identification and lead optimization. Taken together, this could significantly shorten the timeline for end-to-end antiviral discovery and optimization times to weeks upon the arrival of a novel zoonotic transmission event. Our perspective serves as a guide for researchers to practice autonomous molecular design in therapeutic discovery. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.06045
The Barrier of meaning in archaeological data science,Luca Casini;Marco Roccetti;Giovanni Delnevo;Nicolo' Marchetti;Valentina Orru',"Archaeologists, like other scientists, are experiencing a data-flood in their discipline, fueled by a surge in computing power and devices that enable the creation, collection, storage and transfer of an increasingly complex (and large) amount of data, such as remotely sensed imagery from a multitude of sources. In this paper, we pose the preliminary question if this increasing availability of information actually needs new computerized techniques, and Artificial Intelligence methods, to make new and deeper understanding into archaeological problems. Simply said, while it is a fact that Deep Learning (DL) has become prevalent as a type of machine learning design inspired by the way humans learn, and utilized to perform automatic actions people might describe as intelligent, we want to anticipate, here, a discussion around the subject whether machines, trained following this procedure, can extrapolate, from archaeological data, concepts and meaning in the same way that humans would do. Even prior to getting to technical results, we will start our reflection with a very basic concept: Is a collection of satellite images with notable archaeological sites informative enough to instruct a DL machine to discover new archaeological sites, as well as other potential locations of interest? Further, what if similar results could be reached with less intelligent machines that learn by having people manually program them with rules? Finally: If with barrier of meaning we refer to the extent to which human-like understanding can be achieved by a machine, where should be posed that barrier in the archaeological data science? △ Less","11 February, 2021",https://arxiv.org/pdf/2102.06022
Artificial intelligence in communication impacts language and social relationships,Jess Hohenstein;Dominic DiFranzo;Rene F. Kizilcec;Zhila Aghajari;Hannah Mieczkowski;Karen Levy;Mor Naaman;Jeff Hancock;Malte Jung,"Artificial intelligence (AI) is now widely used to facilitate social interaction, but its impact on social relationships and communication is not well understood. We study the social consequences of one of the most pervasive AI applications: algorithmic response suggestions (""smart replies""). Two randomized experiments (n = 1036) provide evidence that a commercially-deployed AI changes how people interact with and perceive one another in pro-social and anti-social ways. We find that using algorithmic responses increases communication efficiency, use of positive emotional language, and positive evaluations by communication partners. However, consistent with common assumptions about the negative implications of AI, people are evaluated more negatively if they are suspected to be using algorithmic responses. Thus, even though AI can increase communication efficiency and improve interpersonal perceptions, it risks changing users' language production and continues to be viewed negatively. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.05756
A Similarity-preserving Neural Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit,Yanis Bahroun;Anirvan M. Sengupta;Dmitri B. Chklovskii,"Learning to detect content-independent transformations from data is one of the central problems in biological and artificial intelligence. An example of such problem is unsupervised learning of a visual motion detector from pairs of consecutive video frames. Rao and Ruderman formulated this problem in terms of learning infinitesimal transformation operators (Lie group generators) via minimizing image reconstruction error. Unfortunately, it is difficult to map their model onto a biologically plausible neural network (NN) with local learning rules. Here we propose a biologically plausible model of motion detection. We also adopt the transformation-operator approach but, instead of reconstruction-error minimization, start with a similarity-preserving objective function. An online algorithm that optimizes such an objective function naturally maps onto an NN with biologically plausible learning rules. The trained NN recapitulates major features of the well-studied motion detector in the fly. In particular, it is consistent with the experimental observation that local motion detectors combine information from at least three adjacent pixels, something that contradicts the celebrated Hassenstein-Reichardt model. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.05503
The human-AI relationship in decision-making: AI explanation to support people on justifying their decisions,Juliana Jansen Ferreira;Mateus Monteiro,"The explanation dimension of Artificial Intelligence (AI) based system has been a hot topic for the past years. Different communities have raised concerns about the increasing presence of AI in people's everyday tasks and how it can affect people's lives. There is a lot of research addressing the interpretability and transparency concepts of explainable AI (XAI), which are usually related to algorithms and Machine Learning (ML) models. But in decision-making scenarios, people need more awareness of how AI works and its outcomes to build a relationship with that system. Decision-makers usually need to justify their decision to others in different domains. If that decision is somehow based on or influenced by an AI-system outcome, the explanation about how the AI reached that result is key to building trust between AI and humans in decision-making scenarios. In this position paper, we discuss the role of XAI in decision-making scenarios, our vision of Decision-Making with AI-system in the loop, and explore one case from the literature about how XAI can impact people justifying their decisions, considering the importance of building the human-AI relationship for those scenarios. △ Less","22 February, 2021",https://arxiv.org/pdf/2102.05460
NUVA: A Naming Utterance Verifier for Aphasia Treatment,David Sabate Barbera;Mark Huckvale;Victoria Fleming;Emily Upton;Henry Coley-Fisher;Catherine Doogan;Ian Shaw;William Latham;Alexander P. Leff;Jenny Crinion,"Anomia (word-finding difficulties) is the hallmark of aphasia, an acquired language disorder most commonly caused by stroke. Assessment of speech performance using picture naming tasks is a key method for both diagnosis and monitoring of responses to treatment interventions by people with aphasia (PWA). Currently, this assessment is conducted manually by speech and language therapists (SLT). Surprisingly, despite advancements in automatic speech recognition (ASR) and artificial intelligence with technologies like deep learning, research on developing automated systems for this task has been scarce. Here we present NUVA, an utterance verification system incorporating a deep learning element that classifies 'correct' versus' incorrect' naming attempts from aphasic stroke patients. When tested on eight native British-English speaking PWA the system's performance accuracy ranged between 83.6% to 93.6%, with a 10-fold cross-validation mean of 89.5%. This performance was not only significantly better than a baseline created for this study using one of the leading commercially available ASRs (Google speech-to-text service) but also comparable in some instances with two independent SLT ratings for the same dataset. △ Less","10 February, 2021",https://arxiv.org/pdf/2102.05408
Principles of Explanation in Human-AI Systems,Shane T. Mueller;Elizabeth S. Veinott;Robert R. Hoffman;Gary Klein;Lamia Alam;Tauseef Mamun;William J. Clancey,"Explainable Artificial Intelligence (XAI) has re-emerged in response to the development of modern AI and ML systems. These systems are complex and sometimes biased, but they nevertheless make decisions that impact our lives. XAI systems are frequently algorithm-focused; starting and ending with an algorithm that implements a basic untested idea about explainability. These systems are often not tested to determine whether the algorithm helps users accomplish any goals, and so their explainability remains unproven. We propose an alternative: to start with human-focused principles for the design, testing, and implementation of XAI systems, and implement algorithms to serve that purpose. In this paper, we review some of the basic concepts that have been used for user-centered XAI systems over the past 40 years of research. Based on these, we describe the ""Self-Explanation Scorecard"", which can help developers understand how they can empower users by enabling self-explanation. Finally, we present a set of empirically-grounded, user-centered design principles that may guide developers to create successful explainable systems. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.04972
"AI-based Blackbox Code Deobfuscation: Understand, Improve and Mitigate",Grégoire Menguy;Sébastien Bardin;Richard Bonichon;Cauim de Souza Lima,"Code obfuscation aims at protecting Intellectual Property and other secrets embedded into software from being retrieved. Recent works leverage advances in artificial intelligence with the hope of getting blackbox deobfuscators completely immune to standard (whitebox) protection mechanisms. While promising, this new field of AI-based blackbox deobfuscation is still in its infancy. In this article we deepen the state of AI-based blackbox deobfuscation in three key directions: understand the current state-of-the-art, improve over it and design dedicated protection mechanisms. In particular, we define a novel generic framework for AI-based blackbox deobfuscation encompassing prior work and highlighting key components; we are the first to point out that the search space underlying code deobfuscation is too unstable for simulation-based methods (e.g., Monte Carlo Tres Search used in prior work) and advocate the use of robust methods such as S-metaheuritics; we propose the new optimized AI-based blackbox deobfuscator Xyntia which significantly outperforms prior work in terms of success rate (especially with small time budget) while being completely immune to the most recent anti-analysis code obfuscation methods; and finally we propose two novel protections against AI-based blackbox deobfuscation, allowing to counter Xyntia's powerful attacks. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.04805
CorrDetector: A Framework for Structural Corrosion Detection from Drone Images using Ensemble Deep Learning,Abdur Rahim Mohammad Forkan;Yong-Bin Kang;Prem Prakash Jayaraman;Kewen Liao;Rohit Kaul;Graham Morgan;Rajiv Ranjan;Samir Sinha,"In this paper, we propose a new technique that applies automated image analysis in the area of structural corrosion monitoring and demonstrate improved efficacy compared to existing approaches. Structural corrosion monitoring is the initial step of the risk-based maintenance philosophy and depends on an engineer's assessment regarding the risk of building failure balanced against the fiscal cost of maintenance. This introduces the opportunity for human error which is further complicated when restricted to assessment using drone captured images for those areas not reachable by humans due to many background noises. The importance of this problem has promoted an active research community aiming to support the engineer through the use of artificial intelligence (AI) image analysis for corrosion detection. In this paper, we advance this area of research with the development of a framework, CorrDetector. CorrDetector uses a novel ensemble deep learning approach underpinned by convolutional neural networks (CNNs) for structural identification and corrosion feature extraction. We provide an empirical evaluation using real-world images of a complicated structure (e.g. telecommunication tower) captured by drones, a typical scenario for engineers. Our study demonstrates that the ensemble approach of \model significantly outperforms the state-of-the-art in terms of classification accuracy. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.04686
Security and Privacy for Artificial Intelligence: Opportunities and Challenges,Ayodeji Oseni;Nour Moustafa;Helge Janicke;Peng Liu;Zahir Tari;Athanasios Vasilakos,"The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications and reviewed several cyber defences that would protect AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies. △ Less","9 February, 2021",https://arxiv.org/pdf/2102.04661
Fairness for Unobserved Characteristics: Insights from Technological Impacts on Queer Communities,Nenad Tomasev;Kevin R. McKee;Jackie Kay;Shakir Mohamed,"Advances in algorithmic fairness have largely omitted sexual orientation and gender identity. We explore queer concerns in privacy, censorship, language, online safety, health, and employment to study the positive and negative effects of artificial intelligence on queer communities. These issues underscore the need for new directions in fairness research that take into account a multiplicity of considerations, from privacy preservation, context sensitivity and process fairness, to an awareness of sociotechnical impact and the increasingly important role of inclusive and participatory research processes. Most current approaches for algorithmic fairness assume that the target characteristics for fairness--frequently, race and legal gender--can be observed or recorded. Sexual orientation and gender identity are prototypical instances of unobserved characteristics, which are frequently missing, unknown or fundamentally unmeasurable. This paper highlights the importance of developing new approaches for algorithmic fairness that break away from the prevailing assumption of observed characteristics. △ Less","28 April, 2021",https://arxiv.org/pdf/2102.04257
AI Development for the Public Interest: From Abstraction Traps to Sociotechnical Risks,McKane Andrus;Sarah Dean;Thomas Krendl Gilbert;Nathan Lambert;Tom Zick,"Despite interest in communicating ethical problems and social contexts within the undergraduate curriculum to advance Public Interest Technology (PIT) goals, interventions at the graduate level remain largely unexplored. This may be due to the conflicting ways through which distinct Artificial Intelligence (AI) research tracks conceive of their interface with social contexts. In this paper we track the historical emergence of sociotechnical inquiry in three distinct subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions of PIT stem from the particular dangers faced by past integration of technical systems within a normative social order. We further interrogate how these histories dictate the response of each subfield to conceptual traps, as defined in the Science and Technology Studies literature. Finally, through a comparative analysis of these currently siloed fields, we present a roadmap for a unified approach to sociotechnical graduate pedagogy in AI. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.04255
A Data-Driven Approach to Violin Making,Sebastian Gonzalez;Davide Salvi;Daniel Baeza;Fabio Antonacci;Augusto Sarti,"Of all the characteristics of a violin, those that concern its shape are probably the most important ones, as the violin maker has complete control over them. Contemporary violin making, however, is still based more on tradition than understanding, and a definitive scientific study of the specific relations that exist between shape and vibrational properties is yet to come and sorely missed. In this article, using standard statistical learning tools, we show that the modal frequencies of violin tops can, in fact, be predicted from geometric parameters, and that artificial intelligence can be successfully applied to traditional violin making. We also study how modal frequencies vary with the thicknesses of the plate (a process often referred to as {\em plate tuning}) and discuss the complexity of this dependency. Finally, we propose a predictive tool for plate tuning, which takes into account material and geometric parameters. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.04254
Convolutional Neural Network Interpretability with General Pattern Theory,Erico Tjoa;Guan Cuntai,"Ongoing efforts to understand deep neural networks (DNN) have provided many insights, but DNNs remain incompletely understood. Improving DNN's interpretability has practical benefits, such as more accountable usage, better algorithm maintenance and improvement. The complexity of dataset structure may contribute to the difficulty in solving interpretability problem arising from DNN's black-box mechanism. Thus, we propose to use pattern theory formulated by Ulf Grenander, in which data can be described as configurations of fundamental objects that allow us to investigate convolutional neural network's (CNN) interpretability in a component-wise manner. Specifically, U-Net-like structure is formed by attaching expansion blocks (EB) to ResNet, allowing it to perform semantic segmentation-like tasks at its EB output channels designed to be compatible with pattern theory's configurations. Through these modules, some heatmap-based explainable artificial intelligence (XAI) methods will be shown to extract explanations w.r.t individual generators that make up a single data sample, potentially reducing the impact of dataset's complexity to interpretability problem. The MNIST-equivalent dataset containing pattern theory's elements is designed to facilitate smoother entry into this framework, along which the theory's generative aspect is naturally presented. △ Less","5 February, 2021",https://arxiv.org/pdf/2102.04247
"Concepts, Properties and an Approach for Compositional Generalization",Yuanpeng Li,"Compositional generalization is the capacity to recognize and imagine a large amount of novel combinations from known components. It is a key in human intelligence, but current neural networks generally lack such ability. This report connects a series of our work for compositional generalization, and summarizes an approach. The first part contains concepts and properties. The second part looks into a machine learning approach. The approach uses architecture design and regularization to regulate information of representations. This report focuses on basic ideas with intuitive and illustrative explanations. We hope this work would be helpful to clarify fundamentals of compositional generalization and lead to advance artificial intelligence. △ Less","8 February, 2021",https://arxiv.org/pdf/2102.04225
What we are is more than what we do,Larissa Albantakis;Giulio Tononi,"If we take the subjective character of consciousness seriously, consciousness becomes a matter of ""being"" rather than ""doing"". Because ""doing"" can be dissociated from ""being"", functional criteria alone are insufficient to decide whether a system possesses the necessary requirements for being a physical substrate of consciousness. The dissociation between ""being"" and ""doing"" is most salient in artificial general intelligence, which may soon replicate any human capacity: computers can perform complex functions (in the limit resembling human behavior) in the absence of consciousness. Complex behavior becomes meaningless if it is not performed by a conscious being. △ Less","21 January, 2021",https://arxiv.org/pdf/2102.04219
Social and behavioral determinants of health in the era of artificial intelligence with electronic health records: A scoping review,Anusha Bompelli;Yanshan Wang;Ruyuan Wan;Esha Singh;Yuqi Zhou;Lin Xu;David Oniani;Bhavani Singh Agnikula Kshatriya;Joyce;E. Balls-Berry;Rui Zhang,"Background: There is growing evidence that social and behavioral determinants of health (SBDH) play a substantial effect in a wide range of health outcomes. Electronic health records (EHRs) have been widely employed to conduct observational studies in the age of artificial intelligence (AI). However, there has been little research into how to make the most of SBDH information from EHRs. Methods: A systematic search was conducted in six databases to find relevant peer-reviewed publications that had recently been published. Relevance was determined by screening and evaluating the articles. Based on selected relevant studies, a methodological analysis of AI algorithms leveraging SBDH information in EHR data was provided. Results: Our synthesis was driven by an analysis of SBDH categories, the relationship between SBDH and healthcare-related statuses, and several NLP approaches for extracting SDOH from clinical literature. Discussion: The associations between SBDH and health outcomes are complicated and diverse; several pathways may be involved. Using Natural Language Processing (NLP) technology to support the extraction of SBDH and other clinical ideas simplifies the identification and extraction of essential concepts from clinical data, efficiently unlocks unstructured data, and aids in the resolution of unstructured data-related issues. Conclusion: Despite known associations between SBDH and disease, SBDH factors are rarely investigated as interventions to improve patient outcomes. Gaining knowledge about SBDH and how SBDH data can be collected from EHRs using NLP approaches and predictive models improves the chances of influencing health policy change for patient wellness, and ultimately promoting health and health equity. Keywords: Social and Behavioral Determinants of Health, Artificial Intelligence, Electronic Health Records, Natural Language Processing, Predictive Model △ Less","13 June, 2021",https://arxiv.org/pdf/2102.04216
My Boss the Computer: A Bayesian analysis of socio-demographic and cross-cultural determinants of attitude toward the Non-Human Resource Management,Mantello Peter;Manh-Tung Ho;Minh-Hoang Nguyen;Quan-Hoang Vuong,"Human resource management technologies have moved from biometric surveillance to emotional artificial intelligence (AI) that monitor employees' engagement and productivity, analyze video interviews and CVs of job applicants. The rise of the US$20 billion emotional AI industry will transform the future workplace. Yet, besides no international consensus on the principles or standards for such technologies, there is a lack of cross-cultural research on future job seekers' attitude toward such use of AI technologies. This study collects a cross-sectional dataset of 1,015 survey responses of international students from 48 countries and 8 regions worldwide. A majority of the respondents (52%) are concerned about being managed by AI. Following the hypothetico-deductivist philosophy of science, we use the MCMC Hamiltonian approach and conduct a detailed comparison of 10 Bayesian network models with the PSIS-LOO method. We consistently find having a higher income, being male, majoring in business, and/or self-rated familiarity with AI correlate with a more positive view of emotional AI in the workplace. There is also a stark cross-cultural and cross-regional difference. Our analysis shows people from economically less developed regions (Africa, Oceania, Central Asia) tend to exhibit less concern for AI managers. And for East Asian countries, 64% of the Japanese, 56% of the South Korean, and 42% of the Chinese professed the trusting attitude. In contrast, an overwhelming majority of 75% of the European and Northern American possesses the worrying/neutral attitude toward being managed by AI. Regarding religion, Muslim students correlate with the most concern toward emotional AI in the workplace. When religiosity is higher, the correlation becomes stronger for Muslim and Buddhist students. △ Less","24 January, 2021",https://arxiv.org/pdf/2102.04213
Guilty Artificial Minds,Michael T. Stuart;Markus Kneer,"The concepts of blameworthiness and wrongness are of fundamental importance in human moral life. But to what extent are humans disposed to blame artificially intelligent agents, and to what extent will they judge their actions to be morally wrong? To make progress on these questions, we adopted two novel strategies. First, we break down attributions of blame and wrongness into more basic judgments about the epistemic and conative state of the agent, and the consequences of the agent's actions. In this way, we are able to examine any differences between the way participants treat artificial agents in terms of differences in these more basic judgments. our second strategy is to compare attributions of blame and wrongness across human, artificial, and group agents (corporations). Others have compared attributions of blame and wrongness between human and artificial agents, but the addition of group agents is significant because these agents seem to provide a clear middle-ground between human agents (for whom the notions of blame and wrongness were created) and artificial agents (for whom the question remains open). △ Less","24 January, 2021",https://arxiv.org/pdf/2102.04209
Improving Artificial Teachers by Considering How People Learn and Forget,Aurélien Nioche;Pierre-Alexandre Murena;Carlos de la Torre-Ortiz;Antti Oulasvirta,"The paper presents a novel model-based method for intelligent tutoring, with particular emphasis on the problem of selecting teaching interventions in interaction with humans. Whereas previous work has focused on either personalization of teaching or optimization of teaching intervention sequences, the proposed individualized model-based planning approach represents convergence of these two lines of research. Model-based planning picks the best interventions via interactive learning of a user memory model's parameters. The approach is novel in its use of a cognitive model that can account for several key individual- and material-specific characteristics related to recall/forgetting, along with a planning technique that considers users' practice schedules. Taking a rule-based approach as a baseline, the authors evaluated the method's benefits in a controlled study of artificial teaching in second-language vocabulary learning (N=53). △ Less","19 February, 2021",https://arxiv.org/pdf/2102.04174
Deep Reinforcement Learning for the Control of Robotic Manipulation: A Focussed Mini-Review,Rongrong Liu;Florent Nageotte;Philippe Zanne;Michel de Mathelin;Birgitta Dresp-Langley,"Deep learning has provided new ways of manipulating, processing and analyzing data. It sometimes may achieve results comparable to, or surpassing human expert performance, and has become a source of inspiration in the era of artificial intelligence. Another subfield of machine learning named reinforcement learning, tries to find an optimal behavior strategy through interactions with the environment. Combining deep learning and reinforcement learning permits resolving critical issues relative to the dimensionality and scalability of data in tasks with sparse reward signals, such as robotic manipulation and control tasks, that neither method permits resolving when applied on its own. In this paper, we present recent significant progress of deep reinforcement learning algorithms, which try to tackle the problems for the application in the domain of robotic manipulation control, such as sample efficiency and generalization. Despite these continuous improvements, currently, the challenges of learning robust and versatile manipulation skills for robots with deep reinforcement learning are still far from being resolved for real world applications. △ Less","8 February, 2021",https://arxiv.org/pdf/2102.04148
Multisource AI Scorecard Table for System Evaluation,Erik Blasch;James Sung;Tao Nguyen,"The paper describes a Multisource AI Scorecard Table (MAST) that provides the developer and user of an artificial intelligence (AI)/machine learning (ML) system with a standard checklist focused on the principles of good analysis adopted by the intelligence community (IC) to help promote the development of more understandable systems and engender trust in AI outputs. Such a scorecard enables a transparent, consistent, and meaningful understanding of AI tools applied for commercial and government use. A standard is built on compliance and agreement through policy, which requires buy-in from the stakeholders. While consistency for testing might only exist across a standard data set, the community requires discussion on verification and validation approaches which can lead to interpretability, explainability, and proper use. The paper explores how the analytic tradecraft standards outlined in Intelligence Community Directive (ICD) 203 can provide a framework for assessing the performance of an AI system supporting various operational needs. These include sourcing, uncertainty, consistency, accuracy, and visualization. Three use cases are presented as notional examples that support security for comparative analysis. △ Less","7 February, 2021",https://arxiv.org/pdf/2102.03985
Consequences of Misaligned AI,Simon Zhuang;Dylan Hadfield-Menell,AI systems often rely on two key components: a specified goal or reward function and an optimization algorithm to compute the optimal behavior for that goal. This approach is intended to provide value for a principal: the user on whose behalf the agent acts. The objectives given to these agents often refer to a partial specification of the principal's goals. We consider the cost of this incompleteness by analyzing a model of a principal and an agent in a resource constrained world where the L attributes of the state correspond to different sources of utility for the principal. We assume that the reward function given to the agent only has support on J < L attributes. The contributions of our paper are as follows: 1) we propose a novel model of an incomplete principal-agent problem from artificial intelligence; 2) we provide necessary and sufficient conditions under which indefinitely optimizing for any incomplete proxy objective leads to arbitrarily low overall utility; and 3) we show how modifying the setup to allow reward functions that reference the full state or allowing the principal to update the proxy objective over time can lead to higher utility solutions. The results in this paper argue that we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable. △ Less,"7 February, 2021",https://arxiv.org/pdf/2102.03896
Single Run Action Detector over Video Stream -- A Privacy Preserving Approach,Anbumalar Saravanan;Justin Sanchez;Hassan Ghasemzadeh;Aurelia Macabasco-O'Connell;Hamed Tabkhi,"This paper takes initial strides at designing and evaluating a vision-based system for privacy ensured activity monitoring. The proposed technology utilizing Artificial Intelligence (AI)-empowered proactive systems offering continuous monitoring, behavioral analysis, and modeling of human activities. To this end, this paper presents Single Run Action Detector (S-RAD) which is a real-time privacy-preserving action detector that performs end-to-end action localization and classification. It is based on Faster-RCNN combined with temporal shift modeling and segment based sampling to capture the human actions. Results on UCF-Sports and UR Fall dataset present comparable accuracy to State-of-the-Art approaches with significantly lower model size and computation demand and the ability for real-time execution on edge embedded device (e.g. Nvidia Jetson Xavier). △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03391
Artificial Intelligence based Sensor Data Analytics Framework for Remote Electricity Network Condition Monitoring,Tharmakulasingam Sirojan,"Rural electrification demands the use of inexpensive technologies such as single wire earth return (SWER) networks. There is a steadily growing energy demand from remote consumers, and the capacity of existing lines may become inadequate soon. Furthermore, high impedance arcing faults (HIF) from SWER lines can cause catastrophic bushfires such as the 2009 Black Saturday event. As a solution, reliable remote electricity networks can be established through breaking the existing systems down into microgrids, and existing SWER lines can be utilised to interconnect those microgrids. The development of such reliable networks with better energy demand management will rely on having an integrated network-wide condition monitoring system. As the first contribution of this thesis, a distributed online monitoring platform is developed that incorporates power quality monitoring, real-time HIF identification and transient classification in SWER network. Artificial Intelligence (AI) based techniques are developed to classify faults and transients. The proposed approach demonstrates higher HIF detection accuracy (98.67%) and reduced detection latency (115.2 ms). Secondly, a remote consumer load identification methodology is developed to detect the load type from its transients. An edge computing-based architecture is proposed to facilitate the high-frequency analysis for load identification. The proposed approach is evaluated in real-time, and it achieves an average accuracy of 98% in identifying different loads. Finally, a deep neural network-based energy disaggregation framework is developed to separate the load specific energy usage from an aggregated signal. The proposed framework is evaluated using a real-world data set. It improves the signal aggregate error by 44% and mean aggregate error by 19% in comparison with the state-of-the-art techniques. △ Less","21 January, 2021",https://arxiv.org/pdf/2102.03356
Reconfigurable Intelligent Surface Assisted Edge Machine Learning,Shanfeng Huang;Shuai Wang;Rui Wang;Miaowen Wen;Kaibin Huang,"The ever-growing popularity and rapid improving of artificial intelligence (AI) have raised rethinking on the evolution of wireless networks. Mobile edge computing (MEC) provides a natural platform for AI applications since it provides rich computation resources to train AI models, as well as low-latency access to the data generated by mobile and Internet of Things devices. In this paper, we present an infrastructure to perform machine learning tasks at an MEC server with the assistance of a reconfigurable intelligent surface (RIS). In contrast to conventional communication systems where the principal criteria are to maximize the throughput, we aim at optimizing the learning performance. Specifically, we minimize the maximum learning error of all users by jointly optimizing the beamforming vectors of the base station and the phase-shift matrix of the RIS. An alternating optimization-based framework is proposed to optimize the two terms iteratively, where closed-form expressions of the beamforming vectors are derived, and an alternating direction method of multipliers (ADMM)-based algorithm is designed together with an error level searching framework to effectively solve the nonconvex optimization problem of the phase-shift matrix. Simulation results demonstrate significant gains of deploying an RIS and validate the advantages of our proposed algorithms over various benchmarks. △ Less","5 February, 2021",https://arxiv.org/pdf/2102.03185
"""I Don't Think So"": Summarizing Policy Disagreements for Agent Comparison",Yotam Amitai;Ofra Amir,"With Artificial Intelligence on the rise, human interaction with autonomous agents becomes more frequent. Effective human-agent collaboration requires users to understand the agent's behavior, as failing to do so may cause reduced productivity, misuse or frustration. Agent strategy summarization methods are used to describe the strategy of an agent to its destined user through demonstration. A summary's objective is to maximize the user's understanding of the agent's aptitude by showcasing its behaviour in a selected set of world states. While shown to be useful, we show that current methods are limited when tasked with comparing between agents, as each summary is independently generated for a specific agent. In this paper, we propose a novel method for generating dependent and contrastive summaries that emphasize the differences between agent policies by identifying states in which the agents disagree on the best course of action. We conduct user studies to assess the usefulness of disagreement-based summaries for identifying superior agents and conveying agent differences. Results show disagreement-based summaries lead to improved user performance compared to summaries generated using HIGHLIGHTS, a strategy summarization algorithm which generates summaries for each agent independently. △ Less","2 December, 2021",https://arxiv.org/pdf/2102.03064
Toward a Rational and Ethical Sociotechnical System of Autonomous Vehicles: A Novel Application of Multi-Criteria Decision Analysis,Veljko Dubljević;George F. List;Jovan Milojevich;Nirav Ajmeri;William Bauer;Munindar P. Singh;Eleni Bardaka;Thomas Birkland;Charles Edwards;Roger Mayer;Ioan Muntean;Thomas Powers;Hesham Rakha;Vance Ricks;M. Shoaib Samandar,"The expansion of artificial intelligence (AI) and autonomous systems has shown the potential to generate enormous social good while also raising serious ethical and safety concerns. AI technology is increasingly adopted in transportation. A survey of various in-vehicle technologies found that approximately 64% of the respondents used a smartphone application to assist with their travel. The top-used applications were navigation and real-time traffic information systems. Among those who used smartphones during their commutes, the top-used applications were navigation and entertainment. There is a pressing need to address relevant social concerns to allow for the development of systems of intelligent agents that are informed and cognizant of ethical standards. Doing so will facilitate the responsible integration of these systems in society. To this end, we have applied Multi-Criteria Decision Analysis (MCDA) to develop a formal Multi-Attribute Impact Assessment (MAIA) questionnaire for examining the social and ethical issues associated with the uptake of AI. We have focused on the domain of autonomous vehicles (AVs) because of their imminent expansion. However, AVs could serve as a stand-in for any domain where intelligent, autonomous agents interact with humans, either on an individual level (e.g., pedestrians, passengers) or a societal level. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.02928
Deep reinforcement learning-based image classification achieves perfect testing set accuracy for MRI brain tumors with a training set of only 30 images,Joseph Stember;Hrithwik Shalu,"Purpose: Image classification may be the fundamental task in imaging artificial intelligence. We have recently shown that reinforcement learning can achieve high accuracy for lesion localization and segmentation even with minuscule training sets. Here, we introduce reinforcement learning for image classification. In particular, we apply the approach to normal vs. tumor-containing 2D MRI brain images. Materials and Methods: We applied multi-step image classification to allow for combined Deep Q learning and TD(0) Q learning. We trained on a set of 30 images (15 normal and 15 tumor-containing). We tested on a separate set of 30 images (15 normal and 15 tumor-containing). For comparison, we also trained and tested a supervised deep-learning classification network on the same set of training and testing images. Results: Whereas the supervised approach quickly overfit the training data and as expected performed poorly on the testing set (57% accuracy, just over random guessing), the reinforcement learning approach achieved an accuracy of 100%. Conclusion: We have shown a proof-of-principle application of reinforcement learning to the classification of brain tumors. We achieved perfect testing set accuracy with a training set of merely 30 images. △ Less","19 March, 2021",https://arxiv.org/pdf/2102.02895
Computational identification of significant actors in paintings through symbols and attributes,David G. Stork;Anthony Bourached;George H. Cann;Ryan-Rhys Griffiths,"The automatic analysis of fine art paintings presents a number of novel technical challenges to artificial intelligence, computer vision, machine learning, and knowledge representation quite distinct from those arising in the analysis of traditional photographs. The most important difference is that many realist paintings depict stories or episodes in order to convey a lesson, moral, or meaning. One early step in automatic interpretation and extraction of meaning in artworks is the identifications of figures (actors). In Christian art, specifically, one must identify the actors in order to identify the Biblical episode or story depicted, an important step in understanding the artwork. We designed an automatic system based on deep convolutional neural networks and simple knowledge database to identify saints throughout six centuries of Christian art based in large part upon saints symbols or attributes. Our work represents initial steps in the broad task of automatic semantic interpretation of messages and meaning in fine art. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.02732
"Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models",Alex Tamkin;Miles Brundage;Jack Clark;Deep Ganguli,"On October 14th, 2020, researchers from OpenAI, the Stanford Institute for Human-Centered Artificial Intelligence, and other universities convened to discuss open research questions surrounding GPT-3, the largest publicly-disclosed dense language model at the time. The meeting took place under Chatham House Rules. Discussants came from a variety of research backgrounds including computer science, linguistics, philosophy, political science, communications, cyber policy, and more. Broadly, the discussion centered around two main questions: 1) What are the technical capabilities and limitations of large language models? 2) What are the societal effects of widespread use of large language models? Here, we provide a detailed summary of the discussion organized by the two themes above. △ Less","4 February, 2021",https://arxiv.org/pdf/2102.02503
Typing Errors in Factual Knowledge Graphs: Severity and Possible Ways Out,Peiran Yao;Denilson Barbosa,"Factual knowledge graphs (KGs) such as DBpedia and Wikidata have served as part of various downstream tasks and are also widely adopted by artificial intelligence research communities as benchmark datasets. However, we found these KGs to be surprisingly noisy. In this study, we question the quality of these KGs, where the typing error rate is estimated to be 27% for coarse-grained types on average, and even 73% for certain fine-grained types. In pursuit of solutions, we propose an active typing error detection algorithm that maximizes the utilization of both gold and noisy labels. We also comprehensively discuss and compare unsupervised, semi-supervised, and supervised paradigms to deal with typing errors in factual KGs. The outcomes of this study provide guidelines for researchers to use noisy factual KGs. To help practitioners deploy the techniques and conduct further research, we published our code and data. △ Less","3 February, 2021",https://arxiv.org/pdf/2102.02307
"Unbox the Black-box for the Medical Explainable AI via Multi-modal and Multi-centre Data Fusion: A Mini-Review, Two Showcases and Beyond",Guang Yang;Qinghao Ye;Jun Xia,"Explainable Artificial Intelligence (XAI) is an emerging research topic of machine learning aimed at unboxing how AI systems' black-box choices are made. This research field inspects the measures and models involved in decision-making and seeks solutions to explain them explicitly. Many of the machine learning algorithms can not manifest how and why a decision has been cast. This is particularly true of the most popular deep neural network approaches currently in use. Consequently, our confidence in AI systems can be hindered by the lack of explainability in these black-box models. The XAI becomes more and more crucial for deep learning powered applications, especially for medical and healthcare studies, although in general these deep neural networks can return an arresting dividend in performance. The insufficient explainability and transparency in most existing AI systems can be one of the major reasons that successful implementation and integration of AI tools into routine clinical practice are uncommon. In this study, we first surveyed the current progress of XAI and in particular its advances in healthcare applications. We then introduced our solutions for XAI leveraging multi-modal and multi-centre data fusion, and subsequently validated in two showcases following real clinical scenarios. Comprehensive quantitative and qualitative analyses can prove the efficacy of our proposed XAI solutions, from which we can envisage successful applications in a broader range of clinical questions. △ Less","3 February, 2021",https://arxiv.org/pdf/2102.01998
BiasFinder: Metamorphic Test Generation to Uncover Bias for Sentiment Analysis Systems,Muhammad Hilmi Asyrofi;Zhou Yang;Imam Nur Bani Yusuf;Hong Jin Kang;Ferdian Thung;David Lo,"Artificial Intelligence (AI) software systems, such as Sentiment Analysis (SA) systems, typically learn from large amounts of data that may reflect human biases. Consequently, the machine learning model in such software systems may exhibit unintended demographic bias based on specific characteristics (e.g., gender, occupation, country-of-origin, etc.). Such biases manifest in an SA system when it predicts a different sentiment for similar texts that differ only in the characteristic of individuals described. Existing studies on revealing bias in SA systems rely on the production of sentences from a small set of short, predefined templates. To address this limitation, we present BisaFinder, an approach to discover biased predictions in SA systems via metamorphic testing. A key feature of BisaFinder is the automatic curation of suitable templates based on the pieces of text from a large corpus, using various Natural Language Processing (NLP) techniques to identify words that describe demographic characteristics. Next, BisaFinder instantiates new text from these templates by filling in placeholders with words associated with a class of a characteristic (e.g., gender-specific words such as female names, ""she"", ""her""). These texts are used to tease out bias in an SA system. BisaFinder identifies a bias-uncovering test case when it detects that the SA system exhibits demographic bias for a pair of texts, i.e., it predicts a different sentiment for texts that differ only in words associated with a different class (e.g., male vs. female) of a target characteristic (e.g., gender). Our empirical evaluation showed that BiasFinder can effectively create a larger number of fluent and diverse test cases that uncover various biases in an SA system. △ Less","4 October, 2021",https://arxiv.org/pdf/2102.01859
TAD: Trigger Approximation based Black-box Trojan Detection for AI,Xinqiao Zhang;Huili Chen;Farinaz Koushanfar,"An emerging amount of intelligent applications have been developed with the surge of Machine Learning (ML). Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled and activated by the stealthy trigger. We call this vulnerable model adversarial artificial intelligence (AI). In this paper, we target to design a robust Trojan detection scheme that inspects whether a pre-trained AI model has been Trojaned before its deployment. Prior works are oblivious of the intrinsic property of trigger distribution and try to reconstruct the trigger pattern using simple heuristics, i.e., stimulating the given model to incorrect outputs. As a result, their detection time and effectiveness are limited. We leverage the observation that the pixel trigger typically features spatial dependency and propose TAD, the first trigger approximation based Trojan detection framework that enables fast and scalable search of the trigger in the input space. Furthermore, TAD can also detect Trojans embedded in the feature space where certain filter transformations are used to activate the Trojan. We perform extensive experiments to investigate the performance of the TAD across various datasets and ML models. Empirical results show that TAD achieves a ROC-AUC score of 0:91 on the public TrojAI dataset 1 and the average detection time per model is 7:1 minutes. △ Less","20 April, 2021",https://arxiv.org/pdf/2102.01815
The Ethical Implications of Shared Medical Decision Making without Providing Adequate Computational Support to the Care Provider and to the Patient,Yuval Shahar,"There is a clear need to involve patients in medical decisions. However, cognitive psychological research has highlighted the cognitive limitations of humans with respect to 1. Probabilistic assessment of the patient state and of potential outcomes of various decisions, 2. Elicitation of the patient utility function, and 3. Integration of the probabilistic knowledge and of patient preferences to determine the optimal strategy. Therefore, without adequate computational support, current shared decision models have severe ethical deficiencies. An informed consent model unfairly transfers the responsibility to a patient who does not have the necessary knowledge, nor the integration capability. A paternalistic model endows with exaggerated power a physician who might not be aware of the patient preferences, is prone to multiple cognitive biases, and whose computational integration capability is bounded. Recent progress in Artificial Intelligence suggests adding a third agent: a computer, in all deliberative medical decisions: Non emergency medical decisions in which more than one alternative exists, the patient preferences can be elicited, the therapeutic alternatives might be influenced by these preferences, medical knowledge exists regarding the likelihood of the decision outcomes, and there is sufficient decision time. Ethical physicians should exploit computational decision support technologies, neither making the decisions solely on their own, nor shirking their duty and shifting the responsibility to patients in the name of informed consent. The resulting three way (patient, care provider, computer) human machine model that we suggest emphasizes the patient preferences, the physician knowledge, and the computational integration of both aspects, does not diminish the physician role, but rather brings out the best in human and machine. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.01811
"A Survey on Understanding, Visualizations, and Explanation of Deep Neural Networks",Atefeh Shahroudnejad,"Recent advancements in machine learning and signal processing domains have resulted in an extensive surge of interest in Deep Neural Networks (DNNs) due to their unprecedented performance and high accuracy for different and challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in control systems and medical applications), it is of paramount importance to understand, trust, and in one word ""explain"" the argument behind deep models' decisions. In many applications, artificial neural networks (including DNNs) are considered as black-box systems, which do not provide sufficient clue on their internal processing actions. Although some recent efforts have been initiated to explain the behaviors and decisions of deep networks, explainable artificial intelligence (XAI) domain, which aims at reasoning about the behavior and decisions of DNNs, is still in its infancy. The aim of this paper is to provide a comprehensive overview on Understanding, Visualization, and Explanation of the internal and overall behavior of DNNs. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.01792
Reliability Analysis of Artificial Intelligence Systems Using Recurrent Events Data from Autonomous Vehicles,Yili Hong;Jie Min;Caleb B. King;William Q. Meeker,"Artificial intelligence (AI) systems have become increasingly common and the trend will continue. Examples of AI systems include autonomous vehicles (AV), computer vision, natural language processing, and AI medical experts. To allow for safe and effective deployment of AI systems, the reliability of such systems needs to be assessed. Traditionally, reliability assessment is based on reliability test data and the subsequent statistical modeling and analysis. The availability of reliability data for AI systems, however, is limited because such data are typically sensitive and proprietary. The California Department of Motor Vehicles (DMV) oversees and regulates an AV testing program, in which many AV manufacturers are conducting AV road tests. Manufacturers participating in the program are required to report recurrent disengagement events to California DMV. This information is being made available to the public. In this paper, we use recurrent disengagement events as a representation of the reliability of the AI system in AV, and propose a statistical framework for modeling and analyzing the recurrent events data from AV driving tests. We use traditional parametric models in software reliability and propose a new nonparametric model based on monotonic splines to describe the event process. We develop inference procedures for selecting the best models, quantifying uncertainty, and testing heterogeneity in the event process. We then analyze the recurrent events data from four AV manufacturers, and make inferences on the reliability of the AI systems in AV. We also describe how the proposed analysis can be applied to assess the reliability of other AI systems. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.01740
Generacion de voces artificiales infantiles en castellano con acento costarricense,Ana Lilia Alvarez-Blanco;Eugenia Cordoba-Warner;Marvin Coto-Jimenez;Vivian Fallas-Lopez;Maribel Morales Rodriguez,"This article evaluates a first experience of generating artificial children's voices with a Costa Rican accent, using the technique of statistical parametric speech synthesis based on Hidden Markov Models. The process of recording the voice samples used for learning the models, the fundamentals of the technique used and the subjective evaluation of the results through the perception of a group of people is described. The results show that the intelligibility of the results, evaluated in isolated words, is lower than the voices recorded by the group of participating children. Similarly, the detection of the age and gender of the speaking person is significantly affected in artificial voices, relative to recordings of natural voices. These results show the need to obtain larger amounts of data, in addition to becoming a numerical reference for future developments resulting from new data or from processes to improve results in the same technique. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.01692
The Privatization of AI Research(-ers): Causes and Potential Consequences -- From university-industry interaction to public research brain-drain?,Roman Jurowetzki;Daniel Hain;Juan Mateos-Garcia;Konstantinos Stathoulopoulos,"The private sector is playing an increasingly important role in basic Artificial Intelligence (AI) R&D. This phenomenon, which is reflected in the perception of a brain drain of researchers from academia to industry, is raising concerns about a privatisation of AI research which could constrain its societal benefits. We contribute to the evidence base by quantifying transition flows between industry and academia and studying its drivers and potential consequences. We find a growing net flow of researchers from academia to industry, particularly from elite institutions into technology companies such as Google, Microsoft and Facebook. Our survival regression analysis reveals that researchers working in the field of deep learning as well as those with higher average impact are more likely to transition into industry. A difference-in-differences analysis of the effect of switching into industry on a researcher's influence proxied by citations indicates that an initial increase in impact declines as researchers spend more time in industry. This points at a privatisation of AI knowledge compared to a counterfactual where those high-impact researchers had remained in academia. Our findings highlight the importance of strengthening the public AI research sphere in order to ensure that the future of this powerful technology is not dominated by private interests. △ Less","15 February, 2021",https://arxiv.org/pdf/2102.01648
Unassisted Noise Reduction of Chemical Reaction Data Sets,Alessandra Toniato;Philippe Schwaller;Antonio Cardinale;Joppe Geluykens;Teodoro Laino,"Existing deep learning models applied to reaction prediction in organic chemistry can reach high levels of accuracy (> 90% for Natural Language Processing-based ones). With no chemical knowledge embedded than the information learnt from reaction data, the quality of the data sets plays a crucial role in the performance of the prediction models. While human curation is prohibitively expensive, the need for unaided approaches to remove chemically incorrect entries from existing data sets is essential to improve artificial intelligence models' performance in synthetic chemistry tasks. Here we propose a machine learning-based, unassisted approach to remove chemically wrong entries from chemical reaction collections. We applied this method to the collection of chemical reactions Pistachio and to an open data set, both extracted from USPTO (United States Patent Office) patents. Our results show an improved prediction quality for models trained on the cleaned and balanced data sets. For the retrosynthetic models, the round-trip accuracy metric grows by 13 percentage points and the value of the cumulative Jensen Shannon divergence decreases by 30% compared to its original record. The coverage remains high with 97%, and the value of the class-diversity is not affected by the cleaning. The proposed strategy is the first unassisted rule-free technique to address automatic noise reduction in chemical data sets. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.01399
"Applications of Federated Learning in Smart Cities: Recent Advances, Taxonomy, and Open Challenges",Zhaohua Zheng;Yize Zhou;Yilong Sun;Zhang Wang;Boyi Liu;Keqiu Li,"Federated learning plays an important role in the process of smart cities. With the development of big data and artificial intelligence, there is a problem of data privacy protection in this process. Federated learning is capable of solving this problem. This paper starts with the current developments of federated learning and its applications in various fields. We conduct a comprehensive investigation. This paper summarize the latest research on the application of federated learning in various fields of smart cities. In-depth understanding of the current development of federated learning from the Internet of Things, transportation, communications, finance, medical and other fields. Before that, we introduce the background, definition and key technologies of federated learning. Further more, we review the key technologies and the latest results. Finally, we discuss the future applications and research directions of federated learning in smart cities. △ Less","13 March, 2021",https://arxiv.org/pdf/2102.01375
"""Alexa, Can I Program You?"": Student Perceptions of Conversational Artificial Intelligence Before and After Programming Alexa",Jessica Van Brummelen;Viktoriya Tabunshchyk;Tommy Heng,"Growing up in an artificial intelligence-filled world, with Siri and Amazon Alexa often within arm's - or speech's - reach, could have significant impact on children. Conversational agents could influence how students anthropomorphize computer systems or develop a theory of mind. Previous research has explored how conversational agents are used and perceived by children within and outside of learning contexts. This study investigates how middle and high school students' perceptions of Alexa change through programming their own conversational agents in week-long AI education workshops. Specifically, we investigate the workshops' influence on student perceptions of Alexa's intelligence, friendliness, aliveness, safeness, trustworthiness, human-likeness, and feelings of closeness. We found that students felt Alexa was more intelligent and felt closer to Alexa after the workshops. We also found strong correlations between students' perceptions of Alexa's friendliness and trustworthiness, and safeness and trustworthiness. Finally, we explored how students tended to more frequently use computer science-related diction and ideas after the workshops. Based on our findings, we recommend designers carefully consider personification, transparency, playfulness and utility when designing CAs for learning contexts. △ Less","2 February, 2021",https://arxiv.org/pdf/2102.01367
AI4VIS: Survey on Artificial Intelligence Approaches for Data Visualization,Aoyu Wu;Yun Wang;Xinhuan Shu;Dominik Moritz;Weiwei Cui;Haidong Zhang;Dongmei Zhang;Huamin Qu,"Visualizations themselves have become a data format. Akin to other data formats such as text and images, visualizations are increasingly created, stored, shared, and (re-)used with artificial intelligence (AI) techniques. In this survey, we probe the underlying vision of formalizing visualizations as an emerging data format and review the recent advance in applying AI techniques to visualization data (AI4VIS). We define visualization data as the digital representations of visualizations in computers and focus on data visualization (e.g., charts and infographics). We build our survey upon a corpus spanning ten different fields in computer science with an eye toward identifying important common interests. Our resulting taxonomy is organized around WHAT is visualization data and its representation, WHY and HOW to apply AI to visualization data. We highlight a set of common tasks that researchers apply to the visualization data and present a detailed discussion of AI approaches developed to accomplish those tasks. Drawing upon our literature review, we discuss several important research questions surrounding the management and exploitation of visualization data, as well as the role of AI in support of those processes. We make the list of surveyed papers and related material available online at ai4vis.github.io. △ Less","19 July, 2021",https://arxiv.org/pdf/2102.01330
The Limits of Global Inclusion in AI Development,Alan Chan;Chinasa T. Okolo;Zachary Terner;Angelina Wang,"Those best-positioned to profit from the proliferation of artificial intelligence (AI) systems are those with the most economic power. Extant global inequality has motivated Western institutions to involve more diverse groups in the development and application of AI systems, including hiring foreign labour and establishing extra-national data centers and laboratories. However, given both the propensity of wealth to abet its own accumulation and the lack of contextual knowledge in top-down AI solutions, we argue that more focus should be placed on the redistribution of power, rather than just on including underrepresented groups. Unless more is done to ensure that opportunities to lead AI development are distributed justly, the future may hold only AI systems which are unsuited to their conditions of application, and exacerbate inequality. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.01265
Diagnosis of Acute Poisoning Using Explainable Artificial Intelligence,Michael Chary;Ed W Boyer;Michele M Burns,"Medical toxicology is the clinical specialty that treats the toxic effects of substances, be it an overdose, a medication error, or a scorpion sting. The volume of toxicological knowledge and research has, as with other medical specialties, outstripped the ability of the individual clinician to entirely master and stay current with it. The application of machine learning techniques to medical toxicology is challenging because initial treatment decisions are often based on a few pieces of textual data and rely heavily on prior knowledge. ML techniques often do not represent knowledge in a way that is transparent for the physician, raising barriers to usability. Rule-based systems and decision tree learning are more transparent approaches, but often generalize poorly and require expert curation to implement and maintain. Here, we construct a probabilistic logic network to represent a portion of the knowledge base of a medical toxicologist. Our approach transparently mimics the knowledge representation and clinical decision-making of practicing clinicians. The software, dubbed Tak, performs comparably to humans on straightforward cases and intermediate difficulty cases, but is outperformed by humans on challenging clinical cases. Tak outperforms a decision tree classifier at all levels of difficulty. Probabilistic logic provides one form of explainable artificial intelligence that may be more acceptable for use in healthcare, if it can achieve acceptable levels of performance. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.01116
Gamified Crowdsourcing for Idiom Corpora Construction,Gülşen Eryiğit;Ali Şentaş;Johanna Monti,"Learning idiomatic expressions is seen as one of the most challenging stages in second language learning because of their unpredictable meaning. A similar situation holds for their identification within natural language processing applications such as machine translation and parsing. The lack of high-quality usage samples exacerbates this challenge not only for humans but also for artificial intelligence systems. This article introduces a gamified crowdsourcing approach for collecting language learning materials for idiomatic expressions; a messaging bot is designed as an asynchronous multiplayer game for native speakers who compete with each other while providing idiomatic and nonidiomatic usage examples and rating other players' entries. As opposed to classical crowdprocessing annotation efforts in the field, for the first time in the literature, a crowdcreating & crowdrating approach is implemented and tested for idiom corpora construction. The approach is language independent and evaluated on two languages in comparison to traditional data preparation techniques in the field. The reaction of the crowd is monitored under different motivational means (namely, gamification affordances and monetary rewards). The results reveal that the proposed approach is powerful in collecting the targeted materials, and although being an explicit crowdsourcing approach, it is found entertaining and useful by the crowd. The approach has been shown to have the potential to speed up the construction of idiom corpora for different natural languages to be used as second language learning material, training data for supervised idiom identification systems, or samples for lexicographic studies. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.00881
Decentralized Federated Learning Preserves Model and Data Privacy,Thorsten Wittkopp;Alexander Acker,"The increasing complexity of IT systems requires solutions, that support operations in case of failure. Therefore, Artificial Intelligence for System Operations (AIOps) is a field of research that is becoming increasingly focused, both in academia and industry. One of the major issues of this area is the lack of access to adequately labeled data, which is majorly due to legal protection regulations or industrial confidentiality. Methods to mitigate this stir from the area of federated learning, whereby no direct access to training data is required. Original approaches utilize a central instance to perform the model synchronization by periodical aggregation of all model parameters. However, there are many scenarios where trained models cannot be published since its either confidential knowledge or training data could be reconstructed from them. Furthermore the central instance needs to be trusted and is a single point of failure. As a solution, we propose a fully decentralized approach, which allows to share knowledge between trained models. Neither original training data nor model parameters need to be transmitted. The concept relies on teacher and student roles that are assigned to the models, whereby students are trained on the output of their teachers via synthetically generated input data. We conduct a case study on log anomaly detection. The results show that an untrained student model, trained on the teachers output reaches comparable F1-scores as the teacher. In addition, we demonstrate that our method allows the synchronization of several models trained on different distinct training data subsets. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.00880
Hierarchical Variational Autoencoder for Visual Counterfactuals,Nicolas Vercheval;Aleksandra Pizurica,"Conditional Variational Auto Encoders (VAE) are gathering significant attention as an Explainable Artificial Intelligence (XAI) tool. The codes in the latent space provide a theoretically sound way to produce counterfactuals, i.e. alterations resulting from an intervention on a targeted semantic feature. To be applied on real images more complex models are needed, such as Hierarchical CVAE. This comes with a challenge as the naive conditioning is no longer effective. In this paper we show how relaxing the effect of the posterior leads to successful counterfactuals and we introduce VAEX an Hierarchical VAE designed for this approach that can visually audit a classifier in applications. △ Less","1 February, 2021",https://arxiv.org/pdf/2102.00854
Counterfactual Planning in AGI Systems,Koen Holtman,"We present counterfactual planning as a design approach for creating a range of safety mechanisms that can be applied in hypothetical future AI systems which have Artificial General Intelligence. The key step in counterfactual planning is to use an AGI machine learning system to construct a counterfactual world model, designed to be different from the real world the system is in. A counterfactual planning agent determines the action that best maximizes expected utility in this counterfactual planning world, and then performs the same action in the real world. We use counterfactual planning to construct an AGI agent emergency stop button, and a safety interlock that will automatically stop the agent before it undergoes an intelligence explosion. We also construct an agent with an input terminal that can be used by humans to iteratively improve the agent's reward function, where the incentive for the agent to manipulate this improvement process is suppressed. As an example of counterfactual planning in a non-agent AGI system, we construct a counterfactual oracle. As a design approach, counterfactual planning is built around the use of a graphical notation for defining mathematical counterfactuals. This two-diagram notation also provides a compact and readable language for reasoning about the complex types of self-referencing and indirect representation which are typically present inside machine learning agents. △ Less","29 January, 2021",https://arxiv.org/pdf/2102.00834
Human Perceptions on Moral Responsibility of AI: A Case Study in AI-Assisted Bail Decision-Making,Gabriel Lima;Nina Grgić-Hlača;Meeyoung Cha,"How to attribute responsibility for autonomous artificial intelligence (AI) systems' actions has been widely debated across the humanities and social science disciplines. This work presents two experiments (N=200 each) that measure people's perceptions of eight different notions of moral responsibility concerning AI and human agents in the context of bail decision-making. Using real-life adapted vignettes, our experiments show that AI agents are held causally responsible and blamed similarly to human agents for an identical task. However, there was a meaningful difference in how people perceived these agents' moral responsibility; human agents were ascribed to a higher degree of present-looking and forward-looking notions of responsibility than AI agents. We also found that people expect both AI and human decision-makers and advisors to justify their decisions regardless of their nature. We discuss policy and HCI implications of these findings, such as the need for explainable AI in high-stakes scenarios. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00625
Interpretable Reinforcement Learning Inspired by Piaget's Theory of Cognitive Development,Aref Hakimzadeh;Yanbo Xue;Peyman Setoodeh,"Endeavors for designing robots with human-level cognitive abilities have led to different categories of learning machines. According to Skinner's theory, reinforcement learning (RL) plays a key role in human intuition and cognition. Majority of the state-of-the-art methods including deep RL algorithms are strongly influenced by the connectionist viewpoint. Such algorithms can significantly benefit from theories of mind and learning in other disciplines. This paper entertains the idea that theories such as language of thought hypothesis (LOTH), script theory, and Piaget's cognitive development theory provide complementary approaches, which will enrich the RL field. Following this line of thinking, a general computational building block is proposed for Piaget's schema theory that supports the notions of productivity, systematicity, and inferential coherence as described by Fodor in contrast with the connectionism theory. Abstraction in the proposed method is completely upon the system itself and is not externally constrained by any predefined architecture. The whole process matches the Neisser's perceptual cycle model. Performed experiments on three typical control problems followed by behavioral analysis confirm the interpretability of the proposed method and its competitiveness compared to the state-of-the-art algorithms. Hence, the proposed framework can be viewed as a step towards achieving human-like cognition in artificial intelligent systems. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00572
Classification of Shoulder X-Ray Images with Deep Learning Ensemble Models,Fatih Uysal;Fırat Hardalaç;Ozan Peker;Tolga Tolunay;Nil Tokgöz,"Fractures occur in the shoulder area, which has a wider range of motion than other joints in the body, for various reasons. To diagnose these fractures, data gathered from Xradiation (X-ray), magnetic resonance imaging (MRI), or computed tomography (CT) are used. This study aims to help physicians by classifying shoulder images taken from X-ray devices as fracture / non-fracture with artificial intelligence. For this purpose, the performances of 26 deep learning-based pretrained models in the detection of shoulder fractures were evaluated on the musculoskeletal radiographs (MURA) dataset, and two ensemble learning models (EL1 and EL2) were developed. The pretrained models used are ResNet, ResNeXt, DenseNet, VGG, Inception, MobileNet, and their spinal fully connected (Spinal FC) versions. In the EL1 and EL2 models developed using pretrained models with the best performance, test accuracy was 0.8455,0.8472, Cohens kappa was 0.6907, 0.6942 and the area that was related with fracture class under the receiver operating characteristic (ROC) curve (AUC) was 0.8862,0.8695. As a result of 28 different classifications in total, the highest test accuracy and Cohens kappa values were obtained in the EL2 model, and the highest AUC value was obtained in the EL1 model. △ Less","20 March, 2021",https://arxiv.org/pdf/2102.00515
Improving Accountability in Recommender Systems Research Through Reproducibility,Alejandro Bellogín;Alan Said,"Reproducibility is a key requirement for scientific progress. It allows the reproduction of the works of others, and, as a consequence, to fully trust the reported claims and results. In this work, we argue that, by facilitating reproducibility of recommender systems experimentation, we indirectly address the issues of accountability and transparency in recommender systems research from the perspectives of practitioners, designers, and engineers aiming to assess the capabilities of published research works. These issues have become increasingly prevalent in recent literature. Reasons for this include societal movements around intelligent systems and artificial intelligence striving towards fair and objective use of human behavioral data (as in Machine Learning, Information Retrieval, or Human-Computer Interaction). Society has grown to expect explanations and transparency standards regarding the underlying algorithms making automated decisions for and around us. This work surveys existing definitions of these concepts, and proposes a coherent terminology for recommender systems research, with the goal to connect reproducibility to accountability. We achieve this by introducing several guidelines and steps that lead to reproducible and, hence, accountable experimental workflows and research. We additionally analyze several instantiations of recommender system implementations available in the literature, and discuss the extent to which they fit in the introduced framework. With this work, we aim to shed light on this important problem, and facilitate progress in the field by increasing the accountability of research. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00482
Beyond the Command: Feminist STS Research and Critical Issues for the Design of Social Machines,Kelly B. Wagman;Lisa Parks,"Machines, from artificially intelligent digital assistants to embodied robots, are becoming more pervasive in everyday life. Drawing on feminist science and technology studies (STS) perspectives, we demonstrate how machine designers are not just crafting neutral objects, but relationships between machines and humans that are entangled in human social issues such as gender and power dynamics. Thus, in order to create a more ethical and just future, the dominant assumptions currently underpinning the design of these human-machine relations must be challenged and reoriented toward relations of justice and inclusivity. This paper contributes the ""social machine"" as a model for technology designers who seek to recognize the importance, diversity and complexity of the social in their work, and to engage with the agential power of machines. In our model, the social machine is imagined as a potentially equitable relationship partner that has agency and as an ""other"" that is distinct from, yet related to, humans, objects, and animals. We critically examine and contrast our model with tendencies in robotics that consider robots as tools, human companions, animals or creatures, and/or slaves. In doing so, we demonstrate ingrained dominant assumptions about human-machine relations and reveal the challenges of radical thinking in the social machine design space. Finally, we present two design challenges based on non-anthropomorphic figuration and mutuality, and call for experimentation, unlearning dominant tendencies, and reimagining of sociotechnical futures. △ Less","31 January, 2021",https://arxiv.org/pdf/2102.00464
Deep Reinforcement Learning-Based Product Recommender for Online Advertising,Milad Vaali Esfahaani;Yanbo Xue;Peyman Setoodeh,"In online advertising, recommender systems try to propose items from a list of products to potential customers according to their interests. Such systems have been increasingly deployed in E-commerce due to the rapid growth of information technology and availability of large datasets. The ever-increasing progress in the field of artificial intelligence has provided powerful tools for dealing with such real-life problems. Deep reinforcement learning (RL) that deploys deep neural networks as universal function approximators can be viewed as a valid approach for design and implementation of recommender systems. This paper provides a comparative study between value-based and policy-based deep RL algorithms for designing recommender systems for online advertising. The RecoGym environment is adopted for training these RL-based recommender systems, where the long short term memory (LSTM) is deployed to build value and policy networks in these two approaches, respectively. LSTM is used to take account of the key role that order plays in the sequence of item observations by users. The designed recommender systems aim at maximising the click-through rate (CTR) for the recommended items. Finally, guidelines are provided for choosing proper RL algorithms for different scenarios that the recommender system is expected to handle. △ Less","30 January, 2021",https://arxiv.org/pdf/2102.00333
An evolutionary view on the emergence of Artificial Intelligence,Matheus E. Leusin;Bjoern Jindra;Daniel S. Hain,"This paper draws upon the evolutionary concepts of technological relatedness and knowledge complexity to enhance our understanding of the long-term evolution of Artificial Intelligence (AI). We reveal corresponding patterns in the emergence of AI - globally and in the context of specific geographies of the US, Japan, South Korea, and China. We argue that AI emergence is associated with increasing related variety due to knowledge commonalities as well as increasing complexity. We use patent-based indicators for the period between 1974-2018 to analyse the evolution of AI's global technological space, to identify its technological core as well as changes to its overall relatedness and knowledge complexity. At the national level, we also measure countries' overall specialisations against AI-specific ones. At the global level, we find increasing overall relatedness and complexity of AI. However, for the technological core of AI, which has been stable over time, we find decreasing related variety and increasing complexity. This evidence points out that AI innovations related to core technologies are becoming increasingly distinct from each other. At the country level, we find that the US and Japan have been increasing the overall relatedness of their innovations. The opposite is the case for China and South Korea, which we associate with the fact that these countries are overall less technologically developed than the US and Japan. Finally, we observe a stable increasing overall complexity for all countries apart from China, which we explain by the focus of this country in technologies not strongly linked to AI. △ Less","30 January, 2021",https://arxiv.org/pdf/2102.00233
NL-CNN: A Resources-Constrained Deep Learning Model based on Nonlinear Convolution,Radu Dogaru;Ioana Dogaru,"A novel convolution neural network model, abbreviated NL-CNN is proposed, where nonlinear convolution is emulated in a cascade of convolution + nonlinearity layers. The code for its implementation and some trained models are made publicly available. Performance evaluation for several widely known datasets is provided, showing several relevant features: i) for small / medium input image sizes the proposed network gives very good testing accuracy, given a low implementation complexity and model size; ii) compares favorably with other widely known resources-constrained models, for instance in comparison to MobileNetv2 provides better accuracy with several times less training times and up to ten times less parameters (memory occupied by the model); iii) has a relevant set of hyper-parameters which can be easily and rapidly tuned due to the fast training specific to it. All these features make NL-CNN suitable for IoT, smart sensing, bio-medical portable instrumentation and other applications where artificial intelligence must be deployed in energy-constrained environments. △ Less","30 January, 2021",https://arxiv.org/pdf/2102.00227
Matching Representations of Explainable Artificial Intelligence and Eye Gaze for Human-Machine Interaction,Tiffany Hwu;Mia Levy;Steven Skorheim;David Huber,"Rapid non-verbal communication of task-based stimuli is a challenge in human-machine teaming, particularly in closed-loop interactions such as driving. To achieve this, we must understand the representations of information for both the human and machine, and determine a basis for bridging these representations. Techniques of explainable artificial intelligence (XAI) such as layer-wise relevance propagation (LRP) provide visual heatmap explanations for high-dimensional machine learning techniques such as deep neural networks. On the side of human cognition, visual attention is driven by the bottom-up and top-down processing of sensory input related to the current task. Since both XAI and human cognition should focus on task-related stimuli, there may be overlaps between their representations of visual attention, potentially providing a means of nonverbal communication between the human and machine. In this work, we examine the correlations between LRP heatmap explanations of a neural network trained to predict driving behavior and eye gaze heatmaps of human drivers. The analysis is used to determine the feasibility of using such a technique for enhancing driving performance. We find that LRP heatmaps show increasing levels of similarity with eye gaze according to the task specificity of the neural network. We then propose how these findings may assist humans by visually directing attention towards relevant areas. To our knowledge, our work provides the first known analysis of LRP and eye gaze for driving tasks. △ Less","30 January, 2021",https://arxiv.org/pdf/2102.00179
Disparate Impact Diminishes Consumer Trust Even for Advantaged Users,Tim Draws;Zoltán Szlávik;Benjamin Timmermans;Nava Tintarev;Kush R. Varshney;Michael Hind,"Systems aiming to aid consumers in their decision-making (e.g., by implementing persuasive techniques) are more likely to be effective when consumers trust them. However, recent research has demonstrated that the machine learning algorithms that often underlie such technology can act unfairly towards specific groups (e.g., by making more favorable predictions for men than for women). An undesired disparate impact resulting from this kind of algorithmic unfairness could diminish consumer trust and thereby undermine the purpose of the system. We studied this effect by conducting a between-subjects user study investigating how (gender-related) disparate impact affected consumer trust in an app designed to improve consumers' financial decision-making. Our results show that disparate impact decreased consumers' trust in the system and made them less likely to use it. Moreover, we find that trust was affected to the same degree across consumer groups (i.e., advantaged and disadvantaged users) despite both of these consumer groups recognizing their respective levels of personal benefit. Our findings highlight the importance of fairness in consumer-oriented artificial intelligence systems. △ Less","5 July, 2021",https://arxiv.org/pdf/2101.12715
Time for AI (Ethics) Maturity Model Is Now,Ville Vakkuri;Marianna Jantunen;Erika Halme;Kai-Kristian Kemell;Anh Nguyen-Duc;Tommi Mikkonen;Pekka Abrahamsson,"There appears to be a common agreement that ethical concerns are of high importance when it comes to systems equipped with some sort of Artificial Intelligence (AI). Demands for ethical AI are declared from all directions. As a response, in recent years, public bodies, governments, and universities have rushed in to provide a set of principles to be considered when AI based systems are designed and used. We have learned, however, that high-level principles do not turn easily into actionable advice for practitioners. Hence, also companies are publishing their own ethical guidelines to guide their AI development. This paper argues that AI software is still software and needs to be approached from the software development perspective. The software engineering paradigm has introduced maturity model thinking, which provides a roadmap for companies to improve their performance from the selected viewpoints known as the key capabilities. We want to voice out a call for action for the development of a maturity model for AI software. We wish to discuss whether the focus should be on AI ethics or, more broadly, the quality of an AI system, called a maturity model for the development of AI systems. △ Less","29 January, 2021",https://arxiv.org/pdf/2101.12701
Sequential Mechanisms for Multi-type Resource Allocation,Sujoy Sikdar;Xiaoxi Guo;Haibin Wang;Lirong Xia;Yongzhi Cao,"Several resource allocation problems involve multiple types of resources, with a different agency being responsible for ""locally"" allocating the resources of each type, while a central planner wishes to provide a guarantee on the properties of the final allocation given agents' preferences. We study the relationship between properties of the local mechanisms, each responsible for assigning all of the resources of a designated type, and the properties of a sequential mechanism which is composed of these local mechanisms, one for each type, applied sequentially, under lexicographic preferences, a well studied model of preferences over multiple types of resources in artificial intelligence and economics. We show that when preferences are O-legal, meaning that agents share a common importance order on the types, sequential mechanisms satisfy the desirable properties of anonymity, neutrality, non-bossiness, or Pareto-optimality if and only if every local mechanism also satisfies the same property, and they are applied sequentially according to the order O. Our main results are that under O-legal lexicographic preferences, every mechanism satisfying strategyproofness and a combination of these properties must be a sequential composition of local mechanisms that are also strategyproof, and satisfy the same combinations of properties. △ Less","21 February, 2021",https://arxiv.org/pdf/2101.12522
A Survey on Personality-Aware Recommendation Systems,Sahraoui Dhelim;Nyothiri Aung;Mohammed Amine Bouras;Huansheng Ning;Erik Cambria,"With the emergence of personality computing as a new research field related to artificial intelligence and personality psychology, we have witnessed an unprecedented proliferation of personality-aware recommendation systems. Unlike conventional recommendation systems, these new systems solve traditional problems such as the cold start and data sparsity problems. This survey aims to study and systematically classify personality-aware recommendation systems. To the best of our knowledge, this survey is the first that focuses on personality-aware recommendation systems. We explore the different design choices of personality-aware recommendation systems, by comparing their personality modeling methods, as well as their recommendation techniques. Furthermore, we present the commonly used datasets and point out some of the challenges of personality-aware recommendation systems. △ Less","28 December, 2021",https://arxiv.org/pdf/2101.12153
Measuring Intelligence and Growth Rate: Variations on Hibbard's Intelligence Measure,Samuel Alexander;Bill Hibbard,"In 2011, Hibbard suggested an intelligence measure for agents who compete in an adversarial sequence prediction game. We argue that Hibbard's idea should actually be considered as two separate ideas: first, that the intelligence of such agents can be measured based on the growth rates of the runtimes of the competitors that they defeat; and second, one specific (somewhat arbitrary) method for measuring said growth rates. Whereas Hibbard's intelligence measure is based on the latter growth-rate-measuring method, we survey other methods for measuring function growth rates, and exhibit the resulting Hibbard-like intelligence measures and taxonomies. Of particular interest, we obtain intelligence taxonomies based on Big-O and Big-Theta notation systems, which taxonomies are novel in that they challenge conventional notions of what an intelligence measure should look like. We discuss how intelligence measurement of sequence predictors can indirectly serve as intelligence measurement for agents with Artificial General Intelligence (AGIs). △ Less","24 January, 2021",https://arxiv.org/pdf/2101.12047
A Taxonomy of Explainable Bayesian Networks,Iena Petronella Derks;Alta de Waal,"Artificial Intelligence (AI), and in particular, the explainability thereof, has gained phenomenal attention over the last few years. Whilst we usually do not question the decision-making process of these systems in situations where only the outcome is of interest, we do however pay close attention when these systems are applied in areas where the decisions directly influence the lives of humans. It is especially noisy and uncertain observations close to the decision boundary which results in predictions which cannot necessarily be explained that may foster mistrust among end-users. This drew attention to AI methods for which the outcomes can be explained. Bayesian networks are probabilistic graphical models that can be used as a tool to manage uncertainty. The probabilistic framework of a Bayesian network allows for explainability in the model, reasoning and evidence. The use of these methods is mostly ad hoc and not as well organised as explainability methods in the wider AI research field. As such, we introduce a taxonomy of explainability in Bayesian networks. We extend the existing categorisation of explainability in the model, reasoning or evidence to include explanation of decisions. The explanations obtained from the explainability methods are illustrated by means of a simple medical diagnostic scenario. The taxonomy introduced in this paper has the potential not only to encourage end-users to efficiently communicate outcomes obtained, but also support their understanding of how and, more importantly, why certain predictions were made. △ Less","28 January, 2021",https://arxiv.org/pdf/2101.11844
Making Responsible AI the Norm rather than the Exception,Abhishek Gupta,"This report prepared by the Montreal AI Ethics Institute provides recommendations in response to the National Security Commission on Artificial Intelligence (NSCAI) Key Considerations for Responsible Development and Fielding of Artificial Intelligence document. The report centres on the idea that Responsible AI should be made the Norm rather than an Exception. It does so by utilizing the guiding principles of: (1) alleviating friction in existing workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an effective translation of abstract standards into actionable engineering practices. After providing some overarching comments on the document from the NSCAI, the report dives into the primary contribution of an actionable framework to help operationalize the ideas presented in the document from the NSCAI. The framework consists of: (1) a learning, knowledge, and information exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an empirically-driven risk-prioritization matrix, and (4) achieving the right level of complexity. All components reinforce each other to move from principles to practice in service of making Responsible AI the norm rather than the exception. △ Less","31 January, 2021",https://arxiv.org/pdf/2101.11832
Easy-GT: Open-Source Software to Facilitate Making the Ground Truth for White Blood Cells Nucleus,Zahra Mousavi Kouzehkanan;Sajad Tavakoli;Arezoo Alipanah,"The nucleus of white blood cells (WBCs) plays a significant role in their detection and classification. Appropriate feature extraction of the nucleus is necessary to fit a suitable artificial intelligence model to classify WBCs. Therefore, designing a method is needed to segment the nucleus accurately. There should be a comparison between the ground truths distinguished by a hematologist and the detected nuclei to evaluate the performance of the nucleus segmentation method accurately. It is a time-consuming and tedious task for experts to establish the ground truth manually. This paper presents an intelligent open-source software called Easy-GT to create the ground truth of WBCs' nucleus faster and easier. This software first detects the nucleus by employing a new Otsu's thresholding-based method with a dice similarity coefficient (DSC) of 95.42 %; the hematologist can then create a more accurate ground truth, using the designed buttons to modify the threshold value. This software can speed up ground truth's forming process more than six times. △ Less","8 June, 2021",https://arxiv.org/pdf/2101.11654
Detecting Deepfake Videos Using Euler Video Magnification,Rashmiranjan Das;Gaurav Negi;Alan F. Smeaton,"Recent advances in artificial intelligence make it progressively hard to distinguish between genuine and counterfeit media, especially images and videos. One recent development is the rise of deepfake videos, based on manipulating videos using advanced machine learning techniques. This involves replacing the face of an individual from a source video with the face of a second person, in the destination video. This idea is becoming progressively refined as deepfakes are getting progressively seamless and simpler to compute. Combined with the outreach and speed of social media, deepfakes could easily fool individuals when depicting someone saying things that never happened and thus could persuade people in believing fictional scenarios, creating distress, and spreading fake news. In this paper, we examine a technique for possible identification of deepfake videos. We use Euler video magnification which applies spatial decomposition and temporal filtering on video data to highlight and magnify hidden features like skin pulsation and subtle motions. Our approach uses features extracted from the Euler technique to train three models to classify counterfeit and unaltered videos and compare the results with existing techniques. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11563
"Evolution of artificial intelligence languages, a systematic literature review",Emmanuel Adetiba;Temitope John;Adekunle Akinrinmade;Funmilayo Moninuola;Oladipupo Akintade;Joke Badejo,"The field of Artificial Intelligence (AI) has undoubtedly received significant attention in recent years. AI is being adopted to provide solutions to problems in fields such as medicine, engineering, education, government and several other domains. In order to analyze the state of the art of research in the field of AI, we present a systematic literature review focusing on the Evolution of AI programming languages. We followed the systematic literature review method by searching relevant databases like SCOPUS, IEEE Xplore and Google Scholar. EndNote reference manager was used to catalog the relevant extracted papers. Our search returned a total of 6565 documents, whereof 69 studies were retained. Of the 69 retained studies, 15 documents discussed LISP programming language, another 34 discussed PROLOG programming language, the remaining 20 documents were spread between Logic and Object Oriented Programming (LOOP), ARCHLOG, Epistemic Ontology Language with Constraints (EOLC), Python, C++, ADA and JAVA programming languages. This review provides information on the year of implementation, development team, capabilities, limitations and applications of each of the AI programming languages discussed. The information in this review could guide practitioners and researchers in AI to make the right choice of languages to implement their novel AI methods. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11501
Challenges Encountered in Turkish Natural Language Processing Studies,Kadir Tohma;Yakup Kutlu,"Natural language processing is a branch of computer science that combines artificial intelligence with linguistics. It aims to analyze a language element such as writing or speaking with software and convert it into information. Considering that each language has its own grammatical rules and vocabulary diversity, the complexity of the studies in this field is somewhat understandable. For instance, Turkish is a very interesting language in many ways. Examples of this are agglutinative word structure, consonant/vowel harmony, a large number of productive derivational morphemes (practically infinite vocabulary), derivation and syntactic relations, a complex emphasis on vocabulary and phonological rules. In this study, the interesting features of Turkish in terms of natural language processing are mentioned. In addition, summary info about natural language processing techniques, systems and various sources developed for Turkish are given. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.11436
Low-Power Audio Keyword Spotting using Tsetlin Machines,Jie Lei;Tousif Rahman;Rishad Shafik;Adrian Wheeldon;Alex Yakovlev;Ole-Christoffer Granmo;Fahim Kawsar;Akhil Mathur,"The emergence of Artificial Intelligence (AI) driven Keyword Spotting (KWS) technologies has revolutionized human to machine interaction. Yet, the challenge of end-to-end energy efficiency, memory footprint and system complexity of current Neural Network (NN) powered AI-KWS pipelines has remained ever present. This paper evaluates KWS utilizing a learning automata powered machine learning algorithm called the Tsetlin Machine (TM). Through significant reduction in parameter requirements and choosing logic over arithmetic based processing, the TM offers new opportunities for low-power KWS while maintaining high learning efficacy. In this paper we explore a TM based keyword spotting (KWS) pipeline to demonstrate low complexity with faster rate of convergence compared to NNs. Further, we investigate the scalability with increasing keywords and explore the potential for enabling low-power on-chip KWS. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11336
"A Survey on 5G Radio Access Network Energy Efficiency: Massive MIMO, Lean Carrier Design, Sleep Modes, and Machine Learning",David Lopez-Perez;Antonio De Domenico;Nicola Piovesan;Harvey Bao;Geng Xinli;Song Qitao;Merouane Debbah,"Cellular networks have changed the world we are living in, and the fifth generation (5G) of radio technology is expected to further revolutionise our everyday lives, by enabling a high degree of automation, through its larger capacity, massive connectivity, and ultra-reliable low-latency communications. In addition, the third generation partnership project (3GPP) new radio (NR) specification also provides tools to significantly decrease the energy consumption and the green house emissions of next generations networks, thus contributing towards information and communication technology (ICT) sustainability targets. In this survey paper, we thoroughly review the state-of-the-art on current energy efficiency research. We first categorise and carefully analyse the different power consumption models and energy efficiency metrics, which have helped to make progress on the understanding of green networks. Then, as a main contribution, we survey in detail -- from a theoretical and a practical viewpoint -- the main energy efficiency enabling technologies that 3GPP NR provides, together with their main benefits and challenges. Special attention is paid to four key enabling technologies, i.e., massive multiple-input multiple-output (MIMO), lean carrier design, and advanced idle modes, together with the role of artificial intelligence capabilities. We dive into their implementation and operational details, and thoroughly discuss their optimal operation points and theoretical-trade-offs from an energy consumption perspective. This will help the reader to grasp the fundamentals of -- and the status on -- green networking. Finally, the areas of research where more effort is needed to make future networks greener are also discussed. △ Less","7 October, 2021",https://arxiv.org/pdf/2101.11246
Automated Crop Field Surveillance using Computer Vision,Tejas Atul Khare;Anuradha C. Phadke,"Artificial Intelligence is everywhere today. But unfortunately, Agriculture has not been able to get that much attention from Artificial Intelligence (AI). A lack of automation persists in the agriculture industry. For over many years, farmers and crop field owners have been facing a problem of trespassing of wild animals for which no feasible solution has been provided. Installing a fence or barrier like structure is neither feasible nor efficient due to the large areas covered by the fields. Also, if the landowner can afford to build a wall or barrier, government policies for building walls are often very irksome. The paper intends to give a simple intelligible solution to the problem with Automated Crop Field Surveillance using Computer Vision. The solution will significantly reduce the cost of crops destroyed annually and completely automate the security of the field. △ Less","27 January, 2021",https://arxiv.org/pdf/2101.11217
Constraint-Handling Techniques for Particle Swarm Optimization Algorithms,Mauro S. Innocente;Johann Sienz,"Population-based methods can cope with a variety of different problems, including problems of remarkably higher complexity than those traditional methods can handle. The main procedure consists of successively updating a population of candidate solutions, performing a parallel exploration instead of traditional sequential exploration. While the origins of the PSO method are linked to bird flock simulations, it is a stochastic optimization method in the sense that it relies on random coefficients to introduce creativity, and a bottom-up artificial intelligence-based approach in the sense that its intelligent behaviour emerges in a higher level than the individuals' rather than deterministically programmed. As opposed to EAs, the PSO involves no operator design and few coefficients to be tuned. Since this paper does not intend to study such tuning, general-purpose settings are taken from previous studies. The PSO algorithm requires the incorporation of some technique to handle constraints. A popular one is the penalization method, which turns the original constrained problem into unconstrained by penalizing infeasible solutions. Other techniques can be specifically designed for PSO. Since these strategies present advantages and disadvantages when compared to one another, there is no obvious best constraint-handling technique (CHT) for all problems. The aim here is to develop and compare different CHTs suitable for PSOs, which are incorporated to an algorithm with general-purpose settings. The comparisons are performed keeping the remaining features of the algorithm the same, while comparisons to other authors' results are offered as a frame of reference for the optimizer as a whole. Thus, the penalization, preserving feasibility and bisection methods are discussed, implemented, and tested on two suites of benchmark problems. Three neighbourhood sizes are also considered in the experiments. △ Less","24 January, 2021",https://arxiv.org/pdf/2101.10933
Artificial Intelligence for Satellite Communication: A Review,Fares Fourati;Mohamed-Slim Alouini,"Satellite communication offers the prospect of service continuity over uncovered and under-covered areas, service ubiquity, and service scalability. However, several challenges must first be addressed to realize these benefits, as the resource management, network control, network security, spectrum management, and energy usage of satellite networks are more challenging than that of terrestrial networks. Meanwhile, artificial intelligence (AI), including machine learning, deep learning, and reinforcement learning, has been steadily growing as a research field and has shown successful results in diverse applications, including wireless communication. In particular, the application of AI to a wide variety of satellite communication aspects have demonstrated excellent potential, including beam-hopping, anti-jamming, network traffic forecasting, channel modeling, telemetry mining, ionospheric scintillation detecting, interference managing, remote sensing, behavior modeling, space-air-ground integrating, and energy managing. This work thus provides a general overview of AI, its diverse sub-fields, and its state-of-the-art algorithms. Several challenges facing diverse aspects of satellite communication systems are then discussed, and their proposed and potential AI-based solutions are presented. Finally, an outlook of field is drawn, and future steps are suggested. △ Less","25 January, 2021",https://arxiv.org/pdf/2101.10899
Toxicity Detection in Drug Candidates using Simplified Molecular-Input Line-Entry System,Mriganka Nath;Subhasish Goswami,The need for analysis of toxicity in new drug candidates and the requirement of doing it fast have asked the consideration of scientists towards the use of artificial intelligence tools to examine toxicity levels and to develop models to a degree where they can be used commercially to measure toxicity levels efficiently in upcoming drugs. Artificial Intelligence based models can be used to predict the toxic nature of a chemical using Quantitative Structure Activity Relationship techniques. Convolutional Neural Network models have demonstrated great outcomes in predicting the qualitative analysis of chemicals in order to determine the toxicity. This paper goes for the study of Simplified Molecular Input Line-Entry System (SMILES) as a parameter to develop Long short term memory (LSTM) based models in order to examine the toxicity of a molecule and the degree to which the need can be fulfilled for practical use alongside its future outlooks for the purpose of real world applications. △ Less,"21 January, 2021",https://arxiv.org/pdf/2101.10831
Intelligent Routing to Enhance Energy Consumption in Wireless Sensor Network: A survey,Yasameen Sajid Razoki;Muntasir Al-Asfoor,"Nowadays, the network and the Internet applications have gained a substantial importance in the digital world, due to the great impact which it provides for health and community services. Among the most important services that have been provided are smart devices and vital factor measurement devices for patients, whether in a hospital or outside the hospital. Furthermore, sensors that collect medical data or measurements of temperature and humidity, in various critical environments. The proper types of network that may be used in such difficult environment are Wireless Sensor Networks, that used to sense and process data. Additionally, the Wireless Sensor Networks have been used in the environment of Internet of Things and smart cities in the general services and health fields. All these reasons have made researchers focus on Wireless Sensor Networks and addressing the challenges that face them. The most important challenge facing this type of network is energy consumption and increase battery life. This paper discusses the methodologies used in energy conservation in Wireless Sensor Networks, such as data reduction technology, shortest path selection and artificial intelligence algorithms used in smart routing and energy saving. Besides, we have introduced comparisons between the standard algorithms which are suggested by the researchers, to make a clear picture of the energy consumption problem and propose some effective solutions in Wireless Sensor Networks field. △ Less","4 January, 2021",https://arxiv.org/pdf/2101.10812
The Consequences of the Framing of Machine Learning Risk Prediction Models: Evaluation of Sepsis in General Wards,Simon Meyer Lauritsen;Bo Thiesson;Marianne Johansson Jørgensen;Anders Hammerich Riis;Ulrick Skipper Espelund;Jesper Bo Weile;Jeppe Lange,"Objectives: To evaluate the consequences of the framing of machine learning risk prediction models. We evaluate how framing affects model performance and model learning in four different approaches previously applied in published artificial-intelligence (AI) models. Setting and participants: We analysed structured secondary healthcare data from 221,283 citizens from four Danish municipalities who were 18 years of age or older. Results: The four models had similar population level performance (a mean area under the receiver operating characteristic curve of 0.73 to 0.82), in contrast to the mean average precision, which varied greatly from 0.007 to 0.385. Correspondingly, the percentage of missing values also varied between framing approaches. The on-clinical-demand framing, which involved samples for each time the clinicians made an early warning score assessment, showed the lowest percentage of missing values among the vital sign parameters, and this model was also able to learn more temporal dependencies than the others. The Shapley additive explanations demonstrated opposing interpretations of SpO2 in the prediction of sepsis as a consequence of differentially framed models. Conclusions: The profound consequences of framing mandate attention from clinicians and AI developers, as the understanding and reporting of framing are pivotal to the successful development and clinical implementation of future AI technology. Model framing must reflect the expected clinical environment. The importance of proper problem framing is by no means exclusive to sepsis prediction and applies to most clinical risk prediction models. △ Less","26 January, 2021",https://arxiv.org/pdf/2101.10790
The Granularity Gap Problem: A Hurdle for Applying Approximate Memory to Complex Data Layout,Soramichi Akiyama;Ryota Shioya,"The main memory access latency has not much improved for more than two decades while the CPU performance had been exponentially increasing until recently. Approximate memory is a technique to reduce the DRAM access latency in return of losing data integrity. It is beneficial for applications that are robust to noisy input and intermediate data such as artificial intelligence, multimedia processing, and graph processing. To obtain reasonable outputs from applications on approximate memory, it is crucial to protect critical data while accelerating accesses to non-critical data. We refer the minimum size of a continuous memory region that the same error rate is applied in approximate memory to as the approximation granularity. A fundamental limitation of approximate memory is that the approximation granularity is as large as a few kilo bytes. However, applications may have critical and non-critical data interleaved with smaller granularity. For example, a data structure for graph nodes can have pointers (critical) to neighboring nodes and its score (non-critical, depending on the use-case). This data structure cannot be directly mapped to approximate memory due to the gap between the approximation granularity and the granularity of data criticality. We refer to this issue as the granularity gap problem. In this paper, we first show that many applications potentially suffer from this problem. Then we propose a framework to quantitatively evaluate the performance overhead of a possible method to avoid this problem using known techniques. The evaluation results show that the performance overhead is non-negligible compared to expected benefit from approximate memory, suggesting that the granularity gap problem is a significant concern. △ Less","26 January, 2021",https://arxiv.org/pdf/2101.10605
Modern Machine and Deep Learning Systems as a way to achieve Man-Computer Symbiosis,Chirag Gupta,"Man-Computer Symbiosis (MCS) was originally envisioned by the famous computer pioneer J.C.R. Licklider in 1960, as a logical evolution of the then inchoate relationship between computer and humans. In his paper, Licklider provided a set of criteria by which to judge if a Man-Computer System is a symbiotic one, and also provided some predictions about such systems in the near and far future. Since then, innovations in computer networks and the invention of the Internet were major developments towards that end. However, with most systems based on conventional logical algorithms, many aspects of Licklider's MCS remained unfulfilled. This paper explores the extent to which modern machine learning systems in general, and deep learning ones in particular best exemplify MCS systems, and why they are the prime contenders to achieve a true Man-Computer Symbiosis as described by Licklider in his original paper in the future. The case for deep learning is built by illustrating each point of the original criteria as well as the criteria laid by subsequent research into MCS systems, with specific examples and applications provided to strengthen the arguments. The efficacy of deep neural networks in achieving Artificial General Intelligence, which would be the perfect version of an MCS system is also explored. △ Less","24 January, 2021",https://arxiv.org/pdf/2101.10534
Test and Evaluation Framework for Multi-Agent Systems of Autonomous Intelligent Agents,Erin Lanus;Ivan Hernandez;Adam Dachowicz;Laura Freeman;Melanie Grande;Andrew Lang;Jitesh H. Panchal;Anthony Patrick;Scott Welch,"Test and evaluation is a necessary process for ensuring that engineered systems perform as intended under a variety of conditions, both expected and unexpected. In this work, we consider the unique challenges of developing a unifying test and evaluation framework for complex ensembles of cyber-physical systems with embedded artificial intelligence. We propose a framework that incorporates test and evaluation throughout not only the development life cycle, but continues into operation as the system learns and adapts in a noisy, changing, and contended environment. The framework accounts for the challenges of testing the integration of diverse systems at various hierarchical scales of composition while respecting that testing time and resources are limited. A generic use case is provided for illustrative purposes and research directions emerging as a result of exploring the use case via the framework are suggested. △ Less","25 January, 2021",https://arxiv.org/pdf/2101.10430
Superiorities of Deep Extreme Learning Machines against Convolutional Neural Networks,Gokhan Altan;Yakup Kutlu,"Deep Learning (DL) is a machine learning procedure for artificial intelligence that analyzes the input data in detail by increasing neuron sizes and number of the hidden layers. DL has a popularity with the common improvements on the graphical processing unit capabilities. Increasing number of the neuron sizes at each layer and hidden layers is directly related to the computation time and training speed of the classifier models. The classification parameters including neuron weights, output weights, and biases need to be optimized for obtaining an optimum model. Most of the popular DL algorithms require long training times for optimization of the parameters with feature learning progresses and back-propagated training procedures. Reducing the training time and providing a real-time decision system are the basic focus points of the novel approaches. Deep Extreme Learning machines (Deep ELM) classifier model is one of the fastest and effective way to meet fast classification problems. In this study, Deep ELM model, its superiorities and weaknesses are discussed, the problems that are more suitable for the classifiers against Convolutional neural network based DL algorithms. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.10265
Game-Theoretic and Machine Learning-based Approaches for Defensive Deception: A Survey,Mu Zhu;Ahmed H. Anwar;Zelin Wan;Jin-Hee Cho;Charles Kamhoua;Munindar P. Singh,"Defensive deception is a promising approach for cyber defense. Via defensive deception, the defender can anticipate attacker actions; it can mislead or lure attacker, or hide real resources. Although defensive deception is increasingly popular in the research community, there has not been a systematic investigation of its key components, the underlying principles, and its tradeoffs in various problem settings. This survey paper focuses on defensive deception research centered on game theory and machine learning, since these are prominent families of artificial intelligence approaches that are widely employed in defensive deception. This paper brings forth insights, lessons, and limitations from prior work. It closes with an outline of some research directions to tackle major gaps in current defensive deception research. △ Less","8 May, 2021",https://arxiv.org/pdf/2101.10121
The Slodderwetenschap (Sloppy Science) of Stochastic Parrots -- A Plea for Science to NOT take the Route Advocated by Gebru and Bender,Michael Lissack,"This article is a position paper written in reaction to the now-infamous paper titled ""On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"" by Timnit Gebru, Emily Bender, and others who were, as of the date of this writing, still unnamed. I find the ethics of the Parrot Paper lacking, and in that lack, I worry about the direction in which computer science, machine learning, and artificial intelligence are heading. At best, I would describe the argumentation and evidentiary practices embodied in the Parrot Paper as Slodderwetenschap (Dutch for Sloppy Science) -- a word which the academic world last widely used in conjunction with the Diederik Stapel affair in psychology [2]. What is missing in the Parrot Paper are three critical elements: 1) acknowledgment that it is a position paper/advocacy piece rather than research, 2) explicit articulation of the critical presuppositions, and 3) explicit consideration of cost/benefit trade-offs rather than a mere recitation of potential ""harms"" as if benefits did not matter. To leave out these three elements is not good practice for either science or research. △ Less","11 January, 2021",https://arxiv.org/pdf/2101.10098
Personalized Education in the AI Era: What to Expect Next?,Setareh Maghsudi;Andrew Lan;Jie Xu;Mihaela van der Schaar,"The objective of personalized learning is to design an effective knowledge acquisition track that matches the learner's strengths and bypasses her weaknesses to ultimately meet her desired goal. This concept emerged several years ago and is being adopted by a rapidly-growing number of educational institutions around the globe. In recent years, the boost of artificial intelligence (AI) and machine learning (ML), together with the advances in big data analysis, has unfolded novel perspectives to enhance personalized education in numerous dimensions. By taking advantage of AI/ML methods, the educational platform precisely acquires the student's characteristics. This is done, in part, by observing the past experiences as well as analyzing the available big data through exploring the learners' features and similarities. It can, for example, recommend the most appropriate content among numerous accessible ones, advise a well-designed long-term curriculum, connect appropriate learners by suggestion, accurate performance evaluation, and the like. Still, several aspects of AI-based personalized education remain unexplored. These include, among others, compensating for the adverse effects of the absence of peers, creating and maintaining motivations for learning, increasing diversity, removing the biases induced by the data and algorithms, and the like. In this paper, while providing a brief review of state-of-the-art research, we investigate the challenges of AI/ML-based personalized education and discuss potential solutions. △ Less","19 January, 2021",https://arxiv.org/pdf/2101.10074
Personalization Paradox in Behavior Change Apps: Lessons from a Social Comparison-Based Personalized App for Physical Activity,Jichen Zhu;Diane H. Dallal;Robert C. Gray;Jennifer Villareale;Santiago Ontañón;Evan M. Forman;Danielle Arigo,"Social comparison-based features are widely used in social computing apps. However, most existing apps are not grounded in social comparison theories and do not consider individual differences in social comparison preferences and reactions. This paper is among the first to automatically personalize social comparison targets. In the context of an m-health app for physical activity, we use artificial intelligence (AI) techniques of multi-armed bandits. Results from our user study (n=53) indicate that there is some evidence that motivation can be increased using the AI-based personalization of social comparison. The detected effects achieved small-to-moderate effect sizes, illustrating the real-world implications of the intervention for enhancing motivation and physical activity. In addition to design implications for social comparison features in social apps, this paper identified the personalization paradox, the conflict between user modeling and adaptation, as a key design challenge of personalized applications for behavior change. Additionally, we propose research directions to mitigate this Personalization Paradox. △ Less","11 February, 2021",https://arxiv.org/pdf/2101.10020
AdderNet and its Minimalist Hardware Design for Energy-Efficient Artificial Intelligence,Yunhe Wang;Mingqiang Huang;Kai Han;Hanting Chen;Wei Zhang;Chunjing Xu;Dacheng Tao,"Convolutional neural networks (CNN) have been widely used for boosting the performance of many machine intelligence tasks. However, the CNN models are usually computationally intensive and energy consuming, since they are often designed with numerous multiply-operations and considerable parameters for the accuracy reason. Thus, it is difficult to directly apply them in the resource-constrained environments such as 'Internet of Things' (IoT) devices and smart phones. To reduce the computational complexity and energy burden, here we present a novel minimalist hardware architecture using adder convolutional neural network (AdderNet), in which the original convolution is replaced by adder kernel using only additions. To maximally excavate the potential energy consumption, we explore the low-bit quantization algorithm for AdderNet with shared-scaling-factor method, and we design both specific and general-purpose hardware accelerators for AdderNet. Experimental results show that the adder kernel with int8/int16 quantization also exhibits high performance, meanwhile consuming much less resources (theoretically ~81% off). In addition, we deploy the quantized AdderNet on FPGA (Field Programmable Gate Array) platform. The whole AdderNet can practically achieve 16% enhancement in speed, 67.6%-71.4% decrease in logic resource utilization and 47.85%-77.9% decrease in power consumption compared to CNN under the same circuit architecture. With a comprehensive comparison on the performance, power consumption, hardware resource consumption and network generalization capability, we conclude the AdderNet is able to surpass all the other competitors including the classical CNN, novel memristor-network, XNOR-Net and the shift-kernel based network, indicating its great potential in future high performance and energy-efficient artificial intelligence applications. △ Less","3 February, 2021",https://arxiv.org/pdf/2101.10015
Optimal Flexural Design of FRP-Reinforced Concrete Beams Using a Particle Swarm Optimizer,M. S. Innocente;Ll. Torres;X. Cahís;G. Barbeta;A. Catalán,"The design of the cross-section of an FRP-reinforced concrete beam is an iterative process of estimating both its dimensions and the reinforcement ratio, followed by the check of the compliance of a number of strength and serviceability constraints. The process continues until a suitable solution is found. Since there are infinite solutions to the problem, it appears convenient to define some optimality criteria so as to measure the relative goodness of the different solutions. This paper intends to develop a preliminary least-cost section design model that follows the recommendations in the ACI 440.1 R-06, and uses a relatively new artificial intelligence technique called particle swarm optimization (PSO) to handle the optimization tasks. The latter is based on the intelligence that emerges from the low-level interactions among a number of relatively non-intelligent individuals within a population. △ Less","25 January, 2021",https://arxiv.org/pdf/2101.09974
Symbiotic System of Systems Design for Safe and Resilient Autonomous Robotics in Offshore Wind Farms,Daniel Mitchell;Jamie Blanche;Osama Zaki;Joshua Roe;Leo Kong;Samuel Harper;Valentin Robu;Theodore Lim;David Flynn,"To reduce Operation and Maintenance (O&M) costs on offshore wind farms, wherein 80% of the O&M cost relates to deploying personnel, the offshore wind sector looks to Robotics and Artificial Intelligence (RAI) for solutions. Barriers to Beyond Visual Line of Sight (BVLOS) robotics include operational safety compliance and resilience, inhibiting the commercialization of autonomous services offshore. To address safety and resilience challenges we propose a Symbiotic System Of Systems Approach (SSOSA), reflecting the lifecycle learning and co-evolution with knowledge sharing for mutual gain of robotic platforms and remote human operators. Our novel methodology enables the run-time verification of safety, reliability and resilience during autonomous missions. To achieve this, a Symbiotic Digital Architecture (SDA) was developed to synchronize digital models of the robot, environment, infrastructure, and integrate front-end analytics and bidirectional communication for autonomous adaptive mission planning and situation reporting to a remote operator. A reliability ontology for the deployed robot, based on our holistic hierarchical-relational model, supports computationally efficient platform data analysis. We demonstrate an asset inspection mission within a confined space through Cooperative, Collaborative and Corroborative (C3) governance (internal and external symbiosis) via decision-making processes and the associated structures. We create a hyper enabled human interaction capability to analyze the mission status, diagnostics of critical sub-systems within the robot to provide automatic updates to our AI-driven run-time reliability ontology. This enables faults to be translated into failure modes for decision-making during the mission. △ Less","22 July, 2021",https://arxiv.org/pdf/2101.09491
Explainable Artificial Intelligence Approaches: A Survey,Sheikh Rabiul Islam;William Eberle;Sheikh Khaled Ghafoor;Mohiuddin Ahmed,"The lack of explainability of a decision from an Artificial Intelligence (AI) based ""black box"" system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications. △ Less","23 January, 2021",https://arxiv.org/pdf/2101.09429
Outlining Traceability: A Principle for Operationalizing Accountability in Computing Systems,Joshua A. Kroll,"Accountability is widely understood as a goal for well governed computer systems, and is a sought-after value in many governance contexts. But how can it be achieved? Recent work on standards for governable artificial intelligence systems offers a related principle: traceability. Traceability requires establishing not only how a system worked but how it was created and for what purpose, in a way that explains why a system has particular dynamics or behaviors. It connects records of how the system was constructed and what the system did mechanically to the broader goals of governance, in a way that highlights human understanding of that mechanical operation and the decision processes underlying it. We examine the various ways in which the principle of traceability has been articulated in AI principles and other policy documents from around the world, distill from these a set of requirements on software systems driven by the principle, and systematize the technologies available to meet those requirements. From our map of requirements to supporting tools, techniques, and procedures, we identify gaps and needs separating what traceability requires from the toolbox available for practitioners. This map reframes existing discussions around accountability and transparency, using the principle of traceability to show how, when, and why transparency can be deployed to serve accountability goals and thereby improve the normative fidelity of systems and their development processes. △ Less","22 January, 2021",https://arxiv.org/pdf/2101.09385
Censorship of Online Encyclopedias: Implications for NLP Models,Eddie Yang;Margaret E. Roberts,"While artificial intelligence provides the backbone for many tools people use around the world, recent work has brought to attention that the algorithms powering AI are not free of politics, stereotypes, and bias. While most work in this area has focused on the ways in which AI can exacerbate existing inequalities and discrimination, very little work has studied how governments actively shape training data. We describe how censorship has affected the development of Wikipedia corpuses, text data which are regularly used for pre-trained inputs into NLP algorithms. We show that word embeddings trained on Baidu Baike, an online Chinese encyclopedia, have very different associations between adjectives and a range of concepts about democracy, freedom, collective action, equality, and people and historical events in China than its regularly blocked but uncensored counterpart - Chinese language Wikipedia. We examine the implications of these discrepancies by studying their use in downstream AI applications. Our paper shows how government repression, censorship, and self-censorship may impact training data and the applications that draw from them. △ Less","22 January, 2021",https://arxiv.org/pdf/2101.09294
The Next Decade of Telecommunications Artificial Intelligence,Ye Ouyang;Lilei Wang;Aidong Yang;Maulik Shah;David Belanger;Tongqing Gao;Leping Wei;Yaqin Zhang,"It has been an exciting journey since the mobile communications and artificial intelligence were conceived 37 years and 64 years ago. While both fields evolved independently and profoundly changed communications and computing industries, the rapid convergence of 5G and deep learning is beginning to significantly transform the core communication infrastructure, network management and vertical applications. The paper first outlines the individual roadmaps of mobile communications and artificial intelligence in the early stage, with a concentration to review the era from 3G to 5G when AI and mobile communications started to converge. With regard to telecommunications artificial intelligence, the paper further introduces in detail the progress of artificial intelligence in the ecosystem of mobile communications. The paper then summarizes the classifications of AI in telecom ecosystems along with its evolution paths specified by various international telecommunications standardization bodies. Towards the next decade, the paper forecasts the prospective roadmap of telecommunications artificial intelligence. In line with 3GPP and ITU-R timeline of 5G & 6G, the paper further explores the network intelligence following 3GPP and ORAN routes respectively, experience and intention driven network management and operation, network AI signalling system, intelligent middle-office based BSS, intelligent customer experience management and policy control driven by BSS and OSS convergence, evolution from SLA to ELA, and intelligent private network for verticals. The paper is concluded with the vision that AI will reshape the future B5G or 6G landscape and we need pivot our R&D, standardizations, and ecosystem to fully take the unprecedented opportunities. △ Less","2 December, 2021",https://arxiv.org/pdf/2101.09163
Will Artificial Intelligence supersede Earth System and Climate Models?,Christopher Irrgang;Niklas Boers;Maike Sonnewald;Elizabeth A. Barnes;Christopher Kadow;Joanna Staneva;Jan Saynisch-Wagner,"We outline a perspective of an entirely new research branch in Earth and climate sciences, where deep neural networks and Earth system models are dismantled as individual methodological approaches and reassembled as learning, self-validating, and interpretable Earth system model-network hybrids. Following this path, we coin the term ""Neural Earth System Modelling"" (NESYM) and highlight the necessity of a transdisciplinary discussion platform, bringing together Earth and climate scientists, big data analysts, and AI experts. We examine the concurrent potential and pitfalls of Neural Earth System Modelling and discuss the open question whether artificial intelligence will not only infuse Earth system modelling, but ultimately render them obsolete. △ Less","22 January, 2021",https://arxiv.org/pdf/2101.09126
Deepfakes and the 2020 US elections: what (did not) happen,João Paulo Meneses,"Alarmed by the volume of disinformation that was assumed to have taken place during the 2016 US elections, scholars, politics and journalists predicted the worst when the first deepfakes began to emerge in 2018. After all, US Elections 2020 were believed to be the most secure in American history. This paper seeks explanations for an apparent contradiction: we believe that it was precisely the multiplication and conjugation of different types of warnings and fears that created the conditions that prevented malicious political deepfakes from affecting the 2020 US elections. From these warnings, we identified four factors (more active role of social networks, new laws, difficulties in accessing Artificial Intelligence and better awareness of society). But while this formula has proven to be effective in the case of the United States, 2020, it is not correct to assume that it can be repeated in other political contexts. △ Less","22 January, 2021",https://arxiv.org/pdf/2101.09092
Chemistry42: An AI-based platform for de novo molecular design,Yan A. Ivanenkov;Alex Zhebrak;Dmitry Bezrukov;Bogdan Zagribelnyy;Vladimir Aladinskiy;Daniil Polykovskiy;Eugene Lane;Petrina Kamya;Alexander Aliper;Alex Zhavoronkov,Chemistry42 is a software platform for de novo small molecule design that integrates Artificial Intelligence (AI) techniques with computational and medicinal chemistry methods. Chemistry42 is unique in its ability to generate novel molecular structures with predefined properties validated through in vitro and in vivo studies. Chemistry42 is a core component of Insilico Medicine Pharma.ai drug discovery suite that also includes target discovery and multi-omics data analysis (PandaOmics) and clinical trial outcomes predictions (InClinico). △ Less,"22 January, 2021",https://arxiv.org/pdf/2101.09050
Applications of artificial intelligence in drug development using real-world data,Zhaoyi Chen;Xiong Liu;William Hogan;Elizabeth Shenkman;Jiang Bian,"The US Food and Drug Administration (FDA) has been actively promoting the use of real-world data (RWD) in drug development. RWD can generate important real-world evidence reflecting the real-world clinical environment where the treatments are used. Meanwhile, artificial intelligence (AI), especially machine- and deep-learning (ML/DL) methods, have been increasingly used across many stages of the drug development process. Advancements in AI have also provided new strategies to analyze large, multidimensional RWD. Thus, we conducted a rapid review of articles from the past 20 years, to provide an overview of the drug development studies that use both AI and RWD. We found that the most popular applications were adverse event detection, trial recruitment, and drug repurposing. Here, we also discuss current research gaps and future opportunities. △ Less","2 February, 2021",https://arxiv.org/pdf/2101.08904
The Internet of Things in Ports: Six Key Security and Governance Challenges for the UK (Policy Brief),Feja Lesniewska;Uchenna D Ani;Jeremy M Watson;Madeline Carr,"In January 2019, the UK Government published its Maritime 2050 on Navigating the Future strategy. In the strategy, the government highlighted the importance of digitalization (with well-designed regulatory support) to achieve its goal of ensuring that the UK plays a global leadership role in the maritime sector. Ports, the gateways for 95% of UK trade movements, were identified as key sites for investment in technological innovation. The government identified the potential of the Internet of Things (IoT), in conjunction with other information-sharing technologies, such as shared data platforms, and Artificial Intelligence applications (AI), to synchronize processes within the port ecosystem leading to improved efficiency, safety, and environmental benefits, including improved air quality and lower greenhouse gas emissions. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.08812
Performance Evaluation of Transmission Mode Selection in D2D communication,Iacovos Ioannou;Christophoros Christophorou;Vasos Vassiliou;Andreas Pitsillides,"Device to Device (D2D) Communication is expected to be a core part of the forthcoming 5G Mobile Communication Networks as it promises improvements in energy efficiency, spectral efficiency, overall system capacity, and higher data rates with the use of the same frequencies for different D2D transmissions in short communication distances within the Cell. However, in order to achieve optimum results, it is important, among others, to select wisely the Transmission Mode of the D2D Device. Towards this end, our previous work proposed an intelligent Transmission mode selection approach in a framework that is utilizing Artificial Intelligence (AI) BDIx agents to collectively satisfy the D2D challenges in a Distributed Artificial Intelligent (DAI) manner autonomously and independently. In this paper, as a first step, a literature review focused on related Transmission mode approaches, is performed. Then, our investigated Transmission mode selection approach is further explained with formulas and evaluated based on different threshold values and investigated how these can affect the overall spectral efficiency and power usage of the network in order to achieve the maximum performance. The investigated thresholds(i.e. D2D Device Weighted Data Rate (WDR) and the D2D Device Battery Power Level) and metrics(i.e. WDR) are also further analyzed and formulated. In addition, the effect the transmission power of the D2D links has on the total spectral efficiency and total power consumption of the network, is also examined. This evaluation results arise some interesting findings that can contribute in other approaches that utilized similar or same thresholds. Also, the results obtained demonstrate that with the right tuning of the thresholds and transmission power, one can achieve a significant improvement in the network power usage and total spectral efficiency. △ Less","20 January, 2021",https://arxiv.org/pdf/2101.08262
TaNTIN: Terrestrial and Non-Terrestrial Integrated Networks-A collaborative technologies perspective for beyond 5G and 6G,Muhammad Waseem Akhtar;Syed Ali Hassan,"The world is moving toward globalization rapidly. Everybody has easy access to information with the spread of Internet technology. Businesses are growing beyond national borders. Internationalization affects every aspect of life. In this scenario, by dispersing functions and tasks across organizational borders, time and space, global organizations have higher requirements for collaboration. In order to allow decision-makers and knowledge workers, situated at different times and spaces, to work more efficiently, collaborative technologies are needed. In this paper, we give an overview of potential collaborative technologies, their benefits, risks and challenges, types, and elements. Based on the conceptualization of terrestrial and non-terrestrial integrated networks (TaNTIN), we highlight artificial intelligence (AI), blockchains, tactile Internet, mobile edge computing (MEC)/fog computing, augmented reality and virtual reality, and so forth as the key features to ensure quality-of-service (QoS) guarantee of futuristic collaborative services such as telemedicine, e-education, online gaming, online businesses, the entertainment industry. We also discuss how these technologies will impact human life in the near future. △ Less","20 January, 2021",https://arxiv.org/pdf/2101.08221
Mathematical foundations of moral preferences,Valerio Capraro;Matjaz Perc,"One-shot anonymous unselfishness in economic games is commonly explained by social preferences, which assume that people care about the monetary payoffs of others. However, during the last ten years, research has shown that different types of unselfish behaviour, including cooperation, altruism, truth-telling, altruistic punishment, and trustworthiness are in fact better explained by preferences for following one's own personal norms - internal standards about what is right or wrong in a given situation. Beyond better organising various forms of unselfish behaviour, this moral preference hypothesis has recently also been used to increase charitable donations, simply by means of interventions that make the morality of an action salient. Here we review experimental and theoretical work dedicated to this rapidly growing field of research, and in doing so we outline mathematical foundations for moral preferences that can be used in future models to better understand selfless human actions and to adjust policies accordingly. These foundations can also be used by artificial intelligence to better navigate the complex landscape of human morality. △ Less","21 January, 2021",https://arxiv.org/pdf/2101.08193
Adversarial Attacks for Tabular Data: Application to Fraud Detection and Imbalanced Data,Francesco Cartella;Orlando Anunciacao;Yuki Funabiki;Daisuke Yamaguchi;Toru Akishita;Olivier Elshocht,"Guaranteeing the security of transactional systems is a crucial priority of all institutions that process transactions, in order to protect their businesses against cyberattacks and fraudulent attempts. Adversarial attacks are novel techniques that, other than being proven to be effective to fool image classification models, can also be applied to tabular data. Adversarial attacks aim at producing adversarial examples, in other words, slightly modified inputs that induce the Artificial Intelligence (AI) system to return incorrect outputs that are advantageous for the attacker. In this paper we illustrate a novel approach to modify and adapt state-of-the-art algorithms to imbalanced tabular data, in the context of fraud detection. Experimental results show that the proposed modifications lead to a perfect attack success rate, obtaining adversarial examples that are also less perceptible when analyzed by humans. Moreover, when applied to a real-world production system, the proposed techniques shows the possibility of posing a serious threat to the robustness of advanced AI-based fraud detection procedures. △ Less","20 January, 2021",https://arxiv.org/pdf/2101.08030
5G D2D Transmission Mode Selection Performance & Cluster Limits Evaluation of Distributed Artificial Intelligence and Machine Learning Techniques,Iacovos Ioannou;Christophoros Christophorou;Vasos Vassiliou;Andreas Pitsillides,"5G D2D Communication promises improvements in energy and spectral efficiency, overall system capacity, and higher data rates. However, to achieve optimum results it is important to select wisely the Transmission mode of the D2D Device to form clusters in the most fruitful positions in terms of Sum Rate and Power Consumption. Towards this end, this paper investigates the use of Distributed Artificial Intelligence (DAI) and innovative to D2D, Machine Learning (ML) approaches to achieve satisfactory results in terms of Spectral Efficiency (SE), Power Consumption (PC) and execution time, with the creation of clusters and backhauling D2D network under existing Base Station/Small Cell. Additionally, one of the major factors that affect the creation of high-quality clusters under a D2D network is the number of the Devices. Therefore, this paper focuses on a small (<=200) number of Devices, with the purpose to identify the limits of each approach in terms of number of devices. Specifically, to identify where it is beneficial to form a cluster, investigate the critical point that gains increases rapidly and at the end examine the applicability of 5G requirements. Additionally, prior work presented a Distributed Artificial Intelligence (DAI) Solution/Framework in D2D and a DAIS Transmission Mode Selection (TMS) plan was proposed. In this paper DAIS is further examined, improved in terms of thresholds evaluation, evaluated, and compared with other approaches (AI/ML). The results obtained demonstrate the exceptional performance of DAIS, compared to all other related approaches in terms of SE, PC, execution time and cluster formation efficiency. Also, results show that the investigated AI/ML approaches are also beneficial for Transmission Mode Selection (TMS) in 5G D2D communication, even with a smaller (i.e., >=5 D2D Relay,>=50 D2D Multi Hop Relay) numbers of devices as a lower limits. △ Less","28 April, 2021",https://arxiv.org/pdf/2101.08014
Machine learning applications for COVID-19: A state-of-the-art review,Firuz Kamalov;Aswani Cherukuri;Hana Sulieman;Fadi Thabtah;Akbar Hossain,"The COVID-19 pandemic has galvanized the machine learning community to create new solutions that can help in the fight against the virus. The body of literature related to applications of machine learning and artificial intelligence to COVID-19 is constantly growing. The goal of this article is to present the latest advances in machine learning research applied to COVID-19. We cover four major areas of research: forecasting, medical diagnostics, drug development, and contact tracing. We review and analyze the most successful state of the art studies. In contrast to other existing surveys on the subject, our article presents a high level overview of the current research that is sufficiently detailed to provide an informed insight. △ Less","19 January, 2021",https://arxiv.org/pdf/2101.07824
Internet of Predictable Things (IoPT) Framework to Increase Cyber-Physical System Resiliency,Umit Cali;Murat Kuzlu;Vinayak Sharma;Manisa Pipattanasomporn;Ferhat Ozgur Catak,"During the last two decades, distributed energy systems, especially renewable energy sources (RES), have become more economically viable with increasing market share and penetration levels on power systems. In addition to decarbonization and decentralization of energy systems, digitalization has also become very important. The use of artificial intelligence (AI), advanced optimization algorithms, Industrial Internet of Things (IIoT), and other digitalization frameworks makes modern power system assets more intelligent, while vulnerable to cybersecurity risks. This paper proposes the concept of the Internet of Predictable Things (IoPT) that incorporates advanced data analytics and machine learning methods to increase the resiliency of cyber-physical systems against cybersecurity risks. The proposed concept is demonstrated using a cyber-physical system testbed under a variety of cyber attack scenarios as a proof of concept (PoC). △ Less","19 January, 2021",https://arxiv.org/pdf/2101.07816
GLocalX -- From Local to Global Explanations of Black Box AI Models,Mattia Setzu;Riccardo Guidotti;Anna Monreale;Franco Turini;Dino Pedreschi;Fosca Giannotti,"Artificial Intelligence (AI) has come to prominence as one of the major components of our society, with applications in most aspects of our lives. In this field, complex and highly nonlinear machine learning models such as ensemble models, deep neural networks, and Support Vector Machines have consistently shown remarkable accuracy in solving complex tasks. Although accurate, AI models often are ""black boxes"" which we are not able to understand. Relying on these models has a multifaceted impact and raises significant concerns about their transparency. Applications in sensitive and critical domains are a strong motivational factor in trying to understand the behavior of black boxes. We propose to address this issue by providing an interpretable layer on top of black box models by aggregating ""local"" explanations. We present GLocalX, a ""local-first"" model agnostic explanation method. Starting from local explanations expressed in form of local decision rules, GLocalX iteratively generalizes them into global explanations by hierarchically aggregating them. Our goal is to learn accurate yet simple interpretable models to emulate the given black box, and, if possible, replace it entirely. We validate GLocalX in a set of experiments in standard and constrained settings with limited or no access to either data or local explanations. Experiments show that GLocalX is able to accurately emulate several models with simple and small models, reaching state-of-the-art performance against natively global solutions. Our findings show how it is often possible to achieve a high level of both accuracy and comprehensibility of classification models, even in complex domains with high-dimensional data, without necessarily trading one property for the other. This is a key requirement for a trustworthy AI, necessary for adoption in high-stakes decision making applications. △ Less","26 January, 2021",https://arxiv.org/pdf/2101.07685
Self-Organizing Intelligent Matter: A blueprint for an AI generating algorithm,Karol Gregor;Frederic Besse,We propose an artificial life framework aimed at facilitating the emergence of intelligent organisms. In this framework there is no explicit notion of an agent: instead there is an environment made of atomic elements. These elements contain neural operations and interact through exchanges of information and through physics-like rules contained in the environment. We discuss how an evolutionary process can lead to the emergence of different organisms made of many such atomic elements which can coexist and thrive in the environment. We discuss how this forms the basis of a general AI generating algorithm. We provide a simplified implementation of such system and discuss what advances need to be made to scale it up further. △ Less,"19 January, 2021",https://arxiv.org/pdf/2101.07627
"Computer Science Communities: Who is Speaking, and Who is Listening to the Women? Using an Ethics of Care to Promote Diverse Voices",Marc Cheong;Kobi Leins;Simon Coghlan,"Those working on policy, digital ethics and governance often refer to issues in `computer science', that includes, but is not limited to, common subfields of Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.07463
Autonomous synthesis of metastable materials,Sebastian Ament;Maximilian Amsler;Duncan R. Sutherland;Ming-Chiang Chang;Dan Guevarra;Aine B. Connolly;John M. Gregoire;Michael O. Thompson;Carla P. Gomes;R. Bruce van Dover,"Autonomous experimentation enabled by artificial intelligence (AI) offers a new paradigm for accelerating scientific discovery. Non-equilibrium materials synthesis is emblematic of complex, resource-intensive experimentation whose acceleration would be a watershed for materials discovery and development. The mapping of non-equilibrium synthesis phase diagrams has recently been accelerated via high throughput experimentation but still limits materials research because the parameter space is too vast to be exhaustively explored. We demonstrate accelerated synthesis and exploration of metastable materials through hierarchical autonomous experimentation governed by the Scientific Autonomous Reasoning Agent (SARA). SARA integrates robotic materials synthesis and characterization along with a hierarchy of AI methods that efficiently reveal the structure of processing phase diagrams. SARA designs lateral gradient laser spike annealing (lg-LSA) experiments for parallel materials synthesis and employs optical spectroscopy to rapidly identify phase transitions. Efficient exploration of the multi-dimensional parameter space is achieved with nested active learning (AL) cycles built upon advanced machine learning models that incorporate the underlying physics of the experiments as well as end-to-end uncertainty quantification. With this, and the coordination of AL at multiple scales, SARA embodies AI harnessing of complex scientific tasks. We demonstrate its performance by autonomously mapping synthesis phase boundaries for the Bi_2O_3 system, leading to orders-of-magnitude acceleration in establishment of a synthesis phase diagram that includes conditions for kinetically stabilizing δ-Bi_2O_3 at room temperature, a critical development for electrochemical technologies such as solid oxide fuel cells. △ Less","19 December, 2021",https://arxiv.org/pdf/2101.07385
Dissonance Between Human and Machine Understanding,Zijian Zhang;Jaspreet Singh;Ujwal Gadiraju;Avishek Anand,"Complex machine learning models are deployed in several critical domains including healthcare and autonomous vehicles nowadays, albeit as functional black boxes. Consequently, there has been a recent surge in interpreting decisions of such complex models in order to explain their actions to humans. Models that correspond to human interpretation of a task are more desirable in certain contexts and can help attribute liability, build trust, expose biases and in turn build better models. It is, therefore, crucial to understand how and which models conform to human understanding of tasks. In this paper, we present a large-scale crowdsourcing study that reveals and quantifies the dissonance between human and machine understanding, through the lens of an image classification task. In particular, we seek to answer the following questions: Which (well-performing) complex ML models are closer to humans in their use of features to make accurate predictions? How does task difficulty affect the feature selection capability of machines in comparison to humans? Are humans consistently better at selecting features that make image recognition more accurate? Our findings have important implications on human-machine collaboration, considering that a long term goal in the field of artificial intelligence is to make machines capable of learning and reasoning like humans. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.07337
Leveraging AI to optimize website structure discovery during Penetration Testing,Diego Antonelli;Roberta Cascella;Gaetano Perrone;Simon Pietro Romano;Antonio Schiano,"Dirbusting is a technique used to brute force directories and file names on web servers while monitoring HTTP responses, in order to enumerate server contents. Such a technique uses lists of common words to discover the hidden structure of the target website. Dirbusting typically relies on response codes as discovery conditions to find new pages. It is widely used in web application penetration testing, an activity that allows companies to detect websites vulnerabilities. Dirbusting techniques are both time and resource consuming and innovative approaches have never been explored in this field. We hence propose an advanced technique to optimize the dirbusting process by leveraging Artificial Intelligence. More specifically, we use semantic clustering techniques in order to organize wordlist items in different groups according to their semantic meaning. The created clusters are used in an ad-hoc implemented next-word intelligent strategy. This paper demonstrates that the usage of clustering techniques outperforms the commonly used brute force methods. Performance is evaluated by testing eight different web applications. Results show a performance increase that is up to 50% for each of the conducted experiments. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.07223
Detection of Insider Attacks in Distributed Projected Subgradient Algorithms,Sissi Xiaoxiao Wu;Gangqiang Li;Shengli Zhang;Xiaohui Lin,"The gossip-based distributed algorithms are widely used to solve decentralized optimization problems in various multi-agent applications, while they are generally vulnerable to data injection attacks by internal malicious agents as each agent locally estimates its decent direction without an authorized supervision. In this work, we explore the application of artificial intelligence (AI) technologies to detect internal attacks. We show that a general neural network is particularly suitable for detecting and localizing the malicious agents, as they can effectively explore nonlinear relationship underlying the collected data. Moreover, we propose to adopt one of the state-of-art approaches in federated learning, i.e., a collaborative peer-to-peer machine learning protocol, to facilitate training our neural network models by gossip exchanges. This advanced approach is expected to make our model more robust to challenges with insufficient training data, or mismatched test data. In our simulations, a least-squared problem is considered to verify the feasibility and effectiveness of AI-based methods. Simulation results demonstrate that the proposed AI-based methods are beneficial to improve performance of detecting and localizing malicious agents over score-based methods, and the peer-to-peer neural network model is indeed robust to target issues. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.06917
Human Activity Recognition Using Multichannel Convolutional Neural Network,Niloy Sikder;Md. Sanaullah Chowdhury;Abu Shamim Mohammad Arif;Abdullah-Al Nahid,"Human Activity Recognition (HAR) simply refers to the capacity of a machine to perceive human actions. HAR is a prominent application of advanced Machine Learning and Artificial Intelligence techniques that utilize computer vision to understand the semantic meanings of heterogeneous human actions. This paper describes a supervised learning method that can distinguish human actions based on data collected from practical human movements. The primary challenge while working with HAR is to overcome the difficulties that come with the cyclostationary nature of the activity signals. This study proposes a HAR classification model based on a two-channel Convolutional Neural Network (CNN) that makes use of the frequency and power features of the collected human action signals. The model was tested on the UCI HAR dataset, which resulted in a 95.25% classification accuracy. This approach will help to conduct further researches on the recognition of human activities based on their biomedical signals. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06709
Adversarial Interaction Attack: Fooling AI to Misinterpret Human Intentions,Nodens Koren;Qiuhong Ke;Yisen Wang;James Bailey;Xingjun Ma,"Understanding the actions of both humans and artificial intelligence (AI) agents is important before modern AI systems can be fully integrated into our daily life. In this paper, we show that, despite their current huge success, deep learning based AI systems can be easily fooled by subtle adversarial noise to misinterpret the intention of an action in interaction scenarios. Based on a case study of skeleton-based human interactions, we propose a novel adversarial attack on interactions, and demonstrate how DNN-based interaction models can be tricked to predict the participants' reactions in unexpected ways. From a broader perspective, the scope of our proposed attack method is not confined to problems related to skeleton data but can also be extended to any type of problems involving sequential regressions. Our study highlights potential risks in the interaction loop with AI and humans, which need to be carefully addressed when deploying AI systems in safety-critical applications. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06704
Zero-touch Continuous Network Slicing Control via Scalable Actor-Critic Learning,Farhad Rezazadeh;Hatim Chergui;Christos Verikoukis,"Artificial intelligence (AI)-driven zero-touch network slicing is envisaged as a promising cutting-edge technology to harness the full potential of heterogeneous 5G and beyond 5G (B5G) communication systems and enable the automation of demand-aware resource management and orchestration (MANO). In this paper, we tackle the issue of B5G radio access network (RAN) joint slice admission control and resource allocation according to proposed slice-enabling cell-free massive multiple-input multiple-output (mMIMO) setup by invoking a continuous deep reinforcement learning (DRL) method. We present a novel Actor-Critic-based network slicing approach called, prioritized twin delayed distributional deep deterministic policy gradient (D-TD3)}. The paper defines and corroborates via extensive experimental results a zero-touch network slicing scheme with a multi-objective approach where the central server learns continuously to accumulate the knowledge learned in the past to solve future problems and re-configure computing resources autonomously while minimizing latency, energy consumption, and virtual network function (VNF) instantiation cost for each slice. Moreover, we pursue a state-action return distribution learning approach with the proposed replay policy and reward-penalty mechanisms. Finally, we present numerical results to showcase the gain of the adopted multi-objective strategy and verify the performance in terms of achieved slice admission rate, latency, energy, CPU utilization, and time efficiency. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06654
HySTER: A Hybrid Spatio-Temporal Event Reasoner,Theophile Sautory;Nuri Cingillioglu;Alessandra Russo,"The task of Video Question Answering (VideoQA) consists in answering natural language questions about a video and serves as a proxy to evaluate the performance of a model in scene sequence understanding. Most methods designed for VideoQA up-to-date are end-to-end deep learning architectures which struggle at complex temporal and causal reasoning and provide limited transparency in reasoning steps. We present the HySTER: a Hybrid Spatio-Temporal Event Reasoner to reason over physical events in videos. Our model leverages the strength of deep learning methods to extract information from video frames with the reasoning capabilities and explainability of symbolic artificial intelligence in an answer set programming framework. We define a method based on general temporal, causal and physics rules which can be transferred across tasks. We apply our model to the CLEVRER dataset and demonstrate state-of-the-art results in question answering accuracy. This work sets the foundations for the incorporation of inductive logic programming in the field of VideoQA. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06644
Continuous Multi-objective Zero-touch Network Slicing via Twin Delayed DDPG and OpenAI Gym,Farhad Rezazadeh;Hatim Chergui;Luis Alonso;Christos Verikoukis,"Artificial intelligence (AI)-driven zero-touch network slicing (NS) is a new paradigm enabling the automation of resource management and orchestration (MANO) in multi-tenant beyond 5G (B5G) networks. In this paper, we tackle the problem of cloud-RAN (C-RAN) joint slice admission control and resource allocation by first formulating it as a Markov decision process (MDP). We then invoke an advanced continuous deep reinforcement learning (DRL) method called twin delayed deep deterministic policy gradient (TD3) to solve it. In this intent, we introduce a multi-objective approach to make the central unit (CU) learn how to re-configure computing resources autonomously while minimizing latency, energy consumption and virtual network function (VNF) instantiation cost for each slice. Moreover, we build a complete 5G C-RAN network slicing environment using OpenAI Gym toolkit where, thanks to its standardized interface, it can be easily tested with different DRL schemes. Finally, we present extensive experimental results to showcase the gain of TD3 as well as the adopted multi-objective strategy in terms of achieved slice admission success rate, latency, energy saving and CPU utilization. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.06617
Understanding in Artificial Intelligence,Stefan Maetschke;David Martinez Iraola;Pieter Barnard;Elaheh ShafieiBavani;Peter Zhong;Ying Xu;Antonio Jimeno Yepes,"Current Artificial Intelligence (AI) methods, most based on deep learning, have facilitated progress in several fields, including computer vision and natural language understanding. The progress of these AI methods is measured using benchmarks designed to solve challenging tasks, such as visual question answering. A question remains of how much understanding is leveraged by these methods and how appropriate are the current benchmarks to measure understanding capabilities. To answer these questions, we have analysed existing benchmarks and their understanding capabilities, defined by a set of understanding capabilities, and current research streams. We show how progress has been made in benchmark development to measure understanding capabilities of AI methods and we review as well how current methods develop understanding capabilities. △ Less","16 January, 2021",https://arxiv.org/pdf/2101.06573
Artificial Intelligence for Emotion-Semantic Trending and People Emotion Detection During COVID-19 Social Isolation,Hamed Jelodar;Rita Orji;Stan Matwin;Swarna Weerasinghe;Oladapo Oyebode;Yongli Wang,"Taking advantage of social media platforms, such as Twitter, this paper provides an effective framework for emotion detection among those who are quarantined. Early detection of emotional feelings and their trends help implement timely intervention strategies. Given the limitations of medical diagnosis of early emotional change signs during the quarantine period, artificial intelligence models provide effective mechanisms in uncovering early signs, symptoms and escalating trends. Novelty of the approach presented herein is a multitask methodological framework of text data processing, implemented as a pipeline for meaningful emotion detection and analysis, based on the Plutchik/Ekman approach to emotion detection and trend detection. We present an evaluation of the framework and a pilot system. Results of confirm the effectiveness of the proposed framework for topic trends and emotion detection of COVID-19 tweets. Our findings revealed Stay-At-Home restrictions result in people expressing on twitter both negative and positive emotional semantics. Semantic trends of safety issues related to staying at home rapidly decreased within the 28 days and also negative feelings related to friends dying and quarantined life increased in some days. These findings have potential to impact public health policy decisions through monitoring trends of emotional feelings of those who are quarantined. The framework presented here has potential to assist in such monitoring by using as an online emotion detection tool kit. △ Less","16 January, 2021",https://arxiv.org/pdf/2101.06484
Slider: On the Design and Modeling of a 2D Floating Satellite Platform,Avijit Banerjee;Jakub Haluska;Sumeet G. Satpute;Dariusz Kominiak;George Nikolakopoulos,"In this article, a floating robotic emulation platform for a virtual demonstration of satellite motion in space is presented. The robotic platform design is characterized by its friction-less, levitating, yet planar motion over a hyper-smooth surface. The robotic platform, integrated with sensor and actuator units, is fully designed and manufactured from the Robotics and Artificial Intelligence Team at Luleå University of Technology. A detailed design description along with the mathematical modeling describing the platform's dynamic motion is formulated. Finally, the proposed design is validated in extensive simulation studies, while the overall test bed experimental setup, as well as the vehicle hardware and software architectures, are discussed in detail. Furthermore, the entire design, including 3D printing CAD model and different testbed elements, is provided in an open-source repository and a test campaign is used to showcase its capabilities and illustrate its operations. △ Less","15 January, 2021",https://arxiv.org/pdf/2101.06335
Player-AI Interaction: What Neural Network Games Reveal About AI as Play,Jichen Zhu;Jennifer Villareale;Nithesh Javvaji;Sebastian Risi;Mathias Löwe;Rush Weigelt;Casper Harteveld,"The advent of artificial intelligence (AI) and machine learning (ML) bring human-AI interaction to the forefront of HCI research. This paper argues that games are an ideal domain for studying and experimenting with how humans interact with AI. Through a systematic survey of neural network games (n = 38), we identified the dominant interaction metaphors and AI interaction patterns in these games. In addition, we applied existing human-AI interaction guidelines to further shed light on player-AI interaction in the context of AI-infused systems. Our core finding is that AI as play can expand current notions of human-AI interaction, which are predominantly productivity-based. In particular, our work suggests that game and UX designers should consider flow to structure the learning curve of human-AI interaction, incorporate discovery-based learning to play around with the AI and observe the consequences, and offer users an invitation to play to explore new forms of human-AI interaction. △ Less","18 January, 2021",https://arxiv.org/pdf/2101.06220
Reviving Purpose Limitation and Data Minimisation in Data-Driven Systems,Asia J. Biega;Michèle Finck,"This paper determines whether the two core data protection principles of data minimisation and purpose limitation can be meaningfully implemented in data-driven systems. While contemporary data processing practices appear to stand at odds with these principles, we demonstrate that systems could technically use much less data than they currently do. This observation is a starting point for our detailed techno-legal analysis uncovering obstacles that stand in the way of meaningful implementation and compliance as well as exemplifying unexpected trade-offs which emerge where data protection law is applied in practice. Our analysis seeks to inform debates about the impact of data protection on the development of artificial intelligence in the European Union, offering practical action points for data controllers, regulators, and researchers. △ Less","16 December, 2021",https://arxiv.org/pdf/2101.06203
Bridging the Gap: the case for an Incompletely Theorized Agreement on AI policy,Charlotte Stix;Matthijs M. Maas,"Recent progress in artificial intelligence (AI) raises a wide array of ethical and societal concerns. Accordingly, an appropriate policy approach is needed today. While there has been a wave of scholarship in this field, the research community at times appears divided amongst those who emphasize near-term concerns, and those focusing on long-term concerns and corresponding policy measures. In this paper, we seek to map and critically examine this alleged gulf, with a view to understanding the practical space for inter-community collaboration on AI policy. This culminates in a proposal to make use of the legal notion of an incompletely theorized agreement. We propose that on certain issue areas, scholars working with near-term and long-term perspectives can converge and cooperate on selected mutually beneficial AI policy projects all the while maintaining divergent perspectives. △ Less","7 January, 2021",https://arxiv.org/pdf/2101.06110
Scientific Relevance and Future of Digital Immortality and Virtual Humans,Daniel Cebo,"We are on the threshold of a significant change in the way we view digital life, which will have a major effect on the physical world. Computers have increasingly emulated deceased human beings through growing awareness in the fields of artificial intelligence, big data, and machine learning, and have symbolically managed to overcome death with the help of technology. One thing is clear, though: now that there are proper and legitimate discussions happening about human immortality, we can be certain that the future is upon us. This article attempts to explain and challenge the ways in which digital immortality, in particular, has manifested itself. This paper summarizes the technological solutions, research findings and technical challenges of major researchers by reviewing the key technologies and general technical schemes in the field of digital human beings. The prospects of digital human beings are being investigated. △ Less","9 January, 2021",https://arxiv.org/pdf/2101.06105
Artificial Intelligence for IT Operations (AIOPS) Workshop White Paper,Jasmin Bogatinovski;Sasho Nedelkoski;Alexander Acker;Florian Schmidt;Thorsten Wittkopp;Soeren Becker;Jorge Cardoso;Odej Kao,"Artificial Intelligence for IT Operations (AIOps) is an emerging interdisciplinary field arising in the intersection between the research areas of machine learning, big data, streaming analytics, and the management of IT operations. AIOps, as a field, is a candidate to produce the future standard for IT operation management. To that end, AIOps has several challenges. First, it needs to combine separate research branches from other research fields like software reliability engineering. Second, novel modelling techniques are needed to understand the dynamics of different systems. Furthermore, it requires to lay out the basis for assessing: time horizons and uncertainty for imminent SLA violations, the early detection of emerging problems, autonomous remediation, decision making, support of various optimization objectives. Moreover, a good understanding and interpretability of these aiding models are important for building trust between the employed tools and the domain experts. Finally, all this will result in faster adoption of AIOps, further increase the interest in this research field and contribute to bridging the gap towards fully-autonomous operating IT systems. The main aim of the AIOPS workshop is to bring together researchers from both academia and industry to present their experiences, results, and work in progress in this field. The workshop aims to strengthen the community and unite it towards the goal of joining the efforts for solving the main challenges the field is currently facing. A consensus and adoption of the principles of openness and reproducibility will boost the research in this emerging area significantly. △ Less","15 January, 2021",https://arxiv.org/pdf/2101.06054
Analysis of hidden feedback loops in continuous machine learning systems,Anton Khritankov,"In this concept paper, we discuss intricacies of specifying and verifying the quality of continuous and lifelong learning artificial intelligence systems as they interact with and influence their environment causing a so-called concept drift. We signify a problem of implicit feedback loops, demonstrate how they intervene with user behavior on an exemplary housing prices prediction system. Based on a preliminary model, we highlight conditions when such feedback loops arise and discuss possible solution approaches. △ Less","17 January, 2021",https://arxiv.org/pdf/2101.05673
Enhanced Audit Techniques Empowered by the Reinforcement Learning Pertaining to IFRS 16 Lease,Byungryul Choi,"The purpose of accounting audit is to have clear understanding on the financial activities of a company, which can be enhanced by machine learning or reinforcement learning as numeric analysis better than manual analysis can be made. For the purpose of assessment on the relevance, completeness and accuracy of the information produced by entity pertaining to the newly implemented International Financial Reporting Standard 16 Lease (IFRS 16) is one of such candidates as its characteristic of requiring the understanding on the nature of contracts and its complete analysis from listing up without omission, which can be enhanced by the digitalization of contracts for the purpose of creating the lists, still leaving the need of auditing cash flows of companies for the possible omission due to the potential error at the stage of data collection, especially for entities with various short or middle term business sites and related leases, such as construction entities. The implementation of the reinforcement learning and its well-known code is to be made for the purpose of drawing the possibility and utilizability of interpreters from domain knowledge to numerical system, also can be called 'gamification interpreter' or 'numericalization interpreter' which can be referred or compared to the extrapolation with nondimensional numbers, such as Froude Number, in physics, which was a source of inspiration at this study. Studies on the interpreters can be able to empower the utilizability of artificial general intelligence in domain and commercial area. △ Less","5 January, 2021",https://arxiv.org/pdf/2101.05633
Finding faults: A scoping study of fault diagnostics for Industrial Cyber-Physical Systems,Barry Dowdeswell;Roopak Sinha;Stephen G. MacDonell,"Context: As Industrial Cyber-Physical Systems (ICPS) become more connected and widely-distributed, often operating in safety-critical environments, we require innovative approaches to detect and diagnose the faults that occur in them. Objective: We profile fault identification and diagnosis techniques employed in the aerospace, automotive, and industrial control domains. By examining both theoretical presentations as well as case studies from production environments, we present a profile of the current approaches being employed and identify gaps. Methodology: A scoping study was used to identify and compare fault detection and diagnosis methodologies that are presented in the current literature. Results: Fault identification and analysis studies from 127 papers published from 2004 to 2019 reveal a wide diversity of promising techniques, both emerging and in-use. These range from traditional Physics-based Models to Data-Driven Artificial Intelligence (AI) and Knowledge-Based approaches. Predictive diagnostics or prognostics featured prominently across all sectors, along with discussions of techniques including Fault trees, Petri nets and Markov approaches. We also profile some of the techniques that have reached the highest Technology Readiness Levels, showing how those methods are being applied in real-world environments beyond the laboratory. Conclusions: Our results suggest that the continuing wide use of both Model-Based and Data-Driven AI techniques across all domains, especially when they are used together in hybrid configuration, reflects the complexity of the current ICPS application space. While creating sufficiently-complete models is labor intensive, Model-free AI techniques were evidenced as a viable way of addressing aspects of this challenge, demonstrating the increasing sophistication of current machine learning systems.(Abridged) △ Less","13 January, 2021",https://arxiv.org/pdf/2101.05451
Formalising Concepts as Grounded Abstractions,Stephen Clark;Alexander Lerchner;Tamara von Glehn;Olivier Tieleman;Richard Tanburn;Misha Dashevskiy;Matko Bosnjak,"The notion of concept has been studied for centuries, by philosophers, linguists, cognitive scientists, and researchers in artificial intelligence (Margolis & Laurence, 1999). There is a large literature on formal, mathematical models of concepts, including a whole sub-field of AI -- Formal Concept Analysis -- devoted to this topic (Ganter & Obiedkov, 2016). Recently, researchers in machine learning have begun to investigate how methods from representation learning can be used to induce concepts from raw perceptual data (Higgins, Sonnerat, et al., 2018). The goal of this report is to provide a formal account of concepts which is compatible with this latest work in deep learning. The main technical goal of this report is to show how techniques from representation learning can be married with a lattice-theoretic formulation of conceptual spaces. The mathematics of partial orders and lattices is a standard tool for modelling conceptual spaces (Ch.2, Mitchell (1997), Ganter and Obiedkov (2016)); however, there is no formal work that we are aware of which defines a conceptual lattice on top of a representation that is induced using unsupervised deep learning (Goodfellow et al., 2016). The advantages of partially-ordered lattice structures are that these provide natural mechanisms for use in concept discovery algorithms, through the meets and joins of the lattice. △ Less","13 January, 2021",https://arxiv.org/pdf/2101.05125
Machine learning approach for biopsy-based identification of eosinophilic esophagitis reveals importance of global features,Tomer Czyzewski;Nati Daniel;Mark Rochman;Julie M. Caldwell;Garrett A. Osswald;Margaret H. Collins;Marc E. Rothenberg;Yonatan Savir,"Goal: Eosinophilic esophagitis (EoE) is an allergic inflammatory condition characterized by eosinophil accumulation in the esophageal mucosa. EoE diagnosis includes a manual assessment of eosinophil levels in mucosal biopsies - a time-consuming, laborious task that is difficult to standardize. One of the main challenges in automating this process, like many other biopsy-based diagnostics, is detecting features that are small relative to the size of the biopsy. Results: In this work, we utilized hematoxylin- and eosin-stained slides from esophageal biopsies from patients with active EoE and control subjects to develop a platform based on a deep convolutional neural network (DCNN) that can classify esophageal biopsies with an accuracy of 85%, sensitivity of 82.5%, and specificity of 87%. Moreover, by combining several downscaling and cropping strategies, we show that some of the features contributing to the correct classification are global rather than specific, local features. Conclusions: We report the ability of artificial intelligence to identify EoE using computer vision analysis of esophageal biopsy slides. Further, the DCNN features associated with EoE are based on not only local eosinophils but also global histologic changes. Our approach can be used for other conditions that rely on biopsy-based histologic diagnostics. △ Less","13 January, 2021",https://arxiv.org/pdf/2101.04989
Towards Energy Efficient Federated Learning over 5G+ Mobile Devices,Dian Shi;Liang Li;Rui Chen;Pavana Prakash;Miao Pan;Yuguang Fang,"The continuous convergence of machine learning algorithms, 5G and beyond (5G+) wireless communications, and artificial intelligence (AI) hardware implementation hastens the birth of federated learning (FL) over 5G+ mobile devices, which pushes AI functions to mobile devices and initiates a new era of on-device AI applications. Despite the remarkable progress made in FL, huge energy consumption is one of the most significant obstacles restricting the development of FL over battery-constrained 5G+ mobile devices. To address this issue, in this paper, we investigate how to develop energy efficient FL over 5G+ mobile devices by making a trade-off between energy consumption for ""working"" (i.e., local computing) and that for ""talking"" (i.e., wireless communications) in order to boost the overall energy efficiency. Specifically, we first examine energy consumption models for graphics processing unit (GPU) computation and wireless transmissions. Then, we overview the state of the art of integrating FL procedure with energy-efficient learning techniques (e.g., gradient sparsification, weight quantization, pruning, etc.). Finally, we present several potential future research directions for FL over 5G+ mobile devices from the perspective of energy efficiency. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.04866
Self-Diagnosis through AI-enabled Chatbot-based Symptom Checkers: User Experiences and Design Considerations,Yue You;Xinning Gui,"Recently, there has been a growing interest in developing AI-enabled chatbot-based symptom checker (CSC) apps in the healthcare market. CSC apps provide potential diagnoses for users and assist them with self-triaging based on Artificial Intelligence (AI) techniques using human-like conversations. Despite the popularity of such CSC apps, little research has been done to investigate their functionalities and user experiences. To do so, we conducted a feature review, a user review analysis, and an interview study. We found that the existing CSC apps lack the functions to support the whole diagnostic process of an offline medical visit. We also found that users perceive the current CSC apps to lack support for a comprehensive medical history, flexible symptom input, comprehensible questions, and diverse diseases and user groups. Based on these results, we derived implications for the future features and conversational design of CSC apps. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.04796
The Medical Authority of AI: A Study of AI-enabled Consumer-facing Health Technology,Yue You;Yubo Kou;Xianghua Ding;Xinning Gui,"Recently, consumer-facing health technologies such as Artificial Intelligence (AI)-based symptom checkers (AISCs) have sprung up in everyday healthcare practice. AISCs solicit symptom information from users and provide medical suggestions and possible diagnoses, a responsibility that people usually entrust with real-person authorities such as physicians and expert patients. Thus, the advent of AISCs begs a question of whether and how they transform the notion of medical authority in everyday healthcare practice. To answer this question, we conducted an interview study with thirty AISC users. We found that users assess the medical authority of AISCs using various factors including automated decisions and interaction design patterns of AISC apps, associations with established medical authorities like hospitals, and comparisons with other health technologies. We reveal how AISCs are used in healthcare delivery, discuss how AI transforms conventional understandings of medical authority, and derive implications for designing AI-enabled health technology. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.04794
AI- and HPC-enabled Lead Generation for SARS-CoV-2: Models and Processes to Extract Druglike Molecules Contained in Natural Language Text,Zhi Hong;J. Gregory Pauloski;Logan Ward;Kyle Chard;Ben Blaiszik;Ian Foster,"Researchers worldwide are seeking to repurpose existing drugs or discover new drugs to counter the disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). A promising source of candidates for such studies is molecules that have been reported in the scientific literature to be drug-like in the context of coronavirus research. We report here on a project that leverages both human and artificial intelligence to detect references to drug-like molecules in free text. We engage non-expert humans to create a corpus of labeled text, use this labeled corpus to train a named entity recognition model, and employ the trained model to extract 10912 drug-like molecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of 198875 papers. Performance analyses show that our automated extraction model can achieve performance on par with that of non-expert humans. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.04617
Reliable Fleet Analytics for Edge IoT Solutions,Emmanuel Raj;Magnus Westerlund;Leonardo Espinosa-Leal,"In recent years we have witnessed a boom in Internet of Things (IoT) device deployments, which has resulted in big data and demand for low-latency communication. This shift in the demand for infrastructure is also enabling real-time decision making using artificial intelligence for IoT applications. Artificial Intelligence of Things (AIoT) is the combination of Artificial Intelligence (AI) technologies and the IoT infrastructure to provide robust and efficient operations and decision making. Edge computing is emerging to enable AIoT applications. Edge computing enables generating insights and making decisions at or near the data source, reducing the amount of data sent to the cloud or a central repository. In this paper, we propose a framework for facilitating machine learning at the edge for AIoT applications, to enable continuous delivery, deployment, and monitoring of machine learning models at the edge (Edge MLOps). The contribution is an architecture that includes services, tools, and methods for delivering fleet analytics at scale. We present a preliminary validation of the framework by performing experiments with IoT devices on a university campus's rooms. For the machine learning experiments, we forecast multivariate time series for predicting air quality in the respective rooms by using the models deployed in respective edge devices. By these experiments, we validate the proposed fleet analytics framework for efficiency and robustness. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.04414
Quantum Mathematics in Artificial Intelligence,Dominic Widdows;Kirsty Kitto;Trevor Cohen,"In the decade since 2010, successes in artificial intelligence have been at the forefront of computer science and technology, and vector space models have solidified a position at the forefront of artificial intelligence. At the same time, quantum computers have become much more powerful, and announcements of major advances are frequently in the news. The mathematical techniques underlying both these areas have more in common than is sometimes realized. Vector spaces took a position at the axiomatic heart of quantum mechanics in the 1930s, and this adoption was a key motivation for the derivation of logic and probability from the linear geometry of vector spaces. Quantum interactions between particles are modelled using the tensor product, which is also used to express objects and operations in artificial neural networks. This paper describes some of these common mathematical areas, including examples of how they are used in artificial intelligence (AI), particularly in automated reasoning and natural language processing (NLP). Techniques discussed include vector spaces, scalar products, subspaces and implication, orthogonal projection and negation, dual vectors, density matrices, positive operators, and tensor products. Application areas include information retrieval, categorization and implication, modelling word-senses and disambiguation, inference in knowledge bases, and semantic composition. Some of these approaches can potentially be implemented on quantum hardware. Many of the practical steps in this implementation are in early stages, and some are already realized. Explaining some of the common mathematical tools can help researchers in both AI and quantum computing further exploit these overlaps, recognizing and exploring new directions along the way. △ Less","16 December, 2021",https://arxiv.org/pdf/2101.04255
Solving Common-Payoff Games with Approximate Policy Iteration,Samuel Sokota;Edward Lockhart;Finbarr Timbers;Elnaz Davoodi;Ryan D'Orazio;Neil Burch;Martin Schmid;Michael Bowling;Marc Lanctot,"For artificially intelligent learning systems to have widespread applicability in real-world settings, it is important that they be able to operate decentrally. Unfortunately, decentralized control is difficult -- computing even an epsilon-optimal joint policy is a NEXP complete problem. Nevertheless, a recently rediscovered insight -- that a team of agents can coordinate via common knowledge -- has given rise to algorithms capable of finding optimal joint policies in small common-payoff games. The Bayesian action decoder (BAD) leverages this insight and deep reinforcement learning to scale to games as large as two-player Hanabi. However, the approximations it uses to do so prevent it from discovering optimal joint policies even in games small enough to brute force optimal solutions. This work proposes CAPI, a novel algorithm which, like BAD, combines common knowledge with deep reinforcement learning. However, unlike BAD, CAPI prioritizes the propensity to discover optimal joint policies over scalability. While this choice precludes CAPI from scaling to games as large as Hanabi, empirical results demonstrate that, on the games to which CAPI does scale, it is capable of discovering optimal joint policies even when other modern multi-agent reinforcement learning algorithms are unable to do so. Code is available at https://github.com/ssokota/capi . △ Less","11 January, 2021",https://arxiv.org/pdf/2101.04237
Technology Readiness Levels for Machine Learning Systems,Alexander Lavin;Ciarán M. Gilligan-Lee;Alessya Visnjic;Siddha Ganju;Dava Newman;Atılım Güneş Baydin;Sujoy Ganguly;Danny Lange;Amit Sharma;Stephan Zheng;Eric P. Xing;Adam Gibson;James Parr;Chris Mattmann;Yarin Gal,"The development and deployment of machine learning (ML) systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. The lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, where mission critical measures and robustness are ingrained in the development process. Drawing on experience in both spacecraft engineering and ML (from research through product across domain areas), we have developed a proven systems engineering approach for machine learning development and deployment. Our ""Machine Learning Technology Readiness Levels"" (MLTRL) framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for ML workflows, including key distinctions from traditional software engineering. Even more, MLTRL defines a lingua franca for people across teams and organizations to work collaboratively on artificial intelligence and machine learning technologies. Here we describe the framework and elucidate it with several real world use-cases of developing ML methods from basic research through productization and deployment, in areas such as medical diagnostics, consumer computer vision, satellite imagery, and particle physics. △ Less","29 November, 2021",https://arxiv.org/pdf/2101.03989
How Much Automation Does a Data Scientist Want?,Dakuo Wang;Q. Vera Liao;Yunfeng Zhang;Udayan Khurana;Horst Samulowitz;Soya Park;Michael Muller;Lisa Amini,"Data science and machine learning (DS/ML) are at the heart of the recent advancements of many Artificial Intelligence (AI) applications. There is an active research thread in AI, \autoai, that aims to develop systems for automating end-to-end the DS/ML Lifecycle. However, do DS and ML workers really want to automate their DS/ML workflow? To answer this question, we first synthesize a human-centered AutoML framework with 6 User Role/Personas, 10 Stages and 43 Sub-Tasks, 5 Levels of Automation, and 5 Types of Explanation, through reviewing research literature and marketing reports. Secondly, we use the framework to guide the design of an online survey study with 217 DS/ML workers who had varying degrees of experience, and different user roles ""matching"" to our 6 roles/personas. We found that different user personas participated in distinct stages of the lifecycle -- but not all stages. Their desired levels of automation and types of explanation for AutoML also varied significantly depending on the DS/ML stage and the user persona. Based on the survey results, we argue there is no rationale from user needs for complete automation of the end-to-end DS/ML lifecycle. We propose new next steps for user-controlled DS/ML automation. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.03970
Explainable Artificial Intelligence (XAI): An Engineering Perspective,F. Hussain;R. Hussain;E. Hossain,"The remarkable advancements in Deep Learning (DL) algorithms have fueled enthusiasm for using Artificial Intelligence (AI) technologies in almost every domain; however, the opaqueness of these algorithms put a question mark on their applications in safety-critical systems. In this regard, the `explainability' dimension is not only essential to both explain the inner workings of black-box algorithms, but it also adds accountability and transparency dimensions that are of prime importance for regulators, consumers, and service providers. eXplainable Artificial Intelligence (XAI) is the set of techniques and methods to convert the so-called black-box AI algorithms to white-box algorithms, where the results achieved by these algorithms and the variables, parameters, and steps taken by the algorithm to reach the obtained results, are transparent and explainable. To complement the existing literature on XAI, in this paper, we take an `engineering' approach to illustrate the concepts of XAI. We discuss the stakeholders in XAI and describe the mathematical contours of XAI from engineering perspective. Then we take the autonomous car as a use-case and discuss the applications of XAI for its different components such as object detection, perception, control, action decision, and so on. This work is an exploratory study to identify new avenues of research in the field of XAI. △ Less","10 January, 2021",https://arxiv.org/pdf/2101.03613
Heatmap-based Object Detection and Tracking with a Fully Convolutional Neural Network,Fabian Amherd;Elias Rodriguez,"The main topic of this paper is a brief overview of the field of Artificial Intelligence. The core of this paper is a practical implementation of an algorithm for object detection and tracking. The ability to detect and track fast-moving objects is crucial for various applications of Artificial Intelligence like autonomous driving, ball tracking in sports, robotics or object counting. As part of this paper the Fully Convolutional Neural Network ""CueNet"" was developed. It detects and tracks the cueball on a labyrinth game robustly and reliably. While CueNet V1 has a single input image, the approach with CueNet V2 was to take three consecutive 240 x 180-pixel images as an input and transform them into a probability heatmap for the cueball's location. The network was tested with a separate video that contained all sorts of distractions to test its robustness. When confronted with our testing data, CueNet V1 predicted the correct cueball location in 99.6% of all frames, while CueNet V2 had 99.8% accuracy. △ Less","10 January, 2021",https://arxiv.org/pdf/2101.03541
A Reconfigurable Convolution-in-Pixel CMOS Image Sensor Architecture,Ruibing Song;Kejie Huang;Zongsheng Wang;Haibin Shen,"The separation of the data capture and analysis in modern vision systems has led to a massive amount of data transfer between the end devices and cloud computers, resulting in long latency, slow response, and high power consumption. Efficient hardware architectures are under focused development to enable Artificial Intelligence (AI) at the resource-limited end sensing devices. One of the most promising solutions is to enable Processing-in-Pixel (PIP) scheme. However, the conventional schemes suffer from the low fill-factor issue. This paper proposes a PIP based CMOS sensor architecture, which allows convolution operation before the column readout circuit to significantly improve the image reading speed with much lower power consumption. The simulation results show that the proposed architecture could support the computing efficiency up to 11.65 TOPS/W at the 8-bit weight configuration, which is three times as high as the conventional schemes. The transistors required for each pixel are only 2.5T, significantly improving the fill-factor. △ Less","13 October, 2021",https://arxiv.org/pdf/2101.03308
Misspelling Correction with Pre-trained Contextual Language Model,Yifei Hu;Xiaonan Jing;Youlim Ko;Julia Taylor Rayz,"Spelling irregularities, known now as spelling mistakes, have been found for several centuries. As humans, we are able to understand most of the misspelled words based on their location in the sentence, perceived pronunciation, and context. Unlike humans, computer systems do not possess the convenient auto complete functionality of which human brains are capable. While many programs provide spelling correction functionality, many systems do not take context into account. Moreover, Artificial Intelligence systems function in the way they are trained on. With many current Natural Language Processing (NLP) systems trained on grammatically correct text data, many are vulnerable against adversarial examples, yet correctly spelled text processing is crucial for learning. In this paper, we investigate how spelling errors can be corrected in context, with a pre-trained language model BERT. We present two experiments, based on BERT and the edit distance algorithm, for ranking and selecting candidate corrections. The results of our experiments demonstrated that when combined properly, contextual word embeddings of BERT and edit distance are capable of effectively correcting spelling errors. △ Less","8 January, 2021",https://arxiv.org/pdf/2101.03204
Slow manifolds in recurrent networks encode working memory efficiently and robustly,Elham Ghazizadeh;ShiNung Ching,"Working memory is a cognitive function involving the storage and manipulation of latent information over brief intervals of time, thus making it crucial for context-dependent computation. Here, we use a top-down modeling approach to examine network-level mechanisms of working memory, an enigmatic issue and central topic of study in neuroscience and machine intelligence. We train thousands of recurrent neural networks on a working memory task and then perform dynamical systems analysis on the ensuing optimized networks, wherein we find that four distinct dynamical mechanisms can emerge. In particular, we show the prevalence of a mechanism in which memories are encoded along slow stable manifolds in the network state space, leading to a phasic neuronal activation profile during memory periods. In contrast to mechanisms in which memories are directly encoded at stable attractors, these networks naturally forget stimuli over time. Despite this seeming functional disadvantage, they are more efficient in terms of how they leverage their attractor landscape and paradoxically, are considerably more robust to noise. Our results provide new dynamical hypotheses regarding how working memory function is encoded in both natural and artificial neural networks. △ Less","8 January, 2021",https://arxiv.org/pdf/2101.03163
Artificial Intelligence enabled Smart Learning,Faisal Khan;Debdeep Bose,"Artificial Intelligence (AI) is a discipline of computer science that deals with machine intelligence. It is essential to bring AI into the context of learning because it helps in analysing the enormous amounts of data that is collected from individual students, teachers and academic staff. The major priorities of implementing AI in education are making innovative use of existing digital technologies for learning, and teaching practices that significantly improve traditional educational methods. The main problem with traditional learning is that it cannot be suited to every student in class. Some students may grasp the concepts well, while some may have difficulties in understanding them and some may be more auditory or visual learners. The World Bank report on education has indicated that the learning gap created by this problem causes many students to drop out (World Development Report, 2018). Personalised learning has been able to solve this grave problem. △ Less","8 January, 2021",https://arxiv.org/pdf/2101.02991
A Novel Word Sense Disambiguation Approach Using WordNet Knowledge Graph,Mohannad AlMousa;Rachid Benlamri;Richard Khoury,"Various applications in computational linguistics and artificial intelligence rely on high-performing word sense disambiguation techniques to solve challenging tasks such as information retrieval, machine translation, question answering, and document clustering. While text comprehension is intuitive for humans, machines face tremendous challenges in processing and interpreting a human's natural language. This paper presents a novel knowledge-based word sense disambiguation algorithm, namely Sequential Contextual Similarity Matrix Multiplication (SCSMM). The SCSMM algorithm combines semantic similarity, heuristic knowledge, and document context to respectively exploit the merits of local context between consecutive terms, human knowledge about terms, and a document's main topic in disambiguating terms. Unlike other algorithms, the SCSMM algorithm guarantees the capture of the maximum sentence context while maintaining the terms' order within the sentence. The proposed algorithm outperformed all other algorithms when disambiguating nouns on the combined gold standard datasets, while demonstrating comparable results to current state-of-the-art word sense disambiguation systems when dealing with each dataset separately. Furthermore, the paper discusses the impact of granularity level, ambiguity rate, sentence size, and part of speech distribution on the performance of the proposed algorithm. △ Less","8 January, 2021",https://arxiv.org/pdf/2101.02875
A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep Neural Network Fairness,Becky Mashaido;Winston Moh Tangongho,"Motivated by the need for fair algorithmic decision making in the age of automation and artificially-intelligent technology, this technical report provides a theoretical insight into adversarial training for fairness in deep learning. We build upon previous work in adversarial fairness, show the persistent tradeoff between fair predictions and model performance, and explore further mechanisms that help in offsetting this tradeoff. △ Less","7 January, 2021",https://arxiv.org/pdf/2101.02831
Privacy-Preserving Cloud-Aided Broad Learning System,Haiyang Liu;Hanlin Zhang;Li Guo;Jia Yu;Jie Lin,"With the rapid development of artificial intelligence and the advent of the 5G era, deep learning has received extensive attention from researchers. Broad Learning System (BLS) is a new deep learning model proposed recently, which shows its effectiveness in many fields, such as image recognition and fault detection. However, the training process still requires vast computations, and therefore cannot be accomplished by some resource-constrained devices. To solve this problem, the resource-constrained device can outsource the BLS algorithm to cloud servers. Nevertheless, some security challenges also follow with the use of cloud computing, including the privacy of the data and the correctness of returned results. In this paper, we propose a secure, efficient, and verifiable outsourcing algorithm for BLS. This algorithm not only improves the efficiency of the algorithm on the client but also ensures that the clients sensitive information is not leaked to the cloud server. In addition, in our algorithm, the client can verify the correctness of returned results with a probability of almost 1. Finally, we analyze the security and efficiency of our algorithm in theory and prove our algorithms feasibility through experiments. △ Less","7 January, 2021",https://arxiv.org/pdf/2101.02826
Argument Schemes and Dialogue for Explainable Planning,Quratul-ain Mahesar;Simon Parsons,"Artificial Intelligence (AI) is being increasingly deployed in practical applications. However, there is a major concern whether AI systems will be trusted by humans. In order to establish trust in AI systems, there is a need for users to understand the reasoning behind their solutions. Therefore, systems should be able to explain and justify their output. In this paper, we propose an argument scheme-based approach to provide explanations in the domain of AI planning. We present novel argument schemes to create arguments that explain a plan and its key elements; and a set of critical questions that allow interaction between the arguments and enable the user to obtain further information regarding the key elements of the plan. Furthermore, we present a novel dialogue system using the argument schemes and critical questions for providing interactive dialectical explanations. △ Less","14 February, 2021",https://arxiv.org/pdf/2101.02648
More Reliable AI Solution: Breast Ultrasound Diagnosis Using Multi-AI Combination,Jian Dai;Shuge Lei;Licong Dong;Xiaona Lin;Huabin Zhang;Desheng Sun;Kehong Yuan,"Objective: Breast cancer screening is of great significance in contemporary women's health prevention. The existing machines embedded in the AI system do not reach the accuracy that clinicians hope. How to make intelligent systems more reliable is a common problem. Methods: 1) Ultrasound image super-resolution: the SRGAN super-resolution network reduces the unclearness of ultrasound images caused by the device itself and improves the accuracy and generalization of the detection model. 2) In response to the needs of medical images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI model: based on the respective advantages of different AI models, we employ two AI models to determine clinical resuls cross validation. And we accept the same results and refuses others. Results: 1) With the help of the super-resolution model, the YOLOv4 model and the CenterNet model both increased the mAP score by 9.6% and 13.8%. 2) Two methods for transforming the target model into a classification model are proposed. And the unified output is in a specified format to facilitate the call of the molti-AI model. 3) In the classification evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%, specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity 92.54%), the multi-AI model will refuse to make judgments on 23.55% of the input data. Correspondingly, the performance has been greatly improved to 95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work makes the AI model more reliable in medical image diagnosis. Significance: 1) The proposed method makes the target detection model more suitable for diagnosing breast ultrasound images. 2) It provides a new idea for artificial intelligence in medical diagnosis, which can more conveniently introduce target detection models from other fields to serve medical lesion screening. △ Less","11 July, 2021",https://arxiv.org/pdf/2101.02639
RANK: AI-assisted End-to-End Architecture for Detecting Persistent Attacks in Enterprise Networks,Hazem M. Soliman;Geoff Salmon;Dušan Sovilj;Mohan Rao,"Advanced Persistent Threats (APTs) are sophisticated multi-step attacks, planned and executed by skilled adversaries targeting modern government and enterprise networks. Intrusion Detection Systems (IDSs) and User and Entity Behavior Analytics (UEBA) are commonly employed to aid a security analyst in the detection of APTs. The prolonged nature of APTs, combined with the granular focus of UEBA and IDS, results in overwhelming the analyst with an increasingly impractical number of alerts. Consequent to this abundance of data, and together with the crucial importance of the problem as well as the high cost of the skilled personnel involved, the problem of APT detection becomes a perfect candidate for automation through Artificial Intelligence (AI). In this paper, we provide, up to our knowledge, the first study and implementation of an end-to-end AI-assisted architecture for detecting APTs -- RANK. The goal of the system is not to replace the analyst, rather, it is to automate the complete pipeline from data sources to a final set of incidents for analyst review. The architecture is composed of four consecutive steps: 1) alert templating and merging, 2) alert graph construction, 3) alert graph partitioning into incidents, and 4) incident scoring and ordering. We evaluate our architecture against the 2000 DARPA Intrusion Detection dataset, as well as a read-world private dataset from a medium-scale enterprise. Extensive results are provided showing a three order of magnitude reduction in the amount of data to be reviewed by the analyst, innovative extraction of incidents and security-wise scoring of extracted incidents. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02573
Phishing Attacks and Websites Classification Using Machine Learning and Multiple Datasets (A Comparative Analysis),Sohail Ahmed Khan;Wasiq Khan;Abir Hussain,"Phishing attacks are the most common type of cyber-attacks used to obtain sensitive information and have been affecting individuals as well as organisations across the globe. Various techniques have been proposed to identify the phishing attacks specifically, deployment of machine intelligence in recent years. However, the deployed algorithms and discriminating factors are very diverse in existing works. In this study, we present a comprehensive analysis of various machine learning algorithms to evaluate their performances over multiple datasets. We further investigate the most significant features within multiple datasets and compare the classification performance with the reduced dimensional datasets. The statistical results indicate that random forest and artificial neural network outperform other classification algorithms, achieving over 97% accuracy using the identified features. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02552
A Low Power In-Memory Multiplication andAccumulation Array with Modified Radix-4 Inputand Canonical Signed Digit Weights,Rui Xiao;Kejie Huang;Yewei Zhang;Haibin Shen,"A mass of data transfer between the processing and storage units has been the leading bottleneck in modern Von-Neuman computing systems, especially when used for Artificial Intelligence (AI) tasks. Computing-in-Memory (CIM) has shown great potential to reduce both latency and power consumption. However, the conventional analog CIM schemes are suffering from reliability issues, which may significantly degenerate the accuracy of the computation. Recently, CIM schemes with digitized input data and weights have been proposed for high reliable computing. However, the properties of the digital memory and input data are not fully utilized. This paper presents a novel low power CIM scheme to further reduce the power consumption by using a Modified Radix-4 (M-RD4) booth algorithm at the input and a Modified Canonical Signed Digit (M-CSD) for the network weights. The simulation results show that M-Rd4 and M-CSD reduce the ratio of 1\times1 by 78.5\% on LeNet and 80.2\% on AlexNet, and improve the computing efficiency by 41.6\% in average. The computing-power rate at the fixed-point 8-bit is 60.68 TOPS/s/W. △ Less","7 January, 2021",https://arxiv.org/pdf/2101.02419
COVID19-HPSMP: COVID-19 Adopted Hybrid and Parallel Deep Information Fusion Framework for Stock Price Movement Prediction,Farnoush Ronaghi;Mohammad Salimibeni;Farnoosh Naderkhani;Arash Mohammadi,"The novel of coronavirus (COVID-19) has suddenly and abruptly changed the world as we knew at the start of the 3rd decade of the 21st century. Particularly, COVID-19 pandemic has negatively affected financial econometrics and stock markets across the globe. Artificial Intelligence (AI) and Machine Learning (ML)-based prediction models, especially Deep Neural Network (DNN) architectures, have the potential to act as a key enabling factor to reduce the adverse effects of the COVID-19 pandemic and future possible ones on financial markets. In this regard, first, a unique COVID-19 related PRIce MOvement prediction (COVID19 PRIMO) dataset is introduced in this paper, which incorporates effects of social media trends related to COVID-19 on stock market price movements. Afterwards, a novel hybrid and parallel DNN-based framework is proposed that integrates different and diversified learning architectures. Referred to as the COVID-19 adopted Hybrid and Parallel deep fusion framework for Stock price Movement Prediction (COVID19-HPSMP), innovative fusion strategies are used to combine scattered social media news related to COVID-19 with historical mark data. The proposed COVID19-HPSMP consists of two parallel paths (hence hybrid), one based on Convolutional Neural Network (CNN) with Local/Global Attention modules, and one integrated CNN and Bi-directional Long Short term Memory (BLSTM) path. The two parallel paths are followed by a multilayer fusion layer acting as a fusion centre that combines localized features. Performance evaluations are performed based on the introduced COVID19 PRIMO dataset illustrating superior performance of the proposed framework. △ Less","8 July, 2021",https://arxiv.org/pdf/2101.02287
"Teach me to play, gamer! Imitative learning in computer games via linguistic description of complex phenomena and decision tree",Clemente Rubio-Manzano;Tomas Lermanda;CLaudia Martinez;Alejandra Segura;Christian Vidal,"In this article, we present a new machine learning model by imitation based on the linguistic description of complex phenomena. The idea consists of, first, capturing the behaviour of human players by creating a computational perception network based on the execution traces of the games and, second, representing it using fuzzy logic (linguistic variables and if-then rules). From this knowledge, a set of data (dataset) is automatically created to generate a learning model based on decision trees. This model will be used later to automatically control the movements of a bot. The result is an artificial agent that mimics the human player. We have implemented, tested and evaluated this technology. The results obtained are interesting and promising, showing that this method can be a good alternative to design and implement the behaviour of intelligent agents in video game development. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02264
Controlling Synthetic Characters in Simulations: A Case for Cognitive Architectures and Sigma,Volkan Ustun;Paul S. Rosenbloom;Seyed Sajjadi;Jeremy Nuttal,"Simulations, along with other similar applications like virtual worlds and video games, require computational models of intelligence that generate realistic and credible behavior for the participating synthetic characters. Cognitive architectures, which are models of the fixed structure underlying intelligent behavior in both natural and artificial systems, provide a conceptually valid common basis, as evidenced by the current efforts towards a standard model of the mind, to generate human-like intelligent behavior for these synthetic characters. Sigma is a cognitive architecture and system that strives to combine what has been learned from four decades of independent work on symbolic cognitive architectures, probabilistic graphical models, and more recently neural models, under its graphical architecture hypothesis. Sigma leverages an extended form of factor graphs towards a uniform grand unification of not only traditional cognitive capabilities but also key non-cognitive aspects, creating unique opportunities for the construction of new kinds of cognitive models that possess a Theory-of-Mind and that are perceptual, autonomous, interactive, affective, and adaptive. In this paper, we will introduce Sigma along with its diverse capabilities and then use three distinct proof-of-concept Sigma models to highlight combinations of these capabilities: (1) Distributional reinforcement learning models in; (2) A pair of adaptive and interactive agent models that demonstrate rule-based, probabilistic, and social reasoning; and (3) A knowledge-free exploration model in which an agent leverages only architectural appraisal variables, namely attention and curiosity, to locate an item while building up a map in a Unity environment. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02231
Artificial Intelligence Methods in In-Cabin Use Cases: A Survey,Yao Rong;Chao Han;Christian Hellert;Antje Loyal;Enkelejda Kasneci,"As interest in autonomous driving increases, efforts are being made to meet requirements for the high-level automation of vehicles. In this context, the functionality inside the vehicle cabin plays a key role in ensuring a safe and pleasant journey for driver and passenger alike. At the same time, recent advances in the field of artificial intelligence (AI) have enabled a whole range of new applications and assistance systems to solve automated problems in the vehicle cabin. This paper presents a thorough survey on existing work that utilizes AI methods for use-cases inside the driving cabin, focusing, in particular, on application scenarios related to (1) driving safety and (2) driving comfort. Results from the surveyed works show that AI technology has a promising future in tackling in-cabin tasks within the autonomous driving aspect. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02082
"Socially Responsible AI Algorithms: Issues, Purposes, and Challenges",Lu Cheng;Kush R. Varshney;Huan Liu,"In the current era, people and society have grown increasingly reliant on artificial intelligence (AI) technologies. AI has the potential to drive us towards a future in which all of humanity flourishes. It also comes with substantial risks for oppression and calamity. Discussions about whether we should (re)trust AI have repeatedly emerged in recent years and in many quarters, including industry, academia, healthcare, services, and so on. Technologists and AI researchers have a responsibility to develop trustworthy AI systems. They have responded with great effort to design more responsible AI algorithms. However, existing technical solutions are narrow in scope and have been primarily directed towards algorithms for scoring or classification tasks, with an emphasis on fairness and unwanted bias. To build long-lasting trust between AI and human beings, we argue that the key is to think beyond algorithmic fairness and connect major aspects of AI that potentially cause AI's indifferent behavior. In this survey, we provide a systematic framework of Socially Responsible AI Algorithms that aims to examine the subjects of AI indifference and the need for socially responsible AI algorithms, define the objectives, and introduce the means by which we may achieve these objectives. We further discuss how to leverage this framework to improve societal well-being through protection, information, and prevention/mitigation. △ Less","21 August, 2021",https://arxiv.org/pdf/2101.02032
Towards an Abolitionist AI: the role of Historically Black Colleges and Universities,Charles C. Earl,"Abolition is the process of destroying and then rebuilding the structures that impede liberation. This paper addresses the particular case of Black folk in the United States, but is relevant to the global decolonization movement. Using notions of abolition and infrastructures of feeling developed by Ruth Wilson Gilmore, I view Historically Black Colleges and Universities ( HBCUs ) as a particular kind of abolitionist project, created for the explicit purpose of nurturing and sustaining Black excellence particularly within the sciences. I then examine how artificial intelligence (AI) in particular and computing in general have contributed to racial oppression and the further confinement and diminishing of Black existence. I conclude by examining how the space held by HBCUs in computing might contribute to a re-imagining of AI as a technology that enhances the possibility and actualization of Black life. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.02011
Cross-Validation and Uncertainty Determination for Randomized Neural Networks with Applications to Mobile Sensors,Ansgar Steland;Bart E. Pieters,"Randomized artificial neural networks such as extreme learning machines provide an attractive and efficient method for supervised learning under limited computing ressources and green machine learning. This especially applies when equipping mobile devices (sensors) with weak artificial intelligence. Results are discussed about supervised learning with such networks and regression methods in terms of consistency and bounds for the generalization and prediction error. Especially, some recent results are reviewed addressing learning with data sampled by moving sensors leading to non-stationary and dependent samples. As randomized networks lead to random out-of-sample performance measures, we study a cross-validation approach to handle the randomness and make use of it to improve out-of-sample performance. Additionally, a computationally efficient approach to determine the resulting uncertainty in terms of a confidence interval for the mean out-of-sample prediction error is discussed based on two-stage estimation. The approach is applied to a prediction problem arising in vehicle integrated photovoltaics. △ Less","6 January, 2021",https://arxiv.org/pdf/2101.01990
Deep Neural Network Based Relation Extraction: An Overview,Hailin Wang;Ke Qin;Rufai Yusuf Zakari;Guoming Lu;Jin Yin,"Knowledge is a formal way of understanding the world, providing a human-level cognition and intelligence for the next-generation artificial intelligence (AI). One of the representations of knowledge is semantic relations between entities. An effective way to automatically acquire this important knowledge, called Relation Extraction (RE), a sub-task of information extraction, plays a vital role in Natural Language Processing (NLP). Its purpose is to identify semantic relations between entities from natural language text. To date, there are several studies for RE in previous works, which have documented these techniques based on Deep Neural Networks (DNNs) become a prevailing technique in this research. Especially, the supervised and distant supervision methods based on DNNs are the most popular and reliable solutions for RE. This article 1) introduces some general concepts, and further 2) gives a comprehensive overview of DNNs in RE from two points of view: supervised RE, which attempts to improve the standard RE systems, and distant supervision RE, which adopts DNNs to design sentence encoder and de-noise method. We further 3) cover some novel methods and recent trends as well as discuss possible future research directions for this task. △ Less","7 February, 2021",https://arxiv.org/pdf/2101.01907
Interpersonal distance in VR: reactions of older adults to the presence of a virtual agent,Grzegorz Pochwatko;Barbara Karpowicz;Anna Chrzanowska;Wiesław Kopeć,"The rapid development of virtual reality technology has increased its availability and, consequently, increased the number of its possible applications. The interest in the new medium has grown due to the entertainment industry (games, VR experiences and movies). The number of freely available training and therapeutic applications is also increasing. Contrary to popular opinion, new technologies are also adopted by older adults. Creating virtual environments tailored to the needs and capabilities of older adults requires intense research on the behaviour of these participants in the most common situations, towards commonly used elements of the virtual environment, in typical sceneries. Comfortable immersion in a virtual environment is key to achieving the impression of presence. Presence is, in turn, necessary to obtain appropriate training, persuasive and therapeutic effects. A virtual agent (a humanoid representation of an algorithm or artificial intelligence) is often an element of the virtual environment interface. Maintaining an appropriate distance to the agent is, therefore, a key parameter for the creator of the VR experience. Older (65+) participants maintain greater distance towards an agent (a young white male) than younger ones (25-35). It may be caused by differences in the level of arousal, but also cultural norms. As a consequence, VR developers are advised to use algorithms that maintain the agent at the appropriate distance, depending on the user's age. △ Less","5 January, 2021",https://arxiv.org/pdf/2101.01652
AI based Service Management for 6G Green Communications,Bomin Mao;Fengxiao Tang;Kawamoto Yuichi;Nei Kato,"Green communications have always been a target for the information industry to alleviate energy overhead and reduce fossil fuel usage. In current 5G and future 6G era, there is no doubt that the volume of network infrastructure and the number of connected terminals will keep exponentially increasing, which results in the surging energy cost. It becomes growing important and urgent to drive the development of green communications. However, 6G will inevitably have increasingly stringent and diversified requirements for Quality of Service (QoS), security, flexibility, and even intelligence, all of which challenge the improvement of energy efficiency. Moreover, the dynamic energy harvesting process, which will be adopted widely in 6G, further complicates the power control and network management. To address these challenges and reduce human intervene, Artificial Intelligence (AI) has been widely recognized and acknowledged as the only solution. Academia and industry have conducted extensive research to alleviate energy demand, improve energy efficiency, and manage energy harvesting in various communication scenarios. In this paper, we present the main considerations for green communications and survey the related research on AI-based green communications. We focus on how AI techniques are adopted to manage the network and improve energy harvesting toward the green era. We analyze how state-of-the-art Machine Learning (ML) and Deep Learning (DL) techniques can cooperate with conventional AI methods and mathematical models to reduce the algorithm complexity and optimize the accuracy rate to accelerate the applications in 6G. Finally, we discuss the existing problems and envision the challenges for these emerging techniques in 6G. △ Less","11 January, 2021",https://arxiv.org/pdf/2101.01588
"""Brilliant AI Doctor"" in Rural China: Tensions and Challenges in AI-Powered CDSS Deployment",Dakuo Wang;Liuping Wang;Zhan Zhang;Ding Wang;Haiyi Zhu;Yvonne Gao;Xiangmin Fan;Feng Tian,"Artificial intelligence (AI) technology has been increasingly used in the implementation of advanced Clinical Decision Support Systems (CDSS). Research demonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical decision making scenarios. However, post-adoption user perception and experience remain understudied, especially in developing countries. Through observations and interviews with 22 clinicians from 6 rural clinics in China, this paper reports the various tensions between the design of an AI-CDSS system (""Brilliant Doctor"") and the rural clinical context, such as the misalignment with local context and workflow, the technical limitations and usability barriers, as well as issues related to transparency and trustworthiness of AI-CDSS. Despite these tensions, all participants expressed positive attitudes toward the future of AI-CDSS, especially acting as ""a doctor's AI assistant"" to realize a Human-AI Collaboration future in clinical settings. Finally we draw on our findings to discuss implications for designing AI-CDSS interventions for rural clinical contexts in developing countries. △ Less","12 January, 2021",https://arxiv.org/pdf/2101.01524
Development of a Respiratory Sound Labeling Software for Training a Deep Learning-Based Respiratory Sound Analysis Model,Fu-Shun Hsu;Chao-Jung Huang;Chen-Yi Kuo;Shang-Ran Huang;Yuan-Ren Cheng;Jia-Horng Wang;Yi-Lin Wu;Tzu-Ling Tzeng;Feipei Lai,"Respiratory auscultation can help healthcare professionals detect abnormal respiratory conditions if adventitious lung sounds are heard. The state-of-the-art artificial intelligence technologies based on deep learning show great potential in the development of automated respiratory sound analysis. To train a deep learning-based model, a huge number of accurate labels of normal breath sounds and adventitious sounds are needed. In this paper, we demonstrate the work of developing a respiratory sound labeling software to help annotators identify and label the inhalation, exhalation, and adventitious respiratory sound more accurately and quickly. Our labeling software integrates six features from MATLAB Audio Labeler, and one commercial audio editor, RX7. As of October, 2019, we have labeled 9,765 15-second-long audio files of breathing lung sounds, and accrued 34,095 inhalation labels,18,349 exhalation labels, 13,883 continuous adventitious sounds (CASs) labels and 15,606 discontinuous adventitious sounds (DASs) labels, which are significantly larger than previously published studies. The trained convolutional recurrent neural networks based on these labels showed good performance with F1-scores of 86.0% on inhalation event detection, 51.6% on CASs event detection and 71.4% on DASs event detection. In conclusion, our results show that our proposed respiratory sound labeling software could easily pre-define a label, perform one-click labeling, and overall facilitate the process of accurately labeling. This software helps develop deep learning-based models that require a huge amount of labeled acoustic data. △ Less","4 January, 2021",https://arxiv.org/pdf/2101.01352
Advancing Computing's Foundation of US Industry & Society,Thomas M. Conte;Ian T. Foster;William Gropp;Mark D. Hill,"While past information technology (IT) advances have transformed society, future advances hold even greater promise. For example, we have only just begun to reap the changes from artificial intelligence (AI), especially machine learning (ML). Underlying IT's impact are the dramatic improvements in computer hardware, which deliver performance that unlock new capabilities. For example, recent successes in AI/ML required the synergy of improved algorithms and hardware architectures (e.g., general-purpose graphics processing units). However, unlike in the 20th Century and early 2000s, tomorrow's performance aspirations must be achieved without continued semiconductor scaling formerly provided by Moore's Law and Dennard Scaling. How will one deliver the next 100x improvement in capability at similar or less cost to enable great value? Can we make the next AI leap without 100x better hardware? This whitepaper argues for a multipronged effort to develop new computing approaches beyond Moore's Law to advance the foundation that computing provides to US industry, education, medicine, science, and government. This impact extends far beyond the IT industry itself, as IT is now central for providing value across society, for example in semi-autonomous vehicles, tele-education, health wearables, viral analysis, and efficient administration. Herein we draw upon considerable visioning work by CRA's Computing Community Consortium (CCC) and the IEEE Rebooting Computing Initiative (IEEE RCI), enabled by thought leader input from industry, academia, and the US government. △ Less","4 January, 2021",https://arxiv.org/pdf/2101.01284
Computing Research Challenges in Next Generation Wireless Networking,Elisa Bertino;Daniel Bliss;Daniel Lopresti;Larry Peterson;Henning Schulzrinne,"By all measures, wireless networking has seen explosive growth over the past decade. Fourth Generation Long Term Evolution (4G LTE) cellular technology has increased the bandwidth available for smartphones, in essence, delivering broadband speeds to mobile devices. The most recent 5G technology is further enhancing the transmission speeds and cell capacity, as well as, reducing latency through the use of different radio technologies and is expected to provide Internet connections that are an order of magnitude faster than 4G LTE. Technology continues to advance rapidly, however, and the next generation, 6G, is already being envisioned. 6G will make possible a wide range of powerful, new applications including holographic telepresence, telehealth, remote education, ubiquitous robotics and autonomous vehicles, smart cities and communities (IoT), and advanced manufacturing (Industry 4.0, sometimes referred to as the Fourth Industrial Revolution), to name but a few. The advances we will see begin at the hardware level and extend all the way to the top of the software ""stack."" Artificial Intelligence (AI) will also start playing a greater role in the development and management of wireless networking infrastructure by becoming embedded in applications throughout all levels of the network. The resulting benefits to society will be enormous. At the same time these exciting new wireless capabilities are appearing rapidly on the horizon, a broad range of research challenges loom ahead. These stem from the ever-increasing complexity of the hardware and software systems, along with the need to provide infrastructure that is robust and secure while simultaneously protecting the privacy of users. Here we outline some of those challenges and provide recommendations for the research that needs to be done to address them. △ Less","4 January, 2021",https://arxiv.org/pdf/2101.01279
A Research Ecosystem for Secure Computing,Nadya Bliss;Lawrence A. Gordon;Daniel Lopresti;Fred Schneider;Suresh Venkatasubramanian,"Computing devices are vital to all areas of modern life and permeate every aspect of our society. The ubiquity of computing and our reliance on it has been accelerated and amplified by the COVID-19 pandemic. From education to work environments to healthcare to defense to entertainment - it is hard to imagine a segment of modern life that is not touched by computing. The security of computers, systems, and applications has been an active area of research in computer science for decades. However, with the confluence of both the scale of interconnected systems and increased adoption of artificial intelligence, there are many research challenges the community must face so that our society can continue to benefit and risks are minimized, not multiplied. Those challenges range from security and trust of the information ecosystem to adversarial artificial intelligence and machine learning. Along with basic research challenges, more often than not, securing a system happens after the design or even deployment, meaning the security community is routinely playing catch-up and attempting to patch vulnerabilities that could be exploited any minute. While security measures such as encryption and authentication have been widely adopted, questions of security tend to be secondary to application capability. There needs to be a sea-change in the way we approach this critically important aspect of the problem: new incentives and education are at the core of this change. Now is the time to refocus research community efforts on developing interconnected technologies with security ""baked in by design"" and creating an ecosystem that ensures adoption of promising research developments. To realize this vision, two additional elements of the ecosystem are necessary - proper incentive structures for adoption and an educated citizenry that is well versed in vulnerabilities and risks. △ Less","4 January, 2021",https://arxiv.org/pdf/2101.01264
"An Artificial Intelligence System for Combined Fruit Detection and Georeferencing, Using RTK-Based Perspective Projection in Drone Imagery",Angus Baird;Stefano Giani,"This work presents an Artificial Intelligence (AI) system, based on the Faster Region-Based Convolution Neural Network (Faster R-CNN) framework, which detects and counts apples from oblique, aerial drone imagery of giant commercial orchards. To reduce computational cost, a novel precursory stage to the network is designed to preprocess raw imagery into cropped images of individual trees. Unique geospatial identifiers are allocated to these using the perspective projection model. This employs Real-Time Kinematic (RTK) data, Digital Terrain and Surface Models (DTM and DSM), as well as internal and external camera parameters. The bulk of experiments however focus on tuning hyperparameters in the detection network itself. Apples which are on trees and apples which are on the ground are treated as separate classes. A mean Average Precision (mAP) metric, calibrated by the size of the two classes, is devised to mitigate spurious results. Anchor box design is of key interest due to the scale of the apples. As such, a k-means clustering approach, never before seen in literature for Faster R-CNN, resulted in the most significant improvements to calibrated mAP. Other experiments showed that the maximum number of box proposals should be 225; the initial learning rate of 0.001 is best applied to the adaptive RMS Prop optimiser; and ResNet 101 is the ideal base feature extractor when considering mAP and, to a lesser extent, inference time. The amalgamation of the optimal hyperparameters leads to a model with a calibrated mAP of 0.7627. △ Less","1 January, 2021",https://arxiv.org/pdf/2101.00339
Efficient Learning-based Scheduling for Information Freshness in Wireless Networks,Bin Li,"Motivated by the recent trend of integrating artificial intelligence into the Internet-of-Things (IoT), we consider the problem of scheduling packets from multiple sensing sources to a central controller over a wireless network. Here, packets from different sensing sources have different values or degrees of importance to the central controller for intelligent decision making. In such a setup, it is critical to provide timely and valuable information for the central controller. In this paper, we develop a parameterized maximum-weight type scheduling policy that combines both the AoI metrics and Upper Confidence Bound (UCB) estimates in its weight measure with parameter η. Here, UCB estimates balance the tradeoff between exploration and exploitation in learning and are critical for yielding a small cumulative regret. We show that our proposed algorithm yields the running average total age at most by O(N^2η). We also prove that our proposed algorithm achieves the cumulative regret over time horizon T at most by O(NT/η+\sqrt{NT\log T}). This reveals a tradeoff between the cumulative regret and the running average total age: when increasing η, the cumulative regret becomes smaller, but is at the cost of increasing running average total age. Simulation results are provided to evaluate the efficiency of our proposed algorithm. △ Less","1 January, 2021",https://arxiv.org/pdf/2101.00257
"Interplay between RIS and AI in Wireless Communications: Fundamentals, Architectures, Applications, and Open Research Problems",Jinghe Wang;Wankai Tang;Yu Han;Shi Jin;Xiao Li;Chao-Kai Wen;Qiang Cheng;Tie Jun Cui,"Future wireless communication networks are expected to fulfill the unprecedented performance requirements to support our highly digitized and globally data-driven society. Various technological challenges must be overcome to achieve our goal. Among many potential technologies, reconfigurable intelligent surface (RIS) and artificial intelligence (AI) have attracted extensive attention, thereby leading to a proliferation of studies for utilizing them in wireless communication systems. The RIS-based wireless communication frameworks and AI-enabled technologies, two of the promising technologies for the sixth-generation networks, interact and promote with each other, striving to collaboratively create a controllable, intelligent, reconfigurable, and programmable wireless propagation environment. This paper explores the road to implementing the combination of RIS and AI; specifically, integrating AI-enabled technologies into RIS-based frameworks for maximizing the practicality of RIS to facilitate the realization of smart radio propagation environments, elaborated from shallow to deep insights. We begin with the basic concept and fundamental characteristics of RIS, followed by the overview of the research status of RIS. Then, we analyze the inevitable trend of RIS to be combined with AI. In particular, we focus on recent research about RIS-based architectures embedded with AI, elucidating from the intelligent structures and systems of metamaterials to the AI-embedded RIS-assisted wireless communication systems. Finally, the challenges and potential of the topic are discussed. △ Less","1 January, 2021",https://arxiv.org/pdf/2101.00250
A Survey on Deep Reinforcement Learning for Audio-Based Applications,Siddique Latif;Heriberto Cuayáhuitl;Farrukh Pervez;Fahad Shamshad;Hafiz Shehbaz Ali;Erik Cambria,"Deep reinforcement learning (DRL) is poised to revolutionise the field of artificial intelligence (AI) by endowing autonomous systems with high levels of understanding of the real world. Currently, deep learning (DL) is enabling DRL to effectively solve various intractable problems in various fields. Most importantly, DRL algorithms are also being employed in audio signal processing to learn directly from speech, music and other sound signals in order to create audio-based autonomous systems that have many promising application in the real world. In this article, we conduct a comprehensive survey on the progress of DRL in the audio domain by bringing together the research studies across different speech and music-related areas. We begin with an introduction to the general field of DL and reinforcement learning (RL), then progress to the main DRL methods and their applications in the audio domain. We conclude by presenting challenges faced by audio-based DRL agents and highlighting open areas for future research and investigation. △ Less","1 January, 2021",https://arxiv.org/pdf/2101.00240
DISCOS: Bridging the Gap between Discourse Knowledge and Commonsense Knowledge,Tianqing Fang;Hongming Zhang;Weiqi Wang;Yangqiu Song;Bin He,"Commonsense knowledge is crucial for artificial intelligence systems to understand natural language. Previous commonsense knowledge acquisition approaches typically rely on human annotations (for example, ATOMIC) or text generation models (for example, COMET.) Human annotation could provide high-quality commonsense knowledge, yet its high cost often results in relatively small scale and low coverage. On the other hand, generation models have the potential to automatically generate more knowledge. Nonetheless, machine learning models often fit the training data well and thus struggle to generate high-quality novel knowledge. To address the limitations of previous approaches, in this paper, we propose an alternative commonsense knowledge acquisition framework DISCOS (from DIScourse to COmmonSense), which automatically populates expensive complex commonsense knowledge to more affordable linguistic knowledge resources. Experiments demonstrate that we can successfully convert discourse knowledge about eventualities from ASER, a large-scale discourse knowledge graph, into if-then commonsense knowledge defined in ATOMIC without any additional annotation effort. Further study suggests that DISCOS significantly outperforms previous supervised approaches in terms of novelty and diversity with comparable quality. In total, we can acquire 3.4M ATOMIC-like inferential commonsense knowledge by populating ATOMIC on the core part of ASER. Codes and data are available at https://github.com/HKUST-KnowComp/DISCOS-commonsense. △ Less","18 February, 2021",https://arxiv.org/pdf/2101.00154
dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python,Hubert Baniecki;Wojciech Kretowicz;Piotr Piatyszek;Jakub Wisniewski;Przemyslaw Biecek,"The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at https://python.drwhy.ai/. △ Less","11 October, 2021",https://arxiv.org/pdf/2012.14406
SG-Net: Syntax Guided Transformer for Language Representation,Zhuosheng Zhang;Yuwei Wu;Junru Zhou;Sufeng Duan;Hai Zhao;Rui Wang,"Understanding human language is one of the key themes of artificial intelligence. For language representation, the capacity of effectively modeling the linguistic knowledge from the detail-riddled and lengthy texts and getting rid of the noises is essential to improve its performance. Traditional attentive models attend to all words without explicit constraint, which results in inaccurate concentration on some dispensable words. In this work, we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanisms for better linguistically motivated word representations. In detail, for self-attention network (SAN) sponsored Transformer-based encoder, we introduce syntactic dependency of interest (SDOI) design into the SAN to form an SDOI-SAN with syntax-guided self-attention. Syntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation. The proposed SG-Net is applied to typical Transformer encoders. Extensive experiments on popular benchmark tasks, including machine reading comprehension, natural language inference, and neural machine translation show the effectiveness of the proposed SG-Net design. △ Less","7 January, 2021",https://arxiv.org/pdf/2012.13915
Logic Tensor Networks,Samy Badreddine;Artur d'Avila Garcez;Luciano Serafini;Michael Spranger,"Artificial Intelligence agents are required to learn from their surroundings and to reason about the knowledge that has been learned in order to make decisions. While state-of-the-art learning from data typically uses sub-symbolic distributed representations, reasoning is normally useful at a higher level of abstraction with the use of a first-order logic language for knowledge representation. As a result, attempts at combining symbolic AI and neural computation into neural-symbolic systems have been on the increase. In this paper, we present Logic Tensor Networks (LTN), a neurosymbolic formalism and computational model that supports learning and reasoning through the introduction of a many-valued, end-to-end differentiable first-order logic called Real Logic as a representation language for deep learning. We show that LTN provides a uniform language for the specification and the computation of several AI tasks such as data clustering, multi-label classification, relational learning, query answering, semi-supervised learning, regression and embedding learning. We implement and illustrate each of the above tasks with a number of simple explanatory examples using TensorFlow 2. Keywords: Neurosymbolic AI, Deep Learning and Reasoning, Many-valued Logic. △ Less","22 December, 2021",https://arxiv.org/pdf/2012.13635
Towards a Universal Continuous Knowledge Base,Gang Chen;Maosong Sun;Yang Liu,"In artificial intelligence (AI), knowledge is the information required by an intelligent system to accomplish tasks. While traditional knowledge bases use discrete, symbolic representations, detecting knowledge encoded in the continuous representations learned from data has received increasing attention recently. In this work, we propose a method for building a continuous knowledge base (CKB) that can store knowledge imported from multiple, diverse neural networks. The key idea of our approach is to define an interface for each neural network and cast knowledge transferring as a function simulation problem. Experiments on text classification show promising results: the CKB imports knowledge from a single model and then exports the knowledge to a new model, achieving comparable performance with the original model. More interesting, we import the knowledge from multiple models to the knowledge base, from which the fused knowledge is exported back to a single model, achieving a higher accuracy than the original model. With the CKB, it is also easy to achieve knowledge distillation and transfer learning. Our work opens the door to building a universal continuous knowledge base to collect, store, and organize all continuous knowledge encoded in various neural networks trained for different AI tasks. △ Less","17 April, 2021",https://arxiv.org/pdf/2012.13568
GANterfactual - Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning,Silvan Mertes;Tobias Huber;Katharina Weitz;Alexander Heimerl;Elisabeth André,"With the ongoing rise of machine learning, the need for methods for explaining decisions made by artificial intelligence systems is becoming a more and more important topic. Especially for image classification tasks, many state-of-the-art tools to explain such classifiers rely on visual highlighting of important areas of the input data. Contrary, counterfactual explanation systems try to enable a counterfactual reasoning by modifying the input image in a way such that the classifier would have made a different prediction. By doing so, the users of counterfactual explanation systems are equipped with a completely different kind of explanatory information. However, methods for generating realistic counterfactual explanations for image classifiers are still rare. Especially in medical contexts, where relevant information often consists of textural and structural information, high-quality counterfactual images have the potential to give meaningful insights into decision processes. In this work, we present GANterfactual, an approach to generate such counterfactual image explanations based on adversarial image-to-image translation techniques. Additionally, we conduct a user study to evaluate our approach in an exemplary medical use case. Our results show that, in the chosen medical use-case, counterfactual explanations lead to significantly better results regarding mental models, explanation satisfaction, trust, emotions, and self-efficacy than two state-of-the-art systems that work with saliency maps, namely LIME and LRP. △ Less","5 August, 2021",https://arxiv.org/pdf/2012.11905
Efficient and Visualizable Convolutional Neural Networks for COVID-19 Classification Using Chest CT,Aksh Garg;Sana Salehi;Marianna La Rocca;Rachael Garner;Dominique Duncan,"With COVID-19 cases rising rapidly, deep learning has emerged as a promising diagnosis technique. However, identifying the most accurate models to characterize COVID-19 patients is challenging because comparing results obtained with different types of data and acquisition processes is non-trivial. In this paper we designed, evaluated, and compared the performance of 20 convolutional neutral networks in classifying patients as COVID-19 positive, healthy, or suffering from other pulmonary lung infections based on Chest CT scans, serving as the first to consider the EfficientNet family for COVID-19 diagnosis and employ intermediate activation maps for visualizing model performance. All models are trained and evaluated in Python using 4173 Chest CT images from the dataset entitled ""A COVID multiclass dataset of CT scans,"" with 2168, 758, and 1247 images of patients that are COVID-19 positive, healthy, or suffering from other pulmonary infections, respectively. EfficientNet-B5 was identified as the best model with an F1 score of 0.9769+/-0.0046, accuracy of 0.9759+/-0.0048, sensitivity of 0.9788+/-0.0055, specificity of 0.9730+/-0.0057, and precision of 0.9751 +/- 0.0051. On an alternate 2-class dataset, EfficientNetB5 obtained an accuracy of 0.9845+/-0.0109, F1 score of 0.9599+/-0.0251, sensitivity of 0.9682+/-0.0099, specificity of 0.9883+/-0.0150, and precision of 0.9526 +/- 0.0523. Intermediate activation maps and Gradient-weighted Class Activation Mappings offered human-interpretable evidence of the model's perception of ground-class opacities and consolidations, hinting towards a promising use-case of artificial intelligence-assisted radiology tools. With a prediction speed of under 0.1 seconds on GPUs and 0.5 seconds on CPUs, our proposed model offers a rapid, scalable, and accurate diagnostic for COVID-19. △ Less","19 July, 2021",https://arxiv.org/pdf/2012.11860
Recognizing Emotion Cause in Conversations,Soujanya Poria;Navonil Majumder;Devamanyu Hazarika;Deepanway Ghosal;Rishabh Bhardwaj;Samson Yu Bai Jian;Pengfei Hong;Romila Ghosh;Abhinaba Roy;Niyati Chhaya;Alexander Gelbukh;Rada Mihalcea,"We address the problem of recognizing emotion cause in conversations, define two novel sub-tasks of this problem, and provide a corresponding dialogue-level dataset, along with strong Transformer-based baselines. The dataset is available at https://github.com/declare-lab/RECCON. Introduction: Recognizing the cause behind emotions in text is a fundamental yet under-explored area of research in NLP. Advances in this area hold the potential to improve interpretability and performance in affect-based models. Identifying emotion causes at the utterance level in conversations is particularly challenging due to the intermingling dynamics among the interlocutors. Method: We introduce the task of Recognizing Emotion Cause in CONversations with an accompanying dataset named RECCON, containing over 1,000 dialogues and 10,000 utterance cause-effect pairs. Furthermore, we define different cause types based on the source of the causes, and establish strong Transformer-based baselines to address two different sub-tasks on this dataset: causal span extraction and causal emotion entailment. Result: Our Transformer-based baselines, which leverage contextual pre-trained embeddings, such as RoBERTa, outperform the state-of-the-art emotion cause extraction approaches Conclusion: We introduce a new task highly relevant for (explainable) emotion-aware artificial intelligence: recognizing emotion cause in conversations, provide a new highly challenging publicly available dialogue-level dataset for this task, and give strong baseline results on this dataset. △ Less","28 July, 2021",https://arxiv.org/pdf/2012.11820
Automatic Diagnosis of Pneumothorax from Chest Radiographs: A Systematic Literature Review,Tahira Iqbal;Arslan Shaukat;Usman Akram;Zartasha Mustansar,"Among various medical imaging tools, chest radiographs are the most important and widely used diagnostic tool for detection of thoracic pathologies. Research is being carried out in order to propose robust automatic diagnostic tool for detection of pathologies from chest radiographs. Artificial Intelligence techniques especially deep learning methodologies have found to be giving promising results in automating the field of medicine. Lot of research has been done for automatic and fast detection of pneumothorax from chest radiographs while proposing several frameworks based on artificial intelligence and machine learning techniques. This study summarizes the existing literature for the automatic detection of pneumothorax from chest x-rays along with describing the available chest radiographs datasets. The comparative analysis of the literature is also provided in terms of goodness. Limitations of the existing literature along with the research gaps is also given for further investigation. The paper provides a brief overview of the present work for pneumothorax detection for helping the researchers in selection of optimal approach for future research. △ Less","16 April, 2021",https://arxiv.org/pdf/2012.11214
Recent Developments in Detection of Central Serous Retinopathy through Imaging and Artificial Intelligence Techniques A Review,Syed Ale Hassan;Shahzad Akbar;Amjad Rehman;Tanzila Saba;Hoshang Kolivand;Saeed Ali Bahaj,"Central Serous Retinopathy (CSR) or Central Serous Chorioretinopathy (CSC) is a significant disease that causes blindness and vision loss among millions of people worldwide. It transpires as a result of accumulation of watery fluids behind the retina. Therefore, detection of CSR at early stages allows preventive measures to avert any impairment to the human eye. Traditionally, several manual methods for detecting CSR have been developed in the past; however, they have shown to be imprecise and unreliable. Consequently, Artificial Intelligence (AI) services in the medical field, including automated CSR detection, are now possible to detect and cure this disease. This review assessed a variety of innovative technologies and researches that contribute to the automatic detection of CSR. In this review, various CSR disease detection techniques, broadly classified into two categories: a) CSR detection based on classical imaging technologies, and b) CSR detection based on Machine/Deep Learning methods, have been reviewed after an elaborated evaluation of 29 different relevant articles. Additionally, it also goes over the advantages, drawbacks and limitations of a variety of traditional imaging techniques, such as Optical Coherence Tomography Angiography (OCTA), Fundus Imaging and more recent approaches that utilize Artificial Intelligence techniques. Finally, it is concluded that the most recent Deep Learning (DL) classifiers deliver accurate, fast, and reliable CSR detection. However, more research needs to be conducted on publicly available datasets to improve computation complexity for the reliable detection and diagnosis of CSR disease. △ Less","25 August, 2021",https://arxiv.org/pdf/2012.10961
XAI4Wind: A Multimodal Knowledge Graph Database for Explainable Decision Support in Operations & Maintenance of Wind Turbines,Joyjit Chatterjee;Nina Dethlefs,"Condition-based monitoring (CBM) has been widely utilised in the wind industry for monitoring operational inconsistencies and failures in turbines, with techniques ranging from signal processing and vibration analysis to artificial intelligence (AI) models using Supervisory Control & Acquisition (SCADA) data. However, existing studies do not present a concrete basis to facilitate explainable decision support in operations and maintenance (O&M), particularly for automated decision support through recommendation of appropriate maintenance action reports corresponding to failures predicted by CBM techniques. Knowledge graph databases (KGs) model a collection of domain-specific information and have played an intrinsic role for real-world decision support in domains such as healthcare and finance, but have seen very limited attention in the wind industry. We propose XAI4Wind, a multimodal knowledge graph for explainable decision support in real-world operational turbines and demonstrate through experiments several use-cases of the proposed KG towards O&M planning through interactive query and reasoning and providing novel insights using graph data science algorithms. The proposed KG combines multimodal knowledge like SCADA parameters and alarms with natural language maintenance actions, images etc. By integrating our KG with an Explainable AI model for anomaly prediction, we show that it can provide effective human-intelligible O&M strategies for predicted operational inconsistencies in various turbine sub-components. This can help instil better trust and confidence in conventionally black-box AI models. We make our KG publicly available and envisage that it can serve as the building ground for providing autonomous decision support in the wind industry. △ Less","23 February, 2021",https://arxiv.org/pdf/2012.10489
Deep Learning and the Global Workspace Theory,Rufin VanRullen;Ryota Kanai,"Recent advances in deep learning have allowed Artificial Intelligence (AI) to reach near human-level performance in many sensory, perceptual, linguistic or cognitive tasks. There is a growing need, however, for novel, brain-inspired cognitive architectures. The Global Workspace theory refers to a large-scale system integrating and distributing information among networks of specialized modules to create higher-level forms of cognition and awareness. We argue that the time is ripe to consider explicit implementations of this theory using deep learning techniques. We propose a roadmap based on unsupervised neural translation between multiple latent spaces (neural networks trained for distinct tasks, on distinct sensory inputs and/or modalities) to create a unique, amodal global latent workspace (GLW). Potential functional advantages of GLW are reviewed, along with neuroscientific implications. △ Less","19 February, 2021",https://arxiv.org/pdf/2012.10390
"Developing Future Human-Centered Smart Cities: Critical Analysis of Smart City Security, Interpretability, and Ethical Challenges",Kashif Ahmad;Majdi Maabreh;Mohamed Ghaly;Khalil Khan;Junaid Qadir;Ala Al-Fuqaha,"As the globally increasing population drives rapid urbanisation in various parts of the world, there is a great need to deliberate on the future of the cities worth living. In particular, as modern smart cities embrace more and more data-driven artificial intelligence services, it is worth remembering that technology can facilitate prosperity, wellbeing, urban livability, or social justice, but only when it has the right analog complements (such as well-thought out policies, mature institutions, responsible governance); and the ultimate objective of these smart cities is to facilitate and enhance human welfare and social flourishing. Researchers have shown that various technological business models and features can in fact contribute to social problems such as extremism, polarization, misinformation, and Internet addiction. In the light of these observations, addressing the philosophical and ethical questions involved in ensuring the security, safety, and interpretability of such AI algorithms that will form the technological bedrock of future cities assumes paramount importance. Globally there are calls for technology to be made more humane and human-centered. In this paper, we analyze and explore key challenges including security, robustness, interpretability, and ethical (data and algorithmic) challenges to a successful deployment of AI in human-centric applications, with a particular emphasis on the convergence of these concepts/challenges. We provide a detailed review of existing literature on these key challenges and analyze how one of these challenges may lead to others or help in solving other challenges. The paper also advises on the current limitations, pitfalls, and future directions of research in these domains, and how it can fill the current gaps and lead to better solutions. We believe such rigorous analysis will provide a baseline for future research in the domain. △ Less","5 December, 2021",https://arxiv.org/pdf/2012.09110
Physical deep learning based on optimal control of dynamical systems,Genki Furuhata;Tomoaki Niiyama;Satoshi Sunada,"Deep learning is the backbone of artificial intelligence technologies, and it can be regarded as a kind of multilayer feedforward neural network. An essence of deep learning is information propagation through layers. This suggests that there is a connection between deep neural networks and dynamical systems in the sense that information propagation is explicitly modeled by the time-evolution of dynamical systems. In this study, we perform pattern recognition based on the optimal control of continuous-time dynamical systems, which is suitable for physical hardware implementation. The learning is based on the adjoint method to optimally control dynamical systems, and the deep (virtual) network structures based on the time evolution of the systems are used for processing input information. As a key example, we apply the dynamics-based recognition approach to an optoelectronic delay system and demonstrate that the use of the delay system allows for image recognition and nonlinear classifications using only a few control signals. This is in contrast to conventional multilayer neural networks, which require a large number of weight parameters to be trained. The proposed approach provides insight into the mechanisms of deep network processing in the framework of an optimal control problem and presents a pathway for realizing physical computing hardware. △ Less","1 April, 2021",https://arxiv.org/pdf/2012.08761
"Accelerated, Scalable and Reproducible AI-driven Gravitational Wave Detection",E. A. Huerta;Asad Khan;Xiaobo Huang;Minyang Tian;Maksim Levental;Ryan Chard;Wei Wei;Maeve Heflin;Daniel S. Katz;Volodymyr Kindratenko;Dawei Mu;Ben Blaiszik;Ian Foster,"The development of reusable artificial intelligence (AI) models for wider use and rigorous validation by the community promises to unlock new opportunities in multi-messenger astrophysics. Here we develop a workflow that connects the Data and Learning Hub for Science, a repository for publishing AI models, with the Hardware Accelerated Learning (HAL) cluster, using funcX as a universal distributed computing service. Using this workflow, an ensemble of four openly available AI models can be run on HAL to process an entire month's worth (August 2017) of advanced Laser Interferometer Gravitational-Wave Observatory data in just seven minutes, identifying all four all four binary black hole mergers previously identified in this dataset and reporting no misclassifications. This approach combines advances in AI, distributed computing, and scientific data infrastructure to open new pathways to conduct reproducible, accelerated, data-driven discovery. △ Less","9 July, 2021",https://arxiv.org/pdf/2012.08545
"Bayes Meets Entailment and Prediction: Commonsense Reasoning with Non-monotonicity, Paraconsistency and Predictive Accuracy",Hiroyuki Kido;Keishi Okamoto,"The recent success of Bayesian methods in neuroscience and artificial intelligence gives rise to the hypothesis that the brain is a Bayesian machine. Since logic and learning are both practices of the human brain, it leads to another hypothesis that there is a Bayesian interpretation underlying both logical reasoning and machine learning. In this paper, we introduce a generative model of logical consequence relations. It formalises the process of how the truth value of a sentence is probabilistically generated from the probability distribution over states of the world. We show that the generative model characterises a classical consequence relation, paraconsistent consequence relation and nonmonotonic consequence relation. In particular, the generative model gives a new consequence relation that outperforms them in reasoning with inconsistent knowledge. We also show that the generative model gives a new classification algorithm that outperforms several representative algorithms in predictive accuracy and complexity on the Kaggle Titanic dataset. △ Less","27 January, 2021",https://arxiv.org/pdf/2012.08479
Toward a 6G AI-Native Air Interface,Jakob Hoydis;Fayçal Ait Aoudia;Alvaro Valcarce;Harish Viswanathan,"Each generation of cellular communication systems is marked by a defining disruptive technology of its time, such as orthogonal frequency division multiplexing (OFDM) for 4G or Massive multiple-input multiple-output (MIMO) for 5G. Since artificial intelligence (AI) is the defining technology of our time, it is natural to ask what role it could play for 6G. While it is clear that 6G must cater to the needs of large distributed learning systems, it is less certain if AI will play a defining role in the design of 6G itself. The goal of this article is to paint a vision of a new air interface which is partially designed by AI to enable optimized communication schemes for any hardware, radio environment, and application. △ Less","30 April, 2021",https://arxiv.org/pdf/2012.08285
Towards open and expandable cognitive AI architectures for large-scale multi-agent human-robot collaborative learning,Georgios Th. Papadopoulos;Margherita Antona;Constantine Stephanidis,"Learning from Demonstration (LfD) constitutes one of the most robust methodologies for constructing efficient cognitive robotic systems. Despite the large body of research works already reported, current key technological challenges include those of multi-agent learning and long-term autonomy. Towards this direction, a novel cognitive architecture for multi-agent LfD robotic learning is introduced, targeting to enable the reliable deployment of open, scalable and expandable robotic systems in large-scale and complex environments. In particular, the designed architecture capitalizes on the recent advances in the Artificial Intelligence (AI) field, by establishing a Federated Learning (FL)-based framework for incarnating a multi-human multi-robot collaborative learning environment. The fundamental conceptualization relies on employing multiple AI-empowered cognitive processes (implementing various robotic tasks) that operate at the edge nodes of a network of robotic platforms, while global AI models (underpinning the aforementioned robotic tasks) are collectively created and shared among the network, by elegantly combining information from a large number of human-robot interaction instances. Regarding pivotal novelties, the designed cognitive architecture a) introduces a new FL-based formalism that extends the conventional LfD learning paradigm to support large-scale multi-agent operational settings, b) elaborates previous FL-based self-learning robotic schemes so as to incorporate the human in the learning loop and c) consolidates the fundamental principles of FL with additional sophisticated AI-enabled learning methodologies for modelling the multi-level inter-dependencies among the robotic tasks. The applicability of the proposed framework is explained using an example of a real-world industrial case study for agile production-based Critical Raw Materials (CRM) recovery. △ Less","29 March, 2021",https://arxiv.org/pdf/2012.08174
An AI-Assisted Design Method for Topology Optimization Without Pre-Optimized Training Data,Alex Halle;L. Flavio Campanile;Alexander Hasse,"Topology optimization is widely used by engineers during the initial product development process to get a first possible geometry design. The state-of-the-art is the iterative calculation, which requires both time and computational power. Some newly developed methods use artificial intelligence to accelerate the topology optimization. These require conventionally pre-optimized data and therefore are dependent on the quality and number of available data. This paper proposes an AI-assisted design method for topology optimization, which does not require pre-optimized data. The designs are provided by an artificial neural network, the predictor, on the basis of boundary conditions and degree of filling (the volume percentage filled by material) as input data. In the training phase, geometries generated on the basis of random input data are evaluated with respect to given criteria. The results of those evaluations flow into an objective function which is minimized by adapting the predictor's parameters. After the training is completed, the presented AI-assisted design procedure supplies geometries which are similar to the ones generated by conventional topology optimizers, but requires a small fraction of the computational effort required by those algorithms. We anticipate our paper to be a starting point for AI-based methods that requires data, that is hard to compute or not available. △ Less","27 July, 2021",https://arxiv.org/pdf/2012.06384
"The Why, What and How of Artificial General Intelligence Chip Development",Alex James,"The AI chips increasingly focus on implementing neural computing at low power and cost. The intelligent sensing, automation, and edge computing applications have been the market drivers for AI chips. Increasingly, the generalisation, performance, robustness, and scalability of the AI chip solutions are compared with human-like intelligence abilities. Such a requirement to transit from application-specific to general intelligence AI chip must consider several factors. This paper provides an overview of this cross-disciplinary field of study, elaborating on the generalisation of intelligence as understood in building artificial general intelligence (AGI) systems. This work presents a listing of emerging AI chip technologies, classification of edge AI implementations, and the funnel design flow for AGI chip development. Finally, the design consideration required for building an AGI chip is listed along with the methods for testing and validating it. △ Less","30 March, 2021",https://arxiv.org/pdf/2012.06338
Adaptive Submodular Meta-Learning,Shaojie Tang;Jing Yuan,"Meta-Learning has gained increasing attention in the machine learning and artificial intelligence communities. In this paper, we introduce and study an adaptive submodular meta-learning problem. The input of our problem is a set of items, where each item has a random state which is initially unknown. The only way to observe an item's state is to select that item. Our objective is to adaptively select a group of items that achieve the best performance over a set of tasks, where each task is represented as an adaptive submodular function that maps sets of items and their states to a real number. To reduce the computational cost while maintaining a personalized solution for each future task, we first select an initial solution set based on previously observed tasks, then adaptively add the remaining items to the initial solution set when a new task arrives. As compared to the solution where a brand new solution is computed for each new task, our meta-learning based approach leads to lower computational overhead at test time since the initial solution set is pre-computed in the training stage. To solve this problem, we propose a two-phase greedy policy and show that it achieves a 1/2 approximation ratio for the monotone case. For the non-monotone case, we develop a two-phase randomized greedy policy that achieves a 1/32 approximation ratio. △ Less","25 March, 2021",https://arxiv.org/pdf/2012.06070
Imitating Interactive Intelligence,Josh Abramson;Arun Ahuja;Iain Barr;Arthur Brussee;Federico Carnevale;Mary Cassin;Rachita Chhaparia;Stephen Clark;Bogdan Damoc;Andrew Dudzik;Petko Georgiev;Aurelia Guy;Tim Harley;Felix Hill;Alden Hung;Zachary Kenton;Jessica Landon;Timothy Lillicrap;Kory Mathewson;Soňa Mokrá;Alistair Muldal;Adam Santoro;Nikolay Savinov;Vikrant Varma;Greg Wayne,"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount. △ Less","20 January, 2021",https://arxiv.org/pdf/2012.05672
Coalgebraic Semantics for Probabilistic Logic Programming,Tao Gu;Fabio Zanasi,"Probabilistic logic programming is increasingly important in artificial intelligence and related fields as a formalism to reason about uncertainty. It generalises logic programming with the possibility of annotating clauses with probabilities. This paper proposes a coalgebraic semantics on probabilistic logic programming. Programs are modelled as coalgebras for a certain functor F, and two semantics are given in terms of cofree coalgebras. First, the F-coalgebra yields a semantics in terms of derivation trees. Second, by embedding F into another type G, as cofree G-coalgebra we obtain a `possible worlds' interpretation of programs, from which one may recover the usual distribution semantics of probabilistic logic programming. Furthermore, we show that a similar approach can be used to provide a coalgebraic semantics to weighted logic programming. △ Less","9 April, 2021",https://arxiv.org/pdf/2012.03916
AI-enabled Prediction of eSports Player Performance Using the Data from Heterogeneous Sensors,Anton Smerdov;Evgeny Burnaev;Andrey Somov;Anton Stepanov,"The emerging progress of eSports lacks the tools for ensuring high-quality analytics and training in Pro and amateur eSports teams. We report on an Artificial Intelligence (AI) enabled solution for predicting the eSports player in-game performance using exclusively the data from sensors. For this reason, we collected the physiological, environmental, and the game chair data from Pro and amateur players. The player performance is assessed from the game logs in a multiplayer game for each moment of time using a recurrent neural network. We have investigated that attention mechanism improves the generalization of the network and provides the straightforward feature importance as well. The best model achieves ROC AUC score 0.73. The prediction of the performance of particular player is realized although his data are not utilized in the training set. The proposed solution has a number of promising applications for Pro eSports teams and amateur players, such as a learning tool or a performance monitoring system. △ Less","24 August, 2021",https://arxiv.org/pdf/2012.03491
Over a Decade of Social Opinion Mining: A Systematic Review,Keith Cortis;Brian Davis,"Social media popularity and importance is on the increase due to people using it for various types of social interaction across multiple channels. This systematic review focuses on the evolving research area of Social Opinion Mining, tasked with the identification of multiple opinion dimensions, such as subjectivity, sentiment polarity, emotion, affect, sarcasm and irony, from user-generated content represented across multiple social media platforms and in various media formats, like text, image, video and audio. Through Social Opinion Mining, natural language can be understood in terms of the different opinion dimensions, as expressed by humans. This contributes towards the evolution of Artificial Intelligence which in turn helps the advancement of several real-world use cases, such as customer service and decision making. A thorough systematic review was carried out on Social Opinion Mining research which totals 485 published studies and spans a period of twelve years between 2007 and 2018. The in-depth analysis focuses on the social media platforms, techniques, social datasets, language, modality, tools and technologies, and other aspects derived. Social Opinion Mining can be utilised in many application areas, ranging from marketing, advertising and sales for product/service management, and in multiple domains and industries, such as politics, technology, finance, healthcare, sports and government. The latest developments in Social Opinion Mining beyond 2018 are also presented together with future research directions, with the aim of leaving a wider academic and societal impact in several real-world applications. △ Less","6 July, 2021",https://arxiv.org/pdf/2012.03091
Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy,Kit T. Rodolfa;Hemank Lamba;Rayid Ghani,"Growing use of machine learning in policy and social impact settings have raised concerns for fairness implications, especially for racial minorities. These concerns have generated considerable interest among machine learning and artificial intelligence researchers, who have developed new methods and established theoretical bounds for improving fairness, focusing on the source data, regularization and model training, or post-hoc adjustments to model scores. However, little work has studied the practical trade-offs between fairness and accuracy in real-world settings to understand how these bounds and methods translate into policy choices and impact on society. Our empirical study fills this gap by investigating the impact of mitigating disparities on accuracy, focusing on the common context of using machine learning to inform benefit allocation in resource-constrained programs across education, mental health, criminal justice, and housing safety. Here we describe applied work in which we find fairness-accuracy trade-offs to be negligible in practice. In each setting studied, explicitly focusing on achieving equity and using our proposed post-hoc disparity mitigation methods, fairness was substantially improved without sacrificing accuracy. This observation was robust across policy contexts studied, scale of resources available for intervention, time, and relative size of the protected groups. These empirical results challenge a commonly held assumption that reducing disparities either requires accepting an appreciable drop in accuracy or the development of novel, complex methods, making reducing disparities in these applications more practical. △ Less","3 September, 2021",https://arxiv.org/pdf/2012.02972
A Survey on Deep Learning for Human Mobility,Massimiliano Luca;Gianni Barlacchi;Bruno Lepri;Luca Pappalardo,"The study of human mobility is crucial due to its impact on several aspects of our society, such as disease spreading, urban planning, well-being, pollution, and more. The proliferation of digital mobility data, such as phone records, GPS traces, and social media posts, combined with the predictive power of artificial intelligence, triggered the application of deep learning to human mobility. Existing surveys focus on single tasks, data sources, mechanistic or traditional machine learning approaches, while a comprehensive description of deep learning solutions is missing. This survey provides a taxonomy of mobility tasks, a discussion on the challenges related to each task and how deep learning may overcome the limitations of traditional models, a description of the most relevant solutions to the mobility tasks described above and the relevant challenges for the future. Our survey is a guide to the leading deep learning solutions to next-location prediction, crowd flow prediction, trajectory generation, and flow generation. At the same time, it helps deep learning scientists and practitioners understand the fundamental concepts and the open challenges of the study of human mobility. △ Less","27 June, 2021",https://arxiv.org/pdf/2012.02825
Critical Evaluation of Deep Neural Networks for Wrist Fracture Detection,Abu Mohammed Raisuddin;Elias Vaattovaara;Mika Nevalainen;Marko Nikki;Elina Järvenpää;Kaisa Makkonen;Pekka Pinola;Tuula Palsio;Arttu Niemensivu;Osmo Tervonen;Aleksei Tiulpin,"Wrist Fracture is the most common type of fracture with a high incidence rate. Conventional radiography (i.e. X-ray imaging) is used for wrist fracture detection routinely, but occasionally fracture delineation poses issues and an additional confirmation by computed tomography (CT) is needed for diagnosis. Recent advances in the field of Deep Learning (DL), a subfield of Artificial Intelligence (AI), have shown that wrist fracture detection can be automated using Convolutional Neural Networks. However, previous studies did not pay close attention to the difficult cases which can only be confirmed via CT imaging. In this study, we have developed and analyzed a state-of-the-art DL-based pipeline for wrist (distal radius) fracture detection -- DeepWrist, and evaluated it against one general population test set, and one challenging test set comprising only cases requiring confirmation by CT. Our results reveal that a typical state-of-the-art approach, such as DeepWrist, while having a near-perfect performance on the general independent test set, has a substantially lower performance on the challenging test set -- average precision of 0.99 (0.99-0.99) vs 0.64 (0.46-0.83), respectively. Similarly, the area under the ROC curve was of 0.99 (0.98-0.99) vs 0.84 (0.72-0.93), respectively. Our findings highlight the importance of a meticulous analysis of DL-based models before clinical use, and unearth the need for more challenging settings for testing medical AI systems. △ Less","5 March, 2021",https://arxiv.org/pdf/2012.02577
SSGD: A safe and efficient method of gradient descent,Jinhuan Duan;Xianxian Li;Shiqi Gao;Jinyan Wang;Zili Zhong,"With the vigorous development of artificial intelligence technology, various engineering technology applications have been implemented one after another. The gradient descent method plays an important role in solving various optimization problems, due to its simple structure, good stability and easy implementation. In multi-node machine learning system, the gradients usually need to be shared. Shared gradients are generally unsafe. Attackers can obtain training data simply by knowing the gradient information. In this paper, to prevent gradient leakage while keeping the accuracy of model, we propose the super stochastic gradient descent approach to update parameters by concealing the modulus length of gradient vectors and converting it or them into a unit vector. Furthermore, we analyze the security of super stochastic gradient descent approach. Our algorithm can defend against attacks on the gradient. Experiment results show that our approach is obviously superior to prevalent gradient descent approaches in terms of accuracy, robustness, and adaptability to large-scale batches. △ Less","26 April, 2021",https://arxiv.org/pdf/2012.02076
From One to All: Learning to Match Heterogeneous and Partially Overlapped Graphs,Weijie Liu;Hui Qian;Chao Zhang;Jiahao Xie;Zebang Shen;Nenggan Zheng,"Recent years have witnessed a flurry of research activity in graph matching, which aims at finding the correspondence of nodes across two graphs and lies at the heart of many artificial intelligence applications. However, matching heterogeneous graphs with partial overlap remains a challenging problem in real-world applications. This paper proposes the first practical learning-to-match method to meet this challenge. The proposed unsupervised method adopts a novel partial OT paradigm to learn a transport plan and node embeddings simultaneously. In a from-one-to-all manner, the entire learning procedure is decomposed into a series of easy-to-solve sub-procedures, each of which only handles the alignment of a single type of nodes. A mechanism for searching the transport mass is also proposed. Experimental results demonstrate that the proposed method outperforms state-of-the-art graph matching methods. △ Less","19 December, 2021",https://arxiv.org/pdf/2012.01252
Reviewing the Need for Explainable Artificial Intelligence (xAI),Julie Gerlings;Arisa Shollo;Ioanna Constantiou,"The diffusion of artificial intelligence (AI) applications in organizations and society has fueled research on explaining AI decisions. The explainable AI (xAI) field is rapidly expanding with numerous ways of extracting information and visualizing the output of AI technologies (e.g. deep neural networks). Yet, we have a limited understanding of how xAI research addresses the need for explainable AI. We conduct a systematic review of xAI literature on the topic and identify four thematic debates central to how xAI addresses the black-box problem. Based on this critical analysis of the xAI scholarship we synthesize the findings into a future research agenda to further the xAI body of knowledge. △ Less","26 January, 2021",https://arxiv.org/pdf/2012.01007
A Data-Driven Study of Commonsense Knowledge using the ConceptNet Knowledge Base,Ke Shen;Mayank Kejriwal,"Acquiring commonsense knowledge and reasoning is recognized as an important frontier in achieving general Artificial Intelligence (AI). Recent research in the Natural Language Processing (NLP) community has demonstrated significant progress in this problem setting. Despite this progress, which is mainly on multiple-choice question answering tasks in limited settings, there is still a lack of understanding (especially at scale) of the nature of commonsense knowledge itself. In this paper, we propose and conduct a systematic study to enable a deeper understanding of commonsense knowledge by doing an empirical and structural analysis of the ConceptNet knowledge base. ConceptNet is a freely available knowledge base containing millions of commonsense assertions presented in natural language. Detailed experimental results on three carefully designed research questions, using state-of-the-art unsupervised graph representation learning ('embedding') and clustering techniques, reveal deep substructures in ConceptNet relations, allowing us to make data-driven and computational claims about the meaning of phenomena such as 'context' that are traditionally discussed only in qualitative terms. Furthermore, our methodology provides a case study in how to use data-science and computational methodologies for understanding the nature of an everyday (yet complex) psychological phenomenon that is an essential feature of human intelligence. △ Less","19 January, 2021",https://arxiv.org/pdf/2011.14084
Rethinking Generalization in American Sign Language Prediction for Edge Devices with Extremely Low Memory Footprint,Aditya Jyoti Paul;Puranjay Mohan;Stuti Sehgal,"Due to the boom in technical compute in the last few years, the world has seen massive advances in artificially intelligent systems solving diverse real-world problems. But a major roadblock in the ubiquitous acceptance of these models is their enormous computational complexity and memory footprint. Hence efficient architectures and training techniques are required for deployment on extremely low resource inference endpoints. This paper proposes an architecture for detection of alphabets in American Sign Language on an ARM Cortex-M7 microcontroller having just 496 KB of framebuffer RAM. Leveraging parameter quantization is a common technique that might cause varying drops in test accuracy. This paper proposes using interpolation as augmentation amongst other techniques as an efficient method of reducing this drop, which also helps the model generalize well to previously unseen noisy data. The proposed model is about 185 KB post-quantization and inference speed is 20 frames per second. △ Less","13 February, 2021",https://arxiv.org/pdf/2011.13741
TStarBot-X: An Open-Sourced and Comprehensive Study for Efficient League Training in StarCraft II Full Game,Lei Han;Jiechao Xiong;Peng Sun;Xinghai Sun;Meng Fang;Qingwei Guo;Qiaobo Chen;Tengfei Shi;Hongsheng Yu;Xipeng Wu;Zhengyou Zhang,"StarCraft, one of the most difficult esport games with long-standing history of professional tournaments, has attracted generations of players and fans, and also, intense attentions in artificial intelligence research. Recently, Google's DeepMind announced AlphaStar, a grandmaster level AI in StarCraft II that can play with humans using comparable action space and operations. In this paper, we introduce a new AI agent, named TStarBot-X, that is trained under orders of less computations and can play competitively with expert human players. TStarBot-X takes advantage of important techniques introduced in AlphaStar, and also benefits from substantial innovations including new league training methods, novel multi-agent roles, rule-guided policy search, stabilized policy improvement, lightweight neural network architecture, and importance sampling in imitation learning, etc. We show that with orders of less computation scale, a faithful reimplementation of AlphaStar's methods can not succeed and the proposed techniques are necessary to ensure TStarBot-X's competitive performance. We reveal all technical details that are complementary to those mentioned in AlphaStar, showing the most sensitive parts in league training, reinforcement learning and imitation learning that affect the performance of the agents. Most importantly, this is an open-sourced study that all codes and resources (including the trained model parameters) are publicly accessible via \url{https://github.com/tencent-ailab/tleague_projpage}. We expect this study could be beneficial for both academic and industrial future research in solving complex problems like StarCraft, and also, might provide a sparring partner for all StarCraft II players and other AI agents. △ Less","30 April, 2021",https://arxiv.org/pdf/2011.13729
The NEOLIX Open Dataset for Autonomous Driving,Lichao Wang;Lanxin Lei;Hongli Song;Weibao Wang,"With the gradual maturity of 5G technology,autonomous driving technology has attracted moreand more attention among the research commu-nity. Autonomous driving vehicles rely on the co-operation of artificial intelligence, visual comput-ing, radar, monitoring equipment and GPS, whichenables computers to operate motor vehicles auto-matically and safely without human interference.However, the large-scale dataset for training andsystem evaluation is still a hot potato in the devel-opment of robust perception models. In this paper,we present the NEOLIX dataset and its applica-tions in the autonomous driving area. Our datasetincludes about 30,000 frames with point cloud la-bels, and more than 600k 3D bounding boxes withannotations. The data collection covers multipleregions, and various driving conditions, includingday, night, dawn, dusk and sunny day. In orderto label this complete dataset, we developed vari-ous tools and algorithms specified for each task tospeed up the labelling process. It is expected thatour dataset and related algorithms can support andmotivate researchers for the further developmentof autonomous driving in the field of computer vi-sion. △ Less","28 January, 2021",https://arxiv.org/pdf/2011.13528
True-data Testbed for 5G/B5G Intelligent Network,Yongming Huang;Shengheng Liu;Cheng Zhang;Xiaohu You;Hequan Wu,"Future beyond fifth-generation (B5G) and sixth-generation (6G) mobile communications will shift from facilitating interpersonal communications to supporting Internet of Everything (IoE), where intelligent communications with full integration of big data and artificial intelligence (AI) will play an important role in improving network efficiency and providing high-quality service. As a rapid evolving paradigm, the AI-empowered mobile communications demand large amounts of data acquired from real network environment for systematic test and verification. Hence, we build the world's first true-data testbed for 5G/B5G intelligent network (TTIN), which comprises 5G/B5G on-site experimental networks, data acquisition & data warehouse, and AI engine & network optimization. In the TTIN, true network data acquisition, storage, standardization, and analysis are available, which enable system-level online verification of B5G/6G-orientated key technologies and support data-driven network optimization through the closed-loop control mechanism. This paper elaborates on the system architecture and module design of TTIN. Detailed technical specifications and some of the established use cases are also showcased. △ Less","4 January, 2021",https://arxiv.org/pdf/2011.13152
A Panoramic Survey of Natural Language Processing in the Arab World,Kareem Darwish;Nizar Habash;Mourad Abbas;Hend Al-Khalifa;Huseein T. Al-Natsheh;Samhaa R. El-Beltagy;Houda Bouamor;Karim Bouzoubaa;Violetta Cavalli-Sforza;Wassim El-Hajj;Mustafa Jarrar;Hamdy Mubarak,"The term natural language refers to any system of symbolic communication (spoken, signed or written) without intentional human planning and design. This distinguishes natural languages such as Arabic and Japanese from artificially constructed languages such as Esperanto or Python. Natural language processing (NLP) is the sub-field of artificial intelligence (AI) focused on modeling natural languages to build applications such as speech recognition and synthesis, machine translation, optical character recognition (OCR), sentiment analysis (SA), question answering, dialogue systems, etc. NLP is a highly interdisciplinary field with connections to computer science, linguistics, cognitive science, psychology, mathematics and others. Some of the earliest AI applications were in NLP (e.g., machine translation); and the last decade (2010-2020) in particular has witnessed an incredible increase in quality, matched with a rise in public awareness, use, and expectations of what may have seemed like science fiction in the past. NLP researchers pride themselves on developing language independent models and tools that can be applied to all human languages, e.g. machine translation systems can be built for a variety of languages using the same basic mechanisms and models. However, the reality is that some languages do get more attention (e.g., English and Chinese) than others (e.g., Hindi and Swahili). Arabic, the primary language of the Arab world and the religious language of millions of non-Arab Muslims is somewhere in the middle of this continuum. Though Arabic NLP has many challenges, it has seen many successes and developments. Next we discuss Arabic's main challenges as a necessary background, and we present a brief history of Arabic NLP. We then survey a number of its research areas, and close with a critical discussion of the future of Arabic NLP. △ Less","27 September, 2021",https://arxiv.org/pdf/2011.12631
Skill-driven Recommendations for Job Transition Pathways,Nikolas Dawson;Mary-Anne Williams;Marian-Andrei Rizoiu,"Job security can never be taken for granted, especially in times of rapid, widespread and unexpected social and economic change. These changes can force workers to transition to new jobs. This may be because new technologies emerge or production is moved abroad. Perhaps it is a global crisis, such as COVID-19, which shutters industries and displaces labor en masse. Regardless of the impetus, people are faced with the challenge of moving between jobs to find new work. Successful transitions typically occur when workers leverage their existing skills in the new occupation. Here, we propose a novel method to measure the similarity between occupations using their underlying skills. We then build a recommender system for identifying optimal transition pathways between occupations using job advertisements (ads) data and a longitudinal household survey. Our results show that not only can we accurately predict occupational transitions (Accuracy = 76%), but we account for the asymmetric difficulties of moving between jobs (it is easier to move in one direction than the other). We also build an early warning indicator for new technology adoption (showcasing Artificial Intelligence), a major driver of rising job transitions. By using real-time data, our systems can respond to labor demand shifts as they occur (such as those caused by COVID-19). They can be leveraged by policy-makers, educators, and job seekers who are forced to confront the often distressing challenges of finding new jobs. △ Less","10 August, 2021",https://arxiv.org/pdf/2011.11801
Accurate and Rapid Diagnosis of COVID-19 Pneumonia with Batch Effect Removal of Chest CT-Scans and Interpretable Artificial Intelligence,Rassa Ghavami Modegh;Mehrab Hamidi;Saeed Masoudian;Amir Mohseni;Hamzeh Lotfalinezhad;Mohammad Ali Kazemi;Behnaz Moradi;Mahyar Ghafoori;Omid Motamedi;Omid Pournik;Kiara Rezaei-Kalantari;Amirreza Manteghinezhad;Shaghayegh Haghjooy Javanmard;Fateme Abdoli Nezhad;Ahmad Enhesari;Mohammad Saeed Kheyrkhah;Razieh Eghtesadi;Javid Azadbakht;Akbar Aliasgharzadeh;Mohammad Reza Sharif;Ali Khaleghi;Abbas Foroutan;Hossein Ghanaati;Hamed Dashti;Hamid R. Rabiee,"COVID-19 is a virus with high transmission rate that demands rapid identification of the infected patients to reduce the spread of the disease. The current gold-standard test, Reverse-Transcription Polymerase Chain Reaction (RT-PCR), has a high rate of false negatives. Diagnosing from CT-scan images as a more accurate alternative has the challenge of distinguishing COVID-19 from other pneumonia diseases. Artificial intelligence can help radiologists and physicians to accelerate the process of diagnosis, increase its accuracy, and measure the severity of the disease. We designed a new interpretable deep neural network to distinguish healthy people, patients with COVID-19, and patients with other pneumonia diseases from axial lung CT-scan images. Our model also detects the infected areas and calculates the percentage of the infected lung volume. We first preprocessed the images to eliminate the batch effects of different devices, and then adopted a weakly supervised method to train the model without having any tags for the infected parts. We trained and evaluated the model on a large dataset of 3359 samples from 6 different medical centers. The model reached sensitivities of 97.75% and 98.15%, and specificities of 87% and 81.03% in separating healthy people from the diseased and COVID-19 from other diseases, respectively. It also demonstrated similar performance for 1435 samples from 6 different medical centers which proves its generalizability. The performance of the model on a large diverse dataset, its generalizability, and interpretability makes it suitable to be used as a reliable diagnostic system. △ Less","8 January, 2021",https://arxiv.org/pdf/2011.11736
Dynamic Hard Pruning of Neural Networks at the Edge of the Internet,Lorenzo Valerio;Franco Maria Nardini;Andrea Passarella;Raffaele Perego,"Neural Networks (NN), although successfully applied to several Artificial Intelligence tasks, are often unnecessarily over-parametrised. In edge/fog computing, this might make their training prohibitive on resource-constrained devices, contrasting with the current trend of decentralising intelligence from remote data centres to local constrained devices. Therefore, we investigate the problem of training effective NN models on constrained devices having a fixed, potentially small, memory budget. We target techniques that are both resource-efficient and performance effective while enabling significant network compression. Our Dynamic Hard Pruning (DynHP) technique incrementally prunes the network during training, identifying neurons that marginally contribute to the model accuracy. DynHP enables a tunable size reduction of the final neural network and reduces the NN memory occupancy during training. Freed memory is reused by a \emph{dynamic batch sizing} approach to counterbalance the accuracy degradation caused by the hard pruning strategy, improving its convergence and effectiveness. We assess the performance of DynHP through reproducible experiments on three public datasets, comparing them against reference competitors. Results show that DynHP compresses a NN up to
times without significant performance drops (up to
additional error w.r.t. the competitors), reducing up to
the training memory occupancy. △ Less","22 October, 2021",https://arxiv.org/pdf/2011.08545
"Deep Learning -- A first Meta-Survey of selected Reviews across Scientific Disciplines, their Commonalities, Challenges and Research Impact",Jan Egger;Antonio Pepe;Christina Gsaxner;Yuan Jin;Jianning Li;Roman Kern,"Deep learning belongs to the field of artificial intelligence, where machines perform tasks that typically require some kind of human intelligence. Similar to the basic structure of a brain, a deep learning algorithm consists of an artificial neural network, which resembles the biological brain structure. Mimicking the learning process of humans with their senses, deep learning networks are fed with (sensory) data, like texts, images, videos or sounds. These networks outperform the state-of-the-art methods in different tasks and, because of this, the whole field saw an exponential growth during the last years. This growth resulted in way over 10,000 publications per year in the last years. For example, the search engine PubMed alone, which covers only a sub-set of all publications in the medical field, provides already over 11,000 results in Q3 2020 for the search term 'deep learning', and around 90% of these results are from the last three years. Consequently, a complete overview over the field of deep learning is already impossible to obtain and, in the near future, it will potentially become difficult to obtain an overview over a subfield. However, there are several review articles about deep learning, which are focused on specific scientific fields or applications, for example deep learning advances in computer vision or in specific tasks like object detection. With these surveys as a foundation, the aim of this contribution is to provide a first high-level, categorized meta-survey of selected reviews on deep learning across different scientific disciplines. The categories (computer vision, language processing, medical informatics and additional works) have been chosen according to the underlying data sources (image, language, medical, mixed). In addition, we review the common architectures, methods, pros, cons, evaluations, challenges and future directions for every sub-category. △ Less","17 November, 2021",https://arxiv.org/pdf/2011.08184
Deep-LIBRA: Artificial intelligence method for robust quantification of breast density with independent validation in breast cancer risk assessment,Omid Haji Maghsoudi;Aimilia Gastounioti;Christopher Scott;Lauren Pantalone;Fang-Fang Wu;Eric A. Cohen;Stacey Winham;Emily F. Conant;Celine Vachon;Despina Kontos,"Breast density is an important risk factor for breast cancer that also affects the specificity and sensitivity of screening mammography. Current federal legislation mandates reporting of breast density for all women undergoing breast screening. Clinically, breast density is assessed visually using the American College of Radiology Breast Imaging Reporting And Data System (BI-RADS) scale. Here, we introduce an artificial intelligence (AI) method to estimate breast percentage density (PD) from digital mammograms. Our method leverages deep learning (DL) using two convolutional neural network architectures to accurately segment the breast area. A machine-learning algorithm combining superpixel generation, texture feature analysis, and support vector machine is then applied to differentiate dense from non-dense tissue regions, from which PD is estimated. Our method has been trained and validated on a multi-ethnic, multi-institutional dataset of 15,661 images (4,437 women), and then tested on an independent dataset of 6,368 digital mammograms (1,702 women; cases=414) for both PD estimation and discrimination of breast cancer. On the independent dataset, PD estimates from Deep-LIBRA and an expert reader were strongly correlated (Spearman correlation coefficient = 0.90). Moreover, Deep-LIBRA yielded a higher breast cancer discrimination performance (area under the ROC curve, AUC = 0.611 [95% confidence interval (CI): 0.583, 0.639]) compared to four other widely-used research and commercial PD assessment methods (AUCs = 0.528 to 0.588). Our results suggest a strong agreement of PD estimates between Deep-LIBRA and gold-standard assessment by an expert reader, as well as improved performance in breast cancer risk assessment over state-of-the-art open-source and commercial methods. △ Less","18 October, 2021",https://arxiv.org/pdf/2011.08001
NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases,Tara Safavi;Jing Zhu;Danai Koutra,"Codifying commonsense knowledge in machines is a longstanding goal of artificial intelligence. Recently, much progress toward this goal has been made with automatic knowledge base (KB) construction techniques. However, such techniques focus primarily on the acquisition of positive (true) KB statements, even though negative (false) statements are often also important for discriminative reasoning over commonsense KBs. As a first step toward the latter, this paper proposes NegatER, a framework that ranks potential negatives in commonsense KBs using a contextual language model (LM). Importantly, as most KBs do not contain negatives, NegatER relies only on the positive knowledge in the LM and does not require ground-truth negative examples. Experiments demonstrate that, compared to multiple contrastive data augmentation approaches, NegatER yields negatives that are more grammatical, coherent, and informative -- leading to statistically significant accuracy improvements in a challenging KB completion task and confirming that the positive knowledge in LMs can be ""re-purposed"" to generate negative knowledge. △ Less","9 September, 2021",https://arxiv.org/pdf/2011.07497
Optimizing AI Service Placement and Resource Allocation in Mobile Edge Intelligence Systems,Zehong Lin;Suzhi Bi;Ying-Jun Angela Zhang,"Leveraging recent advances on mobile edge computing (MEC), edge intelligence has emerged as a promising paradigm to support mobile artificial intelligence (AI) applications at the network edge. In this paper, we consider the AI service placement problem in a multi-user MEC system, where the access point (AP) places the most up-to-date AI program at user devices to enable local computing/task execution at the user side. To fully utilize the stringent wireless spectrum and edge computing resources, the AP sends the AI service program to a user only when enabling local computing at the user yields a better system performance. We formulate a mixed-integer non-linear programming (MINLP) problem to minimize the total computation time and energy consumption of all users by jointly optimizing the service placement (i.e., which users to receive the program) and resource allocation (on local CPU frequencies, uplink bandwidth, and edge CPU frequency). To tackle the MINLP problem, we derive analytical expressions to calculate the optimal resource allocation decisions with low complexity. This allows us to efficiently obtain the optimal service placement solution by search-based algorithms such as meta-heuristic or greedy search algorithms. To enhance the algorithm scalability in large-sized networks, we further propose an ADMM (alternating direction method of multipliers) based method to decompose the optimization problem into parallel tractable MINLP subproblems. The ADMM method eliminates the need of searching in a high-dimensional space for service placement decisions and thus has a low computational complexity that grows linearly with the number of users. Simulation results show that the proposed algorithms perform extremely close to the optimum and significantly outperform the other representative benchmark algorithms. △ Less","24 July, 2021",https://arxiv.org/pdf/2011.05708
Collective Knowledge: organizing research projects as a database of reusable components and portable workflows with common APIs,Grigori Fursin,"This article provides the motivation and overview of the Collective Knowledge framework (CK or cKnowledge). The CK concept is to decompose research projects into reusable components that encapsulate research artifacts and provide unified application programming interfaces (APIs), command-line interfaces (CLIs), meta descriptions and common automation actions for related artifacts. The CK framework is used to organize and manage research projects as a database of such components. Inspired by the USB ""plug and play"" approach for hardware, CK also helps to assemble portable workflows that can automatically plug in compatible components from different users and vendors (models, datasets, frameworks, compilers, tools). Such workflows can build and run algorithms on different platforms and environments in a unified way using the universal CK program pipeline with software detection plugins and the automatic installation of missing packages. This article presents a number of industrial projects in which the modular CK approach was successfully validated in order to automate benchmarking, auto-tuning and co-design of efficient software and hardware for machine learning (ML) and artificial intelligence (AI) in terms of speed, accuracy, energy, size and various costs. The CK framework also helped to automate the artifact evaluation process at several computer science conferences as well as to make it easier to reproduce, compare and reuse research techniques from published papers, deploy them in production, and automatically adapt them to continuously changing datasets, models and systems. The long-term goal is to accelerate innovation by connecting researchers and practitioners to share and reuse all their knowledge, best practices, artifacts, workflows and experimental results in a common, portable and reproducible format at https://cKnowledge.io . △ Less","30 January, 2021",https://arxiv.org/pdf/2011.01149
Causal Campbell-Goodhart's law and Reinforcement Learning,Hal Ashton,"Campbell-Goodhart's law relates to the causal inference error whereby decision-making agents aim to influence variables which are correlated to their goal objective but do not reliably cause it. This is a well known error in Economics and Political Science but not widely labelled in Artificial Intelligence research. Through a simple example, we show how off-the-shelf deep Reinforcement Learning (RL) algorithms are not necessarily immune to this cognitive error. The off-policy learning method is tricked, whilst the on-policy method is not. The practical implication is that naive application of RL to complex real life problems can result in the same types of policy errors that humans make. Great care should be taken around understanding the causal model that underpins a solution derived from Reinforcement Learning. △ Less","18 February, 2021",https://arxiv.org/pdf/2011.01010
A comparison of Monte Carlo dropout and bootstrap aggregation on the performance and uncertainty estimation in radiation therapy dose prediction with deep learning neural networks,Dan Nguyen;Azar Sadeghnejad Barkousaraie;Gyanendra Bohara;Anjali Balagopal;Rafe McBeth;Mu-Han Lin;Steve Jiang,"Recently, artificial intelligence technologies and algorithms have become a major focus for advancements in treatment planning for radiation therapy. As these are starting to become incorporated into the clinical workflow, a major concern from clinicians is not whether the model is accurate, but whether the model can express to a human operator when it does not know if its answer is correct. We propose to use Monte Carlo dropout (MCDO) and the bootstrap aggregation (bagging) technique on deep learning models to produce uncertainty estimations for radiation therapy dose prediction. We show that both models are capable of generating a reasonable uncertainty map, and, with our proposed scaling technique, creating interpretable uncertainties and bounds on the prediction and any relevant metrics. Performance-wise, bagging provides statistically significant reduced loss value and errors in most of the metrics investigated in this study. The addition of bagging was able to further reduce errors by another 0.34% for Dmean and 0.19% for Dmax, on average, when compared to the baseline framework. Overall, the bagging framework provided significantly lower MAE of 2.62, as opposed to the baseline framework's MAE of 2.87. The usefulness of bagging, from solely a performance standpoint, does highly depend on the problem and the acceptable predictive error, and its high upfront computational cost during training should be factored in to deciding whether it is advantageous to use it. In terms of deployment with uncertainty estimations turned on, both frameworks offer the same performance time of about 12 seconds. As an ensemble-based metaheuristic, bagging can be used with existing machine learning architectures to improve stability and performance, and MCDO can be applied to any deep learning models that have dropout as part of their architecture. △ Less","11 January, 2021",https://arxiv.org/pdf/2011.00388
Artificial Intelligence (AI) in Action: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP),Qingyu Chen;Robert Leaman;Alexis Allot;Ling Luo;Chih-Hsuan Wei;Shankai Yan;Zhiyong Lu,"The COVID-19 pandemic has had a significant impact on society, both because of the serious health effects of COVID-19 and because of public health measures implemented to slow its spread. Many of these difficulties are fundamentally information needs; attempts to address these needs have caused an information overload for both researchers and the public. Natural language processing (NLP), the branch of artificial intelligence that interprets human language, can be applied to address many of the information needs made urgent by the COVID-19 pandemic. This review surveys approximately 150 NLP studies and more than 50 systems and datasets addressing the COVID-19 pandemic. We detail work on four core NLP tasks: information retrieval, named entity recognition, literature-based discovery, and question answering. We also describe work that directly addresses aspects of the pandemic through four additional tasks: topic modeling, sentiment and emotion analysis, caseload forecasting, and misinformation detection. We conclude by discussing observable trends and remaining challenges. △ Less","5 September, 2021",https://arxiv.org/pdf/2010.16413
PIINET: A 360-degree Panoramic Image Inpainting Network Using a Cube Map,Seo Woo Han;Doug Young Suh,"Inpainting has been continuously studied in the field of computer vision. As artificial intelligence technology developed, deep learning technology was introduced in inpainting research, helping to improve performance. Currently, the input target of an inpainting algorithm using deep learning has been studied from a single image to a video. However, deep learning-based inpainting technology for panoramic images has not been actively studied. We propose a 360-degree panoramic image inpainting method using generative adversarial networks (GANs). The proposed network inputs a 360-degree equirectangular format panoramic image converts it into a cube map format, which has relatively little distortion and uses it as a training network. Since the cube map format is used, the correlation of the six sides of the cube map should be considered. Therefore, all faces of the cube map are used as input for the whole discriminative network, and each face of the cube map is used as input for the slice discriminative network to determine the authenticity of the generated image. The proposed network performed qualitatively better than existing single-image inpainting algorithms and baseline algorithms. △ Less","26 January, 2021",https://arxiv.org/pdf/2010.16003
Federated Transfer Learning: concept and applications,Sudipan Saha;Tahir Ahmad,"Development of Artificial Intelligence (AI) is inherently tied to the development of data. However, in most industries data exists in form of isolated islands, with limited scope of sharing between different organizations. This is an hindrance to the further development of AI. Federated learning has emerged as a possible solution to this problem in the last few years without compromising user privacy. Among different variants of the federated learning, noteworthy is federated transfer learning (FTL) that allows knowledge to be transferred across domains that do not have many overlapping features and users. In this work we provide a comprehensive survey of the existing works on this topic. In more details, we study the background of FTL and its different existing applications. We further analyze FTL from privacy and machine learning perspective. △ Less","6 March, 2021",https://arxiv.org/pdf/2010.15561
Explainable Artificial Intelligence for Manufacturing Cost Estimation and Machining Feature Visualization,Soyoung Yoo;Namwoo Kang,"Studies on manufacturing cost prediction based on deep learning have begun in recent years, but the cost prediction rationale cannot be explained because the models are still used as a black box. This study aims to propose a manufacturing cost prediction process for 3D computer-aided design (CAD) models using explainable artificial intelligence. The proposed process can visualize the machining features of the 3D CAD model that are influencing the increase in manufacturing costs. The proposed process consists of (1) data collection and pre-processing, (2) 3D deep learning architecture exploration, and (3) visualization to explain the prediction results. The proposed deep learning model shows high predictability of manufacturing cost for the computer numerical control (CNC) machined parts. In particular, using 3D gradient-weighted class activation mapping proves that the proposed model not only can detect the CNC machining features but also can differentiate the machining difficulty for the same feature. Using the proposed process, we can provide a design guidance to engineering designers in reducing manufacturing costs during the conceptual design phase. We can also provide real-time quotations and redesign proposals to online manufacturing platform customers. △ Less","13 June, 2021",https://arxiv.org/pdf/2010.14824
Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure,Ben Hutchinson;Andrew Smart;Alex Hanna;Emily Denton;Christina Greer;Oddur Kjartansson;Parker Barnes;Margaret Mitchell,"Rising concern for the societal implications of artificial intelligence systems has inspired demands for greater transparency and accountability. However the datasets which empower machine learning are often used, shared and re-used with little visibility into the processes of deliberation which led to their creation. Which stakeholder groups had their perspectives included when the dataset was conceived? Which domain experts were consulted regarding how to model subgroups and other phenomena? How were questions of representational biases measured and addressed? Who labeled the data? In this paper, we introduce a rigorous framework for dataset development transparency which supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields a set of documents that facilitate improved communication and decision-making, as well as drawing attention the value and necessity of careful data work. The proposed framework is intended to contribute to closing the accountability gap in artificial intelligence systems, by making visible the often overlooked work that goes into dataset creation. △ Less","29 January, 2021",https://arxiv.org/pdf/2010.13561
Biases in Generative Art -- A Causal Look from the Lens of Art History,Ramya Srinivasan;Kanji Uchino,"With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art. △ Less","16 February, 2021",https://arxiv.org/pdf/2010.13266
New Approaches for Natural Language Understanding based on the Idea that Natural Language encodes both Information and its Processing Procedures,Limin Zhang,"We must recognize that natural language is a way of information encoding, and it encodes not only the information but also the procedures for how information is processed. To understand natural language, the same as we conceive and design computer languages, the first step is to separate information (or data) and the processing procedures of information (or data). In natural language, some processing procedures of data are encoded directly as the structure chunk and the pointer chunk (this paper has reclassified lexical chunks as the data chunk, structure chunk, and the pointer chunk); some processing procedures of data imply in sentences structures; some requests of processing procedures are expressed by information senders and processed by information receivers. For the data parts, the classification encoding system of attribute information and the information organization architecture (including constitutional structures of information sets and the hierarchy between the information sets) were discussed. In section 2, the theoretical part elaborated in section 2 has been verified in examples and proofed that the studies in this paper have achieved the goal of enabling machines to understand the information conveyed in the dialogue. In section 4, the author summarizes the basic conditions of ""Understanding"", rethinks what ""Understanding"" is and how to proceed. The study in this paper provides a practical, theoretical basis and research methods for NLU. It also can be applied in large-scale and multi-type information processing in the artificial intelligence (AI) area. △ Less","12 January, 2021",https://arxiv.org/pdf/2010.12789
Eye Tracking Data Collection Protocol for VR for Remotely Located Subjects using Blockchain and Smart Contracts,Efe Bozkir;Shahram Eivazi;Mete Akgün;Enkelejda Kasneci,"Eye tracking data collection in the virtual reality context is typically carried out in laboratory settings, which usually limits the number of participants or consumes at least several months of research time. In addition, under laboratory settings, subjects may not behave naturally due to being recorded in an uncomfortable environment. In this work, we propose a proof-of-concept eye tracking data collection protocol and its implementation to collect eye tracking data from remotely located subjects, particularly for virtual reality using Ethereum blockchain and smart contracts. With the proposed protocol, data collectors can collect high quality eye tracking data from a large number of human subjects with heterogeneous socio-demographic characteristics. The quality and the amount of data can be helpful for various tasks in data-driven human-computer interaction and artificial intelligence. △ Less","14 July, 2021",https://arxiv.org/pdf/2010.12570
Quantum Superposition Inspired Spiking Neural Network,Yinqian Sun;Yi Zeng;Tielin Zhang,"Despite advances in artificial intelligence models, neural networks still cannot achieve human performance, partly due to differences in how information is encoded and processed compared to human brain. Information in an artificial neural network (ANN) is represented using a statistical method and processed as a fitting function, enabling handling of structural patterns in image, text, and speech processing. However, substantial changes to the statistical characteristics of the data, for example, reversing the background of an image, dramatically reduce the performance. Here, we propose a quantum superposition spiking neural network (QS-SNN) inspired by quantum mechanisms and phenomena in the brain, which can handle reversal of image background color. The QS-SNN incorporates quantum theory with brain-inspired spiking neural network models from a computational perspective, resulting in more robust performance compared with traditional ANN models, especially when processing noisy inputs. The results presented here will inform future efforts to develop brain-inspired artificial intelligence. △ Less","17 November, 2021",https://arxiv.org/pdf/2010.12197
Kwame: A Bilingual AI Teaching Assistant for Online SuaCode Courses,George Boateng,"Introductory hands-on courses such as our smartphone-based coding course, SuaCode require a lot of support for students to accomplish learning goals. Online environments make it even more difficult to get assistance especially more recently because of COVID-19. Given the multilingual context of SuaCode students - learners across 42 African countries that are mostly Anglophone or Francophone - in this work, we developed a bilingual Artificial Intelligence (AI) Teaching Assistant (TA) - Kwame - that provides answers to students' coding questions from SuaCode courses in English and French. Kwame is a Sentence-BERT (SBERT)-based question-answering (QA) system that we trained and evaluated offline using question-answer pairs created from the course's quizzes, lesson notes and students' questions in past cohorts. Kwame finds the paragraph most semantically similar to the question via cosine similarity. We compared the system with TF-IDF and Universal Sentence Encoder. Our results showed that fine-tuning on the course data and returning the top 3 and 5 answers improved the accuracy results. Kwame will make it easy for students to get quick and accurate answers to questions in SuaCode courses. △ Less","13 June, 2021",https://arxiv.org/pdf/2010.11387
Deep Learning for Distinguishing Normal versus Abnormal Chest Radiographs and Generalization to Unseen Diseases,Zaid Nabulsi;Andrew Sellergren;Shahar Jamshy;Charles Lau;Edward Santos;Atilla P. Kiraly;Wenxing Ye;Jie Yang;Rory Pilgrim;Sahar Kazemzadeh;Jin Yu;Sreenivasa Raju Kalidindi;Mozziyar Etemadi;Florencia Garcia-Vicente;David Melnick;Greg S. Corrado;Lily Peng;Krish Eswaran;Daniel Tse;Neeral Beladia;Yun Liu;Po-Hsuan Cameron Chen;Shravya Shetty,"Chest radiography (CXR) is the most widely-used thoracic clinical imaging modality and is crucial for guiding the management of cardiothoracic conditions. The detection of specific CXR findings has been the main focus of several artificial intelligence (AI) systems. However, the wide range of possible CXR abnormalities makes it impractical to build specific systems to detect every possible condition. In this work, we developed and evaluated an AI system to classify CXRs as normal or abnormal. For development, we used a de-identified dataset of 248,445 patients from a multi-city hospital network in India. To assess generalizability, we evaluated our system using 6 international datasets from India, China, and the United States. Of these datasets, 4 focused on diseases that the AI was not trained to detect: 2 datasets with tuberculosis and 2 datasets with coronavirus disease 2019. Our results suggest that the AI system generalizes to new patient populations and abnormalities. In a simulated workflow where the AI system prioritized abnormal cases, the turnaround time for abnormal cases reduced by 7-28%. These results represent an important step towards evaluating whether AI can be safely used to flag cases in a general setting where previously unseen abnormalities exist. △ Less","29 October, 2021",https://arxiv.org/pdf/2010.11375
Average-reward model-free reinforcement learning: a systematic review and literature mapping,Vektor Dewanto;George Dunn;Ali Eshragh;Marcus Gallagher;Fred Roosta,"Reinforcement learning is important part of artificial intelligence. In this paper, we review model-free reinforcement learning that utilizes the average reward optimality criterion in the infinite horizon setting. Motivated by the solo survey by Mahadevan (1996a), we provide an updated review of work in this area and extend it to cover policy-iteration and function approximation methods (in addition to the value-iteration and tabular counterparts). We present a comprehensive literature mapping. We also identify and discuss opportunities for future work. △ Less","3 August, 2021",https://arxiv.org/pdf/2010.08920
Threats and Corrective Measures for IoT Security with Observance of Cybercrime: A Survey,Sita Rani;Aman Kataria;Vishal Sharma;Smarajit Ghosh;Vinod Karar;Kyungroul Lee;Chang Choi,"Internet of Things (IoT) is the utmost assuring framework to facilitate human life with quality and comfort. IoT has contributed significantly to numerous application areas. The stormy expansion of smart devices and their credence for data transfer using wireless mechanics boosts their susceptibility to cyber-attacks. Consequently, the rate of cybercrime is increasing day by day. Hence, the study of IoT security threats and possible corrective measures can benefit the researchers to identify appropriate solutions to deal with various challenges in cybercrime investigation. IoT forensics plays a vital role in cybercrime investigations. This review paper presents an overview of the IoT framework consisting of IoT architecture, protocols, and technologies. Various security issues at each layer and corrective measures are also discussed in detail. This paper also presents the role of IoT forensics in cybercrime investigation in various domains like smart homes, smart cities, automated vehicles, healthcare, etc. Along with the role of advanced technologies like Artificial Intelligence, Machine Learning, Cloud computing, Edge computing, Fog computing, and Blockchain technology in cybercrime investigation are also discussed. At last, various open research challenges in IoT to assist cybercrime investigation are explained, which provide a new direction for further research. △ Less","15 April, 2021",https://arxiv.org/pdf/2010.08793
A general approach to compute the relevance of middle-level input features,Andrea Apicella;Salvatore Giugliano;Francesco Isgrò;Roberto Prevete,"This work proposes a novel general framework, in the context of eXplainable Artificial Intelligence (XAI), to construct explanations for the behaviour of Machine Learning (ML) models in terms of middle-level features. One can isolate two different ways to provide explanations in the context of XAI: low and middle-level explanations. Middle-level explanations have been introduced for alleviating some deficiencies of low-level explanations such as, in the context of image classification, the fact that human users are left with a significant interpretive burden: starting from low-level explanations, one has to identify properties of the overall input that are perceptually salient for the human visual system. However, a general approach to correctly evaluate the elements of middle-level explanations with respect ML model responses has never been proposed in the literature. △ Less","27 January, 2021",https://arxiv.org/pdf/2010.08639
OnRAMP for Regulating AI in Medical Products,David Higgins,"Medical Artificial Intelligence (AI) involves the application of machine learning algorithms to biomedical datasets in order to improve medical practices. Products incorporating medical AI require certification before deployment in most jurisdictions. To date, clear pathways for regulating medical AI are still under development. Below the level of formal pathways lies the actual practice of developing a medical AI solution. This Perspective proposes best practice guidelines for development compatible with the production of a regulatory package which, regardless of the formal regulatory path, will form a core component of a certification process. The approach is predicated on a statistical risk perspective, typical of medical device regulators, and a deep understanding of machine learning methodologies. These guidelines will allow all parties to communicate more clearly in the development of a common Good Machine Learning Practice (GMLP), and thus lead to the enhanced development of both medical AI products and regulations. △ Less","26 July, 2021",https://arxiv.org/pdf/2010.07038
Basic principles and concept design of a real-time clinical decision support system for managing medical emergencies on missions to Mars,Juan M Garcia-Gomez,"Space agencies and private companies prepare the beginning of human space exploration for the 2030s with missions to put the first human on the Mars surface. The absence of gravity and radiation, along with distance, isolation and hostile environments, are expected to increase medical events where previously unseen manifestations may arise. The current healthcare strategy based on telemedicine and the possibility to stabilize and transport the injured crewmember to a terrestrial definitive medical facility is not applicable in exploration class missions. Therefore, the need for deploying the full autonomous capability to solve medical emergencies may guide the design of future onboard healthcare systems. We present ten basic principles and concept design of a software suite to bring onboard decision support to help the crew dealing with medical emergencies taking into consideration physiological disturbances in space and spaceflight restrictions. 1) give real-time support for emergency medical decision making, 2) give patient-specific advice for executive problem-solving, 3) take into account available information from life support and monitoring of crewmembers, 4) be fully autonomous from remote facilities, 5) continuously adapt predictions to physiological disturbance and changing conditions, 6) optimize emergency medical decision making in terms of mission fundamental priorities, 7) take into account medical supplies and equipment on board, 8) apply health standards for the level of care V, 9) implement ethics responsibilities for spaceflights, and 10) apply ethical standards for artificial intelligence. Based on these principles, we propose an autonomous clinical decision support system (CDSS) to provide real-time advice for emergency medical interventions on board of space exploration missions. △ Less","27 February, 2021",https://arxiv.org/pdf/2010.07029
Singularity and Coordination Problems: Pandemic Lessons from 2020,Nicholas Kluge Corrêa;Nythamar De Oliveira,"Are there any indications that a Technological Singularity may be on the horizon? In trying to answer these questions, the authors made a small introduction to the area of safety research in artificial intelligence. The authors review some of the current paradigms in the development of autonomous intelligent systems, searching for evidence that may indicate the coming of a possible Technological Singularity. Finally, the authors present a reflection using the COVID-19 pandemic, something that showed that global society biggest problem in managing existential risks is its lack of coordination skills as a global society. △ Less","1 October, 2021",https://arxiv.org/pdf/2010.07018
AI-assisted super-resolution cosmological simulations,Yin Li;Yueying Ni;Rupert A. C. Croft;Tiziana Di Matteo;Simeon Bird;Yu Feng,"Cosmological simulations of galaxy formation are limited by finite computational resources. We draw from the ongoing rapid advances in Artificial Intelligence (specifically Deep Learning) to address this problem. Neural networks have been developed to learn from high-resolution (HR) image data, and then make accurate super-resolution (SR) versions of different low-resolution (LR) images. We apply such techniques to LR cosmological N-body simulations, generating SR versions. Specifically, we are able to enhance the simulation resolution by generating 512 times more particles and predicting their displacements from the initial positions. Therefore our results can be viewed as new simulation realizations themselves rather than projections, e.g., to their density fields. Furthermore, the generation process is stochastic, enabling us to sample the small-scale modes conditioning on the large-scale environment. Our model learns from only 16 pairs of small-volume LR-HR simulations, and is then able to generate SR simulations that successfully reproduce the HR matter power spectrum to percent level up to 16\,h^{-1}\mathrm{Mpc}, and the HR halo mass function to within 10 \% down to 10^{11} \, M_\odot. We successfully deploy the model in a box 1000 times larger than the training simulation box, showing that high-resolution mock surveys can be generated rapidly. We conclude that AI assistance has the potential to revolutionize modeling of small-scale galaxy formation physics in large cosmological volumes. △ Less","4 May, 2021",https://arxiv.org/pdf/2010.06608
General stochastic separation theorems with optimal bounds,Bogdan Grechuk;Alexander N. Gorban;Ivan Y. Tyukin,"Phenomenon of stochastic separability was revealed and used in machine learning to correct errors of Artificial Intelligence (AI) systems and analyze AI instabilities. In high-dimensional datasets under broad assumptions each point can be separated from the rest of the set by simple and robust Fisher's discriminant (is Fisher separable). Errors or clusters of errors can be separated from the rest of the data. The ability to correct an AI system also opens up the possibility of an attack on it, and the high dimensionality induces vulnerabilities caused by the same stochastic separability that holds the keys to understanding the fundamentals of robustness and adaptivity in high-dimensional data-driven AI. To manage errors and analyze vulnerabilities, the stochastic separation theorems should evaluate the probability that the dataset will be Fisher separable in given dimensionality and for a given class of distributions. Explicit and optimal estimates of these separation probabilities are required, and this problem is solved in present work. The general stochastic separation theorems with optimal probability estimates are obtained for important classes of distributions: log-concave distribution, their convex combinations and product distributions. The standard i.i.d. assumption was significantly relaxed. These theorems and estimates can be used both for correction of high-dimensional data driven AI systems and for analysis of their vulnerabilities. The third area of application is the emergence of memories in ensembles of neurons, the phenomena of grandmother's cells and sparse coding in the brain, and explanation of unexpected effectiveness of small neural ensembles in high-dimensional brain. △ Less","9 January, 2021",https://arxiv.org/pdf/2010.05241
A Practical Tutorial on Graph Neural Networks,Isaac Ronald Ward;Jack Joyner;Casey Lickfold;Yulan Guo;Mohammed Bennamoun,"Graph neural networks (GNNs) have recently grown in popularity in the field of artificial intelligence (AI) due to their unique ability to ingest relatively unstructured data types as input data. Although some elements of the GNN architecture are conceptually similar in operation to traditional neural networks (and neural network variants), other elements represent a departure from traditional deep learning techniques. This tutorial exposes the power and novelty of GNNs to AI practitioners by collating and presenting details regarding the motivations, concepts, mathematics, and applications of the most common and performant variants of GNNs. Importantly, we present this tutorial concisely, alongside practical examples, thus providing a practical and accessible tutorial on the topic of GNNs. △ Less","25 December, 2021",https://arxiv.org/pdf/2010.05234
A Series of Unfortunate Counterfactual Events: the Role of Time in Counterfactual Explanations,Andrea Ferrario;Michele Loi,"Counterfactual explanations are a prominent example of post-hoc interpretability methods in the explainable Artificial Intelligence research domain. They provide individuals with alternative scenarios and a set of recommendations to achieve a sought-after machine learning model outcome. Recently, the literature has identified desiderata of counterfactual explanations, such as feasibility, actionability and sparsity that should support their applicability in real-world contexts. However, we show that the literature has neglected the problem of the time dependency of counterfactual explanations. We argue that, due to their time dependency and because of the provision of recommendations, even feasible, actionable and sparse counterfactual explanations may not be appropriate in real-world applications. This is due to the possible emergence of what we call ""unfortunate counterfactual events."" These events may occur due to the retraining of machine learning models whose outcomes have to be explained via counterfactual explanation. Series of unfortunate counterfactual events frustrate the efforts of those individuals who successfully implemented the recommendations of counterfactual explanations. This negatively affects people's trust in the ability of institutions to provide machine learning-supported decisions consistently. We introduce an approach to address the problem of the emergence of unfortunate counterfactual events that makes use of histories of counterfactual explanations. In the final part of the paper we propose an ethical analysis of two distinct strategies to cope with the challenge of unfortunate counterfactual events. We show that they respond to an ethically responsible imperative to preserve the trustworthiness of credit lending organizations, the decision models they employ, and the social-economic function of credit lending. △ Less","18 January, 2021",https://arxiv.org/pdf/2010.04687
"Artificial Intelligence based Anomaly Detection of Energy Consumption in Buildings: A Review, Current Trends and New Perspectives",Yassine Himeur;Khalida Ghanem;Abdullah Alsalemi;Faycal Bensaali;Abbes Amira,"Enormous amounts of data are being produced everyday by sub-meters and smart sensors installed in residential buildings. If leveraged properly, that data could assist end-users, energy producers and utility companies in detecting anomalous power consumption and understanding the causes of each anomaly. Therefore, anomaly detection could stop a minor problem becoming overwhelming. Moreover, it will aid in better decision-making to reduce wasted energy and promote sustainable and energy efficient behavior. In this regard, this paper is an in-depth review of existing anomaly detection frameworks for building energy consumption based on artificial intelligence. Specifically, an extensive survey is presented, in which a comprehensive taxonomy is introduced to classify existing algorithms based on different modules and parameters adopted, such as machine learning algorithms, feature extraction approaches, anomaly detection levels, computing platforms and application scenarios. To the best of the authors' knowledge, this is the first review article that discusses anomaly detection in building energy consumption. Moving forward, important findings along with domain-specific problems, difficulties and challenges that remain unresolved are thoroughly discussed, including the absence of: (i) precise definitions of anomalous power consumption, (ii) annotated datasets, (iii) unified metrics to assess the performance of existing solutions, (iv) platforms for reproducibility and (v) privacy-preservation. Following, insights about current research trends are discussed to widen the applications and effectiveness of the anomaly detection technology before deriving future directions attracting significant attention. This article serves as a comprehensive reference to understand the current technological progress in anomaly detection of energy consumption based on artificial intelligence. △ Less","12 February, 2021",https://arxiv.org/pdf/2010.04560
Tuning Convolutional Spiking Neural Network with Biologically-plausible Reward Propagation,Tielin Zhang;Shuncheng Jia;Xiang Cheng;Bo Xu,"Spiking Neural Networks (SNNs) contain more biologically realistic structures and biologically-inspired learning principles than those in standard Artificial Neural Networks (ANNs). SNNs are considered the third generation of ANNs, powerful on the robust computation with a low computational cost. The neurons in SNNs are non-differential, containing decayed historical states and generating event-based spikes after their states reaching the firing threshold. These dynamic characteristics of SNNs make it difficult to be directly trained with the standard backpropagation (BP), which is also considered not biologically plausible. In this paper, a Biologically-plausible Reward Propagation (BRP) algorithm is proposed and applied to the SNN architecture with both spiking-convolution (with both 1D and 2D convolutional kernels) and full-connection layers. Unlike the standard BP that propagates error signals from post to presynaptic neurons layer by layer, the BRP propagates target labels instead of errors directly from the output layer to all pre-hidden layers. This effort is more consistent with the top-down reward-guiding learning in cortical columns of the neocortex. Synaptic modifications with only local gradient differences are induced with pseudo-BP that might also be replaced with the Spike-Timing Dependent Plasticity (STDP). The performance of the proposed BRP-SNN is further verified on the spatial (including MNIST and Cifar-10) and temporal (including TIDigits and DvsGesture) tasks, where the SNN using BRP has reached a similar accuracy compared to other state-of-the-art BP-based SNNs and saved 50% more computational cost than ANNs. We think the introduction of biologically plausible learning rules to the training procedure of biologically realistic SNNs will give us more hints and inspirations toward a better understanding of the biological system's intelligent nature. △ Less","31 May, 2021",https://arxiv.org/pdf/2010.04434
"Adaptive Subcarrier, Parameter, and Power Allocation for Partitioned Edge Learning Over Broadband Channels",Dingzhu Wen;Ki-Jun Jeon;Mehdi Bennis;Kaibin Huang,"In this paper, we consider partitioned edge learning (PARTEL), which implements parameter-server training, a well known distributed learning method, in a wireless network. Thereby, PARTEL leverages distributed computation resources at edge devices to train a large-scale artificial intelligence (AI) model by dynamically partitioning the model into parametric blocks for separated updating at devices. Targeting broadband channels, we consider the joint control of parameter allocation, sub-channel allocation, and transmission power to improve the performance of PARTEL. Specifically, the policies for joint SUbcarrier, Parameter, and POweR allocaTion (SUPPORT) are optimized under the criterion of minimum learning latency. Two cases are considered. First, for the case of decomposable models (e.g., logistic regression), the latency-minimization problem is a mixed-integer program and non-convex. Due to its intractability, we develop a practical solution by integer relaxation and transforming it into an equivalent convex problem of model size maximization under a latency constraint. Thereby, a low-complexity algorithm is designed to compute the SUPPORT policy. Second, consider the case of deep neural network (DNN) models which can be trained using PARTEL by introducing some auxiliary variables. This, however, introduces constraints on model partitioning reducing the granularity of parameter allocation. The preceding policy is extended to DNN models by applying the proposed techniques of load rounding and proportional adjustment to rein in latency expansion caused by the load granularity constraints. △ Less","18 March, 2021",https://arxiv.org/pdf/2010.04061
Learning the aerodynamic design of supercritical airfoils through deep reinforcement learning,Runze Li;Yufei Zhang;Haixin Chen,"The aerodynamic design of modern civil aircraft requires a true sense of intelligence since it requires a good understanding of transonic aerodynamics and sufficient experience. Reinforcement learning is an artificial general intelligence that can learn sophisticated skills by trial-and-error, rather than simply extracting features or making predictions from data. The present paper utilizes a deep reinforcement learning algorithm to learn the policy for reducing the aerodynamic drag of supercritical airfoils. The policy is designed to take actions based on features of the wall Mach number distribution so that the learned policy can be more general. The initial policy for reinforcement learning is pretrained through imitation learning, and the result is compared with randomly generated initial policies. The policy is then trained in environments based on surrogate models, of which the mean drag reduction of 200 airfoils can be effectively improved by reinforcement learning. The policy is also tested by multiple airfoils in different flow conditions using computational fluid dynamics calculations. The results show that the policy is effective in both the training condition and other similar conditions, and the policy can be applied repeatedly to achieve greater drag reduction. △ Less","12 March, 2021",https://arxiv.org/pdf/2010.03651
AI Lifecycle Models Need To Be Revised. An Exploratory Study in Fintech,Mark Haakman;Luís Cruz;Hennie Huijgens;Arie van Deursen,"Tech-leading organizations are embracing the forthcoming artificial intelligence revolution. Intelligent systems are replacing and cooperating with traditional software components. Thus, the same development processes and standards in software engineering ought to be complied in artificial intelligence systems. This study aims to understand the processes by which artificial intelligence-based systems are developed and how state-of-the-art lifecycle models fit the current needs of the industry. We conducted an exploratory case study at ING, a global bank with a strong European base. We interviewed 17 people with different roles and from different departments within the organization. We have found that the following stages have been overlooked by previous lifecycle models: data collection, feasibility study, documentation, model monitoring, and model risk assessment. Our work shows that the real challenges of applying Machine Learning go much beyond sophisticated learning algorithms - more focus is needed on the entire lifecycle. In particular, regardless of the existing development tools for Machine Learning, we observe that they are still not meeting the particularities of this field. △ Less","2 June, 2021",https://arxiv.org/pdf/2010.02716
Explaining Deep Neural Networks,Oana-Maria Camburu,"Deep neural networks are becoming more and more popular due to their revolutionary success in diverse areas, such as computer vision, natural language processing, and speech recognition. However, the decision-making processes of these models are generally not interpretable to users. In various domains, such as healthcare, finance, or law, it is critical to know the reasons behind a decision made by an artificial intelligence system. Therefore, several directions for explaining neural models have recently been explored. In this thesis, I investigate two major directions for explaining deep neural networks. The first direction consists of feature-based post-hoc explanatory methods, that is, methods that aim to explain an already trained and fixed model (post-hoc), and that provide explanations in terms of input features, such as tokens for text and superpixels for images (feature-based). The second direction consists of self-explanatory neural models that generate natural language explanations, that is, models that have a built-in module that generates explanations for the predictions of the model. △ Less","13 October, 2021",https://arxiv.org/pdf/2010.01496
GraphXCOVID: Explainable Deep Graph Diffusion Pseudo-Labelling for Identifying COVID-19 on Chest X-rays,Angelica I Aviles-Rivero;Philip Sellars;Carola-Bibiane Schönlieb;Nicolas Papadakis,"Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing Artificial Intelligence techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi-supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist's mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision. △ Less","4 July, 2021",https://arxiv.org/pdf/2010.00378
Research and Education Towards Smart and Sustainable World,Jukka Riekki;Aarne Mämmelä,"We propose a vision for directing research and education in the ICT field. Our Smart and Sustainable World vision targets at prosperity for the people and the planet through better awareness and control of both human-made and natural environment. The needs of the society, individuals, and industries are fulfilled with intelligent systems that sense their environment, make proactive decisions on actions advancing their goals, and perform the actions on the environment. We emphasize artificial intelligence, feedback loops, human acceptance and control, intelligent use of basic resources, performance parameters, mission-oriented interdisciplinary research, and a holistic systems view complementing the conventional analytical reductive view as a research paradigm especially for complex problems. To serve a broad audience, we explain these concepts and list the essential literature. We suggest planning research and education by specifying, in a step-wise manner, scenarios, performance criteria, system models, research problems and education content, resulting in common goals and a coherent project portfolio as well as education curricula. Research and education produce feedback to support evolutionary development and encourage creativity in research. Finally, we propose concrete actions for realizing this approach. △ Less","27 April, 2021",https://arxiv.org/pdf/2009.13849
"The Grey Hoodie Project: Big Tobacco, Big Tech, and the threat on academic integrity",Mohamed Abdalla;Moustafa Abdalla,"As governmental bodies rely on academics' expert advice to shape policy regarding Artificial Intelligence, it is important that these academics not have conflicts of interests that may cloud or bias their judgement. Our work explores how Big Tech can actively distort the academic landscape to suit its needs. By comparing the well-studied actions of another industry (Big Tobacco) to the current actions of Big Tech we see similar strategies employed by both industries. These strategies enable either industry to sway and influence academic and public discourse. We examine the funding of academic research as a tool used by Big Tech to put forward a socially responsible public image, influence events hosted by and decisions made by funded universities, influence the research questions and plans of individual scientists, and discover receptive academics who can be leveraged. We demonstrate how Big Tech can affect academia from the institutional level down to individual researchers. Thus, we believe that it is vital, particularly for universities and other institutions of higher learning, to discuss the appropriateness and the tradeoffs of accepting funding from Big Tech, and what limitations or conditions should be put in place. △ Less","27 April, 2021",https://arxiv.org/pdf/2009.13676
Landscape of R packages for eXplainable Artificial Intelligence,Szymon Maksymiuk;Alicja Gosiewska;Przemyslaw Biecek,"The growing availability of data and computing power fuels the development of predictive models. In order to ensure the safe and effective functioning of such models, we need methods for exploration, debugging, and validation. New methods and tools for this purpose are being developed within the eXplainable Artificial Intelligence (XAI) subdomain of machine learning. In this work (1) we present the taxonomy of methods for model explanations, (2) we identify and compare 27 packages available in R to perform XAI analysis, (3) we present an example of an application of particular packages, (4) we acknowledge recent trends in XAI. The article is primarily devoted to the tools available in R, but since it is easy to integrate the Python code, we will also show examples for the most popular libraries from Python. △ Less","26 March, 2021",https://arxiv.org/pdf/2009.13248
Pareto-Optimal Bit Allocation for Collaborative Intelligence,Saeed Ranjbar Alvar;Ivan V. Bajić,"In recent studies, collaborative intelligence (CI) has emerged as a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile/edge devices. In CI, the AI model (a deep neural network) is split between the edge and the cloud, and intermediate features are sent from the edge sub-model to the cloud sub-model. In this paper, we study bit allocation for feature coding in multi-stream CI systems. We model task distortion as a function of rate using convex surfaces similar to those found in distortion-rate theory. Using such models, we are able to provide closed-form bit allocation solutions for single-task systems and scalarized multi-task systems. Moreover, we provide analytical characterization of the full Pareto set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream 2-task systems. Analytical results are examined on a variety of DNN models from the literature to demonstrate wide applicability of the results △ Less","29 April, 2021",https://arxiv.org/pdf/2009.12430
Artificial Intelligence for UAV-enabled Wireless Networks: A Survey,Mohamed-Amine Lahmeri;Mustafa A. Kishk;Mohamed-Slim Alouini,"Unmanned aerial vehicles (UAVs) are considered as one of the promising technologies for the next-generation wireless communication networks. Their mobility and their ability to establish line of sight (LOS) links with the users made them key solutions for many potential applications. In the same vein, artificial intelligence (AI) is growing rapidly nowadays and has been very successful, particularly due to the massive amount of the available data. As a result, a significant part of the research community has started to integrate intelligence at the core of UAVs networks by applying AI algorithms in solving several problems in relation to drones. In this article, we provide a comprehensive overview of some potential applications of AI in UAV-based networks. We also highlight the limits of the existing works and outline some potential future applications of AI for UAV networks. △ Less","28 January, 2021",https://arxiv.org/pdf/2009.11522
DTI-SNNFRA: Drug-Target interaction prediction by shared nearest neighbors and fuzzy-rough approximation,Sk Mazharul Islam;Sk Md Mosaddek Hossain;Sumanta Ray,"In-silico prediction of repurposable drugs is an effective drug discovery strategy that supplements de-nevo drug discovery from scratch. Reduced development time, less cost and absence of severe side effects are significant advantages of using drug repositioning. Most recent and most advanced artificial intelligence (AI) approaches have boosted drug repurposing in terms of throughput and accuracy enormously. However, with the growing number of drugs, targets and their massive interactions produce imbalanced data which may not be suitable as input to the classification model directly. Here, we have proposed DTI-SNNFRA, a framework for predicting drug-target interaction (DTI), based on shared nearest neighbour (SNN) and fuzzy-rough approximation (FRA). It uses sampling techniques to collectively reduce the vast search space covering the available drugs, targets and millions of interactions between them. DTI-SNNFRA operates in two stages: first, it uses SNN followed by a partitioning clustering for sampling the search space. Next, it computes the degree of fuzzy-rough approximations and proper degree threshold selection for the negative samples' undersampling from all possible interaction pairs between drugs and targets obtained in the first stage. Finally, classification is performed using the positive and selected negative samples. We have evaluated the efficacy of DTI-SNNFRA using AUC (Area under ROC Curve), Geometric Mean, and F1 Score. The model performs exceptionally well with a high prediction score of 0.95 for ROC-AUC. The predicted drug-target interactions are validated through an existing drug-target database (Connectivity Map (Cmap)). △ Less","20 February, 2021",https://arxiv.org/pdf/2009.10766
Local Post-Hoc Explanations for Predictive Process Monitoring in Manufacturing,Nijat Mehdiyev;Peter Fettke,"This study proposes an innovative explainable predictive quality analytics solution to facilitate data-driven decision-making for process planning in manufacturing by combining process mining, machine learning, and explainable artificial intelligence (XAI) methods. For this purpose, after integrating the top-floor and shop-floor data obtained from various enterprise information systems, a deep learning model was applied to predict the process outcomes. Since this study aims to operationalize the delivered predictive insights by embedding them into decision-making processes, it is essential to generate relevant explanations for domain experts. To this end, two complementary local post-hoc explanation approaches, Shapley values and Individual Conditional Expectation (ICE) plots are adopted, which are expected to enhance the decision-making capabilities by enabling experts to examine explanations from different perspectives. After assessing the predictive strength of the applied deep neural network with relevant binary classification evaluation measures, a discussion of the generated explanations is provided. △ Less","10 June, 2021",https://arxiv.org/pdf/2009.10513
Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images,Lucas O. Teixeira;Rodolfo M. Pereira;Diego Bertolini;Luiz S. Oliveira;Loris Nanni;George D. C. Cavalcanti;Yandre M. G. Costa,"COVID-19 frequently provokes pneumonia, which can be diagnosed using imaging exams. Chest X-ray (CXR) is often useful because it is cheap, fast, widespread, and uses less radiation. Here, we demonstrate the impact of lung segmentation in COVID-19 identification using CXR images and evaluate which contents of the image influenced the most. Semantic segmentation was performed using a U-Net CNN architecture, and the classification using three CNN architectures (VGG, ResNet, and Inception). Explainable Artificial Intelligence techniques were employed to estimate the impact of segmentation. A three-classes database was composed: lung opacity (pneumonia), COVID-19, and normal. We assessed the impact of creating a CXR image database from different sources, and the COVID-19 generalization from one source to another. The segmentation achieved a Jaccard distance of 0.034 and a Dice coefficient of 0.982. The classification using segmented images achieved an F1-Score of 0.88 for the multi-class setup, and 0.83 for COVID-19 identification. In the cross-dataset scenario, we obtained an F1-Score of 0.74 and an area under the ROC curve of 0.9 for COVID-19 identification using segmented images. Experiments support the conclusion that even after segmentation, there is a strong bias introduced by underlying factors from different sources. △ Less","13 September, 2021",https://arxiv.org/pdf/2009.09780
A Joint introduction to Gaussian Processes and Relevance Vector Machines with Connections to Kalman filtering and other Kernel Smoothers,Luca Martino;Jesse Read,"The expressive power of Bayesian kernel-based methods has led them to become an important tool across many different facets of artificial intelligence, and useful to a plethora of modern application domains, providing both power and interpretability via uncertainty analysis. This article introduces and discusses two methods which straddle the areas of probabilistic Bayesian schemes and kernel methods for regression: Gaussian Processes and Relevance Vector Machines. Our focus is on developing a common framework with which to view these methods, via intermediate methods a probabilistic version of the well-known kernel ridge regression, and drawing connections among them, via dual formulations, and discussion of their application in the context of major tasks: regression, smoothing, interpolation, and filtering. Overall, we provide understanding of the mathematical concepts behind these models, and we summarize and discuss in depth different interpretations and highlight the relationship to other methods, such as linear kernel smoothers, Kalman filtering and Fourier approximations. Throughout, we provide numerous figures to promote understanding, and we make numerous recommendations to practitioners. Benefits and drawbacks of the different techniques are highlighted. To our knowledge, this is the most in-depth study of its kind to date focused on these two methods, and will be relevant to theoretical understanding and practitioners throughout the domains of data-science, signal processing, machine learning, and artificial intelligence in general. △ Less","11 July, 2021",https://arxiv.org/pdf/2009.09217
Value Alignment Equilibrium in Multiagent Systems,Nieves Montes;Carles Sierra,"Value alignment has emerged in recent years as a basic principle to produce beneficial and mindful Artificial Intelligence systems. It mainly states that autonomous entities should behave in a way that is aligned with our human values. In this work, we summarize a previously developed model that considers values as preferences over states of the world and defines alignment between the governing norms and the values. We provide a use-case for this framework with the Iterated Prisoner's Dilemma model, which we use to exemplify the definitions we review. We take advantage of this use-case to introduce new concepts to be integrated with the established framework: alignment equilibrium and Pareto optimal alignment. These are inspired on the classical Nash equilibrium and Pareto optimality, but are designed to account for any value we wish to model in the system. △ Less","25 June, 2021",https://arxiv.org/pdf/2009.07619
Optimality of short-term synaptic plasticity in modelling certain dynamic environments,Timoleon Moraitis;Abu Sebastian;Evangelos Eleftheriou,"Biological neurons and their in-silico emulations for neuromorphic artificial intelligence (AI) use extraordinarily energy-efficient mechanisms, such as spike-based communication and local synaptic plasticity. It remains unclear whether these neuronal mechanisms only offer efficiency or also underlie the superiority of biological intelligence. Here, we prove rigorously that, indeed, the Bayes-optimal prediction and inference of randomly but continuously transforming environments, a common natural setting, relies on short-term spike-timing-dependent plasticity, a hallmark of biological synapses. Further, this dynamic Bayesian inference through plasticity enables circuits of the cerebral cortex in simulations to recognize previously unseen, highly distorted dynamic stimuli. Strikingly, this also introduces a biologically-modelled AI, the first to overcome multiple limitations of deep learning and outperform artificial neural networks in a visual task. The cortical-like network is spiking and event-based, trained only with unsupervised and local plasticity, on a small, narrow, and static training dataset, but achieves recognition of unseen, transformed, and dynamic data better than deep neural networks with continuous activations, trained with supervised backpropagation on the transforming data. These results link short-term plasticity to high-level cortical function, suggest optimality of natural intelligence for natural environments, and repurpose neuromorphic AI from mere efficiency to computational supremacy altogether. △ Less","15 June, 2021",https://arxiv.org/pdf/2009.06808
SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition,Liangzhi Li;Bowen Wang;Manisha Verma;Yuta Nakashima;Ryo Kawasaki;Hajime Nagahara,"Explainable artificial intelligence has been gaining attention in the past few years. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER's explanation is involved in the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells ""why the image is of a certain category"" or ""why the image is not of a certain category."" We design a new loss tailored for SCOUTER that controls the model's behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations in terms of various metrics while keeping good accuracy on small and medium-sized datasets. △ Less","20 August, 2021",https://arxiv.org/pdf/2009.06138
Semi-Supervised Active Learning for COVID-19 Lung Ultrasound Multi-symptom Classification,Lei Liu;Wentao Lei;Yongfang Luo;Cheng Feng;Xiang Wan;Li Liu,"Ultrasound (US) is a non-invasive yet effective medical diagnostic imaging technique for the COVID-19 global pandemic. However, due to complex feature behaviors and expensive annotations of US images, it is difficult to apply Artificial Intelligence (AI) assisting approaches for lung's multi-symptom (multi-label) classification. To overcome these difficulties, we propose a novel semi-supervised Two-Stream Active Learning (TSAL) method to model complicated features and reduce labeling costs in an iterative procedure. The core component of TSAL is the multi-label learning mechanism, in which label correlations information is used to design multi-label margin (MLM) strategy and confidence validation for automatically selecting informative samples and confident labels. On this basis, a multi-symptom multi-label (MSML) classification network is proposed to learn discriminative features of lung symptoms, and a human-machine interaction is exploited to confirm the final annotations that are used to fine-tune MSML with progressively labeled data. Moreover, a novel lung US dataset named COVID19-LUSMS is built, currently containing 71 clinical patients with 6,836 images sampled from 678 videos. Experimental evaluations show that TSAL using only 20% data can achieve superior performance to the baseline and the state-of-the-art. Qualitatively, visualization of both attention map and sample distribution confirms the good consistency with the clinic knowledge. △ Less","28 February, 2021",https://arxiv.org/pdf/2009.05436
A Vision of Self-Evolving Network Management for Future Intelligent Vertical HetNet,Tasneem Darwish;Gunes Karabulut Kurt;Halim Yanikomeroglu;Gamini Senarath;Peiying Zhu,"Future integrated terrestrial-aerial-satellite networks will have to exhibit some unprecedented characteristics for the provision of both communications and computation services, and security for a tremendous number of devices with very broad and demanding requirements across multiple networks, operators, and ecosystems. Although 3GPP introduced the concept of self-organization networks (SONs) in 4G and 5G documents to automate network management, even this progressive concept will face several challenges as it may not be sufficiently agile in coping with the immense levels of complexity, heterogeneity, and mobility in the envisioned beyond-5G integrated networks. In the presented vision, we discuss how future integrated networks can be intelligently and autonomously managed to efficiently utilize resources, reduce operational costs, and achieve the targeted Quality of Experience (QoE). We introduce the novel concept of ""self-evolving networks (SENs)"" framework, which utilizes artificial intelligence, enabled by machine learning (ML) algorithms, to make future integrated networks fully automated and intelligently evolve with respect to the provision, adaptation, optimization, and management aspects of networking, communications, computation, and infrastructure nodes' mobility. To envisage the concept of SEN in future integrated networks, we use the Intelligent Vertical Heterogeneous Network (I-VHetNet) architecture as our reference. The paper discusses five prominent scenarios where SEN plays the main role in providing automated network management. Numerical results provide an insight on how the SEN framework improves the performance of future integrated networks. The paper presents the leading enablers and examines the challenges associated with the application of SEN concept in future integrated networks. △ Less","9 March, 2021",https://arxiv.org/pdf/2009.02771
"Efficient, high-performance pancreatic segmentation using multi-scale feature extraction",Moritz Knolle;Georgios Kaissis;Friederike Jungmann;Sebastian Ziegelmayer;Daniel Sasse;Marcus Makowski;Daniel Rueckert;Rickmer Braren,"For artificial intelligence-based image analysis methods to reach clinical applicability, the development of high-performance algorithms is crucial. For example, existent segmentation algorithms based on natural images are neither efficient in their parameter use nor optimized for medical imaging. Here we present MoNet, a highly optimized neural-network-based pancreatic segmentation algorithm focused on achieving high performance by efficient multi-scale image feature utilization. △ Less","12 January, 2021",https://arxiv.org/pdf/2009.00872
Uncertainty quantification for Markov Random Fields,Panagiota Birmpa;Markos A. Katsoulakis,"We present an information-based uncertainty quantification method for general Markov Random Fields. Markov Random Fields (MRF) are structured, probabilistic graphical models over undirected graphs, and provide a fundamental unifying modeling tool for statistical mechanics, probabilistic machine learning, and artificial intelligence. Typically MRFs are complex and high-dimensional with nodes and edges (connections) built in a modular fashion from simpler, low-dimensional probabilistic models and their local connections; in turn, this modularity allows to incorporate available data to MRFs and efficiently simulate them by leveraging their graph-theoretic structure. Learning graphical models from data and/or constructing them from physical modeling and constraints necessarily involves uncertainties inherited from data, modeling choices, or numerical approximations. These uncertainties in the MRF can be manifested either in the graph structure or the probability distribution functions, and necessarily will propagate in predictions for quantities of interest. Here we quantify such uncertainties using tight, information based bounds on the predictions of quantities of interest; these bounds take advantage of the graphical structure of MRFs and are capable of handling the inherent high-dimensionality of such graphical models. We demonstrate our methods in MRFs for medical diagnostics and statistical mechanics models. In the latter, we develop uncertainty quantification bounds for finite size effects and phase diagrams, which constitute two of the typical predictions goals of statistical mechanics modeling. △ Less","17 July, 2021",https://arxiv.org/pdf/2009.00038
Evaluation of machine learning algorithms for Health and Wellness applications: a tutorial,Jussi Tohka;Mark van Gils,"Research on decision support applications in healthcare, such as those related to diagnosis, prediction, treatment planning, etc., have seen enormously increased interest recently. This development is thanks to the increase in data availability as well as advances in artificial intelligence and machine learning research. Highly promising research examples are published daily. However, at the same time, there are some unrealistic expectations with regards to the requirements for reliable development and objective validation that is needed in healthcare settings. These expectations may lead to unmet schedules and disappointments (or non-uptake) at the end-user side. It is the aim of this tutorial to provide practical guidance on how to assess performance reliably and efficiently and avoid common traps. Instead of giving a list of do's and don't s, this tutorial tries to build a better understanding behind these do's and don't s and presents both the most relevant performance evaluation criteria as well as how to compute them. Along the way, we will indicate common mistakes and provide references discussing various topics more in-depth. △ Less","24 March, 2021",https://arxiv.org/pdf/2008.13690
Machine learning thermal circuit network model for thermal design optimization of electronic circuit board layout with transient heating chips,Daiki Otaki;Hirofumi Nonaka;Noboru Yamada,"This paper describes a method combining Bayesian optimization (BO) and a lamped-capacitance thermal circuit network model that is effective for speeding up the thermal design optimization of an electronic circuit board layout with transient heating chips. As electronic devices have become smaller and more complex, the importance of thermal design optimization to ensure heat dissipation performance has increased. However, such thermal design optimization is difficult because it is necessary to consider various trade-offs associated with packaging and transient temperature changes of heat-generating components. This study aims to improve the performance of thermal design optimization by artificial intelligence. BO using a Gaussian process was combined with the lamped-capacitance thermal circuit network model, and its performance was verified by case studies. As a result, BO successfully found the ideal circuit board layout as well as particle swarm optimization (PSO) and genetic algorithm (GA) could. The CPU time for BO was 1/5 and 1/4 of that for PSO and GA, respectively. In addition, BO found a non-intuitive optimal solution in approximately 7 minutes from 10 million layout patterns. It was estimated that this was 1/1000 of the CPU time required for analyzing all layout patterns. △ Less","16 January, 2021",https://arxiv.org/pdf/2008.13571
Pay Attention to Evolution: Time Series Forecasting with Deep Graph-Evolution Learning,Gabriel Spadon;Shenda Hong;Bruno Brandoli;Stan Matwin;Jose F. Rodrigues-Jr;Jimeng Sun,"Time-series forecasting is one of the most active research topics in artificial intelligence. Applications in real-world time series should consider two factors for achieving reliable predictions: modeling dynamic dependencies among multiple variables and adjusting the model's intrinsic hyperparameters. A still open gap in that literature is that statistical and ensemble learning approaches systematically present lower predictive performance than deep learning methods. They generally disregard the data sequence aspect entangled with multivariate data represented in more than one time series. Conversely, this work presents a novel neural network architecture for time-series forecasting that combines the power of graph evolution with deep recurrent learning on distinct data distributions; we named our method Recurrent Graph Evolution Neural Network (ReGENN). The idea is to infer multiple multivariate relationships between co-occurring time-series by assuming that the temporal data depends not only on inner variables and intra-temporal relationships (i.e., observations from itself) but also on outer variables and inter-temporal relationships (i.e., observations from other-selves). An extensive set of experiments was conducted comparing ReGENN with dozens of ensemble methods and classical statistical ones, showing sound improvement of up to 64.87% over the competing algorithms. Furthermore, we present an analysis of the intermediate weights arising from ReGENN, showing that by looking at inter and intra-temporal relationships simultaneously, time-series forecasting is majorly improved if paying attention to how multiple multivariate data synchronously evolve. △ Less","26 May, 2021",https://arxiv.org/pdf/2008.12833
A principled analysis of Behavior Trees and their generalisations,Oliver Biggar;Mohammad Zamani;Iman Shames,"As complex autonomous robotic systems become more widespread, the need for transparent and reusable Artificial Intelligence (AI) designs becomes more apparent. In this paper we analyse how the principles behind Behavior Trees (BTs), an increasingly popular tree-structured control architecture, are applicable to these goals. Using structured programming as a guide, we analyse the BT principles of reactiveness and modularity in a formal framework of action selection. Proceeding from these principles, we review a number of challenging use cases of BTs in the literature, and show that reasoning via these principles leads to compatible solutions. Extending these arguments, we introduce a new class of control architectures we call generalised BTs or k-BTs and show how they can extend the applicability of BTs to some of the aforementioned challenging BT use cases while preserving the BT principles. △ Less","25 May, 2021",https://arxiv.org/pdf/2008.11906
Physically Unclonable Functions and AI: Two Decades of Marriage,Fatemeh Ganji;Shahin Tajik,"The current chapter aims at establishing a relationship between artificial intelligence (AI) and hardware security. Such a connection between AI and software security has been confirmed and well-reviewed in the relevant literature. The main focus here is to explore the methods borrowed from AI to assess the security of a hardware primitive, namely physically unclonable functions (PUFs), which has found applications in cryptographic protocols, e.g., authentication and key generation. Metrics and procedures devised for this are further discussed. Moreover, By reviewing PUFs designed by applying AI techniques, we give insight into future research directions in this area. △ Less","11 February, 2021",https://arxiv.org/pdf/2008.11355
"A review of deep learning in medical imaging: Imaging traits, technology trends, case studies with progress highlights, and future promises",S. Kevin Zhou;Hayit Greenspan;Christos Davatzikos;James S. Duncan;Bram van Ginneken;Anant Madabhushi;Jerry L. Prince;Daniel Rueckert;Ronald M. Summers,"Since its renaissance, deep learning has been widely used in various medical imaging tasks and has achieved remarkable success in many medical imaging applications, thereby propelling us into the so-called artificial intelligence (AI) era. It is known that the success of AI is mostly attributed to the availability of big data with annotations for a single task and the advances in high performance computing. However, medical imaging presents unique challenges that confront deep learning approaches. In this survey paper, we first present traits of medical imaging, highlight both clinical needs and technical challenges in medical imaging, and describe how emerging trends in deep learning are addressing these issues. We cover the topics of network architecture, sparse and noisy labels, federating learning, interpretability, uncertainty quantification, etc. Then, we present several case studies that are commonly found in clinical practice, including digital pathology and chest, brain, cardiovascular, and abdominal imaging. Rather than presenting an exhaustive literature survey, we instead describe some prominent research highlights related to these case study applications. We conclude with a discussion and presentation of promising future directions. △ Less","5 March, 2021",https://arxiv.org/pdf/2008.09104
Intelligent Radio Signal Processing: A Survey,Quoc-Viet Pham;Nhan Thanh Nguyen;Thien Huynh-The;Long Bao Le;Kyungchun Lee;Won-Joo Hwang,"Intelligent signal processing for wireless communications is a vital task in modern wireless systems, but it faces new challenges because of network heterogeneity, diverse service requirements, a massive number of connections, and various radio characteristics. Owing to recent advancements in big data and computing technologies, artificial intelligence (AI) has become a useful tool for radio signal processing and has enabled the realization of intelligent radio signal processing. This survey covers four intelligent signal processing topics for the wireless physical layer, including modulation classification, signal detection, beamforming, and channel estimation. In particular, each theme is presented in a dedicated section, starting with the most fundamental principles, followed by a review of up-to-date studies and a summary. To provide the necessary background, we first present a brief overview of AI techniques such as machine learning, deep learning, and federated learning. Finally, we highlight a number of research challenges and future directions in the area of intelligent radio signal processing. We expect this survey to be a good source of information for anyone interested in intelligent radio signal processing, and the perspectives we provide therein will stimulate many more novel ideas and contributions in the future. △ Less","3 June, 2021",https://arxiv.org/pdf/2008.08264
Music Boundary Detection using Convolutional Neural Networks: A comparative analysis of combined input features,Carlos Hernandez-Olivan;Jose R. Beltran;David Diaz-Guerra,"The analysis of the structure of musical pieces is a task that remains a challenge for Artificial Intelligence, especially in the field of Deep Learning. It requires prior identification of structural boundaries of the music pieces. This structural boundary analysis has recently been studied with unsupervised methods and \textit{end-to-end} techniques such as Convolutional Neural Networks (CNN) using Mel-Scaled Log-magnitude Spectograms features (MLS), Self-Similarity Matrices (SSM) or Self-Similarity Lag Matrices (SSLM) as inputs and trained with human annotations. Several studies have been published divided into unsupervised and \textit{end-to-end} methods in which pre-processing is done in different ways, using different distance metrics and audio characteristics, so a generalized pre-processing method to compute model inputs is missing. The objective of this work is to establish a general method of pre-processing these inputs by comparing the inputs calculated from different pooling strategies, distance metrics and audio characteristics, also taking into account the computing time to obtain them. We also establish the most effective combination of inputs to be delivered to the CNN in order to establish the most efficient way to extract the limits of the structure of the music pieces. With an adequate combination of input matrices and pooling strategies we obtain a measurement accuracy F_1 of 0.411 that outperforms the current one obtained under the same conditions. △ Less","1 December, 2021",https://arxiv.org/pdf/2008.07527
AIPerf: Automated machine learning as an AI-HPC benchmark,Zhixiang Ren;Yongheng Liu;Tianhui Shi;Lei Xie;Yue Zhou;Jidong Zhai;Youhui Zhang;Yunquan Zhang;Wenguang Chen,"The plethora of complex artificial intelligence (AI) algorithms and available high performance computing (HPC) power stimulates the expeditious development of AI components with heterogeneous designs. Consequently, the need for cross-stack performance benchmarking of AI-HPC systems emerges rapidly. The de facto HPC benchmark LINPACK can not reflect AI computing power and I/O performance without representative workload. The current popular AI benchmarks like MLPerf have fixed problem size therefore limited scalability. To address these issues, we propose an end-to-end benchmark suite utilizing automated machine learning (AutoML), which not only represents real AI scenarios, but also is auto-adaptively scalable to various scales of machines. We implement the algorithms in a highly parallel and flexible way to ensure the efficiency and optimization potential on diverse systems with customizable configurations. We utilize operations per second (OPS), which is measured in an analytical and systematic approach, as the major metric to quantify the AI performance. We perform evaluations on various systems to ensure the benchmark's stability and scalability, from 4 nodes with 32 NVIDIA Tesla T4 (56.1 Tera-OPS measured), up to 512 nodes with 4096 Huawei Ascend 910 (194.53 Peta-OPS measured), and the results show near-linear weak scalability. With flexible workload and single metric, our benchmark can scale and rank AI-HPC easily. △ Less","14 March, 2021",https://arxiv.org/pdf/2008.07141
Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs,Seowoo Jang;Soyoung Yoo;Namwoo Kang,"Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention. △ Less","16 February, 2021",https://arxiv.org/pdf/2008.07119
Artificial Neural Networks and Fault Injection Attacks,Shahin Tajik;Fatemeh Ganji,"This chapter is on the security assessment of artificial intelligence (AI) and neural network (NN) accelerators in the face of fault injection attacks. More specifically, it discusses the assets on these platforms and compares them with ones known and well-studied in the field of cryptographic systems. This is a crucial step that must be taken in order to define the threat models precisely. With respect to that, fault attacks mounted on NNs and AI accelerators are explored. △ Less","11 February, 2021",https://arxiv.org/pdf/2008.07072
ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis,Gwenolé Quellec;Hassan Al Hajj;Mathieu Lamard;Pierre-Henri Conze;Pascale Massin;Béatrice Cochener,"In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the ""black-box"" nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment. △ Less","22 July, 2021",https://arxiv.org/pdf/2008.05731
Ortus: an Emotion-Driven Approach to (artificial) Biological Intelligence,Andrew W. E. McDonald;Sean Grimes;David E. Breen,"Ortus is a simple virtual organism that also serves as an initial framework for investigating and developing biologically-based artificial intelligence. Born from a goal to create complex virtual intelligence and an initial attempt to model C. elegans, Ortus implements a number of mechanisms observed in organic nervous systems, and attempts to fill in unknowns based upon plausible biological implementations and psychological observations. Implemented mechanisms include excitatory and inhibitory chemical synapses, bidirectional gap junctions, and Hebbian learning with its Stentian extension. We present an initial experiment that showcases Ortus' fundamental principles; specifically, a cyclic respiratory circuit, and emotionally-driven associative learning with respect to an input stimulus. Finally, we discuss the implications and future directions for Ortus and similar systems. △ Less","16 February, 2021",https://arxiv.org/pdf/2008.04875
Creative AI Through Evolutionary Computation: Principles and Examples,Risto Miikkulainen,"The main power of artificial intelligence is not in modeling what we already know, but in creating solutions that are new. Such solutions exist in extremely large, high-dimensional, and complex search spaces. Population-based search techniques, i.e. variants of evolutionary computation, are well suited to finding them. These techniques make it possible to find creative solutions to practical problems in the real world, making creative AI through evolutionary computation the likely ""next deep learning."" △ Less","14 February, 2021",https://arxiv.org/pdf/2008.04212
Deep Learning Based Defect Detection for Solder Joints on Industrial X-Ray Circuit Board Images,Qianru Zhang;Meng Zhang;Chinthaka Gamanayake;Chau Yuen;Zehao Geng;Hirunima Jayasekara;Xuewen Zhang;Chia-wei Woo;Jenny Low;Xiang Liu,"Quality control is of vital importance during electronics production. As the methods of producing electronic circuits improve, there is an increasing chance of solder defects during assembling the printed circuit board (PCB). Many technologies have been incorporated for inspecting failed soldering, such as X-ray imaging, optical imaging, and thermal imaging. With some advanced algorithms, the new technologies are expected to control the production quality based on the digital images. However, current algorithms sometimes are not accurate enough to meet the quality control. Specialists are needed to do a follow-up checking. For automated X-ray inspection, joint of interest on the X-ray image is located by region of interest (ROI) and inspected by some algorithms. Some incorrect ROIs deteriorate the inspection algorithm. The high dimension of X-ray images and the varying sizes of image dimensions also challenge the inspection algorithms. On the other hand, recent advances on deep learning shed light on image-based tasks and are competitive to human levels. In this paper, deep learning is incorporated in X-ray imaging based quality control during PCB quality inspection. Two artificial intelligence (AI) based models are proposed and compared for joint defect detection. The noised ROI problem and the varying sizes of imaging dimension problem are addressed. The efficacy of the proposed methods are verified through experimenting on a real-world 3D X-ray dataset. By incorporating the proposed methods, specialist inspection workload is largely saved. △ Less","25 March, 2021",https://arxiv.org/pdf/2008.02604
A critical analysis of metrics used for measuring progress in artificial intelligence,Kathrin Blagec;Georg Dorffner;Milad Moradi;Matthias Samwald,"Comparing model performances on benchmark datasets is an integral part of measuring and driving progress in artificial intelligence. A model's performance on a benchmark dataset is commonly assessed based on a single or a small set of performance metrics. While this enables quick comparisons, it may entail the risk of inadequately reflecting model performance if the metric does not sufficiently cover all performance characteristics. It is unknown to what extent this might impact benchmarking efforts. To address this question, we analysed the current landscape of performance metrics based on data covering 3867 machine learning model performance results from the open repository 'Papers with Code'. Our results suggest that the large majority of metrics currently used have properties that may result in an inadequate reflection of a models' performance. While alternative metrics that address problematic properties have been proposed, they are currently rarely used. Furthermore, we describe ambiguities in reported metrics, which may lead to difficulties in interpreting and comparing model performances. △ Less","8 November, 2021",https://arxiv.org/pdf/2008.02577
"The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies",Aniek F. Markus;Jan A. Kors;Peter R. Rijnbeek,"Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation). △ Less","5 January, 2021",https://arxiv.org/pdf/2007.15911
A Vision and Framework for the High Altitude Platform Station (HAPS) Networks of the Future,Gunes Kurt;Mohammad G. Khoshkholgh;Safwan Alfattani;Ahmed Ibrahim;Tasneem S. J. Darwish;Md Sahabul Alam;Halim Yanikomeroglu;Abbas Yongacoglu,"A High Altitude Platform Station (HAPS) is a network node that operates in the stratosphere at an of altitude around 20 km and is instrumental for providing communication services. Precipitated by technological innovations in the areas of autonomous avionics, array antennas, solar panel efficiency levels, and battery energy densities, and fueled by flourishing industry ecosystems, the HAPS has emerged as an indispensable component of next-generations of wireless networks. In this article, we provide a vision and framework for the HAPS networks of the future supported by a comprehensive and state-of-the-art literature review. We highlight the unrealized potential of HAPS systems and elaborate on their unique ability to serve metropolitan areas. The latest advancements and promising technologies in the HAPS energy and payload systems are discussed. The integration of the emerging Reconfigurable Smart Surface (RSS) technology in the communications payload of HAPS systems for providing a cost-effective deployment is proposed. A detailed overview of the radio resource management in HAPS systems is presented along with synergistic physical layer techniques, including Faster-Than-Nyquist (FTN) signaling. Numerous aspects of handoff management in HAPS systems are described. The notable contributions of Artificial Intelligence (AI) in HAPS, including machine learning in the design, topology management, handoff, and resource allocation aspects are emphasized. The extensive overview of the literature we provide is crucial for substantiating our vision that depicts the expected deployment opportunities and challenges in the next 10 years (next-generation networks), as well as in the subsequent 10 years (next-next-generation networks). △ Less","17 March, 2021",https://arxiv.org/pdf/2007.15088
Evaluation of Federated Learning in Phishing Email Detection,Chandra Thapa;Jun Wen Tang;Alsharif Abuadbba;Yansong Gao;Seyit Camtepe;Surya Nepal;Mahathir Almashor;Yifeng Zheng,"The use of Artificial Intelligence (AI) to detect phishing emails is primarily dependent on large-scale centralized datasets, which opens it up to a myriad of privacy, trust, and legal issues. Moreover, organizations are loathed to share emails, given the risk of leakage of commercially sensitive information. So, it is uncommon to obtain sufficient emails to train a global AI model efficiently. Accordingly, privacy-preserving distributed and collaborative machine learning, particularly Federated Learning (FL), is a desideratum. Already prevalent in the healthcare sector, questions remain regarding the effectiveness and efficacy of FL-based phishing detection within the context of multi-organization collaborations. To the best of our knowledge, the work herein is the first to investigate the use of FL in email anti-phishing. This paper builds upon a deep neural network model, particularly RNN and BERT for phishing email detection. It analyzes the FL-entangled learning performance under various settings, including balanced and asymmetrical data distribution. Our results corroborate comparable performance statistics of FL in phishing email detection to centralized learning for balanced datasets, and low organization counts. Moreover, we observe a variation in performance when increasing organizational counts. For a fixed total email dataset, the global RNN based model suffers by a 1.8% accuracy drop when increasing organizational counts from 2 to 10. In contrast, BERT accuracy rises by 0.6% when going from 2 to 5 organizations. However, if we allow increasing the overall email dataset with the introduction of new organizations in the FL framework, the organizational level performance is improved by achieving a faster convergence speed. Besides, FL suffers in its overall global model performance due to highly unstable outputs if the email dataset distribution is highly asymmetric. △ Less","21 May, 2021",https://arxiv.org/pdf/2007.13300
Few-Shot Bearing Fault Diagnosis Based on Model-Agnostic Meta-Learning,Shen Zhang;Fei Ye;Bingnan Wang;Thomas G. Habetler,"The rapid development of artificial intelligence and deep learning has provided many opportunities to further enhance the safety, stability, and accuracy of industrial Cyber-Physical Systems (CPS). As indispensable components to many mission-critical CPS assets and equipment, mechanical bearings need to be monitored to identify any trace of abnormal conditions. Most of the data-driven approaches applied to bearing fault diagnosis up-to-date are trained using a large amount of fault data collected a priori. In many practical applications, however, it can be unsafe and time-consuming to collect sufficient data samples for each fault category, making it challenging to train a robust classifier. In this paper, we propose a few-shot learning framework for bearing fault diagnosis based on model-agnostic meta-learning (MAML), which targets for training an effective fault classifier using limited data. In addition, it can leverage the training data and learn to identify new fault scenarios more efficiently. Case studies on the generalization to new artificial faults show that the proposed framework achieves an overall accuracy up to 25% higher than a Siamese network-based benchmark study. Finally, the robustness and the generalization capability of the proposed framework are further validated by applying it to identify real bearing damages using data from artificial damages, which compares favorably against 6 state-of-the-art few-shot learning algorithms using consistent test environments. △ Less","24 June, 2021",https://arxiv.org/pdf/2007.12851
Artificial Intelligence in the Creative Industries: A Review,Nantheera Anantrasirichai;David Bull,"This paper reviews the current state of the art in Artificial Intelligence (AI) technologies and applications in the context of the creative industries. A brief background of AI, and specifically Machine Learning (ML) algorithms, is provided including Convolutional Neural Network (CNNs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement Learning (DRL). We categorise creative applications into five groups related to how AI technologies are used: i) content creation, ii) information analysis, iii) content enhancement and post production workflows, iv) information extraction and enhancement, and v) data compression. We critically examine the successes and limitations of this rapidly advancing technology in each of these areas. We further differentiate between the use of AI as a creative tool and its potential as a creator in its own right. We foresee that, in the near future, machine learning-based AI will be adopted widely as a tool or collaborative assistant for creativity. In contrast, we observe that the successes of machine learning in domains with fewer constraints, where AI is the `creator', remain modest. The potential of AI (or its developers) to win awards for its original creations in competition with human creatives is also limited, based on contemporary technologies. We therefore conclude that, in the context of creative industries, maximum benefit from AI will be derived where its focus is human centric -- where it is designed to augment, rather than replace, human creativity. △ Less","2 July, 2021",https://arxiv.org/pdf/2007.12391
A Guideline on Pseudorandom Number Generation (PRNG) in the IoT,Peter Kietzmann;Thomas C. Schmidt;Matthias Wählisch,"Random numbers are an essential input to many functions on the Internet of Things (IoT). Common use cases of randomness range from low-level packet transmission to advanced algorithms of artificial intelligence as well as security and trust, which heavily rely on unpredictable random sources. In the constrained IoT, though, unpredictable random sources are a challenging desire due to limited resources, deterministic real-time operations, and frequent lack of a user interface. In this paper, we revisit the generation of randomness from the perspective of an IoT operating system (OS) that needs to support general purpose or crypto-secure random numbers. We analyse the potential attack surface, derive common requirements, and discuss the potentials and shortcomings of current IoT OSs. A systematic evaluation of current IoT hardware components and popular software generators based on well-established test suits and on experiments for measuring performance give rise to a set of clear recommendations on how to build such a random subsystem and which generators to use. △ Less","14 July, 2021",https://arxiv.org/pdf/2007.11839
Towards Quantum-Secure Authentication and Key Agreement via Abstract Multi-Agent Interaction,Ibrahim H. Ahmed;Josiah P. Hanna;Elliot Fosong;Stefano V. Albrecht,"Current methods for authentication and key agreement based on public-key cryptography are vulnerable to quantum computing. We propose a novel approach based on artificial intelligence research in which communicating parties are viewed as autonomous agents which interact repeatedly using their private decision models. Authentication and key agreement are decided based on the agents' observed behaviors during the interaction. The security of this approach rests upon the difficulty of modeling the decisions of interacting agents from limited observations, a problem which we conjecture is also hard for quantum computing. We release PyAMI, a prototype authentication and key agreement system based on the proposed method. We empirically validate our method for authenticating legitimate users while detecting different types of adversarial attacks. Finally, we show how reinforcement learning techniques can be used to train server models which effectively probe a client's decisions to achieve more sample-efficient authentication. △ Less","9 July, 2021",https://arxiv.org/pdf/2007.09327
Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study,Joseph Bae;Saarthak Kapse;Gagandeep Singh;Rishabh Gattu;Syed Ali;Neal Shah;Colin Marshall;Jonathan Pierce;Tej Phatak;Amit Gupta;Jeremy Green;Nikhil Madan;Prateek Prasanna,"We predict mechanical ventilation requirement and mortality using computational modeling of chest radiographs (CXRs) for coronavirus disease 2019 (COVID-19) patients. This two-center, retrospective study analyzed 530 deidentified CXRs from 515 COVID-19 patients treated at Stony Brook University Hospital and Newark Beth Israel Medical Center between March and August 2020. DL and machine learning classifiers to predict mechanical ventilation requirement and mortality were trained and evaluated using patient CXRs. A novel radiomic embedding framework was also explored for outcome prediction. All results are compared against radiologist grading of CXRs (zone-wise expert severity scores). Radiomic and DL classification models had mAUCs of 0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02 and 0.79+/-0.05 for mechanical ventilation requirement and mortality prediction, respectively. Combined classifiers using both radiomics and expert severity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each prediction task, demonstrating improvement over either artificial intelligence or radiologist interpretation alone. Our results also suggest instances where inclusion of radiomic features in DL improves model predictions, something that might be explored in other pathologies. The models proposed in this study and the prognostic information they provide might aid physician decision making and resource allocation during the COVID-19 pandemic. △ Less","1 July, 2021",https://arxiv.org/pdf/2007.08028
Lifelong Learning of Compositional Structures,Jorge A. Mendez;Eric Eaton,"A hallmark of human intelligence is the ability to construct self-contained chunks of knowledge and adequately reuse them in novel combinations for solving different yet structurally related problems. Learning such compositional structures has been a significant challenge for artificial systems, due to the combinatorial nature of the underlying search problem. To date, research into compositional learning has largely proceeded separately from work on lifelong or continual learning. We integrate these two lines of work to present a general-purpose framework for lifelong learning of compositional structures that can be used for solving a stream of related tasks. Our framework separates the learning process into two broad stages: learning how to best combine existing components in order to assimilate a novel problem, and learning how to adapt the set of existing components to accommodate the new problem. This separation explicitly handles the trade-off between the stability required to remember how to solve earlier tasks and the flexibility required to solve new tasks, as we show empirically in an extensive evaluation. △ Less","17 March, 2021",https://arxiv.org/pdf/2007.07732
VINNAS: Variational Inference-based Neural Network Architecture Search,Martin Ferianc;Hongxiang Fan;Miguel Rodrigues,"In recent years, neural architecture search (NAS) has received intensive scientific and industrial interest due to its capability of finding a neural architecture with high accuracy for various artificial intelligence tasks such as image classification or object detection. In particular, gradient-based NAS approaches have become one of the more popular approaches thanks to their computational efficiency during the search. However, these methods often experience a mode collapse, where the quality of the found architectures is poor due to the algorithm resorting to choosing a single operation type for the entire network, or stagnating at a local minima for various datasets or search spaces. To address these defects, we present a differentiable variational inference-based NAS method for searching sparse convolutional neural networks. Our approach finds the optimal neural architecture by dropping out candidate operations in an over-parameterised supergraph using variational dropout with automatic relevance determination prior, which makes the algorithm gradually remove unnecessary operations and connections without risking mode collapse. The evaluation is conducted through searching two types of convolutional cells that shape the neural network for classifying different image datasets. Our method finds diverse network cells, while showing state-of-the-art accuracy with up to almost 2 times fewer non-zero parameters. △ Less","14 January, 2021",https://arxiv.org/pdf/2007.06103
A Survey on Applications of Artificial Intelligence in Fighting Against COVID-19,Jianguo Chen;Kenli Li;Zhaolei Zhang;Keqin Li;Philip S. Yu,"The COVID-19 pandemic caused by the SARS-CoV-2 virus has spread rapidly worldwide, leading to a global outbreak. Most governments, enterprises, and scientific research institutions are participating in the COVID-19 struggle to curb the spread of the pandemic. As a powerful tool against COVID-19, artificial intelligence (AI) technologies are widely used in combating this pandemic. In this survey, we investigate the main scope and contributions of AI in combating COVID-19 from the aspects of disease detection and diagnosis, virology and pathogenesis, drug and vaccine development, and epidemic and transmission prediction. In addition, we summarize the available data and resources that can be used for AI-based COVID-19 research. Finally, the main challenges and potential directions of AI in fighting against COVID-19 are discussed. Currently, AI mainly focuses on medical image inspection, genomics, drug development, and transmission prediction, and thus AI still has great potential in this field. This survey presents medical and AI researchers with a comprehensive view of the existing and potential applications of AI technology in combating COVID-19 with the goal of inspiring researchers to continue to maximize the advantages of AI and big data to fight COVID-19. △ Less","11 March, 2021",https://arxiv.org/pdf/2007.02202
Expected Eligibility Traces,Hado van Hasselt;Sephora Madjiheurem;Matteo Hessel;David Silver;André Barreto;Diana Borsa,"The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state. In this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(λ). Finally, we discuss possible extensions and connections to related ideas, such as successor features. △ Less","8 February, 2021",https://arxiv.org/pdf/2007.01839
Volesti: Volume Approximation and Sampling for Convex Polytopes in R,Apostolos Chalkis;Vissarion Fisikopoulos,"Sampling from high dimensional distributions and volume approximation of convex bodies are fundamental operations that appear in optimization, finance, engineering, artificial intelligence and machine learning. In this paper we present volesti, an R package that provides efficient, scalable algorithms for volume estimation, uniform and Gaussian sampling from convex polytopes. volesti scales to hundreds of dimensions, handles efficiently three different types of polyhedra and provides non existing sampling routines to R. We demonstrate the power of volesti by solving several challenging problems using the R language. △ Less","4 September, 2021",https://arxiv.org/pdf/2007.01578
Deep Learning for Neuroimaging-based Diagnosis and Rehabilitation of Autism Spectrum Disorder: A Review,Marjane Khodatars;Afshin Shoeibi;Delaram Sadeghi;Navid Ghassemi;Mahboobeh Jafari;Parisa Moridian;Ali Khadem;Roohallah Alizadehsani;Assef Zare;Yinan Kong;Abbas Khosravi;Saeid Nahavandi;Sadiq Hussain;U. Rajendra Acharya;Michael Berk,"Accurate diagnosis of Autism Spectrum Disorder (ASD) followed by effective rehabilitation is essential for the management of this disorder. Artificial intelligence (AI) techniques can aid physicians to apply automatic diagnosis and rehabilitation procedures. AI techniques comprise traditional machine learning (ML) approaches and deep learning (DL) techniques. Conventional ML methods employ various feature extraction and classification techniques, but in DL, the process of feature extraction and classification is accomplished intelligently and integrally. DL methods for diagnosis of ASD have been focused on neuroimaging-based approaches. Neuroimaging techniques are non-invasive disease markers potentially useful for ASD diagnosis. Structural and functional neuroimaging techniques provide physicians substantial information about the structure (anatomy and structural connectivity) and function (activity and functional connectivity) of the brain. Due to the intricate structure and function of the brain, proposing optimum procedures for ASD diagnosis with neuroimaging data without exploiting powerful AI techniques like DL may be challenging. In this paper, studies conducted with the aid of DL networks to distinguish ASD are investigated. Rehabilitation tools provided for supporting ASD patients utilizing DL networks are also assessed. Finally, we will present important challenges in the automated detection and rehabilitation of ASD and propose some future works. △ Less","1 November, 2021",https://arxiv.org/pdf/2007.01285
Epileptic Seizures Detection Using Deep Learning Techniques: A Review,Afshin Shoeibi;Marjane Khodatars;Navid Ghassemi;Mahboobeh Jafari;Parisa Moridian;Roohallah Alizadehsani;Maryam Panahiazar;Fahime Khozeimeh;Assef Zare;Hossein Hosseini-Nejad;Abbas Khosravi;Amir F. Atiya;Diba Aminshahidi;Sadiq Hussain;Modjtaba Rouhani;Saeid Nahavandi;Udyavara Rajendra Acharya,"A variety of screening approaches have been proposed to diagnose epileptic seizures, using electroencephalography (EEG) and magnetic resonance imaging (MRI) modalities. Artificial intelligence encompasses a variety of areas, and one of its branches is deep learning (DL). Before the rise of DL, conventional machine learning algorithms involving feature extraction were performed. This limited their performance to the ability of those handcrafting the features. However, in DL, the extraction of features and classification are entirely automated. The advent of these techniques in many areas of medicine, such as in the diagnosis of epileptic seizures, has made significant advances. In this study, a comprehensive overview of works focused on automated epileptic seizure detection using DL techniques and neuroimaging modalities is presented. Various methods proposed to diagnose epileptic seizures automatically using EEG and MRI modalities are described. In addition, rehabilitation systems developed for epileptic seizures using DL have been analyzed, and a summary is provided. The rehabilitation tools include cloud computing techniques and hardware required for implementation of DL algorithms. The important challenges in accurate detection of automated epileptic seizures using DL with EEG and MRI modalities are discussed. The advantages and limitations in employing DL-based techniques for epileptic seizures diagnosis are presented. Finally, the most promising DL models proposed and possible future works on automated epileptic seizure detection are delineated. △ Less","29 May, 2021",https://arxiv.org/pdf/2007.01276
Multi-Task Variational Information Bottleneck,Weizhu Qian;Bowei Chen;Yichao Zhang;Guanghui Wen;Franck Gechter,"Multi-task learning (MTL) is an important subject in machine learning and artificial intelligence. Its applications to computer vision, signal processing, and speech recognition are ubiquitous. Although this subject has attracted considerable attention recently, the performance and robustness of the existing models to different tasks have not been well balanced. This article proposes an MTL model based on the architecture of the variational information bottleneck (VIB), which can provide a more effective latent representation of the input features for the downstream tasks. Extensive observations on three public data sets under adversarial attacks show that the proposed model is competitive to the state-of-the-art algorithms concerning the prediction accuracy. Experimental results suggest that combining the VIB and the task-dependent uncertainties is a very effective way to abstract valid information from the input features for accomplishing multiple tasks. △ Less","1 March, 2021",https://arxiv.org/pdf/2007.00339
Material Recognition for Automated Progress Monitoring using Deep Learning Methods,Hadi Mahami;Navid Ghassemi;Mohammad Tayarani Darbandy;Afshin Shoeibi;Sadiq Hussain;Farnad Nasirzadeh;Roohallah Alizadehsani;Darius Nahavandi;Abbas Khosravi;Saeid Nahavandi,"Recent advancements in Artificial intelligence, especially deep learning, has changed many fields irreversibly by introducing state of the art methods for automation. Construction monitoring has not been an exception; as a part of construction monitoring systems, material classification and recognition have drawn the attention of deep learning and machine vision researchers. However, to create production-ready systems, there is still a long path to cover. Real-world problems such as varying illuminations and reaching acceptable accuracies need to be addressed in order to create robust systems. In this paper, we have addressed these issues and reached a state of the art performance, i.e., 97.35% accuracy rate for this task. Also, a new dataset containing 1231 images of 11 classes taken from several construction sites is gathered and publicly published to help other researchers in this field. △ Less","16 April, 2021",https://arxiv.org/pdf/2006.16344
Show me the Way: Intrinsic Motivation from Demonstrations,Léonard Hussenot;Robert Dadashi;Matthieu Geist;Olivier Pietquin,"The study of exploration in the domain of decision making has a long history but remains actively debated. From the vast literature that addressed this topic for decades under various points of view (e.g., developmental psychology, experimental design, artificial intelligence), intrinsic motivation emerged as a concept that can practically be transferred to artificial agents. Especially, in the recent field of Deep Reinforcement Learning (RL), agents implement such a concept (mainly using a novelty argument) in the shape of an exploration bonus, added to the task reward, that encourages visiting the whole environment. This approach is supported by the large amount of theory on RL for which convergence to optimality assumes exhaustive exploration. Yet, Human Beings and mammals do not exhaustively explore the world and their motivation is not only based on novelty but also on various other factors (e.g., curiosity, fun, style, pleasure, safety, competition, etc.). They optimize for life-long learning and train to learn transferable skills in playgrounds without obvious goals. They also apply innate or learned priors to save time and stay safe. For these reasons, we propose to learn an exploration bonus from demonstrations that could transfer these motivations to an artificial agent with little assumptions about their rationale. Using an inverse RL approach, we show that complex exploration behaviors, reflecting different motivations, can be learnt and efficiently used by RL agents to solve tasks for which exhaustive exploration is prohibitive. △ Less","13 January, 2021",https://arxiv.org/pdf/2006.12917
Lessons Learned from Designing an AI-Enabled Diagnosis Tool for Pathologists,Hongyan Gu;Jingbin Huang;Lauren Hung;Xiang 'Anthony' Chen,"Despite the promises of data-driven artificial intelligence (AI), little is known about how we can bridge the gulf between traditional physician-driven diagnosis and a plausible future of medicine automated by AI. Specifically, how can we involve AI usefully in physicians' diagnosis workflow given that most AI is still nascent and error-prone (e.g., in digital pathology)? To explore this question, we first propose a series of collaborative techniques to engage human pathologists with AI given AI's capabilities and limitations, based on which we prototype Impetus - a tool where an AI takes various degrees of initiatives to provide various forms of assistance to a pathologist in detecting tumors from histological slides. We summarize observations and lessons learned from a study with eight pathologists and discuss recommendations for future work on human-centered medical AI systems. △ Less","10 February, 2021",https://arxiv.org/pdf/2006.12695
Where Responsible AI meets Reality: Practitioner Perspectives on Enablers for shifting Organizational Practices,Bogdana Rakova;Jingying Yang;Henriette Cramer;Rumman Chowdhury,"Large and ever-evolving technology companies continue to invest more time and resources to incorporate responsible Artificial Intelligence (AI) into production-ready systems to increase algorithmic accountability. This paper examines and seeks to offer a framework for analyzing how organizational culture and structure impact the effectiveness of responsible AI initiatives in practice. We present the results of semi-structured qualitative interviews with practitioners working in industry, investigating common challenges, ethical tensions, and effective enablers for responsible AI initiatives. Focusing on major companies developing or utilizing AI, we have mapped what organizational structures currently support or hinder responsible AI initiatives, what aspirational future processes and structures would best enable effective initiatives, and what key elements comprise the transition from current work practices to the aspirational future. △ Less","2 March, 2021",https://arxiv.org/pdf/2006.12358
"COVID-19 Vaccine Acceptance in the US and UK in the Early Phase of the Pandemic: AI-Generated Vaccines Hesitancy for Minors, and the Role of Governments",Gabriel Lima;Meeyoung Cha;Chiyoung Cha;Hyeyoung Hwang,"This study presents survey results of the public's willingness to get vaccinated against COVID-19 during an early phase of the pandemic and examines factors that could influence vaccine acceptance based on a between-subjects design. A representative quota sample of 572 adults in the US and UK participated in an online survey. First, the participants' medical use tendencies and initial vaccine acceptance were assessed; then, short vignettes were provided to evaluate their changes in attitude towards COVID-19 vaccines. For data analysis, ANOVA and post hoc pairwise comparisons were used. The participants were more reluctant to vaccinate their children than themselves and the elderly. The use of artificial intelligence (AI) in vaccine development did not influence vaccine acceptance. Vignettes that explicitly stated the high effectiveness of vaccines led to an increase in vaccine acceptance. Our study suggests public policies emphasizing the vaccine effectiveness against the virus could lead to higher vaccination rates. We also discuss the public's expectations of governments concerning vaccine safety and present a series of implications based on our findings. △ Less","23 June, 2021",https://arxiv.org/pdf/2006.08164
Unifying Regularisation Methods for Continual Learning,Frederik Benzing,"Continual Learning addresses the challenge of learning a number of different tasks sequentially. The goal of maintaining knowledge of earlier tasks without re-accessing them starkly conflicts with standard SGD training for artificial neural networks. An influential method to tackle this problem without storing old data are so-called regularisation approaches. They measure the importance of each parameter for solving a given task and subsequently protect important parameters from large changes. In the literature, three ways to measure parameter importance have been put forward and they have inspired a large body of follow-up work. Here, we present strong theoretical and empirical evidence that these three methods, Elastic Weight Consolidation (EWC), Synaptic Intelligence (SI) and Memory Aware Synapses (MAS), are surprisingly similar and are all linked to the same theoretical quantity. Concretely, we show that, despite stemming from very different motivations, both SI and MAS approximate the square root of the Fisher Information, with the Fisher being the theoretically justified basis of EWC. Moreover, we show that for SI the relation to the Fisher -- and in fact its performance -- is due to a previously unknown bias. On top of uncovering unknown similarities and unifying regularisation approaches, we also demonstrate that our insights enable practical performance improvements for large batch training. △ Less","3 February, 2021",https://arxiv.org/pdf/2006.06357
Can artificial intelligence (AI) be used to accurately detect tuberculosis (TB) from chest X-rays? An evaluation of five AI products for TB screening and triaging in a high TB burden setting,Zhi Zhen Qin;Shahriar Ahmed;Mohammad Shahnewaz Sarker;Kishor Paul;Ahammad Shafiq Sikder Adel;Tasneem Naheyan;Rachael Barrett;Sayera Banu;Jacob Creswell,"Artificial intelligence (AI) products can be trained to recognize tuberculosis (TB)-related abnormalities on chest radiographs. Various AI products are available commercially, yet there is lack of evidence on how their performance compared with each other and with radiologists. We evaluated five AI software products for screening and triaging TB using a large dataset that had not been used to train any commercial AI products. Individuals (>=15 years old) presenting to three TB screening centers in Dhaka, Bangladesh, were recruited consecutively. All CXR were read independently by a group of three Bangladeshi registered radiologists and five commercial AI products: CAD4TB (v7), InferReadDR (v2), Lunit INSIGHT CXR (v4.9.0), JF CXR-1 (v2), and qXR (v3). All five AI products significantly outperformed the Bangladeshi radiologists. The areas under the receiver operating characteristic curve are qXR: 90.81% (95% CI:90.33-91.29%), CAD4TB: 90.34% (95% CI:89.81-90.87), Lunit INSIGHT CXR: 88.61% (95% CI:88.03%-89.20%), InferReadDR: 84.90% (95% CI: 84.27-85.54%) and JF CXR-1: 84.89% (95% CI:84.26-85.53%). Only qXR met the TPP with 74.3% specificity at 90% sensitivity. Five AI algorithms can reduce the number of Xpert tests required by 50%, while maintaining a sensitivity above 90%. All AI algorithms performed worse among the older age and people with prior TB history. AI products can be highly accurate and useful screening and triage tools for TB detection in high burden regions and outperform human readers. △ Less","28 May, 2021",https://arxiv.org/pdf/2006.05509
The Tragedy of the AI Commons,Travis LaCroix;Aydin Mohseni,"Policy and guideline proposals for ethical artificial-intelligence research have proliferated in recent years. These are supposed to guide the socially-responsible development of AI for the common good. However, there typically exist incentives for non-cooperation (i.e., non-adherence to such policies and guidelines); and, these proposals often lack effective mechanisms to enforce their own normative claims. The situation just described constitutes a social dilemma; namely, a situation where no one has an individual incentive to cooperate, though mutual cooperation would lead to the best outcome for all involved. In this paper, we use stochastic evolutionary game dynamics to model this social dilemma in the context of the ethical development of artificial intelligence. This formalism allows us to isolate variables that may be intervened upon, thus providing actionable suggestions for increased cooperation amongst numerous stakeholders in AI. Our results show how stochastic effects can help make cooperation viable in such a scenario. They suggest that coordination for a common good should be attempted in smaller groups in which the cost for cooperation is low, and the perceived risk of failure is high. This provides insight into the conditions under which we should expect such ethics proposals to be successful with regard to their scope, scale, and content. △ Less","18 January, 2021",https://arxiv.org/pdf/2006.05203
Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms,Akshat Pandey;Aylin Caliskan,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications. The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations. △ Less","3 May, 2021",https://arxiv.org/pdf/2006.04599
Biometric Quality: Review and Application to Face Recognition with FaceQnet,Javier Hernandez-Ortega;Javier Galbally;Julian Fierrez;Laurent Beslay,"""The output of a computerised system can only be as accurate as the information entered into it."" This rather trivial statement is the basis behind one of the driving concepts in biometric recognition: biometric quality. Quality is nowadays widely regarded as the number one factor responsible for the good or bad performance of automated biometric systems. It refers to the ability of a biometric sample to be used for recognition purposes and produce consistent, accurate, and reliable results. Such a subjective term is objectively estimated by the so-called biometric quality metrics. These algorithms play nowadays a pivotal role in the correct functioning of systems, providing feedback to the users and working as invaluable audit tools. In spite of their unanimously accepted relevance, some of the most used and deployed biometric characteristics are lacking behind in the development of these methods. This is the case of face recognition. After a gentle introduction to the general topic of biometric quality and a review of past efforts in face quality metrics, in the present work, we address the need for better face quality metrics by developing FaceQnet. FaceQnet is a novel open-source face quality assessment tool, inspired and powered by deep learning technology, which assigns a scalar quality measure to facial images, as prediction of their recognition accuracy. Two versions of FaceQnet have been thoroughly evaluated both in this work and also independently by NIST, showing the soundness of the approach and its competitiveness with respect to current state-of-the-art metrics. Even though our work is presented here particularly in the framework of face biometrics, the proposed methodology for building a fully automated quality metric can be very useful and easily adapted to other artificial intelligence tasks. △ Less","28 February, 2021",https://arxiv.org/pdf/2006.03298
"Least k
th-Order and Rényi Generative Adversarial Networks",Himesh Bhatia;William Paul;Fady Alajaji;Bahman Gharesifard;Philippe Burlina,"We investigate the use of parametrized families of information-theoretic measures to generalize the loss functions of generative adversarial networks (GANs) with the objective of improving performance. A new generator loss function, called least kth-order GAN (LkGAN), is first introduced, generalizing the least squares GANs (LSGANs) by using a kth order absolute error distortion measure with k \geq 1 (which recovers the LSGAN loss function when k=2). It is shown that minimizing this generalized loss function under an (unconstrained) optimal discriminator is equivalent to minimizing the kth-order Pearson-Vajda divergence. Another novel GAN generator loss function is next proposed in terms of Rényi cross-entropy functionals with order α>0, α\neq 1. It is demonstrated that this Rényi-centric generalized loss function, which provably reduces to the original GAN loss function as α\to1, preserves the equilibrium point satisfied by the original GAN based on the Jensen-Rényi divergence, a natural extension of the Jensen-Shannon divergence. Experimental results indicate that the proposed loss functions, applied to the MNIST and CelebA datasets, under both DCGAN and StyleGAN architectures, confer performance benefits by virtue of the extra degrees of freedom provided by the parameters k and α, respectively. More specifically, experiments show improvements with regard to the quality of the generated images as measured by the Fréchet Inception Distance (FID) score and training stability. While it was applied to GANs in this study, the proposed approach is generic and can be used in other applications of information theory to deep learning, e.g., the issues of fairness or privacy in artificial intelligence. △ Less","11 March, 2021",https://arxiv.org/pdf/2006.02479
Integrating Deep Learning into CAD/CAE System: Generative Design and Evaluation of 3D Conceptual Wheel,Soyoung Yoo;Sunghee Lee;Seongsin Kim;Kwang Hyeon Hwang;Jong Ho Park;Namwoo Kang,"Engineering design research integrating artificial intelligence (AI) into computer-aided design (CAD) and computer-aided engineering (CAE) is actively being conducted. This study proposes a deep learning-based CAD/CAE framework in the conceptual design phase that automatically generates 3D CAD designs and evaluates their engineering performance. The proposed framework comprises seven stages: (1) 2D generative design, (2) dimensionality reduction, (3) design of experiment in latent space, (4) CAD automation, (5) CAE automation, (6) transfer learning, and (7) visualization and analysis. The proposed framework is demonstrated through a road wheel design case study and indicates that AI can be practically incorporated into an end-use product design project. Engineers and industrial designers can jointly review a large number of generated 3D CAD models by using this framework along with the engineering performance results estimated by AI and find conceptual design candidates for the subsequent detailed design stage. △ Less","13 June, 2021",https://arxiv.org/pdf/2006.02138
Exploring Thermal Images for Object Detection in Underexposure Regions for Autonomous Driving,Farzeen Munir;Shoaib Azam;Muhammd Aasim Rafique;Ahmad Muqeem Sheri;Moongu Jeon;Witold Pedrycz,"Underexposure regions are vital to construct a complete perception of the surroundings for safe autonomous driving. The availability of thermal cameras has provided an essential alternate to explore regions where other optical sensors lack in capturing interpretable signals. A thermal camera captures an image using the heat difference emitted by objects in the infrared spectrum, and object detection in thermal images becomes effective for autonomous driving in challenging conditions. Although object detection in the visible spectrum domain imaging has matured, thermal object detection lacks effectiveness. A significant challenge is scarcity of labeled data for the thermal domain which is desiderata for SOTA artificial intelligence techniques. This work proposes a domain adaptation framework which employs a style transfer technique for transfer learning from visible spectrum images to thermal images. The framework uses a generative adversarial network (GAN) to transfer the low-level features from the visible spectrum domain to the thermal domain through style consistency. The efficacy of the proposed method of object detection in thermal images is evident from the improved results when used styled images from publicly available thermal image datasets (FLIR ADAS and KAIST Multi-Spectral). △ Less","3 May, 2021",https://arxiv.org/pdf/2006.00821
Multimodal Feature Fusion and Knowledge-Driven Learning via Experts Consult for Thyroid Nodule Classification,Danilo Avola;Luigi Cinque;Alessio Fagioli;Sebastiano Filetti;Giorgio Grani;Emanuele Rodolà,"Computer-aided diagnosis (CAD) is becoming a prominent approach to assist clinicians spanning across multiple fields. These automated systems take advantage of various computer vision (CV) procedures, as well as artificial intelligence (AI) techniques, to formulate a diagnosis of a given image, e.g., computed tomography and ultrasound. Advances in both areas (CV and AI) are enabling ever increasing performances of CAD systems, which can ultimately avoid performing invasive procedures such as fine-needle aspiration. In this study, a novel end-to-end knowledge-driven classification framework is presented. The system focuses on multimodal data generated by thyroid ultrasonography, and acts as a CAD system by providing a thyroid nodule classification into the benign and malignant categories. Specifically, the proposed system leverages cues provided by an ensemble of experts to guide the learning phase of a densely connected convolutional network (DenseNet). The ensemble is composed by various networks pretrained on ImageNet, including AlexNet, ResNet, VGG, and others. The previously computed multimodal feature parameters are used to create ultrasonography domain experts via transfer learning, decreasing, moreover, the number of samples required for training. To validate the proposed method, extensive experiments were performed, providing detailed performances for both the experts ensemble and the knowledge-driven DenseNet. As demonstrated by the results, the proposed system achieves relevant performances in terms of qualitative metrics for the thyroid nodule classification task, thus resulting in a great asset when formulating a diagnosis. △ Less","25 October, 2021",https://arxiv.org/pdf/2005.14117
"Design of a dynamic and self adapting system, supported with artificial intelligence, machine learning and real time intelligence for predictive cyber risk analytics in extreme environments, cyber risk in the colonisation of Mars",Petar Radanliev;David De Roure;Kevin Page;Max Van Kleek;Omar Santos;La Treall Maddox;Pete Burnap;Eirini Anthi;Carsten Maple,"Multiple governmental agencies and private organisations have made commitments for the colonisation of Mars. Such colonisation requires complex systems and infrastructure that could be very costly to repair or replace in cases of cyber attacks. This paper surveys deep learning algorithms, IoT cyber security and risk models, and established mathematical formulas to identify the best approach for developing a dynamic and self adapting system for predictive cyber risk analytics supported with Artificial Intelligence and Machine Learning and real time intelligence in edge computing. The paper presents a new mathematical approach for integrating concepts for cognition engine design, edge computing and Artificial Intelligence and Machine Learning to automate anomaly detection. This engine instigates a step change by applying Artificial Intelligence and Machine Learning embedded at the edge of IoT networks, to deliver safe and functional real time intelligence for predictive cyber risk analytics. This will enhance capacities for risk analytics and assists in the creation of a comprehensive and systematic understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when Artificial Intelligence and Machine Learning technologies are migrated to the periphery of the internet and into local IoT networks. △ Less","11 February, 2021",https://arxiv.org/pdf/2005.12150
Dynamic Cognition Applied to Value Learning in Artificial Intelligence,Nythamar de Oliveira;Nicholas Kluge Corrêa,"Experts in Artificial Intelligence (AI) development predict that advances in the development of intelligent systems and agents will reshape vital areas in our society. Nevertheless, if such an advance isn't done with prudence, it can result in negative outcomes for humanity. For this reason, several researchers in the area are trying to develop a robust, beneficial, and safe concept of artificial intelligence. Currently, several of the open problems in the field of AI research arise from the difficulty of avoiding unwanted behaviors of intelligent agents, and at the same time specifying what we want such systems to do. It is of utmost importance that artificial intelligent agents have their values aligned with human values, given the fact that we cannot expect an AI to develop our moral preferences simply because of its intelligence, as discussed in the Orthogonality Thesis. Perhaps this difficulty comes from the way we are addressing the problem of expressing objectives, values, and ends, using representational cognitive methods. A solution to this problem would be the dynamic cognitive approach proposed by Dreyfus, whose phenomenological philosophy defends that the human experience of being-in-the-world cannot be represented by the symbolic or connectionist cognitive methods. A possible approach to this problem would be to use theoretical models such as SED (situated embodied dynamics) to address the values learning problem in AI. △ Less","23 August, 2021",https://arxiv.org/pdf/2005.05538
A Survey of Algorithms for Black-Box Safety Validation of Cyber-Physical Systems,Anthony Corso;Robert J. Moss;Mark Koren;Ritchie Lee;Mykel J. Kochenderfer,"Autonomous cyber-physical systems (CPS) can improve safety and efficiency for safety-critical applications, but require rigorous testing before deployment. The complexity of these systems often precludes the use of formal verification and real-world testing can be too dangerous during development. Therefore, simulation-based techniques have been developed that treat the system under test as a black box operating in a simulated environment. Safety validation tasks include finding disturbances in the environment that cause the system to fail (falsification), finding the most-likely failure, and estimating the probability that the system fails. Motivated by the prevalence of safety-critical artificial intelligence, this work provides a survey of state-of-the-art safety validation techniques for CPS with a focus on applied algorithms and their modifications for the safety validation problem. We present and discuss algorithms in the domains of optimization, path planning, reinforcement learning, and importance sampling. Problem decomposition techniques are presented to help scale algorithms to large state spaces, which are common for CPS. A brief overview of safety-critical applications is given, including autonomous vehicles and aircraft collision avoidance systems. Finally, we present a survey of existing academic and commercially available safety validation tools. △ Less","14 October, 2021",https://arxiv.org/pdf/2005.02979
A Deep Convolutional Neural Network for COVID-19 Detection Using Chest X-Rays,Pedro R. A. S. Bassi;Romis Attux,"Purpose: We present image classifiers based on Dense Convolutional Networks and transfer learning to classify chest X-ray images according to three labels: COVID-19, pneumonia and normal. Methods: We fine-tuned neural networks pretrained on ImageNet and applied a twice transfer learning approach, using NIH ChestX-ray14 dataset as an intermediate step. We also suggested a novelty called output neuron keeping, which changes the twice transfer learning technique. In order to clarify the modus operandi of the models, we used Layer-wise Relevance Propagation (LRP) to generate heatmaps. Results: We were able to reach test accuracy of 100% on our test dataset. Twice transfer learning and output neuron keeping showed promising results improving performances, mainly in the beginning of the training process. Although LRP revealed that words on the X-rays can influence the networks' predictions, we discovered this had only a very small effect on accuracy. Conclusion: Although clinical studies and larger datasets are still needed to further ensure good generalization, the state-of-the-art performances we achieved show that, with the help of artificial intelligence, chest X-rays can become a cheap and accurate auxiliary method for COVID-19 diagnosis. Heatmaps generated by LRP improve the interpretability of the deep neural networks and indicate an analytical path for future research on diagnosis. Twice transfer learning with output neuron keeping improved performances. △ Less","12 January, 2021",https://arxiv.org/pdf/2005.01578
Bayesian Entailment Hypothesis: How Brains Implement Monotonic and Non-monotonic Reasoning,Hiroyuki Kido,"Recent success of Bayesian methods in neuroscience and artificial intelligence gives rise to the hypothesis that the brain is a Bayesian machine. Since logic, as the laws of thought, is a product and practice of the human brain, it leads to another hypothesis that there is a Bayesian algorithm and data-structure for logical reasoning. In this paper, we give a Bayesian account of entailment and characterize its abstract inferential properties. The Bayesian entailment is shown to be a monotonic consequence relation in an extreme case. In general, it is a sort of non-monotonic consequence relation without Cautious monotony or Cut. The preferential entailment, which is a representative non-monotonic consequence relation, is shown to be maximum a posteriori entailment, which is an approximation of the Bayesian entailment. We finally discuss merits of our proposals in terms of encoding preferences on defaults, handling change and contradiction, and modeling human entailment. △ Less","27 January, 2021",https://arxiv.org/pdf/2005.00961
A Framework for Enhancing Deep Neural Networks Against Adversarial Malware,Deqiang Li;Qianmu Li;Yanfang Ye;Shouhuai Xu,"Machine learning-based malware detection is known to be vulnerable to adversarial evasion attacks. The state-of-the-art is that there are no effective defenses against these attacks. As a response to the adversarial malware classification challenge organized by the MIT Lincoln Lab and associated with the AAAI-19 Workshop on Artificial Intelligence for Cyber Security (AICS'2019), we propose six guiding principles to enhance the robustness of deep neural networks. Some of these principles have been scattered in the literature, but the others are introduced in this paper for the first time. Under the guidance of these six principles, we propose a defense framework to enhance the robustness of deep neural networks against adversarial malware evasion attacks. By conducting experiments with the Drebin Android malware dataset, we show that the framework can achieve a 98.49\% accuracy (on average) against grey-box attacks, where the attacker knows some information about the defense and the defender knows some information about the attack, and an 89.14% accuracy (on average) against the more capable white-box attacks, where the attacker knows everything about the defense and the defender knows some information about the attack. The framework wins the AICS'2019 challenge by achieving a 76.02% accuracy, where neither the attacker (i.e., the challenge organizer) knows the framework or defense nor we (the defender) know the attacks. This gap highlights the importance of knowing about the attack. △ Less","15 January, 2021",https://arxiv.org/pdf/2004.07919
Prune2Edge: A Multi-Phase Pruning Pipelines to Deep Ensemble Learning in IIoT,Besher Alhalabi;Mohamed Gaber;Shadi Basurra,"Most recently, with the proliferation of IoT devices, computational nodes in manufacturing systems IIoT(Industrial-Internet-of-things) and the lunch of 5G networks, there will be millions of connected devices generating a massive amount of data. In such an environment, the controlling systems need to be intelligent enough to deal with a vast amount of data to detect defects in a real-time process. Driven by such a need, artificial intelligence models such as deep learning have to be deployed into IIoT systems. However, learning and using deep learning models are computationally expensive, so an IoT device with limited computational power could not run such models. To tackle this issue, edge intelligence had emerged as a new paradigm towards running Artificial Intelligence models on edge devices. Although a considerable amount of studies have been proposed in this area, the research is still in the early stages. In this paper, we propose a novel edge-based multi-phase pruning pipelines to ensemble learning on IIoT devices. In the first phase, we generate a diverse ensemble of pruned models, then we apply integer quantisation, next we prune the generated ensemble using a clustering-based technique. Finally, we choose the best representative from each generated cluster to be deployed to a distributed IoT environment. On CIFAR-100 and CIFAR-10, our proposed approach was able to outperform the predictability levels of a baseline model (up to 7%), more importantly, the generated learners have small sizes (up to 90% reduction in the model size) that minimise the required computational capabilities to make an inference on the resource-constraint devices. △ Less","21 January, 2021",https://arxiv.org/pdf/2004.04710
Model-based actor-critic: GAN (model generator) + DRL (actor-critic) => AGI,Aras Dargazany,"Our effort is toward unifying GAN and DRL algorithms into a unifying AI model (AGI or general-purpose AI or artificial general intelligence which has general-purpose applications to: (A) offline learning (of stored data) like GAN in (un/semi-/fully-)SL setting such as big data analytics (mining) and visualization; (B) online learning (of real or simulated devices) like DRL in RL setting (with/out environment reward) such as (real or simulated) robotics and control; Our core proposal is adding an (generative/predictive) environment model to the actor-critic (model-free) architecture which results in a model-based actor-critic architecture with temporal-differencing (TD) error and an episodic memory. The proposed AI model is similar to (model-free) DDPG and therefore it's called model-based DDPG. To evaluate it, we compare it with (model-free) DDPG by applying them both to a variety (wide range) of independent simulated robotic and control task environments in OpenAI Gym and Unity Agents. Our initial limited experiments show that DRL and GAN in model-based actor-critic results in an incremental goal-driven intellignce required to solve each task with similar performance to (model-free) DDPG. Our future focus is to investigate the proposed AI model potential to: (A) unify DRL field inside AI by producing competitive performance compared to the best of model-based (PlaNet) and model-free (D4PG) approaches; (B) bridge the gap between AI and robotics communities by solving the important problem of reward engineering with learning the reward function by demonstration. △ Less","20 April, 2021",https://arxiv.org/pdf/2004.04574
ConsciousControlFlow(CCF): A Demonstration for conscious Artificial Intelligence,Hongzhi Wang;Bozhou Chen;Yueyang Xu;Kaixin Zhang;Shengwen Zheng,"In this demo, we present ConsciousControlFlow(CCF), a prototype system to demonstrate conscious Artificial Intelligence (AI). The system is based on the computational model for consciousness and the hierarchy of needs. CCF supports typical scenarios to show the behaviors and the mental activities of conscious AI. We demonstrate that CCF provides a useful tool for effective machine consciousness demonstration and human behavior study assistance. △ Less","5 February, 2021",https://arxiv.org/pdf/2004.04376
"Dendrite Net: A White-Box Module for Classification, Regression, and System Identification",Gang Liu;Jing Wang,"The simulation of biological dendrite computations is vital for the development of artificial intelligence (AI). This paper presents a basic machine learning algorithm, named Dendrite Net or DD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's main concept is that the algorithm can recognize this class after learning, if the output's logical expression contains the corresponding class's logical relationship among inputs (and\backslashor\backslashnot). Experiments and main results: DD, a white-box machine learning algorithm, showed excellent system identification performance for the black-box system. Secondly, it was verified by nine real-world applications that DD brought better generalization capability relative to MLP architecture that imitated neurons' cell body (Cell body Net) for regression. Thirdly, by MNIST and FASHION-MNIST datasets, it was verified that DD showed higher testing accuracy under greater training loss than Cell body Net for classification. The number of modules can effectively adjust DD's logical expression capacity, which avoids over-fitting and makes it easy to get a model with outstanding generalization capability. Finally, repeated experiments in MATLAB and PyTorch (Python) demonstrated that DD was faster than Cell body Net both in epoch and forward-propagation. The main contribution of this paper is the basic machine learning algorithm (DD) with a white-box attribute, controllable precision for better generalization capability, and lower computational complexity. Not only can DD be used for generalized engineering, but DD has vast development potential as a module for deep learning. DD code is available at GitHub: https://github.com/liugang1234567/Gang-neuron . △ Less","18 November, 2021",https://arxiv.org/pdf/2004.03955
Classical and quantum regression analysis for the optoelectronic performance of NTCDA/p-Si UV photodiode,Ahmed M. El-Mahalawy;Kareem H. El-Safty,"Due to the pivotal role of UV photodiodes in many technological applications in tandem with the high efficiency achieved by machine learning techniques in regression and classification problems, different artificial intelligence techniques are adopted model the performance of organic/inorganic heterojunction UV photodiode. Herein, the performance of a fabricated Au/NTCDA/p-Si/Al photodiode was explained in details and showed an excellent responsivity, and detectivity for UV light of intensities ranges from 20 to 80 {mW/cm^2}. The fabricated photodiodes exhibited a linear current-irradiance relationship under illumination up to 65 {mW/cm^2}. It also exhibits good response times of {t_{rise} = 408} ms and {t_{fall} = 490} ms. Furthermore, we have not only fitted the characteristic I-V curve but also evaluated three classical algorithms; k-nearest neighbour, artificial neural network, and genetic programming besides using a quantum neural network to predict the behaviour of the fabricated device. The models have achieved outstanding results and managed to capture the trend of the target values. The Quantum Neural Network has been used for the first time to model the photodiode. The models can be used instead of repeating the fabrication process. This means a reduction in cost and manufacturing time. △ Less","11 April, 2021",https://arxiv.org/pdf/2004.01257
Architecture Disentanglement for Deep Neural Networks,Jie Hu;Liujuan Cao;Qixiang Ye;Tong Tong;ShengChuan Zhang;Ke Li;Feiyue Huang;Rongrong Ji;Ling Shao,"Understanding the inner workings of deep neural networks (DNNs) is essential to provide trustworthy artificial intelligence techniques for practical applications. Existing studies typically involve linking semantic concepts to units or layers of DNNs, but fail to explain the inference process. In this paper, we introduce neural architecture disentanglement (NAD) to fill the gap. Specifically, NAD learns to disentangle a pre-trained DNN into sub-architectures according to independent tasks, forming information flows that describe the inference processes. We investigate whether, where, and how the disentanglement occurs through experiments conducted with handcrafted and automatically-searched network architectures, on both object-based and scene-based datasets. Based on the experimental results, we present three new findings that provide fresh insights into the inner logic of DNNs. First, DNNs can be divided into sub-architectures for independent tasks. Second, deeper layers do not always correspond to higher semantics. Third, the connection type in a DNN affects how the information flows across layers, leading to different disentanglement behaviors. With NAD, we further explain why DNNs sometimes give wrong predictions. Experimental results show that misclassified images have a high probability of being assigned to task sub-architectures similar to the correct ones. Code will be available at: https://github.com/hujiecpp/NAD. △ Less","23 March, 2021",https://arxiv.org/pdf/2003.13268
InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining,Junyang Lin;An Yang;Yichang Zhang;Jie Liu;Jingren Zhou;Hongxia Yang,"Multi-modal pretraining for learning high-level multi-modal representation is a further step towards deep learning and artificial intelligence. In this work, we propose a novel model, namely InterBERT (BERT for Interaction), which is the first model of our series of multimodal pretraining methods M6 (MultiModality-to-MultiModality Multitask Mega-transformer). The model owns strong capability of modeling interaction between the information flows of different modalities. The single-stream interaction module is capable of effectively processing information of multiple modalilties, and the two-stream module on top preserves the independence of each modality to avoid performance downgrade in single-modal tasks. We pretrain the model with three pretraining tasks, including masked segment modeling (MSM), masked region modeling (MRM) and image-text matching (ITM); and finetune the model on a series of vision-and-language downstream tasks. Experimental results demonstrate that InterBERT outperforms a series of strong baselines, including the most recent multi-modal pretraining methods, and the analysis shows that MSM and MRM are effective for pretraining and our method can achieve performances comparable to BERT in single-modal tasks. Besides, we propose a large-scale dataset for multi-modal pretraining in Chinese, and we develop the Chinese InterBERT which is the first Chinese multi-modal pretrained model. We pretrain the Chinese InterBERT on our proposed dataset of 3.1M image-text pairs from the mobile Taobao, the largest Chinese e-commerce platform. We finetune the model for text-based image retrieval, and recently we deployed the model online for topic-based recommendation. △ Less","22 April, 2021",https://arxiv.org/pdf/2003.13198
Mapping the Landscape of Artificial Intelligence Applications against COVID-19,Joseph Bullock;Alexandra Luccioni;Katherine Hoffmann Pham;Cynthia Sin Nga Lam;Miguel Luengo-Oroz,"COVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Machine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID-19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets, tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international cooperation to maximize the potential of AI in this and future pandemics. △ Less","11 January, 2021",https://arxiv.org/pdf/2003.11336
The Conflict Between People's Urge to Punish AI and Legal Systems,Gabriel Lima;Meeyoung Cha;Chihyung Jeon;Kyungsin Park,"Regulating artificial intelligence (AI) has become necessary in light of its deployment in high-risk scenarios. This paper explores the proposal to extend legal personhood to AI and robots, which had not yet been examined through the lens of the general public. We present two studies (N = 3,559) to obtain people's views of electronic legal personhood vis-à-vis existing liability models. Our study reveals people's desire to punish automated agents even though these entities are not recognized any mental state. Furthermore, people did not believe automated agents' punishment would fulfill deterrence nor retribution and were unwilling to grant them legal punishment preconditions, namely physical independence and assets. Collectively, these findings suggest a conflict between the desire to punish automated agents and its perceived impracticability. We conclude by discussing how future design and legal decisions may influence how the public reacts to automated agents' wrongdoings. △ Less","10 November, 2021",https://arxiv.org/pdf/2003.06507
Developing and Operating Artificial Intelligence Models in Trustworthy Autonomous Systems,Silverio Martínez-Fernández;Xavier Franch;Andreas Jedlitschka;Marc Oriol;Adam Trendowicz,"Companies dealing with Artificial Intelligence (AI) models in Autonomous Systems (AS) face several problems, such as users' lack of trust in adverse or unknown conditions, gaps between software engineering and AI model development, and operation in a continuously changing operational environment. This work-in-progress paper aims to close the gap between the development and operation of trustworthy AI-based AS by defining an approach that coordinates both activities. We synthesize the main challenges of AI-based AS in industrial settings. We reflect on the research efforts required to overcome these challenges and propose a novel, holistic DevOps approach to put it into practice. We elaborate on four research directions: (a) increased users' trust by monitoring operational AI-based AS and identifying self-adaptation needs in critical situations; (b) integrated agile process for the development and evolution of AI models and AS; (c) continuous deployment of different context-specific instances of AI models in a distributed setting of AS; and (d) holistic DevOps-based lifecycle for AI-based AS. △ Less","23 April, 2021",https://arxiv.org/pdf/2003.05434
Transformation Importance with Applications to Cosmology,Chandan Singh;Wooseok Ha;Francois Lanusse;Vanessa Boehm;Jia Liu;Bin Yu,"Machine learning lies at the heart of new possibilities for scientific discovery, knowledge generation, and artificial intelligence. Its potential benefits to these fields requires going beyond predictive accuracy and focusing on interpretability. In particular, many scientific problems require interpretations in a domain-specific interpretable feature space (e.g. the frequency domain) whereas attributions to the raw features (e.g. the pixel space) may be unintelligible or even misleading. To address this challenge, we propose TRIM (TRansformation IMportance), a novel approach which attributes importances to features in a transformed space and can be applied post-hoc to a fully trained model. TRIM is motivated by a cosmological parameter estimation problem using deep neural networks (DNNs) on simulated data, but it is generally applicable across domains/models and can be combined with any local interpretation method. In our cosmology example, combining TRIM with contextual decomposition shows promising results for identifying which frequencies a DNN uses, helping cosmologists to understand and validate that the model learns appropriate physical features rather than simulation artifacts. △ Less","14 June, 2021",https://arxiv.org/pdf/2003.01926
Enabling AI in Future Wireless Networks: A Data Life Cycle Perspective,Dinh C. Nguyen;Peng Cheng;Ming Ding;David Lopez-Perez;Pubudu N. Pathirana;Jun Li;Aruna Seneviratne;Yonghui Li;H. Vincent Poor,"Recent years have seen rapid deployment of mobile computing and Internet of Things (IoT) networks, which can be mostly attributed to the increasing communication and sensing capabilities of wireless systems. Big data analysis, pervasive computing, and eventually artificial intelligence (AI) are envisaged to be deployed on top of the IoT and create a new world featured by data-driven AI. In this context, a novel paradigm of merging AI and wireless communications, called Wireless AI that pushes AI frontiers to the network edge, is widely regarded as a key enabler for future intelligent network evolution. To this end, we present a comprehensive survey of the latest studies in wireless AI from the data-driven perspective. Specifically, we first propose a novel Wireless AI architecture that covers five key data-driven AI themes in wireless networks, including Sensing AI, Network Device AI, Access AI, User Device AI and Data-provenance AI. Then, for each data-driven AI theme, we present an overview on the use of AI approaches to solve the emerging data-related problems and show how AI can empower wireless network functionalities. Particularly, compared to the other related survey papers, we provide an in-depth discussion on the Wireless AI applications in various data-driven domains wherein AI proves extremely useful for wireless network design and optimization. Finally, research challenges and future visions are also discussed to spur further research in this promising area. △ Less","27 April, 2021",https://arxiv.org/pdf/2003.00866
Assessment Modeling: Fundamental Pre-training Tasks for Interactive Educational Systems,Youngduck Choi;Youngnam Lee;Junghyun Cho;Jineon Baek;Dongmin Shin;Hangyeol Yu;Yugeun Shim;Seewoo Lee;Jonghun Shin;Chan Bae;Byungsoo Kim;Jaewe Heo,"Like many other domains in Artificial Intelligence (AI), there are specific tasks in the field of AI in Education (AIEd) for which labels are scarce and expensive, such as predicting exam score or review correctness. A common way of circumventing label-scarce problems is pre-training a model to learn representations of the contents of learning items. However, such methods fail to utilize the full range of student interaction data available and do not model student learning behavior. To this end, we propose Assessment Modeling, a class of fundamental pre-training tasks for general interactive educational systems. An assessment is a feature of student-system interactions which can serve as a pedagogical evaluation. Examples include the correctness and timeliness of a student's answer. Assessment Modeling is the prediction of assessments conditioned on the surrounding context of interactions. Although it is natural to pre-train on interactive features available in large amounts, limiting the prediction targets to assessments focuses the tasks' relevance to the label-scarce educational problems and reduces less-relevant noise. While the effectiveness of different combinations of assessments is open for exploration, we suggest Assessment Modeling as a first-order guiding principle for selecting proper pre-training tasks for label-scarce educational problems. △ Less","28 June, 2021",https://arxiv.org/pdf/2002.05505
Towards explainable meta-learning,Katarzyna Woźnica;Przemysław Biecek,"Meta-learning is a field that aims at discovering how different machine learning algorithms perform on a wide range of predictive tasks. Such knowledge speeds up the hyperparameter tuning or feature engineering. With the use of surrogate models various aspects of the predictive task such as meta-features, landmarker models e.t.c. are used to predict the expected performance. State of the art approaches are focused on searching for the best meta-model but do not explain how these different aspects contribute to its performance. However, to build a new generation of meta-models we need a deeper understanding of the importance and effect of meta-features on the model tunability. In this paper, we propose techniques developed for eXplainable Artificial Intelligence (XAI) to examine and extract knowledge from black-box surrogate models. To our knowledge, this is the first paper that shows how post-hoc explainability can be used to improve the meta-learning. △ Less","12 July, 2021",https://arxiv.org/pdf/2002.04276
Self-recognition in conversational agents,Yigit Oktar;Erdem Okur;Mehmet Turkan,"In a standard Turing test, a machine has to prove its humanness to the judges. By successfully imitating a thinking entity such as a human, this machine then proves that it can also think. Some objections claim that Turing test is not a tool to demonstrate the existence of general intelligence or thinking activity. A compelling alternative is the Lovelace test, in which the agent must originate a product that the agent's creator cannot explain. Therefore, the agent must be the owner of an original product. However, for this to happen the agent must exhibit the idea of self and distinguish oneself from others. Sustaining the idea of self within the Turing test is still possible if the judge decides to act as a textual mirror. Self-recognition tests applied on animals through mirrors appear to be viable tools to demonstrate the existence of a type of general intelligence. Methodology here constructs a textual version of the mirror test by placing the agent as the one and only judge to figure out whether the contacted one is an other, a mimicker, or oneself in an unsupervised manner. This textual version of the mirror test is objective, self-contained, and devoid of humanness. Any agent passing this textual mirror test should have or can acquire a thought mechanism that can be referred to as the inner-voice, answering the original and long lasting question of Turing ""Can machines think?"" in a constructive manner still within the bounds of the Turing test. Moreover, it is possible that a successful self-recognition might pave way to stronger notions of self-awareness in artificial beings. △ Less","5 September, 2021",https://arxiv.org/pdf/2002.02334
Deceptive AI Explanations: Creation and Detection,Johannes Schneider;Christian Meske;Michalis Vlachos,"Artificial intelligence (AI) comes with great opportunities but can also pose significant risks. Automatically generated explanations for decisions can increase transparency and foster trust, especially for systems based on automated predictions by AI models. However, given, e.g., economic incentives to create dishonest AI, to what extent can we trust explanations? To address this issue, our work investigates how AI models (i.e., deep learning, and existing instruments to increase transparency regarding AI decisions) can be used to create and detect deceptive explanations. As an empirical evaluation, we focus on text classification and alter the explanations generated by GradCAM, a well-established explanation technique in neural networks. Then, we evaluate the effect of deceptive explanations on users in an experiment with 200 participants. Our findings confirm that deceptive explanations can indeed fool humans. However, one can deploy machine learning (ML) methods to detect seemingly minor deception attempts with accuracy exceeding 80% given sufficient domain knowledge. Without domain knowledge, one can still infer inconsistencies in the explanations in an unsupervised manner, given basic knowledge of the predictive model under scrutiny. △ Less","2 December, 2021",https://arxiv.org/pdf/2001.07641
Measuring Diversity of Artificial Intelligence Conferences,Ana Freire;Lorenzo Porcaro;Emilia Gómez,"The lack of diversity of the Artificial Intelligence (AI) field is nowadays a concern, and several initiatives such as funding schemes and mentoring programs have been designed to overcome it. However, there is no indication on how these initiatives actually impact AI diversity in the short and long term. This work studies the concept of diversity in this particular context and proposes a small set of diversity indicators (i.e. indexes) of AI scientific events. These indicators are designed to quantify the diversity of the AI field and monitor its evolution. We consider diversity in terms of gender, geographical location and business (understood as the presence of academia versus industry). We compute these indicators for the different communities of a conference: authors, keynote speakers and organizing committee. From these components we compute a summarized diversity indicator for each AI event. We evaluate the proposed indexes for a set of recent major AI conferences and we discuss their values and limitations. △ Less","22 March, 2021",https://arxiv.org/pdf/2001.07038
Mean-Field and Kinetic Descriptions of Neural Differential Equations,M. Herty;T. Trimborn;G. Visconti,"Nowadays, neural networks are widely used in many applications as artificial intelligence models for learning tasks. Since typically neural networks process a very large amount of data, it is convenient to formulate them within the mean-field and kinetic theory. In this work we focus on a particular class of neural networks, i.e. the residual neural networks, assuming that each layer is characterized by the same number of neurons N, which is fixed by the dimension of the data. This assumption allows to interpret the residual neural network as a time-discretized ordinary differential equation, in analogy with neural differential equations. The mean-field description is then obtained in the limit of infinitely many input data. This leads to a Vlasov-type partial differential equation which describes the evolution of the distribution of the input data. We analyze steady states and sensitivity with respect to the parameters of the network, namely the weights and the bias. In the simple setting of a linear activation function and one-dimensional input data, the study of the moments provides insights on the choice of the parameters of the network. Furthermore, a modification of the microscopic dynamics, inspired by stochastic residual neural networks, leads to a Fokker-Planck formulation of the network, in which the concept of network training is replaced by the task of fitting distributions. The performed analysis is validated by artificial numerical simulations. In particular, results on classification and regression problems are presented. △ Less","8 November, 2021",https://arxiv.org/pdf/2001.04294
The Logic of Strategic Assets: From Oil to Artificial Intelligence,Jeffrey Ding;Allan Dafoe,"What resources and technologies are strategic? This question is often the focus of policy and theoretical debates, where the label ""strategic"" designates those assets that warrant the attention of the highest levels of the state. But these conversations are plagued by analytical confusion, flawed heuristics, and the rhetorical use of ""strategic"" to advance particular agendas. We aim to improve these conversations through conceptual clarification, introducing a theory based on important rivalrous externalities for which socially optimal behavior will not be produced alone by markets or individual national security entities. We distill and theorize the most important three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. We then employ these logics to clarify three important cases: the Avon 2 engine in the 1950s, the U.S.-Japan technology rivalry in the late 1980s, and contemporary conversations about artificial intelligence. △ Less","31 May, 2021",https://arxiv.org/pdf/2001.03246
"Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial Intelligence in 8 Countries",Patrick Gage Kelley;Yongwei Yang;Courtney Heldreth;Christopher Moessner;Aaron Sedley;Andreas Kramm;David T. Newman;Allison Woodruff,"As the influence and use of artificial intelligence (AI) have grown and its transformative potential has become more apparent, many questions have been raised regarding the economic, political, social, and ethical implications of its use. Public opinion plays an important role in these discussions, influencing product adoption, commercial development, research funding, and regulation. In this paper we present results of an in-depth survey of public opinion of artificial intelligence conducted with 10,005 respondents spanning eight countries and six continents. We report widespread perception that AI will have significant impact on society, accompanied by strong support for the responsible development and use of AI, and also characterize the public's sentiment towards AI with four key themes (exciting, useful, worrying, and futuristic) whose prevalence distinguishes response to AI in different countries. △ Less","18 May, 2021",https://arxiv.org/pdf/2001.00081
Exploring the Jungle of Intuitionistic Temporal Logics,Joseph Boudou;Martín Diéguez;David Fernández-Duque;Philip Kremer,"The importance of intuitionistic temporal logics in Computer Science and Artificial Intelligence has become increasingly clear in the last few years. From the proof-theory point of view, intuitionistic temporal logics have made it possible to extend functional languages with new features via type theory, while from its semantical perspective several logics for reasoning about dynamical systems and several semantics for logic programming have their roots in this framework. In this paper we consider several axiomatic systems for intuitionistic linear temporal logic and show that each of these systems is sound for a class of structures based either on Kripke frames or on dynamic topological systems. Our topological semantics features a new interpretation for the `henceforth' modality that is a natural intuitionistic variant of the classical one. Using the soundness results, we show that the seven logics obtained from the axiomatic systems are distinct. △ Less","12 March, 2021",https://arxiv.org/pdf/1912.12895
Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients,Brenden K. Petersen;Mikel Landajuela;T. Nathan Mundhenk;Claudio P. Santiago;Soo K. Kim;Joanne T. Kim,"Discovering the underlying mathematical expressions describing a dataset is a core challenge for artificial intelligence. This is the problem of \textit{symbolic regression}. Despite recent advances in training neural networks to solve complex tasks, deep learning approaches to symbolic regression are underexplored. We propose a framework that leverages deep learning for symbolic regression via a simple idea: use a large model to search the space of small models. Specifically, we use a recurrent neural network to emit a distribution over tractable mathematical expressions and employ a novel risk-seeking policy gradient to train the network to generate better-fitting expressions. Our algorithm outperforms several baseline methods (including Eureqa, the gold standard for symbolic regression) in its ability to exactly recover symbolic expressions on a series of benchmark problems, both with and without added noise. More broadly, our contributions include a framework that can be applied to optimize hierarchical, variable-length objects under a black-box performance metric, with the ability to incorporate constraints in situ, and a risk-seeking policy gradient formulation that optimizes for best-case performance instead of expected performance. △ Less","5 April, 2021",https://arxiv.org/pdf/1912.04871
Corpus-Level End-to-End Exploration for Interactive Systems,Zhiwen Tang;Grace Hui Yang,"A core interest in building Artificial Intelligence (AI) agents is to let them interact with and assist humans. One example is Dynamic Search (DS), which models the process that a human works with a search engine agent to accomplish a complex and goal-oriented task. Early DS agents using Reinforcement Learning (RL) have only achieved limited success for (1) their lack of direct control over which documents to return and (2) the difficulty to recover from wrong search trajectories. In this paper, we present a novel corpus-level end-to-end exploration (CE3) method to address these issues. In our method, an entire text corpus is compressed into a global low-dimensional representation, which enables the agent to gain access to the full state and action spaces, including the under-explored areas. We also propose a new form of retrieval function, whose linear approximation allows end-to-end manipulation of documents. Experiments on the Text REtrieval Conference (TREC) Dynamic Domain (DD) Track show that CE3 outperforms the state-of-the-art DS systems. △ Less","8 June, 2021",https://arxiv.org/pdf/1912.00753
The Transformative Potential of Artificial Intelligence,Ross Gruetzemacher;Jess Whittlestone,"The terms 'human-level artificial intelligence' and 'artificial general intelligence' are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term 'transformative AI' is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be 'transformative' is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area. △ Less","23 October, 2021",https://arxiv.org/pdf/1912.00747
Abstract Argumentation and the Rational Man,Timotheus Kampik;Juan Carlos Nieves,"Abstract argumentation has emerged as a method for non-monotonic reasoning that has gained popularity in the symbolic artificial intelligence community. In the literature, the different approaches to abstract argumentation that were refined over the years are typically evaluated from a formal logics perspective; an analysis that is based on models of economically rational decision-making does not exist. In this paper, we work towards addressing this issue by analyzing abstract argumentation from the perspective of the rational man paradigm in microeconomic theory. To assess under which conditions abstract argumentation-based decision-making can be considered economically rational, we derive reference independence as a non-monotonic inference property from a formal model of economic rationality and create a new argumentation principle that ensures compliance with this property. We then compare the reference independence principle with other reasoning principles, in particular with cautious monotony and rational monotony. We show that the argumentation semantics as proposed in Dung's seminal paper, as well as other semantics we evaluate -- with the exception of naive semantics and the SCC-recursive CF2 semantics -- violate the reference independence principle. Consequently, we investigate how structural properties of argumentation frameworks impact the reference independence principle, and identify cyclic expansions (both even and odd cycles) as the root of the problem. Finally, we put reference independence into the context of preference-based argumentation and show that for this argumentation variant, which explicitly models preferences, reference independence cannot be ensured in a straight-forward manner. △ Less","8 January, 2021",https://arxiv.org/pdf/1911.13024
Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM,Alberto Barbado;Óscar Corcho;Richard Benjamins,"OneClass SVM is a popular method for unsupervised anomaly detection. As many other methods, it suffers from the black box problem: it is difficult to justify, in an intuitive and simple manner, why the decision frontier is identifying data points as anomalous or non anomalous. Such type of problem is being widely addressed for supervised models. However, it is still an uncharted area for unsupervised learning. In this paper, we evaluate several rule extraction techniques over OneClass SVM models, as well as present alternative designs for some of those algorithms. Together with that, we propose algorithms to compute metrics related with eXplainable Artificial Intelligence (XAI) regarding the ""comprehensibility"", ""representativeness"", ""stability"" and ""diversity"" of the extracted rules. We evaluate our proposals with different datasets, including real-world data coming from industry. With this, our proposal contributes to extend XAI techniques to unsupervised machine learning models. △ Less","1 April, 2021",https://arxiv.org/pdf/1911.09315
Knowledge Distillation for Incremental Learning in Semantic Segmentation,Umberto Michieli;Pietro Zanuttigh,"Deep learning architectures have shown remarkable results in scene understanding problems, however they exhibit a critical drop of performances when they are required to learn incrementally new tasks without forgetting old ones. This catastrophic forgetting phenomenon impacts on the deployment of artificial intelligence in real world scenarios where systems need to learn new and different representations over time. Current approaches for incremental learning deal only with image classification and object detection tasks, while in this work we formally introduce incremental learning for semantic segmentation. We tackle the problem applying various knowledge distillation techniques on the previous model. In this way, we retain the information about learned classes, whilst updating the current model to learn the new ones. We developed four main methodologies of knowledge distillation working on both output layers and internal feature representations. We do not store any image belonging to previous training stages and only the last model is used to preserve high accuracy on previously learned classes. Extensive experimental results on the Pascal VOC2012 and MSRC-v2 datasets show the effectiveness of the proposed approaches in several incremental learning scenarios. △ Less","20 January, 2021",https://arxiv.org/pdf/1911.03462
SAFA: a Semi-Asynchronous Protocol for Fast Federated Learning with Low Overhead,Wentai Wu;Ligang He;Weiwei Lin;Rui Mao;Carsten Maple;Stephen Jarvis,"Federated learning (FL) has attracted increasing attention as a promising approach to driving a vast number of end devices with artificial intelligence. However, it is very challenging to guarantee the efficiency of FL considering the unreliable nature of end devices while the cost of device-server communication cannot be neglected. In this paper, we propose SAFA, a semi-asynchronous FL protocol, to address the problems in federated learning such as low round efficiency and poor convergence rate in extreme conditions (e.g., clients dropping offline frequently). We introduce novel designs in the steps of model distribution, client selection and global aggregation to mitigate the impacts of stragglers, crashes and model staleness in order to boost efficiency and improve the quality of the global model. We have conducted extensive experiments with typical machine learning tasks. The results demonstrate that the proposed protocol is effective in terms of shortening federated round duration, reducing local resource wastage, and improving the accuracy of the global model at an acceptable communication cost. △ Less","23 April, 2021",https://arxiv.org/pdf/1910.01355
Beating humans in a penny-matching game by leveraging cognitive hierarchy theory and Bayesian learning,Ran Tian;Nan Li;Ilya Kolmanovsky;Anouck Girard,"It is a long-standing goal of artificial intelligence (AI) to be superior to human beings in decision making. Games are suitable for testing AI capabilities of making good decisions in non-numerical tasks. In this paper, we develop a new AI algorithm to play the penny-matching game considered in Shannon's ""mind-reading machine"" (1953) against human players. In particular, we exploit cognitive hierarchy theory and Bayesian learning techniques to continually evolve a model for predicting human player decisions, and let the AI player make decisions according to the model predictions to pursue the best chance of winning. Experimental results show that our AI algorithm beats 27 out of 30 volunteer human players. △ Less","13 February, 2021",https://arxiv.org/pdf/1909.12701
Conditional LSTM-GAN for Melody Generation from Lyrics,Yi Yu;Abhishek Srivastava;Simon Canales,"Melody generation from lyrics has been a challenging research issue in the field of artificial intelligence and music, which enables to learn and discover latent relationship between interesting lyrics and accompanying melody. Unfortunately, the limited availability of paired lyrics-melody dataset with alignment information has hindered the research progress. To address this problem, we create a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment through leveraging different music sources where alignment relationship between syllables and music attributes is extracted. Most importantly, we propose a novel deep generative model, conditional Long Short-Term Memory - Generative Adversarial Network (LSTM-GAN) for melody generation from lyrics, which contains a deep LSTM generator and a deep LSTM discriminator both conditioned on lyrics. In particular, lyrics-conditioned melody and alignment relationship between syllables of given lyrics and notes of predicted melody are generated simultaneously. Experimental results have proved the effectiveness of our proposed lyrics-to-melody generative model, where plausible and tuneful sequences can be inferred from lyrics. △ Less","20 April, 2021",https://arxiv.org/pdf/1908.05551
Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective,Tom Everitt;Marcus Hutter;Ramana Kumar;Victoria Krakovna,"Can humans get arbitrarily capable reinforcement learning (RL) agents to do their bidding? Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal? This question impacts how far RL can be scaled, and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper, we study when an RL agent has an instrumental goal to tamper with its reward process, and describe design principles that prevent instrumental goals for two different types of reward tampering (reward function tampering and RF-input tampering). Combined, the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations. △ Less","26 March, 2021",https://arxiv.org/pdf/1908.04734
Logic could be learned from images,Qian Guo;Yuhua Qian;Xinyan Liang;Yanhong She;Deyu Li;Jiye Liang,"Logic reasoning is a significant ability of human intelligence and also an important task in artificial intelligence. The existing logic reasoning methods, quite often, need to design some reasoning patterns beforehand. This has led to an interesting question: can logic reasoning patterns be directly learned from given data? The problem is termed as a data concept logic. In this study, a learning logic task from images, called a LiLi task, first is proposed. This task is to learn and reason the logic relation from images, without presetting any reasoning patterns. As a preliminary exploration, we design six LiLi data sets (Bitwise And, Bitwise Or, Bitwise Xor, Addition, Subtraction and Multiplication), in which each image is embedded with a n-digit number. It is worth noting that a learning model beforehand does not know the meaning of the n-digit numbers embedded in images and the relation between the input images and the output image. In order to tackle the task, in this work we use many typical neural network models and produce fruitful results. However, these models have the poor performances on the difficult logic task. For furthermore addressing this task, a novel network framework called a divide and conquer model by adding some label information is designed, achieving a high testing accuracy. △ Less","29 June, 2021",https://arxiv.org/pdf/1908.01931
A Sufficient Statistic for Influence in Structured Multiagent Environments,Frans A. Oliehoek;Stefan Witwicki;Leslie P. Kaelbling,"Making decisions in complex environments is a key challenge in artificial intelligence (AI). Situations involving multiple decision makers are particularly complex, leading to computational intractability of principled solution methods. A body of work in AI has tried to mitigate this problem by trying to distill interaction to its essence: how does the policy of one agent influence another agent? If we can find more compact representations of such influence, this can help us deal with the complexity, for instance by searching the space of influences rather than the space of policies. However, so far these notions of influence have been restricted in their applicability to special cases of interaction. In this paper we formalize influence-based abstraction (IBA), which facilitates the elimination of latent state factors without any loss in value, for a very general class of problems described as factored partially observable stochastic games (fPOSGs). On the one hand, this generalizes existing descriptions of influence, and thus can serve as the foundation for improvements in scalability and other insights in decision making in complex multiagent settings. On the other hand, since the presence of other agents can be seen as a generalization of single agent settings, our formulation of IBA also provides a sufficient statistic for decision making under abstraction for a single agent. We also give a detailed discussion of the relations to such previous works, identifying new insights and interpretations of these approaches. In these ways, this paper deepens our understanding of abstraction in a wide range of sequential decision making settings, providing the basis for new approaches and algorithms for a large class of problems. △ Less","1 March, 2021",https://arxiv.org/pdf/1907.09278
Artificial Intelligence: A Child's Play,Ravi Kashyap,"We discuss the objectives of any endeavor in creating artificial intelligence, AI, and provide a possible alternative. Intelligence might be an unintended consequence of curiosity left to roam free, best exemplified by a frolicking infant. This suggests that our attempts at AI could have been misguided. What we actually need to strive for can be termed artificial curiosity, AC, and intelligence happens as a consequence of those efforts. For this unintentional yet welcome aftereffect to set in a foundational list of guiding principles needs to be present. We start with the intuition for this line of reasoning and formalize it with a series of definitions, assumptions, ingredients, models and iterative improvements that will be necessary to make the incubation of intelligence a reality. Our discussion provides conceptual modifications to the Turing Test and to Searle's Chinese room argument. We discuss the future implications for society as AI becomes an integral part of life. We provide a road-map for creating intelligence with the technical parts relegated to the appendix so that the article is accessible to a wide audience. The central techniques in our formal approach to creating intelligence draw upon tools and concepts widely used in physics, cognitive science, psychology, evolutionary biology, statistics, linguistics, communication systems, pattern recognition, marketing, economics, finance, information science and computational theory highlighting that solutions for creating artificial intelligence have to transcend the artificial barriers between various fields and be highly multi-disciplinary. △ Less","30 January, 2021",https://arxiv.org/pdf/1907.04659
Outlier Exposure with Confidence Control for Out-of-Distribution Detection,Aristotelis-Angelos Papadopoulos;Mohammad Reza Rajati;Nazim Shaikh;Jiamian Wang,"Deep neural networks have achieved great success in classification tasks during the last years. However, one major problem to the path towards artificial intelligence is the inability of neural networks to accurately detect samples from novel class distributions and therefore, most of the existent classification algorithms assume that all classes are known prior to the training stage. In this work, we propose a methodology for training a neural network that allows it to efficiently detect out-of-distribution (OOD) examples without compromising much of its classification accuracy on the test examples from known classes. We propose a novel loss function that gives rise to a novel method, Outlier Exposure with Confidence Control (OECC), which achieves superior results in OOD detection with OE both on image and text classification tasks without requiring access to OOD samples. Additionally, we experimentally show that the combination of OECC with state-of-the-art post-training OOD detection methods, like the Mahalanobis Detector (MD) and the Gramian Matrices (GM) methods, further improves their performance in the OOD detection task, demonstrating the potential of combining training and post-training methods for OOD detection. △ Less","2 February, 2021",https://arxiv.org/pdf/1906.03509
HUMBO: Bridging Response Generation and Facial Expression Synthesis,Shang-Yu Su;Po-Wei Lin;Yun-Nung Chen,"Spoken dialogue systems that assist users to solve complex tasks such as movie ticket booking have become an emerging research topic in artificial intelligence and natural language processing areas. With a well-designed dialogue system as an intelligent personal assistant, people can accomplish certain tasks more easily via natural language interactions. Today there are several virtual intelligent assistants in the market; however, most systems only focus on textual or vocal interaction. In this paper, we present HUMBO, a system aiming at generating dialogue responses and simultaneously synthesize corresponding visual expressions on faces for better multimodal interaction. HUMBO can (1) let users determine the appearances of virtual assistants by a single image, and (2) generate coherent emotional utterances and facial expressions on the user-provided image. This is not only a brand new research direction but more importantly, an ultimate step toward more human-like virtual assistants. △ Less","31 August, 2021",https://arxiv.org/pdf/1905.11240
SCANN: Synthesis of Compact and Accurate Neural Networks,Shayan Hassantabar;Zeyu Wang;Niraj K. Jha,"Deep neural networks (DNNs) have become the driving force behind recent artificial intelligence (AI) research. An important problem with implementing a neural network is the design of its architecture. Typically, such an architecture is obtained manually by exploring its hyperparameter space and kept fixed during training. This approach is time-consuming and inefficient. Another issue is that modern neural networks often contain millions of parameters, whereas many applications and devices require small inference models. However, efforts to migrate DNNs to such devices typically entail a significant loss of classification accuracy. To address these challenges, we propose a two-step neural network synthesis methodology, called DR+SCANN, that combines two complementary approaches to design compact and accurate DNNs. At the core of our framework is the SCANN methodology that uses three basic architecture-changing operations, namely connection growth, neuron growth, and connection pruning, to synthesize feed-forward architectures with arbitrary structure. SCANN encapsulates three synthesis methodologies that apply a repeated grow-and-prune paradigm to three architectural starting points. DR+SCANN combines the SCANN methodology with dataset dimensionality reduction to alleviate the curse of dimensionality. We demonstrate the efficacy of SCANN and DR+SCANN on various image and non-image datasets. We evaluate SCANN on MNIST and ImageNet benchmarks. In addition, we also evaluate the efficacy of using dimensionality reduction alongside SCANN (DR+SCANN) on nine small to medium-size datasets. We also show that our synthesis methodology yields neural networks that are much better at navigating the accuracy vs. energy efficiency space. This would enable neural network-based inference even on Internet-of-Things sensors. △ Less","28 March, 2021",https://arxiv.org/pdf/1904.09090
Synthesized Policies for Transfer and Adaptation across Tasks and Environments,Hexiang Hu;Liyu Chen;Boqing Gong;Fei Sha,"The ability to transfer in reinforcement learning is key towards building an agent of general artificial intelligence. In this paper, we consider the problem of learning to simultaneously transfer across both environments (ENV) and tasks (TASK), probably more importantly, by learning from only sparse (ENV, TASK) pairs out of all the possible combinations. We propose a novel compositional neural network architecture which depicts a meta rule for composing policies from the environment and task embeddings. Notably, one of the main challenges is to learn the embeddings jointly with the meta rule. We further propose new training methods to disentangle the embeddings, making them both distinctive signatures of the environments and tasks and effective building blocks for composing the policies. Experiments on GridWorld and Thor, of which the agent takes as input an egocentric view, show that our approach gives rise to high success rates on all the (ENV, TASK) pairs after learning from only 40% of them. △ Less","26 May, 2021",https://arxiv.org/pdf/1904.03276
Rallying Adversarial Techniques against Deep Learning for Network Security,Joseph Clements;Yuzhe Yang;Ankur Sharma;Hongxin Hu;Yingjie Lao,"Recent advances in artificial intelligence and the increasing need for powerful defensive measures in the domain of network security, have led to the adoption of deep learning approaches for use in network intrusion detection systems. These methods have achieved superior performance against conventional network attacks, which enable the deployment of practical security systems to unique and dynamic sectors. Adversarial machine learning, unfortunately, has recently shown that deep learning models are inherently vulnerable to adversarial modifications on their input data. Because of this susceptibility, the deep learning models deployed to power a network defense could in fact be the weakest entry point for compromising a network system. In this paper, we show that by modifying on average as little as 1.38 of the input features, an adversary can generate malicious inputs which effectively fool a deep learning based NIDS. Therefore, when designing such systems, it is crucial to consider the performance from not only the conventional network security perspective but also the adversarial machine learning domain. △ Less","24 October, 2021",https://arxiv.org/pdf/1903.11688
"Parallel Medical Imaging for Intelligent Medical Image Analysis: Concepts, Methods, and Applications",Chao Gou;Tianyu Shen;Wenbo Zheng;Huadan Xue;Hui Yu;Qiang Ji;Zhengyu Jin;Fei-Yue Wang,"There has been much progress in data-driven artificial intelligence technology for medical image analysis in the last decades. However, it still remains challenging due to its distinctive complexity of acquiring and annotating image data, extracting medical domain knowledge, and explaining the diagnostic decision for medical image analysis. In this paper, we propose a data-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for intelligent medical image analysis based on the methodology of interactive ACP-based parallel intelligence. In the PMI framework, computational experiments with predictive learning in a data-driven way are conducted to extract medical knowledge for diagnostic decision support. Artificial imaging systems are introduced to select and prescriptively generate medical image data in a knowledge-driven way to utilize medical domain knowledge. Through the closed-loop optimization based on parallel execution, our proposed PMI framework can boost the generalization ability and alleviate the limitation of medical interpretation for diagnostic decisions. Furthermore, we illustrate the preliminary implementation of PMI method through the case studies of mammogram analysis and skin lesion image analysis. Experimental results on several public medical image datasets demonstrate the effectiveness of proposed PMI. △ Less","29 June, 2021",https://arxiv.org/pdf/1903.04855
AI-Aided Online Adaptive OFDM Receiver: Design and Experimental Results,Peiwen Jiang;Tianqi Wang;Bin Han;Xuanxuan Gao;Jing Zhang;Chao-Kai Wen;Shi Jin;Geoffrey Ye Li,"Orthogonal frequency division multiplexing (OFDM) has been widely applied in current communication systems. The artificial intelligence (AI)-aided OFDM receivers are currently brought to the forefront to replace and improve the traditional OFDM receivers. In this study, we first compare two AI-aided OFDM receivers, namely, data-driven fully connected deep neural network and model-driven ComNet, through extensive simulation and real-time video transmission using a 5G rapid prototyping system for an over-the-air (OTA) test. We find a performance gap between the simulation and the OTA test caused by the discrepancy between the channel model for offline training and the real environment. We develop a novel online training system, which is called SwitchNet receiver, to address this issue. This receiver has a flexible and extendable architecture and can adapt to real channels by training only several parameters online. From the OTA test, the AI-aided OFDM receivers, especially the SwitchNet receiver, are robust to real environments and promising for future communication systems. We discuss potential challenges and future research inspired by our initial study in this paper. △ Less","24 December, 2021",https://arxiv.org/pdf/1812.06638
Stovepiping and Malicious Software: A Critical Review of AGI Containment,Jason M. Pittman;Jesus P. Espinoza;Courtney Crosby,"Awareness of the possible impacts associated with artificial intelligence has risen in proportion to progress in the field. While there are tremendous benefits to society, many argue that there are just as many, if not more, concerns related to advanced forms of artificial intelligence. Accordingly, research into methods to develop artificial intelligence safely is increasingly important. In this paper, we provide an overview of one such safety paradigm: containment with a critical lens aimed toward generative adversarial networks and potentially malicious artificial intelligence. Additionally, we illuminate the potential for a developmental blindspot in the stovepiping of containment mechanisms. △ Less","1 August, 2021",https://arxiv.org/pdf/1811.03653
An Introduction to Probabilistic Programming,Jan-Willem van de Meent;Brooks Paige;Hongseok Yang;Frank Wood,"This book is a graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a first-order probabilistic programming language (PPL) whose programs correspond to graphical models with a known, finite, set of random variables. In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented. We then turn to higher-order probabilistic programming languages. Programs in such languages can define models with dynamic computation graphs, which may not instantiate the same set of random variables in each execution. Inference requires methods that generate samples by repeatedly evaluating the program. Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller. Finally we consider the intersection of probabilistic and differentiable programming. We begin with a discussion of automatic differentiation, and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo. We then discuss gradient-based maximum likelihood estimation in programs that are parameterized using neural networks, how to amortize inference using by learning neural approximations to the program posterior, and how language features impact the design of deep probabilistic programming systems. △ Less","19 October, 2021",https://arxiv.org/pdf/1809.10756
Ontology Reasoning with Deep Neural Networks,Patrick Hohenecker;Thomas Lukasiewicz,"The ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. Traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. More recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. This is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. We present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. Furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view. △ Less","8 January, 2021",https://arxiv.org/pdf/1808.07980
Security and Privacy Issues in Deep Learning,Ho Bae;Jaehee Jang;Dahuin Jung;Hyemi Jang;Heonseok Ha;Hyungyu Lee;Sungroh Yoon,"To promote secure and private artificial intelligence (SPAI), we review studies on the model security and data privacy of DNNs. Model security allows system to behave as intended without being affected by malicious external influences that can compromise its integrity and efficiency. Security attacks can be divided based on when they occur: if an attack occurs during training, it is known as a poisoning attack, and if it occurs during inference (after training) it is termed an evasion attack. Poisoning attacks compromise the training process by corrupting the data with malicious examples, while evasion attacks use adversarial examples to disrupt entire classification process. Defenses proposed against such attacks include techniques to recognize and remove malicious data, train a model to be insensitive to such data, and mask the model's structure and parameters to render attacks more challenging to implement. Furthermore, the privacy of the data involved in model training is also threatened by attacks such as the model-inversion attack, or by dishonest service providers of AI applications. To maintain data privacy, several solutions that combine existing data-privacy techniques have been proposed, including differential privacy and modern cryptography techniques. In this paper, we describe the notions of some of methods, e.g., homomorphic encryption, and review their advantages and challenges when implemented in deep-learning models. △ Less","9 March, 2021",https://arxiv.org/pdf/1807.11655
Institutional Metaphors for Designing Large-Scale Distributed AI versus AI Techniques for Running Institutions,Alexander Boer;Giovanni Sileno,"Artificial Intelligence (AI) started out with an ambition to reproduce the human mind, but, as the sheer scale of that ambition became manifest, it quickly retreated into either studying specialized intelligent behaviours, or proposing over-arching architectural concepts for interfacing specialized intelligent behaviour components, conceived of as agents in a kind of organization. This agent-based modeling paradigm, in turn, proves to have interesting applications in understanding, simulating, and predicting the behaviour of social and legal structures on an aggregate level. For these reasons, this chapter examines a number of relevant cross-cutting concerns, conceptualizations, modeling problems and design challenges in large-scale distributed Artificial Intelligence, as well as in institutional systems, and identifies potential grounds for novel advances. △ Less","15 June, 2021",https://arxiv.org/pdf/1803.03407
A Cyber Science Based Ontology for Artificial General Intelligence Containment,Jason M. Pittman;Courtney Crosby,"The development of artificial general intelligence is considered by many to be inevitable. What such intelligence does after becoming aware is not so certain. To that end, research suggests that the likelihood of artificial general intelligence becoming hostile to humans is significant enough to warrant inquiry into methods to limit such potential. Thus, containment of artificial general intelligence is a timely and meaningful research topic. While there is limited research exploring possible containment strategies, such work is bounded by the underlying field the strategies draw upon. Accordingly, we set out to construct an ontology to describe necessary elements in any future containment technology. Using existing academic literature, we developed a single domain ontology containing five levels, 32 codes, and 32 associated descriptors. Further, we constructed ontology diagrams to demonstrate intended relationships. We then identified humans, AGI, and the cyber world as novel agent objects necessary for future containment activities. Collectively, the work addresses three critical gaps: (a) identifying and arranging fundamental constructs; (b) situating AGI containment within cyber science; and (c) developing scientific rigor within the field. △ Less","1 August, 2021",https://arxiv.org/pdf/1801.09317
Taking Visual Motion Prediction To New Heightfields,Sebastien Ehrhardt;Aron Monszpart;Niloy Mitra;Andrea Vedaldi,"While the basic laws of Newtonian mechanics are well understood, explaining a physical scenario still requires manually modeling the problem with suitable equations and estimating the associated parameters. In order to be able to leverage the approximation capabilities of artificial intelligence techniques in such physics related contexts, researchers have handcrafted the relevant states, and then used neural networks to learn the state transitions using simulation runs as training data. Unfortunately, such approaches are unsuited for modeling complex real-world scenarios, where manually authoring relevant state spaces tend to be tedious and challenging. In this work, we investigate if neural networks can implicitly learn physical states of real-world mechanical processes only based on visual data while internally modeling non-homogeneous environment and in the process enable long-term physical extrapolation. We develop a recurrent neural network architecture for this task and also characterize resultant uncertainties in the form of evolving variance estimates. We evaluate our setup to extrapolate motion of rolling ball(s) on bowls of varying shape and orientation, and on arbitrary heightfields using only images as input. We report significant improvements over existing image-based methods both in terms of accuracy of predictions and complexity of scenarios; and report competitive performance with approaches that, unlike us, assume access to internal physical states. △ Less","10 December, 2021",https://arxiv.org/pdf/1712.09448
A Survey on Bayesian Deep Learning,Hao Wang;Dit-Yan Yeung,"A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to https://github.com/js05212/BayesianDeepLearning-Survey. △ Less","5 January, 2021",https://arxiv.org/pdf/1604.01662
Introduzione all'Intelligenza Artificiale,Fabrizio Riguzzi,"The paper presents an introduction to Artificial Intelligence (AI) in an accessible and informal but precise form. The paper focuses on the algorithmic aspects of the discipline, presenting the main techniques used in AI systems groped in symbolic and subsymbolic. The last part of the paper is devoted to the discussion ongoing among experts in the field and the public at large about on the advantages and disadvantages of AI and in particular on the possible dangers. The personal opinion of the author on this subject concludes the paper. -- -- L'articolo presenta un'introduzione all'Intelligenza Artificiale (IA) in forma divulgativa e informale ma precisa. L'articolo affronta prevalentemente gli aspetti informatici della disciplina, presentando le principali tecniche usate nei sistemi di IA divise in simboliche e subsimboliche. L'ultima parte dell'articolo presenta il dibattito in corso tra gli esperi e il pubblico su vantaggi e svantaggi dell'IA e in particolare sui possibili pericoli. L'articolo termina con l'opinione dell'autore al riguardo. △ Less","11 May, 2021",https://arxiv.org/pdf/1511.04352
