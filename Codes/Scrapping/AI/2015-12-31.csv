title,authors,abstract,submitted_date,pdf_link
Word Existence Algorithm,Tejeswini Sundaram;Vyom Chabbra,"The current scenario in the field of computing is largely affected by the speed at which data can be accessed and recalled. In this paper, we present the word existence algorithm which is used to check if the word given as an input is part of a particular database or not. We have taken the English language as an example here. This algorithm tries to solve the problem of lookup by using a uniformly distributed hash function. We have also addressed the problem of clustering and collision. A further contribution is that we follow a direct hashed model where each hash value is linked to another table if the continuity for the function holds true. The core of the algorithm lies in the data model being used during preordering. Our focus lies on the formation of a continuity series and validating the words that exists in the database. This algorithm can be used in applications where we there is a requirement to search for just the existence of a word, example Artificial Intelligence responding to input ,look up for neural networks and dictionary lookups and more. We have observed that this algorithm provides a faster search time △ Less","27 November, 2015",https://arxiv.org/pdf/1601.04248
Information-Theoretic Bounded Rationality,Pedro A. Ortega;Daniel A. Braun;Justin Dyer;Kee-Eung Kim;Naftali Tishby,"Bounded rationality, that is, decision-making and planning under resource limitations, is widely regarded as an important open problem in artificial intelligence, reinforcement learning, computational neuroscience and economics. This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas. We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions. This functional possesses three crucial properties: it controls the size of the solution space; it has Monte Carlo planners that are exact, yet bypass the need for exhaustive search; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions. We discuss the single-step decision-making case, and show how to extend it to sequential decisions using equivalence transformations. This extension yields a very general class of decision problems that encompass classical decision rules (e.g. EXPECTIMAX and MINIMAX) as limit cases, as well as trust- and risk-sensitive planning. △ Less","21 December, 2015",https://arxiv.org/pdf/1512.06789
Constrained Sampling and Counting: Universal Hashing Meets SAT Solving,Kuldeep S. Meel;Moshe Vardi;Supratik Chakraborty;Daniel J. Fremont;Sanjit A. Seshia;Dror Fried;Alexander Ivrii;Sharad Malik,"Constrained sampling and counting are two fundamental problems in artificial intelligence with a diverse range of applications, spanning probabilistic reasoning and planning to constrained-random verification. While the theory of these problems was thoroughly investigated in the 1980s, prior work either did not scale to industrial size instances or gave up correctness guarantees to achieve scalability. Recently, we proposed a novel approach that combines universal hashing and SAT solving and scales to formulas with hundreds of thousands of variables without giving up correctness guarantees. This paper provides an overview of the key ingredients of the approach and discusses challenges that need to be overcome to handle larger real-world instances. △ Less","21 December, 2015",https://arxiv.org/pdf/1512.06633
Subsumptive reflection in SNOMED CT: a large description logic-based terminology for diagnosis,A. M. Mohan Rao,"Description logic (DL) based biomedical terminology (SNOMED CT) is used routinely in medical practice. However, diagnostic inference using such terminology is precluded by its complexity. Here we propose a model that simplifies these inferential components. We propose three concepts that classify clinical features and examined their effect on inference using SNOMED CT. We used PAIRS (Physician Assistant Artificial Intelligence Reference System) database (1964 findings for 485 disorders, 18 397 disease feature links) for our analysis. We also use a 50-million medical word corpus for estimating the vectors of disease-feature links. Our major results are 10% of finding-disorder links are concomitant in both assertion and negation where as 90% are either concomitant in assertion or negation. Logical implications of PAIRS data on SNOMED CT include 70% of the links do not share any common system while 18% share organ and 12% share both system and organ. Applications of these principles for inference are discussed and suggestions are made for deriving a diagnostic process using SNOMED CT. Limitations of these processes and suggestions for improvements are also discussed. △ Less","10 December, 2015",https://arxiv.org/pdf/1512.03516
Mobile Robots Adaptive Control Using Neural Networks,Ioan Dumitrache;Monica Dragoicea,The paper proposes a feed-forward control strategy for mobile robot control that accounts for a non-linear model of the vehicle with interaction between inputs and outputs. It is possible to include specific model uncertainties in the dynamic model of the mobile robot in order to see how the control problem should be addressed taking into consideration the complete dynamic mobile robot model. By means of a neural network feed-forward controller a real non-linear mathematical model of the vehicle can be taken into consideration. The classical velocity control strategy can be extended using artificial neural networks in order to compensate for the modelling uncertainties. It is possible to develop an intelligent strategy for mobile robot control. △ Less,"10 December, 2015",https://arxiv.org/pdf/1512.03345
"Digital Genesis: Computers, Evolution and Artificial Life",Tim Taylor;Alan Dorin;Kevin Korb,"The application of evolution in the digital realm, with the goal of creating artificial intelligence and artificial life, has a history as long as that of the digital computer itself. We illustrate the intertwined history of these ideas, starting with the early theoretical work of John von Neumann and the pioneering experimental work of Nils Aall Barricelli. We argue that evolutionary thinking and artificial life will continue to play an integral role in the future development of the digital world. △ Less","7 December, 2015",https://arxiv.org/pdf/1512.02100
What Makes it Difficult to Understand a Scientific Literature?,Mengyun Cao;Jiao Tian;Dezhi Cheng;Jin Liu;Xiaoping Sun,"In the artificial intelligence area, one of the ultimate goals is to make computers understand human language and offer assistance. In order to achieve this ideal, researchers of computer science have put forward a lot of models and algorithms attempting at enabling the machine to analyze and process human natural language on different levels of semantics. Although recent progress in this field offers much hope, we still have to ask whether current research can provide assistance that people really desire in reading and comprehension. To this end, we conducted a reading comprehension test on two scientific papers which are written in different styles. We use the semantic link models to analyze the understanding obstacles that people will face in the process of reading and figure out what makes it difficult for human to understand a scientific literature. Through such analysis, we summarized some characteristics and problems which are reflected by people with different levels of knowledge on the comprehension of difficult science and technology literature, which can be modeled in semantic link network. We believe that these characteristics and problems will help us re-examine the existing machine models and are helpful in the designing of new one. △ Less","4 December, 2015",https://arxiv.org/pdf/1512.01409
A Study on Artificial Intelligence IQ and Standard Intelligent Model,Feng Liu;Yong Shi,"Currently, potential threats of artificial intelligence (AI) to human have triggered a large controversy in society, behind which, the nature of the issue is whether the artificial intelligence (AI) system can be evaluated quantitatively. This article analyzes and evaluates the challenges that the AI development level is facing, and proposes that the evaluation methods for the human intelligence test and the AI system are not uniform; and the key reason for which is that none of the models can uniformly describe the AI system and the beings like human. Aiming at this problem, a standard intelligent system model is established in this study to describe the AI system and the beings like human uniformly. Based on the model, the article makes an abstract mathematical description, and builds the standard intelligent machine mathematical model; expands the Von Neumann architecture and proposes the Liufeng - Shiyong architecture; gives the definition of the artificial intelligence IQ, and establishes the artificial intelligence scale and the evaluation method; conduct the test on 50 search engines and three human subjects at different ages across the world, and finally obtains the ranking of the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014. △ Less","3 December, 2015",https://arxiv.org/pdf/1512.00977
Evaluating Morphological Computation in Muscle and DC-motor Driven Models of Human Hopping,Keyan Ghazi-Zahedi;Daniel F. B. Haeufle;Guido Montufar;Syn Schmitt;Nihat Ay,"In the context of embodied artificial intelligence, morphological computation refers to processes which are conducted by the body (and environment) that otherwise would have to be performed by the brain. Exploiting environmental and morphological properties is an important feature of embodied systems. The main reason is that it allows to significantly reduce the controller complexity. An important aspect of morphological computation is that it cannot be assigned to an embodied system per se, but that it is, as we show, behavior- and state-dependent. In this work, we evaluate two different measures of morphological computation that can be applied in robotic systems and in computer simulations of biological movement. As an example, these measures were evaluated on muscle and DC-motor driven hopping models. We show that a state-dependent analysis of the hopping behaviors provides additional insights that cannot be gained from the averaged measures alone. This work includes algorithms and computer code for the measures. △ Less","11 December, 2015",https://arxiv.org/pdf/1512.00250
Some Epistemological Problems with the Knowledge Level in Cognitive Architectures,Antonio Lieto,"This article addresses an open problem in the area of cognitive systems and architectures: namely the problem of handling (in terms of processing and reasoning capabilities) complex knowledge structures that can be at least plausibly comparable, both in terms of size and of typology of the encoded information, to the knowledge that humans process daily for executing everyday activities. Handling a huge amount of knowledge, and selectively retrieve it ac- cording to the needs emerging in different situational scenarios, is an important aspect of human intelligence. For this task, in fact, humans adopt a wide range of heuristics (Gigerenzer and Todd) due to their bounded rationality (Simon, 1957). In this perspective, one of the re- quirements that should be considered for the design, the realization and the evaluation of intelligent cognitively inspired systems should be represented by their ability of heuristically identify and retrieve, from the general knowledge stored in their artificial Long Term Memory (LTM), that one which is synthetically and contextually relevant. This require- ment, however, is often neglected. Currently, artificial cognitive systems and architectures are not able, de facto, to deal with complex knowledge structures that can be even slightly comparable to the knowledge heuris- tically managed by humans. In this paper I will argue that this is not only a technological problem but also an epistemological one and I will briefly sketch a proposal for a possible solution. △ Less","26 November, 2015",https://arxiv.org/pdf/1511.08512
An Introduction to Convolutional Neural Networks,Keiron O'Shea;Ryan Nash,"The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning. △ Less","2 December, 2015",https://arxiv.org/pdf/1511.08458
Strategic Dialogue Management via Deep Reinforcement Learning,Heriberto Cuayáhuitl;Simon Keizer;Oliver Lemon,"Artificially intelligent agents equipped with strategic skills that can negotiate during their interactions with other natural or artificial agents are still underdeveloped. This paper describes a successful application of Deep Reinforcement Learning (DRL) for training intelligent agents with strategic conversational skills, in a situated dialogue setting. Previous studies have modelled the behaviour of strategic agents using supervised learning and traditional reinforcement learning techniques, the latter using tabular representations or learning with linear function approximation. In this study, we apply DRL with a high-dimensional state space to the strategic board game of Settlers of Catan---where players can offer resources in exchange for others and they can also reply to offers made by other players. Our experimental results report that the DRL-based learnt policies significantly outperformed several baselines including random, rule-based, and supervised-based behaviours. The DRL-based policy has a 53% win rate versus 3 automated players (`bots'), whereas a supervised player trained on a dialogue corpus in this setting achieved only 27%, versus the same 3 bots. This result supports the claim that DRL is a promising framework for training dialogue systems, and strategic agents with negotiation abilities. △ Less","25 November, 2015",https://arxiv.org/pdf/1511.08099
A Python Engine for Teaching Artificial Intelligence in Games,Mark O. Riedl,"Computer games play an important role in our society and motivate people to learn computer science. Since artificial intelligence is integral to most games, they can also be used to teach artificial intelligence. We introduce the Game AI Game Engine (GAIGE), a Python game engine specifically designed to teach about how AI is used in computer games. A progression of seven assignments builds toward a complete, working Multi-User Battle Arena (MOBA) game. We describe the engine, the assignments, and our experiences using it in a class on Game Artificial Intelligence. △ Less","24 November, 2015",https://arxiv.org/pdf/1511.07714
A Model for Web-Intelligence Index to Evaluate the Web Intelligence Capacity of Government Web Sites of Sri Lanka,Prabath Chaminda Abeysiriwardana;S. R. Kodituwakku,"Web intelligence can be considered as a subset of Artificial Intelligence. It uses existing data in web to produce new data, knowledge and wisdom to support decision making and new predictions for web users. Artificial Intelligence is ever changing and evolving field of computer science and it is extensively used in wide array of web based business applications. Although it is used substantially in web based systems in developed countries, it is not examined whether it is being substantially used in Sri Lanka. Every Sri Lankan citizen depends on Public Service more or less throughout his/ her life time and at least more than 3 times: at birth, marriage and death. So providing most of these services to its citizen, Sri Lankan Government uses more or less of its country web portal. This paper presents a model to evaluate web intelligence capability based on weight to key functionalities with respect to web intelligence. The government websites were checked by the proposed criteria to show the potential of using web intelligent technology to provide website based services. The result indicates that the use of web intelligence techniques openly and publicly to provide web based services through government web portal to its citizens is not satisfactory. It also indicates that lack of using the technologies pertaining to web intelligence in the public service web hinders the most of the advantages that citizen and government can gain from such technological involvement. △ Less","20 November, 2015",https://arxiv.org/pdf/1511.06787
Taxonomy of Pathways to Dangerous AI,Roman V. Yampolskiy,"In order to properly handle a dangerous Artificially Intelligent (AI) system it is important to understand how the system came to be in such a state. In popular culture (science fiction movies/books) AIs/Robots became self-aware and as a result rebel against humanity and decide to destroy it. While it is one possible scenario, it is probably the least likely path to appearance of dangerous AI. In this work, we survey, classify and analyze a number of circumstances, which might lead to arrival of malicious AI. To the best of our knowledge, this is the first attempt to systematically classify types of pathways leading to malevolent AI. Previous relevant work either surveyed specific goals/meta-rules which might lead to malevolent behavior in AIs (Özkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit at different stages of its development (Alexey Turchin, July 10 2015, July 10, 2015). △ Less","11 November, 2015",https://arxiv.org/pdf/1511.03246
Design of an Alarm System for Isfahan Ozone Level based on Artificial Intelligence Predictor Models,Ehsan Lotfi,"The ozone level prediction is an important task of air quality agencies of modern cities. In this paper, we design an ozone level alarm system (OLP) for Isfahan city and test it through the real word data from 1-1-2000 to 7-6-2011. We propose a computer based system with three inputs and single output. The inputs include three sensors of solar ultraviolet (UV), total solar radiation (TSR) and total ozone (O3). And the output of the system is the predicted O3 of the next day and the alarm massages. A developed artificial intelligence (AI) algorithm is applied to determine the output, based on the inputs variables. For this issue, AI models, including supervised brain emotional learning (BEL), adaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks (ANNs), are compared in order to find the best model. The simulation of the proposed system shows that it can be used successfully in prediction of major cities ozone level. △ Less","7 November, 2015",https://arxiv.org/pdf/1511.02420
Finetuning Randomized Heuristic Search For 2D Path Planning: Finding The Best Input Parameters For R* Algorithm Through Series Of Experiments,Konstantin Yakovlev;Egor Baskin;Ivan Hramoin,"Path planning is typically considered in Artificial Intelligence as a graph searching problem and R* is state-of-the-art algorithm tailored to solve it. The algorithm decomposes given path finding task into the series of subtasks each of which can be easily (in computational sense) solved by well-known methods (such as A*). Parameterized random choice is used to perform the decomposition and as a result R* performance largely depends on the choice of its input parameters. In our work we formulate a range of assumptions concerning possible upper and lower bounds of R* parameters, their interdependency and their influence on R* performance. Then we evaluate these assumptions by running a large number of experiments. As a result we formulate a set of heuristic rules which can be used to initialize the values of R* parameters in a way that leads to algorithm's best performance. △ Less","3 November, 2015",https://arxiv.org/pdf/1511.00840
Optimized Mission Planning for Planetary Exploration Rovers,Alexander Lavin,"The exploration of planetary surfaces is predominately unmanned, calling for a landing vehicle and an autonomous and/or teleoperated rover. Artificial intelligence and machine learning techniques can be leveraged for better mission planning. This paper describes the coordinated use of both global navigation and metaheuristic optimization algorithms to plan the safe, efficient missions. The aim is to determine the least-cost combination of a safe landing zone (LZ) and global path plan, where avoiding terrain hazards for the lander and rover minimizes cost. Computer vision methods were used to identify surface craters, mounds, and rocks as obstacles. Multiple search methods were investigated for the rover global path plan. Several combinatorial optimization algorithms were implemented to select the shortest distance path as the preferred mission plan. Simulations were run for a sample Google Lunar X Prize mission. The result of this study is an optimization scheme that path plans with the A* search method, and uses simulated annealing to select ideal LZ-path- goal combination for the mission. Simulation results show the methods are effective in minimizing the risk of hazards and increasing efficiency. This paper is specific to a lunar mission, but the resulting architecture may be applied to a large variety of planetary missions and rovers. △ Less","31 October, 2015",https://arxiv.org/pdf/1511.00195
Cells in the Internet of Things,Ayush Shah;H. B. Acharya;Ambar Pal,"The Internet of Things combines various earlier areas of research. As a result, research on the subject is still organized around these pre-existing areas: distributed computing with services and objects, networks (usually combining 6lowpan with Zigbee etc. for the last-hop), artificial intelligence and semantic web, and human-computer interaction. We are yet to create a unified model that covers all these perspectives - domain, device, service, agent, etc. In this paper, we propose the concept of cells as units of structure and context in the Internet of things. This allows us to have a unified vocabulary to refer to single entities (whether dumb motes, intelligent spimes, or virtual services), intranets of things, and finally the complete Internet of things. The question that naturally follows, is what criteria we choose to demarcate boundaries; we suggest various possible answers to this question. We also mention how this concept ties into the existing visions and protocols, and suggest how it may be used as the foundation of a formal model. △ Less","27 October, 2015",https://arxiv.org/pdf/1510.07861
On the Computability of AIXI,Jan Leike;Marcus Hutter,"How could we solve the machine learning and the artificial intelligence problem if we had infinite computation? Solomonoff induction and the reinforcement learning agent AIXI are proposed answers to this question. Both are known to be incomputable. In this paper, we quantify this using the arithmetical hierarchy, and prove upper and corresponding lower bounds for incomputability. We show that AIXI is not limit computable, thus it cannot be approximated using finite computation. Our main result is a limit-computable ε-optimal version of AIXI with infinite horizon that maximizes expected rewards. △ Less","19 October, 2015",https://arxiv.org/pdf/1510.05572
Artificial Intelligence and Asymmetric Information Theory,Tshilidzi Marwala;Evan Hurwitz,"When human agents come together to make decisions, it is often the case that one human agent has more information than the other. This phenomenon is called information asymmetry and this distorts the market. Often if one human agent intends to manipulate a decision in its favor the human agent can signal wrong or right information. Alternatively, one human agent can screen for information to reduce the impact of asymmetric information on decisions. With the advent of artificial intelligence, signaling and screening have been made easier. This paper studies the impact of artificial intelligence on the theory of asymmetric information. It is surmised that artificial intelligent agents reduce the degree of information asymmetry and thus the market where these agents are deployed become more efficient. It is also postulated that the more artificial intelligent agents there are deployed in the market the less is the volume of trades in the market. This is because for many trades to happen the asymmetry of information on goods and services to be traded should exist, creating a sense of arbitrage. △ Less","14 October, 2015",https://arxiv.org/pdf/1510.02867
Ensemble UCT Needs High Exploitation,S. Ali Mirsoleimani;Aske Plaat;Jaap van den Herik,"Recent results have shown that the MCTS algorithm (a new, adaptive, randomized optimization algorithm) is effective in a remarkably diverse set of applications in Artificial Intelligence, Operations Research, and High Energy Physics. MCTS can find good solutions without domain dependent heuristics, using the UCT formula to balance exploitation and exploration. It has been suggested that the optimum in the exploitation- exploration balance differs for different search tree sizes: small search trees needs more exploitation; large search trees need more exploration. Small search trees occur in variations of MCTS, such as parallel and ensemble approaches. This paper investigates the possibility of improving the performance of Ensemble UCT by increasing the level of exploitation. As the search trees becomes smaller we achieve an improved performance. The results are important for improving the performance of large scale parallelism of MCTS. △ Less","28 September, 2015",https://arxiv.org/pdf/1509.08434
Analysis of Intelligent Classifiers and Enhancing the Detection Accuracy for Intrusion Detection System,Mohanad Albayati;Biju Issac,"In this paper we discuss and analyze some of the intelligent classifiers which allows for automatic detection and classification of networks attacks for any intrusion detection system. We will proceed initially with their analysis using the WEKA software to work with the classifiers on a well-known IDS (Intrusion Detection Systems) dataset like NSL-KDD dataset. The NSL-KDD dataset of network attacks was created in a military network by MIT Lincoln Labs. Then we will discuss and experiment some of the hybrid AI (Artificial Intelligence) classifiers that can be used for IDS, and finally we developed a Java software with three most efficient classifiers and compared it with other options. The outputs would show the detection accuracy and efficiency of the single and combined classifiers used. △ Less","28 September, 2015",https://arxiv.org/pdf/1509.08239
Quantification of sand fraction from seismic attributes using Neuro-Fuzzy approach,Akhilesh K Verma;Soumi Chaki;Aurobinda Routray;William K Mohanty;Mamata Jenamani,"In this paper, we illustrate the modeling of a reservoir property (sand fraction) from seismic attributes namely seismic impedance, seismic amplitude, and instantaneous frequency using Neuro-Fuzzy (NF) approach. Input dataset includes 3D post-stacked seismic attributes and six well logs acquired from a hydrocarbon field located in the western coast of India. Presence of thin sand and shale layers in the basin area makes the modeling of reservoir characteristic a challenging task. Though seismic data is helpful in extrapolation of reservoir properties away from boreholes; yet, it could be challenging to delineate thin sand and shale reservoirs using seismic data due to its limited resolvability. Therefore, it is important to develop state-of-art intelligent methods for calibrating a nonlinear mapping between seismic data and target reservoir variables. Neural networks have shown its potential to model such nonlinear mappings; however, uncertainties associated with the model and datasets are still a concern. Hence, introduction of Fuzzy Logic (FL) is beneficial for handling these uncertainties. More specifically, hybrid variants of Artificial Neural Network (ANN) and fuzzy logic, i.e., NF methods, are capable for the modeling reservoir characteristics by integrating the explicit knowledge representation power of FL with the learning ability of neural networks. The documented results in this study demonstrate acceptable resemblance between target and predicted variables, and hence, encourage the application of integrated machine learning approaches such as Neuro-Fuzzy in reservoir characterization domain. Furthermore, visualization of the variation of sand probability in the study area would assist in identifying placement of potential wells for future drilling operations. △ Less","23 September, 2015",https://arxiv.org/pdf/1509.07074
Agent enabled Mining of Distributed Protein Data Banks,G. S. Bhamra;A. K. Verma;R. B. Patel,"Mining biological data is an emergent area at the intersection between bioinformatics and data mining (DM). The intelligent agent based model is a popular approach in constructing Distributed Data Mining (DDM) systems to address scalable mining over large scale distributed data. The nature of associations between different amino acids in proteins has also been a subject of great anxiety. There is a strong need to develop new models and exploit and analyze the available distributed biological data sources. In this study, we have designed and implemented a multi-agent system (MAS) called Agent enriched Quantitative Association Rules Mining for Amino Acids in distributed Protein Data Banks (AeQARM-AAPDB). Such globally strong association rules enhance understanding of protein composition and are desirable for synthesis of artificial proteins. A real protein data bank is used to validate the system. △ Less","19 June, 2015",https://arxiv.org/pdf/1509.03198
Turing's Imitation Game has been Improved,Norbert Bátfai,"Using the recently introduced universal computing model, called orchestrated machine, that represents computations in a dissipative environment, we consider a new kind of interpretation of Turing's Imitation Game. In addition we raise the question whether the intelligence may show fractal properties. Then we sketch a vision of what robotic cars are going to do in the future. Finally we give the specification of an artificial life game based on the concept of orchestrated machines. The purpose of this paper is to start the search for possible relationships between these different topics. △ Less","2 September, 2015",https://arxiv.org/pdf/1509.00584
Financial Market Modeling with Quantum Neural Networks,Carlos Pedro Gonçalves,"Econophysics has developed as a research field that applies the formalism of Statistical Mechanics and Quantum Mechanics to address Economics and Finance problems. The branch of Econophysics that applies of Quantum Theory to Economics and Finance is called Quantum Econophysics. In Finance, Quantum Econophysics' contributions have ranged from option pricing to market dynamics modeling, behavioral finance and applications of Game Theory, integrating the empirical finding, from human decision analysis, that shows that nonlinear update rules in probabilities, leading to non-additive decision weights, can be computationally approached from quantum computation, with resulting quantum interference terms explaining the non-additive probabilities. The current work draws on these results to introduce new tools from Quantum Artificial Intelligence, namely Quantum Artificial Neural Networks as a way to build and simulate financial market models with adaptive selection of trading rules, leading to turbulence and excess kurtosis in the returns distributions for a wide range of parameters. △ Less","26 August, 2015",https://arxiv.org/pdf/1508.06586
Proposal for the creation of a research facility for the development of the SP machine,J. Gerard Wolff;Vasile Palade,"This is a proposal to create a research facility for the development of a high-parallel version of the ""SP machine"", based on the ""SP theory of intelligence"". We envisage that the new version of the SP machine will be an open-source software virtual machine, derived from the existing ""SP computer model"", and hosted on an existing high-performance computer. It will be a means for researchers everywhere to explore what can be done with the system and to create new versions of it. The SP system is a unique attempt to simplify and integrate observations and concepts across artificial intelligence, mainstream computing, mathematics, and human perception and cognition, with information compression as a unifying theme. Potential benefits and applications include helping to solve problems associated with big data; facilitating the development of autonomous robots; unsupervised learning, natural language processing, several kinds of reasoning, fuzzy pattern recognition at multiple levels of abstraction, computer vision, best-match and semantic forms of information retrieval, software engineering, medical diagnosis, simplification of computing systems, and the seamless integration of diverse kinds of knowledge and diverse aspects of intelligence. Additional motivations include the potential of the SP system to help solve problems in defence, security, and the detection and prevention of crime; potential in terms of economic, social, environmental, and academic criteria, and in terms of publicity; and the potential for international influence in research. The main elements of the proposed facility are described, including support for the development of ""SP-neural"", a neural version of the SP machine. The facility should be permanent in the sense that it should be available for the foreseeable future, and it should be designed to facilitate its use by researchers anywhere in the world. △ Less","19 August, 2015",https://arxiv.org/pdf/1508.04570
On environments as systemic exoskeletons: Crosscutting optimizers and antifragility enablers,Vincenzo De Florio,"Classic approaches to General Systems Theory often adopt an individual perspective and a limited number of systemic classes. As a result, those classes include a wide number and variety of systems that result equivalent to each other. This paper introduces a different approach: First, systems belonging to a same class are further differentiated according to five major general characteristics. This introduces a ""horizontal dimension"" to system classification. A second component of our approach considers systems as nested compositional hierarchies of other sub-systems. The resulting ""vertical dimension"" further specializes the systemic classes and makes it easier to assess similarities and differences regarding properties such as resilience, performance, and quality-of-experience. Our approach is exemplified by considering a telemonitoring system designed in the framework of Flemish project ""Little Sister"". We show how our approach makes it possible to design intelligent environments able to closely follow a system's horizontal and vertical organization and to artificially augment its features by serving as crosscutting optimizers and as enablers of antifragile behaviors. △ Less","19 October, 2015",https://arxiv.org/pdf/1508.01869
Adaptive Automation: Leveraging Machine Learning to Support Uninterrupted Automated Testing of Software Applications,Rajesh Mathur;Scott Miles;Miao Du,"Checking software application suitability using automated software tools has become a vital element for most organisations irrespective of whether they produce in-house software or simply customise off-the-shelf software applications for internal use. As software solutions become ever more complex, the industry becomes increasingly dependent on software automation tools, yet the brittle nature of the available software automation tools limits their effectiveness. Companies invest significantly in obtaining and implementing automation software but most of the tools fail to deliver when the cost of maintaining an effective automation test suite exceeds the cost and time that would have otherwise been spent on manual testing. A failing in the current generation of software automation tools is they do not adapt to unexpected modifications and obstructions without frequent (and time expensive) manual interference. Such issues are commonly acknowledged amongst industry practitioners, yet none of the current generation of tools have leveraged the advances in machine learning and artificial intelligence to address these problems. This paper proposes a framework solution that utilises machine learning concepts, namely fuzzy matching and error recovery. The suggested solution applies adaptive techniques to recover from unexpected obstructions that would otherwise have prevented the script from proceeding. Recovery details are presented to the user in a report which can be analysed to determine if the recovery procedure was acceptable and the framework will adapt future runs based on the decisions of the user. Using this framework, a practitioner can run the automated suits without human intervention while minimising the risk of schedule delays. △ Less","4 August, 2015",https://arxiv.org/pdf/1508.00671
A Minimal Architecture for General Cognition,Michael S. Gashler;Zachariah Kindle;Michael R. Smith,"A minimalistic cognitive architecture called MANIC is presented. The MANIC architecture requires only three function approximating models, and one state machine. Even with so few major components, it is theoretically sufficient to achieve functional equivalence with all other cognitive architectures, and can be practically trained. Instead of seeking to transfer architectural inspiration from biology into artificial intelligence, MANIC seeks to minimize novelty and follow the most well-established constructs that have evolved within various sub-fields of data science. From this perspective, MANIC offers an alternate approach to a long-standing objective of artificial intelligence. This paper provides a theoretical analysis of the MANIC architecture. △ Less","31 July, 2015",https://arxiv.org/pdf/1508.00019
"A Model for Foraging Ants, Controlled by Spiking Neural Networks and Double Pheromones",Cristian Jimenez-Romero;David Sousa-Rodrigues;Jeffrey H. Johnson;Vitorino Ramos,"A model of an Ant System where ants are controlled by a spiking neural circuit and a second order pheromone mechanism in a foraging task is presented. A neural circuit is trained for individual ants and subsequently the ants are exposed to a virtual environment where a swarm of ants performed a resource foraging task. The model comprises an associative and unsupervised learning strategy for the neural circuit of the ant. The neural circuit adapts to the environment by means of classical conditioning. The initially unknown environment includes different types of stimuli representing food and obstacles which, when they come in direct contact with the ant, elicit a reflex response in the motor neural system of the ant: moving towards or away from the source of the stimulus. The ants are released on a landscape with multiple food sources where one ant alone would have difficulty harvesting the landscape to maximum efficiency. The introduction of a double pheromone mechanism yields better results than traditional ant colony optimization strategies. Traditional ant systems include mainly a positive reinforcement pheromone. This approach uses a second pheromone that acts as a marker for forbidden paths (negative feedback). This blockade is not permanent and is controlled by the evaporation rate of the pheromones. The combined action of both pheromones acts as a collective stigmergic memory of the swarm, which reduces the search space of the problem. This paper explores how the adaptation and learning abilities observed in biologically inspired cognitive architectures is synergistically enhanced by swarm optimization strategies. The model portraits two forms of artificial intelligent behaviour: at the individual level the spiking neural network is the main controller and at the collective level the pheromone distribution is a map towards the solution emerged by the colony. △ Less","18 September, 2015",https://arxiv.org/pdf/1507.08467
Turbo-Like Beamforming Based on Tabu Search Algorithm for Millimeter-Wave Massive MIMO Systems,Xinyu Gao;Linglong Dai;Chau Yuen;Zhaocheng Wang,"For millimeter-wave (mmWave) massive MIMO systems, the codebook-based analog beamforming (including transmit precoding and receive combining) is usually used to compensate the severe attenuation of mmWave signals. However, conventional beamforming schemes involve complicated search among pre-defined codebooks to find out the optimal pair of analog precoder and analog combiner. To solve this problem, by exploring the idea of turbo equalizer together with tabu search (TS) algorithm, we propose a Turbo-like beamforming scheme based on TS, which is called Turbo-TS beamforming in this paper, to achieve the near-optimal performance with low complexity. Specifically, the proposed Turbo-TS beamforming scheme is composed of the following two key components: 1) Based on the iterative information exchange between the base station and the user, we design a Turbo-like joint search scheme to find out the near-optimal pair of analog precoder and analog combiner; 2) Inspired by the idea of TS algorithm developed in artificial intelligence, we propose a TS-based precoding/combining scheme to intelligently search the best precoder/combiner in each iteration of Turbo-like joint search with low complexity. Analysis shows that the proposed Turbo-TS beamforming can considerably reduce the searching complexity, and simulation results verify that it can achieve the near-optimal performance. △ Less","16 July, 2015",https://arxiv.org/pdf/1507.04603
Scaling Monte Carlo Tree Search on Intel Xeon Phi,S. Ali Mirsoleimani;Aske Plaat;Jaap van den Herik;Jos Vermaseren,"Many algorithms have been parallelized successfully on the Intel Xeon Phi coprocessor, especially those with regular, balanced, and predictable data access patterns and instruction flows. Irregular and unbalanced algorithms are harder to parallelize efficiently. They are, for instance, present in artificial intelligence search algorithms such as Monte Carlo Tree Search (MCTS). In this paper we study the scaling behavior of MCTS, on a highly optimized real-world application, on real hardware. The Intel Xeon Phi allows shared memory scaling studies up to 61 cores and 244 hardware threads. We compare work-stealing (Cilk Plus and TBB) and work-sharing (FIFO scheduling) approaches. Interestingly, we find that a straightforward thread pool with a work-sharing FIFO queue shows the best performance. A crucial element for this high performance is the controlling of the grain size, an approach that we call Grain Size Controlled Parallel MCTS. Our subsequent comparing with the Xeon CPUs shows an even more comprehensible distinction in performance between different threading libraries. We achieve, to the best of our knowledge, the fastest implementation of a parallel MCTS on the 61 core Intel Xeon Phi using a real application (47 relative to a sequential run). △ Less","15 July, 2015",https://arxiv.org/pdf/1507.04383
Using Automated Theorem Provers to Teach Knowledge Representation in First-Order Logic,Angelo Kyrilov;David Noelle,"Undergraduate students of artificial intelligence often struggle with representing knowledge as logical sentences. This is a skill that seems to require extensive practice to obtain, suggesting a teaching strategy that involves the assignment of numerous exercises involving the formulation of some bit of knowledge, communicated using a natural language such as English, as a sentence in some logic. The number of such exercises needed to master this skill is far too large to allow typical artificial intelligence course teaching teams to provide prompt feedback on student efforts. Thus, an automated assessment system for such exercises is needed to ensure that students receive an adequate amount of practice, with the rapid delivery of feedback allowing students to identify errors in their understanding and correct them. This paper describes an automated grading system for knowledge representation exercises using first-order logic. A resolution theorem prover, \textit{Prover9}, is used to check if a student-submitted formula is logically equivalent to a solution provided by the instructor. This system has been used by students enrolled in undergraduate artificial intelligence classes for several years. Use of this teaching tool resulted in a statistically significant improvement on first-order logic knowledge representation questions appearing on the course final examination. This article explains how this system works, provides an analysis of changes in student learning outcomes, and explores potential enhancements of this system, including the possibility of providing rich formative feedback by replacing the resolution theorem prover with a tableaux-based method. △ Less","13 July, 2015",https://arxiv.org/pdf/1507.03670
Analysis of Microarray Data using Artificial Intelligence Based Techniques,Khalid Raza,"Microarray is one of the essential technologies used by the biologist to measure genome-wide expression levels of genes in a particular organism under some particular conditions or stimuli. As microarrays technologies have become more prevalent, the challenges of analyzing these data for getting better insight about biological processes have essentially increased. Due to availability of artificial intelligence based sophisticated computational techniques, such as artificial neural networks, fuzzy logic, genetic algorithms, and many other nature-inspired algorithms, it is possible to analyse microarray gene expression data in more better way. Here, we reviewed artificial intelligence based techniques for the analysis of microarray gene expression data. Further, challenges in the field and future work direction have also been suggested. △ Less","10 July, 2015",https://arxiv.org/pdf/1507.02870
Scientific Discovery by Machine Intelligence: A New Avenue for Drug Research,Carlo A. Trugenberger,"The majority of big data is unstructured and of this majority the largest chunk is text. While data mining techniques are well developed and standardized for structured, numerical data, the realm of unstructured data is still largely unexplored. The general focus lies on information extraction, which attempts to retrieve known information from text. The Holy Grail, however is knowledge discovery, where machines are expected to unearth entirely new facts and relations that were not previously known by any human expert. Indeed, understanding the meaning of text is often considered as one of the main characteristics of human intelligence. The ultimate goal of semantic artificial intelligence is to devise software that can understand the meaning of free text, at least in the practical sense of providing new, actionable information condensed out of a body of documents. As a stepping stone on the road to this vision I will introduce a totally new approach to drug research, namely that of identifying relevant information by employing a self-organizing semantic engine to text mine large repositories of biomedical research papers, a technique pioneered by Merck with the InfoCodex software. I will describe the methodology and a first successful experiment for the discovery of new biomarkers and phenotypes for diabetes and obesity on the basis of PubMed abstracts, public clinical trials and Merck internal documents. The reported approach shows much promise and has potential to impact fundamentally pharmaceutical research as a way to shorten time-to-market of novel drugs, and for early recognition of dead ends. △ Less","23 June, 2015",https://arxiv.org/pdf/1506.07116
A Survey of Current Datasets for Vision and Language Research,Francis Ferraro;Nasrin Mostafazadeh;Ting-Hao;Huang;Lucy Vanderwende;Jacob Devlin;Michel Galley;Margaret Mitchell,"Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. △ Less","19 August, 2015",https://arxiv.org/pdf/1506.06833
Artificial general intelligence through recursive data compression and grounded reasoning: a position paper,Arthur Franz,"This paper presents a tentative outline for the construction of an artificial, generally intelligent system (AGI). It is argued that building a general data compression algorithm solving all problems up to a complexity threshold should be the main thrust of research. A measure for partial progress in AGI is suggested. Although the details are far from being clear, some general properties for a general compression algorithm are fleshed out. Its inductive bias should be flexible and adapt to the input data while constantly searching for a simple, orthogonal and complete set of hypotheses explaining the data. It should recursively reduce the size of its representations thereby compressing the data increasingly at every iteration. Abstract Based on that fundamental ability, a grounded reasoning system is proposed. It is argued how grounding and flexible feature bases made of hypotheses allow for resourceful thinking. While the simulation of representation contents on the mental stage accounts for much of the power of propositional logic, compression leads to simple sets of hypotheses that allow the detection and verification of universally quantified statements. Abstract Together, it is highlighted how general compression and grounded reasoning could account for the birth and growth of first concepts about the world and the commonsense reasoning about them. △ Less","14 June, 2015",https://arxiv.org/pdf/1506.04366
Place classification with a graph regularized deep neural network model,Yiyi Liao;Sarath Kodagoda;Yue Wang;Lei Shi;Yong Liu,"Place classification is a fundamental ability that a robot should possess to carry out effective human-robot interactions. It is a nontrivial classification problem which has attracted many research. In recent years, there is a high exploitation of Artificial Intelligent algorithms in robotics applications. Inspired by the recent successes of deep learning methods, we propose an end-to-end learning approach for the place classification problem. With the deep architectures, this methodology automatically discovers features and contributes in general to higher classification accuracies. The pipeline of our approach is composed of three parts. Firstly, we construct multiple layers of laser range data to represent the environment information in different levels of granularity. Secondly, each layer of data is fed into a deep neural network model for classification, where a graph regularization is imposed to the deep architecture for keeping local consistency between adjacent samples. Finally, the predicted labels obtained from all the layers are fused based on confidence trees to maximize the overall confidence. Experimental results validate the effective- ness of our end-to-end place classification framework in which both the multi-layer structure and the graph regularization promote the classification performance. Furthermore, results show that the features automatically learned from the raw input range data can achieve competitive results to the features constructed based on statistical and geometrical information. △ Less","12 June, 2015",https://arxiv.org/pdf/1506.03899
Open Ended Intelligence: The individuation of Intelligent Agents,David Weinbaum;Viktoras Veitas,"Artificial General Intelligence is a field of research aiming to distill the principles of intelligence that operate independently of a specific problem domain or a predefined context and utilize these principles in order to synthesize systems capable of performing any intellectual task a human being is capable of and eventually go beyond that. While ""narrow"" artificial intelligence which focuses on solving specific problems such as speech recognition, text comprehension, visual pattern recognition, robotic motion, etc. has shown quite a few impressive breakthroughs lately, understanding general intelligence remains elusive. In the paper we offer a novel theoretical approach to understanding general intelligence. We start with a brief introduction of the current conceptual approach. Our critique exposes a number of serious limitations that are traced back to the ontological roots of the concept of intelligence. We then propose a paradigm shift from intelligence perceived as a competence of individual agents defined in relation to an a priori given problem domain or a goal, to intelligence perceived as a formative process of self-organization by which intelligent agents are individuated. We call this process open-ended intelligence. Open-ended intelligence is developed as an abstraction of the process of cognitive development so its application can be extended to general agents and systems. We introduce and discuss three facets of the idea: the philosophical concept of individuation, sense-making and the individuation of general cognitive agents. We further show how open-ended intelligence can be framed in terms of a distributed, self-organizing network of interacting elements and how such process is scalable. The framework highlights an important relation between coordination and intelligence and a new understanding of values. We conclude with a number of questions for future research. △ Less","12 June, 2015",https://arxiv.org/pdf/1505.06366
The evolutionary origins of hierarchy,Henok Mengistu;Joost Huizinga;Jean-Baptiste Mouret;Jeff Clune,"Hierarchical organization -- the recursive composition of sub-modules -- is ubiquitous in biological networks, including neural, metabolic, ecological, and genetic regulatory networks, and in human-made systems, such as large organizations and the Internet. To date, most research on hierarchy in networks has been limited to quantifying this property. However, an open, important question in evolutionary biology is why hierarchical organization evolves in the first place. It has recently been shown that modularity evolves because of the presence of a cost for network connections. Here we investigate whether such connection costs also tend to cause a hierarchical organization of such modules. In computational simulations, we find that networks without a connection cost do not evolve to be hierarchical, even when the task has a hierarchical structure. However, with a connection cost, networks evolve to be both modular and hierarchical, and these networks exhibit higher overall performance and evolvability (i.e. faster adaptation to new environments). Additional analyses confirm that hierarchy independently improves adaptability after controlling for modularity. Overall, our results suggest that the same force--the cost of connections--promotes the evolution of both hierarchy and modularity, and that these properties are important drivers of network performance and adaptability. In addition to shedding light on the emergence of hierarchy across the many domains in which it appears, these findings will also accelerate future research into evolving more complex, intelligent computational brains in the fields of artificial intelligence and robotics. △ Less","23 May, 2015",https://arxiv.org/pdf/1505.06353
Towards a Simulation-Based Programming Paradigm for AI applications,Jörg Pührer,We present initial ideas for a programming paradigm based on simulation that is targeted towards applications of artificial intelligence (AI). The approach aims at integrating techniques from different areas of AI and is based on the idea that simulated entities may freely exchange data and behavioural patterns. We define basic notions of a simulation-based programming paradigm and show how it can be used for implementing AI applications. △ Less,"20 May, 2015",https://arxiv.org/pdf/1505.05373
What is Learning? A primary discussion about information and Representation,Hao Wu,"Nowadays, represented by Deep Learning techniques, the field of machine learning is experiencing unprecedented prosperity and its influence is demonstrated in academia, industry and civil society. ""Intelligent"" has become a label which could not be neglected for most applications; celebrities and scientists also warned that the development of full artificial intelligence may spell the end of the human race. It seems that the answer to building a computer system that could automatically improve with experience is right on the next corner. While for AI and machine learning researchers, it is a consensus that we are not anywhere near the core technique which could bring the Terminator, Number 5 or R2D2 into real life, and there is not even a formal definition about what is intelligence, or one of its basic properties: Learning. Therefore, even though researchers know these concerns are not necessary currently, there is no generalized explanation about why these concerns are not necessary, and what properties people should take into account that would make these concerns to be necessary. In this paper, starts from analysing the relation between information and its representation, a necessary condition for a model to be a learning model is proposed. This condition and related future works could be used to verify whether a system is able to learn or not, and enrich our understanding of learning: one important property of Intelligence. △ Less","18 May, 2015",https://arxiv.org/pdf/1505.04813
Do PageRank-based author rankings outperform simple citation counts?,Dalibor Fiala;Lovro Šubelj;Slavko Žitnik;Marko Bajec,"The basic indicators of a researcher's productivity and impact are still the number of publications and their citation counts. These metrics are clear, straightforward, and easy to obtain. When a ranking of scholars is needed, for instance in grant, award, or promotion procedures, their use is the fastest and cheapest way of prioritizing some scientists over others. However, due to their nature, there is a danger of oversimplifying scientific achievements. Therefore, many other indicators have been proposed including the usage of the PageRank algorithm known for the ranking of webpages and its modifications suited to citation networks. Nevertheless, this recursive method is computationally expensive and even if it has the advantage of favouring prestige over popularity, its application should be well justified, particularly when compared to the standard citation counts. In this study, we analyze three large datasets of computer science papers in the categories of artificial intelligence, software engineering, and theory and methods and apply 12 different ranking methods to the citation networks of authors. We compare the resulting rankings with self-compiled lists of outstanding researchers selected as frequent editorial board members of prestigious journals in the field and conclude that there is no evidence of PageRank-based methods outperforming simple citation counts. △ Less","12 May, 2015",https://arxiv.org/pdf/1505.03008
Graphical Potential Games,Luis E. Ortiz,"Potential games, originally introduced in the early 1990's by Lloyd Shapley, the 2012 Nobel Laureate in Economics, and his colleague Dov Monderer, are a very important class of models in game theory. They have special properties such as the existence of Nash equilibria in pure strategies. This note introduces graphical versions of potential games. Special cases of graphical potential games have already found applicability in many areas of science and engineering beyond economics, including artificial intelligence, computer vision, and machine learning. They have been effectively applied to the study and solution of important real-world problems such as routing and congestion in networks, distributed resource allocation (e.g., public goods), and relaxation-labeling for image segmentation. Implicit use of graphical potential games goes back at least 40 years. Several classes of games considered standard in the literature, including coordination games, local interaction games, lattice games, congestion games, and party-affiliation games, are instances of graphical potential games. This note provides several characterizations of graphical potential games by leveraging well-known results from the literature on probabilistic graphical models. A major contribution of the work presented here that particularly distinguishes it from previous work is establishing that the convergence of certain type of game-playing rules implies that the agents/players must be embedded in some graphical potential game. △ Less","6 May, 2015",https://arxiv.org/pdf/1505.01539
Ascribing Consciousness to Artificial Intelligence,Murray Shanahan,"This paper critically assesses the anti-functionalist stance on consciousness adopted by certain advocates of integrated information theory (IIT), a corollary of which is that human-level artificial intelligence implemented on conventional computing hardware is necessarily not conscious. The critique draws on variations of a well-known gradual neuronal replacement thought experiment, as well as bringing out tensions in IIT's treatment of self-knowledge. The aim, though, is neither to reject IIT outright nor to champion functionalism in particular. Rather, it is suggested that both ideas have something to offer a scientific understanding of consciousness, as long as they are not dressed up as solutions to illusory metaphysical problems. As for human-level AI, we must await its development before we can decide whether or not to ascribe consciousness to it. △ Less","5 September, 2015",https://arxiv.org/pdf/1504.05696
Online Learning Algorithm for Time Series Forecasting Suitable for Low Cost Wireless Sensor Networks Nodes,Juan Pardo;Francisco Zamora-Martinez;Paloma Botella-Rocamora,"Time series forecasting is an important predictive methodology which can be applied to a wide range of problems. Particularly, forecasting the indoor temperature permits an improved utilization of the HVAC (Heating, Ventilating and Air Conditioning) systems in a home and thus a better energy efficiency. With such purpose the paper describes how to implement an Artificial Neural Network (ANN) algorithm in a low cost system-on-chip to develop an autonomous intelligent wireless sensor network. The present paper uses a Wireless Sensor Networks (WSN) to monitor and forecast the indoor temperature in a smart home, based on low resources and cost microcontroller technology as the 8051MCU. An on-line learning approach, based on Back-Propagation (BP) algorithm for ANNs, has been developed for real-time time series learning. It performs the model training with every new data that arrive to the system, without saving enormous quantities of data to create a historical database as usual, i.e., without previous knowledge. Consequently to validate the approach a simulation study through a Bayesian baseline model have been tested in order to compare with a database of a real application aiming to see the performance and accuracy. The core of the paper is a new algorithm, based on the BP one, which has been described in detail, and the challenge was how to implement a computational demanding algorithm in a simple architecture with very few hardware resources. △ Less","21 April, 2015",https://arxiv.org/pdf/1504.05517
An Optimized Hybrid Approach for Path Finding,Ahlam Ansari;Mohd Amin Sayyed;Khatija Ratlamwala;Parvin Shaikh,"Path finding algorithm addresses problem of finding shortest path from source to destination avoiding obstacles. There exist various search algorithms namely A*, Dijkstra's and ant colony optimization. Unlike most path finding algorithms which require destination co-ordinates to compute path, the proposed algorithm comprises of a new method which finds path using backtracking without requiring destination co-ordinates. Moreover, in existing path finding algorithm, the number of iterations required to find path is large. Hence, to overcome this, an algorithm is proposed which reduces number of iterations required to traverse the path. The proposed algorithm is hybrid of backtracking and a new technique(modified 8- neighbor approach). The proposed algorithm can become essential part in location based, network, gaming applications. grid traversal, navigation, gaming applications, mobile robot and Artificial Intelligence. △ Less","9 April, 2015",https://arxiv.org/pdf/1504.02281
"Universal Psychometrics Tasks: difficulty, composition and decomposition",Jose Hernandez-Orallo,"This note revisits the concepts of task and difficulty. The notion of cognitive task and its use for the evaluation of intelligent systems is still replete with issues. The view of tasks as MDP in the context of reinforcement learning has been especially useful for the formalisation of learning tasks. However, this alternate interaction does not accommodate well for some other tasks that are usual in artificial intelligence and, most especially, in animal and human evaluation. In particular, we want to have a more general account of episodes, rewards and responses, and, most especially, the computational complexity of the algorithm behind an agent solving a task. This is crucial for the determination of the difficulty of a task as the (logarithm of the) number of computational steps required to acquire an acceptable policy for the task, which includes the exploration of policies and their verification. We introduce a notion of asynchronous-time stochastic tasks. Based on this interpretation, we can see what task difficulty is, what instance difficulty is (relative to a task) and also what task compositions and decompositions are. △ Less","25 March, 2015",https://arxiv.org/pdf/1503.07587
The concept of free will as an infinite metatheoretic recursion,Hanaan Hashim;R. Srikanth,"It is argued that the concept of free will, like the concept of truth in formal languages, requires a separation between an object level and a meta-level for being consistently defined. The Jamesian two-stage model, which deconstructs free will into the causally open ""free"" stage with its closure in the ""will"" stage, is implicitly a move in this direction. However, to avoid the dilemma of determinism, free will additionally requires an infinite regress of causal meta-stages, making free choice a hypertask. We use this model to define free will of the rationalist-compatibilist type. This is shown to provide a natural three-way distinction between quantum indeterminism, freedom and free will, applicable respectively to artificial intelligence (AI), animal agents and human agents. We propose that the causal hierarchy in our model corresponds to a hierarchy of Turing uncomputability. Possible neurobiological and behavioral tests to demonstrate free will experimentally are suggested. Ramifications of the model for physics, evolutionary biology, neuroscience, neuropathological medicine and moral philosophy are briefly outlined. △ Less","21 June, 2015",https://arxiv.org/pdf/1503.06485
Quantifying Morphological Computation based on an Information Decomposition of the Sensorimotor Loop,Keyan Ghazi-Zahedi;Johannes Rauh,"The question how an agent is affected by its embodiment has attracted growing attention in recent years. A new field of artificial intelligence has emerged, which is based on the idea that intelligence cannot be understood without taking into account embodiment. We believe that a formal approach to quantifying the embodiment's effect on the agent's behaviour is beneficial to the fields of artificial life and artificial intelligence. The contribution of an agent's body and environment to its behaviour is also known as morphological computation. Therefore, in this work, we propose a quantification of morphological computation, which is based on an information decomposition of the sensorimotor loop into shared, unique and synergistic information. In numerical simulation based on a formal representation of the sensorimotor loop, we show that the unique information of the body and environment is a good measure for morphological computation. The results are compared to our previously derived quantification of morphological computation. △ Less","17 March, 2015",https://arxiv.org/pdf/1503.05113
How the symbol grounding of living organisms can be realized in artificial agents,J. H. van Hateren,"A system with artificial intelligence usually relies on symbol manipulation, at least partly and implicitly. However, the interpretation of the symbols - what they represent and what they are about - is ultimately left to humans, as designers and users of the system. How symbols can acquire meaning for the system itself, independent of external interpretation, is an unsolved problem. Some grounding of symbols can be obtained by embodiment, that is, by causally connecting symbols (or sub-symbolic variables) to the physical environment, such as in a robot with sensors and effectors. However, a causal connection as such does not produce representation and aboutness of the kind that symbols have for humans. Here I present a theory that explains how humans and other living organisms have acquired the capability to have symbols and sub-symbolic variables that represent, refer to, and are about something else. The theory shows how reference can be to physical objects, but also to abstract objects, and even how it can be misguided (errors in reference) or be about non-existing objects. I subsequently abstract the primary components of the theory from their biological context, and discuss how and under what conditions the theory could be implemented in artificial agents. A major component of the theory is the strong nonlinearity associated with (potentially unlimited) self-reproduction. The latter is likely not acceptable in artificial systems. It remains unclear if goals other than those inherently serving self-reproduction can have aboutness and if such goals could be stabilized. △ Less","17 March, 2015",https://arxiv.org/pdf/1503.04941
Autonomic Resource Management in Virtual Networks,Rashid Mijumbi;Joan Serrat;Juan-Luis Gorricho,"Virtualization enables the building of multiple virtual networks over a shared substrate. One of the challenges to virtualisation is efficient resource allocation. This problem has been found to be NP hard. Therefore, most approaches to it have not only proposed static solutions, but have also made many assumptions to simplify it. In this paper, we propose a distributed, autonomic and artificial intelligence based solution to resource allocation. Our aim is to obtain self-configuring, selfoptimizing, self-healing and context aware virtual networks △ Less","16 March, 2015",https://arxiv.org/pdf/1503.04576
A Minimal Active Inference Agent,Simon McGregor;Manuel Baltieri;Christopher L. Buckley,"Research on the so-called ""free-energy principle'' (FEP) in cognitive neuroscience is becoming increasingly high-profile. To date, introductions to this theory have proved difficult for many readers to follow, but it depends mainly upon two relatively simple ideas: firstly that normative or teleological values can be expressed as probability distributions (active inference), and secondly that approximate Bayesian reasoning can be effectively performed by gradient descent on model parameters (the free-energy principle). The notion of active inference is of great interest for a number of disciplines including cognitive science and artificial intelligence, as well as cognitive neuroscience, and deserves to be more widely known. This paper attempts to provide an accessible introduction to active inference and informational free-energy, for readers from a range of scientific backgrounds. In this work introduce an agent-based model with an agent trying to make predictions about its position in a one-dimensional discretized world using methods from the FEP. △ Less","13 March, 2015",https://arxiv.org/pdf/1503.04187
QoS Guaranteed Intelligent Routing Using Hybrid PSO-GA in Wireless Mesh Networks,V. Sarasvathi;N. Ch. S. N. Iyengar;Snehanshu Saha,"In Multi-Channel Multi-Radio Wireless Mesh Networks (MCMR-WMN), finding the optimal routing by satisfying the Quality of Service (QoS) constraints is an ambitious task. Multiple paths are available from the source node to the gateway for reliability, and sometimes it is necessary to deal with failures of the link in WMN. A major challenge in a MCMR-WMN is finding the routing with QoS satisfied and an interference free path from the redundant paths, in order to transmit the packets through this path. The Particle Swarm Optimization (PSO) is an optimization technique to find the candidate solution in the search space optimally, and it applies artificial intelligence to solve the routing problem. On the other hand, the Genetic Algorithm (GA) is a population based meta-heuristic optimization algorithm inspired by the natural evolution, such as selection,mutation and crossover. PSO can easily fall into a local optimal solution, at the same time GA is not suitable for dynamic data due to the underlying dynamic network. In this paper we propose an optimal intelligent routing, using a Hybrid PSO-GA, which also meets the QoS constraints. Moreover, it integrates the strength of PSO and GA. The QoS constraints, such as bandwidth, delay, jitter and interference are transformed into penalty functions. The simulation results show that the hybrid approach outperforms PSO and GA individually, and it takes less convergence time comparatively, keeping away from converging prematurely. Keywords: Wireless mesh networks, Multi-radio, Multi-channel, Particle swarm optimization, Genetic algorithm, Quality of service. △ Less","12 March, 2015",https://arxiv.org/pdf/1503.03639
Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets,Armand Joulin;Tomas Mikolov,"Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory. △ Less","1 June, 2015",https://arxiv.org/pdf/1503.01007
A Hebbian/Anti-Hebbian Network Derived from Online Non-Negative Matrix Factorization Can Cluster and Discover Sparse Features,Cengiz Pehlevan;Dmitri B. Chklovskii,"Despite our extensive knowledge of biophysical properties of neurons, there is no commonly accepted algorithmic theory of neuronal function. Here we explore the hypothesis that single-layer neuronal networks perform online symmetric nonnegative matrix factorization (SNMF) of the similarity matrix of the streamed data. By starting with the SNMF cost function we derive an online algorithm, which can be implemented by a biologically plausible network with local learning rules. We demonstrate that such network performs soft clustering of the data as well as sparse feature discovery. The derived algorithm replicates many known aspects of sensory anatomy and biophysical properties of neurons including unipolar nature of neuronal activity and synaptic weights, local synaptic plasticity rules and the dependence of learning rate on cumulative neuronal activity. Thus, we make a step towards an algorithmic theory of neuronal function, which should facilitate large-scale neural circuit simulations and biologically inspired artificial intelligence. △ Less","2 March, 2015",https://arxiv.org/pdf/1503.00680
Unified vector space mapping for knowledge representation systems,Dmytro Filatov;Taras Filatov,"One of the most significant problems which inhibits further developments in the areas of Knowledge Representation and Artificial Intelligence is a problem of semantic alignment or knowledge mapping. The progress in its solution will be greatly beneficial for further advances of information retrieval, ontology alignment, relevance calculation, text mining, natural language processing etc. In the paper the concept of multidimensional global knowledge map, elaborated through unsupervised extraction of dependencies from large documents corpus, is proposed. In addition, the problem of direct Human - Knowledge Representation System interface is addressed and a concept of adaptive decoder proposed for the purpose of interaction with previously described unified mapping model. In combination these two approaches are suggested as basis for a development of a new generation of knowledge representation systems. △ Less","21 February, 2015",https://arxiv.org/pdf/1502.06124
Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review,Selma Dilek;Hüseyin Çakır;Mustafa Aydın,"With the advances in information technology (IT) criminals are using cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly vulnerable to intrusions and other threats. Physical devices and human intervention are not sufficient for monitoring and protection of these infrastructures; hence, there is a need for more sophisticated cyber defense systems that need to be flexible, adaptable and robust, and able to detect a wide variety of threats and make intelligent real-time decisions. Numerous bio-inspired computing methods of Artificial Intelligence have been increasingly playing an important role in cyber crime detection and prevention. The purpose of this study is to present advances made so far in the field of applying AI techniques for combating cyber crimes, to demonstrate how these techniques can be an effective tool for detection and prevention of cyber attacks, as well as to give the scope for future work. △ Less","12 February, 2015",https://arxiv.org/pdf/1502.03552
A Quantum Production Model,Luís Tarrataca;Andreas Wichert,The production system is a theoretical model of computation relevant to the artificial intelligence field allowing for problem solving procedures such as hierarchical tree search. In this work we explore some of the connections between artificial intelligence and quantum computation by presenting a model for a quantum production system. Our approach focuses on initially developing a model for a reversible production system which is a simple mapping of Bennett's reversible Turing machine. We then expand on this result in order to accommodate for the requirements of quantum computation. We present the details of how our proposition can be used alongside Grover's algorithm in order to yield a speedup comparatively to its classical counterpart. We discuss the requirements associated with such a speedup and how it compares against a similar quantum hierarchical search approach. △ Less,"6 February, 2015",https://arxiv.org/pdf/1502.02029
"The Information-theoretic and Algorithmic Approach to Human, Animal and Artificial Cognition",Nicolas Gauvrit;Hector Zenil;Jesper Tegnér,"We survey concepts at the frontier of research connecting artificial, animal and human cognition to computation and information processing---from the Turing test to Searle's Chinese Room argument, from Integrated Information Theory to computational and algorithmic complexity. We start by arguing that passing the Turing test is a trivial computational problem and that its pragmatic difficulty sheds light on the computational nature of the human mind more than it does on the challenge of artificial intelligence. We then review our proposed algorithmic information-theoretic measures for quantifying and characterizing cognition in various forms. These are capable of accounting for known biases in human behavior, thus vindicating a computational algorithmic view of cognition as first suggested by Turing, but this time rooted in the concept of algorithmic probability, which in turn is based on computational universality while being independent of computational model, and which has the virtue of being predictive and testable as a model theory of cognitive behavior. △ Less","24 December, 2015",https://arxiv.org/pdf/1501.04242
High performance photonic reservoir computer based on a coherently driven passive cavity,Quentin Vinckier;François Duport;Anteo Smerieri;Kristof Vandoorne;Peter Bienstman;Marc Haelterman;Serge Massar,"Reservoir computing is a recent bio-inspired approach for processing time-dependent signals. It has enabled a breakthrough in analog information processing, with several experiments, both electronic and optical, demonstrating state-of-the-art performances for hard tasks such as speech recognition, time series prediction and nonlinear channel equalization. A proof-of-principle experiment using a linear optical circuit on a photonic chip to process digital signals was recently reported. Here we present a photonic implementation of a reservoir computer based on a coherently driven passive fiber cavity processing analog signals. Our experiment has error rate as low or lower than previous experiments on a wide variety of tasks, and also has lower power consumption. Furthermore, the analytical model describing our experiment is also of interest, as it constitutes a very simple high performance reservoir computer algorithm. The present experiment, given its good performances, low energy consumption and conceptual simplicity, confirms the great potential of photonic reservoir computing for information processing applications ranging from artificial intelligence to telecommunications △ Less","20 May, 2015",https://arxiv.org/pdf/1501.03024
Tri-Subject Kinship Verification: Understanding the Core of A Family,Xiaoqian Qin;Xiaoyang Tan;Songcan Chen,"One major challenge in computer vision is to go beyond the modeling of individual objects and to investigate the bi- (one-versus-one) or tri- (one-versus-two) relationship among multiple visual entities, answering such questions as whether a child in a photo belongs to given parents. The child-parents relationship plays a core role in a family and understanding such kin relationship would have fundamental impact on the behavior of an artificial intelligent agent working in the human world. In this work, we tackle the problem of one-versus-two (tri-subject) kinship verification and our contributions are three folds: 1) a novel relative symmetric bilinear model (RSBM) introduced to model the similarity between the child and the parents, by incorporating the prior knowledge that a child may resemble a particular parent more than the other; 2) a spatially voted method for feature selection, which jointly selects the most discriminative features for the child-parents pair, while taking local spatial information into account; 3) a large scale tri-subject kinship database characterized by over 1,000 child-parents families. Extensive experiments on KinFaceW, Family101 and our newly released kinship database show that the proposed method outperforms several previous state of the art methods, while could also be used to significantly boost the performance of one-versus-one kinship verification when the information about both parents are available. △ Less","21 July, 2015",https://arxiv.org/pdf/1501.02555
Translating Videos to Natural Language Using Deep Recurrent Neural Networks,Subhashini Venugopalan;Huijuan Xu;Jeff Donahue;Marcus Rohrbach;Raymond Mooney;Kate Saenko,"Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation. △ Less","30 April, 2015",https://arxiv.org/pdf/1412.4729
Quantum Deep Learning,Nathan Wiebe;Ashish Kapoor;Krysta M. Svore,"In recent years, deep learning has had a profound impact on machine learning and artificial intelligence. At the same time, algorithms for quantum computers have been shown to efficiently solve some problems that are intractable on conventional, classical computers. We show that quantum computing not only reduces the time required to train a deep restricted Boltzmann machine, but also provides a richer and more comprehensive framework for deep learning than classical computing and leads to significant improvements in the optimization of the underlying objective function. Our quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well known classical counterparts. △ Less","21 May, 2015",https://arxiv.org/pdf/1412.3489
Declarative Statistical Modeling with Datalog,Vince Barany;Balder ten Cate;Benny Kimelfeld;Dan Olteanu;Zografoula Vagena,"Formalisms for specifying statistical models, such as probabilistic-programming languages, typically consist of two components: a specification of a stochastic process (the prior), and a specification of observations that restrict the probability space to a conditional subspace (the posterior). Use cases of such formalisms include the development of algorithms in machine learning and artificial intelligence. We propose and investigate a declarative framework for specifying statistical models on top of a database, through an appropriate extension of Datalog. By virtue of extending Datalog, our framework offers a natural integration with the database, and has a robust declarative semantics. Our Datalog extension provides convenient mechanisms to include numerical probability functions; in particular, conclusions of rules may contain values drawn from such functions. The semantics of a program is a probability distribution over the possible outcomes of the input database with respect to the program; these outcomes are minimal solutions with respect to a related program with existentially quantified variables in conclusions. Observations are naturally incorporated by means of integrity constraints over the extensional and intensional relations. We focus on programs that use discrete numerical distributions, but even then the space of possible outcomes may be uncountable (as a solution can be infinite). We define a probability measure over possible outcomes by applying the known concept of cylinder sets to a probabilistic chase procedure. We show that the resulting semantics is robust under different chases. We also identify conditions guaranteeing that all possible outcomes are finite (and then the probability space is discrete). We argue that the framework we propose retains the purely declarative nature of Datalog, and allows for natural specifications of statistical models. △ Less","5 January, 2015",https://arxiv.org/pdf/1412.2221
Show and Tell: A Neural Image Caption Generator,Oriol Vinyals;Alexander Toshev;Samy Bengio;Dumitru Erhan,"Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art. △ Less","20 April, 2015",https://arxiv.org/pdf/1411.4555
The Limitations of Standardized Science Tests as Benchmarks for Artificial Intelligence Research: Position Paper,Ernest Davis,"In this position paper, I argue that standardized tests for elementary science such as SAT or Regents tests are not very good benchmarks for measuring the progress of artificial intelligence systems in understanding basic science. The primary problem is that these tests are designed to test aspects of knowledge and ability that are challenging for people; the aspects that are challenging for AI systems are very different. In particular, standardized tests do not test knowledge that is obvious for people; none of this knowledge can be assumed in AI systems. Individual standardized tests also have specific features that are not necessarily appropriate for an AI benchmark. I analyze the Physics subject SAT in some detail and the New York State Regents Science test more briefly. I also argue that the apparent advantages offered by using standardized tests are mostly either minor or illusory. The one major real advantage is that the significance is easily explained to the public; but I argue that even this is a somewhat mixed blessing. I conclude by arguing that, first, more appropriate collections of exam style problems could be assembled, and second, that there are better kinds of benchmarks than exam-style problems. In an appendix I present a collection of sample exam-style problems that test kinds of knowledge missing from the standardized tests. △ Less","16 October, 2015",https://arxiv.org/pdf/1411.1629
Ethical Artificial Intelligence,Bill Hibbard,"This book-length article combines several peer reviewed papers and new material to analyze the issues of ethical artificial intelligence (AI). The behavior of future AI systems can be described by mathematical equations, which are adapted to analyze possible unintended AI behaviors and ways that AI designs can avoid them. This article makes the case for utility-maximizing agents and for avoiding infinite sets in agent definitions. It shows how to avoid agent self-delusion using model-based utility functions and how to avoid agents that corrupt their reward generators (sometimes called ""perverse instantiation"") using utility functions that evaluate outcomes at one point in time from the perspective of humans at a different point in time. It argues that agents can avoid unintended instrumental actions (sometimes called ""basic AI drives"" or ""instrumental goals"") by accurately learning human values. This article defines a self-modeling agent framework and shows how it can avoid problems of resource limits, being predicted by other agents, and inconsistency between the agent's utility function and its definition (one version of this problem is sometimes called ""motivated value selection""). This article also discusses how future AI will differ from current AI, the politics of AI, and the ultimate use of AI to help understand the nature of the universe and our place in it. △ Less","17 November, 2015",https://arxiv.org/pdf/1411.1373
Towards a General Framework for Actual Causation Using CP-logic,Sander Beckers;Joost Vennekens,"Since Pearl's seminal work on providing a formal language for causality, the subject has garnered a lot of interest among philosophers and researchers in artificial intelligence alike. One of the most debated topics in this context regards the notion of actual causation, which concerns itself with specific - as opposed to general - causal claims. The search for a proper formal definition of actual causation has evolved into a controversial debate, that is pervaded with ambiguities and confusion. The goal of our research is twofold. First, we wish to provide a clear way to compare competing definitions. Second, we also want to improve upon these definitions so they can be applied to a more diverse range of instances, including non-deterministic ones. To achieve these goals we will provide a general, abstract definition of actual causation, formulated in the context of the expressive language of CP-logic (Causal Probabilistic logic). We will then show that three recent definitions by Ned Hall (originally formulated for structural models) and a definition of our own (formulated for CP-logic directly) can be viewed and directly compared as instantiations of this abstract definition, which allows them to deal with a broader range of examples. △ Less","29 October, 2015",https://arxiv.org/pdf/1410.7063
Convolutional Neural Networks over Tree Structures for Programming Language Processing,Lili Mou;Ge Li;Lu Zhang;Tao Wang;Zhi Jin,"Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional NLP models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network (TBCNN) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. TBCNN is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. TBCNN outperforms baseline methods, including several neural models for NLP. △ Less","8 December, 2015",https://arxiv.org/pdf/1409.5718
Giving the AI definition a form suitable for the engineer,Dimiter Dobrev,"Artificial Intelligence - what is this? That is the question! In earlier papers we already gave a formal definition for AI, but if one desires to build an actual AI implementation, the following issues require attention and are treated here: the data format to be used, the idea of Undef and Nothing symbols, various ways for defining the ""meaning of life"", and finally, a new notion of ""incorrect move"". These questions are of minor importance in the theoretical discussion, but we already know the answer of the question ""Does AI exist?"" Now we want to make the next step and to create this program. △ Less","31 March, 2015",https://arxiv.org/pdf/1312.5713
Machine Learning Techniques for Intrusion Detection,Mahdi Zamani;Mahnush Movahedi,"An Intrusion Detection System (IDS) is a software that monitors a single or a network of computers for malicious activities (attacks) that are aimed at stealing or censoring information or corrupting network protocols. Most techniques used in today's IDS are not able to deal with the dynamic and complex nature of cyber attacks on computer networks. Hence, efficient adaptive methods like various techniques of machine learning can result in higher detection rates, lower false alarm rates and reasonable computation and communication costs. In this paper, we study several such schemes and compare their performance. We divide the schemes into methods based on classical artificial intelligence (AI) and methods based on computational intelligence (CI). We explain how various characteristics of CI techniques can be used to build efficient IDS. △ Less","9 May, 2015",https://arxiv.org/pdf/1312.2177
"Information Compression, Intelligence, Computing, and Mathematics",J. Gerard Wolff,"This paper presents evidence for the idea that much of artificial intelligence, human perception and cognition, mainstream computing, and mathematics, may be understood as compression of information via the matching and unification of patterns. This is the basis for the ""SP theory of intelligence"", outlined in the paper and fully described elsewhere. Relevant evidence may be seen: in empirical support for the SP theory; in some advantages of information compression (IC) in terms of biology and engineering; in our use of shorthands and ordinary words in language; in how we merge successive views of any one thing; in visual recognition; in binocular vision; in visual adaptation; in how we learn lexical and grammatical structures in language; and in perceptual constancies. IC via the matching and unification of patterns may be seen in both computing and mathematics: in IC via equations; in the matching and unification of names; in the reduction or removal of redundancy from unary numbers; in the workings of Post's Canonical System and the transition function in the Universal Turing Machine; in the way computers retrieve information from memory; in systems like Prolog; and in the query-by-example technique for information retrieval. The chunking-with-codes technique for IC may be seen in the use of named functions to avoid repetition of computer code. The schema-plus-correction technique may be seen in functions with parameters and in the use of classes in object-oriented programming. And the run-length coding technique may be seen in multiplication, in division, and in several other devices in mathematics and computing. The SP theory resolves the apparent paradox of ""decompression by compression"". And computing and cognition as IC is compatible with the uses of redundancy in such things as backup copies to safeguard data and understanding speech in a noisy environment. △ Less","13 July, 2015",https://arxiv.org/pdf/1310.8599
Evolution of swarming behavior is shaped by how predators attack,Randal S. Olson;David B. Knoester;Christoph Adami,"Animal grouping behaviors have been widely studied due to their implications for understanding social intelligence, collective cognition, and potential applications in engineering, artificial intelligence, and robotics. An important biological aspect of these studies is discerning which selection pressures favor the evolution of grouping behavior. In the past decade, researchers have begun using evolutionary computation to study the evolutionary effects of these selection pressures in predator-prey models. The selfish herd hypothesis states that concentrated groups arise because prey selfishly attempt to place their conspecifics between themselves and the predator, thus causing an endless cycle of movement toward the center of the group. Using an evolutionary model of a predator-prey system, we show that how predators attack is critical to the evolution of the selfish herd. Following this discovery, we show that density-dependent predation provides an abstraction of Hamilton's original formulation of ``domains of danger.'' Finally, we verify that density-dependent predation provides a sufficient selective advantage for prey to evolve the selfish herd in response to predation by coevolving predators. Thus, our work corroborates Hamilton's selfish herd hypothesis in a digital evolutionary model, refines the assumptions of the selfish herd hypothesis, and generalizes the domain of danger concept to density-dependent predation. △ Less","24 November, 2015",https://arxiv.org/pdf/1310.6012
The SP theory of intelligence: an overview,J. Gerard Wolff,"This article is an overview of the ""SP theory of intelligence"". The theory aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme. It is conceived as a brain-like system that receives 'New' information and stores some or all of it in compressed form as 'Old' information. It is realised in the form of a computer model -- a first version of the SP machine. The concept of ""multiple alignment"" is a powerful central idea. Using heuristic techniques, the system builds multiple alignments that are 'good' in terms of information compression. For each multiple alignment, probabilities may be calculated. These provide the basis for calculating the probabilities of inferences. The system learns new structures from partial matches between patterns. Using heuristic techniques, the system searches for sets of structures that are 'good' in terms of information compression. These are normally ones that people judge to be 'natural', in accordance with the 'DONSVIC' principle -- the discovery of natural structures via information compression. The SP theory may be applied in several areas including 'computing', aspects of mathematics and logic, representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, information storage and retrieval, planning and problem solving, information compression, neuroscience, and human perception and cognition. Examples include the parsing and production of language including discontinuous dependencies in syntax, pattern recognition at multiple levels of abstraction and its integration with part-whole relations, nonmonotonic reasoning and reasoning with default values, reasoning in Bayesian networks including 'explaining away', causal diagnosis, and the solving of a geometric analogy problem. △ Less","7 January, 2015",https://arxiv.org/pdf/1306.3888
