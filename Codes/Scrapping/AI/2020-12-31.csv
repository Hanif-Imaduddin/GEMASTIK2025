title,authors,abstract,submitted_date,pdf_link
"LAIF: AI, Deep Learning for Germany Suetterlin Letter Recognition and Generation",Enkhtogtokh Togootogtokh;Christian Klasen,"One of the successful early implementation of deep learning AI technology was on letter recognition. With the recent breakthrough of artificial intelligence (AI) brings more solid technology for complex problems like handwritten letter recognition and even automatic generation of them. In this research, we proposed deep learning framework called Ludwig AI Framework(LAIF) for Germany Suetterlin letter recognition and generation. To recognize Suetterlin letter, we proposed deep convolutional neural network. Since lack of big amount of data to train for the deep models and huge cost to label existing hard copy of handwritten letters, we also introduce the methodology with deep generative adversarial network to generate handwritten letters as synthetic data. Main source code is in https://github.com/enkhtogtokh/LAIF repository. △ Less","30 December, 2020",https://arxiv.org/pdf/2101.10450
"Neural Networks, Artificial Intelligence and the Computational Brain",Martin C. Nwadiugwu,"In recent years, several studies have provided insight on the functioning of the brain which consists of neurons and form networks via interconnection among them by synapses. Neural networks are formed by interconnected systems of neurons, and are of two types, namely, the Artificial Neural Network (ANNs) and Biological Neural Network (interconnected nerve cells). The ANNs are computationally influenced by human neurons and are used in modelling neural systems. The reasoning foundations of ANNs have been useful in anomaly detection, in areas of medicine such as instant physician, electronic noses, pattern recognition, and modelling biological systems. Advancing research in artificial intelligence using the architecture of the human brain seeks to model systems by studying the brain rather than looking to technology for brain models. This study explores the concept of ANNs as a simulator of the biological neuron, and its area of applications. It also explores why brain-like intelligence is needed and how it differs from computational framework by comparing neural networks to contemporary computers and their modern day implementation. △ Less","25 December, 2020",https://arxiv.org/pdf/2101.08635
The case for psychometric artificial general intelligence,Mark McPherson,"A short review of the literature on measurement and detection of artificial general intelligence is made. Proposed benchmarks and tests for artificial general intelligence are critically evaluated against multiple criteria. Based on the findings, the most promising approaches are identified and some useful directions for future work are proposed. △ Less","27 December, 2020",https://arxiv.org/pdf/2101.02179
A survey of the European Union's artificial intelligence ecosystem,Charlotte Stix,"Compared to other global powers, the European Union (EU) is rarely considered a leading player in the development of artificial intelligence (AI). Why is this, and does this in fact accurately reflect the activities of the EU? What would it take for the EU to take a more leading role in AI? This report surveys core components of the current AI ecosystem of the EU, providing the crucial background context for answering these questions. △ Less","28 December, 2020",https://arxiv.org/pdf/2101.02039
A Review of Artificial Intelligence Technologies for Early Prediction of Alzheimer's Disease,Kuo Yang;Emad A. Mohammed,"Alzheimer's Disease (AD) is a severe brain disorder, destroying memories and brain functions. AD causes chronically, progressively, and irreversibly cognitive declination and brain damages. The reliable and effective evaluation of early dementia has become essential research with medical imaging technologies and computer-aided algorithms. This trend has moved to modern Artificial Intelligence (AI) technologies motivated by deeplearning success in image classification and natural language processing. The purpose of this review is to provide an overview of the latest research involving deep-learning algorithms in evaluating the process of dementia, diagnosing the early stage of AD, and discussing an outlook for this research. This review introduces various applications of modern AI algorithms in AD diagnosis, including Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Automatic Image Segmentation, Autoencoder, Graph CNN (GCN), Ensemble Learning, and Transfer Learning. The advantages and disadvantages of the proposed methods and their performance are discussed. The conclusion section summarizes the primary contributions and medical imaging preprocessing techniques applied in the reviewed research. Finally, we discuss the limitations and future outlooks. △ Less","21 December, 2020",https://arxiv.org/pdf/2101.01781
Deep Unsupervised Identification of Selected SNPs between Adapted Populations on Pool-seq Data,Julia Siekiera;Stefan Kramer,"The exploration of selected single nucleotide polymorphisms (SNPs) to identify genetic diversity between different sequencing population pools (Pool-seq) is a fundamental task in genetic research. As underlying sequence reads and their alignment are error-prone and univariate statistical solutions only take individual positions of the genome into account, the identification of selected SNPs remains a challenging process. Deep learning models like convolutional neural networks (CNNs) are able to consider large input areas in their decisions. We suggest an unsupervised pipeline to be independent of a rarely known ground truth. We train a supervised discriminator CNN to distinguish alignments from different populations and utilize the model for unsupervised SNP calling by applying explainable artificial intelligence methods. Our proposed multivariate method is based on two main assumptions: We assume (i) that instances having a high predictive certainty of being distinguishable are likely to contain genetic variants, and (ii) that selected SNPs are located at regions with input features having the highest influence on the model's decision process. We directly compare our method with statistical results on two different Pool-seq datasets and show that our solution is able to extend statistical results. △ Less","28 December, 2020",https://arxiv.org/pdf/2101.00004
Limitations of Deep Neural Networks: a discussion of G. Marcus' critical appraisal of deep learning,Stefanos Tsimenidis,"Deep neural networks have triggered a revolution in artificial intelligence, having been applied with great results in medical imaging, semi-autonomous vehicles, ecommerce, genetics research, speech recognition, particle physics, experimental art, economic forecasting, environmental science, industrial manufacturing, and a wide variety of applications in nearly every field. This sudden success, though, may have intoxicated the research community and blinded them to the potential pitfalls of assigning deep learning a higher status than warranted. Also, research directed at alleviating the weaknesses of deep learning may seem less attractive to scientists and engineers, who focus on the low-hanging fruit of finding more and more applications for deep learning models, thus letting short-term benefits hamper long-term scientific progress. Gary Marcus wrote a paper entitled Deep Learning: A Critical Appraisal, and here we discuss Marcus' core ideas, as well as attempt a general assessment of the subject. This study examines some of the limitations of deep neural networks, with the intention of pointing towards potential paths for future research, and of clearing up some metaphysical misconceptions, held by numerous researchers, that may misdirect them. △ Less","22 December, 2020",https://arxiv.org/pdf/2012.15754
The HyperTrac Project: Recent Progress and Future Research Directions on Hypergraph Decompositions,Georg Gottlob;Matthias Lanzinger;Davide Mario Longo;Cem Okulmus;Reinhard Pichler,"Constraint Satisfaction Problems (CSPs) play a central role in many applications in Artificial Intelligence and Operations Research. In general, solving CSPs is NP-complete. The structure of CSPs is best described by hypergraphs. Therefore, various forms of hypergraph decompositions have been proposed in the literature to identify tractable fragments of CSPs. However, also the computation of a concrete hypergraph decomposition is a challenging task in itself. In this paper, we report on recent progress in the study of hypergraph decompositions and we outline several directions for future research. △ Less","29 December, 2020",https://arxiv.org/pdf/2012.14762
Leveraging AI and Intelligent Reflecting Surface for Energy-Efficient Communication in 6G IoT,Qianqian Pan;Jun Wu;Xi Zheng;Jianhua Li;Shenghong Li;Athanasios V. Vasilakos,"The ever-increasing data traffic, various delay-sensitive services, and the massive deployment of energy-limited Internet of Things (IoT) devices have brought huge challenges to the current communication networks, motivating academia and industry to move to the sixth-generation (6G) network. With the powerful capability of data transmission and processing, 6G is considered as an enabler for IoT communication with low latency and energy cost. In this paper, we propose an artificial intelligence (AI) and intelligent reflecting surface (IRS) empowered energy-efficiency communication system for 6G IoT. First, we design a smart and efficient communication architecture including the IRS-aided data transmission and the AI-driven network resource management mechanisms. Second, an energy efficiency-maximizing model under given transmission latency for 6G IoT system is formulated, which jointly optimizes the settings of all communication participants, i.e. IoT transmission power, IRS-reflection phase shift, and BS detection matrix. Third, a deep reinforcement learning (DRL) empowered network resource control and allocation scheme is proposed to solve the formulated optimization model. Based on the network and channel status, the DRL-enabled scheme facilities the energy-efficiency and low-latency communication. Finally, experimental results verified the effectiveness of our proposed communication system for 6G IoT. △ Less","29 December, 2020",https://arxiv.org/pdf/2012.14716
Model Optimization for Deep Space Exploration via Simulators and Deep Learning,James Bird;Kellan Colburn;Linda Petzold;Philip Lubin,"Machine learning, and eventually true artificial intelligence techniques, are extremely important advancements in astrophysics and astronomy. We explore the application of deep learning using neural networks in order to automate the detection of astronomical bodies for future exploration missions, such as missions to search for signatures or suitability of life. The ability to acquire images, analyze them, and send back those that are important, as determined by the deep learning algorithm, is critical in bandwidth-limited applications. Our previous foundational work solidified the concept of using simulator images and deep learning in order to detect planets. Optimization of this process is of vital importance, as even a small loss in accuracy might be the difference between capturing and completely missing a possibly-habitable nearby planet. Through computer vision, deep learning, and simulators, we introduce methods that optimize the detection of exoplanets. We show that maximum achieved accuracy can hit above 98% for multiple model architectures, even with a relatively small training set. △ Less","27 December, 2020",https://arxiv.org/pdf/2012.14092
SMART: A Situation Model for Algebra Story Problems via Attributed Grammar,Yining Hong;Qing Li;Ran Gong;Daniel Ciao;Siyuan Huang;Song-Chun Zhu,"Solving algebra story problems remains a challenging task in artificial intelligence, which requires a detailed understanding of real-world situations and a strong mathematical reasoning capability. Previous neural solvers of math word problems directly translate problem texts into equations, lacking an explicit interpretation of the situations, and often fail to handle more sophisticated situations. To address such limits of neural solvers, we introduce the concept of a \emph{situation model}, which originates from psychology studies to represent the mental states of humans in problem-solving, and propose \emph{SMART}, which adopts attributed grammar as the representation of situation models for algebra story problems. Specifically, we first train an information extraction module to extract nodes, attributes, and relations from problem texts and then generate a parse graph based on a pre-defined attributed grammar. An iterative learning strategy is also proposed to improve the performance of SMART further. To rigorously study this task, we carefully curate a new dataset named \emph{ASP6.6k}. Experimental results on ASP6.6k show that the proposed model outperforms all previous neural solvers by a large margin while preserving much better interpretability. To test these models' generalization capability, we also design an out-of-distribution (OOD) evaluation, in which problems are more complex than those in the training set. Our model exceeds state-of-the-art models by 17\% in the OOD evaluation, demonstrating its superior generalization ability. △ Less","27 December, 2020",https://arxiv.org/pdf/2012.14011
Towards sample-efficient episodic control with DAC-ML,Ismael T. Freire;Adrián F. Amil;Vasiliki Vouloutsi;Paul F. M. J. Verschure,"The sample-inefficiency problem in Artificial Intelligence refers to the inability of current Deep Reinforcement Learning models to optimize action policies within a small number of episodes. Recent studies have tried to overcome this limitation by adding memory systems and architectural biases to improve learning speed, such as in Episodic Reinforcement Learning. However, despite achieving incremental improvements, their performance is still not comparable to how humans learn behavioral policies. In this paper, we capitalize on the design principles of the Distributed Adaptive Control (DAC) theory of mind and brain to build a novel cognitive architecture (DAC-ML) that, by incorporating a hippocampus-inspired sequential memory system, can rapidly converge to effective action policies that maximize reward acquisition in a challenging foraging task. △ Less","26 December, 2020",https://arxiv.org/pdf/2012.13779
Understanding Team Collaboration in Artificial Intelligence from the perspective of Geographic Distance,Xuli Tang;Xin Li;Ying Ding;Feicheng Ma,"This paper analyzes team collaboration in the field of Artificial Intelligence (AI) from the perspective of geographic distance. We obtained 1,584,175 AI related publications during 1950-2019 from the Microsoft Academic Graph. Three latitude-and-longitude-based indicators were employed to quantify the geographic distance of collaborations in AI over time at domestic and international levels. The results show team collaborations in AI has been more popular in the field over time with around 42,000 (38.4%) multiple-affiliation AI publications in 2019. The changes in geographic distances of team collaborations indicate the increase of breadth and density for both domestic and international collaborations in AI over time. In addition, the United States produced the largest number of single-country and internationally collaborated AI publications, and China has played an important role in international collaborations in AI after 2010. △ Less","25 December, 2020",https://arxiv.org/pdf/2012.13560
Reconfigurable Intelligent Surface Assisted Mobile Edge Computing with Heterogeneous Learning Tasks,Shanfeng Huang;Shuai Wang;Rui Wang;Miaowen Wen;Kaibin Huang,"The ever-growing popularity and rapid improving of artificial intelligence (AI) have raised rethinking on the evolution of wireless networks. Mobile edge computing (MEC) provides a natural platform for AI applications since it is with rich computation resources to train machine learning (ML) models, as well as low-latency access to the data generated by mobile and internet of things (IoT) devices. In this paper, we present an infrastructure to perform ML tasks at an MEC server with the assistance of a reconfigurable intelligent surface (RIS). In contrast to conventional communication systems where the principal criterions are to maximize the throughput, we aim at maximizing the learning performance. Specifically, we minimize the maximum learning error of all participating users by jointly optimizing transmit power of mobile users, beamforming vectors of the base station (BS), and the phase-shift matrix of the RIS. An alternating optimization (AO)-based framework is proposed to optimize the three terms iteratively, where a successive convex approximation (SCA)-based algorithm is developed to solve the power allocation problem, closed-form expressions of the beamforming vectors are derived, and an alternating direction method of multipliers (ADMM)-based algorithm is designed together with an error level searching (ELS) framework to effectively solve the challenging nonconvex optimization problem of the phase-shift matrix. Simulation results demonstrate significant gains of deploying an RIS and validate the advantages of our proposed algorithms over various benchmarks. Lastly, a unified communication-training-inference platform is developed based on the CARLA platform and the SECOND network, and a use case (3D object detection in autonomous driving) for the proposed scheme is demonstrated on the developed platform. △ Less","25 December, 2020",https://arxiv.org/pdf/2012.13533
Security of Connected and Automated Vehicles,Mashrur Chowdhury;Mhafuzul Islam;Zadid Khan,"The transportation system is rapidly evolving with new connected and automated vehicle (CAV) technologies that integrate CAVs with other vehicles and roadside infrastructure in a cyberphysical system (CPS). Through connectivity, CAVs affect their environments and vice versa, increasing the size of the cyberattack surface and the risk of exploitation of security vulnerabilities by malicious actors. Thus, greater understanding of potential CAV-CPS cyberattacks and of ways to prevent them is a high priority. In this article we describe CAV-CPS cyberattack surfaces and security vulnerabilities, and outline potential cyberattack detection and mitigation strategies. We examine emerging technologies - artificial intelligence, software-defined networks, network function virtualization, edge computing, information-centric and virtual dispersive networking, fifth generation (5G) cellular networks, blockchain technology, and quantum and postquantum cryptography - as potential solutions aiding in securing CAVs and transportation infrastructure against existing and future cyberattacks. △ Less","24 December, 2020",https://arxiv.org/pdf/2012.13464
Learning with Retrospection,Xiang Deng;Zhongfei Zhang,"Deep neural networks have been successfully deployed in various domains of artificial intelligence, including computer vision and natural language processing. We observe that the current standard procedure for training DNNs discards all the learned information in the past epochs except the current learned weights. An interesting question is: is this discarded information indeed useless? We argue that the discarded information can benefit the subsequent training. In this paper, we propose learning with retrospection (LWR) which makes use of the learned information in the past epochs to guide the subsequent training. LWR is a simple yet effective training framework to improve accuracies, calibration, and robustness of DNNs without introducing any additional network parameters or inference cost, only with a negligible training overhead. Extensive experiments on several benchmark datasets demonstrate the superiority of LWR for training DNNs. △ Less","23 December, 2020",https://arxiv.org/pdf/2012.13098
Antitrust and Artificial Intelligence (AAI): Antitrust Vigilance Lifecycle and AI Legal Reasoning Autonomy,Lance Eliot,"There is an increasing interest in the entwining of the field of antitrust with the field of Artificial Intelligence (AI), frequently referred to jointly as Antitrust and AI (AAI) in the research literature. This study focuses on the synergies entangling antitrust and AI, doing so to extend the literature by proffering the primary ways that these two fields intersect, consisting of: (1) the application of antitrust to AI, and (2) the application of AI to antitrust. To date, most of the existing research on this intermixing has concentrated on the former, namely the application of antitrust to AI, entailing how the marketplace will be altered by the advent of AI and the potential for adverse antitrust behaviors arising accordingly. Opting to explore more deeply the other side of this coin, this research closely examines the application of AI to antitrust and establishes an antitrust vigilance lifecycle to which AI is predicted to be substantively infused for purposes of enabling and bolstering antitrust detection, enforcement, and post-enforcement monitoring. Furthermore, a gradual and incremental injection of AI into antitrust vigilance is anticipated to occur as significant advances emerge amidst the Levels of Autonomy (LoA) for AI Legal Reasoning (AILR). △ Less","23 December, 2020",https://arxiv.org/pdf/2012.13016
The Last State of Artificial Intelligence in Project Management,Mohammad Reza Davahli,"Artificial intelligence (AI) has been used to advance different fields, such as education, healthcare, and finance. However, the application of AI in the field of project management (PM) has not progressed equally. This paper reports on a systematic review of the published studies used to investigate the application of AI in PM. This systematic review identified relevant papers using Web of Science, Science Direct, and Google Scholar databases. Of the 652 articles found, 58 met the predefined criteria and were included in the review. Included papers were classified per the following dimensions: PM knowledge areas, PM processes, and AI techniques. The results indicated that the application of AI in PM was in its early stages and AI models have not applied for multiple PM processes especially in processes groups of project stakeholder management, project procurements management, and project communication management. However, the most popular PM processes among included papers were project effort prediction and cost estimation, and the most popular AI techniques were support vector machines, neural networks, and genetic algorithms. △ Less","16 December, 2020",https://arxiv.org/pdf/2012.12262
Domain Adaptation of NMT models for English-Hindi Machine Translation Task at AdapMT ICON 2020,Ramchandra Joshi;Rushabh Karnavat;Kaustubh Jirapure;Raviraj Joshi,"Recent advancements in Neural Machine Translation (NMT) models have proved to produce a state of the art results on machine translation for low resource Indian languages. This paper describes the neural machine translation systems for the English-Hindi language presented in AdapMT Shared Task ICON 2020. The shared task aims to build a translation system for Indian languages in specific domains like Artificial Intelligence (AI) and Chemistry using a small in-domain parallel corpus. We evaluated the effectiveness of two popular NMT models i.e, LSTM, and Transformer architectures for the English-Hindi machine translation task based on BLEU scores. We train these models primarily using the out of domain data and employ simple domain adaptation techniques based on the characteristics of the in-domain dataset. The fine-tuning and mixed-domain data approaches are used for domain adaptation. Our team was ranked first in the chemistry and general domain En-Hi translation task and second in the AI domain En-Hi translation task. △ Less","23 December, 2020",https://arxiv.org/pdf/2012.12112
A Maturity Assessment Framework for Conversational AI Development Platforms,Johan Aronsson;Philip Lu;Daniel Strüber;Thorsten Berger,"Conversational Artificial Intelligence (AI) systems have recently sky-rocketed in popularity and are now used in many applications, from car assistants to customer support. The development of conversational AI systems is supported by a large variety of software platforms, all with similar goals, but different focus points and functionalities. A systematic foundation for classifying conversational AI platforms is currently lacking. We propose a framework for assessing the maturity level of conversational AI development platforms. Our framework is based on a systematic literature review, in which we extracted common and distinguishing features of various open-source and commercial (or in-house) platforms. Inspired by language reference frameworks, we identify different maturity levels that a conversational AI development platform may exhibit in understanding and responding to user inputs. Our framework can guide organizations in selecting a conversational AI development platform according to their needs, as well as helping researchers and platform developers improving the maturity of their platforms. △ Less","22 December, 2020",https://arxiv.org/pdf/2012.11976
A Feasibility study for Deep learning based automated brain tumor segmentation using Magnetic Resonance Images,Shanaka Ramesh Gunasekara;HNTK Kaldera;Maheshi B. Dissanayake,"Deep learning algorithms have accounted for the rapid acceleration of research in artificial intelligence in medical image analysis, interpretation, and segmentation with many potential applications across various sub disciplines in medicine. However, only limited number of research which investigates these application scenarios, are deployed into the clinical sector for the evaluation of the real requirement and the practical challenges of the model deployment. In this research, a deep convolutional neural network (CNN) based classification network and Faster RCNN based localization network were developed for brain tumor MR image classification and tumor localization. A typical edge detection algorithm called Prewitt was used for tumor segmentation task, based on the output of the tumor localization. Overall performance of the proposed tumor segmentation architecture, was analyzed using objective quality parameters including Accuracy, Boundary Displacement Error (BDE), Dice score and confidence interval. A subjective quality assessment of the model was conducted based on the Double Stimulus Impairment Scale (DSIS) protocol using the input of medical expertise. It was observed that the confidence level of our segmented output was in a similar range to that of experts. Also, the Neurologists have rated the output of our model as highly accurate segmentation. △ Less","22 December, 2020",https://arxiv.org/pdf/2012.11952
Interpreting Deep Learning Models for Epileptic Seizure Detection on EEG signals,Valentin Gabeff;Tomas Teijeiro;Marina Zapater;Leila Cammoun;Sylvain Rheims;Philippe Ryvlin;David Atienza,"While Deep Learning (DL) is often considered the state-of-the art for Artificial Intelligence-based medical decision support, it remains sparsely implemented in clinical practice and poorly trusted by clinicians due to insufficient interpretability of neural network models. We have tackled this issue by developing interpretable DL models in the context of online detection of epileptic seizure, based on EEG signal. This has conditioned the preparation of the input signals, the network architecture, and the post-processing of the output in line with the domain knowledge. Specifically, we focused the discussion on three main aspects: 1) how to aggregate the classification results on signal segments provided by the DL model into a larger time scale, at the seizure-level; 2) what are the relevant frequency patterns learned in the first convolutional layer of different models, and their relation with the delta, theta, alpha, beta and gamma frequency bands on which the visual interpretation of EEG is based; and 3) the identification of the signal waveforms with larger contribution towards the ictal class, according to the activation differences highlighted using the DeepLIFT method. Results show that the kernel size in the first layer determines the interpretability of the extracted features and the sensitivity of the trained models, even though the final performance is very similar after post-processing. Also, we found that amplitude is the main feature leading to an ictal prediction, suggesting that a larger patient population would be required to learn more complex frequency patterns. Still, our methodology was successfully able to generalize patient inter-variability for the majority of the studied population with a classification F1-score of 0.873 and detecting 90% of the seizures. △ Less","22 December, 2020",https://arxiv.org/pdf/2012.11933
A Fast Edge-Based Synchronizer for Tasks in Real-Time Artificial Intelligence Applications,Richard Olaniyan;Muthucumaru Maheswaran,"Real-time artificial intelligence (AI) applications mapped onto edge computing need to perform data capture, process data, and device actuation within given bounds while using the available devices. Task synchronization across the devices is an important problem that affects the timely progress of an AI application by determining the quality of the captured data, time to process the data, and the quality of actuation. In this paper, we develop a fast edge-based synchronization scheme that can time align the execution of input-output tasks as well compute tasks. The primary idea of the fast synchronizer is to cluster the devices into groups that are highly synchronized in their task executions and statically determine few synchronization points using a game-theoretic solver. The cluster of devices use a late notification protocol to select the best point among the pre-computed synchronization points to reach a time aligned task execution as quickly as possible. We evaluate the performance of our synchronization scheme using trace-driven simulations and we compare the performance with existing distributed synchronization schemes for real-time AI application tasks. We implement our synchronization scheme and compare its training accuracy and training time with other parameter server synchronization frameworks. △ Less","21 December, 2020",https://arxiv.org/pdf/2012.11731
Optimizing Deep Neural Networks through Neuroevolution with Stochastic Gradient Descent,Haichao Zhang;Kuangrong Hao;Lei Gao;Bing Wei;Xuesong Tang,"Deep neural networks (DNNs) have achieved remarkable success in computer vision; however, training DNNs for satisfactory performance remains challenging and suffers from sensitivity to empirical selections of an optimization algorithm for training. Stochastic gradient descent (SGD) is dominant in training a DNN by adjusting neural network weights to minimize the DNNs loss function. As an alternative approach, neuroevolution is more in line with an evolutionary process and provides some key capabilities that are often unavailable in SGD, such as the heuristic black-box search strategy based on individual collaboration in neuroevolution. This paper proposes a novel approach that combines the merits of both neuroevolution and SGD, enabling evolutionary search, parallel exploration, and an effective probe for optimal DNNs. A hierarchical cluster-based suppression algorithm is also developed to overcome similar weight updates among individuals for improving population diversity. We implement the proposed approach in four representative DNNs based on four publicly-available datasets. Experiment results demonstrate that the four DNNs optimized by the proposed approach all outperform corresponding ones optimized by only SGD on all datasets. The performance of DNNs optimized by the proposed approach also outperforms state-of-the-art deep networks. This work also presents a meaningful attempt for pursuing artificial general intelligence. △ Less","21 December, 2020",https://arxiv.org/pdf/2012.11184
Artificial Intelligence ordered 3D vertex importance,Iva Vasic;Bata Vasic;Zorica Nikolic,"Ranking vertices of multidimensional networks is crucial in many areas of research, including selecting and determining the importance of decisions. Some decisions are significantly more important than others, and their weight categorization is also imortant. This paper defines a completely new method for determining the weight decisions using artificial intelligence for importance ranking of three-dimensional network vertices, improving the existing Ordered Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on modulation of quantized indices (QIM) and error correction codes. The technique we propose in this paper offers significant improvements the efficiency of determination the importance of network vertices in relation to statistical OSVETA criteria, replacing heuristic methods with methods of precise prediction of modern neural networks. The new artificial intelligence technique enables a significantly better definition of the 3D meshes and a better assessment of their topological features. The new method contributions result in a greater precision in defining stable vertices, significantly reducing the probability of deleting mesh vertices. △ Less","17 December, 2020",https://arxiv.org/pdf/2012.10232
DistB-SDoIndustry: Enhancing Security in Industry 4.0 Services based on Distributed Blockchain through Software Defined Networking-IoT Enabled Architecture,Anichur Rahman;Umme Sara;Dipanjali Kundu;Saiful Islam;Md. Jahidul Islam;Mahedi Hasan;Ziaur Rahman;Mostofa Kamal Nasir,"The concept of Industry 4.0 is a newly emerging focus of research throughout the world. However, it has lots of challenges to control data, and it can be addressed with various technologies like Internet of Things (IoT), Big Data, Artificial Intelligence (AI), Software Defined Networking (SDN), and Blockchain (BC) for managing data securely. Further, the complexity of sensors, appliances, sensor networks connecting to the internet and the model of Industry 4.0 has created the challenge of designing systems, infrastructure and smart applications capable of continuously analyzing the data produced. Regarding these, the authors present a distributed Blockchain-based security to industry 4.0 applications with SDN-IoT enabled environment. Where the Blockchain can be capable of leading the robust, privacy and confidentiality to our desired system. In addition, the SDN-IoT incorporates the different services of industry 4.0 with more security as well as flexibility. Furthermore, the authors offer an excellent combination among the technologies like IoT, SDN and Blockchain to improve the security and privacy of Industry 4.0 services properly. Finally , the authors evaluate performance and security in a variety of ways in the presented architecture. △ Less","17 December, 2020",https://arxiv.org/pdf/2012.10011
Deep Molecular Dreaming: Inverse machine learning for de-novo molecular design and interpretability with surjective representations,Cynthia Shen;Mario Krenn;Sagi Eppel;Alan Aspuru-Guzik,"Computer-based de-novo design of functional molecules is one of the most prominent challenges in cheminformatics today. As a result, generative and evolutionary inverse designs from the field of artificial intelligence have emerged at a rapid pace, with aims to optimize molecules for a particular chemical property. These models 'indirectly' explore the chemical space; by learning latent spaces, policies, distributions or by applying mutations on populations of molecules. However, the recent development of the SELFIES string representation of molecules, a surjective alternative to SMILES, have made possible other potential techniques. Based on SELFIES, we therefore propose PASITHEA, a direct gradient-based molecule optimization that applies inceptionism techniques from computer vision. PASITHEA exploits the use of gradients by directly reversing the learning process of a neural network, which is trained to predict real-valued chemical properties. Effectively, this forms an inverse regression model, which is capable of generating molecular variants optimized for a certain property. Although our results are preliminary, we observe a shift in distribution of a chosen property during inverse-training, a clear indication of PASITHEA's viability. A striking property of inceptionism is that we can directly probe the model's understanding of the chemical space it was trained on. We expect that extending PASITHEA to larger datasets, molecules and more complex properties will lead to advances in the design of new functional molecules as well as the interpretation and explanation of machine learning models. △ Less","17 December, 2020",https://arxiv.org/pdf/2012.09712
Firearm Detection via Convolutional Neural Networks: Comparing a Semantic Segmentation Model Against End-to-End Solutions,Alexander Egiazarov;Fabio Massimo Zennaro;Vasileios Mavroeidis,"Threat detection of weapons and aggressive behavior from live video can be used for rapid detection and prevention of potentially deadly incidents such as terrorism, general criminal offences, or even domestic violence. One way for achieving this is through the use of artificial intelligence and, in particular, machine learning for image analysis. In this paper we conduct a comparison between a traditional monolithic end-to-end deep learning model and a previously proposed model based on an ensemble of simpler neural networks detecting fire-weapons via semantic segmentation. We evaluated both models from different points of view, including accuracy, computational and data complexity, flexibility and reliability. Our results show that a semantic segmentation model provides considerable amount of flexibility and resilience in the low data environment compared to classical deep model models, although its configuration and tuning presents a challenge in achieving the same levels of accuracy as an end-to-end model. △ Less","17 December, 2020",https://arxiv.org/pdf/2012.09662
Validate and Enable Machine Learning in Industrial AI,Hongbo Zou;Guangjing Chen;Pengtao Xie;Sean Chen;Yongtian He;Hochih Huang;Zheng Nie;Hongbao Zhang;Tristan Bala;Kazi Tulip;Yuqi Wang;Shenlin Qin;Eric P. Xing,"Industrial Artificial Intelligence (Industrial AI) is an emerging concept which refers to the application of artificial intelligence to industry. Industrial AI promises more efficient future industrial control systems. However, manufacturers and solution partners need to understand how to implement and integrate an AI model into the existing industrial control system. A well-trained machine learning (ML) model provides many benefits and opportunities for industrial control optimization; however, an inferior Industrial AI design and integration limits the capability of ML models. To better understand how to develop and integrate trained ML models into the traditional industrial control system, test the deployed AI control system, and ultimately outperform traditional systems, manufacturers and their AI solution partners need to address a number of challenges. Six top challenges, which were real problems we ran into when deploying Industrial AI, are explored in the paper. The Petuum Optimum system is used as an example to showcase the challenges in making and testing AI models, and more importantly, how to address such challenges in an Industrial AI system. △ Less","30 October, 2020",https://arxiv.org/pdf/2012.09610
Draw your Neural Networks,Jatin Sharma;Shobha Lata,"Deep Neural Networks are the basic building blocks of modern Artificial Intelligence. They are increasingly replacing or augmenting existing software systems due to their ability to learn directly from the data and superior accuracy on variety of tasks. Existing Software Development Life Cycle (SDLC) methodologies fall short on representing the unique capabilities and requirements of AI Development and must be replaced with Artificial Intelligence Development Life Cycle (AIDLC) methodologies. In this paper, we discuss an alternative and more natural approach to develop neural networks that involves intuitive GUI elements such as blocks and lines to draw them instead of complex computer programming. We present Sketch framework, that uses this GUI-based approach to design and modify the neural networks and provides interoperability with traditional frameworks. The system provides popular layers and operations out-of-the-box and could import any supported pre-trained model making it a faster method to design and train complex neural networks and ultimately democratizing the AI by removing the learning curve. △ Less","12 December, 2020",https://arxiv.org/pdf/2012.09609
Computational principles of intelligence: learning and reasoning with neural networks,Abel Torres Montoya,"Despite significant achievements and current interest in machine learning and artificial intelligence, the quest for a theory of intelligence, allowing general and efficient problem solving, has done little progress. This work tries to contribute in this direction by proposing a novel framework of intelligence based on three principles. First, the generative and mirroring nature of learned representations of inputs. Second, a grounded, intrinsically motivated and iterative process for learning, problem solving and imagination. Third, an ad hoc tuning of the reasoning mechanism over causal compositional representations using inhibition rules. Together, those principles create a systems approach offering interpretability, continuous learning, common sense and more. This framework is being developed from the following perspectives: as a general problem solving method, as a human oriented tool and finally, as model of information processing in the brain. △ Less","17 December, 2020",https://arxiv.org/pdf/2012.09477
Applying Deutsch's concept of good explanations to artificial intelligence and neuroscience -- an initial exploration,Daniel C. Elton,"Artificial intelligence has made great strides since the deep learning revolution, but AI systems still struggle to extrapolate outside of their training data and adapt to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls ""reach"", is due to scientific theories being hard to vary. In this work we investigate Deutsch's hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam's razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to accurately predict new, out-of-distribution data. We discuss how to measure internal variability using the size of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain and distinguish two learning systems in the brain. The first system operates similar to deep learning and likely underlies most of perception and motor control while the second is a more creative system capable of generating hard-to-vary explanations of the world. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence. We make contact with the framework of Popperian epistemology which rejects induction and asserts that knowledge generation is an evolutionary process which proceeds through conjecture and refutation. △ Less","24 December, 2020",https://arxiv.org/pdf/2012.09318
"Infrastructure for Artificial Intelligence, Quantum and High Performance Computing",William Gropp;Sujata Banerjee;Ian Foster,"High Performance Computing (HPC), Artificial Intelligence (AI)/Machine Learning (ML), and Quantum Computing (QC) and communications offer immense opportunities for innovation and impact on society. Researchers in these areas depend on access to computing infrastructure, but these resources are in short supply and are typically siloed in support of their research communities, making it more difficult to pursue convergent and interdisciplinary research. Such research increasingly depends on complex workflows that require different resources for each stage. This paper argues that a more-holistic approach to computing infrastructure, one that recognizes both the convergence of some capabilities and the complementary capabilities from new computing approaches, be it commercial cloud to Quantum Computing, is needed to support computer science research. △ Less","16 December, 2020",https://arxiv.org/pdf/2012.09303
Empathic Chatbot: Emotional Intelligence for Empathic Chatbot: Emotional Intelligence for Mental Health Well-being,Sarada Devaram,"Conversational chatbots are Artificial Intelligence (AI)-powered applications that assist users with various tasks by responding in natural language and are prevalent across different industries. Most of the chatbots that we encounter on websites and digital assistants such as Alexa, Siri does not express empathy towards the user, and their ability to empathise remains immature. Lack of empathy towards the user is not critical for a transactional or interactive chatbot, but the bots designed to support mental healthcare patients need to understand the emotional state of the user and tailor the conversations. This research explains the different types of emotional intelligence methodologies adopted in the development of an empathic chatbot and how far they have been adopted and succeeded. △ Less","15 December, 2020",https://arxiv.org/pdf/2012.09130
A Systematic Mapping Study in AIOps,Paolo Notaro;Jorge Cardoso;Michael Gerndt,"IT systems of today are becoming larger and more complex, rendering their human supervision more difficult. Artificial Intelligence for IT Operations (AIOps) has been proposed to tackle modern IT administration challenges thanks to AI and Big Data. However, past AIOps contributions are scattered, unorganized and missing a common terminology convention, which renders their discovery and comparison impractical. In this work, we conduct an in-depth mapping study to collect and organize the numerous scattered contributions to AIOps in a unique reference index. We create an AIOps taxonomy to build a foundation for future contributions and allow an efficient comparison of AIOps papers treating similar problems. We investigate temporal trends and classify AIOps contributions based on the choice of algorithms, data sources and the target components. Our results show a recent and growing interest towards AIOps, specifically to those contributions treating failure-related tasks (62%), such as anomaly detection and root cause analysis. △ Less","15 December, 2020",https://arxiv.org/pdf/2012.09108
Open Problems in Cooperative AI,Allan Dafoe;Edward Hughes;Yoram Bachrach;Tantum Collins;Kevin R. McKee;Joel Z. Leibo;Kate Larson;Thore Graepel,"Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences. △ Less","15 December, 2020",https://arxiv.org/pdf/2012.08630
Grounding Artificial Intelligence in the Origins of Human Behavior,Eleni Nisioti;Clément Moulin-Frier,"Recent advances in Artificial Intelligence (AI) have revived the quest for agents able to acquire an open-ended repertoire of skills. However, although this ability is fundamentally related to the characteristics of human intelligence, research in this field rarely considers the processes that may have guided the emergence of complex cognitive capacities during the evolution of the species. Research in Human Behavioral Ecology (HBE) seeks to understand how the behaviors characterizing human nature can be conceived as adaptive responses to major changes in the structure of our ecological niche. In this paper, we propose a framework highlighting the role of environmental complexity in open-ended skill acquisition, grounded in major hypotheses from HBE and recent contributions in Reinforcement learning (RL). We use this framework to highlight fundamental links between the two disciplines, as well as to identify feedback loops that bootstrap ecological complexity and create promising research directions for AI researchers. △ Less","17 December, 2020",https://arxiv.org/pdf/2012.08564
Gegelati: Lightweight Artificial Intelligence through Generic and Evolvable Tangled Program Graphs,Karol Desnos;Nicolas Sourbier;Pierre-Yves Raumer;Olivier Gesny;Maxime Pelcat,"Tangled Program Graph (TPG) is a reinforcement learning technique based on genetic programming concepts. On state-of-the-art learning environments, TPGs have been shown to offer comparable competence with Deep Neural Networks (DNNs), for a fraction of their computational and storage cost. This lightness of TPGs, both for training and inference, makes them an interesting model to implement Artificial Intelligences (AIs) on embedded systems with limited computational and storage resources. In this paper, we introduce the Gegelati library for TPGs. Besides introducing the general concepts and features of the library, two main contributions are detailed in the paper: 1/ The parallelization of the deterministic training process of TPGs, for supporting heterogeneous Multiprocessor Systems-on-Chips (MPSoCs). 2/ The support for customizable instruction sets and data types within the genetically evolved programs of the TPG model. The scalability of the parallel training process is demonstrated through experiments on architectures ranging from a high-end 24-core processor to a low-power heterogeneous MPSoC. The impact of customizable instructions on the outcome of a training process is demonstrated on a state-of-the-art reinforcement learning environment. CCS Concepts: \bullet Computer systems organization \rightarrow Embedded systems; \bullet Computing methodologies \rightarrow Machine learning. △ Less","15 December, 2020",https://arxiv.org/pdf/2012.08296
Fair and Efficient Allocations under Lexicographic Preferences,Hadi Hosseini;Sujoy Sikdar;Rohit Vaish;Lirong Xia,"Envy-freeness up to any good (EFX) provides a strong and intuitive guarantee of fairness in the allocation of indivisible goods. But whether such allocations always exist or whether they can be efficiently computed remains an important open question. We study the existence and computation of EFX in conjunction with various other economic properties under lexicographic preferences--a well-studied preference model in artificial intelligence and economics. In sharp contrast to the known results for additive valuations, we not only prove the existence of EFX and Pareto optimal allocations, but in fact provide an algorithmic characterization of these two properties. We also characterize the mechanisms that are, in addition, strategyproof, non-bossy, and neutral. When the efficiency notion is strengthened to rank-maximality, we obtain non-existence and computational hardness results, and show that tractability can be restored when EFX is relaxed to another well-studied fairness notion called maximin share guarantee (MMS). △ Less","14 December, 2020",https://arxiv.org/pdf/2012.07680
Hospital Capacity Planning Using Discrete Event Simulation Under Special Consideration of the COVID-19 Pandemic,Thomas Bartz-Beielstein;Frederik Rehbach;Olaf Mersmann;Eva Bartz,"We present a resource-planning tool for hospitals under special consideration of the COVID-19 pandemic, called babsim.hospital. It provides many advantages for crisis teams, e.g., comparison with their own local planning, simulation of local events, simulation of several scenarios (worst / best case). There are benefits for medical professionals, e.g, analysis of the pandemic at local, regional, state and federal level, the consideration of special risk groups, tools for validating the length of stays and transition probabilities. Finally, there are potential advantages for administration, management, e.g., assessment of the situation of individual hospitals taking local events into account, consideration of relevant resources such as beds, ventilators, rooms, protective clothing, and personnel planning, e.g., medical and nursing staff. babsim.hospital combines simulation, optimization, statistics, and artificial intelligence processes in a very efficient way. The core is a discrete, event-based simulation model. △ Less","13 December, 2020",https://arxiv.org/pdf/2012.07188
Learning Contextual Causality from Time-consecutive Images,Hongming Zhang;Yintong Huo;Xinran Zhao;Yangqiu Song;Dan Roth,"Causality knowledge is crucial for many artificial intelligence systems. Conventional textual-based causality knowledge acquisition methods typically require laborious and expensive human annotations. As a result, their scale is often limited. Moreover, as no context is provided during the annotation, the resulting causality knowledge records (e.g., ConceptNet) typically do not take the context into consideration. To explore a more scalable way of acquiring causality knowledge, in this paper, we jump out of the textual domain and investigate the possibility of learning contextual causality from the visual signal. Compared with pure text-based approaches, learning causality from the visual signal has the following advantages: (1) Causality knowledge belongs to the commonsense knowledge, which is rarely expressed in the text but rich in videos; (2) Most events in the video are naturally time-ordered, which provides a rich resource for us to mine causality knowledge from; (3) All the objects in the video can be used as context to study the contextual property of causal relations. In detail, we first propose a high-quality dataset Vis-Causal and then conduct experiments to demonstrate that with good language and visual representation models as well as enough training signals, it is possible to automatically discover meaningful causal knowledge from the videos. Further analysis also shows that the contextual property of causal relations indeed exists, taking which into consideration might be crucial if we want to use the causality knowledge in real applications, and the visual signal could serve as a good resource for learning such contextual causality. △ Less","13 December, 2020",https://arxiv.org/pdf/2012.07138
Computing Machinery and Knowledge,Raymond Anneborg,"The purpose of this paper is to discuss the possibilities for computing machinery, or AI agents, to know and to possess knowledge. This is done mainly from a virtue epistemology perspective and definition of knowledge. However, this inquiry also shed light on the human condition, what it means for a human to know, and to possess knowledge. The paper argues that it is possible for an AI agent to know and examines this from both current state-of-the-art in artificial intelligence as well as from the perspective of what the future AI development might bring in terms of superintelligent AI agents. △ Less","31 October, 2020",https://arxiv.org/pdf/2012.06686
AIforCOVID: predicting the clinical outcomes in patients with COVID-19 applying AI to chest-X-rays. An Italian multicentre study,Paolo Soda;Natascha Claudia D'Amico;Jacopo Tessadori;Giovanni Valbusa;Valerio Guarrasi;Chandra Bortolotto;Muhammad Usman Akbar;Rosa Sicilia;Ermanno Cordelli;Deborah Fazzini;Michaela Cellina;Giancarlo Oliva;Giovanni Callea;Silvia Panella;Maurizio Cariati;Diletta Cozzi;Vittorio Miele;Elvira Stellato;Gian Paolo Carrafiello;Giulia Castorani;Annalisa Simeone;Lorenzo Preda;Giulio Iannello;Alessio Del Bue;Fabio Tedoldi,"Recent epidemiological data report that worldwide more than 53 million people have been infected by SARS-CoV-2, resulting in 1.3 million deaths. The disease has been spreading very rapidly and few months after the identification of the first infected, shortage of hospital resources quickly became a problem. In this work we investigate whether chest X-ray (CXR) can be used as a possible tool for the early identification of patients at risk of severe outcome, like intensive care or death. CXR is a radiological technique that compared to computed tomography (CT) it is simpler, faster, more widespread and it induces lower radiation dose. We present a dataset including data collected from 820 patients by six Italian hospitals in spring 2020 during the first COVID-19 emergency. The dataset includes CXR images, several clinical attributes and clinical outcomes. We investigate the potential of artificial intelligence to predict the prognosis of such patients, distinguishing between severe and mild cases, thus offering a baseline reference for other researchers and practitioners. To this goal, we present three approaches that use features extracted from CXR images, either handcrafted or automatically by convolutional neuronal networks, which are then integrated with the clinical data. Exhaustive evaluation shows promising performance both in 10-fold and leave-one-centre-out cross-validation, implying that clinical data and images have the potential to provide useful information for the management of patients and hospital resources. △ Less","11 December, 2020",https://arxiv.org/pdf/2012.06531
Generating Human-Like Movement: A Comparison Between Two Approaches Based on Environmental Features,A. Zonta;S. K. Smit;A. E. Eiben,"Modelling realistic human behaviours in simulation is an ongoing challenge that resides between several fields like social sciences, philosophy, and artificial intelligence. Human movement is a special type of behaviour driven by intent (e.g. to get groceries) and the surrounding environment (e.g. curiosity to see new interesting places). Services available online and offline do not normally consider the environment when planning a path, which is decisive especially on a leisure trip. Two novel algorithms have been presented to generate human-like trajectories based on environmental features. The Attraction-Based A* algorithm includes in its computation information from the environmental features meanwhile, the Feature-Based A* algorithm also injects information from the real trajectories in its computation. The human-likeness aspect has been tested by a human expert judging the final generated trajectories as realistic. This paper presents a comparison between the two approaches in some key metrics like efficiency, efficacy, and hyper-parameters sensitivity. We show how, despite generating trajectories that are closer to the real one according to our predefined metrics, the Feature-Based A* algorithm fall short in time efficiency compared to the Attraction-Based A* algorithm, hindering the usability of the model in the real world. △ Less","11 December, 2020",https://arxiv.org/pdf/2012.06474
Privacy-preserving medical image analysis,Alexander Ziller;Jonathan Passerat-Palmbach;Théo Ryffel;Dmitrii Usynin;Andrew Trask;Ionésio Da Lima Costa Junior;Jason Mancuso;Marcus Makowski;Daniel Rueckert;Rickmer Braren;Georgios Kaissis,"The utilisation of artificial intelligence in medicine and healthcare has led to successful clinical applications in several domains. The conflict between data usage and privacy protection requirements in such systems must be resolved for optimal results as well as ethical and legal compliance. This calls for innovative solutions such as privacy-preserving machine learning (PPML). We present PriMIA (Privacy-preserving Medical Image Analysis), a software framework designed for PPML in medical imaging. In a real-life case study we demonstrate significantly better classification performance of a securely aggregated federated learning model compared to human experts on unseen datasets. Furthermore, we show an inference-as-a-service scenario for end-to-end encrypted diagnosis, where neither the data nor the model are revealed. Lastly, we empirically evaluate the framework's security against a gradient-based model inversion attack and demonstrate that no usable information can be recovered from the model. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.06354
Artificial Intelligence for COVID-19 Detection -- A state-of-the-art review,Parsa Sarosh;Shabir A. Parah;Romany F Mansur;G. M. Bhat,"The emergence of COVID-19 has necessitated many efforts by the scientific community for its proper management. An urgent clinical reaction is required in the face of the unending devastation being caused by the pandemic. These efforts include technological innovations for improvement in screening, treatment, vaccine development, contact tracing and, survival prediction. The use of Deep Learning (DL) and Artificial Intelligence (AI) can be sought in all of the above-mentioned spheres. This paper aims to review the role of Deep Learning and Artificial intelligence in various aspects of the overall COVID-19 management and particularly for COVID-19 detection and classification. The DL models are developed to analyze clinical modalities like CT scans and X-Ray images of patients and predict their pathological condition. A DL model aims to detect the COVID-19 pneumonia, classify and distinguish between COVID-19, Community-Acquired Pneumonia (CAP), Viral and Bacterial pneumonia, and normal conditions. Furthermore, sophisticated models can be built to segment the affected area in the lungs and quantify the infection volume for a better understanding of the extent of damage. Many models have been developed either independently or with the help of pre-trained models like VGG19, ResNet50, and AlexNet leveraging the concept of transfer learning. Apart from model development, data preprocessing and augmentation are also performed to cope with the challenge of insufficient data samples often encountered in medical applications. It can be evaluated that DL and AI can be effectively implemented to withstand the challenges posed by the global emergency △ Less","25 November, 2020",https://arxiv.org/pdf/2012.06310
Conceptualization and Framework of Hybrid Intelligence Systems,Nikhil Prakash;Kory W. Mathewson,"As artificial intelligence (AI) systems are getting ubiquitous within our society, issues related to its fairness, accountability, and transparency are increasing rapidly. As a result, researchers are integrating humans with AI systems to build robust and reliable hybrid intelligence systems. However, a proper conceptualization of these systems does not underpin this rapid growth. This article provides a precise definition of hybrid intelligence systems as well as explains its relation with other similar concepts through our proposed framework and examples from contemporary literature. The framework breakdowns the relationship between a human and a machine in terms of the degree of coupling and the directive authority of each party. Finally, we argue that all AI systems are hybrid intelligence systems, so human factors need to be examined at every stage of such systems' lifecycle. △ Less","11 December, 2020",https://arxiv.org/pdf/2012.06161
Artificial Intelligence & Cooperation,Elisa Bertino;Finale Doshi-Velez;Maria Gini;Daniel Lopresti;David Parkes,"The rise of Artificial Intelligence (AI) will bring with it an ever-increasing willingness to cede decision-making to machines. But rather than just giving machines the power to make decisions that affect us, we need ways to work cooperatively with AI systems. There is a vital need for research in ""AI and Cooperation"" that seeks to understand the ways in which systems of AIs and systems of AIs with people can engender cooperative behavior. Trust in AI is also key: trust that is intrinsic and trust that can only be earned over time. Here we use the term ""AI"" in its broadest sense, as employed by the recent 20-Year Community Roadmap for AI Research (Gil and Selman, 2019), including but certainly not limited to, recent advances in deep learning. With success, cooperation between humans and AIs can build society just as human-human cooperation has. Whether coming from an intrinsic willingness to be helpful, or driven through self-interest, human societies have grown strong and the human species has found success through cooperation. We cooperate ""in the small"" -- as family units, with neighbors, with co-workers, with strangers -- and ""in the large"" as a global community that seeks cooperative outcomes around questions of commerce, climate change, and disarmament. Cooperation has evolved in nature also, in cells and among animals. While many cases involving cooperation between humans and AIs will be asymmetric, with the human ultimately in control, AI systems are growing so complex that, even today, it is impossible for the human to fully comprehend their reasoning, recommendations, and actions when functioning simply as passive observers. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.06034
Reinforcement Learning Agents for Ubisoft's Roller Champions,Nancy Iskander;Aurelien Simoni;Eloi Alonso;Maxim Peter,"In recent years, Reinforcement Learning (RL) has seen increasing popularity in research and popular culture. However, skepticism still surrounds the practicality of RL in modern video game development. In this paper, we demonstrate by example that RL can be a great tool for Artificial Intelligence (AI) design in modern, non-trivial video games. We present our RL system for Ubisoft's Roller Champions, a 3v3 Competitive Multiplayer Sports Game played on an oval-shaped skating arena. Our system is designed to keep up with agile, fast-paced development, taking 1--4 days to train a new model following gameplay changes. The AIs are adapted for various game modes, including a 2v2 mode, a Training with Bots mode, in addition to the Classic game mode where they replace players who have disconnected. We observe that the AIs develop sophisticated co-ordinated strategies, and can aid in balancing the game as an added bonus. Please see the accompanying video at https://vimeo.com/466780171 (password: rollerRWRL2020) for examples. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.06031
Neurosymbolic AI: The 3rd Wave,Artur d'Avila Garcez;Luis C. Lamb,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems. △ Less","16 December, 2020",https://arxiv.org/pdf/2012.05876
AI Driven Knowledge Extraction from Clinical Practice Guidelines: Turning Research into Practice,Musarrat Hussain;Jamil Hussain;Taqdir Ali;Fahad Ahmed Satti;Sungyoung Lee,"Background and Objectives: Clinical Practice Guidelines (CPGs) represent the foremost methodology for sharing state-of-the-art research findings in the healthcare domain with medical practitioners to limit practice variations, reduce clinical cost, improve the quality of care, and provide evidence based treatment. However, extracting relevant knowledge from the plethora of CPGs is not feasible for already burdened healthcare professionals, leading to large gaps between clinical findings and real practices. It is therefore imperative that state-of-the-art Computing research, especially machine learning is used to provide artificial intelligence based solution for extracting the knowledge from CPGs and reducing the gap between healthcare research/guidelines and practice. Methods: This research presents a novel methodology for knowledge extraction from CPGs to reduce the gap and turn the latest research findings into clinical practice. First, our system classifies the CPG sentences into four classes such as condition-action, condition-consequences, action, and not-applicable based on the information presented in a sentence. We use deep learning with state-of-the-art word embedding, improved word vectors technique in classification process. Second, it identifies qualifier terms in the classified sentences, which assist in recognizing the condition and action phrases in a sentence. Finally, the condition and action phrase are processed and transformed into plain rule If Condition(s) Then Action format. Results: We evaluate the methodology on three different domains guidelines including Hypertension, Rhinosinusitis, and Asthma. The deep learning model classifies the CPG sentences with an accuracy of 95%. While rule extraction was validated by user-centric approach, which achieved a Jaccard coefficient of 0.6, 0.7, and 0.4 with three human experts extracted rules, respectively. △ Less","10 December, 2020",https://arxiv.org/pdf/2012.05489
Artificial Intelligence at the Edge,Elisa Bertino;Sujata Banerjee,"The Internet of Things (IoT) and edge computing applications aim to support a variety of societal needs, including the global pandemic situation that the entire world is currently experiencing and responses to natural disasters. The need for real-time interactive applications such as immersive video conferencing, augmented/virtual reality, and autonomous vehicles, in education, healthcare, disaster recovery and other domains, has never been higher. At the same time, there have been recent technological breakthroughs in highly relevant fields such as artificial intelligence (AI)/machine learning (ML), advanced communication systems (5G and beyond), privacy-preserving computations, and hardware accelerators. 5G mobile communication networks increase communication capacity, reduce transmission latency and error, and save energy -- capabilities that are essential for new applications. The envisioned future 6G technology will integrate many more technologies, including for example visible light communication, to support groundbreaking applications, such as holographic communications and high precision manufacturing. Many of these applications require computations and analytics close to application end-points: that is, at the edge of the network, rather than in a centralized cloud. AI techniques applied at the edge have tremendous potential both to power new applications and to need more efficient operation of edge infrastructure. However, it is critical to understand where to deploy AI systems within complex ecosystems consisting of advanced applications and the specific real-time requirements towards AI systems. △ Less","9 December, 2020",https://arxiv.org/pdf/2012.05410
Predicting Prostate Cancer-Specific Mortality with A.I.-based Gleason Grading,Ellery Wulczyn;Kunal Nagpal;Matthew Symonds;Melissa Moran;Markus Plass;Robert Reihs;Farah Nader;Fraser Tan;Yuannan Cai;Trissia Brown;Isabelle Flament-Auvigne;Mahul B. Amin;Martin C. Stumpe;Heimo Muller;Peter Regitnig;Andreas Holzinger;Greg S. Corrado;Lily H. Peng;Po-Hsuan Cameron Chen;David F. Steiner;Kurt Zatloukal;Yun Liu;Craig H. Mermel,"Gleason grading of prostate cancer is an important prognostic factor but suffers from poor reproducibility, particularly among non-subspecialist pathologists. Although artificial intelligence (A.I.) tools have demonstrated Gleason grading on-par with expert pathologists, it remains an open question whether A.I. grading translates to better prognostication. In this study, we developed a system to predict prostate-cancer specific mortality via A.I.-based Gleason grading and subsequently evaluated its ability to risk-stratify patients on an independent retrospective cohort of 2,807 prostatectomy cases from a single European center with 5-25 years of follow-up (median: 13, interquartile range 9-17). The A.I.'s risk scores produced a C-index of 0.84 (95%CI 0.80-0.87) for prostate cancer-specific mortality. Upon discretizing these risk scores into risk groups analogous to pathologist Grade Groups (GG), the A.I. had a C-index of 0.82 (95%CI 0.78-0.85). On the subset of cases with a GG in the original pathology report (n=1,517), the A.I.'s C-indices were 0.87 and 0.85 for continuous and discrete grading, respectively, compared to 0.79 (95%CI 0.71-0.86) for GG obtained from the reports. These represent improvements of 0.08 (95%CI 0.01-0.15) and 0.07 (95%CI 0.00-0.14) respectively. Our results suggest that A.I.-based Gleason grading can lead to effective risk-stratification and warrants further evaluation for improving disease management. △ Less","24 November, 2020",https://arxiv.org/pdf/2012.05197
Binding and Perspective Taking as Inference in a Generative Neural Network Model,Mahdi Sadeghi;Fabian Schrodt;Sebastian Otte;Martin V. Butz,"The ability to flexibly bind features into coherent wholes from different perspectives is a hallmark of cognition and intelligence. Importantly, the binding problem is not only relevant for vision but also for general intelligence, sensorimotor integration, event processing, and language. Various artificial neural network models have tackled this problem with dynamic neural fields and related approaches. Here we focus on a generative encoder-decoder architecture that adapts its perspective and binds features by means of retrospective inference. We first train a model to learn sufficiently accurate generative models of dynamic biological motion or other harmonic motion patterns, such as a pendulum. We then scramble the input to a certain extent, possibly vary the perspective onto it, and propagate the prediction error back onto a binding matrix, that is, hidden neural states that determine feature binding. Moreover, we propagate the error further back onto perspective taking neurons, which rotate and translate the input features onto a known frame of reference. Evaluations show that the resulting gradient-based inference process solves the perspective taking and binding problem for known biological motion patterns, essentially yielding a Gestalt perception mechanism. In addition, redundant feature properties and population encodings are shown to be highly useful. While we evaluate the algorithm on biological motion patterns, the principled approach should be applicable to binding and Gestalt perception problems in other domains. △ Less","9 December, 2020",https://arxiv.org/pdf/2012.05152
Could robots be regarded as humans in future?,Huansheng Ning;Feifei Shi,"With the overwhelming advances in Artificial Intelligence (AI), brain science and neuroscience, robots are developing towards a direction of much more human-like and human-friendly. We can't help but wonder whether robots could be regarded as humans in future? In this article, we propose a novel perspective to analyze the essential difference between humans and robots, that is based on their respective living spaces, particularly the independent and intrinsic thinking space. We finally come to the conclusion that, only when robots own the independent and intrinsic thinking space as humans, could they have the prerequisites to be regarded as humans. △ Less","1 December, 2020",https://arxiv.org/pdf/2012.05054
Recent Advances in Computer Audition for Diagnosing COVID-19: An Overview,Kun Qian;Bjorn W. Schuller;Yoshiharu Yamamoto,"Computer audition (CA) has been demonstrated to be efficient in healthcare domains for speech-affecting disorders (e.g., autism spectrum, depression, or Parkinson's disease) and body sound-affecting abnormalities (e. g., abnormal bowel sounds, heart murmurs, or snore sounds). Nevertheless, CA has been underestimated in the considered data-driven technologies for fighting the COVID-19 pandemic caused by the SARS-CoV-2 coronavirus. In this light, summarise the most recent advances in CA for COVID-19 speech and/or sound analysis. While the milestones achieved are encouraging, there are yet not any solid conclusions that can be made. This comes mostly, as data is still sparse, often not sufficiently validated and lacking in systematic comparison with related diseases that affect the respiratory system. In particular, CA-based methods cannot be a standalone screening tool for SARS-CoV-2. We hope this brief overview can provide a good guidance and attract more attention from a broader artificial intelligence community. △ Less","8 December, 2020",https://arxiv.org/pdf/2012.04650
Using Side Channel Information and Artificial Intelligence for Malware Detection,Paul Maxwell;David Niblick;Daniel C. Ruiz,Cybersecurity continues to be a difficult issue for society especially as the number of networked systems grows. Techniques to protect these systems range from rules-based to artificial intelligence-based intrusion detection systems and anti-virus tools. These systems rely upon the information contained in the network packets and download executables to function. Side channel information leaked from hardware has been shown to reveal secret information in systems such as encryption keys. This work demonstrates that side channel information can be used to detect malware running on a computing platform without access to the code involved. △ Less,"3 December, 2020",https://arxiv.org/pdf/2012.03750
Explainable AI for Interpretable Credit Scoring,Lara Marie Demajo;Vince Vella;Alexiei Dingli,"With the ever-growing achievements in Artificial Intelligence (AI) and the recent boosted enthusiasm in Financial Technology (FinTech), applications such as credit scoring have gained substantial academic interest. Credit scoring helps financial experts make better decisions regarding whether or not to accept a loan application, such that loans with a high probability of default are not accepted. Apart from the noisy and highly imbalanced data challenges faced by such credit scoring models, recent regulations such as the `right to explanation' introduced by the General Data Protection Regulation (GDPR) and the Equal Credit Opportunity Act (ECOA) have added the need for model interpretability to ensure that algorithmic decisions are understandable and coherent. An interesting concept that has been recently introduced is eXplainable AI (XAI), which focuses on making black-box models more interpretable. In this work, we present a credit scoring model that is both accurate and interpretable. For classification, state-of-the-art performance on the Home Equity Line of Credit (HELOC) and Lending Club (LC) Datasets is achieved using the Extreme Gradient Boosting (XGBoost) model. The model is then further enhanced with a 360-degree explanation framework, which provides different explanations (i.e. global, local feature-based and local instance-based) that are required by different people in different situations. Evaluation through the use of functionallygrounded, application-grounded and human-grounded analysis show that the explanations provided are simple, consistent as well as satisfy the six predetermined hypotheses testing for correctness, effectiveness, easy understanding, detail sufficiency and trustworthiness. △ Less","3 December, 2020",https://arxiv.org/pdf/2012.03749
Roof fall hazard detection with convolutional neural networks using transfer learning,Ergin Isleyen;Sebnem Duzgun;McKell R. Carter,"Roof falls due to geological conditions are major safety hazards in mining and tunneling industries, causing lost work times, injuries, and fatalities. Several large-opening limestone mines in the Eastern and Midwestern United States have roof fall problems caused by high horizontal stresses. The typical hazard management approach for this type of roof fall hazard relies heavily on visual inspections and expert knowledge. In this study, we propose an artificial intelligence (AI) based system for the detection roof fall hazards caused by high horizontal stresses. We use images depicting hazardous and non-hazardous roof conditions to develop a convolutional neural network for autonomous detection of hazardous roof conditions. To compensate for limited input data, we utilize a transfer learning approach. In transfer learning, an already-trained network is used as a starting point for classification in a similar domain. Results confirm that this approach works well for classifying roof conditions as hazardous or safe, achieving a statistical accuracy of 86%. However, accuracy alone is not enough to ensure a reliable hazard management system. System constraints and reliability are improved when the features being used by the network are understood. Therefore, we used a deep learning interpretation technique called integrated gradients to identify the important geologic features in each image for prediction. The analysis of integrated gradients shows that the system mimics expert judgment on roof fall hazard detection. The system developed in this paper demonstrates the potential of deep learning in geological hazard management to complement human experts, and likely to become an essential part of autonomous tunneling operations in those cases where hazard identification heavily depends on expert knowledge. △ Less","12 November, 2020",https://arxiv.org/pdf/2012.03681
Design of an EEG-based Drone Swarm Control System using Endogenous BCI Paradigms,Dae-Hyeok Lee;Hyung-Ju Ahn;Ji-Hoon Jeong;Seong-Whan Lee,"Non-invasive brain-computer interface (BCI) has been developed for understanding users' intentions by using electroencephalogram (EEG) signals. With the recent development of artificial intelligence, there have been many developments in the drone control system. BCI characteristic that can reflect the users' intentions led to the BCI-based drone control system. When using drone swarm, we can have more advantages, such as mission diversity, than using a single drone. In particular, BCI-based drone swarm control could provide many advantages to various industries such as military service or industry disaster. BCI Paradigms consist of the exogenous and endogenous paradigms. The endogenous paradigms can operate with the users' intentions independently of any stimulus. In this study, we designed endogenous paradigms (i.e., motor imagery (MI), visual imagery (VI), and speech imagery (SI)) specialized in drone swarm control, and EEG-based various task classifications related to drone swarm control were conducted. Five subjects participated in the experiment and the performance was evaluated using the basic machine learning algorithm. The grand-averaged accuracies were 51.1%, 53.2%, and 41.9% in MI, VI, and SI, respectively. Hence, we confirmed the feasibility of increasing the degree of freedom for drone swarm control using various endogenous paradigms. △ Less","12 December, 2020",https://arxiv.org/pdf/2012.03507
Brain Co-Processors: Using AI to Restore and Augment Brain Function,Rajesh P. N. Rao,"Brain-computer interfaces (BCIs) use decoding algorithms to control prosthetic devices based on brain signals for restoration of lost function. Computer-brain interfaces (CBIs), on the other hand, use encoding algorithms to transform external sensory signals into neural stimulation patterns for restoring sensation or providing sensory feedback for closed-loop prosthetic control. In this article, we introduce brain co-processors, devices that combine decoding and encoding in a unified framework using artificial intelligence (AI) to supplement or augment brain function. Brain co-processors can be used for a range of applications, from inducing Hebbian plasticity for rehabilitation after brain injury to reanimating paralyzed limbs and enhancing memory. A key challenge is simultaneous multi-channel neural decoding and encoding for optimization of external behavioral or task-related goals. We describe a new framework for developing brain co-processors based on artificial neural networks, deep learning and reinforcement learning. These ""neural co-processors"" allow joint optimization of cost functions with the nervous system to achieve desired behaviors. By coupling artificial neural networks with their biological counterparts, neural co-processors offer a new way of restoring and augmenting the brain, as well as a new scientific tool for brain research. We conclude by discussing the potential applications and ethical implications of brain co-processors. △ Less","6 December, 2020",https://arxiv.org/pdf/2012.03378
Social Responsibility of Algorithms,Alexis Tsoukiàs,"Should we be concerned by the massive use of devices and algorithms which automatically handle an increasing number of everyday activities within our societies? The paper makes a short overview of the scientific investigation around this topic, showing that the development, existence and use of such autonomous artifacts is much older than the recent interest in machine learning monopolised artificial intelligence. We then categorise the impact of using such artifacts to the whole process of data collection, structuring, manipulation as well as in recommendation and decision making. The suggested framework allows to identify a number of challenges for the whole community of decision analysts, both researchers and practitioners. △ Less","6 December, 2020",https://arxiv.org/pdf/2012.03319
CoEdge: Cooperative DNN Inference with Adaptive Workload Partitioning over Heterogeneous Edge Devices,Liekang Zeng;Xu Chen;Zhi Zhou;Lei Yang;Junshan Zhang,"Recent advances in artificial intelligence have driven increasing intelligent applications at the network edge, such as smart home, smart factory, and smart city. To deploy computationally intensive Deep Neural Networks (DNNs) on resource-constrained edge devices, traditional approaches have relied on either offloading workload to the remote cloud or optimizing computation at the end device locally. However, the cloud-assisted approaches suffer from the unreliable and delay-significant wide-area network, and the local computing approaches are limited by the constrained computing capability. Towards high-performance edge intelligence, the cooperative execution mechanism offers a new paradigm, which has attracted growing research interest recently. In this paper, we propose CoEdge, a distributed DNN computing system that orchestrates cooperative DNN inference over heterogeneous edge devices. CoEdge utilizes available computation and communication resources at the edge and dynamically partitions the DNN inference workload adaptive to devices' computing capabilities and network conditions. Experimental evaluations based on a realistic prototype show that CoEdge outperforms status-quo approaches in saving energy with close inference latency, achieving up to 25.5%~66.9% energy reduction for four widely-adopted CNN models. △ Less","6 December, 2020",https://arxiv.org/pdf/2012.03257
Review: Deep Learning Methods for Cybersecurity and Intrusion Detection Systems,Mayra Macas;Chunming Wu,"As the number of cyber-attacks is increasing, cybersecurity is evolving to a key concern for any business. Artificial Intelligence (AI) and Machine Learning (ML) (in particular Deep Learning - DL) can be leveraged as key enabling technologies for cyber-defense, since they can contribute in threat detection and can even provide recommended actions to cyber analysts. A partnership of industry, academia, and government on a global scale is necessary in order to advance the adoption of AI/ML to cybersecurity and create efficient cyber defense systems. In this paper, we are concerned with the investigation of the various deep learning techniques employed for network intrusion detection and we introduce a DL framework for cybersecurity applications. △ Less","4 December, 2020",https://arxiv.org/pdf/2012.02891
Understanding Attention: In Minds and Machines,Shriraj P. Sawant;Shruti Singh,"Attention is a complex and broad concept, studied across multiple disciplines spanning artificial intelligence, cognitive science, psychology, neuroscience, and related fields. Although many of the ideas regarding attention do not significantly overlap among these fields, there is a common theme of adaptive control of limited resources. In this work, we review the concept and variants of attention in artificial neural networks (ANNs). We also discuss the origin of attention from the neuroscience point of view parallel to that of ANNs. Instead of having seemingly disconnected dialogues between varied disciplines, we suggest grounding the ideas on common conceptual frameworks for a systematic analysis of attention and towards possible unification of ideas in AI and Neuroscience. △ Less","4 December, 2020",https://arxiv.org/pdf/2012.02659
Radar Artifact Labeling Framework (RALF): Method for Plausible Radar Detections in Datasets,Simon T. Isele;Marcel P. Schilling;Fabian E. Klein;Sascha Saralajew;J. Marius Zoellner,"Research on localization and perception for Autonomous Driving is mainly focused on camera and LiDAR datasets, rarely on radar data. Manually labeling sparse radar point clouds is challenging. For a dataset generation, we propose the cross sensor Radar Artifact Labeling Framework (RALF). Automatically generated labels for automotive radar data help to cure radar shortcomings like artifacts for the application of artificial intelligence. RALF provides plausibility labels for radar raw detections, distinguishing between artifacts and targets. The optical evaluation backbone consists of a generalized monocular depth image estimation of surround view cameras plus LiDAR scans. Modern car sensor sets of cameras and LiDAR allow to calibrate image-based relative depth information in overlapping sensing areas. K-Nearest Neighbors matching relates the optical perception point cloud with raw radar detections. In parallel, a temporal tracking evaluation part considers the radar detections' transient behavior. Based on the distance between matches, respecting both sensor and model uncertainties, we propose a plausibility rating of every radar detection. We validate the results by evaluating error metrics on semi-manually labeled ground truth dataset of 3.28\cdot10^6 points. Besides generating plausible radar detections, the framework enables further labeled low-level radar signal datasets for applications of perception and Autonomous Driving learning tasks. △ Less","3 December, 2020",https://arxiv.org/pdf/2012.01993
SoK: Exploring the State of the Art and the Future Potential of Artificial Intelligence in Digital Forensic Investigation,Xiaoyu Du;Chris Hargreaves;John Sheppard;Felix Anda;Asanka Sayakkara;Nhien-An Le-Khac;Mark Scanlon,"Multi-year digital forensic backlogs have become commonplace in law enforcement agencies throughout the globe. Digital forensic investigators are overloaded with the volume of cases requiring their expertise compounded by the volume of data to be processed. Artificial intelligence is often seen as the solution to many big data problems. This paper summarises existing artificial intelligence based tools and approaches in digital forensics. Automated evidence processing leveraging artificial intelligence based techniques shows great promise in expediting the digital forensic analysis process while increasing case processing capacities. For each application of artificial intelligence highlighted, a number of current challenges and future potential impact is discussed. △ Less","2 December, 2020",https://arxiv.org/pdf/2012.01987
Automated Artefact Relevancy Determination from Artefact Metadata and Associated Timeline Events,Xiaoyu Du;Quan Le;Mark Scanlon,"Case-hindering, multi-year digital forensic evidence backlogs have become commonplace in law enforcement agencies throughout the world. This is due to an ever-growing number of cases requiring digital forensic investigation coupled with the growing volume of data to be processed per case. Leveraging previously processed digital forensic cases and their component artefact relevancy classifications can facilitate an opportunity for training automated artificial intelligence based evidence processing systems. These can significantly aid investigators in the discovery and prioritisation of evidence. This paper presents one approach for file artefact relevancy determination building on the growing trend towards a centralised, Digital Forensics as a Service (DFaaS) paradigm. This approach enables the use of previously encountered pertinent files to classify newly discovered files in an investigation. Trained models can aid in the detection of these files during the acquisition stage, i.e., during their upload to a DFaaS system. The technique generates a relevancy score for file similarity using each artefact's filesystem metadata and associated timeline events. The approach presented is validated against three experimental usage scenarios. △ Less","2 December, 2020",https://arxiv.org/pdf/2012.01972
IoT DoS and DDoS Attack Detection using ResNet,Faisal Hussain;Syed Ghazanfar Abbas;Muhammad Husnain;Ubaid Ullah Fayyaz;Farrukh Shahzad;Ghalib A. Shah,"The network attacks are increasing both in frequency and intensity with the rapid growth of internet of things (IoT) devices. Recently, denial of service (DoS) and distributed denial of service (DDoS) attacks are reported as the most frequent attacks in IoT networks. The traditional security solutions like firewalls, intrusion detection systems, etc., are unable to detect the complex DoS and DDoS attacks since most of them filter the normal and attack traffic based upon the static predefined rules. However, these solutions can become reliable and effective when integrated with artificial intelligence (AI) based techniques. During the last few years, deep learning models especially convolutional neural networks achieved high significance due to their outstanding performance in the image processing field. The potential of these convolutional neural network (CNN) models can be used to efficiently detect the complex DoS and DDoS by converting the network traffic dataset into images. Therefore, in this work, we proposed a methodology to convert the network traffic data into image form and trained a state-of-the-art CNN model, i.e., ResNet over the converted data. The proposed methodology accomplished 99.99\% accuracy for detecting the DoS and DDoS in case of binary classification. Furthermore, the proposed methodology achieved 87\% average precision for recognizing eleven types of DoS and DDoS attack patterns which is 9\% higher as compared to the state-of-the-art. △ Less","2 December, 2020",https://arxiv.org/pdf/2012.01971
Transfer Learning as an Enabler of the Intelligent Digital Twin,Benjamin Maschler;Dominik Braun;Nasser Jazdi;Michael Weyrich,"Digital Twins have been described as beneficial in many areas, such as virtual commissioning, fault prediction or reconfiguration planning. Equipping Digital Twins with artificial intelligence functionalities can greatly expand those beneficial applications or open up altogether new areas of application, among them cross-phase industrial transfer learning. In the context of machine learning, transfer learning represents a set of approaches that enhance learning new tasks based upon previously acquired knowledge. Here, knowledge is transferred from one lifecycle phase to another in order to reduce the amount of data or time needed to train a machine learning algorithm. Looking at common challenges in developing and deploying industrial machinery with deep learning functionalities, embracing this concept would offer several advantages: Using an intelligent Digital Twin, learning algorithms can be designed, configured and tested in the design phase before the physical system exists and real data can be collected. Once real data becomes available, the algorithms must merely be fine-tuned, significantly speeding up commissioning and reducing the probability of costly modifications. Furthermore, using the Digital Twin's simulation capabilities virtually injecting rare faults in order to train an algorithm's response or using reinforcement learning, e.g. to teach a robot, become practically feasible. This article presents several cross-phase industrial transfer learning use cases utilizing intelligent Digital Twins. A real cyber physical production system consisting of an automated welding machine and an automated guided vehicle equipped with a robot arm is used to illustrate the respective benefits. △ Less","3 December, 2020",https://arxiv.org/pdf/2012.01913
Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems,Jan Strohschein;Andreas Fischbach;Andreas Bunte;Heide Faeskorn-Woyke;Natalia Moriz;Thomas Bartz-Beielstein,"This paper presents the cognitive module of the cognitive architecture for artificial intelligence (CAAI) in cyber-physical production systems (CPPS). The goal of this architecture is to reduce the implementation effort of artificial intelligence (AI) algorithms in CPPS. Declarative user goals and the provided algorithm-knowledge base allow the dynamic pipeline orchestration and configuration. A big data platform (BDP) instantiates the pipelines and monitors the CPPS performance for further evaluation through the cognitive module. Thus, the cognitive module is able to select feasible and robust configurations for process pipelines in varying use cases. Furthermore, it automatically adapts the models and algorithms based on model quality and resource consumption. The cognitive module also instantiates additional pipelines to test algorithms from different classes. CAAI relies on well-defined interfaces to enable the integration of additional modules and reduce implementation effort. Finally, an implementation based on Docker, Kubernetes, and Kafka for the virtualization and orchestration of the individual modules and as messaging-technology for module communication is used to evaluate a real-world use case. △ Less","3 December, 2020",https://arxiv.org/pdf/2012.01823
Explainable AI for Software Engineering,Chakkrit Tantithamthavorn;Jirayus Jiarpakdee;John Grundy,"Artificial Intelligence/Machine Learning techniques have been widely used in software engineering to improve developer productivity, the quality of software systems, and decision-making. However, such AI/ML models for software engineering are still impractical, not explainable, and not actionable. These concerns often hinder the adoption of AI/ML models in software engineering practices. In this article, we first highlight the need for explainable AI in software engineering. Then, we summarize three successful case studies on how explainable AI techniques can be used to address the aforementioned challenges by making software defect prediction models more practical, explainable, and actionable. △ Less","2 December, 2020",https://arxiv.org/pdf/2012.01614
DecisiveNets: Training Deep Associative Memories to Solve Complex Machine Learning Problems,Vincent Gripon;Carlos Lassance;Ghouthi Boukli Hacene,"Learning deep representations to solve complex machine learning tasks has become the prominent trend in the past few years. Indeed, Deep Neural Networks are now the golden standard in domains as various as computer vision, natural language processing or even playing combinatorial games. However, problematic limitations are hidden behind this surprising universal capability. Among other things, explainability of the decisions is a major concern, especially since deep neural networks are made up of a very large number of trainable parameters. Moreover, computational complexity can quickly become a problem, especially in contexts constrained by real time or limited resources. Therefore, understanding how information is stored and the impact this storage can have on the system remains a major and open issue. In this chapter, we introduce a method to transform deep neural network models into deep associative memories, with simpler, more explicable and less expensive operations. We show through experiments that these transformations can be done without penalty on predictive performance. The resulting deep associative memories are excellent candidates for artificial intelligence that is easier to theorize and manipulate. △ Less","2 December, 2020",https://arxiv.org/pdf/2012.01509
Empirical Study on the Software Engineering Practices in Open Source ML Package Repositories,Minke Xiu;Ellis E. Eghan;Zhen Ming;Jiang;Bram Adams,"Recent advances in Artificial Intelligence (AI), especially in Machine Learning (ML), have introduced various practical applications (e.g., virtual personal assistants and autonomous cars) that enhance the experience of everyday users. However, modern ML technologies like Deep Learning require considerable technical expertise and resources to develop, train and deploy such models, making effective reuse of the ML models a necessity. Such discovery and reuse by practitioners and researchers are being addressed by public ML package repositories, which bundle up pre-trained models into packages for publication. Since such repositories are a recent phenomenon, there is no empirical data on their current state and challenges. Hence, this paper conducts an exploratory study that analyzes the structure and contents of two popular ML package repositories, TFHub and PyTorch Hub, comparing their information elements (features and policies), package organization, package manager functionalities and usage contexts against popular software package repositories (npm, PyPI, and CRAN). Through these studies, we have identified unique SE practices and challenges for sharing ML packages. These findings and implications would be useful for data scientists, researchers and software developers who intend to use these shared ML packages. △ Less","8 December, 2020",https://arxiv.org/pdf/2012.01403
Artificial intelligence techniques for integrative structural biology of intrinsically disordered proteins,Arvind Ramanathan;Heng Ma;Akash Parvatikar;Chakra S. Chennubhotla,"We outline recent developments in artificial intelligence (AI) and machine learning (ML) techniques for integrative structural biology of intrinsically disordered proteins (IDP) ensembles. IDPs challenge the traditional protein structure-function paradigm by adapting their conformations in response to specific binding partners leading them to mediate diverse, and often complex cellular functions such as biological signaling, self organization and compartmentalization. Obtaining mechanistic insights into their function can therefore be challenging for traditional structural determination techniques. Often, scientists have to rely on piecemeal evidence drawn from diverse experimental techniques to characterize their functional mechanisms. Multiscale simulations can help bridge critical knowledge gaps about IDP structure function relationships - however, these techniques also face challenges in resolving emergent phenomena within IDP conformational ensembles. We posit that scalable statistical inference techniques can effectively integrate information gleaned from multiple experimental techniques as well as from simulations, thus providing access to atomistic details of these emergent phenomena. △ Less","1 December, 2020",https://arxiv.org/pdf/2012.00885
A Chatbot for Information Security,Sofian Hamad;Taoufik Yeferny,"Advancements in artificial intelligence (AI), speech recognition systems (ASR), and machine learning have enabled the development of intelligent computer programs called chatbots. Many chatbots have been proposed to provide different services in many areas such as customer service, sales and marketing. However, the use of chatbot as advisers in the field of information security is not yet considered. Furthermore, people, especially normal users who have no technical background, are unaware about many of aspects in information security. Therefore, in this paper we proposed a chatbot that acts as an adviser in information security. The proposed adviser uses a knowledge base with json file. Having such chatbot provides many features including raising the awareness in field of information security by offering accurate advice, based on different opinions from information security expertise, for many users on different. Furthermore, this chatbot is currently deployed through Telegram platform, which is one of widely used social network platforms. The deployment of the proposed chatbot over different platforms is considered as the future work. △ Less","1 December, 2020",https://arxiv.org/pdf/2012.00826
A Framework and Dataset for Abstract Art Generation via CalligraphyGAN,Jinggang Zhuo;Ling Fan;Harry Jiannan Wang,"With the advancement of deep learning, artificial intelligence (AI) has made many breakthroughs in recent years and achieved superhuman performance in various tasks such as object detection, reading comprehension, and video games. Generative Modeling, such as various Generative Adversarial Networks (GAN) models, has been applied to generate paintings and music. Research in Natural Language Processing (NLP) also had a leap forward in 2018 since the release of the pre-trained contextual neural language models such as BERT and recently released GPT3. Despite the exciting AI applications aforementioned, AI is still significantly lagging behind humans in creativity, which is often considered the ultimate moonshot for AI. Our work is inspired by Chinese calligraphy, which is a unique form of visual art where the character itself is an aesthetic painting. We also draw inspirations from paintings of the Abstract Expressionist movement in the 1940s and 1950s, such as the work by American painter Franz Kline. In this paper, we present a creative framework based on Conditional Generative Adversarial Networks and Contextual Neural Language Model to generate abstract artworks that have intrinsic meaning and aesthetic value, which is different from the existing work, such as image captioning and text-to-image generation, where the texts are the descriptions of the images. In addition, we have publicly released a Chinese calligraphy image dataset and demonstrate our framework using a prototype system and a user study. △ Less","2 December, 2020",https://arxiv.org/pdf/2012.00744
Neural language models for text classification in evidence-based medicine,Andres Carvallo;Denis Parra;Gabriel Rada;Daniel Perez;Juan Ignacio Vasquez;Camilo Vergara,"The COVID-19 has brought about a significant challenge to the whole of humanity, but with a special burden upon the medical community. Clinicians must keep updated continuously about symptoms, diagnoses, and effectiveness of emergent treatments under a never-ending flood of scientific literature. In this context, the role of evidence-based medicine (EBM) for curating the most substantial evidence to support public health and clinical practice turns essential but is being challenged as never before due to the high volume of research articles published and pre-prints posted daily. Artificial Intelligence can have a crucial role in this situation. In this article, we report the results of an applied research project to classify scientific articles to support Epistemonikos, one of the most active foundations worldwide conducting EBM. We test several methods, and the best one, based on the XLNet neural language model, improves the current approach by 93\% on average F1-score, saving valuable time from physicians who volunteer to curate COVID-19 research articles manually. △ Less","1 December, 2020",https://arxiv.org/pdf/2012.00584
A Neural Dynamic Model based on Activation Diffusion and a Micro-Explanation for Cognitive Operations,Hui Wei,"The neural mechanism of memory has a very close relation with the problem of representation in artificial intelligence. In this paper a computational model was proposed to simulate the network of neurons in brain and how they process information. The model refers to morphological and electrophysiological characteristics of neural information processing, and is based on the assumption that neurons encode their firing sequence. The network structure, functions for neural encoding at different stages, the representation of stimuli in memory, and an algorithm to form a memory were presented. It also analyzed the stability and recall rate for learning and the capacity of memory. Because neural dynamic processes, one succeeding another, achieve a neuron-level and coherent form by which information is represented and processed, it may facilitate examination of various branches of Artificial Intelligence, such as inference, problem solving, pattern recognition, natural language processing and learning. The processes of cognitive manipulation occurring in intelligent behavior have a consistent representation while all being modeled from the perspective of computational neuroscience. Thus, the dynamics of neurons make it possible to explain the inner mechanisms of different intelligent behaviors by a unified model of cognitive architecture at a micro-level. △ Less","26 November, 2020",https://arxiv.org/pdf/2012.00104
Why model why? Assessing the strengths and limitations of LIME,Jürgen Dieber;Sabrina Kirrane,"When it comes to complex machine learning models, commonly referred to as black boxes, understanding the underlying decision making process is crucial for domains such as healthcare and financial services, and also when it is used in connection with safety critical systems such as autonomous vehicles. As such interest in explainable artificial intelligence (xAI) tools and techniques has increased in recent years. However, the effectiveness of existing xAI frameworks, especially concerning algorithms that work with data as opposed to images, is still an open research question. In order to address this gap, in this paper we examine the effectiveness of the Local Interpretable Model-Agnostic Explanations (LIME) xAI framework, one of the most popular model agnostic frameworks found in the literature, with a specific focus on its performance in terms of making tabular models more interpretable. In particular, we apply several state of the art machine learning algorithms on a tabular dataset, and demonstrate how LIME can be used to supplement conventional performance assessment methods. In addition, we evaluate the understandability of the output produced by LIME both via a usability study, involving participants who are not familiar with LIME, and its overall usability via an assessment framework, which is derived from the International Organisation for Standardisation 9241-11:1998 standard. △ Less","30 November, 2020",https://arxiv.org/pdf/2012.00093
A Review of Recent Advances of Binary Neural Networks for Edge Computing,Wenyu Zhao;Teli Ma;Xuan Gong;Baochang Zhang;David Doermann,"Edge computing is promising to become one of the next hottest topics in artificial intelligence because it benefits various evolving domains such as real-time unmanned aerial systems, industrial applications, and the demand for privacy protection. This paper reviews recent advances on binary neural network (BNN) and 1-bit CNN technologies that are well suitable for front-end, edge-based computing. We introduce and summarize existing work and classify them based on gradient approximation, quantization, architecture, loss functions, optimization method, and binary neural architecture search. We also introduce applications in the areas of computer vision and speech recognition and discuss future applications for edge computing. △ Less","23 November, 2020",https://arxiv.org/pdf/2011.14824
Bringing AI To Edge: From Deep Learning's Perspective,Di Liu;Hao Kong;Xiangzhong Luo;Weichen Liu;Ravi Subramaniam,"Edge computing and artificial intelligence (AI), especially deep learning for nowadays, are gradually intersecting to build a novel system, called edge intelligence. However, the development of edge intelligence systems encounters some challenges, and one of these challenges is the \textit{computational gap} between computation-intensive deep learning algorithms and less-capable edge systems. Due to the computational gap, many edge intelligence systems cannot meet the expected performance requirements. To bridge the gap, a plethora of deep learning techniques and optimization methods are proposed in the past years: light-weight deep learning models, network compression, and efficient neural architecture search. Although some reviews or surveys have partially covered this large body of literature, we lack a systematic and comprehensive review to discuss all aspects of these deep learning techniques which are critical for edge intelligence implementation. As various and diverse methods which are applicable to edge systems are proposed intensively, a holistic review would enable edge computing engineers and community to know the state-of-the-art deep learning techniques which are instrumental for edge intelligence and to facilitate the development of edge intelligence systems. This paper surveys the representative and latest deep learning techniques that are useful for edge intelligence systems, including hand-crafted models, model compression, hardware-aware neural architecture search and adaptive deep learning models. Finally, based on observations and simple experiments we conducted, we discuss some future directions. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.14808
Feature Space Singularity for Out-of-Distribution Detection,Haiwen Huang;Zhihan Li;Lulu Wang;Sishuo Chen;Bin Dong;Xinyu Zhou,"Out-of-Distribution (OoD) detection is important for building safe artificial intelligence systems. However, current OoD detection methods still cannot meet the performance requirements for practical deployment. In this paper, we propose a simple yet effective algorithm based on a novel observation: in a trained neural network, OoD samples with bounded norms well concentrate in the feature space. We call the center of OoD features the Feature Space Singularity (FSS), and denote the distance of a sample feature to FSS as FSSD. Then, OoD samples can be identified by taking a threshold on the FSSD. Our analysis of the phenomenon reveals why our algorithm works. We demonstrate that our algorithm achieves state-of-the-art performance on various OoD detection benchmarks. Besides, FSSD also enjoys robustness to slight corruption in test data and can be further enhanced by ensembling. These make FSSD a promising algorithm to be employed in real world. We release our code at \url{https://github.com/megvii-research/FSSD_OoD_Detection}. △ Less","16 December, 2020",https://arxiv.org/pdf/2011.14654
"Audio, Speech, Language, & Signal Processing for COVID-19: A Comprehensive Overview",Gauri Deshpande;Björn W. Schuller,"The Coronavirus (COVID-19) pandemic has been the research focus world-wide in the year 2020. Several efforts, from collection of COVID-19 patients' data to screening them for the virus's detection are taken with rigour. A major portion of COVID-19 symptoms are related to the functioning of the respiratory system, which in-turn critically influences the human speech production system. This drives the research focus towards identifying the markers of COVID-19 in speech and other human generated audio signals. In this paper, we give an overview of the speech and other audio signal, language and general signal processing-based work done using Artificial Intelligence techniques to screen, diagnose, monitor, and spread the awareness aboutCOVID-19. We also briefly describe the research related to detect accord-ing COVID-19 symptoms carried out so far. We aspire that this collective information will be useful in developing automated systems, which can help in the context of COVID-19 using non-obtrusive and easy to use modalities such as audio, speech, and language. △ Less","29 November, 2020",https://arxiv.org/pdf/2011.14445
Methods Matter: A Trading Agent with No Intelligence Routinely Outperforms AI-Based Traders,Dave Cliff;Michael Rollins,"There's a long tradition of research using computational intelligence (methods from artificial intelligence (AI) and machine learning (ML)), to automatically discover, implement, and fine-tune strategies for autonomous adaptive automated trading in financial markets, with a sequence of research papers on this topic published at AI conferences such as IJCAI and in journals such as Artificial Intelligence: we show here that this strand of research has taken a number of methodological mis-steps and that actually some of the reportedly best-performing public-domain AI/ML trading strategies can routinely be out-performed by extremely simple trading strategies that involve no AI or ML at all. The results that we highlight here could easily have been revealed at the time that the relevant key papers were published, more than a decade ago, but the accepted methodology at the time of those publications involved a somewhat minimal approach to experimental evaluation of trader-agents, making claims on the basis of a few thousand test-sessions of the trader-agent in a small number of market scenarios. In this paper we present results from exhaustive testing over wide ranges of parameter values, using parallel cloud-computing facilities, where we conduct millions of tests and thereby create much richer data from which firmer conclusions can be drawn. We show that the best public-domain AI/ML traders in the published literature can be routinely outperformed by a ""sub-zero-intelligence"" trading strategy that at face value appears to be so simple as to be financially ruinous, but which interacts with the market in such a way that in practice it is more profitable than the well-known AI/ML strategies from the research literature. That such a simple strategy can outperform established AI/ML-based strategies is a sign that perhaps the AI/ML trading strategies were good answers to the wrong question. △ Less","29 November, 2020",https://arxiv.org/pdf/2011.14346
Monte Carlo Tree Search for a single target search game on a 2-D lattice,Elana Kozak;Scott Hottovy,"Monte Carlo Tree Search (MCTS) is a branch of stochastic modeling that utilizes decision trees for optimization, mostly applied to artificial intelligence (AI) game players. This project imagines a game in which an AI player searches for a stationary target within a 2-D lattice. We analyze its behavior with different target distributions and compare its efficiency to the Levy Flight Search, a model for animal foraging behavior. In addition to simulated data analysis we prove two theorems about the convergence of MCTS when computation constraints neglected. △ Less","28 November, 2020",https://arxiv.org/pdf/2011.14246
Privacy-Preserving Federated Learning for UAV-Enabled Networks: Learning-Based Joint Scheduling and Resource Management,Helin Yang;Jun Zhao;Zehui Xiong;Kwok-Yan Lam;Sumei Sun;Liang Xiao,"Unmanned aerial vehicles (UAVs) are capable of serving as flying base stations (BSs) for supporting data collection, artificial intelligence (AI) model training, and wireless communications. However, due to the privacy concerns of devices and limited computation or communication resource of UAVs, it is impractical to send raw data of devices to UAV servers for model training. Moreover, due to the dynamic channel condition and heterogeneous computing capacity of devices in UAV-enabled networks, the reliability and efficiency of data sharing require to be further improved. In this paper, we develop an asynchronous federated learning (AFL) framework for multi-UAV-enabled networks, which can provide asynchronous distributed computing by enabling model training locally without transmitting raw sensitive data to UAV servers. The device selection strategy is also introduced into the AFL framework to keep the low-quality devices from affecting the learning efficiency and accuracy. Moreover, we propose an asynchronous advantage actor-critic (A3C) based joint device selection, UAVs placement, and resource management algorithm to enhance the federated convergence speed and accuracy. Simulation results demonstrate that our proposed framework and algorithm achieve higher learning accuracy and faster federated execution time compared to other existing solutions. △ Less","28 November, 2020",https://arxiv.org/pdf/2011.14197
Meta-learning in natural and artificial intelligence,Jane X. Wang,"Meta-learning, or learning to learn, has gained renewed interest in recent years within the artificial intelligence community. However, meta-learning is incredibly prevalent within nature, has deep roots in cognitive science and psychology, and is currently studied in various forms within neuroscience. The aim of this review is to recast previous lines of research in the study of biological intelligence within the lens of meta-learning, placing these works into a common framework. More recent points of interaction between AI and neuroscience will be discussed, as well as interesting new directions that arise under this perspective. △ Less","26 November, 2020",https://arxiv.org/pdf/2011.13464
Nose to Glass: Looking In to Get Beyond,Josephine Seah,"Brought into the public discourse through investigative work by journalists and scholars, awareness of algorithmic harms is at an all-time high. An increasing amount of research has been conducted under the banner of enhancing responsible artificial intelligence (AI), with the goal of addressing, alleviating, and eventually mitigating the harms brought on by the roll out of algorithmic systems. Nonetheless, implementation of such tools remains low. Given this gap, this paper offers a modest proposal: that the field, particularly researchers concerned with responsible research and innovation, may stand to gain from supporting and prioritising more ethnographic work. This embedded work can flesh out implementation frictions and reveal organisational and institutional norms that existing work on responsible artificial intelligence AI has not yet been able to offer. In turn, this can contribute to more insights about the anticipation of risks and mitigation of harm. This paper reviews similar empirical work typically found elsewhere, commonly in science and technology studies and safety science research, and lays out challenges of this form of inquiry. △ Less","1 December, 2020",https://arxiv.org/pdf/2011.13153
Real-time error correction and performance aid for MIDI instruments,Georgi Marinov,"Making a slight mistake during live music performance can easily be spotted by an astute listener, even if the performance is an improvisation or an unfamiliar piece. An example might be a highly dissonant chord played by mistake in a classical-era sonata, or a sudden off-key note in a recurring motif. The problem of identifying and correcting such errors can be approached with artificial intelligence -- if a trained human can easily do it, maybe a computer can be trained to spot the errors quickly and just as accurately. The ability to identify and auto-correct errors in real-time would be not only extremely useful to performing musicians, but also a valuable asset for producers, allowing much fewer overdubs and re-recording of takes due to small imperfections. This paper examines state-of-the-art solutions to related problems and explores novel solutions for music error detection and correction, focusing on their real-time applicability. The explored approaches consider error detection through music context and theory, as well as supervised learning models with no predefined musical information or rules, trained on appropriate datasets. Focusing purely on correcting musical errors, the presented solutions operate on a high-level representation of the audio (MIDI) instead of the raw audio domain, taking input from an electronic instrument (MIDI keyboard/piano) and altering it when needed before it is sent to the sampler. This work proposes multiple general recurrent neural network designs for real-time error correction and performance aid for MIDI instruments, discusses the results, limitations, and possible future improvements. It also emphasizes on making the research results easily accessible to the end user - music enthusiasts, producers and performers -- by using the latest artificial intelligence platforms and tools. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.13122
Learning Causal Bayesian Networks from Text,Farhad Moghimifar;Afshin Rahimi;Mahsa Baktashmotlagh;Xue Li,"Causal relationships form the basis for reasoning and decision-making in Artificial Intelligence systems. To exploit the large volume of textual data available today, the automatic discovery of causal relationships from text has emerged as a significant challenge in recent years. Existing approaches in this realm are limited to the extraction of low-level relations among individual events. To overcome the limitations of the existing approaches, in this paper, we propose a method for automatic inference of causal relationships from human written language at conceptual level. To this end, we leverage the characteristics of hierarchy of concepts and linguistic variables created from text, and represent the extracted causal relationships in the form of a Causal Bayesian Network. Our experiments demonstrate superiority of our approach over the existing approaches in inferring complex causal reasoning from the text. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.13115
The Evolution of Concept-Acquisition based on Developmental Psychology,Hui Wei,"A conceptual system with rich connotation is key to improving the performance of knowledge-based artificial intelligence systems. While a conceptual system, which has abundant concepts and rich semantic relationships, and is developable, evolvable, and adaptable to multi-task environments, its actual construction is not only one of the major challenges of knowledge engineering, but also the fundamental goal of research on knowledge and conceptualization. Finding a new method to represent concepts and construct a conceptual system will therefore greatly improve the performance of many intelligent systems. Fortunately the core of human cognition is a system with relatively complete concepts and a mechanism that ensures the establishment and development of the system. The human conceptual system can not be achieved immediately, but rather must develop gradually. Developmental psychology carefully observes the process of concept acquisition in humans at the behavioral level, and along with cognitive psychology has proposed some rough explanations of those observations. However, due to the lack of research in aspects such as representation, systematic models, algorithm details and realization, many of the results of developmental psychology have not been applied directly to the building of artificial conceptual systems. For example, Karmiloff-Smith's Representation Redescription (RR) supposition reflects a concept-acquisition process that re-describes a lower level representation of a concept to a higher one. This paper is inspired by this developmental psychology viewpoint. We use an object-oriented approach to re-explain and materialize RR supposition from the formal semantic perspective, because the OO paradigm is a natural way to describe the outside world, and it also has strict grammar regulations. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.13089
European Strategy on AI: Are we truly fostering social good?,Francesca Foffano;Teresa Scantamburlo;Atia Cortés;Chiara Bissolo,"Artificial intelligence (AI) is already part of our daily lives and is playing a key role in defining the economic and social shape of the future. In 2018, the European Commission introduced its AI strategy able to compete in the next years with world powers such as China and US, but relying on the respect of European values and fundamental rights. As a result, most of the Member States have published their own National Strategy with the aim to work on a coordinated plan for Europe. In this paper, we present an ongoing study on how European countries are approaching the field of Artificial Intelligence, with its promises and risks, through the lens of their national AI strategies. In particular, we aim to investigate how European countries are investing in AI and to what extent the stated plans can contribute to the benefit of the whole society. This paper reports the main findings of a qualitative analysis of the investment plans reported in 15 European National Strategies △ Less","25 November, 2020",https://arxiv.org/pdf/2011.12863
Supervised Learning Achieves Human-Level Performance in MOBA Games: A Case Study of Honor of Kings,Deheng Ye;Guibin Chen;Peilin Zhao;Fuhao Qiu;Bo Yuan;Wen Zhang;Sheng Chen;Mingfei Sun;Xiaoqian Li;Siqin Li;Jing Liang;Zhenjie Lian;Bei Shi;Liang Wang;Tengfei Shi;Qiang Fu;Wei Yang;Lanxiao Huang,"We present JueWu-SL, the first supervised-learning-based artificial intelligence (AI) program that achieves human-level performance in playing multiplayer online battle arena (MOBA) games. Unlike prior attempts, we integrate the macro-strategy and the micromanagement of MOBA-game-playing into neural networks in a supervised and end-to-end manner. Tested on Honor of Kings, the most popular MOBA at present, our AI performs competitively at the level of High King players in standard 5v5 games. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.12582
Measuring Happiness Around the World Through Artificial Intelligence,Rustem Ozakar;Rafet Efe Gazanfer;Y. Sinan Hanay,"In this work, we analyze the happiness levels of countries using an unbiased emotion detector, artificial intelligence (AI). To date, researchers proposed many factors that may affect happiness such as wealth, health and safety. Even though these factors all seem relevant, there is no clear consensus between sociologists on how to interpret these, and the models to estimate the cost of these utilities include some assumptions. Researchers in social sciences have been working on determination of the happiness levels in society and exploration of the factors correlated with it through polls and different statistical methods. In our work, by using artificial intelligence, we introduce a different and relatively unbiased approach to this problem. By using AI, we make no assumption about what makes a person happy, and leave the decision to AI to detect the emotions from the faces of people collected from publicly available street footages. We analyzed the happiness levels in eight different cities around the world through available footage on the Internet and found out that there is no statistically significant difference between countries in terms of happiness. △ Less","25 November, 2020",https://arxiv.org/pdf/2011.12548
Blind deblurring for microscopic pathology images using deep learning networks,Cheng Jiang;Jun Liao;Pei Dong;Zhaoxuan Ma;De Cai;Guoan Zheng;Yueping Liu;Hong Bu;Jianhua Yao,"Artificial Intelligence (AI)-powered pathology is a revolutionary step in the world of digital pathology and shows great promise to increase both diagnosis accuracy and efficiency. However, defocus and motion blur can obscure tissue or cell characteristics hence compromising AI algorithms'accuracy and robustness in analyzing the images. In this paper, we demonstrate a deep-learning-based approach that can alleviate the defocus and motion blur of a microscopic image and output a sharper and cleaner image with retrieved fine details without prior knowledge of the blur type, blur extent and pathological stain. In this approach, a deep learning classifier is first trained to identify the image blur type. Then, two encoder-decoder networks are trained and used alone or in combination to deblur the input image. It is an end-to-end approach and introduces no corrugated artifacts as traditional blind deconvolution methods do. We test our approach on different types of pathology specimens and demonstrate great performance on image blur correction and the subsequent improvement on the diagnosis outcome of AI algorithms. △ Less","23 November, 2020",https://arxiv.org/pdf/2011.11879
When Machine Learning Meets Privacy: A Survey and Outlook,Bo Liu;Ming Ding;Sina Shaham;Wenny Rahayu;Farhad Farokhi;Zihuai Lin,"The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field. △ Less","23 November, 2020",https://arxiv.org/pdf/2011.11819
Integrating Deep Learning in Domain Sciences at Exascale,Rick Archibald;Edmond Chow;Eduardo D'Azevedo;Jack Dongarra;Markus Eisenbach;Rocco Febbo;Florent Lopez;Daniel Nichols;Stanimire Tomov;Kwai Wong;Junqi Yin,"This paper presents some of the current challenges in designing deep learning artificial intelligence (AI) and integrating it with traditional high-performance computing (HPC) simulations. We evaluate existing packages for their ability to run deep learning models and applications on large-scale HPC systems efficiently, identify challenges, and propose new asynchronous parallelization and optimization techniques for current large-scale heterogeneous systems and upcoming exascale systems. These developments, along with existing HPC AI software capabilities, have been integrated into MagmaDNN, an open-source HPC deep learning framework. Many deep learning frameworks are targeted at data scientists and fall short in providing quality integration into existing HPC workflows. This paper discusses the necessities of an HPC deep learning framework and how those needs can be provided (e.g., as in MagmaDNN) through a deep integration with existing HPC libraries, such as MAGMA and its modular memory management, MPI, CuBLAS, CuDNN, MKL, and HIP. Advancements are also illustrated through the use of algorithmic enhancements in reduced- and mixed-precision, as well as asynchronous optimization methods. Finally, we present illustrations and potential solutions for enhancing traditional compute- and data-intensive applications at ORNL and UTK with AI. The approaches and future challenges are illustrated in materials science, imaging, and climate applications. △ Less","22 November, 2020",https://arxiv.org/pdf/2011.11188
Learning a Deep Generative Model like a Program: the Free Category Prior,Eli Sennesh,"Humans surpass the cognitive abilities of most other animals in our ability to ""chunk"" concepts into words, and then combine the words to combine the concepts. In this process, we make ""infinite use of finite means"", enabling us to learn new concepts quickly and nest concepts within each-other. While program induction and synthesis remain at the heart of foundational theories of artificial intelligence, only recently has the community moved forward in attempting to use program learning as a benchmark task itself. The cognitive science community has thus often assumed that if the brain has simulation and reasoning capabilities equivalent to a universal computer, then it must employ a serialized, symbolic representation. Here we confront that assumption, and provide a counterexample in which compositionality is expressed via network structure: the free category prior over programs. We show how our formalism allows neural networks to serve as primitives in probabilistic programs. We learn both program structure and model parameters end-to-end. △ Less","22 November, 2020",https://arxiv.org/pdf/2011.11063
Investigating Emotion-Color Association in Deep Neural Networks,Shivi Gupta;Shashi Kant Gupta,"It has been found that representations learned by Deep Neural Networks (DNNs) correlate very well to neural responses measured in primates' brains and psychological representations exhibited by human similarity judgment. On another hand, past studies have shown that particular colors can be associated with specific emotion arousal in humans. Do deep neural networks also learn this behavior? In this study, we investigate if DNNs can learn implicit associations in stimuli, particularly, an emotion-color association between image stimuli. Our study was conducted in two parts. First, we collected human responses on a forced-choice decision task in which subjects were asked to select a color for a specified emotion-inducing image. Next, we modeled this decision task on neural networks using the similarity between deep representation (extracted using DNNs trained on object classification tasks) of the images and images of colors used in the task. We found that our model showed a fuzzy linear relationship between the two decision probabilities. This results in two interesting findings, 1. The representations learned by deep neural networks can indeed show an emotion-color association 2. The emotion-color association is not just random but involves some cognitive phenomena. Finally, we also show that this method can help us in the emotion classification task, specifically when there are very few examples to train the model. This analysis can be relevant to psychologists studying emotion-color associations and artificial intelligence researchers modeling emotional intelligence in machines or studying representations learned by deep neural networks. △ Less","22 November, 2020",https://arxiv.org/pdf/2011.11058
Computation harvesting in road traffic dynamics,Hiroyasu Ando;T. Okamoto;H. Chang;T. Noguchi;Shinji Nakaoka,"Owing to recent advances in artificial intelligence and internet of things (IoT) technologies, collected big data facilitates high computational performance, while its computational resources and energy cost are large. Moreover, data are often collected but not used. To solve these problems, we propose a framework for a computational model that follows a natural computational system, such as the human brain, and does not rely heavily on electronic computers. In particular, we propose a methodology based on the concept of `computation harvesting', which uses IoT data collected from rich sensors and leaves most of the computational processes to real-world phenomena as collected data. This aspect assumes that large-scale computations can be fast and resilient. Herein, we perform prediction tasks using real-world road traffic data to show the feasibility of computation harvesting. First, we show that the substantial computation in traffic flow is resilient against sensor failure and real-time traffic changes due to several combinations of harvesting from spatiotemporal dynamics to synthesize specific patterns. Next, we show the practicality of this method as a real-time prediction because of its low computational cost. Finally, we show that, compared to conventional methods, our method requires lower resources while providing a comparable performance. △ Less","21 November, 2020",https://arxiv.org/pdf/2011.10744
GenderRobustness: Robustness of Gender Detection in Facial Recognition Systems with variation in Image Properties,Sharadha Srinivasan;Madan Musuvathi,"In recent times, there have been increasing accusations on artificial intelligence systems and algorithms of computer vision of possessing implicit biases. Even though these conversations are more prevalent now and systems are improving by performing extensive testing and broadening their horizon, biases still do exist. One such class of systems where bias is said to exist is facial recognition systems, where bias has been observed on the basis of gender, ethnicity, skin tone and other facial attributes. This is even more disturbing, given the fact that these systems are used in practically every sector of the industries today. From as critical as criminal identification to as simple as getting your attendance registered, these systems have gained a huge market, especially in recent years. That in itself is a good enough reason for developers of these systems to ensure that the bias is kept to a bare minimum or ideally non-existent, to avoid major issues like favoring a particular gender, race, or class of people or rather making a class of people susceptible to false accusations due to inability of these systems to correctly recognize those people. △ Less","26 November, 2020",https://arxiv.org/pdf/2011.10472
Exploring the political pulse of a country using data science tools,Miguel G. Folgado;Verónica Sanz,"In this paper we illustrate the use of Data Science techniques to analyse complex human communication. In particular, we consider tweets from leaders of political parties as a dynamical proxy to political programmes and ideas. We also study the temporal evolution of their contents as a reaction to specific events. We analyse levels of positive and negative sentiment in the tweets using new tools adapted to social media. We also train an Artificial Intelligence to recognise the political affiliation of a tweet. The AI is able to predict the origin of the tweet with a precision in the range of 71-75\%, and the political leaning (left or right) with a precision of around 90\%. This study is meant to be viewed as a proof-of-concept of interdisciplinary nature, at the interface between Data Science and political analysis. △ Less","20 November, 2020",https://arxiv.org/pdf/2011.10264
Collaborative Storytelling with Large-scale Neural Language Models,Eric Nichols;Leo Gao;Randy Gomez,"Storytelling plays a central role in human socializing and entertainment. However, much of the research on automatic storytelling generation assumes that stories will be generated by an agent without any human interaction. In this paper, we introduce the task of collaborative storytelling, where an artificial intelligence agent and a person collaborate to create a unique story by taking turns adding to it. We present a collaborative storytelling system which works with a human storyteller to create a story by generating new utterances based on the story so far. We constructed the storytelling system by tuning a publicly-available large scale language model on a dataset of writing prompts and their accompanying fictional works. We identify generating sufficiently human-like utterances to be an important technical issue and propose a sample-and-rank approach to improve utterance quality. Quantitative evaluation shows that our approach outperforms a baseline, and we present qualitative evaluation of our system's capabilities. △ Less","19 November, 2020",https://arxiv.org/pdf/2011.10208
Data Representing Ground-Truth Explanations to Evaluate XAI Methods,Shideh Shams Amiri;Rosina O. Weber;Prateek Goel;Owen Brooks;Archer Gandley;Brian Kitchell;Aaron Zehm,"Explainable artificial intelligence (XAI) methods are currently evaluated with approaches mostly originated in interpretable machine learning (IML) research that focus on understanding models such as comparison against existing attribution approaches, sensitivity analyses, gold set of features, axioms, or through demonstration of images. There are problems with these methods such as that they do not indicate where current XAI approaches fail to guide investigations towards consistent progress of the field. They do not measure accuracy in support of accountable decisions, and it is practically impossible to determine whether one XAI method is better than the other or what the weaknesses of existing models are, leaving researchers without guidance on which research questions will advance the field. Other fields usually utilize ground-truth data and create benchmarks. Data representing ground-truth explanations is not typically used in XAI or IML. One reason is that explanations are subjective, in the sense that an explanation that satisfies one user may not satisfy another. To overcome these problems, we propose to represent explanations with canonical equations that can be used to evaluate the accuracy of XAI methods. The contributions of this paper include a methodology to create synthetic data representing ground-truth explanations, three data sets, an evaluation of LIME using these data sets, and a preliminary analysis of the challenges and potential benefits in using these data to evaluate existing XAI approaches. Evaluation methods based on human-centric studies are outside the scope of this paper. △ Less","18 November, 2020",https://arxiv.org/pdf/2011.09892
Explainable Incipient Fault Detection Systems for Photovoltaic Panels,S. Sairam;Seshadhri Srinivasan;G. Marafioti;B. Subathra;G. Mathisen;Korkut Bekiroglu,"This paper presents an eXplainable Fault Detection and Diagnosis System (XFDDS) for incipient faults in PV panels. The XFDDS is a hybrid approach that combines the model-based and data-driven framework. Model-based FDD for PV panels lacks high fidelity models at low irradiance conditions for detecting incipient faults. To overcome this, a novel irradiance based three diode model (IB3DM) is proposed. It is a nine parameter model that provides higher accuracy even at low irradiance conditions, an important aspect for detecting incipient faults from noise. To exploit PV data, extreme gradient boosting (XGBoost) is used due to its ability to detecting incipient faults. Lack of explainability, feature variability for sample instances, and false alarms are challenges with data-driven FDD methods. These shortcomings are overcome by hybridization of XGBoost and IB3DM, and using eXplainable Artificial Intelligence (XAI) techniques. To combine the XGBoost and IB3DM, a fault-signature metric is proposed that helps reducing false alarms and also trigger an explanation on detecting incipient faults. To provide explainability, an eXplainable Artificial Intelligence (XAI) application is developed. It uses the local interpretable model-agnostic explanations (LIME) framework and provides explanations on classifier outputs for data instances. These explanations help field engineers/technicians for performing troubleshooting and maintenance operations. The proposed XFDDS is illustrated using experiments on different PV technologies and our results demonstrate the perceived benefits. △ Less","19 November, 2020",https://arxiv.org/pdf/2011.09843
A Cognitive Approach based on the Actionable Knowledge Graph for supporting Maintenance Operations,Giuseppe Fenza;Mariacristina Gallo;Vincenzo Loia;Domenico Marino;Francesco Orciuoli,"In the era of Industry 4.0, cognitive computing and its enabling technologies (Artificial Intelligence, Machine Learning, etc.) allow to define systems able to support maintenance by providing relevant information, at the right time, retrieved from structured companies' databases, and unstructured documents, like technical manuals, intervention reports, and so on. Moreover, contextual information plays a crucial role in tailoring the support both during the planning and the execution of interventions. Contextual information can be detected with the help of sensors, wearable devices, indoor and outdoor positioning systems, and object recognition capabilities (using fixed or wearable cameras), all of which can collect historical data for further analysis. In this work, we propose a cognitive system that learns from past interventions to generate contextual recommendations for improving maintenance practices in terms of time, budget, and scope. The system uses formal conceptual models, incremental learning, and ranking algorithms to accomplish these objectives. △ Less","18 November, 2020",https://arxiv.org/pdf/2011.09554
Explainable AI for System Failures: Generating Explanations that Improve Human Assistance in Fault Recovery,Devleena Das;Siddhartha Banerjee;Sonia Chernova,"With the growing capabilities of intelligent systems, the integration of artificial intelligence (AI) and robots in everyday life is increasing. However, when interacting in such complex human environments, the failure of intelligent systems, such as robots, can be inevitable, requiring recovery assistance from users. In this work, we develop automated, natural language explanations for failures encountered during an AI agents' plan execution. These explanations are developed with a focus of helping non-expert users understand different point of failures to better provide recovery assistance. Specifically, we introduce a context-based information type for explanations that can both help non-expert users understand the underlying cause of a system failure, and select proper failure recoveries. Additionally, we extend an existing sequence-to-sequence methodology to automatically generate our context-based explanations. By doing so, we are able develop a model that can generalize context-based explanations over both different failure types and failure scenarios. △ Less","19 November, 2020",https://arxiv.org/pdf/2011.09407
Using Unity to Help Solve Intelligence,Tom Ward;Andrew Bolt;Nik Hemmings;Simon Carter;Manuel Sanchez;Ricardo Barreira;Seb Noury;Keith Anderson;Jay Lemmon;Jonathan Coe;Piotr Trochim;Tom Handley;Adrian Bolton,"In the pursuit of artificial general intelligence, our most significant measurement of progress is an agent's ability to achieve goals in a wide range of environments. Existing platforms for constructing such environments are typically constrained by the technologies they are founded on, and are therefore only able to provide a subset of scenarios necessary to evaluate progress. To overcome these shortcomings, we present our use of Unity, a widely recognized and comprehensive game engine, to create more diverse, complex, virtual simulations. We describe the concepts and components developed to simplify the authoring of these environments, intended for use predominantly in the field of reinforcement learning. We also introduce a practical approach to packaging and re-distributing environments in a way that attempts to improve the robustness and reproducibility of experiment results. To illustrate the versatility of our use of Unity compared to other solutions, we highlight environments already created using our approach from published papers. We hope that others can draw inspiration from how we adapted Unity to our needs, and anticipate increasingly varied and complex environments to emerge from our approach as familiarity grows. △ Less","18 November, 2020",https://arxiv.org/pdf/2011.09294
A Transfer Learning Based Active Learning Framework for Brain Tumor Classification,Ruqian Hao;Khashayar Namdar;Lin Liu;Farzad Khalvati,"Brain tumor is one of the leading causes of cancer-related death globally among children and adults. Precise classification of brain tumor grade (low-grade and high-grade glioma) at early stage plays a key role in successful prognosis and treatment planning. With recent advances in deep learning, Artificial Intelligence-enabled brain tumor grading systems can assist radiologists in the interpretation of medical images within seconds. The performance of deep learning techniques is, however, highly depended on the size of the annotated dataset. It is extremely challenging to label a large quantity of medical images given the complexity and volume of medical data. In this work, we propose a novel transfer learning based active learning framework to reduce the annotation cost while maintaining stability and robustness of the model performance for brain tumor classification. We employed a 2D slice-based approach to train and finetune our model on the Magnetic Resonance Imaging (MRI) training dataset of 203 patients and a validation dataset of 66 patients which was used as the baseline. With our proposed method, the model achieved Area Under Receiver Operating Characteristic (ROC) Curve (AUC) of 82.89% on a separate test dataset of 66 patients, which was 2.92% higher than the baseline AUC while saving at least 40% of labeling cost. In order to further examine the robustness of our method, we created a balanced dataset, which underwent the same procedure. The model achieved AUC of 82% compared with AUC of 78.48% for the baseline, which reassures the robustness and stability of our proposed transfer learning augmented with active learning framework while significantly reducing the size of training data. △ Less","16 November, 2020",https://arxiv.org/pdf/2011.09265
"Game Plan: What AI can do for Football, and What Football can do for AI",Karl Tuyls;Shayegan Omidshafiei;Paul Muller;Zhe Wang;Jerome Connor;Daniel Hennes;Ian Graham;William Spearman;Tim Waskett;Dafydd Steele;Pauline Luc;Adria Recasens;Alexandre Galashov;Gregory Thornton;Romuald Elie;Pablo Sprechmann;Pol Moreno;Kris Cao;Marta Garnelo;Praneet Dutta;Michal Valko;Nicolas Heess;Alex Bridgland;Julien Perolat;Bart De Vylder,"The rapid progress in artificial intelligence (AI) and machine learning has opened unprecedented analytics possibilities in various team and individual sports, including baseball, basketball, and tennis. More recently, AI techniques have been applied to football, due to a huge increase in data collection by professional teams, increased computational power, and advances in machine learning, with the goal of better addressing new scientific challenges involved in the analysis of both individual players' and coordinated teams' behaviors. The research challenges associated with predictive and prescriptive football analytics require new developments and progress at the intersection of statistical learning, game theory, and computer vision. In this paper, we provide an overarching perspective highlighting how the combination of these fields, in particular, forms a unique microcosm for AI research, while offering mutual benefits for professional teams, spectators, and broadcasters in the years to come. We illustrate that this duality makes football analytics a game changer of tremendous value, in terms of not only changing the game of football itself, but also in terms of what this domain can mean for the field of AI. We review the state-of-the-art and exemplify the types of analysis enabled by combining the aforementioned fields, including illustrative examples of counterfactual analysis using predictive models, and the combination of game-theoretic analysis of penalty kicks with statistical learning of player attributes. We conclude by highlighting envisioned downstream impacts, including possibilities for extensions to other sports (real and virtual). △ Less","18 November, 2020",https://arxiv.org/pdf/2011.09192
Exploring Energy-Accuracy Tradeoffs in AI Hardware,Cory Merkel,"Artificial intelligence (AI) is playing an increasingly significant role in our everyday lives. This trend is expected to continue, especially with recent pushes to move more AI to the edge. However, one of the biggest challenges associated with AI on edge devices (mobile phones, unmanned vehicles, sensors, etc.) is their associated size, weight, and power constraints. In this work, we consider the scenario where an AI system may need to operate at less-than-maximum accuracy in order to meet application-dependent energy requirements. We propose a simple function that divides the cost of using an AI system into the cost of the decision making process and the cost of decision execution. For simple binary decision problems with convolutional neural networks, it is shown that minimizing the cost corresponds to using fewer than the maximum number of resources (e.g. convolutional neural network layers and filters). Finally, it is shown that the cost associated with energy can be significantly reduced by leveraging high-confidence predictions made in lower-level layers of the network. △ Less","17 November, 2020",https://arxiv.org/pdf/2011.08779
Towards evaluating and eliciting high-quality documentation for intelligent systems,David Piorkowski;Daniel González;John Richards;Stephanie Houde,"A vital component of trust and transparency in intelligent systems built on machine learning and artificial intelligence is the development of clear, understandable documentation. However, such systems are notorious for their complexity and opaqueness making quality documentation a non-trivial task. Furthermore, little is known about what makes such documentation ""good."" In this paper, we propose and evaluate a set of quality dimensions to identify in what ways this type of documentation falls short. Then, using those dimensions, we evaluate three different approaches for eliciting intelligent system documentation. We show how the dimensions identify shortcomings in such documentation and posit how such dimensions can be use to further enable users to provide documentation that is suitable to a given persona or use case. △ Less","17 November, 2020",https://arxiv.org/pdf/2011.08774
Flame Stability Analysis of Flame Spray Pyrolysis by Artificial Intelligence,Jessica Pan;Joseph A. Libera;Noah H. Paulson;Marius Stan,"Flame spray pyrolysis (FSP) is a process used to synthesize nanoparticles through the combustion of an atomized precursor solution; this process has applications in catalysts, battery materials, and pigments. Current limitations revolve around understanding how to consistently achieve a stable flame and the reliable production of nanoparticles. Machine learning and artificial intelligence algorithms that detect unstable flame conditions in real time may be a means of streamlining the synthesis process and improving FSP efficiency. In this study, the FSP flame stability is first quantified by analyzing the brightness of the flame's anchor point. This analysis is then used to label data for both unsupervised and supervised machine learning approaches. The unsupervised learning approach allows for autonomous labelling and classification of new data by representing data in a reduced dimensional space and identifying combinations of features that most effectively cluster it. The supervised learning approach, on the other hand, requires human labeling of training and test data, but is able to classify multiple objects of interest (such as the burner and pilot flames) within the video feed. The accuracy of each of these techniques is compared against the evaluations of human experts. Both the unsupervised and supervised approaches can track and classify FSP flame conditions in real time to alert users of unstable flame conditions. This research has the potential to autonomously track and manage flame spray pyrolysis as well as other flame technologies by monitoring and classifying the flame stability. △ Less","22 October, 2020",https://arxiv.org/pdf/2011.08673
Occams Razor for Big Data? On Detecting Quality in Large Unstructured Datasets,Birgitta Dresp-Langley;Ole Kristian Ekseth;Jan Fesl;Seiichi Gohshi;Marc Kurz;Hans-Werner Sehring,"Detecting quality in large unstructured datasets requires capacities far beyond the limits of human perception and communicability and, as a result, there is an emerging trend towards increasingly complex analytic solutions in data science to cope with this problem. This new trend towards analytic complexity represents a severe challenge for the principle of parsimony or Occams Razor in science. This review article combines insight from various domains such as physics, computational science, data engineering, and cognitive science to review the specific properties of big data. Problems for detecting data quality without losing the principle of parsimony are then highlighted on the basis of specific examples. Computational building block approaches for data clustering can help to deal with large unstructured datasets in minimized computation time, and meaning can be extracted rapidly from large sets of unstructured image or video data parsimoniously through relatively simple unsupervised machine learning algorithms. Why we still massively lack in expertise for exploiting big data wisely to extract relevant information for specific tasks, recognize patterns, generate new information, or store and further process large amounts of sensor data is then reviewed; examples illustrating why we need subjective views and pragmatic methods to analyze big data contents are brought forward. The review concludes on how cultural differences between East and West are likely to affect the course of big data analytics, and the development of increasingly autonomous artificial intelligence aimed at coping with the big data deluge in the near future. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.08663
"Empowering Things with Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things",Jing Zhang;Dacheng Tao,"In the Internet of Things (IoT) era, billions of sensors and devices collect and process data from the environment, transmit them to cloud centers, and receive feedback via the internet for connectivity and perception. However, transmitting massive amounts of heterogeneous data, perceiving complex environments from these data, and then making smart decisions in a timely manner are difficult. Artificial intelligence (AI), especially deep learning, is now a proven success in various areas including computer vision, speech recognition, and natural language processing. AI introduced into the IoT heralds the era of artificial intelligence of things (AIoT). This paper presents a comprehensive survey on AIoT to show how AI can empower the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly present the AIoT architecture in the context of cloud computing, fog computing, and edge computing. Then, we present progress in AI research for IoT from four perspectives: perceiving, learning, reasoning, and behaving. Next, we summarize some promising applications of AIoT that are likely to profoundly reshape our world. Finally, we highlight the challenges facing AIoT and some potential research opportunities. △ Less","17 November, 2020",https://arxiv.org/pdf/2011.08612
Explainable Artificial Intelligence Recommendation System by Leveraging the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype Development,Nariman Ammar;Arash Shaban-Nejad,"The study of adverse childhood experiences and their consequences has emerged over the past 20 years. In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve surveillance of adverse childhood experiences. We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners ability to provide explanations for the decisions they make. △ Less","6 November, 2020",https://arxiv.org/pdf/2011.08090
Towards Map-Based Validation of Semantic Segmentation Masks,Laura von Rueden;Tim Wirtz;Fabian Hueger;Jan David Schneider;Christian Bauckhage,"Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness. We propose to validate machine learning models for self-driving vehicles not only with given ground truth labels, but also with additional a-priori knowledge. In particular, we suggest to validate the drivable area in semantic segmentation masks using given street map data. We present first results, which indicate that prediction errors can be uncovered by map-based validation. △ Less","26 November, 2020",https://arxiv.org/pdf/2011.08008
"Good proctor or ""Big Brother""? AI Ethics and Online Exam Supervision Technologies",Simon Coghlan;Tim Miller;Jeannie Paterson,"This article philosophically analyzes online exam supervision technologies, which have been thrust into the public spotlight due to campus lockdowns during the COVID-19 pandemic and the growing demand for online courses. Online exam proctoring technologies purport to provide effective oversight of students sitting online exams, using artificial intelligence (AI) systems and human invigilators to supplement and review those systems. Such technologies have alarmed some students who see them as `Big Brother-like', yet some universities defend their judicious use. Critical ethical appraisal of online proctoring technologies is overdue. This article philosophically analyzes these technologies, focusing on the ethical concepts of academic integrity, fairness, non-maleficence, transparency, privacy, respect for autonomy, liberty, and trust. Most of these concepts are prominent in the new field of AI ethics and all are relevant to the education context. The essay provides ethical considerations that educational institutions will need to carefully review before electing to deploy and govern specific online proctoring technologies. △ Less","15 November, 2020",https://arxiv.org/pdf/2011.07647
FAIR: Fair Adversarial Instance Re-weighting,Andrija Petrović;Mladen Nikolić;Sandro Radovanović;Boris Delibašić;Miloš Jovanović,"With growing awareness of societal impact of artificial intelligence, fairness has become an important aspect of machine learning algorithms. The issue is that human biases towards certain groups of population, defined by sensitive features like race and gender, are introduced to the training data through data collection and labeling. Two important directions of fairness ensuring research have focused on (i) instance weighting in order to decrease the impact of more biased instances and (ii) adversarial training in order to construct data representations informative of the target variable, but uninformative of the sensitive attributes. In this paper we propose a Fair Adversarial Instance Re-weighting (FAIR) method, which uses adversarial training to learn instance weighting function that ensures fair predictions. Merging the two paradigms, it inherits desirable properties from both -- interpretability of reweighting and end-to-end trainability of adversarial training. We propose four different variants of the method and, among other things, demonstrate how the method can be cast in a fully probabilistic framework. Additionally, theoretical analysis of FAIR models' properties have been studied extensively. We compare FAIR models to 7 other related and state-of-the-art models and demonstrate that FAIR is able to achieve a better trade-off between accuracy and unfairness. To the best of our knowledge, this is the first model that merges reweighting and adversarial approaches by means of a weighting function that can provide interpretable information about fairness of individual instances. △ Less","15 November, 2020",https://arxiv.org/pdf/2011.07495
Molecular Mechanics-Driven Graph Neural Network with Multiplex Graph for Molecular Structures,Shuo Zhang;Yang Liu;Lei Xie,"The prediction of physicochemical properties from molecular structures is a crucial task for artificial intelligence aided molecular design. A growing number of Graph Neural Networks (GNNs) have been proposed to address this challenge. These models improve their expressive power by incorporating auxiliary information in molecules while inevitably increase their computational complexity. In this work, we aim to design a GNN which is both powerful and efficient for molecule structures. To achieve such goal, we propose a molecular mechanics-driven approach by first representing each molecule as a two-layer multiplex graph, where one layer contains only local connections that mainly capture the covalent interactions and another layer contains global connections that can simulate non-covalent interactions. Then for each layer, a corresponding message passing module is proposed to balance the trade-off of expression power and computational complexity. Based on these two modules, we build Multiplex Molecular Graph Neural Network (MXMNet). When validated by the QM9 dataset for small molecules and PDBBind dataset for large protein-ligand complexes, MXMNet achieves superior results to the existing state-of-the-art models under restricted resources. △ Less","15 November, 2020",https://arxiv.org/pdf/2011.07457
On the Benefits of Early Fusion in Multimodal Representation Learning,George Barnum;Sabera Talukder;Yisong Yue,"Intelligently reasoning about the world often requires integrating data from multiple modalities, as any individual modality may contain unreliable or incomplete information. Prior work in multimodal learning fuses input modalities only after significant independent processing. On the other hand, the brain performs multimodal processing almost immediately. This divide between conventional multimodal learning and neuroscience suggests that a detailed study of early multimodal fusion could improve artificial multimodal representations. To facilitate the study of early multimodal fusion, we create a convolutional LSTM network architecture that simultaneously processes both audio and visual inputs, and allows us to select the layer at which audio and visual information combines. Our results demonstrate that immediate fusion of audio and visual inputs in the initial C-LSTM layer results in higher performing networks that are more robust to the addition of white noise in both audio and visual inputs. △ Less","13 November, 2020",https://arxiv.org/pdf/2011.07191
Qualitative Investigation in Explainable Artificial Intelligence: A Bit More Insight from Social Science,Adam J. Johs;Denise E. Agosto;Rosina O. Weber,"We present a focused analysis of user studies in explainable artificial intelligence (XAI) entailing qualitative investigation. We draw on social science corpora to suggest ways for improving the rigor of studies where XAI researchers use observations, interviews, focus groups, and/or questionnaires to capture qualitative data. We contextualize the presentation of the XAI papers included in our analysis according to the components of rigor described in the qualitative research literature: 1) underlying theories or frameworks, 2) methodological approaches, 3) data collection methods, and 4) data analysis processes. The results of our analysis support calls from others in the XAI community advocating for collaboration with experts from social disciplines to bolster rigor and effectiveness in user studies. △ Less","18 December, 2020",https://arxiv.org/pdf/2011.07130
Continual Learning with Deep Artificial Neurons,Blake Camp;Jaya Krishna Mandivarapu;Rolando Estrada,"Neurons in real brains are enormously complex computational units. Among other things, they're responsible for transforming inbound electro-chemical vectors into outbound action potentials, updating the strengths of intermediate synapses, regulating their own internal states, and modulating the behavior of other nearby neurons. One could argue that these cells are the only things exhibiting any semblance of real intelligence. It is odd, therefore, that the machine learning community has, for so long, relied upon the assumption that this complexity can be reduced to a simple sum and fire operation. We ask, might there be some benefit to substantially increasing the computational power of individual neurons in artificial systems? To answer this question, we introduce Deep Artificial Neurons (DANs), which are themselves realized as deep neural networks. Conceptually, we embed DANs inside each node of a traditional neural network, and we connect these neurons at multiple synaptic sites, thereby vectorizing the connections between pairs of cells. We demonstrate that it is possible to meta-learn a single parameter vector, which we dub a neuronal phenotype, shared by all DANs in the network, which facilitates a meta-objective during deployment. Here, we isolate continual learning as our meta-objective, and we show that a suitable neuronal phenotype can endow a single network with an innate ability to update its synapses with minimal forgetting, using standard backpropagation, without experience replay, nor separate wake/sleep phases. We demonstrate this ability on sequential non-linear regression tasks. △ Less","13 November, 2020",https://arxiv.org/pdf/2011.07035
DeepMind Lab2D,Charles Beattie;Thomas Köppe;Edgar A. Duéñez-Guzmán;Joel Z. Leibo,"We present DeepMind Lab2D, a scalable environment simulator for artificial intelligence research that facilitates researcher-led experimentation with environment design. DeepMind Lab2D was built with the specific needs of multi-agent deep reinforcement learning researchers in mind, but it may also be useful beyond that particular subfield. △ Less","12 December, 2020",https://arxiv.org/pdf/2011.07027
Critic PI2: Master Continuous Planning via Policy Improvement with Path Integrals and Deep Actor-Critic Reinforcement Learning,Jiajun Fan;He Ba;Xian Guo;Jianye Hao,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods from AlphaGo to Muzero have enjoyed huge success in discrete domains, such as chess and Go. Unfortunately, in real-world applications like robot control and inverted pendulum, whose action space is normally continuous, those tree-based planning techniques will be struggling. To address those limitations, in this paper, we present a novel model-based reinforcement learning frameworks called Critic PI2, which combines the benefits from trajectory optimization, deep actor-critic learning, and model-based reinforcement learning. Our method is evaluated for inverted pendulum models with applicability to many continuous control systems. Extensive experiments demonstrate that Critic PI2 achieved a new state of the art in a range of challenging continuous domains. Furthermore, we show that planning with a critic significantly increases the sample efficiency and real-time performance. Our work opens a new direction toward learning the components of a model-based planning system and how to use them. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.06752
Encoded Value-at-Risk: A Predictive Machine for Financial Risk Management,Hamidreza Arian;Mehrdad Moghimi;Ehsan Tabatabaei;Shiva Zamani,"Measuring risk is at the center of modern financial risk management. As the world economy is becoming more complex and standard modeling assumptions are violated, the advanced artificial intelligence solutions may provide the right tools to analyze the global market. In this paper, we provide a novel approach for measuring market risk called Encoded Value-at-Risk (Encoded VaR), which is based on a type of artificial neural network, called Variational Auto-encoders (VAEs). Encoded VaR is a generative model which can be used to reproduce market scenarios from a range of historical cross-sectional stock returns, while increasing the signal-to-noise ratio present in the financial data, and learning the dependency structure of the market without any assumptions about the joint distribution of stock returns. We compare Encoded VaR out-of-sample results with eleven other methods and show that it is competitive to many other well-known VaR algorithms presented in the literature. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.06742
World Trade Center responders in their own words: Predicting PTSD symptom trajectories with AI-based language analyses of interviews,Youngseo Son;Sean A. P. Clouston;Roman Kotov;Johannes C. Eichstaedt;Evelyn J. Bromet;Benjamin J. Luft;H Andrew Schwartz,"Background: Oral histories from 9/11 responders to the World Trade Center (WTC) attacks provide rich narratives about distress and resilience. Artificial Intelligence (AI) models promise to detect psychopathology in natural language, but they have been evaluated primarily in non-clinical settings using social media. This study sought to test the ability of AI-based language assessments to predict PTSD symptom trajectories among responders. Methods: Participants were 124 responders whose health was monitored at the Stony Brook WTC Health and Wellness Program who completed oral history interviews about their initial WTC experiences. PTSD symptom severity was measured longitudinally using the PTSD Checklist (PCL) for up to 7 years post-interview. AI-based indicators were computed for depression, anxiety, neuroticism, and extraversion along with dictionary-based measures of linguistic and interpersonal style. Linear regression and multilevel models estimated associations of AI indicators with concurrent and subsequent PTSD symptom severity (significance adjusted by false discovery rate). Results: Cross-sectionally, greater depressive language (beta=0.32; p=0.043) and first-person singular usage (beta=0.31; p=0.044) were associated with increased symptom severity. Longitudinally, anxious language predicted future worsening in PCL scores (beta=0.31; p=0.031), whereas first-person plural usage (beta=-0.37; p=0.007) and longer words usage (beta=-0.36; p=0.007) predicted improvement. Conclusions: This is the first study to demonstrate the value of AI in understanding PTSD in a vulnerable population. Future studies should extend this application to other trauma exposures and to other demographic groups, especially under-represented minorities. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.06457
A deep Q-Learning based Path Planning and Navigation System for Firefighting Environments,Manish Bhattarai;Manel Martinez-Ramon,"Live fire creates a dynamic, rapidly changing environment that presents a worthy challenge for deep learning and artificial intelligence methodologies to assist firefighters with scene comprehension in maintaining their situational awareness, tracking and relay of important features necessary for key decisions as they tackle these catastrophic events. We propose a deep Q-learning based agent who is immune to stress induced disorientation and anxiety and thus able to make clear decisions for navigation based on the observed and stored facts in live fire environments. As a proof of concept, we imitate structural fire in a gaming engine called Unreal Engine which enables the interaction of the agent with the environment. The agent is trained with a deep Q-learning algorithm based on a set of rewards and penalties as per its actions on the environment. We exploit experience replay to accelerate the learning process and augment the learning of the agent with human-derived experiences. The agent trained under this deep Q-learning approach outperforms agents trained through alternative path planning systems and demonstrates this methodology as a promising foundation on which to build a path planning navigation assistant capable of safely guiding fire fighters through live fire environments. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.06450
Generalized Constraints as A New Mathematical Problem in Artificial Intelligence: A Review and Perspective,Bao-Gang Hu;Han-Bing Qu,"In this comprehensive review, we describe a new mathematical problem in artificial intelligence (AI) from a mathematical modeling perspective, following the philosophy stated by Rudolf E. Kalman that ""Once you get the physics right, the rest is mathematics"". The new problem is called ""Generalized Constraints (GCs)"", and we adopt GCs as a general term to describe any type of prior information in modelings. To understand better about GCs to be a general problem, we compare them with the conventional constraints (CCs) and list their extra challenges over CCs. In the construction of AI machines, we basically encounter more often GCs for modeling, rather than CCs with well-defined forms. Furthermore, we discuss the ultimate goals of AI and redefine transparent, interpretable, and explainable AI in terms of comprehension levels about machines. We review the studies in relation to the GC problems although most of them do not take the notion of GCs. We demonstrate that if AI machines are simplified by a coupling with both knowledge-driven submodel and data-driven submodel, GCs will play a critical role in a knowledge-driven submodel as well as in the coupling form between the two submodels. Examples are given to show that the studies in view of a generalized constraint problem will help us perceive and explore novel subjects in AI, or even in mathematics, such as generalized constraint learning (GCL). △ Less","11 November, 2020",https://arxiv.org/pdf/2011.06156
Statistical learning for change point and anomaly detection in graphs,Anna Malinovskaya;Philipp Otto;Torben Peters,"Complex systems which can be represented in the form of static and dynamic graphs arise in different fields, e.g. communication, engineering and industry. One of the interesting problems in analysing dynamic network structures is to monitor changes in their development. Statistical learning, which encompasses both methods based on artificial intelligence and traditional statistics, can be used to progress in this research area. However, the majority of approaches apply only one or the other framework. In this paper, we discuss the possibility of bringing together both disciplines in order to create enhanced network monitoring procedures focussing on the example of combining statistical process control and deep learning algorithms. Together with the presentation of change point and anomaly detection in network data, we propose to monitor the response times of ambulance services, applying jointly the control chart for quantile function values and a graph convolutional network. △ Less","10 November, 2020",https://arxiv.org/pdf/2011.06080
Domain Wall Leaky Integrate-and-Fire Neurons with Shape-Based Configurable Activation Functions,Wesley H. Brigner;Naimul Hassan;Xuan Hu;Christopher H. Bennett;Felipe Garcia-Sanchez;Can Cui;Alvaro Velasquez;Matthew J. Marinella;Jean Anne C. Incorvia;Joseph S. Friedman,"Complementary metal oxide semiconductor (CMOS) devices display volatile characteristics, and are not well suited for analog applications such as neuromorphic computing. Spintronic devices, on the other hand, exhibit both non-volatile and analog features, which are well-suited to neuromorphic computing. Consequently, these novel devices are at the forefront of beyond-CMOS artificial intelligence applications. However, a large quantity of these artificial neuromorphic devices still require the use of CMOS, which decreases the efficiency of the system. To resolve this, we have previously proposed a number of artificial neurons and synapses that do not require CMOS for operation. Although these devices are a significant improvement over previous renditions, their ability to enable neural network learning and recognition is limited by their intrinsic activation functions. This work proposes modifications to these spintronic neurons that enable configuration of the activation functions through control of the shape of a magnetic domain wall track. Linear and sigmoidal activation functions are demonstrated in this work, which can be extended through a similar approach to enable a wide variety of activation functions. △ Less","11 November, 2020",https://arxiv.org/pdf/2011.06075
AIRSENSE-TO-ACT: A Concept Paper for COVID-19 Countermeasures based on Artificial Intelligence algorithms and multi-sources Data Processing,A. Sebastianelli;F. Mauro;G. Di Cosmo;F. Passarini;M. Carminati;S. L. Ullo,"Aim of this paper is the description of a new tool to support institutions in the implementation of targeted countermeasures, based on quantitative and multi-scale elements, for the fight and prevention of emergencies, such as the current COVID-19 pandemic. The tool is a centralized system (web application), single multi-user platform, which relies on Artificial Intelligence (AI) algorithms for the processing of heterogeneous data, and which can produce an output level of risk. The model includes a specific neural network which will be first trained to learn the correlation between selected inputs, related to the case of interest: environmental variables (chemical-physical, such as meteorological), human activity (such as traffic and crowding), level of pollution (in particular the concentration of particulate matter), and epidemiological variables related to the evolution of the contagion. The tool realized in the first phase of the project will serve later both as a decision support system (DSS) with predictive capacity, when fed by the actual measured data, and as a simulation bench performing the tuning of certain input values, to identify which of them lead to a decrease in the degree of risk. In this way, the authors aim to design different scenarios to compare different restrictive strategies and the actual expected benefits, to adopt measures sized to the actual need, and adapted to the specific areas of analysis, useful to safeguard human health, but also the economic and social impact of the choices. △ Less","7 November, 2020",https://arxiv.org/pdf/2011.05808
Detecting Synthetic Phenomenology in a Contained Artificial General Intelligence,Jason M. Pittman;Ashlyn Hanks,"Human-like intelligence in a machine is a contentious subject. Whether mankind should or should not pursue the creation of artificial general intelligence is hotly debated. As well, researchers have aligned in opposing factions according to whether mankind can create it. For our purposes, we assume mankind can and will do so. Thus, it becomes necessary to contemplate how to do so in a safe and trusted manner -- enter the idea of boxing or containment. As part of such thinking, we wonder how a phenomenology might be detected given the operational constraints imposed by any potential containment system. Accordingly, this work provides an analysis of existing measures of phenomenology through qualia and extends those ideas into the context of a contained artificial general intelligence. △ Less","6 November, 2020",https://arxiv.org/pdf/2011.05807
Classification of COVID-19 in Chest CT Images using Convolutional Support Vector Machines,Umut Özkaya;Şaban Öztürk;Serkan Budak;Farid Melgani;Kemal Polat,"Purpose: Coronavirus 2019 (COVID-19), which emerged in Wuhan, China and affected the whole world, has cost the lives of thousands of people. Manual diagnosis is inefficient due to the rapid spread of this virus. For this reason, automatic COVID-19 detection studies are carried out with the support of artificial intelligence algorithms. Methods: In this study, a deep learning model that detects COVID-19 cases with high performance is presented. The proposed method is defined as Convolutional Support Vector Machine (CSVM) and can automatically classify Computed Tomography (CT) images. Unlike the pre-trained Convolutional Neural Networks (CNN) trained with the transfer learning method, the CSVM model is trained as a scratch. To evaluate the performance of the CSVM method, the dataset is divided into two parts as training (%75) and testing (%25). The CSVM model consists of blocks containing three different numbers of SVM kernels. Results: When the performance of pre-trained CNN networks and CSVM models is assessed, CSVM (7x7, 3x3, 1x1) model shows the highest performance with 94.03% ACC, 96.09% SEN, 92.01% SPE, 92.19% PRE, 94.10% F1-Score, 88.15% MCC and 88.07% Kappa metric values. Conclusion: The proposed method is more effective than other methods. It has proven in experiments performed to be an inspiration for combating COVID and for future studies. △ Less","11 November, 2020",https://arxiv.org/pdf/2011.05746
Deep Neural Mobile Networking,Chaoyun Zhang,"The next generation of mobile networks is set to become increasingly complex, as these struggle to accommodate tremendous data traffic demands generated by ever-more connected devices that have diverse performance requirements in terms of throughput, latency, and reliability. This makes monitoring and managing the multitude of network elements intractable with existing tools and impractical for traditional machine learning algorithms that rely on hand-crafted feature engineering. In this context, embedding machine intelligence into mobile networks becomes necessary, as this enables systematic mining of valuable information from mobile big data and automatically uncovering correlations that would otherwise have been too difficult to extract by human experts. In particular, deep learning based solutions can automatically extract features from raw data, without human expertise. The performance of artificial intelligence (AI) has achieved in other domains draws unprecedented interest from both academia and industry in employing deep learning approaches to address technical challenges in mobile networks. This thesis attacks important problems in the mobile networking area from various perspectives by harnessing recent advances in deep neural networks. △ Less","23 October, 2020",https://arxiv.org/pdf/2011.05267
Pristine annotations-based multi-modal trained artificial intelligence solution to triage chest X-ray for COVID-19,Tao Tan;Bipul Das;Ravi Soni;Mate Fejes;Sohan Ranjan;Daniel Attila Szabo;Vikram Melapudi;K S Shriram;Utkarsh Agrawal;Laszlo Rusko;Zita Herczeg;Barbara Darazs;Pal Tegzes;Lehel Ferenczi;Rakesh Mullick;Gopal Avinash,"The COVID-19 pandemic continues to spread and impact the well-being of the global population. The front-line modalities including computed tomography (CT) and X-ray play an important role for triaging COVID patients. Considering the limited access of resources (both hardware and trained personnel) and decontamination considerations, CT may not be ideal for triaging suspected subjects. Artificial intelligence (AI) assisted X-ray based applications for triaging and monitoring require experienced radiologists to identify COVID patients in a timely manner and to further delineate the disease region boundary are seen as a promising solution. Our proposed solution differs from existing solutions by industry and academic communities, and demonstrates a functional AI model to triage by inferencing using a single x-ray image, while the deep-learning model is trained using both X-ray and CT data. We report on how such a multi-modal training improves the solution compared to X-ray only training. The multi-modal solution increases the AUC (area under the receiver operating characteristic curve) from 0.89 to 0.93 and also positively impacts the Dice coefficient (0.59 to 0.62) for localizing the pathology. To the best our knowledge, it is the first X-ray solution by leveraging multi-modal information for the development. △ Less","10 November, 2020",https://arxiv.org/pdf/2011.05186
Batchwise Probabilistic Incremental Data Cleaning,Paulo H. Oliveira;Daniel S. Kaster;Caetano Traina-Jr.;Ihab F. Ilyas,"Lack of data and data quality issues are among the main bottlenecks that prevent further artificial intelligence adoption within many organizations, pushing data scientists to spend most of their time cleaning data before being able to answer analytical questions. Hence, there is a need for more effective and efficient data cleaning solutions, which, not surprisingly, is rife with theoretical and engineering problems. This report addresses the problem of performing holistic data cleaning incrementally, given a fixed rule set and an evolving categorical relational dataset acquired in sequential batches. To the best of our knowledge, our contributions compose the first incremental framework that cleans data (i) independently of user interventions, (ii) without requiring knowledge about the incoming dataset, such as the number of classes per attribute, and (iii) holistically, enabling multiple error types to be repaired simultaneously, and thus avoiding conflicting repairs. Extensive experiments show that our approach outperforms the competitors with respect to repair quality, execution time, and memory consumption. △ Less","9 November, 2020",https://arxiv.org/pdf/2011.04730
Sketch-Inspector: a Deep Mixture Model for High-Quality Sketch Generation of Cats,Yunkui Pang;Zhiqing Pan;Ruiyang Sun;Shuchong Wang,"With the involvement of artificial intelligence (AI), sketches can be automatically generated under certain topics. Even though breakthroughs have been made in previous studies in this area, a relatively high proportion of the generated figures are too abstract to recognize, which illustrates that AIs fail to learn the general pattern of the target object when drawing. This paper posits that supervising the process of stroke generation can lead to a more accurate sketch interpretation. Based on that, a sketch generating system with an assistant convolutional neural network (CNN) predictor to suggest the shape of the next stroke is presented in this paper. In addition, a CNN-based discriminator is introduced to judge the recognizability of the end product. Since the base-line model is ineffective at generating multi-class sketches, we restrict the model to produce one category. Because the image of a cat is easy to identify, we consider cat sketches selected from the QuickDraw data set. This paper compares the proposed model with the original Sketch-RNN on 75K human-drawn cat sketches. The result indicates that our model produces sketches with higher quality than human's sketches. △ Less","9 November, 2020",https://arxiv.org/pdf/2011.04280
Evolution of Artificial Intelligent Plane,Puneet Kumar,"With the growth of the internet, it is becoming hard to manage, configure and monitor networks. Recent trends to control and operate them is artificial intelligence based automation to minimize human intervention. Albeit this concept has been introduced since a decade with several different names, but the underlying goal remains the same, which is to make network intelligent enough to assemble, reassemble if configuration changes, and detect a problem on its own and fix it. As a result, in addition to Data Plane, Control Plane and Management Plane, a new plane called Artificial Intelligence (AI) Plane is introduced. Our main objective is to analyze all major AI plane techniques, frameworks and algorithms proposed in various types of networks. We propose a comprehensive and network independent framework to cover all aspects of AI plane, in particular we provide a systematically means of comparison. In conjunction to make AI plane understand simpler, this framework highlights relevant challenges and design considerations for future research. To the best of our knowledge this is the first survey report which represents a complete comparison of AI planes with their investigation issues in several types of networks. △ Less","8 November, 2020",https://arxiv.org/pdf/2011.04105
FairLens: Auditing Black-box Clinical Decision Support Systems,Cecilia Panigutti;Alan Perotti;Andrè Panisson;Paolo Bajardi;Dino Pedreschi,"The pervasive application of algorithmic decision-making is raising concerns on the risk of unintended bias in AI systems deployed in critical settings such as healthcare. The detection and mitigation of biased models is a very delicate task which should be tackled with care and involving domain experts in the loop. In this paper we introduce FairLens, a methodology for discovering and explaining biases. We show how our tool can be used to audit a fictional commercial black-box model acting as a clinical decision support system. In this scenario, the healthcare facility experts can use FairLens on their own historical data to discover the model's biases before incorporating it into the clinical decision flow. FairLens first stratifies the available patient data according to attributes such as age, ethnicity, gender and insurance; it then assesses the model performance on such subgroups of patients identifying those in need of expert evaluation. Finally, building on recent state-of-the-art XAI (eXplainable Artificial Intelligence) techniques, FairLens explains which elements in patients' clinical history drive the model error in the selected subgroup. Therefore, FairLens allows experts to investigate whether to trust the model and to spotlight group-specific biases that might constitute potential fairness issues. △ Less","8 November, 2020",https://arxiv.org/pdf/2011.04049
Software engineering for artificial intelligence and machine learning software: A systematic literature review,Elizamary Nascimento;Anh Nguyen-Duc;Ingrid Sundbø;Tayana Conte,"Artificial Intelligence (AI) or Machine Learning (ML) systems have been widely adopted as value propositions by companies in all industries in order to create or extend the services and products they offer. However, developing AI/ML systems has presented several engineering problems that are different from those that arise in, non-AI/ML software development. This study aims to investigate how software engineering (SE) has been applied in the development of AI/ML systems and identify challenges and practices that are applicable and determine whether they meet the needs of professionals. Also, we assessed whether these SE practices apply to different contexts, and in which areas they may be applicable. We conducted a systematic review of literature from 1990 to 2019 to (i) understand and summarize the current state of the art in this field and (ii) analyze its limitations and open challenges that will drive future research. Our results show these systems are developed on a lab context or a large company and followed a research-driven development process. The main challenges faced by professionals are in areas of testing, AI software quality, and data management. The contribution types of most of the proposed SE practices are guidelines, lessons learned, and tools. △ Less","7 November, 2020",https://arxiv.org/pdf/2011.03751
Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks: A Tutorial,Amal Feriani;Ekram Hossain,"Deep Reinforcement Learning (DRL) has recently witnessed significant advances that have led to multiple successes in solving sequential decision-making problems in various domains, particularly in wireless communications. The future sixth-generation (6G) networks are expected to provide scalable, low-latency, ultra-reliable services empowered by the application of data-driven Artificial Intelligence (AI). The key enabling technologies of future 6G networks, such as intelligent meta-surfaces, aerial networks, and AI at the edge, involve more than one agent which motivates the importance of multi-agent learning techniques. Furthermore, cooperation is central to establishing self-organizing, self-sustaining, and decentralized networks. In this context, this tutorial focuses on the role of DRL with an emphasis on deep Multi-Agent Reinforcement Learning (MARL) for AI-enabled 6G networks. The first part of this paper will present a clear overview of the mathematical frameworks for single-agent RL and MARL. The main idea of this work is to motivate the application of RL beyond the model-free perspective which was extensively adopted in recent years. Thus, we provide a selective description of RL algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight their potential applications in 6G wireless networks. Finally, we overview the state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC), Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and identify promising future research directions. We expect this tutorial to stimulate more research endeavors to build scalable and decentralized systems based on MARL. △ Less","6 November, 2020",https://arxiv.org/pdf/2011.03615
Explainable AI meets Healthcare: A Study on Heart Disease Dataset,Devam Dave;Het Naik;Smiti Singhal;Pankesh Patel,"With the increasing availability of structured and unstructured data and the swift progress of analytical techniques, Artificial Intelligence (AI) is bringing a revolution to the healthcare industry. With the increasingly indispensable role of AI in healthcare, there are growing concerns over the lack of transparency and explainability in addition to potential bias encountered by predictions of the model. This is where Explainable Artificial Intelligence (XAI) comes into the picture. XAI increases the trust placed in an AI system by medical practitioners as well as AI researchers, and thus, eventually, leads to an increasingly widespread deployment of AI in healthcare. In this paper, we present different interpretability techniques. The aim is to enlighten practitioners on the understandability and interpretability of explainable AI systems using a variety of techniques available which can be very advantageous in the health-care domain. Medical diagnosis model is responsible for human life and we need to be confident enough to treat a patient as instructed by a black-box model. Our paper contains examples based on the heart disease dataset and elucidates on how the explainability techniques should be preferred to create trustworthiness while using AI systems in healthcare. △ Less","6 November, 2020",https://arxiv.org/pdf/2011.03195
Artificial Intelligence and its impact on the Fourth Industrial Revolution: A Review,Gissel Velarde,"Artificial Intelligence may revolutionize everything during the so-called fourth industrial revolution, which carries several emerging technologies and could progress without precedents in human history due to its speed and scope. Government, academia, industry, and civil society show interest in understanding the multidimensional impact of the emerging industrial revolution; however, its development is hard to predict. Experts consider emerging technologies could bring tremendous benefits to humanity; at the same time, they could pose an existential risk. This paper reviews the development and trends in AI, as well as the benefits, risks, and strategies in the field. During the course of the emerging industrial revolution, the common good may be achieved in a collaborative environment of shared interests and the hardest work will be the implementation and monitoring of projects at a global scale. △ Less","5 November, 2020",https://arxiv.org/pdf/2011.03044
"Augmenting Organizational Decision-Making with Deep Learning Algorithms: Principles, Promises, and Challenges",Yash Raj Shrestha;Vaibhav Krishna;Georg von Krogh,"The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning (DL) algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.02834
An ontology-based chatbot for crises management: use case coronavirus,Khouloud Hwerbi,"Today is the era of intelligence in machines. With the advances in Artificial Intelligence, machines have started to impersonate different human traits, a chatbot is the next big thing in the domain of conversational services. A chatbot is a virtual person who is capable to carry out a natural conversation with people. They can include skills that enable them to converse with the humans in audio, visual, or textual formats. Artificial intelligence conversational entities, also called chatbots, conversational agents, or dialogue system, are an excellent example of such machines. Obtaining the right information at the right time and place is the key to effective disaster management. The term ""disaster management"" encompasses both natural and human-caused disasters. To assist citizens, our project is to create a COVID Assistant to provide the need of up to date information to be available 24 hours. With the growth in the World Wide Web, it is quite intelligible that users are interested in the swift and relatedly correct information for their hunt. A chatbot can be seen as a question-and-answer system in which experts provide knowledge to solicit users. This master thesis is dedicated to discuss COVID Assistant chatbot and explain each component in detail. The design of the proposed chatbot is introduced by its seven components: Ontology, Web Scraping module, DB, State Machine, keyword Extractor, Trained chatbot, and User Interface. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.02340
Personalized Multimorbidity Management for Patients with Type 2 Diabetes Using Reinforcement Learning of Electronic Health Records,Hua Zheng;Ilya O. Ryzhov;Wei Xie;Judy Zhong,"Comorbid chronic conditions are common among people with type 2 diabetes. We developed an Artificial Intelligence algorithm, based on Reinforcement Learning (RL), for personalized diabetes and multi-morbidity management with strong potential to improve health outcomes relative to current clinical practice. In this paper, we modeled glycemia, blood pressure and cardiovascular disease (CVD) risk as health outcomes using a retrospective cohort of 16,665 patients with type 2 diabetes from New York University Langone Health ambulatory care electronic health records in 2009 to 2017. We trained a RL prescription algorithm that recommends a treatment regimen optimizing patients' cumulative health outcomes using their individual characteristics and medical history at each encounter. The RL recommendations were evaluated on an independent subset of patients. The results demonstrate that the proposed personalized reinforcement learning prescriptive framework for type 2 diabetes yielded high concordance with clinicians' prescriptions and substantial improvements in glycemia, blood pressure, cardiovascular disease risk outcomes. △ Less","28 October, 2020",https://arxiv.org/pdf/2011.02287
A Comprehensive Study of Class Incremental Learning Algorithms for Visual Tasks,Eden Belouadah;Adrian Popescu;Ioannis Kanellos,"The ability of artificial agents to increment their capabilities when confronted with new data is an open challenge in artificial intelligence. The main challenge faced in such cases is catastrophic forgetting, i.e., the tendency of neural networks to underfit past data when new ones are ingested. A first group of approaches tackles forgetting by increasing deep model capacity to accommodate new knowledge. A second type of approaches fix the deep model size and introduce a mechanism whose objective is to ensure a good compromise between stability and plasticity of the model. While the first type of algorithms were compared thoroughly, this is not the case for methods which exploit a fixed size model. Here, we focus on the latter, place them in a common conceptual and experimental framework and propose the following contributions: (1) define six desirable properties of incremental learning algorithms and analyze them according to these properties, (2) introduce a unified formalization of the class-incremental learning problem, (3) propose a common evaluation framework which is more thorough than existing ones in terms of number of datasets, size of datasets, size of bounded memory and number of incremental states, (4) investigate the usefulness of herding for past exemplars selection, (5) provide experimental evidence that it is possible to obtain competitive performance without the use of knowledge distillation to tackle catastrophic forgetting and (6) facilitate reproducibility by integrating all tested methods in a common open-source repository. The main experimental finding is that none of the existing algorithms achieves the best results in all evaluated settings. Important differences arise notably if a bounded memory of past classes is allowed or not. △ Less","15 December, 2020",https://arxiv.org/pdf/2011.01844
Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models,Tom Heskes;Evi Sijben;Ioan Gabriel Bucur;Tom Claassen,"Shapley values underlie one of the most popular model-agnostic methods within explainable artificial intelligence. These values are designed to attribute the difference between a model's prediction and an average baseline to the different features used as input to the model. Being based on solid game-theoretic principles, Shapley values uniquely satisfy several desirable properties, which is why they are increasingly used to explain the predictions of possibly complex and highly non-linear machine learning models. Shapley values are well calibrated to a user's intuition when features are independent, but may lead to undesirable, counterintuitive explanations when the independence assumption is violated. In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl's do-calculus, we show how these 'causal' Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example. △ Less","3 November, 2020",https://arxiv.org/pdf/2011.01625
NSF Convergence Approach to Transition Basic Research into Practice,Shelby Smith;Chaitanya Baru,"The National Science Foundation Convergence Accelerator addresses national-scale societal challenges through use-inspired convergence research. Leveraging a convergence approach the Convergence Accelerator builds upon basic research and discovery to make timely investments to strengthen the Nations innovation ecosystem associated with several key R&D priority areas and practices to include the coronavirus disease 2019, harnessing the data revolution, the future of work, and quantum technology. Artificial Intelligence is a key underlying theme across all of these areas. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.01251
Boost Image Captioning with Knowledge Reasoning,Feicheng Huang;Zhixin Li;Haiyang Wei;Canlong Zhang;Huifang Ma,"Automatically generating a human-like description for a given image is a potential research in artificial intelligence, which has attracted a great of attention recently. Most of the existing attention methods explore the mapping relationships between words in sentence and regions in image, such unpredictable matching manner sometimes causes inharmonious alignments that may reduce the quality of generated captions. In this paper, we make our efforts to reason about more accurate and meaningful captions. We first propose word attention to improve the correctness of visual attention when generating sequential descriptions word-by-word. The special word attention emphasizes on word importance when focusing on different regions of the input image, and makes full use of the internal annotation knowledge to assist the calculation of visual attention. Then, in order to reveal those incomprehensible intentions that cannot be expressed straightforwardly by machines, we introduce a new strategy to inject external knowledge extracted from knowledge graph into the encoder-decoder framework to facilitate meaningful captioning. Finally, we validate our model on two freely available captioning benchmarks: Microsoft COCO dataset and Flickr30k dataset. The results demonstrate that our approach achieves state-of-the-art performance and outperforms many of the existing approaches. △ Less","2 November, 2020",https://arxiv.org/pdf/2011.00927
Multi-Modal Active Learning for Automatic Liver Fibrosis Diagnosis based on Ultrasound Shear Wave Elastography,Lufei Gao;Ruisong Zhou;Changfeng Dong;Cheng Feng;Zhen Li;Xiang Wan;Li Liu,"With the development of radiomics, noninvasive diagnosis like ultrasound (US) imaging plays a very important role in automatic liver fibrosis diagnosis (ALFD). Due to the noisy data, expensive annotations of US images, the application of Artificial Intelligence (AI) assisting approaches encounters a bottleneck. Besides, the use of mono-modal US data limits the further improve of the classification results. In this work, we innovatively propose a multi-modal fusion network with active learning (MMFN-AL) for ALFD to exploit the information of multiple modalities, eliminate the noisy data and reduce the annotation cost. Four image modalities including US and three types of shear wave elastography (SWEs) are exploited. A new dataset containing these modalities from 214 candidates is well-collected and pre-processed, with the labels obtained from the liver biopsy results. Experimental results show that our proposed method outperforms the state-of-the-art performance using less than 30% data, and by using only around 80% data, the proposed fusion network achieves high AUC 89.27% and accuracy 70.59%. △ Less","1 November, 2020",https://arxiv.org/pdf/2011.00694
Triage of Potential COVID-19 Patients from Chest X-ray Images using Hierarchical Convolutional Networks,Kapal Dev;Sunder Ali Khowaja;Ankur Singh Bist;Vaibhav Saini;Surbhi Bhatia,"The current COVID-19 pandemic has motivated the researchers to use artificial intelligence techniques for a potential alternative to reverse transcription-polymerase chain reaction (RT-PCR) due to the limited scale of testing. The chest X-ray (CXR) is one of the alternatives to achieve fast diagnosis but the unavailability of large-scale annotated data makes the clinical implementation of machine learning-based COVID detection difficult. Another issue is the usage of ImageNet pre-trained networks which does not extract reliable feature representations from medical images. In this paper, we propose the use of hierarchical convolutional network (HCN) architecture to naturally augment the data along with diversified features. The HCN uses the first convolution layer from COVIDNet followed by the convolutional layers from well-known pre-trained networks to extract the features. The use of the convolution layer from COVIDNet ensures the extraction of representations relevant to the CXR modality. We also propose the use of ECOC for encoding multiclass problems to binary classification for improving the recognition performance. Experimental results show that HCN architecture is capable of achieving better results in comparison to the existing studies. The proposed method can accurately triage potential COVID-19 patients through CXR images for sharing the testing load and increasing the testing capacity. △ Less","15 December, 2020",https://arxiv.org/pdf/2011.00618
Polymer Informatics: Current Status and Critical Next Steps,Lihua Chen;Ghanshyam Pilania;Rohit Batra;Tran Doan Huan;Chiho Kim;Christopher Kuenneth;Rampi Ramprasad,"Artificial intelligence (AI) based approaches are beginning to impact several domains of human life, science and technology. Polymer informatics is one such domain where AI and machine learning (ML) tools are being used in the efficient development, design and discovery of polymers. Surrogate models are trained on available polymer data for instant property prediction, allowing screening of promising polymer candidates with specific target property requirements. Questions regarding synthesizability, and potential (retro)synthesis steps to create a target polymer, are being explored using statistical means. Data-driven strategies to tackle unique challenges resulting from the extraordinary chemical and physical diversity of polymers at small and large scales are being explored. Other major hurdles for polymer informatics are the lack of widespread availability of curated and organized data, and approaches to create machine-readable representations that capture not just the structure of complex polymeric situations but also synthesis and processing conditions. Methods to solve inverse problems, wherein polymer recommendations are made using advanced AI algorithms that meet application targets, are being investigated. As various parts of the burgeoning polymer informatics ecosystem mature and become integrated, efficiency improvements, accelerated discoveries and increased productivity can result. Here, we review emergent components of this polymer informatics ecosystem and discuss imminent challenges and opportunities. △ Less","1 November, 2020",https://arxiv.org/pdf/2011.00508
Photonics for artificial intelligence and neuromorphic computing,Bhavin J. Shastri;Alexander N. Tait;Thomas Ferreira de Lima;Wolfram H. P. Pernice;Harish Bhaskaran;C. David Wright;Paul R. Prucnal,"Research in photonic computing has flourished due to the proliferation of optoelectronic components on photonic integration platforms. Photonic integrated circuits have enabled ultrafast artificial neural networks, providing a framework for a new class of information processing machines. Algorithms running on such hardware have the potential to address the growing demand for machine learning and artificial intelligence, in areas such as medical diagnosis, telecommunications, and high-performance and scientific computing. In parallel, the development of neuromorphic electronics has highlighted challenges in that domain, in particular, related to processor latency. Neuromorphic photonics offers sub-nanosecond latencies, providing a complementary opportunity to extend the domain of artificial intelligence. Here, we review recent advances in integrated photonic neuromorphic systems, discuss current and future challenges, and outline the advances in science and technology needed to meet those challenges. △ Less","12 November, 2020",https://arxiv.org/pdf/2011.00111
A Cross-lingual Natural Language Processing Framework for Infodemic Management,Ridam Pal;Rohan Pandey;Vaibhav Gautam;Kanav Bhagat;Tavpritesh Sethi,"The COVID-19 pandemic has put immense pressure on health systems which are further strained due to the misinformation surrounding it. Under such a situation, providing the right information at the right time is crucial. There is a growing demand for the management of information spread using Artificial Intelligence. Hence, we have exploited the potential of Natural Language Processing for identifying relevant information that needs to be disseminated amongst the masses. In this work, we present a novel Cross-lingual Natural Language Processing framework to provide relevant information by matching daily news with trusted guidelines from the World Health Organization. The proposed pipeline deploys various techniques of NLP such as summarizers, word embeddings, and similarity metrics to provide users with news articles along with a corresponding healthcare guideline. A total of 36 models were evaluated and a combination of LexRank based summarizer on Word2Vec embedding with Word Mover distance metric outperformed all other models. This novel open-source approach can be used as a template for proactive dissemination of relevant healthcare information in the midst of misinformation spread associated with epidemics. △ Less","30 October, 2020",https://arxiv.org/pdf/2010.16357
Ethical Decision Making During Automated Vehicle Crashes,Noah Goodall,"Automated vehicles have received much attention recently, particularly the DARPA Urban Challenge vehicles, Google's self-driving cars, and various others from auto manufacturers. These vehicles have the potential to significantly reduce crashes and improve roadway efficiency by automating the responsibilities of the driver. Still, automated vehicles are expected to crash occasionally, even when all sensors, vehicle control components, and algorithms function perfectly. If a human driver is unable to take control in time, a computer will be responsible for pre-crash behavior. Unlike other automated vehicles--such as aircraft, where every collision is catastrophic, and guided track systems, which can only avoid collisions in one dimension--automated roadway vehicles can predict various crash trajectory alternatives and select a path with the lowest damage or likelihood of collision. In some situations, the preferred path may be ambiguous. This study investigates automated vehicle crashing and concludes the following: (1) automated vehicles will almost certainly crash, (2) an automated vehicle's decisions preceding certain crashes will have a moral component, and (3) there is no obvious way to effectively encode complex human morals in software. A three-phase approach to developing ethical crashing algorithms is presented, consisting of a rational approach, an artificial intelligence approach, and a natural language requirement. The phases are theoretical and should be implemented as the technology becomes available. △ Less","30 October, 2020",https://arxiv.org/pdf/2010.16309
"""Thy algorithm shalt not bear false witness"": An Evaluation of Multiclass Debiasing Methods on Word Embeddings",Thalea Schlender;Gerasimos Spanakis,"With the vast development and employment of artificial intelligence applications, research into the fairness of these algorithms has been increased. Specifically, in the natural language processing domain, it has been shown that social biases persist in word embeddings and are thus in danger of amplifying these biases when used. As an example of social bias, religious biases are shown to persist in word embeddings and the need for its removal is highlighted. This paper investigates the state-of-the-art multiclass debiasing techniques: Hard debiasing, SoftWEAT debiasing and Conceptor debiasing. It evaluates their performance when removing religious bias on a common basis by quantifying bias removal via the Word Embedding Association Test (WEAT), Mean Average Cosine Similarity (MAC) and the Relative Negative Sentiment Bias (RNSB). By investigating the religious bias removal on three widely used word embeddings, namely: Word2Vec, GloVe, and ConceptNet, it is shown that the preferred method is ConceptorDebiasing. Specifically, this technique manages to decrease the measured religious bias on average by 82,42%, 96,78% and 54,76% for the three word embedding sets respectively. △ Less","4 November, 2020",https://arxiv.org/pdf/2010.16228
AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for Clinical Depression Diagnosis,Muhammad Muzammel;Hanan Salam;Yann Hoffmann;Mohamed Chetouani;Alice Othmani,"Depression is a common and serious mood disorder that negatively affects the patient's capacity of functioning normally in daily tasks. Speech is proven to be a vigorous tool in depression diagnosis. Research in psychiatry concentrated on performing fine-grained analysis on word-level speech components contributing to the manifestation of depression in speech and revealed significant variations at the phoneme-level in depressed speech. On the other hand, research in Machine Learning-based automatic recognition of depression from speech focused on the exploration of various acoustic features for the detection of depression and its severity level. Few have focused on incorporating phoneme-level speech components in automatic assessment systems. In this paper, we propose an Artificial Intelligence (AI) based application for clinical depression recognition and assessment from speech. We investigate the acoustic characteristics of phoneme units, specifically vowels and consonants for depression recognition via Deep Learning. We present and compare three spectrogram-based Deep Neural Network architectures, trained on phoneme consonant and vowel units and their fusion respectively. Our experiments show that the deep learned consonant-based acoustic characteristics lead to better recognition results than vowel-based ones. The fusion of vowel and consonant speech characteristics through a deep network significantly outperforms the single space networks as well as the state-of-art deep learning approaches on the DAIC-WOZ database. △ Less","4 November, 2020",https://arxiv.org/pdf/2010.16201
Formalizing IMO Problems and Solutions in Isabelle/HOL,Filip Marić;Sana Stojanović-{\Dj}urđević,"The International Mathematical Olympiad (IMO) is perhaps the most celebrated mental competition in the world and as such is among the greatest grand challenges for Artificial Intelligence (AI). The IMO Grand Challenge, recently formulated, requires to build an AI that can win a gold medal in the competition. We present some initial steps that could help to tackle this goal by creating a public repository of mechanically checked solutions of IMO Problems in the interactive theorem prover Isabelle/HOL. This repository is actively maintained by students of the Faculty of Mathematics, University of Belgrade, Serbia within the course ""Introduction to Interactive Theorem Proving"". △ Less","29 October, 2020",https://arxiv.org/pdf/2010.16015
Unsupervised One-shot Learning of Both Specific Instances and Generalised Classes with a Hippocampal Architecture,Gideon Kowadlo;Abdelrahman Ahmed;David Rawlinson,"Established experimental procedures for one-shot machine learning do not test the ability to learn or remember specific instances of classes, a key feature of animal intelligence. Distinguishing specific instances is necessary for many real-world tasks, such as remembering which cup belongs to you. Generalisation within classes conflicts with the ability to separate instances of classes, making it difficult to achieve both capabilities within a single architecture. We propose an extension to the standard Omniglot classification-generalisation framework that additionally tests the ability to distinguish specific instances after one exposure and introduces noise and occlusion corruption. Learning is defined as an ability to classify as well as recall training samples. Complementary Learning Systems (CLS) is a popular model of mammalian brain regions believed to play a crucial role in learning from a single exposure to a stimulus. We created an artificial neural network implementation of CLS and applied it to the extended Omniglot benchmark. Our unsupervised model demonstrates comparable performance to existing supervised ANNs on the Omniglot classification task (requiring generalisation), without the need for domain-specific inductive biases. On the extended Omniglot instance-recognition task, the same model also demonstrates significantly better performance than a baseline nearest-neighbour approach, given partial occlusion and noise. △ Less","29 October, 2020",https://arxiv.org/pdf/2010.15999
Enjeux éthiques de l'IA en santé : une humanisation du parcours de soin par l'intelligence artificielle ?,Fabrice Muhlenbach,"Considering the use of artificial intelligence for greater personalization of patient care and better management of human and material resources may seem like an opportunity not to be missed. In order to offer a better humanization of the care pathway, artificial intelligence is a tool that decision-makers in the hospital sector must appropriate by taking care of the new ethical issues and conflicts of values that this technology generates. Envisager le recours à l'intelligence artificielle pour une plus grande personnalisation de la prise en charge du patient et une meilleure gestion des ressources humaines et matérielles peut sembler une opportunité à ne pas manquer. Afin de proposer une meilleure humanisation du parcours de soin, l'intelligence artificielle est un outil que les décideurs du milieu hospitalier doivent s'approprier en veillant aux nouveaux enjeux éthiques et conflits de valeurs que cette technologie engendre. △ Less","23 October, 2020",https://arxiv.org/pdf/2010.15590
Panel: Economic Policy and Governance during Pandemics using AI,Feras A. Batarseh;Munisamy Gopinath,"The global food supply chain (starting at farms and ending with consumers) has been seriously disrupted by many outlier events such as trade wars, the China demand shock, natural disasters, and pandemics. Outlier events create uncertainty along the entire supply chain in addition to intervening policy responses to mitigate their adverse effects. Artificial Intelligence (AI) methods (i.e. machine/reinforcement/deep learning) provide an opportunity to better understand outcomes during outlier events by identifying regular, irregular and contextual components. Employing AI can provide guidance to decision making suppliers, farmers, processors, wholesalers, and retailers along the supply chain, and policy makers to facilitate welfare-improving outcomes. This panel discusses these issues. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.15585
The De-democratization of AI: Deep Learning and the Compute Divide in Artificial Intelligence Research,Nur Ahmed;Muntasir Wahed,"Increasingly, modern Artificial Intelligence (AI) research has become more computationally intensive. However, a growing concern is that due to unequal access to computing power, only certain firms and elite universities have advantages in modern AI research. Using a novel dataset of 171394 papers from 57 prestigious computer science conferences, we document that firms, in particular, large technology firms and elite universities have increased participation in major AI conferences since deep learning's unanticipated rise in 2012. The effect is concentrated among elite universities, which are ranked 1-50 in the QS World University Rankings. Further, we find two strategies through which firms increased their presence in AI research: first, they have increased firm-only publications; and second, firms are collaborating primarily with elite universities. Consequently, this increased presence of firms and elite universities in AI research has crowded out mid-tier (QS ranked 201-300) and lower-tier (QS ranked 301-500) universities. To provide causal evidence that deep learning's unanticipated rise resulted in this divergence, we leverage the generalized synthetic control method, a data-driven counterfactual estimator. Using machine learning based text analysis methods, we provide additional evidence that the divergence between these two groups - large firms and non-elite universities - is driven by access to computing power or compute, which we term as the ""compute divide"". This compute divide between large firms and non-elite universities increases concerns around bias and fairness within AI technology, and presents an obstacle towards ""democratizing"" AI. These results suggest that a lack of access to specialized equipment such as compute can de-democratize knowledge production. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.15581
Exploring the Nuances of Designing (with/for) Artificial Intelligence,Niya Stoimenova;Rebecca Price,"Solutions relying on artificial intelligence are devised to predict data patterns and answer questions that are clearly defined, involve an enumerable set of solutions, clear rules, and inherently binary decision mechanisms. Yet, as they become exponentially implemented in our daily activities, they begin to transcend these initial boundaries and to affect the larger sociotechnical system in which they are situated. In this arrangement, a solution is under pressure to surpass true or false criteria and move to an ethical evaluation of right and wrong. Neither algorithmic solutions, nor purely humanistic ones will be enough to fully mitigate undesirable outcomes in the narrow state of AI or its future incarnations. We must take a holistic view. In this paper we explore the construct of infrastructure as a means to simultaneously address algorithmic and societal issues when designing AI. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.15578
Investigating the Robustness of Artificial Intelligent Algorithms with Mixture Experiments,Jiayi Lian;Laura Freeman;Yili Hong;Xinwei Deng,"Artificial intelligent (AI) algorithms, such as deep learning and XGboost, are used in numerous applications including computer vision, autonomous driving, and medical diagnostics. The robustness of these AI algorithms is of great interest as inaccurate prediction could result in safety concerns and limit the adoption of AI systems. In this paper, we propose a framework based on design of experiments to systematically investigate the robustness of AI classification algorithms. A robust classification algorithm is expected to have high accuracy and low variability under different application scenarios. The robustness can be affected by a wide range of factors such as the imbalance of class labels in the training dataset, the chosen prediction algorithm, the chosen dataset of the application, and a change of distribution in the training and test datasets. To investigate the robustness of AI classification algorithms, we conduct a comprehensive set of mixture experiments to collect prediction performance results. Then statistical analyses are conducted to understand how various factors affect the robustness of AI classification algorithms. We summarize our findings and provide suggestions to practitioners in AI applications. △ Less","10 October, 2020",https://arxiv.org/pdf/2010.15551
A multi-agent model for growing spiking neural networks,Javier Lopez Randulfe;Leon Bonde Larsen,"Artificial Intelligence has looked into biological systems as a source of inspiration. Although there are many aspects of the brain yet to be discovered, neuroscience has found evidence that the connections between neurons continuously grow and reshape as a part of the learning process. This differs from the design of Artificial Neural Networks, that achieve learning by evolving the weights in the synapses between them and their topology stays unaltered through time. This project has explored rules for growing the connections between the neurons in Spiking Neural Networks as a learning mechanism. These rules have been implemented on a multi-agent system for creating simple logic functions, that establish a base for building up more complex systems and architectures. Results in a simulation environment showed that for a given set of parameters it is possible to reach topologies that reproduce the tested functions. This project also opens the door to the usage of techniques like genetic algorithms for obtaining the best suited values for the model parameters, and hence creating neural networks that can adapt to different functions. △ Less","21 September, 2020",https://arxiv.org/pdf/2010.15045
The Amazing Race TM: Robot Edition,Jared Sigurd Johansen;Thomas Victor Ilyevsky;Jeffrey Mark Siskind,"State-of-the-art natural-language-driven autonomous-navigation systems generally lack the ability to operate in real unknown environments without crutches, such as having a map of the environment in advance or requiring a strict syntactic structure for natural-language commands. Practical artificial-intelligent systems should not have to depend on such prior knowledge. To encourage effort towards this goal, we propose The Amazing Race TM: Robot Edition, a new task of finding a room in an unknown and unmodified office environment by following instructions obtained in spoken dialog from an untrained person. We present a solution that treats this challenge as a series of sub-tasks: natural-language interpretation, autonomous navigation, and semantic mapping. The solution consists of a finite-state-machine system design whose states solve these sub-tasks to complete The Amazing Race TM. Our design is deployed on a real robot and its performance is demonstrated in 52 trials on 4 floors of each of 3 different previously unseen buildings with 13 untrained volunteers. △ Less","28 October, 2020",https://arxiv.org/pdf/2010.15033
High-dimensional inference: a statistical mechanics perspective,Jean Barbier,"Statistical inference is the science of drawing conclusions about some system from data. In modern signal processing and machine learning, inference is done in very high dimension: very many unknown characteristics about the system have to be deduced from a lot of high-dimensional noisy data. This ""high-dimensional regime"" is reminiscent of statistical mechanics, which aims at describing the macroscopic behavior of a complex system based on the knowledge of its microscopic interactions. It is by now clear that there are many connections between inference and statistical physics. This article aims at emphasizing some of the deep links connecting these apparently separated disciplines through the description of paradigmatic models of high-dimensional inference in the language of statistical mechanics. This article has been published in the issue on artificial intelligence of Ithaca, an Italian popularization-of-science journal. The selected topics and references are highly biased and not intended to be exhaustive in any ways. Its purpose is to serve as introduction to statistical mechanics of inference through a very specific angle that corresponds to my own tastes and limited knowledge. △ Less","28 October, 2020",https://arxiv.org/pdf/2010.14863
"A Fast, Scalable, Universal Approach For Distributed Data Aggregations",Niranda Perera;Vibhatha Abeykoon;Chathura Widanage;Supun Kamburugamuve;Thejaka Amila Kanewala;Pulasthi Wickramasinghe;Ahmet Uyar;Hasara Maithree;Damitha Lenadora;Geoffrey Fox,"In the current era of Big Data, data engineering has transformed into an essential field of study across many branches of science. Advancements in Artificial Intelligence (AI) have broadened the scope of data engineering and opened up new applications in both enterprise and research communities. Aggregations (also termed reduce in functional programming) are an integral functionality in these applications. They are traditionally aimed at generating meaningful information on large data-sets, and today, they are being used for engineering more effective features for complex AI models. Aggregations are usually carried out on top of data abstractions such as tables/ arrays and are combined with other operations such as grouping of values. There are frameworks that excel in the said domains individually. But, we believe that there is an essential requirement for a data analytics tool that can universally integrate with existing frameworks, and thereby increase the productivity and efficiency of the entire data analytics pipeline. Cylon endeavors to fulfill this void. In this paper, we present Cylon's fast and scalable aggregation operations implemented on top of a distributed in-memory table structure that universally integrates with existing frameworks. △ Less","14 December, 2020",https://arxiv.org/pdf/2010.14596
Artificial intelligence based writer identification generates new evidence for the unknown scribes of the Dead Sea Scrolls exemplified by the Great Isaiah Scroll (1QIsaa),Mladen Popović;Maruf A. Dhali;Lambert Schomaker,"The Dead Sea Scrolls are tangible evidence of the Bible's ancient scribal culture. Palaeography - the study of ancient handwriting - can provide access to this scribal culture. However, one of the problems of traditional palaeography is to determine writer identity when the writing style is near uniform. This is exemplified by the Great Isaiah Scroll (1QIsaa). To this end, we used pattern recognition and artificial intelligence techniques to innovate the palaeography of the scrolls regarding writer identification and to pioneer the microlevel of individual scribes to open access to the Bible's ancient scribal culture. Although many scholars believe that 1QIsaa was written by one scribe, we report new evidence for a breaking point in the series of columns in this scroll. Without prior assumption of writer identity, based on point clouds of the reduced-dimensionality feature-space, we found that columns from the first and second halves of the manuscript ended up in two distinct zones of such scatter plots, notably for a range of digital palaeography tools, each addressing very different featural aspects of the script samples. In a secondary, independent, analysis, now assuming writer difference and using yet another independent feature method and several different types of statistical testing, a switching point was found in the column series. A clear phase transition is apparent around column 27. Given the statistically significant differences between the two halves, a tertiary, post-hoc analysis was performed. Demonstrating that two main scribes were responsible for the Great Isaiah Scroll, this study sheds new light on the Bible's ancient scribal culture by providing new, tangible evidence that ancient biblical texts were not copied by a single scribe only but that multiple scribes could closely collaborate on one particular manuscript. △ Less","27 October, 2020",https://arxiv.org/pdf/2010.14476
An Experimentation Platform for Explainable Coalition Situational Understanding,Katie Barrett-Powell;Jack Furby;Liam Hiley;Marc Roig Vilamala;Harrison Taylor;Federico Cerutti;Alun Preece;Tianwei Xing;Luis Garcia;Mani Srivastava;Dave Braines,"We present an experimentation platform for coalition situational understanding research that highlights capabilities in explainable artificial intelligence/machine learning (AI/ML) and integration of symbolic and subsymbolic AI/ML approaches for event processing. The Situational Understanding Explorer (SUE) platform is designed to be lightweight, to easily facilitate experiments and demonstrations, and open. We discuss our requirements to support coalition multi-domain operations with emphasis on asset interoperability and ad hoc human-machine teaming in a dense urban terrain setting. We describe the interface functionality and give examples of SUE applied to coalition situational understanding tasks. △ Less","9 November, 2020",https://arxiv.org/pdf/2010.14388
The DigitalTwin from an Artificial Intelligence Perspective,Oliver Niggemann;Alexander Diedrich;Christian Kuehnert;Erik Pfannstiel;Joshua Schraven,"Services for Cyber-Physical Systems based on Artificial Intelligence and Machine Learning require a virtual representation of the physical. To reduce modeling efforts and to synchronize results, for each system, a common and unique virtual representation used by all services during the whole system life-cycle is needed, i.e. a DigitalTwin. In this paper such a DigitalTwin, namely the AI reference model AITwin, is defined. This reference model is verified by using a running example from process industry and by analyzing the work done in recent projects. △ Less","27 October, 2020",https://arxiv.org/pdf/2010.14376
Spiking Neural Networks -- Part III: Neuromorphic Communications,Nicolas Skatchkovsky;Hyeryung Jang;Osvaldo Simeone,"Synergies between wireless communications and artificial intelligence are increasingly motivating research at the intersection of the two fields. On the one hand, the presence of more and more wirelessly connected devices, each with its own data, is driving efforts to export advances in machine learning (ML) from high performance computing facilities, where information is stored and processed in a single location, to distributed, privacy-minded, processing at the end user. On the other hand, ML can address algorithm and model deficits in the optimization of communication protocols. However, implementing ML models for learning and inference on battery-powered devices that are connected via bandwidth-constrained channels remains challenging. This paper explores two ways in which Spiking Neural Networks (SNNs) can help address these open problems. First, we discuss federated learning for the distributed training of SNNs, and then describe the integration of neuromorphic sensing, SNNs, and impulse radio technologies for low-power remote inference. △ Less","9 December, 2020",https://arxiv.org/pdf/2010.14220
Reading Between the Lines: Exploring Infilling in Visual Narratives,Khyathi Raghavi Chandu;Ruo-Ping Dong;Alan Black,"Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence. In this regard, there is often crucial subtext that is derived from the surrounding contexts. The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts. In this paper, we tackle this problem by using \textit{infilling} techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images. We also present a new large scale \textit{visual procedure telling} (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies. Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts. We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling. We also demonstrate the effects of interposing new text with missing images during inference. The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/. △ Less","26 October, 2020",https://arxiv.org/pdf/2010.13944
Diptychs of human and machine perceptions,Vivien Cabannes;Thomas Kerdreux;Louis Thiry,"We propose visual creations that put differences in algorithms and humans \emph{perceptions} into perspective. We exploit saliency maps of neural networks and visual focus of humans to create diptychs that are reinterpretations of an original image according to both machine and human attentions. Using those diptychs as a qualitative evaluation of perception, we discuss some crucial issues of current \textit{task-oriented} artificial intelligence. △ Less","12 October, 2020",https://arxiv.org/pdf/2010.13864
A Path-Dependent Variational Framework for Incremental Information Gathering,William Clark;Maani Ghaffari,"Information gathered along a path is inherently submodular; the incremental amount of information gained along a path decreases due to redundant observations. In addition to submodularity, the incremental amount of information gained is a function of not only the current state but also the entire history as well. This paper presents the construction of the first-order necessary optimality conditions for memory (history-dependent) Lagrangians. Path-dependent problems frequently appear in robotics and artificial intelligence, where the state such as a map is partially observable, and information can only be obtained along a trajectory by local sensing. Robotic exploration and environmental monitoring has numerous real-world applications and can be formulated using the proposed approach. △ Less","26 October, 2020",https://arxiv.org/pdf/2010.13813
Interpreting convolutional networks trained on textual data,Reza Marzban;Christopher John Crick,"There have been many advances in the artificial intelligence field due to the emergence of deep learning. In almost all sub-fields, artificial neural networks have reached or exceeded human-level performance. However, most of the models are not interpretable. As a result, it is hard to trust their decisions, especially in life and death scenarios. In recent years, there has been a movement toward creating explainable artificial intelligence, but most work to date has concentrated on image processing models, as it is easier for humans to perceive visual patterns. There has been little work in other fields like natural language processing. In this paper, we train a convolutional model on textual data and analyze the global logic of the model by studying its filter values. In the end, we find the most important words in our corpus to our models logic and remove the rest (95%). New models trained on just the 5% most important words can achieve the same performance as the original model while reducing training time by more than half. Approaches such as this will help us to understand NLP models, explain their decisions according to their word choices, and improve them by finding blind spots and biases. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.13585
Understanding understanding: a renormalization group inspired model of (artificial) intelligence,A. Jakovac;D. Berenyi;P. Posfay,"This paper is about the meaning of understanding in scientific and in artificial intelligent systems. We give a mathematical definition of the understanding, where, contrary to the common wisdom, we define the probability space on the input set, and we treat the transformation made by an intelligent actor not as a loss of information, but instead a reorganization of the information in the framework of a new coordinate system. We introduce, following the ideas of physical renormalization group, the notions of relevant and irrelevant parameters, and discuss, how the different AI tasks can be interpreted along these concepts, and how the process of learning can be described. We show, how scientific understanding fits into this framework, and demonstrate, what is the difference between a scientific task and pattern recognition. We also introduce a measure of relevance, which is useful for performing lossy compression. △ Less","26 October, 2020",https://arxiv.org/pdf/2010.13482
Video-based Facial Expression Recognition using Graph Convolutional Networks,Daizong Liu;Hongting Zhang;Pan Zhou,"Facial expression recognition (FER), aiming to classify the expression present in the facial image or video, has attracted a lot of research interests in the field of artificial intelligence and multimedia. In terms of video based FER task, it is sensible to capture the dynamic expression variation among the frames to recognize facial expression. However, existing methods directly utilize CNN-RNN or 3D CNN to extract the spatial-temporal features from different facial units, instead of concentrating on a certain region during expression variation capturing, which leads to limited performance in FER. In our paper, we introduce a Graph Convolutional Network (GCN) layer into a common CNN-RNN based model for video-based FER. First, the GCN layer is utilized to learn more significant facial expression features which concentrate on certain regions after sharing information between extracted CNN features of nodes. Then, a LSTM layer is applied to learn long-term dependencies among the GCN learned features to model the variation. In addition, a weight assignment mechanism is also designed to weight the output of different nodes for final classification by characterizing the expression intensities in each frame. To the best of our knowledge, it is the first time to use GCN in FER task. We evaluate our method on three widely-used datasets, CK+, Oulu-CASIA and MMI, and also one challenging wild dataset AFEW8.0, and the experimental results demonstrate that our method has superior performance to existing methods. △ Less","26 October, 2020",https://arxiv.org/pdf/2010.13386
Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification,Linyi Yang;Eoin M. Kenny;Tin Lok James Ng;Yi Yang;Barry Smyth;Ruihai Dong,"Corporate mergers and acquisitions (M&A) account for billions of dollars of investment globally every year, and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust and accurate model, but be able to generate useful explanations to garner a user's trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user's trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials. △ Less","23 October, 2020",https://arxiv.org/pdf/2010.12512
Deep Learning Framework for Measuring the Digital Strategy of Companies from Earnings Calls,Ahmed Ghanim Al-Ali;Robert Phaal;Donald Sull,"Companies today are racing to leverage the latest digital technologies, such as artificial intelligence, blockchain, and cloud computing. However, many companies report that their strategies did not achieve the anticipated business results. This study is the first to apply state of the art NLP models on unstructured data to understand the different clusters of digital strategy patterns that companies are Adopting. We achieve this by analyzing earnings calls from Fortune Global 500 companies between 2015 and 2019. We use Transformer based architecture for text classification which show a better understanding of the conversation context. We then investigate digital strategy patterns by applying clustering analysis. Our findings suggest that Fortune 500 companies use four distinct strategies which are product led, customer experience led, service led, and efficiency led. This work provides an empirical baseline for companies and researchers to enhance our understanding of the field. △ Less","2 December, 2020",https://arxiv.org/pdf/2010.12418
Cloud Energy Micro-Moment Data Classification: A Platform Study,Abdullah Alsalemi;Ayman Al-Kababji;Yassine Himeur;Faycal Bensaali;Abbes Amira,"Energy efficiency is a crucial factor in the well-being of our planet. In parallel, Machine Learning (ML) plays an instrumental role in automating our lives and creating convenient workflows for enhancing behavior. So, analyzing energy behavior can help understand weak points and lay the path towards better interventions. Moving towards higher performance, cloud platforms can assist researchers in conducting classification trials that need high computational power. Under the larger umbrella of the Consumer Engagement Towards Energy Saving Behavior by means of Exploiting Micro Moments and Mobile Recommendation Systems (EM)3 framework, we aim to influence consumers behavioral change via improving their power consumption consciousness. In this paper, common cloud artificial intelligence platforms are benchmarked and compared for micro-moment classification. The Amazon Web Services, Google Cloud Platform, Google Colab, and Microsoft Azure Machine Learning are employed on simulated and real energy consumption datasets. The KNN, DNN, and SVM classifiers have been employed. Superb performance has been observed in the selected cloud platforms, showing relatively close performance. Yet, the nature of some algorithms limits the training performance. △ Less","1 November, 2020",https://arxiv.org/pdf/2010.12064
"""Healthy surveillance"": Designing a concept for privacy-preserving mask recognition AI in the age of pandemics",Niklas Kühl;Dominik Martin;Clemens Wolff;Melanie Volkamer,"The obligation to wear masks in times of pandemics reduces the risk of spreading viruses. In case of the COVID-19 pandemic in 2020, many governments recommended or even obligated their citizens to wear masks as an effective countermeasure. In order to continuously monitor the compliance of this policy measure in public spaces like restaurants or tram stations by public authorities, one scalable and automatable option depicts the application of surveillance systems, i.e., CCTV. However, large-scale monitoring of mask recognition does not only require a well-performing Artificial Intelligence, but also ensure that no privacy issues are introduced, as surveillance is a deterrent for citizens and regulations like General Data Protection Regulation (GDPR) demand strict regulations of such personal data. In this work, we show how a privacy-preserving mask recognition artifact could look like, demonstrate different options for implementation and evaluate performances. Our conceptual deep-learning based Artificial Intelligence is able to achieve detection performances between 95% and 99% in a privacy-friendly setting. On that basis, we elaborate on the trade-off between the level of privacy preservation and Artificial Intelligence performance, i.e. the ""price of privacy"". △ Less","20 October, 2020",https://arxiv.org/pdf/2010.12026
Ultra-low power on-chip learning of speech commands with phase-change memories,Venkata Pavan Kumar Miriyala;Masatoshi Ishii,"Embedding artificial intelligence at the edge (edge-AI) is an elegant solution to tackle the power and latency issues in the rapidly expanding Internet of Things. As edge devices typically spend most of their time in sleep mode and only wake-up infrequently to collect and process sensor data, non-volatile in-memory computing (NVIMC) is a promising approach to design the next generation of edge-AI devices. Recently, we proposed an NVIMC-based neuromorphic accelerator using the phase change memories (PCMs), which we call as Raven. In this work, we demonstrate the ultra-low-power on-chip training and inference of speech commands using Raven. We showed that Raven can be trained on-chip with power consumption as low as 30~uW, which is suitable for edge applications. Furthermore, we showed that at iso-accuracies, Raven needs 70.36x and 269.23x less number of computations to be performed than a deep neural network (DNN) during inference and training, respectively. Owing to such low power and computational requirements, Raven provides a promising pathway towards ultra-low-power training and inference at the edge. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.11741
Optimising Stochastic Routing for Taxi Fleets with Model Enhanced Reinforcement Learning,Shen Ren;Qianxiao Li;Liye Zhang;Zheng Qin;Bo Yang,"The future of mobility-as-a-Service (Maas)should embrace an integrated system of ride-hailing, street-hailing and ride-sharing with optimised intelligent vehicle routing in response to a real-time, stochastic demand pattern. We aim to optimise routing policies for a large fleet of vehicles for street-hailing services, given a stochastic demand pattern in small to medium-sized road networks. A model-based dispatch algorithm, a high performance model-free reinforcement learning based algorithm and a novel hybrid algorithm combining the benefits of both the top-down approach and the model-free reinforcement learning have been proposed to route the \emph{vacant} vehicles. We design our reinforcement learning based routing algorithm using proximal policy optimisation and combined intrinsic and extrinsic rewards to strike a balance between exploration and exploitation. Using a large-scale agent-based microscopic simulation platform to evaluate our proposed algorithms, our model-free reinforcement learning and hybrid algorithm show excellent performance on both artificial road network and community-based Singapore road network with empirical demands, and our hybrid algorithm can significantly accelerate the model-free learner in the process of learning. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.11738
Efficient RDF Graph Storage based on Reinforcement Learning,Lei Zheng;Ziming Shen;Hongzhi Wang,"Knowledge graph is an important cornerstone of artificial intelligence. The construction and release of large-scale knowledge graphs in various fields pose new challenges to knowledge graph data management. Due to the maturity and stability, relational database is also suitable for RDF data storage. However, the complex structure of RDF graph brings challenges to storage structure design for RDF graph in the relational database. To address the difficult problem, this paper adopts reinforcement learning (RL) to optimize the storage partition method of RDF graph based on the relational database. We transform the graph storage into a Markov decision process, and develop the reinforcement learning algorithm for graph storage design. For effective RL-based storage design, we propose the data feature extraction method of RDF tables and the query rewriting priority policy during model training. The extensive experimental results demonstrate that our approach outperforms existing RDF storage design methods. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.11538
Beta Embeddings for Multi-Hop Logical Reasoning in Knowledge Graphs,Hongyu Ren;Jure Leskovec,"One of the fundamental problems in Artificial Intelligence is to perform complex multi-hop logical reasoning over the facts captured by a knowledge graph (KG). This problem is challenging, because KGs can be massive and incomplete. Recent approaches embed KG entities in a low dimensional space and then use these embeddings to find the answer entities. However, it has been an outstanding challenge of how to handle arbitrary first-order logic (FOL) queries as present methods are limited to only a subset of FOL operators. In particular, the negation operator is not supported. An additional limitation of present methods is also that they cannot naturally model uncertainty. Here, we present BetaE, a probabilistic embedding framework for answering arbitrary FOL queries over KGs. BetaE is the first method that can handle a complete set of first-order logical operations: conjunction (\wedge), disjunction (\vee), and negation (\neg). A key insight of BetaE is to use probabilistic distributions with bounded support, specifically the Beta distribution, and embed queries/entities as distributions, which as a consequence allows us to also faithfully model uncertainty. Logical operations are performed in the embedding space by neural operators over the probabilistic embeddings. We demonstrate the performance of BetaE on answering arbitrary FOL queries on three large, incomplete KGs. While being more general, BetaE also increases relative performance by up to 25.4% over the current state-of-the-art KG reasoning methods that can only handle conjunctive queries without negation. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.11465
I-nteract 2.0: A Cyber-Physical System to Design 3D Models using Mixed Reality Technologies and Deep Learning for Additive Manufacturing,Ammar Malik;Hugo Lhachemi;Robert Shorten,"I-nteract is a cyber-physical system that enables real-time interaction with both virtual and real artifacts to design 3D models for additive manufacturing by leveraging on mixed reality technologies. This paper presents novel advances in the development of the interaction platform I-nteract to generate 3D models using both constructive solid geometry and artificial intelligence. The system also enables the user to adjust the dimensions of the 3D models with respect to their physical workspace. The effectiveness of the system is demonstrated by generating 3D models of furniture (e.g., chairs and tables) and fitting them into the physical space in a mixed reality environment. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.11025
Explaining black-box text classifiers for disease-treatment information extraction,Milad Moradi;Matthias Samwald,"Deep neural networks and other intricate Artificial Intelligence (AI) models have reached high levels of accuracy on many biomedical natural language processing tasks. However, their applicability in real-world use cases may be limited due to their vague inner working and decision logic. A post-hoc explanation method can approximate the behavior of a black-box AI model by extracting relationships between feature values and outcomes. In this paper, we introduce a post-hoc explanation method that utilizes confident itemsets to approximate the behavior of black-box classifiers for medical information extraction. Incorporating medical concepts and semantics into the explanation process, our explanator finds semantic relations between inputs and outputs in different parts of the decision space of a black-box classifier. The experimental results show that our explanation method can outperform perturbation and decision set based explanators in terms of fidelity and interpretability of explanations produced for predictions on a disease-treatment information extraction task. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.10873
Boosting Gradient for White-Box Adversarial Attacks,Hongying Liu;Zhenyu Zhou;Fanhua Shang;Xiaoyu Qi;Yuanyuan Liu;Licheng Jiao,"Deep neural networks (DNNs) are playing key roles in various artificial intelligence applications such as image classification and object recognition. However, a growing number of studies have shown that there exist adversarial examples in DNNs, which are almost imperceptibly different from original samples, but can greatly change the network output. Existing white-box attack algorithms can generate powerful adversarial examples. Nevertheless, most of the algorithms concentrate on how to iteratively make the best use of gradients to improve adversarial performance. In contrast, in this paper, we focus on the properties of the widely-used ReLU activation function, and discover that there exist two phenomena (i.e., wrong blocking and over transmission) misleading the calculation of gradients in ReLU during the backpropagation. Both issues enlarge the difference between the predicted changes of the loss function from gradient and corresponding actual changes, and mislead the gradients which results in larger perturbations. Therefore, we propose a universal adversarial example generation method, called ADV-ReLU, to enhance the performance of gradient based white-box attack algorithms. During the backpropagation of the network, our approach calculates the gradient of the loss function versus network input, maps the values to scores, and selects a part of them to update the misleading gradients. Comprehensive experimental results on \emph{ImageNet} demonstrate that our ADV-ReLU can be easily integrated into many state-of-the-art gradient-based white-box attack algorithms, as well as transferred to black-box attack attackers, to further decrease perturbations in the {\ell _2}-norm. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.10712
Scalable HPC and AI Infrastructure for COVID-19 Therapeutics,Hyungro Lee;Andre Merzky;Li Tan;Mikhail Titov;Matteo Turilli;Dario Alfe;Agastya Bhati;Alex Brace;Austin Clyde;Peter Coveney;Heng Ma;Arvind Ramanathan;Rick Stevens;Anda Trifan;Hubertus Van Dam;Shunzhou Wan;Sean Wilkinson;Shantenu Jha,"COVID-19 has claimed more 1 million lives and resulted in over 40 million infections. There is an urgent need to identify drugs that can inhibit SARS-CoV-2. In response, the DOE recently established the Medical Therapeutics project as part of the National Virtual Biotechnology Laboratory, and tasked it with creating the computational infrastructure and methods necessary to advance therapeutics development. We discuss innovations in computational infrastructure and methods that are accelerating and advancing drug design. Specifically, we describe several methods that integrate artificial intelligence and simulation-based approaches, and the design of computational infrastructure to support these methods at scale. We discuss their implementation and characterize their performance, and highlight science advances that these capabilities have enabled. △ Less","20 October, 2020",https://arxiv.org/pdf/2010.10517
Anti-Distillation: Improving reproducibility of deep networks,Gil I. Shamir;Lorenzo Coviello,"Deep networks have been revolutionary in improving performance of machine learning and artificial intelligence systems. Their high prediction accuracy, however, comes at a price of \emph{model irreproducibility\/} in very high levels that do not occur with classical linear models. Two models, even if they are supposedly identical, with identical architecture and identical trained parameter sets, and that are trained on the same set of training examples, while possibly providing identical average prediction accuracies, may predict very differently on individual, previously unseen, examples. \emph{Prediction differences\/} may be as large as the order of magnitude of the predictions themselves. Ensembles have been shown to somewhat mitigate this behavior, but without an extra push, may not be utilizing their full potential. In this work, a novel approach, \emph{Anti-Distillation\/}, is proposed to address irreproducibility in deep networks, where ensemble models are used to generate predictions. Anti-Distillation forces ensemble components away from one another by techniques like de-correlating their outputs over mini-batches of examples, forcing them to become even more different and more diverse. Doing so enhances the benefit of ensembles, making the final predictions more reproducible. Empirical results demonstrate substantial prediction difference reductions achieved by Anti-Distillation on benchmark and real datasets. △ Less","19 October, 2020",https://arxiv.org/pdf/2010.09923
Deep Reinforcement Learning for Adaptive Network Slicing in 5G for Intelligent Vehicular Systems and Smart Cities,Almuthanna Nassar;Yasin Yilmaz,"Intelligent vehicular systems and smart city applications are the fastest growing Internet of things (IoT) implementations at a compound annual growth rate of 30%. In view of the recent advances in IoT devices and the emerging new breed of IoT applications driven by artificial intelligence (AI), fog radio access network (F-RAN) has been recently introduced for the fifth generation (5G) wireless communications to overcome the latency limitations of cloud-RAN (C-RAN). We consider the network slicing problem of allocating the limited resources at the network edge (fog nodes) to vehicular and smart city users with heterogeneous latency and computing demands in dynamic environments. We develop a network slicing model based on a cluster of fog nodes (FNs) coordinated with an edge controller (EC) to efficiently utilize the limited resources at the network edge. For each service request in a cluster, the EC decides which FN to execute the task, i.e., locally serve the request at the edge, or to reject the task and refer it to the cloud. We formulate the problem as infinite-horizon Markov decision process (MDP) and propose a deep reinforcement learning (DRL) solution to adaptively learn the optimal slicing policy. The performance of the proposed DRL-based slicing method is evaluated by comparing it with other slicing approaches in dynamic environments and for different scenarios of design objectives. Comprehensive simulation results corroborate that the proposed DRL-based EC quickly learns the optimal policy through interaction with the environment, which enables adaptive and automated network slicing for efficient resource allocation in dynamic vehicular and smart city environments. △ Less","19 October, 2020",https://arxiv.org/pdf/2010.09916
Learning to Reconstruct and Segment 3D Objects,Bo Yang,"To endow machines with the ability to perceive the real-world in a three dimensional representation as we do as humans is a fundamental and long-standing topic in Artificial Intelligence. Given different types of visual inputs such as images or point clouds acquired by 2D/3D sensors, one important goal is to understand the geometric structure and semantics of the 3D environment. Traditional approaches usually leverage hand-crafted features to estimate the shape and semantics of objects or scenes. However, they are difficult to generalize to novel objects and scenarios, and struggle to overcome critical issues caused by visual occlusions. By contrast, we aim to understand scenes and the objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. To achieve these aims, this thesis makes three core contributions from object-level 3D shape estimation from single or multiple views to scene-level semantic understanding. △ Less","19 October, 2020",https://arxiv.org/pdf/2010.09582
"Introducing and Applying Newtonian Blurring: An Augmented Dataset of 126,000 Human Connectomes at braingraph.org",Laszlo Keresztes;Evelin Szogi;Balint Varga;Vince Grolmusz,"Gaussian blurring is a well-established method for image data augmentation: it may generate a large set of images from a small set of pictures for training and testing purposes for Artificial Intelligence (AI) applications. When we apply AI for non-imagelike biological data, hardly any related method exists. Here we introduce the ""Newtonian blurring"" in human braingraph (or connectome) augmentation: Started from a dataset of 1053 subjects, we first repeat a probabilistic weighted braingraph construction algorithm 10 times for describing the connections of distinct cerebral areas, then take 7 repetitions in every possible way, delete the lower and upper extremes, and average the remaining 7-2=5 edge-weights for the data of each subject. This way we augment the 1053 graph-set to 120 x 1053 = 126,360 graphs. In augmentation techniques, it is an important requirement that no artificial additions should be introduced into the dataset. Gaussian blurring and also this Newtonian blurring satisfy this goal. The resulting dataset of 126,360 graphs, each in 5 resolutions (i.e., 631,800 graphs in total), is freely available at the site https://braingraph.org/cms/download-pit-group-connectomes/. Augmenting with Newtonian blurring may also be applicable in other non-image related fields, where probabilistic processing and data averaging are implemented. △ Less","21 October, 2020",https://arxiv.org/pdf/2010.09568
It's the Best Only When It Fits You Most: Finding Related Models for Serving Based on Dynamic Locality Sensitive Hashing,Lixi Zhou;Zijie Wang;Amitabh Das;Jia Zou,"In recent, deep learning has become the most popular direction in machine learning and artificial intelligence. However, preparation of training data is often a bottleneck in the lifecycle of deploying a deep learning model for production or research. Reusing models for inferencing a dataset can greatly save the human costs required for training data creation. Although there exist a number of model sharing platform such as TensorFlow Hub, PyTorch Hub, DLHub, most of these systems require model uploaders to manually specify the details of each model and model downloaders to screen keyword search results for selecting a model. They are in lack of an automatic model searching tool. This paper proposes an end-to-end process of searching related models for serving based on the similarity of the target dataset and the training datasets of the available models. While there exist many similarity measurements, we study how to efficiently apply these metrics without pair-wise comparison and compare the effectiveness of these metrics. We find that our proposed adaptivity measurement which is based on Jensen-Shannon (JS) divergence, is an effective measurement, and its computation can be significantly accelerated by using the technique of locality sensitive hashing. △ Less","13 October, 2020",https://arxiv.org/pdf/2010.09474
The NVIDIA PilotNet Experiments,Mariusz Bojarski;Chenyi Chen;Joyjit Daw;Alperen Değirmenci;Joya Deri;Bernhard Firner;Beat Flepp;Sachin Gogri;Jesse Hong;Lawrence Jackel;Zhenhua Jia;BJ Lee;Bo Liu;Fei Liu;Urs Muller;Samuel Payne;Nischal Kota Nagendra Prasad;Artem Provodin;John Roach;Timur Rvachov;Neha Tadimeti;Jesper van Engelen;Haiguang Wen;Eric Yang;Zongyi Yang,"Four years ago, an experimental system known as PilotNet became the first NVIDIA system to steer an autonomous car along a roadway. This system represents a departure from the classical approach for self-driving in which the process is manually decomposed into a series of modules, each performing a different task. In PilotNet, on the other hand, a single deep neural network (DNN) takes pixels as input and produces a desired vehicle trajectory as output; there are no distinct internal modules connected by human-designed interfaces. We believe that handcrafted interfaces ultimately limit performance by restricting information flow through the system and that a learned approach, in combination with other artificial intelligence systems that add redundancy, will lead to better overall performing systems. We continue to conduct research toward that goal. This document describes the PilotNet lane-keeping effort, carried out over the past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here we present a snapshot of system status in mid-2020 and highlight some of the work done by the PilotNet group. △ Less","17 October, 2020",https://arxiv.org/pdf/2010.08776
Studying the Similarity of COVID-19 Sounds based on Correlation Analysis of MFCC,Mohamed Bader;Ismail Shahin;Abdelfatah Hassan,"Recently there has been a formidable work which has been put up from the people who are working in the frontlines such as hospitals, clinics, and labs alongside researchers and scientists who are also putting tremendous efforts in the fight against COVID-19 pandemic. Due to the preposterous spread of the virus, the integration of the artificial intelligence has taken a considerable part in the health sector, by implementing the fundamentals of Automatic Speech Recognition (ASR) and deep learning algorithms. In this paper, we illustrate the importance of speech signal processing in the extraction of the Mel-Frequency Cepstral Coefficients (MFCCs) of the COVID-19 and non-COVID-19 samples and find their relationship using Pearson correlation coefficients. Our results show high similarity in MFCCs between different COVID-19 cough and breathing sounds, while MFCC of voice is more robust between COVID-19 and non-COVID-19 samples. Moreover, our results are preliminary, and there is a possibility to exclude the voices of COVID-19 patients from further processing in diagnosing the disease. △ Less","17 October, 2020",https://arxiv.org/pdf/2010.08770
Squashing activation functions in benchmark tests: towards eXplainable Artificial Intelligence using continuous-valued logic,Daniel Zeltner;Benedikt Schmid;Gabor Csiszar;Orsolya Csiszar,"Over the past few years, deep neural networks have shown excellent results in multiple tasks, however, there is still an increasing need to address the problem of interpretability to improve model transparency, performance, and safety. Achieving eXplainable Artificial Intelligence (XAI) by combining neural networks with continuous logic and multi-criteria decision-making tools is one of the most promising ways to approach this problem: by this combination, the black-box nature of neural models can be reduced. The continuous logic-based neural model uses so-called Squashing activation functions, a parametric family of functions that satisfy natural invariance requirements and contain rectified linear units as a particular case. This work demonstrates the first benchmark tests that measure the performance of Squashing functions in neural networks. Three experiments were carried out to examine their usability and a comparison with the most popular activation functions was made for five different network types. The performance was determined by measuring the accuracy, loss, and time per epoch. These experiments and the conducted benchmarks have proven that the use of Squashing functions is possible and similar in performance to conventional activation functions. Moreover, a further experiment was conducted by implementing nilpotent logical gates to demonstrate how simple classification tasks can be solved successfully and with high performance. The results indicate that due to the embedded nilpotent logical operators and the differentiability of the Squashing function, it is possible to solve classification problems, where other commonly used activation functions fail. △ Less","17 October, 2020",https://arxiv.org/pdf/2010.08760
Federated TON_IoT Windows Datasets for Evaluating AI-based Security Applications,Nour Moustafa;Marwa Keshk;Essam Debie;Helge Janicke,"Existing cyber security solutions have been basically developed using knowledge-based models that often cannot trigger new cyber-attack families. With the boom of Artificial Intelligence (AI), especially Deep Learning (DL) algorithms, those security solutions have been plugged-in with AI models to discover, trace, mitigate or respond to incidents of new security events. The algorithms demand a large number of heterogeneous data sources to train and validate new security systems. This paper presents the description of new datasets, the so-called ToN_IoT, which involve federated data sources collected from telemetry datasets of IoT services, operating system datasets of Windows and Linux, and datasets of network traffic. The paper introduces the testbed and description of TON_IoT datasets for Windows operating systems. The testbed was implemented in three layers: edge, fog and cloud. The edge layer involves IoT and network devices, the fog layer contains virtual machines and gateways, and the cloud layer involves cloud services, such as data analytics, linked to the other two layers. These layers were dynamically managed using the platforms of software-Defined Network (SDN) and Network-Function Virtualization (NFV) using the VMware NSX and vCloud NFV platform. The Windows datasets were collected from audit traces of memories, processors, networks, processes and hard disks. The datasets would be used to evaluate various AI-based cyber security solutions, including intrusion detection, threat intelligence and hunting, privacy preservation and digital forensics. This is because the datasets have a wide range of recent normal and attack features and observations, as well as authentic ground truth events. The datasets can be publicly accessed from this link [1]. △ Less","4 October, 2020",https://arxiv.org/pdf/2010.08522
Data Analytics-enabled Intrusion Detection: Evaluations of ToN_IoT Linux Datasets,Nour Moustafa;Mohiuddin Ahmed;Sherif Ahmed,"With the widespread of Artificial Intelligence (AI)- enabled security applications, there is a need for collecting heterogeneous and scalable data sources for effectively evaluating the performances of security applications. This paper presents the description of new datasets, named ToN IoT datasets that include distributed data sources collected from Telemetry datasets of Internet of Things (IoT) services, Operating systems datasets of Windows and Linux, and datasets of Network traffic. The paper aims to describe the new testbed architecture used to collect Linux datasets from audit traces of hard disk, memory and process. The architecture was designed in three distributed layers of edge, fog, and cloud. The edge layer comprises IoT and network systems, the fog layer includes virtual machines and gateways, and the cloud layer includes data analytics and visualization tools connected with the other two layers. The layers were programmatically controlled using Software-Defined Network (SDN) and Network-Function Virtualization (NFV) using the VMware NSX and vCloud NFV platform. The Linux ToN IoT datasets would be used to train and validate various new federated and distributed AI-enabled security solutions such as intrusion detection, threat intelligence, privacy preservation and digital forensics. Various Data analytical and machine learning methods are employed to determine the fidelity of the datasets in terms of examining feature engineering, statistics of legitimate and security events, and reliability of security events. The datasets can be publicly accessed from [1]. △ Less","4 October, 2020",https://arxiv.org/pdf/2010.08521
Online Decision Trees with Fairness,Wenbin Zhang;Liang Zhao,"While artificial intelligence (AI)-based decision-making systems are increasingly popular, significant concerns on the potential discrimination during the AI decision-making process have been observed. For example, the distribution of predictions is usually biased and dependents on the sensitive attributes (e.g., gender and ethnicity). Numerous approaches have therefore been proposed to develop decision-making systems that are discrimination-conscious by-design, which are typically batch-based and require the simultaneous availability of all the training data for model learning. However, in the real-world, the data streams usually come on the fly which requires the model to process each input data once ""on arrival"" and without the need for storage and reprocessing. In addition, the data streams might also evolve over time, which further requires the model to be able to simultaneously adapt to non-stationary data distributions and time-evolving bias patterns, with an effective and robust trade-off between accuracy and fairness. In this paper, we propose a novel framework of online decision tree with fairness in the data stream with possible distribution drifting. Specifically, first, we propose two novel fairness splitting criteria that encode the data as well as possible, while simultaneously removing dependence on the sensitive attributes, and further adapts to non-stationary distribution with fine-grained control when needed. Second, we propose two fairness decision tree online growth algorithms that fulfills different online fair decision-making requirements. Our experiments show that our algorithms are able to deal with discrimination in massive and non-stationary streaming environments, with a better trade-off between fairness and predictive performance. △ Less","14 October, 2020",https://arxiv.org/pdf/2010.08146
Testing the Quantitative Spacetime Hypothesis using Artificial Narrative Comprehension (I) : Bootstrapping Meaning from Episodic Narrative viewed as a Feature Landscape,Mark Burgess,"The problem of extracting important and meaningful parts of a sensory data stream, without prior training, is studied for symbolic sequences, by using textual narrative as a test case. This is part of a larger study concerning the extraction of concepts from spacetime processes, and their knowledge representations within hybrid symbolic-learning `Artificial Intelligence'. Most approaches to text analysis make extensive use of the evolved human sense of language and semantics. In this work, streams are parsed without knowledge of semantics, using only measurable patterns (size and time) within the changing stream of symbols -- as an event `landscape'. This is a form of interferometry. Using lightweight procedures that can be run in just a few seconds on a single CPU, this work studies the validity of the Semantic Spacetime Hypothesis, for the extraction of concepts as process invariants. This `semantic preprocessor' may then act as a front-end for more sophisticated long-term graph-based learning techniques. The results suggest that what we consider important and interesting about sensory experience is not solely based on higher reasoning, but on simple spacetime process cues, and this may be how cognitive processing is bootstrapped in the beginning. △ Less","23 September, 2020",https://arxiv.org/pdf/2010.08126
Research on AI Composition Recognition Based on Music Rules,Yang Deng;Ziyao Xu;Li Zhou;Huanping Liu;Anqi Huang,"The development of artificial intelligent composition has resulted in the increasing popularity of machine-generated pieces, with frequent copyright disputes consequently emerging. There is an insufficient amount of research on the judgement of artificial and machine-generated works; the creation of a method to identify and distinguish these works is of particular importance. Starting from the essence of the music, the article constructs a music-rule-identifying algorithm through extracting modes, which will identify the stability of the mode of machine-generated music, to judge whether it is artificial intelligent. The evaluation datasets used are provided by the Conference on Sound and Music Technology(CSMT). Experimental results demonstrate the algorithm to have a successful distinguishing ability between datasets with different source distributions. The algorithm will also provide some technological reference to the benign development of the music copyright and artificial intelligent music. △ Less","15 October, 2020",https://arxiv.org/pdf/2010.07805
Enhancement of the e-Invoicing Systems by Increasing the Efficiency of Workflows via Disruptive Technologies,Hiruni Gunaratne;Ingrid Pappel,"E-invoicing is a rapidly growing e-service in Europe as well as in the world. It is identified as a substantially significant element in progressing towards the goals of Digital Economy in the European Union. This thesis focuses on identifying inefficiencies in e-invoicing systems currently in use and the opportunities to apply emerging technologies such as artificial intelligence and robotic process automation, in order to increase efficiency and level of automatization. The study incorporates expert opinions and users perceptions in e-invoicing systems on the status quo and the necessities for higher automation. We focus on e-invoicing systems in the Baltic region consisting of the countries Estonia, Latvia and Lithuania. Based on the conducted research, the drawbacks in e-invoicing systems were identified related to operational, technological and information security related. Furthermore, the automation opportunities and general requirements for automation were identified. The functionalities that can be improved are discovered as well discussed in this thesis and the advantages of using emerging technologies in the context are explained. Based on research outcomes we propose a conceptual e-invoicing ecosystem and present recommenda-tions for its application along the future work needed in that field. △ Less","15 October, 2020",https://arxiv.org/pdf/2010.07636
A Methodology for Ethics-by-Design AI Systems: Dealing with Human Value Conflicts,Fabrice Muhlenbach,"The introduction of artificial intelligence into activities traditionally carried out by human beings produces brutal changes. This is not without consequences for human values. This paper is about designing and implementing models of ethical behaviors in AI-based systems, and more specifically it presents a methodology for designing systems that take ethical aspects into account at an early stage while finding an innovative solution to prevent human values from being affected. Two case studies where AI-based innovations complement economic and social proposals with this methodology are presented: one in the field of culture and operated by a private company, the other in the field of scientific research and supported by a state organization. △ Less","15 October, 2020",https://arxiv.org/pdf/2010.07610
Solar Coronal Magnetic Field Extrapolation from Synchronic Data with AI-generated Farside,Hyun-Jin Jeong;Yong-Jae Moon;Eunsu Park;Harim Lee,"Solar magnetic fields play a key role in understanding the nature of the coronal phenomena. Global coronal magnetic fields are usually extrapolated from photospheric fields, for which farside data is taken when it was at the frontside, about two weeks earlier. For the first time we have constructed the extrapolations of global magnetic fields using frontside and artificial intelligence (AI)-generated farside magnetic fields at a near-real time basis. We generate the farside magnetograms from three channel farside observations of Solar Terrestrial Relations Observatory (STEREO) Ahead (A) and Behind (B) by our deep learning model trained with frontside Solar Dynamics Observatory extreme ultraviolet images and magnetograms. For frontside testing data sets, we demonstrate that the generated magnetic field distributions are consistent with the real ones; not only active regions (ARs), but also quiet regions of the Sun. We make global magnetic field synchronic maps in which conventional farside data are replaced by farside ones generated by our model. The synchronic maps show much better not only the appearance of ARs but also the disappearance of others on the solar surface than before. We use these synchronized magnetic data to extrapolate the global coronal fields using Potential Field Source Surface (PFSS) model. We show that our results are much more consistent with coronal observations than those of the conventional method in view of solar active regions and coronal holes. We present several positive prospects of our new methodology for the study of solar corona, heliosphere, and space weather. △ Less","1 November, 2020",https://arxiv.org/pdf/2010.07553
Tracking Results and Utilization of Artificial Intelligence (tru-AI) in Radiology: Early-Stage COVID-19 Pandemic Observations,Axel Wismüller;Larry Stockmaster,"Objective: To introduce a method for tracking results and utilization of Artificial Intelligence (tru-AI) in radiology. By tracking both large-scale utilization and AI results data, the tru-AI approach is designed to calculate surrogates for measuring important disease-related observational quantities over time, such as the prevalence of intracranial hemorrhage during the COVID-19 pandemic outbreak. Methods: To quantitatively investigate the clinical applicability of the tru-AI approach, we analyzed service requests for automatically identifying intracranial hemorrhage (ICH) on head CT using a commercial AI solution. This software is typically used for AI-based prioritization of radiologists' reading lists for reducing turnaround times in patients with emergent clinical findings, such as ICH or pulmonary embolism.We analyzed data of N=9,421 emergency-setting non-contrast head CT studies at a major US healthcare system acquired from November 1, 2019 through June 2, 2020, and compared two observation periods, namely (i) a pre-pandemic epoch from November 1, 2019 through February 29, 2020, and (ii) a period during the COVID-19 pandemic outbreak, April 1-30, 2020. Results: Although daily CT scan counts were significantly lower during (40.1 +/- 7.9) than before (44.4 +/- 7.6) the COVID-19 outbreak, we found that ICH was more likely to be observed by AI during than before the COVID-19 outbreak (p<0.05), with approximately one daily ICH+ case more than statistically expected. Conclusion: Our results suggest that, by tracking both large-scale utilization and AI results data in radiology, the tru-AI approach can contribute clinical value as a versatile exploratory tool, aiming at a better understanding of pandemic-related effects on healthcare. △ Less","14 October, 2020",https://arxiv.org/pdf/2010.07437
Reconfigurable Intelligent Surface: Design the Channel -- a New Opportunity for Future Wireless Networks,Miguel Dajer;Zhengxiang Ma;Leonard Piazzi;Narayan Prasad;Xiao-Feng Qi;Baoling Sheen;Jin Yang;Guosen Yue,"In this paper, we survey state-of-the-art research outcomes in the burgeoning field of reconfigurable intelligent surface (RIS) in view of its potential for significant performance enhancement for next generation wireless communication networks by means of adapting the propagation environment. Emphasis has been placed on several aspects gating the commercially viability of a future network deployment. Comprehensive summaries are provided for practical hardware design considerations and broad implications of artificial intelligence techniques, so are in-depth outlooks on salient aspects of system models, use cases, and physical layer optimization techniques. △ Less","14 October, 2020",https://arxiv.org/pdf/2010.07408
Optimal Assistance for Object-Rearrangement Tasks in Augmented Reality,Benjamin Newman;Kevin Carlberg;Ruta Desai,"Augmented-reality (AR) glasses that will have access to onboard sensors and an ability to display relevant information to the user present an opportunity to provide user assistance in quotidian tasks. Many such tasks can be characterized as object-rearrangement tasks. We introduce a novel framework for computing and displaying AR assistance that consists of (1) associating an optimal action sequence with the policy of an embodied agent and (2) presenting this sequence to the user as suggestions in the AR system's heads-up display. The embodied agent comprises a ""hybrid"" between the AR system and the user, with the AR system's observation space (i.e., sensors) and the user's action space (i.e., task-execution actions); its policy is learned by minimizing the task-completion time. In this initial study, we assume that the AR system's observations include the environment's map and localization of the objects and the user. These choices allow us to formalize the problem of computing AR assistance for any object-rearrangement task as a planning problem, specifically as a capacitated vehicle-routing problem. Further, we introduce a novel AR simulator that can enable web-based evaluation of AR-like assistance and associated at-scale data collection via the Habitat simulator for embodied artificial intelligence. Finally, we perform a study that evaluates user response to the proposed form of AR assistance on a specific quotidian object-rearrangement task, house cleaning, using our proposed AR simulator on mechanical turk. In particular, we study the effect of the proposed AR assistance on users' task performance and sense of agency over a range of task difficulties. Our results indicate that providing users with such assistance improves their overall performance and while users report a negative impact to their agency, they may still prefer the proposed assistance to having no assistance at all. △ Less","14 October, 2020",https://arxiv.org/pdf/2010.07358
IMPECCABLE: Integrated Modeling PipelinE for COVID Cure by Assessing Better LEads,Aymen Al Saadi;Dario Alfe;Yadu Babuji;Agastya Bhati;Ben Blaiszik;Thomas Brettin;Kyle Chard;Ryan Chard;Peter Coveney;Anda Trifan;Alex Brace;Austin Clyde;Ian Foster;Tom Gibbs;Shantenu Jha;Kristopher Keipert;Thorsten Kurth;Dieter Kranzlmüller;Hyungro Lee;Zhuozhao Li;Heng Ma;Andre Merzky;Gerald Mathias;Alexander Partin;Junqi Yin,"The drug discovery process currently employed in the pharmaceutical industry typically requires about 10 years and $2-3 billion to deliver one new drug. This is both too expensive and too slow, especially in emergencies like the COVID-19 pandemic. In silicomethodologies need to be improved to better select lead compounds that can proceed to later stages of the drug discovery protocol accelerating the entire process. No single methodological approach can achieve the necessary accuracy with required efficiency. Here we describe multiple algorithmic innovations to overcome this fundamental limitation, development and deployment of computational infrastructure at scale integrates multiple artificial intelligence and simulation-based approaches. Three measures of performance are:(i) throughput, the number of ligands per unit time; (ii) scientific performance, the number of effective ligands sampled per unit time and (iii) peak performance, in flop/s. The capabilities outlined here have been used in production for several months as the workhorse of the computational infrastructure to support the capabilities of the US-DOE National Virtual Biotechnology Laboratory in combination with resources from the EU Centre of Excellence in Computational Biomedicine. △ Less","13 October, 2020",https://arxiv.org/pdf/2010.06574
Kartta Labs: Collaborative Time Travel,Sasan Tavakkol;Feng Han;Brandon Mayer;Mark Phillips;Cyrus Shahabi;Yao-Yi Chiang;Raimondas Kiveris,"We introduce the modular and scalable design of Kartta Labs, an open source, open data, and scalable system for virtually reconstructing cities from historical maps and photos. Kartta Labs relies on crowdsourcing and artificial intelligence consisting of two major modules: Maps and 3D models. Each module, in turn, consists of sub-modules that enable the system to reconstruct a city from historical maps and photos. The result is a spatiotemporal reference that can be used to integrate various collected data (curated, sensed, or crowdsourced) for research, education, and entertainment purposes. The system empowers the users to experience collaborative time travel such that they work together to reconstruct the past and experience it on an open source and open data platform. △ Less","6 October, 2020",https://arxiv.org/pdf/2010.06536
"Artificial Intelligence, speech and language processing approaches to monitoring Alzheimer's Disease: a systematic review",Sofia de la Fuente Garcia;Craig Ritchie;Saturnino Luz,"Language is a valuable source of clinical information in Alzheimer's Disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. This paper summarises current findings on the use of artificial intelligence, speech and language processing to predict cognitive decline in the context of Alzheimer's Disease, detailing current research procedures, highlighting their limitations and suggesting strategies to address them. We conducted a systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase) and Web of Science. Bibliographies of relevant papers were screened until December 2019. From 3,654 search results 51 articles were selected against the eligibility criteria. Four tables summarise their findings: study details (aim, population, interventions, comparisons, methods and outcomes), data details (size, type, modalities, annotation, balance, availability and language of study), methodology (pre-processing, feature generation, machine learning, evaluation and results) and clinical applicability (research implications, clinical potential, risk of bias and strengths/limitations). While promising results are reported across nearly all 51 studies, very few have been implemented in clinical research or practice. We concluded that the main limitations of the field are poor standardisation, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Attempts to close these gaps should support translation of future research into clinical practice. △ Less","12 October, 2020",https://arxiv.org/pdf/2010.06047
Chatbot Interaction with Artificial Intelligence: Human Data Augmentation with T5 and Language Transformer Ensemble for Text Classification,Jordan J. Bird;Anikó Ekárt;Diego R. Faria,"In this work, we present the Chatbot Interaction with Artificial Intelligence (CI-AI) framework as an approach to the training of deep learning chatbots for task classification. The intelligent system augments human-sourced data via artificial paraphrasing in order to generate a large set of training data for further classical, attention, and language transformation-based learning approaches for Natural Language Processing. Human beings are asked to paraphrase commands and questions for task identification for further execution of a machine. The commands and questions are split into training and validation sets. A total of 483 responses were recorded. Secondly, the training set is paraphrased by the T5 model in order to augment it with further data. Seven state-of-the-art transformer-based text classification algorithms (BERT, DistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are benchmarked for both sets after fine-tuning on the training data for two epochs. We find that all models are improved when training data is augmented by the T5 model, with an average increase of classification accuracy by 4.01%. The best result was the RoBERTa model trained on T5 augmented data which achieved 98.96% classification accuracy. Finally, we found that an ensemble of the five best-performing transformer models via Logistic Regression of output label predictions led to an accuracy of 99.59% on the dataset of human responses. A highly-performing model allows the intelligent system to interpret human commands at the social-interaction level through a chatbot-like interface (e.g. ""Robot, can we have a conversation?"") and allows for better accessibility to AI by non-technical users. △ Less","22 October, 2020",https://arxiv.org/pdf/2010.05990
Deep Learning for Information Systems Research,Sagar Samtani;Hongyi Zhu;Balaji Padmanabhan;Yidong Chai;Hsinchun Chen,"Artificial Intelligence (AI) has rapidly emerged as a key disruptive technology in the 21st century. At the heart of modern AI lies Deep Learning (DL), an emerging class of algorithms that has enabled today's platforms and organizations to operate at unprecedented efficiency, effectiveness, and scale. Despite significant interest, IS contributions in DL have been limited, which we argue is in part due to issues with defining, positioning, and conducting DL research. Recognizing the tremendous opportunity here for the IS community, this work clarifies, streamlines, and presents approaches for IS scholars to make timely and high-impact contributions. Related to this broader goal, this paper makes five timely contributions. First, we systematically summarize the major components of DL in a novel Deep Learning for Information Systems Research (DL-ISR) schematic that illustrates how technical DL processes are driven by key factors from an application environment. Second, we present a novel Knowledge Contribution Framework (KCF) to help IS scholars position their DL contributions for maximum impact. Third, we provide ten guidelines to help IS scholars generate rigorous and relevant DL-ISR in a systematic, high-quality fashion. Fourth, we present a review of prevailing journal and conference venues to examine how IS scholars have leveraged DL for various research inquiries. Finally, we provide a unique perspective on how IS scholars can formulate DL-ISR inquiries by carefully considering the interplay of business function(s), application areas(s), and the KCF. This perspective intentionally emphasizes inter-disciplinary, intra-disciplinary, and cross-IS tradition perspectives. Taken together, these contributions provide IS scholars a timely framework to advance the scale, scope, and impact of deep learning research. △ Less","7 October, 2020",https://arxiv.org/pdf/2010.05774
Logical Judges Challenge Human Judges on the Strange Case of B.C.-Valjean,Viviana Mascardi;Domenico Pellegrini,"On May 12th, 2020, during the course entitled Artificial Intelligence and Jurisdiction Practice organized by the Italian School of Magistracy, more than 70 magistrates followed our demonstration of a Prolog logical judge reasoning on an armed robbery case. Although the implemented logical judge is just an exercise of knowledge representation and simple deductive reasoning, a practical demonstration of an automated reasoning tool to such a large audience of potential end-users represents a first and unique attempt in Italy and, to the best of our knowledge, in the international panorama. In this paper we present the case addressed by the logical judge - a real case already addressed by a human judge in 2015 - and the feedback on the demonstration collected from the attendees. △ Less","21 September, 2020",https://arxiv.org/pdf/2010.05694
Drawing with AI -- Exploring Collaborative Inking Experiences Based on Mid-air Pointing and Reinforcement Learning,Franziska Geiger;Michelle Martin;Monika Pichlmair;Ilhan Aslan;Hannes Ritschel;Björn Bittner;Elisabeth André,"Digitalization is changing the nature of tools and materials, which are used in artistic practices in professional and non-professional settings. For example, today it is common that even children express their ideas and explore their creativity by drawing on tablets as digital canvases. While there are many software-based tools, which resemble traditional tools, such as various forms of virtual brushes, erasers, etc. in contrast to traditional materials there is potential in augmenting software-based tools and digital canvases with artificial intelligence. Curious about how it would feel to interact with a digital canvas, which would be in contrast to a traditional canvas dynamic, responsive, and potentially able to continuously adapt to its user's input, we developed a drawing application and conducted a qualitative study with 14 users. In this paper, we describe details of our design process, which lead up to using a k-armed bandit as a simple form of reinforcement learning and a LeapMotion sensor to allow people from all walks of like, old and young to draw on pervasive displays, small and large, positioned near or far. △ Less","10 October, 2020",https://arxiv.org/pdf/2010.05047
The emergence of Explainability of Intelligent Systems: Delivering Explainable and Personalised Recommendations for Energy Efficiency,Christos Sardianos;Iraklis Varlamis;Christos Chronis;George Dimitrakopoulos;Abdullah Alsalemi;Yassine Himeur;Faycal Bensaali;Abbes Amira,"The recent advances in artificial intelligence namely in machine learning and deep learning, have boosted the performance of intelligent systems in several ways. This gave rise to human expectations, but also created the need for a deeper understanding of how intelligent systems think and decide. The concept of explainability appeared, in the extent of explaining the internal system mechanics in human terms. Recommendation systems are intelligent systems that support human decision making, and as such, they have to be explainable in order to increase user trust and improve the acceptance of recommendations. In this work, we focus on a context-aware recommendation system for energy efficiency and develop a mechanism for explainable and persuasive recommendations, which are personalized to user preferences and habits. The persuasive facts either emphasize on the economical saving prospects (Econ) or on a positive ecological impact (Eco) and explanations provide the reason for recommending an energy saving action. Based on a study conducted using a Telegram bot, different scenarios have been validated with actual data and human feedback. Current results show a total increase of 19\% on the recommendation acceptance ratio when both economical and ecological persuasive facts are employed. This revolutionary approach on recommendation systems, demonstrates how intelligent recommendations can effectively encourage energy saving behavior. △ Less","26 October, 2020",https://arxiv.org/pdf/2010.04990
Deep RL With Information Constrained Policies: Generalization in Continuous Control,Tailia Malloy;Chris R. Sims;Tim Klinger;Miao Liu;Matthew Riemer;Gerald Tesauro,"Biological agents learn and act intelligently in spite of a highly limited capacity to process and store information. Many real-world problems involve continuous control, which represents a difficult task for artificial intelligence agents. In this paper we explore the potential learning advantages a natural constraint on information flow might confer onto artificial agents in continuous control tasks. We focus on the model-free reinforcement learning (RL) setting and formalize our approach in terms of an information-theoretic constraint on the complexity of learned policies. We show that our approach emerges in a principled fashion from the application of rate-distortion theory. We implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate it within a broader family of RL algorithms such as the Soft Actor Critic (SAC) and Mutual Information Reinforcement Learning (MIRL) algorithm. Our experiments using continuous control tasks show that compared to alternative approaches, CLAC offers improvements in generalization between training and modified test environments. This is achieved in the CLAC model while displaying the high sample efficiency of similar methods. △ Less","9 October, 2020",https://arxiv.org/pdf/2010.04646
Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction,Tom Weber;Stefan Wermter,"Explainable artificial intelligence (XAI) can help foster trust in and acceptance of intelligent and autonomous systems. Moreover, understanding the motivation for an agent's behavior results in better and more successful collaborations between robots and humans. However, not only can humans benefit from a robot's explanation but the robot itself can also benefit from explanations given to him. Currently, most attention is paid to explaining deep neural networks and black-box models. However, a lot of these approaches are not applicable to humanoid robots. Therefore, in this position paper, current problems with adapting XAI methods to explainable neurorobotics are described. Furthermore, NICO, an open-source humanoid robot platform, is introduced and how the interaction of intrinsic explanations by the robot itself and extrinsic explanations provided by the environment enable efficient robotic behavior. △ Less","9 October, 2020",https://arxiv.org/pdf/2010.04602
AI Centered on Scene Fitting and Dynamic Cognitive Network,Feng Chen,"This paper briefly analyzes the advantages and problems of AI mainstream technology and puts forward: To achieve stronger Artificial Intelligence, the end-to-end function calculation must be changed and adopt the technology system centered on scene fitting. It also discusses the concrete scheme named Dynamic Cognitive Network model (DC Net). Discussions : The knowledge and data in the comprehensive domain are uniformly represented by using the rich connection heterogeneous Dynamic Cognitive Network constructed by conceptualized elements; A network structure of two dimensions and multi layers is designed to achieve unified implementation of AI core processing such as combination and generalization; This paper analyzes the implementation differences of computer systems in different scenes, such as open domain, closed domain, significant probability and non-significant probability, and points out that the implementation in open domain and significant probability scene is the key of AI, and a cognitive probability model combining bidirectional conditional probability, probability passing and superposition, probability col-lapse is designed; An omnidirectional network matching-growth algorithm system driven by target and probability is designed to realize the integration of parsing, generating, reasoning, querying, learning and so on; The principle of cognitive network optimization is proposed, and the basic framework of Cognitive Network Learning algorithm (CNL) is designed that structure learning is the primary method and parameter learning is the auxiliary. The logical similarity of implementation between DC Net model and human intelligence is analyzed in this paper. △ Less","2 October, 2020",https://arxiv.org/pdf/2010.04551
Incorporating planning intelligence into deep learning: A planning support tool for street network design,Zhou Fang;Ying Jin;Tianren Yang,"Deep learning applications in shaping ad hoc planning proposals are limited by the difficulty in integrating professional knowledge about cities with artificial intelligence. We propose a novel, complementary use of deep neural networks and planning guidance to automate street network generation that can be context-aware, example-based and user-guided. The model tests suggest that the incorporation of planning knowledge (e.g., road junctions and neighborhood types) in the model training leads to a more realistic prediction of street configurations. Furthermore, the new tool provides both professional and lay users an opportunity to systematically and intuitively explore benchmark proposals for comparisons and further evaluations. △ Less","9 October, 2020",https://arxiv.org/pdf/2010.04536
Artificial intelligence supported anemia control system (AISACS) to prevent anemia in maintenance hemodialysis patients,Toshiaki Ohara;Hiroshi Ikeda;Yoshiki Sugitani;Hiroshi Suito;Viet Quang Huy Huynh;Masaru Kinomura;Soichiro Haraguchi;Kazufumi Sakurama,"Anemia, for which erythropoiesis-stimulating agents (ESAs) and iron supplements (ISs) are used as preventive measures, presents important difficulties for hemodialysis patients. Nevertheless, the number of physicians able to manage such medications appropriately is not keeping pace with the rapid increase of hemodialysis patients. Moreover, the high cost of ESAs imposes heavy burdens on medical insurance systems. An artificial-intelligence-supported anemia control system (AISACS) trained using administration direction data from experienced physicians has been developed by the authors. For the system, appropriate data selection and rectification techniques play important roles. Decision making related to ESAs poses a multi-class classification problem for which a two-step classification technique is introduced. Several validations have demonstrated that AISACS exhibits high performance with correct classification rates of 72-87% and clinically appropriate classification rates of 92-98%. △ Less","5 October, 2020",https://arxiv.org/pdf/2010.03948
Association rules over time,Iztok Fister Jr.;Iztok Fister,"Decisions made nowadays by Artificial Intelligence powered systems are usually hard for users to understand. One of the more important issues faced by developers is exposed as how to create more explainable Machine Learning models. In line with this, more explainable techniques need to be developed, where visual explanation also plays a more important role. This technique could also be applied successfully for explaining the results of Association Rule Mining.This Chapter focuses on two issues: (1) How to discover the relevant association rules, and (2) How to express relations between more attributes visually. For the solution of the first issue, the proposed method uses Differential Evolution, while Sankey diagrams are adopted to solve the second one. This method was applied to a transaction database containing data generated by an amateur cyclist in past seasons, using a mobile device worn during the realization of training sessions that is divided into four time periods. The results of visualization showed that a trend in improving performance of an athlete can be indicated by changing the attributes appearing in the selected association rules in different time periods. △ Less","8 October, 2020",https://arxiv.org/pdf/2010.03834
A novel control mode of bionic morphing tail based on deep reinforcement learning,Liming Zheng;Zhou Zhou;Pengbo Sun;Zhilin Zhang;Rui Wang,"In the field of fixed wing aircraft, many morphing technologies have been applied to the wing, such as adaptive airfoil, variable span aircraft, variable swept angle aircraft, etc., but few are aimed at the tail. The traditional fixed wing tail includes horizontal and vertical tail. Inspired by the bird tail, this paper will introduce a new bionic tail. The tail has a novel control mode, which has multiple control variables. Compared with the traditional fixed wing tail, it adds the area control and rotation control around the longitudinal symmetry axis, so it can control the pitch and yaw of the aircraft at the same time. When the area of the tail changes, the maneuverability and stability of the aircraft can be changed, and the aerodynamic efficiency of the aircraft can also be improved. The aircraft with morphing ability is often difficult to establish accurate mathematical model, because the model has a strong nonlinear, model-based control method is difficult to deal with the strong nonlinear aircraft. In recent years, with the rapid development of artificial intelligence technology, learning based control methods are also brilliant, in which the deep reinforcement learning algorithm can be a good solution to the control object which is difficult to establish model. In this paper, the model-free control algorithm PPO is used to control the tail, and the traditional PID is used to control the aileron and throttle. After training in simulation, the tail shows excellent attitude control ability. △ Less","8 October, 2020",https://arxiv.org/pdf/2010.03814
Vision Skills Needed to Answer Visual Questions,Xiaoyu Zeng;Yanan Wang;Tai-Yin Chiu;Nilavra Bhattacharya;Danna Gurari,"The task of answering questions about images has garnered attention as a practical service for assisting populations with visual impairments as well as a visual Turing test for the artificial intelligence community. Our first aim is to identify the common vision skills needed for both scenarios. To do so, we analyze the need for four vision skills---object recognition, text recognition, color recognition, and counting---on over 27,000 visual questions from two datasets representing both scenarios. We next quantify the difficulty of these skills for both humans and computers on both datasets. Finally, we propose a novel task of predicting what vision skills are needed to answer a question about an image. Our results reveal (mis)matches between aims of real users of such services and the focus of the AI community. We conclude with a discussion about future directions for addressing the visual question answering task. △ Less","7 October, 2020",https://arxiv.org/pdf/2010.03160
Chess as a Testing Grounds for the Oracle Approach to AI Safety,James D. Miller;Roman Yampolskiy;Olle Haggstrom;Stuart Armstrong,"To reduce the danger of powerful super-intelligent AIs, we might make the first such AIs oracles that can only send and receive messages. This paper proposes a possibly practical means of using machine learning to create two classes of narrow AI oracles that would provide chess advice: those aligned with the player's interest, and those that want the player to lose and give deceptively bad advice. The player would be uncertain which type of oracle it was interacting with. As the oracles would be vastly more intelligent than the player in the domain of chess, experience with these oracles might help us prepare for future artificial general intelligence oracles. △ Less","6 October, 2020",https://arxiv.org/pdf/2010.02911
Legal Sentiment Analysis and Opinion Mining (LSAOM): Assimilating Advances in Autonomous AI Legal Reasoning,Lance Eliot,"An expanding field of substantive interest for the theory of the law and the practice-of-law entails Legal Sentiment Analysis and Opinion Mining (LSAOM), consisting of two often intertwined phenomena and actions underlying legal discussions and narratives: (1) Sentiment Analysis (SA) for the detection of expressed or implied sentiment about a legal matter within the context of a legal milieu, and (2) Opinion Mining (OM) for the identification and illumination of explicit or implicit opinion accompaniments immersed within legal discourse. Efforts to undertake LSAOM have historically been performed by human hand and cognition, and only thinly aided in more recent times by the use of computer-based approaches. Advances in Artificial Intelligence (AI) involving especially Natural Language Processing (NLP) and Machine Learning (ML) are increasingly bolstering how automation can systematically perform either or both of Sentiment Analysis and Opinion Mining, all of which is being inexorably carried over into engagement within a legal context for improving LSAOM capabilities. This research paper examines the evolving infusion of AI into Legal Sentiment Analysis and Opinion Mining and proposes an alignment with the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR), plus provides additional insights regarding AI LSAOM in its mechanizations and potential impact to the study of law and the practicing of law. △ Less","2 October, 2020",https://arxiv.org/pdf/2010.02726
Assessing Automated Machine Learning service to detect COVID-19 from X-Ray and CT images: A Real-time Smartphone Application case study,Razib Mustafiz;Khaled Mohsin,"The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a non interventional and sustainable AI solution. Lung disease remains a major healthcare challenge with high morbidity and mortality worldwide. The predominant lung disease was lung cancer. Until recently, the world has witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We have experienced how viral infection of lung and heart claimed thousands of lives worldwide. With the unprecedented advancement of Artificial Intelligence in recent years, Machine learning can be used to easily detect and classify medical imagery. It is much faster and most of the time more accurate than human radiologists. Once implemented, it is more cost-effective and time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive Service to detect and classify COVID19 induced pneumonia from other Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the implication and accuracy of the Automated ML-based Rapid Application Development (RAD) environment in the field of Medical Image diagnosis. This study will better equip us to respond with an ML-based diagnostic Decision Support System(DSS) for a Pandemic situation like COVID19. After optimization, the trained network achieved 96.8% Average Precision which was implemented as a Web Application for consumption. However, the same trained network did not perform the same like Web Application when ported to Smartphone for Real-time inference. Which was our main interest of study. The authors believe, there is scope for further study on this issue. One of the main goal of this study was to develop and evaluate the performance of AI-powered Smartphone-based Real-time Application. Facilitating primary diagnostic services in less equipped and understaffed rural healthcare centers of the world with unreliable internet service. △ Less","3 October, 2020",https://arxiv.org/pdf/2010.02715
Universal Natural Language Processing with Limited Annotations: Try Few-shot Textual Entailment as a Start,Wenpeng Yin;Nazneen Fatema Rajani;Dragomir Radev;Richard Socher;Caiming Xiong,"A standard way to address different NLP problems is by first constructing a problem-specific dataset, then building a model to fit this dataset. To build the ultimate artificial intelligence, we desire a single machine that can handle diverse new problems, for which task-specific annotations are limited. We bring up textual entailment as a unified solver for such NLP problems. However, current research of textual entailment has not spilled much ink on the following questions: (i) How well does a pretrained textual entailment system generalize across domains with only a handful of domain-specific examples? and (ii) When is it worth transforming an NLP task into textual entailment? We argue that the transforming is unnecessary if we can obtain rich annotations for this task. Textual entailment really matters particularly when the target NLP task has insufficient annotations. Universal NLP can be probably achieved through different routines. In this work, we introduce Universal Few-shot textual Entailment (UFO-Entail). We demonstrate that this framework enables a pretrained entailment model to work well on new entailment domains in a few-shot setting, and show its effectiveness as a unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited. Code: https://github.com/salesforce/UniversalFewShotNLP △ Less","6 October, 2020",https://arxiv.org/pdf/2010.02584
"ERFit: Entropic Regression Fit Matlab Package, for Data-Driven System Identification of Underlying Dynamic Equations",Abd AlRahman AlMomani;Erik Bollt,"Data-driven sparse system identification becomes the general framework for a wide range of problems in science and engineering. It is a problem of growing importance in applied machine learning and artificial intelligence algorithms. In this work, we developed the Entropic Regression Software Package (ERFit), a MATLAB package for sparse system identification using the entropic regression method. The code requires minimal supervision, with a wide range of options that make it adapt easily to different problems in science and engineering. The ERFit is available at https://github.com/almomaa/ERFit-Package △ Less","5 October, 2020",https://arxiv.org/pdf/2010.02411
Medical Imaging and Computational Image Analysis in COVID-19 Diagnosis: A Review,Shahabedin Nabavi;Azar Ejmalian;Mohsen Ebrahimi Moghaddam;Ahmad Ali Abin;Alejandro F. Frangi;Mohammad Mohammadi;Hamidreza Saligheh Rad,"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. The disease presents with symptoms such as shortness of breath, fever, dry cough, and chronic fatigue, amongst others. Sometimes the symptoms of the disease increase so much they lead to the death of the patients. The disease may be asymptomatic in some patients in the early stages, which can lead to increased transmission of the disease to others. Many studies have tried to use medical imaging for early diagnosis of COVID-19. This study attempts to review papers on automatic methods for medical image analysis and diagnosis of COVID-19. For this purpose, PubMed, Google Scholar, arXiv and medRxiv were searched to find related studies by the end of April 2020, and the essential points of the collected studies were summarised. The contribution of this study is four-fold: 1) to use as a tutorial of the field for both clinicians and technologists, 2) to comprehensively review the characteristics of COVID-19 as presented in medical images, 3) to examine automated artificial intelligence-based approaches for COVID-19 diagnosis based on the accuracy and the method used, 4) to express the research limitations in this field and the methods used to overcome them. COVID-19 reveals signs in medical images can be used for early diagnosis of the disease even in asymptomatic patients. Using automated machine learning-based methods can diagnose the disease with high accuracy from medical images and reduce time, cost and error of diagnostic procedure. It is recommended to collect bulk imaging data from patients in the shortest possible time to improve the performance of COVID-19 automated diagnostic methods. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.02154
Quantum Bayesian decision-making*,Michael de Oliveira;Luis Soares Barbosa,"As a compact representation of joint probability distributions over a dependence graph of random variables, and a tool for modelling and reasoning in the presence of uncertainty, Bayesian networks are of great importance for artificial intelligence to combine domain knowledge, capture causal relationships, or learn from incomplete datasets. Known as a NP-hard problem in a classical setting, Bayesian inference pops up as a class of algorithms worth to explore in a quantum framework. This paper explores such a research direction and improves on previous proposals by a judicious use of the utility function in an entangled configuration. It proposes a completely quantum mechanical decision-making process with a proven computational advantage. A prototype implementation in Qiskit (a Python-based program development kit for the IBM Q machine) is discussed as a proof-of-concept. △ Less","5 October, 2020",https://arxiv.org/pdf/2010.02088
Self-Supervised Variational Auto-Encoders,Ioannis Gatopoulos;Jakub M. Tomczak,"Density estimation, compression and data generation are crucial tasks in artificial intelligence. Variational Auto-Encoders (VAEs) constitute a single framework to achieve these goals. Here, we present a novel class of generative models, called self-supervised Variational Auto-Encoder (selfVAE), that utilizes deterministic and discrete variational posteriors. This class of models allows to perform both conditional and unconditional sampling, while simplifying the objective function. First, we use a single self-supervised transformation as a latent variable, where a transformation is either downscaling or edge detection. Next, we consider a hierarchical architecture, i.e., multiple transformations, and we show its benefits compared to the VAE. The flexibility of selfVAE in data reconstruction finds a particularly interesting use case in data compression tasks, where we can trade-off memory for better data quality, and vice-versa. We present performance of our approach on three benchmark image data (Cifar10, Imagenette64, and CelebA). △ Less","6 October, 2020",https://arxiv.org/pdf/2010.02014
A Finite-Time Technological Singularity Model With Artificial Intelligence Self-Improvement,Ihor Kendiukhov,"Recent advances in the development of artificial intelligence, technological progress acceleration, long-term trends of macroeconomic dynamics increase the relevance of technological singularity hypothesis. In this paper, we build a model of finite-time technological singularity assuming that artificial intelligence will replace humans for artificial intelligence engineers after some point in time when it is developed enough. This model implies the following: let A be the level of development of artificial intelligence. Then, the moment of technological singularity n is defined as the point in time where artificial intelligence development function approaches infinity. Thus, it happens in finite time. Although infinite level of development of artificial intelligence cannot be reached practically, this approximation is useful for several reasons, firstly because it allows modeling a phase transition or a change of regime. In the model, intelligence growth function appears to be hyperbolic function under relatively broad conditions which we list and compare. Subsequently, we also add a stochastic term (Brownian motion) to the model and investigate the changes in its behavior. The results can be applied for the modeling of dynamics of various processes characterized by multiplicative growth. △ Less","31 August, 2020",https://arxiv.org/pdf/2010.01961
Exploring Semantic Capacity of Terms,Jie Huang;Zilong Wang;Kevin Chen-Chuan Chang;Wen-mei Hwu;Jinjun Xiong,"We introduce and study semantic capacity of terms. For example, the semantic capacity of artificial intelligence is higher than that of linear regression since artificial intelligence possesses a broader meaning scope. Understanding semantic capacity of terms will help many downstream tasks in natural language processing. For this purpose, we propose a two-step model to investigate semantic capacity of terms, which takes a large text corpus as input and can evaluate semantic capacity of terms if the text corpus can provide enough co-occurrence information of terms. Extensive experiments in three fields demonstrate the effectiveness and rationality of our model compared with well-designed baselines and human-level evaluations. △ Less","5 October, 2020",https://arxiv.org/pdf/2010.01898
Data-driven Operation of the Resilient Electric Grid: A Case of COVID-19,Hossein Noorazar;Anurag. k. Srivastava;K. Sadanandan Sajan;Sanjeev Pannala,"Electrical energy is a vital part of modern life, and expectations for grid resilience to allow a continuous and reliable energy supply has tremendously increased even during adverse events (e.g., Ukraine cyber-attack, Hurricane Maria). The global pandemic COVID-19 has raised the electric energy reliability risk due to potential workforce disruptions, supply chain interruptions, and increased possible cybersecurity threats. The pandemic introduces a significant degree of uncertainly to the grid operation in the presence of other extreme events like natural disasters, unprecedented outages, aging power grids, high proliferation of distributed generation, and cyber-attacks. This situation increases the need for measures for the resiliency of power grids to mitigate the impacts of the pandemic as well as simultaneous extreme events. Solutions to manage such an adverse scenario will be multi-fold: a) emergency planning and organizational support, b) following safety protocol, c) utilizing enhanced automation and sensing for situational awareness, and d) integration of advanced technologies and data points for ML-driven enhanced decision support. Enhanced digitalization and automation resulted in better network visibility at various levels, including generation, transmission, and distribution. These data or information can be utilized to take advantage of advanced machine learning techniques for automation and increased power grid resilience. In this paper, a) we review the impact of COVID-19 on power grid operations and actions taken by operators/organizations to minimize the impact of COVID-19, and b) we have presented the recently developed tool and concepts using natural language processing (NLP) in the domain of machine learning and artificial intelligence that can be used for increasing resiliency of power systems in normal and in extreme scenarios such as COVID-19 pandemics. △ Less","26 December, 2020",https://arxiv.org/pdf/2010.01746
Deep Reinforcement Learning for Collaborative Edge Computing in Vehicular Networks,Mushu Li;Jie Gao;Lian Zhao;Xuemin Shen,"Mobile edge computing (MEC) is a promising technology to support mission-critical vehicular applications, such as intelligent path planning and safety applications. In this paper, a collaborative edge computing framework is developed to reduce the computing service latency and improve service reliability for vehicular networks. First, a task partition and scheduling algorithm (TPSA) is proposed to decide the workload allocation and schedule the execution order of the tasks offloaded to the edge servers given a computation offloading strategy. Second, an artificial intelligence (AI) based collaborative computing approach is developed to determine the task offloading, computing, and result delivery policy for vehicles. Specifically, the offloading and computing problem is formulated as a Markov decision process. A deep reinforcement learning technique, i.e., deep deterministic policy gradient, is adopted to find the optimal solution in a complex urban transportation network. By our approach, the service cost, which includes computing service latency and service failure penalty, can be minimized via the optimal workload assignment and server selection in collaborative computing. Simulation results show that the proposed AI-based collaborative computing approach can adapt to a highly dynamic environment with outstanding performance. △ Less","4 October, 2020",https://arxiv.org/pdf/2010.01722
Explainability via Responsibility,Faraz Khadivpour;Matthew Guzdial,"Procedural Content Generation via Machine Learning (PCGML) refers to a group of methods for creating game content (e.g. platformer levels, game maps, etc.) using machine learning models. PCGML approaches rely on black box models, which can be difficult to understand and debug by human designers who do not have expert knowledge about machine learning. This can be even more tricky in co-creative systems where human designers must interact with AI agents to generate game content. In this paper we present an approach to explainable artificial intelligence in which certain training instances are offered to human users as an explanation for the AI agent's actions during a co-creation process. We evaluate this approach by approximating its ability to provide human users with the explanations of AI agent's actions and helping them to more efficiently cooperate with the AI agent. △ Less","4 October, 2020",https://arxiv.org/pdf/2010.01676
Explanation Ontology: A Model of Explanations for User-Centered AI,Shruthi Chari;Oshani Seneviratne;Daniel M. Gruen;Morgan A. Foreman;Amar K. Das;Deborah L. McGuinness,"Explainability has been a goal for Artificial Intelligence (AI) systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide. △ Less","3 October, 2020",https://arxiv.org/pdf/2010.01479
Explanation Ontology in Action: A Clinical Use-Case,Shruthi Chari;Oshani Seneviratne;Daniel M. Gruen;Morgan A. Foreman;Amar K. Das;Deborah L. McGuinness,"We addressed the problem of a lack of semantic representation for user-centric explanations and different explanation types in our Explanation Ontology (https://purl.org/heals/eo). Such a representation is increasingly necessary as explainability has become an important problem in Artificial Intelligence with the emergence of complex methods and an uptake in high-precision and user-facing settings. In this submission, we provide step-by-step guidance for system designers to utilize our ontology, introduced in our resource track paper, to plan and model for explanations during the design of their Artificial Intelligence systems. We also provide a detailed example with our utilization of this guidance in a clinical setting. △ Less","3 October, 2020",https://arxiv.org/pdf/2010.01478
Stuttering Speech Disfluency Prediction using Explainable Attribution Vectors of Facial Muscle Movements,Arun Das;Jeffrey Mock;Henry Chacon;Farzan Irani;Edward Golob;Peyman Najafirad,"Speech disorders such as stuttering disrupt the normal fluency of speech by involuntary repetitions, prolongations and blocking of sounds and syllables. In addition to these disruptions to speech fluency, most adults who stutter (AWS) also experience numerous observable secondary behaviors before, during, and after a stuttering moment, often involving the facial muscles. Recent studies have explored automatic detection of stuttering using Artificial Intelligence (AI) based algorithm from respiratory rate, audio, etc. during speech utterance. However, most methods require controlled environments and/or invasive wearable sensors, and are unable explain why a decision (fluent vs stuttered) was made. We hypothesize that pre-speech facial activity in AWS, which can be captured non-invasively, contains enough information to accurately classify the upcoming utterance as either fluent or stuttered. Towards this end, this paper proposes a novel explainable AI (XAI) assisted convolutional neural network (CNN) classifier to predict near future stuttering by learning temporal facial muscle movement patterns of AWS and explains the important facial muscles and actions involved. Statistical analyses reveal significantly high prevalence of cheek muscles (p<0.005) and lip muscles (p<0.005) to predict stuttering and shows a behavior conducive of arousal and anticipation to speak. The temporal study of these upper and lower facial muscles may facilitate early detection of stuttering, promote automated assessment of stuttering and have application in behavioral therapies by providing automatic non-invasive feedback in realtime. △ Less","2 October, 2020",https://arxiv.org/pdf/2010.01231
Artificial Intelligence Enabled Traffic Monitoring System,Vishal Mandal;Abdul Rashid Mussah;Peng Jin;Yaw Adu-Gyamfi,"Manual traffic surveillance can be a daunting task as Traffic Management Centers operate a myriad of cameras installed over a network. Injecting some level of automation could help lighten the workload of human operators performing manual surveillance and facilitate making proactive decisions which would reduce the impact of incidents and recurring congestion on roadways. This article presents a novel approach to automatically monitor real time traffic footage using deep convolutional neural networks and a stand-alone graphical user interface. The authors describe the results of research received in the process of developing models that serve as an integrated framework for an artificial intelligence enabled traffic monitoring system. The proposed system deploys several state-of-the-art deep learning algorithms to automate different traffic monitoring needs. Taking advantage of a large database of annotated video surveillance data, deep learning-based models are trained to detect queues, track stationary vehicles, and tabulate vehicle counts. A pixel-level segmentation approach is applied to detect traffic queues and predict severity. Real-time object detection algorithms coupled with different tracking systems are deployed to automatically detect stranded vehicles as well as perform vehicular counts. At each stages of development, interesting experimental results are presented to demonstrate the effectiveness of the proposed system. Overall, the results demonstrate that the proposed framework performs satisfactorily under varied conditions without being immensely impacted by environmental hazards such as blurry camera views, low illumination, rain, or snow. △ Less","2 October, 2020",https://arxiv.org/pdf/2010.01217
Syntax Representation in Word Embeddings and Neural Networks -- A Survey,Tomasz Limisiewicz;David Mareček,Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. This indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. We mainly summarize re-search on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks. △ Less,"2 October, 2020",https://arxiv.org/pdf/2010.01063
6G Cellular Networks and Connected Autonomous Vehicles,Jianhua He;Kun Yang;Hsiao-Hwa Chen,"With 5G mobile communication systems been commercially rolled out, research discussions on next generation mobile systems, i.e., 6G, have started. On the other hand, vehicular technologies are also evolving rapidly, from connected vehicles as coined by V2X (vehicle to everything) to autonomous vehicles to the combination of the two, i.e., the networks of connected autonomous vehicles (CAV). How fast the evolution of these two areas will go head-in-head is of great importance, which is the focus of this paper. Based on a brief overview on the technological evolution of V2X to CAV and 6G key technologies, this paper explores two complementary research directions, namely, 6G for CAVs versus CAVs for 6G. The former investigates how various 6G key enablers, such as THz, cell free communication and artificial intelligence (AI), can be utilized to provide CAV mission-critical services. The latter discusses how CAVs can facilitate effective deployment and operation of 6G systems. This paper attempts to investigate the interactions between the two technologies to spark more research efforts in these areas. △ Less","2 October, 2020",https://arxiv.org/pdf/2010.00972
"Artificial Creations: Ascription, Ownership, Time-Specific Monopolies",Raj Shekhar,"Creativity has always been synonymous with humans. No other living species could boast of creativity as humans could. Even the smartest computers thrived only on the ingenious imaginations of its coders. However, that is steadily changing with highly advanced artificially intelligent systems that demonstrate incredible capabilities to autonomously (i.e., with minimal or no human input) produce creative products that would ordinarily deserve intellectual property status if created by a human. These systems could be called artificial creators and their creative products artificial creations. The use of artificial creators is likely to become a part of mainstream production practices in the creative and innovation industries sooner than we realize. When they do, intellectual property regimes (that are inherently designed to reward human creativity) must be sufficiently prepared to aptly respond to the phenomenon of what could be called artificial creativity. Needless to say, any such response must be guided by considerations of public welfare. This study analyzes what that response ought to look like by revisiting the determinants of intellectual property and critiquing its nature and modes. This understanding of intellectual property is then applied to investigate the determinants of intellectual property in artificial creations so as to determine the intrinsic justifications for intellectual property rewards for artificial creativity, and accordingly, develop general modalities for granting intellectual property status to artificial creations. Finally, the treatment of artificial works (i.e., copyrightable artificial creations) and artificial inventions (i.e., patentable artificial creations) by current intellectual property regimes is critiqued, and specific modalities for granting intellectual property status to artificial works and artificial inventions are developed. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00543
Mediating Artificial Intelligence Developments through Negative and Positive Incentives,The Anh Han;Luis Moniz Pereira;Tom Lenaerts;Francisco C. Santos,"The field of Artificial Intelligence (AI) is going through a period of great expectations, introducing a certain level of anxiety in research, business and also policy. This anxiety is further energised by an AI race narrative that makes people believe they might be missing out. Whether real or not, a belief in this narrative may be detrimental as some stake-holders will feel obliged to cut corners on safety precautions, or ignore societal consequences just to ""win"". Starting from a baseline model that describes a broad class of technology races where winners draw a significant benefit compared to others (such as AI advances, patent race, pharmaceutical technologies), we investigate here how positive (rewards) and negative (punishments) incentives may beneficially influence the outcomes. We uncover conditions in which punishment is either capable of reducing the development speed of unsafe participants or has the capacity to reduce innovation through over-regulation. Alternatively, we show that, in several scenarios, rewarding those that follow safety measures may increase the development speed while ensuring safe choices. Moreover, in {the latter} regimes, rewards do not suffer from the issue of over-regulation as is the case for punishment. Overall, our findings provide valuable insights into the nature and kinds of regulatory actions most suitable to improve safety compliance in the contexts of both smooth and sudden technological shifts. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00403
When will the mist clear? On the Interpretability of Machine Learning for Medical Applications: a survey,Antonio-Jesús Banegas-Luna;Jorge Peña-García;Adrian Iftene;Fiorella Guadagni;Patrizia Ferroni;Noemi Scarpato;Fabio Massimo Zanzotto;Andrés Bueno-Crespo;Horacio Pérez-Sánchez,"Artificial Intelligence is providing astonishing results, with medicine being one of its favourite playgrounds. In a few decades, computers may be capable of formulating diagnoses and choosing the correct treatment, while robots may perform surgical operations, and conversational agents could interact with patients as virtual coaches. Machine Learning and, in particular, Deep Neural Networks are behind this revolution. In this scenario, important decisions will be controlled by standalone machines that have learned predictive models from provided data. Among the most challenging targets of interest in medicine are cancer diagnosis and therapies but, to start this revolution, software tools need to be adapted to cover the new requirements. In this sense, learning tools are becoming a commodity in Python and Matlab libraries, just to name two, but to exploit all their possibilities, it is essential to fully understand how models are interpreted and which models are more interpretable than others. In this survey, we analyse current machine learning models, frameworks, databases and other related tools as applied to medicine - specifically, to cancer research - and we discuss their interpretability, performance and the necessary input data. From the evidence available, ANN, LR and SVM have been observed to be the preferred models. Besides, CNNs, supported by the rapid development of GPUs and tensor-oriented programming libraries, are gaining in importance. However, the interpretability of results by doctors is rarely considered which is a factor that needs to be improved. We therefore consider this study to be a timely contribution to the issue. △ Less","1 October, 2020",https://arxiv.org/pdf/2010.00353
Towards Self-learning Edge Intelligence in 6G,Yong Xiao;Guangming Shi;Yingyu Li;Walid Saad;H. Vincent Poor,"Edge intelligence, also called edge-native artificial intelligence (AI), is an emerging technological framework focusing on seamless integration of AI, communication networks, and mobile edge computing. It has been considered to be one of the key missing components in the existing 5G network and is widely recognized to be one of the most sought-after functions for tomorrow's wireless 6G cellular systems. In this article, we identify the key requirements and challenges of edge-native AI in 6G. A self-learning architecture based on self-supervised Generative Adversarial Nets (GANs) is introduced to \blu{demonstrate the potential performance improvement that can be achieved by automatic data learning and synthesizing at the edge of the network}. We evaluate the performance of our proposed self-learning architecture in a university campus shuttle system connected via a 5G network. Our result shows that the proposed architecture has the potential to identify and classify unknown services that emerge in edge computing networks. Future trends and key research problems for self-learning-enabled 6G edge intelligence are also discussed. △ Less","30 September, 2020",https://arxiv.org/pdf/2010.00176
Legal Judgment Prediction (LJP) Amid the Advent of Autonomous AI Legal Reasoning,Lance Eliot,"Legal Judgment Prediction (LJP) is a longstanding and open topic in the theory and practice-of-law. Predicting the nature and outcomes of judicial matters is abundantly warranted, keenly sought, and vigorously pursued by those within the legal industry and also by society as a whole. The tenuous act of generating judicially laden predictions has been limited in utility and exactitude, requiring further advancement. Various methods and techniques to predict legal cases and judicial actions have emerged over time, especially arising via the advent of computer-based modeling. There has been a wide range of approaches attempted, including simple calculative methods to highly sophisticated and complex statistical models. Artificial Intelligence (AI) based approaches have also been increasingly utilized. In this paper, a review of the literature encompassing Legal Judgment Prediction is undertaken, along with innovatively proposing that the advent of AI Legal Reasoning (AILR) will have a pronounced impact on how LJP is performed and its predictive accuracy. Legal Judgment Prediction is particularly examined using the Levels of Autonomy (LoA) of AI Legal Reasoning, plus, other considerations are explored including LJP probabilistic tendencies, biases handling, actor predictors, transparency, judicial reliance, legal case outcomes, and other crucial elements entailing the overarching legal judicial milieu. △ Less","28 September, 2020",https://arxiv.org/pdf/2009.14620
Machine Learning and Computational Mathematics,Weinan E,"Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of ""black box"" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, {can} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics. △ Less","23 September, 2020",https://arxiv.org/pdf/2009.14596
Towards decolonising computational sciences,Abeba Birhane;Olivia Guest,"This article sets out our perspective on how to begin the journey of decolonising computational fields, such as data and cognitive sciences. We see this struggle as requiring two basic steps: a) realisation that the present-day system has inherited, and still enacts, hostile, conservative, and oppressive behaviours and principles towards women of colour (WoC); and b) rejection of the idea that centering individual people is a solution to system-level problems. The longer we ignore these two steps, the more ""our"" academic system maintains its toxic structure, excludes, and harms Black women and other minoritised groups. This also keeps the door open to discredited pseudoscience, like eugenics and physiognomy. We propose that grappling with our fields' histories and heritage holds the key to avoiding mistakes of the past. For example, initiatives such as ""diversity boards"" can still be harmful because they superficially appear reformatory but nonetheless center whiteness and maintain the status quo. Building on the shoulders of many WoC's work, who have been paving the way, we hope to advance the dialogue required to build both a grass-roots and a top-down re-imagining of computational sciences -- including but not limited to psychology, neuroscience, cognitive science, computer science, data science, statistics, machine learning, and artificial intelligence. We aspire for these fields to progress away from their stagnant, sexist, and racist shared past into carving and maintaining an ecosystem where both a diverse demographics of researchers and scientific ideas that critically challenge the status quo are welcomed. △ Less","29 September, 2020",https://arxiv.org/pdf/2009.14258
A Survey on Deep Learning Techniques for Video Anomaly Detection,Jessie James P. Suarez;Prospero C. Naval Jr,"Anomaly detection in videos is a problem that has been studied for more than a decade. This area has piqued the interest of researchers due to its wide applicability. Because of this, there has been a wide array of approaches that have been proposed throughout the years and these approaches range from statistical-based approaches to machine learning-based approaches. Numerous surveys have already been conducted on this area but this paper focuses on providing an overview on the recent advances in the field of anomaly detection using Deep Learning. Deep Learning has been applied successfully in many fields of artificial intelligence such as computer vision, natural language processing and more. This survey, however, focuses on how Deep Learning has improved and provided more insights to the area of video anomaly detection. This paper provides a categorization of the different Deep Learning approaches with respect to their objectives. Additionally, it also discusses the commonly used datasets along with the common evaluation metrics. Afterwards, a discussion synthesizing all of the recent approaches is made to provide direction and possible areas for future research. △ Less","29 September, 2020",https://arxiv.org/pdf/2009.14146
Understanding Human Intelligence through Human Limitations,Thomas L. Griffiths,"Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution. △ Less","29 September, 2020",https://arxiv.org/pdf/2009.14050
The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning,Nuobei Shi;Qin Zeng;Raymond Lee,"In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by GPT-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of XAI (Explainable Artificial Intelligence) to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer learning as back-end to interpret the responses by ontology graph. △ Less","29 September, 2020",https://arxiv.org/pdf/2009.13984
SwiftFace: Real-Time Face Detection,Leonardo Ramos;Bernardo Morales,"Computer vision is a field of artificial intelligence that trains computers to interpret the visual world in a way similar to that of humans. Due to the rapid advancements in technology and the increasing availability of sufficiently large training datasets, the topics within computer vision have experienced a steep growth in the last decade. Among them, one of the most promising fields is face detection. Being used daily in a wide variety of fields; from mobile apps and augmented reality for entertainment purposes, to social studies and security cameras; designing high-performance models for face detection is crucial. On top of that, with the aforementioned growth in face detection technologies, precision and accuracy are no longer the only relevant factors: for real-time face detection, speed of detection is essential. SwiftFace is a novel deep learning model created solely to be a fast face detection model. By focusing only on detecting faces, SwiftFace performs 30% faster than current state-of-the-art face detection models. Code available at https://github.com/leo7r/swiftface △ Less","28 September, 2020",https://arxiv.org/pdf/2009.13743
Breaking the Memory Wall for AI Chip with a New Dimension,Eugene Tam;Shenfei Jiang;Paul Duan;Shawn Meng;Yue Pang;Cayden Huang;Yi Han;Jacke Xie;Yuanjun Cui;Jinsong Yu;Minggui Lu,"Recent advancements in deep learning have led to the widespread adoption of artificial intelligence (AI) in applications such as computer vision and natural language processing. As neural networks become deeper and larger, AI modeling demands outstrip the capabilities of conventional chip architectures. Memory bandwidth falls behind processing power. Energy consumption comes to dominate the total cost of ownership. Currently, memory capacity is insufficient to support the most advanced NLP models. In this work, we present a 3D AI chip, called Sunrise, with near-memory computing architecture to address these three challenges. This distributed, near-memory computing architecture allows us to tear down the performance-limiting memory wall with an abundance of data bandwidth. We achieve the same level of energy efficiency on 40nm technology as competing chips on 7nm technology. By moving to similar technologies as other AI chips, we project to achieve more than ten times the energy efficiency, seven times the performance of the current state-of-the-art chips, and twenty times of memory capacity as compared with the best chip in each benchmark. △ Less","28 September, 2020",https://arxiv.org/pdf/2009.13664
Towards a Measure of Individual Fairness for Deep Learning,Krystal Maughan;Joseph P. Near,"Deep learning has produced big advances in artificial intelligence, but trained neural networks often reflect and amplify bias in their training data, and thus produce unfair predictions. We propose a novel measure of individual fairness, called prediction sensitivity, that approximates the extent to which a particular prediction is dependent on a protected attribute. We show how to compute prediction sensitivity using standard automatic differentiation capabilities present in modern deep learning frameworks, and present preliminary empirical results suggesting that prediction sensitivity may be effective for measuring bias in individual predictions. △ Less","28 September, 2020",https://arxiv.org/pdf/2009.13650
Fair Meta-Learning For Few-Shot Classification,Chen Zhao;Changbin Li;Jincheng Li;Feng Chen,"Artificial intelligence nowadays plays an increasingly prominent role in our life since decisions that were once made by humans are now delegated to automated systems. A machine learning algorithm trained based on biased data, however, tends to make unfair predictions. Developing classification algorithms that are fair with respect to protected attributes of the data thus becomes an important problem. Motivated by concerns surrounding the fairness effects of sharing and few-shot machine learning tools, such as the Model Agnostic Meta-Learning framework, we propose a novel fair fast-adapted few-shot meta-learning approach that efficiently mitigates biases during meta-train by ensuring controlling the decision boundary covariance that between the protected variable and the signed distance from the feature vectors to the decision boundary. Through extensive experiments on two real-world image benchmarks over three state-of-the-art meta-learning algorithms, we empirically demonstrate that our proposed approach efficiently mitigates biases on model output and generalizes both accuracy and fairness to unseen tasks with a limited amount of training samples. △ Less","23 September, 2020",https://arxiv.org/pdf/2009.13516
Artificial Intelligence in Surgery: Neural Networks and Deep Learning,Deepak Alapatt;Pietro Mascagni;Vinkle Srivastav;Nicolas Padoy,"Deep neural networks power most recent successes of artificial intelligence, spanning from self-driving cars to computer aided diagnosis in radiology and pathology. The high-stake data intensive process of surgery could highly benefit from such computational methods. However, surgeons and computer scientists should partner to develop and assess deep learning applications of value to patients and healthcare systems. This chapter and the accompanying hands-on material were designed for surgeons willing to understand the intuitions behind neural networks, become familiar with deep learning concepts and tasks, grasp what implementing a deep learning model in surgery means, and finally appreciate the specific challenges and limitations of deep neural networks in surgery. For the associated hands-on material, please see https://github.com/CAMMA-public/ai4surgery. △ Less","28 September, 2020",https://arxiv.org/pdf/2009.13411
Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities,Tyler J. Shipp;Daniel J. Clouse;Michael J. De Lucia;Metin B. Ahiskali;Kai Steverson;Jonathan M. Mullin;Nathaniel D. Bastian,"Artificial intelligence (AI) and machine learning (ML) have become increasingly vital in the development of novel defense and intelligence capabilities across all domains of warfare. An adversarial AI (A2I) and adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is imperative that AI/ML models can defend against these attacks. A2I/AML defenses will help provide the necessary assurance of these advanced capabilities that use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research and development of assured AI/ML capabilities via new A2I/AML defenses by fostering a collaborative environment across the U.S. Department of Defense and U.S. Intelligence Community. The A2IWG aims to identify specific challenges that it can help solve or address more directly, with initial focus on three topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture Vulnerabilities. △ Less","24 September, 2020",https://arxiv.org/pdf/2009.13250
The Development of Visualization Psychology Analysis Tools to Account for Trust,Rita Borgo;Darren J Edwards,"Defining trust is an important endeavor given its applicability to assessing public mood to much of the innovation in the newly formed autonomous industry, such as artificial intelligence (AI),medical bots, drones, autonomous vehicles, and smart factories [19].Through developing a reliable index or means to measure trust,this may have wide impact from fostering acceptance and adoption of smart systems to informing policy makers about the public atmosphere and willingness to adopt innovate change, and has been identified as an important indicator in a recent UK policy brief [8].In this paper, we reflect on the importance and potential impact of developing Visualization Psychology in the context of solving definitions and policy decision making problems for complex constructs such as ""trust"". △ Less","28 September, 2020",https://arxiv.org/pdf/2009.13200
Persuasion Meets AI: Ethical Considerations for the Design of Social Engineering Countermeasures,Nicolas E. Díaz Ferreyra;Esma Aïmeur;Hicham Hage;Maritta Heisel;Catherine García van Hoogstraten,"Privacy in Social Network Sites (SNSs) like Facebook or Instagram is closely related to people's self-disclosure decisions and their ability to foresee the consequences of sharing personal information with large and diverse audiences. Nonetheless, online privacy decisions are often based on spurious risk judgements that make people liable to reveal sensitive data to untrusted recipients and become victims of social engineering attacks. Artificial Intelligence (AI) in combination with persuasive mechanisms like nudging is a promising approach for promoting preventative privacy behaviour among the users of SNSs. Nevertheless, combining behavioural interventions with high levels of personalization can be a potential threat to people's agency and autonomy even when applied to the design of social engineering countermeasures. This paper elaborates on the ethical challenges that nudging mechanisms can introduce to the development of AI-based countermeasures, particularly to those addressing unsafe self-disclosure practices in SNSs. Overall, it endorses the elaboration of personalized risk awareness solutions as i) an ethical approach to counteract social engineering, and ii) as an effective means for promoting reflective privacy decisions. △ Less","27 September, 2020",https://arxiv.org/pdf/2009.12853
Democratizing Artificial Intelligence in Healthcare: A Study of Model Development Across Two Institutions Incorporating Transfer Learning,Vikash Gupta1;Holger Roth;Varun Buch3;Marcio A. B. C. Rockenbach;Richard D White;Dong Yang;Olga Laur;Brian Ghoshhajra;Ittai Dayan;Daguang Xu;Mona G. Flores;Barbaros Selnur Erdal,"The training of deep learning models typically requires extensive data, which are not readily available as large well-curated medical-image datasets for development of artificial intelligence (AI) models applied in Radiology. Recognizing the potential for transfer learning (TL) to allow a fully trained model from one institution to be fine-tuned by another institution using a much small local dataset, this report describes the challenges, methodology, and benefits of TL within the context of developing an AI model for a basic use-case, segmentation of Left Ventricular Myocardium (LVM) on images from 4-dimensional coronary computed tomography angiography. Ultimately, our results from comparisons of LVM segmentation predicted by a model locally trained using random initialization, versus one training-enhanced by TL, showed that a use-case model initiated by TL can be developed with sparse labels with acceptable performance. This process reduces the time required to build a new model in the clinical environment at a different institution. △ Less","25 September, 2020",https://arxiv.org/pdf/2009.12437
Developing FB Chatbot Based on Deep Learning Using RASA Framework for University Enquiries,Yurio Windiatmoko;Ahmad Fathan Hidayatullah;Ridho Rahmadi,"Smart systems for Universities powered by Artificial Intelligence have been massively developed to help humans in various tasks. The chatbot concept is not something new in today society which is developing with recent technology. College students or candidates of college students often need actual information like asking for something to customer service, especially during this pandemic, when it is difficult to have an immediate face-to-face meeting. Chatbots are functionally helping in several things such as curriculum information, admission for new students, schedule info for any lecture courses, students grade information, and some adding features for Muslim worships schedule, also weather forecast information. This Chatbot is developed by Deep Learning models, which was adopted by an artificial intelligence model that replicates human intelligence with some specific training schemes. This kind of Deep Learning is based on RNN which has some specific memory savings scheme for the Deep Learning Model, specifically this chatbot using LSTM which already integrates by RASA framework. LSTM is also known as Long Short Term Memory which efficiently saves some required memory but will remove some memory that is not needed. This Chatbot uses the FB platform because of the FB users have already reached up to 60.8% of its entire population in Indonesia. Here's the chatbot only focuses on case studies at campus of the Magister Informatics FTI University of Islamic Indonesia. This research is a first stage development within fairly sufficient simulate data. △ Less","25 September, 2020",https://arxiv.org/pdf/2009.12341
Cloud2Edge Elastic AI Framework for Prototyping and Deployment of AI Inference Engines in Autonomous Vehicles,Sorin Grigorescu;Tiberiu Cocias;Bogdan Trasnea;Andrea Margheri;Federico Lombardi;Leonardo Aniello,"Self-driving cars and autonomous vehicles are revolutionizing the automotive sector, shaping the future of mobility altogether. Although the integration of novel technologies such as Artificial Intelligence (AI) and Cloud/Edge computing provides golden opportunities to improve autonomous driving applications, there is the need to modernize accordingly the whole prototyping and deployment cycle of AI components. This paper proposes a novel framework for developing so-called AI Inference Engines for autonomous driving applications based on deep learning modules, where training tasks are deployed elastically over both Cloud and Edge resources, with the purpose of reducing the required network bandwidth, as well as mitigating privacy issues. Based on our proposed data driven V-Model, we introduce a simple yet elegant solution for the AI components development cycle, where prototyping takes place in the cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment and evaluation on the target ECUs (Electronic Control Units) is performed as Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework is demonstrated using two real-world use-cases of AI inference engines for autonomous vehicles, that is environment perception and most probable path prediction. △ Less","23 September, 2020",https://arxiv.org/pdf/2009.11722
Principles and Practice of Explainable Machine Learning,Vaishak Belle;Ioannis Papantonis,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with significant challenges: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods -- machine learning (ML) and pattern recognition models in particular -- so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature, or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. △ Less","18 September, 2020",https://arxiv.org/pdf/2009.11698
The Next Era of American Law Amid the Advent of Autonomous AI Legal Reasoning,Lance Eliot,"Legal scholars have postulated that there have been three eras of American law to-date, consisting in chronological order of the initial Age of Discovery, the Age of Faith, and then the Age of Anxiety. An open question that has received erudite attention in legal studies is what the next era, the fourth era, might consist of, and for which various proposals exist including examples such as the Age of Consent, the Age of Information, etc. There is no consensus in the literature as yet on what the fourth era is, and nor whether the fourth era has already begun or will instead emerge in the future. This paper examines the potential era-elucidating impacts amid the advent of autonomous Artificial Intelligence Legal Reasoning (AILR), entailing whether such AILR will be an element of a fourth era or a driver of a fourth, fifth, or perhaps the sixth era of American law. Also, a set of meta-characteristics about the means of identifying a legal era changeover are introduced, along with an innovative discussion of the role entailing legal formalism versus legal realism in the emergence of the American law eras. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.11647
Pandora: A Cyber Range Environment for the Safe Testing and Deployment of Autonomous Cyber Attack Tools,Hetong Jiang;Taejun Choi;Ryan K. L. Ko,"Cybersecurity tools are increasingly automated with artificial intelligent (AI) capabilities to match the exponential scale of attacks, compensate for the relatively slower rate of training new cybersecurity talents, and improve of the accuracy and performance of both tools and users. However, the safe and appropriate usage of autonomous cyber attack tools - especially at the development stages for these tools - is still largely an unaddressed gap. Our survey of current literature and tools showed that most of the existing cyber range designs are mostly using manual tools and have not considered augmenting automated tools or the potential security issues caused by the tools. In other words, there is still room for a novel cyber range design which allow security researchers to safely deploy autonomous tools and perform automated tool testing if needed. In this paper, we introduce Pandora, a safe testing environment which allows security researchers and cyber range users to perform experiments on automated cyber attack tools that may have strong potential of usage and at the same time, a strong potential for risks. Unlike existing testbeds and cyber ranges which have direct compatibility with enterprise computer systems and the potential for risk propagation across the enterprise network, our test system is intentionally designed to be incompatible with enterprise real-world computing systems to reduce the risk of attack propagation into actual infrastructure. Our design also provides a tool to convert in-development automated cyber attack tools into to executable test binaries for validation and usage realistic enterprise system environments if required. Our experiments tested automated attack tools on our proposed system to validate the usability of our proposed environment. Our experiments also proved the safety of our environment by compatibility testing using simple malicious code. △ Less","24 September, 2020",https://arxiv.org/pdf/2009.11484
Collective and synchronous dynamics of photonic spiking neurons,Takahiro Inagaki;Kensuke Inaba;Timothée Leleu;Toshimori Honjo;Takuya Ikuta;Koji Enbutsu;Takeshi Umeki;Ryoichi Kasahara;Kazuyuki Aihara;Hiroki Takesue,"Nonlinear dynamics of spiking neural networks has recently attracted much interest as an approach to understand possible information processing in the brain and apply it to artificial intelligence. Since information can be processed by collective spiking dynamics of neurons, the fine control of spiking dynamics is desirable for neuromorphic devices. Here we show that photonic spiking neurons implemented with paired nonlinear optical oscillators can be controlled to generate two modes of bio-realistic spiking dynamics by changing the optical pump amplitude. When they are coupled in a network, we found that the interaction between the photonic neurons induces an effective change in the pump amplitude depending on the order parameter that characterizes synchronization. The experimental results show that the effective change causes spontaneous modification of the spiking modes and firing rates of clustered neurons, and such collective dynamics can be utilized to realize efficient heuristics for solving NP-hard combinatorial optimization problems. △ Less","23 September, 2020",https://arxiv.org/pdf/2009.11454
The Agent Web Model -- Modelling web hacking for reinforcement learning,Laszlo Erdodi;Fabio Massimo Zennaro,"Website hacking is a frequent attack type used by malicious actors to obtain confidential information, modify the integrity of web pages or make websites unavailable. The tools used by attackers are becoming more and more automated and sophisticated, and malicious machine learning agents seems to be the next development in this line. In order to provide ethical hackers with similar tools, and to understand the impact and the limitations of artificial agents, we present in this paper a model that formalizes web hacking tasks for reinforcement learning agents. Our model, named Agent Web Model, considers web hacking as a capture-the-flag style challenge, and it defines reinforcement learning problems at seven different levels of abstraction. We discuss the complexity of these problems in terms of actions and states an agent has to deal with, and we show that such a model allows to represent most of the relevant web vulnerabilities. Aware that the driver of advances in reinforcement learning is the availability of standardized challenges, we provide an implementation for the first three abstraction layers, in the hope that the community would consider these challenges in order to develop intelligent web hacking agents. △ Less","23 September, 2020",https://arxiv.org/pdf/2009.11274
Enterprise AI Canvas -- Integrating Artificial Intelligence into Business,U. Kerzel,"Artificial Intelligence (AI) and Machine Learning have enormous potential to transform businesses and disrupt entire industry sectors. However, companies wishing to integrate algorithmic decisions into their face multiple challenges: They have to identify use-cases in which artificial intelligence can create value, as well as decisions that can be supported or executed automatically. Furthermore, the organization will need to be transformed to be able to integrate AI based systems into their human work-force. Furthermore, the more technical aspects of the underlying machine learning model have to be discussed in terms of how they impact the various units of a business: Where do the relevant data come from, which constraints have to be considered, how is the quality of the data and the prediction evaluated? The Enterprise AI canvas is designed to bring Data Scientist and business expert together to discuss and define all relevant aspects which need to be clarified in order to integrate AI based systems into a digital enterprise. It consists of two parts where part one focuses on the business view and organizational aspects, whereas part two focuses on the underlying machine learning model and the data it uses. △ Less","18 September, 2020",https://arxiv.org/pdf/2009.11190
AI and Legal Argumentation: Aligning the Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Legal argumentation is a vital cornerstone of justice, underpinning an adversarial form of law, and extensive research has attempted to augment or undertake legal argumentation via the use of computer-based automation including Artificial Intelligence (AI). AI advances in Natural Language Processing (NLP) and Machine Learning (ML) have especially furthered the capabilities of leveraging AI for aiding legal professionals, doing so in ways that are modeled here as CARE, namely Crafting, Assessing, Refining, and Engaging in legal argumentation. In addition to AI-enabled legal argumentation serving to augment human-based lawyering, an aspirational goal of this multi-disciplinary field consists of ultimately achieving autonomously effected human-equivalent legal argumentation. As such, an innovative meta-approach is proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to the maturation of AI and Legal Argumentation (AILA), proffering a new means of gauging progress in this ever-evolving and rigorously sought domain. △ Less","11 September, 2020",https://arxiv.org/pdf/2009.11180
AI assisted Malware Analysis: A Course for Next Generation Cybersecurity Workforce,Maanak Gupta;Sudip Mittal;Mahmoud Abdelsalam,"The use of Artificial Intelligence (AI) and Machine Learning (ML) to solve cybersecurity problems has been gaining traction within industry and academia, in part as a response to widespread malware attacks on critical systems, such as cloud infrastructures, government offices or hospitals, and the vast amounts of data they generate. AI- and ML-assisted cybersecurity offers data-driven automation that could enable security systems to identify and respond to cyber threats in real time. However, there is currently a shortfall of professionals trained in AI and ML for cybersecurity. Here we address the shortfall by developing lab-intensive modules that enable undergraduate and graduate students to gain fundamental and advanced knowledge in applying AI and ML techniques to real-world datasets to learn about Cyber Threat Intelligence (CTI), malware analysis, and classification, among other important topics in cybersecurity. Here we describe six self-contained and adaptive modules in ""AI-assisted Malware Analysis."" Topics include: (1) CTI and malware attack stages, (2) malware knowledge representation and CTI sharing, (3) malware data collection and feature identification, (4) AI-assisted malware detection, (5) malware classification and attribution, and (6) advanced malware research topics and case studies such as adversarial learning and Advanced Persistent Threat (APT) detection. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.11101
Engaging Teachers to Co-Design Integrated AI Curriculum for K-12 Classrooms,Jessica Van Brummelen;Phoebe Lin,"Artificial Intelligence (AI) education is an increasingly popular topic area for K-12 teachers. However, little research has investigated how AI education can be designed to be more accessible to all learners. We organized co-design workshops with 15 K-12 teachers to identify opportunities to integrate AI education into core curriculum to leverage learners' interests. During the co-design workshops, teachers and researchers co-created lesson plans where AI concepts were embedded into various core subjects. We found that K-12 teachers need additional scaffolding in the curriculum to facilitate ethics and data discussions, and value supports for learner engagement, collaboration, and reflection. We identify opportunities for researchers and teachers to collaborate to make AI education more accessible, and present an exemplar lesson plan that shows entry points for teaching AI in non-computing subjects. We also reflect on co-designing with K-12 teachers in a remote setting. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.11100
When Deep Reinforcement Learning Meets Federated Learning: Intelligent Multi-Timescale Resource Management for Multi-access Edge Computing in 5G Ultra Dense Network,Shuai Yu;Xu Chen;Zhi Zhou;Xiaowen Gong;Di Wu,"Ultra-dense edge computing (UDEC) has great potential, especially in the 5G era, but it still faces challenges in its current solutions, such as the lack of: i) efficient utilization of multiple 5G resources (e.g., computation, communication, storage and service resources); ii) low overhead offloading decision making and resource allocation strategies; and iii) privacy and security protection schemes. Thus, we first propose an intelligent ultra-dense edge computing (I-UDEC) framework, which integrates blockchain and Artificial Intelligence (AI) into 5G ultra-dense edge computing networks. First, we show the architecture of the framework. Then, in order to achieve real-time and low overhead computation offloading decisions and resource allocation strategies, we design a novel two-timescale deep reinforcement learning (\textit{2Ts-DRL}) approach, consisting of a fast-timescale and a slow-timescale learning process, respectively. The primary objective is to minimize the total offloading delay and network resource usage by jointly optimizing computation offloading, resource allocation and service caching placement. We also leverage federated learning (FL) to train the \textit{2Ts-DRL} model in a distributed manner, aiming to protect the edge devices' data privacy. Simulation results corroborate the effectiveness of both the \textit{2Ts-DRL} and FL in the I-UDEC framework and prove that our proposed algorithm can reduce task execution time up to 31.87%. △ Less","22 September, 2020",https://arxiv.org/pdf/2009.10601
Privacy Preserving K-Means Clustering: A Secure Multi-Party Computation Approach,Daniel Hurtado Ramírez;J. M. Auñón,"Knowledge discovery is one of the main goals of Artificial Intelligence. This Knowledge is usually stored in databases spread in different environments, being a tedious (or impossible) task to access and extract data from them. To this difficulty we must add that these datasources may contain private data, therefore the information can never leave the source. Privacy Preserving Machine Learning (PPML) helps to overcome this difficulty, employing cryptographic techniques, allowing knowledge discovery while ensuring data privacy. K-means is one of the data mining techniques used in order to discover knowledge, grouping data points in clusters that contain similar features. This paper focuses in Privacy Preserving Machine Learning applied to K-means using recent protocols from the field of criptography. The algorithm is applied to different scenarios where data may be distributed either horizontally or vertically. △ Less","22 September, 2020",https://arxiv.org/pdf/2009.10453
Model-Driven Requirements for Humans-on-the-Loop Multi-UAV Missions,Ankit Agrawal;Jan-Philipp Steghofer;Jane Cleland-Huang,"The use of semi-autonomous Unmanned Aerial Vehicles (UAVs or drones) to support emergency response scenarios, such as fire surveillance and search-and-rescue, has the potential for huge societal benefits. Onboard sensors and artificial intelligence (AI) allow these UAVs to operate autonomously in the environment. However, human intelligence and domain expertise are crucial in planning and guiding UAVs to accomplish the mission. Therefore, humans and multiple UAVs need to collaborate as a team to conduct a time-critical mission successfully. We propose a meta-model to describe interactions among the human operators and the autonomous swarm of UAVs. The meta-model also provides a language to describe the roles of UAVs and humans and the autonomous decisions. We complement the meta-model with a template of requirements elicitation questions to derive models for specific missions. We also identify common scenarios where humans should collaborate with UAVs to augment the autonomy of the UAVs. We introduce the meta-model and the requirements elicitation process with examples drawn from a search-and-rescue mission in which multiple UAVs collaborate with humans to respond to the emergency. We then apply it to a second scenario in which UAVs support first responders in fighting a structural fire. Our results show that the meta-model and the template of questions support the modeling of the human-on-the-loop human interactions for these complex missions, suggesting that it is a useful tool for modeling the human-on-the-loop interactions for multi-UAVs missions. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.10267
Extending Answer Set Programs with Neural Networks,Zhun Yang,"The integration of low-level perception with high-level reasoning is one of the oldest problems in Artificial Intelligence. Recently, several proposals were made to implement the reasoning process in complex neural network architectures. While these works aim at extending neural networks with the capability of reasoning, a natural question that we consider is: can we extend answer set programs with neural networks to allow complex and high-level reasoning on neural network outputs? As a preliminary result, we propose NeurASP -- a simple extension of answer set programs by embracing neural networks where neural network outputs are treated as probability distributions over atomic facts in answer set programs. We show that NeurASP can not only improve the perception accuracy of a pre-trained neural network, but also help to train a neural network better by giving restrictions through logic rules. However, training with NeurASP would take much more time than pure neural network training due to the internal use of a symbolic reasoning engine. For future work, we plan to investigate the potential ways to solve the scalability issue of NeurASP. One potential way is to embed logic programs directly in neural networks. On this route, we plan to first design a SAT solver using neural networks, then extend such a solver to allow logic programs. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.10256
Constraint Programming Algorithms for Route Planning Exploiting Geometrical Information,Alessandro Bertagnon,"Problems affecting the transport of people or goods are plentiful in industry and commerce and they also appear to be at the origin of much more complex problems. In recent years, the logistics and transport sector keeps growing supported by technological progress, i.e. companies to be competitive are resorting to innovative technologies aimed at efficiency and effectiveness. This is why companies are increasingly using technologies such as Artificial Intelligence (AI), Blockchain and Internet of Things (IoT). Artificial intelligence, in particular, is often used to solve optimization problems in order to provide users with the most efficient ways to exploit available resources. In this work we present an overview of our current research activities concerning the development of new algorithms, based on CLP techniques, for route planning problems exploiting the geometric information intrinsically present in many of them or in some of their variants. The research so far has focused in particular on the Euclidean Traveling Salesperson Problem (Euclidean TSP) with the aim to exploit the results obtained also to other problems of the same category, such as the Euclidean Vehicle Routing Problem (Euclidean VRP), in the future. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.10253
"Designing AI Learning Experiences for K-12: Emerging Works, Future Opportunities and a Design Framework",Xiaofei Zhou;Jessica Van Brummelen;Phoebe Lin,"Artificial intelligence (AI) literacy is a rapidly growing research area and a critical addition to K-12 education. However, support for designing tools and curriculum to teach K-12 AI literacy is still limited. There is a need for additional interdisciplinary human-computer interaction and education research investigating (1) how general AI literacy is currently implemented in learning experiences and (2) what additional guidelines are required to teach AI literacy in specifically K-12 learning contexts. In this paper, we analyze a collection of K-12 AI and education literature to show how core competencies of AI literacy are applied successfully and organize them into an educator-friendly chart to enable educators to efficiently find appropriate resources for their classrooms. We also identify future opportunities and K-12 specific design guidelines, which we synthesized into a conceptual framework to support researchers, designers, and educators in creating K-12 AI learning experiences. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.10228
Exploring Instance Generation for Automated Planning,Özgür Akgün;Nguyen Dang;Joan Espasa;Ian Miguel;András Z. Salamon;Christopher Stone,"Many of the core disciplines of artificial intelligence have sets of standard benchmark problems well known and widely used by the community when developing new algorithms. Constraint programming and automated planning are examples of these areas, where the behaviour of a new algorithm is measured by how it performs on these instances. Typically the efficiency of each solving method varies not only between problems, but also between instances of the same problem. Therefore, having a diverse set of instances is crucial to be able to effectively evaluate a new solving method. Current methods for automatic generation of instances for Constraint Programming problems start with a declarative model and search for instances with some desired attributes, such as hardness or size. We first explore the difficulties of adapting this approach to generate instances starting from problem specifications written in PDDL, the de-facto standard language of the automated planning community. We then propose a new approach where the whole planning problem description is modelled using Essence, an abstract modelling language that allows expressing high-level structures without committing to a particular low level representation in PDDL. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.10156
Clinical trial of an AI-augmented intervention for HIV prevention in youth experiencing homelessness,Bryan Wilder;Laura Onasch-Vera;Graham Diguiseppi;Robin Petering;Chyna Hill;Amulya Yadav;Eric Rice;Milind Tambe,"Youth experiencing homelessness (YEH) are subject to substantially greater risk of HIV infection, compounded both by their lack of access to stable housing and the disproportionate representation of youth of marginalized racial, ethnic, and gender identity groups among YEH. A key goal for health equity is to improve adoption of protective behaviors in this population. One promising strategy for intervention is to recruit peer leaders from the population of YEH to promote behaviors such as condom usage and regular HIV testing to their social contacts. This raises a computational question: which youth should be selected as peer leaders to maximize the overall impact of the intervention? We developed an artificial intelligence system to optimize such social network interventions in a community health setting. We conducted a clinical trial enrolling 713 YEH at drop-in centers in a large US city. The clinical trial compared interventions planned with the algorithm to those where the highest-degree nodes in the youths' social network were recruited as peer leaders (the standard method in public health) and to an observation-only control group. Results from the clinical trial show that youth in the AI group experience statistically significant reductions in key risk behaviors for HIV transmission, while those in the other groups do not. This provides, to our knowledge, the first empirical validation of the usage of AI methods to optimize social network interventions for health. We conclude by discussing lessons learned over the course of the project which may inform future attempts to use AI in community-level interventions. △ Less","6 November, 2020",https://arxiv.org/pdf/2009.09559
Stochastic Gradient Langevin Dynamics Algorithms with Adaptive Drifts,Sehwan Kim;Qifan Song;Faming Liang,"Bayesian deep learning offers a principled way to address many issues concerning safety of artificial intelligence (AI), such as model uncertainty,model interpretability, and prediction bias. However, due to the lack of efficient Monte Carlo algorithms for sampling from the posterior of deep neural networks (DNNs), Bayesian deep learning has not yet powered our AI system. We propose a class of adaptive stochastic gradient Markov chain Monte Carlo (SGMCMC) algorithms, where the drift function is biased to enhance escape from saddle points and the bias is adaptively adjusted according to the gradient of past samples. We establish the convergence of the proposed algorithms under mild conditions, and demonstrate via numerical examples that the proposed algorithms can significantly outperform the existing SGMCMC algorithms, such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian Monte Carlo (SGHMC) and preconditioned SGLD, in both simulation and optimization tasks. △ Less","20 September, 2020",https://arxiv.org/pdf/2009.09535
Measurement in AI Policy: Opportunities and Challenges,Saurabh Mishra;Jack Clark;C. Raymond Perrault,"As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area. △ Less","10 September, 2020",https://arxiv.org/pdf/2009.09071
Is there a role for statistics in artificial intelligence?,Sarah Friedrich;Gerd Antes;Sigrid Behr;Harald Binder;Werner Brannath;Florian Dumpert;Katja Ickstadt;Hans Kestler;Johannes Lederer;Heinz Leitgöb;Markus Pauly;Ansgar Steland;Adalbert Wilhelm;Tim Friede,"The research on and application of artificial intelligence (AI) has triggered a comprehensive scientific, economic, social and political discussion. Here we argue that statistics, as an interdisciplinary scientific field, plays a substantial role both for the theoretical and practical understanding of AI and for its future development. Statistics might even be considered a core element of AI. With its specialist knowledge of data evaluation, starting with the precise formulation of the research question and passing through a study design stage on to analysis and interpretation of the results, statistics is a natural partner for other disciplines in teaching, research and practice. This paper aims at contributing to the current discussion by highlighting the relevance of statistical methodology in the context of AI development. In particular, we discuss contributions of statistics to the field of artificial intelligence concerning methodological development, planning and design of studies, assessment of data quality and data collection, differentiation of causality and associations and assessment of uncertainty in results. Moreover, the paper also deals with the equally necessary and meaningful extension of curricula in schools and universities. △ Less","13 September, 2020",https://arxiv.org/pdf/2009.09070
Hacking with God: a Common Programming Language of Robopsychology and Robophilosophy,Norbert Bátfai,"This note is a sketch of how the concept of robopsychology and robophilosophy could be reinterpreted and repositioned in the spirit of the original vocation of psychology and philosophy. The notion of the robopsychology as a fictional science and a fictional occupation was introduced by Asimov in the middle of the last century. The robophilosophy, on the other hand, is only a few years old today. But at this moment, none of these new emerging disciplines focus on the fundamental and overall issues of the development of artificial general intelligence. Instead, they focus only on issues that, although are extremely important, play a complementary role, such as moral or ethical ones, rather than the big questions of life. We try to outline a conception in which the robophilosophy and robopsychology will be able to play a similar leading rule in the progress of artificial intelligence than the philosophy and psychology have done in the progress of human intelligence. To facilitate this, we outline the idea of a visual artificial language and interactive theorem prover-based computer application called Prime Convo Assistant. The question to be decided in the future is whether we can develop such an application. And if so, can we build a computer game on it, or even an esport game? It may be an interesting question in order for this game will be able to transform human thinking on the widest possible social scale and will be able to develop a standard mathematical logic-based communication channel between human and machine intelligence. △ Less","16 September, 2020",https://arxiv.org/pdf/2009.09068
On the spatiotemporal behavior in biology-mimicking computing systems,János Végh;Ádám J. Berki,"The payload performance of conventional computing systems, from single processors to supercomputers, reached its limits the nature enables. Both the growing demand to cope with ""big data"" (based on, or assisted by, artificial intelligence) and the interest in understanding the operation of our brain more completely, stimulated the efforts to build biology-mimicking computing systems from inexpensive conventional components and build different (""neuromorphic"") computing systems. On one side, those systems require an unusually large number of processors, which introduces performance limitations and nonlinear scaling. On the other side, the neuronal operation drastically differs from the conventional workloads. The conventional computing (including both its mathematical background and physical implementation) is based on assuming instant interaction, while the biological neuronal systems have a ""spatiotemporal"" behavior. This difference alone makes imitating biological behavior in technical implementation hard. Besides, the recent issues in computing called the attention to that the temporal behavior is a general feature of computing systems, too. Some of their effects in both biological and technical systems were already noticed. Nevertheless, handling of those issues is incomplete/improper. Introducing temporal logic, based on the Minkowski transform, gives quantitative insight into the operation of both kinds of computing systems, furthermore provides a natural explanation of decades-old empirical phenomena. Without considering their temporal behavior correctly, neither effective implementation nor a true imitation of biological neural systems are possible. △ Less","23 September, 2020",https://arxiv.org/pdf/2009.08841
Learning Emotional-Blinded Face Representations,Alejandro Peña;Julian Fierrez;Agata Lapedriza;Aythami Morales,"We propose two face representations that are blind to facial expressions associated to emotional responses. This work is in part motivated by new international regulations for personal data protection, which enforce data controllers to protect any kind of sensitive information involved in automatic processes. The advances in Affective Computing have contributed to improve human-machine interfaces but, at the same time, the capacity to monitorize emotional responses triggers potential risks for humans, both in terms of fairness and privacy. We propose two different methods to learn these expression-blinded facial features. We show that it is possible to eliminate information related to emotion recognition tasks, while the performance of subject verification, gender recognition, and ethnicity classification are just slightly affected. We also present an application to train fairer classifiers in a case study of attractiveness classification with respect to a protected facial expression attribute. The results demonstrate that it is possible to reduce emotional information in the face representation while retaining competitive performance in other face-based artificial intelligence tasks. △ Less","18 September, 2020",https://arxiv.org/pdf/2009.08704
Optimal Sepsis Patient Treatment using Human-in-the-loop Artificial Intelligence,Akash Gupta;Michael T. Lash;Senthil K. Nachimuthu,"Sepsis is one of the leading causes of death in Intensive Care Units (ICU). The strategy for treating sepsis involves the infusion of intravenous (IV) fluids and administration of antibiotics. Determining the optimal quantity of IV fluids is a challenging problem due to the complexity of a patient's physiology. In this study, we develop a data-driven optimization solution that derives the optimal quantity of IV fluids for individual patients. The proposed method minimizes the probability of severe outcomes by controlling the prescribed quantity of IV fluids and utilizes human-in-the-loop artificial intelligence. We demonstrate the performance of our model on 1122 ICU patients with sepsis diagnosis extracted from the MIMIC-III dataset. The results show that, on average, our model can reduce mortality by 22%. This study has the potential to help physicians synthesize optimal, patient-specific treatment strategies. △ Less","16 September, 2020",https://arxiv.org/pdf/2009.07963
Malicious Network Traffic Detection via Deep Learning: An Information Theoretic View,Erick Galinkin,"The attention that deep learning has garnered from the academic community and industry continues to grow year over year, and it has been said that we are in a new golden age of artificial intelligence research. However, neural networks are still often seen as a ""black box"" where learning occurs but cannot be understood in a human-interpretable way. Since these machine learning systems are increasingly being adopted in security contexts, it is important to explore these interpretations. We consider an Android malware traffic dataset for approaching this problem. Then, using the information plane, we explore how homeomorphism affects learned representation of the data and the invariance of the mutual information captured by the parameters on that data. We empirically validate these results, using accuracy as a second measure of similarity of learned representations. Our results suggest that although the details of learned representations and the specific coordinate system defined over the manifold of all parameters differ slightly, the functional approximations are the same. Furthermore, our results show that since mutual information remains invariant under homeomorphism, only feature engineering methods that alter the entropy of the dataset will change the outcome of the neural network. This means that for some datasets and tasks, neural networks require meaningful, human-driven feature engineering or changes in architecture to provide enough information for the neural network to generate a sufficient statistic. Applying our results can serve to guide analysis methods for machine learning engineers and suggests that neural networks that can exploit the convolution theorem are equally accurate as standard convolutional neural networks, and can be more computationally efficient. △ Less","16 September, 2020",https://arxiv.org/pdf/2009.07753
Better Model Selection with a new Definition of Feature Importance,Fan Fang;Carmine Ventre;Lingbo Li;Leslie Kanthan;Fan Wu;Michail Basios,"Feature importance aims at measuring how crucial each input feature is for model prediction. It is widely used in feature engineering, model selection and explainable artificial intelligence (XAI). In this paper, we propose a new tree-model explanation approach for model selection. Our novel concept leverages the Coefficient of Variation of a feature weight (measured in terms of the contribution of the feature to the prediction) to capture the dispersion of importance over samples. Extensive experimental results show that our novel feature explanation performs better than general cross validation method in model selection both in terms of time efficiency and accuracy performance. △ Less","16 September, 2020",https://arxiv.org/pdf/2009.07708
Too Much Information Kills Information: A Clustering Perspective,Yicheng Xu;Vincent Chau;Chenchen Wu;Yong Zhang;Vassilis Zissimopoulos;Yifei Zou,"Clustering is one of the most fundamental tools in the artificial intelligence area, particularly in the pattern recognition and learning theory. In this paper, we propose a simple, but novel approach for variance-based k-clustering tasks, included in which is the widely known k-means clustering. The proposed approach picks a sampling subset from the given dataset and makes decisions based on the data information in the subset only. With certain assumptions, the resulting clustering is provably good to estimate the optimum of the variance-based objective with high probability. Extensive experiments on synthetic datasets and real-world datasets show that to obtain competitive results compared with k-means method (Llyod 1982) and k-means++ method (Arthur and Vassilvitskii 2007), we only need 7% information of the dataset. If we have up to 15% information of the dataset, then our algorithm outperforms both the k-means method and k-means++ method in at least 80% of the clustering tasks, in terms of the quality of clustering. Also, an extended algorithm based on the same idea guarantees a balanced k-clustering result. △ Less","15 September, 2020",https://arxiv.org/pdf/2009.07417
Creation and Validation of a Chest X-Ray Dataset with Eye-tracking and Report Dictation for AI Development,Alexandros Karargyris;Satyananda Kashyap;Ismini Lourentzou;Joy Wu;Arjun Sharma;Matthew Tong;Shafiq Abedin;David Beymer;Vandana Mukherjee;Elizabeth A Krupinski;Mehdi Moradi,"We developed a rich dataset of Chest X-Ray (CXR) images to assist investigators in artificial intelligence. The data were collected using an eye tracking system while a radiologist reviewed and reported on 1,083 CXR images. The dataset contains the following aligned data: CXR image, transcribed radiology report text, radiologist's dictation audio and eye gaze coordinates data. We hope this dataset can contribute to various areas of research particularly towards explainable and multimodal deep learning / machine learning methods. Furthermore, investigators in disease classification and localization, automated radiology report generation, and human-machine interaction can benefit from these data. We report deep learning experiments that utilize the attention maps produced by eye gaze dataset to show the potential utility of this data. △ Less","8 October, 2020",https://arxiv.org/pdf/2009.07386
Image Based Artificial Intelligence in Wound Assessment: A Systematic Review,D. M. Anisuzzaman;Chuanbo Wang;Behrouz Rostami;Sandeep Gopalakrishnan;Jeffrey Niezgoda;Zeyun Yu,"Efficient and effective assessment of acute and chronic wounds can help wound care teams in clinical practice to greatly improve wound diagnosis, optimize treatment plans, ease the workload and achieve health related quality of life to the patient population. While artificial intelligence (AI) has found wide applications in health-related sciences and technology, AI-based systems remain to be developed clinically and computationally for high-quality wound care. To this end, we have carried out a systematic review of intelligent image-based data analysis and system developments for wound assessment. Specifically, we provide an extensive review of research methods on wound measurement (segmentation) and wound diagnosis (classification). We also reviewed recent work on wound assessment systems (including hardware, software, and mobile apps). More than 250 articles were retrieved from various publication databases and online resources, and 115 of them were carefully selected to cover the breadth and depth of most recent and relevant work to convey the current review to its fulfillment. △ Less","15 September, 2020",https://arxiv.org/pdf/2009.07141
FairCVtest Demo: Understanding Bias in Multimodal Learning with a Testbed in Fair Automatic Recruitment,Alejandro Peña;Ignacio Serna;Aythami Morales;Julian Fierrez,"With the aim of studying how current multimodal AI algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, this demonstrator experiments over an automated recruitment testbed based on Curriculum Vitae: FairCVtest. The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. This demo shows the capacity of the Artificial Intelligence (AI) behind a recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Aditionally, the demo includes a new algorithm (SensitiveNets) for discrimination-aware learning which eliminates sensitive information in our multimodal AI framework. △ Less","12 September, 2020",https://arxiv.org/pdf/2009.07025
Microscope Based HER2 Scoring System,Jun Zhang;Kuan Tian;Pei Dong;Haocheng Shen;Kezhou Yan;Jianhua Yao;Junzhou Huang;Xiao Han,"The overexpression of human epidermal growth factor receptor 2 (HER2) has been established as a therapeutic target in multiple types of cancers, such as breast and gastric cancers. Immunohistochemistry (IHC) is employed as a basic HER2 test to identify the HER2-positive, borderline, and HER2-negative patients. However, the reliability and accuracy of HER2 scoring are affected by many factors, such as pathologists' experience. Recently, artificial intelligence (AI) has been used in various disease diagnosis to improve diagnostic accuracy and reliability, but the interpretation of diagnosis results is still an open problem. In this paper, we propose a real-time HER2 scoring system, which follows the HER2 scoring guidelines to complete the diagnosis, and thus each step is explainable. Unlike the previous scoring systems based on whole-slide imaging, our HER2 scoring system is integrated into an augmented reality (AR) microscope that can feedback AI results to the pathologists while reading the slide. The pathologists can help select informative fields of view (FOVs), avoiding the confounding regions, such as DCIS. Importantly, we illustrate the intermediate results with membrane staining condition and cell classification results, making it possible to evaluate the reliability of the diagnostic results. Also, we support the interactive modification of selecting regions-of-interest, making our system more flexible in clinical practice. The collaboration of AI and pathologists can significantly improve the robustness of our system. We evaluate our system with 285 breast IHC HER2 slides, and the classification accuracy of 95\% shows the effectiveness of our HER2 scoring system. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.06816
Analysis of Models for Decentralized and Collaborative AI on Blockchain,Justin D. Harris,"Machine learning has recently enabled large advances in artificial intelligence, but these results can be highly centralized. The large datasets required are generally proprietary; predictions are often sold on a per-query basis; and published models can quickly become out of date without effort to acquire more data and maintain them. Published proposals to provide models and data for free for certain tasks include Microsoft Research's Decentralized and Collaborative AI on Blockchain. The framework allows participants to collaboratively build a dataset and use smart contracts to share a continuously updated model on a public blockchain. The initial proposal gave an overview of the framework omitting many details of the models used and the incentive mechanisms in real world scenarios. In this work, we evaluate the use of several models and configurations in order to propose best practices when using the Self-Assessment incentive mechanism so that models can remain accurate and well-intended participants that submit correct data have the chance to profit. We have analyzed simulations for each of three models: Perceptron, Naïve Bayes, and a Nearest Centroid Classifier, with three different datasets: predicting a sport with user activity from Endomondo, sentiment analysis on movie reviews from IMDB, and determining if a news article is fake. We compare several factors for each dataset when models are hosted in smart contracts on a public blockchain: their accuracy over time, balances of a good and bad user, and transaction costs (or gas) for deploying, updating, collecting refunds, and collecting rewards. A free and open source implementation for the Ethereum blockchain and simulations written in Python is provided at https://github.com/microsoft/0xDeCA10B. This version has updated gas costs using newer optimizations written after the original publication. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.06756
Should We Trust (X)AI? Design Dimensions for Structured Experimental Evaluations,Fabian Sperrle;Mennatallah El-Assady;Grace Guo;Duen Horng Chau;Alex Endert;Daniel Keim,"This paper systematically derives design dimensions for the structured evaluation of explainable artificial intelligence (XAI) approaches. These dimensions enable a descriptive characterization, facilitating comparisons between different study designs. They further structure the design space of XAI, converging towards a precise terminology required for a rigorous study of XAI. Our literature review differentiates between comparative studies and application papers, revealing methodological differences between the fields of machine learning, human-computer interaction, and visual analytics. Generally, each of these disciplines targets specific parts of the XAI process. Bridging the resulting gaps enables a holistic evaluation of XAI in real-world scenarios, as proposed by our conceptual model characterizing bias sources and trust-building. Furthermore, we identify and discuss the potential for future work based on observed research gaps that should lead to better coverage of the proposed model. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.06433
A novel combination of theoretical analysis and data-driven method for reconstruction of structural defects,Qi Li;Yihui Da;Yinghong Zhang;Bin Wang;Dianzi Liu;Zhenghua Qian,"Ultrasonic guided wave technology has played a significant role in the field of non-destructive testing as it employs acoustic waves that have advantages of high propagation efficiency and low energy consumption during the inspect process. However, theoretical solutions to guided wave scattering problems using assumptions such as Born approximation, have led to the poor quality of the reconstructed results. To address this issue, a novel approach to quantitative reconstruction of defects using the integration of data-driven method with the guided wave scattering analysis has been proposed in this paper. Based on the geometrical information of defects and initial results by the theoretical analysis of defect reconstructions, a deep learning neural network model is built to reveal the physical relationship between defects and the received signals. This data-driven model is then applied to quantitatively assess and characterize defect profiles in structures, reduce the inaccuracy of the theoretical modelling and eliminate the impact of noise pollution in the process of inspection. To demonstrate advantages of the developed approach to reconstructions of defects with complex profiles, numerical examples including basic defect profiles and a defect with the noisy fringe have been examined. Results show that this approach has greater accuracy for reconstruction of defects in structures as compared with the analytical method and provides a valuable insight into the development of artificial intelligence-assisted inspection systems with high accuracy and efficiency in the field of non-destructive testing. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.06276
A Vertical Federated Learning Method for Interpretable Scorecard and Its Application in Credit Scoring,Fanglan Zheng;Erihe;Kun Li;Jiang Tian;Xiaojia Xiang,"With the success of big data and artificial intelligence in many fields, the applications of big data driven models are expected in financial risk management especially credit scoring and rating. Under the premise of data privacy protection, we propose a projected gradient-based method in the vertical federated learning framework for the traditional scorecard, which is based on logistic regression with bounded constraints, namely FL-LRBC. The latter enables multiple agencies to jointly train an optimized scorecard model in a single training session. It leads to the formation of the model with positive coefficients, while the time-consuming parameter-tuning process can be avoided. Moreover, the performance in terms of both AUC and the Kolmogorov-Smirnov (KS) statistics is significantly improved due to data enrichment using FL-LRBC. At present, FL-LRBC has already been applied to credit business in a China nation-wide financial holdings group. △ Less","14 September, 2020",https://arxiv.org/pdf/2009.06218
An Argumentation-based Approach for Explaining Goal Selection in Intelligent Agents,Mariela Morveli-Espinoza;Cesar Augusto Tacla;Henrique Jasinski,"During the first step of practical reasoning, i.e. deliberation or goals selection, an intelligent agent generates a set of pursuable goals and then selects which of them he commits to achieve. Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions. In the context of goals selection, agents should be able to explain the reasoning path that leads them to select (or not) a certain goal. In this article, we use an argumentation-based approach for generating explanations about that reasoning path. Besides, we aim to enrich the explanations with information about emerging conflicts during the selection process and how such conflicts were resolved. We propose two types of explanations: the partial one and the complete one and a set of explanatory schemes to generate pseudo-natural explanations. Finally, we apply our proposal to the cleaner world scenario. △ Less","13 September, 2020",https://arxiv.org/pdf/2009.06131
Receptivity of an AI Cognitive Assistant by the Radiology Community: A Report on Data Collected at RSNA,Karina Kanjaria;Anup Pillai;Chaitanya Shivade;Marina Bendersky;Ashutosh Jadhav;Vandana Mukherjee;Tanveer Syeda-Mahmood,"Due to advances in machine learning and artificial intelligence (AI), a new role is emerging for machines as intelligent assistants to radiologists in their clinical workflows. But what systematic clinical thought processes are these machines using? Are they similar enough to those of radiologists to be trusted as assistants? A live demonstration of such a technology was conducted at the 2016 Scientific Assembly and Annual Meeting of the Radiological Society of North America (RSNA). The demonstration was presented in the form of a question-answering system that took a radiology multiple choice question and a medical image as inputs. The AI system then demonstrated a cognitive workflow, involving text analysis, image analysis, and reasoning, to process the question and generate the most probable answer. A post demonstration survey was made available to the participants who experienced the demo and tested the question answering system. Of the reported 54,037 meeting registrants, 2,927 visited the demonstration booth, 1,991 experienced the demo, and 1,025 completed a post-demonstration survey. In this paper, the methodology of the survey is shown and a summary of its results are presented. The results of the survey show a very high level of receptiveness to cognitive computing technology and artificial intelligence among radiologists. △ Less","13 September, 2020",https://arxiv.org/pdf/2009.06082
Deconstructing Legal Text_Object Oriented Design in Legal Adjudication,Megan Ma;Dmitriy Podkopaev;Avalon Campbell-Cousins;Adam Nicholas,"Rules are pervasive in the law. In the context of computer engineering, the translation of legal text to algorithmic form is seemingly direct. In large part, law may be a ripe field for expert systems and machine learning. For engineers, existing law appears formulaic and logically reducible to ""if, then"" statements. The underlying assumption is that the legal language is both self-referential and universal. Moreover, description is considered distinct from interpretation; that in describing the law, the language is seen as quantitative and objectifiable. Nevertheless, is descriptive formal language purely dissociative? From the logic machine of the 1970s to the modern fervor for artificial intelligence (AI), governance by numbers is making a persuasive return. Could translation be possible? The project follows a fundamentally semantic conundrum: what is the significance of ""meaning"" in legal language? The project, therefore, tests translation by deconstructing sentences from existing legal judgments to their constituent factors. Definitions are then extracted in accordance with the interpretations of the judges. The intent is to build an expert system predicated on alleged rules of legal reasoning. The authors apply both linguistic modelling and natural language processing technology to parse the legal judgments. The project extends beyond prior research in the area, combining a broadly statistical model of context with the relative precision of syntactic structure. The preliminary hypothesis is that, by analyzing the components of legal language with a variety of techniques, we can begin to translate law to numerical form. △ Less","13 September, 2020",https://arxiv.org/pdf/2009.06054
Argumentation-based Agents that Explain their Decisions,Mariela Morveli-Espinoza;Ayslan Possebom;Cesar Augusto Tacla,"Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots. △ Less","12 September, 2020",https://arxiv.org/pdf/2009.05897
To Root Artificial Intelligence Deeply in Basic Science for a New Generation of AI,Jingan Yang;Yang Peng,"One of the ambitions of artificial intelligence is to root artificial intelligence deeply in basic science while developing brain-inspired artificial intelligence platforms that will promote new scientific discoveries. The challenges are essential to push artificial intelligence theory and applied technologies research forward. This paper presents the grand challenges of artificial intelligence research for the next 20 years which include:~(i) to explore the working mechanism of the human brain on the basis of understanding brain science, neuroscience, cognitive science, psychology and data science; (ii) how is the electrical signal transmitted by the human brain? What is the coordination mechanism between brain neural electrical signals and human activities? (iii)~to root brain-computer interface~(BCI) and brain-muscle interface~(BMI) technologies deeply in science on human behaviour; (iv)~making research on knowledge-driven visual commonsense reasoning~(VCR), develop a new inference engine for cognitive network recognition~(CNR); (v)~to develop high-precision, multi-modal intelligent perceptrons; (vi)~investigating intelligent reasoning and fast decision-making systems based on knowledge graph~(KG). We believe that the frontier theory innovation of AI, knowledge-driven modeling methodologies for commonsense reasoning, revolutionary innovation and breakthroughs of the novel algorithms and new technologies in AI, and developing responsible AI should be the main research strategies of AI scientists in the future. △ Less","11 September, 2020",https://arxiv.org/pdf/2009.05678
Teaching Tech to Talk: K-12 Conversational Artificial Intelligence Literacy Curriculum and Development Tools,Jessica Van Brummelen;Tommy Heng;Viktoriya Tabunshchyk,"With children talking to smart-speakers, smart-phones and even smart-microwaves daily, it is increasingly important to educate students on how these agents work-from underlying mechanisms to societal implications. Researchers are developing tools and curriculum to teach K-12 students broadly about artificial intelligence (AI); however, few studies have evaluated these tools with respect to AI-specific learning outcomes, and even fewer have addressed student learning about AI-based conversational agents. We evaluate our Conversational Agent Interface for MIT App Inventor and workshop curriculum with respect to eight AI competencies from the literature. Furthermore, we analyze teacher (n=9) and student (n=47) feedback from workshops with the interface and recommend that future work leverages design considerations from the literature to optimize engagement, collaborates with teachers, and addresses a range of student abilities through pacing and opportunities for extension. We found students struggled most with the concepts of AI ethics and learning, and recommend emphasizing these topics when teaching. The appendix, including a demo video, can be found here: https://gist.github.com/jessvb/1cd959e32415a6ad4389761c49b54bbf △ Less","11 September, 2020",https://arxiv.org/pdf/2009.05653
Physically Embedded Planning Problems: New Challenges for Reinforcement Learning,Mehdi Mirza;Andrew Jaegle;Jonathan J. Hunt;Arthur Guez;Saran Tunyasuvunakool;Alistair Muldal;Théophane Weber;Peter Karkus;Sébastien Racanière;Lars Buesing;Timothy Lillicrap;Nicolas Heess,"Recent work in deep reinforcement learning (RL) has produced algorithms capable of mastering challenging games such as Go, chess, or shogi. In these works the RL agent directly observes the natural state of the game and controls that state directly with its actions. However, when humans play such games, they do not just reason about the moves but also interact with their physical environment. They understand the state of the game by looking at the physical board in front of them and modify it by manipulating pieces using touch and fine-grained motor control. Mastering complicated physical systems with abstract goals is a central challenge for artificial intelligence, but it remains out of reach for existing RL algorithms. To encourage progress towards this goal we introduce a set of physically embedded planning problems and make them publicly available. We embed challenging symbolic tasks (Sokoban, tic-tac-toe, and Go) in a physics engine to produce a set of tasks that require perception, reasoning, and motor control over long time horizons. Although existing RL algorithms can tackle the symbolic versions of these tasks, we find that they struggle to master even the simplest of their physically embedded counterparts. As a first step towards characterizing the space of solution to these tasks, we introduce a strong baseline that uses a pre-trained expert game player to provide hints in the abstract space to an RL agent's policy while training it on the full sensorimotor control task. The resulting agent solves many of the tasks, underlining the need for methods that bridge the gap between abstract planning and embodied control. See illustrating video at https://youtu.be/RwHiHlym_1k. △ Less","29 October, 2020",https://arxiv.org/pdf/2009.05524
The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and Industrial Q Needs,Markus Borg,"AI solutions seem to appear in any and all application domains. As AI becomes more pervasive, the importance of quality assurance increases. Unfortunately, there is no consensus on what artificial intelligence means and interpretations range from simple statistical analysis to sentient humanoid robots. On top of that, quality is a notoriously hard concept to pinpoint. What does this mean for AI quality? In this paper, we share our working definition and a pragmatic approach to address the corresponding quality assurance with a focus on testing. Finally, we present our ongoing work on establishing the AIQ Meta-Testbed. △ Less","11 September, 2020",https://arxiv.org/pdf/2009.05260
The use of Recommender Systems in web technology and an in-depth analysis of Cold State problem,Denis Selimi;Krenare Pireva Nuci,"In the WWW (World Wide Web), dynamic development and spread of data has resulted a tremendous amount of information available on the Internet, yet user is unable to find relevant information in a short span of time. Consequently, a system called recommendation system developed to help users find their infromation with ease through their browsing activities. In other words, recommender systems are tools for interacting with large amount of information that provide personalized view for prioritizing items likely to be of keen for users. They have developed over the years in artificial intelligence techniques that include machine learning and data mining amongst many to mention. Furthermore, the recommendation systems have personalized on an e-commerce, on-line applications such as Amazon.com, Netflix, and Booking.com. As a result, this has inspired many researchers to extend the reach of recommendation systems into new sets of challenges and problem areas that are yet to be truly solved, primarily a problem with the case of making a recommendation to a new user that is called cold-state (i.e. cold-start) user problem where the new user might likely not yield much of information searched. Therfore, the purpose of this paper is to tackle the said cold-start problem with a few effecient methods and challenges, as well as identify and overview the current state of recommendation system as a whole △ Less","10 September, 2020",https://arxiv.org/pdf/2009.04780
TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments,Tom Bewley;Jonathan Lawry,"In explainable artificial intelligence, there is increasing interest in understanding the behaviour of autonomous agents to build trust and validate performance. Modern agent architectures, such as those trained by deep reinforcement learning, are currently so lacking in interpretable structure as to effectively be black boxes, but insights may still be gained from an external, behaviourist perspective. Inspired by conceptual spaces theory, we suggest that a versatile first step towards general understanding is to discretise the state space into convex regions, jointly capturing similarities over the agent's action, value function and temporal dynamics within a dataset of observations. We create such a representation using a novel variant of the CART decision tree algorithm, and demonstrate how it facilitates practical understanding of black box agents through prediction, visualisation and rule-based explanation. △ Less","21 September, 2020",https://arxiv.org/pdf/2009.04743
A Methodological Approach to Model CBR-based Systems,Eliseu M. Oliveira;Rafael F. Reale;Joberto S. B. Martins,"Artificial intelligence (AI) has been used in various areas to support system optimization and find solutions where the complexity makes it challenging to use algorithmic and heuristics. Case-based Reasoning (CBR) is an AI technique intensively exploited in domains like management, medicine, design, construction, retail and smart grid. CBR is a technique for problem-solving and captures new knowledge by using past experiences. One of the main CBR deployment challenges is the target system modeling process. This paper presents a straightforward methodological approach to model CBR-based applications using the concepts of abstract and concrete models. Splitting the modeling process with two models facilitates the allocation of expertise between the application domain and the CBR technology. The methodological approach intends to facilitate the CBR modeling process and to foster CBR use in various areas outside computer science. △ Less","9 September, 2020",https://arxiv.org/pdf/2009.04346
Proxy Network for Few Shot Learning,Bin Xiao;Chien-Liang Liu;Wen-Hoar Hsaio,"The use of a few examples for each class to train a predictive model that can be generalized to novel classes is a crucial and valuable research direction in artificial intelligence. This work addresses this problem by proposing a few-shot learning (FSL) algorithm called proxy network under the architecture of meta-learning. Metric-learning based approaches assume that the data points within the same class should be close, whereas the data points in the different classes should be separated as far as possible in the embedding space. We conclude that the success of metric-learning based approaches lies in the data embedding, the representative of each class, and the distance metric. In this work, we propose a simple but effective end-to-end model that directly learns proxies for class representative and distance metric from data simultaneously. We conduct experiments on CUB and mini-ImageNet datasets in 1-shot-5-way and 5-shot-5-way scenarios, and the experimental results demonstrate the superiority of our proposed method over state-of-the-art methods. Besides, we provide a detailed analysis of our proposed method. △ Less","9 September, 2020",https://arxiv.org/pdf/2009.04292
Convolution Neural Networks for diagnosing colon and lung cancer histopathological images,Sanidhya Mangal;Aanchal Chaurasia;Ayush Khajanchi,"Lung and Colon cancer are one of the leading causes of mortality and morbidity in adults. Histopathological diagnosis is one of the key components to discern cancer type. The aim of the present research is to propose a computer aided diagnosis system for diagnosing squamous cell carcinomas and adenocarcinomas of lung as well as adenocarcinomas of colon using convolutional neural networks by evaluating the digital pathology images for these cancers. Hereby, rendering artificial intelligence as useful technology in the near future. A total of 2500 digital images were acquired from LC25000 dataset containing 5000 images for each class. A shallow neural network architecture was used classify the histopathological slides into squamous cell carcinomas, adenocarcinomas and benign for the lung. Similar model was used to classify adenocarcinomas and benign for colon. The diagnostic accuracy of more than 97% and 96% was recorded for lung and colon respectively. △ Less","8 September, 2020",https://arxiv.org/pdf/2009.03878
High-throughput relation extraction algorithm development associating knowledge articles and electronic health records,Yucong Lin;Keming Lu;Yulin Chen;Chuan Hong;Sheng Yu,"Objective: Medical relations are the core components of medical knowledge graphs that are needed for healthcare artificial intelligence. However, the requirement of expert annotation by conventional algorithm development processes creates a major bottleneck for mining new relations. In this paper, we present Hi-RES, a framework for high-throughput relation extraction algorithm development. We also show that combining knowledge articles with electronic health records (EHRs) significantly increases the classification accuracy. Methods: We use relation triplets obtained from structured databases and semistructured webpages to label sentences from target corpora as positive training samples. Two methods are also provided for creating improved negative samples by combining positive samples with naïve negative samples. We propose a common model that summarizes sentence information using large-scale pretrained language models and multi-instance attention, which then joins with the concept embeddings trained from the EHRs for relation prediction. Results: We apply the Hi-RES framework to develop classification algorithms for disorder-disorder relations and disorder-location relations. Millions of sentences are created as training data. Using pretrained language models and EHR-based embeddings individually provides considerable accuracy increases over those of previous models. Joining them together further tremendously increases the accuracy to 0.947 and 0.998 for the two sets of relations, respectively, which are 10-17 percentage points higher than those of previous models. Conclusion: Hi-RES is an efficient framework for achieving high-throughput and accurate relation extraction algorithm development. △ Less","7 September, 2020",https://arxiv.org/pdf/2009.03506
Towards an Interoperable Data Protocol Aimed at Linking the Fashion Industry with AI Companies,Mohammed Al-Rawi;Joeran Beel,"The fashion industry is looking forward to use artificial intelligence technologies to enhance their processes, services, and applications. Although the amount of fashion data currently in use is increasing, there is a large gap in data exchange between the fashion industry and the related AI companies, not to mention the different structure used for each fashion dataset. As a result, AI companies are relying on manually annotated fashion data to build different applications. Furthermore, as of this writing, the terminology, vocabulary and methods of data representation used to denote fashion items are still ambiguous and confusing. Hence, it is clear that the fashion industry and AI companies will benefit from a protocol that allows them to exchange and organise fashion information in a unified way. To achieve this goal we aim (1) to define a protocol called DDOIF that will allow interoperability of fashion data; (2) for DDOIF to contain diverse entities including extensive information on clothing and accessories attributes in the form of text and various media formats; and (3)To design and implement an API that includes, among other things, functions for importing and exporting a file built according to the DDOIF protocol that stores all information about a single item of clothing. To this end, we identified over 1000 class and subclass names used to name fashion items and use them to build the DDOIF dictionary. We make DDOIF publicly available to all interested users and developers and look forward to engaging more collaborators to improve and enrich it. △ Less","7 September, 2020",https://arxiv.org/pdf/2009.03005
Optimization of High-dimensional Simulation Models Using Synthetic Data,Thomas Bartz-Beielstein;Eva Bartz;Frederik Rehbach;Olaf Mersmann,"Simulation models are valuable tools for resource usage estimation and capacity planning. In many situations, reliable data is not available. We introduce the BuB simulator, which requires only the specification of plausible intervals for the simulation parameters. By performing a surrogate-model based optimization, improved simulation model parameters can be determined. Furthermore, a detailed statistical analysis can be performed, which allows deep insights into the most important model parameters and their interactions. This information can be used to screen the parameters that should be further investigated. To exemplify our approach, a capacity and resource planning task for a hospital was simulated and optimized. The study explicitly covers difficulties caused by the COVID-19 pandemic. It can be shown, that even if only limited real-world data is available, the BuB simulator can be beneficially used to consider worst- and best-case scenarios. The BuB simulator can be extended in many ways, e.g., by adding further resources (personal protection equipment, staff, pharmaceuticals) or by specifying several cohorts (based on age, health status, etc.). Keywords: Synthetic data, discrete-event simulation, surrogate-model-based optimization, COVID-19, machine learning, artificial intelligence, hospital resource planning, prediction tool, capacity planning. △ Less","6 September, 2020",https://arxiv.org/pdf/2009.02781
Generalization on the Enhancement of Layerwise Relevance Interpretability of Deep Neural Network,Erico Tjoa;Guan Cuntai,"The practical application of deep neural networks are still limited by their lack of transparency. One of the efforts to provide explanation for decisions made by artificial intelligence (AI) is the use of saliency or heat maps highlighting relevant regions that contribute significantly to its prediction. A layer-wise amplitude filtering method was previously introduced to improve the quality of heatmaps, performing error corrections by noise-spike suppression. In this study, we generalize the layerwise error correction by considering any identifiable error and assuming there exists a groundtruth interpretable information. The forms of errors propagated through layerwise relevance methods are studied and we propose a filtering technique for interpretability signal rectification taylored to the trend of signal amplitude of the particular neural network used. Finally, we put forth arguments for the use of groundtruth interpretable information. △ Less","18 October, 2020",https://arxiv.org/pdf/2009.02516
Chain-Net: Learning Deep Model for Modulation Classification Under Synthetic Channel Impairment,Thien Huynh-The;Van-Sang Doan;Cam-Hao Hua;Quoc-Viet Pham;Dong-Seong Kim,"Modulation classification, an intermediate process between signal detection and demodulation in a physical layer, is now attracting more interest to the cognitive radio field, wherein the performance is powered by artificial intelligence algorithms. However, most existing conventional approaches pose the obstacle of effectively learning weakly discriminative modulation patterns. This paper proposes a robust modulation classification method by taking advantage of deep learning to capture the meaningful information of modulation signal at multi-scale feature representations. To this end, a novel architecture of convolutional neural network, namely Chain-Net, is developed with various asymmetric kernels organized in two processing flows and associated via depth-wise concatenation and element-wise addition for optimizing feature utilization. The network is evaluated on a big dataset of 14 challenging modulation formats, including analog and high-order digital techniques. The simulation results demonstrate that Chain-Net robustly classifies the modulation of radio signals suffering from a synthetic channel deterioration and further performs better than other deep networks. △ Less","4 September, 2020",https://arxiv.org/pdf/2009.02023
"The Pace of Artificial Intelligence Innovations: Speed, Talent, and Trial-and-Error",Xuli Tang;Xin Li;Ying Ding;Min Song;Yi Bu,"Innovations in artificial intelligence (AI) are occurring at speeds faster than ever witnessed before. However, few studies have managed to measure or depict this increasing velocity of innovations in the field of AI. In this paper, we combine data on AI from arXiv and Semantic Scholar to explore the pace of AI innovations from three perspectives: AI publications, AI players, and AI updates (trial and error). A research framework and three novel indicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed (US), are proposed to measure the pace of innovations in the field of AI. The results show that: (1) in 2019, more than 3 AI preprints were submitted to arXiv per hour, over 148 times faster than in 1994. Furthermore, there was one deep learning-related preprint submitted to arXiv every 0.87 hours in 2019, over 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers entered into the field of AI each hour in 2019, more than 175 times faster than in the 1990s. (3) As for AI updates (trial and error), one updated AI preprint was submitted to arXiv every 41 days, with around 33% of AI preprints having been updated at least twice in 2019. In addition, as reported in 2019, it took, on average, only around 0.2 year for AI preprints to receive their first citations, which is 5 times faster than 2000-2007. This swift pace in AI illustrates the increase in popularity of AI innovation. The systematic and fine-grained analysis of the AI field enabled to portrait the pace of AI innovation and demonstrated that the proposed approach can be adopted to understand other fast-growing fields such as cancer research and nano science. △ Less","3 September, 2020",https://arxiv.org/pdf/2009.01812
A free web service for fast COVID-19 classification of chest X-Ray images,Jose David Bermudez Castro;Ricardo Rei;Jose E. Ruiz;Pedro Achanccaray Diaz;Smith Arauco Canchumuni;Cristian Muñoz Villalobos;Felipe Borges Coelho;Leonardo Forero Mendoza;Marco Aurelio C. Pacheco,"The coronavirus outbreak became a major concern for society worldwide. Technological innovation and ingenuity are essential to fight COVID-19 pandemic and bring us one step closer to overcome it. Researchers over the world are working actively to find available alternatives in different fields, such as the Healthcare System, pharmaceutic, health prevention, among others. With the rise of artificial intelligence (AI) in the last 10 years, IA-based applications have become the prevalent solution in different areas because of its higher capability, being now adopted to help combat against COVID-19. This work provides a fast detection system of COVID-19 characteristics in X-Ray images based on deep learning (DL) techniques. This system is available as a free web deployed service for fast patient classification, alleviating the high demand for standards method for COVID-19 diagnosis. It is constituted of two deep learning models, one to differentiate between X-Ray and non-X-Ray images based on Mobile-Net architecture, and another one to identify chest X-Ray images with characteristics of COVID-19 based on the DenseNet architecture. For real-time inference, it is provided a pair of dedicated GPUs, which reduce the computational time. The whole system can filter out non-chest X-Ray images, and detect whether the X-Ray presents characteristics of COVID-19, highlighting the most sensitive regions. △ Less","27 August, 2020",https://arxiv.org/pdf/2009.01657
Deep Learning in Science,Stefano Bianchini;Moritz Müller;Pierre Pelletier,"Much of the recent success of Artificial Intelligence (AI) has been spurred on by impressive achievements within a broader family of machine learning methods, commonly referred to as Deep Learning (DL). This paper provides insights on the diffusion and impact of DL in science. Through a Natural Language Processing (NLP) approach on the arXiv.org publication corpus, we delineate the emerging DL technology and identify a list of relevant search terms. These search terms allow us to retrieve DL-related publications from Web of Science across all sciences. Based on that sample, we document the DL diffusion process in the scientific system. We find i) an exponential growth in the adoption of DL as a research tool across all sciences and all over the world, ii) regional differentiation in DL application domains, and iii) a transition from interdisciplinary DL applications to disciplinary research within application domains. In a second step, we investigate how the adoption of DL methods affects scientific development. Therefore, we empirically assess how DL adoption relates to re-combinatorial novelty and scientific impact in the health sciences. We find that DL adoption is negatively correlated with re-combinatorial novelty, but positively correlated with expectation as well as variance of citation performance. Our findings suggest that DL does not (yet?) work as an autopilot to navigate complex knowledge landscapes and overthrow their structure. However, the 'DL principle' qualifies for its versatility as the nucleus of a general scientific method that advances science in a measurable way. △ Less","4 September, 2020",https://arxiv.org/pdf/2009.01575
P6: A Declarative Language for Integrating Machine Learning in Visual Analytics,Jianping Kelvin Li;Kwan-Liu Ma,"We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics. △ Less","2 September, 2020",https://arxiv.org/pdf/2009.01399
Perceptual Deep Neural Networks: Adversarial Robustness through Input Recreation,Danilo Vasconcellos Vargas;Bingli Liao;Takahiro Kanzaki,"Adversarial examples have shown that albeit highly accurate, models learned by machines, differently from humans, have many weaknesses. However, humans' perception is also fundamentally different from machines, because we do not see the signals which arrive at the retina but a rather complex recreation of them. In this paper, we explore how machines could recreate the input as well as investigate the benefits of such an augmented perception. In this regard, we propose Perceptual Deep Neural Networks (\varphiDNN) which also recreate their own input before further processing. The concept is formalized mathematically and two variations of it are developed (one based on inpainting the whole image and the other based on a noisy resized super resolution recreation). Experiments reveal that \varphiDNNs and their adversarial training variations can increase the robustness substantially, surpassing both state-of-the-art defenses and pre-processing types of defenses in 100% of the tests. \varphiDNNs are shown to scale well to bigger image sizes, keeping a similar high accuracy throughout; while the state-of-the-art worsen up to 35%. Moreover, the recreation process intentionally corrupts the input image. Interestingly, we show by ablation tests that corrupting the input is, although counter-intuitive, beneficial. Thus, \varphiDNNs reveal that input recreation has strong benefits for artificial neural networks similar to biological ones, shedding light into the importance of purposely corrupting the input as well as pioneering an area of perception models based on GANs and autoencoders for robust recognition in artificial intelligence. △ Less","30 November, 2020",https://arxiv.org/pdf/2009.01110
Travel time prediction for congested freeways with a dynamic linear model,Semin Kwak;Nikolas Geroliminis,"Accurate prediction of travel time is an essential feature to support Intelligent Transportation Systems (ITS). The non-linearity of traffic states, however, makes this prediction a challenging task. Here we propose to use dynamic linear models (DLMs) to approximate the non-linear traffic states. Unlike a static linear regression model, the DLMs assume that their parameters are changing across time. We design a DLM with model parameters defined at each time unit to describe the spatio-temporal characteristics of time-series traffic data. Based on our DLM and its model parameters analytically trained using historical data, we suggest an optimal linear predictor in the minimum mean square error (MMSE) sense. We compare our prediction accuracy of travel time for freeways in California (I210-E and I5-S) under highly congested traffic conditions with those of other methods: the instantaneous travel time, k-nearest neighbor, support vector regression, and artificial neural network. We show significant improvements in the accuracy, especially for short-term prediction. △ Less","2 September, 2020",https://arxiv.org/pdf/2009.01016
Embedded Development Boards for Edge-AI: A Comprehensive Report,Hamza Ali Imran;Usama Mujahid;Saad Wazir;Usama Latif;Kiran Mehmood,"The use of Deep Learning and Machine Learning is becoming pervasive day by day which is opening doors to new opportunities in every aspect of technology. Its application Ranges from Health-care to Self-driving Cars, Home Automation to Smart-agriculture, and Industry 4.0. Traditionally the majority of the processing for IoT applications is being done on a central cloud but that has its issues; which include latency, security, bandwidth, and privacy, etc. It is estimated that there will be around 20 Million IoT devices by 2020 which will increase problems with sending data to the cloud and doing the processing there. A new trend of processing the data on the edge of the network is emerging. The idea is to do processing as near the point of data production as possible. Doing processing on the nodes generating the data is called Edge Computing and doing processing on a layer between the cloud and the point of data production is called Fog computing. There are no standard definitions for any of these, hence they are usually used interchangeably. In this paper, we have reviewed the development boards available for running Artificial Intelligence algorithms on the Edge △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00803
Estimating the Brittleness of AI: Safety Integrity Levels and the Need for Testing Out-Of-Distribution Performance,Andrew J. Lohn,"Test, Evaluation, Verification, and Validation (TEVV) for Artificial Intelligence (AI) is a challenge that threatens to limit the economic and societal rewards that AI researchers have devoted themselves to producing. A central task of TEVV for AI is estimating brittleness, where brittleness implies that the system functions well within some bounds and poorly outside of those bounds. This paper argues that neither of those criteria are certain of Deep Neural Networks. First, highly touted AI successes (eg. image classification and speech recognition) are orders of magnitude more failure-prone than are typically certified in critical systems even within design bounds (perfectly in-distribution sampling). Second, performance falls off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced emphasis is needed on designing systems that are resilient despite failure-prone AI components as well as on evaluating and improving OOD performance in order to get AI to where it can clear the challenging hurdles of TEVV and certification. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00802
Intelligent Hotel ROS-based Service Robot,Yanyu Zhang;Xiu Wang;Xuan Wu;Wenjing Zhang;Meiqian Jiang;Mahmood Al-Khassaweneh,"With the advances of artificial intelligence (AI) technology, many studies and work have been carried out on how robots could replace human labor. In this paper, we present a ROS based intelligence hotel robot, which simplifies the check-in process. We use pioneer 3dx robot and considered different environment settings. The robot combined with Hokuyo Lidar and Kinect Xbox camera, can plan the routes accurately and reach rooms in different floors. In addition, we added an intelligent voice system which provides an assistant for the customers. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00594
A Deep 2-Dimensional Dynamical Spiking Neuronal Network for Temporal Encoding trained with STDP,Matthew Evanusa;Cornelia Fermuller;Yiannis Aloimonos,"The brain is known to be a highly complex, asynchronous dynamical system that is highly tailored to encode temporal information. However, recent deep learning approaches to not take advantage of this temporal coding. Spiking Neural Networks (SNNs) can be trained using biologically-realistic learning mechanisms, and can have neuronal activation rules that are biologically relevant. This type of network is also structured fundamentally around accepting temporal information through a time-decaying voltage update, a kind of input that current rate-encoding networks have difficulty with. Here we show that a large, deep layered SNN with dynamical, chaotic activity mimicking the mammalian cortex with biologically-inspired learning rules, such as STDP, is capable of encoding information from temporal data. We argue that the randomness inherent in the network weights allow the neurons to form groups that encode the temporal data being inputted after self-organizing with STDP. We aim to show that precise timing of input stimulus is critical in forming synchronous neural groups in a layered network. We analyze the network in terms of network entropy as a metric of information transfer. We hope to tackle two problems at once: the creation of artificial temporal neural systems for artificial intelligence, as well as solving coding mechanisms in the brain. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00581
Large Intelligent Surface Aided Physical Layer Security Transmission,Biqian Feng;Yongpeng Wu;Mengfan Zheng;Xiang-Gen Xia;Yongjian Wang;Chengshan Xiao,"In this paper, we investigate a large intelligent surface-enhanced (LIS-enhanced) system, where a LIS is deployed to assist secure transmission. Our design aims to maximize the achievable secrecy rates in different channel models, i.e., Rician fading and (or) independent and identically distributed Gaussian fading for the legitimate and eavesdropper channels. In addition, we take into consideration an artificial noise-aided transmission structure for further improving system performance. The difficulties of tackling the aforementioned problems are the structure of the expected secrecy rate expressions and the non-convex phase shift constraint. To facilitate the design, we propose two frameworks, namely the sample average approximation based (SAA-based) algorithm and the hybrid stochastic projected gradient-convergent policy (hybrid SPG-CP) algorithm, to calculate the expectation terms in the secrecy rate expressions. Meanwhile, majorization minimization (MM) is adopted to address the non-convexity of the phase shift constraint. In addition, we give some analyses on two special scenarios by making full use of the expectation terms. Simulation results show that the proposed algorithms effectively optimize the secrecy communication rate for the considered setup, and the LIS-enhanced system greatly improves secrecy performance compared to conventional architectures without LIS. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00473
Advancing from Predictive Maintenance to Intelligent Maintenance with AI and IIoT,Haining Zheng;Antonio R. Paiva;Chris S. Gurciullo,"As Artificial Intelligent (AI) technology advances and increasingly large amounts of data become readily available via various Industrial Internet of Things (IIoT) projects, we evaluate the state of the art of predictive maintenance approaches and propose our innovative framework to improve the current practice. The paper first reviews the evolution of reliability modelling technology in the past 90 years and discusses major technologies developed in industry and academia. We then introduce the next generation maintenance framework - Intelligent Maintenance, and discuss its key components. This AI and IIoT based Intelligent Maintenance framework is composed of (1) latest machine learning algorithms including probabilistic reliability modelling with deep learning, (2) real-time data collection, transfer, and storage through wireless smart sensors, (3) Big Data technologies, (4) continuously integration and deployment of machine learning models, (5) mobile device and AR/VR applications for fast and better decision-making in the field. Particularly, we proposed a novel probabilistic deep learning reliability modelling approach and demonstrate it in the Turbofan Engine Degradation Dataset. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00351
Robust and Secure Communications in Intelligent Reflecting Surface Assisted NOMA networks,Zheng Zhang;Lu Lv;Qingqing Wu;Hao Deng;Jian Chen,"This letter investigates secure transmission in an intelligent reflecting surface (IRS) assisted non-orthogonal multiple access (NOMA) network. Consider a practical eavesdropping scenario with imperfect channel state information of the eavesdropper, we propose a robust beamforming scheme using artificial noise to guarantee secure NOMA transmission with the IRS. A joint transmit beamforming and IRS phase shift optimization problem is formulated to minimize the transmit power. Since the problem is non-convex and challenging to resolve, we develop an effective alternative optimization (AO) algorithm to obtain stationary point solutions. Simulation results validate the security advantage of the robust beamforming scheme and the effectiveness of the AO algorithm. △ Less","1 September, 2020",https://arxiv.org/pdf/2009.00267
"A Multisite, Report-Based, Centralized Infrastructure for Feedback and Monitoring of Radiology AI/ML Development and Clinical Deployment",Menashe Benjamin;Guy Engelhard;Alex Aisen;Yinon Aradi;Elad Benjamin,"An infrastructure for multisite, geographically-distributed creation and collection of diverse, high-quality, curated and labeled radiology image data is crucial for the successful automated development, deployment, monitoring and continuous improvement of Artificial Intelligence (AI)/Machine Learning (ML) solutions in the real world. An interactive radiology reporting approach that integrates image viewing, dictation, natural language processing (NLP) and creation of hyperlinks between image findings and the report, provides localized labels during routine interpretation. These images and labels can be captured and centralized in a cloud-based system. This method provides a practical and efficient mechanism with which to monitor algorithm performance. It also supplies feedback for iterative development and quality improvement of new and existing algorithmic models. Both feedback and monitoring are achieved without burdening the radiologist. The method addresses proposed regulatory requirements for post-marketing surveillance and external data. Comprehensive multi-site data collection assists in reducing bias. Resource requirements are greatly reduced compared to dedicated retrospective expert labeling. △ Less","31 August, 2020",https://arxiv.org/pdf/2008.13781
Machine learning for metal additive manufacturing: Predicting temperature and melt pool fluid dynamics using physics-informed neural networks,Qiming Zhu;Zeliang Liu;Jinhui Yan,"The recent explosion of machine learning (ML) and artificial intelligence (AI) shows great potential in the breakthrough of metal additive manufacturing (AM) process modeling. However, the success of conventional machine learning tools in data science is primarily attributed to the unprecedented large amount of labeled data-sets (big data), which can be either obtained by experiments or first-principle simulations. Unfortunately, these labeled data-sets are expensive to obtain in AM due to the high expense of the AM experiments and prohibitive computational cost of high-fidelity simulations. We propose a physics-informed neural network (PINN) framework that fuses both data and first physical principles, including conservation laws of momentum, mass, and energy, into the neural network to inform the learning processes. To the best knowledge of the authors, this is the first application of PINN to three dimensional AM processes modeling. Besides, we propose a hard-type approach for Dirichlet boundary conditions (BCs) based on a Heaviside function, which can not only enforce the BCs but also accelerate the learning process. The PINN framework is applied to two representative metal manufacturing problems, including the 2018 NIST AM-Benchmark test series. We carefully assess the performance of the PINN model by comparing the predictions with available experimental data and high-fidelity simulation results. The investigations show that the PINN, owed to the additional physical knowledge, can accurately predict the temperature and melt pool dynamics during metal AM processes with only a moderate amount of labeled data-sets. The foray of PINN to metal AM shows the great potential of physics-informed deep learning for broader applications to advanced manufacturing. △ Less","16 September, 2020",https://arxiv.org/pdf/2008.13547
Theoretical Modeling of the Iterative Properties of User Discovery in a Collaborative Filtering Recommender System,Sami Khenissi;Mariem Boujelbene;Olfa Nasraoui,"The closed feedback loop in recommender systems is a common setting that can lead to different types of biases. Several studies have dealt with these biases by designing methods to mitigate their effect on the recommendations. However, most existing studies do not consider the iterative behavior of the system where the closed feedback loop plays a crucial role in incorporating different biases into several parts of the recommendation steps. We present a theoretical framework to model the asymptotic evolution of the different components of a recommender system operating within a feedback loop setting, and derive theoretical bounds and convergence properties on quantifiable measures of the user discovery and blind spots. We also validate our theoretical findings empirically using a real-life dataset and empirically test the efficiency of a basic exploration strategy within our theoretical framework. Our findings lay the theoretical basis for quantifying the effect of feedback loops and for designing Artificial Intelligence and machine learning algorithms that explicitly incorporate the iterative nature of feedback loops in the machine learning and recommendation process. △ Less","21 August, 2020",https://arxiv.org/pdf/2008.13526
Statistical Tree-based Population Seeding for Rolling Horizon EAs in General Video Game Playing,Edgar Galván;Oxana Gorshkova;Peter Mooney;Fred Valdez Ameneyro;Erik Cuevas,"Multiple Artificial Intelligence (AI) methods have been proposed over recent years to create controllers to play multiple video games of different nature and complexity without revealing the specific mechanics of each of these games to the AI methods. In recent years, Evolutionary Algorithms (EAs) employing rolling horizon mechanisms have achieved extraordinary results in these type of problems. However, some limitations are present in Rolling Horizon EAs making it a grand challenge of AI. These limitations include the wasteful mechanism of creating a population and evolving it over a fraction of a second to propose an action to be executed by the game agent. Another limitation is to use a scalar value (fitness value) to direct evolutionary search instead of accounting for a mechanism that informs us how a particular agent behaves during the rolling horizon simulation. In this work, we address both of these issues. We introduce the use of a statistical tree that tackles the latter limitation. Furthermore, we tackle the former limitation by employing a mechanism that allows us to seed part of the population using Monte Carlo Tree Search, a method that has dominated multiple General Video Game AI competitions. We show how the proposed novel mechanism, called Statistical Tree-based Population Seeding, achieves better results compared to vanilla Rolling Horizon EAs in a set of 20 games, including 10 stochastic and 10 deterministic games. △ Less","30 August, 2020",https://arxiv.org/pdf/2008.13253
AI-based Modeling and Data-driven Evaluation for Smart Manufacturing Processes,Mohammadhossein Ghahramani;Yan Qiao;MengChu Zhou;Adrian OHagan;James Sweeney,"Smart Manufacturing refers to optimization techniques that are implemented in production operations by utilizing advanced analytics approaches. With the widespread increase in deploying Industrial Internet of Things (IIoT) sensors in manufacturing processes, there is a progressive need for optimal and effective approaches to data management. Embracing Machine Learning and Artificial Intelligence to take advantage of manufacturing data can lead to efficient and intelligent automation. In this paper, we conduct a comprehensive analysis based on Evolutionary Computing and Deep Learning algorithms toward making semiconductor manufacturing smart. We propose a dynamic algorithm for gaining useful insights about semiconductor manufacturing processes and to address various challenges. We elaborate on the utilization of a Genetic Algorithm and Neural Network to propose an intelligent feature selection algorithm. Our objective is to provide an advanced solution for controlling manufacturing processes and to gain perspective on various dimensions that enable manufacturers to access effective predictive technologies. △ Less","29 August, 2020",https://arxiv.org/pdf/2008.12987
Shannon Entropy Rate of Hidden Markov Processes,Alexandra M. Jurgens;James P. Crutchfield,"Hidden Markov chains are widely applied statistical models of stochastic processes, from fundamental physics and chemistry to finance, health, and artificial intelligence. The hidden Markov processes they generate are notoriously complicated, however, even if the chain is finite state: no finite expression for their Shannon entropy rate exists, as the set of their predictive features is generically infinite. As such, to date one cannot make general statements about how random they are nor how structured. Here, we address the first part of this challenge by showing how to efficiently and accurately calculate their entropy rates. We also show how this method gives the minimal set of infinite predictive features. A sequel addresses the challenge's second part on structure. △ Less","28 August, 2020",https://arxiv.org/pdf/2008.12886
A Metamodel and Framework for AGI,Hugo Latapie;Ozkan Kilic,"Can artificial intelligence systems exhibit superhuman performance, but in critical ways, lack the intelligence of even a single-celled organism? The answer is clearly 'yes' for narrow AI systems. Animals, plants, and even single-celled organisms learn to reliably avoid danger and move towards food. This is accomplished via a physical knowledge preserving metamodel that autonomously generates useful models of the world. We posit that preserving the structure of knowledge is critical for higher intelligences that manage increasingly higher levels of abstraction, be they human or artificial. This is the key lesson learned from applying AGI subsystems to complex real-world problems that require continuous learning and adaptation. In this paper, we introduce the Deep Fusion Reasoning Engine (DFRE), which implements a knowledge-preserving metamodel and framework for constructing applied AGI systems. The DFRE metamodel exhibits some important fundamental knowledge preserving properties such as clear distinctions between symmetric and antisymmetric relations, and the ability to create a hierarchical knowledge representation that clearly delineates between levels of abstraction. The DFRE metamodel, which incorporates these capabilities, demonstrates how this approach benefits AGI in specific ways such as managing combinatorial explosion and enabling cumulative, distributed and federated learning. Our experiments show that the proposed framework achieves 94% accuracy on average on unsupervised object detection and recognition. This work is inspired by the state-of-the-art approaches to AGI, recent AGI-aspiring work, the granular computing community, as well as Alfred Korzybski's general semantics. △ Less","6 September, 2020",https://arxiv.org/pdf/2008.12879
From the digital data revolution to digital health and digital economy toward a digital society: Pervasiveness of Artificial Intelligence,Frank Emmert-Streib,"Technological progress has led to powerful computers and communication technologies that penetrate nowadays all areas of science, industry and our private lives. As a consequence, all these areas are generating digital traces of data amounting to big data resources. This opens unprecedented opportunities but also challenges toward the analysis, management, interpretation and utilization of these data. Fortunately, recent breakthroughs in deep learning algorithms complement now machine learning and statistics methods for an efficient analysis of such data. Furthermore, advances in text mining and natural language processing, e.g., word-embedding methods, enable also the processing of large amounts of text data from diverse sources as governmental reports, blog entries in social media or clinical health records of patients. In this paper, we present a perspective on the role of artificial intelligence in these developments and discuss also potential problems we are facing in a digital society. △ Less","3 November, 2020",https://arxiv.org/pdf/2008.12672
Optical oxygen sensing with artificial intelligence,Umberto Michelucci;Michael Baumgartner;Francesca Venturini,"Luminescence-based sensors for measuring oxygen concentration are widely used both in industry and research due to the practical advantages and sensitivity of this type of sensing. The measuring principle is the luminescence quenching by oxygen molecules, which results in a change of the luminescence decay time and intensity. In the classical approach, this change is related to an oxygen concentration using the Stern-Volmer equation. This equation, which in most of the cases is non-linear, is parametrized through device-specific constants. Therefore, to determine these parameters every sensor needs to be precisely calibrated at one or more known concentrations. This work explores an entirely new artificial intelligence approach and demonstrates the feasibility of oxygen sensing through machine learning. The specifically developed neural network learns very efficiently to relate the input quantities to the oxygen concentration. The results show a mean deviation of the predicted from the measured concentration of 0.5 percent air, comparable to many commercial and low-cost sensors. Since the network was trained using synthetically generated data, the accuracy of the model predictions is limited by the ability of the generated data to describe the measured data, opening up future possibilities for significant improvement by using a large number of experimental measurements for training. The approach described in this work demonstrates the applicability of artificial intelligence to sensing of sensors. △ Less","27 July, 2020",https://arxiv.org/pdf/2008.12629
Simulation-supervised deep learning for analysing organelles states and behaviour in living cells,Arif Ahmed Sekh;Ida S. Opstad;Rohit Agarwal;Asa Birna Birgisdottir;Truls Myrmel;Balpreet Singh Ahluwalia;Krishna Agarwal;Dilip K. Prasad,"In many real-world scientific problems, generating ground truth (GT) for supervised learning is almost impossible. The causes include limitations imposed by scientific instrument, physical phenomenon itself, or the complexity of modeling. Performing artificial intelligence (AI) tasks such as segmentation, tracking, and analytics of small sub-cellular structures such as mitochondria in microscopy videos of living cells is a prime example. The 3D blurring function of microscope, digital resolution from pixel size, optical resolution due to the character of light, noise characteristics, and complex 3D deformable shapes of mitochondria, all contribute to making this problem GT hard. Manual segmentation of 100s of mitochondria across 1000s of frames and then across many such videos is not only herculean but also physically inaccurate because of the instrument and phenomena imposed limitations. Unsupervised learning produces less than optimal results and accuracy is important if inferences relevant to therapy are to be derived. In order to solve this unsurmountable problem, we bring modeling and deep learning to a nexus. We show that accurate physics based modeling of microscopy data including all its limitations can be the solution for generating simulated training datasets for supervised learning. We show here that our simulation-supervised segmentation approach is a great enabler for studying mitochondrial states and behaviour in heart muscle cells, where mitochondria have a significant role to play in the health of the cells. We report unprecedented mean IoU score of 91% for binary segmentation (19% better than the best performing unsupervised approach) of mitochondria in actual microscopy videos of living cells. We further demonstrate the possibility of performing multi-class classification, tracking, and morphology associated analytics at the scale of individual mitochondrion. △ Less","26 August, 2020",https://arxiv.org/pdf/2008.12617
An Impact Model of AI on the Principles of Justice: Encompassing the Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Efforts furthering the advancement of Artificial Intelligence (AI) will increasingly encompass AI Legal Reasoning (AILR) as a crucial element in the practice of law. It is argued in this research paper that the infusion of AI into existing and future legal activities and the judicial structure needs to be undertaken by mindfully observing an alignment with the core principles of justice. As such, the adoption of AI has a profound twofold possibility of either usurping the principles of justice, doing so in a Dystopian manner, and yet also capable to bolster the principles of justice, doing so in a Utopian way. By examining the principles of justice across the Levels of Autonomy (LoA) of AI Legal Reasoning, the case is made that there is an ongoing tension underlying the efforts to develop and deploy AI that can demonstrably determine the impacts and sway upon each core principle of justice and the collective set. △ Less","26 August, 2020",https://arxiv.org/pdf/2008.12615
Biomechanic Posture Stabilisation via Iterative Training of Multi-policy Deep Reinforcement Learning Agents,Mohammed Hossny;Julie Iskander,"It is not until we become senior citizens do we recognise how much we took maintaining a simple standing posture for granted. It is truly fascinating to observe the magnitude of control the human brain exercises, in real time, to activate and deactivate the lower body muscles and solve a multi-link 3D inverted pendulum problem in order to maintain a stable standing posture. This realisation is even more apparent when training an artificial intelligence (AI) agent to maintain a standing posture of a digital musculoskeletal avatar due to the error propagation problem. In this work we address the error propagation problem by introducing an iterative training procedure for deep reinforcement learning which allows the agent to learn a finite set of actions and how to coordinate between them in order to achieve a stable standing posture. The proposed training approach allowed the agent to increase standing duration from 4 seconds using the traditional training method to 348 seconds using the proposed method. The proposed training method allowed the agent to generalise and accommodate perception and actuation noise for almost 108 seconds. △ Less","21 August, 2020",https://arxiv.org/pdf/2008.12210
Computational Models of Human Decision-Making with Application to the Internet of Everything,Setareh Maghsudi;Max Davy,"The concept of the Internet of Things (IoT) first appeared a few decades ago. Today, by the ubiquitous wireless connectivity, the boost of machine learning and artificial intelligence, and the advances in big data analytics, it is safe to say that IoT has evolved to a new concept called the Internet of Everything (IoE) or the Internet of All. IoE has four pillars: Things, human, data, and processes, which render it as an inhomogeneous large-scale network. A crucial challenge of such a network is to develop management, analysis, and optimization policies that besides utility-maximizer machines, also take irrational humans into account. We discuss several networking applications in which appropriate modeling of human decision-making is vital. We then provide a brief review of computational models of human decision-making. Based on one such model, we develop a solution for a task offloading problem in fog computing and we analyze the implications of including humans in the loop. △ Less","27 August, 2020",https://arxiv.org/pdf/2008.11958
Relation/Entity-Centric Reading Comprehension,Takeshi Onishi,"Constructing a machine that understands human language is one of the most elusive and long-standing challenges in artificial intelligence. This thesis addresses this challenge through studies of reading comprehension with a focus on understanding entities and their relationships. More specifically, we focus on question answering tasks designed to measure reading comprehension. We focus on entities and relations because they are typically used to represent the semantics of natural language. △ Less","27 August, 2020",https://arxiv.org/pdf/2008.11940
CLAN: Continuous Learning using Asynchronous Neuroevolution on Commodity Edge Devices,Parth Mannan;Ananda Samajdar;Tushar Krishna,"Recent advancements in machine learning algorithms, especially the development of Deep Neural Networks (DNNs) have transformed the landscape of Artificial Intelligence (AI). With every passing day, deep learning based methods are applied to solve new problems with exceptional results. The portal to the real world is the edge. The true impact of AI can only be fully realized if we can have AI agents continuously interacting with the real world and solving everyday problems. Unfortunately, high compute and memory requirements of DNNs acts a huge barrier towards this vision. Today we circumvent this problem by deploying special purpose inference hardware on the edge while procuring trained models from the cloud. This approach, however, relies on constant interaction with the cloud for transmitting all the data, training on massive GPU clusters, and downloading updated models. This is challenging for bandwidth, privacy, and constant connectivity concerns that autonomous agents may exhibit. In this paper we evaluate techniques for enabling adaptive intelligence on edge devices with zero interaction with any high-end cloud/server. We build a prototype distributed system of Raspberry Pis communicating via WiFi running NeuroEvolutionary (NE) learning and inference. We evaluate the performance of such a collaborative system and detail the compute/communication characteristics of different arrangements of the system that trade-off parallelism versus communication. Using insights from our analysis, we also propose algorithmic modifications to reduce communication by up to 3.6x during the learning phase to enhance scalability even further and match performance of higher end computing devices at scale. We believe that these insights will enable algorithm-hardware co-design efforts for enabling continuous learning on the edge. △ Less","26 August, 2020",https://arxiv.org/pdf/2008.11881
Optimising AI Training Deployments using Graph Compilers and Containers,Nina Mujkanovic;Karthee Sivalingam;Alfio Lazzaro,"Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN) or Deep Learning (DL) have become popular due to their success in solving problems likeimage analysis and speech recognition. Training a DNN is computationally intensive and High Performance Computing(HPC) has been a key driver in AI growth. Virtualisation and container technology have led to the convergence of cloud and HPC infrastructure. These infrastructures with diverse hardware increase the complexity of deploying and optimising AI training workloads. AI training deployments in HPC or cloud can be optimised with target-specific libraries, graph compilers, andby improving data movement or IO. Graph compilers aim to optimise the execution of a DNN graph by generating an optimised code for a target hardware/backend. As part of SODALITE (a Horizon 2020 project), MODAK tool is developed to optimise application deployment in software defined infrastructures. Using input from the data scientist and performance modelling, MODAK maps optimal application parameters to a target infrastructure and builds an optimised container. In this paper, we introduce MODAK and review container technologies and graph compilers for AI. We illustrate optimisation of AI training deployments using graph compilers and Singularity containers. Evaluation using MNIST-CNN and ResNet50 training workloads shows that custom built optimised containers outperform the official images from DockerHub. We also found that the performance of graph compilers depends on the target hardware and the complexity of the neural network. △ Less","17 September, 2020",https://arxiv.org/pdf/2008.11675
An 8-bit In Resistive Memory Computing Core with Regulated Passive Neuron and Bit Line Weight Mapping,Yewei Zhang;Kejie Huang;Rui Xiao;Haibin Shen,"The rapid development of Artificial Intelligence (AI) and Internet of Things (IoT) increases the requirement for edge computing with low power and relatively high processing speed devices. The Computing-In-Memory(CIM) schemes based on emerging resistive Non-Volatile Memory(NVM) show great potential in reducing the power consumption for AI computing. However, the device inconsistency of the non-volatile memory may significantly degenerate the performance of the neural network. In this paper, we propose a low power Resistive RAM (RRAM) based CIM core to not only achieve high computing efficiency but also greatly enhance the robustness by bit line regulator and bit line weight mapping algorithm. The simulation results show that the power consumption of our proposed 8-bit CIM core is only 3.61mW (256*256). The SFDR and SNDR of the CIM core achieve 59.13 dB and 46.13 dB, respectively. The proposed bit line weight mapping scheme improves the top-1 accuracy by 2.46% and 3.47% for AlexNet and VGG16 on ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC 2012) in 8-bit mode, respectively. △ Less","26 August, 2020",https://arxiv.org/pdf/2008.11669
Large-scale neuromorphic optoelectronic computing with a reconfigurable diffractive processing unit,Tiankuang Zhou;Xing Lin;Jiamin Wu;Yitong Chen;Hao Xie;Yipeng Li;Jintao Fan;Huaqiang Wu;Lu Fang;Qionghai Dai,"Application-specific optical processors have been considered disruptive technologies for modern computing that can fundamentally accelerate the development of artificial intelligence (AI) by offering substantially improved computing performance. Recent advancements in optical neural network architectures for neural information processing have been applied to perform various machine learning tasks. However, the existing architectures have limited complexity and performance; and each of them requires its own dedicated design that cannot be reconfigured to switch between different neural network models for different applications after deployment. Here, we propose an optoelectronic reconfigurable computing paradigm by constructing a diffractive processing unit (DPU) that can efficiently support different neural networks and achieve a high model complexity with millions of neurons. It allocates almost all of its computational operations optically and achieves extremely high speed of data modulation and large-scale network parameter updating by dynamically programming optical modulators and photodetectors. We demonstrated the reconfiguration of the DPU to implement various diffractive feedforward and recurrent neural networks and developed a novel adaptive training approach to circumvent the system imperfections. We applied the trained networks for high-speed classifying of handwritten digit images and human action videos over benchmark datasets, and the experimental results revealed a comparable classification accuracy to the electronic computing approaches. Furthermore, our prototype system built with off-the-shelf optoelectronic components surpasses the performance of state-of-the-art graphics processing units (GPUs) by several times on computing speed and more than an order of magnitude on system energy efficiency. △ Less","26 August, 2020",https://arxiv.org/pdf/2008.11659
Few Shot Text-Independent speaker verification using 3D-CNN,Prateek Mishra,"Facial recognition system is one of the major successes of Artificial intelligence and has been used a lot over the last years. But, images are not the only biometric present: audio is another possible biometric that can be used as an alternative to the existing recognition systems. However, the text-independent audio data is not always available for tasks like speaker verification and also no work has been done in the past for text-independent speaker verification assuming very little training data. Therefore, In this paper, we have proposed a novel method to verify the identity of the claimed speaker using very few training data. To achieve this we are using a Siamese neural network with center loss and speaker bias loss. Experiments conducted on the VoxCeleb1 dataset show that the proposed model accuracy even on training with very few data is near to the state of the art model on text-independent speaker verification △ Less","25 August, 2020",https://arxiv.org/pdf/2008.11088
MyPDDL: Tools for efficiently creating PDDL domains and problems,Volker Strobel;Alexandra Kirsch,"The Planning Domain Definition Language (PDDL) is the state-of-the-art language for specifying planning problems in artificial intelligence research. Writing and maintaining these planning problems, however, can be time-consuming and error prone. To address this issue, we present myPDDL-a modular toolkit for developing and manipulating PDDL domains and problems. To evaluate myPDDL, we compare its features to existing knowledge engineering tools for PDDL. In a user test, we additionally assess two of its modules, namely the syntax highlighting feature and the type diagram generator. The users of syntax highlighting detected 36% more errors than non-users in an erroneous domain file. The average time on task for questions on a PDDL type hierarchy was reduced by 48% when making the type diagram generator available. This implies that myPDDL can support knowledge engineers well in the PDDL design and analysis process. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.11069
Applications of Artificial Intelligence in Live Action Role-Playing Games (LARP),Christoph Salge;Emily Short;Mike Preuss;Spyridion Samothrakis;Pieter Spronck,"Live Action Role-Playing (LARP) games and similar experiences are becoming a popular game genre. Here, we discuss how artificial intelligence techniques, particularly those commonly used in AI for Games, could be applied to LARP. We discuss the specific properties of LARP that make it a surprisingly suitable application field, and provide a brief overview of some existing approaches. We then outline several directions where utilizing AI seems beneficial, by both making LARPs easier to organize, and by enhancing the player experience with elements not possible without AI. △ Less","25 August, 2020",https://arxiv.org/pdf/2008.11003
Contextualized moral inference,Jing Yi Xie;Graeme Hirst;Yang Xu,"Developing moral awareness in intelligent systems has shifted from a topic of philosophical inquiry to a critical and practical issue in artificial intelligence over the past decades. However, automated inference of everyday moral situations remains an under-explored problem. We present a text-based approach that predicts people's intuitive judgment of moral vignettes. Our methodology builds on recent work in contextualized language models and textual inference of moral sentiment. We show that a contextualized representation offers a substantial advantage over alternative representations based on word embeddings and emotion sentiment in inferring human moral judgment, evaluated and reflected in three independent datasets from moral psychology. We discuss the promise and limitations of our approach toward automated textual moral reasoning. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.10762
Unsupervised Multi-Modal Representation Learning for Affective Computing with Multi-Corpus Wearable Data,Kyle Ross;Paul Hungler;Ali Etemad,"With recent developments in smart technologies, there has been a growing focus on the use of artificial intelligence and machine learning for affective computing to further enhance the user experience through emotion recognition. Typically, machine learning models used for affective computing are trained using manually extracted features from biological signals. Such features may not generalize well for large datasets and may be sub-optimal in capturing the information from the raw input data. One approach to address this issue is to use fully supervised deep learning methods to learn latent representations of the biosignals. However, this method requires human supervision to label the data, which may be unavailable or difficult to obtain. In this work we propose an unsupervised framework reduce the reliance on human supervision. The proposed framework utilizes two stacked convolutional autoencoders to learn latent representations from wearable electrocardiogram (ECG) and electrodermal activity (EDA) signals. These representations are utilized within a random forest model for binary arousal classification. This approach reduces human supervision and enables the aggregation of datasets allowing for higher generalizability. To validate this framework, an aggregated dataset comprised of the AMIGOS, ASCERTAIN, CLEAS, and MAHNOB-HCI datasets is created. The results of our proposed method are compared with using convolutional neural networks, as well as methods that employ manual extraction of hand-crafted features. The methodology used for fusing the two modalities is also investigated. Lastly, we show that our method outperforms current state-of-the-art results that have performed arousal detection on the same datasets using ECG and EDA biosignals. The results show the wide-spread applicability for stacked convolutional autoencoders to be used with machine learning for affective computing. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.10726
Multidimensionality of Legal Singularity: Parametric Analysis and the Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Legal scholars have in the last several years embarked upon an ongoing discussion and debate over a potential Legal Singularity that might someday occur, involving a variant or law-domain offshoot leveraged from the Artificial Intelligence (AI) realm amid its many decades of deliberations about an overarching and generalized technological singularity (referred to classically as The Singularity). This paper examines the postulated Legal Singularity and proffers that such AI and Law cogitations can be enriched by these three facets addressed herein: (1) dovetail additionally salient considerations of The Singularity into the Legal Singularity, (2) make use of an in-depth and innovative multidimensional parametric analysis of the Legal Singularity as posited in this paper, and (3) align and unify the Legal Singularity with the Levels of Autonomy (LoA) associated with AI Legal Reasoning (AILR) as propounded in this paper. △ Less","24 August, 2020",https://arxiv.org/pdf/2008.10575
PermuteAttack: Counterfactual Explanation of Machine Learning Credit Scorecards,Masoud Hashemi;Ali Fathi,"This paper is a note on new directions and methodologies for validation and explanation of Machine Learning (ML) models employed for retail credit scoring in finance. Our proposed framework draws motivation from the field of Artificial Intelligence (AI) security and adversarial ML where the need for certifying the performance of the ML algorithms in the face of their overwhelming complexity poses a need for rethinking the traditional notions of model architecture selection, sensitivity analysis and stress testing. Our point of view is that the phenomenon of adversarial perturbations when detached from the AI security domain, has purely algorithmic roots and fall within the scope of model risk assessment. We propose a model criticism and explanation framework based on adversarially generated counterfactual examples for tabular data. A counterfactual example to a given instance in this context is defined as a synthetically generated data point sampled from the estimated data distribution which is treated differently by a model. The counterfactual examples can be used to provide a black-box instance-level explanation of the model behaviour as well as studying the regions in the input space where the model performance deteriorates. Adversarial example generating algorithms are extensively studied in the image and natural language processing (NLP) domains. However, most financial data come in tabular format and naive application of the existing techniques on this class of datasets generates unrealistic samples. In this paper, we propose a counterfactual example generation method capable of handling tabular data including discrete and categorical variables. Our proposed algorithm uses a gradient-free optimization based on genetic algorithms and therefore is applicable to any classification model. △ Less","28 August, 2020",https://arxiv.org/pdf/2008.10138
m2caiSeg: Semantic Segmentation of Laparoscopic Images using Convolutional Neural Networks,Salman Maqbool;Aqsa Riaz;Hasan Sajid;Osman Hasan,"Autonomous surgical procedures, in particular minimal invasive surgeries, are the next frontier for Artificial Intelligence research. However, the existing challenges include precise identification of the human anatomy and the surgical settings, and modeling the environment for training of an autonomous agent. To address the identification of human anatomy and the surgical settings, we propose a deep learning based semantic segmentation algorithm to identify and label the tissues and organs in the endoscopic video feed of the human torso region. We present an annotated dataset, m2caiSeg, created from endoscopic video feeds of real-world surgical procedures. Overall, the data consists of 307 images, each of which is annotated for the organs and different surgical instruments present in the scene. We propose and train a deep convolutional neural network for the semantic segmentation task. To cater for the low quantity of annotated data, we use unsupervised pre-training and data augmentation. The trained model is evaluated on an independent test set of the proposed dataset. We obtained a F1 score of 0.33 while using all the labeled categories for the semantic segmentation task. Secondly, we labeled all instruments into an 'Instruments' superclass to evaluate the model's performance on discerning the various organs and obtained a F1 score of 0.57. We propose a new dataset and a deep learning method for pixel level identification of various organs and instruments in a endoscopic surgical scene. Surgical scene understanding is one of the first steps towards automating surgical procedures. △ Less","10 December, 2020",https://arxiv.org/pdf/2008.10134
Authorized and Unauthorized Practices of Law: The Role of Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Advances in Artificial Intelligence (AI) and Machine Learning (ML) that are being applied to legal efforts have raised controversial questions about the existent restrictions imposed on the practice-of-law. Generally, the legal field has sought to define Authorized Practices of Law (APL) versus Unauthorized Practices of Law (UPL), though the boundaries are at times amorphous and some contend capricious and self-serving, rather than being devised holistically for the benefit of society all told. A missing ingredient in these arguments is the realization that impending legal profession disruptions due to AI can be more robustly discerned by examining the matter through the lens of a framework utilizing the autonomous levels of AI Legal Reasoning (AILR). This paper explores a newly derived instrumental grid depicting the key characteristics underlying APL and UPL as they apply to the AILR autonomous levels and offers key insights for the furtherance of these crucial practice-of-law debates. △ Less","19 August, 2020",https://arxiv.org/pdf/2008.09507
Utilizing Explainable AI for Quantization and Pruning of Deep Neural Networks,Muhammad Sabih;Frank Hannig;Juergen Teich,"For many applications, utilizing DNNs (Deep Neural Networks) requires their implementation on a target architecture in an optimized manner concerning energy consumption, memory requirement, throughput, etc. DNN compression is used to reduce the memory footprint and complexity of a DNN before its deployment on hardware. Recent efforts to understand and explain AI (Artificial Intelligence) methods have led to a new research area, termed as explainable AI. Explainable AI methods allow us to understand better the inner working of DNNs, such as the importance of different neurons and features. The concepts from explainable AI provide an opportunity to improve DNN compression methods such as quantization and pruning in several ways that have not been sufficiently explored so far. In this paper, we utilize explainable AI methods: mainly DeepLIFT method. We use these methods for (1) pruning of DNNs; this includes structured and unstructured pruning of \ac{CNN} filters pruning as well as pruning weights of fully connected layers, (2) non-uniform quantization of DNN weights using clustering algorithm; this is also referred to as Weight Sharing, and (3) integer-based mixed-precision quantization; this is where each layer of a DNN may use a different number of integer bits. We use typical image classification datasets with common deep learning image classification models for evaluation. In all these three cases, we demonstrate significant improvements as well as new insights and opportunities from the use of explainable AI in DNN compression. △ Less","20 August, 2020",https://arxiv.org/pdf/2008.09072
"Considerations, Good Practices, Risks and Pitfalls in Developing AI Solutions Against COVID-19",Alexandra Luccioni;Joseph Bullock;Katherine Hoffmann Pham;Cynthia Sin Nga Lam;Miguel Luengo-Oroz,"The COVID-19 pandemic has been a major challenge to humanity, with 12.7 million confirmed cases as of July 13th, 2020 [1]. In previous work, we described how Artificial Intelligence can be used to tackle the pandemic with applications at the molecular, clinical, and societal scales [2]. In the present follow-up article, we review these three research directions, and assess the level of maturity and feasibility of the approaches used, as well as their potential for operationalization. We also summarize some commonly encountered risks and practical pitfalls, as well as guidelines and best practices for formulating and deploying AI applications at different scales. △ Less","13 August, 2020",https://arxiv.org/pdf/2008.09043
Generative chemistry: drug discovery with deep learning generative models,Yuemin Bian;Xiang-Qun Xie,"The de novo design of molecular structures using deep learning generative models introduces an encouraging solution to drug discovery in the face of the continuously increased cost of new drug development. From the generation of original texts, images, and videos, to the scratching of novel molecular structures, the incredible creativity of deep learning generative models surprised us about the height machine intelligence can achieve. The purpose of this paper is to review the latest advances in generative chemistry which relies on generative modeling to expedite the drug discovery process. This review starts with a brief history of artificial intelligence in drug discovery to outline this emerging paradigm. Commonly used chemical databases, molecular representations, and tools in cheminformatics and machine learning are covered as the infrastructure for the generative chemistry. The detailed discussions on utilizing cutting-edge generative architectures, including recurrent neural network, variational autoencoder, adversarial autoencoder, and generative adversarial network for compound generation are focused. Challenges and future perspectives follow. △ Less","20 August, 2020",https://arxiv.org/pdf/2008.09000
Mediating Community-AI Interaction through Situated Explanation: The Case of AI-Led Moderation,Yubo Kou;Xinning Gui,"Artificial intelligence (AI) has become prevalent in our everyday technologies and impacts both individuals and communities. The explainable AI (XAI) scholarship has explored the philosophical nature of explanation and technical explanations, which are usually driven by experts in lab settings and can be challenging for laypersons to understand. In addition, existing XAI research tends to focus on the individual level. Little is known about how people understand and explain AI-led decisions in the community context. Drawing from XAI and activity theory, a foundational HCI theory, we theorize how explanation is situated in a community's shared values, norms, knowledge, and practices, and how situated explanation mediates community-AI interaction. We then present a case study of AI-led moderation, where community members collectively develop explanations of AI-led decisions, most of which are automated punishments. Lastly, we discuss the implications of this framework at the intersection of CSCW, HCI, and XAI. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.08202
Clustering and Analysis of Vulnerabilities Present in Different Robot Types,Chinwe Ekenna;Bharvee Acharya,"Due to the new advancements in automation using Artificial Intelligence, Robotics and Internet of Things it has become crucial to pay attention to possible vulnerabilities in order to avoid cyber attack and hijacking that can occur which can be catastrophic. There have been many consequences of disasters due to vulnerabilities in Robotics, these vulnerabilities need to be analyzed to target the severe ones before they cause cataclysm. This paper aims to highlight the areas and severity of each type of vulnerability by analyzing issues categorized under the type of vulnerability. This we achieve by careful analysis of the data and application of information retrieval techniques like Term Frequency - Inverse Document Frequency, dimension reduction techniques like Principal Component Analysis and Clustering using Machine Learning techniques like K-means. By performing this analysis, the severity of robotic issues in different domains and the severity of the issue based on type of issue is detected. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.08166
Benchmarking network fabrics for data distributed training of deep neural networks,Siddharth Samsi;Andrew Prout;Michael Jones;Andrew Kirby;Bill Arcand;Bill Bergeron;David Bestor;Chansup Byun;Vijay Gadepally;Michael Houle;Matthew Hubbell;Anna Klein;Peter Michaleas;Lauren Milechin;Julie Mullen;Antonio Rosa;Charles Yee;Albert Reuther;Jeremy Kepner,"Artificial Intelligence/Machine Learning applications require the training of complex models on large amounts of labelled data. The large computational requirements for training deep models have necessitated the development of new methods for faster training. One such approach is the data parallel approach, where the training data is distributed across multiple compute nodes. This approach is simple to implement and supported by most of the commonly used machine learning frameworks. The data parallel approach leverages MPI for communicating gradients across all nodes. In this paper, we examine the effects of using different physical hardware interconnects and network-related software primitives for enabling data distributed deep learning. We compare the effect of using GPUDirect and NCCL on Ethernet and OmniPath fabrics. Our results show that using Ethernet-based networking in shared HPC systems does not have a significant effect on the training times for commonly used deep neural network architectures or traditional HPC applications such as Computational Fluid Dynamics. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.08057
Neural networks in day-ahead electricity price forecasting: Single vs. multiple outputs,Grzegorz Marcjasz;Jesus Lago;Rafał Weron,"Recent advancements in the fields of artificial intelligence and machine learning methods resulted in a significant increase of their popularity in the literature, including electricity price forecasting. Said methods cover a very broad spectrum, from decision trees, through random forests to various artificial neural network models and hybrid approaches. In electricity price forecasting, neural networks are the most popular machine learning method as they provide a non-linear counterpart for well-tested linear regression models. Their application, however, is not straightforward, with multiple implementation factors to consider. One of such factors is the network's structure. This paper provides a comprehensive comparison of two most common structures when using the deep neural networks -- one that focuses on each hour of the day separately, and one that reflects the daily auction structure and models vectors of the prices. The results show a significant accuracy advantage of using the latter, confirmed on data from five distinct power exchanges. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.08006
XNAP: Making LSTM-based Next Activity Predictions Explainable by Using LRP,Sven Weinzierl;Sandra Zilker;Jens Brunk;Kate Revoredo;Martin Matzner;Jörg Becker,"Predictive business process monitoring (PBPM) is a class of techniques designed to predict behaviour, such as next activities, in running traces. PBPM techniques aim to improve process performance by providing predictions to process analysts, supporting them in their decision making. However, the PBPM techniques` limited predictive quality was considered as the essential obstacle for establishing such techniques in practice. With the use of deep neural networks (DNNs), the techniques` predictive quality could be improved for tasks like the next activity prediction. While DNNs achieve a promising predictive quality, they still lack comprehensibility due to their hierarchical approach of learning representations. Nevertheless, process analysts need to comprehend the cause of a prediction to identify intervention mechanisms that might affect the decision making to secure process performance. In this paper, we propose XNAP, the first explainable, DNN-based PBPM technique for the next activity prediction. XNAP integrates a layer-wise relevance propagation method from the field of explainable artificial intelligence to make predictions of a long short-term memory DNN explainable by providing relevance values for activities. We show the benefit of our approach through two real-life event logs. △ Less","23 December, 2020",https://arxiv.org/pdf/2008.07993
Turing Test and the Practice of Law: The Role of Autonomous Levels of AI Legal Reasoning,Lance Eliot,"Artificial Intelligence (AI) is increasingly being applied to law and a myriad of legal tasks amid attempts to bolster AI Legal Reasoning (AILR) autonomous capabilities. A major question that has generally been unaddressed involves how we will know when AILR has achieved autonomous capacities. The field of AI has grappled with similar quandaries over how to assess the attainment of Artificial General Intelligence (AGI), a persistently discussed issue among scholars since the inception of AI, with the Turing Test communally being considered as the bellwether for ascertaining such matters. This paper proposes a variant of the Turing Test that is customized for specific use in the AILR realm, including depicting how this famous gold standard of AI fulfillment can be robustly applied across the autonomous levels of AI Legal Reasoning. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.07743
Trust and Medical AI: The challenges we face and the expertise needed to overcome them,Thomas P. Quinn;Manisha Senadeera;Stephan Jacobs;Simon Coghlan;Vuong Le,"Artificial intelligence (AI) is increasingly of tremendous interest in the medical field. However, failures of medical AI could have serious consequences for both clinical outcomes and the patient experience. These consequences could erode public trust in AI, which could in turn undermine trust in our healthcare institutions. This article makes two contributions. First, it describes the major conceptual, technical, and humanistic challenges in medical AI. Second, it proposes a solution that hinges on the education and accreditation of new expert groups who specialize in the development, verification, and operation of medical AI technologies. These groups will be required to maintain trust in our healthcare institutions. △ Less","18 August, 2020",https://arxiv.org/pdf/2008.07734
A Survey on the Use of AI and ML for Fighting the COVID-19 Pandemic,Muhammad Nazrul Islam;Toki Tahmid Inan;Suzzana Rafi;Syeda Sabrina Akter;Iqbal H. Sarker;A. K. M. Najmul Islam,"Artificial intelligence (AI) and machine learning (ML) have made a paradigm shift in health care which, eventually can be used for decision support and forecasting by exploring the medical data. Recent studies showed that AI and ML can be used to fight against the COVID-19 pandemic. Therefore, the objective of this review study is to summarize the recent AI and ML based studies that have focused to fight against COVID-19 pandemic. From an initial set of 634 articles, a total of 35 articles were finally selected through an extensive inclusion-exclusion process. In our review, we have explored the objectives/aims of the existing studies (i.e., the role of AI/ML in fighting COVID-19 pandemic); context of the study (i.e., study focused to a specific country-context or with a global perspective); type and volume of dataset; methodology, algorithms or techniques adopted in the prediction or diagnosis processes; and mapping the algorithms/techniques with the data type highlighting their prediction/classification accuracy. We particularly focused on the uses of AI/ML in analyzing the pandemic data in order to depict the most recent progress of AI for fighting against COVID-19 and pointed out the potential scope of further research. △ Less","3 August, 2020",https://arxiv.org/pdf/2008.07449
An Overview on the Web of Clinical Data,Marco Gori,"In the last few years there has been an impressive growth of connections between medicine and artificial intelligence (AI) that have been characterized by the specific focus on single problems along with corresponding clinical data. This paper proposes a new perspective in which the focus is on the progressive accumulation of a universal repository of clinical hyperlinked data in the spirit that gave rise to the birth of the Web. The underlining idea is that this repository, that is referred to as the Web of Clinical Data (WCD), will dramatically change the AI approach to medicine and its effectiveness. It is claimed that research and AI-based applications will undergo an evolution process that will likely reinforce systematically the solutions implemented in medical apps made available in the WCD. The distinctive architectural feature of the WCD is that this universal repository will be under control of clinical units and hospitals, which is claimed to be the natural context for dealing with the critical issues of clinical data. △ Less","14 August, 2020",https://arxiv.org/pdf/2008.07432
A Patient-Centric Dataset of Images and Metadata for Identifying Melanomas Using Clinical Context,Veronica Rotemberg;Nicholas Kurtansky;Brigid Betz-Stablein;Liam Caffery;Emmanouil Chousakos;Noel Codella;Marc Combalia;Stephen Dusza;Pascale Guitera;David Gutman;Allan Halpern;Harald Kittler;Kivanc Kose;Steve Langer;Konstantinos Lioprys;Josep Malvehy;Shenara Musthaq;Jabpani Nanda;Ofer Reiter;George Shih;Alexander Stratigos;Philipp Tschandl;Jochen Weber;H. Peter Soyer,"Prior skin image datasets have not addressed patient-level information obtained from multiple skin lesions from the same patient. Though artificial intelligence classification algorithms have achieved expert-level performance in controlled studies examining single images, in practice dermatologists base their judgment holistically from multiple lesions on the same patient. The 2020 SIIM-ISIC Melanoma Classification challenge dataset described herein was constructed to address this discrepancy between prior challenges and clinical practice, providing for each image in the dataset an identifier allowing lesions from the same patient to be mapped to one another. This patient-level contextual information is frequently used by clinicians to diagnose melanoma and is especially useful in ruling out false positives in patients with many atypical nevi. The dataset represents 2,056 patients from three continents with an average of 16 lesions per patient, consisting of 33,126 dermoscopic images and 584 histopathologically confirmed melanomas compared with benign melanoma mimickers. △ Less","7 August, 2020",https://arxiv.org/pdf/2008.07360
"Data, Power and Bias in Artificial Intelligence",Susan Leavy;Barry O'Sullivan;Eugenia Siapera,"Artificial Intelligence has the potential to exacerbate societal bias and set back decades of advances in equal rights and civil liberty. Data used to train machine learning algorithms may capture social injustices, inequality or discriminatory attitudes that may be learned and perpetuated in society. Attempts to address this issue are rapidly emerging from different perspectives involving technical solutions, social justice and data governance measures. While each of these approaches are essential to the development of a comprehensive solution, often discourse associated with each seems disparate. This paper reviews ongoing work to ensure data justice, fairness and bias mitigation in AI systems from different domains exploring the interrelated dynamics of each and examining whether the inevitability of bias in AI training data may in fact be used for social good. We highlight the complexity associated with defining policies for dealing with bias. We also consider technical challenges in addressing issues of societal bias. △ Less","28 July, 2020",https://arxiv.org/pdf/2008.07341
An Ontological AI-and-Law Framework for the Autonomous Levels of AI Legal Reasoning,Lance Eliot,"A framework is proposed that seeks to identify and establish a set of robust autonomous levels articulating the realm of Artificial Intelligence and Legal Reasoning (AILR). Doing so provides a sound and parsimonious basis for being able to assess progress in the application of AI to the law, and can be utilized by scholars in academic pursuits of AI legal reasoning, along with being used by law practitioners and legal professionals in gauging how advances in AI are aiding the practice of law and the realization of aspirational versus achieved results. A set of seven levels of autonomy for AI and Legal Reasoning are meticulously proffered and mindfully discussed. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.07328
Progressing Towards Responsible AI,Teresa Scantamburlo;Atia Cortés;Marie Schacht,"The field of Artificial Intelligence (AI) and, in particular, the Machine Learning area, counts on a wide range of performance metrics and benchmark data sets to assess the problem-solving effectiveness of its solutions. However, the appearance of research centres, projects or institutions addressing AI solutions from a multidisciplinary and multi-stakeholder perspective suggests a new approach to assessment comprising ethical guidelines, reports or tools and frameworks to help both academia and business to move towards a responsible conceptualisation of AI. They all highlight the relevance of three key aspects: (i) enhancing cooperation among the different stakeholders involved in the design, deployment and use of AI; (ii) promoting multidisciplinary dialogue, including different domains of expertise in this process; and (iii) fostering public engagement to maximise a trusted relation with new technologies and practitioners. In this paper, we introduce the Observatory on Society and Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU aimed at stimulating reflection on a broad spectrum of issues of AI (ethical, legal, social, economic and cultural). In particular, we describe our work in progress around OSAI and suggest how this and similar initiatives can promote a wider appraisal of progress in AI. This will give us the opportunity to present our vision and our modus operandi to enhance the implementation of these three fundamental dimensions. △ Less","11 August, 2020",https://arxiv.org/pdf/2008.07326
Expected Utilitarianism,Heather M. Roff,"We want artificial intelligence (AI) to be beneficial. This is the grounding assumption of most of the attitudes towards AI research. We want AI to be ""good"" for humanity. We want it to help, not hinder, humans. Yet what exactly this entails in theory and in practice is not immediately apparent. Theoretically, this declarative statement subtly implies a commitment to a consequentialist ethics. Practically, some of the more promising machine learning techniques to create a robust AI, and perhaps even an artificial general intelligence (AGI) also commit one to a form of utilitarianism. In both dimensions, the logic of the beneficial AI movement may not in fact create ""beneficial AI"" in either narrow applications or in the form of AGI if the ethical assumptions are not made explicit and clear. Additionally, as it is likely that reinforcement learning (RL) will be an important technique for machine learning in this area, it is also important to interrogate how RL smuggles in a particular type of consequentialist reasoning into the AI: particularly, a brute form of hedonistic act utilitarianism. Since the mathematical logic commits one to a maximization function, the result is that an AI will inevitably be seeking more and more rewards. We have two conclusions that arise from this. First, is that if one believes that a beneficial AI is an ethical AI, then one is committed to a framework that posits 'benefit' is tantamount to the greatest good for the greatest number. Second, if the AI relies on RL, then the way it reasons about itself, the environment, and other agents, will be through an act utilitarian morality. This proposition may, or may not, in fact be actually beneficial for humanity. △ Less","19 July, 2020",https://arxiv.org/pdf/2008.07321
Bias and Discrimination in AI: a cross-disciplinary perspective,Xavier Ferrer;Tom van Nuenen;Jose M. Such;Mark Coté;Natalia Criado,"With the widespread and pervasive use of Artificial Intelligence (AI) for automated decision-making systems, AI bias is becoming more apparent and problematic. One of its negative consequences is discrimination: the unfair, or unequal treatment of individuals based on certain characteristics. However, the relationship between bias and discrimination is not always clear. In this paper, we survey relevant literature about bias and discrimination in AI from an interdisciplinary perspective that embeds technical, legal, social and ethical dimensions. We show that finding solutions to bias and discrimination in AI requires robust cross-disciplinary collaborations. △ Less","11 August, 2020",https://arxiv.org/pdf/2008.07309
A Standardized Radiograph-Agnostic Framework and Platform For Evaluating AI Radiological Systems,Darlington Ahiale Akogo,"Radiology has been essential to accurately diagnosing diseases and assessing responses to treatment. The challenge however lies in the shortage of radiologists globally. As a response to this, a number of Artificial Intelligence solutions are being developed. The challenge Artificial Intelligence radiological solutions however face is the lack of a benchmarking and evaluation standard, and the difficulties of collecting diverse data to truly assess the ability of such systems to generalise and properly handle edge cases. We are proposing a radiograph-agnostic platform and framework that would allow any Artificial Intelligence radiological solution to be assessed on its ability to generalise across diverse geographical location, gender and age groups. △ Less","2 August, 2020",https://arxiv.org/pdf/2008.07276
Explainability in Deep Reinforcement Learning,Alexandre Heuillet;Fabien Couthouis;Natalia Díaz-Rodríguez,"A large set of the explainable Artificial Intelligence (XAI) literature is emerging on feature relevance techniques to explain a deep neural network (DNN) output or explaining models that ingest image source data. However, assessing how XAI techniques can help understand models beyond classification tasks, e.g. for reinforcement learning (RL), has not been extensively studied. We review recent works in the direction to attain Explainable Reinforcement Learning (XRL), a relatively new subfield of Explainable Artificial Intelligence, intended to be used in general public applications, with diverse audiences, requiring ethical, responsible and trustable algorithms. In critical situations where it is essential to justify and explain the agent's behaviour, better explainability and interpretability of RL models could help gain scientific insight on the inner workings of what is still considered a black box. We evaluate mainly studies directly linking explainability to RL, and split these into two categories according to the way the explanations are generated: transparent algorithms and post-hoc explainaility. We also review the most prominent XAI works from the lenses of how they could potentially enlighten the further deployment of the latest advances in RL, in the demanding present and future of everyday problems. △ Less","18 December, 2020",https://arxiv.org/pdf/2008.06693
Survey of XAI in digital pathology,Milda Pocevičiūtė;Gabriel Eilertsen;Claes Lundström,"Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of AI to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions. △ Less","14 August, 2020",https://arxiv.org/pdf/2008.06353
An Improved Deep Convolutional Neural Network-Based Autonomous Road Inspection Scheme Using Unmanned Aerial Vehicles,Syed Ali Hassan;Tariq Rahim;Soo Young Shin,"Advancements in artificial intelligence (AI) gives a great opportunity to develop an autonomous devices. The contribution of this work is an improved convolutional neural network (CNN) model and its implementation for the detection of road cracks, potholes, and yellow lane in the road. The purpose of yellow lane detection and tracking is to realize autonomous navigation of unmanned aerial vehicle (UAV) by following yellow lane while detecting and reporting the road cracks and potholes to the server through WIFI or 5G medium. The fabrication of own data set is a hectic and time-consuming task. The data set is created, labeled and trained using default and an improved model. The performance of both these models is benchmarked with respect to accuracy, mean average precision (mAP) and detection time. In the testing phase, it was observed that the performance of the improved model is better in respect of accuracy and mAP. The improved model is implemented in UAV using the robot operating system for the autonomous detection of potholes and cracks in roads via UAV front camera vision in real-time. △ Less","14 August, 2020",https://arxiv.org/pdf/2008.06189
Creativity in the era of artificial intelligence,Philippe Esling;Ninon Devis,"Creativity is a deeply debated topic, as this concept is arguably quintessential to our humanity. Across different epochs, it has been infused with an extensive variety of meanings relevant to that era. Along these, the evolution of technology have provided a plurality of novel tools for creative purposes. Recently, the advent of Artificial Intelligence (AI), through deep learning approaches, have seen proficient successes across various applications. The use of such technologies for creativity appear in a natural continuity to the artistic trend of this century. However, the aura of a technological artefact labeled as intelligent has unleashed passionate and somewhat unhinged debates on its implication for creative endeavors. In this paper, we aim to provide a new perspective on the question of creativity at the era of AI, by blurring the frontier between social and computational sciences. To do so, we rely on reflections from social science studies of creativity to view how current AI would be considered through this lens. As creativity is a highly context-prone concept, we underline the limits and deficiencies of current AI, requiring to move towards artificial creativity. We argue that the objective of trying to purely mimic human creative traits towards a self-contained ex-nihilo generative machine would be highly counterproductive, putting us at risk of not harnessing the almost unlimited possibilities offered by the sheer computational power of artificial agents. △ Less","13 August, 2020",https://arxiv.org/pdf/2008.05959
"A clarification of misconceptions, myths and desired status of artificial intelligence",Frank Emmert-Streib;Olli Yli-Harja;Matthias Dehmer,"The field artificial intelligence (AI) has been founded over 65 years ago. Starting with great hopes and ambitious goals the field progressed though various stages of popularity and received recently a revival in the form of deep neural networks. Some problems of AI are that so far neither 'intelligence' nor the goals of AI are formally defined causing confusion when comparing AI to other fields. In this paper, we present a perspective on the desired and current status of AI in relation to machine learning and statistics and clarify common misconceptions and myths. Our discussion is intended to uncurtain the veil of vagueness surrounding AI to see its true countenance. △ Less","3 August, 2020",https://arxiv.org/pdf/2008.05607
Refining Network Intents for Self-Driving Networks,Arthur Selle Jacobs;Ricardo José Pfitscher;Ronaldo Alves Ferreira;Lisandro Zambenedetti Granville,"Recent advances in artificial intelligence (AI) offer an opportunity for the adoption of self-driving networks. However, network operators or home-network users still do not have the right tools to exploit these new advancements in AI, since they have to rely on low-level languages to specify network policies. Intent-based networking (IBN) allows operators to specify high-level policies that dictate how the network should behave without worrying how they are translated into configuration commands in the network devices. However, the existing research proposals for IBN fail to exploit the knowledge and feedback from the network operator to validate or improve the translation of intents. In this paper, we introduce a novel intent-refinement process that uses machine learning and feedback from the operator to translate the operator's utterances into network configurations. Our refinement process uses a sequence-to-sequence learning model to extract intents from natural language and the feedback from the operator to improve learning. The key insight of our process is an intermediate representation that resembles natural language that is suitable to collect feedback from the operator but is structured enough to facilitate precise translations. Our prototype interacts with a network operator using natural language and translates the operator input to the intermediate representation before translating to SDN rules. Our experimental results show that our process achieves a correlation coefficient squared (i.e., R-squared) of 0.99 for a dataset with 5000 entries and the operator feedback significantly improves the accuracy of our model. △ Less","12 August, 2020",https://arxiv.org/pdf/2008.05509
PneumoXttention: A CNN compensating for Human Fallibility when Detecting Pneumonia through CXR images with Attention,Sanskriti Singh,"Automatic Chest Radiograph X-ray (CXR) interpretation by machines is an important research topic of Artificial Intelligence. As part of my journey through the California Science Fair, I have developed an algorithm that can detect pneumonia from a CXR image to compensate for human fallibility. My algorithm, PneumoXttention, is an ensemble of two 13 layer convolutional neural network trained on the RSNA dataset, a dataset provided by the Radiological Society of North America, containing 26,684 frontal X-ray images split into the categories of pneumonia and no pneumonia. The dataset was annotated by many professional radiologists in North America. It achieved an impressive F1 score, 0.82, on the test set (20% random split of RSNA dataset) and completely compensated Human Radiologists on a random set of 25 test images drawn from RSNA and NIH. I don't have a direct comparison but Stanford's Chexnet has a F1 score of 0.435 on the NIH dataset for category Pneumonia. △ Less","11 August, 2020",https://arxiv.org/pdf/2008.04907
Artificial Intelligence to Assist in Exclusion of Coronary Atherosclerosis during CCTA Evaluation of Chest-Pain in the Emergency Department: Preparing an Application for Real-World Use,Richard D. White;Barbaros S. Erdal;Mutlu Demirer;Vikash Gupta;Matthew T. Bigelow;Engin Dikici;Sema Candemir;Mauricio S. Galizia;Jessica L. Carpenter;Thomas P. O Donnell;Abdul H. Halabi;Luciano M. Prevedello,"Coronary Computed Tomography Angiography (CCTA) evaluation of chest-pain patients in an Emergency Department (ED) is considered appropriate. While a negative CCTA interpretation supports direct patient discharge from an ED, labor-intensive analyses are required, with accuracy in jeopardy from distractions. We describe the development of an Artificial Intelligence (AI) algorithm and workflow for assisting interpreting physicians in CCTA screening for the absence of coronary atherosclerosis. The two-phase approach consisted of (1) Phase 1 - focused on the development and preliminary testing of an algorithm for vessel-centerline extraction classification in a balanced study population (n = 500 with 50% disease prevalence) derived by retrospective random case selection; and (2) Phase 2 - concerned with simulated-clinical Trialing of the developed algorithm on a per-case basis in a more real-world study population (n = 100 with 28% disease prevalence) from an ED chest-pain series. This allowed pre-deployment evaluation of the AI-based CCTA screening application which provides a vessel-by-vessel graphic display of algorithm inference results integrated into a clinically capable viewer. Algorithm performance evaluation used Area Under the Receiver-Operating-Characteristic Curve (AUC-ROC); confusion matrices reflected ground-truth vs AI determinations. The vessel-based algorithm demonstrated strong performance with AUC-ROC = 0.96. In both Phase 1 and Phase 2, independent of disease prevalence differences, negative predictive values at the case level were very high at 95%. The rate of completion of the algorithm workflow process (96% with inference results in 55-80 seconds) in Phase 2 depended on adequate image quality. There is potential for this AI application to assist in CCTA interpretation to help extricate atherosclerosis from chest-pain presentations. △ Less","10 August, 2020",https://arxiv.org/pdf/2008.04802
Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of AI/AGI Using Multiple Intelligences and Learning Styles,Andrzej Cichocki;Alexander P. Kuleshov,"This article discusses some trends and concepts in developing new generation of future Artificial General Intelligence (AGI) systems which relate to complex facets and different types of human intelligence, especially social, emotional, attentional and ethical intelligence. We describe various aspects of multiple human intelligences and learning styles, which may impact on a variety of AI problem domains. Using the concept of 'multiple intelligences' rather than a single type of intelligence, we categorize and provide working definitions of various AGI depending on their cognitive skills or capacities. Future AI systems will be able not only to communicate with human users and each other, but also to efficiently exchange knowledge and wisdom with abilities of cooperation, collaboration and even co-creating something new and valuable and have meta-learning capacities. Multi-agent systems such as these can be used to solve problems that would be difficult to solve by any individual intelligent agent. Key words: Artificial General Intelligence (AGI), multiple intelligences, learning styles, physical intelligence, emotional intelligence, social intelligence, attentional intelligence, moral-ethical intelligence, responsible decision making, creative-innovative intelligence, cognitive functions, meta-learning of AI systems. △ Less","11 December, 2020",https://arxiv.org/pdf/2008.04793
SafetyOps,Umair Siddique,"Safety assurance is a paramount factor in the large-scale deployment of various autonomous systems (e.g., self-driving vehicles). However, the execution of safety engineering practices and processes have been challenged by an increasing complexity of modern safety-critical systems. This attribute has become more critical for autonomous systems that involve artificial intelligence (AI) and data-driven techniques along with the complex interactions of the physical world and digital computing platforms. In this position paper, we highlight some challenges of applying current safety processes to modern autonomous systems. Then, we introduce the concept of SafetyOps - a set of practices, which combines DevOps, TestOps, DataOps, and MLOps to provide an efficient, continuous and traceable system safety lifecycle. We believe that SafetyOps can play a significant role in scalable integration and adaptation of safety engineering into various industries relying on AI and data. △ Less","10 August, 2020",https://arxiv.org/pdf/2008.04461
Explainable Artificial Intelligence Based Fault Diagnosis and Insight Harvesting for Steel Plates Manufacturing,Athar Kharal,"With the advent of Industry 4.0, Data Science and Explainable Artificial Intelligence (XAI) has received considerable intrest in recent literature. However, the entry threshold into XAI, in terms of computer coding and the requisite mathematical apparatus, is really high. For fault diagnosis of steel plates, this work reports on a methodology of incorporating XAI based insights into the Data Science process of development of high precision classifier. Using Synthetic Minority Oversampling Technique (SMOTE) and notion of medoids, insights from XAI tools viz. Ceteris Peribus profiles, Partial Dependence and Breakdown profiles have been harvested. Additionally, insights in the form of IF-THEN rules have also been extracted from an optimized Random Forest and Association Rule Mining. Incorporating all the insights into a single ensemble classifier, a 10 fold cross validated performance of 94% has been achieved. In sum total, this work makes three main contributions viz.: methodology based upon utilization of medoids and SMOTE, of gleaning insights and incorporating into model development process. Secondly the insights themselves are contribution, as they benefit the human experts of steel manufacturing industry, and thirdly a high precision fault diagnosis classifier has been developed. △ Less","10 August, 2020",https://arxiv.org/pdf/2008.04448
Fighting Deepfake by Exposing the Convolutional Traces on Images,Luca Guarnera;Oliver Giudice;Sebastiano Battiato,"Advances in Artificial Intelligence and Image Processing are changing the way people interacts with digital images and video. Widespread mobile apps like FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to produce extreme transformations on human face photos such gender swap, aging, etc. The results are utterly realistic and extremely easy to be exploited even for non-experienced users. This kind of media object took the name of Deepfake and raised a new challenge in the multimedia forensics field: the Deepfake detection challenge. Indeed, discriminating a Deepfake from a real image could be a difficult task even for human eyes but recent works are trying to apply the same technology used for generating images for discriminating them with preliminary good results but with many limitations: employed Convolutional Neural Networks are not so robust, demonstrate to be specific to the context and tend to extract semantics from images. In this paper, a new approach aimed to extract a Deepfake fingerprint from images is proposed. The method is based on the Expectation-Maximization algorithm trained to detect and extract a fingerprint that represents the Convolutional Traces (CT) left by GANs during image generation. The CT demonstrates to have high discriminative power achieving better results than state-of-the-art in the Deepfake detection task also proving to be robust to different attacks. Achieving an overall classification accuracy of over 98%, considering Deepfakes from 10 different GAN architectures not only involved in images of faces, the CT demonstrates to be reliable and without any dependence on image semantic. Finally, tests carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the fake detection task, demonstrated the effectiveness of the proposed technique on a real-case scenario. △ Less","7 August, 2020",https://arxiv.org/pdf/2008.04095
AI Failures: A Review of Underlying Issues,Debarag Narayan Banerjee;Sasanka Sekhar Chanda,"Instances of Artificial Intelligence (AI) systems failing to deliver consistent, satisfactory performance are legion. We investigate why AI failures occur. We address only a narrow subset of the broader field of AI Safety. We focus on AI failures on account of flaws in conceptualization, design and deployment. Other AI Safety issues like trade-offs between privacy and security or convenience, bad actors hacking into AI systems to create mayhem or bad actors deploying AI for purposes harmful to humanity and are out of scope of our discussion. We find that AI systems fail on account of omission and commission errors in the design of the AI system, as well as upon failure to develop an appropriate interpretation of input information. Moreover, even when there is no significant flaw in the AI software, an AI system may fail because the hardware is incapable of robust performance across environments. Finally an AI system is quite likely to fail in situations where, in effect, it is called upon to deliver moral judgments -- a capability AI does not possess. We observe certain trade-offs in measures to mitigate a subset of AI failures and provide some recommendations. △ Less","18 July, 2020",https://arxiv.org/pdf/2008.04073
On Controllability of AI,Roman V. Yampolskiy,"Invention of artificial general intelligence is predicted to cause a shift in the trajectory of human civilization. In order to reap the benefits and avoid pitfalls of such powerful technology it is important to be able to control it. However, possibility of controlling artificial general intelligence and its more advanced version, superintelligence, has not been formally established. In this paper, we present arguments as well as supporting evidence from multiple domains indicating that advanced AI can't be fully controlled. Consequences of uncontrollability of AI are discussed with respect to future of humanity and research on AI, and AI safety and security. △ Less","18 July, 2020",https://arxiv.org/pdf/2008.04071
Quran Intelligent Ontology Construction Approach Using Association Rules Mining,Fouzi Harrag;Abdullah Al-Nasser;Abdullah Al-Musnad;Rayan Al-Shaya,"Ontology can be seen as a formal representation of knowledge. They have been investigated in many artificial intelligence studies including semantic web, software engineering, and information retrieval. The aim of ontology is to develop knowledge representations that can be shared and reused. This research project is concerned with the use of association rules to extract the Quran ontology. The manual acquisition of ontologies from Quran verses can be very costly; therefore, we need an intelligent system for Quran ontology construction using patternbased schemes and associations rules to discover Quran concepts and semantics relations from Quran verses. Our system is based on the combination of statistics and linguistics methods to extract concepts and conceptual relations from Quran. In particular, a linguistic pattern-based approach is exploited to extract specific concepts from the Quran, while the conceptual relations are found based on association rules technique. The Quran ontology will offer a new and powerful representation of Quran knowledge, and the association rules will help to represent the relations between all classes of connected concepts in the Quran ontology. △ Less","9 August, 2020",https://arxiv.org/pdf/2008.03232
Modelling Multi-Agent Epistemic Planning in ASP,Alessandro Burigana;Francesco Fabiano;Agostino Dovier;Enrico Pontelli,"Designing agents that reason and act upon the world has always been one of the main objectives of the Artificial Intelligence community. While for planning in ""simple"" domains the agents can solely rely on facts about the world, in several contexts, e.g., economy, security, justice and politics, the mere knowledge of the world could be insufficient to reach a desired goal. In these scenarios, epistemic reasoning, i.e., reasoning about agents' beliefs about themselves and about other agents' beliefs, is essential to design winning strategies. This paper addresses the problem of reasoning in multi-agent epistemic settings exploiting declarative programming techniques. In particular, the paper presents an actual implementation of a multi-shot Answer Set Programming-based planner that can reason in multi-agent epistemic settings, called PLATO (ePistemic muLti-agent Answer seT programming sOlver). The ASP paradigm enables a concise and elegant design of the planner, w.r.t. other imperative implementations, facilitating the development of formal verification of correctness. The paper shows how the planner, exploiting an ad-hoc epistemic state representation and the efficiency of ASP solvers, has competitive performance results on benchmarks collected from the literature. It is under consideration for acceptance in TPLP. △ Less","7 August, 2020",https://arxiv.org/pdf/2008.03007
Modelos dinâmicos aplicados à aprendizagem de valores em inteligência artificial,Nicholas Kluge Corrêa;Nythamar De Oliveira,"Experts in Artificial Intelligence (AI) development predict that advances in the development of intelligent systems and agents will reshape vital areas in our society. Nevertheless, if such an advance is not made prudently and critically, reflexively, it can result in negative outcomes for humanity. For this reason, several researchers in the area have developed a robust, beneficial, and safe concept of AI for the preservation of humanity and the environment. Currently, several of the open problems in the field of AI research arise from the difficulty of avoiding unwanted behaviors of intelligent agents and systems, and at the same time specifying what we really want such systems to do, especially when we look for the possibility of intelligent agents acting in several domains over the long term. It is of utmost importance that artificial intelligent agents have their values aligned with human values, given the fact that we cannot expect an AI to develop human moral values simply because of its intelligence, as discussed in the Orthogonality Thesis. Perhaps this difficulty comes from the way we are addressing the problem of expressing objectives, values, and ends, using representational cognitive methods. A solution to this problem would be the dynamic approach proposed by Dreyfus, whose phenomenological philosophy shows that the human experience of being-in-the-world in several aspects is not well represented by the symbolic or connectionist cognitive method, especially in regards to the question of learning values. A possible approach to this problem would be to use theoretical models such as SED (situated embodied dynamics) to address the values learning problem in AI. △ Less","29 July, 2020",https://arxiv.org/pdf/2008.02783
Analysing Risk of Coronary Heart Disease through Discriminative Neural Networks,Ayush Khaneja;Siddharth Srivastava;Astha Rai;A S Cheema;P K Srivastava,"The application of data mining, machine learning and artificial intelligence techniques in the field of diagnostics is not a new concept, and these techniques have been very successfully applied in a variety of applications, especially in dermatology and cancer research. But, in the case of medical problems that involve tests resulting in true or false (binary classification), the data generally has a class imbalance with samples majorly belonging to one class (ex: a patient undergoes a regular test and the results are false). Such disparity in data causes problems when trying to model predictive systems on the data. In critical applications like diagnostics, this class imbalance cannot be overlooked and must be given extra attention. In our research, we depict how we can handle this class imbalance through neural networks using a discriminative model and contrastive loss using a Siamese neural network structure. Such a model does not work on a probability-based approach to classify samples into labels. Instead it uses a distance-based approach to differentiate between samples classified under different labels. The code is available at https://tinyurl.com/DiscriminativeCHD/ △ Less","17 June, 2020",https://arxiv.org/pdf/2008.02731
An Intelligent Non-Invasive Real Time Human Activity Recognition System for Next-Generation Healthcare,William Taylor;Syed Aziz Shah;Kia Dashtipour;Adnan Zahid;Qammer H. Abbasi;Muhammad Ali Imran,"Human motion detection is getting considerable attention in the field of Artificial Intelligence (AI) driven healthcare systems. Human motion can be used to provide remote healthcare solutions for vulnerable people by identifying particular movements such as falls, gait and breathing disorders. This can allow people to live more independent lifestyles and still have the safety of being monitored if more direct care is needed. At present wearable devices can provide real time monitoring by deploying equipment on a person's body. However, putting devices on a person's body all the time make it uncomfortable and the elderly tends to forget it to wear as well in addition to the insecurity of being tracked all the time. This paper demonstrates how human motions can be detected in quasi-real-time scenario using a non-invasive method. Patterns in the wireless signals presents particular human body motions as each movement induces a unique change in the wireless medium. These changes can be used to identify particular body motions. This work produces a dataset that contains patterns of radio wave signals obtained using software defined radios (SDRs) to establish if a subject is standing up or sitting down as a test case. The dataset was used to create a machine learning model, which was used in a developed application to provide a quasi-real-time classification of standing or sitting state. The machine learning model was able to achieve 96.70 % accuracy using the Random Forest algorithm using 10 fold cross validation. A benchmark dataset of wearable devices was compared to the proposed dataset and results showed the proposed dataset to have similar accuracy of nearly 90 %. The machine learning models developed in this paper are tested for two activities but the developed system is designed and applicable for detecting and differentiating x number of activities. △ Less","6 August, 2020",https://arxiv.org/pdf/2008.02567
Conceptual Metaphors Impact Perceptions of Human-AI Collaboration,Pranav Khadpe;Ranjay Krishna;Li Fei-Fei;Jeffrey Hancock;Michael Bernstein,"With the emergence of conversational artificial intelligence (AI) agents, it is important to understand the mechanisms that influence users' experiences of these agents. We study a common tool in the designer's toolkit: conceptual metaphors. Metaphors can present an agent as akin to a wry teenager, a toddler, or an experienced butler. How might a choice of metaphor influence our experience of the AI agent? Sampling metaphors along the dimensions of warmth and competence---defined by psychological theories as the primary axes of variation for human social perception---we perform a study (N=260) where we manipulate the metaphor, but not the behavior, of a Wizard-of-Oz conversational agent. Following the experience, participants are surveyed about their intention to use the agent, their desire to cooperate with the agent, and the agent's usability. Contrary to the current tendency of designers to use high competence metaphors to describe AI products, we find that metaphors that signal low competence lead to better evaluations of the agent than metaphors that signal high competence. This effect persists despite both high and low competence agents featuring human-level performance and the wizards being blind to condition. A second study confirms that intention to adopt decreases rapidly as competence projected by the metaphor increases. In a third study, we assess effects of metaphor choices on potential users' desire to try out the system and find that users are drawn to systems that project higher competence and warmth. These results suggest that projecting competence may help attract new users, but those users may discard the agent unless it can quickly correct with a lower competence metaphor. We close with a retrospective analysis that finds similar patterns between metaphors and user attitudes towards past conversational agents such as Xiaoice, Replika, Woebot, Mitsuku, and Tay. △ Less","5 August, 2020",https://arxiv.org/pdf/2008.02311
More Than Privacy: Applying Differential Privacy in Key Areas of Artificial Intelligence,Tianqing Zhu;Dayong Ye;Wei Wang;Wanlei Zhou;Philip S. Yu,"Artificial Intelligence (AI) has attracted a great deal of attention in recent years. However, alongside all its advancements, problems have also emerged, such as privacy violations, security issues and model fairness. Differential privacy, as a promising mathematical model, has several attractive properties that can help solve these problems, making it quite a valuable tool. For this reason, differential privacy has been broadly applied in AI but to date, no study has documented which differential privacy mechanisms can or have been leveraged to overcome its issues or the properties that make this possible. In this paper, we show that differential privacy can do more than just privacy preservation. It can also be used to improve security, stabilize learning, build fair models, and impose composition in selected areas of AI. With a focus on regular machine learning, distributed machine learning, deep learning, and multi-agent systems, the purpose of this article is to deliver a new view on many possibilities for improving AI performance with differential privacy techniques. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.01916
High performance on-demand de-identification of a petabyte-scale medical imaging data lake,Joseph Mesterhazy;Garrick Olson;Somalee Datta,"With the increase in Artificial Intelligence driven approaches, researchers are requesting unprecedented volumes of medical imaging data which far exceed the capacity of traditional on-premise client-server approaches for making the data research analysis-ready. We are making available a flexible solution for on-demand de-identification that combines the use of mature software technologies with modern cloud-based distributed computing techniques to enable faster turnaround in medical imaging research. The solution is part of a broader platform that supports a secure high performance clinical data science platform. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.01827
EasyRL: A Simple and Extensible Reinforcement Learning Framework,Neil Hulbert;Sam Spillers;Brandon Francis;James Haines-Temons;Ken Gil Romero;Benjamin De Jager;Sam Wong;Kevin Flora;Bowei Huang;Athirai A. Irissappane,"In recent years, Reinforcement Learning (RL), has become a popular field of study as well as a tool for enterprises working on cutting-edge artificial intelligence research. To this end, many researchers have built RL frameworks such as openAI Gym and KerasRL for ease of use. While these works have made great strides towards bringing down the barrier of entry for those new to RL, we propose a much simpler framework called EasyRL, by providing an interactive graphical user interface for users to train and evaluate RL agents. As it is entirely graphical, EasyRL does not require programming knowledge for training and testing simple built-in RL agents. EasyRL also supports custom RL agents and environments, which can be highly beneficial for RL researchers in evaluating and comparing their RL models. △ Less","5 November, 2020",https://arxiv.org/pdf/2008.01700
Inducing game rules from varying quality game play,Alastair Flynn,"General Game Playing (GGP) is a framework in which an artificial intelligence program is required to play a variety of games successfully. It acts as a test bed for AI and motivator of research. The AI is given a random game description at runtime which it then plays. The framework includes repositories of game rules. The Inductive General Game Playing (IGGP) problem challenges machine learning systems to learn these GGP game rules by watching the game being played. In other words, IGGP is the problem of inducing general game rules from specific game observations. Inductive Logic Programming (ILP) has shown to be a promising approach to this problem though it has been demonstrated that it is still a hard problem for ILP systems. Existing work on IGGP has always assumed that the game player being observed makes random moves. This is not representative of how a human learns to play a game. With random gameplay situations that would normally be encountered when humans play are not present. To address this limitation, we analyse the effect of using intelligent versus random gameplay traces as well as the effect of varying the number of traces in the training set. We use Sancho, the 2014 GGP competition winner, to generate intelligent game traces for a large number of games. We then use the ILP systems, Metagol, Aleph and ILASP to induce game rules from the traces. We train and test the systems on combinations of intelligent and random data including a mixture of both. We also vary the volume of training data. Our results show that whilst some games were learned more effectively in some of the experiments than others no overall trend was statistically significant. The implications of this work are that varying the quality of training data as described in this paper has strong effects on the accuracy of the learned game rules; however one solution does not work for all games. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.01664
"Distributed Linguistic Representations in Decision Making: Taxonomy, Key Elements and Applications, and Challenges in Data Science and Explainable Artificial Intelligence",Yuzhu Wu;Zhen Zhang;Gang Kou;Hengjie Zhang;Xiangrui Chao;Cong-Cong Li;Yucheng Dong;Francisco Herrera,"Distributed linguistic representations are powerful tools for modelling the uncertainty and complexity of preference information in linguistic decision making. To provide a comprehensive perspective on the development of distributed linguistic representations in decision making, we present the taxonomy of existing distributed linguistic representations. Then, we review the key elements of distributed linguistic information processing in decision making, including the distance measurement, aggregation methods, distributed linguistic preference relations, and distributed linguistic multiple attribute decision making models. Next, we provide a discussion on ongoing challenges and future research directions from the perspective of data science and explainable artificial intelligence. △ Less","7 August, 2020",https://arxiv.org/pdf/2008.01499
Collecting the Public Perception of AI and Robot Rights,Gabriel Lima;Changyeon Kim;Seungho Ryu;Chihyung Jeon;Meeyoung Cha,"Whether to give rights to artificial intelligence (AI) and robots has been a sensitive topic since the European Parliament proposed advanced robots could be granted ""electronic personalities."" Numerous scholars who favor or disfavor its feasibility have participated in the debate. This paper presents an experiment (N=1270) that 1) collects online users' first impressions of 11 possible rights that could be granted to autonomous electronic agents of the future and 2) examines whether debunking common misconceptions on the proposal modifies one's stance toward the issue. The results indicate that even though online users mainly disfavor AI and robot rights, they are supportive of protecting electronic agents from cruelty (i.e., favor the right against cruel treatment). Furthermore, people's perceptions became more positive when given information about rights-bearing non-human entities or myth-refuting statements. The style used to introduce AI and robot rights significantly affected how the participants perceived the proposal, similar to the way metaphors function in creating laws. For robustness, we repeated the experiment over a more representative sample of U.S. residents (N=164) and found that perceptions gathered from online users and those by the general population are similar. △ Less","4 August, 2020",https://arxiv.org/pdf/2008.01339
Safety design concepts for statistical machine learning components toward accordance with functional safety standards,Akihisa Morikawa;Yutaka Matsubara,"In recent years, curial incidents and accidents have been reported due to un-intended control caused by misjudgment of statistical machine learning (SML), which include deep learning. The international functional safety standards for Electric/Electronic/Programmable (E/E/P) systems have been widely spread to improve safety. However, most of them do not recom-mended to use SML in safety critical systems so far. In practical the new concepts and methods are urgently required to enable SML to be safely used in safety critical systems. In this paper, we organize five kinds of technical safety concepts (TSCs) for SML components toward accordance with functional safety standards. We discuss not only quantitative evaluation criteria, but also development process based on XAI (eXplainable Artificial Intelligence) and Automotive SPICE to improve explainability and reliability in development phase. Fi-nally, we briefly compare the TSCs in cost and difficulty, and expect to en-courage further discussion in many communities and domain. △ Less","3 August, 2020",https://arxiv.org/pdf/2008.01263
Enhancing autonomy transparency: an option-centric rationale approach,Ruikun Luo;Na Du;X. Jessie Yang,"While the advances in artificial intelligence and machine learning empower a new generation of autonomous systems for assisting human performance, one major concern arises from the human factors perspective: Humans have difficulty deciphering autonomy-generated solutions and increasingly perceive autonomy as a mysterious black box. The lack of transparency contributes to the lack of trust in autonomy and sub-optimal team performance. To enhance autonomy transparency, this study proposed an option-centric rationale display and evaluated its effectiveness. We developed a game Treasure Hunter wherein a human uncovers a map for treasures with the help from an intelligent assistant, and conducted a human-in-the-loop experiment with 34 participants. Results indicated that by conveying the intelligent assistant's decision-making rationale via the option-centric rationale display, participants had higher trust in the system and calibrated their trust faster. Additionally, higher trust led to higher acceptance of recommendations from the intelligent assistant, and in turn higher task performance. △ Less","3 August, 2020",https://arxiv.org/pdf/2008.01051
Blackbox Trojanising of Deep Learning Models : Using non-intrusive network structure and binary alterations,Jonathan Pan,"Recent advancements in Artificial Intelligence namely in Deep Learning has heightened its adoption in many applications. Some are playing important roles to the extent that we are heavily dependent on them for our livelihood. However, as with all technologies, there are vulnerabilities that malicious actors could exploit. A form of exploitation is to turn these technologies, intended for good, to become dual-purposed instruments to support deviant acts like malicious software trojans. As part of proactive defense, researchers are proactively identifying such vulnerabilities so that protective measures could be developed subsequently. This research explores a novel blackbox trojanising approach using a simple network structure modification to any deep learning image classification model that would transform a benign model into a deviant one with a simple manipulation of the weights to induce specific types of errors. Propositions to protect the occurrence of such simple exploits are discussed in this research. This research highlights the importance of providing sufficient safeguards to these models so that the intended good of AI innovation and adoption may be protected. △ Less","2 August, 2020",https://arxiv.org/pdf/2008.00408
A Comparative Study of AI-based Intrusion Detection Techniques in Critical Infrastructures,Safa Otoum;Burak Kantarci;Hussein Mouftah,"Volunteer computing uses Internet-connected devices (laptops, PCs, smart devices, etc.), in which their owners volunteer them as storage and computing power resources, has become an essential mechanism for resource management in numerous applications. The growth of the volume and variety of data traffic in the Internet leads to concerns on the robustness of cyberphysical systems especially for critical infrastructures. Therefore, the implementation of an efficient Intrusion Detection System for gathering such sensory data has gained vital importance. In this paper, we present a comparative study of Artificial Intelligence (AI)-driven intrusion detection systems for wirelessly connected sensors that track crucial applications. Specifically, we present an in-depth analysis of the use of machine learning, deep learning and reinforcement learning solutions to recognize intrusive behavior in the collected traffic. We evaluate the proposed mechanisms by using KD'99 as real attack data-set in our simulations. Results present the performance metrics for three different IDSs namely the Adaptively Supervised and Clustered Hybrid IDS (ASCH-IDS), Restricted Boltzmann Machine-based Clustered IDS (RBC-IDS) and Q-learning based IDS (QL-IDS) to detect malicious behaviors. We also present the performance of different reinforcement learning techniques such as State-Action-Reward-State-Action Learning (SARSA) and the Temporal Difference learning (TD). Through simulations, we show that QL-IDS performs with 100% detection rate while SARSA-IDS and TD-IDS perform at the order of 99.5%. △ Less","24 July, 2020",https://arxiv.org/pdf/2008.00088
Artificial Intelligence in Music and Performance: A Subjective Art-Research Inquiry,Baptiste Caramiaux;Marco Donnarumma,"This article presents a five-year collaboration situated at the intersection of Art practice and Scientific research in Human-Computer Interaction (HCI). At the core of our collaborative work is a hybrid, Art and Science methodology that combines computational learning technology -- Machine Learning (ML) and Artificial Intelligence (AI) -- with interactive music performance and choreography. This article first exposes our thoughts on combining art, science, movement and sound research. We then describe two of our artistic works \textit{Corpus Nil} and \textit{Humane Methods} -- created five years apart from each other -- that crystallize our collaborative research process. We present the scientific and artistic motivations, framed through our research interests and cultural environment of the time. We conclude by reflecting on the methodology we developed during the collaboration and on the conceptual shift of computational learning technologies, from ML to AI, and its impact on Music performance. △ Less","31 July, 2020",https://arxiv.org/pdf/2007.15843
LEMMA: A Multi-view Dataset for Learning Multi-agent Multi-task Activities,Baoxiong Jia;Yixin Chen;Siyuan Huang;Yixin Zhu;Song-chun Zhu,"Understanding and interpreting human actions is a long-standing challenge and a critical indicator of perception in artificial intelligence. However, a few imperative components of daily human activities are largely missed in prior literature, including the goal-directed actions, concurrent multi-tasks, and collaborations among multi-agents. We introduce the LEMMA dataset to provide a single home to address these missing dimensions with meticulously designed settings, wherein the number of tasks and agents varies to highlight different learning objectives. We densely annotate the atomic-actions with human-object interactions to provide ground-truths of the compositionality, scheduling, and assignment of daily activities. We further devise challenging compositional action recognition and action/task anticipation benchmarks with baseline models to measure the capability of compositional action understanding and temporal reasoning. We hope this effort would drive the machine vision community to examine goal-directed human activities and further study the task scheduling and assignment in the real world. △ Less","30 July, 2020",https://arxiv.org/pdf/2007.15781
Improving Multi-Agent Cooperation using Theory of Mind,Terence X. Lim;Sidney Tio;Desmond C. Ong,"Recent advances in Artificial Intelligence have produced agents that can beat human world champions at games like Go, Starcraft, and Dota2. However, most of these models do not seem to play in a human-like manner: People infer others' intentions from their behaviour, and use these inferences in scheming and strategizing. Here, using a Bayesian Theory of Mind (ToM) approach, we investigated how much an explicit representation of others' intentions improves performance in a cooperative game. We compared the performance of humans playing with optimal-planning agents with and without ToM, in a cooperative game where players have to flexibly cooperate to achieve joint goals. We find that teams with ToM agents significantly outperform non-ToM agents when collaborating with all types of partners: non-ToM, ToM, as well as human players, and that the benefit of ToM increases the more ToM agents there are. These findings have implications for designing better cooperative agents. △ Less","30 July, 2020",https://arxiv.org/pdf/2007.15703
Deep learning for lithological classification of carbonate rock micro-CT images,Carlos E. M. dos Anjos;Manuel R. V. Avila;Adna G. P. Vasconcelos;Aurea M. P. Neta;Lizianne C. Medeiros;Alexandre G. Evsukoff;Rodrigo Surmas,"In addition to the ongoing development, pre-salt carbonate reservoir characterization remains a challenge, primarily due to inherent geological particularities. These challenges stimulate the use of well-established technologies, such as artificial intelligence algorithms, for image classification tasks. Therefore, this work intends to present an application of deep learning techniques to identify patterns in Brazilian pre-salt carbonate rock microtomographic images, thus making possible lithological classification. Four convolutional neural network models were proposed. The first model includes three convolutional layers followed by fully connected layers and is used as a base model for the following proposals. In the next two models, we replace the max pooling layer with a spatial pyramid pooling and a global average pooling layer. The last model uses a combination of spatial pyramid pooling followed by global average pooling in place of the last pooling layer. All models are compared using original images, when possible, as well as resized images. The dataset consists of 6,000 images from three different classes. The model performances were evaluated by each image individually, as well as by the most frequently predicted class for each sample. According to accuracy, Model 2 trained on resized images achieved the best results, reaching an average of 75.54% for the first evaluation approach and an average of 81.33% for the second. We developed a workflow to automate and accelerate the lithology classification of Brazilian pre-salt carbonate samples by categorizing microtomographic images using deep learning algorithms in a non-destructive way. △ Less","30 July, 2020",https://arxiv.org/pdf/2007.15693
Design Guidelines for Blockchain-Assisted 5G-UAV Networks,Moayad Aloqaily;Ouns Bouachir;Azzedine Boukerche;Ismaeel Al Ridhawi,"Fifth Generation (5G) wireless networks are designed to meet various end-user Quality of Service (QoS) requirements through high data rates (typically of Gbps order) and low latencies. Coupled with Fog and Mobile Edge Computing (MEC), 5G can achieve high data rates, enabling complex autonomous smart city services such as the large deployment of self-driving vehicles and large-scale Artificial Intelligence (AI)-enabled industrial manufacturing. However, to meet the exponentially growing number of connected IoT devices and irregular data and service requests in both low and highly dense locations, the process of enacting traditional cells supported through fixed and costly base stations requires rethought to enable on-demand mobile access points in the form of Unmanned Aerial Vehicles (UAV) for diversified smart city scenarios. This article envisions a 5G network environment that is supported by blockchain-enabled UAVs to meet dynamic user demands with network access supply. The solution enables decentralized service delivery (Drones as a Service) and routing to and from end-users in a reliable and secure manner. Both public and private blockchains are deployed within the UAVs, supported by fog and cloud computing devices and data centers to provide wide range of complex authenticated service and data availability. Particular attention is paid tocomparing data delivery success rates and message exchange in the proposed solution against traditional UAV-supported cellular networks. Challenges and future research are also discussed with highlights on emerging technologies such as Federated Learning. △ Less","30 July, 2020",https://arxiv.org/pdf/2007.15286
Swarm Intelligence for Next-Generation Wireless Networks: Recent Advances and Applications,Quoc-Viet Pham;Dinh C. Nguyen;Seyedali Mirjalili;Dinh Thai Hoang;Diep N. Nguyen;Pubudu N. Pathirana;Won-Joo Hwang,"Due to the proliferation of smart devices and emerging applications, many next-generation technologies have been paid for the development of wireless networks. Even though commercial 5G has just been widely deployed in some countries, there have been initial efforts from academia and industrial communities for 6G systems. In such a network, a very large number of devices and applications are emerged, along with heterogeneity of technologies, architectures, mobile data, etc., and optimizing such a network is of utmost importance. Besides convex optimization and game theory, swarm intelligence (SI) has recently appeared as a promising optimization tool for wireless networks. As a new subdivision of artificial intelligence, SI is inspired by the collective behaviors of societies of biological species. In SI, simple agents with limited capabilities would achieve intelligent strategies for high-dimensional and challenging problems, so it has recently found many applications in next-generation wireless networks (NGN). However, researchers may not be completely aware of the full potential of SI techniques. In this work, our primary focus will be the integration of these two domains: NGN and SI. Firstly, we provide an overview of SI techniques from fundamental concepts to well-known optimizers. Secondly, we review the applications of SI to settle emerging issues in NGN, including spectrum management and resource allocation, wireless caching and edge computing, network security, and several other miscellaneous issues. Finally, we highlight open challenges and issues in the literature, and introduce some interesting directions for future research. △ Less","30 July, 2020",https://arxiv.org/pdf/2007.15221
Improving probability selecting based weights for Satisfiability Problem,Huimin Fu;Yang Xu;Jun Liu;Guanfeng Wu;Sutcliffe Geoff,"The Boolean Satisfiability problem (SAT) is important on artificial intelligence community and the impact of its solving on complex problems. Recently, great breakthroughs have been made respectively on stochastic local search (SLS) algorithms for uniform random k-SAT resulting in several state-of-the-art SLS algorithms Score2SAT, YalSAT, ProbSAT, CScoreSAT and on a hybrid algorithm for hard random SAT (HRS) resulting in one state-of-the-art hybrid algorithm SparrowToRiss. However, there is no an algorithm which can effectively solve both uniform random k-SAT and HRS. In this paper, we present a new SLS algorithm named SelectNTS for uniform random k-SAT and HRS. SelectNTS is an improved probability selecting based local search algorithm for SAT problem. The core of SelectNTS relies on new clause and variable selection heuristics. The new clause selection heuristic uses a new clause weighting scheme and a biased random walk. The new variable selection heuristic uses a probability selecting strategy with the variation of CC strategy based on a new variable weighting scheme. Extensive experimental results on the well-known random benchmarks instances from the SAT Competitions in 2017 and 2018, and on randomly generated problems, show that our algorithm outperforms state-of-the-art random SAT algorithms, and our SelectNTS can effectively solve both uniform random k-SAT and HRS. △ Less","29 July, 2020",https://arxiv.org/pdf/2007.15185
Enhanced well-being assessment as basis for the practical implementation of ethical and rights-based normative principles for AI,Marek Havrda;Bogdana Rakova,"Artificial Intelligence (AI) has an increasing impact on all areas of people's livelihoods. A detailed look at existing interdisciplinary and transdisciplinary metrics frameworks could bring new insights and enable practitioners to navigate the challenge of understanding and assessing the impact of Autonomous and Intelligent Systems (A/IS). There has been emerging consensus on fundamental ethical and rights-based AI principles proposed by scholars, governments, civil rights organizations, and technology companies. In order to move from principles to real-world implementation, we adopt a lens motivated by regulatory impact assessments and the well-being movement in public policy. Similar to public policy interventions, outcomes of AI systems implementation may have far-reaching complex impacts. In public policy, indicators are only part of a broader toolbox, as metrics inherently lead to gaming and dissolution of incentives and objectives. Similarly, in the case of A/IS, there's a need for a larger toolbox that allows for the iterative assessment of identified impacts, inclusion of new impacts in the analysis, and identification of emerging trade-offs. In this paper, we propose the practical application of an enhanced well-being impact assessment framework for A/IS that could be employed to address ethical and rights-based normative principles in AI. This process could enable a human-centered algorithmically-supported approach to the understanding of the impacts of AI systems. Finally, we propose a new testing infrastructure which would allow for governments, civil rights organizations, and others, to engage in cooperating with A/IS developers towards implementation of enhanced well-being impact assessments. △ Less","15 September, 2020",https://arxiv.org/pdf/2007.14826
A Hybrid Adaptive Educational eLearning Project based on Ontologies Matching and Recommendation System,Vasiliki Demertzi;Konstantinos Demertzis,"The implementation of teaching interventions in learning needs has received considerable attention, as the provision of the same educational conditions to all students, is pedagogically ineffective. In contrast, more effectively considered the pedagogical strategies that adapt to the real individual skills of the students. An important innovation in this direction is the Adaptive Educational Systems (AES) that support automatic modeling study and adjust the teaching content on educational needs and students' skills. Effective utilization of these educational approaches can be enhanced with Artificial Intelligence (AI) technologies in order to the substantive content of the web acquires structure and the published information is perceived by the search engines. This study proposes a novel Adaptive Educational eLearning System (AEeLS) that has the capacity to gather and analyze data from learning repositories and to adapt these to the educational curriculum according to the student skills and experience. It is a novel hybrid machine learning system that combines a Semi-Supervised Classification method for ontology matching and a Recommendation Mechanism that uses a hybrid method from neighborhood-based collaborative and content-based filtering techniques, in order to provide a personalized educational environment for each student. △ Less","9 October, 2020",https://arxiv.org/pdf/2007.14771
Approaches to Fraud Detection on Credit Card Transactions Using Artificial Intelligence Methods,Yusuf Yazici,"Credit card fraud is an ongoing problem for almost all industries in the world, and it raises millions of dollars to the global economy each year. Therefore, there is a number of research either completed or proceeding in order to detect these kinds of frauds in the industry. These researches generally use rule-based or novel artificial intelligence approaches to find eligible solutions. The ultimate goal of this paper is to summarize state-of-the-art approaches to fraud detection using artificial intelligence and machine learning techniques. While summarizing, we will categorize the common problems such as imbalanced dataset, real time working scenarios, and feature engineering challenges that almost all research works encounter, and identify general approaches to solve them. The imbalanced dataset problem occurs because the number of legitimate transactions is much higher than the fraudulent ones whereas applying the right feature engineering is substantial as the features obtained from the industries are limited, and applying feature engineering methods and reforming the dataset is crucial. Also, adapting the detection system to real time scenarios is a challenge since the number of credit card transactions in a limited time period is very high. In addition, we will discuss how evaluation metrics and machine learning methods differentiate among each research. △ Less","29 July, 2020",https://arxiv.org/pdf/2007.14622
Implementation of Ternary Weights with Resistive RAM Using a Single Sense Operation per Synapse,Axel Laborieux;Marc Bocquet;Tifenn Hirtzlin;Jacques-Olivier Klein;Etienne Nowak;Elisa Vianello;Jean-Michel Portal;Damien Querlioz,"The design of systems implementing low precision neural networks with emerging memories such as resistive random access memory (RRAM) is a significant lead for reducing the energy consumption of artificial intelligence. To achieve maximum energy efficiency in such systems, logic and memory should be integrated as tightly as possible. In this work, we focus on the case of ternary neural networks, where synaptic weights assume ternary values. We propose a two-transistor/two-resistor memory architecture employing a precharge sense amplifier, where the weight value can be extracted in a single sense operation. Based on experimental measurements on a hybrid 130 nm CMOS/RRAM chip featuring this sense amplifier, we show that this technique is particularly appropriate at low supply voltage, and that it is resilient to process, voltage, and temperature variations. We characterize the bit error rate in our scheme. We show based on neural network simulation on the CIFAR-10 image recognition task that the use of ternary neural networks significantly increases neural network performance, with regards to binary ones, which are often preferred for inference hardware. We finally evidence that the neural network is immune to the type of bit errors observed in our scheme, which can therefore be used without error correction. △ Less","14 October, 2020",https://arxiv.org/pdf/2007.14234
Emotion Correlation Mining Through Deep Learning Models on Natural Language Text,Xinzhi Wang;Luyao Kou;Vijayan Sugumaran;Xiangfeng Luo;Hui Zhang,"Emotion analysis has been attracting researchers' attention. Most previous works in the artificial intelligence field focus on recognizing emotion rather than mining the reason why emotions are not or wrongly recognized. Correlation among emotions contributes to the failure of emotion recognition. In this paper, we try to fill the gap between emotion recognition and emotion correlation mining through natural language text from web news. Correlation among emotions, expressed as the confusion and evolution of emotion, is primarily caused by human emotion cognitive bias. To mine emotion correlation from emotion recognition through text, three kinds of features and two deep neural network models are presented. The emotion confusion law is extracted through orthogonal basis. The emotion evolution law is evaluated from three perspectives, one-step shift, limited-step shifts, and shortest path transfer. The method is validated using three datasets-the titles, the bodies, and the comments of news articles, covering both objective and subjective texts in varying lengths (long and short). The experimental results show that, in subjective comments, emotions are easily mistaken as anger. Comments tend to arouse emotion circulations of love-anger and sadness-anger. In objective news, it is easy to recognize text emotion as love and cause fear-joy circulation. That means, journalists may try to attract attention using fear and joy words but arouse the emotion love instead; After news release, netizens generate emotional comments to express their intense emotions, i.e., anger, sadness, and love. These findings could provide insights for applications regarding affective interaction such as network public sentiment, social media communication, and human-computer interaction. △ Less","28 July, 2020",https://arxiv.org/pdf/2007.14071
Enhancing Secure MIMO Transmission via Intelligent Reflecting Surface,Limeng Dong;Hui-Ming Wang,"In this paper, we consider an intelligent reflecting surface (IRS) assisted Guassian multiple-input multiple-output (MIMO) wiretap channel (WTC), and focus on enhancing its secrecy rate. Due to MIMO setting, all the existing solutions for enhancing the secrecy rate over multiple-input single-output WTC completely fall to this work. Furthermore, all the existing studies are simply based on an ideal assumption that full channel state information (CSI) of eavesdropper (Ev) is available. Therefore, we propose numerical solutions to enhance the secrecy rate of this channel under both full and no Ev's CSI cases. For the full CSI case, we propose a barrier method and one-by-one (OBO) optimization combined alternating optimization (AO) algorithm to jointly optimize the transmit covariance R at transmitter (Tx) and phase shift coefficient Q at IRS. For the case of no Ev's CSI, we develop an artificial noise (AN) aided joint transmission scheme to enhance the secrecy rate. In this scheme, a bisection search (BS) and OBO optimization combined AO algorithm is proposed to jointly optimize R and Q. Such scheme is also applied to enhance the secrecy rate under a special scenario in which the direct link between Tx and receiver/Ev is blocked due to obstacles. In particular, we propose a BS and minorization-maximization (MM) combined AO algorithm with slightly faster convergence to optimize R and Q for this scenario. Simulation results have validated the monotonic convergence of the proposed algorithms, and it is shown that the proposed algorithms for the IRS-assisted design achieve significantly larger secrecy rate than the other benchmark schemes under full CSI. When Ev's CSI is unknown, the secrecy performance of this channel also can be enhanced by the proposed AN aided scheme, and there is a trade-off between increasing the quality of service at Rx and enhancing the secrecy rate. △ Less","27 July, 2020",https://arxiv.org/pdf/2007.13944
Automatic Detection and Classification of Waste Consumer Medications for Proper Management and Disposal,Bahram Marami;Atabak Reza Royaee,"Every year, millions of pounds of medicines remain unused in the U.S. and are subject to an in-home disposal, i.e., kept in medicine cabinets, flushed in toilet or thrown in regular trash. In-home disposal, however, can negatively impact the environment and public health. The drug take-back programs (drug take-backs) sponsored by the Drug Enforcement Administration (DEA) and its state and industry partners collect unused consumer medications and provide the best alternative to in-home disposal of medicines. However, the drug take-backs are expensive to operate and not widely available. In this paper, we show that artificial intelligence (AI) can be applied to drug take-backs to render them operationally more efficient. Since identification of any waste is crucial to a proper disposal, we showed that it is possible to accurately identify loose consumer medications solely based on the physical features and visual appearance. We have developed an automatic technique that uses deep neural networks and computer vision to identify and segregate solid medicines. We applied the technique to images of about one thousand loose pills and succeeded in correctly identifying the pills with an accuracy of 0.912 and top-5 accuracy of 0.984. We also showed that hazardous pills could be distinguished from non-hazardous pills within the dataset with an accuracy of 0.984. We believe that the power of artificial intelligence could be harnessed in products that would facilitate the operation of the drug take-backs more efficiently and help them become widely available throughout the country. △ Less","27 July, 2020",https://arxiv.org/pdf/2007.13903
Overview of digital health surveillance system during COVID-19 pandemic: public health issues and misapprehensions,Molla Rashied Hussein;Ehsanul Hoque Apu;Shahriar Shahabuddin;Abdullah Bin Shams;Russell Kabir,"Without proper medication and vaccination for the COVID-19, many governments are using automated digital healthcare surveillance system to prevent and control the spread. There is not enough literature explaining the concerns and privacy issues; hence, we have briefly explained the topics in this paper. We focused on digital healthcare surveillance system's privacy concerns and different segments. Further research studies should be conducted in different sectors. This paper provides an overview based on the published articles, which are not focusing on the privacy issues that much. Artificial intelligence and 5G networks combine the advanced digital healthcare surveillance system; whereas Bluetooth-based contact tracing systems have fewer privacy concerns. More studies are required to find the appropriate digital healthcare surveillance system, which would be ideal for monitoring, controlling, and predicting the COVID-19 trajectory. △ Less","27 July, 2020",https://arxiv.org/pdf/2007.13633
Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving Simulators in HMI Design,Lia Morra;Fabrizio Lamberti;F. Gabriele Pratticó;Salvatore La Rosa;Paolo Montuschi,"The investigation of factors contributing at making humans trust Autonomous Vehicles (AVs) will play a fundamental role in the adoption of such technology. The user's ability to form a mental model of the AV, which is crucial to establish trust, depends on effective user-vehicle communication; thus, the importance of Human-Machine Interaction (HMI) is poised to increase. In this work, we propose a methodology to validate the user experience in AVs based on continuous, objective information gathered from physiological signals, while the user is immersed in a Virtual Reality-based driving simulation. We applied this methodology to the design of a head-up display interface delivering visual cues about the vehicle' sensory and planning systems. Through this approach, we obtained qualitative and quantitative evidence that a complete picture of the vehicle's surrounding, despite the higher cognitive load, is conducive to a less stressful experience. Moreover, after having been exposed to a more informative interface, users involved in the study were also more willing to test a real AV. The proposed methodology could be extended by adjusting the simulation environment, the HMI and/or the vehicle's Artificial Intelligence modules to dig into other aspects of the user experience. △ Less","27 July, 2020",https://arxiv.org/pdf/2007.13371
From Robotic Process Automation to Intelligent Process Automation: Emerging Trends,Tathagata Chakraborti;Vatche Isahagian;Rania Khalaf;Yasaman Khazaeni;Vinod Muthusamy;Yara Rizk;Merve Unuvar,"In this survey, we study how recent advances in machine intelligence are disrupting the world of business processes. Over the last decade, there has been steady progress towards the automation of business processes under the umbrella of ``robotic process automation'' (RPA). However, we are currently at an inflection point in this evolution, as a new paradigm called ``Intelligent Process Automation'' (IPA) emerges, bringing machine learning (ML) and artificial intelligence (AI) technologies to bear in order to improve business process outcomes. The purpose of this paper is to provide a survey of this emerging theme and identify key open research challenges at the intersection of AI and business processes. We hope that this emerging theme will spark engaging conversations at the RPA Forum. △ Less","26 July, 2020",https://arxiv.org/pdf/2007.13257
What Government by Algorithm Might Look Like,Rustam Tagiew,"Algocracy is the rule by algorithms. This paper summarises technologies useful to create algocratic social machines and presents idealistic examples of their application. In particular, it describes smart contracts and their implementations, challenges of behaviour mining and prediction, as well as game-theoretic and AI approaches to mechanism design. The presented idealistic examples of new algocratic solutions are picked from the reality of a modern state. The examples are science funding, trade by organisations, regulation of rental agreements, ranking of significance and sortition. Artificial General Intelligence is not in the scope of this feasibility study. △ Less","26 July, 2020",https://arxiv.org/pdf/2007.13127
A Novel Approach to the Diagnosis of Heart Disease using Machine Learning and Deep Neural Networks,Sahithi Ankireddy,"Heart disease is the leading cause of death worldwide. Currently, 33% of cases are misdiagnosed, and approximately half of myocardial infarctions occur in people who are not predicted to be at risk. The use of Artificial Intelligence could reduce the chance of error, leading to possible earlier diagnoses, which could be the difference between life and death for some. The objective of this project was to develop an application for assisted heart disease diagnosis using Machine Learning (ML) and Deep Neural Network (DNN) algorithms. The dataset was provided from the Cleveland Clinic Foundation, and the models were built based on various optimization and hyper parametrization techniques including a Grid Search algorithm. The application, running on Flask, and utilizing Bootstrap was developed using the DNN, as it performed higher than the Random Forest ML model with a total accuracy rate of 92%. △ Less","25 July, 2020",https://arxiv.org/pdf/2007.12998
Insightful Assistant: AI-compatible Operation Graph Representations for Enhancing Industrial Conversational Agents,Bekir Bayrak;Florian Giger;Christian Meurisch,"Advances in voice-controlled assistants paved the way into the consumer market. For professional or industrial use, the capabilities of such assistants are too limited or too time-consuming to implement due to the higher complexity of data, possible AI-based operations, and requests. In the light of these deficits, this paper presents Insightful Assistant---a pipeline concept based on a novel operation graph representation resulting from the intents detected. Using a predefined set of semantically annotated (executable) functions, each node of the operation graph is assigned to a function for execution. Besides basic operations, such functions can contain artificial intelligence (AI) based operations (e.g., anomaly detection). The result is then visualized to the user according to type and extracted user preferences in an automated way. We further collected a unique crowd-sourced set of 869 requests, each with four different variants expected visualization, for an industrial dataset. The evaluation of our proof-of-concept prototype on this dataset shows its feasibility: it achieves an accuracy of up to 95.0% (74.5%) for simple (complex) request detection with different variants and a top3-accuracy up to 95.4% for data-/user-adaptive visualization. △ Less","25 July, 2020",https://arxiv.org/pdf/2007.12929
Privacy vs National Security,Tajdar Jawaid,"There are growing concerns and anxiety about privacy among the general public especially after the revelations of former NSA contractor and whistleblowers like Edward Snowden and others. While privacy is the fundamental concept of being human, the growing tug-of-war between an individuals privacy and freedom vs national security has renewed the concerns about where the fine balance should lie between the two. For the first time in history the technological advancement has made the mass data gathering, analysis, and storage a financially and technologically feasible option for the governments and private businesses. This has led to the growing interest of governments and security agencies around the globe to develop sophisticated algorithms using the power of Big-Data, Machine-Learning and Artificial Intelligence. The technology has enabled governments and private businesses to collect and store thousands of data points on every individual, which has put an individuals privacy under constant threat. This article analyses the individual's privacy concepts and its perceived link with national security. The article will also discuss the various aspects of privacy and national-security, arguments of both sides and where a boundary should be drawn between privacy and national security. △ Less","10 July, 2020",https://arxiv.org/pdf/2007.12633
Introduction to Behavior Algorithms for Fighting Games,Ignacio Gajardo;Felipe Besoain;Nicolas A. Barriga,"The quality of opponent Artificial Intelligence (AI) in fighting videogames is crucial. Some other game genres can rely on their story or visuals, but fighting games are all about the adversarial experience. In this paper, we will introduce standard behavior algorithms in videogames, such as Finite-State Machines and Behavior Trees, as well as more recent developments, such as Monte-Carlo Tree Search. We will also discuss the existing and potential combinations of these algorithms, and how they might be used in fighting games. Since we are at the financial peak of fighting games, both for casual players and in tournaments, it is important to build and expand on fighting game AI, as it is one of the pillars of this growing market. △ Less","6 July, 2020",https://arxiv.org/pdf/2007.12586
Monte-Carlo Tree Search as Regularized Policy Optimization,Jean-Bastien Grill;Florent Altché;Yunhao Tang;Thomas Hubert;Michal Valko;Ioannis Antonoglou;Rémi Munos,"The combination of Monte-Carlo tree search (MCTS) with deep reinforcement learning has led to significant advances in artificial intelligence. However, AlphaZero, the current state-of-the-art MCTS algorithm, still relies on handcrafted heuristics that are only partially understood. In this paper, we show that AlphaZero's search heuristics, along with other common ones such as UCT, are an approximation to the solution of a specific regularized policy optimization problem. With this insight, we propose a variant of AlphaZero which uses the exact solution to this policy optimization problem, and show experimentally that it reliably outperforms the original algorithm in multiple domains. △ Less","24 July, 2020",https://arxiv.org/pdf/2007.12509
Clinical Recommender System: Predicting Medical Specialty Diagnostic Choices with Neural Network Ensembles,Morteza Noshad;Ivana Jankovic;Jonathan H. Chen,"The growing demand for key healthcare resources such as clinical expertise and facilities has motivated the emergence of artificial intelligence (AI) based decision support systems. We address the problem of predicting clinical workups for specialty referrals. As an alternative for manually-created clinical checklists, we propose a data-driven model that recommends the necessary set of diagnostic procedures based on the patients' most recent clinical record extracted from the Electronic Health Record (EHR). This has the potential to enable health systems expand timely access to initial medical specialty diagnostic workups for patients. The proposed approach is based on an ensemble of feed-forward neural networks and achieves significantly higher accuracy compared to the conventional clinical checklists. △ Less","23 July, 2020",https://arxiv.org/pdf/2007.12161
"Time Perception: A Review on Psychological, Computational and Robotic Models",Hamit Basgol;Inci Ayhan;Emre Ugur,"Animals exploit time to survive in the world. Temporal information is required for higher-level cognitive abilities such as planning, decision making, communication, and effective cooperation. Since time is an inseparable part of cognition, there is a growing interest in the artificial intelligence approach to subjective time, which has a possibility of advancing the field. The current survey study aims to provide researchers with an interdisciplinary perspective on time perception. Firstly, we introduce a brief background from the psychology and neuroscience literature, covering the characteristics and models of time perception and related abilities. Secondly, we summarize the emergent computational and robotic models of time perception. A general overview to the literature reveals that a substantial amount of timing models are based on a dedicated time processing like the emergence of a clock-like mechanism from the neural network dynamics and reveal a relationship between the embodiment and time perception. We also notice that most models of timing are developed for either sensory timing (i.e. ability to assess an interval) or motor timing (i.e. ability to reproduce an interval). The number of timing models capable of retrospective timing, which is the ability to track time without paying attention, is insufficient. In this light, we discuss the possible research directions to promote interdisciplinary collaboration in the field of time perception. △ Less","25 December, 2020",https://arxiv.org/pdf/2007.11845
Privacy-preserving Artificial Intelligence Techniques in Biomedicine,Reihaneh Torkzadehmahani;Reza Nasirigerdeh;David B. Blumenthal;Tim Kacprowski;Markus List;Julian Matschinske;Julian Späth;Nina Kerstin Wenke;Béla Bihari;Tobias Frisch;Anne Hartebrodt;Anne-Christin Hausschild;Dominik Heider;Andreas Holzinger;Walter Hötzendorfer;Markus Kastelitz;Rudolf Mayer;Cristian Nogales;Anastasia Pustozerova;Richard Röttger;Harald H. H. W. Schmidt;Ameli Schwalber;Christof Tschohl;Andrea Wohner;Jan Baumbach,"Artificial intelligence (AI) has been successfully applied in numerous scientific domains. In biomedicine, AI has already shown tremendous potential, e.g. in the interpretation of next-generation sequencing data and in the design of clinical decision support systems. However, training an AI model on sensitive data raises concerns about the privacy of individual participants. For example, summary statistics of a genome-wide association study can be used to determine the presence or absence of an individual in a given dataset. This considerable privacy risk has led to restrictions in accessing genomic and other biomedical data, which is detrimental for collaborative research and impedes scientific progress. Hence, there has been a substantial effort to develop AI methods that can learn from sensitive data while protecting individuals' privacy. This paper provides a structured overview of recent advances in privacy-preserving AI techniques in biomedicine. It places the most important state-of-the-art approaches within a unified taxonomy and discusses their strengths, limitations, and open problems. As the most promising direction, we suggest combining federated machine learning as a more scalable approach with other additional privacy preserving techniques. This would allow to merge the advantages to provide privacy guarantees in a distributed way for biomedical applications. Nonetheless, more research is necessary as hybrid approaches pose new challenges such as additional network or computation overhead. △ Less","6 November, 2020",https://arxiv.org/pdf/2007.11621
Regulating human control over autonomous systems,Mikolaj firlej;Araz Taeihagh,"In recent years, many sectors have experienced significant progress in automation, associated with the growing advances in artificial intelligence and machine learning. There are already automated robotic weapons, which are able to evaluate and engage with targets on their own, and there are already autonomous vehicles that do not need a human driver. It is argued that the use of increasingly autonomous systems (AS) should be guided by the policy of human control, according to which humans should execute a certain significant level of judgment over AS. While in the military sector there is a fear that AS could mean that humans lose control over life and death decisions, in the transportation domain, on the contrary, there is a strongly held view that autonomy could bring significant operational benefits by removing the need for a human driver. This article explores the notion of human control in the United States in the two domains of defense and transportation. The operationalization of emerging policies of human control results in the typology of direct and indirect human controls exercised over the use of AS. The typology helps to steer the debate away from the linguistic complexities of the term autonomy. It identifies instead where human factors are undergoing important changes and ultimately informs about more detailed rules and standards formulation, which differ across domains, applications, and sectors. △ Less","22 July, 2020",https://arxiv.org/pdf/2007.11218
Accelerating Deep Learning Applications in Space,Martina Lofqvist;José Cano,"Computing at the edge offers intriguing possibilities for the development of autonomy and artificial intelligence. The advancements in autonomous technologies and the resurgence of computer vision have led to a rise in demand for fast and reliable deep learning applications. In recent years, the industry has introduced devices with impressive processing power to perform various object detection tasks. However, with real-time detection, devices are constrained in memory, computational capacity, and power, which may compromise the overall performance. This could be solved either by optimizing the object detector or modifying the images. In this paper, we investigate the performance of CNN-based object detectors on constrained devices when applying different image compression techniques. We examine the capabilities of a NVIDIA Jetson Nano; a low-power, high-performance computer, with an integrated GPU, small enough to fit on-board a CubeSat. We take a closer look at the Single Shot MultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN) that are pre-trained on DOTA - a Large Scale Dataset for Object Detection in Aerial Images. The performance is measured in terms of inference time, memory consumption, and accuracy. By applying image compression techniques, we are able to optimize performance. The two techniques applied, lossless compression and image scaling, improves speed and memory consumption with no or little change in accuracy. The image scaling technique achieves a 100% runnable dataset and we suggest combining both techniques in order to optimize the speed/memory/accuracy trade-off. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.11089
Forecasting Brazilian and American COVID-19 cases based on artificial intelligence coupled with climatic exogenous variables,Ramon Gomes da Silva;Matheus Henrique Dal Molin Ribeiro;Viviana Cocco Mariani;Leandro dos Santos Coelho,"The novel coronavirus disease (COVID-19) is a public health problem once according to the World Health Organization up to June 10th, 2020, more than 7.1 million people were infected, and more than 400 thousand have died worldwide. In the current scenario, the Brazil and the United States of America present a high daily incidence of new cases and deaths. It is important to forecast the number of new cases in a time window of one week, once this can help the public health system developing strategic planning to deals with the COVID-19. In this paper, Bayesian regression neural network, cubist regression, k-nearest neighbors, quantile random forest, and support vector regression, are used stand-alone, and coupled with the recent pre-processing variational mode decomposition (VMD) employed to decompose the time series into several intrinsic mode functions. All Artificial Intelligence techniques are evaluated in the task of time-series forecasting with one, three, and six-days-ahead the cumulative COVID-19 cases in five Brazilian and American states up to April 28th, 2020. Previous cumulative COVID-19 cases and exogenous variables as daily temperature and precipitation were employed as inputs for all forecasting models. The hybridization of VMD outperformed single forecasting models regarding the accuracy, specifically when the horizon is six-days-ahead, achieving better accuracy in 70% of the cases. Regarding the exogenous variables, the importance ranking as predictor variables is past cases, temperature, and precipitation. Due to the efficiency of evaluated models to forecasting cumulative COVID-19 cases up to six-days-ahead, the adopted models can be recommended as a promising models for forecasting and be used to assist in the development of public policies to mitigate the effects of COVID-19 outbreak. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.10981
DeepNetQoE: Self-adaptive QoE Optimization Framework of Deep Networks,Rui Wang;Min Chen;Nadra Guizani;Yong Li;Hamid Gharavi;Kai Hwang,"Future advances in deep learning and its impact on the development of artificial intelligence (AI) in all fields depends heavily on data size and computational power. Sacrificing massive computing resources in exchange for better precision rates of the network model is recognized by many researchers. This leads to huge computing consumption and satisfactory results are not always expected when computing resources are limited. Therefore, it is necessary to find a balance between resources and model performance to achieve satisfactory results. This article proposes a self-adaptive quality of experience (QoE) framework, DeepNetQoE, to guide the training of deep networks. A self-adaptive QoE model is set up that relates the model's accuracy with the computing resources required for training which will allow the experience value of the model to improve. To maximize the experience value when computer resources are limited, a resource allocation model and solutions need to be established. In addition, we carry out experiments based on four network models to analyze the experience values with respect to the crowd counting example. Experimental results show that the proposed DeepNetQoE is capable of adaptively obtaining a high experience value according to user needs and therefore guiding users to determine the computational resources allocated to the network models. △ Less","16 July, 2020",https://arxiv.org/pdf/2007.10878
Intelligent Reflecting Surfaces Assisted Secure Transmission Without Eavesdropper's CSI,Hui-Ming Wang;Jiale Bai;Limeng Dong,"In this letter, improving the security of an intelligent reflecting surface (IRS) assisted multiple-input single-output (MISO) communication system is studied. Different from the ideal assumption in existing literatures that full eavesdropper's (Eve's) channel state information (CSI) is available, we consider a more practical scenario without Eve's CSI. To enhance the security of this system given a total transmit power at transmitter (Alice), we propose a joint beamforming and jamming approach, in which a minimum transmit power is firstly optimized at Alice so as to meet the quality of service (QoS) at legitimate user (Bob), and then artificial noise (AN) is emitted to jam the eavesdropper by using the residual power at Alice. Two efficient algorithms exploiting oblique manifold (OM) and minorizationmaximization (MM) algorithms, respectively, are developed for solving the resulting non-convex optimization problem. Simulation results have been provided to validate the performance and convergence of the proposed algorithms. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.10788
Melody: Generating and Visualizing Machine Learning Model Summary to Understand Data and Classifiers Together,Gromit Yeuk-Yin Chan;Enrico Bertini;Luis Gustavo Nonato;Brian Barr;Claudio T. Silva,"With the increasing sophistication of machine learning models, there are growing trends of developing model explanation techniques that focus on only one instance (local explanation) to ensure faithfulness to the original model. While these techniques provide accurate model interpretability on various data primitive (e.g., tabular, image, or text), a holistic Explainable Artificial Intelligence (XAI) experience also requires a global explanation of the model and dataset to enable sensemaking in different granularity. Thus, there is a vast potential in synergizing the model explanation and visual analytics approaches. In this paper, we present MELODY, an interactive algorithm to construct an optimal global overview of the model and data behavior by summarizing the local explanations using information theory. The result (i.e., an explanation summary) does not require additional learning models, restrictions of data primitives, or the knowledge of machine learning from the users. We also design MELODY UI, an interactive visual analytics system to demonstrate how the explanation summary connects the dots in various XAI tasks from a global overview to local inspections. We present three usage scenarios regarding tabular, image, and text classifications to illustrate how to generalize model interpretability of different data. Our experiments show that our approaches: (1) provides a better explanation summary compared to a straightforward information-theoretic summarization and (2) achieves a significant speedup in the end-to-end data modeling pipeline. △ Less","21 July, 2020",https://arxiv.org/pdf/2007.10614
AI Tax: The Hidden Cost of AI Data Center Applications,Daniel Richins;Dharmisha Doshi;Matthew Blackmore;Aswathy Thulaseedharan Nair;Neha Pathapati;Ankit Patel;Brainard Daguman;Daniel Dobrijalowski;Ramesh Illikkal;Kevin Long;David Zimmerman;Vijay Janapa Reddi,"Artificial intelligence and machine learning are experiencing widespread adoption in industry and academia. This has been driven by rapid advances in the applications and accuracy of AI through increasingly complex algorithms and models; this, in turn, has spurred research into specialized hardware AI accelerators. Given the rapid pace of advances, it is easy to forget that they are often developed and evaluated in a vacuum without considering the full application environment. This paper emphasizes the need for a holistic, end-to-end analysis of AI workloads and reveals the ""AI tax."" We deploy and characterize Face Recognition in an edge data center. The application is an AI-centric edge video analytics application built using popular open source infrastructure and ML tools. Despite using state-of-the-art AI and ML algorithms, the application relies heavily on pre-and post-processing code. As AI-centric applications benefit from the acceleration promised by accelerators, we find they impose stresses on the hardware and software infrastructure: storage and network bandwidth become major bottlenecks with increasing AI acceleration. By specializing for AI applications, we show that a purpose-built edge data center can be designed for the stresses of accelerated AI at 15% lower TCO than one derived from homogeneous servers and infrastructure. △ Less","20 July, 2020",https://arxiv.org/pdf/2007.10571
The Future AI in Healthcare: A Tsunami of False Alarms or a Product of Experts?,Gari D. Clifford,"Recent significant increases in affordable and accessible computational power and data storage have enabled machine learning to provide almost unbelievable classification and prediction performances compared to well-trained humans. There have been some promising (but limited) results in the complex healthcare landscape, particularly in imaging. This promise has led some individuals to leap to the conclusion that we will solve an ever-increasing number of problems in human health and medicine by applying `artificial intelligence' to `big (medical) data'. The scientific literature has been inundated with algorithms, outstripping our ability to review them effectively. Unfortunately, I argue that most, if not all of these publications or commercial algorithms make several fundamental errors. I argue that because everyone (and therefore every algorithm) has blind spots, there are multiple `best' algorithms, each of which excels on different types of patients or in different contexts. Consequently, we should vote many algorithms together, weighted by their overall performance, their independence from each other, and a set of features that define the context (i.e., the features that maximally discriminate between the situations when one algorithm outperforms another). This approach not only provides a better performing classifier or predictor but provides confidence intervals so that a clinician can judge how to respond to an alert. Moreover, I argue that a sufficient number of (mostly) independent algorithms that address the same problem can be generated through a large international competition/challenge, lasting many months and define the conditions for a successful event. Finally, I propose introducing the requirement for major grantees to run challenges in the final year of funding to maximize the value of research and select a new generation of grantees. △ Less","26 July, 2020",https://arxiv.org/pdf/2007.10502
Deep multi-metric learning for text-independent speaker verification,Jiwei Xu;Xinggang Wang;Bin Feng;Wenyu Liu,"Text-independent speaker verification is an important artificial intelligence problem that has a wide spectrum of applications, such as criminal investigation, payment certification, and interest-based customer services. The purpose of text-independent speaker verification is to determine whether two given uncontrolled utterances originate from the same speaker or not. Extracting speech features for each speaker using deep neural networks is a promising direction to explore and a straightforward solution is to train the discriminative feature extraction network by using a metric learning loss function. However, a single loss function often has certain limitations. Thus, we use deep multi-metric learning to address the problem and introduce three different losses for this problem, i.e., triplet loss, n-pair loss and angular loss. The three loss functions work in a cooperative way to train a feature extraction network equipped with Residual connections and squeeze-and-excitation attention. We conduct experiments on the large-scale \texttt{VoxCeleb2} dataset, which contains over a million utterances from over 6,000 speakers, and the proposed deep neural network obtains an equal error rate of 3.48\%, which is a very competitive result. Codes for both training and testing and pretrained models are available at \url{https://github.com/GreatJiweix/DmmlTiSV}, which is the first publicly available code repository for large-scale text-independent speaker verification with performance on par with the state-of-the-art systems. △ Less","17 July, 2020",https://arxiv.org/pdf/2007.10479
Future Smart Connected Communities to Fight COVID-19 Outbreak,Deepti Gupta;Smriti Bhatt;Maanak Gupta;Ali Saman Tosun,"Internet of Things (IoT) has grown rapidly in the last decade and continue to develop in terms of dimension and complexity offering wide range of devices to support diverse set of applications. With ubiquitous Internet, connected sensors and actuators, networking and communication technology, and artificial intelligence (AI), smart cyber-physical systems (CPS) provide services rendering assistance to humans in their daily lives. However, the recent outbreak of COVID-19 (also known as coronavirus) pandemic has exposed and highlighted the limitations of current technological deployments to curtail this disease. IoT and smart connected technologies together with data-driven applications can play a crucial role not only in prevention, continuous monitoring, and mitigation of the disease, but also enable prompt enforcement of guidelines, rules and government orders to contain such future outbreaks. In this paper, we envision an IoT-enabled ecosystem for intelligent monitoring, pro-active prevention and control, and mitigation of COVID-19. We propose different architectures, applications and technology systems for various smart infrastructures including E-health, smart home, smart supply chain management, smart locality, and smart city, to develop future connected communities to manage and mitigate similar outbreaks. Furthermore, we present research challenges together with future directions to enable and develop these smart communities and infrastructures to fight and prepare against such outbreaks. △ Less","26 November, 2020",https://arxiv.org/pdf/2007.10477
On the Use of AI for Satellite Communications,Miguel Ángel Vázquez;Pol Henarejos;Ana I. Pérez-Neira;Elena Grechi;Andreas Voight;Juan Carlos Gil;Irene Pappalardo;Federico Di Credico;Rocco Michele Lancellotti,"This document presents an initial approach to the investigation and development of artificial intelligence (AI) mechanisms in satellite communication (SatCom) systems. We first introduce the nowadays SatCom operations which are strongly dependent on the human intervention. Along with those use cases, we present an initial way of automatizing some of those tasks and we show the key AI tools capable of dealing with those challenges. Finally, the long term AI developments in the SatCom sector is discussed. △ Less","15 July, 2020",https://arxiv.org/pdf/2007.10110
Antarjami: Exploring psychometric evaluation through a computer-based game,Anirban Lahiri;Utanko Mitra;Sunreeta Sen;Mrinal Chakraborty;Max Kleiman-Weiner;Rajlakshmi Guha;Pabitra Mitra;Anupam Basu;Partha Pratim Chakraborty,"A number of questionnaire based psychometric testing frameworks are globally for example OCEAN (Five factor) indicator, MBTI (Myers Brigg Type Indicator) etc. However, questionnaire based psychometric tests have some known shortcomings. This work explores whether these shortcomings can be mitigated through computer-based gaming platforms for evaluating psychometric parameters. A computer based psychometric game framework called Antarjami has been developed for evaluating OCEAN (Five factor) indicators. It investigates the feasibility of extracting psychometric parameters through computer-based games, utilizing underlying improvements in the area of modern artificial intelligence. The candidates for the test are subjected to a number scenarios as part of the computer based game and their reactions/responses are used to evaluate their psychometric parameters. As part of the study, the parameters obtained from the game were compared with those evaluated using paper based tests and scores given by a panel of psychologists. The achieved results were very promising. △ Less","16 July, 2020",https://arxiv.org/pdf/2007.10089
CobotGear: Interaction with Collaborative Robots using Wearable Optical Motion Capturing Systems,Juan Heredia;Miguel Altamirano Cabrera;Jonathan Tirado;Vladislav Panov;Dzmitry Tsetserukou,"In industrial applications, complex tasks require human collaboration since the robot doesn't have enough dexterity. However, the robots are still implemented as tools and not as collaborative intelligent systems. To ensure safety in the human-robot collaboration, we introduce a system that presents a new method that integrates low-cost wearable mocap, and an improved collision avoidance algorithm based on the artificial potential fields. Wearable optical motion capturing allows to track the human hand position with high accuracy and low latency on large working areas. To increase the efficiency of the proposed algorithm, two obstacle types are discriminated according to their collision probability. A preliminary experiment was performed to analyze the algorithm behavior and to select the best values for the obstacle's threshold angle θ_{OBS}, and for the avoidance threshold distance d_{AT}. The second experiment was carried out to evaluate the system performance with d_{AT} = 0.2 m and θ_{OBS} = 45 degrees. The third experiment evaluated the system in a real collaborative task. The results demonstrate the robust performance of the robotic arm generating smooth collision-free trajectories. The proposed technology will allow consumer robots to safely collaborate with humans in cluttered environments, e.g., factories, kitchens, living rooms, and restaurants. △ Less","20 July, 2020",https://arxiv.org/pdf/2007.10015
An Open-World Simulated Environment for Developmental Robotics,SM Mazharul Islam;Md Ashaduzzaman Rubel Mondol;Aishwarya Pothula;Deokgun Park,"As the current trend of artificial intelligence is shifting towards self-supervised learning, conventional norms such as highly curated domain-specific data, application-specific learning models, extrinsic reward based learning policies etc. might not provide with the suitable ground for such developments. In this paper, we introduce SEDRo, a Simulated Environment for Developmental Robotics which allows a learning agent to have similar experiences that a human infant goes through from the fetus stage up to 12 months. A series of simulated tests based on developmental psychology will be used to evaluate the progress of a learning model. △ Less","17 July, 2020",https://arxiv.org/pdf/2007.09300
KubeEdge.AI: AI Platform for Edge Devices,Sean Wang;Yuxiao Hu;Jason Wu,"The demand for smartness in embedded systems has been mounting up drastically in the past few years. Embedded system today must address the fundamental challenges introduced by cloud computing and artificial intelligence. KubeEdge [1] is an edge computing framework build on top of Kubernetes [2]. It provides compute resource management, deployment, runtime and operation capabilities on geo-located edge computing resources, from the cloud, which is a natural fit for embedded systems. Here we propose KubeEdge.AI, an edge AI framework on top of KubeEdge. It provides a set of key modules and interfaces: a data handling and processing engine, a concise AI runtime, a decision engine, and a distributed data query interface. KubeEdge.AI will help reduce the burdens for developing specific edge/embedded AI systems and promote edge-cloud coordination and synergy. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.09227
Breaking Moravec's Paradox: Visual-Based Distribution in Smart Fashion Retail,Shin Woong Sung;Hyunsuk Baek;Hyeonjun Sim;Eun Hie Kim;Hyunwoo Hwangbo;Young Jae Jang,"In this paper, we report an industry-academia collaborative study on the distribution method of fashion products using an artificial intelligence (AI) technique combined with an optimization method. To meet the current fashion trend of short product lifetimes and an increasing variety of styles, the company produces limited volumes of a large variety of styles. However, due to the limited volume of each style, some styles may not be distributed to some off-line stores. As a result, this high-variety, low-volume strategy presents another challenge to distribution managers. We collaborated with KOLON F/C, one of the largest fashion business units in South Korea, to develop models and an algorithm to optimally distribute the products to the stores based on the visual images of the products. The team developed a deep learning model that effectively represents the styles of clothes based on their visual image. Moreover, the team created an optimization model that effectively determines the product mix for each store based on the image representation of clothes. In the past, computers were only considered to be useful for conducting logical calculations, and visual perception and cognition were considered to be difficult computational tasks. The proposed approach is significant in that it uses both AI (perception and cognition) and mathematical optimization (logical calculation) to address a practical supply chain problem, which is why the study was called ""Breaking Moravec's Paradox."" △ Less","9 July, 2020",https://arxiv.org/pdf/2007.09102
Standing on the Shoulders of Giants: Hardware and Neural Architecture Co-Search with Hot Start,Weiwen Jiang;Lei Yang;Sakyasingha Dasgupta;Jingtong Hu;Yiyu Shi,"Hardware and neural architecture co-search that automatically generates Artificial Intelligence (AI) solutions from a given dataset is promising to promote AI democratization; however, the amount of time that is required by current co-search frameworks is in the order of hundreds of GPU hours for one target hardware. This inhibits the use of such frameworks on commodity hardware. The root cause of the low efficiency in existing co-search frameworks is the fact that they start from a ""cold"" state (i.e., search from scratch). In this paper, we propose a novel framework, namely HotNAS, that starts from a ""hot"" state based on a set of existing pre-trained models (a.k.a. model zoo) to avoid lengthy training time. As such, the search time can be reduced from 200 GPU hours to less than 3 GPU hours. In HotNAS, in addition to hardware design space and neural architecture search space, we further integrate a compression space to conduct model compressing during the co-search, which creates new opportunities to reduce latency but also brings challenges. One of the key challenges is that all of the above search spaces are coupled with each other, e.g., compression may not work without hardware design support. To tackle this issue, HotNAS builds a chain of tools to design hardware to support compression, based on which a global optimizer is developed to automatically co-search all the involved search spaces. Experiments on ImageNet dataset and Xilinx FPGA show that, within the timing constraint of 5ms, neural architectures generated by HotNAS can achieve up to 5.79% Top-1 and 3.97% Top-5 accuracy gain, compared with the existing ones. △ Less","17 July, 2020",https://arxiv.org/pdf/2007.09087
Conservative AI and social inequality: Conceptualizing alternatives to bias through social theory,Mike Zajko,"In response to calls for greater interdisciplinary involvement from the social sciences and humanities in the development, governance, and study of artificial intelligence systems, this paper presents one sociologist's view on the problem of algorithmic bias and the reproduction of societal bias. Discussions of bias in AI cover much of the same conceptual terrain that sociologists studying inequality have long understood using more specific terms and theories. Concerns over reproducing societal bias should be informed by an understanding of the ways that inequality is continually reproduced in society -- processes that AI systems are either complicit in, or can be designed to disrupt and counter. The contrast presented here is between conservative and radical approaches to AI, with conservatism referring to dominant tendencies that reproduce and strengthen the status quo, while radical approaches work to disrupt systemic forms of inequality. The limitations of conservative approaches to class, gender, and racial bias are discussed as specific examples, along with the social structures and processes that biases in these areas are linked to. Societal issues can no longer be out of scope for AI and machine learning, given the impact of these systems on human lives. This requires engagement with a growing body of critical AI scholarship that goes beyond biased data to analyze structured ways of perpetuating inequality, opening up the possibility for radical alternatives. △ Less","16 July, 2020",https://arxiv.org/pdf/2007.08666
Co-generation of game levels and game-playing agents,Aaron Dharna;Julian Togelius;L. B. Soros,"Open-endedness, primarily studied in the context of artificial life, is the ability of systems to generate potentially unbounded ontologies of increasing novelty and complexity. Engineering generative systems displaying at least some degree of this ability is a goal with clear applications to procedural content generation in games. The Paired Open-Ended Trailblazer (POET) algorithm, heretofore explored only in a biped walking domain, is a coevolutionary system that simultaneously generates environments and agents that can solve them. This paper introduces a POET-Inspired Neuroevolutionary System for KreativitY (PINSKY) in games, which co-generates levels for multiple video games and agents that play them. This system leverages the General Video Game Artificial Intelligence (GVGAI) framework to enable co-generation of levels and agents for the 2D Atari-style games Zelda and Solar Fox. Results demonstrate the ability of PINSKY to generate curricula of game levels, opening up a promising new avenue for research at the intersection of procedural content generation and artificial life. At the same time, results in these challenging game domains highlight the limitations of the current algorithm and opportunities for improvement. △ Less","28 August, 2020",https://arxiv.org/pdf/2007.08497
Deep ahead-of-threat virtual patching,Fady Copty;Andre Kassis;Sharon Keidar-Barner;Dov Murik,"Many applications have security vulnerabilities that can be exploited. It is practically impossible to find all of them due to the NP-complete nature of the testing problem. Security solutions provide defenses against these attacks through continuous application testing, fast-patching of vulnerabilities, automatic deployment of patches, and virtual patching detection techniques deployed in network and endpoint security tools. These techniques are limited by the need to find vulnerabilities before the black-hats. We propose an innovative technique to virtually patch vulnerabilities before they are found. We leverage testing techniques for supervised-learning data generation, and show how artificial intelligence techniques can use this data to create predictive deep neural-network models that read an application's input and predict in real time whether it is a potential malicious input. We set up an ahead-of-threat experiment in which we generated data on old versions of an application, and then evaluated the predictive model accuracy on vulnerabilities found years later. Our experiments show ahead-of-threat detection on LibXML2 and LibTIFF vulnerabilities with 91.3% and 93.7% accuracy, respectively. We expect to continue work on this field of research and provide ahead-of-threat virtual patching for more libraries. Success in this research can change the current state of endless racing after application vulnerabilities and put the defenders one step ahead of the attackers △ Less","16 July, 2020",https://arxiv.org/pdf/2007.08296
Opening the Software Engineering Toolbox for the Assessment of Trustworthy AI,Mohit Kumar Ahuja;Mohamed-Bachir Belaid;Pierre Bernabé;Mathieu Collet;Arnaud Gotlieb;Chhagan Lal;Dusica Marijan;Sagar Sen;Aizaz Sharif;Helge Spieker,"Trustworthiness is a central requirement for the acceptance and success of human-centered artificial intelligence (AI). To deem an AI system as trustworthy, it is crucial to assess its behaviour and characteristics against a gold standard of Trustworthy AI, consisting of guidelines, requirements, or only expectations. While AI systems are highly complex, their implementations are still based on software. The software engineering community has a long-established toolbox for the assessment of software systems, especially in the context of software testing. In this paper, we argue for the application of software engineering and testing practices for the assessment of trustworthy AI. We make the connection between the seven key requirements as defined by the European Commission's AI high-level expert group and established procedures from software engineering and raise questions for future work. △ Less","30 August, 2020",https://arxiv.org/pdf/2007.07768
NERD: Neural Network for Edict of Risky Data Streams,Sandro Passarelli;Cem Gündogan;Lars Stiemert;Matthias Schopp;Peter Hillmann,"Cyber incidents can have a wide range of cause from a simple connection loss to an insistent attack. Once a potential cyber security incidents and system failures have been identified, deciding how to proceed is often complex. Especially, if the real cause is not directly in detail determinable. Therefore, we developed the concept of a Cyber Incident Handling Support System. The developed system is enriched with information by multiple sources such as intrusion detection systems and monitoring tools. It uses over twenty key attributes like sync-package ratio to identify potential security incidents and to classify the data into different priority categories. Afterwards, the system uses artificial intelligence to support the further decision-making process and to generate corresponding reports to brief the Board of Directors. Originating from this information, appropriate and detailed suggestions are made regarding the causes and troubleshooting measures. Feedback from users regarding the problem solutions are included into future decision-making by using labelled flow data as input for the learning process. The prototype shows that the decision making can be sustainably improved and the Cyber Incident Handling process becomes much more effective. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.07753
"Preliminary Results from a Peer-Led, Social Network Intervention, Augmented by Artificial Intelligence to Prevent HIV among Youth Experiencing Homelessness",Eric Rice;Laura Onasch-Vera;Graham T. DiGuiseppi;Bryan Wilder;Robin Petering;Chyna Hill;Amulya Yadav;Milind Tambe,"Each year, there are nearly 4 million youth experiencing homelessness (YEH) in the United States with HIV prevalence ranging from 3 to 11.5%. Peer change agent (PCA) models for HIV prevention have been used successfully in many populations, but there have been notable failures. In recent years, network interventionists have suggested that these failures could be attributed to PCA selection procedures. The change agents themselves who are selected to do the PCA work can often be as important as the messages they convey. To address this concern, we tested a new PCA intervention for YEH, with three arms: (1) an arm using an artificial intelligence (AI) planning algorithm to select PCA, (2) a popularity arm--the standard PCA approach--operationalized as highest degree centrality (DC), and (3) an observation only comparison group (OBS). PCA models that promote HIV testing, HIV knowledge, and condom use are efficacious for YEH. Both the AI and DC arms showed improvements over time. AI-based PCA selection led to better outcomes and increased the speed of intervention effects. Specifically, the changes in behavior observed in the AI arm occurred by 1 month, but not until 3 months in the DC arm. Given the transient nature of YEH and the high risk for HIV infection, more rapid intervention effects are desirable. △ Less","10 July, 2020",https://arxiv.org/pdf/2007.07747
"Human \neq
AGI",Roman V. Yampolskiy,"Terms Artificial General Intelligence (AGI) and Human-Level Artificial Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail of Artificial Intelligence (AI) research, creation of a machine capable of achieving goals in a wide range of environments. However, widespread implicit assumption of equivalence between capabilities of AGI and HLAI appears to be unjustified, as humans are not general intelligences. In this paper, we will prove this distinction. △ Less","11 July, 2020",https://arxiv.org/pdf/2007.07710
Energy-Efficient Resource Management for Federated Edge Learning with CPU-GPU Heterogeneous Computing,Qunsong Zeng;Yuqing Du;Kaibin Huang;Kin K. Leung,"Edge machine learning involves the deployment of learning algorithms at the network edge to leverage massive distributed data and computation resources to train artificial intelligence (AI) models. Among others, the framework of federated edge learning (FEEL) is popular for its data-privacy preservation. FEEL coordinates global model training at an edge server and local model training at edge devices that are connected by wireless links. This work contributes to the energy-efficient implementation of FEEL in wireless networks by designing joint computation-and-communication resource management (\text{C}^2RM). The design targets the state-of-the-art heterogeneous mobile architecture where parallel computing using both a CPU and a GPU, called heterogeneous computing, can significantly improve both the performance and energy efficiency. To minimize the sum energy consumption of devices, we propose a novel \text{C}^2RM framework featuring multi-dimensional control including bandwidth allocation, CPU-GPU workload partitioning and speed scaling at each device, and \text{C}^2 time division for each link. The key component of the framework is a set of equilibriums in energy rates with respect to different control variables that are proved to exist among devices or between processing units at each device. The results are applied to designing efficient algorithms for computing the optimal \text{C}^2RM policies faster than the standard optimization tools. Based on the equilibriums, we further design energy-efficient schemes for device scheduling and greedy spectrum sharing that scavenges ""spectrum holes"" resulting from heterogeneous \text{C}^2 time divisions among devices. Using a real dataset, experiments are conducted to demonstrate the effectiveness of \text{C}^2RM on improving the energy efficiency of a FEEL system. △ Less","15 July, 2020",https://arxiv.org/pdf/2007.07122
Inertial Sensing Meets Artificial Intelligence: Opportunity or Challenge?,You Li;Ruizhi Chen;Xiaoji Niu;Yuan Zhuang;Zhouzheng Gao;Xin Hu;Naser El-Sheimy,"The inertial navigation system (INS) has been widely used to provide self-contained and continuous motion estimation in intelligent transportation systems. Recently, the emergence of chip-level inertial sensors has expanded the relevant applications from positioning, navigation, and mobile mapping to location-based services, unmanned systems, and transportation big data. Meanwhile, benefit from the emergence of big data and the improvement of algorithms and computing power, artificial intelligence (AI) has become a consensus tool that has been successfully applied in various fields. This article reviews the research on using AI technology to enhance inertial sensing from various aspects, including sensor design and selection, calibration and error modeling, navigation and motion-sensing algorithms, multi-sensor information fusion, system evaluation, and practical application. Based on the over 30 representative articles selected from the nearly 300 related publications, this article summarizes the state of the art, advantages, and challenges on each aspect. Finally, it summarizes nine advantages and nine challenges of AI-enhanced inertial sensing and then points out future research directions. △ Less","13 July, 2020",https://arxiv.org/pdf/2007.06727
Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI,Amirhossein Tavanaei,"Understanding intermediate layers of a deep learning model and discovering the driving features of stimuli have attracted much interest, recently. Explainable artificial intelligence (XAI) provides a new way to open an AI black box and makes a transparent and interpretable decision. This paper proposes a new explainable convolutional neural network (XCNN) which represents important and driving visual features of stimuli in an end-to-end model architecture. This network employs encoder-decoder neural networks in a CNN architecture to represent regions of interest in an image based on its category. The proposed model is trained without localization labels and generates a heat-map as part of the network architecture without extra post-processing steps. The experimental results on the CIFAR-10, Tiny ImageNet, and MNIST datasets showed the success of our algorithm (XCNN) to make CNNs explainable. Based on visual assessment, the proposed model outperforms the current algorithms in class-specific feature representation and interpretable heatmap generation while providing a simple and flexible network architecture. The initial success of this approach warrants further study to enhance weakly supervised localization and semantic segmentation in explainable frameworks. △ Less","19 June, 2020",https://arxiv.org/pdf/2007.06712
"A comparative study of forecasting Corporate Credit Ratings using Neural Networks, Support Vector Machines, and Decision Trees",Parisa Golbayani;Ionuţ Florescu;Rupak Chatterjee,"Credit ratings are one of the primary keys that reflect the level of riskiness and reliability of corporations to meet their financial obligations. Rating agencies tend to take extended periods of time to provide new ratings and update older ones. Therefore, credit scoring assessments using artificial intelligence has gained a lot of interest in recent years. Successful machine learning methods can provide rapid analysis of credit scores while updating older ones on a daily time scale. Related studies have shown that neural networks and support vector machines outperform other techniques by providing better prediction accuracy. The purpose of this paper is two fold. First, we provide a survey and a comparative analysis of results from literature applying machine learning techniques to predict credit rating. Second, we apply ourselves four machine learning techniques deemed useful from previous studies (Bagged Decision Trees, Random Forest, Support Vector Machine and Multilayer Perceptron) to the same datasets. We evaluate the results using a 10-fold cross validation technique. The results of the experiment for the datasets chosen show superior performance for decision tree based models. In addition to the conventional accuracy measure of classifiers, we introduce a measure of accuracy based on notches called ""Notch Distance"" to analyze the performance of the above classifiers in the specific context of credit rating. This measure tells us how far the predictions are from the true ratings. We further compare the performance of three major rating agencies, Standard \& Poors, Moody's and Fitch where we show that the difference in their ratings is comparable with the decision tree prediction versus the actual rating on the test dataset. △ Less","13 July, 2020",https://arxiv.org/pdf/2007.06617
Smart technology in the classroom: a systematic review.Prospects for algorithmic accountability,Arian Garshi;Malin Wist Jakobsen;Jørgen Nyborg-Christensen;Daniel Ostnes;Maria Ovchinnikova,"Artificial intelligence (AI) algorithms have emerged in the educational domain as a tool to make learning more efficient. Different applications for mastering particular skills, learning new languages, and tracking their progress are used by children. What is the impact on children from using this smart technology? We conducted a systematic review to understand the state of the art. We explored the literature in several sub-disciplines: wearables, child psychology, AI and education, school surveillance, and accountability. Our review identified the need for more research for each established topic. We managed to find both positive and negative effects of using wearables, but cannot conclude if smart technology use leads to lowering the young children's performance. Based on our insights we propose a framework to effectively identify accountability for smart technology in education. △ Less","13 July, 2020",https://arxiv.org/pdf/2007.06374
Data-driven geophysics: from dictionary learning to deep learning,Siwei Yu;Jianwei Ma,"Understanding the principles of geophysical phenomena is an essential and challenging task. ""Model-driven"" approaches have supported the development of geophysics for a long time; however, such methods suffer from the curse of dimensionality and may inaccurately model the subsurface. ""Data-driven"" techniques may overcome these issues with increasingly available geophysical data. In this article, we review the basic concepts of and recent advances in data-driven approaches from dictionary learning to deep learning in a variety of geophysical scenarios. Explorational geophysics including data processing, inversion and interpretation will be mainly focused. Artificial intelligence applications on geoscience involving deep Earth, earthquake, water resource, atmospheric science, satellite remoe sensing and space sciences are also reviewed. We present a coding tutorial and a summary of tips for beginners and interested geophysical readers to rapidly explore deep learning. Some promising directions are provided for future research involving deep learning in geophysics, such as unsupervised learning, transfer learning, multimodal deep learning, federated learning, uncertainty estimation, and activate learning. △ Less","29 September, 2020",https://arxiv.org/pdf/2007.06183
Locality Guided Neural Networks for Explainable Artificial Intelligence,Randy Tan;Naimul Khan;Ling Guan,"In current deep network architectures, deeper layers in networks tend to contain hundreds of independent neurons which makes it hard for humans to understand how they interact with each other. By organizing the neurons by correlation, humans can observe how clusters of neighbouring neurons interact with each other. In this paper, we propose a novel algorithm for back propagation, called Locality Guided Neural Network(LGNN) for training networks that preserves locality between neighbouring neurons within each layer of a deep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to enforce a local topology on each layer of a deep network such that neighbouring neurons are highly correlated with each other. This method contributes to the domain of Explainable Artificial Intelligence (XAI), which aims to alleviate the black-box nature of current AI methods and make them understandable by humans. Our method aims to achieve XAI in deep learning without changing the structure of current models nor requiring any post processing. This paper focuses on Convolutional Neural Networks (CNNs), but can theoretically be applied to any type of deep learning architecture. In our experiments, we train various VGG and Wide ResNet (WRN) networks for image classification on CIFAR100. In depth analyses presenting both qualitative and quantitative results demonstrate that our method is capable of enforcing a topology on each layer while achieving a small increase in classification accuracy △ Less","12 July, 2020",https://arxiv.org/pdf/2007.06131
Relational-Grid-World: A Novel Relational Reasoning Environment and An Agent Model for Relational Information Extraction,Faruk Kucuksubasi;Elif Surer,"Reinforcement learning (RL) agents are often designed specifically for a particular problem and they generally have uninterpretable working processes. Statistical methods-based agent algorithms can be improved in terms of generalizability and interpretability using symbolic Artificial Intelligence (AI) tools such as logic programming. In this study, we present a model-free RL architecture that is supported with explicit relational representations of the environmental objects. For the first time, we use the PrediNet network architecture in a dynamic decision-making problem rather than image-based tasks, and Multi-Head Dot-Product Attention Network (MHDPA) as a baseline for performance comparisons. We tested two networks in two environments ---i.e., the baseline Box-World environment and our novel environment, Relational-Grid-World (RGW). With the procedurally generated RGW environment, which is complex in terms of visual perceptions and combinatorial selections, it is easy to measure the relational representation performance of the RL agents. The experiments were carried out using different configurations of the environment so that the presented module and the environment were compared with the baselines. We reached similar policy optimization performance results with the PrediNet architecture and MHDPA; additionally, we achieved to extract the propositional representation explicitly ---which makes the agent's statistical policy logic more interpretable and tractable. This flexibility in the agent's policy provides convenience for designing non-task-specific agent architectures. The main contributions of this study are two-fold ---an RL agent that can explicitly perform relational reasoning, and a new environment that measures the relational reasoning capabilities of RL agents. △ Less","12 July, 2020",https://arxiv.org/pdf/2007.05961
The Future of Work Is Here: Toward a Comprehensive Approach to Artificial Intelligence and Labour,Julian Posada,"This commentary traces contemporary discourses on the relationship between artificial intelligence and labour and explains why these principles must be comprehensive in their approach to labour and AI. First, the commentary asserts that ethical frameworks in AI alone are not enough to guarantee workers' rights since they lack enforcement mechanisms and the representation of different stakeholders. Secondly, it argues that current discussions on AI and labour focus on the deployment of these technologies in the workplace but ignore the essential role of human labour in their development, particularly in the different cases of outsourced labour around the world. Finally, it recommends using existing human rights frameworks for working conditions to provide more comprehensive ethical principles and regulations. The commentary concludes by arguing that the central question regarding the future of work should not be whether intelligent machines will replace humans, but who will own these systems and have a say in their development and operation. △ Less","15 July, 2020",https://arxiv.org/pdf/2007.05843
Lightweight Modules for Efficient Deep Learning based Image Restoration,Avisek Lahiri;Sourav Bairagya;Sutanu Bera;Siddhant Haldar;Prabir Kumar Biswas,"Low level image restoration is an integral component of modern artificial intelligence (AI) driven camera pipelines. Most of these frameworks are based on deep neural networks which present a massive computational overhead on resource constrained platform like a mobile phone. In this paper, we propose several lightweight low-level modules which can be used to create a computationally low cost variant of a given baseline model. Recent works for efficient neural networks design have mainly focused on classification. However, low-level image processing falls under the image-to-image' translation genre which requires some additional computational modules not present in classification. This paper seeks to bridge this gap by designing generic efficient modules which can replace essential components used in contemporary deep learning based image restoration networks. We also present and analyse our results highlighting the drawbacks of applying depthwise separable convolutional kernel (a popular method for efficient classification network) for sub-pixel convolution based upsampling (a popular upsampling strategy for low-level vision applications). This shows that concepts from domain of classification cannot always be seamlessly integrated into image-to-image translation tasks. We extensively validate our findings on three popular tasks of image inpainting, denoising and super-resolution. Our results show that proposed networks consistently output visually similar reconstructions compared to full capacity baselines with significant reduction of parameters, memory footprint and execution speeds on contemporary mobile devices. △ Less","11 July, 2020",https://arxiv.org/pdf/2007.05835
Enhanced Behavioral Cloning Based self-driving Car Using Transfer Learning,Uppala Sumanth;Narinder Singh Punn;Sanjay Kumar Sonbhadra;Sonali Agarwal,"With the growing phase of artificial intelligence and autonomous learning, the self-driving car is one of the promising area of research and emerging as a center of focus for automobile industries. Behavioral cloning is the process of replicating human behavior via visuomotor policies by means of machine learning algorithms. In recent years, several deep learning-based behavioral cloning approaches have been developed in the context of self-driving cars specifically based on the concept of transfer learning. Concerning the same, the present paper proposes a transfer learning approach using VGG16 architecture, which is fine tuned by retraining the last block while keeping other blocks as non-trainable. The performance of proposed architecture is further compared with existing NVIDIA architecture and its pruned variants (pruned by 22.2% and 33.85% using 1x1 filter to decrease the total number of parameters). Experimental results show that the VGG16 with transfer learning architecture has outperformed other discussed approaches with faster convergence. △ Less","11 July, 2020",https://arxiv.org/pdf/2007.05740
Neuromorphic Processing and Sensing: Evolutionary Progression of AI to Spiking,Philippe Reiter;Geet Rose Jose;Spyridon Bizmpikis;Ionela-Ancuţa Cîrjilă,"The increasing rise in machine learning and deep learning applications is requiring ever more computational resources to successfully meet the growing demands of an always-connected, automated world. Neuromorphic technologies based on Spiking Neural Network algorithms hold the promise to implement advanced artificial intelligence using a fraction of the computations and power requirements by modeling the functioning, and spiking, of the human brain. With the proliferation of tools and platforms aiding data scientists and machine learning engineers to develop the latest innovations in artificial and deep neural networks, a transition to a new paradigm will require building from the current well-established foundations. This paper explains the theoretical workings of neuromorphic technologies based on spikes, and overviews the state-of-art in hardware processors, software platforms and neuromorphic sensing devices. A progression path is paved for current machine learning specialists to update their skillset, as well as classification or predictive models from the current generation of deep neural networks to SNNs. This can be achieved by leveraging existing, specialized hardware in the form of SpiNNaker and the Nengo migration toolkit. First-hand, experimental results of converting a VGG-16 neural network to an SNN are shared. A forward gaze into industrial, medical and commercial applications that can readily benefit from SNNs wraps up this investigation into the neuromorphic computing future. △ Less","10 July, 2020",https://arxiv.org/pdf/2007.05606
Grading video interviews with fairness considerations,Abhishek Singhania;Abhishek Unnam;Varun Aggarwal,"There has been considerable interest in predicting human emotions and traits using facial images and videos. Lately, such work has come under criticism for poor labeling practices, inconclusive prediction results and fairness considerations. We present a careful methodology to automatically derive social skills of candidates based on their video response to interview questions. We, for the first time, include video data from multiple countries encompassing multiple ethnicities. Also, the videos were rated by individuals from multiple racial backgrounds, following several best practices, to achieve a consensus and unbiased measure of social skills. We develop two machine-learning models to predict social skills. The first model employs expert-guidance to use plausibly causal features. The second uses deep learning and depends solely on the empirical correlations present in the data. We compare errors of both these models, study the specificity of the models and make recommendations. We further analyze fairness by studying the errors of models by race and gender. We verify the usefulness of our models by determining how well they predict interview outcomes for candidates. Overall, the study provides strong support for using artificial intelligence for video interview scoring, while taking care of fairness and ethical considerations. △ Less","2 July, 2020",https://arxiv.org/pdf/2007.05461
AGI Agent Safety by Iteratively Improving the Utility Function,Koen Holtman,"While it is still unclear if agents with Artificial General Intelligence (AGI) could ever be built, we can already use mathematical models to investigate potential safety systems for these agents. We present an AGI safety layer that creates a special dedicated input terminal to support the iterative improvement of an AGI agent's utility function. The humans who switched on the agent can use this terminal to close any loopholes that are discovered in the utility function's encoding of agent goals and constraints, to direct the agent towards new goals, or to force the agent to switch itself off. An AGI agent may develop the emergent incentive to manipulate the above utility function improvement process, for example by deceiving, restraining, or even attacking the humans involved. The safety layer will partially, and sometimes fully, suppress this dangerous incentive. The first part of this paper generalizes earlier work on AGI emergency stop buttons. We aim to make the mathematical methods used to construct the layer more accessible, by applying them to an MDP model. We discuss two provable properties of the safety layer, and show ongoing work in mapping it to a Causal Influence Diagram (CID). In the second part, we develop full mathematical proofs, and show that the safety layer creates a type of bureaucratic blindness. We then present the design of a learning agent, a design that wraps the safety layer around either a known machine learning system, or a potential future AGI-level learning system. The resulting agent will satisfy the provable safety properties from the moment it is first switched on. Finally, we show how this agent can be mapped from its model to a real-life implementation. We review the methodological issues involved in this step, and discuss how these are typically resolved. △ Less","10 July, 2020",https://arxiv.org/pdf/2007.05411
A Survey on Autonomous Vehicle Control in the Era of Mixed-Autonomy: From Physics-Based to AI-Guided Driving Policy Learning,Xuan Di;Rongye Shi,"This paper serves as an introduction and overview of the potentially useful models and methodologies from artificial intelligence (AI) into the field of transportation engineering for autonomous vehicle (AV) control in the era of mixed autonomy. We will discuss state-of-the-art applications of AI-guided methods, identify opportunities and obstacles, raise open questions, and help suggest the building blocks and areas where AI could play a role in mixed autonomy. We divide the stage of autonomous vehicle (AV) deployment into four phases: the pure HVs, the HV-dominated, the AVdominated, and the pure AVs. This paper is primarily focused on the latter three phases. It is the first-of-its-kind survey paper to comprehensively review literature in both transportation engineering and AI for mixed traffic modeling. Models used for each phase are summarized, encompassing game theory, deep (reinforcement) learning, and imitation learning. While reviewing the methodologies, we primarily focus on the following research questions: (1) What scalable driving policies are to control a large number of AVs in mixed traffic comprised of human drivers and uncontrollable AVs? (2) How do we estimate human driver behaviors? (3) How should the driving behavior of uncontrollable AVs be modeled in the environment? (4) How are the interactions between human drivers and autonomous vehicles characterized? Hopefully this paper will not only inspire our transportation community to rethink the conventional models that are developed in the data-shortage era, but also reach out to other disciplines, in particular robotics and machine learning, to join forces towards creating a safe and efficient mixed traffic ecosystem. △ Less","10 July, 2020",https://arxiv.org/pdf/2007.05156
"Guru, Partner, or Pencil Sharpener? Understanding Designers' Attitudes Towards Intelligent Creativity Support Tools",Angus Main;Mick Grierson,"Creativity Support Tools (CST) aim to enhance human creativity, but the deeply personal and subjective nature of creativity makes the design of universal support tools challenging. Individuals develop personal approaches to creativity, particularly in the context of commercial design where signature styles and techniques are valuable commodities. Artificial Intelligence (AI) and Machine Learning (ML) techniques could provide a means of creating 'intelligent' CST which learn and adapt to personal styles of creativity. Identifying what kind of role such tools could play in the design process requires a better understanding of designers' attitudes towards working with AI, and their willingness to include it in their personal creative process. This paper details the results of a survey of professional designers which indicates a positive and pragmatic attitude towards collaborating with AI tools, and a particular opportunity for incorporating them in the research stages of a design project. △ Less","9 July, 2020",https://arxiv.org/pdf/2007.04848
Challenges of AI in Wireless Networks for IoT,Ijaz Ahmad;Shahriar Shahabuddin;Tanesh Kumar;Erkki Harjula;Marcus Meisel;Markku Juntti;Thilo Sauter;Mika Ylianttila,"The Internet of Things (IoT), hailed as the enabler of the next industrial revolution, will require ubiquitous connectivity, context-aware and dynamic service mobility, and extreme security through the wireless network infrastructure. Artificial Intelligence (AI), thus, will play a major role in the underlying network infrastructure. However, a number of challenges will surface while using the concepts, tools and algorithms of AI in wireless networks used by IoT. In this article, the main challenges in using AI in the wireless network infrastructure that facilitate end-to-end IoT communication are highlighted with potential generalized solution and future research directions. △ Less","9 July, 2020",https://arxiv.org/pdf/2007.04705
Building Robust Industrial Applicable Object Detection Models Using Transfer Learning and Single Pass Deep Learning Architectures,Steven Puttemans;Timothy Callemein;Toon Goedemé,"The uprising trend of deep learning in computer vision and artificial intelligence can simply not be ignored. On the most diverse tasks, from recognition and detection to segmentation, deep learning is able to obtain state-of-the-art results, reaching top notch performance. In this paper we explore how deep convolutional neural networks dedicated to the task of object detection can improve our industrial-oriented object detection pipelines, using state-of-the-art open source deep learning frameworks, like Darknet. By using a deep learning architecture that integrates region proposals, classification and probability estimation in a single run, we aim at obtaining real-time performance. We focus on reducing the needed amount of training data drastically by exploring transfer learning, while still maintaining a high average precision. Furthermore we apply these algorithms to two industrially relevant applications, one being the detection of promotion boards in eye tracking data and the other detecting and recognizing packages of warehouse products for augmented advertisements. △ Less","9 July, 2020",https://arxiv.org/pdf/2007.04666
"Artificial Intelligence and Machine Learning in 5G Network Security: Opportunities, advantages, and future research trends",Noman Haider;Muhammad Zeeshan Baig;Muhammad Imran,"Recent technological and architectural advancements in 5G networks have proven their worth as the deployment has started over the world. Key performance elevating factor from access to core network are softwareization, cloudification and virtualization of key enabling network functions. Along with the rapid evolution comes the risks, threats and vulnerabilities in the system for those who plan to exploit it. Therefore, ensuring fool proof end-to-end (E2E) security becomes a vital concern. Artificial intelligence (AI) and machine learning (ML) can play vital role in design, modelling and automation of efficient security protocols against diverse and wide range of threats. AI and ML has already proven their effectiveness in different fields for classification, identification and automation with higher accuracy. As 5G networks' primary selling point has been higher data rates and speed, it will be difficult to tackle wide range of threats from different points using typical/traditional protective measures. Therefore, AI and ML can play central role in protecting highly data-driven softwareized and virtualized network components. This article presents AI and ML driven applications for 5G network security, their implications and possible research directions. Also, an overview of key data collection points in 5G architecture for threat classification and anomaly detection are discussed. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.04490
Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence,Shakir Mohamed;Marie-Therese Png;William Isaac,"This paper explores the important role of critical science, and in particular of post-colonial and decolonial theories, in understanding and shaping the ongoing advances in artificial intelligence. Artificial Intelligence (AI) is viewed as amongst the technological advances that will reshape modern societies and their relations. Whilst the design and deployment of systems that continually adapt holds the promise of far-reaching positive change, they simultaneously pose significant risks, especially to already vulnerable peoples. Values and power are central to this discussion. Decolonial theories use historical hindsight to explain patterns of power that shape our intellectual, political, economic, and social world. By embedding a decolonial critical approach within its technical practice, AI communities can develop foresight and tactics that can better align research and technology development with established ethical principles, centring vulnerable peoples who continue to bear the brunt of negative impacts of innovation and scientific progress. We highlight problematic applications that are instances of coloniality, and using a decolonial lens, submit three tactics that can form a decolonial field of artificial intelligence: creating a critical technical practice of AI, seeking reverse tutelage and reverse pedagogies, and the renewal of affective and political communities. The years ahead will usher in a wave of new scientific breakthroughs and technologies driven by AI research, making it incumbent upon AI communities to strengthen the social contract through ethical foresight and the multiplicity of intellectual perspectives available to us; ultimately supporting future technologies that enable greater well-being, with the goal of beneficence and justice for all. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.04068
Deep Reinforcement Learning and its Neuroscientific Implications,Matthew Botvinick;Jane X. Wang;Will Dabney;Kevin J. Miller;Zeb Kurth-Nelson,"The emergence of powerful artificial intelligence is defining new research directions in neuroscience. To date, this research has focused largely on deep neural networks trained using supervised learning, in tasks such as image classification. However, there is another area of recent AI work which has so far received less attention from neuroscientists, but which may have profound neuroscientific implications: deep reinforcement learning. Deep RL offers a comprehensive framework for studying the interplay among learning, representation and decision-making, offering to the brain sciences a new set of research tools and a wide range of novel hypotheses. In the present review, we provide a high-level introduction to deep RL, discuss some of its initial applications to neuroscience, and survey its wider implications for research on brain and behavior, concluding with a list of opportunities for next-stage research. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.03750
Resonator networks for factoring distributed representations of data structures,E. Paxon Frady;Spencer Kent;Bruno A. Olshausen;Friedrich T. Sommer,"The ability to encode and manipulate data structures with distributed neural representations could qualitatively enhance the capabilities of traditional neural networks by supporting rule-based symbolic reasoning, a central property of cognition. Here we show how this may be accomplished within the framework of Vector Symbolic Architectures (VSA) (Plate, 1991; Gayler, 1998; Kanerva, 1996), whereby data structures are encoded by combining high-dimensional vectors with operations that together form an algebra on the space of distributed representations. In particular, we propose an efficient solution to a hard combinatorial search problem that arises when decoding elements of a VSA data structure: the factorization of products of multiple code vectors. Our proposed algorithm, called a resonator network, is a new type of recurrent neural network that interleaves VSA multiplication operations and pattern completion. We show in two examples -- parsing of a tree-like data structure and parsing of a visual scene -- how the factorization problem arises and how the resonator network can solve it. More broadly, resonator networks open the possibility to apply VSAs to myriad artificial intelligence problems in real-world domains. A companion paper (Kent et al., 2020) presents a rigorous analysis and evaluation of the performance of resonator networks, showing it out-performs alternative approaches. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.03748
DRIVE: A Digital Network Oracle for Cooperative Intelligent Transportation Systems,Ioannis Mavromatis;Robert J. Piechocki;Mahesh Sooriyabandara;Arjun Parekh,"In a world where Artificial Intelligence revolutionizes inference, prediction and decision-making tasks, Digital Twins emerge as game-changing tools. A case in point is the development and optimization of Cooperative Intelligent Transportation Systems (C-ITSs): a confluence of cyber-physical digital infrastructure and (semi)automated mobility. Herein we introduce Digital Twin for self-dRiving Intelligent VEhicles (DRIVE). The developed framework tackles shortcomings of traditional vehicular and network simulators. It provides a flexible, modular, and scalable implementation to ensure large-scale, city-wide experimentation with a moderate computational cost. The defining feature of our Digital Twin is a unique architecture allowing for submission of sequential queries, to which the Digital Twin provides instantaneous responses with the ""state of the world"", and hence is an Oracle. With such bidirectional interaction with external intelligent agents and realistic mobility traces, DRIVE provides the environment for development, training and optimization of Machine Learning based C-ITS solutions. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.03680
Artificial Stupidity,Michael Falk,"Public debate about AI is dominated by Frankenstein Syndrome, the fear that AI will become superhuman and escape human control. Although superintelligence is certainly a possibility, the interest it excites can distract the public from a more imminent concern: the rise of Artificial Stupidity (AS). This article discusses the roots of Frankenstein Syndrome in Mary Shelley's famous novel of 1818. It then provides a philosophical framework for analysing the stupidity of artificial agents, demonstrating that modern intelligent systems can be seen to suffer from 'stupidity of judgement'. Finally it identifies an alternative literary tradition that exposes the perils and benefits of AS. In the writings of Edmund Spenser, Jonathan Swift and E.T.A. Hoffmann, ASs replace, oppress or seduce their human users. More optimistically, Joseph Furphy and Laurence Sterne imagine ASs that can serve human intellect as maps or as pipes. These writers provide a strong counternarrative to the myths that currently drive the AI debate. They identify ways in which even stupid artificial agents can evade human control, for instance by appealing to stereotypes or distancing us from reality. And they underscore the continuing importance of the literary imagination in an increasingly automated society. △ Less","1 July, 2020",https://arxiv.org/pdf/2007.03616
Using Semantic Web Services for AI-Based Research in Industry 4.0,Lukas Malburg;Patrick Klein;Ralph Bergmann,"The transition to Industry 4.0 requires smart manufacturing systems that are easily configurable and provide a high level of flexibility during manufacturing in order to achieve mass customization or to support cloud manufacturing. To realize this, Cyber-Physical Systems (CPSs) combined with Artificial Intelligence (AI) methods find their way into manufacturing shop floors. For using AI methods in the context of Industry 4.0, semantic web services are indispensable to provide a reasonable abstraction of the underlying manufacturing capabilities. In this paper, we present semantic web services for AI-based research in Industry 4.0. Therefore, we developed more than 300 semantic web services for a physical simulation factory based on Web Ontology Language for Web Services (OWL-S) and Web Service Modeling Ontology (WSMO) and linked them to an already existing domain ontology for intelligent manufacturing control. Suitable for the requirements of CPS environments, our pre- and postconditions are verified in near real-time by invoking other semantic web services in contrast to complex reasoning within the knowledge base. Finally, we evaluate our implementation by executing a cyber-physical workflow composed of semantic web services using a workflow management system. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.03580
A Vision-based Social Distancing and Critical Density Detection System for COVID-19,Dongfang Yang;Ekim Yurtsever;Vishnu Renganathan;Keith A. Redmill;Ümit Özgüner,"Social distancing has been proven as an effective measure against the spread of the infectious COronaVIrus Disease 2019 (COVID-19). However, individuals are not used to tracking the required 6-feet (2-meters) distance between themselves and their surroundings. An active surveillance system capable of detecting distances between individuals and warning them can slow down the spread of the deadly disease. Furthermore, measuring social density in a region of interest (ROI) and modulating inflow can decrease social distancing violation occurrence chance. On the other hand, recording data and labeling individuals who do not follow the measures will breach individuals' rights in free-societies. Here we propose an Artificial Intelligence (AI) based real-time social distancing detection and warning system considering four important ethical factors: (1) the system should never record/cache data, (2) the warnings should not target the individuals, (3) no human supervisor should be in the detection/warning loop, and (4) the code should be open-source and accessible to the public. Against this backdrop, we propose using a monocular camera and deep learning-based real-time object detectors to measure social distancing. If a violation is detected, a non-intrusive audio-visual warning signal is emitted without targeting the individual who breached the social distancing measure. Also, if the social density is over a critical value, the system sends a control signal to modulate inflow into the ROI. We tested the proposed method across real-world datasets to measure its generality and performance. The proposed method is ready for deployment, and our code is open-sourced. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.03578
3D Topology Transformation with Generative Adversarial Networks,Luca Stornaiuolo;Nima Dehmamy;Albert-László Barabási;Mauro Martino,"Generation and transformation of images and videos using artificial intelligence have flourished over the past few years. Yet, there are only a few works aiming to produce creative 3D shapes, such as sculptures. Here we show a novel 3D-to-3D topology transformation method using Generative Adversarial Networks (GAN). We use a modified pix2pix GAN, which we call Vox2Vox, to transform the volumetric style of a 3D object while retaining the original object shape. In particular, we show how to transform 3D models into two new volumetric topologies - the 3D Network and the Ghirigoro. We describe how to use our approach to construct customized 3D representations. We believe that the generated 3D shapes are novel and inspirational. Finally, we compare the results between our approach and a baseline algorithm that directly convert the 3D shapes, without using our GAN. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.03532
"RCModel, a Risk Chain Model for Risk Reduction in AI Services",Takashi Matsumoto;Arisa Ema,"With the increasing use of artificial intelligence (AI) services and products in recent years, issues related to their trustworthiness have emerged and AI service providers need to be prepared for various risks. In this policy recommendation, we propose a risk chain model (RCModel) that supports AI service providers in proper risk assessment and control. We hope that RCModel will contribute to the realization of trustworthy AI services. △ Less","7 July, 2020",https://arxiv.org/pdf/2007.03215
Partially Conditioned Generative Adversarial Networks,Francisco J. Ibarrola;Nishant Ravikumar;Alejandro F. Frangi,"Generative models are undoubtedly a hot topic in Artificial Intelligence, among which the most common type is Generative Adversarial Networks (GANs). These architectures let one synthesise artificial datasets by implicitly modelling the underlying probability distribution of a real-world training dataset. With the introduction of Conditional GANs and their variants, these methods were extended to generating samples conditioned on ancillary information available for each sample within the dataset. From a practical standpoint, however, one might desire to generate data conditioned on partial information. That is, only a subset of the ancillary conditioning variables might be of interest when synthesising data. In this work, we argue that standard Conditional GANs are not suitable for such a task and propose a new Adversarial Network architecture and training strategy to deal with the ensuing problems. Experiments illustrating the value of the proposed approach in digit and face image synthesis under partial conditioning information are presented, showing that the proposed method can effectively outperform the standard approach under these circumstances. △ Less","6 July, 2020",https://arxiv.org/pdf/2007.02845
A Modern Non-SQL Approach to Radiology-Centric Search Engine Design with Clinical Validation,Ningcheng Li;Guy Maresh;Maxwell Cretcher;Khashayar Farsad;Ramsey Al-Hakim;John Kaufman;Judy Gichoya,"Healthcare data is increasing in size at an unprecedented speed with much attention on big data analysis and Artificial Intelligence application for quality assurance, clinical training, severity triaging, and decision support. Radiology is well-suited for innovation given its intrinsically paired linguistic and visual data. Previous attempts to unlock this information goldmine were encumbered by heterogeneity of human language, proprietary search algorithms, and lack of medicine-specific search performance matrices. We present a de novo process of developing a document-based, secure, efficient, and accurate search engine in the context of Radiology. We assess our implementation of the search engine with comparison to pre-existing manually collected clinical databases used previously for clinical research projects in addition to computational performance benchmarks and survey feedback. By leveraging efficient database architecture, search capability, and clinical thinking, radiologists are at the forefront of harnessing the power of healthcare data. △ Less","4 July, 2020",https://arxiv.org/pdf/2007.02124
Deep learning for scene recognition from visual data: a survey,Alina Matei;Andreea Glavan;Estefania Talavera,"The use of deep learning techniques has exploded during the last few years, resulting in a direct contribution to the field of artificial intelligence. This work aims to be a review of the state-of-the-art in scene recognition with deep learning models from visual data. Scene recognition is still an emerging field in computer vision, which has been addressed from a single image and dynamic image perspective. We first give an overview of available datasets for image and video scene recognition. Later, we describe ensemble techniques introduced by research papers in the field. Finally, we give some remarks on our findings and discuss what we consider challenges in the field and future lines of research. This paper aims to be a future guide for model selection for the task of scene recognition. △ Less","3 July, 2020",https://arxiv.org/pdf/2007.01806
Am I Building a White Box Agent or Interpreting a Black Box Agent?,Tom Bewley,"The rule extraction literature contains the notion of a fidelity-accuracy dilemma: when building an interpretable model of a black box function, optimising for fidelity is likely to reduce performance on the underlying task, and vice versa. I reassert the relevance of this dilemma for the modern field of explainable artificial intelligence, and highlight how it is compounded when the black box is an agent interacting with a dynamic environment. I then discuss two independent research directions - building white box agents and interpreting black box agents - which are both coherent and worthy of attention, but must not be conflated by researchers embarking on projects in the domain of agent interpretability. △ Less","8 July, 2020",https://arxiv.org/pdf/2007.01187
Testing match-3 video games with Deep Reinforcement Learning,Nicholas Napolitano,"Testing a video game is a critical step for the production process and requires a great effort in terms of time and resources spent. Some software houses are trying to use the artificial intelligence to reduce the need of human resources using systems able to replace a human agent. We study the possibility to use the Deep Reinforcement Learning to automate the testing process in match-3 video games and suggest to approach the problem in the framework of a Dueling Deep Q-Network paradigm. We test this kind of network on the Jelly Juice game, a match-3 video game developed by the redBit Games. The network extracts the essential information from the game environment and infers the next move. We compare the results with the random player performance, finding that the network shows a highest success rate. The results are in most cases similar with those obtained by real users, and the network also succeeds in learning over time the different features that distinguish the game levels and adapts its strategy to the increasing difficulties. △ Less","24 November, 2020",https://arxiv.org/pdf/2007.01137
"Federated Learning and Differential Privacy: Software tools analysis, the Sherpa.ai FL framework and methodological guidelines for preserving data privacy",Nuria Rodríguez-Barroso;Goran Stipcich;Daniel Jiménez-López;José Antonio Ruiz-Millán;Eugenio Martínez-Cámara;Gerardo González-Seco;M. Victoria Luzón;Miguel Ángel Veganzones;Francisco Herrera,"The high demand of artificial intelligence services at the edges that also preserve data privacy has pushed the research on novel machine learning paradigms that fit those requirements. Federated learning has the ambition to protect data privacy through distributed learning methods that keep the data in their data silos. Likewise, differential privacy attains to improve the protection of data privacy by measuring the privacy loss in the communication among the elements of federated learning. The prospective matching of federated learning and differential privacy to the challenges of data privacy protection has caused the release of several software tools that support their functionalities, but they lack of the needed unified vision for those techniques, and a methodological workflow that support their use. Hence, we present the Sherpa.ai Federated Learning framework that is built upon an holistic view of federated learning and differential privacy. It results from the study of how to adapt the machine learning paradigm to federated learning, and the definition of methodological guidelines for developing artificial intelligence services based on federated learning and differential privacy. We show how to follow the methodological guidelines with the Sherpa.ai Federated Learning framework by means of a classification and a regression use cases. △ Less","6 October, 2020",https://arxiv.org/pdf/2007.00914
An encoder-decoder-based method for COVID-19 lung infection segmentation,Omar Elharrouss;Nandhini Subramanian;Somaya Al-Maadeed,"The novelty of the COVID-19 disease and the speed of spread has created a colossal chaos, impulse among researchers worldwide to exploit all the resources and capabilities to understand and analyze characteristics of the coronavirus in term of the ways it spreads and virus incubation time. For that, the existing medical features like CT and X-ray images are used. For example, CT-scan images can be used for the detection of lung infection. But the challenges of these features such as the quality of the image and infection characteristics limitate the effectiveness of these features. Using artificial intelligence (AI) tools and computer vision algorithms, the accuracy of detection can be more accurate and can help to overcome these issues. This paper proposes a multi-task deep-learning-based method for lung infection segmentation using CT-scan images. Our proposed method starts by segmenting the lung regions that can be infected. Then, segmenting the infections in these regions. Also, to perform a multi-class segmentation the proposed model is trained using the two-stream inputs. The multi-task learning used in this paper allows us to overcome shortage of labeled data. Also, the multi-input stream allows the model to do the learning on many features that can improve the results. To evaluate the proposed method, many features have been used. Also, from the experiments, the proposed method can segment lung infections with a high degree performance even with shortage of data and labeled images. In addition, comparing with the state-of-the-art method our method achieves good performance results. △ Less","4 July, 2020",https://arxiv.org/pdf/2007.00861
Drug discovery with explainable artificial intelligence,José Jiménez-Luna;Francesca Grisoni;Gisbert Schneider,"Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for 'explainable' deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and dares a forecast of the future opportunities, potential applications, and remaining challenges. △ Less","2 July, 2020",https://arxiv.org/pdf/2007.00523
Multifunctional Meta-Optic Systems: Inversely Designed with Artificial Intelligence,Dayu Zhu;Zhaocheng Liu;Lakshmi Raju;Andrew S. Kim;Wenshan Cai,"Flat optics foresees a new era of ultra-compact optical devices, where metasurfaces serve as the foundation. Conventional designs of metasurfaces start with a certain structure as the prototype, followed by an extensive parametric sweep to accommodate the requirements of phase and amplitude of the emerging light. Regardless of how computation-consuming the process is, a predefined structure can hardly realize the independent control over the polarization, frequency, and spatial channels, which hinders the potential of metasurfaces to be multifunctional. Besides, achieving complicated and multiple functions calls for designing a meta-optic system with multiple cascading layers of metasurfaces, which introduces super exponential complexity. In this work we present an artificial intelligence framework for designing multilayer meta-optic systems with multifunctional capabilities. We demonstrate examples of a polarization-multiplexed dual-functional beam generator, a second order differentiator for all-optical computation, and a space-polarization-wavelength multiplexed hologram. These examples are barely achievable by single-layer metasurfaces and unattainable by traditional design processes. △ Less","30 June, 2020",https://arxiv.org/pdf/2007.00130
Turbulence on the Global Economy influenced by Artificial Intelligence and Foreign Policy Inefficiencies,Kwadwo Osei Bonsu;Jie Song,"It is said that Data and Information are the new oil. One, who handles the data, handles the emerging future of the global economy. Complex algorithms and intelligence-based filter programs are utilized to manage, store, handle and maneuver vast amounts of data for the fulfillment of specific purposes. This paper seeks to find the bridge between artificial intelligence and its impact on the international policy implementation in the light of geopolitical influence, global economy and the future of labor markets. We hypothesize that the distortion in the labor markets caused by artificial intelligence can be mitigated by a collaborative international foreign policy on the deployment of AI in the industrial circles. We, in this paper, then proceed to propose a disposition for the essentials of AI-based foreign policy and implementation, while asking questions such as 'could AI become the real Invisible Hand discussed by economists?'. △ Less","19 June, 2020",https://arxiv.org/pdf/2006.16911
Beyond accuracy: quantifying trial-by-trial behaviour of CNNs and humans by measuring error consistency,Robert Geirhos;Kristof Meding;Felix A. Wichmann,"A central problem in cognitive science and behavioural neuroscience as well as in machine learning and artificial intelligence research is to ascertain whether two or more decision makers (be they brains or algorithms) use the same strategy. Accuracy alone cannot distinguish between strategies: two systems may achieve similar accuracy with very different strategies. The need to differentiate beyond accuracy is particularly pressing if two systems are near ceiling performance, like Convolutional Neural Networks (CNNs) and humans on visual object recognition. Here we introduce trial-by-trial error consistency, a quantitative analysis for measuring whether two decision making systems systematically make errors on the same inputs. Making consistent errors on a trial-by-trial basis is a necessary condition for similar processing strategies between decision makers. Our analysis is applicable to compare algorithms with algorithms, humans with humans, and algorithms with humans. When applying error consistency to object recognition we obtain three main findings: (1.) Irrespective of architecture, CNNs are remarkably consistent with one another. (2.) The consistency between CNNs and human observers, however, is little above what can be expected by chance alone -- indicating that humans and CNNs are likely implementing very different strategies. (3.) CORnet-S, a recurrent model termed the ""current best model of the primate ventral visual stream"", fails to capture essential characteristics of human behavioural data and behaves essentially like a standard purely feedforward ResNet-50 in our analysis. Taken together, error consistency analysis suggests that the strategies used by human and machine vision are still very different -- but we envision our general-purpose error consistency analysis to serve as a fruitful tool for quantifying future progress. △ Less","18 December, 2020",https://arxiv.org/pdf/2006.16736
Spiking Associative Memory for Spatio-Temporal Patterns,Simon Davidson;Stephen B. Furber;Oliver Rhodes,"Spike Timing Dependent Plasticity is form of learning that has been demonstrated in real cortical tissue, but attempts to use it for artificial systems have not produced good results. This paper seeks to remedy this with two significant advances. The first is the development a simple stochastic learning rule called cyclic STDP that can extract patterns encoded in the precise spiking times of a group of neurons. We show that a population of neurons endowed with this learning rule can act as an effective short-term associative memory, storing and reliably recalling a large set of pattern associations over an extended period of time. The second major theme examines the challenges associated with training a neuron to produce a spike at a precise time and for the fidelity of spike recall time to be maintained as further learning occurs. The strong constraint of working with precisely-timed spikes (so-called temporal coding) is mandated by the learning rule but is also consistent with the believe in the necessity of such an encoding scheme to render a spiking neural network a competitive solution for flexible intelligent systems in continuous learning environments. The encoding and learning rules are demonstrated in the design of a single-layer associative memory (an input layer consisting of 3,200 spiking neurons fully-connected to a similar sized population of memory neurons), which we simulate and characterise. Design considerations and clarification of the role of parameters under the control of the designer are explored. △ Less","30 June, 2020",https://arxiv.org/pdf/2006.16684
COVID-19 Screening Using Residual Attention Network an Artificial Intelligence Approach,Vishal Sharma;Curtis Dyreson,"Coronavirus Disease 2019 (COVID-19) is caused by severe acute respiratory syndrome coronavirus 2 virus (SARS-CoV-2). The virus transmits rapidly; it has a basic reproductive number R of 2.2-2.7. In March 2020, the World Health Organization declared the COVID-19 outbreak a pandemic. COVID-19 is currently affecting more than 200 countries with 6M active cases. An effective testing strategy for COVID-19 is crucial to controlling the outbreak but the demand for testing surpasses the availability of test kits that use Reverse Transcription Polymerase Chain Reaction (RT-PCR). In this paper, we present a technique to screen for COVID-19 using artificial intelligence. Our technique takes only seconds to screen for the presence of the virus in a patient. We collected a dataset of chest X-ray images and trained several popular deep convolution neural network-based models (VGG, MobileNet, Xception, DenseNet, InceptionResNet) to classify the chest X-rays. Unsatisfied with these models, we then designed and built a Residual Attention Network that was able to screen COVID-19 with a testing accuracy of 98% and a validation accuracy of 100%. A feature maps visual of our model show areas in a chest X-ray which are important for classification. Our work can help to increase the adaptation of AI-assisted applications in clinical practice. The code and dataset used in this project are available at https://github.com/vishalshar/covid-19-screening-using-RAN-on-X-ray-images. △ Less","20 October, 2020",https://arxiv.org/pdf/2006.16106
Hybrid Tensor Decomposition in Neural Network Compression,Bijiao Wu;Dingheng Wang;Guangshe Zhao;Lei Deng;Guoqi Li,"Deep neural networks (DNNs) have enabled impressive breakthroughs in various artificial intelligence (AI) applications recently due to its capability of learning high-level features from big data. However, the current demand of DNNs for computational resources especially the storage consumption is growing due to that the increasing sizes of models are being required for more and more complicated applications. To address this problem, several tensor decomposition methods including tensor-train (TT) and tensor-ring (TR) have been applied to compress DNNs and shown considerable compression effectiveness. In this work, we introduce the hierarchical Tucker (HT), a classical but rarely-used tensor decomposition method, to investigate its capability in neural network compression. We convert the weight matrices and convolutional kernels to both HT and TT formats for comparative study, since the latter is the most widely used decomposition method and the variant of HT. We further theoretically and experimentally discover that the HT format has better performance on compressing weight matrices, while the TT format is more suited for compressing convolutional kernels. Based on this phenomenon we propose a strategy of hybrid tensor decomposition by combining TT and HT together to compress convolutional and fully connected parts separately and attain better accuracy than only using the TT or HT format on convolutional neural networks (CNNs). Our work illuminates the prospects of hybrid tensor decomposition for neural network compression. △ Less","20 September, 2020",https://arxiv.org/pdf/2006.15938
FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based IIoT Applications,Yunfei Song;Tian Liu;Tongquan Wei;Xiangfeng Wang;Zhe Tao;Mingsong Chen,"Along with the proliferation of Artificial Intelligence (AI) and Internet of Things (IoT) techniques, various kinds of adversarial attacks are increasingly emerging to fool Deep Neural Networks (DNNs) used by Industrial IoT (IIoT) applications. Due to biased training data or vulnerable underlying models, imperceptible modifications on inputs made by adversarial attacks may result in devastating consequences. Although existing methods are promising in defending such malicious attacks, most of them can only deal with limited existing attack types, which makes the deployment of large-scale IIoT devices a great challenge. To address this problem, we present an effective federated defense approach named FDA3 that can aggregate defense knowledge against adversarial examples from different sources. Inspired by federated learning, our proposed cloud-based architecture enables the sharing of defense capabilities against different attacks among IIoT devices. Comprehensive experimental results show that the generated DNNs by our approach can not only resist more malicious attacks than existing attack-specific adversarial training methods, but also can prevent IIoT applications from new attacks. △ Less","28 June, 2020",https://arxiv.org/pdf/2006.15632
Machine learning-based clinical prediction modeling -- A practical guide for clinicians,Julius M. Kernbach;Victor E. Staartjes,"In the emerging era of big data, larger available clinical datasets and computational advances have sparked a massive interest in machine learning-based approaches. The number of manuscripts related to machine learning or artificial intelligence has exponentially increased over the past years. As analytical machine learning tools become readily available for clinicians to use, the understanding of key concepts and the awareness of analytical pitfalls are increasingly required for clinicians, investigators, reviewers and editors, who even as experts in their clinical field, sometimes find themselves insufficiently equipped to evaluate machine learning methodologies. In the first section, we provide explanations on the general principles of machine learning, as well as analytical steps required for successful machine learning-based predictive modelling - which is the focus of this series. In further sections, we review the importance of resampling, overfitting and model generalizability as well as feature reduction and selection (Part II), strategies for model evaluation, reporting and discussion of common caveats and other points of significance (Part III), as well as offer a practical guide to classification (Part IV) and regression modelling (Part V), with a complete coding pipeline. Methodological rigor and clarity as well as understanding of the underlying reasoning of the internal workings of a machine learning approach are required, otherwise predictive applications despite being strong analytical tools are not well accepted into the clinical routine. Going forward, machine learning and artificial intelligence shape and influence modern medicine across disciplines including the field of neurosurgery. △ Less","23 June, 2020",https://arxiv.org/pdf/2006.15069
Could regulating the creators deliver trustworthy AI?,Labhaoise Ni Fhaolain;Andrew Hines,"Is a new regulated profession, such as Artificial Intelligence (AI) Architect who is responsible and accountable for AI outputs necessary to ensure trustworthy AI? AI is becoming all pervasive and is often deployed in everyday technologies, devices and services without our knowledge. There is heightened awareness of AI in recent years which has brought with it fear. This fear is compounded by the inability to point to a trustworthy source of AI, however even the term ""trustworthy AI"" itself is troublesome. Some consider trustworthy AI to be that which complies with relevant laws, while others point to the requirement to comply with ethics and standards (whether in addition to or in isolation of the law). This immediately raises questions of whose ethics and which standards should be applied and whether these are sufficient to produce trustworthy AI in any event. △ Less","25 June, 2020",https://arxiv.org/pdf/2006.14750
The State of AI Ethics Report (June 2020),Abhishek Gupta;Camylle Lanteigne;Victoria Heath;Marianna Bergamaschi Ganapini;Erick Galinkin;Allison Cohen;Tania De Gasperis;Mo Akif;Renjie Butalid,"These past few months have been especially challenging, and the deployment of technology in ways hitherto untested at an unrivalled pace has left the internet and technology watchers aghast. Artificial intelligence has become the byword for technological progress and is being used in everything from helping us combat the COVID-19 pandemic to nudging our attention in different directions as we all spend increasingly larger amounts of time online. It has never been more important that we keep a sharp eye out on the development of this field and how it is shaping our society and interactions with each other. With this inaugural edition of the State of AI Ethics we hope to bring forward the most important developments that caught our attention at the Montreal AI Ethics Institute this past quarter. Our goal is to help you navigate this ever-evolving field swiftly and allow you and your organization to make informed decisions. This pulse-check for the state of discourse, research, and development is geared towards researchers and practitioners alike who are making decisions on behalf of their organizations in considering the societal impacts of AI-enabled solutions. We cover a wide set of areas in this report spanning Agency and Responsibility, Security and Risk, Disinformation, Jobs and Labor, the Future of AI Ethics, and more. Our staff has worked tirelessly over the past quarter surfacing signal from the noise so that you are equipped with the right tools and knowledge to confidently tread this complex yet consequential domain. △ Less","25 June, 2020",https://arxiv.org/pdf/2006.14662
Ultra-Low-Power FDSOI Neural Circuits for Extreme-Edge Neuromorphic Intelligence,Arianna Rubino;Can Livanelioglu;Ning Qiao;Melika Payvand;Giacomo Indiveri,"Recent years have seen an increasing interest in the development of artificial intelligence circuits and systems for edge computing applications. In-memory computing mixed-signal neuromorphic architectures provide promising ultra-low-power solutions for edge-computing sensory-processing applications, thanks to their ability to emulate spiking neural networks in real-time. The fine-grain parallelism offered by this approach allows such neural circuits to process the sensory data efficiently by adapting their dynamics to the ones of the sensed signals, without having to resort to the time-multiplexed computing paradigm of von Neumann architectures. To reduce power consumption even further, we present a set of mixed-signal analog/digital circuits that exploit the features of advanced Fully-Depleted Silicon on Insulator (FDSOI) integration processes. Specifically, we explore the options of advanced FDSOI technologies to address analog design issues and optimize the design of the synapse integrator and of the adaptive neuron circuits accordingly. We present circuit simulation results and demonstrate the circuit's ability to produce biologically plausible neural dynamics with compact designs, optimized for the realization of large-scale spiking neural networks in neuromorphic processors. △ Less","14 July, 2020",https://arxiv.org/pdf/2006.14270
A Simple Approach to Case-Based Reasoning in Knowledge Bases,Rajarshi Das;Ameya Godbole;Shehzaad Dhuliawala;Manzil Zaheer;Andrew McCallum,"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires \emph{no training}, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Consider the task of finding a target entity given a source entity and a binary relation. Our non-parametric approach derives crisp logical rules for each query by finding multiple \textit{graph path patterns} that connect similar source entities through the given relation. Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches △ Less","18 July, 2020",https://arxiv.org/pdf/2006.14198
Consistency of Anchor-based Spectral Clustering,Henry-Louis de Kergorlay;Desmond John Higham,"Anchor-based techniques reduce the computational complexity of spectral clustering algorithms. Although empirical tests have shown promising results, there is currently a lack of theoretical support for the anchoring approach. We define a specific anchor-based algorithm and show that it is amenable to rigorous analysis, as well as being effective in practice. We establish the theoretical consistency of the method in an asymptotic setting where data is sampled from an underlying continuous probability distribution. In particular, we provide sharp asymptotic conditions for the algorithm parameters which ensure that the anchor-based method can recover with high probability disjoint clusters that are mutually separated by a positive distance. We illustrate the performance of the algorithm on synthetic data and explain how the theoretical convergence analysis can be used to inform the practical choice of parameter scalings. We also test the accuracy and efficiency of the algorithm on two large scale real data sets. We find that the algorithm offers clear advantages over standard spectral clustering. We also find that it is competitive with the state-of-the-art LSC method of Chen and Cai (Twenty-Fifth AAAI Conference on Artificial Intelligence, 2011), while having the added benefit of a consistency guarantee. △ Less","27 June, 2020",https://arxiv.org/pdf/2006.13984
Role of Edge Device and Cloud Machine Learning in Point-of-Care Solutions Using Imaging Diagnostics for Population Screening,Amit Kharat;Vinay Duddalwar;Krishna Saoji;Ashrika Gaikwad;Viraj Kulkarni;Gunjan Naik;Rohit Lokwani;Swaraj Kasliwal;Sudeep Kondal;Tanveer Gupte;Aniruddha Pant,"Edge devices are revolutionizing diagnostics. Edge devices can reside within or adjacent to imaging tools such as digital Xray, CT, MRI, or ultrasound equipment. These devices are either CPUs or GPUs with advanced processing deep and machine learning (artificial intelligence) algorithms that assist in classification and triage solutions to flag studies as either normal or abnormal, TB or healthy (in case of TB screening), suspected COVID-19/other pneumonia or unremarkable (in hospital or hotspot settings). These can be deployed as screening point-of-care (PoC) solutions; this is particularly true for digital and portable X-ray devices. Edge device learning can also be used for mammography and CT studies where it can identify microcalcification and stroke, respectively. These solutions can be considered the first line of pre-screening before the imaging specialist actually reviews scans and makes a final diagnosis. The key advantage of these tools is that they are instant, can be deployed remotely where experts are not available to perform pre-screening before the experts actually review, and are not limited by internet bandwidth as the nano learning data centers are placed next to the device. △ Less","18 June, 2020",https://arxiv.org/pdf/2006.13808
Using Deep Learning and Explainable Artificial Intelligence in Patients' Choices of Hospital Levels,Lichin Chen;Yu Tsao;Ji-Tian Sheu,"In countries that enabled patients to choose their own providers, a common problem is that the patients did not make rational decisions, and hence, fail to use healthcare resources efficiently. This might cause problems such as overwhelming tertiary facilities with mild condition patients, thus limiting their capacity of treating acute and critical patients. To address such maldistributed patient volume, it is essential to oversee patients choices before further evaluation of a policy or resource allocation. This study used nationwide insurance data, accumulated possible features discussed in existing literature, and used a deep neural network to predict the patients choices of hospital levels. This study also used explainable artificial intelligence methods to interpret the contribution of features for the general public and individuals. In addition, we explored the effectiveness of changing data representations. The results showed that the model was able to predict with high area under the receiver operating characteristics curve (AUC) (0.90), accuracy (0.90), sensitivity (0.94), and specificity (0.97) with highly imbalanced label. Generally, social approval of the provider by the general public (positive or negative) and the number of practicing physicians serving per ten thousand people of the located area are listed as the top effecting features. The changing data representation had a positive effect on the prediction improvement. Deep learning methods can process highly imbalanced data and achieve high accuracy. The effecting features affect the general public and individuals differently. Addressing the sparsity and discrete nature of insurance data leads to better prediction. Applications using deep learning technology are promising in health policy making. More work is required to interpret models and practice implementation. △ Less","23 June, 2020",https://arxiv.org/pdf/2006.13427
Performance Evaluation of Fuzzy Integrated Firewall Model for Hybrid Cloud based on Packet Utilization,Ziaur Rahman;Asma Islam Swapna;Habibur Rahman Habib;Akramuzzaman Shaoun,"Cloud computing is one of the highly flexible, confidential and easily accessible medium of platforms and provides powerful service for sharing information over the Internet. Cloud security has become an emerging issue as network manager eventually encounter its data protection, vulnerability during information exchange on the cloud system. We can protect our data from unwanted access on a hybrid cloud through controlling the respective firewall of the network. But, the firewall has already proved its weakness as it is unable to ensure multi-layered, secured accessibility of the cloud network. Efficient packet utilization sometimes causes high response time in accessing hybrid cloud. In this paper, a Cloud Model with Hybrid functionality and a secure Fuzzy Integrated Firewall for that Hybrid Cloud is proposed and thereby evaluated for the performance in traffic response. Experimental result illustrated that having a fuzzified firewall gives high point-to-point packet utilization decreasing the response time than a conventional firewall. Results from this research work will highly be implemented in transplanting artificial intelligence in future Internet of Things (IoT). △ Less","21 June, 2020",https://arxiv.org/pdf/2006.12736
Efficient Hyperparameter Optimization in Deep Learning Using a Variable Length Genetic Algorithm,Xueli Xiao;Ming Yan;Sunitha Basodi;Chunyan Ji;Yi Pan,"Convolutional Neural Networks (CNN) have gained great success in many artificial intelligence tasks. However, finding a good set of hyperparameters for a CNN remains a challenging task. It usually takes an expert with deep knowledge, and trials and errors. Genetic algorithms have been used in hyperparameter optimizations. However, traditional genetic algorithms with fixed-length chromosomes may not be a good fit for optimizing deep learning hyperparameters, because deep learning models have variable number of hyperparameters depending on the model depth. As the depth increases, the number of hyperparameters grows exponentially, and searching becomes exponentially harder. It is important to have an efficient algorithm that can find a good model in reasonable time. In this article, we propose to use a variable length genetic algorithm (GA) to systematically and automatically tune the hyperparameters of a CNN to improve its performance. Experimental results show that our algorithm can find good CNN hyperparameters efficiently. It is clear from our experiments that if more time is spent on optimizing the hyperparameters, better results could be achieved. Theoretically, if we had unlimited time and CPU power, we could find the optimized hyperparameters and achieve the best results in the future. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12703
Artificial Intelligence-Assisted Energy and Thermal Comfort Control for Sustainable Buildings: An Extended Representation of the Systematic Review,Ghezlane Halhoul Merabet;Mohamed Essaaidi;Mohamed Ben-Haddou;Basheer Qolomany;Junaid Qadir;Muhammad Anan;Ala Al-Fuqaha;Riduan Mohamed Abid;Driss Benhaddou,"Different factors such as thermal comfort, humidity, air quality, and noise have significant combined effects on the acceptability and quality of the activities performed by the building occupants who spend most of their times indoors. Among the factors cited, thermal comfort, which contributes to the human well-being because of its connection with the thermoregulation of the human body. Therefore, the creation of thermally comfortable and energy efficient environments is of great importance in the design of the buildings and hence the heating, ventilation and air-conditioning systems. Recent works have been directed towards more advanced control strategies, based mainly on artificial intelligence which has the ability to imitate human behavior. This systematic literature review aims to provide an overview of the intelligent control strategies inside building and to investigate their ability to balance thermal comfort and energy efficiency optimization in indoor environments. Methods. A systematic literature review examined the peer-reviewed research works using ACM Digital Library, Scopus, Google Scholar, IEEE Xplore (IEOL), Web of Science, and Science Direct (SDOL), besides other sources from manual search. With the following string terms: thermal comfort, comfort temperature, preferred temperature, intelligent control, advanced control, artificial intelligence, computational intelligence, building, indoors, and built environment. Inclusion criteria were: English, studies monitoring, mainly, human thermal comfort in buildings and energy efficiency simultaneously based on control strategies using the intelligent approaches. Preferred Reporting Items for Systematic Reviews and Meta-Analysis guidelines were used. Initially, 1,077 articles were yielded, and 120 ultimately met inclusion criteria and were reviewed. △ Less","4 August, 2020",https://arxiv.org/pdf/2006.12559
Fully-parallel Convolutional Neural Network Hardware,Christiam F. Frasser;Pablo Linares-Serrano;V. Canals;Miquel Roca;T. Serrano-Gotarredona;Josep L. Rossello,"A new trans-disciplinary knowledge area, Edge Artificial Intelligence or Edge Intelligence, is beginning to receive a tremendous amount of interest from the machine learning community due to the ever increasing popularization of the Internet of Things (IoT). Unfortunately, the incorporation of AI characteristics to edge computing devices presents the drawbacks of being power and area hungry for typical machine learning techniques such as Convolutional Neural Networks (CNN). In this work, we propose a new power-and-area-efficient architecture for implementing Articial Neural Networks (ANNs) in hardware, based on the exploitation of correlation phenomenon in Stochastic Computing (SC) systems. The architecture purposed can solve the difficult implementation challenges that SC presents for CNN applications, such as the high resources used in binary-tostochastic conversion, the inaccuracy produced by undesired correlation between signals, and the stochastic maximum function implementation. Compared with traditional binary logic implementations, experimental results showed an improvement of 19.6x and 6.3x in terms of speed performance and energy efficiency, for the FPGA implementation. We have also realized a full VLSI implementation of the proposed SC-CNN architecture demonstrating that our optimization achieve a 18x area reduction over previous SC-DNN architecture VLSI implementation in a comparable technological node. For the first time, a fully-parallel CNN as LENET-5 is embedded and tested in a single FPGA, showing the benefits of using stochastic computing for embedded applications, in contrast to traditional binary logic implementations. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12439
Game Theory on the Ground: The Effect of Increased Patrols on Deterring Poachers,Lily Xu;Andrew Perrault;Andrew Plumptre;Margaret Driciru;Fred Wanyama;Aggrey Rwetsiba;Milind Tambe,"Applications of artificial intelligence for wildlife protection have focused on learning models of poacher behavior based on historical patterns. However, poachers' behaviors are described not only by their historical preferences, but also their reaction to ranger patrols. Past work applying machine learning and game theory to combat poaching have hypothesized that ranger patrols deter poachers, but have been unable to find evidence to identify how or even if deterrence occurs. Here for the first time, we demonstrate a measurable deterrence effect on real-world poaching data. We show that increased patrols in one region deter poaching in the next timestep, but poachers then move to neighboring regions. Our findings offer guidance on how adversaries should be modeled in realistic game-theoretic settings. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12411
AI-Augmented Multi Function Radar Engineering with Digital Twin: Towards Proactivity,Mathieu Klein;Thomas Carpentier;Eric Jeanclaude;Rami Kassab;Konstantinos Varelas;Nico de Bruijn;Frédéric Barbaresco;Yann Briheche;Yann Semet;Florence Aligne,"Thales new generation digital multi-missions radars, fully-digital and software-defined, like the Sea Fire and Ground Fire radars, benefit from a considerable increase of accessible degrees of freedoms to optimally design their operational modes. To effectively leverage these design choices and turn them into operational capabilities, it is necessary to develop new engineering tools, using artificial intelligence. Innovative optimization algorithms in the discrete and continuous domains, coupled with a radar Digital Twins, allowed construction of a generic tool for ""search"" mode design (beam synthesis, waveform and volume grid) compliant with the available radar time budget. The high computation speeds of these algorithms suggest tool application in a ""Proactive Radar"" configuration, which would dynamically propose to the operator, operational modes better adapted to environment, threats and the equipment failure conditions. △ Less","18 June, 2020",https://arxiv.org/pdf/2006.12384
Artificial intelligence in space,George Anthony Gal;Cristiana Santos;Lucien Rapp;Réeka Markovich;Leendert van der Torre,"In the next coming years, space activities are expected to undergo a radical transformation with the emergence of new satellite systems or new services which will incorporate the contributions of artificial intelligence and machine learning defined as covering a wide range of innovations from autonomous objects with their own decision-making power to increasingly sophisticated services exploiting very large volumes of information from space. This chapter identifies some of the legal and ethical challenges linked to its use. These legal and ethical challenges call for solutions which the international treaties in force are not sufficient to determine and implement. For this reason, a legal methodology must be developed that makes it possible to link intelligent systems and services to a system of rules applicable thereto. It discusses existing legal AI-based tools amenable for making space law actionable, interoperable and machine readable for future compliance tools. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12362
"Always-On, Sub-300-nW, Event-Driven Spiking Neural Network based on Spike-Driven Clock-Generation and Clock- and Power-Gating for an Ultra-Low-Power Intelligent Device",Dewei Wang;Pavan Kumar Chundi;Sung Justin Kim;Minhao Yang;Joao Pedro Cerqueira;Joonsung Kang;Seungchul Jung;Sangjoon Kim;Mingoo Seok,"Always-on artificial intelligent (AI) functions such as keyword spotting (KWS) and visual wake-up tend to dominate total power consumption in ultra-low power devices. A key observation is that the signals to an always-on function are sparse in time, which a spiking neural network (SNN) classifier can leverage for power savings, because the switching activity and power consumption of SNNs tend to scale with spike rate. Toward this goal, we present a novel SNN classifier architecture for always-on functions, demonstrating sub-300nW power consumption at the competitive inference accuracy for a KWS and other always-on classification workloads. △ Less","23 June, 2020",https://arxiv.org/pdf/2006.12314
Approaches For Multi-View Redescription Mining,Matej Mihelčić;Tomislav Šmuc,"The task of redescription mining explores ways to re-describe different subsets of entities contained in a dataset and to reveal non-trivial associations between different subsets of attributes, called views. This interesting and challenging task is encountered in different scientific fields, and is addressed by a number of approaches that obtain redescriptions and allow for the exploration and analyses of attribute associations. The main limitation of existing approaches to this task is their inability to use more than two views. Our work alleviates this drawback. We present a memory efficient, extensible multi-view redescription mining framework that can be used to relate multiple, i.e. more than two views, disjoint sets of attributes describing one set of entities. The framework can use any multi-target regression or multi-label classification algorithm, with models that can be represented as sets of rules, to generate redescriptions. Multi-view redescriptions are built using incremental view-extending heuristic from initially created two-view redescriptions. In this work, we use different types of Predictive Clustering trees algorithms (regular, extra, with random output selection) and the Random Forest thereof in order to improve the quality of final redescription sets and/or execution time needed to generate them. We provide multiple performance analyses of the proposed framework and compare it against the naive approach to multi-view redescription mining. We demonstrate the usefulness of the proposed multi-view extension on several datasets, including a use-case on understanding of machine learning models - a topic of growing importance in machine learning and artificial intelligence in general. △ Less","17 November, 2020",https://arxiv.org/pdf/2006.12227
Potential customer mining application of smart home products based on LightGBM PU learning and Spark ML algorithm practice,Duan Zhihua;Wang JiaLin,"This paper studies the case of big data-based intelligent product potential customer mining internal competition in China Telecom Shanghai Company. Huge amounts of data based on big data table, the use of machine Learning and data analysis technology, using the algorithm of LightGBM, PySpark machine Learning algorithms, Positive Unlabeled Learning algorithm, and predict whether customers buy whole house product, precision marketing into artificial intelligence for the customer, large data capacity, promote the development of intelligent products of the company. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12191
Exploiting Non-Taxonomic Relations for Measuring Semantic Similarity and Relatedness in WordNet,Mohannad AlMousa;Rachid Benlamri;Richard Khoury,"Various applications in the areas of computational linguistics and artificial intelligence employ semantic similarity to solve challenging tasks, such as word sense disambiguation, text classification, information retrieval, machine translation, and document clustering. Previous work on semantic similarity followed a mono-relational approach using mostly the taxonomic relation ""ISA"". This paper explores the benefits of using all types of non-taxonomic relations in large linked data, such as WordNet knowledge graph, to enhance existing semantic similarity and relatedness measures. We propose a holistic poly-relational approach based on a new relation-based information content and non-taxonomic-based weighted paths to devise a comprehensive semantic similarity and relatedness measure. To demonstrate the benefits of exploiting non-taxonomic relations in a knowledge graph, we used three strategies to deploy non-taxonomic relations at different granularity levels. We conducted experiments on four well-known gold standard datasets, and the results demonstrated the robustness and scalability of the proposed semantic similarity and relatedness measure, which significantly improves existing similarity measures. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12106
Online Handbook of Argumentation for AI: Volume 1,OHAAI Collaboration;Federico Castagna;Timotheus Kampik;Atefeh Keshavarzi Zafarghandi;Mickaël Lafages;Jack Mumford;Christos T. Rodosthenous;Samy Sá;Stefan Sarkadi;Joseph Singleton;Kenneth Skiba;Andreas Xydis,"This volume contains revised versions of the papers selected for the first volume of the Online Handbook of Argumentation for AI (OHAAI). Previously, formal theories of argument and argument interaction have been proposed and studied, and this has led to the more recent study of computational models of argument. Argumentation, as a field within artificial intelligence (AI), is highly relevant for researchers interested in symbolic representations of knowledge and defeasible reasoning. The purpose of this handbook is to provide an open access and curated anthology for the argumentation research community. OHAAI is designed to serve as a research hub to keep track of the latest and upcoming PhD-driven research on the theory and application of argumentation in all areas related to AI. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.12020
Emergent cooperation through mutual information maximization,Santiago Cuervo;Marco Alzate,"With artificial intelligence systems becoming ubiquitous in our society, its designers will soon have to start to consider its social dimension, as many of these systems will have to interact among them to work efficiently. With this in mind, we propose a decentralized deep reinforcement learning algorithm for the design of cooperative multi-agent systems. The algorithm is based on the hypothesis that highly correlated actions are a feature of cooperative systems, and hence, we propose the insertion of an auxiliary objective of maximization of the mutual information between the actions of agents in the learning problem. Our system is applied to a social dilemma, a problem whose optimal solution requires that agents cooperate to maximize a macroscopic performance function despite the divergent individual objectives of each agent. By comparing the performance of the proposed system to a system without the auxiliary objective, we conclude that the maximization of mutual information among agents promotes the emergence of cooperation in social dilemmas. △ Less","21 June, 2020",https://arxiv.org/pdf/2006.11769
Access Control Management for Computer-Aided Diagnosis Systems using Blockchain,Mayra Samaniego;Sara Hosseinzadeh Kassani;Cristian Espana;Ralph Deters,"Computer-Aided Diagnosis (CAD) systems have emerged to support clinicians in interpreting medical images. CAD systems are traditionally combined with artificial intelligence (AI), computer vision, and data augmentation to evaluate suspicious structures in medical images. This evaluation generates vast amounts of data. Traditional CAD systems belong to a single institution and handle data access management centrally. However, the advent of CAD systems for research among multiple institutions demands distributed access management. This research proposes a blockchain-based solution to enable distributed data access management in CAD systems. This solution has been developed as a distributed application (DApp) using Ethereum in a consortium network. △ Less","20 June, 2020",https://arxiv.org/pdf/2006.11522
Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey,Arun Das;Paul Rad,"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation. △ Less","22 June, 2020",https://arxiv.org/pdf/2006.11371
Reconfigurable Intelligent Surfaces and Metamaterials: The Potential of Wave Propagation Control for 6G Wireless Communications,George C. Alexandropoulos;Geoffroy Lerosey;Merouane Debbah;Mathias Fink,"The future 6G of wireless communication networks will have to meet multiple requirements in increasingly demanding levels, either individually or in combinations in small groups. This trend has spurred recent research activities on transceiver hardware architectures and novel wireless connectivity concepts. Among the emerging wireless hardware architectures belong the Reconfigurable Intelligent Surfaces (RISs), which are artificial planar structures with integrated electronic circuits that can be programmed to manipulate an incoming ElectroMagnetic (EM) field in a wide variety of functionalities. Incorporating RISs in wireless networks has been recently advocated as a revolutionary means to transform any naturally passive wireless communication environment to an active one. This can be accomplished by deploying cost-effective and easy to coat RISs to the environment's objects (e.g., building facades and indoor walls/ceilings), thus, offering increased environmental intelligence for the scope of diverse wireless networking objectives. In this paper, we first provide a brief history on wave propagation control for optics and acoustics, and overview two representative indoor wireless trials at 2.47GHz for spatial EM modulation with a passive discrete RIS. The first trial dating back to 2014 showcases the feasibility of highly accurate spatiotemporal focusing and nulling, while the second very recent one demonstrates that passive RISs can enrich multipath scattering, thus, enabling throughput boosted communication links. Motivated by the late research excitement on the RIS potential for intelligent EM wave propagation modulation, we describe the status on RIS hardware architectures and present key open challenges and future research directions for RIS design and RIS-empowered 6G wireless communications. △ Less","19 June, 2020",https://arxiv.org/pdf/2006.11136
"Artificial Buildings: Safety, Complexity and a Quantifiable Measure of Beauty",Arash Mehrjou,"A place to live is one of the most crucial necessities for all living organisms since the advent of life on planet Earth. The nature of homes has changed considerably over time. At the very early stages, human begins lived in natural places such as caves. Later on, they started to use their intelligence to build places with special purposes. Nowadays, modern technologies such as robotics and artificial intelligence have made their ways into the construction process and opened up a whole new area of opportunities and concerns that may be of interest to both technologists and philosophers. In this article, I review the evolution of buildings from fully natural to fully artificial and discuss philosophical thoughts that a fully automated construction technology may raise. I elaborate on the safety concerns of a fully automated architectural process. Then, I'll borrow Kolmogorov complexity from algorithmic information theory to define a complexity measure for buildings. The proposed measure is then used to provide a quantifiable measure of beauty. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.11113
Pervasive Communications Technologies For Managing Pandemics,Muhammad Ilyas;Basit Qureshi,"Pandemics always have had serious consequences unless they were effectively contained. Recent experiences with COVID-19 show that by using a smart and swift approach to deal with pandemics, avoids overwhelming of healthcare systems, and reduces the loss of precious life. This paper is about using smart technologies such as Mobile Edge Clouds (MEC), Internet of Things (IoT), and Artificial Intelligence (AI), as an approach to effectively manage pandemics. IoT provides pervasive connectivity among various devices and can be used for collecting information such as location and symptoms of potentially infected individuals. MECs provide cloud services on the edge, integrating IoT infrastructure and execution of sophisticated AI algorithms in the Cloud. In this paper, we develop a prototype to demonstrate the convergence of pervasive technologies to support research in managing pandemics. Low-cost Single Board Computers (SBC) based clusters are integrated within MEC to support remote medical teams in the field. The prototype implements a lightweight Docker container orchestrated by Kubernetes eco-system which is deployed on the clusters. The prototype successfully demonstrates that mobile medical facilities can utilize the proposed solution to collect information and execute AI algorithms while on the go. Finally, we present a discussion on the role of converging pervasive technologies on managing pandemics. △ Less","18 June, 2020",https://arxiv.org/pdf/2006.10805
Genetic Programming visitation scheduling solution can deliver a less austere COVID-19 pandemic population lockdown,Daniel Howard,"A computational methodology is introduced to minimize infection opportunities for people suffering some degree of lockdown in response to a pandemic, as is the 2020 COVID-19 pandemic. Persons use their mobile phone or computational device to request trips to places of their need or interest indicating a rough time of day: `morning', `afternoon', `night' or `any time' when they would like to undertake these outings as well as the desired place to visit. An artificial intelligence methodology which is a variant of Genetic Programming studies all requests and responds with specific time allocations for such visits that minimize the overall risks of infection, hospitalization and death of people. A number of alternatives for this computation are presented and results of numerical experiments involving over 230 people of various ages and background health levels in over 1700 visits that take place over three consecutive days. A novel partial infection model is introduced to discuss these proof of concept solutions which are compared to round robin uninformed time scheduling for visits to places. The computations indicate vast improvements with far fewer dead and hospitalized. These auger well for a more realistic study using accurate infection models with the view to test deployment in the real world. The input that drives the infection model is the degree of infection by taxonomic class, such as the information that may arise from population testing for COVID-19 or, alternatively, any contamination model. The taxonomy class assumed in the computations is the likely level of infection by age group. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.10748
Artificial Musical Intelligence: A Survey,Elad Liebman;Peter Stone,"Computers have been used to analyze and create music since they were first introduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the Internet and large scale platforms for music recommendation and retrieval have made music an increasingly prevalent domain of machine learning and artificial intelligence research. While still nascent, several different approaches have been employed to tackle what may broadly be referred to as ""musical intelligence."" This article provides a definition of musical intelligence, introduces a taxonomy of its constituent components, and surveys the wide range of AI methods that can be, and have been, brought to bear in its pursuit, with a particular emphasis on machine learning methods. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.10553
LimeOut: An Ensemble Approach To Improve Process Fairness,Vaishnavi Bhargava;Miguel Couceiro;Amedeo Napoli,"Artificial Intelligence and Machine Learning are becoming increasingly present in several aspects of human life, especially, those dealing with decision making. Many of these algorithmic decisions are taken without human supervision and through decision making processes that are not transparent. This raises concerns regarding the potential bias of these processes towards certain groups of society, which may entail unfair results and, possibly, violations of human rights. Dealing with such biased models is one of the major concerns to maintain the public trust. In this paper, we address the question of process or procedural fairness. More precisely, we consider the problem of making classifiers fairer by reducing their dependence on sensitive features while increasing (or, at least, maintaining) their accuracy. To achieve both, we draw inspiration from ""dropout"" techniques in neural based approaches, and propose a framework that relies on ""feature drop-out"" to tackle process fairness. We make use of ""LIME Explanations"" to assess a classifier's fairness and to determine the sensitive features to remove. This produces a pool of classifiers (through feature dropout) whose ensemble is shown empirically to be less dependent on sensitive features, and with improved or no impact on accuracy. △ Less","17 June, 2020",https://arxiv.org/pdf/2006.10531
Privacy-Preserving Technology to Help Millions of People: Federated Prediction Model for Stroke Prevention,Ce Ju;Ruihui Zhao;Jichao Sun;Xiguang Wei;Bo Zhao;Yang Liu;Hongshan Li;Tianjian Chen;Xinwei Zhang;Dashan Gao;Ben Tan;Han Yu;Chuning He;Yuan Jin,"Prevention of stroke with its associated risk factors has been one of the public health priorities worldwide. Emerging artificial intelligence technology is being increasingly adopted to predict stroke. Because of privacy concerns, patient data are stored in distributed electronic health record (EHR) databases, voluminous clinical datasets, which prevent patient data from being aggregated and restrains AI technology to boost the accuracy of stroke prediction with centralized training data. In this work, our scientists and engineers propose a privacy-preserving scheme to predict the risk of stroke and deploy our federated prediction model on cloud servers. Our system of federated prediction model asynchronously supports any number of client connections and arbitrary local gradient iterations in each communication round. It adopts federated averaging during the model training process, without patient data being taken out of the hospitals during the whole process of model training and forecasting. With the privacy-preserving mechanism, our federated prediction model trains over all the healthcare data from hospitals in a certain city without actual data sharing among them. Therefore, it is not only secure but also more accurate than any single prediction model that trains over the data only from one single hospital. Especially for small hospitals with few confirmed stroke cases, our federated model boosts model performance by 10%~20% in several machine learning metrics. To help stroke experts comprehend the advantage of our prediction system more intuitively, we developed a mobile app that collects the key information of patients' statistics and demonstrates performance comparisons between the federated prediction model and the single prediction model during the federated training process. △ Less","14 December, 2020",https://arxiv.org/pdf/2006.10517
Automated Radiological Report Generation For Chest X-Rays With Weakly-Supervised End-to-End Deep Learning,Shuai Zhang;Xiaoyan Xin;Yang Wang;Yachong Guo;Qiuqiao Hao;Xianfeng Yang;Jun Wang;Jian Zhang;Bing Zhang;Wei Wang,"The chest X-Ray (CXR) is the one of the most common clinical exam used to diagnose thoracic diseases and abnormalities. The volume of CXR scans generated daily in hospitals is huge. Therefore, an automated diagnosis system able to save the effort of doctors is of great value. At present, the applications of artificial intelligence in CXR diagnosis usually use pattern recognition to classify the scans. However, such methods rely on labeled databases, which are costly and usually have large error rates. In this work, we built a database containing more than 12,000 CXR scans and radiological reports, and developed a model based on deep convolutional neural network and recurrent network with attention mechanism. The model learns features from the CXR scans and the associated raw radiological reports directly; no additional labeling of the scans are needed. The model provides automated recognition of given scans and generation of reports. The quality of the generated reports was evaluated with both the CIDEr scores and by radiologists as well. The CIDEr scores are found to be around 5.8 on average for the testing dataset. Further blind evaluation suggested a comparable performance against human radiologist. △ Less","18 June, 2020",https://arxiv.org/pdf/2006.10347
Delta Schema Network in Model-based Reinforcement Learning,Andrey Gorodetskiy;Alexandra Shlychkova;Aleksandr I. Panov,"This work is devoted to unresolved problems of Artificial General Intelligence - the inefficiency of transfer learning. One of the mechanisms that are used to solve this problem in the area of reinforcement learning is a model-based approach. In the paper we are expanding the schema networks method which allows to extract the logical relationships between objects and actions from the environment data. We present algorithms for training a Delta Schema Network (DSN), predicting future states of the environment and planning actions that will lead to positive reward. DSN shows strong performance of transfer learning on the classic Atari game environment. △ Less","8 July, 2020",https://arxiv.org/pdf/2006.09950
Quality Management of Machine Learning Systems,P. Santhanam,"In the past decade, Artificial Intelligence (AI) has become a part of our daily lives due to major advances in Machine Learning (ML) techniques. In spite of an explosive growth in the raw AI technology and in consumer facing applications on the internet, its adoption in business applications has conspicuously lagged behind. For business/mission-critical systems, serious concerns about reliability and maintainability of AI applications remain. Due to the statistical nature of the output, software 'defects' are not well defined. Consequently, many traditional quality management techniques such as program debugging, static code analysis, functional testing, etc. have to be reevaluated. Beyond the correctness of an AI model, many other new quality attributes, such as fairness, robustness, explainability, transparency, etc. become important in delivering an AI system. The purpose of this paper is to present a view of a holistic quality management framework for ML applications based on the current advances and identify new areas of software engineering research to achieve a more trustworthy AI. △ Less","16 June, 2020",https://arxiv.org/pdf/2006.09529
Response by the Montreal AI Ethics Institute to the European Commission's Whitepaper on AI,Abhishek Gupta;Camylle Lanteigne,"In February 2020, the European Commission (EC) published a white paper entitled, On Artificial Intelligence - A European approach to excellence and trust. This paper outlines the EC's policy options for the promotion and adoption of artificial intelligence (AI) in the European Union. The Montreal AI Ethics Institute (MAIEI) reviewed this paper and published a response addressing the EC's plans to build an ""ecosystem of excellence"" and an ""ecosystem of trust,"" as well as the safety and liability implications of AI, the internet of things (IoT), and robotics. MAIEI provides 15 recommendations in relation to the sections outlined above, including: 1) focus efforts on the research and innovation community, member states, and the private sector; 2) create alignment between trading partners' policies and EU policies; 3) analyze the gaps in the ecosystem between theoretical frameworks and approaches to building trustworthy AI; 4) focus on coordination and policy alignment; 5) focus on mechanisms that promote private and secure sharing of data; 6) create a network of AI research excellence centres to strengthen the research and innovation community; 7) promote knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8) add nuance to the discussion regarding the opacity of AI systems; 9) create a process for individuals to appeal an AI system's decision or output; 10) implement new rules and strengthen existing regulations; 11) ban the use of facial recognition technology; 12) hold all AI systems to similar standards and compulsory requirements; 13) ensure biometric identification systems fulfill the purpose for which they are implemented; 14) implement a voluntary labelling system for systems that are not considered high-risk; 15) appoint individuals to the oversight process who understand AI systems well and are able to communicate potential risks. △ Less","16 June, 2020",https://arxiv.org/pdf/2006.09428
G1020: A Benchmark Retinal Fundus Image Dataset for Computer-Aided Glaucoma Detection,Muhammad Naseer Bajwa;Gur Amrit Pal Singh;Wolfgang Neumeier;Muhammad Imran Malik;Andreas Dengel;Sheraz Ahmed,"Scarcity of large publicly available retinal fundus image datasets for automated glaucoma detection has been the bottleneck for successful application of artificial intelligence towards practical Computer-Aided Diagnosis (CAD). A few small datasets that are available for research community usually suffer from impractical image capturing conditions and stringent inclusion criteria. These shortcomings in already limited choice of existing datasets make it challenging to mature a CAD system so that it can perform in real-world environment. In this paper we present a large publicly available retinal fundus image dataset for glaucoma classification called G1020. The dataset is curated by conforming to standard practices in routine ophthalmology and it is expected to serve as standard benchmark dataset for glaucoma detection. This database consists of 1020 high resolution colour fundus images and provides ground truth annotations for glaucoma diagnosis, optic disc and optic cup segmentation, vertical cup-to-disc ratio, size of neuroretinal rim in inferior, superior, nasal and temporal quadrants, and bounding box location for optic disc. We also report baseline results by conducting extensive experiments for automated glaucoma diagnosis and segmentation of optic disc and optic cup. △ Less","28 May, 2020",https://arxiv.org/pdf/2006.09158
Lio -- A Personal Robot Assistant for Human-Robot Interaction and Care Applications,Justinas Miseikis;Pietro Caroni;Patricia Duchamp;Alina Gasser;Rastislav Marko;Nelija Miseikiene;Frederik Zwilling;Charles de Castelbajac;Lucas Eicher;Michael Fruh;Hansruedi Fruh,"Lio is a mobile robot platform with a multi-functional arm explicitly designed for human-robot interaction and personal care assistant tasks. The robot has already been deployed in several health care facilities, where it is functioning autonomously, assisting staff and patients on an everyday basis. Lio is intrinsically safe by having full coverage in soft artificial-leather material as well as having collision detection, limited speed and forces. Furthermore, the robot has a compliant motion controller. A combination of visual, audio, laser, ultrasound and mechanical sensors are used for safe navigation and environment understanding. The ROS-enabled setup allows researchers to access raw sensor data as well as have direct control of the robot. The friendly appearance of Lio has resulted in the robot being well accepted by health care staff and patients. Fully autonomous operation is made possible by a flexible decision engine, autonomous navigation and automatic recharging. Combined with time-scheduled task triggers, this allows Lio to operate throughout the day, with a battery life of up to 8 hours and recharging during idle times. A combination of powerful on-board computing units provides enough processing power to deploy artificial intelligence and deep learning-based solutions on-board the robot without the need to send any sensitive data to cloud services, guaranteeing compliance with privacy requirements. During the COVID-19 pandemic, Lio was rapidly adjusted to perform additional functionality like disinfection and remote elevated body temperature detection. It complies with ISO13482 - Safety requirements for personal care robots, meaning it can be directly tested and deployed in care facilities. △ Less","16 June, 2020",https://arxiv.org/pdf/2006.09019
Explicit Gradient Learning,Mor Sinay;Elad Sarafian;Yoram Louzoun;Noa Agmon;Sarit Kraus,"Black-Box Optimization (BBO) methods can find optimal policies for systems that interact with complex environments with no analytical representation. As such, they are of interest in many Artificial Intelligence (AI) domains. Yet classical BBO methods fall short in high-dimensional non-convex problems. They are thus often overlooked in real-world AI tasks. Here we present a BBO method, termed Explicit Gradient Learning (EGL), that is designed to optimize high-dimensional ill-behaved functions. We derive EGL by finding weak-spots in methods that fit the objective function with a parametric Neural Network (NN) model and obtain the gradient signal by calculating the parametric gradient. Instead of fitting the function, EGL trains a NN to estimate the objective gradient directly. We prove the convergence of EGL in convex optimization and its robustness in the optimization of integrable functions. We evaluate EGL and achieve state-of-the-art results in two challenging problems: (1) the COCO test suite against an assortment of standard BBO methods; and (2) in a high-dimensional non-convex image generation task. △ Less","9 June, 2020",https://arxiv.org/pdf/2006.08711
A systematic review and taxonomy of explanations in decision support and recommender systems,Ingrid Nunes;Dietmar Jannach,"With the recent advances in the field of artificial intelligence, an increasing number of decision-making tasks are delegated to software systems. A key requirement for the success and adoption of such systems is that users must trust system choices or even fully automated decisions. To achieve this, explanation facilities have been widely investigated as a means of establishing trust in these systems since the early years of expert systems. With today's increasingly sophisticated machine learning algorithms, new challenges in the context of explanations, accountability, and trust towards such systems constantly arise. In this work, we systematically review the literature on explanations in advice-giving systems. This is a family of systems that includes recommender systems, which is one of the most successful classes of advice-giving software in practice. We investigate the purposes of explanations as well as how they are generated, presented to users, and evaluated. As a result, we derive a novel comprehensive taxonomy of aspects to be considered when designing explanation facilities for current and future decision support systems. The taxonomy includes a variety of different facets, such as explanation objective, responsiveness, content and presentation. Moreover, we identified several challenges that remain unaddressed so far, for example related to fine-grained issues associated with the presentation of explanations and how explanation facilities are evaluated. △ Less","15 June, 2020",https://arxiv.org/pdf/2006.08672
Scientometric analysis and knowledge mapping of literature-based discovery (1986-2020),Andrej Kastrin;Dimitar Hristovski,"Literature-based discovery (LBD) aims to discover valuable latent relationships between disparate sets of literatures. This paper presents the first inclusive scientometric overview of LBD research. We utilize a comprehensive scientometric approach incorporating CiteSpace to systematically analyze the literature on LBD from the last four decades (1986-2020). After manual cleaning, we have retrieved a total of 409 documents from six bibliographic databases and two preprint servers. The 35 years' history of LBD could be partitioned into three phases according to the published papers per year: incubation (1986-2003), developing (2004-2008), and mature phase (2009-2020). The annual production of publications follows Price's law. The co-authorship network exhibits many subnetworks, indicating that LBD research is composed of many small and medium-sized groups with little collaboration among them. Science mapping reveals that mainstream research in LBD has shifted from baseline co-occurrence approaches to semantic-based methods at the beginning of the new millennium. In the last decade, we can observe the leaning of LBD towards modern network science ideas. In an applied sense, the LBD is increasingly used in predicting adverse drug reactions and drug repurposing. Besides theoretical considerations, the researchers have put a lot of effort into the development of Web-based LBD applications. Nowadays, LBD is becoming increasingly interdisciplinary and involves methods from information science, scientometrics, and machine learning. Unfortunately, LBD is mainly limited to the biomedical domain. The cascading citation expansion announces deep learning and explainable artificial intelligence as emerging topics in LBD. The results indicate that LBD is still growing and evolving. △ Less","29 November, 2020",https://arxiv.org/pdf/2006.08486
Symbolic Logic meets Machine Learning: A Brief Survey in Infinite Domains,Vaishak Belle,"The tension between deduction and induction is perhaps the most fundamental issue in areas such as philosophy, cognition and artificial intelligence (AI). The deduction camp concerns itself with questions about the expressiveness of formal languages for capturing knowledge about the world, together with proof systems for reasoning from such knowledge bases. The learning camp attempts to generalize from examples about partial descriptions about the world. In AI, historically, these camps have loosely divided the development of the field, but advances in cross-over areas such as statistical relational learning, neuro-symbolic systems, and high-level control have illustrated that the dichotomy is not very constructive, and perhaps even ill-formed. In this article, we survey work that provides further evidence for the connections between logic and learning. Our narrative is structured in terms of three strands: logic versus learning, machine learning for logic, and logic for machine learning, but naturally, there is considerable overlap. We place an emphasis on the following ""sore"" point: there is a common misconception that logic is for discrete properties, whereas probability theory and machine learning, more generally, is for continuous properties. We report on results that challenge this view on the limitations of logic, and expose the role that logic can play for learning in infinite domains. △ Less","15 June, 2020",https://arxiv.org/pdf/2006.08480
Machine Common Sense,Alexander Gavrilenko;Katerina Morozova,"Machine common sense remains a broad, potentially unbounded problem in artificial intelligence (AI). There is a wide range of strategies that can be employed to make progress on this challenge. This article deals with the aspects of modeling commonsense reasoning focusing on such domain as interpersonal interactions. The basic idea is that there are several types of commonsense reasoning: one is manifested at the logical level of physical actions, the other deals with the understanding of the essence of human-human interactions. Existing approaches, based on formal logic and artificial neural networks, allow for modeling only the first type of common sense. To model the second type, it is vital to understand the motives and rules of human behavior. This model is based on real-life heuristics, i.e., the rules of thumb, developed through knowledge and experience of different generations. Such knowledge base allows for development of an expert system with inference and explanatory mechanisms (commonsense reasoning algorithms and personal models). Algorithms provide tools for a situation analysis, while personal models make it possible to identify personality traits. The system so designed should perform the function of amplified intelligence for interactions, including human-machine. △ Less","15 June, 2020",https://arxiv.org/pdf/2006.08409
Sensor Artificial Intelligence and its Application to Space Systems -- A White Paper,Anko Börner;Heinz-Wilhelm Hübers;Odej Kao;Florian Schmidt;Sören Becker;Joachim Denzler;Daniel Matolin;David Haber;Sergio Lucia;Wojciech Samek;Rudolph Triebel;Sascha Eichstädt;Felix Biessmann;Anna Kruspe;Peter Jung;Manon Kok;Guillermo Gallego;Ralf Berger,"Information and communication technologies have accompanied our everyday life for years. A steadily increasing number of computers, cameras, mobile devices, etc. generate more and more data, but at the same time we realize that the data can only partially be analyzed with classical approaches. The research and development of methods based on artificial intelligence (AI) made enormous progress in the area of interpretability of data in recent years. With growing experience, both, the potential and limitations of these new technologies are increasingly better understood. Typically, AI approaches start with the data from which information and directions for action are derived. However, the circumstances under which such data are collected and how they change over time are rarely considered. A closer look at the sensors and their physical properties within AI approaches will lead to more robust and widely applicable algorithms. This holistic approach which considers entire signal chains from the origin to a data product, ""Sensor AI"", is a highly relevant topic with great potential. It will play a decisive role in autonomous driving as well as in areas of automated production, predictive maintenance or space research. The goal of this white paper is to establish ""Sensor AI"" as a dedicated research topic. We want to exchange knowledge on the current state-of-the-art on Sensor AI, to identify synergies among research groups and thus boost the collaboration in this key technology for science and industry. △ Less","9 June, 2020",https://arxiv.org/pdf/2006.08368
Deep-CAPTCHA: a deep learning based CAPTCHA solver for vulnerability assessment,Zahra Noury;Mahdi Rezaei,"CAPTCHA is a human-centred test to distinguish a human operator from bots, attacking programs, or other computerised agents that tries to imitate human intelligence. In this research, we investigate a way to crack visual CAPTCHA tests by an automated deep learning based solution. The goal of this research is to investigate the weaknesses and vulnerabilities of the CAPTCHA generator systems; hence, developing more robust CAPTCHAs, without taking the risks of manual try and fail efforts. We develop a Convolutional Neural Network called Deep-CAPTCHA to achieve this goal. The proposed platform is able to investigate both numerical and alphanumerical CAPTCHAs. To train and develop an efficient model, we have generated a dataset of 500,000 CAPTCHAs to train our model. In this paper, we present our customised deep neural network model, we review the research gaps, the existing challenges, and the solutions to cope with the issues. Our network's cracking accuracy leads to a high rate of 98.94% and 98.31% for the numerical and the alpha-numerical test datasets, respectively. That means more works is required to develop robust CAPTCHAs, to be non-crackable against automated artificial agents. As the outcome of this research, we identify some efficient techniques to improve the security of the CAPTCHAs, based on the performance analysis conducted on the Deep-CAPTCHA model. △ Less","24 June, 2020",https://arxiv.org/pdf/2006.08296
Ethical Considerations for AI Researchers,Kyle Dent,"Use of artificial intelligence is growing and expanding into applications that impact people's lives. People trust their technology without really understanding it or its limitations. There is the potential for harm and we are already seeing examples of that in the world. AI researchers have an obligation to consider the impact of intelligent applications they work on. While the ethics of AI is not clear-cut, there are guidelines we can consider to minimize the harm we might introduce. △ Less","13 June, 2020",https://arxiv.org/pdf/2006.07558
Open Questions in Creating Safe Open-ended AI: Tensions Between Control and Creativity,Adrien Ecoffet;Jeff Clune;Joel Lehman,"Artificial life originated and has long studied the topic of open-ended evolution, which seeks the principles underlying artificial systems that innovate continually, inspired by biological evolution. Recently, interest has grown within the broader field of AI in a generalization of open-ended evolution, here called open-ended search, wherein such questions of open-endedness are explored for advancing AI, whatever the nature of the underlying search algorithm (e.g. evolutionary or gradient-based). For example, open-ended search might design new architectures for neural networks, new reinforcement learning algorithms, or most ambitiously, aim at designing artificial general intelligence. This paper proposes that open-ended evolution and artificial life have much to contribute towards the understanding of open-ended AI, focusing here in particular on the safety of open-ended search. The idea is that AI systems are increasingly applied in the real world, often producing unintended harms in the process, which motivates the growing field of AI safety. This paper argues that open-ended AI has its own safety challenges, in particular, whether the creativity of open-ended systems can be productively and predictably controlled. This paper explains how unique safety problems manifest in open-ended search, and suggests concrete contributions and research questions to explore them. The hope is to inspire progress towards creative, useful, and safe open-ended search algorithms. △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07495
The 4th Industrial Revolution Effect on the Enterprise Cyber Strategy,Christopher Gorham,"The Fourth (4th) Industrial Revolution represents the profound advancement of technology that will likely transform the boundaries between the digital and physical worlds in modern society. The impact of advance technology will disrupt almost every aspect of business and government communities alike. In the past few years, the advancement of information technologies has opened the door to artificial intelligence (AI), block chain technologies, robotics, virtual reality and the possibility of quantum computing being released in the commercial sector. The use of these innovative technologies will likely impact society by leveraging modern technological platforms such as cloud computing and AI. This also includes the release of 5G network technologies by Internet Service Providers (ISP) beginning in 2019. Networks that rely upon 5G technologies in combination with cloud computing platforms will open the door allow greater innovations and change the nature of how work is performed in the 4th Industrial Revolution. △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07488
Returning the Favor: What Wireless Networking Can Offer to AI and Edge Learning,Sameh Sorour;Umair Mohammad;Amr Abutuleb;Hossam Hassanein,"Machine learning (ML) and artificial intelligence (AI) have recently made a significant impact on improving the operations of wireless networks and establishing intelligence at the edge. In return, rare efforts were made to explore how adapting, optimizing, and arranging wireless networks can contribute to implementing ML/AI at the edge. This article aims to address this void by setting a vision on how wireless networking researchers can leverage their expertise to return the favor to edge learning. It will review the enabling technologies, summarize the inaugural works on this path, and shed light on different directions to establish a comprehensive framework for mobile edge learning (MEL). △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07453
"The Threats of Artificial Intelligence Scale (TAI). Development, Measurement and Test Over Three Application Domains",Kimon Kieslich;Marco Lünich;Frank Marcinkowski,"In recent years Artificial Intelligence (AI) has gained much popularity, with the scientific community as well as with the public. AI is often ascribed many positive impacts for different social domains such as medicine and the economy. On the other side, there is also growing concern about its precarious impact on society and individuals. Several opinion polls frequently query the public fear of autonomous robots and artificial intelligence (FARAI), a phenomenon coming also into scholarly focus. As potential threat perceptions arguably vary with regard to the reach and consequences of AI functionalities and the domain of application, research still lacks necessary precision of a respective measurement that allows for wide-spread research applicability. We propose a fine-grained scale to measure threat perceptions of AI that accounts for four functional classes of AI systems and is applicable to various domains of AI applications. Using a standardized questionnaire in a survey study (N=891), we evaluate the scale over three distinct AI domains (loan origination, job recruitment and medical treatment). The data support the dimensional structure of the proposed Threats of AI (TAI) scale as well as the internal consistency and factoral validity of the indicators. Implications of the results and the empirical application of the scale are discussed in detail. Recommendations for further empirical use of the TAI scale are provided. △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07211
Response to Office of the Privacy Commissioner of Canada Consultation Proposals pertaining to amendments to PIPEDA relative to Artificial Intelligence,Mirka Snyder Caron;Abhishek Gupta,"In February 2020, the Montreal AI Ethics Institute (MAIEI) was invited by the Office of the Privacy Commissioner of Canada (OPCC) to provide for comments both at a closed roundtable and in writing on the OPCC consultation proposal for amendments relative to Artificial Intelligence (AI), to the Canadian privacy legislation, the Personal Information Protection and Electronic Documents Act (PIPEDA). The present document includes MAIEI comments and recommendations in writing. Per MAIEI's mission and mandate to act as a catalyst for public feedback pertaining to AI Ethics and regulatory technology developments, as well as to provide for public competence-building workshops on critical topics in such domains, the reader will also find such public feedback and propositions by Montrealers who participated at MAIEI's workshops, submitted as Schedule 1 to the present report. For each of OPCC 12 proposals, and underlying questions, as described on its website, MAIEI provides a short reply, a summary list of recommendations, as well as comments relevant to the question at hand. We leave you with three general statements to keep in mind while going through the next pages: 1) AI systems should be used to augment human capacity for meaningful and purposeful connections and associations, not as a substitute for trust. 2) Humans have collectively accepted to uphold the rule of law, but for machines, the code is rule. Where socio-technical systems are deployed to make important decisions, profiles or inferences about individuals, we will increasingly have to attempt the difficult exercise of drafting and encoding our law in a manner learnable by machines. 3) Let us work collectively towards a world where Responsible AI becomes the rule, before our socio-technical systems become ""too connected to fail"". △ Less","12 June, 2020",https://arxiv.org/pdf/2006.07025
A robust modeling framework for energy analysis of data centers,Nuoa Lei,"Global digitalization has given birth to the explosion of digital services in approximately every sector of contemporary life. Applications of artificial intelligence, blockchain technologies, and internet of things are promising to accelerate digitalization further. As a consequence, the number of data centers, which provide the services of data processing, storage, and communication services, is also increasing rapidly. Because data centers are energy-intensive with significant and growing electricity demand, an energy model of data centers with temporal, spatial, and predictive analysis capability is critical for guiding industry and governmental authorities for making technology investment decisions. However, current models fail to provide consistent and high dimensional energy analysis for data centers due to severe data gaps. This can be further attributed to the lack of the modeling capabilities for energy analysis of data center components including IT equipment and data center cooling and power provisioning infrastructure in current energy models. In this research, a technology-based modeling framework, in hybrid with a data-driven approach, is proposed to address the knowledge gaps in current data center energy models. The research aims to provide policy makers and data center energy analysts with comprehensive understanding of data center energy use and efficiency opportunities and a better understanding of macro-level data center energy demand and energy saving potentials, in addition to the technological barriers for adopting energy efficiency measures. △ Less","11 June, 2020",https://arxiv.org/pdf/2006.06819
"Synergetic Learning Systems: Concept, Architecture, and Algorithms",Ping Guo;Qian Yin,"Drawing on the idea that brain development is a Darwinian process of ``evolution + selection'' and the idea that the current state is a local equilibrium state of many bodies with self-organization and evolution processes driven by the temperature and gravity in our universe, in this work, we describe an artificial intelligence system called the ``Synergetic Learning Systems''. The system is composed of two or more subsystems (models, agents or virtual bodies), and it is an open complex giant system. Inspired by natural intelligence, the system achieves intelligent information processing and decision-making in a given environment through cooperative/competitive synergetic learning. The intelligence evolved by the natural law of ``it is not the strongest of the species that survives, but the one most responsive to change,'' while an artificial intelligence system should adopt the law of ``human selection'' in the evolution process. Therefore, we expect that the proposed system architecture can also be adapted in human-machine synergy or multi-agent synergetic systems. It is also expected that under our design criteria, the proposed system will eventually achieve artificial general intelligence through long term coevolution. △ Less","14 June, 2020",https://arxiv.org/pdf/2006.06367
Montreal AI Ethics Institute's Response to Scotland's AI Strategy,Abhishek Gupta,"In January and February 2020, the Scottish Government released two documents for review by the public regarding their artificial intelligence (AI) strategy. The Montreal AI Ethics Institute (MAIEI) reviewed these documents and published a response on 4 June 2020. MAIEI's response examines several questions that touch on the proposed definition of AI; the people-centered nature of the strategy; considerations to ensure that everyone benefits from AI; the strategy's overarching vision; Scotland's AI ecosystem; the proposed strategic themes; and how to grow public confidence in AI by building responsible and ethical systems. In addition to examining the points above, MAIEI suggests that the strategy be extended to include considerations on biometric data and how that will be processed and used in the context of AI. It also highlights the importance of tackling head-on the inherently stochastic nature of deep learning systems and developing concrete guidelines to ensure that these systems are built responsibly and ethically, particularly as machine learning becomes more accessible. Finally, it concludes that any national AI strategy must clearly address the measurements of success in regards to the strategy's stated goals and vision to ensure that they are interpreted and applied consistently. To do this, there must be inclusion and transparency between those building the systems and those using them in their work. △ Less","11 June, 2020",https://arxiv.org/pdf/2006.06300
Performance in the Courtroom: Automated Processing and Visualization of Appeal Court Decisions in France,Paul Boniol;George Panagopoulos;Christos Xypolopoulos;Rajaa El Hamdani;David Restrepo Amariles;Michalis Vazirgiannis,"Artificial Intelligence techniques are already popular and important in the legal domain. We extract legal indicators from judicial judgment to decrease the asymmetry of information of the legal system and the access-to-justice gap. We use NLP methods to extract interesting entities/data from judgments to construct networks of lawyers and judgments. We propose metrics to rank lawyers based on their experience, wins/loss ratio and their importance in the network of lawyers. We also perform community detection in the network of judgments and propose metrics to represent the difficulty of cases capitalising on communities features. △ Less","9 July, 2020",https://arxiv.org/pdf/2006.06251
"Analyzing Power Grid, ICT, and Market Without Domain Knowledge Using Distributed Artificial Intelligence",Eric MSP Veith;Stephan Balduin;Nils Wenninghoff;Martin Tröschel;Lars Fischer;Astrid Nieße;Thomas Wolgast;Richard Sethmann;Bastian Fraune;Torben Woltjen,"Modern cyber-physical systems (CPS), such as our energy infrastructure, are becoming increasingly complex: An ever-higher share of Artificial Intelligence (AI)-based technologies use the Information and Communication Technology (ICT) facet of energy systems for operation optimization, cost efficiency, and to reach CO2 goals worldwide. At the same time, markets with increased flexibility and ever shorter trade horizons enable the multi-stakeholder situation that is emerging in this setting. These systems still form critical infrastructures that need to perform with highest reliability. However, today's CPS are becoming too complex to be analyzed in the traditional monolithic approach, where each domain, e.g., power grid and ICT as well as the energy market, are considered as separate entities while ignoring dependencies and side-effects. To achieve an overall analysis, we introduce the concept for an application of distributed artificial intelligence as a self-adaptive analysis tool that is able to analyze the dependencies between domains in CPS by attacking them. It eschews pre-configured domain knowledge, instead exploring the CPS domains for emergent risk situations and exploitable loopholes in codices, with a focus on rational market actors that exploit the system while still following the market rules. △ Less","10 June, 2020",https://arxiv.org/pdf/2006.06074
Rinascimento: using event-value functions for playing Splendor,Ivan Bravi;Simon Lucas,"In the realm of games research, Artificial General Intelligence algorithms often use score as main reward signal for learning or playing actions. However this has shown its severe limitations when the point rewards are very rare or absent until the end of the game. This paper proposes a new approach based on event logging: the game state triggers an event every time one of its features changes. These events are processed by an Event-value Function (EF) that assigns a value to a single action or a sequence. The experiments have shown that such approach can mitigate the problem of scarce point rewards and improve the AI performance. Furthermore this represents a step forward in controlling the strategy adopted by the artificial agent, by describing a much richer and controllable behavioural space through the EF. Tuned EF are able to neatly synthesise the relevance of the events in the game. Agents using an EF show more robust when playing games with several opponents. △ Less","10 June, 2020",https://arxiv.org/pdf/2006.05894
"A systematic review on the role of artificial intelligence in sonographic diagnosis of thyroid cancer: Past, present and future",Fatemeh Abdolali;Atefeh Shahroudnejad;Abhilash Rakkunedeth Hareendranathan;Jacob L Jaremko;Michelle Noga;Kumaradevan Punithakumar,"Thyroid cancer is common worldwide, with a rapid increase in prevalence across North America in recent years. While most patients present with palpable nodules through physical examination, a large number of small and medium-sized nodules are detected by ultrasound examination. Suspicious nodules are then sent for biopsy through fine needle aspiration. Since biopsies are invasive and sometimes inconclusive, various research groups have tried to develop computer-aided diagnosis systems. Earlier approaches along these lines relied on clinically relevant features that were manually identified by radiologists. With the recent success of artificial intelligence (AI), various new methods are being developed to identify these features in thyroid ultrasound automatically. In this paper, we present a systematic review of state-of-the-art on AI application in sonographic diagnosis of thyroid cancer. This review follows a methodology-based classification of the different techniques available for thyroid cancer diagnosis. With more than 50 papers included in this review, we reflect on the trends and challenges of the field of sonographic diagnosis of thyroid malignancies and potential of computer-aided diagnosis to increase the impact of ultrasound applications on the future of thyroid cancer diagnosis. Machine learning will continue to play a fundamental role in the development of future thyroid cancer diagnosis frameworks. △ Less","10 June, 2020",https://arxiv.org/pdf/2006.05861
Interpretable Multimodal Learning for Intelligent Regulation in Online Payment Systems,Shuoyao Wang;Diwei Zhu,"With the explosive growth of transaction activities in online payment systems, effective and realtime regulation becomes a critical problem for payment service providers. Thanks to the rapid development of artificial intelligence (AI), AI-enable regulation emerges as a promising solution. One main challenge of the AI-enabled regulation is how to utilize multimedia information, i.e., multimodal signals, in Financial Technology (FinTech). Inspired by the attention mechanism in nature language processing, we propose a novel cross-modal and intra-modal attention network (CIAN) to investigate the relation between the text and transaction. More specifically, we integrate the text and transaction information to enhance the text-trade jointembedding learning, which clusters positive pairs and push negative pairs away from each other. Another challenge of intelligent regulation is the interpretability of complicated machine learning models. To sustain the requirements of financial regulation, we design a CIAN-Explainer to interpret how the attention mechanism interacts the original features, which is formulated as a low-rank matrix approximation problem. With the real datasets from the largest online payment system, WeChat Pay of Tencent, we conduct experiments to validate the practical application value of CIAN, where our method outperforms the state-of-the-art methods. △ Less","10 June, 2020",https://arxiv.org/pdf/2006.05669
Real-time Neural Networks Implementation Proposal for Microcontrollers,Caio J. B. V. Guimarães;Marcelo A. C. Fernandes,"The adoption of intelligent systems with Artificial Neural Networks (ANNs) embedded in hardware for real-time applications currently faces a growing demand in fields like the Internet of Things (IoT) and Machine to Machine (M2M). However, the application of ANNs in this type of system poses a significant challenge due to the high computational power required to process its basic operations. This paper aims to show an implementation strategy of a Multilayer Perceptron (MLP) type neural network, in a microcontroller (a low-cost, low-power platform). A modular matrix-based MLP with the full classification process was implemented, and also the backpropagation training in the microcontroller. The testing and validation were performed through Hardware in the Loop (HIL) of the Mean Squared Error (MSE) of the training process, classification result, and the processing time of each implementation module. The results revealed a linear relationship between the values of the hyperparameters and the processing time required for classification, also the processing time concurs with the required time for many applications on the fields mentioned above. These findings show that this implementation strategy and this platform can be applied successfully on real-time applications that require the capabilities of ANNs. △ Less","7 June, 2020",https://arxiv.org/pdf/2006.05344
Artificial Intelligence (AI)-Centric Management of Resources in Modern Distributed Computing Systems,Shashikant Ilager;Rajeev Muralidhar;Rajkumar Buyya,"Contemporary Distributed Computing Systems (DCS) such as Cloud Data Centres are large scale, complex, heterogeneous, and distributed across multiple networks and geographical boundaries. On the other hand, the Internet of Things (IoT)-driven applications are producing a huge amount of data that requires real-time processing and fast response. Managing these resources efficiently to provide reliable services to end-users or applications is a challenging task. The existing Resource Management Systems (RMS) rely on either static or heuristic solutions inadequate for such composite and dynamic systems. The advent of Artificial Intelligence (AI) due to data availability and processing capabilities manifested into possibilities of exploring data-driven solutions in RMS tasks that are adaptive, accurate, and efficient. In this regard, this paper aims to draw the motivations and necessities for data-driven solutions in resource management. It identifies the challenges associated with it and outlines the potential future research directions detailing where and how to apply the data-driven techniques in the different RMS tasks. Finally, it provides a conceptual data-driven RMS model for DCS and presents the two real-time use cases (GPU frequency scaling and data centre resource management from Google Cloud and Microsoft Azure) demonstrating AI-centric approaches' feasibility. △ Less","6 November, 2020",https://arxiv.org/pdf/2006.05075
A Survey of Cybersecurity of Digital Manufacturing,Priyanka Mahesh;Akash Tiwari;Chenglu Jin;Panganamala R. Kumar;A. L. Narasimha Reddy;Satish T. S. Bukkapatanam;Nikhil Gupta;Ramesh Karri,"The Industry 4.0 concept promotes a digital manufacturing (DM) paradigm that can enhance quality and productivity, that reduces inventory and the lead-time for delivering custom, batch-of-one products based on achieving convergence of Additive, Subtractive, and Hybrid manufacturing machines, Automation and Robotic Systems, Sensors, Computing, and Communication Networks, Artificial Intelligence, and Big Data. A DM system consists of embedded electronics, sensors, actuators, control software, and inter-connectivity to enable the machines and the components within them to exchange data with other machines, components therein, the plant operators, the inventory managers, and customers. This paper presents the cybersecurity risks in the emerging DM context, assesses the impact on manufacturing, and identifies approaches to secure DM. △ Less","15 October, 2020",https://arxiv.org/pdf/2006.05042
AI Research Considerations for Human Existential Safety (ARCHES),Andrew Critch;David Krueger,"Framed in positive terms, this report examines how technical AI research might be steered in a manner that is more attentive to humanity's long-term prospects for survival as a species. In negative terms, we ask what existential risks humanity might face from AI development in the next century, and by what principles contemporary technical research might be directed to address those risks. A key property of hypothetical AI technologies is introduced, called \emph{prepotence}, which is useful for delineating a variety of potential existential risks from artificial intelligence, even as AI paradigms might shift. A set of \auxref{dirtot} contemporary research \directions are then examined for their potential benefit to existential safety. Each research direction is explained with a scenario-driven motivation, and examples of existing work from which to build. The research directions present their own risks and benefits to society that could occur at various scales of impact, and in particular are not guaranteed to benefit existential safety if major developments in them are deployed without adequate forethought and oversight. As such, each direction is accompanied by a consideration of potentially negative side effects. △ Less","29 May, 2020",https://arxiv.org/pdf/2006.04948
How Smart is the Grid?,Ermanno Lo Cascio;Zhenjun Ma;François Maréchal,"Ancient Romans called 'urbs' the set of buildings and infrastructures, and 'civitas' the Roman citizens. Today instead, while the society is surfing the digital tsunami, 'urbs' and 'civitas' tend to become much closer, almost merging, that we might attempt to condensate these into a single concept: 'smart grid'. Internet of things, artificial intelligence, blockchain, quantum cryptography is only a few of the paradigms that are likely to contribute to determining the final portrait of the future smart grid. However, to understand the effective sustainability of complex grids, specific tools are required. To this end, in this article, a systematic review of the emerging paradigms is presented, identifying intersectoral synergies and limitations with respect to the `smart grid' concept. Further, a taxonomic framework for assessing the level of sustainability of the grid is proposed. Finally, from the scenario portrayed, a set of issues involving engineering, regulation, security, and social frameworks have been derived in a theoretical fashion. The findings are likely to suggest the urgent need for multidisciplinary cooperation to wisely address engineering and ontological challenges gravitating around the smart grid concept. △ Less","24 June, 2020",https://arxiv.org/pdf/2006.04943
Principles to Practices for Responsible AI: Closing the Gap,Daniel Schiff;Bogdana Rakova;Aladdin Ayesh;Anat Fanti;Michael Lennon,"Companies have considered adoption of various high-level artificial intelligence (AI) principles for responsible AI, but there is less clarity on how to implement these principles as organizational practices. This paper reviews the principles-to-practices gap. We outline five explanations for this gap ranging from a disciplinary divide to an overabundance of tools. In turn, we argue that an impact assessment framework which is broad, operationalizable, flexible, iterative, guided, and participatory is a promising approach to close the principles-to-practices gap. Finally, to help practitioners with applying these recommendations, we review a case study of AI's use in forest ecosystem restoration, demonstrating how an impact assessment framework can translate into effective and responsible AI practices. △ Less","8 June, 2020",https://arxiv.org/pdf/2006.04707
Kafka-ML: connecting the data stream with ML/AI frameworks,Cristian Martín;Peter Langendoerfer;Pouya Soltani Zarrin;Manuel Díaz;Bartolomé Rubio,"Machine Learning (ML) and Artificial Intelligence (AI) have a dependency on data sources to train, improve and make predictions through their algorithms. With the digital revolution and current paradigms like the Internet of Things, this information is turning from static data into continuous data streams. However, most of the ML/AI frameworks used nowadays are not fully prepared for this revolution. In this paper, we proposed Kafka-ML, an open-source framework that enables the management of TensorFlow ML/AI pipelines through data streams (Apache Kafka). Kafka-ML provides an accessible and user-friendly Web User Interface where users can easily define ML models, to then train, evaluate and deploy them for inference. Kafka-ML itself and its deployed components are fully managed through containerization technologies, which ensure its portability and easy distribution and other features such as fault-tolerance and high availability. Finally, a novel approach has been introduced to manage and reuse data streams, which may lead to the (no) utilization of data storage and file systems. △ Less","16 July, 2020",https://arxiv.org/pdf/2006.04105
Artificial Intelligence-based Clinical Decision Support for COVID-19 -- Where Art Thou?,Mathias Unberath;Kimia Ghobadi;Scott Levin;Jeremiah Hinson;Gregory D Hager,"The COVID-19 crisis has brought about new clinical questions, new workflows, and accelerated distributed healthcare needs. While artificial intelligence (AI)-based clinical decision support seemed to have matured, the application of AI-based tools for COVID-19 has been limited to date. In this perspective piece, we identify opportunities and requirements for AI-based clinical decision support systems and highlight challenges that impact ""AI readiness"" for rapidly emergent healthcare challenges. △ Less","5 June, 2020",https://arxiv.org/pdf/2006.03434
Fast CRDNN: Towards on Site Training of Mobile Construction Machines,Yusheng Xiang;Tian Tang;Tianqing Su;Christine Brach;Libo Liu;Samuel Mao;Marcus Geimer,"The CRDNN is a combined neural network that can increase the holistic efficiency of torque based mobile working machines by about 9% by means of accurately detecting the truck loading cycles. On the one hand, it is a robust but offline learning algorithm so that it is more accurate and much quicker than the previous methods. However, on the other hand, its accuracy can not always be guaranteed because of the diversity of the mobile machines industry and the nature of the offline method. To address the problem, we utilize the transfer learning algorithm and the Internet of Things (IoT) technology. Concretely, the CRDNN is first trained by computer and then saved in the on-board ECU. In case that the pre-trained CRDNN is not suitable for the new machine, the operator can label some new data by our App connected to the on-board ECU of that machine through Bluetooth. With the newly labeled data, we can directly further train the pretrained CRDNN on the ECU without overloading since transfer learning requires less computation effort than training the networks from scratch. In our paper, we prove this idea and show that CRDNN is always competent, with the help of transfer learning and IoT technology by field experiment, even the new machine may have a different distribution. Also, we compared the performance of other SOTA multivariate time series algorithms on predicting the working state of the mobile machines, which denotes that the CRDNNs are still the most suitable solution. As a by-product, we build up a human-machine communication system to label the dataset, which can be operated by engineers without knowledge about Artificial Intelligence (AI). △ Less","4 June, 2020",https://arxiv.org/pdf/2006.03169
SIDU: Similarity Difference and Uniqueness Method for Explainable AI,Satya M. Muddamsetty;Mohammad N. S. Jahromi;Thomas B. Moeslund,"A new brand of technical artificial intelligence ( Explainable AI ) research has focused on trying to open up the 'black box' and provide some explainability. This paper presents a novel visual explanation method for deep learning networks in the form of a saliency map that can effectively localize entire object regions. In contrast to the current state-of-the art methods, the proposed method shows quite promising visual explanations that can gain greater trust of human expert. Both quantitative and qualitative evaluations are carried out on both general and clinical data sets to confirm the effectiveness of the proposed method. △ Less","4 June, 2020",https://arxiv.org/pdf/2006.03122
"Federated Learning for 6G Communications: Challenges, Methods, and Future Directions",Yi Liu;Xingliang Yuan;Zehui Xiong;Jiawen Kang;Xiaofei Wang;Dusit Niyato,"As the 5G communication networks are being widely deployed worldwide, both industry and academia have started to move beyond 5G and explore 6G communications. It is generally believed that 6G will be established on ubiquitous Artificial Intelligence (AI) to achieve data-driven Machine Learning (ML) solutions in heterogeneous and massive-scale networks. However, traditional ML techniques require centralized data collection and processing by a central server, which is becoming a bottleneck of large-scale implementation in daily life due to significantly increasing privacy concerns. Federated learning, as an emerging distributed AI approach with privacy preservation nature, is particularly attractive for various wireless applications, especially being treated as one of the vital solutions to achieve ubiquitous AI in 6G. In this article, we first introduce the integration of 6G and federated learning and provide potential federated learning applications for 6G. We then describe key technical challenges, the corresponding federated learning methods, and open problems for future research on federated learning in the context of 6G communications. △ Less","12 July, 2020",https://arxiv.org/pdf/2006.02931
Assessing Intelligence in Artificial Neural Networks,Nicholas J. Schaub;Nathan Hotaling,"The purpose of this work was to develop of metrics to assess network architectures that balance neural network size and task performance. To this end, the concept of neural efficiency is introduced to measure neural layer utilization, and a second metric called artificial intelligence quotient (aIQ) was created to balance neural network performance and neural network efficiency. To study aIQ and neural efficiency, two simple neural networks were trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32% less accurate but contained 30,912 times fewer parameters than the highest accuracy network. Both batch normalization and dropout layers were found to increase neural efficiency. Finally, high aIQ networks are shown to be memorization and overtraining resistant, capable of learning proper digit classification with an accuracy of 92.51% even when 75% of the class labels are randomized. These results demonstrate the utility of aIQ and neural efficiency as metrics for balancing network performance and size. △ Less","3 June, 2020",https://arxiv.org/pdf/2006.02909
Neural Network for Low-Memory IoT Devices and MNIST Image Recognition Using Kernels Based on Logistic Map,Andrei Velichko,"This study presents a neural network which uses filters based on logistic mapping (LogNNet). LogNNet has a feedforward network structure, but possesses the properties of reservoir neural networks. The input weight matrix, set by a recurrent logistic mapping, forms the kernels that transform the input space to the higher-dimensional feature space. The most effective recognition of a handwritten digit from MNIST-10 occurs under chaotic behavior of the logistic map. The correlation of classification accuracy with the value of the Lyapunov exponent was obtained. An advantage of LogNNet implementation on IoT devices is the significant savings in memory used. At the same time, LogNNet has a simple algorithm and performance indicators comparable to those of the best resource-efficient algorithms available at the moment. The presented network architecture uses an array of weights with a total memory size from 1 to 29 kB and achieves a classification accuracy of 80.3-96.3%. Memory is saved due to the processor, which sequentially calculates the required weight coefficients during the network operation using the analytical equation of the logistic mapping. The proposed neural network can be used in implementations of artificial intelligence based on constrained devices with limited memory, which are integral blocks for creating ambient intelligence in modern IoT environments. From a research perspective, LogNNet can contribute to the understanding of the fundamental issues of the influence of chaos on the behavior of reservoir-type neural networks. △ Less","3 September, 2020",https://arxiv.org/pdf/2006.02824
Seq2Seq AI Chatbot with Attention Mechanism,Abonia Sojasingarayar,"Intelligent Conversational Agent development using Artificial Intelligence or Machine Learning technique is an interesting problem in the field of Natural Language Processing. With the rise of deep learning, these models were quickly replaced by end to end trainable neural networks. △ Less","4 June, 2020",https://arxiv.org/pdf/2006.02767
Characterizing the Weight Space for Different Learning Models,Saurav Musunuru;Jay N. Paranjape;Rahul Kumar Dubey;Vijendran G. Venkoparao,"Deep Learning has become one of the primary research areas in developing intelligent machines. Most of the well-known applications (such as Speech Recognition, Image Processing and NLP) of AI are driven by Deep Learning. Deep Learning algorithms mimic human brain using artificial neural networks and progressively learn to accurately solve a given problem. But there are significant challenges in Deep Learning systems. There have been many attempts to make deep learning models imitate the biological neural network. However, many deep learning models have performed poorly in the presence of adversarial examples. Poor performance in adversarial examples leads to adversarial attacks and in turn leads to safety and security in most of the applications. In this paper we make an attempt to characterize the solution space of a deep neural network in terms of three different subsets viz. weights belonging to exact trained patterns, weights belonging to generalized pattern set and weights belonging to adversarial pattern sets. We attempt to characterize the solution space with two seemingly different learning paradigms viz. the Deep Neural Networks and the Dense Associative Memory Model, which try to achieve learning via quite different mechanisms. We also show that adversarial attacks are generally less successful against Associative Memory Models than Deep Neural Networks. △ Less","4 June, 2020",https://arxiv.org/pdf/2006.02724
An optimizable scalar objective value cannot be objective and should not be the sole objective,Isabel Kloumann;Mark Tygert,"This paper concerns the ethics and morality of algorithms and computational systems, and has been circulating internally at Facebook for the past couple years. The paper reviews many Nobel laureates' work, as well as the work of other prominent scientists such as Richard Dawkins, Andrei Kolmogorov, Vilfredo Pareto, and John von Neumann. The paper draws conclusions based on such works, as summarized in the title. The paper argues that the standard approach to modern machine learning and artificial intelligence is bound to be biased and unfair, and that longstanding traditions in the professions of law, justice, politics, and medicine should help. △ Less","3 June, 2020",https://arxiv.org/pdf/2006.02577
Targeting SARS-CoV-2 with AI- and HPC-enabled Lead Generation: A First Data Release,Yadu Babuji;Ben Blaiszik;Tom Brettin;Kyle Chard;Ryan Chard;Austin Clyde;Ian Foster;Zhi Hong;Shantenu Jha;Zhuozhao Li;Xuefeng Liu;Arvind Ramanathan;Yi Ren;Nicholaus Saint;Marcus Schwarting;Rick Stevens;Hubertus van Dam;Rick Wagner,"Researchers across the globe are seeking to rapidly repurpose existing drugs or discover new drugs to counter the the novel coronavirus disease (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). One promising approach is to train machine learning (ML) and artificial intelligence (AI) tools to screen large numbers of small molecules. As a contribution to that effort, we are aggregating numerous small molecules from a variety of sources, using high-performance computing (HPC) to computer diverse properties of those molecules, using the computed properties to train ML/AI models, and then using the resulting models for screening. In this first data release, we make available 23 datasets collected from community sources representing over 4.2 B molecules enriched with pre-computed: 1) molecular fingerprints to aid similarity searches, 2) 2D images of molecules to enable exploration and application of image-based deep learning methods, and 3) 2D and 3D molecular descriptors to speed development of machine learning models. This data release encompasses structural information on the 4.2 B molecules and 60 TB of pre-computed data. Future releases will expand the data to include more detailed molecular simulations, computed models, and other products. △ Less","27 May, 2020",https://arxiv.org/pdf/2006.02431
"Mapping the co-evolution of artificial intelligence, robotics, and the internet of things over 20 years (1998-2017)",Katy Börner;Olga Scrivner;Leonard E. Cross;Michael Gallant;Shutian Ma;Adam S. Martin;Elizabeth Record;Haici Yang;Jonathan M. Dilger,"Understanding the emergence, co-evolution, and convergence of science and technology (S&T) areas offers competitive intelligence for researchers, managers, policy makers, and others. The resulting data-driven decision support helps set proper research and development (R&D) priorities; develop future S&T investment strategies; monitor key authors, organizations, or countries; perform effective research program assessment; and implement cutting-edge education/training efforts. This paper presents new funding, publication, and scholarly network metrics and visualizations that were validated via expert surveys. The metrics and visualizations exemplify the emergence and convergence of three areas of strategic interest: artificial intelligence (AI), robotics, and internet of things (IoT) over the last 20 years (1998-2017). For 32,716 publications and 4,497 NSF awards, we identify their conceptual space (using the UCSD map of science), geospatial network, and co-evolution landscape. The findings demonstrate how the transition of knowledge (through cross-discipline publications and citations) and the emergence of new concepts (through term bursting) create a tangible potential for interdisciplinary research and new disciplines. △ Less","3 June, 2020",https://arxiv.org/pdf/2006.02366
Communication-Computation Trade-Off in Resource-Constrained Edge Inference,Jiawei Shao;Jun Zhang,"The recent breakthrough in artificial intelligence (AI), especially deep neural networks (DNNs), has affected every branch of science and technology. Particularly, edge AI has been envisioned as a major application scenario to provide DNN-based services at edge devices. This article presents effective methods for edge inference at resource-constrained devices. It focuses on device-edge co-inference, assisted by an edge computing server, and investigates a critical trade-off among the computation cost of the on-device model and the communication cost of forwarding the intermediate feature to the edge server. A three-step framework is proposed for the effective inference: (1) model split point selection to determine the on-device model, (2) communication-aware model compression to reduce the on-device computation and the resulting communication overhead simultaneously, and (3) task-oriented encoding of the intermediate feature to further reduce the communication overhead. Experiments demonstrate that our proposed framework achieves a better trade-off and significantly reduces the inference latency than baseline methods. △ Less","14 October, 2020",https://arxiv.org/pdf/2006.02166
Aligning Superhuman AI with Human Behavior: Chess as a Model System,Reid McIlroy-Young;Siddhartha Sen;Jon Kleinberg;Ashton Anderson,"As artificial intelligence becomes increasingly intelligent---in some cases, achieving superhuman performance---there is growing potential for humans to learn from and collaborate with algorithms. However, the ways in which AI systems approach problems are often different from the ways people do, and thus may be uninterpretable and hard to learn from. A crucial step in bridging this gap between human and artificial intelligence is modeling the granular actions that constitute human behavior, rather than simply matching aggregate human performance. We pursue this goal in a model system with a long history in artificial intelligence: chess. The aggregate performance of a chess player unfolds as they make decisions over the course of a game. The hundreds of millions of games played online by players at every skill level form a rich source of data in which these decisions, and their exact context, are recorded in minute detail. Applying existing chess engines to this data, including an open-source implementation of AlphaZero, we find that they do not predict human moves well. We develop and introduce Maia, a customized version of Alpha-Zero trained on human chess games, that predicts human moves at a much higher accuracy than existing engines, and can achieve maximum accuracy when predicting decisions made by players at a specific skill level in a tuneable way. For a dual task of predicting whether a human will make a large mistake on the next move, we develop a deep neural network that significantly outperforms competitive baselines. Taken together, our results suggest that there is substantial promise in designing artificial intelligence systems with human collaboration in mind by first accurately modeling granular human decision-making. △ Less","14 July, 2020",https://arxiv.org/pdf/2006.01855
A network paradigm for very high capacity mobile and fixed telecommunications ecosystem sustainable evolution,Francesco Vatalaro;Gianfranco Ciccarella,"For very high capacity networks (VHC), the main objective is to improve the quality of the end-user experience. This implies compliance with key performance indicators (KPIs) required by applications. Key performance indicators at the application level are throughput, download time, round trip time, and video delay. They depend on the end-to-end connection between the server and the end-user device. For VHC networks, Telco operators must provide the required application quality. Moreover, they must meet the objectives of economic sustainability. Today, Telco operators rarely achieve the above objectives, mainly due to the push to increase the bit-rate of access networks without considering the end-to-end KPIs of the applications. The main contribution of this paper concerns the definition of a deployment framework to address performance and cost issues for VHC networks. We show three actions on which it is necessary to focus. First, limiting bit-rate through video compression. Second, contain the rate of packet loss through artificial intelligence algorithms for line stabilization. Third, reduce latency (i.e., round-trip time) with edge-cloud computing. The concerted and gradual application of these measures can allow a Telco to get out of the ultra-broadband ""trap"" of the access network, as defined in the paper. We propose to work on end-to-end optimization of the bandwidth utilization ratio. This leads to a better performance experienced by the end-user. It also allows a Telco operator to create new business models and obtain new revenue streams at a sustainable cost. To give a clear example, we describe how to realize mobile virtual and augmented reality, which is one of the most challenging future services. △ Less","7 June, 2020",https://arxiv.org/pdf/2006.01674
"BWCNN: Blink to Word, a Real-Time Convolutional Neural Network Approach",Albara Ah Ramli;Rex Liu;Rahul Krishnamoorthy;Vishal I B;Xiaoxiao Wang;Ilias Tagkopoulos;Xin Liu,"Amyotrophic lateral sclerosis (ALS) is a progressive neurodegenerative disease of the brain and the spinal cord, which leads to paralysis of motor functions. Patients retain their ability to blink, which can be used for communication. Here, We present an Artificial Intelligence (AI) system that uses eye-blinks to communicate with the outside world, running on real-time Internet-of-Things (IoT) devices. The system uses a Convolutional Neural Network (CNN) to find the blinking pattern, which is defined as a series of Open and Closed states. Each pattern is mapped to a collection of words that manifest the patient's intent. To investigate the best trade-off between accuracy and latency, we investigated several Convolutional Network architectures, such as ResNet, SqueezeNet, DenseNet, and InceptionV3, and evaluated their performance. We found that the InceptionV3 architecture, after hyper-parameter fine-tuning on the specific task led to the best performance with an accuracy of 99.20% and 94ms latency. This work demonstrates how the latest advances in deep learning architectures can be adapted for clinical systems that ameliorate the patient's quality of life regardless of the point-of-care. △ Less","1 June, 2020",https://arxiv.org/pdf/2006.01232
Crowd simulation for crisis management: the outcomes of the last decade,George Sidiropoulos;Chairi Kiourt;Lefteris Moussiades,"The last few decades, crowd simulation for crisis management is highlighted as an important topic of interest for many scientific fields. As the continues evolution of computational resources increases, along with the capabilities of Artificial Intelligence, the demand for better and more realistic simulation has become more attractive and popular to scientists. Along those years, there have been published hundreds of research articles and have been created numerous different systems that aim to simulate crowd behaviors, crisis cases and emergency evacuation scenarios. For better outcomes, recent research has focused on the separation of the problem of crisis management, to multiple research sub-fields (categories), such as the navigation of the simulated pedestrians, their psychology, the group dynamics etc. There have been extended research works suggesting new methods and techniques for those categories of problems. In this paper, we propose three main research categories, each one consist of several sub-categories, relying on crowd simulation for crisis management aspects and we present the outcomes of the last decade, focusing mostly on works exploiting multi-agent technologies. We analyze a number of technologies, methodologies, techniques, tools and systems introduced throughout the last years. A comparative review and discussion of the proposed categories is presented towards the identification of the most efficient aspects of the proposed categories. A general framework, towards the future crowd simulation for crisis management is presented based on the most efficient to yield the most realistic outcomes of the last decades. The paper is concluded with some highlights and open questions for future directions. △ Less","7 July, 2020",https://arxiv.org/pdf/2006.01216
Should artificial agents ask for help in human-robot collaborative problem-solving?,Adrien Bennetot;Vicky Charisi;Natalia Díaz-Rodríguez,"Transferring as fast as possible the functioning of our brain to artificial intelligence is an ambitious goal that would help advance the state of the art in AI and robotics. It is in this perspective that we propose to start from hypotheses derived from an empirical study in a human-robot interaction and to verify if they are validated in the same way for children as for a basic reinforcement learning algorithm. Thus, we check whether receiving help from an expert when solving a simple close-ended task (the Towers of Hanoï) allows to accelerate or not the learning of this task, depending on whether the intervention is canonical or requested by the player. Our experiences have allowed us to conclude that, whether requested or not, a Q-learning algorithm benefits in the same way from expert help as children do. △ Less","25 May, 2020",https://arxiv.org/pdf/2006.00882
G-IDS: Generative Adversarial Networks Assisted Intrusion Detection System,Md Hasan Shahriar;Nur Imtiazul Haque;Mohammad Ashiqur Rahman;Miguel Alonso Jr,"The boundaries of cyber-physical systems (CPS) and the Internet of Things (IoT) are converging together day by day to introduce a common platform on hybrid systems. Moreover, the combination of artificial intelligence (AI) with CPS creates a new dimension of technological advancement. All these connectivity and dependability are creating massive space for the attackers to launch cyber attacks. To defend against these attacks, intrusion detection system (IDS) has been widely used. However, emerging CPS technologies suffer from imbalanced and missing sample data, which makes the training of IDS difficult. In this paper, we propose a generative adversarial network (GAN) based intrusion detection system (G-IDS), where GAN generates synthetic samples, and IDS gets trained on them along with the original ones. G-IDS also fixes the difficulties of imbalanced or missing data problems. We model a network security dataset for an emerging CPS using NSL KDD-99 dataset and evaluate our proposed model's performance using different metrics. We find that our proposed G-IDS model performs much better in attack detection and model stabilization during the training process than a standalone IDS. △ Less","31 May, 2020",https://arxiv.org/pdf/2006.00676
Incentive Mechanism Design for Resource Sharing in Collaborative Edge Learning,Wei Yang Bryan Lim;Jer Shyuan Ng;Zehui Xiong;Dusit Niyato;Cyril Leung;Chunyan Miao;Qiang Yang,"In 5G and Beyond networks, Artificial Intelligence applications are expected to be increasingly ubiquitous. This necessitates a paradigm shift from the current cloud-centric model training approach to the Edge Computing based collaborative learning scheme known as edge learning, in which model training is executed at the edge of the network. In this article, we first introduce the principles and technologies of collaborative edge learning. Then, we establish that a successful, scalable implementation of edge learning requires the communication, caching, computation, and learning resources (3C-L) of end devices and edge servers to be leveraged jointly in an efficient manner. However, users may not consent to contribute their resources without receiving adequate compensation. In consideration of the heterogeneity of edge nodes, e.g., in terms of available computation resources, we discuss the challenges of incentive mechanism design to facilitate resource sharing for edge learning. Furthermore, we present a case study involving optimal auction design using Deep Learning to price fresh data contributed for edge learning. The performance evaluation shows the revenue maximizing properties of our proposed auction over the benchmark schemes. △ Less","31 May, 2020",https://arxiv.org/pdf/2006.00511
Complex Sequential Understanding through the Awareness of Spatial and Temporal Concepts,Bo Pang;Kaiwen Zha;Hanwen Cao;Jiajun Tang;Minghui Yu;Cewu Lu,"Understanding sequential information is a fundamental task for artificial intelligence. Current neural networks attempt to learn spatial and temporal information as a whole, limited their abilities to represent large scale spatial representations over long-range sequences. Here, we introduce a new modeling strategy called Semi-Coupled Structure (SCS), which consists of deep neural networks that decouple the complex spatial and temporal concepts learning. Semi-Coupled Structure can learn to implicitly separate input information into independent parts and process these parts respectively. Experiments demonstrate that a Semi-Coupled Structure can successfully annotate the outline of an object in images sequentially and perform video action recognition. For sequence-to-sequence problems, a Semi-Coupled Structure can predict future meteorological radar echo images based on observed images. Taken together, our results demonstrate that a Semi-Coupled Structure has the capacity to improve the performance of LSTM-like models on large scale sequential tasks. △ Less","30 May, 2020",https://arxiv.org/pdf/2006.00212
Explanations of Black-Box Model Predictions by Contextual Importance and Utility,Sule Anjomshoae;Kary Främling;Amro Najjar,"The significant advances in autonomous systems together with an immensely wider application domain have increased the need for trustable intelligent systems. Explainable artificial intelligence is gaining considerable attention among researchers and developers to address this requirement. Although there is an increasing number of works on interpretable and transparent machine learning algorithms, they are mostly intended for the technical users. Explanations for the end-user have been neglected in many usable and practical applications. In this work, we present the Contextual Importance (CI) and Contextual Utility (CU) concepts to extract explanations that are easily understandable by experts as well as novice users. This method explains the prediction results without transforming the model into an interpretable one. We present an example of providing explanations for linear and non-linear models to demonstrate the generalizability of the method. CI and CU are numerical values that can be represented to the user in visuals and natural language form to justify actions and explain reasoning for individual instances, situations, and contexts. We show the utility of explanations in car selection example and Iris flower classification by presenting complete (i.e. the causes of an individual prediction) and contrastive explanation (i.e. contrasting instance against the instance of interest). The experimental results show the feasibility and validity of the provided explanation methods. △ Less","30 May, 2020",https://arxiv.org/pdf/2006.00199
Explainable Artificial Intelligence: a Systematic Review,Giulia Vilone;Luca Longo,"Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions. △ Less","12 October, 2020",https://arxiv.org/pdf/2006.00093
A Definition and Framework for Vehicular Knowledge Networking,Duncan Deveaux;Takamasa Higuchi;Seyhan Uçar;Jérôme Härri;Onur Altintas,"To operate intelligent vehicular applications such as automated driving, machine learning, artificial intelligence and other mechanisms are used to abstract from information what is commonly referred to as knowledge. Defined as a state of understanding obtained through experience and analysis of collected information, knowledge is promising for vehicular applications. However, it lacks a unified framework to be cooperatively created and shared to achieve its full potential. This paper investigates on the meaning and scope of knowledge applied to vehicular networks, and suggests a structure for vehicular knowledge description, storage and sharing. Through the example of passenger comfort-based rerouting, it exposes the potential benefits for network load and delay of such knowledge structuring. △ Less","29 May, 2020",https://arxiv.org/pdf/2005.14505
Unlucky Explorer: A Complete non-Overlapping Map Exploration,Mohammad Sina Kiarostami;Saleh Khalaj Monfared;Mohammadreza Daneshvaramoli;Ali Oliayi;Negar Yousefian;Dara Rahmati;Saeid Gorgin,"Nowadays, the field of Artificial Intelligence in Computer Games (AI in Games) is going to be more alluring since computer games challenge many aspects of AI with a wide range of problems, particularly general problems. One of these kinds of problems is Exploration, which states that an unknown environment must be explored by one or several agents. In this work, we have first introduced the Maze Dash puzzle as an exploration problem where the agent must find a Hamiltonian Path visiting all the cells. Then, we have investigated to find suitable methods by a focus on Monte-Carlo Tree Search (MCTS) and SAT to solve this puzzle quickly and accurately. An optimization has been applied to the proposed MCTS algorithm to obtain a promising result. Also, since the prefabricated test cases of this puzzle are not large enough to assay the proposed method, we have proposed and employed a technique to generate solvable test cases to evaluate the approaches. Eventually, the MCTS-based method has been assessed by the auto-generated test cases and compared with our implemented SAT approach that is considered a good rival. Our comparison indicates that the MCTS-based approach is an up-and-coming method that could cope with the test cases with small and medium sizes with faster run-time compared to SAT. However, for certain discussed reasons, including the features of the problem, tree search organization, and also the approach of MCTS in the Simulation step, MCTS takes more time to execute in Large size scenarios. Consequently, we have found the bottleneck for the MCTS-based method in significant test cases that could be improved in two real-world problems. △ Less","28 May, 2020",https://arxiv.org/pdf/2005.14156
Early Screening of SARS-CoV-2 by Intelligent Analysis of X-Ray Images,D. Gil;K. Díaz-Chito;C. Sánchez;A. Hernández-Sabaté,"Future SARS-CoV-2 virus outbreak COVID-XX might possibly occur during the next years. However the pathology in humans is so recent that many clinical aspects, like early detection of complications, side effects after recovery or early screening, are currently unknown. In spite of the number of cases of COVID-19, its rapid spread putting many sanitary systems in the edge of collapse has hindered proper collection and analysis of the data related to COVID-19 clinical aspects. We describe an interdisciplinary initiative that integrates clinical research, with image diagnostics and the use of new technologies such as artificial intelligence and radiomics with the aim of clarifying some of SARS-CoV-2 open questions. The whole initiative addresses 3 main points: 1) collection of standardize data including images, clinical data and analytics; 2) COVID-19 screening for its early diagnosis at primary care centers; 3) define radiomic signatures of COVID-19 evolution and associated pathologies for the early treatment of complications. In particular, in this paper we present a general overview of the project, the experimental design and first results of X-ray COVID-19 detection using a classic approach based on HoG and feature selection. Our experiments include a comparison to some recent methods for COVID-19 screening in X-Ray and an exploratory analysis of the feasibility of X-Ray COVID-19 screening. Results show that classic approaches can outperform deep-learning methods in this experimental setting, indicate the feasibility of early COVID-19 screening and that non-COVID infiltration is the group of patients most similar to COVID-19 in terms of radiological description of X-ray. Therefore, an efficient COVID-19 screening should be complemented with other clinical data to better discriminate these cases. △ Less","28 May, 2020",https://arxiv.org/pdf/2005.13928
"The Adversarial Resilience Learning Architecture for AI-based Modelling, Exploration, and Operation of Complex Cyber-Physical Systems",Eric MSP Veith;Nils Wenninghoff;Emilie Frost,"Modern algorithms in the domain of Deep Reinforcement Learning (DRL) demonstrated remarkable successes; most widely known are those in game-based scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time strategy game. However, applications in the domain of modern Cyber-Physical Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We assume that the benefits would be considerable: Modern CPS have become increasingly complex and evolved beyond traditional methods of modelling and analysis. At the same time, these CPS are confronted with an increasing amount of stochastic inputs, from volatile energy sources in power grids to broad user participation stemming from markets. Approaches of system modelling that use techniques from the domain of Artificial Intelligence (AI) do not focus on analysis and operation. In this paper, we describe the concept of Adversarial Resilience Learning (ARL) that formulates a new approach to complex environment checking and resilient operation: It defines two agent classes, attacker and defender agents. The quintessence of ARL lies in both agents exploring the system and training each other without any domain knowledge. Here, we introduce the ARL software architecture that allows to use a wide range of model-free as well as model-based DRL-based algorithms, and document results of concrete experiment runs on a complex power grid. △ Less","27 May, 2020",https://arxiv.org/pdf/2005.13601
Med-BERT: pre-trained contextualized embeddings on large-scale structured electronic health records for disease prediction,Laila Rasmy;Yang Xiang;Ziqian Xie;Cui Tao;Degui Zhi,"Deep learning (DL) based predictive models from electronic health records (EHR) deliver impressive performance in many clinical tasks. Large training cohorts, however, are often required to achieve high accuracy, hindering the adoption of DL-based models in scenarios with limited training data size. Recently, bidirectional encoder representations from transformers (BERT) and related models have achieved tremendous successes in the natural language processing domain. The pre-training of BERT on a very large training corpus generates contextualized embeddings that can boost the performance of models trained on smaller datasets. We propose Med-BERT, which adapts the BERT framework for pre-training contextualized embedding models on structured diagnosis data from 28,490,650 patients EHR dataset. Fine-tuning experiments are conducted on two disease-prediction tasks: (1) prediction of heart failure in patients with diabetes and (2) prediction of pancreatic cancer from two clinical databases. Med-BERT substantially improves prediction accuracy, boosting the area under receiver operating characteristics curve (AUC) by 2.02-7.12%. In particular, pre-trained Med-BERT substantially improves the performance of tasks with very small fine-tuning training sets (300-500 samples) boosting the AUC by more than 20% or equivalent to the AUC of 10 times larger training set. We believe that Med-BERT will benefit disease-prediction studies with small local training datasets, reduce data collection expenses, and accelerate the pace of artificial intelligence aided healthcare. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.12833
Wind Speed Prediction and Visualization Using Long Short-Term Memory Networks (LSTM),Md Amimul Ehsan;Amir Shahirinia;Nian Zhang;Timothy Oladunni,"Climate change is one of the most concerning issues of this century. Emission from electric power generation is a crucial factor that drives the concern to the next level. Renewable energy sources are widespread and available globally, however, one of the major challenges is to understand their characteristics in a more informative way. This paper proposes the prediction of wind speed that simplifies wind farm planning and feasibility study. Twelve artificial intelligence algorithms were used for wind speed prediction from collected meteorological parameters. The model performances were compared to determine the wind speed prediction accuracy. The results show a deep learning approach, long short-term memory (LSTM) outperforms other models with the highest accuracy of 97.8%. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.12401
A Big Data Based Framework for Executing Complex Query Over COVID-19 Datasets (COVID-QF),Eman A. Khashan;Ali I. Eldesouky;M. Fadel;Sally M. Elghamrawy,"COVID-19's rapid global spread has driven innovative tools for Big Data Analytics. These have guided organizations in all fields of the health industry to track and minimized the effects of virus. Researchers are required to detect coronaviruses through artificial intelligence, machine learning, and natural language processing, and to gain a complete understanding of the disease. COVID-19 takes place in different countries in the world, with which only big data application and the work of NOSQL databases are suitable. There is a great number of platforms used for processing NOSQL Databases model like: Spark, H2O and Hadoop HDFS/MapReduce, which are proper to control and manage the enormous amount of data. Many challenges faced by large applications programmers, especially those that work on the COVID-19 databases through hybrid data models through different APIs and query. In this context, this paper proposes a storage framework to handle both SQL and NOSQL databases named (COVID-QF) for COVID-19 datasets in order to treat and handle the problems caused by virus spreading worldwide by reducing treatment times. In case of NoSQL database, COVID-QF uses Hadoop HDFS/Map Reduce and Apache Spark. The COVID-QF consists of three Layers: data collection layer, storage layer, and query Processing layer. The data is collected in the data collection layer. The storage layer divides data into collection of data-saving and processing blocks, and it connects the Connector of the spark with different databases engine to reduce time of saving and retrieving. While the Processing layer executes the request query and sends results. The proposed framework used three datasets increased for time for COVID-19 data (COVID-19-Merging, COVID-19-inside-Hubei and COVID-19-ex-Hubei) to test experiments of this study. The results obtained insure the superiority of the COVID-QF framework. △ Less","24 May, 2020",https://arxiv.org/pdf/2005.12271
Artificial Intelligence (AI) and IT identity: Antecedents Identifying with AI Applications,Rasha Alahmad;Lionel Robert,"In the age of Artificial Intelligence and automation, machines have taken over many key managerial tasks. Replacing managers with AI systems may have a negative impact on workers outcomes. It is unclear if workers receive the same benefits from their relationships with AI systems, raising the question: What degree does the relationship between AI systems and workers impact worker outcomes? We draw on IT identity to understand the influence of identification with AI systems on job performance. From this theoretical perspective, we propose a research model and conduct a survey of 97 MTurk workers to test the model. The findings reveal that work role identity and organizational identity are key determinants of identification with AI systems. Furthermore, the findings show that identification with AI systems does increase job performance. △ Less","15 May, 2020",https://arxiv.org/pdf/2005.12196
The challenges of deploying artificial intelligence models in a rapidly evolving pandemic,Yipeng Hu;Joseph Jacob;Geoffrey JM Parker;David J Hawkes;John R Hurst;Danail Stoyanov,"The COVID-19 pandemic, caused by the severe acute respiratory syndrome coronavirus 2, emerged into a world being rapidly transformed by artificial intelligence (AI) based on big data, computational power and neural networks. The gaze of these networks has in recent years turned increasingly towards applications in healthcare. It was perhaps inevitable that COVID-19, a global disease propagating health and economic devastation, should capture the attention and resources of the world's computer scientists in academia and industry. The potential for AI to support the response to the pandemic has been proposed across a wide range of clinical and societal challenges, including disease forecasting, surveillance and antiviral drug discovery. This is likely to continue as the impact of the pandemic unfolds on the world's people, industries and economy but a surprising observation on the current pandemic has been the limited impact AI has had to date in the management of COVID-19. This correspondence focuses on exploring potential reasons behind the lack of successful adoption of AI models developed for COVID-19 diagnosis and prognosis, in front-line healthcare services. We highlight the moving clinical needs that models have had to address at different stages of the epidemic, and explain the importance of translating models to reflect local healthcare environments. We argue that both basic and applied research are essential to accelerate the potential of AI models, and this is particularly so during a rapidly evolving pandemic. This perspective on the response to COVID-19, may provide a glimpse into how the global scientific community should react to combat future disease outbreaks more effectively. △ Less","19 May, 2020",https://arxiv.org/pdf/2005.12137
An interpretable automated detection system for FISH-based HER2 oncogene amplification testing in histo-pathological routine images of breast and gastric cancer diagnostics,Sarah Schmell;Falk Zakrzewski;Walter de Back;Martin Weigert;Uwe Schmidt;Torsten Wenke;Silke Zeugner;Robert Mantey;Christian Sperling;Ingo Roeder;Pia Hoenscheid;Daniela Aust;Gustavo Baretton,"Histo-pathological diagnostics are an inherent part of the everyday work but are particularly laborious and associated with time-consuming manual analysis of image data. In order to cope with the increasing diagnostic case numbers due to the current growth and demographic change of the global population and the progress in personalized medicine, pathologists ask for assistance. Profiting from digital pathology and the use of artificial intelligence, individual solutions can be offered (e.g. detect labeled cancer tissue sections). The testing of the human epidermal growth factor receptor 2 (HER2) oncogene amplification status via fluorescence in situ hybridization (FISH) is recommended for breast and gastric cancer diagnostics and is regularly performed at clinics. Here, we develop an interpretable, deep learning (DL)-based pipeline which automates the evaluation of FISH images with respect to HER2 gene amplification testing. It mimics the pathological assessment and relies on the detection and localization of interphase nuclei based on instance segmentation networks. Furthermore, it localizes and classifies fluorescence signals within each nucleus with the help of image classification and object detection convolutional neural networks (CNNs). Finally, the pipeline classifies the whole image regarding its HER2 amplification status. The visualization of pixels on which the networks' decision occurs, complements an essential part to enable interpretability by pathologists. △ Less","25 May, 2020",https://arxiv.org/pdf/2005.12066
Evaluating Generalisation in General Video Game Playing,Martin Balla;Simon M. Lucas;Diego Perez-Liebana,"The General Video Game Artificial Intelligence (GVGAI) competition has been running for several years with various tracks. This paper focuses on the challenge of the GVGAI learning track in which 3 games are selected and 2 levels are given for training, while 3 hidden levels are left for evaluation. This setup poses a difficult challenge for current Reinforcement Learning (RL) algorithms, as they typically require much more data. This work investigates 3 versions of the Advantage Actor-Critic (A2C) algorithm trained on a maximum of 2 levels from the available 5 from the GVGAI framework and compares their performance on all levels. The selected sub-set of games have different characteristics, like stochasticity, reward distribution and objectives. We found that stochasticity improves the generalisation, but too much can cause the algorithms to fail to learn the training levels. The quality of the training levels also matters, different sets of training levels can boost generalisation over all levels. In the GVGAI competition agents are scored based on their win rates and then their scores achieved in the games. We found that solely using the rewards provided by the game might not encourage winning. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.11247
On the Potential of Smarter Multi-layer Maps,Francesco Verdoja;Ville Kyrki,"The most common way for robots to handle environmental information is by using maps. At present, each kind of data is hosted on a separate map, which complicates planning because a robot attempting to perform a task needs to access and process information from many different maps. Also, most often correlation among the information contained in maps obtained from different sources is not evaluated or exploited. In this paper, we argue that in robotics a shift from single-source maps to a multi-layer mapping formalism has the potential to revolutionize the way robots interact with knowledge about their environment. This observation stems from the raise in metric-semantic mapping research, but expands to include in its formulation also layers containing other information sources, e.g., people flow, room semantic, or environment topology. Such multi-layer maps, here named hypermaps, not only can ease processing spatial data information but they can bring added benefits arising from the interaction between maps. We imagine that a new research direction grounded in such multi-layer mapping formalism for robots can use artificial intelligence to process the information it stores to present to the robot task-specific information simplifying planning and bringing us one step closer to high-level reasoning in robots. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.11094
Regulating Artificial Intelligence: Proposal for a Global Solution,Olivia J. Erdélyi;Judy Goldsmith,"With increasing ubiquity of artificial intelligence (AI) in modern societies, individual countries and the international community are working hard to create an innovation-friendly, yet safe, regulatory environment. Adequate regulation is key to maximize the benefits and minimize the risks stemming from AI technologies. Developing regulatory frameworks is, however, challenging due to AI's global reach and the existence of widespread misconceptions about the notion of regulation. We argue that AI-related challenges cannot be tackled effectively without sincere international coordination supported by robust, consistent domestic and international governance arrangements. Against this backdrop, we propose the establishment of an international AI governance framework organized around a new AI regulatory agency that -- drawing on interdisciplinary expertise -- could help creating uniform standards for the regulation of AI technologies and inform the development of AI policies around the world. We also believe that a fundamental change of mindset on what constitutes regulation is necessary to remove existing barriers that hamper contemporary efforts to develop AI regulatory regimes, and put forward some recommendations on how to achieve this, and what opportunities doing so would present. △ Less","22 May, 2020",https://arxiv.org/pdf/2005.11072
Deploying Scientific AI Networks at Petaflop Scale on Secure Large Scale HPC Production Systems with Containers,David Brayford;Sofia Vallercorsa,"There is an ever-increasing need for computational power to train complex artificial intelligence (AI) & machine learning (ML) models to tackle large scientific problems. High performance computing (HPC) resources are required to efficiently compute and scale complex models across tens of thousands of compute nodes. In this paper, we discuss the issues associated with the deployment of machine learning frameworks on large scale secure HPC systems and how we successfully deployed a standard machine learning framework on a secure large scale HPC production system, to train a complex three-dimensional convolutional GAN (3DGAN), with petaflop performance. 3DGAN is an example from the high energy physics domain, designed to simulate the energy pattern produced by showers of secondary particles inside a particle detector on various HPC systems. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.10676
A Robust Interpretable Deep Learning Classifier for Heart Anomaly Detection Without Segmentation,Theekshana Dissanayake;Tharindu Fernando;Simon Denman;Sridha Sridharan;Houman Ghaemmaghami;Clinton Fookes,"Traditionally, abnormal heart sound classification is framed as a three-stage process. The first stage involves segmenting the phonocardiogram to detect fundamental heart sounds; after which features are extracted and classification is performed. Some researchers in the field argue the segmentation step is an unwanted computational burden, whereas others embrace it as a prior step to feature extraction. When comparing accuracies achieved by studies that have segmented heart sounds before analysis with those who have overlooked that step, the question of whether to segment heart sounds before feature extraction is still open. In this study, we explicitly examine the importance of heart sound segmentation as a prior step for heart sound classification, and then seek to apply the obtained insights to propose a robust classifier for abnormal heart sound detection. Furthermore, recognizing the pressing need for explainable Artificial Intelligence (AI) models in the medical domain, we also unveil hidden representations learned by the classifier using model interpretation techniques. Experimental results demonstrate that the segmentation plays an essential role in abnormal heart sound classification. Our new classifier is also shown to be robust, stable and most importantly, explainable, with an accuracy of almost 100% on the widely used PhysioNet dataset. △ Less","29 September, 2020",https://arxiv.org/pdf/2005.10480
Automated Question Answer medical model based on Deep Learning Technology,Abdelrahman Abdallah;Mahmoud Kasem;Mohamed Hamada;Shaymaa Sdeek,"Artificial intelligence can now provide more solutions for different problems, especially in the medical field. One of those problems the lack of answers to any given medical/health-related question. The Internet is full of forums that allow people to ask some specific questions and get great answers for them. Nevertheless, browsing these questions in order to locate one similar to your own, also finding a satisfactory answer is a difficult and time-consuming task. This research will introduce a solution to this problem by automating the process of generating qualified answers to these questions and creating a kind of digital doctor. Furthermore, this research will train an end-to-end model using the framework of RNN and the encoder-decoder to generate sensible and useful answers to a small set of medical/health issues. The proposed model was trained and evaluated using data from various online services, such as WebMD, HealthTap, eHealthForums, and iCliniq. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.10416
Safeguarding MIMO Communications with Reconfigurable Metasurfaces and Artificial Noise,George C. Alexandropoulos;Konstantinos Katsanos;Miaowen Wen;Daniel B. da Costa,"Wireless communications empowered by Reconfigurable Intelligent (meta)Surfaces (RISs) are recently gaining remarkable research attention due to the increased system design flexibility offered by RISs for diverse functionalities. In this paper, we consider a Multiple Input Multiple Output (MIMO) physical layer security system with multiple data streams including one legitimate and one eavesdropping passive RISs, with the former being transparent to the eavesdropper and the latter's presence being unknown at the legitimate link. We first focus on the eavesdropping subsystem and present a joint design framework for the eavesdropper's combining vector and the reflection coefficients of the eavesdropping RIS. Then, focusing on the secrecy rate maximization, we propose a physical layer security scheme that jointly designs the legitimate precoding vector and the Artificial Noise (AN) covariance matrix, as well as the legitimate combining vector and the reflection coefficients of the legitimate RIS. Our simulation results reveal that, in the absence of a legitimate RIS, transceiver spatial filtering and AN are incapable of offering nonzero secrecy rates, even for eavesdropping RISs with small numbers of elements. However, when a L-element legitimate RIS is deployed, confidential communication can be safeguarded against cases with even more than a 5L-element eavesdropping RIS. △ Less","2 November, 2020",https://arxiv.org/pdf/2005.10062
Data-Importance Aware Radio Resource Allocation: Wireless Communication Helps Machine Learning,Yuan Liu;Zhi Zeng;Weijun Tang;Fangjiong Chen,"The rich mobile data and edge computing enabled wireless networks motivate to deploy artificial intelligence (AI) at network edge, known as \emph{edge AI}, which integrates wireless communication and machine learning. In communication, data bits are equally important, while in machine learning some data bits are more important. Therefore we can allocate more radio resources to the more important data and allocate less radio resources to the less important data, so as to efficiently utilize the limited radio resources. To this end, how to define ""more or less important"" of data is the key problem. In this article, we propose two importance criteria to differentiate data's importance based on their effects on machine learning, one for centralized edge machine learning and the other for distributed edge machine learning. Then, the corresponding radio resource allocation schemes are proposed to improve performance of machine learning. Extensive experiments are conducted for verifying the effectiveness of the proposed data-importance aware radio resource allocation schemes. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.09868
Automated Copper Alloy Grain Size Evaluation Using a Deep-learning CNN,George S. Baggs;Paul Guerrier;Andrew Loeb;Jason C. Jones,"Moog Inc. has automated the evaluation of copper (Cu) alloy grain size using a deep-learning convolutional neural network (CNN). The proof-of-concept automated image acquisition and batch-wise image processing offers the potential for significantly reduced labor, improved accuracy of grain evaluation, and decreased overall turnaround times for approving Cu alloy bar stock for use in flight critical aircraft hardware. A classification accuracy of 91.1% on individual sub-images of the Cu alloy coupons was achieved. Process development included minimizing the variation in acquired image color, brightness, and resolution to create a dataset with 12300 sub-images, and then optimizing the CNN hyperparameters on this dataset using statistical design of experiments (DoE). Over the development of the automated Cu alloy grain size evaluation, a degree of ""explainability"" in the artificial intelligence (XAI) output was realized, based on the decomposition of the large raw images into many smaller dataset sub-images, through the ability to explain the CNN ensemble image output via inspection of the classification results from the individual smaller sub-images. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.09634
In-memory Implementation of On-chip Trainable and Scalable ANN for AI/ML Applications,Abhash Kumar;Jawar Singh;Sai Manohar Beeraka;Bharat Gupta,"Traditional von Neumann architecture based processors become inefficient in terms of energy and throughput as they involve separate processing and memory units, also known as~\textit{memory wall}. The memory wall problem is further exacerbated when massive parallelism and frequent data movement are required between processing and memory units for real-time implementation of artificial neural network (ANN) that enables many intelligent applications. One of the most promising approach to address the memory wall problem is to carry out computations inside the memory core itself that enhances the memory bandwidth and energy efficiency for extensive computations. This paper presents an in-memory computing architecture for ANN enabling artificial intelligence (AI) and machine learning (ML) applications. The proposed architecture utilizes deep in-memory architecture based on standard six transistor (6T) static random access memory (SRAM) core for the implementation of a multi-layered perceptron. Our novel on-chip training and inference in-memory architecture reduces energy cost and enhances throughput by simultaneously accessing the multiple rows of SRAM array per precharge cycle and eliminating the frequent access of data. The proposed architecture realizes backpropagation which is the keystone during the network training using newly proposed different building blocks such as weight updation, analog multiplication, error calculation, signed analog to digital conversion, and other necessary signal control units. The proposed architecture was trained and tested on the IRIS dataset which exhibits \approx46\times more energy efficient per MAC (multiply and accumulate) operation compared to earlier classifiers. △ Less","19 May, 2020",https://arxiv.org/pdf/2005.09526
Applying Genetic Programming to Improve Interpretability in Machine Learning Models,Leonardo Augusto Ferreira;Frederico Gadelha Guimarães;Rodrigo Silva,"Explainable Artificial Intelligence (or xAI) has become an important research topic in the fields of Machine Learning and Deep Learning. In this paper, we propose a Genetic Programming (GP) based approach, named Genetic Programming Explainer (GPX), to the problem of explaining decisions computed by AI systems. The method generates a noise set located in the neighborhood of the point of interest, whose prediction should be explained, and fits a local explanation model for the analyzed sample. The tree structure generated by GPX provides a comprehensible analytical, possibly non-linear, symbolic expression which reflects the local behavior of the complex model. We considered three machine learning techniques that can be recognized as complex black-box models: Random Forest, Deep Neural Network and Support Vector Machine in twenty data sets for regression and classifications problems. Our results indicate that the GPX is able to produce more accurate understanding of complex models than the state of the art. The results validate the proposed approach as a novel way to deploy GP to improve interpretability. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.09512
"The Skincare project, an interactive deep learning system for differential diagnosis of malignant skin lesions. Technical Report",Daniel Sonntag;Fabrizio Nunnari;Hans-Jürgen Profitlich,"A shortage of dermatologists causes long wait times for patients who seek dermatologic care. In addition, the diagnostic accuracy of general practitioners has been reported to be lower than the accuracy of artificial intelligence software. This article describes the Skincare project (H2020, EIT Digital). Contributions include enabling technology for clinical decision support based on interactive machine learning (IML), a reference architecture towards a Digital European Healthcare Infrastructure (also cf. EIT MCPS), technical components for aggregating digitised patient information, and the integration of decision support technology into clinical test-bed environments. However, the main contribution is a diagnostic and decision support system in dermatology for patients and doctors, an interactive deep learning system for differential diagnosis of malignant skin lesions. In this article, we describe its functionalities and the user interfaces to facilitate machine learning from human input. The baseline deep learning system, which delivers state-of-the-art results and the potential to augment general practitioners and even dermatologists, was developed and validated using de-identified cases from a dermatology image data base (ISIC), which has about 20000 cases for development and validation, provided by board-certified dermatologists defining the reference standard for every case. ISIC allows for differential diagnosis, a ranked list of eight diagnoses, that is used to plan treatments in the common setting of diagnostic ambiguity. We give an overall description of the outcome of the Skincare project, and we focus on the steps to support communication and coordination between humans and machine in IML. This is an integral part of the development of future cognitive assistants in the medical domain, and we describe the necessary intelligent user interfaces. △ Less","19 May, 2020",https://arxiv.org/pdf/2005.09448
Controlled Language and Baby Turing Test for General Conversational Intelligence,Anton Kolonin,"General conversational intelligence appears to be an important part of artificial general intelligence. Respectively, it requires accessible measures of the intelligence quality and controllable ways of its achievement, ideally - having the linguistic and semantic models represented in a reasonable way. Our work is suggesting to use Baby Turing Test approach to extend the classic Turing Test for conversational intelligence and controlled language based on semantic graph representation extensible for arbitrary subject domain. We describe how the two can be used together to build a general-purpose conversational system such as an intelligent assistant for online media and social network data processing. △ Less","19 May, 2020",https://arxiv.org/pdf/2005.09280
Scaling Exact Inference for Discrete Probabilistic Programs,Steven Holtzen;Guy Van den Broeck;Todd Millstein,"Probabilistic programming languages (PPLs) are an expressive means of representing and reasoning about probabilistic models. The computational challenge of probabilistic inference remains the primary roadblock for applying PPLs in practice. Inference is fundamentally hard, so there is no one-size-fits all solution. In this work, we target scalable inference for an important class of probabilistic programs: those whose probability distributions are discrete. Discrete distributions are common in many fields, including text analysis, network verification, artificial intelligence, and graph analysis, but they prove to be challenging for existing PPLs. We develop a domain-specific probabilistic programming language called Dice that features a new approach to exact discrete probabilistic program inference. Dice exploits program structure in order to factorize inference, enabling us to perform exact inference on probabilistic programs with hundreds of thousands of random variables. Our key technical contribution is a new reduction from discrete probabilistic programs to weighted model counting (WMC). This reduction separates the structure of the distribution from its parameters, enabling logical reasoning tools to exploit that structure for probabilistic inference. We (1) show how to compositionally reduce Dice inference to WMC, (2) prove this compilation correct with respect to a denotational semantics, (3) empirically demonstrate the performance benefits over prior approaches, and (4) analyze the types of structure that allow Dice to scale to large probabilistic programs. △ Less","16 October, 2020",https://arxiv.org/pdf/2005.09089
An Artificial-intelligence/Statistics Solution to Quantify Material Distortion for Thermal Compensation in Additive Manufacturing,Chao Wang;Shaofan Li;Danielle Zeng;Xinhai Zhu,"In this paper, we introduce a probabilistic statistics solution or artificial intelligence (AI) approach to identify and quantify permanent (non-zero strain) continuum/material deformation only based on the scanned material data in the spatial configuration and the shape of the initial design configuration or the material configuration. The challenge of this problem is that we only know the scanned material data in the spatial configuration and the shape of the design configuration of three-dimensional (3D) printed products, whereas for a specific scanned material point we do not know its corresponding material coordinates in the initial or designed referential configuration, provided that we do not know the detailed information on actual physical deformation process. Different from physics-based modeling, the method developed here is a data-driven artificial intelligence method, which solves the problem with incomplete deformation data or with missing information of actual physical deformation process. We coined the method is an AI-based material deformation finding algorithm. This method has practical significance and important applications in finding and designing thermal compensation configuration of a 3D printed product in additive manufacturing, which is at the heart of the cutting edge 3D printing technology. In this paper, we demonstrate that the proposed AI continuum/material deformation finding approach can accurately find permanent thermal deformation configuration for a complex 3D printed structure component, and hence to identify the thermal compensation design configuration in order to minimizing the impact of temperature fluctuations on 3D printed structure components that are sensitive to changes of temperature. △ Less","14 May, 2020",https://arxiv.org/pdf/2005.09084
Ethical Issues Regarding the Use of AI Profiling Services for Recruiting: The Japanese Rikunabi Data Scandal,Kudo Fumiko;Hiromi Arai;Arisa Ema,"The ethical, legal, and social challenges involved in the use of profiling services for recruitment are the focus of many previous studies; however, the processes vary depending on the social system and cultural practices. In August 2019, a scandal occurred in Japan in which a recruitment management company was found to have breached users' and students' trust by selling their data to clients. By sharing the Japanese recruitment context and associated laws, this article contributes to our understanding of the ethical issues involved in artificial intelligence profiling and in handling sensitive personal information. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.08663
Brain-inspired Distributed Cognitive Architecture,Leendert A Remmelzwaal;Amit K Mishra;George F R Ellis,"In this paper we present a brain-inspired cognitive architecture that incorporates sensory processing, classification, contextual prediction, and emotional tagging. The cognitive architecture is implemented as three modular web-servers, meaning that it can be deployed centrally or across a network for servers. The experiments reveal two distinct operations of behaviour, namely high- and low-salience modes of operations, which closely model attention in the brain. In addition to modelling the cortex, we have demonstrated that a bio-inspired architecture introduced processing efficiencies. The software has been published as an open source platform, and can be easily extended by future research teams. This research lays the foundations for bio-realistic attention direction and sensory selection, and we believe that it is a key step towards achieving a bio-realistic artificial intelligent system. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.08603
Imposing Regulation on Advanced Algorithms,Fotios Fitsilis,"This book discusses the necessity and perhaps urgency for the regulation of algorithms on which new technologies rely; technologies that have the potential to re-shape human societies. From commerce and farming to medical care and education, it is difficult to find any aspect of our lives that will not be affected by these emerging technologies. At the same time, artificial intelligence, deep learning, machine learning, cognitive computing, blockchain, virtual reality and augmented reality, belong to the fields most likely to affect law and, in particular, administrative law. The book examines universally applicable patterns in administrative decisions and judicial rulings. First, similarities and divergence in behavior among the different cases are identified by analyzing parameters ranging from geographical location and administrative decisions to judicial reasoning and legal basis. As it turns out, in several of the cases presented, sources of general law, such as competition or labor law, are invoked as a legal basis, due to the lack of current specialized legislation. This book also investigates the role and significance of national and indeed supranational regulatory bodies for advanced algorithms and considers ENISA, an EU agency that focuses on network and information security, as an interesting candidate for a European regulator of advanced algorithms. Lastly, it discusses the involvement of representative institutions in algorithmic regulation. △ Less","16 May, 2020",https://arxiv.org/pdf/2005.08092
A Deep Learning based Wearable Healthcare IoT Device for AI-enabled Hearing Assistance Automation,Fraser Young;L Zhang;Richard Jiang;Han Liu;Conor Wall,"With the recent booming of artificial intelligence (AI), particularly deep learning techniques, digital healthcare is one of the prevalent areas that could gain benefits from AI-enabled functionality. This research presents a novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266 platform capable of assisting those who suffer from impairment of hearing or deafness to communicate with others in conversations. In the proposed solution, a server application is created that leverages Google's online speech recognition service to convert the received conversations into texts, then deployed to a micro-display attached to the glasses to display the conversation contents to deaf people, to enable and assist conversation as normal with the general population. Furthermore, in order to raise alert of traffic or dangerous scenarios, an 'urban-emergency' classifier is developed using a deep learning model, Inception-v4, with transfer learning to detect/recognize alerting/alarming sounds, such as a horn sound or a fire alarm, with texts generated to alert the prospective user. The training of Inception-v4 was carried out on a consumer desktop PC and then implemented into the AI based IoT application. The empirical results indicate that the developed prototype system achieves an accuracy rate of 92% for sound recognition and classification with real-time performance. △ Less","16 May, 2020",https://arxiv.org/pdf/2005.08076
Uncovering Gender Bias in Media Coverage of Politicians with Machine Learning,Susan Leavy,"This paper presents research uncovering systematic gender bias in the representation of political leaders in the media, using artificial intelligence. Newspaper coverage of Irish ministers over a fifteen year period was gathered and analysed with natural language processing techniques and machine learning. Findings demonstrate evidence of gender bias in the portrayal of female politicians, the kind of policies they were associated with and how they were evaluated in terms of their performance as political leaders. This paper also sets out a methodology whereby media content may be analysed on a large scale utilising techniques from artificial intelligence within a theoretical framework founded in gender theory and feminist linguistics. △ Less","15 May, 2020",https://arxiv.org/pdf/2005.07734
Face Identity Disentanglement via Latent Space Mapping,Yotam Nitzan;Amit Bermano;Yangyan Li;Daniel Cohen-Or,"Learning disentangled representations of data is a fundamental problem in artificial intelligence. Specifically, disentangled latent representations allow generative models to control and compose the disentangled factors in the synthesis process. Current methods, however, require extensive supervision and training, or instead, noticeably compromise quality. In this paper, we present a method that learns how to represent data in a disentangled way, with minimal supervision, manifested solely using available pre-trained networks. Our key insight is to decouple the processes of disentanglement and synthesis, by employing a leading pre-trained unconditional image generator, such as StyleGAN. By learning to map into its latent space, we leverage both its state-of-the-art quality, and its rich and expressive latent space, without the burden of training it. We demonstrate our approach on the complex and high dimensional domain of human heads. We evaluate our method qualitatively and quantitatively, and exhibit its success with de-identification operations and with temporal identity coherency in image sequences. Through extensive experimentation, we show that our method successfully disentangles identity from other facial attributes, surpassing existing methods, even though they require more training and supervision. △ Less","19 October, 2020",https://arxiv.org/pdf/2005.07728
Machine Learning as a Catalyst for Value-Based Health Care,Matthew G. Crowson;Timothy C. Y. Chan,"In this manuscript, we present an argument that machine learning, a subfield of artificial intelligence, can drive improvement in value-based health care through reducing error in clinical decision making. Much of what has been previously published on machine learning in medicine represent single-use or proof-of-concept cases, as well as broad reviews of the advantages and limitations of machine learning. It is timely to look at the broader strategy for artificial intelligence implementation in medicine and emphasize how machine learning can positively influence value-based care. △ Less","15 May, 2020",https://arxiv.org/pdf/2005.07534
Robot Accident Investigation: a case study in Responsible Robotics,Alan F. T. Winfield;Katie Winkle;Helena Webb;Ulrik Lyngs;Marina Jirotka;Carl Macrae,"Robot accidents are inevitable. Although rare, they have been happening since assembly-line robots were first introduced in the 1960s. But a new generation of social robots are now becoming commonplace. Often with sophisticated embedded artificial intelligence (AI) social robots might be deployed as care robots to assist elderly or disabled people to live independently. Smart robot toys offer a compelling interactive play experience for children and increasingly capable autonomous vehicles (AVs) the promise of hands-free personal transport and fully autonomous taxis. Unlike industrial robots which are deployed in safety cages, social robots are designed to operate in human environments and interact closely with humans; the likelihood of robot accidents is therefore much greater for social robots than industrial robots. This paper sets out a draft framework for social robot accident investigation; a framework which proposes both the technology and processes that would allow social robot accidents to be investigated with no less rigour than we expect of air or rail accident investigations. The paper also places accident investigation within the practice of responsible robotics, and makes the case that social robotics without accident investigation would be no less irresponsible than aviation without air accident investigation. △ Less","15 May, 2020",https://arxiv.org/pdf/2005.07474
Evolved Explainable Classifications for Lymph Node Metastases,Iam Palatnik de Sousa;Marley Maria Bernardes Rebuzzi Vellasco;Eduardo Costa da Silva,"A novel evolutionary approach for Explainable Artificial Intelligence is presented: the ""Evolved Explanations"" model (EvEx). This methodology consists in combining Local Interpretable Model Agnostic Explanations (LIME) with Multi-Objective Genetic Algorithms to allow for automated segmentation parameter tuning in image classification tasks. In this case, the dataset studied is Patch-Camelyon, comprised of patches from pathology whole slide images. A publicly available Convolutional Neural Network (CNN) was trained on this dataset to provide a binary classification for presence/absence of lymph node metastatic tissue. In turn, the classifications are explained by means of evolving segmentations, seeking to optimize three evaluation goals simultaneously. The final explanation is computed as the mean of all explanations generated by Pareto front individuals, evolved by the developed genetic algorithm. To enhance reproducibility and traceability of the explanations, each of them was generated from several different seeds, randomly chosen. The observed results show remarkable agreement between different seeds. Despite the stochastic nature of LIME explanations, regions of high explanation weights proved to have good agreement in the heat maps, as computed by pixel-wise relative standard deviations. The found heat maps coincide with expert medical segmentations, which demonstrates that this methodology can find high quality explanations (according to the evaluation metrics), with the novel advantage of automated parameter fine tuning. These results give additional insight into the inner workings of neural network black box decision making for medical data. △ Less","14 May, 2020",https://arxiv.org/pdf/2005.07229
Mitigating Gender Bias in Machine Learning Data Sets,Susan Leavy;Gerardine Meaney;Karen Wade;Derek Greene,"Artificial Intelligence has the capacity to amplify and perpetuate societal biases and presents profound ethical implications for society. Gender bias has been identified in the context of employment advertising and recruitment tools, due to their reliance on underlying language processing and recommendation algorithms. Attempts to address such issues have involved testing learned associations, integrating concepts of fairness to machine learning and performing more rigorous analysis of training data. Mitigating bias when algorithms are trained on textual data is particularly challenging given the complex way gender ideology is embedded in language. This paper proposes a framework for the identification of gender bias in training data for machine learning.The work draws upon gender theory and sociolinguistics to systematically indicate levels of bias in textual training data and associated neural word embedding models, thus highlighting pathways for both removing bias from training data and critically assessing its impact. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.06898
Large Scale Font Independent Urdu Text Recognition System,Atique Ur Rehman;Sibt Ul Hussain,"OCR algorithms have received a significant improvement in performance recently, mainly due to the increase in the capabilities of artificial intelligence algorithms. However, this advancement is not evenly distributed over all languages. Urdu is among the languages which did not receive much attention, especially in the font independent perspective. There exists no automated system that can reliably recognize printed Urdu text in images and videos across different fonts. To help bridge this gap, we have developed Qaida, a large scale data set with 256 fonts, and a complete Urdu lexicon. We have also developed a Convolutional Neural Network (CNN) based classification model which can recognize Urdu ligatures with 84.2% accuracy. Moreover, we demonstrate that our recognition network can not only recognize the text in the fonts it is trained on but can also reliably recognize text in unseen (new) fonts. To this end, this paper makes following contributions: (i) we introduce a large scale, multiple fonts based data set for printed Urdu text recognition;(ii) we have designed, trained and evaluated a CNN based model for Urdu text recognition; (iii) we experiment with incremental learning methods to produce state-of-the-art results for Urdu text recognition. All the experiment choices were thoroughly validated via detailed empirical analysis. We believe that this study can serve as the basis for further improvement in the performance of font independent Urdu OCR systems. △ Less","14 May, 2020",https://arxiv.org/pdf/2005.06752
Dense-Resolution Network for Point Cloud Classification and Segmentation,Shi Qiu;Saeed Anwar;Nick Barnes,"Point cloud analysis is attracting attention from Artificial Intelligence research since it can be widely used in applications such as robotics, Augmented Reality, self-driving. However, it is always challenging due to irregularities, unorderedness, and sparsity. In this article, we propose a novel network named Dense-Resolution Network (DRNet) for point cloud analysis. Our DRNet is designed to learn local point features from the point cloud in different resolutions. In order to learn local point groups more effectively, we present a novel grouping method for local neighborhood searching and an error-minimizing module for capturing local features. In addition to validating the network on widely used point cloud segmentation and classification benchmarks, we also test and visualize the performance of the components. Comparing with other state-of-the-art methods, our network shows superiority on ModelNet40, ShapeNet synthetic and ScanObjectNN real point cloud datasets. △ Less","17 November, 2020",https://arxiv.org/pdf/2005.06734
Smart Urban Mobility: When Mobility Systems Meet Smart Data,Zineb Mahrez;Essaid Sabir;Elarbi Badidi;Walid Saad;Mohamed Sadik,"Cities around the world are expanding dramatically, with urban population growth reaching nearly 2.5 billion people in urban areas and road traffic growth exceeding 1.2 billion cars by 2050. The economic contribution of the transport sector represents 5% of the GDP in Europe and costs an average of US $482.05 billion in the United States. These figures indicate the rapid rise of industrial cities and the urgent need to move from traditional cities to smart cities. This article provides a survey of different approaches and technologies such as intelligent transportation systems (ITS) that leverage communication technologies to help maintain road users safe while driving, as well as support autonomous mobility through the optimization of control systems. The role of ITS is strengthened when combined with accurate artificial intelligence models that are built to optimize urban planning, analyze crowd behavior and predict traffic conditions. AI-driven ITS is becoming possible thanks to the existence of a large volume of mobility data generated by billions of users through their use of new technologies and online social media. The optimization of urban planning enhances vehicle routing capabilities and solves traffic congestion problems, as discussed in this paper. From an ecological perspective, we discuss the measures and incentives provided to foster the use of mobility systems. We also underline the role of the political will in promoting open data in the transport sector, considered as an essential ingredient for developing technological solutions necessary for cities to become healthier and more sustainable. △ Less","9 May, 2020",https://arxiv.org/pdf/2005.06626
IEEE 7010: A New Standard for Assessing the Well-being Implications of Artificial Intelligence,Daniel S. Schiff;Aladdin Ayesh;Laura Musikanski;John C. Havens,"Artificial intelligence (AI) enabled products and services are becoming a staple of everyday life. While governments and businesses are eager to enjoy the benefits of AI innovations, the mixed impact of these autonomous and intelligent systems on human well-being has become a pressing issue. This article introduces one of the first international standards focused on the social and ethical implications of AI: The Institute of Electrical and Electronics Engineering (IEEE) Standard (Std) 7010-2020 Recommended Practice for Assessing the Impact of Autonomous and Intelligent Systems on Human Well-being. Incorporating well-being factors throughout the lifecycle of AI is both challenging and urgent and IEEE 7010 provides key guidance for those who design, deploy, and procure these technologies. We begin by articulating the benefits of an approach for AI centered around well-being and the measurement of well-being data. Next, we provide an overview of IEEE 7010, including its key principles and how the standard relates to approaches and perspectives in place in the AI community. Finally, we indicate where future efforts are needed. △ Less","17 December, 2020",https://arxiv.org/pdf/2005.06620
Essential requirements for establishing and operating data trusts: practical guidance based on a working meeting of fifteen Canadian organizations and initiatives,P. Alison Paprica;Eric Sutherland;Andrea Smith;Michael Brudno;Rosario G. Cartagena;Monique Crichlow;Brian K Courtney;Chris Loken;Kimberlyn M. McGrail;Alex Ryan;Michael J Schull;Adrian Thorogood;Carl Virtanen;Kathleen Yang,"Introduction: Increasingly, the label data trust is being applied to repeatable mechanisms or approaches to sharing data in a timely, fair, safe and equitable way. However, there is a gap in terms of practical guidance about how to establish and operate a data trust. Aim and Approach: In December 2019, the Canadian Institute for Health Information and the Vector Institute for Artificial Intelligence convened a working meeting of 19 people representing 15 Canadian organizations/initiatives involved in data sharing, most of which focus on public sector health data. The objective was to identify essential requirements for the establishment and operation of data trusts. Preliminary findings were presented during the meeting then refined as participants and co-authors identified relevant literature and contributed to this manuscript. Results: Twelve (12) minimum specification requirements (min specs) for data trusts were identified. The foundational min spec is that data trusts must meet all legal requirements, including legal authority to collect, hold or share data. In addition, there was agreement that data trusts must have (i) an accountable governing body which ensures the data trust advances its stated purpose and is transparent, (ii) comprehensive data management including responsible parties and clear processes for the collection, storage, access, disclosure and use of data, (iii) training and accountability requirements for all data users and (iv) ongoing public and stakeholder engagement. Conclusion / Implications: Based on a review of the literature and advice from participants from 15 Canadian organizations/initiatives, practical guidance in the form of twelve min specs for data trusts were agreed on. Public engagement and continued exchange of insights and experience is recommended on this evolving topic. △ Less","4 May, 2020",https://arxiv.org/pdf/2005.06604
Fashion Recommendation and Compatibility Prediction Using Relational Network,Maryam Moosaei;Yusan Lin;Hao Yang,"Fashion is an inherently visual concept and computer vision and artificial intelligence (AI) are playing an increasingly important role in shaping the future of this domain. Many research has been done on recommending fashion products based on the learned user preferences. However, in addition to recommending single items, AI can also help users create stylish outfits from items they already have, or purchase additional items that go well with their current wardrobe. Compatibility is the key factor in creating stylish outfits from single items. Previous studies have mostly focused on modeling pair-wise compatibility. There are a few approaches that consider an entire outfit, but these approaches have limitations such as requiring rich semantic information, category labels, and fixed order of items. Thus, they fail to effectively determine compatibility when such information is not available. In this work, we adopt a Relation Network (RN) to develop new compatibility learning models, Fashion RN and FashionRN-VSE, that addresses the limitations of existing approaches. FashionRN learns the compatibility of an entire outfit, with an arbitrary number of items, in an arbitrary order. We evaluated our model using a large dataset of 49,740 outfits that we collected from Polyvore website. Quantitatively, our experimental results demonstrate state of the art performance compared with alternative methods in the literature in both compatibility prediction and fill-in-the-blank test. Qualitatively, we also show that the item embedding learned by FashionRN indicate the compatibility among fashion items. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06584
Deep Learning for Political Science,Kakia Chatsiou;Slava Jankin Mikhaylov,"Political science, and social science in general, have traditionally been using computational methods to study areas such as voting behavior, policy making, international conflict, and international development. More recently, increasingly available quantities of data are being combined with improved algorithms and affordable computational resources to predict, learn, and discover new insights from data that is large in volume and variety. New developments in the areas of machine learning, deep learning, natural language processing (NLP), and, more generally, artificial intelligence (AI) are opening up new opportunities for testing theories and evaluating the impact of interventions and programs in a more dynamic and effective way. Applications using large volumes of structured and unstructured data are becoming common in government and industry, and increasingly also in social science research. This chapter offers an introduction to such methods drawing examples from political science. Focusing on the areas where the strengths of the methods coincide with challenges in these fields, the chapter first presents an introduction to AI and its core technology - machine learning, with its rapidly developing subfield of deep learning. The discussion of deep neural networks is illustrated with the NLP tasks that are relevant to political science. The latest advances in deep learning methods for NLP are also reviewed, together with their potential for improving information extraction and pattern recognition from political science texts. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06540
MosMedData: Chest CT Scans With COVID-19 Related Findings Dataset,S. P. Morozov;A. E. Andreychenko;N. A. Pavlov;A. V. Vladzymyrskyy;N. V. Ledikhova;V. A. Gombolevskiy;I. A. Blokhin;P. B. Gelezhe;A. V. Gonchar;V. Yu. Chernina,"This dataset contains anonymised human lung computed tomography (CT) scans with COVID-19 related findings, as well as without such findings. A small subset of studies has been annotated with binary pixel masks depicting regions of interests (ground-glass opacifications and consolidations). CT scans were obtained between 1st of March, 2020 and 25th of April, 2020, and provided by municipal hospitals in Moscow, Russia. Permanent link: https://mosmed.ai/datasets/covid19_1110. This dataset is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 Unported (CC BY-NC-ND 3.0) License. Key words: artificial intelligence, COVID-19, machine learning, dataset, CT, chest, imaging △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06465
High Performance and Portable Convolution Operators for ARM-based Multicore Processors,Pablo San Juan;Adrián Castelló;Manuel F. Dolz;Pedro Alonso-Jordá;Enrique S. Quintana-Ortí,"The considerable impact of Convolutional Neural Networks on many Artificial Intelligence tasks has led to the development of various high performance algorithms for the convolution operator present in this type of networks. One of these approaches leverages the \imcol transform followed by a general matrix multiplication (GEMM) in order to take advantage of the highly optimized realizations of the GEMM kernel in many linear algebra libraries. The main problems of this approach are 1) the large memory workspace required to host the intermediate matrices generated by the IM2COL transform; and 2) the time to perform the IM2COL transform, which is not negligible for complex neural networks. This paper presents a portable high performance convolution algorithm based on the BLIS realization of the GEMM kernel that avoids the use of the intermediate memory by taking advantage of the BLIS structure. In addition, the proposed algorithm eliminates the cost of the explicit IM2COL transform, while maintaining the portability and performance of the underlying realization of GEMM in BLIS. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06410
Dyslexia and Dysgraphia prediction: A new machine learning approach,Gilles Richard;Mathieu Serrurier,"Learning disabilities like dysgraphia, dyslexia, dyspraxia, etc. interfere with academic achievements but have also long terms consequences beyond the academic time. It is widely admitted that between 5% to 10% of the world population is subject to this kind of disabilities. For assessing such disabilities in early childhood, children have to solve a battery of tests. Human experts score these tests, and decide whether the children require specific education strategy on the basis of their marks. The assessment can be lengthy, costly and emotionally painful. In this paper, we investigate how Artificial Intelligence can help in automating this assessment. Gathering a dataset of handwritten text pictures and audio recordings, both from standard children and from dyslexic and/or dysgraphic children, we apply machine learning techniques for classification in order to analyze the differences between dyslexic/dysgraphic and standard readers/writers and to build a model. The model is trained on simple features obtained by analysing the pictures and the audio files. Our preliminary implementation shows relatively high performances on the dataset we have used. This suggests the possibility to screen dyslexia and dysgraphia via non-invasive methods in an accurate way as soon as enough data are available. △ Less","15 April, 2020",https://arxiv.org/pdf/2005.06401
Neural Architecture Search for Gliomas Segmentation on Multimodal Magnetic Resonance Imaging,Feifan Wang,"Past few years have witnessed the artificial intelligence inspired evolution in various medical fields. The diagnosis and treatment of gliomas -- one of the most commonly seen brain tumors with low survival rate -- rely heavily on the computer assisted segmentation process undertaken on the magnetic resonance imaging (MRI) scans. Although the encoder-decoder shaped deep learning networks have been the de facto standard style for semantic segmentation tasks in medical imaging analysis, enormous effort is still required to be spent on designing the detailed architecture of the down-sampling and up-sampling blocks. In this work, we propose a neural architecture search (NAS) based solution to brain tumor segmentation tasks on multimodal volumetric MRI scans. Three sets of candidate operations are composed respectively for three kinds of basic building blocks in which each operation is assigned with a specific probabilistic parameter to be learned. Through alternately updating the weights of operations and the other parameters in the network, the searching mechanism ends up with two optimal structures for the upward and downward blocks. Moreover, the developed solution also integrates normalization and patching strategies tailored for brain MRI processing. Extensive comparative experiments on the BraTS 2019 dataset demonstrate that the proposed algorithm not only could relieve the pressure of fabricating block architectures but also possesses competitive feasibility and scalability. △ Less","20 May, 2020",https://arxiv.org/pdf/2005.06338
Explainable Reinforcement Learning: A Survey,Erika Puiutta;Eric MSP Veith,"Explainable Artificial Intelligence (XAI), i.e., the development of more transparent and interpretable AI models, has gained increased traction over the last few years. This is due to the fact that, in conjunction with their growth into powerful and ubiquitous tools, AI models exhibit one detrimential characteristic: a performance-transparency trade-off. This describes the fact that the more complex a model's inner workings, the less clear it is how its predictions or decisions were achieved. But, especially considering Machine Learning (ML) methods like Reinforcement Learning (RL) where the system learns autonomously, the necessity to understand the underlying reasoning for their decisions becomes apparent. Since, to the best of our knowledge, there exists no single work offering an overview of Explainable Reinforcement Learning (XRL) methods, this survey attempts to address this gap. We give a short summary of the problem, a definition of important terms, and offer a classification and assessment of current XRL methods. We found that a) the majority of XRL methods function by mimicking and simplifying a complex model instead of designing an inherently simple one, and b) XRL (and XAI) methods often neglect to consider the human side of the equation, not taking into account research from related fields like psychology or philosophy. Thus, an interdisciplinary effort is needed to adapt the generated explanations to a (non-expert) human user in order to effectively progress in the field of XRL and XAI in general. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06247
Boosting on the shoulders of giants in quantum device calibration,Alex Wozniakowski;Jayne Thompson;Mile Gu;Felix Binder,"Traditional machine learning applications, such as optical character recognition, arose from the inability to explicitly program a computer to perform a routine task. In this context, learning algorithms usually derive a model exclusively from the evidence present in a massive dataset. Yet in some scientific disciplines, obtaining an abundance of data is an impractical luxury, however; there is an explicit model of the domain based upon previous scientific discoveries. Here we introduce a new approach to machine learning that is able to leverage prior scientific discoveries in order to improve generalizability over a scientific model. We show its efficacy in predicting the entire energy spectrum of a Hamiltonian on a superconducting quantum device, a key task in present quantum computer calibration. Our accuracy surpasses the current state-of-the-art by over 20\%. Our approach thus demonstrates how artificial intelligence can be further enhanced by ""standing on the shoulders of giants."" △ Less","13 May, 2020",https://arxiv.org/pdf/2005.06194
Trust Considerations for Explainable Robots: A Human Factors Perspective,Lindsay Sanneman;Julie A. Shah,"Recent advances in artificial intelligence (AI) and robotics have drawn attention to the need for AI systems and robots to be understandable to human users. The explainable AI (XAI) and explainable robots literature aims to enhance human understanding and human-robot team performance by providing users with necessary information about AI and robot behavior. Simultaneously, the human factors literature has long addressed important considerations that contribute to human performance, including human trust in autonomous systems. In this paper, drawing from the human factors literature, we discuss three important trust-related considerations for the design of explainable robot systems: the bases of trust, trust calibration, and trust specificity. We further detail existing and potential metrics for assessing trust in robotic systems based on explanations provided by explainable robots. △ Less","12 May, 2020",https://arxiv.org/pdf/2005.05940
Argument Schemes for Explainable Planning,Quratul-ain Mahesar;Simon Parsons,"Artificial Intelligence (AI) is being increasingly used to develop systems that produce intelligent solutions. However, there is a major concern that whether the systems built will be trusted by humans. In order to establish trust in AI systems, there is a need for the user to understand the reasoning behind their solutions and therefore, the system should be able to explain and justify its output. In this paper, we use argumentation to provide explanations in the domain of AI planning. We present argument schemes to create arguments that explain a plan and its components; and a set of critical questions that allow interaction between the arguments and enable the user to obtain further information regarding the key elements of the plan. Finally, we present some properties of the plan arguments. △ Less","12 May, 2020",https://arxiv.org/pdf/2005.05849
A Survey of Behavior Trees in Robotics and AI,Matteo Iovino;Edvards Scukins;Jonathan Styrud;Petter Ögren;Christian Smith,"Behavior Trees (BTs) were invented as a tool to enable modular AI in computer games, but have received an increasing amount of attention in the robotics community in the last decade. With rising demands on agent AI complexity, game programmers found that the Finite State Machines (FSM) that they used scaled poorly and were difficult to extend, adapt and reuse. In BTs, the state transition logic is not dispersed across the individual states, but organized in a hierarchical tree structure, with the states as leaves. This has a significant effect on modularity, which in turn simplifies both synthesis and analysis by humans and algorithms alike. These advantages are needed not only in game AI design, but also in robotics, as is evident from the research being done. In this paper we present a comprehensive survey of the topic of BTs in Artificial Intelligence and Robotic applications. The existing literature is described and categorized based on methods, application areas and contributions, and the paper is concluded with a list of open research challenges. △ Less","13 May, 2020",https://arxiv.org/pdf/2005.05842
Fostering Event Compression using Gated Surprise,Dania Humaidan;Sebastian Otte;Martin V. Butz,"Our brain receives a dynamically changing stream of sensorimotor data. Yet, we perceive a rather organized world, which we segment into and perceive as events. Computational theories of cognitive science on event-predictive cognition suggest that our brain forms generative, event-predictive models by segmenting sensorimotor data into suitable chunks of contextual experiences. Here, we introduce a hierarchical, surprise-gated recurrent neural network architecture, which models this process and develops compact compressions of distinct event-like contexts. The architecture contains a contextual LSTM layer, which develops generative compressions of ongoing and subsequent contexts. These compressions are passed into a GRU-like layer, which uses surprise signals to update its recurrent latent state. The latent state is passed forward into another LSTM layer, which processes actual dynamic sensory flow in the light of the provided latent, contextual compression signals. Our model shows to develop distinct event compressions and achieves the best performance on multiple event processing tasks. The architecture may be very useful for the further development of resource-efficient learning, hierarchical model-based reinforcement learning, as well as the development of artificial event-predictive cognition and intelligence. △ Less","12 May, 2020",https://arxiv.org/pdf/2005.05704
"Replication Markets: Results, Lessons, Challenges and Opportunities in AI Replication",Yang Liu;Michael Gordon;Juntao Wang;Michael Bishop;Yiling Chen;Thomas Pfeiffer;Charles Twardy;Domenico Viganola,"The last decade saw the emergence of systematic large-scale replication projects in the social and behavioral sciences, (Camerer et al., 2016, 2018; Ebersole et al., 2016; Klein et al., 2014, 2018; Collaboration, 2015). These projects were driven by theoretical and conceptual concerns about a high fraction of ""false positives"" in the scientific publications (Ioannidis, 2005) (and a high prevalence of ""questionable research practices"" (Simmons, Nelson, and Simonsohn, 2011). Concerns about the credibility of research findings are not unique to the behavioral and social sciences; within Computer Science, Artificial Intelligence (AI) and Machine Learning (ML) are areas of particular concern (Lucic et al., 2018; Freire, Bonnet, and Shasha, 2012; Gundersen and Kjensmo, 2018; Henderson et al., 2018). Given the pioneering role of the behavioral and social sciences in the promotion of novel methodologies to improve the credibility of research, it is a promising approach to analyze the lessons learned from this field and adjust strategies for Computer Science, AI and ML In this paper, we review approaches used in the behavioral and social sciences and in the DARPA SCORE project. We particularly focus on the role of human forecasting of replication outcomes, and how forecasting can leverage the information gained from relatively labor and resource-intensive replications. We will discuss opportunities and challenges of using these approaches to monitor and improve the credibility of research areas in Computer Science, AI, and ML. △ Less","9 May, 2020",https://arxiv.org/pdf/2005.04543
Transforming task representations to perform novel tasks,Andrew K. Lampinen;James L. McClelland,"An important aspect of intelligence is the ability to adapt to a novel task without any direct experience (zero-shot), based on its relationship to previous tasks. Humans can exhibit this cognitive flexibility. By contrast, models that achieve superhuman performance in specific tasks often fail to adapt to even slight task alterations. To address this, we propose a general computational framework for adapting to novel tasks based on their relationship to prior tasks. We begin by learning vector representations of tasks. To adapt to new tasks, we propose meta-mappings, higher-order tasks that transform basic task representations. We demonstrate the effectiveness of this framework across a wide variety of tasks and computational paradigms, ranging from regression to image classification and reinforcement learning. We compare to both human adaptability and language-based approaches to zero-shot learning. Across these domains, meta-mapping is successful, often achieving 80-90% performance, without any data, on a novel task, even when the new task directly contradicts prior experience. We further show that meta-mapping can not only generalize to new tasks via learned relationships, but can also generalize using novel relationships unseen during training. Finally, using meta-mapping as a starting point can dramatically accelerate later learning on a new task, and reduce learning time and cumulative error substantially. Our results provide insight into a possible computational basis of intelligent adaptability and offer a possible framework for modeling cognitive flexibility and building more flexible artificial intelligence systems. △ Less","6 October, 2020",https://arxiv.org/pdf/2005.04318
A Hybrid Method for Training Convolutional Neural Networks,Vasco Lopes;Paulo Fazendeiro,"Artificial Intelligence algorithms have been steadily increasing in popularity and usage. Deep Learning, allows neural networks to be trained using huge datasets and also removes the need for human extracted features, as it automates the feature learning process. In the hearth of training deep neural networks, such as Convolutional Neural Networks, we find backpropagation, that by computing the gradient of the loss function with respect to the weights of the network for a given input, it allows the weights of the network to be adjusted to better perform in the given task. In this paper, we propose a hybrid method that uses both backpropagation and evolutionary strategies to train Convolutional Neural Networks, where the evolutionary strategies are used to help to avoid local minimas and fine-tune the weights, so that the network achieves higher accuracy results. We show that the proposed hybrid method is capable of improving upon regular training in the task of image classification in CIFAR-10, where a VGG16 model was used and the final test results increased 0.61%, in average, when compared to using only backpropagation. △ Less","15 April, 2020",https://arxiv.org/pdf/2005.04153
Choose Your Own Question: Encouraging Self-Personalization in Learning Path Construction,Youngduck Choi;Yoonho Na;Youngjik Yoon;Jonghun Shin;Chan Bae;Hongseok Suh;Byungsoo Kim;Jaewe Heo,"Learning Path Recommendation is the heart of adaptive learning, the educational paradigm of an Interactive Educational System (IES) providing a personalized learning experience based on the student's history of learning activities. In typical existing IESs, the student must fully consume a recommended learning item to be provided a new recommendation. This workflow comes with several limitations. For example, there is no opportunity for the student to give feedback on the choice of learning items made by the IES. Furthermore, the mechanism by which the choice is made is opaque to the student, limiting the student's ability to track their learning. To this end, we introduce Rocket, a Tinder-like User Interface for a general class of IESs. Rocket provides a visual representation of Artificial Intelligence (AI)-extracted features of learning materials, allowing the student to quickly decide whether the material meets their needs. The student can choose between engaging with the material and receiving a new recommendation by swiping or tapping. Rocket offers the following potential improvements for IES User Interfaces: First, Rocket enhances the explainability of IES recommendations by showing students a visual summary of the meaningful AI-extracted features used in the decision-making process. Second, Rocket enables self-personalization of the learning experience by leveraging the students' knowledge of their own abilities and needs. Finally, Rocket provides students with fine-grained information on their learning path, giving them an avenue to assess their own skills and track their learning progress. We present the source code of Rocket, in which we emphasize the independence and extensibility of each component, and make it publicly available for all purposes. △ Less","7 May, 2020",https://arxiv.org/pdf/2005.03818
Lenia and Expanded Universe,Bert Wang-Chak Chan,"We report experimental extensions of Lenia, a continuous cellular automata family capable of producing lifelike self-organizing autonomous patterns. The rule of Lenia was generalized into higher dimensions, multiple kernels, and multiple channels. The final architecture approaches what can be seen as a recurrent convolutional neural network. Using semi-automatic search e.g. genetic algorithm, we discovered new phenomena like polyhedral symmetries, individuality, self-replication, emission, growth by ingestion, and saw the emergence of ""virtual eukaryotes"" that possess internal division of labor and type differentiation. We discuss the results in the contexts of biology, artificial life, and artificial intelligence. △ Less","7 May, 2020",https://arxiv.org/pdf/2005.03742
Generative Feature Replay with Orthogonal Weight Modification for Continual Learning,Gehui Shen;Song Zhang;Xiang Chen;Zhi-Hong Deng,"The ability of intelligent agents to learn and remember multiple tasks sequentially is crucial to achieving artificial general intelligence. Many continual learning (CL) methods have been proposed to overcome catastrophic forgetting which results from non i.i.d data in the sequential learning of neural networks. In this paper we focus on class incremental learning, a challenging CL scenario. For this scenario, generative replay is a promising strategy which generates and replays pseudo data for previous tasks to alleviate catastrophic forgetting. However, it is hard to train a generative model continually for relatively complex data. Based on recently proposed orthogonal weight modification (OWM) algorithm which can approximately keep previously learned feature invariant when learning new tasks, we propose to 1) replay penultimate layer feature with a generative model; 2) leverage a self-supervised auxiliary task to further enhance the stability of feature. Empirical results on several datasets show our method always achieves substantial improvement over powerful OWM while conventional generative replay always results in a negative effect. Meanwhile our method beats several strong baselines including one based on real data storage. In addition, we conduct experiments to study why our method is effective. △ Less","11 September, 2020",https://arxiv.org/pdf/2005.03490
Arranging Test Tubes in Racks Using Combined Task and Motion Planning,Weiwei Wan;Takeyuki Kotaka;Kensuke Harada,"The paper develops a robotic manipulation system to treat the pressing needs for handling a large number of test tubes in clinical examination and replace or reduce human labor. It presents the technical details of the system, which separates and arranges test tubes in racks with the help of 3D vision and artificial intelligence (AI) reasoning/planning. The developed system only requires a person to put a rack with mixed and non-arranged tubes in front of a robot. The robot autonomously performs recognition, reasoning, planning, manipulation, etc., and returns a rack with separated and arranged tubes. The system is simple-to-use, and there are no requests for expert knowledge in robotics. We expect such a system to play an important role in helping managing public health and hope similar systems could be extended to other clinical manipulation like handling mixers and pipettes in the future. △ Less","7 May, 2020",https://arxiv.org/pdf/2005.03342
Recognizing Exercises and Counting Repetitions in Real Time,Talal Alatiah;Chen Chen,"Artificial intelligence technology has made its way absolutely necessary in a variety of industries including the fitness industry. Human pose estimation is one of the important researches in the field of Computer Vision for the last few years. In this project, pose estimation and deep machine learning techniques are combined to analyze the performance and report feedback on the repetitions of performed exercises in real-time. Involving machine learning technology in the fitness industry could help the judges to count repetitions of any exercise during Weightlifting or CrossFit competitions. △ Less","6 May, 2020",https://arxiv.org/pdf/2005.03194
A Proposal for Intelligent Agents with Episodic Memory,David Murphy;Thomas S. Paula;Wagston Staehler;Juliano Vacaro;Gabriel Paz;Guilherme Marques;Bruna Oliveira,"In the future we can expect that artificial intelligent agents, once deployed, will be required to learn continually from their experience during their operational lifetime. Such agents will also need to communicate with humans and other agents regarding the content of their experience, in the context of passing along their learnings, for the purpose of explaining their actions in specific circumstances or simply to relate more naturally to humans concerning experiences the agent acquires that are not necessarily related to their assigned tasks. We argue that to support these goals, an agent would benefit from an episodic memory; that is, a memory that encodes the agent's experience in such a way that the agent can relive the experience, communicate about it and use its past experience, inclusive of the agents own past actions, to learn more effective models and policies. In this short paper, we propose one potential approach to provide an AI agent with such capabilities. We draw upon the ever-growing body of work examining the function and operation of the Medial Temporal Lobe (MTL) in mammals to guide us in adding an episodic memory capability to an AI agent composed of artificial neural networks (ANNs). Based on that, we highlight important aspects to be considered in the memory organization and we propose an architecture combining ANNs and standard Computer Science techniques for supporting storage and retrieval of episodic memories. Despite being initial work, we hope this short paper can spark discussions around the creation of intelligent agents with memory or, at least, provide a different point of view on the subject. △ Less","6 May, 2020",https://arxiv.org/pdf/2005.03182
AIOps for a Cloud Object Storage Service,Anna Levin;Shelly Garion;Elliot K. Kolodner;Dean H. Lorenz;Katherine Barabash;Mike Kugler;Niall McShane,"With the growing reliance on the ubiquitous availability of IT systems and services, these systems become more global, scaled, and complex to operate. To maintain business viability, IT service providers must put in place reliable and cost efficient operations support. Artificial Intelligence for IT Operations (AIOps) is a promising technology for alleviating operational complexity of IT systems and services. AIOps platforms utilize big data, machine learning and other advanced analytics technologies to enhance IT operations with proactive actionable dynamic insight. In this paper we share our experience applying the AIOps approach to a production cloud object storage service to get actionable insights into system's behavior and health. We describe a real-life production cloud scale service and its operational data, present the AIOps platform we have created, and show how it has helped us resolving operational pain points. △ Less","6 May, 2020",https://arxiv.org/pdf/2005.03094
Towards the Role of Theory of Mind in Explanation,Maayan Shvo;Toryn Q. Klassen;Sheila A. McIlraith,"Theory of Mind is commonly defined as the ability to attribute mental states (e.g., beliefs, goals) to oneself, and to others. A large body of previous work - from the social sciences to artificial intelligence - has observed that Theory of Mind capabilities are central to providing an explanation to another agent or when explaining that agent's behaviour. In this paper, we build and expand upon previous work by providing an account of explanation in terms of the beliefs of agents and the mechanism by which agents revise their beliefs given possible explanations. We further identify a set of desiderata for explanations that utilize Theory of Mind. These desiderata inform our belief-based account of explanation. △ Less","6 May, 2020",https://arxiv.org/pdf/2005.02963
Exploring Exploration: Comparing Children with RL Agents in Unified Environments,Eliza Kosoy;Jasmine Collins;David M. Chan;Sandy Huang;Deepak Pathak;Pulkit Agrawal;John Canny;Alison Gopnik;Jessica B. Hamrick,"Research in developmental psychology consistently shows that children explore the world thoroughly and efficiently and that this exploration allows them to learn. In turn, this early learning supports more robust generalization and intelligent behavior later in life. While much work has gone into developing methods for exploration in machine learning, artificial agents have not yet reached the high standard set by their human counterparts. In this work we propose using DeepMind Lab (Beattie et al., 2016) as a platform to directly compare child and agent behaviors and to develop new exploration techniques. We outline two ongoing experiments to demonstrate the effectiveness of a direct comparison, and outline a number of open research questions that we believe can be tested using this methodology. △ Less","1 July, 2020",https://arxiv.org/pdf/2005.02880
The computerization of archaeology: survey on AI techniques,Lorenzo Mantovan;Loris Nanni,"This paper analyses the application of artificial intelligence techniques to various areas of archaeology and more specifically: a) The use of software tools as a creative stimulus for the organization of exhibitions; the use of humanoid robots and holographic displays as guides that interact and involve museum visitors; b) The analysis of methods for the classification of fragments found in archaeological excavations and for the reconstruction of ceramics, with the recomposition of the parts of text missing from historical documents and epigraphs; c) The cataloguing and study of human remains to understand the social and historical context of belonging with the demonstration of the effectiveness of the AI techniques used; d) The detection of particularly difficult terrestrial archaeological sites with the analysis of the architectures of the Artificial Neural Networks most suitable for solving the problems presented by the site; the design of a study for the exploration of marine archaeological sites, located at depths that cannot be reached by man, through the construction of a freely explorable 3D version. △ Less","30 June, 2020",https://arxiv.org/pdf/2005.02863
AI in society and culture: decision making and values,Katalin Feher;Asta Zelenkauskaite,"With the increased expectation of artificial intelligence, academic research face complex questions of human-centred, responsible and trustworthy technology embedded into society and culture. Several academic debates, social consultations and impact studies are available to reveal the key aspects of the changing human-machine ecosystem. To contribute to these studies, hundreds of related academic sources are summarized below regarding AI-driven decisions and valuable AI. In details, sociocultural filters, taxonomy of human-machine decisions and perspectives of value-based AI are in the focus of this literature review. For better understanding, it is proposed to invite stakeholders in the prepared large-scale survey about the next generation AI that investigates issues that go beyond the technology. △ Less","29 April, 2020",https://arxiv.org/pdf/2005.02777
Evolutionary Multi-Objective Design of SARS-CoV-2 Protease Inhibitor Candidates,Tim Cofala;Lars Elend;Philip Mirbach;Jonas Prellberg;Thomas Teusch;Oliver Kramer,"Computational drug design based on artificial intelligence is an emerging research area. At the time of writing this paper, the world suffers from an outbreak of the coronavirus SARS-CoV-2. A promising way to stop the virus replication is via protease inhibition. We propose an evolutionary multi-objective algorithm (EMOA) to design potential protease inhibitors for SARS-CoV-2's main protease. Based on the SELFIES representation the EMOA maximizes the binding of candidate ligands to the protein using the docking tool QuickVina 2, while at the same time taking into account further objectives like drug-likeliness or the fulfillment of filter constraints. The experimental part analyzes the evolutionary process and discusses the inhibitor candidates. △ Less","18 May, 2020",https://arxiv.org/pdf/2005.02666
Compact Device Models for FinFET and Beyond,Darsen D. Lu;Mohan V. Dunga;Ali M. Niknejad;Chenming Hu;Fu-Xiang Liang;Wei-Chen Hung;Jia-Wei Lee;Chun-Hsiang Hsu;Meng-Hsueh Chiang,"Compact device models play a significant role in connecting device technology and circuit design. BSIM-CMG and BSIM-IMG are industry standard compact models suited for the FinFET and UTBB technologies, respectively. Its surface potential based modeling framework and symmetry preserving properties make them suitable for both analog/RF and digital design. In the era of artificial intelligence / deep learning, compact models further enhanced our ability to explore RRAM and other NVM-based neuromorphic circuits. We have demonstrated simulation of RRAM neuromorphic circuits with Verilog-A based compact model at NCKU. Further abstraction with macromodels is performed to enable larger scale machine learning simulation. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.02580
Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,Henrique Lemos;Pedro Avelar;Marcelo Prates;Luís Lamb;Artur Garcez,"The recent developments and growing interest in neural-symbolic models has shown that hybrid approaches can offer richer models for Artificial Intelligence. The integration of effective relational learning and reasoning methods is one of the key challenges in this direction, as neural learning and symbolic reasoning offer complementary characteristics that can benefit the development of AI systems. Relational labelling or link prediction on knowledge graphs has become one of the main problems in deep learning-based natural language processing research. Moreover, other fields which make use of neural-symbolic techniques may also benefit from such research endeavours. There have been several efforts towards the identification of missing facts from existing ones in knowledge graphs. Two lines of research try and predict knowledge relations between two entities by considering all known facts connecting them or several paths of facts connecting them. We propose a neural-symbolic graph neural network which applies learning over all the paths by feeding the model with the embedding of the minimal subset of the knowledge graph containing such paths. By learning to produce representations for entities and facts corresponding to word embeddings, we show how the model can be trained end-to-end to decode these representations and infer relations between entities in a multitask approach. Our contribution is two-fold: a neural-symbolic methodology leverages the resolution of relational inference in large graphs, and we also demonstrate that such neural-symbolic model is shown more effective than path-based approaches △ Less","5 May, 2020",https://arxiv.org/pdf/2005.02525
Scalable and Secure Architecture for Distributed IoT Systems,Najmeddine Dhieb;Hakim Ghazzai;Hichem Besbes;Yehia Massoud,"Internet-of-things (IoT) is perpetually revolutionizing our daily life and rapidly transforming physical objects into an ubiquitous connected ecosystem. Due to their massive deployment and moderate security levels, those devices face a lot of security, management, and control challenges. Their classical centralized architecture is still cloaking vulnerabilities and anomalies that can be exploited by hackers for spying, eavesdropping, and taking control of the network. In this paper, we propose to improve the IoT architecture with additional security features using Artificial Intelligence (AI) and blockchain technology. We propose a novel architecture based on permissioned blockchain technology in order to build a scalable and decentralized end-to-end secure IoT system. Furthermore, we enhance the IoT system security with an AI-component at the gateway level to detect and classify suspected activities, malware, and cyber-attacks using machine learning techniques. Simulations and practical implementation show that the proposed architecture delivers high performance against cyber-attacks. △ Less","20 April, 2020",https://arxiv.org/pdf/2005.02456
Classification-Based Anomaly Detection for General Data,Liron Bergman;Yedid Hoshen,"Anomaly detection, finding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artificial intelligence. Recently, classification-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random affine transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.02359
Don't Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition,Mahsan Nourani;Chiradeep Roy;Tahrima Rahman;Eric D. Ragan;Nicholas Ruozzi;Vibhav Gogate,"Explainable machine learning and artificial intelligence models have been used to justify a model's decision-making process. This added transparency aims to help improve user performance and understanding of the underlying model. However, in practice, explainable systems face many open questions and challenges. Specifically, designers might reduce the complexity of deep learning models in order to provide interpretability. The explanations generated by these simplified models, however, might not accurately justify and be truthful to the model. This can further add confusion to the users as they might not find the explanations meaningful with respect to the model predictions. Understanding how these explanations affect user behavior is an ongoing challenge. In this paper, we explore how explanation veracity affects user performance and agreement in intelligent systems. Through a controlled user study with an explainable activity recognition system, we compare variations in explanation veracity for a video review and querying task. The results suggest that low veracity explanations significantly decrease user performance and agreement compared to both accurate explanations and a system without explanations. These findings demonstrate the importance of accurate and understandable explanations and caution that poor explanations can sometimes be worse than no explanations with respect to their effect on user performance and reliance on an AI system. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.02335
Stereotype-Free Classification of Fictitious Faces,Mohammadhossein Toutiaee;Soheyla Amirian;John A. Miller;Sheng Li,"Equal Opportunity and Fairness are receiving increasing attention in artificial intelligence. Stereotyping is another source of discrimination, which yet has been unstudied in literature. GAN-made faces would be exposed to such discrimination, if they are classified by human perception. It is possible to eliminate the human impact on fictitious faces classification task by the use of statistical approaches. We present a novel approach through penalized regression to label stereotype-free GAN-generated synthetic unlabeled images. The proposed approach aids labeling new data (fictitious output images) by minimizing a penalized version of the least squares cost function between realistic pictures and target pictures. △ Less","29 April, 2020",https://arxiv.org/pdf/2005.02157
Improving Target-driven Visual Navigation with Attention on 3D Spatial Relationships,Yunlian Lv;Ning Xie;Yimin Shi;Zijiao Wang;Heng Tao Shen,"Embodied artificial intelligence (AI) tasks shift from tasks focusing on internet images to active settings involving embodied agents that perceive and act within 3D environments. In this paper, we investigate the target-driven visual navigation using deep reinforcement learning (DRL) in 3D indoor scenes, whose navigation task aims to train an agent that can intelligently make a series of decisions to arrive at a pre-specified target location from any possible starting positions only based on egocentric views. However, most navigation methods currently struggle against several challenging problems, such as data efficiency, automatic obstacle avoidance, and generalization. Generalization problem means that agent does not have the ability to transfer navigation skills learned from previous experience to unseen targets and scenes. To address these issues, we incorporate two designs into classic DRL framework: attention on 3D knowledge graph (KG) and target skill extension (TSE) module. On the one hand, our proposed method combines visual features and 3D spatial representations to learn navigation policy. On the other hand, TSE module is used to generate sub-targets which allow agent to learn from failures. Specifically, our 3D spatial relationships are encoded through recently popular graph convolutional network (GCN). Considering the real world settings, our work also considers open action and adds actionable targets into conventional navigation situations. Those more difficult settings are applied to test whether DRL agent really understand its task, navigating environment, and can carry out reasoning. Our experiments, performed in the AI2-THOR, show that our model outperforms the baselines in both SR and SPL metrics, and improves generalization ability across targets and scenes. △ Less","29 April, 2020",https://arxiv.org/pdf/2005.02153
Post-hoc explanation of black-box classifiers using confident itemsets,Milad Moradi;Matthias Samwald,"Black-box Artificial Intelligence (AI) methods, e.g. deep neural networks, have been widely utilized to build predictive models that can extract complex relationships in a dataset and make predictions for new unseen data records. However, it is difficult to trust decisions made by such methods since their inner working and decision logic is hidden from the user. Explainable Artificial Intelligence (XAI) refers to systems that try to explain how a black-box AI model produces its outcomes. Post-hoc XAI methods approximate the behavior of a black-box by extracting relationships between feature values and the predictions. Perturbation-based and decision set methods are among commonly used post-hoc XAI systems. The former explanators rely on random perturbations of data records to build local or global linear models that explain individual predictions or the whole model. The latter explanators use those feature values that appear more frequently to construct a set of decision rules that produces the same outcomes as the target black-box. However, these two classes of XAI methods have some limitations. Random perturbations do not take into account the distribution of feature values in different subspaces, leading to misleading approximations. Decision sets only pay attention to frequent feature values and miss many important correlations between features and class labels that appear less frequently but accurately represent decision boundaries of the model. In this paper, we address the above challenges by proposing an explanation method named Confident Itemsets Explanation (CIE). We introduce confident itemsets, a set of feature values that are highly correlated to a specific class label. CIE utilizes confident itemsets to discretize the whole decision space of a model to smaller subspaces. △ Less","20 September, 2020",https://arxiv.org/pdf/2005.01992
One-step regression and classification with crosspoint resistive memory arrays,Zhong Sun;Giacomo Pedretti;Alessandro Bricalli;Daniele Ielmini,"Machine learning has been getting a large attention in the recent years, as a tool to process big data generated by ubiquitous sensors in our daily life. High speed, low energy computing machines are in demand to enable real-time artificial intelligence at the edge, i.e., without the support of a remote frame server in the cloud. Such requirements challenge the complementary metal-oxide-semiconductor (CMOS) technology, which is limited by the Moore's law approaching its end and the communication bottleneck in conventional computing architecture. Novel computing concepts, architectures and devices are thus strongly needed to accelerate data-intensive applications. Here we show a crosspoint resistive memory circuit with feedback configuration can execute linear regression and logistic regression in just one step by computing the pseudoinverse matrix of the data within the memory. The most elementary learning operation, that is the regression of a sequence of data and the classification of a set of data, can thus be executed in one single computational step by the novel technology. One-step learning is further supported by simulations of the prediction of the cost of a house in Boston and the training of a 2-layer neural network for MNIST digit recognition. The results are all obtained in one computational step, thanks to the physical, parallel, and analog computing within the crosspoint array. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.01988
Low Power In-Memory Implementation of Ternary Neural Networks with Resistive RAM-Based Synapse,Axel Laborieux;Marc Bocquet;Tifenn Hirtzlin;Jacques-Olivier Klein;Liza Herrera Diez;Etienne Nowak;Elisa Vianello;Jean-Michel Portal;Damien Querlioz,"The design of systems implementing low precision neural networks with emerging memories such as resistive random access memory (RRAM) is a major lead for reducing the energy consumption of artificial intelligence (AI). Multiple works have for example proposed in-memory architectures to implement low power binarized neural networks. These simple neural networks, where synaptic weights and neuronal activations assume binary values, can indeed approach state-of-the-art performance on vision tasks. In this work, we revisit one of these architectures where synapses are implemented in a differential fashion to reduce bit errors, and synaptic weights are read using precharge sense amplifiers. Based on experimental measurements on a hybrid 130 nm CMOS/RRAM chip and on circuit simulation, we show that the same memory array architecture can be used to implement ternary weights instead of binary weights, and that this technique is particularly appropriate if the sense amplifier is operated in near-threshold regime. We also show based on neural network simulation on the CIFAR-10 image recognition task that going from binary to ternary neural networks significantly increases neural network performance. These results highlight that AI circuits function may sometimes be revisited when operated in low power regimes. △ Less","5 May, 2020",https://arxiv.org/pdf/2005.01973
A multi-component framework for the analysis and design of explainable artificial intelligence,S. Atakishiyev;H. Babiker;N. Farruque;R. Goebel1;M-Y. Kima;M. H. Motallebi;J. Rabelo;T. Syed;O. R. Zaïane,"The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, which have created high expectations for industrial, commercial and social value. Second, the emergence of concern for creating trusted AI systems, including the creation of regulatory principles to ensure transparency and trust of AI systems.These two threads have created a kind of ""perfect storm"" of research activity, all eager to create and deliver it any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science, and which provides a basis for the development of a framework for transparent XAI. Here we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a history of XAI ideas, and synthesize those ideas into a simple framework to calibrate five successive levels of XAI. △ Less","4 May, 2020",https://arxiv.org/pdf/2005.01908
Navigating the Landscape of Multiplayer Games,Shayegan Omidshafiei;Karl Tuyls;Wojciech M. Czarnecki;Francisco C. Santos;Mark Rowland;Jerome Connor;Daniel Hennes;Paul Muller;Julien Perolat;Bart De Vylder;Audrunas Gruslys;Remi Munos,"Multiplayer games have long been used as testbeds in artificial intelligence research, aptly referred to as the Drosophila of artificial intelligence. Traditionally, researchers have focused on using well-known games to build strong agents. This progress, however, can be better informed by characterizing games and their topological landscape. Tackling this latter question can facilitate understanding of agents and help determine what game an agent should target next as part of its training. Here, we show how network measures applied to response graphs of large-scale games enable the creation of a landscape of games, quantifying relationships between games of varying sizes and characteristics. We illustrate our findings in domains ranging from canonical games to complex empirical games capturing the performance of trained agents pitted against one another. Our results culminate in a demonstration leveraging this information to generate new and interesting games, including mixtures of empirical games synthesized from real world games. △ Less","17 November, 2020",https://arxiv.org/pdf/2005.01642
Robotic Self-Assessment of Competence,Gertjan J. Burghouts;Albert Huizing;Mark A. Neerincx,"In robotics, one of the main challenges is that the on-board Artificial Intelligence (AI) must deal with different or unexpected environments. Such AI agents may be incompetent there, while the underlying model itself may not be aware of this (e.g., deep learning models are often overly confident). This paper proposes two methods for the online assessment of the competence of the AI model, respectively for situations when nothing is known about competence beforehand, and when there is prior knowledge about competence (in semantic form). The proposed method assesses whether the current environment is known. If not, it asks a human for feedback about its competence. If it knows the environment, it assesses its competence by generalizing from earlier experience. Results on real data show the merit of competence assessment for a robot moving through various environments in which it sometimes is competent and at other times it is not competent. We discuss the role of the human in robot's self-assessment of its competence, and the challenges to acquire complementary information from the human that reinforces the assessments. △ Less","4 May, 2020",https://arxiv.org/pdf/2005.01546
Using Artificial Intelligence to Analyze Fashion Trends,Mengyun Shi;Van Dyk Lewis,"Analyzing fashion trends is essential in the fashion industry. Current fashion forecasting firms, such as WGSN, utilize the visual information from around the world to analyze and predict fashion trends. However, analyzing fashion trends is time-consuming and extremely labor intensive, requiring individual employees' manual editing and classification. To improve the efficiency of data analysis of such image-based information and lower the cost of analyzing fashion images, this study proposes a data-driven quantitative abstracting approach using an artificial intelligence (A.I.) algorithm. Specifically, an A.I. model was trained on fashion images from a large-scale dataset under different scenarios, for example in online stores and street snapshots. This model was used to detect garments and classify clothing attributes such as textures, garment style, and details for runway photos and videos. It was found that the A.I. model can generate rich attribute descriptions of detected regions and accurately bind the garments in the images. Adoption of A.I. algorithm demonstrated promising results and the potential to classify garment types and details automatically, which can make the process of trend forecasting more cost-effective and faster. △ Less","3 May, 2020",https://arxiv.org/pdf/2005.00986
How deep the machine learning can be,János Végh,"Today we live in the age of artificial intelligence and machine learning; from small startups to HW or SW giants, everyone wants to build machine intelligence chips, applications. The task, however, is hard: not only because of the size of the problem: the technology one can utilize (and the paradigm it is based upon) strongly degrades the chances to succeed efficiently. Today the single-processor performance practically reached the limits the laws of nature enable. The only feasible way to achieve the needed high computing performance seems to be parallelizing many sequentially working units. The laws of the (massively) parallelized computing, however, are different from those experienced in connection with assembling and utilizing systems comprising just-a-few single processors. As machine learning is mostly based on the conventional computing (processors), we scrutinize the (known, but somewhat faded) laws of the parallel computing, concerning AI. This paper attempts to review some of the caveats, especially concerning scaling the computing performance of the AI solutions. △ Less","2 May, 2020",https://arxiv.org/pdf/2005.00872
SEEK: Segmented Embedding of Knowledge Graphs,Wentao Xu;Shun Zheng;Liang He;Bin Shao;Jian Yin;Tie-Yan Liu,"In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering. However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory. To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity. Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations. It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases. Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework. Source codes and data can be found at \url{https://github.com/Wentao-Xu/SEEK}. △ Less","22 June, 2020",https://arxiv.org/pdf/2005.00856
ProtoQA: A Question Answering Dataset for Prototypical Common-Sense Reasoning,Michael Boratko;Xiang Lorraine Li;Rajarshi Das;Tim O'Gorman;Dan Le;Andrew McCallum,"Given questions regarding some prototypical situation such as Name something that people usually do before they leave the house for work? a human can easily answer them via acquired experiences. There can be multiple right answers for such questions, with some more common for a situation than others. This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations. The training set is gathered from an existing set of questions played in a long-running international game show FAMILY- FEUD. The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers. We also propose a generative evaluation task where a model has to output a ranked list of answers, ideally covering all prototypical answers for a question. After presenting multiple competitive baseline models, we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap, supporting the challenging nature of the task. △ Less","27 October, 2020",https://arxiv.org/pdf/2005.00771
Rebooting Neuromorphic Hardware Design -- A Complexity Engineering Approach,Natesh Ganesh,"As the compute demands for machine learning and artificial intelligence applications continue to grow, neuromorphic hardware has been touted as a potential solution. New emerging devices like memristors, atomic switches, etc have shown tremendous potential to replace CMOS-based circuits but have been hindered by multiple challenges with respect to device variability, stochastic behavior and scalability. In this paper we will introduce a Description<->Design framework to analyze past successes in computing, understand current problems and identify solutions moving forward. Engineering systems with these emerging devices might require the modification of both the type of descriptions of learning that we will design for, and the design methodologies we employ in order to realize these new descriptions. We will explore ideas from complexity engineering and analyze the advantages and challenges they offer over traditional approaches to neuromorphic design with novel computing fabrics. A reservoir computing example is used to understand the specific changes that would accompany in moving towards a complexity engineering approach. The time is ideal for a significant reboot of our design methodologies and success will represent a radical shift in how neuromorphic hardware is designed and pave the way for a new paradigm. △ Less","22 September, 2020",https://arxiv.org/pdf/2005.00522
Visuo-Linguistic Question Answering (VLQA) Challenge,Shailaja Keyur Sampat;Yezhou Yang;Chitta Baral,"Understanding images and text together is an important aspect of cognition and building advanced Artificial Intelligence (AI) systems. As a community, we have achieved good benchmarks over language and vision domains separately, however joint reasoning is still a challenge for state-of-the-art computer vision and natural language processing (NLP) systems. We propose a novel task to derive joint inference about a given image-text modality and compile the Visuo-Linguistic Question Answering (VLQA) challenge corpus in a question answering setting. Each dataset item consists of an image and a reading passage, where questions are designed to combine both visual and textual information i.e., ignoring either modality would make the question unanswerable. We first explore the best existing vision-language architectures to solve VLQA subsets and show that they are unable to reason well. We then develop a modular method with slightly better baseline performance, but it is still far behind human performance. We believe that VLQA will be a good benchmark for reasoning over a visuo-linguistic context. The dataset, code and leaderboard is available at https://shailaja183.github.io/vlqa/. △ Less","18 November, 2020",https://arxiv.org/pdf/2005.00330
TransOMCS: From Linguistic Graphs to Commonsense Knowledge,Hongming Zhang;Daniel Khashabi;Yangqiu Song;Dan Roth,"Commonsense knowledge acquisition is a key problem for artificial intelligence. Conventional methods of acquiring commonsense knowledge generally require laborious and costly human annotations, which are not feasible on a large scale. In this paper, we explore a practical way of mining commonsense knowledge from linguistic graphs, with the goal of transferring cheap knowledge obtained with linguistic patterns into expensive commonsense knowledge. The result is a conversion of ASER [Zhang et al., 2020], a large-scale selectional preference knowledge resource, into TransOMCS, of the same representation as ConceptNet [Liu and Singh, 2004] but two orders of magnitude larger. Experimental results demonstrate the transferability of linguistic knowledge to commonsense knowledge and the effectiveness of the proposed approach in terms of quantity, novelty, and quality. TransOMCS is publicly available at: https://github.com/HKUST-KnowComp/TransOMCS. △ Less","1 May, 2020",https://arxiv.org/pdf/2005.00206
"An Early Study on Intelligent Analysis of Speech under COVID-19: Severity, Sleep Quality, Fatigue, and Anxiety",Jing Han;Kun Qian;Meishu Song;Zijiang Yang;Zhao Ren;Shuo Liu;Juan Liu;Huaiyuan Zheng;Wei Ji;Tomoya Koike;Xiao Li;Zixing Zhang;Yoshiharu Yamamoto;Björn W. Schuller,"The COVID-19 outbreak was announced as a global pandemic by the World Health Organisation in March 2020 and has affected a growing number of people in the past few weeks. In this context, advanced artificial intelligence techniques are brought to the fore in responding to fight against and reduce the impact of this global health crisis. In this study, we focus on developing some potential use-cases of intelligent speech analysis for COVID-19 diagnosed patients. In particular, by analysing speech recordings from these patients, we construct audio-only-based models to automatically categorise the health state of patients from four aspects, including the severity of illness, sleep quality, fatigue, and anxiety. For this purpose, two established acoustic feature sets and support vector machines are utilised. Our experiments show that an average accuracy of .69 obtained estimating the severity of illness, which is derived from the number of days in hospitalisation. We hope that this study can foster an extremely fast, low-cost, and convenient way to automatically detect the COVID-19 disease. △ Less","14 May, 2020",https://arxiv.org/pdf/2005.00096
"Memristors -- from In-memory computing, Deep Learning Acceleration, Spiking Neural Networks, to the Future of Neuromorphic and Bio-inspired Computing",Adnan Mehonic;Abu Sebastian;Bipin Rajendran;Osvaldo Simeone;Eleni Vasilaki;Anthony J. Kenyon,"Machine learning, particularly in the form of deep learning, has driven most of the recent fundamental developments in artificial intelligence. Deep learning is based on computational models that are, to a certain extent, bio-inspired, as they rely on networks of connected simple computing units operating in parallel. Deep learning has been successfully applied in areas such as object/pattern recognition, speech and natural language processing, self-driving vehicles, intelligent self-diagnostics tools, autonomous robots, knowledgeable personal assistants, and monitoring. These successes have been mostly supported by three factors: availability of vast amounts of data, continuous growth in computing power, and algorithmic innovations. The approaching demise of Moore's law, and the consequent expected modest improvements in computing power that can be achieved by scaling, raise the question of whether the described progress will be slowed or halted due to hardware limitations. This paper reviews the case for a novel beyond CMOS hardware technology, memristors, as a potential solution for the implementation of power-efficient in-memory computing, deep learning accelerators, and spiking neural networks. Central themes are the reliance on non-von-Neumann computing architectures and the need for developing tailored learning and inference algorithms. To argue that lessons from biology can be useful in providing directions for further progress in artificial intelligence, we briefly discuss an example based reservoir computing. We conclude the review by speculating on the big picture view of future neuromorphic and brain-inspired computing systems. △ Less","30 April, 2020",https://arxiv.org/pdf/2004.14942
PeerNomination: Relaxing Exactness for Increased Accuracy in Peer Selection,Nicholas Mattei;Paolo Turrini;Stanislav Zhydkov,"In peer selection agents must choose a subset of themselves for an award or a prize. As agents are self-interested, we want to design algorithms that are impartial, so that an individual agent cannot affect their own chance of being selected. This problem has broad application in resource allocation and mechanism design and has received substantial attention in the artificial intelligence literature. Here, we present a novel algorithm for impartial peer selection, PeerNomination, and provide a theoretical analysis of its accuracy. Our algorithm possesses various desirable features. In particular, it does not require an explicit partitioning of the agents, as previous algorithms in the literature. We show empirically that it achieves higher accuracy than the exiting algorithms over several metrics. △ Less","30 April, 2020",https://arxiv.org/pdf/2004.14939
6G White Paper on Edge Intelligence,Ella Peltonen;Mehdi Bennis;Michele Capobianco;Merouane Debbah;Aaron Ding;Felipe Gil-Castiñeira;Marko Jurmu;Teemu Karvonen;Markus Kelanti;Adrian Kliks;Teemu Leppänen;Lauri Lovén;Tommi Mikkonen;Ashwin Rao;Sumudu Samarakoon;Kari Seppänen;Paweł Sroka;Sasu Tarkoma;Tingting Yang,"In this white paper we provide a vision for 6G Edge Intelligence. Moving towards 5G and beyond the future 6G networks, intelligent solutions utilizing data-driven machine learning and artificial intelligence become crucial for several real-world applications including but not limited to, more efficient manufacturing, novel personal smart device environments and experiences, urban computing and autonomous traffic settings. We present edge computing along with other 6G enablers as a key component to establish the future 2030 intelligent Internet technologies as shown in this series of 6G White Papers. In this white paper, we focus in the domains of edge computing infrastructure and platforms, data and edge network management, software development for edge, and real-time and distributed training of ML/AI algorithms, along with security, privacy, pricing, and end-user aspects. We discuss the key enablers and challenges and identify the key research questions for the development of the Intelligent Edge services. As a main outcome of this white paper, we envision a transition from Internet of Things to Intelligent Internet of Intelligent Things and provide a roadmap for development of 6G Intelligent Edge. △ Less","30 April, 2020",https://arxiv.org/pdf/2004.14850
A Survey on Time-Sensitive Resource Allocation in the Cloud Continuum,Saravanan Ramanathan;Nitin Shivaraman;Seima Suryasekaran;Arvind Easwaran;Etienne Borde;Sebastian Steinhorst,"Artificial Intelligence (AI) and Internet of Things (IoT) applications are rapidly growing in today's world where they are continuously connected to the internet and process, store and exchange information among the devices and the environment. The cloud and edge platform is very crucial to these applications due to their inherent compute-intensive and resource-constrained nature. One of the foremost challenges in cloud and edge resource allocation is the efficient management of computation and communication resources to meet the performance and latency guarantees of the applications. The heterogeneity of cloud resources (processors, memory, storage, bandwidth), variable cost structure and unpredictable workload patterns make the design of resource allocation techniques complex. Numerous research studies have been carried out to address this intricate problem. In this paper, the current state-of-the-art resource allocation techniques for the cloud continuum, in particular those that consider time-sensitive applications, are reviewed. Furthermore, we present the key challenges in the resource allocation problem for the cloud continuum, a taxonomy to classify the existing literature and the potential research gaps. △ Less","29 April, 2020",https://arxiv.org/pdf/2004.14559
Advancing computerized cognitive training for early Alzheimer's disease in a Covid-19 pandemic and post-pandemic world,Kaylee A. Bodner;Terry E. Goldberg;D. P. Devanand;P. Murali Doraiswamy,"The COVID-19 pandemic has transformed mobile health applications and telemedicine from nice to have tools into essential healthcare infrastructure. This need is particularly great for the elderly who, due to their greater risk for infection, may avoid medical facilities or be required to self-isolate. These are also the very groups at highest risk for cognitive decline. For example, during the COVID-19 pandemic artificially intelligent conversational agents were employed by hospitals and government agencies (such as the CDC) to field queries from patients about symptoms and treatments. Digital health tools also proved invaluable to provide neuropsychiatric and psychological self-help to people isolated at home or in retirement centers and nursing homes. △ Less","15 May, 2020",https://arxiv.org/pdf/2004.14344
The Holy Grail of Quantum Artificial Intelligence: Major Challenges in Accelerating the Machine Learning Pipeline,Thomas Gabor;Leo Sünkel;Fabian Ritz;Thomy Phan;Lenz Belzner;Christoph Roch;Sebastian Feld;Claudia Linnhoff-Popien,"We discuss the synergetic connection between quantum computing and artificial intelligence. After surveying current approaches to quantum artificial intelligence and relating them to a formal model for machine learning processes, we deduce four major challenges for the future of quantum artificial intelligence: (i) Replace iterative training with faster quantum algorithms, (ii) distill the experience of larger amounts of data into the training process, (iii) allow quantum and classical components to be easily combined and exchanged, and (iv) build tools to thoroughly analyze whether observed benefits really stem from quantum properties of the algorithm. △ Less","29 April, 2020",https://arxiv.org/pdf/2004.14035
Neural translation and automated recognition of ICD10 medical entities from natural language,Louis Falissard;Claire Morgand;Sylvie Roussel;Claire Imbaud;Walid Ghosn;Karim Bounebache;Grégoire Rey,"The recognition of medical entities from natural language is an ubiquitous problem in the medical field, with applications ranging from medical act coding to the analysis of electronic health data for public health. It is however a complex task usually requiring human expert intervention, thus making it expansive and time consuming. The recent advances in artificial intelligence, specifically the raise of deep learning methods, has enabled computers to make efficient decisions on a number of complex problems, with the notable example of neural sequence models and their powerful applications in natural language processing. They however require a considerable amount of data to learn from, which is typically their main limiting factor. However, the CépiDc stores an exhaustive database of death certificates at the French national scale, amounting to several millions of natural language examples provided with their associated human coded medical entities available to the machine learning practitioner. This article investigates the applications of deep neural sequence models to the medical entity recognition from natural language problem. △ Less","6 May, 2020",https://arxiv.org/pdf/2004.13839
Unifying Neural Learning and Symbolic Reasoning for Spinal Medical Report Generation,Zhongyi Han;Benzheng Wei;Yilong Yin;Shuo Li,"Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When it employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation as well as show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis. △ Less","28 April, 2020",https://arxiv.org/pdf/2004.13577
Communication-Efficient Edge AI Inference Over Wireless Networks,Kai Yang;Yong Zhou;Zhanpeng Yang;Yuanming Shi,"Given the fast growth of intelligent devices, it is expected that a large number of high-stake artificial intelligence (AI) applications, e.g., drones, autonomous cars, tactile robots, will be deployed at the edge of wireless networks in the near future. As such, the intelligent communication networks will be designed to leverage advanced wireless techniques and edge computing technologies to support AI-enabled applications at various end devices with limited communication, computation, hardware and energy resources. In this article, we shall present the principles of efficient deployment of model inference at network edge to provide low-latency and energy-efficient AI services. This includes the wireless distributed computing framework for low-latency device distributed model inference as well as the wireless cooperative transmission strategy for energy-efficient edge cooperative model inference. The communication efficiency of edge inference systems is further improved by building up a smart radio propagation environment via intelligent reflecting surface. △ Less","28 April, 2020",https://arxiv.org/pdf/2004.13351
Natural language processing for achieving sustainable development: the case of neural labelling to enhance community profiling,Costanza Conforti;Stephanie Hirmer;David Morgan;Marco Basaldella;Yau Ben Or,"In recent years, there has been an increasing interest in the application of Artificial Intelligence - and especially Machine Learning - to the field of Sustainable Development (SD). However, until now, NLP has not been applied in this context. In this research paper, we show the high potential of NLP applications to enhance the sustainability of projects. In particular, we focus on the case of community profiling in developing countries, where, in contrast to the developed world, a notable data gap exists. In this context, NLP could help to address the cost and time barrier of structuring qualitative data that prohibits its widespread use and associated benefits. We propose the new task of Automatic UPV classification, which is an extreme multi-class multi-label classification problem. We release Stories2Insights, an expert-annotated dataset, provide a detailed corpus analysis, and implement a number of strong neural baselines to address the task. Experimental results show that the problem is challenging, and leave plenty of room for future research at the intersection of NLP and SD. △ Less","17 November, 2020",https://arxiv.org/pdf/2004.12935
A New Age of Computing and the Brain,Polina Golland;Jack Gallant;Greg Hager;Hanspeter Pfister;Christos Papadimitriou;Stefan Schaal;Joshua T. Vogelstein,"The history of computer science and brain sciences are intertwined. In his unfinished manuscript ""The Computer and the Brain,"" von Neumann debates whether or not the brain can be thought of as a computing machine and identifies some of the similarities and differences between natural and artificial computation. Turing, in his 1950 article in Mind, argues that computing devices could ultimately emulate intelligence, leading to his proposed Turing test. Herbert Simon predicted in 1957 that most psychological theories would take the form of a computer program. In 1976, David Marr proposed that the function of the visual system could be abstracted and studied at computational and algorithmic levels that did not depend on the underlying physical substrate. In December 2014, a two-day workshop supported by the Computing Community Consortium (CCC) and the National Science Foundation's Computer and Information Science and Engineering Directorate (NSF CISE) was convened in Washington, DC, with the goal of bringing together computer scientists and brain researchers to explore these new opportunities and connections, and develop a new, modern dialogue between the two research communities. Specifically, our objectives were: 1. To articulate a conceptual framework for research at the interface of brain sciences and computing and to identify key problems in this interface, presented in a way that will attract both CISE and brain researchers into this space. 2. To inform and excite researchers within the CISE research community about brain research opportunities and to identify and explain strategic roles they can play in advancing this initiative. 3. To develop new connections, conversations and collaborations between brain sciences and CISE researchers that will lead to highly relevant and competitive proposals, high-impact research, and influential publications. △ Less","27 April, 2020",https://arxiv.org/pdf/2004.12926
"AI-Driven CT-based quantification, staging and short-term outcome prediction of COVID-19 pneumonia",Guillaume Chassagnon;Maria Vakalopoulou;Enzo Battistella;Stergios Christodoulidis;Trieu-Nghi Hoang-Thi;Severine Dangeard;Eric Deutsch;Fabrice Andre;Enora Guillo;Nara Halm;Stefany El Hajj;Florian Bompard;Sophie Neveu;Chahinez Hani;Ines Saab;Alienor Campredon;Hasmik Koulakian;Souhail Bennani;Gael Freche;Aurelien Lombard;Laure Fournier;Hippolyte Monnier;Teodor Grand;Jules Gregory;Antoine Khalil,"Chest computed tomography (CT) is widely used for the management of Coronavirus disease 2019 (COVID-19) pneumonia because of its availability and rapidity. The standard of reference for confirming COVID-19 relies on microbiological tests but these tests might not be available in an emergency setting and their results are not immediately available, contrary to CT. In addition to its role for early diagnosis, CT has a prognostic role by allowing visually evaluating the extent of COVID-19 lung abnormalities. The objective of this study is to address prediction of short-term outcomes, especially need for mechanical ventilation. In this multi-centric study, we propose an end-to-end artificial intelligence solution for automatic quantification and prognosis assessment by combining automatic CT delineation of lung disease meeting performance of experts and data-driven identification of biomarkers for its prognosis. AI-driven combination of variables with CT-based biomarkers offers perspectives for optimal patient management given the shortage of intensive care beds and ventilators. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.12852
Adaptive model selection in photonic reservoir computing by reinforcement learning,Kazutaka Kanno;Makoto Naruse;Atsushi Uchida,"Photonic reservoir computing is an emergent technology toward beyond-Neumann computing. Although photonic reservoir computing provides superior performance in environments whose characteristics are coincident with the training datasets for the reservoir, the performance is significantly degraded if these characteristics deviate from the original knowledge used in the training phase. Here, we propose a scheme of adaptive model selection in photonic reservoir computing using reinforcement learning. In this scheme, a temporal waveform is generated by different dynamic source models that change over time. The system autonomously identifies the best source model for the task of time series prediction using photonic reservoir computing and reinforcement learning. We prepare two types of output weights for the source models, and the system adaptively selected the correct model using reinforcement learning, where the prediction errors are associated with rewards. We succeed in adaptive model selection when the source signal is temporally mixed, having originally been generated by two different dynamic system models, as well as when the signal is a mixture from the same model but with different parameter values. This study paves the way for autonomous behavior in photonic artificial intelligence and could lead to new applications in load forecasting and multi-objective control, where frequent environment changes are expected. △ Less","26 April, 2020",https://arxiv.org/pdf/2004.12575
Predicting Plans and Actions in Two-Player Repeated Games,Najma Mathema;Michael A. Goodrich;Jacob W. Crandall,"Artificial intelligence (AI) agents will need to interact with both other AI agents and humans. Creating models of associates help to predict the modeled agents' actions, plans, and intentions. This work introduces algorithms that predict actions, plans and intentions in repeated play games, with providing an exploration of algorithms. We form a generative Bayesian approach to model S#. S# is designed as a robust algorithm that learns to cooperate with its associate in 2 by 2 matrix games. The actions, plans and intentions associated with each S# expert are identified from the literature, grouping the S# experts accordingly, and thus predicting actions, plans, and intentions based on their state probabilities. Two prediction methods are explored for Prisoners Dilemma: the Maximum A Posteriori (MAP) and an Aggregation approach. MAP (~89% accuracy) performed the best for action prediction. Both methods predicted plans of S# with ~88% accuracy. Paired T-test shows that MAP performs significantly better than Aggregation for predicting S#'s actions without cheap talk. Intention is explored based on the goals of the S# experts; results show that goals are predicted precisely when modeling S#. The obtained results show that the proposed Bayesian approach is well suited for modeling agents in two-player repeated games. △ Less","26 April, 2020",https://arxiv.org/pdf/2004.12480
Power of Artificial Intelligence to Diagnose and Prevent Further COVID-19 Outbreak: A Short Communication,Muhammad Lawan Jibril;Usman Sani Sharif,"Novel coronavirus-19 (2019-nCoV or COVID-19) is by far the most dangerous coronavirus ever identified for the third time in the three decades capable of infecting not only the animals but also the humans across the globe. Nearly 6000 deaths have been recorded due mainly to COVID-19 outbreak worldwide and more than 50% of these deaths appeared to have evolved from China where the virus was thought to originate. The endemicity of COVID-19 dramatically surpassed severe acute respiratory syndrome coronavirus (SARS-CoV) and Middle East respiratory syndrome coronavirus (MERS-CoV) that were so far discovered in 2003 and 2012 respectively. Thus, the World Health Organization (WHO) has declared the 2019-nCoV outbreak not only a public health emergency but also pandemic in nature. Currently, over 120 countries including Nigeria were reported to have more than 157,844 confirmed cases and 5,846 deaths due mainly to COVID-19 outbreak as of March 15, 2020, 10:55 GMT. Artificial Intelligence (AI) is widely used to aid in the prediction, detection, response, recovery of disease and making clinical diagnosis. In this study, we highlighted the power of AI in the containment and mitigation of the spread of COVID-19 outbreak in African countries such as Nigeria where human to human contact is apparently inevitable. △ Less","26 April, 2020",https://arxiv.org/pdf/2004.12463
KrakN: Transfer Learning framework for thin crack detection in infrastructure maintenance,Mateusz Żarski;Bartosz Wójcik;Jarosław Adam Miszczak,"Monitoring the technical condition of infrastructure is a crucial element to its maintenance. Currently applied methods are outdated, labour-intensive and inaccurate. At the same time, the latest methods using Artificial Intelligence techniques are severely limited in their application due to two main factors -- labour-intensive gathering of new datasets and high demand for computing power. We propose to utilize custom made framework -- KrakN, to overcome these limiting factors. It enables the development of unique infrastructure defects detectors on digital images, achieving the accuracy of above 90%. The framework supports semi-automatic creation of new datasets and has modest computing power requirements. It is implemented in the form of a ready-to-use software package openly distributed to the public. Thus, it can be used to immediately implement the methods proposed in this paper in the process of infrastructure management by government units, regardless of their financial capabilities. △ Less","11 October, 2020",https://arxiv.org/pdf/2004.12337
An Extension of LIME with Improvement of Interpretability and Fidelity,Sheng Shi;Yangzhou Du;Wei Fan,"While deep learning makes significant achievements in Artificial Intelligence (AI), the lack of transparency has limited its broad application in various vertical domains. Explainability is not only a gateway between AI and real world, but also a powerful feature to detect flaw of the models and bias of the data. Local Interpretable Model-agnostic Explanation (LIME) is a widely-accepted technique that explains the prediction of any classifier faithfully by learning an interpretable model locally around the predicted instance. As an extension of LIME, this paper proposes an high-interpretability and high-fidelity local explanation method, known as Local Explanation using feature Dependency Sampling and Nonlinear Approximation (LEDSNA). Given an instance being explained, LEDSNA enhances interpretability by feature sampling with intrinsic dependency. Besides, LEDSNA improves the local explanation fidelity by approximating nonlinear boundary of local decision. We evaluate our method with classification tasks in both image domain and text domain. Experiments show that LEDSNA's explanation of the back-box model achieves much better performance than original LIME in terms of interpretability and fidelity. △ Less","25 April, 2020",https://arxiv.org/pdf/2004.12277
How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,Haoxi Zhong;Chaojun Xiao;Cunchao Tu;Tianyang Zhang;Zhiyuan Liu;Maosong Sun,"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM. △ Less","18 May, 2020",https://arxiv.org/pdf/2004.12158
Quantum machine learning and quantum biomimetics: A perspective,Lucas Lamata,"Quantum machine learning has emerged as an exciting and promising paradigm inside quantum technologies. It may permit, on the one hand, to carry out more efficient machine learning calculations by means of quantum devices, while, on the other hand, to employ machine learning techniques to better control quantum systems. Inside quantum machine learning, quantum reinforcement learning aims at developing ""intelligent"" quantum agents that may interact with the outer world and adapt to it, with the strategy of achieving some final goal. Another paradigm inside quantum machine learning is that of quantum autoencoders, which may allow one for employing fewer resources in a quantum device via a training process. Moreover, the field of quantum biomimetics aims at establishing analogies between biological and quantum systems, to look for previously inadvertent connections that may enable useful applications. Two recent examples are the concepts of quantum artificial life, as well as of quantum memristors. In this Perspective, we give an overview of these topics, describing the related research carried out by the scientific community. △ Less","30 May, 2020",https://arxiv.org/pdf/2004.12076
SAIA: Split Artificial Intelligence Architecture for Mobile Healthcare System,Di Zhuang;Nam Nguyen;Keyu Chen;J. Morris Chang,"As the advancement of deep learning (DL), the Internet of Things and cloud computing techniques for biomedical and healthcare problems, mobile healthcare systems have received unprecedented attention. Since DL techniques usually require enormous amount of computation, most of them cannot be directly deployed on the resource-constrained mobile and IoT devices. Hence, most of the mobile healthcare systems leverage the cloud computing infrastructure, where the data collected by the mobile and IoT devices would be transmitted to the cloud computing platforms for analysis. However, in the contested environments, relying on the cloud might not be practical at all times. For instance, the satellite communication might be denied or disrupted. We propose SAIA, a Split Artificial Intelligence Architecture for mobile healthcare systems. Unlike traditional approaches for artificial intelligence (AI) which solely exploits the computational power of the cloud server, SAIA could not only relies on the cloud computing infrastructure while the wireless communication is available, but also utilizes the lightweight AI solutions that work locally on the client side, hence, it can work even when the communication is impeded. In SAIA, we propose a meta-information based decision unit, that could tune whether a sample captured by the client should be operated by the embedded AI (i.e., keeping on the client) or the networked AI (i.e., sending to the server), under different conditions. In our experimental evaluation, extensive experiments have been conducted on two popular healthcare datasets. Our results show that SAIA consistently outperforms its baselines in terms of both effectiveness and efficiency. △ Less","9 May, 2020",https://arxiv.org/pdf/2004.12059
Question Answering over Curated and Open Web Sources,Rishiraj Saha Roy;Avishek Anand,"The last few years have seen an explosion of research on the topic of automated question answering (QA), spanning the communities of information retrieval, natural language processing, and artificial intelligence. This tutorial would cover the highlights of this really active period of growth for QA to give the audience a grasp over the families of algorithms that are currently being used. We partition research contributions by the underlying source from where answers are retrieved: curated knowledge graphs, unstructured text, or hybrid corpora. We choose this dimension of partitioning as it is the most discriminative when it comes to algorithm design. Other key dimensions are covered within each sub-topic: like the complexity of questions addressed, and degrees of explainability and interactivity introduced in the systems. We would conclude the tutorial with the most promising emerging trends in the expanse of QA, that would help new entrants into this field make the best decisions to take the community forward. Much has changed in the community since the last tutorial on QA in SIGIR 2016, and we believe that this timely overview will indeed benefit a large number of conference participants. △ Less","7 August, 2020",https://arxiv.org/pdf/2004.11980
"Deep learning for smart fish farming: applications, opportunities and challenges",Xinting Yang;Song Zhang;Jintao Liu;Qinfeng Gao;Shuanglin Dong;Chao Zhou,"With the rapid emergence of deep learning (DL) technology, it has been successfully used in various fields including aquaculture. This change can create new opportunities and a series of challenges for information and data processing in smart fish farming. This paper focuses on the applications of DL in aquaculture, including live fish identification, species classification, behavioral analysis, feeding decision-making, size or biomass estimation, water quality prediction. In addition, the technical details of DL methods applied to smart fish farming are also analyzed, including data, algorithms, computing power, and performance. The results of this review show that the most significant contribution of DL is the ability to automatically extract features. However, challenges still exist; DL is still in an era of weak artificial intelligence. A large number of labeled data are needed for training, which has become a bottleneck restricting further DL applications in aquaculture. Nevertheless, DL still offers breakthroughs in the handling of complex data in aquaculture. In brief, our purpose is to provide researchers and practitioners with a better understanding of the current state of the art of DL in aquaculture, which can provide strong support for the implementation of smart fish farming. △ Less","30 June, 2020",https://arxiv.org/pdf/2004.11848
Robust Transmission Design for Intelligent Reflecting Surface Aided Secure Communication Systems with Imperfect Cascaded CSI,Sheng Hong;Cunhua Pan;Hong Ren;Kezhi Wang;Kok Keong Chai;Arumugam Nallanathan,"In this paper, we investigate the design of robust and secure transmission in intelligent reflecting surface (IRS) aided wireless communication systems. In particular, a multi-antenna access point (AP) communicates with a single-antenna legitimate receiver in the presence of multiple single-antenna eavesdroppers, where the artificial noise (AN) is transmitted to enhance the security performance. Besides, we assume that the cascaded AP-IRS-user channels are imperfect due to the channel estimation error. To minimize the transmit power, the beamforming vector at the transmitter, the AN covariance matrix, and the IRS phase shifts are jointly optimized subject to the outage rate probability constraints under the statistical cascaded channel state information (CSI) error model that usually models the channel estimation error. To handle the resulting non-convex optimization problem, we first approximate the outage rate probability constraints by using the Bernstein-type inequality. Then, we develop a suboptimal algorithm based on alternating optimization, the penalty-based and semidefinite relaxation methods. Simulation results reveal that the proposed scheme significantly reduces the transmit power compared to other benchmark schemes. △ Less","3 December, 2020",https://arxiv.org/pdf/2004.11580
Why an Android App is Classified as Malware? Towards Malware Classification Interpretation,Bozhi Wu;Sen Chen;Cuiyun Gao;Lingling Fan;Yang Liu;Weiping Wen;Michael R. Lyu,"Machine learning (ML) based approach is considered as one of the most promising techniques for Android malware detection and has achieved high accuracy by leveraging commonly-used features. In practice, most of the ML classifications only provide a binary label to mobile users and app security analysts. However, stakeholders are more interested in the reason why apps are classified as malicious in both academia and industry. This belongs to the research area of interpretable ML but in a specific research domain (i.e., mobile malware detection). Although several interpretable ML methods have been exhibited to explain the final classification results in many cutting-edge Artificial Intelligent (AI) based research fields, till now, there is no study interpreting why an app is classified as malware or unveiling the domain-specific challenges. In this paper, to fill this gap, we propose a novel and interpretable ML-based approach (named XMal) to classify malware with high accuracy and explain the classification result meanwhile. (1) The first classification phase of XMal hinges multi-layer perceptron (MLP) and attention mechanism, and also pinpoints the key features most related to the classification result. (2) The second interpreting phase aims at automatically producing neural language descriptions to interpret the core malicious behaviors within apps. We evaluate the behavior description results by comparing with the existing interpretable ML-based methods (i.e., Drebin and LIME) to demonstrate the effectiveness of XMal. We find that XMal is able to reveal the malicious behaviors more accurately. Additionally, our experiments show that XMal can also interpret the reason why some samples are misclassified by ML classifiers. Our study peeks into the interpretable ML through the research of Android malware detection and analysis. △ Less","4 September, 2020",https://arxiv.org/pdf/2004.11516
Responsible AI and Its Stakeholders,Gabriel Lima;Meeyoung Cha,"Responsible Artificial Intelligence (AI) proposes a framework that holds all stakeholders involved in the development of AI to be responsible for their systems. It, however, fails to accommodate the possibility of holding AI responsible per se, which could close some legal and moral gaps concerning the deployment of autonomous and self-learning systems. We discuss three notions of responsibility (i.e., blameworthiness, accountability, and liability) for all stakeholders, including AI, and suggest the roles of jurisdiction and the general public in this matter. △ Less","23 April, 2020",https://arxiv.org/pdf/2004.11434
Human-Machine Collaboration for Democratizing Data Science,Clément Gautrais;Yann Dauxais;Stefano Teso;Samuel Kolb;Gust Verbruggen;Luc De Raedt,"Everybody wants to analyse their data, but only few posses the data science expertise to to this. Motivated by this observation we introduce a novel framework and system \textsc{VisualSynth} for human-machine collaboration in data science. It wants to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. \textsc{VisualSynth} relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques. △ Less","23 April, 2020",https://arxiv.org/pdf/2004.11113
Local Adaptation Improves Accuracy of Deep Learning Model for Automated X-Ray Thoracic Disease Detection : A Thai Study,Isarun Chamveha;Trongtum Tongdee;Pairash Saiviroonporn;Warasinee Chaisangmongkon,"Despite much promising research in the area of artificial intelligence for medical image diagnosis, there has been no large-scale validation study done in Thailand to confirm the accuracy and utility of such algorithms when applied to local datasets. Here we present a wide-reaching development and testing of a deep learning algorithm for automated thoracic disease detection, utilizing 421,859 local chest radiographs. Our study shows that convolutional neural networks can achieve remarkable performance in detecting 13 common abnormality conditions on chest X-ray, and the incorporation of local images into the training set is key to the model's success. This paper presents a state-of-the-art model for CXR abnormality detection, reaching an average AUROC of 0.91. This model, if integrated to the workflow, can result in up to 55.6% work reduction for medical practitioners in the CXR analysis process. Our work emphasizes the importance of investing in local research of medical diagnosis algorithms to ensure safe and efficient usage within the intended region. △ Less","12 May, 2020",https://arxiv.org/pdf/2004.10975
Few-Shot Class-Incremental Learning,Xiaoyu Tao;Xiaopeng Hong;Xinyuan Chang;Songlin Dong;Xing Wei;Yihong Gong,"The ability to incrementally learn new classes is crucial to the development of real-world artificial intelligence systems. In this paper, we focus on a challenging but practical few-shot class-incremental learning (FSCIL) problem. FSCIL requires CNN models to incrementally learn new classes from very few labelled samples, without forgetting the previously learned ones. To address this problem, we represent the knowledge using a neural gas (NG) network, which can learn and preserve the topology of the feature manifold formed by different classes. On this basis, we propose the TOpology-Preserving knowledge InCrementer (TOPIC) framework. TOPIC mitigates the forgetting of the old classes by stabilizing NG's topology and improves the representation learning for few-shot new classes by growing and adapting NG to new training samples. Comprehensive experimental results demonstrate that our proposed method significantly outperforms other state-of-the-art class-incremental learning methods on CIFAR100, miniImageNet, and CUB200 datasets. △ Less","23 April, 2020",https://arxiv.org/pdf/2004.10956
PERMDNN: Efficient Compressed DNN Architecture with Permuted Diagonal Matrices,Chunhua Deng;Siyu Liao;Yi Xie;Keshab K. Parhi;Xuehai Qian;Bo Yuan,"Deep neural network (DNN) has emerged as the most important and popular artificial intelligent (AI) technique. The growth of model size poses a key energy efficiency challenge for the underlying computing platform. Thus, model compression becomes a crucial problem. However, the current approaches are limited by various drawbacks. Specifically, network sparsification approach suffers from irregularity, heuristic nature and large indexing overhead. On the other hand, the recent structured matrix-based approach (i.e., CirCNN) is limited by the relatively complex arithmetic computation (i.e., FFT), less flexible compression ratio, and its inability to fully utilize input sparsity. To address these drawbacks, this paper proposes PermDNN, a novel approach to generate and execute hardware-friendly structured sparse DNN models using permuted diagonal matrices. Compared with unstructured sparsification approach, PermDNN eliminates the drawbacks of indexing overhead, non-heuristic compression effects and time-consuming retraining. Compared with circulant structure-imposing approach, PermDNN enjoys the benefits of higher reduction in computational complexity, flexible compression ratio, simple arithmetic computation and full utilization of input sparsity. We propose PermDNN architecture, a multi-processing element (PE) fully-connected (FC) layer-targeted computing engine. The entire architecture is highly scalable and flexible, and hence it can support the needs of different applications with different model configurations. We implement a 32-PE design using CMOS 28nm technology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout, 5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher throughput and 3.89x better energy efficiency. △ Less","22 April, 2020",https://arxiv.org/pdf/2004.10936
Hybrid Blockchain-Enabled Secure Microservices Fabric for Decentralized Multi-Domain Avionics Systems,Ronghua Xu;Yu Chen;Erik Blasch;Alexander Aved;Genshe Chen;Dan Shen,"Advancement in artificial intelligence (AI) and machine learning (ML), dynamic data driven application systems (DDDAS), and hierarchical cloud-fog-edge computing paradigm provide opportunities for enhancing multi-domain systems performance. As one example that represents multi-domain scenario, a ""fly-by-feel"" system utilizes DDDAS framework to support autonomous operations and improve maneuverability, safety and fuel efficiency. The DDDAS ""fly-by-feel"" avionics system can enhance multi-domain coordination to support domain specific operations. However, conventional enabling technologies rely on a centralized manner for data aggregation, sharing and security policy enforcement, and it incurs critical issues related to bottleneck of performance, data provenance and consistency. Inspired by the containerized microservices and blockchain technology, this paper introduces BLEM, a hybrid BLockchain-Enabled secure Microservices fabric to support decentralized, secure and efficient data fusion and multi-domain operations for avionics systems. Leveraging the fine-granularity and loose-coupling features of the microservices architecture, multidomain operations and security functionalities are decoupled into multiple containerized microservices. A hybrid blockchain fabric based on two-level committee consensus protocols is proposed to enable decentralized security architecture and support immutability, auditability and traceability for data provenience in existing multi-domain avionics system. Our evaluation results show the feasibility of the proposed BLEM mechanism to support decentralized security service and guarantee immutability, auditability and traceability for data provenience across domain boundaries. △ Less","15 April, 2020",https://arxiv.org/pdf/2004.10674
Where is the context? -- A critique of recent dialogue datasets,Johannes E. M. Mosig;Vladimir Vlasov;Alan Nichol,"Recent dialogue datasets like MultiWOZ 2.1 and Taskmaster-1 constitute some of the most challenging tasks for present-day dialogue models and, therefore, are widely used for system evaluation. We identify several issues with the above-mentioned datasets, such as history independence, strong knowledge base dependence, and ambiguous system responses. Finally, we outline key desiderata for future datasets that we believe would be more suitable for the construction of conversational artificial intelligence. △ Less","22 April, 2020",https://arxiv.org/pdf/2004.10473
A Deep Learning System for Sentiment Analysis of Service Calls,Yanan Jia;Sony SungChu,"Sentiment analysis is crucial for the advancement of artificial intelligence (AI). Sentiment understanding can help AI to replicate human language and discourse. Studying the formation and response of sentiment state from well-trained Customer Service Representatives (CSRs) can help make the interaction between humans and AI more intelligent. In this paper, a sentiment analysis pipeline is first carried out with respect to real-world multi-party conversations - that is, service calls. Based on the acoustic and linguistic features extracted from the source information, a novel aggregated method for voice sentiment recognition framework is built. Each party's sentiment pattern during the communication is investigated along with the interaction sentiment pattern between all parties. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.10320
Observations on Annotations,Georg Rehm,"The annotation of textual information is a fundamental activity in Linguistics and Computational Linguistics. This article presents various observations on annotations. It approaches the topic from several angles including Hypertext, Computational Linguistics and Language Technology, Artificial Intelligence and Open Science. Annotations can be examined along different dimensions. In terms of complexity, they can range from trivial to highly sophisticated, in terms of maturity from experimental to standardised. Annotations can be annotated themselves using more abstract annotations. Primary research data such as, e.g., text documents can be annotated on different layers concurrently, which are independent but can be exploited using multi-layer querying. Standards guarantee interoperability and reusability of data sets. The chapter concludes with four final observations, formulated as research questions or rather provocative remarks on the current state of annotation research. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.10283
Implementing AI Ethics in Practice: An Empirical Evaluation of the RESOLVEDD Strategy,Ville Vakkuri;Kai-Kristian Kemell,"As Artificial Intelligence (AI) systems exert a growing influence on society, real-life incidents begin to underline the importance of AI Ethics. Though calls for more ethical AI systems have been voiced by scholars and the general public alike, few empirical studies on the topic exist. Similarly, few tools and methods designed for implementing AI ethics into practice currently exist. To provide empirical data into this on-going discussion, we empirically evaluate an existing method from the field of business ethics, the RESOLVEDD strategy, in the context of ethical system development. We evaluated RESOLVEDD by means of a multiple case study of five student projects where its use was given as one of the design requirements for the projects. One of our key findings is that, even though the use of the ethical method was forced upon the participants, its utilization nonetheless facilitated of ethical consideration in the projects. Specifically, it resulted in the developers displaying more responsibility, even though the use of the tool did not stem from intrinsic motivation. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.10191
COVID-19 and Company Knowledge Graphs: Assessing Golden Powers and Economic Impact of Selective Lockdown via AI Reasoning,Luigi Bellomarini;Marco Benedetti;Andrea Gentili;Rosario Laurendi;Davide Magnanimi;Antonio Muci;Emanuel Sallinger,"In the COVID-19 outbreak, governments have applied progressive restrictions to production activities, permitting only those that are considered strategic or that provide essential services. This is particularly apparent in countries that have been stricken hard by the virus, with Italy being a major example. Yet we know that companies are not just isolated entities: They organize themselves into intricate shareholding structures --- forming company networks --- distributing decision power and dividends in sophisticated schemes for various purposes. One tool from the Artificial Intelligence (AI) toolbox that is particularly effective to perform reasoning tasks on domains characterized by many entities highly interconnected with one another is Knowledge Graphs (KG). In this work, we present a visionary opinion and report on ongoing work about the application of Automated Reasoning and Knowledge Graph technology to address the impact of the COVID-19 outbreak on the network of Italian companies and support the application of legal instruments for the protection of strategic companies from takeovers. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.10119
ICT Intervention in the Containment of the Pandemic Spread of COVID-19: An Exploratory Study,Akib Zaman;Muhammad Nazrul Islam;Tarannum Zaki;Mohammad Sajjad Hossain,"The objective of this article is to explore the Information and Communication Technology (ICT) interventions and its strengths, weaknesses, opportunities and threats for the containment of the pandemic spread of novel Coronavirus. The research adopted a qualitative research approach, while the study data were collected through online content review and Focus Group Discussion (FGD). Starting with a preliminary set of about 1200 electronic resources or contents, 56 were selected for review study, applying an inclusion and exclusion criteria. The review study revealed ICT interventions that include websites and dashboards, mobile applications, robotics and drones, artificial intelligence (AI), data analytic, wearable and sensor technology, social media and learning tools, and interactive voice response (IVR) as well as explored their respective usages to combat the pandemic spread of COVID-19. Later, the FGD was replicated with 22 participants and explored the possible strengths, weaknesses, opportunities, and threats (SWOT) of deploying such technologies to fight against the COVID-19 pandemic. This research not only explores the exiting status of ICT interventions to fight with the COVID-19 pandemic but also provides a number of implications for the government, practitioners, doctors, policymakers and researchers for the effective utilization of the existing ICT interventions and for the future potential research and technological development to the containment of the pandemic spread of COVID-19 and future pandemics. △ Less","21 April, 2020",https://arxiv.org/pdf/2004.09888
Computer Vision For COVID-19 Control: A Survey,Anwaar Ulhaq;Asim Khan;Douglas Gomes;Manoranjan Paul,"The COVID-19 pandemic has triggered an urgent need to contribute to the fight against an immense threat to the human population. Computer Vision, as a subfield of Artificial Intelligence, has enjoyed recent success in solving various complex problems in health care and has the potential to contribute to the fight of controlling COVID-19. In response to this call, computer vision researchers are putting their knowledge base at work to devise effective ways to counter COVID-19 challenge and serve the global community. New contributions are being shared with every passing day. It motivated us to review the recent work, collect information about available research resources and an indication of future research directions. We want to make it available to computer vision researchers to save precious time. This survey paper is intended to provide a preliminary review of the available literature on the computer vision efforts against COVID-19 pandemic. △ Less","5 May, 2020",https://arxiv.org/pdf/2004.09420
A Novel Multi-Agent System for Complex Scheduling Problems,Peter Hillmann;Tobias Uhlig;Gabi Dreo Rodosek;Oliver Rose,"Complex scheduling problems require a large amount computation power and innovative solution methods. The objective of this paper is the conception and implementation of a multi-agent system that is applicable in various problem domains. Independent specialized agents handle small tasks, to reach a superordinate target. Effective coordination is therefore required to achieve productive cooperation. Role models and distributed artificial intelligence are employed to tackle the resulting challenges. We simulate a NP-hard scheduling problem to demonstrate the validity of our approach. In addition to the general agent based framework we propose new simulation-based optimization heuristics to given scheduling problems. Two of the described optimization algorithms are implemented using agents. This paper highlights the advantages of the agent-based approach, like the reduction in layout complexity, improved control of complicated systems, and extendability. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.09312
A Practical Guide to Studying Emergent Communication through Grounded Language Games,Jens Nevens;Paul Van Eecke;Katrien Beuls,"The question of how an effective and efficient communication system can emerge in a population of agents that need to solve a particular task attracts more and more attention from researchers in many fields, including artificial intelligence, linguistics and statistical physics. A common methodology for studying this question consists of carrying out multi-agent experiments in which a population of agents takes part in a series of scripted and task-oriented communicative interactions, called 'language games'. While each individual language game is typically played by two agents in the population, a large series of games allows the population to converge on a shared communication system. Setting up an experiment in which a rich system for communicating about the real world emerges is a major enterprise, as it requires a variety of software components for running multi-agent experiments, for interacting with sensors and actuators, for conceptualising and interpreting semantic structures, and for mapping between these semantic structures and linguistic utterances. The aim of this paper is twofold. On the one hand, it introduces a high-level robot interface that extends the Babel software system, presenting for the first time a toolkit that provides flexible modules for dealing with each subtask involved in running advanced grounded language game experiments. On the other hand, it provides a practical guide to using the toolkit for implementing such experiments, taking a grounded colour naming game experiment as a didactic example. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.09218
"Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense",Yixin Zhu;Tao Gao;Lifeng Fan;Siyuan Huang;Mark Edmonds;Hangxin Liu;Feng Gao;Chi Zhang;Siyuan Qi;Ying Nian Wu;Joshua B. Tenenbaum;Song-Chun Zhu,"Recent progress in deep learning is essentially based on a ""big data for small tasks"" paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a ""small data for big tasks"" paradigm, wherein a single artificial intelligence (AI) system is challenged to develop ""common sense"", enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility (FPICU) as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of ""why"" and ""how"", beyond the dominant ""what"" and ""where"" framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the ""dark matter"" of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace ""dark"" humanlike common sense for solving novel tasks. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.09044
Automatically Characterizing Targeted Information Operations Through Biases Present in Discourse on Twitter,Autumn Toney;Akshat Pandey;Wei Guo;David Broniatowski;Aylin Caliskan,"This paper considers the problem of automatically characterizing overall attitudes and biases that may be associated with emerging information operations via artificial intelligence. Accurate analysis of these emerging topics usually requires laborious, manual analysis by experts to annotate millions of tweets to identify biases in new topics. We introduce extensions of the Word Embedding Association Test from Caliskan et al. to a new domain (Caliskan, 2017). Our practical and unsupervised method is used to quantify biases promoted in information operations. We validate our method using known information operation-related tweets from Twitter's Transparency Report. We perform a case study on the COVID-19 pandemic to evaluate our method's performance on non-labeled Twitter data, demonstrating its usability in emerging domains. △ Less","3 December, 2020",https://arxiv.org/pdf/2004.08726
Three Modern Roles for Logic in AI,Adnan Darwiche,"We consider three modern roles for logic in artificial intelligence, which are based on the theory of tractable Boolean circuits: (1) logic as a basis for computation, (2) logic for learning from a combination of data and knowledge, and (3) logic for reasoning about the behavior of machine learning systems.","18 April, 2020",https://arxiv.org/pdf/2004.08599
ECCOLA -- a Method for Implementing Ethically Aligned AI Systems,Ville Vakkuri;Kai-Kristian Kemell;Pekka Abrahamsson,"Various recent Artificial Intelligence (AI) system failures, some of which have made the global headlines, have highlighted issues in these systems. These failures have resulted in calls for more ethical AI systems that better take into account their effects on various stakeholders. However, implementing AI ethics into practice is still an on-going challenge. High-level guidelines for doing so exist, devised by governments and private organizations alike, but lack practicality for developers. To address this issue, in this paper, we present a method for implementing AI ethics. The method, ECCOLA, has been iteratively developed using a cyclical action design research approach. The method aims at making the high-level AI ethics principles more practical, making it possible for developers to more easily implement them in practice. △ Less","9 November, 2020",https://arxiv.org/pdf/2004.08377
Symmetry as an Organizing Principle for Geometric Intelligence,Snejana Sheghava;Ashok Goel,"The exploration of geometrical patterns stimulates imagination and encourages abstract reasoning which is a distinctive feature of human intelligence. In cognitive science, Gestalt principles such as symmetry have often explained significant aspects of human perception. We present a computational technique for building artificial intelligence (AI) agents that use symmetry as the organizing principle for addressing Dehaene's test of geometric intelligence \cite{dehaene2006core}. The performance of our model is on par with extant AI models of problem solving on the Dehaene's test and seems correlated with some elements of human behavior on the same test. △ Less","16 April, 2020",https://arxiv.org/pdf/2004.07879
Investigating Efficient Learning and Compositionality in Generative LSTM Networks,Sarah Fabi;Sebastian Otte;Jonas Gregor Wiese;Martin V. Butz,"When comparing human with artificial intelligence, one major difference is apparent: Humans can generalize very broadly from sparse data sets because they are able to recombine and reintegrate data components in compositional manners. To investigate differences in efficient learning, Joshua B. Tenenbaum and colleagues developed the character challenge: First an algorithm is trained in generating handwritten characters. In a next step, one version of a new type of character is presented. An efficient learning algorithm is expected to be able to re-generate this new character, to identify similar versions of this character, to generate new variants of it, and to create completely new character types. In the past, the character challenge was only met by complex algorithms that were provided with stochastic primitives. Here, we tackle the challenge without providing primitives. We apply a minimal recurrent neural network (RNN) model with one feedforward layer and one LSTM layer and train it to generate sequential handwritten character trajectories from one-hot encoded inputs. To manage the re-generation of untrained characters, when presented with only one example of them, we introduce a one-shot inference mechanism: the gradient signal is backpropagated to the feedforward layer weights only, leaving the LSTM layer untouched. We show that our model is able to meet the character challenge by recombining previously learned dynamic substructures, which are visible in the hidden LSTM states. Making use of the compositional abilities of RNNs in this way might be an important step towards bridging the gap between human and artificial intelligence. △ Less","20 October, 2020",https://arxiv.org/pdf/2004.07754
Joint Supervised and Self-Supervised Learning for 3D Real-World Challenges,Antonio Alliegro;Davide Boscaini;Tatiana Tommasi,"Point cloud processing and 3D shape understanding are very challenging tasks for which deep learning techniques have demonstrated great potentials. Still further progresses are essential to allow artificial intelligent agents to interact with the real world, where the amount of annotated data may be limited and integrating new sources of knowledge becomes crucial to support autonomous learning. Here we consider several possible scenarios involving synthetic and real-world point clouds where supervised learning fails due to data scarcity and large domain gaps. We propose to enrich standard feature representations by leveraging self-supervision through a multi-task model that can solve a 3D puzzle while learning the main task of shape classification or part segmentation. An extensive analysis investigating few-shot, transfer learning and cross-domain settings shows the effectiveness of our approach with state-of-the-art results for 3D shape classification and part segmentation. △ Less","15 April, 2020",https://arxiv.org/pdf/2004.07392
Network Medicine Framework for Identifying Drug Repurposing Opportunities for COVID-19,Deisy Morselli Gysi;Ítalo Do Valle;Marinka Zitnik;Asher Ameli;Xiao Gan;Onur Varol;Susan Dina Ghiassian;JJ Patten;Robert Davey;Joseph Loscalzo;Albert-László Barabási,"The current pandemic has highlighted the need for methodologies that can quickly and reliably prioritize clinically approved compounds for their potential effectiveness for SARS-CoV-2 infections. In the past decade, network medicine has developed and validated multiple predictive algorithms for drug repurposing, exploiting the sub-cellular network-based relationship between a drug's targets and disease genes. Here, we deployed algorithms relying on artificial intelligence, network diffusion, and network proximity, tasking each of them to rank 6,340 drugs for their expected efficacy against SARS-CoV-2. To test the predictions, we used as ground truth 918 drugs that had been experimentally screened in VeroE6 cells, and the list of drugs under clinical trial, that capture the medical community's assessment of drugs with potential COVID-19 efficacy. We find that while most algorithms offer predictive power for these ground truth data, no single method offers consistently reliable outcomes across all datasets and metrics. This prompted us to develop a multimodal approach that fuses the predictions of all algorithms, showing that a consensus among the different predictive methods consistently exceeds the performance of the best individual pipelines. We find that 76 of the 77 drugs that successfully reduced viral infection do not bind the proteins targeted by SARS-CoV-2, indicating that these drugs rely on network-based actions that cannot be identified using docking-based strategies. These advances offer a methodological pathway to identify repurposable drugs for future pathogens and neglected diseases underserved by the costs and extended timeline of de novo drug development. △ Less","9 August, 2020",https://arxiv.org/pdf/2004.07229
Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,Miles Brundage;Shahar Avin;Jasmine Wang;Haydn Belfield;Gretchen Krueger;Gillian Hadfield;Heidy Khlaaf;Jingying Yang;Helen Toner;Ruth Fong;Tegan Maharaj;Pang Wei Koh;Sara Hooker;Jade Leung;Andrew Trask;Emma Bluemke;Jonathan Lebensold;Cullen O'Keefe;Mark Koren;Théo Ryffel;JB Rubinovitz;Tamay Besiroglu;Federica Carugati;Jack Clark;Peter Eckersley,"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms. △ Less","20 April, 2020",https://arxiv.org/pdf/2004.07213
Bias in Multimodal AI: Testbed for Fair Automatic Recruitment,Alejandro Peña;Ignacio Serna;Aythami Morales;Julian Fierrez,"The presence of decision-making algorithms in society is rapidly increasing nowadays, while concerns about their transparency and the possibility of these algorithms becoming new sources of discrimination are arising. In fact, many relevant automated systems have been shown to make decisions based on sensitive information or discriminate certain social groups (e.g. certain biometric systems for person recognition). With the aim of studying how current multimodal algorithms based on heterogeneous sources of information are affected by sensitive elements and inner biases in the data, we propose a fictitious automated recruitment testbed: FairCVtest. We train automatic recruitment algorithms using a set of multimodal synthetic profiles consciously scored with gender and racial biases. FairCVtest shows the capacity of the Artificial Intelligence (AI) behind such recruitment tool to extract sensitive information from unstructured data, and exploit it in combination to data biases in undesirable (unfair) ways. Finally, we present a list of recent works developing techniques capable of removing sensitive information from the decision-making process of deep learning architectures. We have used one of these algorithms (SensitiveNets) to experiment discrimination-aware learning for the elimination of sensitive information in our multimodal AI framework. Our methodology and results show how to generate fairer AI-based tools in general, and in particular fairer automated recruitment systems. △ Less","15 April, 2020",https://arxiv.org/pdf/2004.07173
Human Evaluation of Interpretability: The Case of AI-Generated Music Knowledge,Haizi Yu;Heinrich Taube;James A. Evans;Lav R. Varshney,"Interpretability of machine learning models has gained more and more attention among researchers in the artificial intelligence (AI) and human-computer interaction (HCI) communities. Most existing work focuses on decision making, whereas we consider knowledge discovery. In particular, we focus on evaluating AI-discovered knowledge/rules in the arts and humanities. From a specific scenario, we present an experimental procedure to collect and assess human-generated verbal interpretations of AI-generated music theory/rules rendered as sophisticated symbolic/numeric objects. Our goal is to reveal both the possibilities and the challenges in such a process of decoding expressive messages from AI sources. We treat this as a first step towards 1) better design of AI representations that are human interpretable and 2) a general methodology to evaluate interpretability of AI-discovered knowledge representations. △ Less","15 April, 2020",https://arxiv.org/pdf/2004.06894
Issues and challenges in Cloud Storage Architecture: A Survey,Anwar Ghani;Afzal Badshah;Saeedullah Jan;Abdulrahman A. Alshdadi;Ali Daud,"From home appliances to industrial enterprises, the Information and Communication Technology (ICT) industry is revolutionizing the world. We are witnessing the emergence of new technologies (e.g, Cloud computing, Fog computing, Internet of Things (IoT), Artificial Intelligence (AI) and Block-chain) which proves the growing use of ICT (e,g. business, education, health, and home appliances), resulting in massive data generation. It is expected that more than 175 ZB data will be processed annually by 75 billion devices by 2025. The 5G technology (i.e. mobile communication technology) dramatically increases network speed, enabling users to upload ultra high definition videos in real-time, which will generate a massive stream of big data. Furthermore, smart devices, having artificial intelligence, will act like a human being (e.g, a self-driving vehicle, etc) on the network, will also generate big data. This sudden shift and massive data generation created serious challenges in storing and managing heterogeneous data at such a large scale. This article presents a state-of-the-art review of the issues and challenges involved in storing heterogeneous big data, their countermeasures (i.e, from security and management perspectives), and future opportunities of cloud storage. These challenges are reviewed in detail and new dynamics for researchers in the field of cloud storage are discovered. △ Less","12 July, 2020",https://arxiv.org/pdf/2004.06809
"Hi Sigma, do I have the Coronavirus?: Call for a New Artificial Intelligence Approach to Support Health Care Professionals Dealing With The COVID-19 Pandemic",Brian Subirana;Ferran Hueto;Prithvi Rajasekaran;Jordi Laguarta;Susana Puig;Josep Malvehy;Oriol Mitja;Antoni Trilla;Carlos Iván Moreno;José Francisco Muñoz Valle;Ana Esther Mercado González;Barbara Vizmanos;Sanjay Sarma,"Just like your phone can detect what song is playing in crowded spaces, we show that Artificial Intelligence transfer learning algorithms trained on cough phone recordings results in diagnostic tests for COVID-19. To gain adoption by the health care community, we plan to validate our results in a clinical trial and three other venues in Mexico, Spain and the USA . However, if we had data from other on-going clinical trials and volunteers, we may do much more. For example, for confirmed stay-at-home COVID-19 patients, a longitudinal audio test could be developed to determine contact-with-hospital recommendations, and for the most critical COVID-19 patients a success ratio forecast test, including patient clinical data, to prioritize ICU allocation. As a challenge to the engineering community and in the context of our clinical trial, the authors suggest distributing cough recordings daily, hoping other trials and crowdsourcing users will contribute more data. Previous approaches to complex AI tasks have either used a static dataset or were private efforts led by large corporations. All existing COVID-19 trials published also follow this paradigm. Instead, we suggest a novel open collective approach to large-scale real-time health care AI. We will be posting updates at https://opensigma.mit.edu. Our personal view is that our approach is the right one for large scale pandemics, and therefore is here to stay - will you join? △ Less","10 April, 2020",https://arxiv.org/pdf/2004.06510
An Efficient UAV-based Artificial Intelligence Framework for Real-Time Visual Tasks,Enkhtogtokh Togootogtokh;Christian Micheloni;Gian Luca Foresti;Niki Martinel,"Modern Unmanned Aerial Vehicles equipped with state of the art artificial intelligence (AI) technologies are opening to a wide plethora of novel and interesting applications. While this field received a strong impact from the recent AI breakthroughs, most of the provided solutions either entirely rely on commercial software or provide a weak integration interface which denies the development of additional techniques. This leads us to propose a novel and efficient framework for the UAV-AI joint technology. Intelligent UAV systems encounter complex challenges to be tackled without human control. One of these complex challenges is to be able to carry out computer vision tasks in real-time use cases. In this paper we focus on this challenge and introduce a multi-layer AI (MLAI) framework to allow easy integration of ad-hoc visual-based AI applications. To show its features and its advantages, we implemented and evaluated different modern visual-based deep learning models for object detection, target tracking and target handover. △ Less","13 April, 2020",https://arxiv.org/pdf/2004.06154
Software-Defined Network for End-to-end Networked Science at the Exascale,Inder Monga;Chin Guok;John MacAuley;Alex Sim;Harvey Newman;Justas Balcas;Phil DeMar;Linda Winkler;Tom Lehman;Xi Yang,"Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence. △ Less","13 April, 2020",https://arxiv.org/pdf/2004.05953
A Survey on Impact of Transient Faults on BNN Inference Accelerators,Navid Khoshavi;Connor Broyles;Yu Bi,"Over past years, the philosophy for designing the artificial intelligence algorithms has significantly shifted towards automatically extracting the composable systems from massive data volumes. This paradigm shift has been expedited by the big data booming which enables us to easily access and analyze the highly large data sets. The most well-known class of big data analysis techniques is called deep learning. These models require significant computation power and extremely high memory accesses which necessitate the design of novel approaches to reduce the memory access and improve power efficiency while taking into account the development of domain-specific hardware accelerators to support the current and future data sizes and model structures.The current trends for designing application-specific integrated circuits barely consider the essential requirement for maintaining the complex neural network computation to be resilient in the presence of soft errors. The soft errors might strike either memory storage or combinational logic in the hardware accelerator that can affect the architectural behavior such that the precision of the results fall behind the minimum allowable correctness. In this study, we demonstrate that the impact of soft errors on a customized deep learning algorithm called Binarized Neural Network might cause drastic image misclassification. Our experimental results show that the accuracy of image classifier can drastically drop by 76.70% and 19.25% in lfcW1A1 and cnvW1A1 networks,respectively across CIFAR-10 and MNIST datasets during the fault injection for the worst-case scenarios △ Less","10 April, 2020",https://arxiv.org/pdf/2004.05915
Federated Machine Learning for Intelligent IoT via Reconfigurable Intelligent Surface,Kai Yang;Yuanming Shi;Yong Zhou;Zhanpeng Yang;Liqun Fu;Wei Chen,"Intelligent Internet-of-Things (IoT) will be transformative with the advancement of artificial intelligence and high-dimensional data analysis, shifting from ""connected things"" to ""connected intelligence"". This shall unleash the full potential of intelligent IoT in a plethora of exciting applications, such as self-driving cars, unmanned aerial vehicles, healthcare, robotics, and supply chain finance. These applications drive the need of developing revolutionary computation, communication and artificial intelligence technologies that can make low-latency decisions with massive real-time data. To this end, federated machine learning, as a disruptive technology, is emerged to distill intelligence from the data at network edge, while guaranteeing device privacy and data security. However, the limited communication bandwidth is a key bottleneck of model aggregation for federated machine learning over radio channels. In this article, we shall develop an over-the-air computation based communication-efficient federated machine learning framework for intelligent IoT networks via exploiting the waveform superposition property of a multi-access channel. Reconfigurable intelligent surface is further leveraged to reduce the model aggregation error via enhancing the signal strength by reconfiguring the wireless propagation environments. △ Less","13 April, 2020",https://arxiv.org/pdf/2004.05843
Deep Learning COVID-19 Features on CXR using Limited Training Data Sets,Yujin Oh;Sangjoon Park;Jong Chul Ye,"Under the global pandemic of COVID-19, the use of artificial intelligence to analyze chest X-ray (CXR) image for COVID-19 diagnosis and patient triage is becoming important. Unfortunately, due to the emergent nature of the COVID-19 pandemic, a systematic collection of the CXR data set for deep neural network training is difficult. To address this problem, here we propose a patch-based convolutional neural network approach with a relatively small number of trainable parameters for COVID-19 diagnosis. The proposed method is inspired by our statistical analysis of the potential imaging biomarkers of the CXR radiographs. Experimental results show that our method achieves state-of-the-art performance and provides clinically interpretable saliency maps, which are useful for COVID-19 diagnosis and patient triage. △ Less","5 May, 2020",https://arxiv.org/pdf/2004.05758
Detection of Covid-19 From Chest X-ray Images Using Artificial Intelligence: An Early Review,Muhammad Ilyas;Hina Rehman;Amine Nait-ali,"In 2019, the entire world is facing a situation of health emergency due to a newly emerged coronavirus (COVID-19). Almost 196 countries are affected by covid-19, while USA, Italy, China, Spain, Iran, and France have the maximum active cases of COVID-19. The issues, medical and healthcare departments are facing in delay of detecting the COVID-19. Several artificial intelligence based system are designed for the automatic detection of COVID-19 using chest x-rays. In this article we will discuss the different approaches used for the detection of COVID-19 and the challenges we are facing. It is mandatory to develop an automatic detection system to prevent the transfer of the virus through contact. Several deep learning architecture are deployed for the detection of COVID-19 such as ResNet, Inception, Googlenet etc. All these approaches are detecting the subjects suffering with pneumonia while its hard to decide whether the pneumonia is caused by COVID-19 or due to any other bacterial or fungal attack. △ Less","11 April, 2020",https://arxiv.org/pdf/2004.05436
OPTIMAM Mammography Image Database: a large scale resource of mammography images and clinical data,Mark D Halling-Brown;Lucy M Warren;Dominic Ward;Emma Lewis;Alistair Mackenzie;Matthew G Wallis;Louise Wilkinson;Rosalind M Given-Wilson;Rita McAvinchey;Kenneth C Young,"A major barrier to medical imaging research and in particular the development of artificial intelligence (AI) is a lack of large databases of medical images which share images with other researchers. Without such databases it is not possible to train generalisable AI algorithms, and large amounts of time and funding is spent collecting smaller datasets at individual research centres. The OPTIMAM image database (OMI-DB) has been developed to overcome these barriers. OMI-DB consists of several relational databases and cloud storage systems, containing mammography images and associated clinical and pathological information. The database contains over 2.5 million images from 173,319 women collected from three UK breast screening centres. This includes 154,832 women with normal breasts, 6909 women with benign findings, 9690 women with screen-detected cancers and 1888 women with interval cancers. Collection is on-going and all women are followed-up and their clinical status updated according to subsequent screening episodes. The availability of prior screening mammograms and interval cancers is a vital resource for AI development. Data from OMI-DB has been shared with over 30 research groups and companies, since 2014. This progressive approach has been possible through sharing agreements between the funder and approved academic and commercial research groups. A research dataset such as the OMI-DB provides a powerful resource for research. △ Less","9 April, 2020",https://arxiv.org/pdf/2004.04742
GGA-MG: Generative Genetic Algorithm for Music Generation,Majid Farzaneh;Rahil Mahdian Toroghi,"Music Generation (MG) is an interesting research topic that links the art of music and Artificial Intelligence (AI). The goal is to train an artificial composer to generate infinite, fresh, and pleasurable musical pieces. Music has different parts such as melody, harmony, and rhythm. In this paper, we propose a Generative Genetic Algorithm (GGA) to produce a melody automatically. The main GGA uses a Long Short-Term Memory (LSTM) recurrent neural network as the objective function, which should be trained by a spectrum of bad-to-good melodies. These melodies have to be provided by another GGA with a different objective function. Good melodies have been provided by CAMPINs collection. We have considered the rhythm in this work, too. The experimental results clearly show that the proposed GGA method is able to generate eligible melodies with natural transitions and without rhythm error. △ Less","7 April, 2020",https://arxiv.org/pdf/2004.04687
Machine Learning in Artificial Intelligence: Towards a Common Understanding,Niklas Kühl;Marc Goutier;Robin Hirt;Gerhard Satzger,"The application of ""machine learning"" and ""artificial intelligence"" has become popular within the last decade. Both terms are frequently used in science and media, sometimes interchangeably, sometimes with different meanings. In this work, we aim to clarify the relationship between these terms and, in particular, to specify the contribution of machine learning to artificial intelligence. We review relevant literature and present a conceptual framework which clarifies the role of machine learning to build (artificial) intelligent agents. Hence, we seek to provide more terminological clarity and a starting point for (interdisciplinary) discussions and future research. △ Less","27 March, 2020",https://arxiv.org/pdf/2004.04686
On Adversarial Examples and Stealth Attacks in Artificial Intelligence Systems,Ivan Y. Tyukin;Desmond J. Higham;Alexander N. Gorban,"In this work we present a formal theoretical framework for assessing and analyzing two classes of malevolent action towards generic Artificial Intelligence (AI) systems. Our results apply to general multi-class classifiers that map from an input space into a decision space, including artificial neural networks used in deep learning applications. Two classes of attacks are considered. The first class involves adversarial examples and concerns the introduction of small perturbations of the input data that cause misclassification. The second class, introduced here for the first time and named stealth attacks, involves small perturbations to the AI system itself. Here the perturbed system produces whatever output is desired by the attacker on a specific small data set, perhaps even a single input, but performs as normal on a validation set (which is unknown to the attacker). We show that in both cases, i.e., in the case of an attack based on adversarial examples and in the case of a stealth attack, the dimensionality of the AI's decision-making space is a major contributor to the AI's susceptibility. For attacks based on adversarial examples, a second crucial parameter is the absence of local concentrations in the data probability distribution, a property known as Smeared Absolute Continuity. According to our findings, robustness to adversarial examples requires either (a) the data distributions in the AI's feature space to have concentrated probability density functions or (b) the dimensionality of the AI's decision variables to be sufficiently small. We also show how to construct stealth attacks on high-dimensional AI systems that are hard to spot unless the validation set is made exponentially large. △ Less","9 April, 2020",https://arxiv.org/pdf/2004.04479
Learning from Learners: Adapting Reinforcement Learning Agents to be Competitive in a Card Game,Pablo Barros;Ana Tanevska;Alessandra Sciutti,"Learning how to adapt to complex and dynamic environments is one of the most important factors that contribute to our intelligence. Endowing artificial agents with this ability is not a simple task, particularly in competitive scenarios. In this paper, we present a broad study on how popular reinforcement learning algorithms can be adapted and implemented to learn and to play a real-world implementation of a competitive multiplayer card game. We propose specific training and validation routines for the learning agents, in order to evaluate how the agents learn to be competitive and explain how they adapt to each others' playing style. Finally, we pinpoint how the behavior of each agent derives from their learning style and create a baseline for future research on this scenario. △ Less","8 April, 2020",https://arxiv.org/pdf/2004.04000
Monte-Carlo Siamese Policy on Actor for Satellite Image Super Resolution,Litu Rout;Saumyaa Shah;S Manthira Moorthi;Debajyoti Dhar,"In the past few years supervised and adversarial learning have been widely adopted in various complex computer vision tasks. It seems natural to wonder whether another branch of artificial intelligence, commonly known as Reinforcement Learning (RL) can benefit such complex vision tasks. In this study, we explore the plausible usage of RL in super resolution of remote sensing imagery. Guided by recent advances in super resolution, we propose a theoretical framework that leverages the benefits of supervised and reinforcement learning. We argue that a straightforward implementation of RL is not adequate to address ill-posed super resolution as the action variables are not fully known. To tackle this issue, we propose to parameterize action variables by matrices, and train our policy network using Monte-Carlo sampling. We study the implications of parametric action space in a model-free environment from theoretical and empirical perspective. Furthermore, we analyze the quantitative and qualitative results on both remote sensing and non-remote sensing datasets. Based on our experiments, we report considerable improvement over state-of-the-art methods by encapsulating supervised models in a reinforcement learning framework. △ Less","8 April, 2020",https://arxiv.org/pdf/2004.03879
Towards Federated Learning in UAV-Enabled Internet of Vehicles: A Multi-Dimensional Contract-Matching Approach,Wei Yang Bryan Lim;Jianqiang Huang;Zehui Xiong;Jiawen Kang;Dusit Niyato;Xian-Sheng Hua;Cyril Leung;Chunyan Miao,"Coupled with the rise of Deep Learning, the wealth of data and enhanced computation capabilities of Internet of Vehicles (IoV) components enable effective Artificial Intelligence (AI) based models to be built. Beyond ground data sources, Unmanned Aerial Vehicles (UAVs) based service providers for data collection and AI model training, i.e., Drones-as-a-Service, is increasingly popular in recent years. However, the stringent regulations governing data privacy potentially impedes data sharing across independently owned UAVs. To this end, we propose the adoption of a Federated Learning (FL) based approach to enable privacy-preserving collaborative Machine Learning across a federation of independent DaaS providers for the development of IoV applications, e.g., for traffic prediction and car park occupancy management. Given the information asymmetry and incentive mismatches between the UAVs and model owners, we leverage on the self-revealing properties of a multi-dimensional contract to ensure truthful reporting of the UAV types, while accounting for the multiple sources of heterogeneity, e.g., in sensing, computation, and transmission costs. Then, we adopt the Gale-Shapley algorithm to match the lowest cost UAV to each subregion. The simulation results validate the incentive compatibility of our contract design, and shows the efficiency of our matching, thus guaranteeing profit maximization for the model owner amid information asymmetry. △ Less","8 April, 2020",https://arxiv.org/pdf/2004.03877
Governance of the Internet of Things (IoT),Lawrence J. Trautman;Mohammed T. Hussein;Louis Ngamassi;Mason J. Molesky,"Today's increasing rate of technological change results from the rapid growth in computer processing speed, when combined with the cost decline of processing capacity, and is of historical import. The daily life of billions of individuals worldwide has been forever changed by technology in just the last few years. Costly data breaches continue at an alarming rate. The challenge facing humans as they attempt to govern the process of artificial intelligence, machine learning, and the impact of billions of sensory devices connected to the Internet is the subject of this Article. We proceed in nine sections. First, we define the Internet of Things (IoT), comment on the explosive growth in sensory devices connected to the Internet, provide examples of IoT devices, and speak to the promise of the IoT. Second, we discuss legal requirements for corporate governance as a foundation for considering the challenge of governing the IoT. Third, we look at potential IoT threats. Fourth, we discuss the Mirai botnet. Fifth, is a look at the IoT threat vector vulnerabilities during times of crisis. Sixth, we discuss the Manufactured Usage Description (MUD) methodology. Seventh, is a discussion of recent regulatory developments. Next, we look at a few recommendations. And finally, we conclude. We believe this Article contributes to our understanding of the widespread exposure to malware associated with IoT and adds to the nascent but emerging literature on governance of enterprise risk, a subject of vital societal importance. △ Less","7 April, 2020",https://arxiv.org/pdf/2004.03765
pAElla: Edge-AI based Real-Time Malware Detection in Data Centers,Antonio Libri;Andrea Bartolini;Luca Benini,"The increasing use of Internet-of-Things (IoT) devices for monitoring a wide spectrum of applications, along with the challenges of ""big data"" streaming support they often require for data analysis, is nowadays pushing for an increased attention to the emerging edge computing paradigm. In particular, smart approaches to manage and analyze data directly on the network edge, are more and more investigated, and Artificial Intelligence (AI) powered edge computing is envisaged to be a promising direction. In this paper, we focus on Data Centers (DCs) and Supercomputers (SCs), where a new generation of high-resolution monitoring systems is being deployed, opening new opportunities for analysis like anomaly detection and security, but introducing new challenges for handling the vast amount of data it produces. In detail, we report on a novel lightweight and scalable approach to increase the security of DCs/SCs, that involves AI-powered edge computing on high-resolution power consumption. The method -- called pAElla -- targets real-time Malware Detection (MD), it runs on an out-of-band IoT-based monitoring system for DCs/SCs, and involves Power Spectral Density of power measurements, along with AutoEncoders. Results are promising, with an F1-score close to 1, and a False Alarm and Malware Miss rate close to 0%. We compare our method with State-of-the-Art MD techniques and show that, in the context of DCs/SCs, pAElla can cover a wider range of malware, significantly outperforming SoA approaches in terms of accuracy. Moreover, we propose a methodology for online training suitable for DCs/SCs in production, and release open dataset and code. △ Less","7 April, 2020",https://arxiv.org/pdf/2004.03670
Neural Analogical Matching,Maxwell Crouse;Constantine Nakos;Ibrahim Abdelaziz;Kenneth Forbus,"Analogy is core to human cognition. It allows us to solve problems based on prior experience, it governs the way we conceptualize new information, and it even influences our visual perception. The importance of analogy to humans has made it an active area of research in the broader field of artificial intelligence, resulting in data-efficient models that learn and reason in human-like ways. While cognitive perspectives of analogy and deep learning have generally been studied independently of one another, the integration of the two lines of research is a promising step towards more robust and efficient learning techniques. As part of a growing body of research on such an integration, we introduce the Analogical Matching Network: a neural architecture that learns to produce analogies between structured, symbolic representations that are largely consistent with the principles of Structure-Mapping Theory. △ Less","15 December, 2020",https://arxiv.org/pdf/2004.03573
Granular Computing: An Augmented Scheme of Degranulation Through a Modified Partition Matrix,Kaijie Xu;Witold Pedrycz;Zhiwu Li;Mengdao Xing,"As an important technology in artificial intelligence Granular Computing (GrC) has emerged as a new multi-disciplinary paradigm and received much attention in recent years. Information granules forming an abstract and efficient characterization of large volumes of numeric data have been considered as the fundamental constructs of GrC. By generating prototypes and partition matrix, fuzzy clustering is a commonly encountered way of information granulation. Degranulation involves data reconstruction completed on a basis of the granular representatives. Previous studies have shown that there is a relationship between the reconstruction error and the performance of the granulation process. Typically, the lower the degranulation error is, the better performance of granulation is. However, the existing methods of degranulation usually cannot restore the original numeric data, which is one of the important reasons behind the occurrence of the reconstruction error. To enhance the quality of degranulation, in this study, we develop an augmented scheme through modifying the partition matrix. By proposing the augmented scheme, we dwell on a novel collection of granulation-degranulation mechanisms. In the constructed approach, the prototypes can be expressed as the product of the dataset matrix and the partition matrix. Then, in the degranulation process, the reconstructed numeric data can be decomposed into the product of the partition matrix and the matrix of prototypes. Both the granulation and degranulation are regarded as generalized rotation between the data subspace and the prototype subspace with the partition matrix and the fuzzification factor. By modifying the partition matrix, the new partition matrix is constructed through a series of matrix operations. We offer a thorough analysis of the developed scheme. The experimental results are in agreement with the underlying conceptual framework △ Less","2 April, 2020",https://arxiv.org/pdf/2004.03379
DAISI: Database for AI Surgical Instruction,Edgar Rojas-Muñoz;Kyle Couperus;Juan Wachs,"Telementoring surgeons as they perform surgery can be essential in the treatment of patients when in situ expertise is not available. Nonetheless, expert mentors are often unavailable to provide trainees with real-time medical guidance. When mentors are unavailable, a fallback autonomous mechanism should provide medical practitioners with the required guidance. However, AI/autonomous mentoring in medicine has been limited by the availability of generalizable prediction models, and surgical procedures datasets to train those models with. This work presents the initial steps towards the development of an intelligent artificial system for autonomous medical mentoring. Specifically, we present the first Database for AI Surgical Instruction (DAISI). DAISI leverages on images and instructions to provide step-by-step demonstrations of how to perform procedures from various medical disciplines. The dataset was acquired from real surgical procedures and data from academic textbooks. We used DAISI to train an encoder-decoder neural network capable of predicting medical instructions given a current view of the surgery. Afterwards, the instructions predicted by the network were evaluated using cumulative BLEU scores and input from expert physicians. According to the BLEU scores, the predicted and ground truth instructions were as high as 67% similar. Additionally, expert physicians subjectively assessed the algorithm using Likert scale, and considered that the predicted descriptions were related to the images. This work provides a baseline for AI algorithms to assist in autonomous medical mentoring. △ Less","22 March, 2020",https://arxiv.org/pdf/2004.02809
"Review of Artificial Intelligence Techniques in Imaging Data Acquisition, Segmentation and Diagnosis for COVID-19",Feng Shi;Jun Wang;Jun Shi;Ziyan Wu;Qian Wang;Zhenyu Tang;Kelei He;Yinghuan Shi;Dinggang Shen,"(This paper was submitted as an invited paper to IEEE Reviews in Biomedical Engineering on April 6, 2020.) The pandemic of coronavirus disease 2019 (COVID-19) is spreading all over the world. Medical imaging such as X-ray and computed tomography (CT) plays an essential role in the global fight against COVID-19, whereas the recently emerging artificial intelligence (AI) technologies further strengthen the power of the imaging tools and help medical specialists. We hereby review the rapid responses in the community of medical imaging (empowered by AI) toward COVID-19. For example, AI-empowered image acquisition can significantly help automate the scanning procedure and also reshape the workflow with minimal contact to patients, providing the best protection to the imaging technicians. Also, AI can improve work efficiency by accurate delination of infections in X-ray and CT images, facilitating subsequent quantification. Moreover, the computer-aided platforms help radiologists make clinical decisions, i.e., for disease diagnosis, tracking, and prognosis. In this review paper, we thus cover the entire pipeline of medical imaging and analysis techniques involved with COVID-19, including image acquisition, segmentation, diagnosis, and follow-up. We particularly focus on the integration of AI with X-ray and CT, both of which are widely used in the frontline hospitals, in order to depict the latest progress of medical imaging and radiology fighting against COVID-19. △ Less","7 April, 2020",https://arxiv.org/pdf/2004.02731
Morphological Computation and Learning to Learn In Natural Intelligent Systems And AI,Gordana Dodig-Crnkovic,"At present, artificial intelligence in the form of machine learning is making impressive progress, especially the field of deep learning (DL) [1]. Deep learning algorithms have been inspired from the beginning by nature, specifically by the human brain, in spite of our incomplete knowledge about its brain function. Learning from nature is a two-way process as discussed in [2][3][4], computing is learning from neuroscience, while neuroscience is quickly adopting information processing models. The question is, what can the inspiration from computational nature at this stage of the development contribute to deep learning and how much models and experiments in machine learning can motivate, justify and lead research in neuroscience and cognitive science and to practical applications of artificial intelligence. △ Less","5 April, 2020",https://arxiv.org/pdf/2004.02304
BlackBox Toolkit: Intelligent Assistance to UI Design,Vinoth Pandian Sermuga Pandian;Sarah Suleri,"User Interface (UI) design is an creative process that involves considerable reiteration and rework. Designers go through multiple iterations of different prototyping fidelities to create a UI design. In this research, we propose to modify the UI design process by assisting it with artificial intelligence (AI). We propose to enable AI to perform repetitive tasks for the designer while allowing the designer to take command of the creative process. This approach makes the machine act as a black box that intelligently assists the designers in creating UI design. We believe this approach would greatly benefit designers in co-creating design solutions with AI. △ Less","7 April, 2020",https://arxiv.org/pdf/2004.01949
Predicting the risk of pancreatic cancer with a CT-based ensemble AI algorithm,Chenjie Zhou MD;Jianhua Ma Ph. D;Xiaoping Xu MD;Lei Feng MD;Adilijiang Yimamu MD;Xianlong Wang MD;Zhiming Li MD;Jianhua Mo MS;Chengyan Huang MS;Dexia Kong MS;Yi Gao MD;Shulong Li Ph. D,"Objectives: Pancreatic cancer is a lethal disease, hard to diagnose and usually results in poor prognosis and high mortality. Developing an artificial intelligence (AI) algorithm to accurately and universally predict the early cancer risk of all kinds of pancreatic cancer is extremely important. We propose an ensemble AI algorithm to predict universally cancer risk of all kinds of pancreatic lesions with noncontrast CT. Methods: Our algorithm combines the radiomics method and a support tensor machine (STM) by the evidence reasoning (ER) technique to construct a binary classifier, called RadSTM-ER. RadSTM-ER takes advantage of the handcrafted features used in radiomics and learning features learned automatically by the STM from the CTs for presenting better characteristics of lesions. The patient cohort consisted of 135 patients with pathological diagnosis results where 97 patients had malignant lesions. Twenty-seven patients were randomly selected as independent test samples, and the remaining patients were used in a 5-fold cross validation experiment to confirm the hyperparameters, select optimal handcrafted features and train the model. Results: RadSTM-ER achieved independent test results: an area under the receiver operating characteristic curve of 0.8951, an accuracy of 85.19%, a sensitivity of 88.89%, a specificity of 77.78%, a positive predictive value of 88.89% and a negative predictive value of 77.78%. Conclusions: These results are better than the diagnostic performance of the five experimental radiologists, four conventional AI algorithms, which initially demonstrate the potential of noncontrast CT-based RadSTM-ER in cancer risk prediction for all kinds of pancreatic lesions. △ Less","3 April, 2020",https://arxiv.org/pdf/2004.01388
AI4COVID-19: AI Enabled Preliminary Diagnosis for COVID-19 from Cough Samples via an App,Ali Imran;Iryna Posokhova;Haneya N. Qureshi;Usama Masood;Muhammad Sajid Riaz;Kamran Ali;Charles N. John;MD Iftikhar Hussain;Muhammad Nabeel,"Background: The inability to test at scale has become humanity's Achille's heel in the ongoing war against the COVID-19 pandemic. A scalable screening tool would be a game changer. Building on the prior work on cough-based diagnosis of respiratory diseases, we propose, develop and test an Artificial Intelligence (AI)-powered screening solution for COVID-19 infection that is deployable via a smartphone app. The app, named AI4COVID-19 records and sends three 3-second cough sounds to an AI engine running in the cloud, and returns a result within two minutes. Methods: Cough is a symptom of over thirty non-COVID-19 related medical conditions. This makes the diagnosis of a COVID-19 infection by cough alone an extremely challenging multidisciplinary problem. We address this problem by investigating the distinctness of pathomorphological alterations in the respiratory system induced by COVID-19 infection when compared to other respiratory infections. To overcome the COVID-19 cough training data shortage we exploit transfer learning. To reduce the misdiagnosis risk stemming from the complex dimensionality of the problem, we leverage a multi-pronged mediator centered risk-averse AI architecture. Results: Results show AI4COVID-19 can distinguish among COVID-19 coughs and several types of non-COVID-19 coughs. The accuracy is promising enough to encourage a large-scale collection of labeled cough data to gauge the generalization capability of AI4COVID-19. AI4COVID-19 is not a clinical grade testing tool. Instead, it offers a screening tool deployable anytime, anywhere, by anyone. It can also be a clinical decision assistance tool used to channel clinical-testing and treatment to those who need it the most, thereby saving more lives. △ Less","27 September, 2020",https://arxiv.org/pdf/2004.01275
Randomized Kernel Multi-view Discriminant Analysis,Xiaoyun Li;Jie Gui;Ping Li,"In many artificial intelligence and computer vision systems, the same object can be observed at distinct viewpoints or by diverse sensors, which raises the challenges for recognizing objects from different, even heterogeneous views. Multi-view discriminant analysis (MvDA) is an effective multi-view subspace learning method, which finds a discriminant common subspace by jointly learning multiple view-specific linear projections for object recognition from multiple views, in a non-pairwise way. In this paper, we propose the kernel version of multi-view discriminant analysis, called kernel multi-view discriminant analysis (KMvDA). To overcome the well-known computational bottleneck of kernel methods, we also study the performance of using random Fourier features (RFF) to approximate Gaussian kernels in KMvDA, for large scale learning. Theoretical analysis on stability of this approximation is developed. We also conduct experiments on several popular multi-view datasets to illustrate the effectiveness of our proposed strategy. △ Less","2 April, 2020",https://arxiv.org/pdf/2004.01143
Neural network based country wise risk prediction of COVID-19,Ratnabali Pal;Arif Ahmed Sekh;Samarjit Kar;Dilip K. Prasad,"The recent worldwide outbreak of the novel coronavirus (COVID-19) has opened up new challenges to the research community. Artificial intelligence (AI) driven methods can be useful to predict the parameters, risks, and effects of such an epidemic. Such predictions can be helpful to control and prevent the spread of such diseases. The main challenges of applying AI is the small volume of data and the uncertain nature. Here, we propose a shallow long short-term memory (LSTM) based neural network to predict the risk category of a country. We have used a Bayesian optimization framework to optimize and automatically design country-specific networks. The results show that the proposed pipeline outperforms state-of-the-art methods for data of 180 countries and can be a useful tool for such risk categorization. We have also experimented with the trend data and weather data combined for the prediction. The outcome shows that the weather does not have a significant role. The tool can be used to predict long-duration outbreak of such an epidemic such that we can take preventive steps earlier △ Less","16 September, 2020",https://arxiv.org/pdf/2004.00959
Applying Transparency in Artificial Intelligence based Personalization Systems,Laura Schelenz;Avi Segal;Kobi Gal,"Artificial Intelligence based systems increasingly use personalization to provide users with relevant content, products, and solutions. Personalization is intended to support users and address their respective needs and preferences. However, users are becoming increasingly vulnerable to online manipulation due to algorithmic advancements and lack of transparency. Such manipulation decreases users' levels of trust, autonomy, and satisfaction concerning the systems with which they interact. Increasing transparency is an important goal for personalization based systems. Unfortunately, system designers lack guidance in assessing and implementing transparency in their developed systems. In this work we combine insights from technology ethics and computer science to generate a list of transparency best practices for machine generated personalization. Based on these best practices, we develop a checklist to be used by designers wishing to evaluate and increase the transparency of their algorithmic systems. Adopting a designer perspective, we apply the checklist to prominent online services and discuss its advantages and shortcomings. We encourage researchers to adopt the checklist in various environments and to work towards a consensus-based tool for measuring transparency in the personalization community. △ Less","21 August, 2020",https://arxiv.org/pdf/2004.00935
Combating The Machine Ethics Crisis: An Educational Approach,Tai Vu,"In recent years, the availability of massive data sets and improved computing power have driven the advent of cutting-edge machine learning algorithms. However, this trend has triggered growing concerns associated with its ethical issues. In response to such a phenomenon, this study proposes a feasible solution that combines ethics and computer science materials in artificial intelligent classrooms. In addition, the paper presents several arguments and evidence in favor of the necessity and effectiveness of this integrated approach. △ Less","2 April, 2020",https://arxiv.org/pdf/2004.00817
Data-driven modelling of nonlinear spatio-temporal fluid flows using a deep convolutional generative adversarial network,M. Cheng;F. Fang;C. C. Pain;I. M. Navon,"Deep learning techniques for improving fluid flow modelling have gained significant attention in recent years. Advanced deep learning techniques achieve great progress in rapidly predicting fluid flows without prior knowledge of the underlying physical relationships. Advanced deep learning techniques achieve great progress in rapidly predicting fluid flows without prior knowledge of the underlying physical relationships. However, most of existing researches focused mainly on either sequence learning or spatial learning, rarely on both spatial and temporal dynamics of fluid flows (Reichstein et al., 2019). In this work, an Artificial Intelligence (AI) fluid model based on a general deep convolutional generative adversarial network (DCGAN) has been developed for predicting spatio-temporal flow distributions. In deep convolutional networks, the high-dimensional flows can be converted into the low-dimensional ""latent"" representations. The complex features of flow dynamics can be captured by the adversarial networks. The above DCGAN fluid model enables us to provide reasonable predictive accuracy of flow fields while maintaining a high computational efficiency. The performance of the DCGAN is illustrated for two test cases of Hokkaido tsunami with different incoming waves along the coastal line. It is demonstrated that the results from the DCGAN are comparable with those from the original high fidelity model (Fluidity). The spatio-temporal flow features have been represented as the flow evolves, especially, the wave phases and flow peaks can be captured accurately. In addition, the results illustrate that the online CPU cost is reduced by five orders of magnitude compared to the original high fidelity model simulations. The promising results show that the DCGAN can provide rapid and reliable spatio-temporal prediction for nonlinear fluid flows. △ Less","12 March, 2020",https://arxiv.org/pdf/2004.00707
Robots in the Danger Zone: Exploring Public Perception through Engagement,David A. Robb;Muneeb I. Ahmad;Carlo Tiseo;Simona Aracri;Alistair C. McConnell;Vincent Page;Christian Dondrup;Francisco J. Chiyah Garcia;Hai-Nguyen Nguyen;Èric Pairet;Paola Ardón Ramírez;Tushar Semwal;Hazel M. Taylor;Lindsay J. Wilson;David Lane;Helen Hastie;Katrin Lohan,"Public perceptions of Robotics and Artificial Intelligence (RAI) are important in the acceptance, uptake, government regulation and research funding of this technology. Recent research has shown that the public's understanding of RAI can be negative or inaccurate. We believe effective public engagement can help ensure that public opinion is better informed. In this paper, we describe our first iteration of a high throughput in-person public engagement activity. We describe the use of a light touch quiz-format survey instrument to integrate in-the-wild research participation into the engagement, allowing us to probe both the effectiveness of our engagement strategy, and public perceptions of the future roles of robots and humans working in dangerous settings, such as in the off-shore energy sector. We critique our methods and share interesting results into generational differences within the public's view of the future of Robotics and AI in hazardous environments. These findings include that older peoples' views about the future of robots in hazardous environments were not swayed by exposure to our exhibit, while the views of younger people were affected by our exhibit, leading us to consider carefully in future how to more effectively engage with and inform older people. △ Less","1 April, 2020",https://arxiv.org/pdf/2004.00689
Distributed Learning in Ad-Hoc Networks: A Multi-player Multi-armed Bandit Framework,Sumit J. Darak;Manjesh K. Hanawal,"Next-generation networks are expected to be ultra-dense with a very high peak rate but relatively lower expected traffic per user. For such scenario, existing central controller based resource allocation may incur substantial signaling (control communications) leading to a negative effect on the quality of service (e.g. drop calls), energy and spectrum efficiency. To overcome this problem, cognitive ad-hoc networks (CAHN) that share spectrum with other networks are being envisioned. They allow some users to identify and communicate in `free slots' thereby reducing signaling load and allowing the higher number of users per base stations (dense networks). Such networks open up many interesting challenges such as resource identification, coordination, dynamic and context-aware adaptation for which Machine Learning and Artificial Intelligence framework offers novel solutions. In this paper, we discuss state-of-the-art multi-armed multi-player bandit based distributed learning algorithms that allow users to adapt to the environment and coordinate with other players/users. We also discuss various open research problems for feasible realization of CAHN and interesting applications in other domains such as energy harvesting, Internet of Things, and Smart grids. △ Less","6 March, 2020",https://arxiv.org/pdf/2004.00367
Technical Report: Developing a Working Data Hub,Vijay Gadepally;Jeremy Kepner,"Data forms a key component of any enterprise. The need for high quality and easy access to data is further amplified by organizations wishing to leverage machine learning or artificial intelligence for their operations. To this end, many organizations are building resources for managing heterogenous data, providing end-users with an organization wide view of available data, and acting as a centralized repository for data owned/collected by an organization. Very broadly, we refer to these class of techniques as a ""data hub."" While there is no clear definition of what constitutes a data hub, some of the key characteristics include: data catalog; links to data sets or owners of data sets or centralized data repository; basic ability to serve / visualize data sets; access control policies that ensure secure data access and respects policies of data owners; and computing capabilities tied with data hub infrastructure. Of course, developing such a data hub entails numerous challenges. This document provides background in databases, data management and outlines best practices and recommendations for developing and deploying a working data hub. △ Less","17 April, 2020",https://arxiv.org/pdf/2004.00190
How to transform the Apple's application 'Find My' into a toolbox for whistleblowers,Amadou Moctar Kane,"The recent introduction of Find My app by Apple will open a large window of opportunities for whistleblowers. Based on a short range Bluetooth signals, an EC P-224 encryption, and an end-to-end encrypted manner using iCloud Keychain, Find My app is probably the first application broadcasting a large number of anonymous public key on this scale. Hence, this new Apple's application may introduce a revolution in secret communication, if we divert it from its primordial use and transform it into a powerful tool to put in the hands of whistleblowers. By using Find My app and an entity authentication protocol based on artificial intelligence, our goal is to make mass surveillance and kleptographic backdoors ineffective in the lifting of the whistleblower's anonymity. However, in some case, Find my app may also be a powerful tool in the hands of dictatorships governments in their fight against whistleblowers and political adversaries. Thus, the aim of this paper is to show with simple examples, how these two previous situation can happen. △ Less","31 March, 2020",https://arxiv.org/pdf/2004.00108
Mimicking Evolution with Reinforcement Learning,João P. Abrantes;Arnaldo J. Abrantes;Frans A. Oliehoek,"Evolution gave rise to human and animal intelligence here on Earth. We argue that the path to developing artificial human-like-intelligence will pass through mimicking the evolutionary process in a nature-like simulation. In Nature, there are two processes driving the development of the brain: evolution and learning. Evolution acts slowly, across generations, and amongst other things, it defines what agents learn by changing their internal reward function. Learning acts fast, across one's lifetime, and it quickly updates agents' policy to maximise pleasure and minimise pain. The reward function is slowly aligned with the fitness function by evolution, however, as agents evolve the environment and its fitness function also change, increasing the misalignment between reward and fitness. It is extremely computationally expensive to replicate these two processes in simulation. This work proposes Evolution via Evolutionary Reward (EvER) that allows learning to single-handedly drive the search for policies with increasingly evolutionary fitness by ensuring the alignment of the reward function with the fitness function. In this search, EvER makes use of the whole state-action trajectories that agents go through their lifetime. In contrast, current evolutionary algorithms discard this information and consequently limit their potential efficiency at tackling sequential decision problems. We test our algorithm in two simple bio-inspired environments and show its superiority at generating more capable agents at surviving and reproducing their genes when compared with a state-of-the-art evolutionary algorithm. △ Less","6 May, 2020",https://arxiv.org/pdf/2004.00048
Application and Assessment of Deep Learning for the Generation of Potential NMDA Receptor Antagonists,Katherine J. Schultz;Sean M. Colby;Yasemin Yesiltepe;Jamie R. Nuñez;Monee Y. McGrady;Ryan R. Renslow,"Uncompetitive antagonists of the N-methyl D-aspartate receptor (NMDAR) have demonstrated therapeutic benefit in the treatment of neurological diseases such as Parkinson's and Alzheimer's, but some also cause dissociative effects that have led to the synthesis of illicit drugs. The ability to generate NMDAR antagonists in silico is therefore desirable both for new medication development and for preempting and identifying new designer drugs. Recently, generative deep learning models have been applied to de novo drug design as a means to expand the amount of chemical space that can be explored for potential drug-like compounds. In this study, we assess the application of a generative model to the NMDAR to achieve two primary objectives: (i) the creation and release of a comprehensive library of experimentally validated NMDAR phencyclidine (PCP) site antagonists to assist the drug discovery community and (ii) an analysis of both the advantages conferred by applying such generative artificial intelligence models to drug design and the current limitations of the approach. We apply, and provide source code for, a variety of ligand- and structure-based assessment techniques used in standard drug discovery analyses to the deep learning-generated compounds. We present twelve candidate antagonists that are not available in existing chemical databases to provide an example of what this type of workflow can achieve, though synthesis and experimental validation of these compounds is still required. △ Less","31 March, 2020",https://arxiv.org/pdf/2003.14360
Will we ever have Conscious Machines?,Patrick Krauss;Andreas Maier,"The question of whether artificial beings or machines could become self-aware or consciousness has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of whether something is really self-aware or merely a clever program that pretends to do so cannot be answered without access to accurate knowledge about the mechanism's inner workings. We review the current state-of-the-art regarding these developments and investigate common machine learning approaches with respect to their potential ability to become self-aware. We realise that many important algorithmic steps towards machines with a core consciousness have already been devised. For human-level intelligence, however, many additional techniques have to be discovered. △ Less","31 March, 2020",https://arxiv.org/pdf/2003.14132
A Pebble in the AI Race,Toby Walsh,"Bhutan is sometimes described as \a pebble between two boulders"", a small country caught between the two most populous nations on earth: India and China. This pebble is, however, about to be caught up in a vortex: the transformation of our economic, political and social orders by new technologies like Artificial Intelligence. What can a small nation like Bhutan hope to do in the face of such change? What should the nation do, not just to weather this storm, but to become a better place in which to live? △ Less","30 March, 2020",https://arxiv.org/pdf/2003.13861
Genetic Algorithmic Parameter Optimisation of a Recurrent Spiking Neural Network Model,Ifeatu Ezenwe;Alok Joshi;KongFatt Wong-Lin,"Neural networks are complex algorithms that loosely model the behaviour of the human brain. They play a significant role in computational neuroscience and artificial intelligence. The next generation of neural network models is based on the spike timing activity of neurons: spiking neural networks (SNNs). However, model parameters in SNNs are difficult to search and optimise. Previous studies using genetic algorithm (GA) optimisation of SNNs were focused mainly on simple, feedforward, or oscillatory networks, but not much work has been done on optimising cortex-like recurrent SNNs. In this work, we investigated the use of GAs to search for optimal parameters in recurrent SNNs to reach targeted neuronal population firing rates, e.g. as in experimental observations. We considered a cortical column based SNN comprising 1000 Izhikevich spiking neurons for computational efficiency and biologically realism. The model parameters explored were the neuronal biased input currents. First, we found for this particular SNN, the optimal parameter values for targeted population averaged firing activities, and the convergence of algorithm by ~100 generations. We then showed that the GA optimal population size was within ~16-20 while the crossover rate that returned the best fitness value was ~0.95. Overall, we have successfully demonstrated the feasibility of implementing GA to optimise model parameters in a recurrent cortical based SNN. △ Less","27 May, 2020",https://arxiv.org/pdf/2003.13850
Suphx: Mastering Mahjong with Deep Reinforcement Learning,Junjie Li;Sotetsu Koyamada;Qiwei Ye;Guoqing Liu;Chao Wang;Ruihan Yang;Li Zhao;Tao Qin;Tie-Yan Liu;Hsiao-Wuen Hon,"Artificial Intelligence (AI) has achieved great success in many domains, and game AI is widely regarded as its beachhead since the dawn of AI. In recent years, studies on game AI have gradually evolved from relatively simple environments (e.g., perfect-information games such as Go, chess, shogi or two-player imperfect-information games such as heads-up Texas hold'em) to more complex ones (e.g., multi-player imperfect-information games such as multi-player Texas hold'em and StartCraft II). Mahjong is a popular multi-player imperfect-information game worldwide but very challenging for AI research due to its complex playing/scoring rules and rich hidden information. We design an AI for Mahjong, named Suphx, based on deep reinforcement learning with some newly introduced techniques including global reward prediction, oracle guiding, and run-time policy adaptation. Suphx has demonstrated stronger performance than most top human players in terms of stable rank and is rated above 99.99% of all the officially ranked human players in the Tenhou platform. This is the first time that a computer program outperforms most top human players in Mahjong. △ Less","31 March, 2020",https://arxiv.org/pdf/2003.13590
Secure Non-Orthogonal Multiple Access: An Interference Engineering Perspective,Lu Lv;Hai Jiang;Zhiguo Ding;Qiang Ye;Naofal Al-Dhahir;Jian Chen,"Non-orthogonal multiple access (NOMA) is an efficient approach that can improve spectrum utilization and support massive connectivity for next-generation wireless networks. However, over a wireless channel, the superimposed NOMA signals are highly susceptible to eavesdropping, potentially leading to severe leakage of confidential information. In this article, we unleash the potential of network interference and exploit it constructively to enhance physical-layer security in NOMA networks. Particularly, three different types of network interference, including artificial noise, specifically-designed jamming signals, and inter-user interference, are well engineered to intentionally reduce information leakage while mitigating the effect on signal reception quality of legitimate users, thereby significantly enhancing the transmission security of NOMA. Furthermore, we propose interference engineering strategies for more advanced full-duplex NOMA, intelligent reflecting surface NOMA, cognitive radio NOMA, and multi-cell NOMA networks, and discuss several open research problems and challenges, which could inspire innovative interference engineering designs for secure NOMA communications. △ Less","23 October, 2020",https://arxiv.org/pdf/2003.13488
A Novel Fuzzy Approximate Reasoning Method Based on Extended Distance Measure in SISO Fuzzy System,I. M. Son;S. I. Kwak;U. J. Han;J. H. Pak;M. Han;J. R. Pyon;U. S. Ryu,"This paper presents an original method of fuzzy approximate reasoning that can open a new direction of research in the uncertainty inference of Artificial Intelligence(AI) and Computational Intelligence(CI). Fuzzy modus ponens (FMP) and fuzzy modus tollens(FMT) are two fundamental and basic models of general fuzzy approximate reasoning in various fuzzy systems. And the reductive property is one of the essential and important properties in the approximate reasoning theory and it is a lot of applications. This paper suggests a kind of extended distance measure (EDM) based approximate reasoning method in the single input single output(SISO) fuzzy system with discrete fuzzy set vectors of different dimensions. The EDM based fuzzy approximate reasoning method is consists of two part, i.e., FMP-EDM and FMT-EDM. The distance measure based fuzzy reasoning method that the dimension of the antecedent discrete fuzzy set is equal to one of the consequent discrete fuzzy set has already solved in other paper. In this paper discrete fuzzy set vectors of different dimensions mean that the dimension of the antecedent discrete fuzzy set differs from one of the consequent discrete fuzzy set in the SISO fuzzy system. That is, this paper is based on EDM. The experimental results highlight that the proposed approximate reasoning method is comparatively clear and effective with respect to the reductive property, and in accordance with human thinking than existing fuzzy reasoning methods. △ Less","26 March, 2020",https://arxiv.org/pdf/2003.13450
Extending Automated Deduction for Commonsense Reasoning,Tanel Tammet,"Commonsense reasoning has long been considered as one of the holy grails of artificial intelligence. Most of the recent progress in the field has been achieved by novel machine learning algorithms for natural language processing. However, without incorporating logical reasoning, these algorithms remain arguably shallow. With some notable exceptions, developers of practical automated logic-based reasoners have mostly avoided focusing on the problem. The paper argues that the methods and algorithms used by existing automated reasoners for classical first-order logic can be extended towards commonsense reasoning. Instead of devising new specialized logics we propose a framework of extensions to the mainstream resolution-based search methods to make these capable of performing search tasks for practical commonsense reasoning with reasonable efficiency. The proposed extensions mostly rely on operating on ordinary proof trees and are devised to handle commonsense knowledge bases containing inconsistencies, default rules, taxonomies, topics, relevance, confidence and similarity measures. We claim that machine learning is best suited for the construction of commonsense knowledge bases while the extended logic-based methods would be well-suited for actually answering queries from these knowledge bases. △ Less","29 March, 2020",https://arxiv.org/pdf/2003.13159
Can AI help in screening Viral and COVID-19 pneumonia?,Muhammad E. H. Chowdhury;Tawsifur Rahman;Amith Khandakar;Rashid Mazhar;Muhammad Abdul Kadir;Zaid Bin Mahbub;Khandaker Reajul Islam;Muhammad Salman Khan;Atif Iqbal;Nasser Al-Emadi;Mamun Bin Ibne Reaz;T. I. Islam,"Coronavirus disease (COVID-19) is a pandemic disease, which has already caused thousands of causalities and infected several millions of people worldwide. Any technological tool enabling rapid screening of the COVID-19 infection with high accuracy can be crucially helpful to healthcare professionals. The main clinical tool currently in use for the diagnosis of COVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which is expensive, less-sensitive and requires specialized medical personnel. X-ray imaging is an easily accessible tool that can be an excellent alternative in the COVID-19 diagnosis. This research was taken to investigate the utility of artificial intelligence (AI) in the rapid and accurate detection of COVID-19 from chest X-ray images. The aim of this paper is to propose a robust technique for automatic detection of COVID-19 pneumonia from digital chest X-ray images applying pre-trained deep-learning algorithms while maximizing the detection accuracy. A public database was created by the authors combining several public databases and also by collecting images from recently published articles. The database contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579 normal chest X-ray images. Transfer learning technique was used with the help of image augmentation to train and validate several pre-trained deep Convolutional Neural Networks (CNNs). The networks were trained to classify two different schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and COVID-19 pneumonia with and without image augmentation. The classification accuracy, precision, sensitivity, and specificity for both the schemes were 99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%, respectively. △ Less","15 June, 2020",https://arxiv.org/pdf/2003.13145
When Autonomous Systems Meet Accuracy and Transferability through AI: A Survey,Chongzhen Zhang;Jianrui Wang;Gary G. Yen;Chaoqiang Zhao;Qiyu Sun;Yang Tang;Feng Qian;Jürgen Kurths,"With widespread applications of artificial intelligence (AI), the capabilities of the perception, understanding, decision-making and control for autonomous systems have improved significantly in the past years. When autonomous systems consider the performance of accuracy and transferability, several AI methods, like adversarial learning, reinforcement learning (RL) and meta-learning, show their powerful performance. Here, we review the learning-based approaches in autonomous systems from the perspectives of accuracy and transferability. Accuracy means that a well-trained model shows good results during the testing phase, in which the testing set shares a same task or a data distribution with the training set. Transferability means that when a well-trained model is transferred to other testing domains, the accuracy is still good. Firstly, we introduce some basic concepts of transfer learning and then present some preliminaries of adversarial learning, RL and meta-learning. Secondly, we focus on reviewing the accuracy or transferability or both of them to show the advantages of adversarial learning, like generative adversarial networks (GANs), in typical computer vision tasks in autonomous systems, including image style transfer, image superresolution, image deblurring/dehazing/rain removal, semantic segmentation, depth estimation, pedestrian detection and person re-identification (re-ID). Then, we further review the performance of RL and meta-learning from the aspects of accuracy or transferability or both of them in autonomous systems, involving pedestrian tracking, robot navigation and robotic manipulation. Finally, we discuss several challenges and future topics for using adversarial learning, RL and meta-learning in autonomous systems. △ Less","24 May, 2020",https://arxiv.org/pdf/2003.12948
Planning with Brain-inspired AI,Naoya Arakawa,"This article surveys engineering and neuroscientific models of planning as a cognitive function, which is regarded as a typical function of fluid intelligence in the discussion of general intelligence. It aims to present existing planning models as references for realizing the planning function in brain-inspired AI or artificial general intelligence (AGI). It also proposes themes for the research and development of brain-inspired AI from the viewpoint of tasks and architecture. △ Less","24 March, 2020",https://arxiv.org/pdf/2003.12353
"α
-Satellite: An AI-driven System and Benchmark Datasets for Hierarchical Community-level Risk Assessment to Help Combat COVID-19",Yanfang Ye;Shifu Hou;Yujie Fan;Yiyue Qian;Yiming Zhang;Shiyu Sun;Qian Peng;Kenneth Laparo,"The novel coronavirus and its deadly outbreak have posed grand challenges to human society: as of March 26, 2020, there have been 85,377 confirmed cases and 1,293 reported deaths in the United States; and the World Health Organization (WHO) characterized coronavirus disease (COVID-19) - which has infected more than 531,000 people with more than 24,000 deaths in at least 171 countries - a global pandemic. A growing number of areas reporting local sub-national community transmission would represent a significant turn for the worse in the battle against the novel coronavirus, which points to an urgent need for expanded surveillance so we can better understand the spread of COVID-19 and thus better respond with actionable strategies for community mitigation. By advancing capabilities of artificial intelligence (AI) and leveraging the large-scale and real-time data generated from heterogeneous sources (e.g., disease related data from official public health organizations, demographic data, mobility data, and user geneated data from social media), in this work, we propose and develop an AI-driven system (named α-Satellite}, as an initial offering, to provide hierarchical community-level risk assessment to assist with the development of strategies for combating the fast evolving COVID-19 pandemic. More specifically, given a specific location (either user input or automatic positioning), the developed system will automatically provide risk indexes associated with it in a hierarchical manner (e.g., state, county, city, specific location) to enable individuals to select appropriate actions for protection while minimizing disruptions to daily life to the extent possible. The developed system and the generated benchmark datasets have been made publicly accessible through our website. The system description and disclaimer are also available in our website. △ Less","27 March, 2020",https://arxiv.org/pdf/2003.12232
"Edge Intelligence: Architectures, Challenges, and Applications",Dianlei Xu;Tong Li;Yong Li;Xiang Su;Sasu Tarkoma;Tao Jiang;Jon Crowcroft;Pan Hui,"Edge intelligence refers to a set of connected systems and devices for data collection, caching, processing, and analysis in locations close to where data is captured based on artificial intelligence. The aim of edge intelligence is to enhance the quality and speed of data processing and protect the privacy and security of the data. Although recently emerged, spanning the period from 2011 to now, this field of research has shown explosive growth over the past five years. In this paper, we present a thorough and comprehensive survey on the literature surrounding edge intelligence. We first identify four fundamental components of edge intelligence, namely edge caching, edge training, edge inference, and edge offloading, based on theoretical and practical results pertaining to proposed and deployed systems. We then aim for a systematic classification of the state of the solutions by examining research results and observations for each of the four components and present a taxonomy that includes practical problems, adopted techniques, and application goals. For each category, we elaborate, compare and analyse the literature from the perspectives of adopted techniques, objectives, performance, advantages and drawbacks, etc. This survey article provides a comprehensive introduction to edge intelligence and its application areas. In addition, we summarise the development of the emerging research field and the current state-of-the-art and discuss the important open issues and possible theoretical and technical solutions. △ Less","12 June, 2020",https://arxiv.org/pdf/2003.12172
Adversarial Examples and the Deeper Riddle of Induction: The Need for a Theory of Artifacts in Deep Learning,Cameron Buckner,"Deep learning is currently the most widespread and successful technology in artificial intelligence. It promises to push the frontier of scientific discovery beyond current limits. However, skeptics have worried that deep neural networks are black boxes, and have called into question whether these advances can really be deemed scientific progress if humans cannot understand them. Relatedly, these systems also possess bewildering new vulnerabilities: most notably a susceptibility to ""adversarial examples"". In this paper, I argue that adversarial examples will become a flashpoint of debate in philosophy and diverse sciences. Specifically, new findings concerning adversarial examples have challenged the consensus view that the networks' verdicts on these cases are caused by overfitting idiosyncratic noise in the training set, and may instead be the result of detecting predictively useful ""intrinsic features of the data geometry"" that humans cannot perceive (Ilyas et al., 2019). These results should cause us to re-examine responses to one of the deepest puzzles at the intersection of philosophy and science: Nelson Goodman's ""new riddle"" of induction. Specifically, they raise the possibility that progress in a number of sciences will depend upon the detection and manipulation of useful features that humans find inscrutable. Before we can evaluate this possibility, however, we must decide which (if any) of these inscrutable features are real but available only to ""alien"" perception and cognition, and which are distinctive artifacts of deep learning-for artifacts like lens flares or Gibbs phenomena can be similarly useful for prediction, but are usually seen as obstacles to scientific theorizing. Thus, machine learning researchers urgently need to develop a theory of artifacts for deep neural networks, and I conclude by sketching some initial directions for this area of research. △ Less","20 March, 2020",https://arxiv.org/pdf/2003.11917
Rectified Linear Postsynaptic Potential Function for Backpropagation in Deep Spiking Neural Networks,Malu Zhang;Jiadong Wang;Burin Amornpaisannon;Zhixuan Zhang;VPK Miriyala;Ammar Belatreche;Hong Qu;Jibin Wu;Yansong Chua;Trevor E. Carlson;Haizhou Li,"Spiking Neural Networks (SNNs) use spatio-temporal spike patterns to represent and transmit information, which is not only biologically realistic but also suitable for ultra-low-power event-driven neuromorphic implementation. Motivated by the success of deep learning, the study of Deep Spiking Neural Networks (DeepSNNs) provides promising directions for artificial intelligence applications. However, training of DeepSNNs is not straightforward because the well-studied error back-propagation (BP) algorithm is not directly applicable. In this paper, we first establish an understanding as to why error back-propagation does not work well in DeepSNNs. To address this problem, we propose a simple yet efficient Rectified Linear Postsynaptic Potential function (ReL-PSP) for spiking neurons and propose a Spike-Timing-Dependent Back-Propagation (STDBP) learning algorithm for DeepSNNs. In STDBP algorithm, the timing of individual spikes is used to convey information (temporal coding), and learning (back-propagation) is performed based on spike timing in an event-driven manner. Our experimental results show that the proposed learning algorithm achieves state-of-the-art classification accuracy in single spike time based learning algorithms of DeepSNNs. Furthermore, by utilizing the trained model parameters obtained from the proposed STDBP learning algorithm, we demonstrate the ultra-low-power inference operations on a recently proposed neuromorphic inference accelerator. Experimental results show that the neuromorphic hardware consumes 0.751~mW of the total power consumption and achieves a low latency of 47.71~ms to classify an image from the MNIST dataset. Overall, this work investigates the contribution of spike timing dynamics to information encoding, synaptic plasticity and decision making, providing a new perspective to design of future DeepSNNs and neuromorphic hardware systems. △ Less","3 November, 2020",https://arxiv.org/pdf/2003.11837
"Beyond STEM, How Can Women Engage Big Data, Analytics, Robotics and Artificial Intelligence? An Exploratory Analysis of Confidence and Educational Factors in the Emerging Technology Waves Influencing the Role of, and Impact Upon, Women",Yana Samuel;Jean George;Jim Samuel,"In spite of the rapidly advancing global technological environment, the professional participation of women in technology, big data, analytics, artificial intelligence and information systems related domains remains proportionately low. Furthermore, it is of no less concern that the number of women in leadership in these domains are in even lower proportions. In spite of numerous initiatives to improve the participation of women in technological domains, there is an increasing need to gain additional insights into this phenomenon especially since it occurs in nations and geographies which have seen a sharp rise in overall female education, without such increase translating into a corresponding spurt in information systems and technological roles for women. The present paper presents findings from an exploratory analysis and outlines a framework to gain insights into educational factors in the emerging technology waves influencing the role of, and impact upon, women. We specifically identify ways for learning and self-efficacy as key factors, which together lead us to the Advancement of Women in Technology (AWT) insights framework. Based on the AWT framework, we also proposition principles that can be used to encourage higher professional engagement of women in emerging and advanced technologies. Key Words- Women's Education, Technology, Artificial Intelligence, Knowing, Confidence, Self-Efficacy, Learning. △ Less","26 March, 2020",https://arxiv.org/pdf/2003.11746
Giving Up Control: Neurons as Reinforcement Learning Agents,Jordan Ott,"Artificial Intelligence has historically relied on planning, heuristics, and handcrafted approaches designed by experts. All the while claiming to pursue the creation of Intelligence. This approach fails to acknowledge that intelligence emerges from the dynamics within a complex system. Neurons in the brain are governed by local rules, where no single neuron, or group of neurons, coordinates or controls the others. This local structure gives rise to the appropriate dynamics in which intelligence can emerge. Populations of neurons must compete with their neighbors for resources, inhibition, and activity representation. At the same time, they must cooperate, so the population and organism can perform high-level functions. To this end, we introduce modeling neurons as reinforcement learning agents. Where each neuron may be viewed as an independent actor, trying to maximize its own self-interest. By framing learning in this way, we open the door to an entirely new approach to building intelligent systems. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.11642
Improved Binary Artificial Bee Colony Algorithm,Rafet Durgut,"The Artificial Bee Colony (ABC) algorithm is an evolutionary optimization algorithm based on swarm intelligence and inspired by the honey bees' food search behavior. Since the ABC algorithm has been developed to achieve optimal solutions by searching in the continuous search space, modification is required to apply this method to binary optimization problems. In this paper, we improve the ABC algorithm to solve binary optimization problems and call it the improved binary Artificial Bee Colony (ibinABC). The proposed method consists of an update mechanism based on fitness values and processing different number of decision variables. Thus, we aim to prevent the ABC algorithm from getting stuck in a local minimum by increasing its exploration ability. We compare the ibinABC algorithm with three variants of the ABC and other meta-heuristic algorithms in the literature. For comparison, we use the wellknown OR-Library dataset containing 15 problem instances prepared for the uncapacitated facility location problem. Computational results show that the proposed method is superior to other methods in terms of convergence speed and robustness. The source code of the algorithm will be available on GitHub after reviewing process △ Less","20 April, 2020",https://arxiv.org/pdf/2003.11641
Bio-inspired Optimization: metaheuristic algorithms for optimization,Pravin S Game;Vinod Vaze;Emmanuel M,"In today's day and time solving real-world complex problems has become fundamentally vital and critical task. Many of these are combinatorial problems, where optimal solutions are sought rather than exact solutions. Traditional optimization methods are found to be effective for small scale problems. However, for real-world large scale problems, traditional methods either do not scale up or fail to obtain optimal solutions or they end-up giving solutions after a long running time. Even earlier artificial intelligence based techniques used to solve these problems could not give acceptable results. However, last two decades have seen many new methods in AI based on the characteristics and behaviors of the living organisms in the nature which are categorized as bio-inspired or nature inspired optimization algorithms. These methods, are also termed meta-heuristic optimization methods, have been proved theoretically and implemented using simulation as well used to create many useful applications. They have been used extensively to solve many industrial and engineering complex problems due to being easy to understand, flexible, simple to adapt to the problem at hand and most importantly their ability to come out of local optima traps. This local optima avoidance property helps in finding global optimal solutions. This paper is aimed at understanding how nature has inspired many optimization algorithms, basic categorization of them, major bio-inspired optimization algorithms invented in recent time with their applications. △ Less","24 February, 2020",https://arxiv.org/pdf/2003.11637
"Artificial Intelligence for EU Decision-Making. Effects on Citizens Perceptions of Input, Throughput and Output Legitimacy",Christopher Starke;Marco Luenich,"A lack of political legitimacy undermines the ability of the European Union to resolve major crises and threatens the stability of the system as a whole. By integrating digital data into political processes, the EU seeks to base decision-making increasingly on sound empirical evidence. In particular, artificial intelligence systems have the potential to increase political legitimacy by identifying pressing societal issues, forecasting potential policy outcomes, informing the policy process, and evaluating policy effectiveness. This paper investigates how citizens perceptions of EU input, throughput, and output legitimacy are influenced by three distinct decision-making arrangements. First, independent human decision-making, HDM, Second, independent algorithmic decision-making, ADM, and, third, hybrid decision-making by EU politicians and AI-based systems together. The results of a pre-registered online experiment with 572 respondents suggest that existing EU decision-making arrangements are still perceived as the most democratic - input legitimacy. However, regarding the decision-making process itself - throughput legitimacy - and its policy outcomes - output legitimacy, no difference was observed between the status quo and hybrid decision-making involving both ADM and democratically elected EU institutions. Where ADM systems are the sole decision-maker, respondents tend to perceive these as illegitimate. The paper discusses the implications of these findings for EU legitimacy and data-driven policy-making. △ Less","25 March, 2020",https://arxiv.org/pdf/2003.11320
Generalized Canonical Correlation Analysis: A Subspace Intersection Approach,Mikael Sørensen;Charilaos I. Kanatsoulis;Nicholas D. Sidiropoulos,"Generalized Canonical Correlation Analysis (GCCA) is an important tool that finds numerous applications in data mining, machine learning, and artificial intelligence. It aims at finding `common' random variables that are strongly correlated across multiple feature representations (views) of the same set of entities. CCA and to a lesser extent GCCA have been studied from the statistical and algorithmic points of view, but not as much from the standpoint of linear algebra. This paper offers a fresh algebraic perspective of GCCA based on a (bi-)linear generative model that naturally captures its essence. It is shown that from a linear algebra point of view, GCCA is tantamount to subspace intersection; and conditions under which the common subspace of the different views is identifiable are provided. A novel GCCA algorithm is proposed based on subspace intersection, which scales up to handle large GCCA tasks. Synthetic as well as real data experiments are provided to showcase the effectiveness of the proposed approach. △ Less","25 March, 2020",https://arxiv.org/pdf/2003.11205
AI loyalty: A New Paradigm for Aligning Stakeholder Interests,Anthony Aguirre;Gaia Dempsey;Harry Surden;Peter B. Reiner,"When we consult with a doctor, lawyer, or financial advisor, we generally assume that they are acting in our best interests. But what should we assume when it is an artificial intelligence (AI) system that is acting on our behalf? Early examples of AI assistants like Alexa, Siri, Google, and Cortana already serve as a key interface between consumers and information on the web, and users routinely rely upon AI-driven systems like these to take automated actions or provide information. Superficially, such systems may appear to be acting according to user interests. However, many AI systems are designed with embedded conflicts of interests, acting in ways that subtly benefit their creators (or funders) at the expense of users. To address this problem, in this paper we introduce the concept of AI loyalty. AI systems are loyal to the degree that they are designed to minimize, and make transparent, conflicts of interest, and to act in ways that prioritize the interests of users. Properly designed, such systems could have considerable functional and competitive - not to mention ethical - advantages relative to those that do not. Loyal AI products hold an obvious appeal for the end-user and could serve to promote the alignment of the long-term interests of AI developers and customers. To this end, we suggest criteria for assessing whether an AI system is sufficiently transparent about conflicts of interest, and acting in a manner that is loyal to the user, and argue that AI loyalty should be considered during the technological design process alongside other important values in AI ethics such as fairness, accountability privacy, and equity. We discuss a range of mechanisms, from pure market forces to strong regulatory frameworks, that could support incorporation of AI loyalty into a variety of future AI systems. △ Less","24 March, 2020",https://arxiv.org/pdf/2003.11157
COVID-19 and Computer Audition: An Overview on What Speech & Sound Analysis Could Contribute in the SARS-CoV-2 Corona Crisis,Björn W. Schuller;Dagmar M. Schuller;Kun Qian;Juan Liu;Huaiyuan Zheng;Xiao Li,"At the time of writing, the world population is suffering from more than 10,000 registered COVID-19 disease epidemic induced deaths since the outbreak of the Corona virus more than three months ago now officially known as SARS-CoV-2. Since, tremendous efforts have been made worldwide to counter-steer and control the epidemic by now labelled as pandemic. In this contribution, we provide an overview on the potential for computer audition (CA), i.e., the usage of speech and sound analysis by artificial intelligence to help in this scenario. We first survey which types of related or contextually significant phenomena can be automatically assessed from speech or sound. These include the automatic recognition and monitoring of breathing, dry and wet coughing or sneezing sounds, speech under cold, eating behaviour, sleepiness, or pain to name but a few. Then, we consider potential use-cases for exploitation. These include risk assessment and diagnosis based on symptom histograms and their development over time, as well as monitoring of spread, social distancing and its effects, treatment and recovery, and patient wellbeing. We quickly guide further through challenges that need to be faced for real-life usage. We come to the conclusion that CA appears ready for implementation of (pre-)diagnosis and monitoring tools, and more generally provides rich and significant, yet so far untapped potential in the fight against COVID-19 spread. △ Less","24 March, 2020",https://arxiv.org/pdf/2003.11117
"EQL -- an extremely easy to learn knowledge graph query language, achieving highspeed and precise search",Han Liu;Shantao Liu,"EQL, also named as Extremely Simple Query Language, can be widely used in the field of knowledge graph, precise search, strong artificial intelligence, database, smart speaker ,patent search and other fields. EQL adopt the principle of minimalism in design and pursues simplicity and easy to learn so that everyone can master it quickly. EQL language and lambda calculus are interconvertible, that reveals the mathematical nature of EQL language, and lays a solid foundation for rigor and logical integrity of EQL language. The EQL language and a comprehensive knowledge graph system with the world's commonsense can together form the foundation of strong AI in the future, and make up for the current lack of understanding of world's commonsense by current AI system. EQL language can be used not only by humans, but also as a basic language for data query and data exchange between robots. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.11105
Data-Driven Failure Prediction in Brittle Materials: A Phase-Field Based Machine Learning Framework,Eduardo A. Barros de Moraes;Hadi Salehi;Mohsen Zayernouri,"Failure in brittle materials led by the evolution of micro- to macro-cracks under repetitive or increasing loads is often catastrophic with no significant plasticity to advert the onset of fracture. Early failure detection with respective location are utterly important features in any practical application, both of which can be effectively addressed using artificial intelligence. In this paper, we develop a supervised machine learning (ML) framework to predict failure in an isothermal, linear elastic and isotropic phase-field model for damage and fatigue of brittle materials. Time-series data of the phase-field model is extracted from virtual sensing nodes at different locations of the geometry. A pattern recognition scheme is introduced to represent time-series data/sensor nodes responses as a pattern with a corresponding label, integrated with ML algorithms, used for damage classification with identified patterns. We perform an uncertainty analysis by superposing random noise to the time-series data to assess the robustness of the framework with noise-polluted data. Results indicate that the proposed framework is capable of predicting failure with acceptable accuracy even in the presence of high noise levels. The findings demonstrate satisfactory performance of the supervised ML framework, and the applicability of artificial intelligence and ML to a practical engineering problem, i.,e, data-driven failure prediction in brittle materials. △ Less","24 March, 2020",https://arxiv.org/pdf/2003.10975
Estimating Uncertainty and Interpretability in Deep Learning for Coronavirus (COVID-19) Detection,Biraja Ghoshal;Allan Tucker,"Deep Learning has achieved state of the art performance in medical imaging. However, these methods for disease detection focus exclusively on improving the accuracy of classification or predictions without quantifying uncertainty in a decision. Knowing how much confidence there is in a computer-based medical diagnosis is essential for gaining clinicians trust in the technology and therefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2) infections are a major healthcare challenge around the world. Detecting COVID-19 in X-ray images is crucial for diagnosis, assessment and treatment. However, diagnostic uncertainty in the report is a challenging and yet inevitable task for radiologist. In this paper, we investigate how drop-weights based Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in Deep Learning solution to improve the diagnostic performance of the human-machine team using publicly available COVID-19 chest X-ray dataset and show that the uncertainty in prediction is highly correlates with accuracy of prediction. We believe that the availability of uncertainty-aware deep learning solution will enable a wider adoption of Artificial Intelligence (AI) in a clinical setting. △ Less","27 March, 2020",https://arxiv.org/pdf/2003.10769
SOL: Effortless Device Support for AI Frameworks without Source Code Changes,Nicolas Weber;Felipe Huici,"Modern high performance computing clusters heavily rely on accelerators to overcome the limited compute power of CPUs. These supercomputers run various applications from different domains such as simulations, numerical applications or artificial intelligence (AI). As a result, vendors need to be able to efficiently run a wide variety of workloads on their hardware. In the AI domain this is in particular exacerbated by the existence of a number of popular frameworks (e.g, PyTorch, TensorFlow, etc.) that have no common code base, and can vary in functionality. The code of these frameworks evolves quickly, making it expensive to keep up with all changes and potentially forcing developers to go through constant rounds of upstreaming. In this paper we explore how to provide hardware support in AI frameworks without changing the framework's source code in order to minimize maintenance overhead. We introduce SOL, an AI acceleration middleware that provides a hardware abstraction layer that allows us to transparently support heterogeneous hardware. As a proof of concept, we implemented SOL for PyTorch with three backends: CPUs, GPUs and vector processors. △ Less","24 March, 2020",https://arxiv.org/pdf/2003.10688
A Developmental Neuro-Robotics Approach for Boosting the Recognition of Handwritten Digits,Alessandro Di Nuovo,"Developmental psychology and neuroimaging research identified a close link between numbers and fingers, which can boost the initial number knowledge in children. Recent evidence shows that a simulation of the children's embodied strategies can improve the machine intelligence too. This article explores the application of embodied strategies to convolutional neural network models in the context of developmental neuro-robotics, where the training information is likely to be gradually acquired while operating rather than being abundant and fully available as the classical machine learning scenarios. The experimental analyses show that the proprioceptive information from the robot fingers can improve network accuracy in the recognition of handwritten Arabic digits when training examples and epochs are few. This result is comparable to brain imaging and longitudinal studies with young children. In conclusion, these findings also support the relevance of the embodiment in the case of artificial agents' training and show a possible way for the humanization of the learning process, where the robotic body can express the internal processes of artificial intelligence making it more understandable for humans. △ Less","23 March, 2020",https://arxiv.org/pdf/2003.10308
From Bit To Bedside: A Practical Framework For Artificial Intelligence Product Development In Healthcare,David Higgins;Vince I. Madai,"Artificial Intelligence (AI) in healthcare holds great potential to expand access to high-quality medical care, whilst reducing overall systemic costs. Despite hitting the headlines regularly and many publications of proofs-of-concept, certified products are failing to breakthrough to the clinic. AI in healthcare is a multi-party process with deep knowledge required in multiple individual domains. The lack of understanding of the specific challenges in the domain is, therefore, the major contributor to the failure to deliver on the big promises. Thus, we present a decision perspective framework, for the development of AI-driven biomedical products, from conception to market launch. Our framework highlights the risks, objectives and key results which are typically required to proceed through a three-phase process to the market launch of a validated medical AI product. We focus on issues related to Clinical validation, Regulatory affairs, Data strategy and Algorithmic development. The development process we propose for AI in healthcare software strongly diverges from modern consumer software development processes. We highlight the key time points to guide founders, investors and key stakeholders throughout their relevant part of the process. Our framework should be seen as a template for innovation frameworks, which can be used to coordinate team communications and responsibilities towards a reasonable product development roadmap, thus unlocking the potential of AI in medicine. △ Less","23 March, 2020",https://arxiv.org/pdf/2003.10303
Smarter Parking: Using AI to Identify Parking Inefficiencies in Vancouver,Devon Graham;Satish Kumar Sarraf;Taylor Lundy;Ali MohammadMehr;Sara Uppal;Tae Yoon Lee;Hedayat Zarkoob;Scott Duke Kominers;Kevin Leyton-Brown,"On-street parking is convenient, but has many disadvantages: on-street spots come at the expense of other road uses such as traffic lanes, transit lanes, bike lanes, or parklets; drivers looking for parking contribute substantially to traffic congestion and hence to greenhouse gas emissions; safety is reduced both due to the fact that drivers looking for spots are more distracted than other road users and that people exiting parked cars pose a risk to cyclists. These social costs may not be worth paying when off-street parking lots are nearby and have surplus capacity. To see where this might be true in downtown Vancouver, we used artificial intelligence techniques to estimate the amount of time it would take drivers to both park on and off street for destinations throughout the city. For on-street parking, we developed (1) a deep-learning model of block-by-block parking availability based on data from parking meters and audits and (2) a computational simulation of drivers searching for an on-street spot. For off-street parking, we developed a computational simulation of the time it would take drivers drive from their original destination to the nearest city-owned off-street lot and then to queue for a spot based on traffic and lot occupancy data. Finally, in both cases we also computed the time it would take the driver to walk from their parking spot to their original destination. We compared these time estimates for destinations in each block of Vancouver's downtown core and each hour of the day. We found many areas where off street would actually save drivers time over searching the streets for a spot, and many more where the time cost for parking off street was small. The identification of such areas provides an opportunity for the city to repurpose valuable curbside space for community-friendly uses more in line with its transportation goals. △ Less","21 March, 2020",https://arxiv.org/pdf/2003.09761
"Towards an Enterprise-Ready Implementation of Artificial Intelligence-Enabled, Blockchain-Based Smart Contracts",Philipp Brune,"Blockchain technology and artificial intelligence (AI) are current hot topics in research and practice. However, the potentials of their combination have been studied just recently to a larger extend. While different use cases for combining AI and blockchain have been discussed, the idea of enabling blockchain-based smart contracts to perform ""smarter"" decisions by using AI or machine learning (ML) models has only been considered on the conceptual level so far. It remained open, how such AI-enabled smart contracts could be implemented in a robust way for real-world applications. Therefore, in this paper a new, enterprise-class implementation of AI-enabled smart contracts is presented and first insights regarding its feasibility are discussed. △ Less","21 March, 2020",https://arxiv.org/pdf/2003.09744
"Comments on Sejnowski's ""The unreasonable effectiveness of deep learning in artificial intelligence"" [arXiv:2002.04806]",Leslie S. Smith,"Terry Sejnowski's 2020 paper [arXiv:2002.04806] is entitled ""The unreasonable effectiveness of deep learning in artificial intelligence"". However, the paper doesn't attempt to answer the implied question of why Deep Convolutional Neural Networks (DCNNs) can approximate so many of the mappings that they have been trained to model. While there are detailed mathematical analyses, this short paper attempts to look at the issue differently, considering the way that these networks are used, the subset of these functions that can be achieved by training (starting from some location in the original function space), as well as the functions to which these networks will actually be applied. △ Less","9 April, 2020",https://arxiv.org/pdf/2003.09415
TiLA: Twin-in-the-Loop Architecture for Cyber-Physical Production Systems,Heejong Park;Arvind Easwaran;Sidharta Andalam,"Digital twin is a virtual replica of a real-world object that lives simultaneously with its physical counterpart. Since its first introduction in 2003 by Grieves, digital twin has gained momentum in a wide range of applications such as industrial manufacturing, automotive and artificial intelligence. However, many digital-twin-related approaches, found in industries as well as literature, mainly focus on modelling individual physical things with high-fidelity methods with limited scalability. In this paper, we introduce a digital-twin architecture called TiLA (Twin-in-the-Loop Architecture). TiLA employs heterogeneous models and online data to create a digital twin, which follows a Globally Asynchronous Locally Synchronous (GALS) model of computation. It facilitates the creation of a scalable digital twin with different levels of modelling abstraction as well as giving GALS formalism for execution strategy. Furthermore, TiLA provides facilities to develop applications around the twin as well as an interface to synchronise the twin with the physical system through an industrial communication protocol. A digital twin for a manufacturing line has been developed as a case study using TiLA. It demonstrates the use of digital twin models together with online data for monitoring and analysing failures in the physical system. △ Less","10 March, 2020",https://arxiv.org/pdf/2003.09370
Distributed and Democratized Learning: Philosophy and Research Challenges,Minh N. H. Nguyen;Shashi Raj Pandey;Kyi Thar;Nguyen H. Tran;Mingzhe Chen;Walid Saad;Choong Seon Hong,"Due to the availability of huge amounts of data and processing abilities, current artificial intelligence (AI) systems are effective in solving complex tasks. However, despite the success of AI in different areas, the problem of designing AI systems that can truly mimic human cognitive capabilities such as artificial general intelligence, remains largely open. Consequently, many emerging cross-device AI applications will require a transition from traditional centralized learning systems towards large-scale distributed AI systems that can collaboratively perform multiple complex learning tasks. In this paper, we propose a novel design philosophy called democratized learning (Dem-AI) whose goal is to build large-scale distributed learning systems that rely on the self-organization of distributed learning agents that are well-connected, but limited in learning capabilities. Correspondingly, inspired by the societal groups of humans, the specialized groups of learning agents in the proposed Dem-AI system are self-organized in a hierarchical structure to collectively perform learning tasks more efficiently. As such, the Dem-AI learning system can evolve and regulate itself based on the underlying duality of two processes which we call specialized and generalized processes. In this regard, we present a reference design as a guideline to realize future Dem-AI systems, inspired by various interdisciplinary fields. Accordingly, we introduce four underlying mechanisms in the design such as plasticity-stability transition mechanism, self-organizing hierarchical structuring, specialized learning, and generalization. Finally, we establish possible extensions and new challenges for the existing learning approaches to provide better scalable, flexible, and more powerful learning systems with the new setting of Dem-AI. △ Less","14 October, 2020",https://arxiv.org/pdf/2003.09301
FocalMix: Semi-Supervised Learning for 3D Medical Image Detection,Dong Wang;Yuan Zhang;Kexin Zhang;Liwei Wang,"Applying artificial intelligence techniques in medical imaging is one of the most promising areas in medicine. However, most of the recent success in this area highly relies on large amounts of carefully annotated data, whereas annotating medical images is a costly process. In this paper, we propose a novel method, called FocalMix, which, to the best of our knowledge, is the first to leverage recent advances in semi-supervised learning (SSL) for 3D medical image detection. We conducted extensive experiments on two widely used datasets for lung nodule detection, LUNA16 and NLST. Results show that our proposed SSL methods can achieve a substantial improvement of up to 17.3% over state-of-the-art supervised learning approaches with 400 unlabeled CT scans. △ Less","20 March, 2020",https://arxiv.org/pdf/2003.09108
Weakly Supervised Context Encoder using DICOM metadata in Ultrasound Imaging,Szu-Yeu Hu;Shuhang Wang;Wei-Hung Weng;JingChao Wang;XiaoHong Wang;Arinc Ozturk;Qian Li;Viksit Kumar;Anthony E. Samir,"Modern deep learning algorithms geared towards clinical adaption rely on a significant amount of high fidelity labeled data. Low-resource settings pose challenges like acquiring high fidelity data and becomes the bottleneck for developing artificial intelligence applications. Ultrasound images, stored in Digital Imaging and Communication in Medicine (DICOM) format, have additional metadata data corresponding to ultrasound image parameters and medical exams. In this work, we leverage DICOM metadata from ultrasound images to help learn representations of the ultrasound image. We demonstrate that the proposed method outperforms the non-metadata based approaches across different downstream tasks. △ Less","19 March, 2020",https://arxiv.org/pdf/2003.09070
Vulnerabilities of Connectionist AI Applications: Evaluation and Defence,Christian Berghoff;Matthias Neu;Arndt von Twickel,"This article deals with the IT security of connectionist artificial intelligence (AI) applications, focusing on threats to integrity, one of the three IT security goals. Such threats are for instance most relevant in prominent AI computer vision applications. In order to present a holistic view on the IT security goal integrity, many additional aspects such as interpretability, robustness and documentation are taken into account. A comprehensive list of threats and possible mitigations is presented by reviewing the state-of-the-art literature. AI-specific vulnerabilities such as adversarial attacks and poisoning attacks as well as their AI-specific root causes are discussed in detail. Additionally and in contrast to former reviews, the whole AI supply chain is analysed with respect to vulnerabilities, including the planning, data acquisition, training, evaluation and operation phases. The discussion of mitigations is likewise not restricted to the level of the AI system itself but rather advocates viewing AI systems in the context of their supply chains and their embeddings in larger IT infrastructures and hardware devices. Based on this and the observation that adaptive attackers may circumvent any single published AI-specific defence to date, the article concludes that single protective measures are not sufficient but rather multiple measures on different levels have to be combined to achieve a minimum level of IT security for AI applications. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.08837
Convergence of Artificial Intelligence and High Performance Computing on NSF-supported Cyberinfrastructure,E. A. Huerta;Asad Khan;Edward Davis;Colleen Bushell;William D. Gropp;Daniel S. Katz;Volodymyr Kindratenko;Seid Koric;William T. C. Kramer;Brendan McGinty;Kenton McHenry;Aaron Saxton,"Significant investments to upgrade and construct large-scale scientific facilities demand commensurate investments in R&D to design algorithms and computing approaches to enable scientific and engineering breakthroughs in the big data era. Innovative Artificial Intelligence (AI) applications have powered transformational solutions for big data challenges in industry and technology that now drive a multi-billion dollar industry, and which play an ever increasing role shaping human social patterns. As AI continues to evolve into a computing paradigm endowed with statistical and mathematical rigor, it has become apparent that single-GPU solutions for training, validation, and testing are no longer sufficient for computational grand challenges brought about by scientific facilities that produce data at a rate and volume that outstrip the computing capabilities of available cyberinfrastructure platforms. This realization has been driving the confluence of AI and high performance computing (HPC) to reduce time-to-insight, and to enable a systematic study of domain-inspired AI architectures and optimization schemes to enable data-driven discovery. In this article we present a summary of recent developments in this field, and describe specific advances that authors in this article are spearheading to accelerate and streamline the use of HPC platforms to design and apply accelerated AI algorithms in academia and industry. △ Less","19 October, 2020",https://arxiv.org/pdf/2003.08394
From Statistical Relational to Neuro-Symbolic Artificial Intelligence,Luc De Raedt;Sebastijan Dumančić;Robin Manhaeve;Giuseppe Marra,Neuro-symbolic and statistical relational artificial intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position neuro-symbolic artificial intelligence approaches but also to identify a number of directions for further research. △ Less,"24 March, 2020",https://arxiv.org/pdf/2003.08316
An Artificial Intelligence-Based System to Assess Nutrient Intake for Hospitalised Patients,Ya Lu;Thomai Stathopoulou;Maria F. Vasiloglou;Stergios Christodoulidis;Zeno Stanga;Stavroula Mougiakakou,"Regular monitoring of nutrient intake in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition. Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve data accuracy and reduce both the burden on participants and health costs. In this paper, we propose a novel system based on artificial intelligence (AI) to accurately estimate nutrient intake, by simply processing RGB Depth (RGB-D) image pairs captured before and after meal consumption. The system includes a novel multi-task contextual network for food segmentation, a few-shot learning-based classifier built by limited training samples for food recognition, and an algorithm for 3D surface construction. This allows sequential food segmentation, recognition, and estimation of the consumed food volume, permitting fully automatic estimation of the nutrient intake for each meal. For the development and evaluation of the system, a dedicated new database containing images and nutrient recipes of 322 meals is assembled, coupled to data annotation using innovative strategies. Experimental results demonstrate that the estimated nutrient intake is highly correlated (> 0.91) to the ground truth and shows very small mean relative errors (< 20%), outperforming existing techniques proposed for nutrient intake assessment. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.08273
Gender Representation in Open Source Speech Resources,Mahault Garnerin;Solange Rossato;Laurent Besacier,"With the rise of artificial intelligence (AI) and the growing use of deep-learning architectures, the question of ethics, transparency and fairness of AI systems has become a central concern within the research community. We address transparency and fairness in spoken language systems by proposing a study about gender representation in speech resources available through the Open Speech and Language Resource platform. We show that finding gender information in open source corpora is not straightforward and that gender balance depends on other corpus characteristics (elicited/non elicited speech, low/high resource language, speech task targeted). The paper ends with recommendations about metadata and gender information for researchers in order to assure better transparency of the speech systems built using such corpora. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.08132
Segmentation and Optimal Region Selection of Physiological Signals using Deep Neural Networks and Combinatorial Optimization,Jorge Oliveira;Margarida Carvalho;Diogo Marcelo Nogueira;Miguel Coimbra,"Physiological signals, such as the electrocardiogram and the phonocardiogram are very often corrupted by noisy sources. Usually, artificial intelligent algorithms analyze the signal regardless of its quality. On the other hand, physicians use a completely orthogonal strategy. They do not assess the entire recording, instead they search for a segment where the fundamental and abnormal waves are easily detected, and only then a prognostic is attempted. Inspired by this fact, a new algorithm that automatically selects an optimal segment for a post-processing stage, according to a criteria defined by the user is proposed. In the process, a Neural Network is used to compute the output state probability distribution for each sample. Using the aforementioned quantities, a graph is designed, whereas state transition constraints are physically imposed into the graph and a set of constraints are used to retrieve a subset of the recording that maximizes the likelihood function, proposed by the user. The developed framework is tested and validated in two applications. In both cases, the system performance is boosted significantly, e.g in heart sound segmentation, sensitivity increases 2.4% when compared to the standard approaches in the literature. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.07981
Enhancing the Monte Carlo Tree Search Algorithm for Video Game Testing,Sinan Ariyurek;Aysu Betin-Can;Elif Surer,"In this paper, we study the effects of several Monte Carlo Tree Search (MCTS) modifications for video game testing. Although MCTS modifications are highly studied in game playing, their impacts on finding bugs are blank. We focused on bug finding in our previous study where we introduced synthetic and human-like test goals and we used these test goals in Sarsa and MCTS agents to find bugs. In this study, we extend the MCTS agent with several modifications for game testing purposes. Furthermore, we present a novel tree reuse strategy. We experiment with these modifications by testing them on three testbed games, four levels each, that contain 45 bugs in total. We use the General Video Game Artificial Intelligence (GVG-AI) framework to create the testbed games and collect 427 human tester trajectories using the GVG-AI framework. We analyze the proposed modifications in three parts: we evaluate their effects on bug finding performances of agents, we measure their success under two different computational budgets, and we assess their effects on human-likeness of the human-like agent. Our results show that MCTS modifications improve the bug finding performance of the agents. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.07813
Formal Scenario-Based Testing of Autonomous Vehicles: From Simulation to the Real World,Daniel J. Fremont;Edward Kim;Yash Vardhan Pant;Sanjit A. Seshia;Atul Acharya;Xantha Bruso;Paul Wells;Steve Lemke;Qiang Lu;Shalin Mehta,"We present a new approach to automated scenario-based testing of the safety of autonomous vehicles, especially those using advanced artificial intelligence-based components, spanning both simulation-based evaluation as well as testing in the real world. Our approach is based on formal methods, combining formal specification of scenarios and safety properties, algorithmic test case generation using formal simulation, test case selection for track testing, executing test cases on the track, and analyzing the resulting data. Experiments with a real autonomous vehicle at an industrial testing facility support our hypotheses that (i) formal simulation can be effective at identifying test cases to run on the track, and (ii) the gap between simulated and real worlds can be systematically evaluated and bridged. △ Less","12 July, 2020",https://arxiv.org/pdf/2003.07739
Flexible and Context-Specific AI Explainability: A Multidisciplinary Approach,Valérie Beaudouin;Isabelle Bloch;David Bounie;Stéphan Clémençon;Florence d'Alché-Buc;James Eagan;Winston Maxwell;Pavlo Mozharovskyi;Jayneel Parekh,"The recent enthusiasm for artificial intelligence (AI) is due principally to advances in deep learning. Deep learning methods are remarkably accurate, but also opaque, which limits their potential use in safety-critical applications. To achieve trust and accountability, designers and operators of machine learning algorithms must be able to explain the inner workings, the results and the causes of failures of algorithms to users, regulators, and citizens. The originality of this paper is to combine technical, legal and economic aspects of explainability to develop a framework for defining the ""right"" level of explain-ability in a given context. We propose three logical steps: First, define the main contextual factors, such as who the audience of the explanation is, the operational context, the level of harm that the system could cause, and the legal/regulatory framework. This step will help characterize the operational and legal needs for explanation, and the corresponding social benefits. Second, examine the technical tools available, including post hoc approaches (input perturbation, saliency maps...) and hybrid AI approaches. Third, as function of the first two steps, choose the right levels of global and local explanation outputs, taking into the account the costs involved. We identify seven kinds of costs and emphasize that explanations are socially useful only when total social benefits exceed costs. △ Less","13 March, 2020",https://arxiv.org/pdf/2003.07703
Rectified Meta-Learning from Noisy Labels for Robust Image-based Plant Disease Diagnosis,Ruifeng Shi;Deming Zhai;Xianming Liu;Junjun Jiang;Wen Gao,"Plant diseases serve as one of main threats to food security and crop production. It is thus valuable to exploit recent advances of artificial intelligence to assist plant disease diagnosis. One popular approach is to transform this problem as a leaf image classification task, which can be then addressed by the powerful convolutional neural networks (CNNs). However, the performance of CNN-based classification approach depends on a large amount of high-quality manually labeled training data, which are inevitably introduced noise on labels in practice, leading to model overfitting and performance degradation. To overcome this problem, we propose a novel framework that incorporates rectified meta-learning module into common CNN paradigm to train a noise-robust deep network without using extra supervision information. The proposed method enjoys the following merits: i) A rectified meta-learning is designed to pay more attention to unbiased samples, leading to accelerated convergence and improved classification accuracy. ii) Our method is free on assumption of label noise distribution, which works well on various kinds of noise. iii) Our method serves as a plug-and-play module, which can be embedded into any deep models optimized by gradient descent based method. Extensive experiments are conducted to demonstrate the superior performance of our algorithm over the state-of-the-arts. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.07603
Directions for Explainable Knowledge-Enabled Systems,Shruthi Chari;Daniel M. Gruen;Oshani Seneviratne;Deborah L. McGuinness,"Interest in the field of Explainable Artificial Intelligence has been growing for decades and has accelerated recently. As Artificial Intelligence models have become more complex, and often more opaque, with the incorporation of complex machine learning techniques, explainability has become more critical. Recently, researchers have been investigating and tackling explainability with a user-centric focus, looking for explanations to consider trustworthiness, comprehensibility, explicit provenance, and context-awareness. In this chapter, we leverage our survey of explanation literature in Artificial Intelligence and closely related fields and use these past efforts to generate a set of explanation types that we feel reflect the expanded needs of explanation for today's artificial intelligence applications. We define each type and provide an example question that would motivate the need for this style of explanation. We believe this set of explanation types will help future system designers in their generation and prioritization of requirements and further help generate explanations that are better aligned to users' and situational needs. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.07523
Foundations of Explainable Knowledge-Enabled Systems,Shruthi Chari;Daniel M. Gruen;Oshani Seneviratne;Deborah L. McGuinness,"Explainability has been an important goal since the early days of Artificial Intelligence. Several approaches for producing explanations have been developed. However, many of these approaches were tightly coupled with the capabilities of the artificial intelligence systems at the time. With the proliferation of AI-enabled systems in sometimes critical settings, there is a need for them to be explainable to end-users and decision-makers. We present a historical overview of explainable artificial intelligence systems, with a focus on knowledge-enabled systems, spanning the expert systems, cognitive assistants, semantic applications, and machine learning domains. Additionally, borrowing from the strengths of past approaches and identifying gaps needed to make explanations user- and context-focused, we propose new definitions for explanations and explainable knowledge-enabled systems. △ Less","17 March, 2020",https://arxiv.org/pdf/2003.07520
A Novel AI-enabled Framework to Diagnose Coronavirus COVID 19 using Smartphone Embedded Sensors: Design Study,Halgurd S. Maghdid;Kayhan Zrar Ghafoor;Ali Safaa Sadiq;Kevin Curran;Danda B. Rawat;Khaled Rabie,"Coronaviruses are a famous family of viruses that cause illness in both humans and animals. The new type of coronavirus COVID-19 was firstly discovered in Wuhan, China. However, recently, the virus has widely spread in most of the world and causing a pandemic according to the World Health Organization (WHO). Further, nowadays, all the world countries are striving to control the COVID-19. There are many mechanisms to detect coronavirus including clinical analysis of chest CT scan images and blood test results. The confirmed COVID-19 patient manifests as fever, tiredness, and dry cough. Particularly, several techniques can be used to detect the initial results of the virus such as medical detection Kits. However, such devices are incurring huge cost, taking time to install them and use. Therefore, in this paper, a new framework is proposed to detect COVID-19 using built-in smartphone sensors. The proposal provides a low-cost solution, since most of radiologists have already held smartphones for different daily-purposes. Not only that but also ordinary people can use the framework on their smartphones for the virus detection purposes. Nowadays Smartphones are powerful with existing computation-rich processors, memory space, and large number of sensors including cameras, microphone, temperature sensor, inertial sensors, proximity, colour-sensor, humidity-sensor, and wireless chipsets/sensors. The designed Artificial Intelligence (AI) enabled framework reads the smartphone sensors signal measurements to predict the grade of severity of the pneumonia as well as predicting the result of the disease. △ Less","30 May, 2020",https://arxiv.org/pdf/2003.07434
LAXARY: A Trustworthy Explainable Twitter Analysis Model for Post-Traumatic Stress Disorder Assessment,Mohammad Arif Ul Alam;Dhawal Kapadia,"Veteran mental health is a significant national problem as large number of veterans are returning from the recent war in Iraq and continued military presence in Afghanistan. While significant existing works have investigated twitter posts-based Post Traumatic Stress Disorder (PTSD) assessment using blackbox machine learning techniques, these frameworks cannot be trusted by the clinicians due to the lack of clinical explainability. To obtain the trust of clinicians, we explore the big question, can twitter posts provide enough information to fill up clinical PTSD assessment surveys that have been traditionally trusted by clinicians? To answer the above question, we propose, LAXARY (Linguistic Analysis-based Exaplainable Inquiry) model, a novel Explainable Artificial Intelligent (XAI) model to detect and represent PTSD assessment of twitter users using a modified Linguistic Inquiry and Word Count (LIWC) analysis. First, we employ clinically validated survey tools for collecting clinical PTSD assessment data from real twitter users and develop a PTSD Linguistic Dictionary using the PTSD assessment survey results. Then, we use the PTSD Linguistic Dictionary along with machine learning model to fill up the survey tools towards detecting PTSD status and its intensity of corresponding twitter users. Our experimental evaluation on 210 clinically validated veteran twitter users provides promising accuracies of both PTSD classification and its intensity estimation. We also evaluate our developed PTSD Linguistic Dictionary's reliability and validity. △ Less","20 July, 2020",https://arxiv.org/pdf/2003.07433
Multi-AI competing and winning against humans in iterated Rock-Paper-Scissors game,Lei Wang;Wenbin Huang;Yuanpeng Li;Julian Evans;Sailing He,"Predicting and modeling human behavior and finding trends within human decision-making processes is a major problem of social science. Rock Paper Scissors (RPS) is the fundamental strategic question in many game theory problems and real-world competitions. Finding the right approach to beat a particular human opponent is challenging. Here we use an AI (artificial intelligence) algorithm based on Markov Models of one fixed memory length (abbreviated as ""single AI"") to compete against humans in an iterated RPS game. We model and predict human competition behavior by combining many Markov Models with different fixed memory lengths (abbreviated as ""multi-AI""), and develop an architecture of multi-AI with changeable parameters to adapt to different competition strategies. We introduce a parameter called ""focus length"" (a positive number such as 5 or 10) to control the speed and sensitivity for our multi-AI to adapt to the opponent's strategy change. The focus length is the number of previous rounds that the multi-AI should look at when determining which Single-AI has the best performance and should choose to play for the next game. We experimented with 52 different people, each playing 300 rounds continuously against one specific multi-AI model, and demonstrated that our strategy could win against more than 95% of human opponents. △ Less","22 November, 2020",https://arxiv.org/pdf/2003.06769
CoCoPIE: Making Mobile AI Sweet As PIE --Compression-Compilation Co-Design Goes a Long Way,Shaoshan Liu;Bin Ren;Xipeng Shen;Yanzhi Wang,"Assuming hardware is the major constraint for enabling real-time mobile intelligence, the industry has mainly dedicated their efforts to developing specialized hardware accelerators for machine learning and inference. This article challenges the assumption. By drawing on a recent real-time AI optimization framework CoCoPIE, it maintains that with effective compression-compiler co-design, it is possible to enable real-time artificial intelligence on mainstream end devices without special hardware. CoCoPIE is a software framework that holds numerous records on mobile AI: the first framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer, language models, and so on; the fastest DNN pruning and acceleration framework, up to 180X faster compared with current DNN pruning on other frameworks such as TensorFlow-Lite; making many representative AI applications able to run in real-time on off-the-shelf mobile devices that have been previously regarded possible only with special hardware support; making off-the-shelf mobile devices outperform a number of representative ASIC and FPGA solutions in terms of energy efficiency and/or performance. △ Less","14 May, 2020",https://arxiv.org/pdf/2003.06700
Ethics in the digital era,David Pastor-Escuredo,"Ethics is an ancient matter for human kind, from the origin of civilizations ethics have been related with the most relevant human concerns and determined cultures. Ethics was initially related to religion, politics and philosophy to then be fragmented into specific communities of practice. The undergoing digital revolution enabled by Artificial Intelligence and Data are bringing ethical wicked problems in the social application of these technologies. However, a broader perspective is also necessary. We now face global and highly dynamics challenges that affect groups and individuals, specially those that are most vulnerable. Individual-oriented ethics are no longer sufficient, the new ethic has to consider the several scales in which the current complex society is organized and the interconnections between different systems. Ethics should also give a response to the systemic changes in behavior produced by external factors and threats. Furthermore, AI and digital technologies are global and make us more connected and smart but also more homogeneous, predictable and ultimately controllable. Ethic must take a stand to preserve and keep promoting individuals rights and uniqueness and cultural heterogeneity. Digital technologies have to the foundation for new models of society and help ensure ethical individual and collective values. For these reasons science has to be at the core of the new ethic as it helps understand the complex world. Finally, AI has advanced through the ambition to humanize matter, so we should expect ethics to give a response to the future status of machines and their interactions with humans. △ Less","10 May, 2020",https://arxiv.org/pdf/2003.06530
Exploring User Opinions of Fairness in Recommender Systems,Jessie Smith;Nasim Sonboli;Casey Fiesler;Robin Burke,"Algorithmic fairness for artificial intelligence has become increasingly relevant as these systems become more pervasive in society. One realm of AI, recommender systems, presents unique challenges for fairness due to trade offs between optimizing accuracy for users and fairness to providers. But what is fair in the context of recommendation--particularly when there are multiple stakeholders? In an initial exploration of this problem, we ask users what their ideas of fair treatment in recommendation might be, and why. We analyze what might cause discrepancies or changes between user's opinions towards fairness to eventually help inform the design of fairer and more transparent recommendation algorithms. △ Less","17 April, 2020",https://arxiv.org/pdf/2003.06461
Comparative analysis of machine learning models for Ammonia Capture of Ionic Liquids,Shahaboddin Shamshirband;Narjes Nabipour;Masoud Hadipoor;Alireza Baghban;Amir Mosavi,"Industry uses various solvents in the processes of refrigeration and ventilation. Among them, the Ionic liquids (ILs) as the relatively new solvents, are known for their proven eco-friendly characteristics. In this research, a comprehensive literature review was carried out to deliver an insight into the ILs and the prediction models used for estimating the ammonia solubility in ILs. Furthermore, a number of advanced machine learning methods, i.e. multilayer perceptron (MLP) and a combination of particle swarm optimization (PSO) and adaptive neuro-fuzzy inference system (ANFIS) models are used to estimate the solubility of ammonia in various ionic liquids. Affecting parameters were molecular weight, critical temperature and pressure of ILs. Furthermore, the salability is also predicted using the two-equation of states. Down the line, some comparisons were drawn between experimental and modeling results which is rarely done. The study shows that the equations of states are not able estimate the solubility of ammonia accurately, by contrast, artificial intelligence methods have produced promising results. △ Less","19 February, 2020",https://arxiv.org/pdf/2003.06224
How the Brain might use Division,Kieran Greer,"One of the most fundamental questions in Biology or Artificial Intelligence is how the human brain performs mathematical functions. How does a neural architecture that may organise itself mostly through statistics, know what to do? One possibility is to extract the problem to something more abstract. This becomes clear when thinking about how the brain handles large numbers, for example to the power of something, when simply summing to an answer is not feasible. In this paper, the author suggests that the maths question can be answered more easily if the problem is changed into one of symbol manipulation and not just number counting. If symbols can be compared and manipulated, maybe without understanding completely what they are, then the mathematical operations become relative and some of them might even be rote learned. The proposed system may also be suggested as an alternative to the traditional computer binary system. Any of the actual maths still breaks down into binary operations, while a more symbolic level above that can manipulate the numbers and reduce the problem size, thus making the binary operations simpler. An interesting result of looking at this is the possibility of a new fractal equation resulting from division, that can be used as a measure of good fit and would help the brain decide how to solve something through self-replacement and a comparison with this good fit. △ Less","24 March, 2020",https://arxiv.org/pdf/2003.05320
Machine Learning for Intelligent Optical Networks: A Comprehensive Survey,Rentao Gu;Zeyuan Yang;Yuefeng Ji,"With the rapid development of Internet and communication systems, both in services and technologies, communication networks have been suffering increasing complexity. It is imperative to improve intelligence in communication network, and several aspects have been incorporating with Artificial Intelligence (AI) and Machine Learning (ML). Optical network, which plays an important role both in core and access network in communication networks, also faces great challenges of system complexity and the requirement of manual operations. To overcome the current limitations and address the issues of future optical networks, it is essential to deploy more intelligence capability to enable autonomous and exible network operations. ML techniques are proved to have superiority on solving complex problems; and thus recently, ML techniques have been used for many optical network applications. In this paper, a detailed survey of existing applications of ML for intelligent optical networks is presented. The applications of ML are classified in terms of their use cases, which are categorized into optical network control and resource management, and optical networks monitoring and survivability. The use cases are analyzed and compared according to the used ML techniques. Besides, a tutorial for ML applications is provided from the aspects of the introduction of common ML algorithms, paradigms of ML, and motivations of applying ML. Lastly, challenges and possible solutions of ML application in optical networks are also discussed, which intends to inspire future innovations in leveraging ML to build intelligent optical networks. △ Less","11 March, 2020",https://arxiv.org/pdf/2003.05290
Uncovering the Data-Related Limits of Human Reasoning Research: An Analysis based on Recommender Systems,Nicolas Riesterer;Daniel Brand;Marco Ragni,"Understanding the fundamentals of human reasoning is central to the development of any system built to closely interact with humans. Cognitive science pursues the goal of modeling human-like intelligence from a theory-driven perspective with a strong focus on explainability. Syllogistic reasoning as one of the core domains of human reasoning research has seen a surge of computational models being developed over the last years. However, recent analyses of models' predictive performances revealed a stagnation in improvement. We believe that most of the problems encountered in cognitive science are not due to the specific models that have been developed but can be traced back to the peculiarities of behavioral data instead. Therefore, we investigate potential data-related reasons for the problems in human reasoning research by comparing model performances on human and artificially generated datasets. In particular, we apply collaborative filtering recommenders to investigate the adversarial effects of inconsistencies and noise in data and illustrate the potential for data-driven methods in a field of research predominantly concerned with gaining high-level theoretical insight into a domain. Our work (i) provides insight into the levels of noise to be expected from human responses in reasoning data, (ii) uncovers evidence for an upper-bound of performance that is close to being reached urging for an extension of the modeling task, and (iii) introduces the tools and presents initial results to pioneer a new paradigm for investigating and modeling reasoning focusing on predicting responses for individual human reasoners. △ Less","11 March, 2020",https://arxiv.org/pdf/2003.05196
Vector symbolic architectures for context-free grammars,Peter beim Graben;Markus Huber;Werner Meyer;Ronald Römer;Matthias Wolff,"Background / introduction. Vector symbolic architectures (VSA) are a viable approach for the hyperdimensional representation of symbolic data, such as documents, syntactic structures, or semantic frames. Methods. We present a rigorous mathematical framework for the representation of phrase structure trees and parse trees of context-free grammars (CFG) in Fock space, i.e. infinite-dimensional Hilbert space as being used in quantum field theory. We define a novel normal form for CFG by means of term algebras. Using a recently developed software toolbox, called FockBox, we construct Fock space representations for the trees built up by a CFG left-corner (LC) parser. Results. We prove a universal representation theorem for CFG term algebras in Fock space and illustrate our findings through a low-dimensional principal component projection of the LC parser states. Conclusions. Our approach could leverage the development of VSA for explainable artificial intelligence (XAI) by means of hyperdimensional deep neural computation. It could be of significance for the improvement of cognitive user interfaces and other applications of VSA in machine learning. △ Less","25 September, 2020",https://arxiv.org/pdf/2003.05171
PANDA: A Gigapixel-level Human-centric Video Dataset,Xueyang Wang;Xiya Zhang;Yinheng Zhu;Yuchen Guo;Xiaoyun Yuan;Liuyu Xiang;Zerun Wang;Guiguang Ding;David J Brady;Qionghai Dai;Lu Fang,"We present PANDA, the first gigaPixel-level humAN-centric viDeo dAtaset, for large-scale, long-term, and multi-object visual analysis. The videos in PANDA were captured by a gigapixel camera and cover real-world scenes with both wide field-of-view (~1 square kilometer area) and high-resolution details (~gigapixel-level/frame). The scenes may contain 4k head counts with over 100x scale variation. PANDA provides enriched and hierarchical ground-truth annotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute labels, 12.7k trajectories, 2.2k groups and 2.9k interactions. We benchmark the human detection and tracking tasks. Due to the vast variance of pedestrian pose, scale, occlusion and trajectory, existing approaches are challenged by both accuracy and efficiency. Given the uniqueness of PANDA with both wide FoV and high resolution, a new task of interaction-aware group detection is introduced. We design a 'global-to-local zoom-in' framework, where global trajectories and local interactions are simultaneously encoded, yielding promising results. We believe PANDA will contribute to the community of artificial intelligence and praxeology by understanding human behaviors and interactions in large-scale real-world scenes. PANDA Website: http://www.panda-dataset.com. △ Less","10 March, 2020",https://arxiv.org/pdf/2003.04852
Spitzoid Lesions Diagnosis based on GA feature selection and Random Forest,Abir Belaala;Labib Sadek;Noureddine Zerhouni;Christine Devalland,"Spitzoid lesions broadly categorized into Spitz Nevus (SN), Atypical Spitz Tumors (AST), and Spitz Melanomas (SM). The accurate diagnosis of these lesions is one of the most challenges for dermapathologists; this is due to the high similarities between them. Data mining techniques are successfully applied to situations like these where complexity exists. This study aims to develop an artificial intelligence model to support the diagnosis of Spitzoid lesions. A private spitzoid lesions dataset have been used to evaluate the system proposed in this study. The proposed system has three stages. In the first stage, SMOTE method applied to solve the imbalance data problem, in the second stage, in order to eliminate irrelevant features; genetic algorithm is used to select significant features. This later reduces the computational complexity and speed up the data mining process. In the third stage, Random forest classifier is employed to make a decision for two different categories of lesions (Spitz nevus or Atypical Spitz Tumors). The performance of our proposed scheme is evaluated using accuracy, sensitivity, specificity, G-mean, F- measure, ROC and AUC. Results obtained with our SMOTE-GA-RF model with GA-based 16 features show a great performance with accuracy 0.97, F-measure 0.98, AUC 0.98, and G-mean 0.97.Results obtained in this study have potential to open new opportunities in diagnosis of spitzoid lesions. △ Less","3 June, 2020",https://arxiv.org/pdf/2003.04745
Physics for Neuromorphic Computing,Danijela Markovic;Alice Mizrahi;Damien Querlioz;Julie Grollier,"Neuromorphic computing takes inspiration from the brain to create energy efficient hardware for information processing, capable of highly sophisticated tasks. In this article, we make the case that building this new hardware necessitates reinventing electronics. We show that research in physics and material science will be key to create artificial nano-neurons and synapses, to connect them together in huge numbers, to organize them in complex systems, and to compute with them efficiently. We describe how some researchers choose to take inspiration from artificial intelligence to move forward in this direction, whereas others prefer taking inspiration from neuroscience, and we highlight recent striking results obtained with these two approaches. Finally, we discuss the challenges and perspectives in neuromorphic physics, which include developing the algorithms and the hardware hand in hand, making significant advances with small toy systems, as well as building large scale networks. △ Less","8 March, 2020",https://arxiv.org/pdf/2003.04711
Neuro-symbolic Architectures for Context Understanding,Alessandro Oltramari;Jonathan Francis;Cory Henson;Kaixin Ma;Ruwan Wickramarachchi,"Computational context understanding refers to an agent's ability to fuse disparate sources of information for decision-making and is, therefore, generally regarded as a prerequisite for sophisticated machine reasoning capabilities, such as in artificial intelligence (AI). Data-driven and knowledge-driven methods are two classical techniques in the pursuit of such machine sense-making capability. However, while data-driven methods seek to model the statistical regularities of events by making observations in the real-world, they remain difficult to interpret and they lack mechanisms for naturally incorporating external knowledge. Conversely, knowledge-driven methods, combine structured knowledge bases, perform symbolic reasoning based on axiomatic principles, and are more interpretable in their inferential processing; however, they often lack the ability to estimate the statistical salience of an inference. To combat these issues, we propose the use of hybrid AI methodology as a general framework for combining the strengths of both approaches. Specifically, we inherit the concept of neuro-symbolism as a way of using knowledge-bases to guide the learning progress of deep neural networks. We further ground our discussion in two applications of neuro-symbolism and, in both cases, show that our systems maintain interpretability while achieving comparable performance, relative to the state-of-the-art. △ Less","9 March, 2020",https://arxiv.org/pdf/2003.04707
Joint Parameter-and-Bandwidth Allocation for Improving the Efficiency of Partitioned Edge Learning,Dingzhu Wen;Mehdi Bennis;Kaibin Huang,"To leverage data and computation capabilities of mobile devices, machine learning algorithms are deployed at the network edge for training artificial intelligence (AI) models, resulting in the new paradigm of edge learning. In this paper, we consider the framework of partitioned edge learning for iteratively training a large-scale model using many resource-constrained devices (called workers). To this end, in each iteration, the model is dynamically partitioned into parametric blocks, which are downloaded to worker groups for updating using data subsets. Then, the local updates are uploaded to and cascaded by the server for updating a global model. To reduce resource usage by minimizing the total learning-and-communication latency, this work focuses on the novel joint design of parameter (computation load) allocation and bandwidth allocation (for downloading and uploading). Two design approaches are adopted. First, a practical sequential approach, called partially integrated parameter-and-bandwidth allocation (PABA), yields two schemes, namely bandwidth aware parameter allocation and parameter aware bandwidth allocation. The former minimizes the load for the slowest (in computing) of worker groups, each training a same parametric block. The latter allocates the largest bandwidth to the worker being the latency bottleneck. Second, PABA are jointly optimized. Despite its being a nonconvex problem, an efficient and optimal solution algorithm is derived by intelligently nesting a bisection search and solving a convex problem. Experimental results using real data demonstrate that integrating PABA can substantially improve the performance of partitioned edge learning in terms of latency (by e.g., 46%) and accuracy (by e.g., 4%). △ Less","29 June, 2020",https://arxiv.org/pdf/2003.04544
Learning to be Global Optimizer,Haotian Zhang;Jianyong Sun;Zongben Xu,"The advancement of artificial intelligence has cast a new light on the development of optimization algorithm. This paper proposes to learn a two-phase (including a minimization phase and an escaping phase) global optimization algorithm for smooth non-convex functions. For the minimization phase, a model-driven deep learning method is developed to learn the update rule of descent direction, which is formalized as a nonlinear combination of historical information, for convex functions. We prove that the resultant algorithm with the proposed adaptive direction guarantees convergence for convex functions. Empirical study shows that the learned algorithm significantly outperforms some well-known classical optimization algorithms, such as gradient descent, conjugate descent and BFGS, and performs well on ill-posed functions. The escaping phase from local optimum is modeled as a Markov decision process with a fixed escaping policy. We further propose to learn an optimal escaping policy by reinforcement learning. The effectiveness of the escaping policies is verified by optimizing synthesized functions and training a deep neural network for CIFAR image classification. The learned two-phase global optimization algorithm demonstrates a promising global search capability on some benchmark functions and machine learning tasks. △ Less","9 March, 2020",https://arxiv.org/pdf/2003.04521
MLography: An Automated Quantitative Metallography Model for Impurities Anomaly Detection using Novel Data Mining and Deep Learning Approach,Matan Rusanovsky;Gal Oren;Sigalit Ifergane;Ofer Beeri,"The micro-structure of most of the engineering alloys contains some inclusions and precipitates, which may affect their properties, therefore it is crucial to characterize them. In this work we focus on the development of a state-of-the-art artificial intelligence model for Anomaly Detection named MLography to automatically quantify the degree of anomaly of impurities in alloys. For this purpose, we introduce several anomaly detection measures: Spatial, Shape and Area anomaly, that successfully detect the most anomalous objects based on their objective, given that the impurities were already labeled. The first two measures quantify the degree of anomaly of each object by how each object is distant and big compared to its neighborhood, and by the abnormally of its own shape respectively. The last measure, combines the former two and highlights the most anomalous regions among all input images, for later (physical) examination. The performance of the model is presented and analyzed based on few representative cases. We stress that although the models presented here were developed for metallography analysis, most of them can be generalized to a wider set of problems in which anomaly detection of geometrical objects is desired. All models as well as the data-set that was created for this work, are publicly available at: https://github.com/matanr/MLography. △ Less","27 February, 2020",https://arxiv.org/pdf/2003.04226
Two Decades of AI4NETS-AI/ML for Data Networks: Challenges & Research Directions,Pedro Casas,"The popularity of Artificial Intelligence (AI) -- and of Machine Learning (ML) as an approach to AI, has dramatically increased in the last few years, due to its outstanding performance in various domains, notably in image, audio, and natural language processing. In these domains, AI success-stories are boosting the applied field. When it comes to AI/ML for data communication Networks (AI4NETS), and despite the many attempts to turn networks into learning agents, the successful application of AI/ML in networking is limited. There is a strong resistance against AI/ML-based solutions, and a striking gap between the extensive academic research and the actual deployments of such AI/ML-based systems in operational environments. The truth is, there are still many unsolved complex challenges associated to the analysis of networking data through AI/ML, which hinders its acceptability and adoption in the practice. In this positioning paper I elaborate on the most important show-stoppers in AI4NETS, and present a research agenda to tackle some of these challenges, enabling a natural adoption of AI/ML for networking. In particular, I focus the future research in AI4NETS around three major pillars: (i) to make AI/ML immediately applicable in networking problems through the concepts of effective learning, turning it into a useful and reliable way to deal with complex data-driven networking problems; (ii) to boost the adoption of AI/ML at the large scale by learning from the Internet-paradigm itself, conceiving novel distributed and hierarchical learning approaches mimicking the distributed topological principles and operation of the Internet itself; and (iii) to exploit the softwarization and distribution of networks to conceive AI/ML-defined Networks (AIDN), relying on the distributed generation and re-usage of knowledge through novel Knowledge Delivery Networks (KDNs). △ Less","2 March, 2020",https://arxiv.org/pdf/2003.04080
ATHAFI: Agile Threat Hunting And Forensic Investigation,Rami Puzis;Polina Zilberman;Yuval Elovici,"Attackers rapidly change their attacks to evade detection. Even the most sophisticated Intrusion Detection Systems that are based on artificial intelligence and advanced data analytic cannot keep pace with the rapid development of new attacks. When standard detection mechanisms fail or do not provide sufficient forensic information to investigate and mitigate attacks, targeted threat hunting performed by competent personnel is used. Unfortunately, many organization do not have enough security analysts to perform threat hunting tasks and today the level of automation of threat hunting is low. In this paper we describe a framework for agile threat hunting and forensic investigation (ATHAFI), which automates the threat hunting process at multiple levels. Adaptive targeted data collection, attack hypotheses generation, hypotheses testing, and continuous threat intelligence feeds allow to perform simple investigations in a fully automated manner. The increased level of automation will significantly boost the analyst's productivity during investigation of the harshest cases. Special Workflow Generation module adapts the threat hunting procedures either to the latest Threat Intelligence obtained from external sources (e.g. National CERT) or to the likeliest attack hypotheses generated by the Attack Hypotheses Generation module. The combination of Attack Hypotheses Generation and Workflows Generation enables intelligent adjustment of workflows, which react to emerging threats effectively. △ Less","7 March, 2020",https://arxiv.org/pdf/2003.03663
A machine learning environment for evaluating autonomous driving software,Jussi Hanhirova;Anton Debner;Matias Hyyppä;Vesa Hirvisalo,"Autonomous vehicles need safe development and testing environments. Many traffic scenarios are such that they cannot be tested in the real world. We see hybrid photorealistic simulation as a viable tool for developing AI (artificial intelligence) software for autonomous driving. We present a machine learning environment for detecting autonomous vehicle corner case behavior. Our environment is based on connecting the CARLA simulation software to TensorFlow machine learning framework and custom AI client software. The AI client software receives data from a simulated world via virtual sensors and transforms the data into information using machine learning models. The AI clients control vehicles in the simulated world. Our environment monitors the state assumed by the vehicle AIs to the ground truth state derived from the simulation model. Our system can search for corner cases where the vehicle AI is unable to correctly understand the situation. In our paper, we present the overall hybrid simulator architecture and compare different configurations. We present performance measurements from real setups, and outline the main parameters affecting the hybrid simulator performance. △ Less","7 March, 2020",https://arxiv.org/pdf/2003.03576
Heterogeneity Loss to Handle Intersubject and Intrasubject Variability in Cancer,Shubham Goswami;Suril Mehta;Dhruva Sahrawat;Anubha Gupta;Ritu Gupta,"Developing nations lack adequate number of hospitals with modern equipment and skilled doctors. Hence, a significant proportion of these nations' population, particularly in rural areas, is not able to avail specialized and timely healthcare facilities. In recent years, deep learning (DL) models, a class of artificial intelligence (AI) methods, have shown impressive results in medical domain. These AI methods can provide immense support to developing nations as affordable healthcare solutions. This work is focused on one such application of blood cancer diagnosis. However, there are some challenges to DL models in cancer research because of the unavailability of a large data for adequate training and the difficulty of capturing heterogeneity in data at different levels ranging from acquisition characteristics, session, to subject-level (within subjects and across subjects). These challenges render DL models prone to overfitting and hence, models lack generalization on prospective subjects' data. In this work, we address these problems in the application of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep learning. We propose heterogeneity loss that captures subject-level heterogeneity, thereby, forcing the neural network to learn subject-independent features. We also propose an unorthodox ensemble strategy that helps us in providing improved classification over models trained on 7-folds giving a weighted-F_1 score of 95.26% on unseen (test) subjects' data that are, so far, the best results on the C-NMC 2019 dataset for B-ALL classification. △ Less","18 March, 2020",https://arxiv.org/pdf/2003.03295
COMPLEX-IT: A Case-Based Modeling and Scenario Simulation Platform for Social Inquiry,Corey Schimpf;Brian Castellani,"COMPLEX-IT is a case-based, mixed-methods platform for social inquiry into complex data/systems, designed to increase non-expert access to the tools of computational social science (i.e., cluster analysis, artificial intelligence, data visualization, data forecasting, and scenario simulation). In particular, COMPLEX-IT aids social inquiry though a heavy emphasis on learning about the complex data/system under study, which it does by (a) identifying and forecasting major and minor clusters/trends; (b) visualizing their complex causality; and (c) simulating scenarios for potential interventions. COMPLEX-IT is accessible through the web or can be run locally and is powered by R and the Shiny web framework. △ Less","6 March, 2020",https://arxiv.org/pdf/2003.03099
Cost-Sensitive Portfolio Selection via Deep Reinforcement Learning,Yifan Zhang;Peilin Zhao;Qingyao Wu;Bin Li;Junzhou Huang;Mingkui Tan,"Portfolio Selection is an important real-world financial task and has attracted extensive attention in artificial intelligence communities. This task, however, has two main difficulties: (i) the non-stationary price series and complex asset correlations make the learning of feature representation very hard; (ii) the practicality principle in financial markets requires controlling both transaction and risk costs. Most existing methods adopt handcraft features and/or consider no constraints for the costs, which may make them perform unsatisfactorily and fail to control both costs in practice. In this paper, we propose a cost-sensitive portfolio selection method with deep reinforcement learning. Specifically, a novel two-stream portfolio policy network is devised to extract both price series patterns and asset correlations, while a new cost-sensitive reward function is developed to maximize the accumulated return and constrain both costs via reinforcement learning. We theoretically analyze the near-optimality of the proposed reward, which shows that the growth rate of the policy regarding this reward function can approach the theoretical optimum. We also empirically evaluate the proposed method on real-world datasets. Promising results demonstrate the effectiveness and superiority of the proposed method in terms of profitability, cost-sensitivity and representation abilities. △ Less","6 March, 2020",https://arxiv.org/pdf/2003.03051
Threats to Federated Learning: A Survey,Lingjuan Lyu;Han Yu;Qiang Yang,"With the emergence of data silos and popular privacy awareness, the traditional centralized approach of training artificial intelligence (AI) models is facing strong challenges. Federated learning (FL) has recently emerged as a promising solution under this new reality. Existing FL protocol design has been shown to exhibit vulnerabilities which can be exploited by adversaries both within and without the system to compromise data privacy. It is thus of paramount importance to make FL system designers to be aware of the implications of future FL algorithm design on privacy-preservation. Currently, there is no survey on this topic. In this paper, we bridge this important gap in FL literature. By providing a concise introduction to the concept of FL, and a unique taxonomy covering threat models and two major attacks on FL: 1) poisoning attacks and 2) inference attacks, this paper provides an accessible review of this important topic. We highlight the intuitions, key techniques as well as fundamental assumptions adopted by various attacks, and discuss promising future research directions towards more robust privacy preservation in FL. △ Less","4 March, 2020",https://arxiv.org/pdf/2003.02133
AI-Mediated Exchange Theory,Xiao Ma;Taylor W. Brown,"As Artificial Intelligence (AI) plays an ever-expanding role in sociotechnical systems, it is important to articulate the relationships between humans and AI. However, the scholarly communities studying human-AI relationships -- including but not limited to social computing, machine learning, science and technology studies, and other social sciences -- are divided by the perspectives that define them. These perspectives vary both by their focus on humans or AI, and in the micro/macro lenses through which they approach subjects. These differences inhibit the integration of findings, and thus impede science and interdisciplinarity. In this position paper, we propose the development of a framework AI-Mediated Exchange Theory (AI-MET) to bridge these divides. As an extension to Social Exchange Theory (SET) in the social sciences, AI-MET views AI as influencing human-to-human relationships via a taxonomy of mediation mechanisms. We list initial ideas of these mechanisms, and show how AI-MET can be used to help human-AI research communities speak to one another. △ Less","4 March, 2020",https://arxiv.org/pdf/2003.02093
Automatic Hyper-Parameter Optimization Based on Mapping Discovery from Data to Hyper-Parameters,Bozhou Chen;Kaixin Zhang;Longshen Ou;Chenmin Ba;Hongzhi Wang;Chunnan Wang,"Machine learning algorithms have made remarkable achievements in the field of artificial intelligence. However, most machine learning algorithms are sensitive to the hyper-parameters. Manually optimizing the hyper-parameters is a common method of hyper-parameter tuning. However, it is costly and empirically dependent. Automatic hyper-parameter optimization (autoHPO) is favored due to its effectiveness. However, current autoHPO methods are usually only effective for a certain type of problems, and the time cost is high. In this paper, we propose an efficient automatic parameter optimization approach, which is based on the mapping from data to the corresponding hyper-parameters. To describe such mapping, we propose a sophisticated network structure. To obtain such mapping, we develop effective network constrution algorithms. We also design strategy to optimize the result futher during the application of the mapping. Extensive experimental results demonstrate that the proposed approaches outperform the state-of-the-art apporaches significantly. △ Less","3 March, 2020",https://arxiv.org/pdf/2003.01751
EXPLAIN-IT: Towards Explainable AI for Unsupervised Network Traffic Analysis,Andrea Morichetta;Pedro Casas;Marco Mellia,"The application of unsupervised learning approaches, and in particular of clustering techniques, represents a powerful exploration means for the analysis of network measurements. Discovering underlying data characteristics, grouping similar measurements together, and identifying eventual patterns of interest are some of the applications which can be tackled through clustering. Being unsupervised, clustering does not always provide precise and clear insight into the produced output, especially when the input data structure and distribution are complex and difficult to grasp. In this paper we introduce EXPLAIN-IT, a methodology which deals with unlabeled data, creates meaningful clusters, and suggests an explanation to the clustering results for the end-user. EXPLAIN-IT relies on a novel explainable Artificial Intelligence (AI) approach, which allows to understand the reasons leading to a particular decision of a supervised learning-based model, additionally extending its application to the unsupervised learning domain. We apply EXPLAIN-IT to the problem of YouTube video quality classification under encrypted traffic scenarios, showing promising results. △ Less","3 March, 2020",https://arxiv.org/pdf/2003.01670
Marketplace for AI Models,Abhishek Kumar;Benjamin Finley;Tristan Braud;Sasu Tarkoma;Pan Hui,"Artificial intelligence shows promise for solving many practical societal problems in areas such as healthcare and transportation. However, the current mechanisms for AI model diffusion such as Github code repositories, academic project webpages, and commercial AI marketplaces have some limitations; for example, a lack of monetization methods, model traceability, and model auditabilty. In this work, we sketch guidelines for a new AI diffusion method based on a decentralized online marketplace. We consider the technical, economic, and regulatory aspects of such a marketplace including a discussion of solutions for problems in these areas. Finally, we include a comparative analysis of several current AI marketplaces that are already available or in development. We find that most of these marketplaces are centralized commercial marketplaces with relatively few models. △ Less","3 March, 2020",https://arxiv.org/pdf/2003.01593
FlexServe: Deployment of PyTorch Models as Flexible REST Endpoints,Edward Verenich;Alvaro Velasquez;M. G. Sarwar Murshed;Faraz Hussain,"The integration of artificial intelligence capabilities into modern software systems is increasingly being simplified through the use of cloud-based machine learning services and representational state transfer architecture design. However, insufficient information regarding underlying model provenance and the lack of control over model evolution serve as an impediment to the more widespread adoption of these services in many operational environments which have strict security requirements. Furthermore, tools such as TensorFlow Serving allow models to be deployed as RESTful endpoints, but require error-prone transformations for PyTorch models as these dynamic computational graphs. This is in contrast to the static computational graphs of TensorFlow. To enable rapid deployments of PyTorch models without intermediate transformations we have developed FlexServe, a simple library to deploy multi-model ensembles with flexible batching. △ Less","29 February, 2020",https://arxiv.org/pdf/2003.01538
Evidence-based explanation to promote fairness in AI systems,Juliana Jansen Ferreira;Mateus de Souza Monteiro,"As Artificial Intelligence (AI) technology gets more intertwined with every system, people are using AI to make decisions on their everyday activities. In simple contexts, such as Netflix recommendations, or in more complex context like in judicial scenarios, AI is part of people's decisions. People make decisions and usually, they need to explain their decision to others or in some matter. It is particularly critical in contexts where human expertise is central to decision-making. In order to explain their decisions with AI support, people need to understand how AI is part of that decision. When considering the aspect of fairness, the role that AI has on a decision-making process becomes even more sensitive since it affects the fairness and the responsibility of those people making the ultimate decision. We have been exploring an evidence-based explanation design approach to 'tell the story of a decision'. In this position paper, we discuss our approach for AI systems using fairness sensitive cases in the literature. △ Less","3 March, 2020",https://arxiv.org/pdf/2003.01525
Digital Collaborator: Augmenting Task Abstraction in Visualization Design with Artificial Intelligence,Aditeya Pandey;Yixuan Zhang;John A. Guerra-Gomez;Andrea G. Parker;Michelle A. Borkin,"In the task abstraction phase of the visualization design process, including in ""design studies"", a practitioner maps the observed domain goals to generalizable abstract tasks using visualization theory in order to better understand and address the users needs. We argue that this manual task abstraction process is prone to errors due to designer biases and a lack of domain background and knowledge. Under these circumstances, a collaborator can help validate and provide sanity checks to visualization practitioners during this important task abstraction stage. However, having a human collaborator is not always feasible and may be subject to the same biases and pitfalls. In this paper, we first describe the challenges associated with task abstraction. We then propose a conceptual Digital Collaborator: an artificial intelligence system that aims to help visualization practitioners by augmenting their ability to validate and reason about the output of task abstraction. We also discuss several practical design challenges of designing and implementing such systems △ Less","2 March, 2020",https://arxiv.org/pdf/2003.01304
BARD: A structured technique for group elicitation of Bayesian networks to support analytic reasoning,Ann E. Nicholson;Kevin B. Korb;Erik P. Nyberg;Michael Wybrow;Ingrid Zukerman;Steven Mascaro;Shreshth Thakur;Abraham Oshni Alvandi;Jeff Riley;Ross Pearson;Shane Morris;Matthieu Herrmann;A. K. M. Azad;Fergus Bolger;Ulrike Hahn;David Lagnado,"In many complex, real-world situations, problem solving and decision making require effective reasoning about causation and uncertainty. However, human reasoning in these cases is prone to confusion and error. Bayesian networks (BNs) are an artificial intelligence technology that models uncertain situations, supporting probabilistic and causal reasoning and decision making. However, to date, BN methodologies and software require significant upfront training, do not provide much guidance on the model building process, and do not support collaboratively building BNs. BARD (Bayesian ARgumentation via Delphi) is both a methodology and an expert system that utilises (1) BNs as the underlying structured representations for better argument analysis, (2) a multi-user web-based software platform and Delphi-style social processes to assist with collaboration, and (3) short, high-quality e-courses on demand, a highly structured process to guide BN construction, and a variety of helpful tools to assist in building and reasoning with BNs, including an automated explanation tool to assist effective report writing. The result is an end-to-end online platform, with associated online training, for groups without prior BN expertise to understand and analyse a problem, build a model of its underlying probabilistic causal structure, validate and reason with the causal model, and use it to produce a written analytic report. Initial experimental results demonstrate that BARD aids in problem solving, reasoning and collaboration. △ Less","2 March, 2020",https://arxiv.org/pdf/2003.01207
CAAI -- A Cognitive Architecture to Introduce Artificial Intelligence in Cyber-Physical Production Systems,Andreas Fischbach;Jan Strohschein;Andreas Bunte;Jörg Stork;Heide Faeskorn-Woyke;Natalia Moriz;Thomas Bartz-Beielstein,"This paper introduces CAAI, a novel cognitive architecture for artificial intelligence in cyber-physical production systems. The goal of the architecture is to reduce the implementation effort for the usage of artificial intelligence algorithms. The core of the CAAI is a cognitive module that processes declarative goals of the user, selects suitable models and algorithms, and creates a configuration for the execution of a processing pipeline on a big data platform. Constant observation and evaluation against performance criteria assess the performance of pipelines for many and varying use cases. Based on these evaluations, the pipelines are automatically adapted if necessary. The modular design with well-defined interfaces enables the reusability and extensibility of pipeline components. A big data platform implements this modular design supported by technologies such as Docker, Kubernetes, and Kafka for virtualization and orchestration of the individual components and their communication. The implementation of the architecture is evaluated using a real-world use case. △ Less","26 February, 2020",https://arxiv.org/pdf/2003.00925
Introducing Fuzzy Layers for Deep Learning,Stanton R. Price;Steven R. Price;Derek T. Anderson,"Many state-of-the-art technologies developed in recent years have been influenced by machine learning to some extent. Most popular at the time of this writing are artificial intelligence methodologies that fall under the umbrella of deep learning. Deep learning has been shown across many applications to be extremely powerful and capable of handling problems that possess great complexity and difficulty. In this work, we introduce a new layer to deep learning: the fuzzy layer. Traditionally, the network architecture of neural networks is composed of an input layer, some combination of hidden layers, and an output layer. We propose the introduction of fuzzy layers into the deep learning architecture to exploit the powerful aggregation properties expressed through fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals. To date, fuzzy approaches taken to deep learning have been through the application of various fusion strategies at the decision level to aggregate outputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16, GoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown to improve accuracy performance for image classification tasks, none have explored the use of fuzzified intermediate, or hidden, layers. Herein, we present a new deep learning strategy that incorporates fuzzy strategies into the deep learning architecture focused on the application of semantic segmentation using per-pixel classification. Experiments are conducted on a benchmark data set as well as a data set collected via an unmanned aerial system at a U.S. Army test site for the task of automatic road segmentation, and preliminary results are promising. △ Less","21 February, 2020",https://arxiv.org/pdf/2003.00880
Learning to Resolve Alliance Dilemmas in Many-Player Zero-Sum Games,Edward Hughes;Thomas W. Anthony;Tom Eccles;Joel Z. Leibo;David Balduzzi;Yoram Bachrach,"Zero-sum games have long guided artificial intelligence research, since they possess both a rich strategy space of best-responses and a clear evaluation metric. What's more, competition is a vital mechanism in many real-world multi-agent systems capable of generating intelligent innovations: Darwinian evolution, the market economy and the AlphaZero algorithm, to name a few. In two-player zero-sum games, the challenge is usually viewed as finding Nash equilibrium strategies, safeguarding against exploitation regardless of the opponent. While this captures the intricacies of chess or Go, it avoids the notion of cooperation with co-players, a hallmark of the major transitions leading from unicellular organisms to human civilization. Beyond two players, alliance formation often confers an advantage; however this requires trust, namely the promise of mutual cooperation in the face of incentives to defect. Successful play therefore requires adaptation to co-players rather than the pursuit of non-exploitability. Here we argue that a systematic study of many-player zero-sum games is a crucial element of artificial intelligence research. Using symmetric zero-sum matrix games, we demonstrate formally that alliance formation may be seen as a social dilemma, and empirically that naïve multi-agent reinforcement learning therefore fails to form alliances. We introduce a toy model of economic competition, and show how reinforcement learning may be augmented with a peer-to-peer contract mechanism to discover and enforce alliances. Finally, we generalize our agent model to incorporate temporally-extended contracts, presenting opportunities for further work. △ Less","27 February, 2020",https://arxiv.org/pdf/2003.00799
Detection and Mitigation of Bias in Ted Talk Ratings,Rupam Acharyya;Shouman Das;Ankani Chattoraj;Oishani Sengupta;Md Iftekar Tanveer,"Unbiased data collection is essential to guaranteeing fairness in artificial intelligence models. Implicit bias, a form of behavioral conditioning that leads us to attribute predetermined characteristics to members of certain groups and informs the data collection process. This paper quantifies implicit bias in viewer ratings of TEDTalks, a diverse social platform assessing social and professional performance, in order to present the correlations of different kinds of bias across sensitive attributes. Although the viewer ratings of these videos should purely reflect the speaker's competence and skill, our analysis of the ratings demonstrates the presence of overwhelming and predominant implicit bias with respect to race and gender. In our paper, we present strategies to detect and mitigate bias that are critical to removing unfairness in AI. △ Less","2 March, 2020",https://arxiv.org/pdf/2003.00683
A review of machine learning applications in wildfire science and management,Piyush Jain;Sean C P Coogan;Sriram Ganapathi Subramanian;Mark Crowley;Steve Taylor;Mike D Flannigan,"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then the field has rapidly progressed congruently with the wide adoption of machine learning (ML) in the environmental sciences. Here, we present a scoping review of ML in wildfire science and management. Our objective is to improve awareness of ML among wildfire scientists and managers, as well as illustrate the challenging range of problems in wildfire science available to data scientists. We first present an overview of popular ML approaches used in wildfire science to date, and then review their use in wildfire science within six problem domains: 1) fuels characterization, fire detection, and mapping; 2) fire weather and climate change; 3) fire occurrence, susceptibility, and risk; 4) fire behavior prediction; 5) fire effects; and 6) fire management. We also discuss the advantages and limitations of various ML approaches and identify opportunities for future advances in wildfire science and management within a data science context. We identified 298 relevant publications, where the most frequently used ML methods included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. There exists opportunities to apply more current ML methods (e.g., deep learning and agent based learning) in wildfire science. However, despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods requires sophisticated knowledge for their application. Finally, we stress that the wildfire research and management community plays an active role in providing relevant, high quality data for use by practitioners of ML methods. △ Less","19 August, 2020",https://arxiv.org/pdf/2003.00646
On Safety Assessment of Artificial Intelligence,Jens Braband;Hendrik Schäbe,"In this paper we discuss how systems with Artificial Intelligence (AI) can undergo safety assessment. This is relevant, if AI is used in safety related applications. Taking a deeper look into AI models, we show, that many models of artificial intelligence, in particular machine learning, are statistical models. Safety assessment would then have t o concentrate on the model that is used in AI, besides the normal assessment procedure. Part of the budget of dangerous random failures for the relevant safety integrity level needs to be used for the probabilistic faulty behavior of the AI system. We demonstrate our thoughts with a simple example and propose a research challenge that may be decisive for the use of AI in safety related systems. △ Less","29 February, 2020",https://arxiv.org/pdf/2003.00260
Cities as they could be: Artificial Life and Urban Systems,Juste Raimbault,"The metaphor of cities as organisms has a long history in urban planning, and a few urban modeling approaches have explicitly been linked to Artificial Life. We propose in that paper to explore the extent of Artificial Life and Artificial Intelligence application to urban issues, by constructing and exploring a citation network of around 225,000 papers. It shows that most of the literature is indeed application of methodologies and a rather strong modularity of approaches. We finally develop ALife concepts which have a strong potential for the development of new urban theories. △ Less","28 February, 2020",https://arxiv.org/pdf/2002.12926
"Neural Network Segmentation of Interstitial Fibrosis, Tubular Atrophy, and Glomerulosclerosis in Renal Biopsies",Brandon Ginley;Kuang-Yu Jen;Avi Rosenberg;Felicia Yen;Sanjay Jain;Agnes Fogo;Pinaki Sarder,"Glomerulosclerosis, interstitial fibrosis, and tubular atrophy (IFTA) are histologic indicators of irrecoverable kidney injury. In standard clinical practice, the renal pathologist visually assesses, under the microscope, the percentage of sclerotic glomeruli and the percentage of renal cortical involvement by IFTA. Estimation of IFTA is a subjective process due to a varied spectrum and definition of morphological manifestations. Modern artificial intelligence and computer vision algorithms have the ability to reduce inter-observer variability through rigorous quantitation. In this work, we apply convolutional neural networks for the segmentation of glomerulosclerosis and IFTA in periodic acid-Schiff stained renal biopsies. The convolutional network approach achieves high performance in intra-institutional holdout data, and achieves moderate performance in inter-intuitional holdout data, which the network had never seen in training. The convolutional approach demonstrated interesting properties, such as learning to predict regions better than the provided ground truth as well as developing its own conceptualization of segmental sclerosis. Subsequent estimations of IFTA and glomerulosclerosis percentages showed high correlation with ground truth. △ Less","28 February, 2020",https://arxiv.org/pdf/2002.12868
Real time Smart Contracts for IoT using Blockchain and Collaborative Intelligence based Dynamic Pricing for the next generation Smart Toll Application,Misha Abraham;Himajit Aithal;Krishnan Mohan,"The confluence of Internet of Things(IoT) , Blockchain(BC) and Artificial Intelligence(AI) acts as a key accelerator for enabling Machine Economy. To be ready for future businesses these technologies needs to be adapted by extending the IoT capabilities to Economy of Things (EoT) capabilities. In this paper we focus on one such implementation experience for Smart Toll Transaction application in the domain of mobility. Our paper showcases a possible solution by leveraging negotiations, decision making, distributed learning capabilities at the devices level using AI-enabled Multi-Agent Systems and the real-time smart contracts between the Cars and Tolls using Blockchain. This solution also showcases the monetization of real time data coming from various IoT devices which are part of vehicles and infrastructure. While blockchain secures the privacy of the participants it also acts as an economic transactional layer and governance layer between the devices in the networ △ Less","28 February, 2020",https://arxiv.org/pdf/2002.12654
Towards a Geometry Automated Provers Competition,Nuno Baeta;Pedro Quaresma;Zoltán Kovács,"The geometry automated theorem proving area distinguishes itself by a large number of specific methods and implementations, different approaches (synthetic, algebraic, semi-synthetic) and different goals and applications (from research in the area of artificial intelligence to applications in education). Apart from the usual measures of efficiency (e.g. CPU time), the possibility of visual and/or readable proofs is also an expected output against which the geometry automated theorem provers (GATP) should be measured. The implementation of a competition between GATP would allow to create a test bench for GATP developers to improve the existing ones and to propose new ones. It would also allow to establish a ranking for GATP that could be used by ""clients"" (e.g. developers of educational e-learning systems) to choose the best implementation for a given intended use. △ Less","28 February, 2020",https://arxiv.org/pdf/2002.12556
Teaching a Formalized Logical Calculus,Asta Halkjær From;Alexander Birch Jensen;Anders Schlichtkrull;Jørgen Villadsen,"Classical first-order logic is in many ways central to work in mathematics, linguistics, computer science and artificial intelligence, so it is worthwhile to define it in full detail. We present soundness and completeness proofs of a sequent calculus for first-order logic, formalized in the interactive proof assistant Isabelle/HOL. Our formalization is based on work by Stefan Berghofer, which we have since updated to use Isabelle's declarative proof style Isar (Archive of Formal Proofs, Entry FOL-Fitting, August 2007 / July 2018). We represent variables with de Bruijn indices; this makes substitution under quantifiers less intuitive for a human reader. However, the nature of natural numbers yields an elegant solution when compared to implementations of substitution using variables represented by strings. The sequent calculus considered has the special property of an always empty antecedent and a list of formulas in the succedent. We obtain the proofs of soundness and completeness for the sequent calculus as a derived result of the inverse duality of its tableau counterpart. We strive to not only present the results of the proofs of soundness and completeness, but also to provide a deep dive into a programming-like approach to the formalization of first-order logic syntax, semantics and the sequent calculus. We use the formalization in a bachelor course on logic for computer science and discuss our experiences. △ Less","28 February, 2020",https://arxiv.org/pdf/2002.12555
Do ML Experts Discuss Explainability for AI Systems? A discussion case in the industry for a domain-specific solution,Juliana Jansen Ferreira;Mateus de Souza Monteiro,"The application of Artificial Intelligence (AI) tools in different domains are becoming mandatory for all companies wishing to excel in their industries. One major challenge for a successful application of AI is to combine the machine learning (ML) expertise with the domain knowledge to have the best results applying AI tools. Domain specialists have an understanding of the data and how it can impact their decisions. ML experts have the ability to use AI-based tools dealing with large amounts of data and generating insights for domain experts. But without a deep understanding of the data, ML experts are not able to tune their models to get optimal results for a specific domain. Therefore, domain experts are key users for ML tools and the explainability of those AI tools become an essential feature in that context. There are a lot of efforts to research AI explainability for different contexts, users and goals. In this position paper, we discuss interesting findings about how ML experts can express concerns about AI explainability while defining features of an ML tool to be developed for a specific domain. We analyze data from two brainstorm sessions done to discuss the functionalities of an ML tool to support geoscientists (domain experts) on analyzing seismic data (domain-specific data) with ML resources. △ Less","27 February, 2020",https://arxiv.org/pdf/2002.12450
Workload Prediction of Business Processes -- An Approach Based on Process Mining and Recurrent Neural Networks,Fabrizio Albertetti;Hatem Ghorbel,"Recent advances in the interconnectedness and digitization of industrial machines, known as Industry 4.0, pave the way for new analytical techniques. Indeed, the availability and the richness of production-related data enables new data-driven methods. In this paper, we propose a process mining approach augmented with artificial intelligence that (1) reconstructs the historical workload of a company and (2) predicts the workload using neural networks. Our method relies on logs, representing the history of business processes related to manufacturing. These logs are used to quantify the supply and demand and are fed into a recurrent neural network model to predict customer orders. The corresponding activities to fulfill these orders are then sampled from history with a replay mechanism, based on criteria such as trace frequency and activities similarity. An evaluation and illustration of the method is performed on the administrative processes of Heraeus Materials SA. The workload prediction on a one-year test set achieves an MAPE score of 19% for a one-week forecast. The case study suggests a reasonable accuracy and confirms that a good understanding of the historical workload combined to articulated predictions are of great help for supporting management decisions and can decrease costs with better resources planning on a medium-term level. △ Less","14 February, 2020",https://arxiv.org/pdf/2002.11675
Death by AI: Where Assured Autonomy in Smart Cities Meets the End-to-End Argument,Gregory Falco,"A smart city involves critical infrastructure systems that have been digitally enabled. Increasingly, many smart city cyber-physical systems are becoming automated. The extent of automation ranges from basic logic gates to sophisticated, artificial intelligence (AI) that enables fully autonomous systems. Because of modern society's reliance on autonomous systems in smart cities, it is crucial for them to operate in a safe manner; otherwise, it is feasible for these systems to cause considerable physical harm or even death. Because smart cities could involve thousands of autonomous systems operating in concert in densely populated areas, safety assurances are required. Challenges abound to consistently manage the safety of such autonomous systems due to their disparate developers, manufacturers, operators and users. A novel network and a sample of associated network functions for autonomous systems is proposed that aims to provide a baseline of safety for autonomous systems. This is accomplished by establishing a custom-designed network for autonomous systems that is separate from the Internet, and can handle certain functions that enable safety through active networking. Such a network design sits at the margins of the end-to-end principle, which is warranted considering the safety of autonomous systems is at stake as is argued in this paper. Without a scalable safety strategy for autonomous systems as proposed, assured autonomy in smart cities will remain elusive. △ Less","13 February, 2020",https://arxiv.org/pdf/2002.11625
"A machine-learning software-systems approach to capture social, regulatory, governance, and climate problems",Christopher A. Tucker,"This paper will discuss the role of an artificially-intelligent computer system as critique-based, implicit-organizational, and an inherently necessary device, deployed in synchrony with parallel governmental policy, as a genuine means of capturing nation-population complexity in quantitative form, public contentment in societal-cooperative economic groups, regulatory proposition, and governance-effectiveness domains. It will discuss a solution involving a well-known algorithm and proffer an improved mechanism for knowledge-representation, thereby increasing range of utility, scope of influence (in terms of differentiating class sectors) and operational efficiency. It will finish with a discussion of these and other historical implications. △ Less","23 February, 2020",https://arxiv.org/pdf/2002.11485
It's Food Fight! Introducing the Chef's Hat Card Game for Affective-Aware HRI,Pablo Barros;Alessandra Sciutti;Anne C. Bloem;Inge M. Hootsmans;Lena M. Opheij;Romain H. A. Toebosch;Emilia Barakova,"Emotional expressions and their changes during an interaction affect heavily how we perceive and behave towards other persons. To design an HRI scenario that makes possible to observe, understand, and model affective interactions and generate the appropriate responses or initiations of a robot is a very challenging task. In this paper, we report our efforts in designing such a scenario, and to propose a modeling strategy of affective interaction by artificial intelligence deployed in autonomous robots. Overall, we present a novel HRI game scenario that was designed to comply with the specific requirements that will allow us to develop the next wave of affective-aware social robots that provide adequate emotional responses. △ Less","9 March, 2020",https://arxiv.org/pdf/2002.11458
A neural network model of perception and reasoning,Paul J. Blazek;Milo M. Lin,"How perception and reasoning arise from neuronal network activity is poorly understood. This is reflected in the fundamental limitations of connectionist artificial intelligence, typified by deep neural networks trained via gradient-based optimization. Despite success on many tasks, such networks remain unexplainable black boxes incapable of symbolic reasoning and concept generalization. Here we show that a simple set of biologically consistent organizing principles confer these capabilities to neuronal networks. To demonstrate, we implement these principles in a novel machine learning algorithm, based on concept construction instead of optimization, to design deep neural networks that reason with explainable neuron activity. On a range of tasks including NP-hard problems, their reasoning capabilities grant additional cognitive functions, like deliberating through self-analysis, tolerating adversarial attacks, and learning transferable rules from simple examples to solve problems of unencountered complexity. The networks also naturally display properties of biological nervous systems inherently absent in current deep neural networks, including sparsity, modularity, and both distributed and localized firing patterns. Because they do not sacrifice performance, compactness, or training time on standard learning tasks, these networks provide a new black-box-free approach to artificial intelligence. They likewise serve as a quantitative framework to understand the emergence of cognition from neuronal networks. △ Less","26 February, 2020",https://arxiv.org/pdf/2002.11319
Fast Lower and Upper Estimates for the Price of Constrained Multiple Exercise American Options by Single Pass Lookahead Search and Nearest-Neighbor Martingale,Nicolas Essis-Breton;Patrice Gaillardetz,"This article presents fast lower and upper estimates for a large class of options: the class of constrained multiple exercise American options. Typical options in this class are swing options with volume and timing constraints, and passport options with multiple lookback rights. The lower estimate algorithm uses the artificial intelligence method of lookahead search. The upper estimate algorithm uses the dual approach to option pricing on a nearest-neighbor basis for the martingale space. Probabilistic convergence guarantees are provided. Several numerical examples illustrate the approaches including a swing option with four constraints, and a passport option with 16 constraints. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.11258
TanksWorld: A Multi-Agent Environment for AI Safety Research,Corban G. Rivera;Olivia Lyons;Arielle Summitt;Ayman Fatima;Ji Pak;William Shao;Robert Chalmers;Aryeh Englander;Edward W. Staley;I-Jeng Wang;Ashley J. Llorens,"The ability to create artificial intelligence (AI) capable of performing complex tasks is rapidly outpacing our ability to ensure the safe and assured operation of AI-enabled systems. Fortunately, a landscape of AI safety research is emerging in response to this asymmetry and yet there is a long way to go. In particular, recent simulation environments created to illustrate AI safety risks are relatively simple or narrowly-focused on a particular issue. Hence, we see a critical need for AI safety research environments that abstract essential aspects of complex real-world applications. In this work, we introduce the AI safety TanksWorld as an environment for AI safety research with three essential aspects: competing performance objectives, human-machine teaming, and multi-agent competition. The AI safety TanksWorld aims to accelerate the advancement of safe multi-agent decision-making algorithms by providing a software framework to support competitions with both system performance and safety objectives. As a work in progress, this paper introduces our research objectives and learning environment with reference code and baseline performance metrics to follow in a future work. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.11174
Very simple statistical evidence that AlphaGo has exceeded human limits in playing GO game,Okyu Kwon,"Deep learning technology is making great progress in solving the challenging problems of artificial intelligence, hence machine learning based on artificial neural networks is in the spotlight again. In some areas, artificial intelligence based on deep learning is beyond human capabilities. It seemed extremely difficult for a machine to beat a human in a Go game, but AlphaGo has shown to beat a professional player in the game. By looking at the statistical distribution of the distance in which the Go stones are laid in succession, we find a clear trace that Alphago has surpassed human abilities. The AlphaGo than professional players and professional players than ordinary players shows the laying of stones in the distance becomes more frequent. In addition, AlphaGo shows a much more pronounced difference than that of ordinary players and professional players. △ Less","24 February, 2020",https://arxiv.org/pdf/2002.11107
Performance Analysis of Combine Harvester using Hybrid Model of Artificial Neural Networks Particle Swarm Optimization,Laszlo Nadai;Felde Imre;Sina Ardabili;Tarahom Mesri Gundoshmian;Pinter Gergo;Amir Mosavi,"Novel applications of artificial intelligence for tuning the parameters of industrial machines for optimal performance are emerging at a fast pace. Tuning the combine harvesters and improving the machine performance can dramatically minimize the wastes during harvesting, and it is also beneficial to machine maintenance. Literature includes several soft computing, machine learning and optimization methods that had been used to model the function of harvesters of various crops. Due to the complexity of the problem, machine learning methods had been recently proposed to predict the optimal performance with promising results. In this paper, through proposing a novel hybrid machine learning model based on artificial neural networks integrated with particle swarm optimization (ANN-PSO), the performance analysis of a common combine harvester is presented. The hybridization of machine learning methods with soft computing techniques has recently shown promising results to improve the performance of the combine harvesters. This research aims at improving the results further by providing more stable models with higher accuracy. △ Less","22 February, 2020",https://arxiv.org/pdf/2002.11041
Wireless 2.0: Towards an Intelligent Radio Environment Empowered by Reconfigurable Meta-Surfaces and Artificial Intelligence,Haris Gacanin;Marco Di Renzo,"We introduce ""Wireless 2.0"": The future generation of wireless communication networks, where the radio environment becomes controllable, programmable, and intelligent by leveraging the emerging technologies of reconfigurable metasurfaces and artificial intelligence (AI). This paper, in particular, puts the emphasis on AI-based computational methods and commence with an overview of the concept of intelligent radio environments based on reconfigurable meta-surfaces. Later we elaborate on data management aspects, the requirements of supervised learning by examples, and the paradigm of reinforcement learning (RL) to learn by acting. Finally, we highlight numerous open challenges and research directions. △ Less","23 February, 2020",https://arxiv.org/pdf/2002.11040
Distributed Ledger for Provenance Tracking of Artificial Intelligence Assets,Philipp Lüthi;Thibault Gagnaux;Marcel Gygli,"High availability of data is responsible for the current trends in Artificial Intelligence (AI) and Machine Learning (ML). However, high-grade datasets are reluctantly shared between actors because of lacking trust and fear of losing control. Provenance tracing systems are a possible measure to build trust by improving transparency. Especially the tracing of AI assets along complete AI value chains bears various challenges such as trust, privacy, confidentiality, traceability, and fair remuneration. In this paper we design a graph-based provenance model for AI assets and their relations within an AI value chain. Moreover, we propose a protocol to exchange AI assets securely to selected parties. The provenance model and exchange protocol are then combined and implemented as a smart contract on a permission-less blockchain. We show how the smart contract enables the tracing of AI assets in an existing industry use case while solving all challenges. Consequently, our smart contract helps to increase traceability and transparency, encourages trust between actors and thus fosters collaboration between them. △ Less","25 February, 2020",https://arxiv.org/pdf/2002.11000
Exploring BERT Parameter Efficiency on the Stanford Question Answering Dataset v2.0,Eric Hulburd,"In this paper we explore the parameter efficiency of BERT arXiv:1810.04805 on version 2.0 of the Stanford Question Answering dataset (SQuAD2.0). We evaluate the parameter efficiency of BERT while freezing a varying number of final transformer layers as well as including the adapter layers proposed in arXiv:1902.00751. Additionally, we experiment with the use of context-aware convolutional (CACNN) filters, as described in arXiv:1709.08294v3, as a final augmentation layer for the SQuAD2.0 tasks. This exploration is motivated in part by arXiv:1907.10597, which made a compelling case for broadening the evaluation criteria of artificial intelligence models to include various measures of resource efficiency. While we do not evaluate these models based on their floating point operation efficiency as proposed in arXiv:1907.10597, we examine efficiency with respect to training time, inference time, and total number of model parameters. Our results largely corroborate those of arXiv:1902.00751 for adapter modules, while also demonstrating that gains in F1 score from adding context-aware convolutional filters are not practical due to the increase in training and inference time. △ Less","3 March, 2020",https://arxiv.org/pdf/2002.10670
Symbolic Learning and Reasoning with Noisy Data for Probabilistic Anchoring,Pedro Zuidberg Dos Martires;Nitesh Kumar;Andreas Persson;Amy Loutfi;Luc De Raedt,"Robotic agents should be able to learn from sub-symbolic sensor data, and at the same time, be able to reason about objects and communicate with humans on a symbolic level. This raises the question of how to overcome the gap between symbolic and sub-symbolic artificial intelligence. We propose a semantic world modeling approach based on bottom-up object anchoring using an object-centered representation of the world. Perceptual anchoring processes continuous perceptual sensor data and maintains a correspondence to a symbolic representation. We extend the definitions of anchoring to handle multi-modal probability distributions and we couple the resulting symbol anchoring system to a probabilistic logic reasoner for performing inference. Furthermore, we use statistical relational learning to enable the anchoring framework to learn symbolic knowledge in the form of a set of probabilistic logic rules of the world from noisy and sub-symbolic sensor input. The resulting framework, which combines perceptual anchoring and statistical relational learning, is able to maintain a semantic world model of all the objects that have been perceived over time, while still exploiting the expressiveness of logical rules to reason about the state of objects which are not directly observed through sensory input data. To validate our approach we demonstrate, on the one hand, the ability of our system to perform probabilistic reasoning over multi-modal probability distributions, and on the other hand, the learning of probabilistic logical rules from anchored objects produced by perceptual observations. The learned logical rules are, subsequently, used to assess our proposed probabilistic anchoring procedure. We demonstrate our system in a setting involving object interactions where object occlusions arise and where probabilistic inference is needed to correctly anchor objects. △ Less","24 February, 2020",https://arxiv.org/pdf/2002.10373
The Archimedean trap: Why traditional reinforcement learning will probably not yield AGI,Samuel Allen Alexander,"After generalizing the Archimedean property of real numbers in such a way as to make it adaptable to non-numeric structures, we demonstrate that the real numbers cannot be used to accurately measure non-Archimedean structures. We argue that, since an agent with Artificial General Intelligence (AGI) should have no problem engaging in tasks that inherently involve non-Archimedean rewards, and since traditional reinforcement learning rewards are real numbers, therefore traditional reinforcement learning probably will not lead to AGI. We indicate two possible ways traditional reinforcement learning could be altered to remove this roadblock. △ Less","19 October, 2020",https://arxiv.org/pdf/2002.10221
Sparse Optimization for Green Edge AI Inference,Xiangyu Yang;Sheng Hua;Yuanming Shi;Hao Wang;Jun Zhang;Khaled B. Letaief,"With the rapid upsurge of deep learning tasks at the network edge, effective edge artificial intelligence (AI) inference becomes critical to provide low-latency intelligent services for mobile users via leveraging the edge computing capability. In such scenarios, energy efficiency becomes a primary concern. In this paper, we present a joint inference task selection and downlink beamforming strategy to achieve energy-efficient edge AI inference through minimizing the overall power consumption consisting of both computation and transmission power consumption, yielding a mixed combinatorial optimization problem. By exploiting the inherent connections between the set of task selection and group sparsity structural transmit beamforming vector, we reformulate the optimization as a group sparse beamforming problem. To solve this challenging problem, we propose a log-sum function based three-stage approach. By adopting the log-sum function to enhance the group sparsity, a proximal iteratively reweighted algorithm is developed. Furthermore, we establish the global convergence analysis and provide the ergodic worst-case convergence rate for this algorithm. Simulation results will demonstrate the effectiveness of the proposed approach for improving energy efficiency in edge AI inference systems. △ Less","13 March, 2020",https://arxiv.org/pdf/2002.10080
Communication-Efficient Edge AI: Algorithms and Systems,Yuanming Shi;Kai Yang;Tao Jiang;Jun Zhang;Khaled B. Letaief,"Artificial intelligence (AI) has achieved remarkable breakthroughs in a wide range of fields, ranging from speech processing, image classification to drug discovery. This is driven by the explosive growth of data, advances in machine learning (especially deep learning), and easy access to vastly powerful computing resources. Particularly, the wide scale deployment of edge devices (e.g., IoT devices) generates an unprecedented scale of data, which provides the opportunity to derive accurate models and develop various intelligent applications at the network edge. However, such enormous data cannot all be sent from end devices to the cloud for processing, due to the varying channel quality, traffic congestion and/or privacy concerns. By pushing inference and training processes of AI models to edge nodes, edge AI has emerged as a promising alternative. AI at the edge requires close cooperation among edge devices, such as smart phones and smart vehicles, and edge servers at the wireless access points and base stations, which however result in heavy communication overheads. In this paper, we present a comprehensive survey of the recent developments in various techniques for overcoming these communication challenges. Specifically, we first identify key communication challenges in edge AI systems. We then introduce communication-efficient techniques, from both algorithmic and system perspectives for training and inference tasks at the network edge. Potential future research directions are also highlighted. △ Less","22 February, 2020",https://arxiv.org/pdf/2002.09668
MODMA dataset: a Multi-modal Open Dataset for Mental-disorder Analysis,Hanshu Cai;Yiwen Gao;Shuting Sun;Na Li;Fuze Tian;Han Xiao;Jianxiu Li;Zhengwu Yang;Xiaowei Li;Qinglin Zhao;Zhenyu Liu;Zhijun Yao;Minqiang Yang;Hong Peng;Jing Zhu;Xiaowei Zhang;Guoping Gao;Fang Zheng;Rui Li;Zhihua Guo;Rong Ma;Jing Yang;Lan Zhang;Xiping Hu;Yumin Li,"According to the World Health Organization, the number of mental disorder patients, especially depression patients, has grown rapidly and become a leading contributor to the global burden of disease. However, the present common practice of depression diagnosis is based on interviews and clinical scales carried out by doctors, which is not only labor-consuming but also time-consuming. One important reason is due to the lack of physiological indicators for mental disorders. With the rising of tools such as data mining and artificial intelligence, using physiological data to explore new possible physiological indicators of mental disorder and creating new applications for mental disorder diagnosis has become a new research hot topic. However, good quality physiological data for mental disorder patients are hard to acquire. We present a multi-modal open dataset for mental-disorder analysis. The dataset includes EEG and audio data from clinically depressed patients and matching normal controls. All our patients were carefully diagnosed and selected by professional psychiatrists in hospitals. The EEG dataset includes not only data collected using traditional 128-electrodes mounted elastic cap, but also a novel wearable 3-electrode EEG collector for pervasive applications. The 128-electrodes EEG signals of 53 subjects were recorded as both in resting state and under stimulation; the 3-electrode EEG signals of 55 subjects were recorded in resting state; the audio data of 52 subjects were recorded during interviewing, reading, and picture description. We encourage other researchers in the field to use it for testing their methods of mental-disorder analysis. △ Less","4 March, 2020",https://arxiv.org/pdf/2002.09283
"Designing Fair AI for Managing Employees in Organizations: A Review, Critique, and Design Agenda",Lionel P. Robert;Casey Pierce;Liz Morris;Sangmi Kim;Rasha Alahmad,"Organizations are rapidly deploying artificial intelligence (AI) systems to manage their workers. However, AI has been found at times to be unfair to workers. Unfairness toward workers has been associated with decreased worker effort and increased worker turnover. To avoid such problems, AI systems must be designed to support fairness and redress instances of unfairness. Despite the attention related to AI unfairness, there has not been a theoretical and systematic approach to developing a design agenda. This paper addresses the issue in three ways. First, we introduce the organizational justice theory, three different fairness types (distributive, procedural, interactional), and the frameworks for redressing instances of unfairness (retributive justice, restorative justice). Second, we review the design literature that specifically focuses on issues of AI fairness in organizations. Third, we propose a design agenda for AI fairness in organizations that applies each of the fairness types to organizational scenarios. Then, the paper concludes with implications for future research. △ Less","20 February, 2020",https://arxiv.org/pdf/2002.09054
"A Model-Based, Decision-Theoretic Perspective on Automated Cyber Response",Lashon B. Booker;Scott A. Musman,"Cyber-attacks can occur at machine speeds that are far too fast for human-in-the-loop (or sometimes on-the-loop) decision making to be a viable option. Although human inputs are still important, a defensive Artificial Intelligence (AI) system must have considerable autonomy in these circumstances. When the AI system is model-based, its behavior responses can be aligned with risk-aware cost/benefit tradeoffs that are defined by user-supplied preferences that capture the key aspects of how human operators understand the system, the adversary and the mission. This paper describes an approach to automated cyber response that is designed along these lines. We combine a simulation of the system to be defended with an anytime online planner to solve cyber defense problems characterized as partially observable Markov decision problems (POMDPs). △ Less","20 February, 2020",https://arxiv.org/pdf/2002.08957
Do you comply with AI? -- Personalized explanations of learning algorithms and their impact on employees' compliance behavior,NIklas Kuhl;Jodie Lobana;Christian Meske,"Machine Learning algorithms are technological key enablers for artificial intelligence (AI). Due to the inherent complexity, these learning algorithms represent black boxes and are difficult to comprehend, therefore influencing compliance behavior. Hence, compliance with the recommendations of such artifacts, which can impact employees' task performance significantly, is still subject to research - and personalization of AI explanations seems to be a promising concept in this regard. In our work, we hypothesize that, based on varying backgrounds like training, domain knowledge and demographic characteristics, individuals have different understandings and hence mental models about the learning algorithm. Personalization of AI explanations, related to the individuals' mental models, may thus be an instrument to affect compliance and therefore employee task performance. Our preliminary results already indicate the importance of personalized explanations in industry settings and emphasize the importance of this research endeavor. △ Less","20 February, 2020",https://arxiv.org/pdf/2002.08777
NAttack! Adversarial Attacks to bypass a GAN based classifier trained to detect Network intrusion,Aritran Piplai;Sai Sree Laya Chukkapalli;Anupam Joshi,"With the recent developments in artificial intelligence and machine learning, anomalies in network traffic can be detected using machine learning approaches. Before the rise of machine learning, network anomalies which could imply an attack, were detected using well-crafted rules. An attacker who has knowledge in the field of cyber-defence could make educated guesses to sometimes accurately predict which particular features of network traffic data the cyber-defence mechanism is looking at. With this information, the attacker can circumvent a rule-based cyber-defense system. However, after the advancements of machine learning for network anomaly, it is not easy for a human to understand how to bypass a cyber-defence system. Recently, adversarial attacks have become increasingly common to defeat machine learning algorithms. In this paper, we show that even if we build a classifier and train it with adversarial examples for network data, we can use adversarial attacks and successfully break the system. We propose a Generative Adversarial Network(GAN)based algorithm to generate data to train an efficient neural network based classifier, and we subsequently break the system using adversarial attacks. △ Less","19 February, 2020",https://arxiv.org/pdf/2002.08527
AI Online Filters to Real World Image Recognition,Hai Xiao;Jin Shang;Mengyuan Huang,"Deep artificial neural networks, trained with labeled data sets are widely used in numerous vision and robotics applications today. In terms of AI, these are called reflex models, referring to the fact that they do not self-evolve or actively adapt to environmental changes. As demand for intelligent robot control expands to many high level tasks, reinforcement learning and state based models play an increasingly important role. Herein, in computer vision and robotics domain, we study a novel approach to add reinforcement controls onto the image recognition reflex models to attain better overall performance, specifically to a wider environment range beyond what is expected of the task reflex models. Follow a common infrastructure with environment sensing and AI based modeling of self-adaptive agents, we implement multiple types of AI control agents. To the end, we provide comparative results of these agents with baseline, and an insightful analysis of their benefit to improve overall image recognition performance in real world. △ Less","11 February, 2020",https://arxiv.org/pdf/2002.08242
A Structured Approach to Trustworthy Autonomous/Cognitive Systems,Henrik J. Putzer;Ernest Wozniak,"Autonomous systems with cognitive features are on their way into the market. Within complex environments, they promise to implement complex and goal oriented behavior even in a safety related context. This behavior is based on a certain level of situational awareness (perception) and advanced de-cision making (deliberation). These systems in many cases are driven by artificial intelligence (e.g. neural networks). The problem with such complex systems and with using AI technology is that there is no generally accepted approach to ensure trustworthiness. This paper presents a framework to exactly fill this gap. It proposes a reference lifecycle as a structured approach that is based on current safety standards and enhanced to meet the requirements of autonomous/cog-nitive systems and trustworthiness. △ Less","19 February, 2020",https://arxiv.org/pdf/2002.08210
Artificial Intelligent Ethics in the Digital Era: an Engineering Ethical Framework Proposal,Esteban García-Cuesta,"Nowadays technology is being adopted on every aspect of our lives and it is one of most important transformation driver in industry. Moreover, many of the systems and digital services that we use daily rely on artificial intelligent technology capable of modeling social or individual behaviors that in turns also modify personal decisions and actions. In this paper, we briefly discuss, from a technological perspective, a number of critical issues including the purpose of promoting trust and ensure social benefit by the proper use of Artificial Intelligent Systems. To achieve this goal we propose a generic ethical technological framework as a first attempt to define a common context towards developing real engineering ethical by design. We hope that this initial proposal to be useful for early adopters and especially for standardization teams. △ Less","18 February, 2020",https://arxiv.org/pdf/2002.07734
A Modified Perturbed Sampling Method for Local Interpretable Model-agnostic Explanation,Sheng Shi;Xinfeng Zhang;Wei Fan,"Explainability is a gateway between Artificial Intelligence and society as the current popular deep learning models are generally weak in explaining the reasoning process and prediction results. Local Interpretable Model-agnostic Explanation (LIME) is a recent technique that explains the predictions of any classifier faithfully by learning an interpretable model locally around the prediction. However, the sampling operation in the standard implementation of LIME is defective. Perturbed samples are generated from a uniform distribution, ignoring the complicated correlation between features. This paper proposes a novel Modified Perturbed Sampling operation for LIME (MPS-LIME), which is formalized as the clique set construction problem. In image classification, MPS-LIME converts the superpixel image into an undirected graph. Various experiments show that the MPS-LIME explanation of the black-box model achieves much better performance in terms of understandability, fidelity, and efficiency. △ Less","18 February, 2020",https://arxiv.org/pdf/2002.07434
An Overview of Distance and Similarity Functions for Structured Data,Santiago Ontañón,"The notions of distance and similarity play a key role in many machine learning approaches, and artificial intelligence (AI) in general, since they can serve as an organizing principle by which individuals classify objects, form concepts and make generalizations. While distance functions for propositional representations have been thoroughly studied, work on distance functions for structured representations, such as graphs, frames or logical clauses, has been carried out in different communities and is much less understood. Specifically, a significant amount of work that requires the use of a distance or similarity function for structured representations of data usually employs ad-hoc functions for specific applications. Therefore, the goal of this paper is to provide an overview of this work to identify connections between the work carried out in different areas and point out directions for future work. △ Less","18 February, 2020",https://arxiv.org/pdf/2002.07420
Identifying the Development and Application of Artificial Intelligence in Scientific Text,James Dunham;Jennifer Melot;Dewey Murdick,"We describe a strategy for identifying the universe of research publications relevant to the application and development of artificial intelligence. The approach leverages the arXiv corpus of scientific preprints, in which authors choose subject tags for their papers from a set defined by editors. We compose a functional definition of AI relevance by learning these subjects from paper metadata, and then inferring the arXiv-subject labels of papers in larger corpora: Clarivate Web of Science, Digital Science Dimensions, and Microsoft Academic Graph. This yields predictive classification F_1 scores between .75 and .86 for Natural Language Processing (cs.CL), Computer Vision (cs.CV), and Robotics (cs.RO). For a single model that learns these and four other AI-relevant subjects (cs.AI, cs.LG, stat.ML, and cs.MA), we see precision of .83 and recall of .85. We evaluate the out-of-domain performance of our classifiers against other sources of topic information and predictions from alternative methods. We find that a supervised solution can generalize to identify publications that belong to the high-level fields of study represented on arXiv. This offers a method for identifying AI-relevant publications that updates at the pace of research output, without reliance on subject-matter experts for query development or labeling. △ Less","28 May, 2020",https://arxiv.org/pdf/2002.07143
Targeted Forgetting and False Memory Formation in Continual Learners through Adversarial Backdoor Attacks,Muhammad Umer;Glenn Dawson;Robi Polikar,"Artificial neural networks are well-known to be susceptible to catastrophic forgetting when continually learning from sequences of tasks. Various continual (or ""incremental"") learning approaches have been proposed to avoid catastrophic forgetting, but they are typically adversary agnostic, i.e., they do not consider the possibility of a malicious attack. In this effort, we explore the vulnerability of Elastic Weight Consolidation (EWC), a popular continual learning algorithm for avoiding catastrophic forgetting. We show that an intelligent adversary can bypass the EWC's defenses, and instead cause gradual and deliberate forgetting by introducing small amounts of misinformation to the model during training. We demonstrate such an adversary's ability to assume control of the model via injection of ""backdoor"" attack samples on both permuted and split benchmark variants of the MNIST dataset. Importantly, once the model has learned the adversarial misinformation, the adversary can then control the amount of forgetting of any task. Equivalently, the malicious actor can create a ""false memory"" about any task by inserting carefully-designed backdoor samples to any fraction of the test instances of that task. Perhaps most damaging, we show this vulnerability to be very acute; neural network memory can be easily compromised with the addition of backdoor samples into as little as 1% of the training data of even a single task. △ Less","17 February, 2020",https://arxiv.org/pdf/2002.07111
Artificial-Noise-Aided Secure MIMO Wireless Communications via Intelligent Reflecting Surface,Sheng Hong;Cunhua Pan;Hong Ren;Kezhi Wang;Arumugam Nallanathan,"This paper considers a MIMO secure wireless communication system aided by the physical layer security technique of sending artificial noise (AN). To further enhance the system security performance, the advanced intelligent reflecting surface (IRS) is invoked in the AN-aided communication system, where the base station (BS), legitimate information receiver (IR) and eavesdropper (Eve) are equipped with multiple antennas. With the aim for maximizing the secrecy rate (SR), the transmit precoding (TPC) matrix at the BS, covariance matrix of AN and phase shifts at the IRS are jointly optimized subject to constrains of transmit power limit and unit modulus of IRS phase shifts. Then, the secrecy rate maximization (SRM) problem is formulated, which is a non-convex problem with multiple coupled variables. To tackle it, we propose to utilize the block coordinate descent (BCD) algorithm to alternately update the TPC matrix, AN covariance matrix, and phase shifts while keeping SR non-decreasing. Specifically, the optimal TPC matrix and AN covariance matrix are derived by Lagrangian multiplier method, and the optimal phase shifts are obtained by Majorization-Minimization (MM) algorithm. Since all variables can be calculated in closed form, the proposed algorithm is very efficient. We also extend the SRM problem to the more general multiple-IRs scenario and propose a BCD algorithm to solve it. Finally, simulation results validate the effectiveness of system security enhancement via an IRS. △ Less","11 September, 2020",https://arxiv.org/pdf/2002.07063
Bit Allocation for Multi-Task Collaborative Intelligence,Saeed Ranjbar Alvar;Ivan V. Bajić,"Recent studies have shown that collaborative intelligence (CI) is a promising framework for deployment of Artificial Intelligence (AI)-based services on mobile devices. In CI, a deep neural network is split between the mobile device and the cloud. Deep features obtained at the mobile are compressed and transferred to the cloud to complete the inference. So far, the methods in the literature focused on transferring a single deep feature tensor from the mobile to the cloud. Such methods are not applicable to some recent, high-performance networks with multiple branches and skip connections. In this paper, we propose the first bit allocation method for multi-stream, multi-task CI. We first establish a model for the joint distortion of the multiple tasks as a function of the bit rates assigned to different deep feature tensors. Then, using the proposed model, we solve the rate-distortion optimization problem under a total rate constraint to obtain the best rate allocation among the tensors to be transferred. Experimental results illustrate the efficacy of the proposed scheme compared to several alternative bit allocation methods. △ Less","13 February, 2020",https://arxiv.org/pdf/2002.07048
Hybrid Embedded Deep Stacked Sparse Autoencoder with w_LPPD SVM Ensemble,Yongming Li;Yan Lei;Pin Wang;Yuchuan Liu,"Deep learning is a kind of feature learning method with strong nonliear feature transformation and becomes more and more important in many fields of artificial intelligence. Deep autoencoder is one representative method of the deep learning methods, and can effectively extract abstract the information of datasets. However, it does not consider the complementarity between the deep features and original features during deep feature transformation. Besides, it suffers from small sample problem. In order to solve these problems, a novel deep autoencoder - hybrid feature embedded stacked sparse autoencoder(HESSAE) has been proposed in this paper. HFESAE is capable to learn discriminant deep features with the help of embedding original features to filter weak hidden-layer outputs during training. For the issue that class representation ability of abstract information is limited by small sample problem, a feature fusion strategy has been designed aiming to combining abstract information learned by HFESAE with original feature and obtain hybrid features for feature reduction. The strategy is hybrid feature selection strategy based on L1 regularization followed by an support vector machine(SVM) ensemble model, in which weighted local discriminant preservation projection (w_LPPD), is designed and employed on each base classifier. At the end of this paper, several representative public datasets are used to verify the effectiveness of the proposed algorithm. The experimental results demonstrated that, the proposed feature learning method yields superior performance compared to other existing and state of art feature learning algorithms including some representative deep autoencoder methods. △ Less","16 February, 2020",https://arxiv.org/pdf/2002.06761
Designing Interaction for Multi-agent Cooperative System in an Office Environment,Chao Wang;Stephan Hasler;Manuel Muehlig;Frank Joublin;Antonello Ceravola;Joerg Deigmoeller;Lydia Fischer,"Future intelligent system will involve very various types of artificial agents, such as mobile robots, smart home infrastructure or personal devices, which share data and collaborate with each other to execute certain tasks.Designing an efficient human-machine interface, which can support users to express needs to the system, supervise the collaboration progress of different entities and evaluate the result, will be challengeable. This paper presents the design and implementation of the human-machine interface of Intelligent Cyber-Physical system (ICPS),which is a multi-entity coordination system of robots and other smart devices in a working environment. ICPS gathers sensory data from entities and then receives users' command, then optimizes plans to utilize the capability of different entities to serve people. Using multi-model interaction methods, e.g. graphical interfaces, speech interaction, gestures and facial expressions, ICPS is able to receive inputs from users through different entities, keep users aware of the progress and accomplish the task efficiently △ Less","15 February, 2020",https://arxiv.org/pdf/2002.06417
The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,Gary Marcus,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible. △ Less","19 February, 2020",https://arxiv.org/pdf/2002.06177
Benchmarking Knowledge Graphs on the Web,Michael Röder;Mohamed Ahmed Sherif;Muhammad Saleem;Felix Conrads;Axel-Cyrille Ngonga Ngomo,"The growing interest in making use of Knowledge Graphs for developing explainable artificial intelligence, there is an increasing need for a comparable and repeatable comparison of the performance of Knowledge Graph-based systems. History in computer science has shown that a main driver to scientific advances, and in fact a core element of the scientific method as a whole, is the provision of benchmarks to make progress measurable. This paper gives an overview of benchmarks used to evaluate systems that process Knowledge Graphs. △ Less","14 February, 2020",https://arxiv.org/pdf/2002.06039
End-to-end Learning of Object Motion Estimation from Retinal Events for Event-based Object Tracking,Haosheng Chen;David Suter;Qiangqiang Wu;Hanzi Wang,"Event cameras, which are asynchronous bio-inspired vision sensors, have shown great potential in computer vision and artificial intelligence. However, the application of event cameras to object-level motion estimation or tracking is still in its infancy. The main idea behind this work is to propose a novel deep neural network to learn and regress a parametric object-level motion/transform model for event-based object tracking. To achieve this goal, we propose a synchronous Time-Surface with Linear Time Decay (TSLTD) representation, which effectively encodes the spatio-temporal information of asynchronous retinal events into TSLTD frames with clear motion patterns. We feed the sequence of TSLTD frames to a novel Retinal Motion Regression Network (RMRNet) to perform an end-to-end 5-DoF object motion regression. Our method is compared with state-of-the-art object tracking methods, that are based on conventional cameras or event cameras. The experimental results show the superiority of our method in handling various challenging environments such as fast motion and low illumination conditions. △ Less","14 February, 2020",https://arxiv.org/pdf/2002.05911
What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations,Michał Kuźba;Przemysław Biecek,"Recently we see a rising number of methods in the field of eXplainable Artificial Intelligence. To our surprise, their development is driven by model developers rather than a study of needs for human end users. The analysis of needs, if done, takes the form of an A/B test rather than a study of open questions. To answer the question ""What would a human operator like to ask the ML model?"" we propose a conversational system explaining decisions of the predictive model. In this experiment, we developed a chatbot called dr_ant to talk about machine learning model trained to predict survival odds on Titanic. People can talk with dr_ant about different aspects of the model to understand the rationale behind its predictions. Having collected a corpus of 1000+ dialogues, we analyse the most common types of questions that users would like to ask. To our knowledge, it is the first study which uses a conversational system to collect the needs of human operators from the interactive and iterative dialogue explorations of a predictive model. △ Less","31 July, 2020",https://arxiv.org/pdf/2002.05674
Hacia los Comités de Ética en Inteligencia Artificial,Sofía Trejo;Ivan Meza;Fernanda López-Escobedo,"The goal of Artificial Intelligence based systems is to take decisions that have an effect in their environment and impact society. This points out to the necessity of mechanism that regulate the impact of this type of system in society. For this reason, it is priority to create the rules and specialized organizations that can oversight the following of such rules, particularly that human rights precepts at local and international level. This work proposes the creation, at the universities, of Ethical Committees or Commissions specialized on Artificial Intelligence that would be in charge of define the principles and will guarantee the following of good practices in the field Artificial Intelligence. △ Less","11 February, 2020",https://arxiv.org/pdf/2002.05673
AI safety: state of the field through quantitative lens,Mislav Juric;Agneza Sandic;Mario Brcic,"Last decade has seen major improvements in the performance of artificial intelligence which has driven wide-spread applications. Unforeseen effects of such mass-adoption has put the notion of AI safety into the public eye. AI safety is a relatively new field of research focused on techniques for building AI beneficial for humans. While there exist survey papers for the field of AI safety, there is a lack of a quantitative look at the research being conducted. The quantitative aspect gives a data-driven insight about the emerging trends, knowledge gaps and potential areas for future research. In this paper, bibliometric analysis of the literature finds significant increase in research activity since 2015. Also, the field is so new that most of the technical issues are open, including: explainability with its long-term utility, and value alignment which we have identified as the most important long-term research topic. Equally, there is a severe lack of research into concrete policies regarding AI. As we expect AI to be the one of the main driving forces of changes in society, AI safety is the field under which we need to decide the direction of humanity's future. △ Less","9 July, 2020",https://arxiv.org/pdf/2002.05671
Trustworthy AI in the Age of Pervasive Computing and Big Data,Abhishek Kumar;Tristan Braud;Sasu Tarkoma;Pan Hui,"The era of pervasive computing has resulted in countless devices that continuously monitor users and their environment, generating an abundance of user behavioural data. Such data may support improving the quality of service, but may also lead to adverse usages such as surveillance and advertisement. In parallel, Artificial Intelligence (AI) systems are being applied to sensitive fields such as healthcare, justice, or human resources, raising multiple concerns on the trustworthiness of such systems. Trust in AI systems is thus intrinsically linked to ethics, including the ethics of algorithms, the ethics of data, or the ethics of practice. In this paper, we formalise the requirements of trustworthy AI systems through an ethics perspective. We specifically focus on the aspects that can be integrated into the design and development of AI systems. After discussing the state of research and the remaining challenges, we show how a concrete use-case in smart cities can benefit from these methods. △ Less","30 January, 2020",https://arxiv.org/pdf/2002.05657
Near-Optimal Hardware Design for Convolutional Neural Networks,Byungik Ahn,"Recently, the demand of low-power deep-learning hardware for industrial applications has been increasing. Most existing artificial intelligence (AI) chips have evolved to rely on new chip technologies rather than on radically new hardware architectures, to maintain their generality. This study proposes a novel, special-purpose, and high-efficiency hardware architecture for convolutional neural networks. The proposed architecture maximizes the utilization of multipliers by designing the computational circuit with the same structure as that of the computational flow of the model, rather than mapping computations to fixed hardware. In addition, a specially designed filter circuit simultaneously provides all the data of the receptive field, using only one memory read operation during each clock cycle; this allows the computation circuit to operate seamlessly without idle cycles. Our reference system based on the proposed architecture uses 97% of the peak-multiplication capability in actual computations required by the computation model throughout the computation period. In addition, overhead components are minimized so that the proportion of the resources constituting the non-multiplier components is smaller than that constituting the multiplier components, which are indispensable for the computational model. The efficiency of the proposed architecture is close to an ideally efficient system that cannot be improved further in terms of the performance-to-resource ratio. An implementation based on the proposed hardware architecture has been applied in commercial AI products. △ Less","6 February, 2020",https://arxiv.org/pdf/2002.05526
HAN-ECG: An Interpretable Atrial Fibrillation Detection Model Using Hierarchical Attention Networks,Sajad Mousavi;Fatemeh Afghah;U. Rajendra Acharya,"Atrial fibrillation (AF) is one of the most prevalent cardiac arrhythmias that affects the lives of more than 3 million people in the U.S. and over 33 million people around the world and is associated with a five-fold increased risk of stroke and mortality. like other problems in healthcare domain, artificial intelligence (AI)-based algorithms have been used to reliably detect AF from patients' physiological signals. The cardiologist level performance in detecting this arrhythmia is often achieved by deep learning-based methods, however, they suffer from the lack of interpretability. In other words, these approaches are unable to explain the reasons behind their decisions. The lack of interpretability is a common challenge toward a wide application of machine learning-based approaches in the healthcare which limits the trust of clinicians in such methods. To address this challenge, we propose HAN-ECG, an interpretable bidirectional-recurrent-neural-network-based approach for the AF detection task. The HAN-ECG employs three attention mechanism levels to provide a multi-resolution analysis of the patterns in ECG leading to AF. The first level, wave level, computes the wave weights, the second level, heartbeat level, calculates the heartbeat weights, and third level, window (i.e., multiple heartbeats) level, produces the window weights in triggering a class of interest. The detected patterns by this hierarchical attention model facilitate the interpretation of the neural network decision process in identifying the patterns in the signal which contributed the most to the final prediction. Experimental results on two AF databases demonstrate that our proposed model performs significantly better than the existing algorithms. Visualization of these attention layers illustrates that our model decides upon the important waves and heartbeats which are clinically meaningful in the detection task. △ Less","12 February, 2020",https://arxiv.org/pdf/2002.05262
Eigenvector Component Calculation Speedup over NumPy for High-Performance Computing,Shrey Dabhi;Manojkumar Parmar,"Applications related to artificial intelligence, machine learning, and system identification simulations essentially use eigenvectors. Calculating eigenvectors for very large matrices using conventional methods is compute-intensive and renders the applications slow. Recently, Eigenvector-Eigenvalue Identity formula promising significant speedup was identified. We study the algorithmic implementation of the formula against the existing state-of-the-art algorithms and their implementations to evaluate the performance gains. We provide a first of its kind systematic study of the implementation of the formula. We demonstrate further improvements using high-performance computing concepts over native NumPy eigenvector implementation which uses LAPACK and BLAS. △ Less","16 June, 2020",https://arxiv.org/pdf/2002.04989
Predictions of 2019-nCoV Transmission Ending via Comprehensive Methods,Tianyu Zeng;Yunong Zhang;Zhenyu Li;Xiao Liu;Binbin Qiu,"Since the SARS outbreak in 2003, a lot of predictive epidemiological models have been proposed. At the end of 2019, a novel coronavirus, termed as 2019-nCoV, has broken out and is propagating in China and the world. Here we propose a multi-model ordinary differential equation set neural network (MMODEs-NN) and model-free methods to predict the interprovincial transmissions in mainland China, especially those from Hubei Province. Compared with the previously proposed epidemiological models, the proposed network can simulate the transportations with the ODEs activation method, while the model-free methods based on the sigmoid function, Gaussian function, and Poisson distribution are linear and fast to generate reasonable predictions. According to the numerical experiments and the realities, the special policies for controlling the disease are successful in some provinces, and the transmission of the epidemic, whose outbreak time is close to the beginning of China Spring Festival travel rush, is more likely to decelerate before February 18 and to end before April 2020. The proposed mathematical and artificial intelligence methods can give consistent and reasonable predictions of the 2019-nCoV ending. We anticipate our work to be a starting point for comprehensive prediction researches of the 2019-nCoV. △ Less","20 February, 2020",https://arxiv.org/pdf/2002.04945
The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence,Terrence J. Sejnowski,"Deep learning networks have been trained to recognize speech, caption photographs and translate text between languages at high levels of performance. Although applications of deep learning networks to real world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and non-convex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals. △ Less","12 February, 2020",https://arxiv.org/pdf/2002.04806
"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence",Sebastian Raschka;Joshua Patterson;Corey Nolet,"Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical ML and scalable general-purpose GPU computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward. △ Less","31 March, 2020",https://arxiv.org/pdf/2002.04803
Efficient Training of Deep Convolutional Neural Networks by Augmentation in Embedding Space,Mohammad Saeed Abrishami;Amir Erfan Eshratifar;David Eigen;Yanzhi Wang;Shahin Nazarian;Massoud Pedram,"Recent advances in the field of artificial intelligence have been made possible by deep neural networks. In applications where data are scarce, transfer learning and data augmentation techniques are commonly used to improve the generalization of deep learning models. However, fine-tuning a transfer model with data augmentation in the raw input space has a high computational cost to run the full network for every augmented input. This is particularly critical when large models are implemented on embedded devices with limited computational and energy resources. In this work, we propose a method that replaces the augmentation in the raw input space with an approximate one that acts purely in the embedding space. Our experimental results show that the proposed method drastically reduces the computation, while the accuracy of models is negligibly compromised. △ Less","11 February, 2020",https://arxiv.org/pdf/2002.04776
Artificial Intelligence Assistance Significantly Improves Gleason Grading of Prostate Biopsies by Pathologists,Wouter Bulten;Maschenka Balkenhol;Jean-Joël Awoumou Belinga;Américo Brilhante;Aslı Çakır;Xavier Farré;Katerina Geronatsiou;Vincent Molinié;Guilherme Pereira;Paromita Roy;Günter Saile;Paulo Salles;Ewout Schaafsma;Joëlle Tschui;Anne-Marie Vos;Hester van Boven;Robert Vink;Jeroen van der Laak;Christina Hulsbergen-van de Kaa;Geert Litjens,"While the Gleason score is the most important prognostic marker for prostate cancer patients, it suffers from significant observer variability. Artificial Intelligence (AI) systems, based on deep learning, have proven to achieve pathologist-level performance at Gleason grading. However, the performance of such systems can degrade in the presence of artifacts, foreign tissue, or other anomalies. Pathologists integrating their expertise with feedback from an AI system could result in a synergy that outperforms both the individual pathologist and the system. Despite the hype around AI assistance, existing literature on this topic within the pathology domain is limited. We investigated the value of AI assistance for grading prostate biopsies. A panel of fourteen observers graded 160 biopsies with and without AI assistance. Using AI, the agreement of the panel with an expert reference standard significantly increased (quadratically weighted Cohen's kappa, 0.799 vs 0.872; p=0.018). Our results show the added value of AI systems for Gleason grading, but more importantly, show the benefits of pathologist-AI synergy. △ Less","11 February, 2020",https://arxiv.org/pdf/2002.04500
Pairwise Neural Networks (PairNets) with Low Memory for Fast On-Device Applications,Luna M. Zhang,"A traditional artificial neural network (ANN) is normally trained slowly by a gradient descent algorithm, such as the backpropagation algorithm, since a large number of hyperparameters of the ANN need to be fine-tuned with many training epochs. Since a large number of hyperparameters of a deep neural network, such as a convolutional neural network, occupy much memory, a memory-inefficient deep learning model is not ideal for real-time Internet of Things (IoT) applications on various devices, such as mobile phones. Thus, it is necessary to develop fast and memory-efficient Artificial Intelligence of Things (AIoT) systems for real-time on-device applications. We created a novel wide and shallow 4-layer ANN called ""Pairwise Neural Network"" (""PairNet"") with high-speed non-gradient-descent hyperparameter optimization. The PairNet is trained quickly with only one epoch since its hyperparameters are directly optimized one-time via simply solving a system of linear equations by using the multivariate least squares fitting method. In addition, an n-input space is partitioned into many n-input data subspaces, and a local PairNet is built in a local n-input subspace. This divide-and-conquer approach can train the local PairNet using specific local features to improve model performance. Simulation results indicate that the three PairNets with incremental learning have smaller average prediction mean squared errors, and achieve much higher speeds than traditional ANNs. An important future work is to develop better and faster non-gradient-descent hyperparameter optimization algorithms to generate effective, fast, and memory-efficient PairNets with incremental learning on optimal subspaces for real-time AIoT on-device applications. △ Less","9 February, 2020",https://arxiv.org/pdf/2002.04458
Static and Dynamic Values of Computation in MCTS,Eren Sezener;Peter Dayan,"Monte-Carlo Tree Search (MCTS) is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations (i.e., simulations) to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the ""myopic"" limitations of existing computation-value-based methods in two senses: (I) we are able to account for the impact of non-immediate (ie, future) computations (II) on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art. △ Less","19 November, 2020",https://arxiv.org/pdf/2002.04335
Trust dynamics and user attitudes on recommendation errors: preliminary results,David A. Pelta;Jose L. Verdegay;Maria T. Lamata;Carlos Cruz Corona,"Artificial Intelligence based systems may be used as digital nudging techniques that can steer or coerce users to make decisions not always aligned with their true interests. When such systems properly address the issues of Fairness, Accountability, Transparency, and Ethics, then the trust of the user in the system would just depend on the system's output. The aim of this paper is to propose a model for exploring how good and bad recommendations affect the overall trust in an idealized recommender system that issues recommendations over a resource with limited capacity. The impact of different users attitudes on trust dynamics is also considered. Using simulations, we ran a large set of experiments that allowed to observe that: 1) under certain circumstances, all the users ended accepting the recommendations; and 2) the user attitude (controlled by a single parameter balancing the gain/loss of trust after a good/bad recommendation) has a great impact in the trust dynamics. △ Less","11 February, 2020",https://arxiv.org/pdf/2002.04302
"Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy",Ben Shneiderman,"Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe & Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility. △ Less","23 February, 2020",https://arxiv.org/pdf/2002.04087
Advances in Deep Space Exploration via Simulators & Deep Learning,James Bird;Linda Petzold;Philip Lubin;Julia Deacon,"The StarLight program conceptualizes fast interstellar travel via small wafer satellites (wafersats) that are propelled by directed energy. This process is wildly different from traditional space travel and trades large and slow spacecraft for small, fast, inexpensive, and fragile ones. The main goal of these wafer satellites is to gather useful images during their deep space journey. We introduce and solve some of the main problems that accompany this concept. First, we need an object detection system that can detect planets that we have never seen before, some containing features that we may not even know exist in the universe. Second, once we have images of exoplanets, we need a way to take these images and rank them by importance. Equipment fails and data rates are slow, thus we need a method to ensure that the most important images to humankind are the ones that are prioritized for data transfer. Finally, the energy on board is minimal and must be conserved and used sparingly. No exoplanet images should be missed, but using energy erroneously would be detrimental. We introduce simulator-based methods that leverage artificial intelligence, mostly in the form of computer vision, in order to solve all three of these issues. Our results confirm that simulators provide an extremely rich training environment that surpasses that of real images, and can be used to train models on features that have yet to be observed by humans. We also show that the immersive and adaptable environment provided by the simulator, combined with deep learning, lets us navigate and save energy in an otherwise implausible way. △ Less","6 June, 2020",https://arxiv.org/pdf/2002.04051
Improving the Evaluation of Generative Models with Fuzzy Logic,Julian Niedermeier;Gonçalo Mordido;Christoph Meinel,"Objective and interpretable metrics to evaluate current artificial intelligent systems are of great importance, not only to analyze the current state of such systems but also to objectively measure progress in the future. In this work, we focus on the evaluation of image generation tasks. We propose a novel approach, called Fuzzy Topology Impact (FTI), that determines both the quality and diversity of an image set using topology representations combined with fuzzy logic. When compared to current evaluation methods, FTI shows better and more stable performance on multiple experiments evaluating the sensitivity to noise, mode dropping and mode inventing. △ Less","3 February, 2020",https://arxiv.org/pdf/2002.03772
Covering the News with (AI) Style,Michele Merler;Cicero Nogueira dos Santos;Mauro Martino;Alfio M. Gliozzo;John R. Smith,"We introduce a multi-modal discriminative and generative frame-work capable of assisting humans in producing visual content re-lated to a given theme, starting from a collection of documents(textual, visual, or both). This framework can be used by edit or to generate images for articles, as well as books or music album covers. Motivated by a request from the The New York Times (NYT) seeking help to use AI to create art for their special section on Artificial Intelligence, we demonstrated the application of our system in producing such image. △ Less","5 January, 2020",https://arxiv.org/pdf/2002.02369
Machine Learning for Predicting Epileptic Seizures Using EEG Signals: A Review,Khansa Rasheed;Adnan Qayyum;Junaid Qadir;Shobi Sivathamboo;Patrick Kwan;Levin Kuhlmann;Terence O'Brien;Adeel Razi,"With the advancement in artificial intelligence (AI) and machine learning (ML) techniques, researchers are striving towards employing these techniques for advancing clinical practice. One of the key objectives in healthcare is the early detection and prediction of disease to timely provide preventive interventions. This is especially the case for epilepsy, which is characterized by recurrent and unpredictable seizures. Patients can be relieved from the adverse consequences of epileptic seizures if it could somehow be predicted in advance. Despite decades of research, seizure prediction remains an unsolved problem. This is likely to remain at least partly because of the inadequate amount of data to resolve the problem. There have been exciting new developments in ML-based algorithms that have the potential to deliver a paradigm shift in the early and accurate prediction of epileptic seizures. Here we provide a comprehensive review of state-of-the-art ML techniques in early prediction of seizures using EEG signals. We will identify the gaps, challenges, and pitfalls in the current research and recommend future directions. △ Less","4 February, 2020",https://arxiv.org/pdf/2002.01925
Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework,Hongyu Li;Dan Meng;Hong Wang;Xiaolin Li,"With strict protections and regulations of data privacy and security, conventional machine learning based on centralized datasets is confronted with significant challenges, making artificial intelligence (AI) impractical in many mission-critical and data-sensitive scenarios, such as finance, government, and health. In the meantime, tremendous datasets are scattered in isolated silos in various industries, organizations, different units of an organization, or different branches of an international organization. These valuable data resources are well underused. To advance AI theories and applications, we propose a comprehensive framework (called Knowledge Federation - KF) to address these challenges by enabling AI while preserving data privacy and ownership. Beyond the concepts of federated learning and secure multi-party computation, KF consists of four levels of federation: (1) information level, low-level statistics and computation of data, meeting the requirements of simple queries, searching and simplistic operators; (2) model level, supporting training, learning, and inference; (3) cognition level, enabling abstract feature representation at various levels of abstractions and contexts; (4) knowledge level, fusing knowledge discovery, representation, and reasoning. We further clarify the relationship and differentiation between knowledge federation and other related research areas. We have developed a reference implementation of KF, called iBond Platform, to offer a production-quality KF platform to enable industrial applications in finance, insurance et al. The iBond platform will also help establish the KF community and a comprehensive ecosystem and usher in a novel paradigm shift towards secure, privacy-preserving and responsible AI. As far as we know, knowledge federation is the first hierarchical and unified framework for secure multi-party computing and learning. △ Less","22 May, 2020",https://arxiv.org/pdf/2002.01647
Transparency and Trust in Human-AI-Interaction: The Role of Model-Agnostic Explanations in Computer Vision-Based Decision Support,Christian Meske;Enrico Bunde,"Computer Vision, and hence Artificial Intelligence-based extraction of information from images, has increasingly received attention over the last years, for instance in medical diagnostics. While the algorithms' complexity is a reason for their increased performance, it also leads to the ""black box"" problem, consequently decreasing trust towards AI. In this regard, ""Explainable Artificial Intelligence"" (XAI) allows to open that black box and to improve the degree of AI transparency. In this paper, we first discuss the theoretical impact of explainability on trust towards AI, followed by showcasing how the usage of XAI in a health-related setting can look like. More specifically, we show how XAI can be applied to understand why Computer Vision, based on deep learning, did or did not detect a disease (malaria) on image data (thin blood smear slide images). Furthermore, we investigate, how XAI can be used to compare the detection strategy of two different deep learning models often used for Computer Vision: Convolutional Neural Network and Multi-Layer Perceptron. Our empirical results show that i) the AI sometimes used questionable or irrelevant data features of an image to detect malaria (even if correctly predicted), and ii) that there may be significant discrepancies in how different deep learning models explain the same prediction. Our theoretical discussion highlights that XAI can support trust in Computer Vision systems, and AI systems in general, especially through an increased understandability and predictability. △ Less","12 July, 2020",https://arxiv.org/pdf/2002.01543
Linear and Fisher Separability of Random Points in the d-dimensional Spherical Layer,Sergey Sidorov;Nikolai Zolotykh,"Stochastic separation theorems play important role in high-dimensional data analysis and machine learning. It turns out that in high dimension any point of a random set of points can be separated from other points by a hyperplane with high probability even if the number of points is exponential in terms of dimension. This and similar facts can be used for constructing correctors for artificial intelligent systems, for determining an intrinsic dimension of data and for explaining various natural intelligence phenomena. In this paper, we refine the estimations for the number of points and for the probability in stochastic separation theorems, thereby strengthening some results obtained earlier. We propose the boundaries for linear and Fisher separability, when the points are drawn randomly, independently and uniformly from a d-dimensional spherical layer. These results allow us to better outline the applicability limits of the stochastic separation theorems in applications. △ Less","18 April, 2020",https://arxiv.org/pdf/2002.01306
Adversarial Attacks to Scale-Free Networks: Testing the Robustness of Physical Criteria,Qi Xuan;Yalu Shan;Jinhuan Wang;Zhongyuan Ruan;Guanrong Chen,"Adversarial attacks have been alerting the artificial intelligence community recently, since many machine learning algorithms were found vulnerable to malicious attacks. This paper studies adversarial attacks to scale-free networks to test their robustness in terms of statistical measures. In addition to the well-known random link rewiring (RLR) attack, two heuristic attacks are formulated and simulated: degree-addition-based link rewiring (DALR) and degree-interval-based link rewiring (DILR). These three strategies are applied to attack a number of strong scale-free networks of various sizes generated from the Barabási-Albert model. It is found that both DALR and DILR are more effective than RLR, in the sense that rewiring a smaller number of links can succeed in the same attack. However, DILR is as concealed as RLR in the sense that they both are constructed by introducing a relatively small number of changes on several typical structural properties such as average shortest path-length, average clustering coefficient, and average diagonal distance. The results of this paper suggest that to classify a network to be scale-free has to be very careful from the viewpoint of adversarial attack effects. △ Less","4 February, 2020",https://arxiv.org/pdf/2002.01249
Four Principles of Explainable AI as Applied to Biometrics and Facial Forensic Algorithms,P. Jonathon Phillips;Mark Przybocki,"Traditionally, researchers in automatic face recognition and biometric technologies have focused on developing accurate algorithms. With this technology being integrated into operational systems, engineers and scientists are being asked, do these systems meet societal norms? The origin of this line of inquiry is `trust' of artificial intelligence (AI) systems. In this paper, we concentrate on adapting explainable AI to face recognition and biometrics, and we present four principles of explainable AI to face recognition and biometrics. The principles are illustrated by \it{four} case studies, which show the challenges and issues in developing algorithms that can produce explanations. △ Less","3 February, 2020",https://arxiv.org/pdf/2002.01014
Driver Identification by Neural Network on Extracted Statistical Features from Smartphone Data,Ruhallah Ahmadian;Mehdi Ghatee,"The future of transportation is driven by the use of artificial intelligence to improve living and transportation. This paper presents a neural network-based system for driver identification using data collected by a smartphone. This system identifies the driver automatically, reliably and in real-time without the need for facial recognition and also does not violate privacy. The system architecture consists of three modules data collection, preprocessing and identification. In the data collection module, the data of the accelerometer and gyroscope sensors are collected using a smartphone. The preprocessing module includes noise removal, data cleaning, and segmentation. In this module, lost values will be retrieved and data of stopped vehicle will be deleted. Finally, effective statistical properties are extracted from data-windows. In the identification module, machine learning algorithms are used to identify drivers' patterns. According to experiments, the best algorithm for driver identification is MLP with a maximum accuracy of 96%. This solution can be used in future transportation to develop driver-based insurance systems as well as the development of systems used to apply penalties and incentives. △ Less","4 February, 2020",https://arxiv.org/pdf/2002.00764
Mixing Patterns in Interdisciplinary Collaboration Networks: Assessing Interdisciplinarity Through Multiple Lenses,Shihui Feng;Alec Kirkley,"There are inherent challenges to interdisciplinary research collaboration, such as bridging cognitive gaps and balancing transaction costs with collaborative benefits. This raises the question: Does interdisciplinary research necessarily result in interdisciplinary collaborations? This study aims to explore this question and assess collaboration preferences in interdisciplinary research at the individual, dyadic, and team level by examining mixing patterns in a collaboration network. Using a network of over 2,000 researchers from the field of artificial intelligence in education, we find that ""interdisciplinarity"" is demonstrated by diverse research experiences of individual researchers rather than diversity among researchers within collaborations. We also examine intergroup mixing by applying a novel approach to classify the active and non-active researchers in the collaboration network based on participation in multiple teams. We find a significant difference in indicators of academic performance and experience between the clusters of active and non-active researchers, suggesting intergroup mixing as a key factor in academic success. Our results shed light on the nature of team formation in interdisciplinary research, as well as highlight the importance of interdisciplinary programs. △ Less","2 February, 2020",https://arxiv.org/pdf/2002.00531
A Machine Consciousness architecture based on Deep Learning and Gaussian Processes,Eduardo C. Garrido Merchán;Martín Molina,"Recent developments in machine learning have pushed the tasks that machines can do outside the boundaries of what was thought to be possible years ago. Methodologies such as deep learning or generative models have achieved complex tasks such as generating art pictures or literature automatically. On the other hand, symbolic resources have also been developed further and behave well in problems such as the ones proposed by common sense reasoning. Machine Consciousness is a field that has been deeply studied and several theories based in the functionalism philosophical theory like the global workspace theory or information integration have been proposed that try to explain the ariseness of consciousness in machines. In this work, we propose an architecture that may arise consciousness in a machine based in the global workspace theory and in the assumption that consciousness appear in machines that has cognitive processes and exhibit conscious behaviour. This architecture is based in processes that use the recent developments in artificial intelligence models which output are these correlated activities. For every one of the modules of this architecture, we provide detailed explanations of the models involved and how they communicate with each other to create the cognitive architecture. △ Less","13 March, 2020",https://arxiv.org/pdf/2002.00509
Parallel convolution processing using an integrated photonic tensor core,Johannes Feldmann;Nathan Youngblood;Maxim Karpov;Helge Gehring;Xuan Li;Maik Stappers;Manuel Le Gallo;Xin Fu;Anton Lukashchuk;Arslan Raja;Junqiu Liu;David Wright;Abu Sebastian;Tobias Kippenberg;Wolfram Pernice;Harish Bhaskaran,"With the proliferation of ultra-high-speed mobile networks and internet-connected devices, along with the rise of artificial intelligence, the world is generating exponentially increasing amounts of data - data that needs to be processed in a fast, efficient and smart way. These developments are pushing the limits of existing computing paradigms, and highly parallelized, fast and scalable hardware concepts are becoming progressively more important. Here, we demonstrate a computational specific integrated photonic tensor core - the optical analog of an ASIC-capable of operating at Tera-Multiply-Accumulate per second (TMAC/s) speeds. The photonic core achieves parallelized photonic in-memory computing using phase-change memory arrays and photonic chip-based optical frequency combs (soliton microcombs). The computation is reduced to measuring the optical transmission of reconfigurable and non-resonant passive components and can operate at a bandwidth exceeding 14 GHz, limited only by the speed of the modulators and photodetectors. Given recent advances in hybrid integration of soliton microcombs at microwave line rates, ultra-low loss silicon nitride waveguides, and high speed on-chip detectors and modulators, our approach provides a path towards full CMOS wafer-scale integration of the photonic tensor core. While we focus on convolution processing, more generally our results indicate the major potential of integrated photonics for parallel, fast, and efficient computational hardware in demanding AI applications such as autonomous driving, live video processing, and next generation cloud computing services. △ Less","12 October, 2020",https://arxiv.org/pdf/2002.00281
Machine Ethics: The Creation of a Virtuous Machine,Mohamed Akrout;Robert Steinbauer,"Artificial intelligence (AI) was initially developed as an implicit moral agent to solve simple and clearly defined tasks where all options are predictable. However, it is now part of our daily life powering cell phones, cameras, watches, thermostats, vacuums, cars, and much more. This has raised numerous concerns and some scholars and practitioners stress the dangers of AI and argue against its development as moral agents that can reason about ethics (e.g., Bryson 2008; Johnson and Miller 2008; Sharkey 2017; Tonkens 2009; van Wynsberghe and Robbins 2019). Even though we acknowledge the potential threat, in line with most other scholars (e.g., Anderson and Anderson 2010; Moor 2006; Scheutz 2016; Wallach 2010), we argue that AI advancements cannot be stopped and developers need to prepare AI to sustain explicit moral agents and face ethical dilemmas in complex and morally salient environments. △ Less","7 February, 2020",https://arxiv.org/pdf/2002.00213
Predicting IoT Service Adoption towards Smart Mobility in Malaysia: SEM-Neural Hybrid Pilot Study,Waqas Ahmed;Sheikh Muhamad Hizam;Ilham Sentosa;Habiba Akter;Eiad Yafi;Jawad Ali,"Smart city is synchronized with digital environment and its transportation system is vitalized with RFID sensors, Internet of Things (IoT) and Artificial Intelligence. However, without user's behavioral assessment of technology, the ultimate usefulness of smart mobility cannot be achieved. This paper aims to formulate the research framework for prediction of antecedents of smart mobility by using SEM-Neural hybrid approach towards preliminary data analysis. This research undertook smart mobility services adoption in Malaysia as study perspective and applied the Technology Acceptance Model (TAM) as theoretical basis. An extended TAM model was hypothesized with five external factors (digital dexterity, IoT service quality, intrusiveness concerns, social electronic word of mouth and subjective norm). The data was collected through a pilot survey in Klang Valley, Malaysia. Then responses were analyzed for reliability, validity and accuracy of model. Finally, the causal relationship was explained by Structural Equation Modeling (SEM) and Artificial Neural Networking (ANN). The paper will share better understanding of road technology acceptance to all stakeholders to refine, revise and update their policies. The proposed framework will suggest a broader approach to individual level technology acceptance. △ Less","14 March, 2020",https://arxiv.org/pdf/2002.00152
Artificial Intelligence Aided Next-Generation Networks Relying on UAVs,Xiao Liu;Mingzhe Chen;Yuanwei Liu;Yue Chen;Shuguang Cui;Lajos Hanzo,"Artificial intelligence (AI) assisted unmanned aerial vehicle (UAV) aided next-generation networking is proposed for dynamic environments. In the AI-enabled UAV-aided wireless networks (UAWN), multiple UAVs are employed as aerial base stations, which are capable of rapidly adapting to the dynamic environment by collecting information about the users' position and tele-traffic demands, learning from the environment and acting upon the feedback received from the users. Moreover, AI enables the interaction amongst a swarm of UAVs for cooperative optimization of the system. As a benefit of the AI framework, several challenges of conventional UAWN may be circumvented, leading to enhanced network performance, improved reliability and agile adaptivity. As a further benefit, dynamic trajectory design and resource allocation are demonstrated. Finally, potential research challenges and opportunities are discussed. △ Less","28 January, 2020",https://arxiv.org/pdf/2001.11958
"Adversarial vs behavioural-based defensive AI with joint, continual and active learning: automated evaluation of robustness to deception, poisoning and concept drift",Alexandre Dey;Marc Velay;Jean-Philippe Fauvelle;Sylvain Navers,"Recent advancements in Artificial Intelligence (AI) have brought new capabilities to behavioural analysis (UEBA) for cyber-security consisting in the detection of hostile action based on the unusual nature of events observed on the Information System.In our previous work (presented at C\&ESAR 2018 and FIC 2019), we have associated deep neural networks auto-encoders for anomaly detection and graph-based events correlation to address major limitations in UEBA systems. This resulted in reduced false positive and false negative rates, improved alert explainability, while maintaining real-time performances and scalability. However, we did not address the natural evolution of behaviours through time, also known as concept drift. To maintain effective detection capabilities, an anomaly-based detection system must be continually trained, which opens a door to an adversary that can conduct the so-called ""frog-boiling"" attack by progressively distilling unnoticed attack traces inside the behavioural models until the complete attack is considered normal. In this paper, we present a solution to effectively mitigate this attack by improving the detection process and efficiently leveraging human expertise. We also present preliminary work on adversarial AI conducting deception attack, which, in term, will be used to help assess and improve the defense system. These defensive and offensive AI implement joint, continual and active learning, in a step that is necessary in assessing, validating and certifying AI-based defensive solutions. △ Less","13 January, 2020",https://arxiv.org/pdf/2001.11821
Mixed-precision deep learning based on computational memory,S. R. Nandakumar;Manuel Le Gallo;Christophe Piveteau;Vinay Joshi;Giovanni Mariani;Irem Boybat;Geethan Karunaratne;Riduan Khaddam-Aljameh;Urs Egger;Anastasios Petropoulos;Theodore Antonakopoulos;Bipin Rajendran;Abu Sebastian;Evangelos Eleftheriou,"Deep neural networks (DNNs) have revolutionized the field of artificial intelligence and have achieved unprecedented success in cognitive tasks such as image and speech recognition. Training of large DNNs, however, is computationally intensive and this has motivated the search for novel computing architectures targeting this application. A computational memory unit with nanoscale resistive memory devices organized in crossbar arrays could store the synaptic weights in their conductance states and perform the expensive weighted summations in place in a non-von Neumann manner. However, updating the conductance states in a reliable manner during the weight update process is a fundamental challenge that limits the training accuracy of such an implementation. Here, we propose a mixed-precision architecture that combines a computational memory unit performing the weighted summations and imprecise conductance updates with a digital processing unit that accumulates the weight updates in high precision. A combined hardware/software training experiment of a multilayer perceptron based on the proposed architecture using a phase-change memory (PCM) array achieves 97.73% test accuracy on the task of classifying handwritten digits (based on the MNIST dataset), within 0.6% of the software baseline. The architecture is further evaluated using accurate behavioral models of PCM on a wide class of networks, namely convolutional neural networks, long-short-term-memory networks, and generative-adversarial networks. Accuracies comparable to those of floating-point implementations are achieved without being constrained by the non-idealities associated with the PCM devices. A system-level study demonstrates 173x improvement in energy efficiency of the architecture when used for training a multilayer perceptron compared with a dedicated fully digital 32-bit implementation. △ Less","31 January, 2020",https://arxiv.org/pdf/2001.11773
Learning of signaling networks: molecular mechanisms,Péter Csermely;Nina Kunsic;Péter Mendik;Márk Kerestély;Teodóra Faragó;Dániel V. Veres;Péter Tompa,"Molecular processes of neuronal learning have been well-described. However, learning mechanisms of non-neuronal cells have not been fully understood at the molecular level. Here, we discuss molecular mechanisms of cellular learning, including conformational memory of intrinsically disordered proteins and prions, signaling cascades, protein translocation, RNAs (microRNA and lncRNA), and chromatin memory. We hypothesize that these processes constitute the learning of signaling networks and correspond to a generalized Hebbian learning process of single, non-neuronal cells, and discuss how cellular learning may open novel directions in drug design and inspire new artificial intelligence methods. △ Less","17 March, 2020",https://arxiv.org/pdf/2001.11679
Scalable Psychological Momentum Forecasting in Esports,Alfonso White;Daniela M. Romano,"The world of competitive Esports and video gaming has seen and continues to experience steady growth in popularity and complexity. Correspondingly, more research on the topic is being published, ranging from social network analyses to the benchmarking of advanced artificial intelligence systems in playing against humans. In this paper, we present ongoing work on an intelligent agent recommendation engine that suggests actions to players in order to maximise success and enjoyment, both in the space of in-game choices, as well as decisions made around play session timing in the broader context. By leveraging temporal data and appropriate models, we show that a learned representation of player psychological momentum, and of tilt, can be used, in combination with player expertise, to achieve state-of-the-art performance in pre- and post-draft win prediction. Our progress toward fulfilling the potential for deriving optimal recommendations is documented. △ Less","14 February, 2020",https://arxiv.org/pdf/2001.11274
An Automated Framework for the Extraction of Semantic Legal Metadata from Legal Texts,Amin Sleimi;Nicolas Sannier;Mehrdad Sabetzadeh;Lionel Briand;Marcello Ceci;John Dann,"Semantic legal metadata provides information that helps with understanding and interpreting legal provisions. Such metadata is therefore important for the systematic analysis of legal requirements. However, manually enhancing a large legal corpus with semantic metadata is prohibitively expensive. Our work is motivated by two observations: (1) the existing requirements engineering (RE) literature does not provide a harmonized view on the semantic metadata types that are useful for legal requirements analysis; (2) automated support for the extraction of semantic legal metadata is scarce, and it does not exploit the full potential of artificial intelligence technologies, notably natural language processing (NLP) and machine learning (ML). Our objective is to take steps toward overcoming these limitations. To do so, we review and reconcile the semantic legal metadata types proposed in the RE literature. Subsequently, we devise an automated extraction approach for the identified metadata types using NLP and ML. We evaluate our approach through two case studies over the Luxembourgish legislation. Our results indicate a high accuracy in the generation of metadata annotations. In particular, in the two case studies, we were able to obtain precision scores of 97.2% and 82.4% and recall scores of 94.9% and 92.4%. △ Less","30 January, 2020",https://arxiv.org/pdf/2001.11245
Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles,Szilárd Aradi,"Academic research in the field of autonomous vehicles has reached high popularity in recent years related to several topics as sensor technologies, V2X communications, safety, security, decision making, control, and even legal and standardization rules. Besides classic control design approaches, Artificial Intelligence and Machine Learning methods are present in almost all of these fields. Another part of research focuses on different layers of Motion Planning, such as strategic decisions, trajectory planning, and control. A wide range of techniques in Machine Learning itself have been developed, and this article describes one of these fields, Deep Reinforcement Learning (DRL). The paper provides insight into the hierarchical motion planning problem and describes the basics of DRL. The main elements of designing such a system are the modeling of the environment, the modeling abstractions, the description of the state and the perception models, the appropriate rewarding, and the realization of the underlying neural network. The paper describes vehicle models, simulation possibilities and computational requirements. Strategic decisions on different layers and the observation models, e.g., continuous and discrete state representations, grid-based, and camera-based solutions are presented. The paper surveys the state-of-art solutions systematized by the different tasks and levels of autonomous driving, such as car-following, lane-keeping, trajectory following, merging, or driving in dense traffic. Finally, open questions and future challenges are discussed. △ Less","30 January, 2020",https://arxiv.org/pdf/2001.11231
On the Convergence of Artificial Intelligence and Distributed Ledger Technology: A Scoping Review and Future Research Agenda,Konstantin D. Pandl;Scott Thiebes;Manuel Schmidt-Kraepelin;Ali Sunyaev,"Developments in Artificial Intelligence (AI) and Distributed Ledger Technology (DLT) currently lead to lively debates in academia and practice. AI processes data to perform tasks that were previously thought possible only for humans. DLT has the potential to create consensus over data among a group of participants in uncertain environments. In recent research, both technologies are used in similar and even the same systems. Examples include the design of secure distributed ledgers or the creation of allied learning systems distributed across multiple nodes. This can lead to technological convergence, which in the past, has paved the way for major innovations in information technology. Previous work highlights several potential benefits of the convergence of AI and DLT but only provides a limited theoretical framework to describe upcoming real-world integration cases of both technologies. We aim to contribute by conducting a systematic literature review on previous work and providing rigorously derived future research opportunities. This work helps researchers active in AI or DLT to overcome current limitations in their field, and practitioners to develop systems along with the convergence of both technologies. △ Less","5 February, 2020",https://arxiv.org/pdf/2001.11017
Data integration and prediction models of photovoltaic production from Brazilian northeastern,Hugo Abreu Mendes;Henrique Ferreira Nunes;Manoel da Nobrega Marinho;Paulo Salgado Gomes de Mattos Neto,"All productive branches of society need an estimate to be able to control their expenses well. In the energy business, electric utilities use this information to control the power flow in the grid. For better energy production estimation of photovoltaic systems, it is necessary to join multiples geospatial and meteorological variables. This work proposes the creation of a satellite data integration platform, with production estimation models, base stations measurement and actual production capacity. This work presents statistical, probabilistic and artificial intelligence models that generate spatial and temporal production estimates that could improve production gains as well as facilitate the monitoring and supervision of new enterprises are presented. △ Less","6 March, 2020",https://arxiv.org/pdf/2001.10866
Explainable Machine Learning Control -- robust control and stability analysis,Markus Quade;Thomas Isele;Markus Abel,"Recently, the term explainable AI became known as an approach to produce models from artificial intelligence which allow interpretation. Since a long time, there are models of symbolic regression in use that are perfectly explainable and mathematically tractable: in this contribution we demonstrate how to use symbolic regression methods to infer the optimal control of a dynamical system given one or several optimization criteria, or cost functions. In previous publications, network control was achieved by automatized machine learning control using genetic programming. Here, we focus on the subsequent analysis of the analytical expressions which result from the machine learning. In particular, we use AUTO to analyze the stability properties of the controlled oscillator system which served as our model. As a result, we show that there is a considerable advantage of explainable models over less accessible neural networks. △ Less","23 January, 2020",https://arxiv.org/pdf/2001.10056
Algorithmic Fairness,Dana Pessach;Erez Shmueli,"An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairness-related datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness. △ Less","21 January, 2020",https://arxiv.org/pdf/2001.09784
Artificial intelligence in medicine and healthcare: a review and classification of current and near-future applications and their ethical and social Impact,Emilio Gómez-González;Emilia Gomez;Javier Márquez-Rivas;Manuel Guerrero-Claro;Isabel Fernández-Lizaranzu;María Isabel Relimpio-López;Manuel E. Dorado;María José Mayorga-Buiza;Guillermo Izquierdo-Ayuso;Luis Capitán-Morales,"This paper provides an overview of the current and near-future applications of Artificial Intelligence (AI) in Medicine and Health Care and presents a classification according to their ethical and societal aspects, potential benefits and pitfalls, and issues that can be considered controversial and are not deeply discussed in the literature. This work is based on an analysis of the state of the art of research and technology, including existing software, personal monitoring devices, genetic tests and editing tools, personalized digital models, online platforms, augmented reality devices, and surgical and companion robotics. Motivated by our review, we present and describe the notion of 'extended personalized medicine', we then review existing applications of AI in medicine and healthcare and explore the public perception of medical AI systems, and how they show, simultaneously, extraordinary opportunities and drawbacks that even question fundamental medical concepts. Many of these topics coincide with urgent priorities recently defined by the World Health Organization for the coming decade. In addition, we study the transformations of the roles of doctors and patients in an age of ubiquitous information, identify the risk of a division of Medicine into 'fake-based', 'patient-generated', and 'scientifically tailored', and draw the attention of some aspects that need further thorough analysis and public debate. △ Less","6 February, 2020",https://arxiv.org/pdf/2001.09778
Towards organizational guidelines for the responsible use of AI,Richard Benjamins,"In the past few years, several large companies have published ethical principles of Artificial Intelligence (AI). National governments, the European Commission, and inter-governmental organizations have come up with requirements to ensure the good use of AI. However, individual organizations that want to join this effort, are faced with many unsolved questions. This paper proposes guidelines for organizations committed to the responsible use of AI, but lack the required knowledge and experience. The guidelines consist of two parts: i) helping organizations to decide what principles to adopt, and ii) a methodology for implementing the principles in organizational processes. In case of future AI regulation, organizations following this approach will be well-prepared. △ Less","5 May, 2020",https://arxiv.org/pdf/2001.09758
Towards a framework for understanding societal and ethical implications of Artificial Intelligence,Richard Benjamins;Idoia Salazar,"Artificial Intelligence (AI) is one of the most discussed technologies today. There are many innovative applications such as the diagnosis and treatment of cancer, customer experience, new business, education, contagious diseases propagation and optimization of the management of humanitarian catastrophes. However, with all those opportunities also comes great responsibility to ensure good and fair practice of AI. The objective of this paper is to identify the main societal and ethical challenges implied by a massive uptake of AI. We have surveyed the literature for the most common challenges and classified them in seven groups: 1) Non-desired effects, 2) Liability, 3) Unknown consequences, 4) Relation people-robots, 5) Concentration of power and wealth, 6) Intentional bad uses, and 7) AI for weapons and warfare. The challenges should be dealt with in different ways depending on their origin; some have technological solutions, while others require ethical, societal, or political answers. Depending on the origin, different stakeholders might need to act. Whatever the identified stakeholder, not treating those issues will lead to uncertainty and unforeseen consequences with potentially large negative societal impact, hurting especially the most vulnerable groups of societies. Technology is helping to take better decisions, and AI is promoting data-driven decisions in addition to experience- and intuition-based discussion, with many improvements happening. However, the negative side effects of this technology need to be well understood and acted upon before we launch them massively into the world. △ Less","3 January, 2020",https://arxiv.org/pdf/2001.09750
SensAI+Expanse Emotional Valence Prediction Studies with Cognition and Memory Integration,Nuno A. C. Henriques;Helder Coelho;Leonel Garcia-Marques,"The humans are affective and cognitive beings relying on memories for their individual and social identities. Also, human dyadic bonds require some common beliefs such as empathetic behaviour for better interaction. In this sense, research studies involving human-agent interaction should resource on affect, cognition, and memory integration. The developed artificial agent system (SensAI+Expanse) includes machine learning algorithms, heuristics, and memory as cognition aids towards emotional valence prediction on the interacting human. Further, an adaptive empathy score is always present in order to engage the human in a recognisable interaction outcome. [...] The agent is resilient on collecting data, adapts its cognitive processes to each human individual in a learning best effort for proper contextualised prediction. The current study make use of an achieved adaptive process. Also, the use of individual prediction models with specific options of the learning algorithm and evaluation metric from a previous research study. The accomplished solution includes a highly performant prediction ability, an efficient energy use, and feature importance explanation for predicted probabilities. Results of the present study show evidence of significant emotional valence behaviour differences between some age ranges and gender combinations. Therefore, this work contributes with an artificial intelligent agent able to assist on cognitive science studies. This ability is about affective disturbances by means of predicting human emotional valence contextualised in space and time. Moreover, contributes with learning processes and heuristics fit to the task including economy of cognition and memory to cope with the environment. Finally, these contributions include an achieved age and gender neutrality on predicting emotional valence states in context and with very good performance for each individual. △ Less","10 March, 2020",https://arxiv.org/pdf/2001.09746
Can an Algorithm be My Healthcare Proxy?,Duncan C McElfresh;Samuel Dooley;Yuan Cui;Kendra Griesman;Weiqin Wang;Tyler Will;Neil Sehgal;John P Dickerson,"Planning for death is not a process in which everyone participates. Yet a lack of planning can have vast impacts on a patient's well-being, the well-being of her family, and the medical community as a whole. Advance Care Planning (ACP) has been a field in the United States for a half-century. Many modern techniques prompting patients to think about end of life (EOL) involve short surveys or questionnaires. Different surveys are targeted to different populations (based off of likely disease progression or cultural factors, for instance), are designed with different intentions, and are administered in different ways. There has been recent work using technology to increase the number of people using advance care planning tools. However, modern techniques from machine learning and artificial intelligence could be employed to make additional changes to the current ACP process. In this paper we will discuss some possible ways in which these tools could be applied. We will discuss possible implications of these applications through vignettes of patient scenarios. We hope that this paper will encourage thought about appropriate applications of artificial intelligence in ACP as well as implementation of AI in order to ensure intentions are honored. △ Less","7 January, 2020",https://arxiv.org/pdf/2001.09742
Performance Analysis and Comparison of Machine and Deep Learning Algorithms for IoT Data Classification,Meysam Vakili;Mohammad Ghamsari;Masoumeh Rezaei,"In recent years, the growth of Internet of Things (IoT) as an emerging technology has been unbelievable. The number of networkenabled devices in IoT domains is increasing dramatically, leading to the massive production of electronic data. These data contain valuable information which can be used in various areas, such as science, industry, business and even social life. To extract and analyze this information and make IoT systems smart, the only choice is entering artificial intelligence (AI) world and leveraging the power of machine learning and deep learning techniques. This paper evaluates the performance of 11 popular machine and deep learning algorithms for classification task using six IoT-related datasets. These algorithms are compared according to several performance evaluation metrics including precision, recall, f1-score, accuracy, execution time, ROC-AUC score and confusion matrix. A specific experiment is also conducted to assess the convergence speed of developed models. The comprehensive experiments indicated that, considering all performance metrics, Random Forests performed better than other machine learning models, while among deep learning models, ANN and CNN achieved more interesting results. △ Less","27 January, 2020",https://arxiv.org/pdf/2001.09636
Practical Fast Gradient Sign Attack against Mammographic Image Classifier,Ibrahim Yilmaz,"Artificial intelligence (AI) has been a topic of major research for many years. Especially, with the emergence of deep neural network (DNN), these studies have been tremendously successful. Today machines are capable of making faster, more accurate decision than human. Thanks to the great development of machine learning (ML) techniques, ML have been used many different fields such as education, medicine, malware detection, autonomous car etc. In spite of having this degree of interest and much successful research, ML models are still vulnerable to adversarial attacks. Attackers can manipulate clean data in order to fool the ML classifiers to achieve their desire target. For instance; a benign sample can be modified as a malicious sample or a malicious one can be altered as benign while this modification can not be recognized by human observer. This can lead to many financial losses, or serious injuries, even deaths. The motivation behind this paper is that we emphasize this issue and want to raise awareness. Therefore, the security gap of mammographic image classifier against adversarial attack is demonstrated. We use mamographic images to train our model then evaluate our model performance in terms of accuracy. Later on, we poison original dataset and generate adversarial samples that missclassified by the model. We then using structural similarity index (SSIM) analyze similarity between clean images and adversarial images. Finally, we show how successful we are to misuse by using different poisoning factors. △ Less","27 January, 2020",https://arxiv.org/pdf/2001.09610
Explainable Artificial Intelligence and Machine Learning: A reality rooted perspective,Frank Emmert-Streib;Olli Yli-Harja;Matthias Dehmer,"We are used to the availability of big data generated in nearly all fields of science as a consequence of technological progress. However, the analysis of such data possess vast challenges. One of these relates to the explainability of artificial intelligence (AI) or machine learning methods. Currently, many of such methods are non-transparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI. In this paper, we do not assume the usual perspective presenting explainable AI as it should be, but rather we provide a discussion what explainable AI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics. △ Less","26 January, 2020",https://arxiv.org/pdf/2001.09464
AI-Powered GUI Attack and Its Defensive Methods,Ning Yu;Zachary Tuttle;Carl Jake Thurnau;Emmanuel Mireku,"Since the first Graphical User Interface (GUI) prototype was invented in the 1970s, GUI systems have been deployed into various personal computer systems and server platforms. Recently, with the development of artificial intelligence (AI) technology, malicious malware powered by AI is emerging as a potential threat to GUI systems. This type of AI-based cybersecurity attack, targeting at GUI systems, is explored in this paper. It is twofold: (1) A malware is designed to attack the existing GUI system by using AI-based object recognition techniques. (2) Its defensive methods are discovered by generating adversarial examples and other methods to alleviate the threats from the intelligent GUI attack. The results have shown that a generic GUI attack can be implemented and performed in a simple way based on current AI techniques and its countermeasures are temporary but effective to mitigate the threats of GUI attack so far. △ Less","25 January, 2020",https://arxiv.org/pdf/2001.09388
Intent Classification in Question-Answering Using LSTM Architectures,Giovanni Di Gennaro;Amedeo Buonanno;Antonio Di Girolamo;Armando Ospedale;Francesco A. N. Palmieri,"Question-answering (QA) is certainly the best known and probably also one of the most complex problem within Natural Language Processing (NLP) and artificial intelligence (AI). Since the complete solution to the problem of finding a generic answer still seems far away, the wisest thing to do is to break down the problem by solving single simpler parts. Assuming a modular approach to the problem, we confine our research to intent classification for an answer, given a question. Through the use of an LSTM network, we show how this type of classification can be approached effectively and efficiently, and how it can be properly used within a basic prototype responder. △ Less","25 January, 2020",https://arxiv.org/pdf/2001.09330
Detection of Thin Boundaries between Different Types of Anomalies in Outlier Detection using Enhanced Neural Networks,Rasoul Kiani;Amin Keshavarzi;Mahdi Bohlouli,"Outlier detection has received special attention in various fields, mainly for those dealing with machine learning and artificial intelligence. As strong outliers, anomalies are divided into the point, contextual and collective outliers. The most important challenges in outlier detection include the thin boundary between the remote points and natural area, the tendency of new data and noise to mimic the real data, unlabelled datasets and different definitions for outliers in different applications. Considering the stated challenges, we defined new types of anomalies called Collective Normal Anomaly and Collective Point Anomaly in order to improve a much better detection of the thin boundary between different types of anomalies. Basic domain-independent methods are introduced to detect these defined anomalies in both unsupervised and supervised datasets. The Multi-Layer Perceptron Neural Network is enhanced using the Genetic Algorithm to detect newly defined anomalies with higher precision so as to ensure a test error less than that calculated for the conventional Multi-Layer Perceptron Neural Network. Experimental results on benchmark datasets indicated reduced error of anomaly detection process in comparison to baselines. △ Less","24 January, 2020",https://arxiv.org/pdf/2001.09209
On the Performance of Metaheuristics: A Different Perspective,Hamid Reza Boveiri;Raouf Khayami,"Nowadays, we are immersed in tens of newly-proposed evolutionary and swam-intelligence metaheuristics, which makes it very difficult to choose a proper one to be applied on a specific optimization problem at hand. On the other hand, most of these metaheuristics are nothing but slightly modified variants of the basic metaheuristics. For example, Differential Evolution (DE) or Shuffled Frog Leaping (SFL) are just Genetic Algorithms (GA) with a specialized operator or an extra local search, respectively. Therefore, what comes to the mind is whether the behavior of such newly-proposed metaheuristics can be investigated on the basis of studying the specifications and characteristics of their ancestors. In this paper, a comprehensive evaluation study on some basic metaheuristics i.e. Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Artificial Bee Colony (ABC), Teaching-Learning-Based Optimization (TLBO), and Cuckoo Optimization algorithm (COA) is conducted, which give us a deeper insight into the performance of them so that we will be able to better estimate the performance and applicability of all other variations originated from them. A large number of experiments have been conducted on 20 different combinatorial optimization benchmark functions with different characteristics, and the results reveal to us some fundamental conclusions besides the following ranking order among these metaheuristics, {ABC, PSO, TLBO, GA, COA} i.e. ABC and COA are the best and the worst methods from the performance point of view, respectively. In addition, from the convergence perspective, PSO and ABC have significant better convergence for unimodal and multimodal functions, respectively, while GA and COA have premature convergence to local optima in many cases needing alternative mutation mechanisms to enhance diversification and global search. △ Less","24 January, 2020",https://arxiv.org/pdf/2001.08928
PairNets: Novel Fast Shallow Artificial Neural Networks on Partitioned Subspaces,Luna M. Zhang,"Traditionally, an artificial neural network (ANN) is trained slowly by a gradient descent algorithm such as the backpropagation algorithm since a large number of hyperparameters of the ANN need to be fine-tuned with many training epochs. To highly speed up training, we created a novel shallow 4-layer ANN called ""Pairwise Neural Network"" (""PairNet"") with high-speed hyperparameter optimization. In addition, a value of each input is partitioned into multiple intervals, and then an n-dimensional space is partitioned into M n-dimensional subspaces. M local PairNets are built in M partitioned local n-dimensional subspaces. A local PairNet is trained very quickly with only one epoch since its hyperparameters are directly optimized one-time via simply solving a system of linear equations by using the multivariate least squares fitting method. Simulation results for three regression problems indicated that the PairNet achieved much higher speeds and lower average testing mean squared errors (MSEs) for the three cases, and lower average training MSEs for two cases than the traditional ANNs. A significant future work is to develop better and faster optimization algorithms based on intelligent methods and parallel computing methods to optimize both partitioned subspaces and hyperparameters to build the fast and effective PairNets for applications in big data mining and real-time machine learning. △ Less","24 January, 2020",https://arxiv.org/pdf/2001.08886
Model-theoretic Characterizations of Existential Rule Languages,Heng Zhang;Yan Zhang;Guifei Jiang,"Existential rules, a.k.a. dependencies in databases, and Datalog+/- in knowledge representation and reasoning recently, are a family of important logical languages widely used in computer science and artificial intelligence. Towards a deep understanding of these languages in model theory, we establish model-theoretic characterizations for a number of existential rule languages such as (disjunctive) embedded dependencies, tuple-generating dependencies (TGDs), (frontier-)guarded TGDs and linear TGDs. All these characterizations hold for arbitrary structures, and most of them also work on the class of finite structures. As a natural application of these characterizations, complexity bounds for the rewritability of above languages are also identified. △ Less","23 January, 2020",https://arxiv.org/pdf/2001.08688
Smart Chest X-ray Worklist Prioritization using Artificial Intelligence: A Clinical Workflow Simulation,Ivo M. Baltruschat;Leonhard Steinmeister;Hannes Nickisch;Axel Saalbach;Michael Grass;Gerhard Adam;Tobias Knopp;Harald Ittrich,"The aim is to evaluate whether smart worklist prioritization by artificial intelligence (AI) can optimize the radiology workflow and reduce report turnaround times (RTAT) for critical findings in chest radiographs (CXRs). Furthermore, we investigate a method to counteract the effect of false negative predictions by AI -- resulting in an extremely and dangerously long RTAT, as CXRs are sorted to the end of the worklist. We developed a simulation framework that models the current workflow at a university hospital by incorporating hospital specific CXR generation rates, reporting rates and pathology distribution. Using this, we simulated the standard worklist processing ""first-in, first-out"" (FIFO) and compared it with a worklist prioritization based on urgency. Examination prioritization was performed by the AI, classifying eight different pathological findings ranked in descending order of urgency: pneumothorax, pleural effusion, infiltrate, congestion, atelectasis, cardiomegaly, mass and foreign object. Furthermore, we introduced an upper limit for the maximum waiting time, after which the highest urgency is assigned to the examination. The average RTAT for all critical findings was significantly reduced in all Prioritization-simulations compared to the FIFO-simulation (e.g. pneumothorax: 35.6 min vs. 80.1 min; p <0.0001), while the maximum RTAT for most findings increased at the same time (e.g. pneumothorax: 1293 min vs 890 min; p <0.0001). Our ""upper limit"" substantially reduced the maximum RTAT all classes (e.g. pneumothorax: 979 min vs. 1293 min / 1178 min; p <0.0001). Our simulations demonstrate that smart worklist prioritization by AI can reduce the average RTAT for critical findings in CXRs while maintaining a small maximum RTAT as FIFO. △ Less","18 June, 2020",https://arxiv.org/pdf/2001.08625
Cooperative Highway Work Zone Merge Control based on Reinforcement Learning in A Connected and Automated Environment,Tianzhu Ren;Yuanchang Xie;Liming Jiang,"Given the aging infrastructure and the anticipated growing number of highway work zones in the United States, it is important to investigate work zone merge control, which is critical for improving work zone safety and capacity. This paper proposes and evaluates a novel highway work zone merge control strategy based on cooperative driving behavior enabled by artificial intelligence. The proposed method assumes that all vehicles are fully automated, connected and cooperative. It inserts two metering zones in the open lane to make space for merging vehicles in the closed lane. In addition, each vehicle in the closed lane learns how to optimally adjust its longitudinal position to find a safe gap in the open lane using an off-policy soft actor critic (SAC) reinforcement learning (RL) algorithm, considering the traffic conditions in its surrounding. The learning results are captured in convolutional neural networks and used to control individual vehicles in the testing phase. By adding the metering zones and taking the locations, speeds, and accelerations of surrounding vehicles into account, cooperation among vehicles is implicitly considered. This RL-based model is trained and evaluated using a microscopic traffic simulator. The results show that this cooperative RL-based merge control significantly outperforms popular strategies such as late merge and early merge in terms of both mobility and safety measures. △ Less","21 January, 2020",https://arxiv.org/pdf/2001.08581
Investigating naturalistic hand movements by behavior mining in long-term video and neural recordings,Satpreet H. Singh;Steven M. Peterson;Rajesh P. N. Rao;Bingni W. Brunton,"Recent technological advances in brain recording and artificial intelligence are propelling a new paradigm in neuroscience beyond the traditional controlled experiment. Rather than focusing on cued, repeated trials, naturalistic neuroscience studies neural processes underlying spontaneous behaviors performed in unconstrained settings. However, analyzing such unstructured data lacking a priori experimental design remains a significant challenge, especially when the data is multi-modal and long-term. Here we describe an automated approach for analyzing simultaneously recorded long-term, naturalistic electrocorticography (ECoG) and naturalistic behavior video data. We take a behavior-first approach to analyzing the long-term recordings. Using a combination of computer vision, discrete latent-variable modeling, and string pattern-matching on the behavioral video data, we find and annotate spontaneous human upper-limb movement events. We show results from our approach applied to data collected for 12 human subjects over 7--9 days for each subject. Our pipeline discovers and annotates over 40,000 instances of naturalistic human upper-limb movement events in the behavioral videos. Analysis of the simultaneously recorded brain data reveals neural signatures of movement that corroborate prior findings from traditional controlled experiments. We also prototype a decoder for a movement initiation detection task to demonstrate the efficacy of our pipeline as a source of training data for brain-computer interfacing applications. Our work addresses the unique data analysis challenges in studying naturalistic human behaviors, and contributes methods that may generalize to other neural recording modalities beyond ECoG. We publicly release our curated dataset, providing a resource to study naturalistic neural and behavioral variability at a scale not previously available. △ Less","19 June, 2020",https://arxiv.org/pdf/2001.08349
Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems,Zana Buçinca;Phoebe Lin;Krzysztof Z. Gajos;Elena L. Glassman,"Explainable artificially intelligent (XAI) systems form part of sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet, current XAI systems are rarely evaluated by measuring the performance of human+AI teams on actual decision-making tasks. We conducted two online experiments and one in-person think-aloud study to evaluate two currently common techniques for evaluating XAI systems: (1) using proxy, artificial tasks such as how well humans predict the AI's decision from the given explanations, and (2) using subjective measures of trust and preference as predictors of actual performance. The results of our experiments demonstrate that evaluations with proxy tasks did not predict the results of the evaluations with the actual decision-making tasks. Further, the subjective measures on evaluations with actual decision-making tasks did not predict the objective performance on those same tasks. Our results suggest that by employing misleading evaluation methods, our field may be inadvertently slowing its progress toward developing human+AI teams that can reliably perform better than humans or AIs alone. △ Less","22 January, 2020",https://arxiv.org/pdf/2001.08298
Artificial Intelligence Enabled Wireless Networking for 5G and Beyond: Recent Advances and Future Challenges,Cheng-Xiang Wang;Marco Di Renzo;Slawomir Stańczak;Sen Wang;Erik G. Larsson,"The fifth generation (5G) wireless communication networks are currently being deployed, and beyond 5G (B5G) networks are expected to be developed over the next decade. Artificial intelligence (AI) technologies and, in particular, machine learning (ML) have the potential to efficiently solve the unstructured and seemingly intractable problems by involving large amounts of data that need to be dealt with in B5G. This article studies how AI and ML can be leveraged for the design and operation of B5G networks. We first provide a comprehensive survey of recent advances and future challenges that result from bringing AI/ML technologies into B5G wireless networks. Our survey touches different aspects of wireless network design and optimization, including channel measurements, modeling, and estimation, physical-layer research, and network management and optimization. Then, ML algorithms and applications to B5G networks are reviewed, followed by an overview of standard developments of applying AI/ML algorithms to B5G networks. We conclude this study by the future challenges on applying AI/ML to B5G networks. △ Less","1 January, 2020",https://arxiv.org/pdf/2001.08159
A Neural Architecture for Person Ontology population,Balaji Ganesan;Riddhiman Dasgupta;Akshay Parekh;Hima Patel;Berthold Reinwald,"A person ontology comprising concepts, attributes and relationships of people has a number of applications in data protection, didentification, population of knowledge graphs for business intelligence and fraud prevention. While artificial neural networks have led to improvements in Entity Recognition, Entity Classification, and Relation Extraction, creating an ontology largely remains a manual process, because it requires a fixed set of semantic relations between concepts. In this work, we present a system for automatically populating a person ontology graph from unstructured data using neural models for Entity Classification and Relation Extraction. We introduce a new dataset for these tasks and discuss our results. △ Less","22 January, 2020",https://arxiv.org/pdf/2001.08013
Machine Learning for Network Slicing Resource Management: A Comprehensive Survey,Bin Han;Hans D. Schotten,"The emerging technology of multi-tenancy network slicing is considered as an essential feature of 5G cellular networks. It provides network slices as a new type of public cloud services, and therewith increases the service flexibility and enhances the network resource efficiency. Meanwhile, it raises new challenges of network resource management. A number of various methods have been proposed over the recent past years, in which machine learning and artificial intelligence techniques are widely deployed. In this article, we provide a survey to existing approaches of network slicing resource management, with a highlight on the roles played by machine learning in them. △ Less","22 January, 2020",https://arxiv.org/pdf/2001.07974
Fairness Metrics: A Comparative Analysis,Pratyush Garg;John Villasenor;Virginia Foggo,"Algorithmic fairness is receiving significant attention in the academic and broader literature due to the increasing use of predictive algorithms, including those based on artificial intelligence. One benefit of this trend is that algorithm designers and users have a growing set of fairness measures to choose from. However, this choice comes with the challenge of identifying how the different fairness measures relate to one another, as well as the extent to which they are compatible or mutually exclusive. We describe some of the most widely used fairness metrics using a common mathematical framework and present new results on the relationships among them. The results presented herein can help place both specialists and non-specialists in a better position to identify the metric best suited for their application and goals. △ Less","27 January, 2020",https://arxiv.org/pdf/2001.07864
Engineering AI Systems: A Research Agenda,Jan Bosch;Ivica Crnkovic;Helena Holmström Olsson,"Artificial intelligence (AI) and machine learning (ML) are increasingly broadly adopted in industry, However, based on well over a dozen case studies, we have learned that deploying industry-strength, production quality ML models in systems proves to be challenging. Companies experience challenges related to data quality, design methods and processes, performance of models as well as deployment and compliance. We learned that a new, structured engineering approach is required to construct and evolve systems that contain ML/DL components. In this paper, we provide a conceptualization of the typical evolution patterns that companies experience when employing ML as well as an overview of the key problems experienced by the companies that we have studied. The main contribution of the paper is a research agenda for AI engineering that provides an overview of the key engineering challenges surrounding ML solutions and an overview of open items that need to be addressed by the research community at large. △ Less","3 June, 2020",https://arxiv.org/pdf/2001.07522
Teaching Software Engineering for AI-Enabled Systems,Christian Kästner;Eunsuk Kang,"Software engineers have significant expertise to offer when building intelligent systems, drawing on decades of experience and methods for building systems that are scalable, responsive and robust, even when built on unreliable components. Systems with artificial-intelligence or machine-learning (ML) components raise new challenges and require careful engineering. We designed a new course to teach software-engineering skills to students with a background in ML. We specifically go beyond traditional ML courses that teach modeling techniques under artificial conditions and focus, in lecture and assignments, on realism with large and changing datasets, robust and evolvable infrastructure, and purposeful requirements engineering that considers ethics and fairness as well. We describe the course and our infrastructure and share experience and all material from teaching the course for the first time. △ Less","18 January, 2020",https://arxiv.org/pdf/2001.06691
Machine learning and AI-based approaches for bioactive ligand discovery and GPCR-ligand recognition,Sebastian Raschka;Benjamin Kaufman,"In the last decade, machine learning and artificial intelligence applications have received a significant boost in performance and attention in both academic research and industry. The success behind most of the recent state-of-the-art methods can be attributed to the latest developments in deep learning. When applied to various scientific domains that are concerned with the processing of non-tabular data, for example, image or text, deep learning has been shown to outperform not only conventional machine learning but also highly specialized tools developed by domain experts. This review aims to summarize AI-based research for GPCR bioactive ligand discovery with a particular focus on the most recent achievements and research trends. To make this article accessible to a broad audience of computational scientists, we provide instructive explanations of the underlying methodology, including overviews of the most commonly used deep learning architectures and feature representations of molecular data. We highlight the latest AI-based research that has led to the successful discovery of GPCR bioactive ligands. However, an equal focus of this review is on the discussion of machine learning-based technology that has been applied to ligand discovery in general and has the potential to pave the way for successful GPCR bioactive ligand discovery in the future. This review concludes with a brief outlook highlighting the recent research trends in deep learning, such as active learning and semi-supervised learning, which have great potential for advancing bioactive ligand discovery. △ Less","6 June, 2020",https://arxiv.org/pdf/2001.06545
Activism by the AI Community: Analysing Recent Achievements and Future Prospects,Haydn Belfield,"The artificial intelligence community (AI) has recently engaged in activism in relation to their employers, other members of the community, and their governments in order to shape the societal and ethical implications of AI. It has achieved some notable successes, but prospects for further political organising and activism are uncertain. We survey activism by the AI community over the last six years; apply two analytical frameworks drawing upon the literature on epistemic communities, and worker organising and bargaining; and explore what they imply for the future prospects of the AI community. Success thus far has hinged on a coherent shared culture, and high bargaining power due to the high demand for a limited supply of AI talent. Both are crucial to the future of AI activism and worthy of sustained attention. △ Less","17 January, 2020",https://arxiv.org/pdf/2001.06528
FedVision: An Online Visual Object Detection Platform Powered by Federated Learning,Yang Liu;Anbu Huang;Yun Luo;He Huang;Youzhi Liu;Yuanyuan Chen;Lican Feng;Tianjian Chen;Han Yu;Qiang Yang,"Visual object detection is a computer vision-based artificial intelligence (AI) technique which has many practical applications (e.g., fire hazard monitoring). However, due to privacy concerns and the high cost of transmitting video data, it is highly challenging to build object detection models on centrally stored large training datasets following the current approach. Federated learning (FL) is a promising approach to resolve this challenge. Nevertheless, there currently lacks an easy to use tool to enable computer vision application developers who are not experts in federated learning to conveniently leverage this technology and apply it in their systems. In this paper, we report FedVision - a machine learning engineering platform to support the development of federated learning powered computer vision applications. The platform has been deployed through a collaboration between WeBank and Extreme Vision to help customers develop computer vision-based safety monitoring solutions in smart city applications. Over four months of usage, it has achieved significant efficiency improvement and cost reduction while removing the need to transmit sensitive data for three major corporate customers. To the best of our knowledge, this is the first real application of FL in computer vision-based tasks. △ Less","17 January, 2020",https://arxiv.org/pdf/2001.06202
Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification,Anandhanarayanan Kamalakannan;Shiva Shankar Ganesan;Govindaraj Rajamanickam,"Image segmentation and classification are the two main fundamental steps in pattern recognition. To perform medical image segmentation or classification with deep learning models, it requires training on large image dataset with annotation. The dermoscopy images (ISIC archive) considered for this work does not have ground truth information for lesion segmentation. Performing manual labelling on this dataset is time-consuming. To overcome this issue, self-learning annotation scheme was proposed in the two-stage deep learning algorithm. The two-stage deep learning algorithm consists of U-Net segmentation model with the annotation scheme and CNN classifier model. The annotation scheme uses a K-means clustering algorithm along with merging conditions to achieve initial labelling information for training the U-Net model. The classifier models namely ResNet-50 and LeNet-5 were trained and tested on the image dataset without segmentation for comparison and with the U-Net segmentation for implementing the proposed self-learning Artificial Intelligence (AI) framework. The classification results of the proposed AI framework achieved training accuracy of 93.8% and testing accuracy of 82.42% when compared with the two classifier models directly trained on the input images. △ Less","4 January, 2020",https://arxiv.org/pdf/2001.05838
Stream-Flow Forecasting of Small Rivers Based on LSTM,Youchuan Hu;Le Yan;Tingting Hang;Jun Feng,"Stream-flow forecasting for small rivers has always been of great importance, yet comparatively challenging due to the special features of rivers with smaller volume. Artificial Intelligence (AI) methods have been employed in this area for long, but improvement of forecast quality is still on the way. In this paper, we tried to provide a new method to do the forecast using the Long-Short Term Memory (LSTM) deep learning model, which aims in the field of time-series data. Utilizing LSTM, we collected the stream flow data from one hydrologic station in Tunxi, China, and precipitation data from 11 rainfall stations around to forecast the stream flow data from that hydrologic station 6 hours in the future. We evaluated the prediction results using three criteria: root mean square error (RMSE), mean absolute error (MAE), and coefficient of determination (R^2). By comparing LSTM's prediction with predictions of Support Vector Regression (SVR) and Multilayer Perceptions (MLP) models, we showed that LSTM has better performance, achieving RMSE of 82.007, MAE of 27.752, and R^2 of 0.970. We also did extended experiments on LSTM model, discussing influence factors of its performance. △ Less","16 January, 2020",https://arxiv.org/pdf/2001.05681
Causal Discovery from Incomplete Data: A Deep Learning Approach,Yuhao Wang;Vlado Menkovski;Hao Wang;Xin Du;Mykola Pechenizkiy,"As systems are getting more autonomous with the development of artificial intelligence, it is important to discover the causal knowledge from observational sensory inputs. By encoding a series of cause-effect relations between events, causal networks can facilitate the prediction of effects from a given action and analyze their underlying data generation mechanism. However, missing data are ubiquitous in practical scenarios. Directly performing existing casual discovery algorithms on partially observed data may lead to the incorrect inference. To alleviate this issue, we proposed a deep learning framework, dubbed Imputated Causal Learning (ICL), to perform iterative missing data imputation and causal structure discovery. Through extensive simulations on both synthetic and real data, we show that ICL can outperform state-of-the-art methods under different missing data mechanisms. △ Less","15 January, 2020",https://arxiv.org/pdf/2001.05343
Automated Anonymisation of Visual and Audio Data in Classroom Studies,Ömer Sümer;Peter Gerjets;Ulrich Trautwein;Enkelejda Kasneci,"Understanding students' and teachers' verbal and non-verbal behaviours during instruction may help infer valuable information regarding the quality of teaching. In education research, there have been many studies that aim to measure students' attentional focus on learning-related tasks: Based on audio-visual recordings and manual or automated ratings of behaviours of teachers and students. Student data is, however, highly sensitive. Therefore, ensuring high standards of data protection and privacy has the utmost importance in current practices. For example, in the context of teaching management studies, data collection is carried out with the consent of pupils, parents, teachers and school administrations. Nevertheless, there may often be students whose data cannot be used for research purposes. Excluding these students from the classroom is an unnatural intrusion into the organisation of the classroom. A possible solution would be to request permission to record the audio-visual recordings of all students (including those who do not voluntarily participate in the study) and to anonymise their data. Yet, the manual anonymisation of audio-visual data is very demanding. In this study, we examine the use of artificial intelligence methods to automatically anonymise the visual and audio data of a particular person. △ Less","14 January, 2020",https://arxiv.org/pdf/2001.05080
High--Dimensional Brain in a High-Dimensional World: Blessing of Dimensionality,Alexander N. Gorban;Valery A. Makarov;Ivan Y. Tyukin,"High-dimensional data and high-dimensional representations of reality are inherent features of modern Artificial Intelligence systems and applications of machine learning. The well-known phenomenon of the ""curse of dimensionality"" states: many problems become exponentially difficult in high dimensions. Recently, the other side of the coin, the ""blessing of dimensionality"", has attracted much attention. It turns out that generic high-dimensional datasets exhibit fairly simple geometric properties. Thus, there is a fundamental tradeoff between complexity and simplicity in high dimensional spaces. Here we present a brief explanatory review of recent ideas, results and hypotheses about the blessing of dimensionality and related simplifying effects relevant to machine learning and neuroscience. △ Less","14 January, 2020",https://arxiv.org/pdf/2001.04959
Knowledge Representations in Technical Systems -- A Taxonomy,Kristina Scharei;Florian Heidecker;Maarten Bieshaar,"The recent usage of technical systems in human-centric environments leads to the question, how to teach technical systems, e.g., robots, to understand, learn, and perform tasks desired by the human. Therefore, an accurate representation of knowledge is essential for the system to work as expected. This article mainly gives insight into different knowledge representation techniques and their categorization into various problem domains in artificial intelligence. Additionally, applications of presented knowledge representations are introduced in everyday robotics tasks. By means of the provided taxonomy, the search for a proper knowledge representation technique regarding a specific problem should be facilitated. △ Less","15 January, 2020",https://arxiv.org/pdf/2001.04835
Preliminary Study of a Google Home Mini,Min Jin Park;Joshua I. James,"Many artificial intelligence (AI) speakers have recently come to market. Beginning with Amazon Echo, many companies producing their own speaker technologies. Due to the limitations of technology, most speakers have similar functions, but the way of handling the data of each speaker is different. In the case of Amazon echo, the API of the cloud is open for any developers to develop their API. The Amazon Echo has been around for a while, and much research has been done on it. However, not much research has been done on Google Home Mini analysis for digital investigations. In this paper, we will conduct some initial research on the data storing and security methods of Google Home Mini. △ Less","13 January, 2020",https://arxiv.org/pdf/2001.04574
Prediction of flow characteristics in the bubble column reactor by the artificial pheromone-based communication of biological ants,Shahab Shamshirband;Meisam Babanezhad;Amir Mosavi;Narjes Nabipour;Eva Hajnal;Laszlo Nadai;Kwok-Wing Chau,"In order to perceive the behavior presented by the multiphase chemical reactors, the ant colony optimization algorithm was combined with computational fluid dynamics (CFD) data. This intelligent algorithm creates a probabilistic technique for computing flow and it can predict various levels of three-dimensional bubble column reactor (BCR). This artificial ant algorithm is mimicking real ant behavior. This method can anticipate the flow characteristics in the reactor using almost 30 % of the whole data in the domain. Following discovering the suitable parameters, the method is used for predicting the points not being simulated with CFD, which represent mesh refinement of Ant colony method. In addition, it is possible to anticipate the bubble-column reactors in the absence of numerical results or training of exact values of evaluated data. The major benefits include reduced computational costs and time savings. The results show a great agreement between ant colony prediction and CFD outputs in different sections of the BCR. The combination of ant colony system and neural network framework can provide the smart structure to estimate biological and nature physics base phenomena. The ant colony optimization algorithm (ACO) framework based on ant behavior can solve all local mathematical answers throughout 3D bubble column reactor. The integration of all local answers can provide the overall solution in the reactor for different characteristics. This new overview of modelling can illustrate new sight into biological behavior in nature. △ Less","9 January, 2020",https://arxiv.org/pdf/2001.04276
Perspectives and Ethics of the Autonomous Artificial Thinking Systems,Joël Colloc,"The feasibility of autonomous artificial thinking systems needs to compare the way the human beings acquire their information and develops the thought with the current capacities of the autonomous information systems. Our model uses four hierarchies: the hierarchy of information systems, the cognitive hierarchy, the linguistic hierarchy and the digital informative hierarchy that combines artificial intelligence, the power of computers models, methods and tools to develop autonomous information systems. The question of the capability of autonomous system to provide a form of artificial thought arises with the ethical consequences on the social life and the perspective of transhumanism. △ Less","13 January, 2020",https://arxiv.org/pdf/2001.04270
Bridging the gap between AI and Healthcare sides: towards developing clinically relevant AI-powered diagnosis systems,Changhee Han;Leonardo Rundo;Kohei Murao;Takafumi Nemoto;Hideki Nakayama,"Despite the success of Convolutional Neural Network-based Computer-Aided Diagnosis research, its clinical applications remain challenging. Accordingly, developing medical Artificial Intelligence (AI) fitting into a clinical environment requires identifying/bridging the gap between AI and Healthcare sides. Since the biggest problem in Medical Imaging lies in data paucity, confirming the clinical relevance for diagnosis of research-proven image augmentation techniques is essential. Therefore, we hold a clinically valuable AI-envisioning workshop among Japanese Medical Imaging experts, physicians, and generalists in Healthcare/Informatics. Then, a questionnaire survey for physicians evaluates our pathology-aware Generative Adversarial Network (GAN)-based image augmentation projects in terms of Data Augmentation and physician training. The workshop reveals the intrinsic gap between AI/Healthcare sides and solutions on Why (i.e., clinical significance/interpretation) and How (i.e., data acquisition, commercial deployment, and safety/feeling safe). This analysis confirms our pathology-aware GANs' clinical relevance as a clinical decision support system and non-expert physician training tool. Our findings would play a key role in connecting inter-disciplinary research and clinical applications, not limited to the Japanese medical context and pathology-aware GANs. △ Less","6 April, 2020",https://arxiv.org/pdf/2001.03923
Should Artificial Intelligence Governance be Centralised? Design Lessons from History,Peter Cihon;Matthijs M. Maas;Luke Kemp,"Can effective international governance for artificial intelligence remain fragmented, or is there a need for a centralised international organisation for AI? We draw on the history of other international regimes to identify advantages and disadvantages in centralising AI governance. Some considerations, such as efficiency and political power, speak in favour of centralisation. Conversely, the risk of creating a slow and brittle institution speaks against it, as does the difficulty in securing participation while creating stringent rules. Other considerations depend on the specific design of a centralised institution. A well-designed body may be able to deter forum shopping and ensure policy coordination. However, forum shopping can be beneficial and a fragmented landscape of institutions can be self-organising. Centralisation entails trade-offs and the details matter. We conclude with two core recommendations. First, the outcome will depend on the exact design of a central institution. A well-designed centralised regime covering a set of coherent issues could be beneficial. But locking-in an inadequate structure may pose a fate worse than fragmentation. Second, for now fragmentation will likely persist. This should be closely monitored to see if it is self-organising or simply inadequate. △ Less","10 January, 2020",https://arxiv.org/pdf/2001.03573
Open Challenges and Issues: Artificial Intelligence for Transactive Management,Asma Khatun;Sk. Golam Sarowar Hossain,"The advancement of Artificial Intelligence (AI) has improved the automation of energy managements. In smart energy management or in a smart grid framework, all the devices and the distributed resources and renewable resources are embedded which leads to reduce cost. A smart energy management system, Transactive management (TM) is a concept to improve the efficiency and reliability of the power system. The aim of this article is to look for the current development of TM methods based on AI and Machine Learning (ML) technology. In AI paradigm, MultiAgent System (MAS) based method is an active research area and are still in evolution. Hence this article describes how MAS based method applied in TM. This paper also finds that MAS based method faces major difficulty to design or set up goal to various agents and describes how ML technique can contribute to that solution. A brief comparison analysis between MAS and ML techniques are also presented. At the end, this article summarizes the most relevant open challenges and issues on the AI based methods for transactive energy management. △ Less","2 January, 2020",https://arxiv.org/pdf/2001.03238
Emo-CNN for Perceiving Stress from Audio Signals: A Brain Chemistry Approach,Anup Anand Deshmukh;Catherine Soladie;Renaud Seguier,"Emotion plays a key role in many applications like healthcare, to gather patients emotional behavior. There are certain emotions which are given more importance due to their effectiveness in understanding human feelings. In this paper, we propose an approach that models human stress from audio signals. The research challenge in speech emotion detection is defining the very meaning of stress and being able to categorize it in a precise manner. Supervised Machine Learning models, including state of the art Deep Learning classification methods, rely on the availability of clean and labelled data. One of the problems in affective computation and emotion detection is the limited amount of annotated data of stress. The existing labelled stress emotion datasets are highly subjective to the perception of the annotator. We address the first issue of feature selection by exploiting the use of traditional MFCC features in Convolutional Neural Network. Our experiments show that Emo-CNN consistently and significantly outperforms the popular existing methods over multiple datasets. It achieves 90.2% categorical accuracy on the Emo-DB dataset. To tackle the second and the more significant problem of subjectivity in stress labels, we use Lovheim's cube, which is a 3-dimensional projection of emotions. The cube aims at explaining the relationship between these neurotransmitters and the positions of emotions in 3D space. The learnt emotion representations from the Emo-CNN are mapped to the cube using three component PCA (Principal Component Analysis) which is then used to model human stress. This proposed approach not only circumvents the need for labelled stress data but also complies with the psychological theory of emotions given by Lovheim's cube. We believe that this work is the first step towards creating a connection between Artificial Intelligence and the chemistry of human emotions. △ Less","7 January, 2020",https://arxiv.org/pdf/2001.02329
Plunge into the Underworld: A Survey on Emergence of Darknet,Victor Adewopo;Bilal Gonen;Said Varlioglu;Murat Ozer,"The availability of sophisticated technologies and methods of perpetrating criminogenic activities in the cyberspace is a pertinent societal problem. Darknet is an encrypted network technology that uses the internet infrastructure and can only be accessed using special network configuration and software tools to access its contents which are not indexed by search engines. Over the years darknets traditionally are used for criminogenic activities and famously acclaimed to promote cybercrime, procurements of illegal drugs, arms deals, and cryptocurrency markets. In countries with oppressive regimes, censorship of digital communications, and strict policies prompted journalists and freedom fighters to seek freedom using darknet technologies anonymously while others simply exploit it for illegal activities. Recently, MIT's Lincoln Laboratory of Artificial Intelligence augmented a tool that can be used to expose illegal activities behind the darknet. We studied relevant literature reviews to help researchers to better understand the darknet technologies, identify future areas of research on the darknet and ultimately to optimize how data-driven insights can be utilized to support governmental agencies in unraveling the depths of darknet technologies. This paper focuses on the use of the internet for crimes, deanonymization of TOR-services, darknet a new digital street for illicit drugs, research questions and hypothesis to guide researchers in further studies. Finally, in this study, we propose a model to examine and investigate anonymous online illicit markets. △ Less","17 March, 2020",https://arxiv.org/pdf/2001.02300
Self learning robot using real-time neural networks,Chirag Gupta;Chikita Nangia;Chetan Kumar,"With the advancements in high volume, low precision computational technology and applied research on cognitive artificially intelligent heuristic systems, machine learning solutions through neural networks with real-time learning has seen an immense interest in the research community as well the industry. This paper involves research, development and experimental analysis of a neural network implemented on a robot with an arm through which evolves to learn to walk in a straight line or as required. The neural network learns using the algorithms of Gradient Descent and Backpropagation. Both the implementation and training of the neural network is done locally on the robot on a raspberry pi 3 so that its learning process is completely independent. The neural network is first tested on a custom simulator developed on MATLAB and then implemented on the raspberry computer. Data at each generation of the evolving network is stored, and analysis both mathematical and graphical is done on the data. Impact of factors like the learning rate and error tolerance on the learning process and final output is analyzed. △ Less","6 January, 2020",https://arxiv.org/pdf/2001.02103
Artificial Intelligence for Social Good: A Survey,Zheyuan Ryan Shi;Claire Wang;Fei Fang,"Artificial intelligence for social good (AI4SG) is a research theme that aims to use and advance artificial intelligence to address societal issues and improve the well-being of the world. AI4SG has received lots of attention from the research community in the past decade with several successful applications. Building on the most comprehensive collection of the AI4SG literature to date with over 1000 contributed papers, we provide a detailed account and analysis of the work under the theme in the following ways. (1) We quantitatively analyze the distribution and trend of the AI4SG literature in terms of application domains and AI techniques used. (2) We propose three conceptual methods to systematically group the existing literature and analyze the eight AI4SG application domains in a unified framework. (3) We distill five research topics that represent the common challenges in AI4SG across various application domains. (4) We discuss five issues that, we hope, can shed light on the future development of the AI4SG research. △ Less","6 January, 2020",https://arxiv.org/pdf/2001.01818
Dissecting Catastrophic Forgetting in Continual Learning by Deep Visualization,Giang Nguyen;Shuan Chen;Thao Do;Tae Joon Jun;Ho-Jin Choi;Daeyoung Kim,"Interpreting the behaviors of Deep Neural Networks (usually considered as a black box) is critical especially when they are now being widely adopted over diverse aspects of human life. Taking the advancements from Explainable Artificial Intelligent, this paper proposes a novel technique called Auto DeepVis to dissect catastrophic forgetting in continual learning. A new method to deal with catastrophic forgetting named critical freezing is also introduced upon investigating the dilemma by Auto DeepVis. Experiments on a captioning model meticulously present how catastrophic forgetting happens, particularly showing which components are forgetting or changing. The effectiveness of our technique is then assessed; and more precisely, critical freezing claims the best performance on both previous and coming tasks over baselines, proving the capability of the investigation. Our techniques could not only be supplementary to existing solutions for completely eradicating catastrophic forgetting for life-long learning but also explainable. △ Less","7 January, 2020",https://arxiv.org/pdf/2001.01578
Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing,Inioluwa Deborah Raji;Andrew Smart;Rebecca N. White;Margaret Mitchell;Timnit Gebru;Ben Hutchinson;Jamila Smith-Loud;Daniel Theron;Parker Barnes,"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development lifecycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity. △ Less","3 January, 2020",https://arxiv.org/pdf/2001.00973
A Framework for Democratizing AI,Shakkeel Ahmed;Ravi S. Mula;Soma S. Dhavala,"Machine Learning and Artificial Intelligence are considered an integral part of the Fourth Industrial Revolution. Their impact, and far-reaching consequences, while acknowledged, are yet to be comprehended. These technologies are very specialized, and few organizations and select highly trained professionals have the wherewithal, in terms of money, manpower, and might, to chart the future. However, concentration of power can lead to marginalization, causing severe inequalities. Regulatory agencies and governments across the globe are creating national policies, and laws around these technologies to protect the rights of the digital citizens, as well as to empower them. Even private, not-for-profit organizations are also contributing to democratizing the technologies by making them \emph{accessible} and \emph{affordable}. However, accessibility and affordability are all but a few of the facets of democratizing the field. Others include, but not limited to, \emph{portability}, \emph{explainability}, \emph{credibility}, \emph{fairness}, among others. As one can imagine, democratizing AI is a multi-faceted problem, and it requires advancements in science, technology and policy. At \texttt{mlsquare}, we are developing scientific tools in this space. Specifically, we introduce an opinionated, extensible, \texttt{Python} framework that provides a single point of interface to a variety of solutions in each of the categories mentioned above. We present the design details, APIs of the framework, reference implementations, road map for development, and guidelines for contributions. △ Less","1 January, 2020",https://arxiv.org/pdf/2001.00818
Towards Intelligent Robotic Process Automation for BPMers,Simone Agostinelli;Andrea Marrella;Massimo Mecella,"Robotic Process Automation (RPA) is a fast-emerging automation technology that sits between the fields of Business Process Management (BPM) and Artificial Intelligence (AI), and allows organizations to automate high volume routines. RPA tools are able to capture the execution of such routines previously performed by a human users on the interface of a computer system, and then emulate their enactment in place of the user by means of a software robot. Nowadays, in the BPM domain, only simple, predictable business processes involving routine work can be automated by RPA tools in situations where there is no room for interpretation, while more sophisticated work is still left to human experts. In this paper, starting from an in-depth experimentation of the RPA tools available on the market, we provide a classification framework to categorize them on the basis of some key dimensions. Then, based on this analysis, we derive four research challenges and discuss prospective approaches necessary to inject intelligence into current RPA technology, in order to achieve more widespread adoption of RPA in the BPM domain. △ Less","3 January, 2020",https://arxiv.org/pdf/2001.00804
The Offense-Defense Balance of Scientific Knowledge: Does Publishing AI Research Reduce Misuse?,Toby Shevlane;Allan Dafoe,"There is growing concern over the potential misuse of artificial intelligence (AI) research. Publishing scientific research can facilitate misuse of the technology, but the research can also contribute to protections against misuse. This paper addresses the balance between these two effects. Our theoretical framework elucidates the factors governing whether the published research will be more useful for attackers or defenders, such as the possibility for adequate defensive measures, or the independent discovery of the knowledge outside of the scientific community. The balance will vary across scientific fields. However, we show that the existing conversation within AI has imported concepts and conclusions from prior debates within computer security over the disclosure of software vulnerabilities. While disclosure of software vulnerabilities often favours defence, this cannot be assumed for AI research. The AI research community should consider concepts and policies from a broad set of adjacent fields, and ultimately needs to craft policy well-suited to its particular challenges. △ Less","9 January, 2020",https://arxiv.org/pdf/2001.00463
Reconfigurable-Intelligent-Surface Empowered Wireless Communications: Challenges and Opportunities,Xiaojun Yuan;Ying-Jun Angela Zhang;Yuanming Shi;Wenjing Yan;Hang Liu,"Reconfigurable intelligent surfaces (RISs) are regarded as a promising emerging hardware technology to improve the spectrum and energy efficiency of wireless networks by artificially reconfiguring the propagation environment of electromagnetic waves. Due to the unique advantages in enhancing wireless channel capacity, RISs have recently become a hot research topic. In this article, we focus on three fundamental physical-layer challenges for the incorporation of RISs into wireless networks, namely, channel state information acquisition, passive information transfer, and low-complexity robust system design. We summarize the state-of-the-art solutions and explore potential research directions. Furthermore, we discuss other promising research directions of RISs, including edge intelligence and physical-layer security. △ Less","16 August, 2020",https://arxiv.org/pdf/2001.00364
Butterfly Detection and Classification Based on Integrated YOLO Algorithm,Bohan Liang;Shangxi Wu;Kaiyuan Xu;Jingyu Hao,"Insects are abundant species on the earth, and the task of identification and identification of insects is complex and arduous. How to apply artificial intelligence technology and digital image processing methods to automatic identification of insect species is a hot issue in current research. In this paper, the problem of automatic detection and classification recognition of butterfly photographs is studied, and a method of bio-labeling suitable for butterfly classification is proposed. On the basis of YOLO algorithm, by synthesizing the results of YOLO models with different training mechanisms, a butterfly automatic detection and classification recognition algorithm based on YOLO algorithm is proposed. It greatly improves the generalization ability of YOLO algorithm and makes it have better ability to solve small sample problems. The experimental results show that the proposed annotation method and integrated YOLO algorithm have high accuracy and recognition rate in butterfly automatic detection and recognition. △ Less","25 April, 2020",https://arxiv.org/pdf/2001.00361
A general anomaly detection framework for fleet-based condition monitoring of machines,Kilian Hendrickx;Wannes Meert;Yves Mollet;Johan Gyselinck;Bram Cornelis;Konstantinos Gryllias;Jesse Davis,"Machine failures decrease up-time and can lead to extra repair costs or even to human casualties and environmental pollution. Recent condition monitoring techniques use artificial intelligence in an effort to avoid time-consuming manual analysis and handcrafted feature extraction. Many of these only analyze a single machine and require a large historical data set. In practice, this can be difficult and expensive to collect. However, some industrial condition monitoring applications involve a fleet of similar operating machines. In most of these applications, it is safe to assume healthy conditions for the majority of machines. Deviating machine behavior is then an indicator for a machine fault. This work proposes an unsupervised, generic, anomaly detection framework for fleet-based condition monitoring. It uses generic building blocks and offers three key advantages. First, a historical data set is not required due to online fleet-based comparisons. Second, it allows incorporating domain expertise by user-defined comparison measures. Finally, contrary to most black-box artificial intelligence techniques, easy interpretability allows a domain expert to validate the predictions made by the framework. Two use-cases on an electrical machine fleet demonstrate the applicability of the framework to detect a voltage unbalance by means of electrical and vibration signatures. △ Less","7 January, 2020",https://arxiv.org/pdf/1912.12941
Animals in Virtual Environments,Hemal Naik;Renaud Bastien;Nassir Navab;Iain Couzin,"The core idea in an XR (VR/MR/AR) application is to digitally stimulate one or more sensory systems (e.g. visual, auditory, olfactory) of the human user in an interactive way to achieve an immersive experience. Since the early 2000s biologists have been using Virtual Environments (VE) to investigate the mechanisms of behavior in non-human animals including insect, fish, and mammals. VEs have become reliable tools for studying vision, cognition, and sensory-motor control in animals. In turn, the knowledge gained from studying such behaviors can be harnessed by researchers designing biologically inspired robots, smart sensors, and multi-agent artificial intelligence. VE for animals is becoming a widely used application of XR technology but such applications have not previously been reported in the technical literature related to XR. Biologists and computer scientists can benefit greatly from deepening interdisciplinary research in this emerging field and together we can develop new methods for conducting fundamental research in behavioral sciences and engineering. To support our argument we present this review which provides an overview of animal behavior experiments conducted in virtual environments. △ Less","2 January, 2020",https://arxiv.org/pdf/1912.12763
Deep Transfer Learning Based Downlink Channel Prediction for FDD Massive MIMO Systems,Yuwen Yang;Feifei Gao;Zhimeng Zhong;Bo Ai;Ahmed Alkhateeb,"Artificial intelligence (AI) based downlink channel state information (CSI) prediction for frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems has attracted growing attention recently. However, existing works focus on the downlink CSI prediction for the users under a given environment and is hard to adapt to users in new environment especially when labeled data is limited. To address this issue, we formulate the downlink channel prediction as a deep transfer learning (DTL) problem, where each learning task aims to predict the downlink CSI from the uplink CSI for one single environment. Specifically, we develop the direct-transfer algorithm based on the fully-connected neural network architecture, where the network is trained on the data from all previous environments in the manner of classical deep learning and is then fine-tuned for new environments. To further improve the transfer efficiency, we propose the meta-learning algorithm that trains the network by alternating inner-task and across-task updates and then adapts to a new environment with a small number of labeled data. Simulation results show that the direct-transfer algorithm achieves better performance than the deep learning algorithm, which implies that the transfer learning benefits the downlink channel prediction in new environments. Moreover, the meta-learning algorithm significantly outperforms the direct-transfer algorithm in terms of both prediction accuracy and stability, which validates its effectiveness and superiority. △ Less","7 September, 2020",https://arxiv.org/pdf/1912.12265
Pruning Deep Convolutional Neural Networks Architectures with Evolution Strategy,Francisco Erivaldo Fernandes Junior;Gary G. Yen,"Currently, Deep Convolutional Neural Networks (DCNNs) are used to solve all kinds of problems in the field of machine learning and artificial intelligence due to their learning and adaptation capabilities. However, most successful DCNN models have a high computational complexity making them difficult to deploy on mobile or embedded platforms. This problem has prompted many researchers to develop algorithms and approaches to help reduce the computational complexity of such models. One of them is called filter pruning, where convolution filters are eliminated to reduce the number of parameters and, consequently, the computational complexity of the given model. In the present work, we propose a novel algorithm to perform filter pruning by using Multi-Objective Evolution Strategy (ES) algorithm, called DeepPruningES. Our approach avoids the need for using any knowledge during the pruning procedure and helps decision-makers by returning three pruned CNN models with different trade-offs between performance and computational complexity. We show that DeepPruningES can significantly reduce a model's computational complexity by testing it on three DCNN architectures: Convolutional Neural Networks (CNNs), Residual Neural Networks (ResNets), and Densely Connected Neural Networks (DenseNets). △ Less","30 November, 2020",https://arxiv.org/pdf/1912.11527
Where Are We? Using Scopus to Map the Literature at the Intersection Between Artificial Intelligence and Research on Crime,Gian Maria Campedelli,"Research on Artificial Intelligence (AI) applications has spread over many scientific disciplines. Scientists have tested the power of intelligent algorithms developed to predict (or learn from) natural, physical and social phenomena. This also applies to crime-related research problems. Nonetheless, studies that map the current state of the art at the intersection between AI and crime are lacking. What are the current research trends in terms of topics in this area? What is the structure of scientific collaboration when considering works investigating criminal issues using machine learning, deep learning, and AI in general? What are the most active countries in this specific scientific sphere? Using data retrieved from the Scopus database, this work quantitatively analyzes 692 published works at the intersection between AI and crime employing network science to respond to these questions. Results show that researchers are mainly focusing on cyber-related criminal topics and that relevant themes such as algorithmic discrimination, fairness, and ethics are considerably overlooked. Furthermore, data highlight the extremely disconnected structure of co-authorship networks. Such disconnectedness may represent a substantial obstacle to a more solid community of scientists interested in these topics. Additionally, the graph of scientific collaboration indicates that countries that are more prone to engage in international partnerships are generally less central in the network. This means that scholars working in highly productive countries (e.g. the United States, China) tend to mostly collaborate domestically. Finally, current issues and future developments within this scientific area are also discussed. △ Less","6 August, 2020",https://arxiv.org/pdf/1912.11084
Questions to Guide the Future of Artificial Intelligence Research,Jordan Ott,"The field of machine learning has focused, primarily, on discretized sub-problems (i.e. vision, speech, natural language) of intelligence. While neuroscience tends to be observation heavy, providing few guiding theories. It is unlikely that artificial intelligence will emerge through only one of these disciplines. Instead, it is likely to be some amalgamation of their algorithmic and observational findings. As a result, there are a number of problems that should be addressed in order to select the beneficial aspects of both fields. In this article, we propose leading questions to guide the future of artificial intelligence research. There are clear computational principles on which the brain operates. The problem is finding these computational needles in a haystack of biological complexity. Biology has clear constraints but by not using it as a guide we are constraining ourselves. △ Less","10 March, 2020",https://arxiv.org/pdf/1912.10305
Learning to grow: control of material self-assembly using evolutionary reinforcement learning,Stephen Whitelam;Isaac Tamblyn,"We show that neural networks trained by evolutionary reinforcement learning can enact efficient molecular self-assembly protocols. Presented with molecular simulation trajectories, networks learn to change temperature and chemical potential in order to promote the assembly of desired structures or choose between competing polymorphs. In the first case, networks reproduce in a qualitative sense the results of previously-known protocols, but faster and with higher fidelity; in the second case they identify strategies previously unknown, from which we can extract physical insight. Networks that take as input the elapsed time of the simulation or microscopic information from the system are both effective, the latter more so. The evolutionary scheme we have used is simple to implement and can be applied to a broad range of examples of experimental self-assembly, whether or not one can monitor the experiment as it proceeds. Our results have been achieved with no human input beyond the specification of which order parameter to promote, pointing the way to the design of synthesis protocols by artificial intelligence. △ Less","28 May, 2020",https://arxiv.org/pdf/1912.08333
On the Explanation of Machine Learning Predictions in Clinical Gait Analysis,Djordje Slijepcevic;Fabian Horst;Sebastian Lapuschkin;Anna-Maria Raberger;Matthias Zeppelzauer;Wojciech Samek;Christian Breiteneder;Wolfgang I. Schöllhorn;Brian Horsak,"Machine learning (ML) is increasingly used to support decision-making in the healthcare sector. While ML approaches provide promising results with regard to their classification performance, most share a central limitation, namely their black-box character. Motivated by the interest to understand the functioning of ML models, methods from the field of Explainable Artificial Intelligence (XAI) have recently become important. This article investigates the usefulness of XAI methods in clinical gait classification. For this purpose, predictions of state-of-the-art classification methods are explained with an established XAI method, i.e., Layer-wise Relevance Propagation (LRP). We propose to evaluate the obtained explanations with two complementary approaches: a statistical analysis of the underlying data using Statistical Parametric Mapping and a qualitative evaluation by a clinical expert. A gait dataset comprising ground reaction force measurements from 132 patients with different lower-body gait disorders and 62 healthy controls is utilized. We investigate several gait classification tasks, employ multiple classification methods, and analyze the impact of data normalization and different signal components for classification performance and explanation quality. Our experiments show that explanations obtained by LRP exhibit promising statistical properties concerning inter-class discriminativity and are also in line with clinically relevant biomechanical gait characteristics. △ Less","19 August, 2020",https://arxiv.org/pdf/1912.07737
Improved Explanatory Efficacy on Human Affect and Workload through Interactive Process in Artificial Intelligence,Byung Hyung Kim;Seunghun Koh;Sejoon Huh;Sungho Jo;Sunghee Choi,"Despite recent advances in the field of explainable artificial intelligence systems, a concrete quantitative measure for evaluating the usability of such systems is nonexistent. Ensuring the success of an explanatory interface in interacting with users requires a cyclic, symbiotic relationship between human and artificial intelligence. We, therefore, propose explanatory efficacy, a novel metric for evaluating the strength of the cyclic relationship the interface exhibits. Furthermore, in a user study, we evaluated the perceived affect and workload and recorded the EEG signals of our participants as they interacted with our custom-built, iterative explanatory interface to build personalized recommendation systems. We found that systems for perceptually driven iterative tasks with greater explanatory efficacy are characterized by statistically significant hemispheric differences in neural signals with 62.4% accuracy, indicating the feasibility of neural correlates as a measure of explanatory efficacy. These findings are beneficial for researchers who aim to study the circular ecosystem of the human-artificial intelligence partnership. △ Less","22 October, 2020",https://arxiv.org/pdf/1912.07416
AutoAIViz: Opening the Blackbox of Automated Artificial Intelligence with Conditional Parallel Coordinates,Daniel Karl I. Weidele;Justin D. Weisz;Eno Oduor;Michael Muller;Josh Andres;Alexander Gray;Dakuo Wang,"Artificial Intelligence (AI) can now automate the algorithm selection, feature engineering, and hyperparameter tuning steps in a machine learning workflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve data scientists from the tedious manual work. However, today's AutoAI systems often present only limited to no information about the process of how they select and generate model results. Thus, users often do not understand the process, neither do they trust the outputs. In this short paper, we provide a first user evaluation by 10 data scientists of an experimental system, AutoAIViz, that aims to visualize AutoAI's model generation process. We find that the proposed system helps users to complete the data science tasks, and increases their understanding, toward the goal of increasing trust in the AutoAI system. △ Less","17 January, 2020",https://arxiv.org/pdf/1912.06723
Blockchain Intelligence: When Blockchain Meets Artificial Intelligence,Zibin Zheng;Hong-Ning Dai;Jiajing Wu,"Blockchain is gaining extensive attention due to its provision of secure and decentralized resource sharing manner. However, the incumbent blockchain systems also suffer from a number of challenges in operational maintenance, quality assurance of smart contracts and malicious behaviour detection of blockchain data. The recent advances in artificial intelligence bring the opportunities in overcoming the above challenges. The integration of blockchain with artificial intelligence can be beneficial to enhance current blockchain systems. This article presents an introduction of the convergence of blockchain and artificial intelligence (namely blockchain intelligence). This article also gives a case study to further demonstrate the feasibility of blockchain intelligence and point out the future directions. △ Less","3 April, 2020",https://arxiv.org/pdf/1912.06485
Extending Machine Language Models toward Human-Level Language Understanding,James L. McClelland;Felix Hill;Maja Rudolph;Jason Baldridge;Hinrich Schütze,"Language is crucial for human intelligence, but what exactly is its role? We take language to be a part of a system for understanding and communicating about situations. The human ability to understand and communicate about situations emerges gradually from experience and depends on domain-general principles of biological neural networks: connection-based learning, distributed representation, and context-sensitive, mutual constraint satisfaction-based processing. Current artificial language processing systems rely on the same domain general principles, embodied in artificial neural networks. Indeed, recent progress in this field depends on \emph{query-based attention}, which extends the ability of these systems to exploit context and has contributed to remarkable breakthroughs. Nevertheless, most current models focus exclusively on language-internal tasks, limiting their ability to perform tasks that depend on understanding situations. These systems also lack memory for the contents of prior situations outside of a fixed contextual span. We describe the organization of the brain's distributed understanding system, which includes a fast learning system that addresses the memory problem. We sketch a framework for future models of understanding drawing equally on cognitive neuroscience and artificial intelligence and exploiting query-based attention. We highlight relevant current directions and consider further developments needed to fully capture human-level language understanding in a computational system. △ Less","4 July, 2020",https://arxiv.org/pdf/1912.05877
Artificial Intelligence-Enabled Intelligent 6G Networks,Helin Yang;Arokiaswami Alphones;Zehui Xiong;Dusit Niyato;Jun Zhao;Kaishun Wu,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (AI) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an AI-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of AI techniques for 6G networks and elaborate how to employ the AI techniques to efficiently and effectively optimize the network performance, including AI-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. Moreover, we highlight important future research directions and potential solutions for AI-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management. △ Less","5 December, 2020",https://arxiv.org/pdf/1912.05744
AI2D-RST: A multimodal corpus of 1000 primary school science diagrams,Tuomo Hiippala;Malihe Alikhani;Jonas Haverinen;Timo Kalliokoski;Evanfiya Logacheva;Serafina Orekhova;Aino Tuomainen;Matthew Stone;John A. Bateman,"This article introduces AI2D-RST, a multimodal corpus of 1000 English-language diagrams that represent topics in primary school natural sciences, such as food webs, life cycles, moon phases and human physiology. The corpus is based on the Allen Institute for Artificial Intelligence Diagrams (AI2D) dataset, a collection of diagrams with crowd-sourced descriptions, which was originally developed to support research on automatic diagram understanding and visual question answering. Building on the segmentation of diagram layouts in AI2D, the AI2D-RST corpus presents a new multi-layer annotation schema that provides a rich description of their multimodal structure. Annotated by trained experts, the layers describe (1) the grouping of diagram elements into perceptual units, (2) the connections set up by diagrammatic elements such as arrows and lines, and (3) the discourse relations between diagram elements, which are described using Rhetorical Structure Theory (RST). Each annotation layer in AI2D-RST is represented using a graph. The corpus is freely available for research and teaching. △ Less","20 March, 2020",https://arxiv.org/pdf/1912.03879
Human-to-AI Coach: Improving Human Inputs to AI Systems,Johannes Schneider,"Humans increasingly interact with Artificial intelligence(AI) systems. AI systems are optimized for objectives such as minimum computation or minimum error rate in recognizing and interpreting inputs from humans. In contrast, inputs created by humans are often treated as a given. We investigate how inputs of humans can be altered to reduce misinterpretation by the AI system and to improve efficiency of input generation for the human while altered inputs should remain as similar as possible to the original inputs. These objectives result in trade-offs that are analyzed for a deep learning system classifying handwritten digits. To create examples that serve as demonstrations for humans to improve, we develop a model based on a conditional convolutional autoencoder (CCAE). Our quantitative and qualitative evaluation shows that in many occasions the generated proposals lead to lower error rates, require less effort to create and differ only modestly from the original samples. △ Less","9 March, 2020",https://arxiv.org/pdf/1912.03652
Tools for Mathematical Ludology,Paul Riggins;David McPherson,"We propose the study of mathematical ludology, which aims to formally interrogate questions of interest to game studies and game design in particular. The goal is to extend our mathematical understanding of complex games beyond decision-making---the typical focus of game theory and artificial intelligence efforts---to explore other aspects such as game mechanics, structure, relationships between games, and connections between game rules and user-interfaces, as well as exploring related gameplay phenomena and typical player behavior. In this paper, we build a basic foundation for this line of study by developing a hierarchy of game descriptions, mathematical formalism to compactly describe complex discrete games, and equivalence relations on the space of game systems. △ Less","6 January, 2020",https://arxiv.org/pdf/1912.03295
EdNet: A Large-Scale Hierarchical Dataset in Education,Youngduck Choi;Youngnam Lee;Dongmin Shin;Junghyun Cho;Seoyon Park;Seewoo Lee;Jineon Baek;Chan Bae;Byungsoo Kim;Jaewe Heo,"With advances in Artificial Intelligence in Education (AIEd) and the ever-growing scale of Interactive Educational Systems (IESs), data-driven approach has become a common recipe for various tasks such as knowledge tracing and learning path recommendation. Unfortunately, collecting real students' interaction data is often challenging, which results in the lack of public large-scale benchmark dataset reflecting a wide variety of student behaviors in modern IESs. Although several datasets, such as ASSISTments, Junyi Academy, Synthetic and STATICS, are publicly available and widely used, they are not large enough to leverage the full potential of state-of-the-art data-driven models and limits the recorded behaviors to question-solving activities. To this end, we introduce EdNet, a large-scale hierarchical dataset of diverse student activities collected by Santa, a multi-platform self-study solution equipped with artificial intelligence tutoring system. EdNet contains 131,441,538 interactions from 784,309 students collected over more than 2 years, which is the largest among the ITS datasets released to the public so far. Unlike existing datasets, EdNet provides a wide variety of student actions ranging from question-solving to lecture consumption and item purchasing. Also, EdNet has a hierarchical structure where the student actions are divided into 4 different levels of abstractions. The features of EdNet are domain-agnostic, allowing EdNet to be extended to different domains easily. The dataset is publicly released under Creative Commons Attribution-NonCommercial 4.0 International license for research purposes. We plan to host challenges in multiple AIEd tasks with EdNet to provide a common ground for the fair comparison between different state of the art models and encourage the development of practical and effective methods. △ Less","1 July, 2020",https://arxiv.org/pdf/1912.03072
Differentiation of Blackbox Combinatorial Solvers,Marin Vlastelica;Anselm Paulus;Vít Musil;Georg Martius;Michal Rolínek,"Achieving fusion of deep learning with combinatorial algorithms promises transformative changes to artificial intelligence. One possible approach is to introduce combinatorial building blocks into neural networks. Such end-to-end architectures have the potential to tackle combinatorial problems on raw input data such as ensuring global consistency in multi-object tracking or route planning on maps in robotics. In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions. We provide both theoretical and experimental backing. In particular, we incorporate the Gurobi MIP solver, Blossom V algorithm, and Dijkstra's algorithm into architectures that extract suitable features from raw inputs for the traveling salesman problem, the min-cost perfect matching problem and the shortest path problem. The code is available at https://github.com/martius-lab/blackbox-backprop. △ Less","16 February, 2020",https://arxiv.org/pdf/1912.02175
Mining Domain Knowledge: Improved Framework towards Automatically Standardizing Anatomical Structure Nomenclature in Radiotherapy,Qiming Yang;Hongyang Chao;Dan Nguyen;Steve Jiang,"The automatic standardization of nomenclature for anatomical structures in radiotherapy (RT) clinical data is a critical prerequisite for data curation and data-driven research in the era of big data and artificial intelligence, but it is currently an unmet need. Existing methods either cannot handle cross-institutional datasets or suffer from heavy imbalance and poor-quality delineation in clinical RT datasets. To solve these problems, we propose an automated structure nomenclature standardization framework, 3D Non-local Network with Voting (3DNNV). This framework consists of an improved data processing strategy, namely, adaptive sampling and adaptive cropping (ASAC) with voting, and an optimized feature extraction module. The framework simulates clinicians' domain knowledge and recognition mechanisms to identify small-volume organs at risk (OARs) with heavily imbalanced data better than other methods. We used partial data from an open-source head-and-neck cancer dataset to train the model, then tested the model on three cross-institutional datasets to demonstrate its generalizability. 3DNNV outperformed the baseline model, achieving higher average true positive rates (TPR) overall categories on the three test datasets (+8.27%, +2.39%, and +5.53%, respectively). More importantly, the 3DNNV outperformed the baseline on the test dataset, 28.63% to 91.17%, in terms of F1 score for a small-volume OAR with only 9 training samples. The results show that 3DNNV can be applied to identify OARs, even error-prone ones. Furthermore, we discussed the limitations and applicability of the framework in practical scenarios. The framework we developed can assist in standardizing structure nomenclature to facilitate data-driven clinical research in cancer radiotherapy. △ Less","10 May, 2020",https://arxiv.org/pdf/1912.02084
Robust and Secure Wireless Communications via Intelligent Reflecting Surfaces,Xianghao Yu;Dongfang Xu;Ying Sun;Derrick Wing Kwan Ng;Robert Schober,"In this paper, intelligent reflecting surfaces (IRSs) are employed to enhance the physical layer security in a challenging radio environment. In particular, a multi-antenna access point (AP) has to serve multiple single-antenna legitimate users, which do not have line-of-sight communication links, in the presence of multiple multi-antenna potential eavesdroppers whose channel state information (CSI) is not perfectly known. Artificial noise (AN) is transmitted from the AP to deliberately impair the eavesdropping channels for security provisioning. We investigate the joint design of the beamformers and AN covariance matrix at the AP and the phase shifters at the IRSs for maximization of the system sum-rate while limiting the maximum information leakage to the potential eavesdroppers. To this end, we formulate a robust nonconvex optimization problem taking into account the impact of the imperfect CSI of the eavesdropping channels. To address the non-convexity of the optimization problem, an efficient algorithm is developed by capitalizing on alternating optimization, a penalty-based approach, successive convex approximation, and semidefinite relaxation. Simulation results show that IRSs can significantly improve the system secrecy performance compared to conventional architectures without IRS. Furthermore, our results unveil that, for physical layer security, uniformly distributing the reflecting elements among multiple IRSs is preferable over deploying them at a single IRS. △ Less","20 April, 2020",https://arxiv.org/pdf/1912.01497
A Free Lunch in Generating Datasets: Building a VQG and VQA System with Attention and Humans in the Loop,Jihyeon Lee;Sho Arora,"Despite their importance in training artificial intelligence systems, large datasets remain challenging to acquire. For example, the ImageNet dataset required fourteen million labels of basic human knowledge, such as whether an image contains a chair. Unfortunately, this knowledge is so simple that it is tedious for human annotators but also tacit enough such that they are necessary. However, human collaborative efforts for tasks like labeling massive amounts of data are costly, inconsistent, and prone to failure, and this method does not resolve the issue of the resulting dataset being static in nature. What if we asked people questions they want to answer and collected their responses as data? This would mean we could gather data at a much lower cost, and expanding a dataset would simply become a matter of asking more questions. We focus on the task of Visual Question Answering (VQA) and propose a system that uses Visual Question Generation (VQG) to produce questions, asks them to social media users, and collects their responses. We present two models that can then parse clean answers from the noisy human responses significantly better than our baselines, with the goal of eventually incorporating the answers into a Visual Question Answering (VQA) dataset. By demonstrating how our system can collect large amounts of data at little to no cost, we envision similar systems being used to improve performance on other tasks in the future. △ Less","28 August, 2020",https://arxiv.org/pdf/1912.00124
Artificial Intelligence in Glioma Imaging: Challenges and Advances,Weina Jin;Mostafa Fatehi;Kumar Abhishek;Mayur Mallya;Brian Toyota;Ghassan Hamarneh,"Primary brain tumors including gliomas continue to pose significant management challenges to clinicians. While the presentation, the pathology, and the clinical course of these lesions are variable, the initial investigations are usually similar. Patients who are suspected to have a brain tumor will be assessed with computed tomography (CT) and magnetic resonance imaging (MRI). The imaging findings are used by neurosurgeons to determine the feasibility of surgical resection and plan such an undertaking. Imaging studies are also an indispensable tool in tracking tumor progression or its response to treatment. As these imaging studies are non-invasive, relatively cheap and accessible to patients, there have been many efforts over the past two decades to increase the amount of clinically-relevant information that can be extracted from brain imaging. Most recently, artificial intelligence (AI) techniques have been employed to segment and characterize brain tumors, as well as to detect progression or treatment-response. However, the clinical utility of such endeavours remains limited due to challenges in data collection and annotation, model training, and the reliability of AI-generated information. We provide a review of recent advances in addressing the above challenges. First, to overcome the challenge of data paucity, different image imputation and synthesis techniques along with annotation collection efforts are summarized. Next, various training strategies are presented to meet multiple desiderata, such as model performance, generalization ability, data privacy protection, and learning with sparse annotations. Finally, standardized performance evaluation and model interpretability methods have been reviewed. We believe that these technical approaches will facilitate the development of a fully-functional AI tool in the clinical care of patients with gliomas. △ Less","10 April, 2020",https://arxiv.org/pdf/1911.12886
Type Safety with JSON Subschema,Andrew Habib;Avraham Shinnar;Martin Hirzel;Michael Pradel,"JSON is a popular data format used pervasively in web APIs, cloud computing, NoSQL databases, and increasingly also machine learning. JSON Schema is a language for declaring the structure of valid JSON data. There are validators that can decide whether a JSON document is valid with respect to a schema. Unfortunately, like all instance-based testing, these validators can only show the presence and never the absence of a bug. This paper presents a complementary technique: JSON subschema checking, which can be used for static type checking with JSON Schema. Deciding whether one schema is a subschema of another is non-trivial because of the richness of the JSON Schema specification language. Given a pair of schemas, our approach first canonicalizes and simplifies both schemas, then decides the subschema question on the canonical forms, dispatching simpler subschema queries to type-specific checkers. We apply an implementation of our subschema checking algorithm to 8,548 pairs of real-world JSON schemas from different domains, demonstrating that it can decide the subschema question for most schema pairs and is always correct for schema pairs that it can decide. We hope that our work will bring more static guarantees to hard-to-debug domains, such as cloud computing and artificial intelligence. △ Less","18 May, 2020",https://arxiv.org/pdf/1911.12651
Artificial Intelligence-Based Image Classification for Diagnosis of Skin Cancer: Challenges and Opportunities,Manu Goyal;Thomas Knackstedt;Shaofeng Yan;Saeed Hassanpour,"Recently, there has been great interest in developing Artificial Intelligence (AI) enabled computer-aided diagnostics solutions for the diagnosis of skin cancer. With the increasing incidence of skin cancers, low awareness among a growing population, and a lack of adequate clinical expertise and services, there is an immediate need for AI systems to assist clinicians in this domain. A large number of skin lesion datasets are available publicly, and researchers have developed AI-based image classification solutions, particularly deep learning algorithms, to distinguish malignant skin lesions from benign lesions in different image modalities such as dermoscopic, clinical, and histopathology images. Despite the various claims of AI systems achieving higher accuracy than dermatologists in the classification of different skin lesions, these AI systems are still in the very early stages of clinical application in terms of being ready to aid clinicians in the diagnosis of skin cancers. In this review, we discuss advancements in the digital image-based AI solutions for the diagnosis of skin cancer, along with some challenges and future opportunities to improve these AI systems to support dermatologists and enhance their ability to diagnose skin cancer. △ Less","20 June, 2020",https://arxiv.org/pdf/1911.11872
Injecting Prior Knowledge into Image Caption Generation,Arushi Goel;Basura Fernando;Thanh-Son Nguyen;Hakan Bilen,"Automatically generating natural language descriptions from an image is a challenging problem in artificial intelligence that requires a good understanding of the visual and textual signals and the correlations between them. The state-of-the-art methods in image captioning struggles to approach human level performance, especially when data is limited. In this paper, we propose to improve the performance of the state-of-the-art image captioning models by incorporating two sources of prior knowledge: (i) a conditional latent topic attention, that uses a set of latent variables (topics) as an anchor to generate highly probable words and, (ii) a regularization technique that exploits the inductive biases in syntactic and semantic structure of captions and improves the generalization of image captioning models. Our experiments validate that our method produces more human interpretable captions and also leads to significant improvements on the MSCOCO dataset in both the full and low data regimes. △ Less","6 August, 2020",https://arxiv.org/pdf/1911.10082
Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response,Sheikh Rabiul Islam;William Eberle;Sheikh K. Ghafoor;Ambareen Siraj;Mike Rogers,"Artificial Intelligence (AI) has become an integral part of modern-day security solutions for its ability to learn very complex functions and handling ""Big Data"". However, the lack of explainability and interpretability of successful AI models is a key stumbling block when trust in a model's prediction is critical. This leads to human intervention, which in turn results in a delayed response or decision. While there have been major advancements in the speed and performance of AI-based intrusion detection systems, the response is still at human speed when it comes to explaining and interpreting a specific prediction or decision. In this work, we infuse popular domain knowledge (i.e., CIA principles) in our model for better explainability and validate the approach on a network intrusion detection test case. Our experimental results suggest that the infusion of domain knowledge provides better explainability as well as a faster decision or response. In addition, the infused domain knowledge generalizes the model to work well with unknown attacks, as well as opens the path to adapt to a large stream of network traffic from numerous IoT devices. △ Less","22 February, 2020",https://arxiv.org/pdf/1911.09853
"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",Julian Schrittwieser;Ioannis Antonoglou;Thomas Hubert;Karen Simonyan;Laurent Sifre;Simon Schmitt;Arthur Guez;Edward Lockhart;Demis Hassabis;Thore Graepel;Timothy Lillicrap;David Silver,"Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess and Go, where a perfect simulator is available. However, in real-world problems the dynamics governing the environment are often complex and unknown. In this work we present the MuZero algorithm which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. MuZero learns a model that, when applied iteratively, predicts the quantities most directly relevant to planning: the reward, the action-selection policy, and the value function. When evaluated on 57 different Atari games - the canonical video game environment for testing AI techniques, in which model-based planning approaches have historically struggled - our new algorithm achieved a new state of the art. When evaluated on Go, chess and shogi, without any knowledge of the game rules, MuZero matched the superhuman performance of the AlphaZero algorithm that was supplied with the game rules. △ Less","21 February, 2020",https://arxiv.org/pdf/1911.08265
AI-based Pilgrim Detection using Convolutional Neural Networks,Marwa Ben Jabra;Adel Ammar;Anis Koubaa;Omar Cheikhrouhou;Habib Hamam,"Pilgrimage represents the most important Islamic religious gathering in the world where millions of pilgrims visit the holy places of Makkah and Madinah to perform their rituals. The safety and security of pilgrims is the highest priority for the authorities. In Makkah, 5000 cameras are spread around the holy for monitoring pilgrims, but it is almost impossible to track all events by humans considering the huge number of images collected every second. To address this issue, we propose to use artificial intelligence technique based on deep learning and convolution neural networks to detect and identify Pilgrims and their features. For this purpose, we built a comprehensive dataset for the detection of pilgrims and their genders. Then, we develop two convolutional neural networks based on YOLOv3 and Faster-RCNN for the detection of Pilgrims. Experiments results show that Faster RCNN with Inception v2 feature extractor provides the best mean average precision over all classes of 51%. △ Less","18 February, 2020",https://arxiv.org/pdf/1911.07509
ViWi: A Deep Learning Dataset Framework for Vision-Aided Wireless Communications,Muhammad Alrabeiah;Andrew Hredzak;Zhenhao Liu;Ahmed Alkhateeb,"The growing role that artificial intelligence and specifically machine learning is playing in shaping the future of wireless communications has opened up many new and intriguing research directions. This paper motivates the research in the novel direction of \textit{vision-aided wireless communications}, which aims at leveraging visual sensory information in tackling wireless communication problems. Like any new research direction driven by machine learning, obtaining a development dataset poses the first and most important challenge to vision-aided wireless communications. This paper addresses this issue by introducing the Vision-Wireless (ViWi) dataset framework. It is developed to be a parametric, systematic, and scalable data generation framework. It utilizes advanced 3D-modeling and ray-tracing softwares to generate high-fidelity synthetic wireless and vision data samples for the same scenes. The result is a framework that does not only offer a way to generate training and testing datasets but helps provide a common ground on which the quality of different machine learning-powered solutions could be assessed. △ Less","22 April, 2020",https://arxiv.org/pdf/1911.06257
Cyber Risk at the Edge: Current and future trends on Cyber Risk Analytics and Artificial Intelligence in the Industrial Internet of Things and Industry 4.0 Supply Chains,Petar Radanliev;David De Roure;Kevin Page;Jason Nurse;Rafael Mantilla Montalvo;Omar Santos;La Treall Maddox;Peter Burnap,"Digital technologies have changed the way supply chain operations are structured. In this article, we conduct systematic syntheses of literature on the impact of new technologies on supply chains and the related cyber risks. A taxonomic/cladistic approach is used for the evaluations of progress in the area of supply chain integration in the Industrial Internet of Things and Industry 4.0, with a specific focus on the mitigation of cyber risks. An analytical framework is presented, based on a critical assessment with respect to issues related to new types of cyber risk and the integration of supply chains with new technologies. This paper identifies a dynamic and self-adapting supply chain system supported with Artificial Intelligence and Machine Learning (AI/ML) and real-time intelligence for predictive cyber risk analytics. The system is integrated into a cognition engine that enables predictive cyber risk analytics with real-time intelligence from IoT networks at the edge. This enhances capacities and assist in the creation of a comprehensive understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when AI/ML technologies are migrated to the periphery of IoT networks. △ Less","14 May, 2020",https://arxiv.org/pdf/1911.05726
"Multimodal Intelligence: Representation Learning, Information Fusion, and Applications",Chao Zhang;Zichao Yang;Xiaodong He;Li Deng,"Deep learning methods have revolutionized speech recognition, image recognition, and natural language processing since 2010. Each of these tasks involves a single modality in their input signals. However, many applications in the artificial intelligence field involve multiple modalities. Therefore, it is of broad interest to study the more difficult and complex problem of modeling and learning across multiple modalities. In this paper, we provide a technical review of available models and learning methods for multimodal intelligence. The main focus of this review is the combination of vision and natural language modalities, which has become an important topic in both the computer vision and natural language processing research communities. This review provides a comprehensive analysis of recent works on multimodal deep learning from three perspectives: learning multimodal representations, fusing multimodal signals at various levels, and multimodal applications. Regarding multimodal representation learning, we review the key concepts of embedding, which unify multimodal signals into a single vector space and thereby enable cross-modality signal processing. We also review the properties of many types of embeddings that are constructed and learned for general downstream tasks. Regarding multimodal fusion, this review focuses on special architectures for the integration of representations of unimodal signals for a particular task. Regarding applications, selected areas of a broad interest in the current literature are covered, including image-to-text caption generation, text-to-image generation, and visual question answering. We believe that this review will facilitate future studies in the emerging field of multimodal intelligence for related communities. △ Less","10 April, 2020",https://arxiv.org/pdf/1911.03977
Adaptivity in Adaptive Submodularity,Hossein Esfandiari;Amin Karbasi;Vahab Mirrokni,"Adaptive sequential decision making is one of the central challenges in machine learning and artificial intelligence. In such problems, the goal is to design an interactive policy that plans for an action to take, from a finite set of n actions, given some partial observations. It has been shown that in many applications such as active learning, robotics, sequential experimental design, and active detection, the utility function satisfies adaptive submodularity, a notion that generalizes the notion of diminishing returns to policies. In this paper, we revisit the power of adaptivity in maximizing an adaptive monotone submodular function. We propose an efficient semi adaptive policy that with O(\log n \times\log k) adaptive rounds of observations can achieve an almost tight 1-1/e-ε approximation guarantee with respect to an optimal policy that carries out k actions in a fully sequential manner. To complement our results, we also show that it is impossible to achieve a constant factor approximation with o(\log n) adaptive rounds. We also extend our result to the case of adaptive stochastic minimum cost coverage where the goal is to reach a desired utility Q with the cheapest policy. We first prove the conjecture of the celebrated work of Golovin and Krause by showing that the greedy policy achieves the asymptotically tight logarithmic approximation guarantee without resorting to stronger notions of adaptivity. We then propose a semi adaptive policy that provides the same guarantee in polylogarithmic adaptive rounds through a similar information-parallelism scheme. Our results shrink the adaptivity gap in adaptive submodular maximization by an exponential factor. △ Less","27 July, 2020",https://arxiv.org/pdf/1911.03620
"When Machine Learning Meets Wireless Cellular Networks: Deployment, Challenges, and Applications",Ursula Challita;Henrik A. Ryden;Hugo Tullberg,"Artificial intelligence (AI) powered wireless networks promise to revolutionize the conventional operation and structure of current networks from network design to infrastructure management, cost reduction, and user performance improvement. Empowering future networks with AI functionalities will enable a shift from reactive/incident driven operations to proactive/data-driven operations. This paper provides an overview on the integration of AI functionalities in 5G and beyond networks. Key factors for successful AI integration such as data, security, and explainable AI are highlighted. We also summarize the various types of network intelligence as well as machine learning based air interface in future networks. Use case examples for the application of AI to the wireless domain are then summarized. We highlight on applications to the physical layer, mobility management, wireless security, and localization. △ Less","1 May, 2020",https://arxiv.org/pdf/1911.03585
AITom: Open-source AI platform for cryo-electron tomography data analysis,Xiangrui Zeng;Min Xu,"Cryo-electron tomography (cryo-ET) is an emerging technology for the 3D visualization of structural organizations and interactions of subcellular components at near-native state and sub-molecular resolution. Tomograms captured by cryo-ET contain heterogeneous structures representing the complex and dynamic subcellular environment. Since the structures are not purified or fluorescently labeled, the spatial organization and interaction between both the known and unknown structures can be studied in their native environment. The rapid advances of cryo-electron tomography (cryo-ET) have generated abundant 3D cellular imaging data. However, the systematic localization, identification, segmentation, and structural recovery of the subcellular components require efficient and accurate large-scale image analysis methods. We introduce AITom, an open-source artificial intelligence platform for cryo-ET researchers. AITom provides many public as well as in-house algorithms for performing cryo-ET data analysis through both the traditional template-based or template-free approach and the deep learning approach. AITom also supports remote interactive analysis. Comprehensive tutorials for each analysis module are provided to guide the user through. We welcome researchers and developers to join this collaborative open-source software development project. Availability: https://github.com/xulabs/aitom △ Less","30 October, 2020",https://arxiv.org/pdf/1911.03044
SIMMC: Situated Interactive Multi-Modal Conversational Data Collection And Evaluation Platform,Paul A. Crook;Shivani Poddar;Ankita De;Semir Shafi;David Whitney;Alborz Geramifard;Rajen Subba,"As digital virtual assistants become ubiquitous, it becomes increasingly important to understand the situated behaviour of users as they interact with these assistants. To this end, we introduce SIMMC, an extension to ParlAI for multi-modal conversational data collection and system evaluation. SIMMC simulates an immersive setup, where crowd workers are able to interact with environments constructed in AI Habitat or Unity while engaging in a conversation. The assistant in SIMMC can be a crowd worker or Artificial Intelligent (AI) agent. This enables both (i) a multi-player / Wizard of Oz setting for data collection, or (ii) a single player mode for model / system evaluation. We plan to open-source a situated conversational data-set collected on this platform for the Conversational AI research community. △ Less","30 January, 2020",https://arxiv.org/pdf/1911.02690
Textual analysis of artificial intelligence manuscripts reveals features associated with peer review outcome,Philippe Vincent-Lamarre;Vincent Larivière,"We analysed a dataset of scientific manuscripts that were submitted to various conferences in artificial intelligence. We performed a combination of semantic, lexical and psycholinguistic analyses of the full text of the manuscripts and compared them with the outcome of the peer review process. We found that accepted manuscripts scored lower than rejected manuscripts on two indicators of readability, and that they also used more scientific and artificial intelligence jargon. We also found that accepted manuscripts were written with words that are less frequent, that are acquired at an older age, and that are more abstract than rejected manuscripts. The analysis of references included in the manuscripts revealed that the subset of accepted submissions were more likely to cite the same publications. This finding was echoed by pairwise comparisons of the word content of the manuscripts (i.e. an indicator or semantic similarity), which were more similar in the subset of accepted manuscripts. Finally, we predicted the peer review outcome of manuscripts with their word content, with words related to machine learning and neural networks positively related with acceptance, whereas words related to logic, symbolic processing and knowledge-based systems negatively related with acceptance. △ Less","3 March, 2020",https://arxiv.org/pdf/1911.02648
CloudifierNet -- Deep Vision Models for Artificial Image Processing,Andrei Damian;Laurentiu Piciu;Alexandru Purdila;Nicolae Tapus,"Today, more and more, it is necessary that most applications and documents developed in previous or current technologies to be accessible online on cloud-based infrastructures. That is why the migration of legacy systems including their hosts of documents to new technologies and online infrastructures, using modern Artificial Intelligence techniques, is absolutely necessary. With the advancement of Artificial Intelligence and Deep Learning with its multitude of applications, a new area of research is emerging - that of automated systems development and maintenance. The underlying work objective that led to this paper aims to research and develop truly intelligent systems able to analyze user interfaces from various sources and generate real and usable inferences ranging from architecture analysis to actual code generation. One key element of such systems is that of artificial scene detection and analysis based on deep learning computer vision systems. Computer vision models and particularly deep directed acyclic graphs based on convolutional modules are generally constructed and trained based on natural images datasets. Due to this fact, the models will develop during the training process natural image feature detectors apart from the base graph modules that will learn basic primitive features. In the current paper, we will present the base principles of a deep neural pipeline for computer vision applied to artificial scenes (scenes generated by user interfaces or similar). Finally, we will present the conclusions based on experimental development and benchmarking against state-of-the-art transfer-learning implemented deep vision models. △ Less","28 July, 2020",https://arxiv.org/pdf/1911.01346
"Digital Twin: Enabling Technologies, Challenges and Open Research",Aidan Fuller;Zhong Fan;Charles Day;Chris Barlow,"Digital Twin technology is an emerging concept that has become the centre of attention for industry and, in more recent years, academia. The advancements in industry 4.0 concepts have facilitated its growth, particularly in the manufacturing industry. The Digital Twin is defined extensively but is best described as the effortless integration of data between a physical and virtual machine in either direction. The challenges, applications, and enabling technologies for Artificial Intelligence, Internet of Things (IoT) and Digital Twins are presented. A review of publications relating to Digital Twins is performed, producing a categorical review of recent papers. The review has categorised them by research areas: manufacturing, healthcare and smart cities, discussing a range of papers that reflect these areas and the current state of research. The paper provides an assessment of the enabling technologies, challenges and open research for Digital Twins. △ Less","18 June, 2020",https://arxiv.org/pdf/1911.01276
Evolving Structures in Complex Systems,Hugo Cisneros;Josef Sivic;Tomas Mikolov,"In this paper we propose an approach for measuring growth of complexity of emerging patterns in complex systems such as cellular automata. We discuss several ways how a metric for measuring the complexity growth can be defined. This includes approaches based on compression algorithms and artificial neural networks. We believe such a metric can be useful for designing systems that could exhibit open-ended evolution, which itself might be a prerequisite for development of general artificial intelligence. We conduct experiments on 1D and 2D grid worlds and demonstrate that using the proposed metric we can automatically construct computational models with emerging properties similar to those found in the Conway's Game of Life, as well as many other emergent phenomena. Interestingly, some of the patterns we observe resemble forms of artificial life. Our metric of structural complexity growth can be applied to a wide range of complex systems, as it is not limited to cellular automata. △ Less","18 March, 2020",https://arxiv.org/pdf/1911.01086
Explaining the Predictions of Any Image Classifier via Decision Trees,Sheng Shi;Xinfeng Zhang;Wei Fan,"Despite outstanding contribution to the significant progress of Artificial Intelligence (AI), deep learning models remain mostly black boxes, which are extremely weak in explainability of the reasoning process and prediction results. Explainability is not only a gateway between AI and society but also a powerful tool to detect flaws in the model and biases in the data. Local Interpretable Model-agnostic Explanation (LIME) is a recent approach that uses an interpretable model to form a local explanation for the individual prediction result. The current implementation of LIME adopts the linear regression as its interpretable function. However, being so restricted and usually over-simplifying the relationships, linear models fail in situations where nonlinear associations and interactions exist among features and prediction results. This paper implements a decision Tree-based LIME approach, which uses a decision tree model to form an interpretable representation that is locally faithful to the original model. Tree-LIME approach can capture nonlinear interactions among features in the data and creates plausible explanations. Various experiments show that the Tree-LIME explanation of multiple black-box models can achieve more reliable performance in terms of understandability, fidelity, and efficiency. △ Less","9 February, 2020",https://arxiv.org/pdf/1911.01058
Mean-field inference methods for neural networks,Marylou Gabrié,"Machine learning algorithms relying on deep neural networks recently allowed a great leap forward in artificial intelligence. Despite the popularity of their applications, the efficiency of these algorithms remains largely unexplained from a theoretical point of view. The mathematical description of learning problems involves very large collections of interacting random variables, difficult to handle analytically as well as numerically. This complexity is precisely the object of study of statistical physics. Its mission, originally pointed towards natural systems, is to understand how macroscopic behaviors arise from microscopic laws. Mean-field methods are one type of approximation strategy developed in this view. We review a selection of classical mean-field methods and recent progress relevant for inference in neural networks. In particular, we remind the principles of derivations of high-temperature expansions, the replica method and message passing algorithms, highlighting their equivalences and complementarities. We also provide references for past and current directions of research on neural networks relying on mean-field methods. △ Less","5 March, 2020",https://arxiv.org/pdf/1911.00890
PODNet: A Neural Network for Discovery of Plannable Options,Ritwik Bera;Vinicius G. Goecks;Gregory M. Gremillion;John Valasek;Nicholas R. Waytowich,"Learning from demonstration has been widely studied in machine learning but becomes challenging when the demonstrated trajectories are unstructured and follow different objectives. This short-paper proposes PODNet, Plannable Option Discovery Network, addressing how to segment an unstructured set of demonstrated trajectories for option discovery. This enables learning from demonstration to perform multiple tasks and plan high-level trajectories based on the discovered option labels. PODNet combines a custom categorical variational autoencoder, a recurrent option inference network, option-conditioned policy network, and option dynamics model in an end-to-end learning architecture. Due to the concurrently trained option-conditioned policy network and option dynamics model, the proposed architecture has implications in multi-task and hierarchical learning, explainable and interpretable artificial intelligence, and applications where the agent is required to learn only from observations. △ Less","28 February, 2020",https://arxiv.org/pdf/1911.00171
Fault Tolerance of Neural Networks in Adversarial Settings,Vasisht Duddu;N. Rajesh Pillai;D. Vijay Rao;Valentina E. Balas,"Artificial Intelligence systems require a through assessment of different pillars of trust, namely, fairness, interpretability, data and model privacy, reliability (safety) and robustness against against adversarial attacks. While these research problems have been extensively studied in isolation, an understanding of the trade-off between different pillars of trust is lacking. To this extent, the trade-off between fault tolerance, privacy and adversarial robustness is evaluated for the specific case of Deep Neural Networks, by considering two adversarial settings under a security and a privacy threat model. Specifically, this work studies the impact of the fault tolerance of the Neural Network on training the model by adding noise to the input (Adversarial Robustness) and noise to the gradients (Differential Privacy). While training models with noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness and fault tolerance are at odds with each other. On the other hand, (ε,δ)-Differentially Private models enhance the fault tolerance, measured using generalisation error, theoretically has an upper bound of e^ε - 1 + δ. This novel study of the trade-off between different elements of trust is pivotal for training a model which satisfies the requirements for different pillars of trust simultaneously. △ Less","7 March, 2020",https://arxiv.org/pdf/1910.13875
Conflict and Cooperation: AI Research and Development in terms of the Economy of Conventions,David Solans;Christopher Tauchmann;Aideen Farrell;Karolin Kappler;Hans-Hendrik Huber;Carlos Castillo;Kristian Kersting,"Artificial Intelligence (AI) and its relation with societies is increasingly becoming an interesting object of study from the perspective of sociology and other disciplines. Theories such as the Economy of Conventions (EC) are usually applied in the context of interpersonal relations but there is still a clear lack of studies around how this and other theories can shed light on interactions between human an autonomous systems. This work is focused into studying a preliminary step that is a key enabler for the subsequent interaction between machines and humans: how the processes of researching, designing and developing AI related systems reflect different moral registers, represented by conventions within the EC. Having a better understanding of those conventions guiding the advances in AI is considered as the first and required advance to understand the conventions afterwards reflected by those autonomous systems in the interactions with societies. For this purpose, we develop an iterative tool based on active learning to label a data set from the field of AI and Machine Learning (ML) research and present preliminary results of a supervised classifier trained on these conventions. To further demonstrate the feasibility of the approach, the results are contrasted with a classifier trained on software conventions. △ Less","1 September, 2020",https://arxiv.org/pdf/1910.12591
Does Gender Matter? Towards Fairness in Dialogue Systems,Haochen Liu;Jamell Dacon;Wenqi Fan;Hui Liu;Zitao Liu;Jiliang Tang,"Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as ""gorillas"". As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems. △ Less","31 October, 2020",https://arxiv.org/pdf/1910.10486
Towards Best Practice in Explaining Neural Network Decisions with LRP,Maximilian Kohlbrenner;Alexander Bauer;Shinichi Nakajima;Alexander Binder;Wojciech Samek;Sebastian Lapuschkin,"Within the last decade, neural network based predictors have demonstrated impressive - and at times super-human - capabilities. This performance is often paid for with an intransparent prediction process and thus has sparked numerous contributions in the novel field of explainable artificial intelligence (XAI). In this paper, we focus on a popular and widely used method of XAI, the Layer-wise Relevance Propagation (LRP). Since its initial proposition LRP has evolved as a method, and a best practice for applying the method has tacitly emerged, based however on humanly observed evidence alone. In this paper we investigate - and for the first time quantify - the effect of this current best practice on feedforward neural networks in a visual object detection setting. The results verify that the layer-dependent approach to LRP applied in recent literature better represents the model's reasoning, and at the same time increases the object localization and class discriminativity of LRP. △ Less","13 July, 2020",https://arxiv.org/pdf/1910.09840
Fully Parallel Hyperparameter Search: Reshaped Space-Filling,M. -L. Cauwet;C. Couprie;J. Dehos;P. Luc;J. Rapin;M. Riviere;F. Teytaud;O. Teytaud,"Space-filling designs such as scrambled-Hammersley, Latin Hypercube Sampling and Jittered Sampling have been proposed for fully parallel hyperparameter search, and were shown to be more effective than random or grid search. In this paper, we show that these designs only improve over random search by a constant factor. In contrast, we introduce a new approach based on reshaping the search distribution, which leads to substantial gains over random search, both theoretically and empirically. We propose two flavors of reshaping. First, when the distribution of the optimum is some known P_0, we propose Recentering, which uses as search distribution a modified version of P_0 tightened closer to the center of the domain, in a dimension-dependent and budget-dependent manner. Second, we show that in a wide range of experiments with P_0 unknown, using a proposed Cauchy transformation, which simultaneously has a heavier tail (for unbounded hyperparameters) and is closer to the boundaries (for bounded hyperparameters), leads to improved performances. Besides artificial experiments and simple real world tests on clustering or Salmon mappings, we check our proposed methods on expensive artificial intelligence tasks such as attend/infer/repeat, video next frame segmentation forecasting and progressive generative adversarial networks. △ Less","20 January, 2020",https://arxiv.org/pdf/1910.08406
A Survey of Deep Learning Techniques for Autonomous Driving,Sorin Grigorescu;Bogdan Trasnea;Tiberiu Cocias;Gigel Macesanu,"The last decade witnessed increasingly rapid progress in self-driving vehicle technology, mainly backed up by advances in the area of deep learning and artificial intelligence. The objective of this paper is to survey the current state-of-the-art on deep learning technologies used in autonomous driving. We start by presenting AI-based self-driving architectures, convolutional and recurrent neural networks, as well as the deep reinforcement learning paradigm. These methodologies form a base for the surveyed driving scene perception, path planning, behavior arbitration and motion control algorithms. We investigate both the modular perception-planning-action pipeline, where each module is built using deep learning methods, as well as End2End systems, which directly map sensory information to steering commands. Additionally, we tackle current challenges encountered in designing AI architectures for autonomous driving, such as their safety, training data sources and computational hardware. The comparison presented in this survey helps to gain insight into the strengths and limitations of deep learning and AI approaches for autonomous driving and assist with design choices △ Less","24 March, 2020",https://arxiv.org/pdf/1910.07738
Building Information Modeling and Classification by Visual Learning At A City Scale,Qian Yu;Chaofeng Wang;Barbaros Cetiner;Stella X. Yu;Frank Mckenna;Ertugrul Taciroglu;Kincho H. Law,"In this paper, we provide two case studies to demonstrate how artificial intelligence can empower civil engineering. In the first case, a machine learning-assisted framework, BRAILS, is proposed for city-scale building information modeling. Building information modeling (BIM) is an efficient way of describing buildings, which is essential to architecture, engineering, and construction. Our proposed framework employs deep learning technique to extract visual information of buildings from satellite/street view images. Further, a novel machine learning (ML)-based statistical tool, SURF, is proposed to discover the spatial patterns in building metadata. The second case focuses on the task of soft-story building classification. Soft-story buildings are a type of buildings prone to collapse during a moderate or severe earthquake. Hence, identifying and retrofitting such buildings is vital in the current earthquake preparedness efforts. For this task, we propose an automated deep learning-based procedure for identifying soft-story buildings from street view images at a regional scale. We also create a large-scale building image database and a semi-automated image labeling approach that effectively annotates new database entries. Through extensive computational experiments, we demonstrate the effectiveness of the proposed method. △ Less","20 July, 2020",https://arxiv.org/pdf/1910.06391
Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations,Oana-Maria Camburu;Brendan Shillingford;Pasquale Minervini;Thomas Lukasiewicz;Phil Blunsom,"To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions. In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ""Because there is a dog in the image"" and ""Because there is no dog in the [same] image"", exposing flaws in either the decision-making process of the model or in the generation of the explanations. We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations. Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks. Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations. △ Less","2 May, 2020",https://arxiv.org/pdf/1910.03065
Keeping Designers in the Loop: Communicating Inherent Algorithmic Trade-offs Across Multiple Objectives,Bowen Yu;Ye Yuan;Loren Terveen;Zhiwei Steven Wu;Jodi Forlizzi;Haiyi Zhu,"Artificial intelligence algorithms have been used to enhance a wide variety of products and services, including assisting human decision making in high-stakes contexts. However, these algorithms are complex and have trade-offs, notably between prediction accuracy and fairness to population subgroups. This makes it hard for designers to understand algorithms and design products or services in a way that respects users' goals, values, and needs. We proposed a method to help designers and users explore algorithms, visualize their trade-offs, and select algorithms with trade-offs consistent with their goals and needs. We evaluated our method on the problem of predicting criminal defendants' likelihood to re-offend through (i) a large-scale Amazon Mechanical Turk experiment, and (ii) in-depth interviews with domain experts. Our evaluations show that our method can help designers and users of these systems better understand and navigate algorithmic trade-offs. This paper contributes a new way of providing designers the ability to understand and control the outcomes of algorithmic systems they are creating. △ Less","5 July, 2020",https://arxiv.org/pdf/1910.03061
Blessing of dimensionality at the edge,Ivan Y. Tyukin;Alexander N. Gorban;Alistair A. McEwan;Sepehr Meshkinfamfard;Lixin Tang,"In this paper we present theory and algorithms enabling classes of Artificial Intelligence (AI) systems to continuously and incrementally improve with a-priori quantifiable guarantees - or more specifically remove classification errors - over time. This is distinct from state-of-the-art machine learning, AI, and software approaches. Another feature of this approach is that, in the supervised setting, the computational complexity of training is linear in the number of training samples. At the time of classification, the computational complexity is bounded by few inner product calculations. Moreover, the implementation is shown to be very scalable. This makes it viable for deployment in applications where computational power and memory are limited, such as embedded environments. It enables the possibility for fast on-line optimisation using improved training samples. The approach is based on the concentration of measure effects and stochastic separation theorems and is illustrated with an example on the identification faulty processes in Computer Numerical Control (CNC) milling and with a case study on adaptive removal of false positives in an industrial video surveillance and analytics system. △ Less","10 July, 2020",https://arxiv.org/pdf/1910.00445
Talk2Car: Taking Control of Your Self-Driving Car,Thierry Deruyttere;Simon Vandenhende;Dusan Grujicic;Luc Van Gool;Marie-Francine Moens,"A long-term goal of artificial intelligence is to have an agent execute commands communicated through natural language. In many cases the commands are grounded in a visual environment shared by the human who gives the command and the agent. Execution of the command then requires mapping the command into the physical visual space, after which the appropriate action can be taken. In this paper we consider the former. Or more specifically, we consider the problem in an autonomous driving setting, where a passenger requests an action that can be associated with an object found in a street scene. Our work presents the Talk2Car dataset, which is the first object referral dataset that contains commands written in natural language for self-driving cars. We provide a detailed comparison with related datasets such as ReferIt, RefCOCO, RefCOCO+, RefCOCOg, Cityscape-Ref and CLEVR-Ref. Additionally, we include a performance analysis using strong state-of-the-art models. The results show that the proposed object referral task is a challenging one for which the models show promising results but still require additional research in natural language processing, computer vision and the intersection of these fields. The dataset can be found on our website: http://macchina-ai.eu/ △ Less","26 August, 2020",https://arxiv.org/pdf/1909.10838
SkyNet: a Hardware-Efficient Method for Object Detection and Tracking on Embedded Systems,Xiaofan Zhang;Haoming Lu;Cong Hao;Jiachen Li;Bowen Cheng;Yuhong Li;Kyle Rupnow;Jinjun Xiong;Thomas Huang;Honghui Shi;Wen-mei Hwu;Deming Chen,"Object detection and tracking are challenging tasks for resource-constrained embedded systems. While these tasks are among the most compute-intensive tasks from the artificial intelligence domain, they are only allowed to use limited computation and memory resources on embedded devices. In the meanwhile, such resource-constrained implementations are often required to satisfy additional demanding requirements such as real-time response, high-throughput performance, and reliable inference accuracy. To overcome these challenges, we propose SkyNet, a hardware-efficient neural network to deliver the state-of-the-art detection accuracy and speed for embedded systems. Instead of following the common top-down flow for compact DNN (Deep Neural Network) design, SkyNet provides a bottom-up DNN design approach with comprehensive understanding of the hardware constraints at the very beginning to deliver hardware-efficient DNNs. The effectiveness of SkyNet is demonstrated by winning the competitive System Design Contest for low power object detection in the 56th IEEE/ACM Design Automation Conference (DAC-SDC), where our SkyNet significantly outperforms all other 100+ competitors: it delivers 0.731 Intersection over Union (IoU) and 67.33 frames per second (FPS) on a TX2 embedded GPU; and 0.716 IoU and 25.05 FPS on an Ultra96 embedded FPGA. The evaluation of SkyNet is also extended to GOT-10K, a recent large-scale high-diversity benchmark for generic object tracking in the wild. For state-of-the-art object trackers SiamRPN++ and SiamMask, where ResNet-50 is employed as the backbone, implementations using our SkyNet as the backbone DNN are 1.60X and 1.73X faster with better or similar accuracy when running on a 1080Ti GPU, and 37.20X smaller in terms of parameter size for significantly better memory and storage footprint. △ Less","29 February, 2020",https://arxiv.org/pdf/1909.09709
Meta-Neighborhoods,Siyuan Shan;Yang Li;Junier Oliva,"Making an adaptive prediction based on one's input is an important ability for general artificial intelligence. In this work, we step forward in this direction and propose a semi-parametric method, Meta-Neighborhoods, where predictions are made adaptively to the neighborhood of the input. We show that Meta-Neighborhoods is a generalization of k-nearest-neighbors. Due to the simpler manifold structure around a local neighborhood, Meta-Neighborhoods represent the predictive distribution p(y \mid x) more accurately. To reduce memory and computation overhead, we propose induced neighborhoods that summarize the training data into a much smaller dictionary. A meta-learning based training mechanism is then exploited to jointly learn the induced neighborhoods and the model. Extensive studies demonstrate the superiority of our method. △ Less","13 October, 2020",https://arxiv.org/pdf/1909.09140
"Instance-dependent \ell_\infty
-bounds for policy evaluation in tabular reinforcement learning",Ashwin Pananjady;Martin J. Wainwright,"Markov reward processes (MRPs) are used to model stochastic phenomena arising in operations research, control engineering, robotics, and artificial intelligence, as well as communication and transportation networks. In many of these cases, such as in the policy evaluation problem encountered in reinforcement learning, the goal is to estimate the long-term value function of such a process without access to the underlying population transition and reward functions. Working with samples generated under the synchronous model, we study the problem of estimating the value function of an infinite-horizon, discounted MRP on finitely many states in the \ell_\infty-norm. We analyze both the standard plug-in approach to this problem and a more robust variant, and establish non-asymptotic bounds that depend on the (unknown) problem instance, as well as data-dependent bounds that can be evaluated based on the observations of state-transitions and rewards. We show that these approaches are minimax-optimal up to constant factors over natural sub-classes of MRPs. Our analysis makes use of a leave-one-out decoupling argument tailored to the policy evaluation problem, one which may be of independent interest. △ Less","15 September, 2020",https://arxiv.org/pdf/1909.08749
Machine Learning in/for Blockchain: Future and Challenges,Fang Chen;Hong Wan;Hua Cai;Guang Cheng,"Machine learning and blockchain are two of the most noticeable technologies in recent years. The first one is the foundation of artificial intelligence and big data, and the second one has significantly disrupted the financial industry. Both technologies are data-driven, and thus there are rapidly growing interests in integrating them for more secure and efficient data sharing and analysis. In this paper, we review the research on combining blockchain and machine learning technologies and demonstrate that they can collaborate efficiently and effectively. In the end, we point out some future directions and expect more researches on deeper integration of the two promising technologies. △ Less","8 December, 2020",https://arxiv.org/pdf/1909.06189
Automatic Critical Mechanic Discovery Using Playtraces in Video Games,Michael Cerny Green;Ahmed Khalifa;Gabriella A. B. Barros;Tiago Machado;Julian Togelius,"We present a new method of automatic critical mechanic discovery for video games using a combination of game description parsing and playtrace information. This method is applied to several games within the General Video Game Artificial Intelligence (GVG-AI) framework. In a user study, human-identified mechanics are compared against system-identified critical mechanics to verify alignment between humans and the system. The results of the study demonstrate that the new method is able to match humans with higher consistency than baseline. Our system is further validated by comparing MCTS agents augmented with critical mechanics and vanilla MCTS agents on 4 games from GVG-AI. Our new playtrace method shows a significant performance improvement over the baseline for all 4 tested games. The proposed method also shows either matched or improved performance over the old method, demonstrating that playtrace information is responsible for more complete critical mechanic discovery. △ Less","15 September, 2020",https://arxiv.org/pdf/1909.03094
Agora: A Unified Asset Ecosystem Going Beyond Marketplaces and Cloud Services,Jonas Traub;Jorge-Arnulfo Quiané-Ruiz;Zoi Kaoudi;Volker Markl,"Data, algorithms, and compute/storage infrastructure are key assets that drive data science and artificial intelligence applications. As providing all these assets requires a huge investment, data science and artificial intelligence technologies are currently dominated by a small number of providers who can afford these investments. This leads to lock-in effects and hinders features that require a flexible exchange of assets among users. In this vision paper, we present Agora, a unified asset ecosystem. The Agora system provides the technical infrastructure that allows for offering and using data and algorithms, as well as physical infrastructure components. Agora is designed as an open ecosystem of asset marketplaces and provides to a broad audience not only data but the entire data value chain (including computational resources and human expertise). Agora (i) leverages a fine-grained exchange of assets, (ii) allows for combining assets to novel applications, and (iii) flexibly executes such applications on available resources. As a result, Agora overcomes lock-in effects and removes entry barriers for new asset providers. In contrast to existing data management systems, Agora operates in a heavily decentralized and dynamic environment: Data, algorithms, and even compute resources are dynamically created, modified, and removed by different stakeholders. Agora presents novel research directions for the data management community as a whole: It requires to combine our traditional expertise in scalable data processing and management with infrastructure provisioning as well as economic and application aspects of data, algorithms, and infrastructure. △ Less","19 July, 2020",https://arxiv.org/pdf/1909.03026
Dynamic Spatial-Temporal Representation Learning for Traffic Flow Prediction,Lingbo Liu;Jiajie Zhen;Guanbin Li;Geng Zhan;Zhaocheng He;Bowen Du;Liang Lin,"As a crucial component in intelligent transportation systems, traffic flow prediction has recently attracted widespread research interest in the field of artificial intelligence (AI) with the increasing availability of massive traffic mobility data. Its key challenge lies in how to integrate diverse factors (such as temporal rules and spatial dependencies) to infer the evolution trend of traffic flow. To address this problem, we propose a unified neural network called Attentive Traffic Flow Machine (ATFM), which can effectively learn the spatial-temporal feature representations of traffic flow with an attention mechanism. In particular, our ATFM is composed of two progressive Convolutional Long Short-Term Memory (ConvLSTM \cite{xingjian2015convolutional}) units connected with a convolutional layer. Specifically, the first ConvLSTM unit takes normal traffic flow features as input and generates a hidden state at each time-step, which is further fed into the connected convolutional layer for spatial attention map inference. The second ConvLSTM unit aims at learning the dynamic spatial-temporal representations from the attentionally weighted traffic flow features. Further, we develop two deep learning frameworks based on ATFM to predict citywide short-term/long-term traffic flow by adaptively incorporating the sequential and periodic data as well as other external influences. Extensive experiments on two standard benchmarks well demonstrate the superiority of the proposed method for traffic flow prediction. Moreover, to verify the generalization of our method, we also apply the customized framework to forecast the passenger pickup/dropoff demands in traffic prediction and show its superior performance. Our code and data are available at {\color{blue}\url{https://github.com/liulingbo918/ATFM}}. △ Less","12 June, 2020",https://arxiv.org/pdf/1909.02902
Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence,Shuiguang Deng;Hailiang Zhao;Weijia Fang;Jianwei Yin;Schahram Dustdar;Albert Y. Zomaya,"Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, Edge Computing, is surging in popularity. Meanwhile, Artificial Intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate Edge Computing and AI, which gives birth to Edge Intelligence. In this paper, we divide Edge Intelligence into AI for edge (Intelligence-enabled Edge Computing) and AI on edge (Artificial Intelligence on Edge). The former focuses on providing more optimal solutions to key problems in Edge Computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This paper provides insights into this new inter-disciplinary field from a broader perspective. It discusses the core concepts and the research road-map, which should provide the necessary background for potential future research initiatives in Edge Intelligence. △ Less","10 February, 2020",https://arxiv.org/pdf/1909.00560
DeepHealth: Review and challenges of artificial intelligence in health informatics,Gloria Hyunjung Kwak;Pan Hui,"Artificial intelligence has provided us with an exploration of a whole new research era. As more data and better computational power become available, the approach is being implemented in various fields. The demand for it in health informatics is also increasing, and we can expect to see the potential benefits of its applications in healthcare. It can help clinicians diagnose disease, identify drug effects for each patient, understand the relationship between genotypes and phenotypes, explore new phenotypes or treatment recommendations, and predict infectious disease outbreaks with high accuracy. In contrast to traditional models, recent artificial intelligence approaches do not require domain-specific data pre-processing, and it is expected that it will ultimately change life in the future. Despite its notable advantages, there are some key challenges on data (high dimensionality, heterogeneity, time dependency, sparsity, irregularity, lack of label, bias) and model (reliability, interpretability, feasibility, security, scalability) for practical use. This article presents a comprehensive review of research applying artificial intelligence in health informatics, focusing on the last seven years in the fields of medical imaging, electronic health records, genomics, sensing, and online communication health, as well as challenges and promising directions for future research. We highlight ongoing popular approaches' research and identify several challenges in building models. △ Less","8 August, 2020",https://arxiv.org/pdf/1909.00384
How Good is Artificial Intelligence at Automatically Answering Consumer Questions Related to Alzheimer's Disease?,Krishna B. Soundararajan;Sunyang Fu;Luke A. Carlson;Rebecca A. Smith;David S. Knopman;Hongfang Liu;Yanshan Wang,"Alzheimer's Disease (AD) is the most common type of dementia, comprising 60-80% of cases. There were an estimated 5.8 million Americans living with Alzheimer's dementia in 2019, and this number will almost double every 20 years. The total lifetime cost of care for someone with dementia is estimated to be $350,174 in 2018, 70% of which is associated with family-provided care. Most family caregivers face emotional, financial and physical difficulties. As a medium to relieve this burden, online communities in social media websites such as Twitter, Reddit, and Yahoo! Answers provide potential venues for caregivers to search relevant questions and answers, or post questions and seek answers from other members. However, there are often a limited number of relevant questions and responses to search from, and posted questions are rarely answered immediately. Due to recent advancement in Artificial Intelligence (AI), particularly Natural Language Processing (NLP), we propose to utilize AI to automatically generate answers to AD-related consumer questions posted by caregivers and evaluate how good AI is at answering those questions. To the best of our knowledge, this is the first study in the literature applying and evaluating AI models designed to automatically answer consumer questions related to AD. △ Less","20 May, 2020",https://arxiv.org/pdf/1908.10678
DAPAS : Denoising Autoencoder to Prevent Adversarial attack in Semantic Segmentation,Seungju Cho;Tae Joon Jun;Byungsoo Oh;Daeyoung Kim,"Nowadays, Deep learning techniques show dramatic performance on computer vision area, and they even outperform human. But it is also vulnerable to some small perturbation called an adversarial attack. This is a problem combined with the safety of artificial intelligence, which has recently been studied a lot. These attacks have shown that they can fool models of image classification, semantic segmentation, and object detection. We point out this attack can be protected by denoise autoencoder, which is used for denoising the perturbation and restoring the original images. We experiment with various noise distributions and verify the effect of denoise autoencoder against adversarial attack in semantic segmentation. △ Less","7 April, 2020",https://arxiv.org/pdf/1908.05195
Efficient Contraction of Large Tensor Networks for Weighted Model Counting through Graph Decompositions,Jeffrey M. Dudek;Leonardo Dueñas-Osorio;Moshe Y. Vardi,"Constrained counting is a fundamental problem in artificial intelligence. A promising new algebraic approach to constrained counting makes use of tensor networks, following a reduction from constrained counting to the problem of tensor-network contraction. Contracting a tensor network efficiently requires determining an efficient order to contract the tensors inside the network, which is itself a difficult problem. In this work, we apply graph decompositions to find contraction orders for tensor networks. We prove that finding an efficient contraction order for a tensor network is equivalent to the well-known problem of finding an optimal carving decomposition. Thus memory-optimal contraction orders for planar tensor networks can be found in cubic time. We show that tree decompositions can be used both to find carving decompositions and to factor tensor networks with high-rank, structured tensors. We implement these algorithms on top of state-of-the-art solvers for tree decompositions and show empirically that the resulting weighted model counter is quite effective and useful as part of a portfolio of counters. △ Less","27 April, 2020",https://arxiv.org/pdf/1908.04381
Tuning Algorithms and Generators for Efficient Edge Inference,Rawan Naous;Lazar Supic;Yoonhwan Kang;Ranko Sredojevic;Anish Singhani;Vladimir Stojanovic,"A surge in artificial intelligence and autonomous technologies have increased the demand toward enhanced edge-processing capabilities. Computational complexity and size of state-of-the-art Deep Neural Networks (DNNs) are rising exponentially with diverse network models and larger datasets. This growth limits the performance scaling and energy-efficiency of both distributed and embedded inference platforms. Embedded designs at the edge are constrained by energy and speed limitations of available processor substrates and processor to memory communication required to fetch the model coefficients. While many hardware accelerator and network deployment frameworks have been in development, a framework is needed to allow the variety of existing architectures, and those in development, to be expressed in critical parts of the flow that perform various optimization steps. Moreover, premature architecture-blind network selection and optimization diminish the effectiveness of schedule optimizations and hardware-specific mappings. In this paper, we address these issues by creating a cross-layer software-hardware design framework that encompasses network training and model compression that is aware of and tuned to the underlying hardware architecture. This approach leverages the available degrees of DNN structure and sparsity to create a converged network that can be partitioned and efficiently scheduled on the target hardware platform, minimizing data movement, and improving the overall throughput and energy. To further streamline the design, we leverage the high-level, flexible SoC generator platform based on RISC-V ROCC framework. This integration allows seamless extensions of the RISC-V instruction set and Chisel-based rapid generator design. Utilizing this approach, we implemented a silicon prototype in a 16 nm TSMC process node achieving record processing efficiency of up to 18 TOPS/W. △ Less","10 May, 2020",https://arxiv.org/pdf/1908.02239
Corrigibility with Utility Preservation,Koen Holtman,"Corrigibility is a safety property for artificially intelligent agents. A corrigible agent will not resist attempts by authorized parties to alter the goals and constraints that were encoded in the agent when it was first started. This paper shows how to construct a safety layer that adds corrigibility to arbitrarily advanced utility maximizing agents, including possible future agents with Artificial General Intelligence (AGI). The layer counter-acts the emergent incentive of advanced agents to resist such alteration. A detailed model for agents which can reason about preserving their utility function is developed, and used to prove that the corrigibility layer works as intended in a large set of non-hostile universes. The corrigible agents have an emergent incentive to protect key elements of their corrigibility layer. However, hostile universes may contain forces strong enough to break safety features. Some open problems related to graceful degradation when an agent is successfully attacked are identified. The results in this paper were obtained by concurrently developing an AGI agent simulator, an agent model, and proofs. The simulator is available under an open source license. The paper contains simulation results which illustrate the safety related properties of corrigible AGI agents in detail. △ Less","3 April, 2020",https://arxiv.org/pdf/1908.01695
Clinical acceptance of software based on artificial intelligence technologies (radiology),S. P. Morozov;A. V. Vladzymyrskyy;V. G. Klyashtornyy;A. E. Andreychenko;N. S. Kulberg;V. A. Gombolevsky;K. A. Sergunova,"Aim: provide a methodological framework for the process of clinical tests, clinical acceptance, and scientific assessment of algorithms and software based on the artificial intelligence (AI) technologies. Clinical tests are considered as a preparation stage for the software registration as a medical product. The authors propose approaches to evaluate accuracy and efficiency of the AI algorithms for radiology. △ Less","27 February, 2020",https://arxiv.org/pdf/1908.00381
Energy-Efficient Processing and Robust Wireless Cooperative Transmission for Edge Inference,Kai Yang;Yuanming Shi;Wei Yu;Zhi Ding,"Edge machine learning can deliver low-latency and private artificial intelligent (AI) services for mobile devices by leveraging computation and storage resources at the network edge. This paper presents an energy-efficient edge processing framework to execute deep learning inference tasks at the edge computing nodes whose wireless connections to mobile devices are prone to channel uncertainties. Aimed at minimizing the sum of computation and transmission power consumption with probabilistic quality-of-service (QoS) constraints, we formulate a joint inference tasking and downlink beamforming problem that is characterized by a group sparse objective function. We provide a statistical learning based robust optimization approach to approximate the highly intractable probabilistic-QoS constraints by nonconvex quadratic constraints, which are further reformulated as matrix inequalities with a rank-one constraint via matrix lifting. We design a reweighted power minimization approach by iteratively reweighted \ell_1 minimization with difference-of-convex-functions (DC) regularization and updating weights, where the reweighted approach is adopted for enhancing group sparsity whereas the DC regularization is designed for inducing rank-one solutions. Numerical results demonstrate that the proposed approach outperforms other state-of-the-art approaches. △ Less","2 March, 2020",https://arxiv.org/pdf/1907.12475
Learning Quintuplet Loss for Large-scale Visual Geo-Localization,Qiang Zhai,"With the maturity of Artificial Intelligence (AI) technology, Large Scale Visual Geo-Localization (LSVGL) is increasingly important in urban computing, where the task is to accurately and efficiently recognize the geo-location of a given query image. The main challenge of LSVGL faced by many experiments due to the appearance of real-word places may differ in various ways. While perspective deviation almost inevitably exists between training images and query images because of the arbitrary perspective. To cope with this situation, in this paper, we in-depth analyze the limitation of triplet loss which is the most commonly used metric learning loss in state-of-the-art LSVGL framework, and propose a new QUInTuplet Loss (QUITLoss) by embedding all the potential positive samples to the primitive triplet loss. Extensive experiments have been conducted to verify the effectiveness of the proposed approach and the results demonstrate that our new loss can enhance various LSVGL methods. △ Less","8 October, 2020",https://arxiv.org/pdf/1907.11350
Quantum Advantage and Y2K Bug: Comparison,Lei Zhang;Andriy Miranskyy;Walid Rjaibi,"Quantum Computers (QCs), once they mature, will be able to solve some problems faster than Classic Computers. This phenomenon is called ""quantum advantage"" (or a stronger term ""quantum supremacy""). Quantum advantage will help us to speed up computations in many areas, from artificial intelligence to medicine. However, QC power can also be leveraged to break modern cryptographic algorithms, which pervade modern software: use cases range from encryption of Internet traffic, to encryption of disks, to signing blockchain ledgers. While the exact date when QCs will evolve to reach quantum advantage is unknown, the consensus is that this future is near. Thus, in order to maintain crypto agility of the software, one needs to start preparing for the era of quantum advantage proactively. In this paper, we recap the effect of quantum advantage on the existing and new software systems, as well as the data that we currently store. We also highlight similarities and differences between the security challenges brought by QCs and the challenges that software engineers faced twenty years ago while fixing widespread Y2K bug. Technically, the Y2K bug and the quantum advantage problems are different: the former was caused by timing-related problems, while the latter is caused by a cryptographic algorithm being non-quantum-resistant. However, conceptually, the problems are similar: we know what the root cause is, the fix (strategically) is straightforward, yet the implementation of the fix is challenging. To address the quantum advantage challenge, we create a seven-step roadmap, deemed 7E. It is inspired by the lessons-learnt from the Y2K era amalgamated with modern knowledge. The roadmap gives developers a structured way to start preparing for the quantum advantage era, helping them to start planning for the creation of new as well as the evolution of the existent software. △ Less","31 March, 2020",https://arxiv.org/pdf/1907.10454
"Deep Reinforcement Learning for Autonomous Internet of Things: Model, Applications and Challenges",Lei Lei;Yue Tan;Kan Zheng;Shiwen Liu;Kuan Zhang;Xuemin;Shen,"The Internet of Things (IoT) extends the Internet connectivity into billions of IoT devices around the world, where the IoT devices collect and share information to reflect status of the physical world. The Autonomous Control System (ACS), on the other hand, performs control functions on the physical systems without external intervention over an extended period of time. The integration of IoT and ACS results in a new concept - autonomous IoT (AIoT). The sensors collect information on the system status, based on which the intelligent agents in the IoT devices as well as the Edge/Fog/Cloud servers make control decisions for the actuators to react. In order to achieve autonomy, a promising method is for the intelligent agents to leverage the techniques in the field of artificial intelligence, especially reinforcement learning (RL) and deep reinforcement learning (DRL) for decision making. In this paper, we first provide a tutorial of DRL, and then propose a general model for the applications of RL/DRL in AIoT. Next, a comprehensive survey of the state-of-art research on DRL for AIoT is presented, where the existing works are classified and summarized under the umbrella of the proposed general DRL model. Finally, the challenges and open issues for future research are identified. △ Less","13 April, 2020",https://arxiv.org/pdf/1907.09059
Convergence of Edge Computing and Deep Learning: A Comprehensive Survey,Xiaofei Wang;Yiwen Han;Victor C. M. Leung;Dusit Niyato;Xueqiang Yan;Xu Chen,"Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people's lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of ""providing artificial intelligence for every person and every organization at everywhere"". Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL. △ Less","28 January, 2020",https://arxiv.org/pdf/1907.08349
A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI,Erico Tjoa;Cuntai Guan,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide ""obviously"" interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematically- and technically-grounded medical education are encouraged. △ Less","10 August, 2020",https://arxiv.org/pdf/1907.07374
Norms for Beneficial A.I.: A Computational Analysis of the Societal Value Alignment Problem,Pedro Fernandes;Francisco C. Santos;Manuel Lopes,"The rise of artificial intelligence (A.I.) based systems is already offering substantial benefits to the society as a whole. However, these systems may also enclose potential conflicts and unintended consequences. Notably, people will tend to adopt an A.I. system if it confers them an advantage, at which point non-adopters might push for a strong regulation if that advantage for adopters is at a cost for them. Here we propose an agent-based game-theoretical model for these conflicts, where agents may decide to resort to A.I. to use and acquire additional information on the payoffs of a stochastic game, striving to bring insights from simulation to what has been, hitherto, a mostly philosophical discussion. We frame our results under the current discussion on ethical A.I. and the conflict between individual and societal gains: the societal value alignment problem. We test the arising equilibria in the adoption of A.I. technology under different norms followed by artificial agents, their ensuing benefits, and the emergent levels of wealth inequality. We show that without any regulation, purely selfish A.I. systems will have the strongest advantage, even when a utilitarian A.I. provides significant benefits for the individual and the society. Nevertheless, we show that it is possible to develop A.I. systems following human conscious policies that, when introduced in society, lead to an equilibrium where the gains for the adopters are not at a cost for non-adopters, thus increasing the overall wealth of the population and lowering inequality. However, as shown, a self-organised adoption of such policies would require external regulation. △ Less","22 December, 2020",https://arxiv.org/pdf/1907.03843
Perspective Taking in Deep Reinforcement Learning Agents,Aqeel Labash;Jaan Aru;Tambet Matiisen;Ardi Tampuu;Raul Vicente,"Perspective taking is the ability to take the point of view of another agent. This skill is not unique to humans as it is also displayed by other animals like chimpanzees. It is an essential ability for social interactions, including efficient cooperation, competition, and communication. Here we present our progress toward building artificial agents with such abilities. We implemented a perspective taking task inspired by experiments done with chimpanzees. We show that agents controlled by artificial neural networks can learn via reinforcement learning to pass simple tests that require perspective taking capabilities. We studied whether this ability is more readily learned by agents with information encoded in allocentric or egocentric form for both their visual perception and motor actions. We believe that, in the long run, building better artificial agents with perspective taking ability can help us develop artificial intelligence that is more human-like and easier to communicate with. △ Less","15 April, 2020",https://arxiv.org/pdf/1907.01851
Application and Computation of Probabilistic Neural Plasticity,Soaad Hossain,"The discovery of neural plasticity has proved that throughout the life of a human being, the brain reorganizes itself through forming new neural connections. The formation of new neural connections are achieved through the brain's effort to adapt to new environments or to changes in the existing environment. Despite the realization of neural plasticity, there is a lack of understanding the probability of neural plasticity occurring given some event. Using ordinary differential equations, neural firing equations and spike-train statistics, we show how an additive short-term memory (STM) equation can be formulated to approach the computation of neural plasticity. We then show how the additive STM equation can be used for probabilistic inference in computable neural plasticity, and the computation of probabilistic neural plasticity. We will also provide a brief introduction to the theory of probabilistic neural plasticity and conclude with showing how it can be applied to multiple disciplines such as behavioural science, machine learning, artificial intelligence and psychiatry. △ Less","6 August, 2020",https://arxiv.org/pdf/1907.00689
Proof of Witness Presence: Blockchain Consensus for Augmented Democracy in Smart Cities,Evangelos Pournaras,"Smart Cities evolve into complex and pervasive urban environments with a citizens' mandate to meet sustainable development goals. Repositioning democratic values of citizens' choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens' trust and overcome security vulnerabilities of GPS localization. Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens' choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities, which are compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined. △ Less","8 July, 2020",https://arxiv.org/pdf/1907.00498
Implementing Ethics in AI: Initial Results of an Industrial Multiple Case Study,Ville Vakkuri;Kai-Kristian Kemell;Pekka Abrahamsson,"Artificial intelligence (AI) is becoming increasingly widespread in system development endeavors. As AI systems affect various stakeholders due to their unique nature, the growing influence of these systems calls for ethical considerations. Academic discussion and practical examples of autonomous system failures have highlighted the need for implementing ethics in software development. However, research on methods and tools for implementing ethics into AI system design and development in practice is still lacking. This paper begins to address this focal problem by providing elements needed for producing a baseline for ethics in AI based software development. We do so by means of an industrial multiple case study on AI systems development in the healthcare sector. Using a research model based on extant, conceptual AI ethics literature, we explore the current state of practice out on the field in the absence of formal methods and tools for ethically aligned design. △ Less","16 June, 2020",https://arxiv.org/pdf/1906.12307
Demystifying Inter-Class Disentanglement,Aviv Gabbay;Yedid Hoshen,"Learning to disentangle the hidden factors of variations within a set of observations is a key task for artificial intelligence. We present a unified formulation for class and content disentanglement and use it to illustrate the limitations of current methods. We therefore introduce LORD, a novel method based on Latent Optimization for Representation Disentanglement. We find that latent optimization, along with an asymmetric noise regularization, is superior to amortized inference for achieving disentangled representations. In extensive experiments, our method is shown to achieve better disentanglement performance than both adversarial and non-adversarial methods that use the same level of supervision. We further introduce a clustering-based approach for extending our method for settings that exhibit in-class variation with promising results on the task of domain translation. △ Less","18 February, 2020",https://arxiv.org/pdf/1906.11796
Discovery of Physics from Data: Universal Laws and Discrepancies,Brian M. de Silva;David M. Higdon;Steven L. Brunton;J. Nathan Kutz,"Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of nuanced issues that must be addressed by modern data-driven methods for automated physics discovery. Specifically, we show that measurement noise and complex secondary physical mechanisms, like unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. We use the sparse identification of nonlinear dynamics (SINDy) method to identify governing equations for real-world measurement data and simulated trajectories. Incorporating into SINDy the assumption that each falling object is governed by a similar physical law is shown to improve the robustness of the learned models, but discrepancies between the predictions and observations persist due to subtleties in drag dynamics. This work highlights the fact that the naive application of ML/AI will generally be insufficient to infer universal physical laws without further modification. △ Less","16 April, 2020",https://arxiv.org/pdf/1906.07906
Multimodal End-to-End Autonomous Driving,Yi Xiao;Felipe Codevilla;Akhil Gurram;Onay Urfalioglu;Antonio M. López,"A crucial component of an autonomous vehicle (AV) is the artificial intelligence (AI) is able to drive towards a desired destination. Today, there are different paradigms addressing the development of AI drivers. On the one hand, we find modular pipelines, which divide the driving task into sub-tasks such as perception and maneuver planning and control. On the other hand, we find end-to-end driving approaches that try to learn a direct mapping from input raw sensor data to vehicle control signals. The later are relatively less studied, but are gaining popularity since they are less demanding in terms of sensor data annotation. This paper focuses on end-to-end autonomous driving. So far, most proposals relying on this paradigm assume RGB images as input sensor data. However, AVs will not be equipped only with cameras, but also with active sensors providing accurate depth information (e.g., LiDARs). Accordingly, this paper analyses whether combining RGB and depth modalities, i.e. using RGBD data, produces better end-to-end AI drivers than relying on a single modality. We consider multimodality based on early, mid and late fusion schemes, both in multisensory and single-sensor (monocular depth estimation) settings. Using the CARLA simulator and conditional imitation learning (CIL), we show how, indeed, early fusion multimodality outperforms single-modality. △ Less","25 October, 2020",https://arxiv.org/pdf/1906.03199
Active inference body perception and action for humanoid robots,Guillermo Oliver;Pablo Lanillos;Gordon Cheng,"Providing artificial agents with the same computational models of biological systems is a way to understand how intelligent behaviours may emerge. We present an active inference body perception and action model working for the first time in a humanoid robot. The model relies on the free energy principle proposed for the brain, where both perception and action goal is to minimise the prediction error through gradient descent on the variational free energy bound. The body state (latent variable) is inferred by minimising the difference between the observed (visual and proprioceptive) sensor values and the predicted ones. Simultaneously, the action makes sensory data sampling to better correspond to the prediction made by the inner model. We formalised and implemented the algorithm on the iCub robot and tested in 2D and 3D visual spaces for online adaptation to visual changes, sensory noise and discrepancies between the model and the real robot. We also compared our approach with classical inverse kinematics in a reaching task, analysing the suitability of such a neuroscience-inspired approach for real-world interaction. The algorithm gave the robot adaptive body perception and upper body reaching with head object tracking (toddler-like), and was able to incorporate visual features online (in a closed-loop manner) without increasing the computational complexity. Moreover, our model predicted involuntary actions in the presence of sensorimotor conflicts showing the path for a potential proof of active inference in humans. △ Less","29 January, 2020",https://arxiv.org/pdf/1906.03022
Interpretable PID Parameter Tuning for Control Engineering using General Dynamic Neural Networks: An Extensive Comparison,Johannes Günther;Elias Reichensdörfer;Patrick M. Pilarski;Klaus Diepold,"Modern automation systems rely on closed loop control, wherein a controller interacts with a controlled process, based on observations. These systems are increasingly complex, yet most controllers are linear Proportional-Integral-Derivative (PID) controllers. PID controllers perform well on linear and near-linear systems but their simplicity is at odds with the robustness required to reliably control complex processes. Modern machine learning offers a way to extend PID controllers beyond their linear capabilities by using neural networks. However, such an extension comes at the cost of losing stability guarantees and controller interpretability. In this paper, we examine the utility of extending PID controllers with recurrent neural networks-namely, General Dynamic Neural Networks (GDNN); we show that GDNN (neural) PID controllers perform well on a range of control systems and highlight how they can be a scalable and interpretable option for control systems. To do so, we provide an extensive study using four benchmark systems that represent the most common control engineering benchmarks. All control benchmarks are evaluated with and without noise as well as with and without disturbances. The neural PID controller performs better than standard PID control in 15 of 16 tasks and better than model-based control in 13 of 16 tasks. As a second contribution, we address the lack of interpretability that prevents neural networks from being used in real-world control processes. We use bounded-input bounded-output stability analysis to evaluate the parameters suggested by the neural network, thus making them understandable. This combination of rigorous evaluation paired with better interpretability is an important step towards the acceptance of neural-network-based control approaches. It is furthermore an important step towards interpretable and safely applied artificial intelligence. △ Less","20 November, 2020",https://arxiv.org/pdf/1905.13268
Asymptotically Unambitious Artificial General Intelligence,Michael K Cohen;Badri Vellambi;Marcus Hutter,"General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artificially constructible. Narrow intelligence, the ability to solve a given particularly difficult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classifiers, and translators. Artificial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI's goals with our own has proven highly elusive. We present the first algorithm we are aware of for asymptotically unambitious AGI, where ""unambitiousness"" includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us. △ Less","21 July, 2020",https://arxiv.org/pdf/1905.12186
Unsupervised Learning from Video with Deep Neural Embeddings,Chengxu Zhuang;Tianwei She;Alex Andonian;Max Sobol Mark;Daniel Yamins,"Because of the rich dynamical structure of videos and their ubiquity in everyday life, it is a natural idea that video data could serve as a powerful unsupervised learning signal for training visual representations in deep neural networks. However, instantiating this idea, especially at large scale, has remained a significant artificial intelligence challenge. Here we present the Video Instance Embedding (VIE) framework, which extends powerful recent unsupervised loss functions for learning deep nonlinear embeddings to multi-stream temporal processing architectures on large-scale video datasets. We show that VIE-trained networks substantially advance the state of the art in unsupervised learning from video datastreams, both for action recognition in the Kinetics dataset, and object recognition in the ImageNet dataset. We show that a hybrid model with both static and dynamic processing pathways is optimal for both transfer tasks, and provide analyses indicating how the pathways differ. Taken in context, our results suggest that deep neural embeddings are a promising approach to unsupervised visual learning across a wide variety of domains. △ Less","10 March, 2020",https://arxiv.org/pdf/1905.11954
"AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence",Jeff Clune,"Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the ""manual AI approach"". This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research. △ Less","31 January, 2020",https://arxiv.org/pdf/1905.10985
An Exploratory Study on Machine Learning Model Stores,Minke Xiu;Zhen Ming;Jiang;Bram Adams,"Recent advances in Artificial Intelligence, especially in Machine Learning (ML), have brought applications previously considered as science fiction (e.g., virtual personal assistants and autonomous cars) into the reach of millions of everyday users. Since modern ML technologies like deep learning require considerable technical expertise and resource to build custom models, reusing existing models trained by experts has become essential. This is why in the past year model stores have been introduced, which, similar to mobile app stores, offer organizations and developers access to pre-trained models and/or their code to train, evaluate, and predict samples. This paper conducts an exploratory study on three popular model stores (AWS marketplace, Wolfram neural net repository, and ModelDepot) that compares the information elements (features and policies) provided by model stores to those used by the two popular mobile app stores (Google Play and Apple's App Store). We have found that the model information elements vary among the different model stores, with 65% elements shared by all three studied stores. Model stores share five information elements with mobile app stores, while eight elements are unique to model stores and four elements unique to app stores. Only few models were available on multiple model stores. Our findings allow to better understand the differences between ML models and ""regular"" source code components or applications, and provide inspiration to identify software engineering practices (e.g., in requirements and delivery) specific to ML applications. △ Less","25 February, 2020",https://arxiv.org/pdf/1905.10677
Can a Multi-Hop Link Relying on Untrusted Amplify-and-Forward Relays Render Security?,Milad Tatar Mamaghani;Ali Kuhestani;Hamid Behroozi,"Cooperative relaying is utilized as an efficient method for data communication in wireless sensor networks and the Internet of Things (IoT). However, sometimes due to the necessity of multi-hop relaying in such communication networks, it is challenging to guarantee the secrecy of cooperative transmissions when the relays may themselves be eavesdroppers, i.e., we may face with the untrusted relaying scenario where the relays are both necessary helpers and potential adversary. To obviate this issue, a new cooperative jamming scheme is proposed in this paper, in which the data can be confidentially communicated from the source to the destination through multiple untrusted relays. In our proposed secure transmission scheme, all the legitimate nodes contribute to providing secure communication by intelligently injecting artificial noises to the network in different communication phases. For the sake of analysis, we consider a multi-hop untrusted relaying network with two successive intermediate nodes, i.e, a three-hop communications network. Given this system model, a new closed-form expression is presented in the high signal-to-noise ratio (SNR) region for the Ergodic secrecy rate (ESR). Furthermore, we evaluate the high SNR slope and power offset of the ESR to gain an insightful comparison of the proposed secure transmission scheme and the state-of-arts. Our numerical results highlight that the proposed secure transmission scheme provides better secrecy rate performance compared with the two-hop untrusted relaying as well as the direct transmission schemes. △ Less","5 July, 2020",https://arxiv.org/pdf/1905.09384
Ethically Aligned Design: An empirical evaluation of the RESOLVEDD-strategy in Software and Systems development context,Ville Vakkuri;Kai-Kristian Kemell;Pekka Abrahamsson,"Use of artificial intelligence (AI) in human contexts calls for ethical considerations for the design and development of AI-based systems. However, little knowledge currently exists on how to provide useful and tangible tools that could help software developers and designers implement ethical considerations into practice. In this paper, we empirically evaluate a method that enables ethically aligned design in a decision-making process. Though this method, titled the RESOLVEDD-strategy, originates from the field of business ethics, it is being applied in other fields as well. We tested the RESOLVEDD-strategy in a multiple case study of five student projects where the use of ethical tools was given as one of the design requirements. A key finding from the study indicates that simply the presence of an ethical tool has an effect on ethical consideration, creating more responsibility even in instances where the use of the tool is not intrinsically motivated. △ Less","22 January, 2020",https://arxiv.org/pdf/1905.06417
Ludii -- The Ludemic General Game System,Éric Piette;Dennis J. N. J. Soemers;Matthew Stephenson;Chiara F. Sironi;Mark H. M. Winands;Cameron Browne,"While current General Game Playing (GGP) systems facilitate useful research in Artificial Intelligence (AI) for game-playing, they are often somewhat specialised and computationally inefficient. In this paper, we describe the ""ludemic"" general game system Ludii, which has the potential to provide an efficient tool for AI researchers as well as game designers, historians, educators and practitioners in related fields. Ludii defines games as structures of ludemes -- high-level, easily understandable game concepts -- which allows for concise and human-understandable game descriptions. We formally describe Ludii and outline its main benefits: generality, extensibility, understandability and efficiency. Experimentally, Ludii outperforms one of the most efficient Game Description Language (GDL) reasoners, based on a propositional network, in all games available in the Tiltyard GGP repository. Moreover, Ludii is also competitive in terms of performance with the more recently proposed Regular Boardgames (RBG) system, and has various advantages in qualitative aspects such as generality. △ Less","21 February, 2020",https://arxiv.org/pdf/1905.05013
Formal Verification of Input-Output Mappings of Tree Ensembles,John Törnblom;Simin Nadjm-Tehrani,"Recent advances in machine learning and artificial intelligence are now being considered in safety-critical autonomous systems where software defects may cause severe harm to humans and the environment. Design organizations in these domains are currently unable to provide convincing arguments that their systems are safe to operate when machine learning algorithms are used to implement their software. In this paper, we present an efficient method to extract equivalence classes from decision trees and tree ensembles, and to formally verify that their input-output mappings comply with requirements. The idea is that, given that safety requirements can be traced to desirable properties on system input-output patterns, we can use positive verification outcomes in safety arguments. This paper presents the implementation of the method in the tool VoTE (Verifier of Tree Ensembles), and evaluates its scalability on two case studies presented in current literature. We demonstrate that our method is practical for tree ensembles trained on low-dimensional data with up to 25 decision trees and tree depths of up to 20. Our work also studies the limitations of the method with high-dimensional data and preliminarily investigates the trade-off between large number of trees and time taken for verification. △ Less","23 March, 2020",https://arxiv.org/pdf/1905.04194
Digitally Capturing Physical Prototypes During Early-Stage Engineering Design Projects for Initial Analysis of Project Output and Progression,Jorgen F. Erichsen;Heikki Sjöman;Martin Steinert;Torgeir Welo,"Aiming to help researchers capture output from the early stages of engineering design projects, this article presents a new research tool for digitally capturing physical prototypes. The motivation for this work is to collect observations that can aid in understanding prototyping in the early stages of engineering design projects, and this article investigates if and how digital capture of physical prototypes can be used for this purpose. Early-stage prototypes are usually rough and of low-fidelity and are thus often discarded or substantially modified through the projects. Hence, retrospective access to prototypes is a challenge when trying to gather accurate empirical data. To capture the prototypes developed through the early stages of a project, a new research tool has been developed for capturing prototypes through multi-view images, along with metadata describing by whom, why, when and where the prototypes were captured. Over the course of 17 months, this research tool has been used to capture more than 800 physical prototypes from 76 individual users across many projects. In this article, one project is shown in detail to demonstrate how this capturing system can gather empirical data for enriching engineering design project cases that focus on prototyping for concept generation. The authors also analyse the metadata provided by the system to give understanding into prototyping patterns in the projects. Lastly, through enabling digital capture of large quantities of data, the research tool presents the foundations for training artificial intelligence-based predictors and classifiers that can be used for analysis in engineering design research. △ Less","19 March, 2020",https://arxiv.org/pdf/1905.01950
Internet of Intelligence: The Collective Advantage for Advancing Communications and Intelligence,Rongpeng Li;Zhifeng Zhao;Xing Xu;Fei Ni;Honggang Zhang,"The fifth-generation cellular networks (5G) has boosted the unprecedented convergence between the information world and physical world. On the other hand, empowered with the enormous amount of data and information, artificial intelligence (AI) has been universally applied and pervasive AI is believed to be an integral part of the six-generation cellular networks (6G). Consequently, benefiting from the advancement in communication technology and AI, we boldly argue that the conditions for collective intelligence (CI) will be mature in the 6G era and CI will emerge among the widely connected beings and things. Afterwards, we highlight the potential huge impact of CI on both communications and intelligence. In particular, we introduce a regular language (i.e., the information economy metalanguage) supporting the future collective communications to augment human intelligence and explain its potential applications in naming Internet information and pushing information centric networks forward. Meanwhile, we propose a stigmergy-based federated collective intelligence and demonstrate its achievement in a simulated scenario where the agents collectively work together to form a pattern through simple indirect communications. In a word, CI could advance both communications and intelligence. △ Less","18 April, 2020",https://arxiv.org/pdf/1905.00719
Causality Extraction based on Self-Attentive BiLSTM-CRF with Transferred Embeddings,Zhaoning Li;Qi Li;Xiaotian Zou;Jiangtao Ren,"Causality extraction from natural language texts is a challenging open problem in artificial intelligence. Existing methods utilize patterns, constraints, and machine learning techniques to extract causality, heavily depending on domain knowledge and requiring considerable human effort and time for feature engineering. In this paper, we formulate causality extraction as a sequence labeling problem based on a novel causality tagging scheme. On this basis, we propose a neural causality extractor with the BiLSTM-CRF model as the backbone, named SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings), which can directly extract cause and effect without extracting candidate causal pairs and identifying their relations separately. To address the problem of data insufficiency, we transfer contextual string embeddings, also known as Flair embeddings, which are trained on a large corpus in our task. In addition, to improve the performance of causality extraction, we introduce a multihead self-attention mechanism into SCITE to learn the dependencies between causal words. We evaluate our method on a public dataset, and experimental results demonstrate that our method achieves significant and consistent improvement compared to baselines. △ Less","8 November, 2020",https://arxiv.org/pdf/1904.07629
DLBC: A Deep Learning-Based Consensus in Blockchains for Deep Learning Services,Boyang Li;Changhao Chenli;Xiaowei Xu;Yiyu Shi;Taeho Jung,"With the increasing artificial intelligence application, deep neural network (DNN) has become an emerging task. However, to train a good deep learning model will suffer from enormous computation cost and energy consumption. Recently, blockchain has been widely used, and during its operation, a huge amount of computation resources are wasted for the Proof of Work (PoW) consensus. In this paper, we propose DLBC to exploit the computation power of miners for deep learning training as proof of useful work instead of calculating hash values. it distinguishes itself from recent proof of useful work mechanisms by addressing various limitations of them. Specifically, DLBC handles multiple tasks, larger model and training datasets, and introduces a comprehensive ranking mechanism that considers tasks difficulty(e.g., model complexity, network burden, data size, queue length). We also applied DNN-watermark [1] to improve the robustness. In Section V, the average overhead of digital signature is 1.25, 0.001, 0.002 and 0.98 seconds, respectively, and the average overhead of network is 3.77, 3.01, 0.37 and 0.41 seconds, respectively. Embedding a watermark takes 3 epochs and removing a watermark takes 30 epochs. This penalty of removing watermark will prevent attackers from stealing, improving, and resubmitting DL models from honest miners. △ Less","30 January, 2020",https://arxiv.org/pdf/1904.07349
A review on Neural Turing Machine,Soroor Malekmohammadi Faradonbeh;Faramarz Safi-Esfahani,"One of the major objectives of Artificial Intelligence is to design learning algorithms that are executed on a general purposes computational machines such as human brain. Neural Turing Machine (NTM) is a step towards realizing such a computational machine. The attempt is made here to run a systematic review on Neural Turing Machine. First, the mind-map and taxonomy of machine learning, neural networks, and Turing machine are introduced. Next, NTM is inspected in terms of concepts, structure, variety of versions, implemented tasks, comparisons, etc. Finally, the paper discusses on issues and ends up with several future works. △ Less","15 November, 2020",https://arxiv.org/pdf/1904.05061
Do Not Trust Additive Explanations,Alicja Gosiewska;Przemyslaw Biecek,"Explainable Artificial Intelligence (XAI)has received a great deal of attention recently. Explainability is being presented as a remedy for the distrust of complex and opaque models. Model agnostic methods such as LIME, SHAP, or Break Down promise instance-level interpretability for any complex machine learning model. But how faithful are these additive explanations? Can we rely on additive explanations for non-additive models? In this paper, we (1) examine the behavior of the most popular instance-level explanations under the presence of interactions, (2) introduce a new method that detects interactions for instance-level explanations, (3) perform a large scale benchmark to see how frequently additive explanations may be misleading. △ Less","8 May, 2020",https://arxiv.org/pdf/1903.11420
A Conceptual Bio-Inspired Framework for the Evolution of Artificial General Intelligence,Sidney Pontes-Filho;Stefano Nichele,"In this work, a conceptual bio-inspired parallel and distributed learning framework for the emergence of general intelligence is proposed, where agents evolve through environmental rewards and learn throughout their lifetime without supervision, i.e., self-learning through embodiment. The chosen control mechanism for agents is a biologically plausible neuron model based on spiking neural networks. Network topologies become more complex through evolution, i.e., the topology is not fixed, while the synaptic weights of the networks cannot be inherited, i.e., newborn brains are not trained and have no innate knowledge of the environment. What is subject to the evolutionary process is the network topology, the type of neurons, and the type of learning. This process ensures that controllers that are passed through the generations have the intrinsic ability to learn and adapt during their lifetime in mutable environments. We envision that the described approach may lead to the emergence of the simplest form of artificial general intelligence. △ Less","22 September, 2020",https://arxiv.org/pdf/1903.10410
ToyArchitecture: Unsupervised Learning of Interpretable Models of the World,Jaroslav Vítků;Petr Dluhoš;Joseph Davidson;Matěj Nikl;Simon Andersson;Přemysl Paška;Jan Šinkora;Petr Hlubuček;Martin Stránský;Martin Hyben;Martin Poliak;Jan Feyereisl;Marek Rosa,"Research in Artificial Intelligence (AI) has focused mostly on two extremes: either on small improvements in narrow AI domains, or on universal theoretical frameworks which are usually uncomputable, incompatible with theories of biological intelligence, or lack practical implementations. The goal of this work is to combine the main advantages of the two: to follow a big picture view, while providing a particular theory and its implementation. In contrast with purely theoretical approaches, the resulting architecture should be usable in realistic settings, but also form the core of a framework containing all the basic mechanisms, into which it should be easier to integrate additional required functionality. In this paper, we present a novel, purposely simple, and interpretable hierarchical architecture which combines multiple different mechanisms into one system: unsupervised learning of a model of the world, learning the influence of one's own actions on the world, model-based reinforcement learning, hierarchical planning and plan execution, and symbolic/sub-symbolic integration in general. The learned model is stored in the form of hierarchical representations with the following properties: 1) they are increasingly more abstract, but can retain details when needed, and 2) they are easy to manipulate in their local and symbolic-like form, thus also allowing one to observe the learning process at each level of abstraction. On all levels of the system, the representation of the data can be interpreted in both a symbolic and a sub-symbolic manner. This enables the architecture to learn efficiently using sub-symbolic methods and to employ symbolic inference. △ Less","9 September, 2020",https://arxiv.org/pdf/1903.08772
Lemotif: An Affective Visual Journal Using Deep Neural Networks,X. Alice Li;Devi Parikh,"We present Lemotif, an integrated natural language processing and image generation system that uses machine learning to (1) parse a text-based input journal entry describing the user's day for salient themes and emotions and (2) visualize the detected themes and emotions in creative and appealing image motifs. Synthesizing approaches from artificial intelligence and psychology, Lemotif acts as an affective visual journal, encouraging users to regularly write and reflect on their daily experiences through visual reinforcement. By making patterns in emotions and their sources more apparent, Lemotif aims to help users better understand their emotional lives, identify opportunities for action, and track the effectiveness of behavioral changes over time. We verify via human studies that prospective users prefer motifs generated by Lemotif over corresponding baselines, find the motifs representative of their journal entries, and think they would be more likely to journal regularly using a Lemotif-based app. △ Less","1 April, 2020",https://arxiv.org/pdf/1903.07766
Artificial intelligence in cyber physical systems,Petar Radanliev;David De Roure;Max Van Kleek;Omar Santos;Uchenna Ani,"This article conducts a literature review of current and future challenges in the use of artificial intelligence (AI) in cyber physical systems. The literature review is focused on identifying a conceptual framework for increasing resilience with AI through automation supporting both, a technical and human level. The methodology applied resembled a literature review and taxonomic analysis of complex internet of things (IoT) interconnected and coupled cyber physical systems. There is an increased attention on propositions on models, infrastructures and frameworks of IoT in both academic and technical papers. These reports and publications frequently represent a juxtaposition of other related systems and technologies (e.g. Industrial Internet of Things, Cyber Physical Systems, Industry 4.0 etc.). We review academic and industry papers published between 2010 and 2020. The results determine a new hierarchical cascading conceptual framework for analysing the evolution of AI decision-making in cyber physical systems. We argue that such evolution is inevitable and autonomous because of the increased integration of connected devices (IoT) in cyber physical systems. To support this argument, taxonomic methodology is adapted and applied for transparency and justifications of concepts selection decisions through building summary maps that are applied for designing the hierarchical cascading conceptual framework. △ Less","12 September, 2020",https://arxiv.org/pdf/1903.04369
Open-Sourced Reinforcement Learning Environments for Surgical Robotics,Florian Richter;Ryan K. Orosco;Michael C. Yip,"Reinforcement Learning (RL) is a machine learning framework for artificially intelligent systems to solve a variety of complex problems. Recent years has seen a surge of successes solving challenging games and smaller domain problems, including simple though non-specific robotic manipulation and grasping tasks. Rapid successes in RL have come in part due to the strong collaborative effort by the RL community to work on common, open-sourced environment simulators such as OpenAI's Gym that allow for expedited development and valid comparisons between different, state-of-art strategies. In this paper, we aim to start the bridge between the RL and the surgical robotics communities by presenting the first open-sourced reinforcement learning environments for surgical robots, called dVRL[3]{dVRL available at https://github.com/ucsdarclab/dVRL}. Through the proposed RL environments, which are functionally equivalent to Gym, we show that it is easy to prototype and implement state-of-art RL algorithms on surgical robotics problems that aim to introduce autonomous robotic precision and accuracy to assisting, collaborative, or repetitive tasks during surgery. Learned policies are furthermore successfully transferable to a real robot. Finally, combining dVRL with the over 40+ international network of da Vinci Surgical Research Kits in active use at academic institutions, we see dVRL as enabling the broad surgical robotics community to fully leverage the newest strategies in reinforcement learning, and for reinforcement learning scientists with no knowledge of surgical robotics to test and develop new algorithms that can solve the real-world, high-impact challenges in autonomous surgery. △ Less","27 January, 2020",https://arxiv.org/pdf/1903.02090
Contextual Word Representations: A Contextual Introduction,Noah A. Smith,"This introduction aims to tell the story of how we put words into computers. It is part of the story of the field of natural language processing (NLP), a branch of artificial intelligence. It targets a wide audience with a basic understanding of computer programming, but avoids a detailed mathematical treatment, and it does not present any algorithms. It also does not focus on any particular application of NLP such as translation, question answering, or information extraction. The ideas presented here were developed by many researchers over many decades, so the citations are not exhaustive but rather direct the reader to a handful of papers that are, in the author's view, seminal. After reading this document, you should have a general understanding of word vectors (also known as word embeddings): why they exist, what problems they solve, where they come from, how they have changed over time, and what some of the open questions about them are. Readers already familiar with word vectors are advised to skip to Section 5 for the discussion of the most recent advance, contextual word vectors. △ Less","17 April, 2020",https://arxiv.org/pdf/1902.06006
Seizure Type Classification using EEG signals and Machine Learning: Setting a benchmark,Subhrajit Roy;Umar Asif;Jianbin Tang;Stefan Harrer,"Accurate classification of seizure types plays a crucial role in the treatment and disease management of epileptic patients. Epileptic seizure types not only impact the choice of drugs but also the range of activities a patient can safely engage in. With recent advances being made towards artificial intelligence enabled automatic seizure detection, the next frontier is the automatic classification of seizure types. On that note, in this paper, we explore the application of machine learning algorithms for multi-class seizure type classification. We used the recently released TUH EEG seizure corpus (V1.4.0 and V1.5.2) and conducted a thorough search space exploration to evaluate the performance of a combination of various pre-processing techniques, machine learning algorithms, and corresponding hyperparameters on this task. We show that our algorithms can reach a weighted F1 score of up to 0.901 for seizure-wise cross validation and 0.561 for patient-wise cross validation thereby setting a benchmark for scalp EEG based multi-class seizure type classification. △ Less","11 August, 2020",https://arxiv.org/pdf/1902.01012
Machine Learning and Deep Learning Algorithms for Bearing Fault Diagnostics -- A Comprehensive Review,Shen Zhang;Shibo Zhang;Bingnan Wang;Thomas G. Habetler,"In this survey paper, we systematically summarize existing literature on bearing fault diagnostics with machine learning (ML) and data mining techniques. While conventional ML methods, including artificial neural network (ANN), principal component analysis (PCA), support vector machines (SVM), etc., have been successfully applied to the detection and categorization of bearing faults for decades, recent developments in deep learning (DL) algorithms in the last five years have sparked renewed interest in both industry and academia for intelligent machine health monitoring. In this paper, we first provide a brief review of conventional ML methods, before taking a deep dive into the state-of-the-art DL algorithms for bearing fault applications. Specifically, the superiority of DL based methods over conventional ML methods are analyzed in terms of fault feature extraction and classification performances; many new functionalities enabled by DL techniques are also summarized. In addition, to obtain a more intuitive insight, a comparative study is conducted on the classification accuracy of different algorithms utilizing the open-source Case Western Reserve University (CWRU) bearing dataset. Finally, to facilitate the transition on applying various DL algorithms to bearing fault diagnostics, detailed recommendations and suggestions are provided for specific application conditions such as the setup environment, the data size, and the number of sensors and sensor types. Future research directions to further enhance the performance of DL algorithms on health monitoring are also discussed. △ Less","6 February, 2020",https://arxiv.org/pdf/1901.08247
"Bonseyes AI Pipeline -- bringing AI to you. End-to-end integration of data, algorithms and deployment tools",Miguel de Prado;Jing Su;Rabia Saeed;Lorenzo Keller;Noelia Vallez;Andrew Anderson;David Gregg;Luca Benini;Tim Llewellynn;Nabil Ouerhani;Rozenn Dahyot and;Nuria Pazos,"Next generation of embedded Information and Communication Technology (ICT) systems are collaborative systems able to perform autonomous tasks. The remarkable expansion of the embedded ICT market, together with the rise and breakthroughs of Artificial Intelligence (AI), have put the focus on the Edge as it stands as one of the keys for the next technological revolution: the seamless integration of AI in our daily life. However, training and deployment of custom AI solutions on embedded devices require a fine-grained integration of data, algorithms, and tools to achieve high accuracy. Such integration requires a high level of expertise that becomes a real bottleneck for small and medium enterprises wanting to deploy AI solutions on the Edge which, ultimately, slows down the adoption of AI on daily-life applications. In this work, we present a modular AI pipeline as an integrating framework to bring data, algorithms, and deployment tools together. By removing the integration barriers and lowering the required expertise, we can interconnect the different stages of tools and provide a modular end-to-end development of AI products for embedded devices. Our AI pipeline consists of four modular main steps: i) data ingestion, ii) model training, iii) deployment optimization and, iv) the IoT hub integration. To show the effectiveness of our pipeline, we provide examples of different AI applications during each of the steps. Besides, we integrate our deployment framework, LPDNN, into the AI pipeline and present its lightweight architecture and deployment capabilities for embedded devices. Finally, we demonstrate the results of the AI pipeline by showing the deployment of several AI applications such as keyword spotting, image classification and object detection on a set of well-known embedded platforms, where LPDNN consistently outperforms all other popular deployment frameworks. △ Less","11 June, 2020",https://arxiv.org/pdf/1901.05049
Comparing Knowledge-based Reinforcement Learning to Neural Networks in a Strategy Game,Liudmyla Nechepurenko;Viktor Voss;Vyacheslav Gritsenko,"The paper reports on an experiment, in which a Knowledge-Based Reinforcement Learning (KB-RL) method was compared to a Neural Network (NN) approach in solving a classical Artificial Intelligence (AI) task. In contrast to NNs, which require a substantial amount of data to learn a good policy, the KB-RL method seeks to encode human knowledge into the solution, considerably reducing the amount of data needed for a good policy. By means of Reinforcement Learning (RL), KB-RL learns to optimize the model and improves the output of the system. Furthermore, KB-RL offers the advantage of a clear explanation of the taken decisions as well as transparent reasoning behind the solution. The goal of the reported experiment was to examine the performance of the KB-RL method in contrast to the Neural Network and to explore the capabilities of KB-RL to deliver a strong solution for the AI tasks. The results show that, within the designed settings, KB-RL outperformed the NN, and was able to learn a better policy from the available amount of data. These results support the opinion that Artificial Intelligence can benefit from the discovery and study of alternative approaches, potentially extending the frontiers of AI. △ Less","17 January, 2020",https://arxiv.org/pdf/1901.04626
Creative AI Through Evolutionary Computation,Risto Miikkulainen,"The main power of artificial intelligence is not in modeling what we already know, but in creating solutions that are new. Such solutions exist in extremely large, high-dimensional, and complex search spaces. Population-based search techniques, i.e. variants of evolutionary computation, are well suited to finding them. These techniques are also well positioned to take advantage of large-scale parallel computing resources, making creative AI through evolutionary computation the likely ""next deep learning"". △ Less","22 February, 2020",https://arxiv.org/pdf/1901.03775
HyPar: Towards Hybrid Parallelism for Deep Learning Accelerator Array,Linghao Song;Jiachen Mao;Youwei Zhuo;Xuehai Qian;Hai Li;Yiran Chen,"With the rise of artificial intelligence in recent years, Deep Neural Networks (DNNs) have been widely used in many domains. To achieve high performance and energy efficiency, hardware acceleration (especially inference) of DNNs is intensively studied both in academia and industry. However, we still face two challenges: large DNN models and datasets, which incur frequent off-chip memory accesses; and the training of DNNs, which is not well-explored in recent accelerator designs. To truly provide high throughput and energy efficient acceleration for the training of deep and large models, we inevitably need to use multiple accelerators to explore the coarse-grain parallelism, compared to the fine-grain parallelism inside a layer considered in most of the existing architectures. It poses the key research question to seek the best organization of computation and dataflow among accelerators. In this paper, inspired by recent work in machine learning systems, we propose a solution HyPar to determine layer-wise parallelism for deep neural network training with an array of DNN accelerators. HyPar partitions the feature map tensors (input and output), the kernel tensors, the gradient tensors, and the error tensors for the DNN accelerators. A partition constitutes the choice of parallelism for weighted layers. The optimization target is to search a partition that minimizes the total communication during training a complete DNN. To solve this problem, we propose a communication model to explain the source and amount of communications. Then, we use a hierarchical layer-wise dynamic programming method to search for the partition for each layer. △ Less","16 January, 2020",https://arxiv.org/pdf/1901.02067
A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems,Sina Mohseni;Niloofar Zarei;Eric D. Ragan,"The need for interpretable and accountable intelligent systems grows along with the prevalence of artificial intelligence applications used in everyday life. Explainable intelligent systems are designed to self-explain the reasoning behind system decisions and predictions, and researchers from different disciplines work together to define, design, and evaluate interpretable systems. However, scholars from different disciplines focus on different objectives and fairly independent topics of interpretable machine learning research, which poses challenges for identifying appropriate design and evaluation methodology and consolidating knowledge across efforts. To this end, this paper presents a survey and framework intended to share knowledge and experiences of XAI design and evaluation methods across multiple disciplines. Aiming to support diverse design goals and evaluation methods in XAI research, after a thorough review of XAI related papers in the fields of machine learning, visualization, and human-computer interaction, we present a categorization of interpretable machine learning design goals and evaluation methods to show a mapping between design goals for different XAI user groups and their evaluation methods. From our findings, we develop a framework with step-by-step design guidelines paired with evaluation methods to close the iterative design and evaluation cycles in multidisciplinary XAI teams. Further, we provide summarized ready-to-use tables of evaluation methods and recommendations for different goals in XAI research. △ Less","5 August, 2020",https://arxiv.org/pdf/1811.11839
Contrastive Explanation: A Structural-Model Approach,Tim Miller,"This paper presents a model of contrastive explanation using structural casual models. The topic of causal explanation in artificial intelligence has gathered interest in recent years as researchers and practitioners aim to increase trust and understanding of intelligent decision-making. While different sub-fields of artificial intelligence have looked into this problem with a sub-field-specific view, there are few models that aim to capture explanation more generally. One general model is based on structural causal models. It defines an explanation as a fact that, if found to be true, would constitute an actual cause of a specific event. However, research in philosophy and social sciences shows that explanations are contrastive: that is, when people ask for an explanation of an event -- the fact -- they (sometimes implicitly) are asking for an explanation relative to some contrast case; that is, ""Why P rather than Q?"". In this paper, we extend the structural causal model approach to define two complementary notions of contrastive explanation, and demonstrate them on two classical problems in artificial intelligence: classification and planning. We believe that this model can help researchers in subfields of artificial intelligence to better understand contrastive explanation. △ Less","2 December, 2020",https://arxiv.org/pdf/1811.03163
"Genie: A Secure, Transparent Sharing and Services Platform for Genetic and Health Data",Shifa Zhang;Anne Kim;Dianbo Liu;Sandeep C. Nuckchady;Lauren Huang;Aditya Masurkar;Jingwei Zhang;Lawrence Tseng;Pratheek Karnati;Laura Martinez;Thomas Hardjono;Manolis Kellis;Zhizhuo Zhang,"Artificial Intelligence (AI) incorporating genetic and medical information have been applied in disease risk prediction, unveiling disease mechanism, and advancing therapeutics. However, AI training relies on highly sensitive and private data which significantly limit their applications and robustness evaluation. Moreover, the data access management after sharing across organization heavily relies on legal restriction, and there is no guarantee in preventing data leaking after sharing. Here, we present Genie, a secure AI platform which allows AI models to be trained on medical data securely. The platform combines the security of Intel Software Guarded eXtensions (SGX), transparency of blockchain technology, and verifiability of open algorithms and source codes. Genie shares insights of genetic and medical data without exposing anyone's raw data. All data is instantly encrypted upon upload and contributed to the models that the user chooses. The usage of the model and the value generated from the genetic and health data will be tracked via a blockchain, giving the data transparent and immutable ownership. △ Less","30 March, 2020",https://arxiv.org/pdf/1811.01431
Actor-Critic Policy Optimization in Partially Observable Multiagent Environments,Sriram Srinivasan;Marc Lanctot;Vinicius Zambaldi;Julien Perolat;Karl Tuyls;Remi Munos;Michael Bowling,"Optimization of parameterized policies for reinforcement learning (RL) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using RL-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero sum games, without any domain-specific state space reductions. △ Less","12 June, 2020",https://arxiv.org/pdf/1810.09026
Analysis of the Generalization Error: Empirical Risk Minimization over Deep Artificial Neural Networks Overcomes the Curse of Dimensionality in the Numerical Approximation of Black-Scholes Partial Differential Equations,Julius Berner;Philipp Grohs;Arnulf Jentzen,"The development of new classification and regression algorithms based on empirical risk minimization (ERM) over deep neural network hypothesis classes, coined deep learning, revolutionized the area of artificial intelligence, machine learning, and data analysis. In particular, these methods have been applied to the numerical solution of high-dimensional partial differential equations with great success. Recent simulations indicate that deep learning-based algorithms are capable of overcoming the curse of dimensionality for the numerical solution of Kolmogorov equations, which are widely used in models from engineering, finance, and the natural sciences. The present paper considers under which conditions ERM over a deep neural network hypothesis class approximates the solution of a d-dimensional Kolmogorov equation with affine drift and diffusion coefficients and typical initial values arising from problems in computational finance up to error \varepsilon. We establish that, with high probability over draws of training samples, such an approximation can be achieved with both the size of the hypothesis class and the number of training samples scaling only polynomially in d and \varepsilon^{-1}. It can be concluded that ERM over deep neural network hypothesis classes overcomes the curse of dimensionality for the numerical solution of linear Kolmogorov equations with affine coefficients. △ Less","11 November, 2020",https://arxiv.org/pdf/1809.03062
Unity: A General Platform for Intelligent Agents,Arthur Juliani;Vincent-Pierre Berges;Ervin Teng;Andrew Cohen;Jonathan Harper;Chris Elion;Chris Goy;Yuan Gao;Hunter Henry;Marwan Mattar;Danny Lange,"Recent advances in artificial intelligence have been driven by the presence of increasingly realistic and complex simulated environments. However, many of the existing environments provide either unrealistic visuals, inaccurate physics, low task complexity, restricted agent perspective, or a limited capacity for interaction among artificial agents. Furthermore, many platforms lack the ability to flexibly configure the simulation, making the simulated environment a black-box from the perspective of the learning system. In this work, we propose a novel taxonomy of existing simulation platforms and discuss the highest level class of general platforms which enable the development of learning environments that are rich in visual, physical, task, and social complexity. We argue that modern game engines are uniquely suited to act as general platforms and as a case study examine the Unity engine and open source Unity ML-Agents Toolkit. We then survey the research enabled by Unity and the Unity ML-Agents Toolkit, discussing the kinds of research a flexible, interactive and easily configurable general platform can facilitate. △ Less","6 May, 2020",https://arxiv.org/pdf/1809.02627
Optimal design of experiments to identify latent behavioral types,Stefano Balietti;Brennan Klein;Christoph Riedl,"Bayesian optimal experiments that maximize the information gained from collected data are critical to efficiently identify behavioral models. We extend a seminal method for designing Bayesian optimal experiments by introducing two computational improvements that make the procedure tractable: (1) a search algorithm from artificial intelligence that efficiently explores the space of possible design parameters, and (2) a sampling procedure which evaluates each design parameter combination more efficiently. We apply our procedure to a game of imperfect information to evaluate and quantify the computational improvements. We then collect data across five different experimental designs to compare the ability of the optimal experimental design to discriminate among competing behavioral models against the experimental designs chosen by a ""wisdom of experts"" prediction experiment. We find that data from the experiment suggested by the optimal design approach requires significantly less data to distinguish behavioral models (i.e., test hypotheses) than data from the experiment suggested by experts. Substantively, we find that reinforcement learning best explains human decision-making in the imperfect information game and that behavior is not adequately described by the Bayesian Nash equilibrium. Our procedure is general and computationally efficient and can be applied to dynamically optimize online experiments. △ Less","6 August, 2020",https://arxiv.org/pdf/1807.07024
Using Reinforcement Learning with Partial Vehicle Detection for Intelligent Traffic Signal Control,Rusheng Zhang;Akihiro Ishikawa;Wenli Wang;Benjamin Striner;Ozan Tonguz,"Intelligent Transportation Systems (ITS) have attracted the attention of researchers and the general public alike as a means to alleviate traffic congestion. Recently, the maturity of wireless technology has enabled a cost-efficient way to achieve ITS by detecting vehicles using Vehicle to Infrastructure (V2I) communications. Traditional ITS algorithms, in most cases, assume that every vehicle is observed, such as by a camera or a loop detector, but a V2I implementation would detect only those vehicles with wireless communications capability. We examine a family of transportation systems, which we will refer to as `Partially Detected Intelligent Transportation Systems'. An algorithm that can act well under a small detection rate is highly desirable due to gradual penetration rates of the underlying wireless technologies such as Dedicated Short Range Communications (DSRC) technology. Artificial Intelligence (AI) techniques for Reinforcement Learning (RL) are suitable tools for finding such an algorithm due to utilizing varied inputs and not requiring explicit analytic understanding or modeling of the underlying system dynamics. In this paper, we report a RL algorithm for partially observable ITS based on DSRC. The performance of this system is studied under different car flows, detection rates, and topologies of the road network. Our system is able to efficiently reduce the average waiting time of vehicles at an intersection, even with a low detection rate. △ Less","28 February, 2020",https://arxiv.org/pdf/1807.01628
Explainable AI as a Social Microscope: A Case Study on Academic Performance,Anahit Sargsyan;Areg Karapetyan;Wei Lee Woon;Aamena Alshamsi,"Academic performance is perceived as a product of complex interactions between students' overall experience, personal characteristics and upbringing. Data science techniques, most commonly involving regression analysis and related approaches, serve as a viable means to explore this interplay. However, these tend to extract factors with wide-ranging impact, while overlooking variations specific to individual students. Focusing on each student's peculiarities is generally impossible with thousands or even hundreds of subjects, yet data mining methods might prove effective in devising more targeted approaches. For instance, subjects with shared characteristics can be assigned to clusters, which can then be examined separately with machine learning algorithms, thereby providing a more nuanced view of the factors affecting individuals in a particular group. In this context, we introduce a data science workflow allowing for fine-grained analysis of academic performance correlates that captures the subtle differences in students' sensitivities to these factors. Leveraging the Local Interpretable Model-Agnostic Explanations (LIME) algorithm from the toolbox of Explainable Artificial Intelligence (XAI) techniques, the proposed pipeline yields groups of students having similar academic attainment indicators, rather than similar features (e.g. familial background) as typically practiced in prior studies. As a proof-of-concept case study, a rich longitudinal dataset is selected to evaluate the effectiveness of the proposed approach versus a standard regression model. △ Less","4 June, 2020",https://arxiv.org/pdf/1806.02615
Deep Nets: What have they ever done for Vision?,Alan L. Yuille;Chenxi Liu,"This is an opinion paper about the strengths and weaknesses of Deep Nets for vision. They are at the heart of the enormous recent progress in artificial intelligence and are of growing importance in cognitive science and neuroscience. They have had many successes but also have several limitations and there is limited understanding of their inner workings. At present Deep Nets perform very well on specific visual tasks with benchmark datasets but they are much less general purpose, flexible, and adaptive than the human visual system. We argue that Deep Nets in their current form are unlikely to be able to overcome the fundamental problem of computer vision, namely how to deal with the combinatorial explosion, caused by the enormous complexity of natural images, and obtain the rich understanding of visual scenes that the human visual achieves. We argue that this combinatorial explosion takes us into a regime where ""big data is not enough"" and where we need to rethink our methods for benchmarking performance and evaluating vision algorithms. We stress that, as vision algorithms are increasingly used in real world applications, that performance evaluation is not merely an academic exercise but has important consequences in the real world. It is impractical to review the entire Deep Net literature so we restrict ourselves to a limited range of topics and references which are intended as entry points into the literature. The views expressed in this paper are our own and do not necessarily represent those of anybody else in the computer vision community. △ Less","25 November, 2020",https://arxiv.org/pdf/1805.04025
But Who Protects the Moderators? The Case of Crowdsourced Image Moderation,Brandon Dang;Martin J. Riedl;Matthew Lease,"Though detection systems have been developed to identify obscene content such as pornography and violence, artificial intelligence is simply not good enough to fully automate this task yet. Due to the need for manual verification, social media companies may hire internal reviewers, contract specialized workers from third parties, or outsource to online labor markets for the purpose of commercial content moderation. These content moderators are often fully exposed to extreme content and may suffer lasting psychological and emotional damage. In this work, we aim to alleviate this problem by investigating the following question: How can we reveal the minimum amount of information to a human reviewer such that an objectionable image can still be correctly identified? We design and conduct experiments in which blurred graphic and non-graphic images are filtered by human moderators on Amazon Mechanical Turk (AMT). We observe how obfuscation affects the moderation experience with respect to image classification accuracy, interface usability, and worker emotional well-being. △ Less","4 January, 2020",https://arxiv.org/pdf/1804.10999
Fractal AI: A fragile theory of intelligence,Sergio Hernandez Cerezo;Guillem Duran Ballester,"Fractal AI is a theory for general artificial intelligence. It allows deriving new mathematical tools that constitute the foundations for a new kind of stochastic calculus, by modelling information using cellular automaton-like structures instead of smooth functions. In the repository included we are presenting a new Agent, derived from the first principles of the theory, which is capable of solving Atari games several orders of magnitude more efficiently than other similar techniques, like Monte Carlo Tree Search. The code provided shows how it is now possible to beat some of the current State of The Art benchmarks on Atari games, without previous learning and using less than 1000 samples to calculate each one of the actions when standard MCTS uses 3 Million samples. Among other things, Fractal AI makes it possible to generate a huge database of top performing examples with a very little amount of computation required, transforming Reinforcement Learning into a supervised problem. The algorithm presented is capable of solving the exploration vs exploitation dilemma on both the discrete and continuous cases, while maintaining control over any aspect of the behaviour of the Agent. From a general approach, new techniques presented here have direct applications to other areas such as Non-equilibrium thermodynamics, chemistry, quantum physics, economics, information theory, and non-linear control theory. △ Less","30 July, 2020",https://arxiv.org/pdf/1803.05049
Multilayered Model of Speech,Andrey Chistyakov,"Human speech is the most important part of General Artificial Intelligence and subject of much research. The hypothesis proposed in this article provides explanation of difficulties that modern science tackles in the field of human brain simulation. The hypothesis is based on the author's conviction that the brain of any given person has different ability to process and store information. Therefore, the approaches that are currently used to create General Artificial Intelligence have to be altered. △ Less","10 February, 2020",https://arxiv.org/pdf/1801.04170
Solving Integer Linear Programs with a Small Number of Global Variables and Constraints,Pavel Dvořák;Eduard Eiben;Robert Ganian;Dušan Knop;Sebastian Ordyniak,"Integer Linear Programming (ILP) has a broad range of applications in various areas of artificial intelligence. Yet in spite of recent advances, we still lack a thorough understanding of which structural restrictions make ILP tractable. Here we study ILP instances consisting of a small number of ""global"" variables and/or constraints such that the remaining part of the instance consists of small and otherwise independent components; this is captured in terms of a structural measure we call fracture backdoors which generalizes, for instance, the well-studied class of N -fold ILP instances. Our main contributions can be divided into three parts. First, we formally develop fracture backdoors and obtain exact and approximation algorithms for computing these. Second, we exploit these backdoors to develop several new parameterized algorithms for ILP; the performance of these algorithms will naturally scale based on the number of global variables or constraints in the instance. Finally, we complement the developed algorithms with matching lower bounds. Altogether, our results paint a near-complete complexity landscape of ILP with respect to fracture backdoors. △ Less","16 March, 2020",https://arxiv.org/pdf/1706.06084
A Popperian Falsification of Artificial Intelligence -- Lighthill Defended,Steven Meyer,The area of computation called artificial intelligence (AI) is falsified by describing a previous 1972 falsification of AI by British mathematical physicist James Lighthill. How Lighthill's arguments continue to apply to current AI is explained. It is argued that AI should use the Popperian scientific method in which it is the duty of scientists to attempt to falsify theories and if theories are falsified to replace or modify them. The paper describes the Popperian method and discusses Paul Nurse's application of the method to cell biology that also involves questions of mechanism and behavior. It is shown how Lighthill's falsifying arguments especially combinatorial explosion continue to apply to modern AI. Various skeptical arguments against the assumptions of AI mostly by physicists especially against Hilbert's philosophical programme that defined knowledge and truth as provable formal sentences. John von Neumann's arguments from natural complexity against neural networks and evolutionary algorithms are discussed. Next the game of chess is discussed to show how modern chess experts have reacted to computer chess programs. It is shown that currently chess masters can defeat any chess program using Kasperov's arguments from his 1997 Deep Blue match and aftermath. The game of 'go' and climate models are discussed to show computer applications where combinatorial explosion may not apply. The paper concludes by advocating studying computation as Peter Naur's Dataology. △ Less,"30 April, 2020",https://arxiv.org/pdf/1704.08111
Towards Verified Artificial Intelligence,Sanjit A. Seshia;Dorsa Sadigh;S. Shankar Sastry,"Verified artificial intelligence (AI) is the goal of designing AI-based systems that that have strong, ideally provable, assurances of correctness with respect to mathematically-specified requirements. This paper considers Verified AI from a formal methods perspective. We describe five challenges for achieving Verified AI, and five corresponding principles for addressing these challenges. △ Less","23 July, 2020",https://arxiv.org/pdf/1606.08514
A metric for sets of trajectories that is practical and mathematically consistent,José Bento;Jia Jie Zhu,"Metrics on the space of sets of trajectories are important for scientists in the field of computer vision, machine learning, robotics, and general artificial intelligence. However, existing notions of closeness between sets of trajectories are either mathematically inconsistent or of limited practical use. In this paper, we outline the limitations in the current mathematically-consistent metrics, which are based on OSPA (Schuhmacher et al. 2008); and the inconsistencies in the heuristic notions of closeness used in practice, whose main ideas are common to the CLEAR MOT measures (Keni and Rainer 2008) widely used in computer vision. In two steps, we then propose a new intuitive metric between sets of trajectories and address these limitations. First, we explain a solution that leads to a metric that is hard to compute. Then we modify this formulation to obtain a metric that is easy to compute while keeping the useful properties of the previous metric. Our notion of closeness is the first demonstrating the following three features: the metric 1) can be quickly computed, 2) incorporates confusion of trajectories' identity in an optimal way, and 3) is a metric in the mathematical sense. △ Less","14 November, 2020",https://arxiv.org/pdf/1601.03094
Mathematical Foundations for Designing and Development of Intelligent Systems of Information Analysis,D. O. Terletskyi;O. I. Provotar,"This article is an attempt to combine different ways of working with sets of objects and their classes for designing and development of artificial intelligent systems (AIS) of analysis information, using object-oriented programming (OOP). This paper contains analysis of basic concepts of OOP and their relation with set theory and artificial intelligence (AI). Process of sets and multisets creation from different sides, in particular mathematical set theory, OOP and AI is considered. Definition of object and its properties, homogeneous and inhomogeneous classes of objects, set of objects, multiset of objects and constructive methods of their creation and classification are proposed. In addition, necessity of some extension of existing OOP tools for the purpose of practical implementation AIS of analysis information, using proposed approach, is shown. △ Less","21 February, 2020",https://arxiv.org/pdf/1510.04183
